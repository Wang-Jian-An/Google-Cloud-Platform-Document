{"title": "Vertex AI - Configure execution caching", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Configure execution caching\nWhen Vertex AI Pipelines runs a pipeline, it checks to see whether or not an exists in Vertex ML Metadata with the interface (cache key) of each pipeline step.\nThe step's interface is defined as the combination of the following:\n- The **pipeline step's inputs** . These inputs include the input parameters' value (if any) and the input artifact id (if any).\n- The **pipeline step's output definition** . This output definition includes output parameter definition (name, if any) and output artifact definition (name, if any).\n- The **component's specification** . This specification includes the image, commands, arguments and environment variables being used, as well as the order of the commands and arguments.\nAdditionally, only the pipelines with the same pipeline name will share the cache.\nIf there is a matching execution in Vertex ML Metadata, the outputs of that execution are used and the step is skipped. This helps to reduce costs by skipping computations that were completed in a previous pipeline run.\nYou can turn off execution caching at task level by setting the following:\n```\neval_task.set_caching_options(False)\n```\nYou can turn off execution caching for an entire pipeline job. When you run a pipeline using `PipelineJob()` , you can use the `enable_caching` argument to specify that this pipeline run does not use caching. All steps within the pipeline job will not use caching. [Learn more about creating pipeline runs](/vertex-ai/docs/pipelines/run-pipeline) .\nUse the following sample to turn off caching:\n```\npl = PipelineJob(\u00a0 \u00a0 display_name=\"My first pipeline\",\u00a0 \u00a0 # Whether or not to enable caching\u00a0 \u00a0 # True = enable the current run to use caching results from previous runs\u00a0 \u00a0 # False = disable the current run's use of caching results from previous runs\u00a0 \u00a0 # None = defer to cache option for each pipeline component in the pipeline definition\u00a0 \u00a0 enable_caching=False,\u00a0 \u00a0 # Local or Cloud Storage path to a compiled pipeline definition\u00a0 \u00a0 template_path=\"pipeline.yaml\",\u00a0 \u00a0 # Dictionary containing input parameters for your pipeline\u00a0 \u00a0 parameter_values=parameter_values,\u00a0 \u00a0 # Cloud Storage path to act as the pipeline root\u00a0 \u00a0 pipeline_root=pipeline_root,)\n```\n**Important:** Pipeline components should be built to be deterministic. A given set of inputs should always produce the same output. Depending on their interface, non-deterministic pipeline components can be unexpectedly skipped due to execution caching.\nThe following limitations apply to this feature:\n- The cached result doesn't have a time-to-live (TTL), and can be reused as long as the entry is not deleted from the Vertex ML Metadata. If the entry is deleted from Vertex ML Metadata, the task will rerun to regenerate the result again", "content": ".", "guide": "Vertex AI"}