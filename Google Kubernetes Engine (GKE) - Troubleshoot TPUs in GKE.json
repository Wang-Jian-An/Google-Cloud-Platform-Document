{"title": "Google Kubernetes Engine (GKE) - Troubleshoot TPUs in GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview", "abstract": "# Google Kubernetes Engine (GKE) - Troubleshoot TPUs in GKE\nThis page shows you how to resolve issues related to TPUs in Google Kubernetes Engine (GKE).\n[Cloud Customer Care](/kubernetes-engine/docs/getting-support)\n", "content": "## Insufficient quota to satisfy the TPU request\nAn error similar to `Insufficient quota to satisfy the request` indicates your Google Cloud project has insufficient quota available to satisfy the request.\nTo resolve this issue, check your project's quota limit and current usage. If needed, request an increase to your TPU quota.\n### Check quota limit and current usage\nIf you are creating a TPU node pool with on-demand or Spot VMs, you must have sufficient TPU quota available in the region that you want to use.\nCreating a TPU node pool that consumes a TPU reservation does require any TPU quota. You may safely skip this section for reserved TPUs.\nCreating an on-demand or Spot TPU node pool in GKE requires Compute Engine API quota. Compute Engine API quota (compute.googleapis.com) is not the same as Cloud TPU API quota (tpu.googleapis.com), which is needed when creating TPUs with the Cloud TPU API.\nTo check the limit and current usage of your Compute Engine API quota for TPUs, follow these steps:\n- Go to the **Quotas** page in the Google Cloud console: [Go to Quotas](https://console.cloud.google.com/iam-admin/quotas) \n- In the filter_list **Filter** box, do the following:- Select the **Service** property, enter **Compute Engine API** , and press **Enter** .\n- Select the **Type** property and choose **Quota** .\n- Select the **Name** property and enter the name of the quota based on the TPU version and machine type. For example, if you plan to create on-demand TPU v5e nodes whose machine type begins with `ct5lp-` , enter `TPU v5 Lite PodSlice chips` .| TPU version | Machine type begins with | Name of the quota for on-demand instances | Name of the quota for Spot2 instances |\n|:--------------|:---------------------------|:--------------------------------------------|:----------------------------------------|\n| TPU v4  | ct4p-      | TPU v4 PodSlice chips      | Preemptible TPU v4 PodSlice chips  |\n| TPU v5e  | ct5l-      | TPU v5 Lite Device chips     | Preemptible TPU v5 Lite Device chips |\n| TPU v5e  | ct5lp-      | TPU v5 Lite PodSlice chips     | Preemptible TPU v5 Lite PodSlice chips |\n| TPU v5p  | ct5p-      | TPU v5p chips        | Preemptible TPU v5p chips    |\n- Select the **Dimensions (e.g. locations)** property and enter `region:` followed by the name of the region in which you plan to create TPUs in GKE. For instance, enter `region:us-west4` if you plan to create TPU nodes in the zone `us-west4-a` . TPU quota is regional, so all zones within the same region consume the same TPU quota.If no quotas match the filter you entered, then the project has not been granted any of the specified quota for the desired region, and you must [request a TPU quota increase](/docs/quota/view-manage#requesting_higher_quota) .\n**Note:** When a TPU reservation is created, both the limit and current use values for the corresponding quota increase by the number of chips in the TPU reservation. For example, when a reservation is created for 16 TPU v5e chips whose machine type begins with `ct5lp-` , then both the **Limit** and **Current usage** for the `TPU v5 Lite PodSlice chips` quota in the relevant region increase by 16.\n- When creating a TPU node pool, use the [--reservation  and --reservation-affinity=specific flags](/sdk/gcloud/reference/container/node-pools/create#--reservation) to  create a reserved instance. TPU reservations are available when  purchasing a commitment. [\u21a9](#fnref1) \n- When creating a TPU node pool, use the [--spot  flag](/sdk/gcloud/reference/container/node-pools/create#--spot) to create a [Spot](/spot-vms) instance. [\u21a9](#fnref2) ## Error when enabling node auto-provisioning in a TPU node pools\nThe following error occurs when you are enabling node auto-provisioning in a GKE cluster that doesn't support TPUs.\nThe error message is similar to the following:\n```\nERROR: (gcloud.container.clusters.create) ResponseError: code=400,\n message=Invalid resource: tpu-v4-podslice.\n```\nTo resolve this issue, [upgrade your GKE cluster to version 1.27.6 or later](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrading_the_cluster) .\n## GKE doesn't automatically provision TPU nodes\nThe following sections describe the cases where GKE doesn't automatically provision TPU nodes and how to fix them.\n### Limit misconfiguration\nGKE doesn't automatically provision TPU nodes if the auto-provisioning limits you defined for a cluster are too low. You may observe the following errors in such scenarios:\n- If a TPU node pool exists, but GKE can't scale up the nodes due to [violating resource limits](/kubernetes-engine/docs/troubleshooting/troubleshooting-autopilot-clusters#noscaleup-resource-limits) , you can see the following error message when running the `kubectl get events` command:```\n11s Normal NotTriggerScaleUp pod/tpu-workload-65b69f6c95-ccxwz pod didn't\ntrigger scale-up: 1 node(s) didn't match Pod's node affinity/selector, 1 max\ncluster cpu, memory limit reached\n```Also, in this scenario, you can see warning messages similar to the following in the Google Cloud console:```\n\"Your cluster has one or more unschedulable Pods\"\n```\n- When GKE attempts to auto-provision a TPU node pool that exceeds resource limits, the cluster autoscaler visibility logs will display the following error message:```\nmessageId: \"no.scale.up.nap.pod.zonal.resources.exceeded\"\n```Also, in this scenario, you can see warning messages similar to the following in the Google Cloud console:```\n\"Can't scale up because node auto-provisioning can't provision a node pool for\nthe Pod if it would exceed resource limits\"\n```\nTo resolve these issues, increase the maximum number of TPU chips, CPU cores, and memory in the cluster.\nTo complete these steps:\n- Calculate the resource requirements for a given TPU machine type and count. Note that you need to add resources for non-TPU node pools, like system workloads.\n- Obtain a description of the available TPU, CPU, and memory for a specific machine type and zone. Use the gcloud CLI:```\ngcloud compute machine-types describe MACHINE_TYPE \\\u00a0 \u00a0 --zone COMPUTE_ZONE\n```Replace the following:- ``: The type of machine to search.\n- ``: The name of the [compute zone](/compute/docs/regions-zones#available) .\nThe output includes a description line similar to the following:```\n description: 240 vCPUs, 407 GB RAM, 4 Google TPUs\n ```\n```\n- Calculate the total number of CPU and memory by multiplying these amounts by the required number of nodes. For example, the `ct4p-hightpu-4t` machine type uses 240 CPU cores and 407 GB RAM with 4 TPU chips. Assuming that you require 20 TPU chips, which corresponds to five nodes, you must define the following values:- `--max-accelerator=type=tpu-v4-podslice,count=20`.\n- `CPU = 1200`(240 times 5 )\n- `memory = 2035`(407 times 5)\nYou should define the limits with some margin to accommodate non-TPU nodes such as system workloads.\n- Update the cluster limits:```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --max-accelerator type=TPU_ACCELERATOR \\\u00a0 \u00a0 count=MAXIMUM_ACCELERATOR \\\u00a0 \u00a0 --max-cpu=MAXIMUM_CPU \\\u00a0 \u00a0 --max-memory=MAXIMUM_MEMORY\n```Replace the following:- ``: The name of the cluster.\n- ``: The name of the TPU accelerator.\n- ``: The maximum number of TPU chips in the cluster.\n- ``: The maximum number of cores in the cluster.\n- ``: The maximum number of gigabytes of memory in the cluster.\n### Workload misconfiguration\nThis error occurs due to misconfiguration of the workload. The following are some of the most common causes of the error:\n- The`cloud.google.com/gke-tpu-accelerator`and`cloud.google.com/gke-tpu-topology`labels are incorrect or missing in the Pod spec. GKE won't provision TPU node pools and the node auto-provision won't be able to scale up the cluster.\n- The Pod spec doesn't specify`google.com/tpu`in their resource requirements.\nTo resolve this issue do one of the following:\n- Check that there are no unsupported labels in your workload node selector. For example, a node selector for`cloud.google.com/gke-nodepool`label will prevent GKE from creating additional node pools for your Pods.\n- Ensure the Pod template specifications, where your TPU workload runs, include the following values:- `cloud.google.com/gke-tpu-accelerator`and`cloud.google.com/gke-tpu-topology`labels in its`nodeSelector`.\n- `google.com/tpu`in its request.To learn how to deploy TPU workloads in GKE, see [Run a workload that displays the number of available TPU chips in a TPU node pool](/kubernetes-engine/docs/how-to/tpus#tpu-chips-node-pool) .\n## Scheduling errors when deploying Pods that consume TPUs in GKE\nThe following issue occurs when GKE can't schedule Pods requesting TPUs on TPU nodes. For example, this might occur if some non-TPU Pods were already scheduled on TPU nodes.\nThe error message, emitted as a `FailedScheduling` event on the Pod, is similar to the following:\n```\nCannot schedule pods: Preemption is not helpful for scheduling.\nError message: 0/2 nodes are available: 2 node(s) had untolerated taint\n{google.com/tpu: present}. preemption: 0/2 nodes are available: 2 Preemption is\nnot helpful for scheduling\n```\nTo resolve this issue, do the following:\nCheck that you have at least one CPU node pool in your cluster so the system critical Pods can run in the non-TPU nodes. To learn more, see [Deploy a Pod to a specific node pool](/kubernetes-engine/docs/how-to/node-pools#deploy) .\n## TPU initialization failed\nThe following issue occurs when GKE can't provision new TPU workloads due to lack of permission to access TPU devices.\nThe error message is similar to the following:\n```\nTPU platform initialization failed: FAILED_PRECONDITION: Couldn't mmap: Resource\ntemporarily unavailable.; Unable to create Node RegisterInterface for node 0,\nconfig: device_path: \"/dev/accel0\" mode: KERNEL debug_data_directory: \"\"\ndump_anomalies_only: true crash_in_debug_dump: false allow_core_dump: true;\ncould not create driver instance\n```\nTo resolve this issue, make sure you either run your TPU container in privileged mode or you increase the [ulimit inside your container](/kubernetes-engine/docs/how-to/tpus#privileged-mode) .\n## Scheduling deadlock\nTwo or more Jobs scheduling might fail in deadlock. For example, in the scenario where all of the following occurs:\n- You have two Jobs (Job A and Job B) with Pod affinity rules. GKE schedules the TPU slices for both Jobs with a TPU topology of`v4-32`.\n- You have two`v4-32`TPU slices in the cluster.\n- Your cluster has ample capacity to schedule both Jobs and, in theory, each Job can be quickly scheduled on each TPU slice.\n- The Kubernetes scheduler schedulesPod from Job A on one slice, and then schedulesPod from Job B on the same slice.\nIn this case, given the Pod affinity rules for Job A, the scheduler attempts to schedule all remaining Pods for Job A and for Job B, on a single TPU slice each. As a result, GKE won't be able to fully schedule either Job A or Job B. Hence, the status of both Jobs will remain Pending.\nTo resolve this issue, use [Pod anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity) with `cloud.google.com/gke-nodepool` as the `topologyKey` , as shown in the following example:\n```\napiVersion: batch/v1kind: Jobmetadata:\u00a0name: pispec:\u00a0parallelism: 2\u00a0template:\u00a0 \u00a0metadata:\u00a0 \u00a0 \u00a0labels:\u00a0 \u00a0 \u00a0 \u00a0job: pi\u00a0 \u00a0spec:\u00a0 \u00a0 \u00a0affinity:\u00a0 \u00a0 \u00a0 \u00a0podAffinity:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0- labelSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0- key: job\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0- pi\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0topologyKey: cloud.google.com/gke-nodepool\u00a0 \u00a0 \u00a0 \u00a0podAntiAffinity:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0- labelSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0- key: job\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0operator: NotIn\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0- pi\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0topologyKey: cloud.google.com/gke-nodepool\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0namespaceSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0- key: kubernetes.io/metadata.name\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0operator: NotIn\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0- kube-system\u00a0 \u00a0 \u00a0containers:\u00a0 \u00a0 \u00a0- name: pi\u00a0 \u00a0 \u00a0 \u00a0image: perl:5.34.0\u00a0 \u00a0 \u00a0 \u00a0command: [\"sleep\", \u00a0\"60\"]\u00a0 \u00a0 \u00a0restartPolicy: Never\u00a0backoffLimit: 4\n```\n## Permission denied during cluster creation in us-central2\nIf you are attempting to create a cluster in `us-central2` (the only region where TPU v4 is available), then you may encounter an error message similar to the following:\n```\nERROR: (gcloud.container.clusters.create) ResponseError: code=403,\nmessage=Permission denied on 'locations/us-central2' (or it may not exist).\n```\nThis error is because the region `us-central2` is a private region.\nTo resolve this issue, [file a support case](/support-hub) or reach out to your account team to ask for `us-central2` to be made visible within your Google Cloud project.\n## Insufficient quota during TPU node pool creation in us-central2\nIf you are attempting to create a TPU node pool in `us-central2` (the only region where TPU v4 is available), then you may need to increase the following GKE-related quotas when you first create TPU v4 node pools:\n- **Persistent Disk SSD (GB) quota in us-central2** : The boot disk of each Kubernetes node requires 100 GB by default. Therefore, this quota should be set at least as high as the product of the maximum number of GKE nodes you anticipate creating in`us-central2`and 100 GB (`maximum_nodes`X`100 GB`).\n- **In-use IP addresses quota in us-central2** : Each Kubernetes node consumes one IP address. Therefore, this quota should be set at least as high as the maximum number of GKE nodes you anticipate creating in`us-central2`.## Missing subnet during GKE cluster creation\nIf you are attempting to create a cluster in `us-central2` (the only region where TPU v4 is available), then you may encounter an error message similar to the following:\n```\nERROR: (gcloud.container.clusters.create) ResponseError: code=404,\nmessage=Not found: project <PROJECT> does not have an auto-mode subnetwork\nfor network \"default\" in region <REGION>.\n```\nA subnet is required in your VPC network to provide connectivity to your GKE nodes. However, in certain regions such as `us-central2` , a default subnet may not be created, even when you use the default VPC network in auto-mode (for subnet creation).\nTo resolve this issue, ensure that you have [created a customsubnet](/vpc/docs/create-modify-vpc-networks#add-subnets) in the region before creating your GKE cluster. This subnet must not overlap with other subnets created in other regions in the same VPC network.\n## What's next\n[Cloud Customer Care](/kubernetes-engine/docs/getting-support)", "guide": "Google Kubernetes Engine (GKE)"}