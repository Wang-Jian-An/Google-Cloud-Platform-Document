{"title": "Cloud Architecture Center - Connecting your visualization software to Hadoop on Google Cloud", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Connecting your visualization software to Hadoop on Google Cloud\nThis tutorial is the second part of a series that shows you how to build an end-to-end solution to give data analysts secure access to data when using business intelligence (BI) tools.\nThis tutorial is intended for operators and IT administrators who set up environments that provide data and processing capabilities to the business intelligence (BI) tools used by data analysts.\n [Tableau](https://www.tableau.com/) is used as the BI tool in this tutorial. To follow along with this tutorial, you must have Tableau Desktop installed on your workstation.\nThe series is made up of the following parts:- The first part of the series, [Architecture for connecting visualization software to Hadoop on Google Cloud](/architecture/hadoop/architecture-for-connecting-visualization-software-to-hadoop-on-google-cloud) , defines the architecture of the solution, its components, and how the components interact.\n- This second part of the series tells you how to set up the architecture components that make up the end-to-end Hive topology on Google Cloud. The tutorial uses open source tools from the Hadoop ecosystem, with Tableau as the BI tool.\nThe code snippets in this tutorial are available in a [GitHub repository](https://github.com/GoogleCloudPlatform/dataproc-connect-visualization) . The GitHub repository also includes Terraform configuration files to help you set up a working prototype.\nThroughout the tutorial, you use the name `sara` as the fictitious user identity of a data analyst. This user identity is in the LDAP directory that both [Apache Knox](https://knox.apache.org/) and [Apache Ranger](https://ranger.apache.org/) use. You can also choose to configure LDAP groups, but this procedure is outside the scope of this tutorial.", "content": "## Objectives\n- Create an end-to-end setup that enables a BI tool to use data from a Hadoop environment.\n- Authenticate and authorize user requests.\n- Set up and use secure communication channels between the BI tool and the cluster.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Dataproc](/dataproc/pricing?) \n- [Cloud SQL](/sql/pricing) \n- [Cloud Storage](/storage/pricing) \n- [Network egress](/compute/network-pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . ## Before you begin\n## Initializing your environment\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- In Cloud Shell, set environment variables with your project ID, and the region and zones of the Dataproc clusters:```\nexport PROJECT_ID=$(gcloud info --format='value(config.project)')export REGION=us-central1export ZONE=us-central1-b\n```You can choose any region and zone, but keep them consistent as you follow this tutorial.\n### Setting up a service account\n- In Cloud Shell, create a [service account](/dataproc/docs/concepts/configuring-clusters/service-accounts#top_of_page) .```\ngcloud iam service-accounts create cluster-service-account \\\u00a0 --description=\"The service account for the cluster to be authenticated as.\" \\\u00a0 --display-name=\"Cluster service account\"\n```The cluster uses this account to access Google Cloud resources.\n- Add the following roles to the service account:- [Dataproc Worker](/dataproc/docs/concepts/iam/iam#roles) : to create and manage Dataproc clusters.\n- [Cloud SQL Editor](/sql/docs/mysql/project-access-control#roles) : for Ranger to connect to its database using [Cloud SQL Proxy](/sql/docs/mysql/sql-proxy) .\n- [Cloud KMS CryptoKey Decrypter](/kms/docs/reference/permissions-and-roles#predefined_roles) : to decrypt the passwords encrypted with Cloud KMS.```\nbash -c 'array=( dataproc.worker cloudsql.editor cloudkms.cryptoKeyDecrypter )for i in \"${array[@]}\"do\u00a0 gcloud projects add-iam-policy-binding ${PROJECT_ID} \\\u00a0 \u00a0 --member \"serviceAccount:cluster-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role roles/$idone'\n```## Creating the backend clusterIn this section, you create the backend cluster where Ranger is located. You also create the Ranger database to store the policy rules, and a sample table in Hive to apply the Ranger policies.\n### Create the Ranger database instance\n- Create a MySQL instance to store the Apache Ranger policies:```\nexport CLOUD_SQL_NAME=cloudsql-mysqlgcloud sql instances create ${CLOUD_SQL_NAME} \\\u00a0 \u00a0 --tier=db-n1-standard-1 --region=${REGION}\n```This command creates an instance called `cloudsql-mysql` with the [machine type](/sql/docs/mysql/instance-settings#tier-values) `db-n1-standard-1` located in the [region](/sql/docs/mysql/instance-settings#region-values) specified by the `${REGION}` variable. For more information, see the [Cloud SQL documentation](/sql/docs/mysql/create-instance#create-2nd-gen) .\n- Set the instance password for the user `root` connecting from any host. You can use the example password for demonstrative purposes, or create your own. If you create your own password, use a minimum of eight characters, including at least one letter and one number.```\ngcloud sql users set-password root \\\u00a0 --host=% --instance ${CLOUD_SQL_NAME} --password mysql-root-password-99\n```\n### Encrypt the passwordsIn this section, you create a [cryptographic key](/kms/docs/object-hierarchy#key) to encrypt the passwords for Ranger and MySQL. To prevent exfiltration, you store the cryptographic key in [Cloud KMS](/kms/docs) . For security purposes, you can't view, extract, or export the key bits.\nYou use the cryptographic key to encrypt the passwords and write them into files. You upload these files into a Cloud Storage bucket so that they are accessible to the service account that is acting on behalf of the clusters. The service account can decrypt these files because it has the `cloudkms.cryptoKeyDecrypter` role and access to the files and the cryptographic key. Even if a file is exfiltrated, the file can't be decrypted without the role and the key.\nAs an extra security measure, you create separate password files for each service. This action minimizes the potential impacted area if a password is exfiltrated.\nFor more information about key management, see the [Cloud KMS documentation](/kms/docs/concepts) .- In Cloud Shell, create a [Cloud KMS key ring](/kms/docs/object-hierarchy#key_ring) to hold your keys:```\ngcloud kms keyrings create my-keyring --location global\n```\n- To encrypt your passwords, create a [Cloud KMS cryptographic key](/kms/docs/object-hierarchy#key) :```\ngcloud kms keys create my-key \\\u00a0 --location global \\\u00a0 --keyring my-keyring \\\u00a0 --purpose encryption\n```\n- Encrypt your Ranger admin user password using the key. You can use the example password or create your own. Your password must be a minimum of eight characters, including at least one letter and one number.```\necho \"ranger-admin-password-99\" | \\gcloud kms encrypt \\\u00a0 --location=global \\\u00a0 --keyring=my-keyring \\\u00a0 --key=my-key \\\u00a0 --plaintext-file=- \\\u00a0 --ciphertext-file=ranger-admin-password.encrypted\n```\n- Encrypt your Ranger database admin user password with the key:```\necho \"ranger-db-admin-password-99\" | \\gcloud kms encrypt \\\u00a0 --location=global \\\u00a0 --keyring=my-keyring \\\u00a0 --key=my-key \\\u00a0 --plaintext-file=- \\\u00a0 --ciphertext-file=ranger-db-admin-password.encrypted\n```\n- Encrypt your MySQL root password with the key:```\necho \"mysql-root-password-99\" | \\gcloud kms encrypt \\\u00a0 --location=global \\\u00a0 --keyring=my-keyring \\\u00a0 --key=my-key \\\u00a0 --plaintext-file=- \\\u00a0 --ciphertext-file=mysql-root-password.encrypted\n```\n- [Create a Cloud Storage bucket](/storage/docs/creating-buckets#storage-create-bucket-cli) to store encrypted password files:```\ngsutil mb -l ${REGION} gs://${PROJECT_ID}-ranger\n```\n- Upload the encrypted password files to the Cloud Storage bucket:```\ngsutil -m cp *.encrypted gs://${PROJECT_ID}-ranger\n```\n### Create the clusterIn this section, you create a backend cluster with Ranger support. For more information about the Ranger optional component in Dataproc, see the [Dataproc Ranger Component documentation page](/dataproc/docs/concepts/components/ranger) .- In Cloud Shell, [create a Cloud Storage bucket](/storage/docs/creating-buckets#storage-create-bucket-cli) to store the Apache Solr audit logs:```\ngsutil mb -l ${REGION} gs://${PROJECT_ID}-solr\n```\n- Export all the variables required in order to create the cluster:```\nexport BACKEND_CLUSTER=backend-clusterexport PROJECT_ID=$(gcloud info --format='value(config.project)')export REGION=us-central1export ZONE=us-central1-bexport CLOUD_SQL_NAME=cloudsql-mysqlexport RANGER_KMS_KEY_URI=\\projects/${PROJECT_ID}/locations/global/keyRings/my-keyring/cryptoKeys/my-keyexport RANGER_ADMIN_PWD_URI=\\gs://${PROJECT_ID}-ranger/ranger-admin-password.encryptedexport RANGER_DB_ADMIN_PWD_URI=\\gs://${PROJECT_ID}-ranger/ranger-db-admin-password.encryptedexport MYSQL_ROOT_PWD_URI=\\gs://${PROJECT_ID}-ranger/mysql-root-password.encrypted\n```For convenience, some of the variables that you set before are repeated in this command so you can modify them as you require.The new variables contain:- The name of the backend cluster.\n- The URI of the cryptographic key so that the service account can decrypt the passwords.\n- The URI of the files containing the encrypted passwords.\nIf you used a different key ring or key, or different filenames, use the corresponding values in your command.\n- Create the backend Dataproc cluster:```\ngcloud beta dataproc clusters create ${BACKEND_CLUSTER} \\\u00a0 --optional-components=SOLR,RANGER \\\u00a0 --region ${REGION} \\\u00a0 --zone ${ZONE} \\\u00a0 --enable-component-gateway \\\u00a0 --scopes=default,sql-admin \\\u00a0 --service-account=cluster-service-account@${PROJECT_ID}.iam.gserviceaccount.com \\\u00a0 --properties=\"\\dataproc:ranger.kms.key.uri=${RANGER_KMS_KEY_URI},\\dataproc:ranger.admin.password.uri=${RANGER_ADMIN_PWD_URI},\\dataproc:ranger.db.admin.password.uri=${RANGER_DB_ADMIN_PWD_URI},\\dataproc:ranger.cloud-sql.instance.connection.name=${PROJECT_ID}:${REGION}:${CLOUD_SQL_NAME},\\dataproc:ranger.cloud-sql.root.password.uri=${MYSQL_ROOT_PWD_URI},\\dataproc:solr.gcs.path=gs://${PROJECT_ID}-solr,\\hive:hive.server2.thrift.http.port=10000,\\hive:hive.server2.thrift.http.path=cliservice,\\hive:hive.server2.transport.mode=http\"\n```This command has the following properties:- The final three lines in the command are the Hive properties to configure HiveServer2 in HTTP mode, so that Apache Knox can call Apache Hive through HTTP.\n- The other parameters in the command operate as follows:- The`--optional-components=SOLR,RANGER`parameter enables Apache Ranger and its Solr dependency.\n- The`--enable-component-gateway`parameter enables the [Dataproc Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) to make the Ranger and other Hadoop user interfaces available directly from the cluster page in Google Cloud console. When you set this parameter, there is no need for SSH tunneling to the backend master node.\n- The`--scopes=default,sql-admin`parameter authorizes Apache Ranger to access its Cloud SQL database.\nIf you need to create an external Hive metastore that persists beyond the lifetime of any cluster and can be used across multiple clusters, see [Using Apache Hive on Dataproc](/solutions/using-apache-hive-on-cloud-dataproc) . To run the procedure, you must run the table creation examples directly on Beeline. While the [gcloud dataproc jobs submit hive](/sdk/gcloud/reference/dataproc/jobs/submit/hive) commands use Hive binary transport, these commands aren't compatible with HiveServer2 when it's configured in HTTP mode.\n### Create a sample Hive table\n- In Cloud Shell, create a Cloud Storage bucket to store a sample [Apache Parquet](https://parquet.apache.org/) file:```\ngsutil mb -l ${REGION} gs://${PROJECT_ID}-hive\n```\n- Copy a publicly available sample Parquet file into your bucket:```\ngsutil cp gs://hive-solution/part-00000.parquet \\\u00a0 gs://${PROJECT_ID}-hive/dataset/transactions/part-00000.parquet\n```\n- Connect to the master node of the backend cluster you created in the previous section using SSH:```\ngcloud compute ssh --zone ${ZONE} ${BACKEND_CLUSTER}-m\n```The name of your cluster master node is the name of the cluster followed by `-m.` The HA cluster master node names have an [extra suffix](/dataproc/docs/concepts/configuring-clusters/high-availability#instance_names) .If it's your first time connecting to your master node from Cloud Shell, you are prompted to generate SSH keys.\n- In the terminal you opened with SSH, connect to the local HiveServer2 using [Apache Beeline](https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline%E2%80%93CommandLineShell) , which is pre-installed on the master node:```\nbeeline -u \"jdbc:hive2://localhost:10000/;transportMode=http;httpPath=cliservice admin admin-password\"\\\u00a0 --hivevar PROJECT_ID=$(gcloud info --format='value(config.project)')\n```This command starts the Beeline command-line tool and passes the name of your Google Cloud project in an environment variable.Hive isn't performing any user authentication, but to perform most tasks it requires a user identity. The `admin` user here is a default user that's configured in Hive. The identity provider that you configure with Apache Knox later in this tutorial handles user authentication for any requests that come from BI tools.\n- In the Beeline prompt, create a table using the Parquet file you previously copied to your Hive bucket:```\nCREATE EXTERNAL TABLE transactions\u00a0 (SubmissionDate DATE, TransactionAmount DOUBLE, TransactionType STRING)\u00a0 STORED AS PARQUET\u00a0 LOCATION 'gs://${PROJECT_ID}-hive/dataset/transactions';\n```\n- Verify that the table was created correctly:```\nSELECT *\u00a0 FROM transactions\u00a0 LIMIT 10;SELECT TransactionType, AVG(TransactionAmount) AS AverageAmount\u00a0 FROM transactions\u00a0 WHERE SubmissionDate = '2017-12-22'\u00a0 GROUP BY TransactionType;\n```The results of the two queries appear in the Beeline prompt.\n- Exit the Beeline command-line tool:```\n!quit\n```\n- Copy the internal DNS name of the backend master:```\nhostname -A | tr -d '[:space:]'; echo\n```You use this name in the next section as `backend-master-internal-dns-name` to configure the Apache Knox topology. You also use the name to configure a service in Ranger.\n- Exit the terminal on the node:```\nexit\n```\n## Creating the proxy clusterIn this section, you create the proxy cluster that has the [Apache Knox initialization action](https://github.com/GoogleCloudDataproc/initialization-actions/tree/master/knox) .\n### Create a topology\n- In Cloud Shell, clone the Dataproc [initialization-actions GitHub repository](https://github.com/GoogleCloudDataproc/initialization-actions) :```\ngit clone https://github.com/GoogleCloudDataproc/initialization-actions.git\n```\n- Create a topology for the backend cluster:```\nexport KNOX_INIT_FOLDER=`pwd`/initialization-actions/knoxcd ${KNOX_INIT_FOLDER}/topologies/mv example-hive-nonpii.xml hive-us-transactions.xml\n```Apache Knox uses the name of the file as the URL path for the topology. In this step, you change the name to represent a topology called `hive-us-transactions` . You can then access the fictitious transaction data that you loaded into Hive in [Create a sample Hive table](#create_a_sample_hive_table) \n- Edit the topology file:```\nvi hive-us-transactions.xml\n```To see how backend services are configured, see the [topology descriptor](https://knox.apache.org/books/knox-1-1-0/user-guide.html#Topology+Descriptors) file. This file defines a topology that points to one or more backend services. Two services are configured with sample values: [WebHDFS](https://hadoop.apache.org/docs/r1.0.4/webhdfs.html) and HIVE. The file also defines the authentication provider for the services in this topology and the authorization ACLs.\n- Add the data analyst sample LDAP user identity `sara` .```\n<param>\u00a0 \u00a0<name>hive.acl</name>\u00a0 \u00a0<value>admin,sara;*;*</value></param>\n```Adding the sample identity lets the user access the Hive backend service through Apache Knox.\n- Change the HIVE URL to point to the backend cluster Hive service. You can find the HIVE service definition at the bottom of the file, under WebHDFS.```\n<service>\u00a0 <role>HIVE</role>\u00a0 <url>http://<backend-master-internal-dns-name>:10000/cliservice</url></service>\n```\n- Replace the `` placeholder with the internal DNS name of the backend cluster that you acquired in [Create a sample Hive table](#create_a_sample_hive_table) .\n- Save the file and [close the editor](https://stackoverflow.blog/2017/05/23/stack-overflow-helping-one-million-developers-exit-vim/) .\nTo create additional topologies, repeat the steps in this section. Create one independent XML descriptor for each topology.\nIn [Create the proxy cluster](#create_the_proxy_cluster) you copy these files into a Cloud Storage bucket. To create new topologies, or change them after you create the proxy cluster, modify the files, and upload them again to the bucket. The Apache Knox initialization action creates a cron job that regularly copies changes from the bucket to the proxy cluster.\n### Configure the SSL/TLS certificateA client uses an SSL/TLS certificate when it communicates with Apache Knox. The initialization action can generate a self-signed certificate, or you can provide your CA-signed certificate.- In Cloud Shell, edit the Apache Knox general configuration file:```\nvi ${KNOX_INIT_FOLDER}/knox-config.yaml\n```\n- Replace `HOSTNAME` with the external DNS name of your proxy master node as the value for the `certificate_hostname` attribute. For this tutorial, use `localhost` .```\ncertificate_hostname: localhost\n```Later in this tutorial, you [create an SSH tunnel](#create_an_ssh_tunnel) and the proxy cluster for the `localhost` value.The Apache Knox general configuration file also contains the `master_key` that encrypts the certificates BI tools use to communicate with the proxy cluster. By default, this key is the word `secret` .\n- If you are providing your own certificate, change the following two properties:```\ngenerate_cert: falsecustom_cert_name: <filename-of-your-custom-certificate>\n```\n- Save the file and close the editor.If you are providing your own certificate, you can specify it in the property `custom_cert_name` .\n### Create the proxy cluster\n- In Cloud Shell, create a Cloud Storage bucket:```\ngsutil mb -l ${REGION} gs://${PROJECT_ID}-knox\n```This bucket provides the configurations you created in the previous section to the Apache Knox initialization action.\n- Copy all the files from the Apache Knox initialization action folder to the bucket:```\ngsutil -m cp -r ${KNOX_INIT_FOLDER}/* gs://${PROJECT_ID}-knox\n```\n- Export all the variables required in order to create the cluster:```\nexport PROXY_CLUSTER=proxy-clusterexport PROJECT_ID=$(gcloud info --format='value(config.project)')export REGION=us-central1export ZONE=us-central1-b\n```In this step, some of the variables that you set before are repeated so that you can make modifications as required.\n- Create the proxy cluster:```\ngcloud dataproc clusters create ${PROXY_CLUSTER} \\\u00a0 --region ${REGION} \\\u00a0 --zone ${ZONE} \\\u00a0 --service-account=cluster-service-account@${PROJECT_ID}.iam.gserviceaccount.com \\\u00a0 --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/knox/knox.sh \\\u00a0 --metadata knox-gw-config=gs://${PROJECT_ID}-knox\n```\n### Verify the connection through proxy\n- After the proxy cluster is created, use SSH to connect to its master node from Cloud Shell:```\ngcloud compute ssh --zone ${ZONE} ${PROXY_CLUSTER}-m\n```\n- From the terminal of the proxy cluster's master node, run the following query:```\nbeeline -u \"jdbc:hive2://localhost:8443/;\\ssl=true;sslTrustStore=/usr/lib/knox/data/security/keystores/gateway-client.jks;trustStorePassword=secret;\\transportMode=http;httpPath=gateway/hive-us-transactions/hive\"\\\u00a0 -e \"SELECT SubmissionDate, TransactionType FROM transactions LIMIT 10;\"\\\u00a0 -n admin -p admin-password\n```\nThis command has the following properties:- The`beeline`command uses`localhost`instead of the DNS internal name because the certificate that you generated when you configured Apache Knox specifies`localhost`as the host name. If you are using your own DNS name or certificate, use the corresponding host name.\n- The port is`8443`, which corresponds to the Apache Knox default SSL port.\n- The line that begins`ssl=true`enables SSL and provides the path and password for the [SSL Trust Store](https://www.educative.io/edpresso/keystore-vs-truststore) to be used by client applications such as Beeline.\n- The`transportMode`line indicates that the request should be sent over HTTP and provides the path for the HiveServer2 service. The path is composed of the keyword`gateway`, the topology name that you defined in a previous section, and the service name configured in the same topology, in this case`hive`.\n- The`-e`parameter provides the query to run on Hive. If you omit this parameter, you open an interactive session in the Beeline command-line tool.\n- The`-n`parameter provides a user identity and password. In this step, you are using the default Hive`admin`user. In the next sections, you create an analyst user identity and set up credentials and authorization policies for this user.\n### Add a user to the authentication storeBy default, Apache Knox includes an authentication provider that is based on [Apache Shiro](https://shiro.apache.org/) . This authentication provider is configured with BASIC authentication against an [ApacheDS](https://directory.apache.org/apacheds/) LDAP store. In this section, you add a sample data analyst user identity `sara` to the authentication store.- From the terminal in the proxy's master node, install the LDAP utilities:```\nsudo apt-get install ldap-utils\n```\n- Create an LDAP Data Interchange Format (LDIF) file for the new user `sara` :```\nexport USER_ID=saraprintf '%s\\n'\\\u00a0 \"# entry for user ${USER_ID}\"\\\u00a0 \"dn: uid=${USER_ID},ou=people,dc=hadoop,dc=apache,dc=org\"\\\u00a0 \"objectclass:top\"\\\u00a0 \"objectclass:person\"\\\u00a0 \"objectclass:organizationalPerson\"\\\u00a0 \"objectclass:inetOrgPerson\"\\\u00a0 \"cn: ${USER_ID}\"\\\u00a0 \"sn: ${USER_ID}\"\\\u00a0 \"uid: ${USER_ID}\"\\\u00a0 \"userPassword:${USER_ID}-password\"\\> new-user.ldif\n```\n- Add the user ID to the LDAP directory:```\nldapadd -f new-user.ldif \\\u00a0 -D 'uid=admin,ou=people,dc=hadoop,dc=apache,dc=org' \\\u00a0 -w 'admin-password' \\\u00a0 -H ldap://localhost:33389\n```The `-D` parameter specifies the distinguished name (DN) to bind when the user that is represented by `ldapadd` accesses the directory. The DN must be a user identity that is already in the directory, in this case the user `admin` .\n- Verify that the new user is in the authentication store:```\nldapsearch -b \"uid=${USER_ID},ou=people,dc=hadoop,dc=apache,dc=org\" \\\u00a0 -D 'uid=admin,ou=people,dc=hadoop,dc=apache,dc=org' \\\u00a0 -w 'admin-password' \\\u00a0 -H ldap://localhost:33389\n```The user details appear in your terminal.\n- Copy and save the internal DNS name of the proxy master node:```\nhostname -A | tr -d '[:space:]'; echo\n```You use this name in the next section as `` to configure the LDAP synchronization.\n- Exit the terminal on the node:```\nexit\n```\n## Setting up authorizationIn this section, you configure identity synchronization between the LDAP service and Ranger.\n### Sync user identities into RangerTo ensure that Ranger policies apply to the same user identities as Apache Knox, you configure the [Ranger UserSync daemon](https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/configuring-ranger-authe-with-unix-ldap-ad/content/ranger_ad_integration_ranger_usersync.html) to sync the identities from the same directory.\nIn this example, you connect to the local LDAP directory that is available by default with Apache Knox. However, in a production environment, we recommend that you set up an external identity directory. For more information, see the [Apache Knox User's Guide](https://knox.apache.org/books/knox-1-4-0/user-guide.html#Authentication) and the Google Cloud [Cloud Identity](https://cloud.google.com/identity) , [Managed Active Directory](/managed-microsoft-ad/docs) , and [Federated AD](/solutions/federating-gcp-with-active-directory-introduction) documentation.- Using SSH, connect to the master node of the backend cluster that you created:```\nexport BACKEND_CLUSTER=backend-clustergcloud compute ssh --zone ${ZONE} ${BACKEND_CLUSTER}-m\n```\n- In the terminal, edit the `UserSync` configuration file:```\nsudo vi /etc/ranger/usersync/conf/ranger-ugsync-site.xml\n```\n- Set the values of the following LDAP properties. Make sure that you modify the `user` properties and not the `group` properties, which have similar names.```\n<property>\u00a0 <name>ranger.usersync.sync.source</name>\u00a0 <value>ldap</value></property><property>\u00a0 <name>ranger.usersync.ldap.url</name>\u00a0 <value>ldap://<proxy-master-internal-dns-name>:33389</value></property><property>\u00a0 <name>ranger.usersync.ldap.binddn</name>\u00a0 <value>uid=admin,ou=people,dc=hadoop,dc=apache,dc=org</value></property><property>\u00a0 <name>ranger.usersync.ldap.ldapbindpassword</name>\u00a0 <value>admin-password</value></property><property>\u00a0 <name>ranger.usersync.ldap.user.searchbase</name>\u00a0 <value>dc=hadoop,dc=apache,dc=org</value></property><property>\u00a0 <name>ranger.usersync.source.impl.class</name>\u00a0 <value>org.apache.ranger.ldapusersync.process.LdapUserGroupBuilder</value></property>\n```Replace the `` placeholder with the internal DNS name of the proxy server, which you retrieved in the last section.These properties are a subset of a full LDAP configuration that syncs both users and groups. For more information, see [How to integrate Ranger with LDAP](http://crazyadmins.com/how-to-integrate-ranger-with-ldap/) .\n- Save the file and close the editor.\n- Restart the `ranger-usersync` daemon:```\nsudo service ranger-usersync restart\n```\n- Run the following command:```\ngrep sara /var/log/ranger-usersync/*\n```If the identities are synched, you see at least one log line for the user `sara` .\n## Creating Ranger policiesIn this section, you configure a new Hive service in Ranger. You also set up and test a Ranger policy to limit the access to the Hive data for a specific identity.\n### Configure the Ranger service\n- From the terminal of the master node, edit the Ranger Hive configuration:```\nsudo vi /etc/hive/conf/ranger-hive-security.xml\n```\n- Edit the `<value>` property of the `ranger.plugin.hive.service.name` property:```\n<property>\u00a0 \u00a0<name>ranger.plugin.hive.service.name</name>\u00a0 \u00a0<value>ranger-hive-service-01</value>\u00a0 \u00a0<description>\u00a0 \u00a0 \u00a0Name of the Ranger service containing policies for this YARN instance\u00a0 \u00a0</description></property>\n```\n- Save the file and close the editor.\n- Restart the HiveServer2 Admin service:```\nsudo service hive-server2 restart\n```You are ready to create Ranger policies.\n### Set up the service in the Ranger Admin console\n- In the Google Cloud console, go to the [Dataproc page](/dataproc) .\n- Click your backend cluster name, and then click **Web Interfaces** .Because you created your cluster with [Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) , you see a list of the Hadoop components that are installed in your cluster.\n- Click the **Ranger** link to open the Ranger console.\n- Log in to Ranger with the user `admin` and your Ranger admin password. The Ranger console shows the Service Manager page with a list of services.\n- Click the plus sign in the HIVE group to create a new Hive service.\n- In the form, set the following values:- Service name:`ranger-hive-service-01`. You previously defined this name in the`ranger-hive-security.xml`configuration file.\n- Username:`admin`\n- Password:`admin-password`\n- `jdbc.driverClassName`: keep the default name as`org.apache.hive.jdbc.HiveDriver`\n- `jdbc.url`:`jdbc:hive2:` `` `:10000/;transportMode=http;httpPath=cliservice`\n- Replace the``placeholder with the name you retrieved in a previous section.\n- Click **Add** .Each Ranger plugin installation [supports a single Hive service](https://stackoverflow.com/questions/33270255/apache-ranger-multiple-policy-repo-for-hive-plugin) . An easy way to configure additional Hive services is to start up additional backend clusters. Each cluster has its own Ranger plugin. These clusters can share the same Ranger DB, so that you have a unified view of all the services whenever you access the Ranger Admin console from any of those clusters.\n### Set up a Ranger policy with limited permissionsThe policy allows the sample analyst LDAP user `sara` access to specific columns of the Hive table.- On the Service Manager window, click the name of the service you created.The Ranger Admin console shows the **Policies** window.\n- Click **Add New Policy** .With this policy, you give `sara` the permission to see only the columns `submissionDate` and `transactionType` from table transactions.\n- In the form, set the following values:- Policy name: any name, for example`allow-tx-columns`\n- Database:`default`\n- Table:`transactions`\n- Hive column:`submissionDate, transactionType`\n- Allow conditions:- Select user:`sara`\n- Permissions:`select`\n- At the bottom of the screen, click **Add** .\n### Test the policy with Beeline\n- In the master node terminal, start the Beeline command-line tool with the user `sara` .```\nbeeline -u \"jdbc:hive2://localhost:10000/;transportMode=http;httpPath=cliservice sara user-password\"\n```Although the Beeline command-line tool doesn't enforce the password, you must provide a password to run the preceding command.\n- Run the following query to verify that Ranger blocks it.```\n\u00a0SELECT *\u00a0 \u00a0FROM transactions\u00a0 \u00a0LIMIT 10;\n```The query includes the column `transactionAmount` , which `sara` doesn't have permission to select.A `Permission denied` error displays.\n- Verify that Ranger allows the following query:```\nSELECT submissionDate, transactionType\u00a0 FROM transactions\u00a0 LIMIT 10;\n```\n- Exit the Beeline command-line tool:```\n!quit\n```\n- Exit the terminal:```\nexit\n```\n- In the Ranger console, click the **Audit** tab. Both denied and allowed events display. You can filter the events by the service name you previously defined, for example, `ranger-hive-service-01` .\n## Connecting from a BI toolThe final step in this tutorial is to query the Hive data from Tableau Desktop.\n### Create a firewall rule\n- Copy and save your [public IP address](https://www.google.com/search?q=what+is+my+ip+address) .\n- In Cloud Shell, create a firewall rule that opens TCP port `8443` for ingress from your workstation:```\ngcloud compute firewall-rules create allow-knox\\\u00a0 --project=${PROJECT_ID} --direction=INGRESS --priority=1000 \\\u00a0 --network=default --action=ALLOW --rules=tcp:8443 \\\u00a0 --target-tags=knox-gateway \\\u00a0 --source-ranges=<your-public-ip>/32\n```Replace the `` placeholder with your public IP address.\n- Apply the network tag from the firewall rule to the proxy cluster's master node:```\ngcloud compute instances add-tags ${PROXY_CLUSTER}-m --zone=${ZONE} \\\u00a0 --tags=knox-gateway\n```\n### Create an SSH tunnelThis procedure is only necessary if you're using a self-signed certificate valid for `localhost` . If you are using your own certificate or your proxy master node has its own external DNS name, you can skip to [Connect to Hive](#connect_to_hive) .- In Cloud Shell, generate the command to create the tunnel:```\necho \"gcloud compute ssh ${PROXY_CLUSTER}-m \\\u00a0 --project ${PROJECT_ID} \\\u00a0 --zone ${ZONE} \\\u00a0 -- -L 8443:localhost:8443\"\n```\n- Run `gcloud init` to authenticate your user account and grant access permissions.\n- Open a terminal in your workstation.\n- Create an SSH tunnel to forward port `8443` . Copy the command generated in the first step and paste it into the workstation terminal, and then run the command.\n- Leave the terminal open so that the tunnel remains active.\n### Connect to Hive\n- On your workstation, install the [Hive ODBC driver](https://www.cloudera.com/downloads/connectors/hive/odbc/2-6-4.html) .\n- Open Tableau Desktop, or restart it if it was open.\n- On the home page under **Connect / To a Server** , select **More** .\n- Search for and then select **Cloudera Hadoop** .\n- Using the sample data analyst LDAP user `sara` as the user identity, fill out the fields as follows:- Server: If you created a tunnel, use`localhost`. If you didn't create a tunnel, use the external DNS name of your proxy master node.\n- Port:`8443`\n- Type:`HiveServer2`\n- Authentication:`Username`and`Password`\n- Username:`sara`\n- Password:`sara-password`\n- HTTP Path:`gateway/hive-us-transactions/hive`\n- Require SSL:`yes`\n- Click **Sign In.** \n### Query Hive data\n- On the **Data Source** screen, click **Select Schema** and search for`default`.\n- Double-click the `default` schema name.The **Table** panel loads.\n- In the **Table** panel, double-click **New Custom SQL** .The **Edit Custom SQL** window opens.\n- Enter the following query, which selects the date and transaction type from the transactions table:```\nSELECT `submissiondate`,\u00a0 \u00a0 \u00a0 \u00a0`transactiontype`FROM `default`.`transactions`\n```\n- Click **OK.** The metadata for the query is retrieved from Hive.\n- Click **Update Now** .Tableau retrieves the data from Hive because `sara` is authorized to read these two columns from the `transactions` table.\n- To try to select all columns from the `transactions` table, in the **Table** panel, double-click **New Custom SQL** again. The **Edit Custom SQL** window opens.\n- Enter the following query:```\nSELECT *FROM `default`.`transactions`\n```\n- Click **OK** . The following error message displays:`Permission denied: user [sara] does not have [SELECT] privilege on [default/transactions/*]` .Because `sara` doesn't have authorization from Ranger to read the `transactionAmount` column, this message is expected. This example shows how you can limit what data Tableau users can access.To see all the columns, repeat the steps using the user `admin` .\n- Close Tableau and your terminal window.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Read the first part of this series: [Architecture to connect your Visualization Software to Hadoop on Google Cloud](/architecture/hadoop/architecture-for-connecting-visualization-software-to-hadoop-on-google-cloud) .\n- Read the [Hadoop migration Security Guide](/architecture/hadoop/hadoop-migration-security-guide) .\n- Learn how to migrate [Apache Spark jobs to Dataproc](/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc) .\n- Learn how to [migrate on-premises Hadoop infrastructure to Google Cloud](/architecture/hadoop) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}