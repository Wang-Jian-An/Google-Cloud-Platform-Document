{"title": "Compute Engine - Running TensorFlow inference workloads with TensorRT5 and NVIDIA T4 GPU", "url": "https://cloud.google.com/compute/docs/instances", "abstract": "# Compute Engine - Running TensorFlow inference workloads with TensorRT5 and NVIDIA T4 GPU\nThis tutorial covers how to run deep learning inferences on large scale workloads by using NVIDIA TensorRT5 GPUs running on Compute Engine.\nBefore you begin, here are some essentials:- Deep learning inference is the stage in the machine learning process where a trained model is used to recognize, process, and classify results.\n- [NVIDIA TensorRT](https://developer.nvidia.com/tensorrt) is a platform that is optimized for running deep learning workloads.\n- GPUs are used to accelerate data-intensive workloads such as machine learning and data processing. A variety of [NVIDIA GPUs are available on Compute Engine](/compute/docs/gpus#introduction) . This tutorial uses T4 GPUs, since T4 GPUs are specifically designed for deep learning inference workloads.\n#", "content": "## ObjectivesIn this tutorial, the following procedures are covered:- Preparing a model using a pre-trained graph.\n- Testing the inference speed for a model with different optimization modes.\n- Converting a custom model to TensorRT.\n- Setting up a multi-zone cluster. This multi-zone cluster is configured as follows:- Built on [Deep Learning VM Images](/deep-learning-vm/docs/images) . These images are preinstalled with TensorFlow, TensorFlow serving, and TensorRT5.\n- Autoscaling enabled. Autoscaling in this tutorial is based on GPU utilization.\n- Load balancing enabled.\n- Firewall enabled.\n- Running an inference workload in the multi-zone cluster.\n ## CostsThe cost of running this tutorial varies by section.\nYou can calculate the cost by using the [pricingcalculator](/products/calculator#id=50e2750e-4c10-4d07-962a-6ce19c2bb0d6) .\nTo estimate the cost to prepare your model and test the inference speeds at different optimization speeds, use the following specifications:- 1 VM instance:`n1-standard-8`(vCPUs: 8, RAM 30GB)\n- 1 NVIDIA T4 GPU\nTo estimate the cost to set up your multi-zone cluster, use the following specifications:- 2 VM instances:`n1-standard-16`(vCPUs: 16, RAM 60GB)\n- 4 GPU NVIDIA T4 for each VM instance\n- 100 GB SSD for each VM instance\n- 1 Forwarding rule\n## Before you begin\n### Project setup### Tools setupTo use the Google Cloud CLI in this tutorial:\n- Install or update to the latest version of the [Google Cloud CLI](/compute/docs/gcloud-compute) .\n- (Optional) [  Set a default region and zone](/compute/docs/gcloud-compute#set_default_zone_and_region_in_your_local_client) .\n## Preparing the modelThis section covers the creation of a virtual machine (VM) instance that is used to run the model. This section also covers how to download a model from the [TensorFlow official models catalog](https://github.com/tensorflow/models/tree/master/official) .- Create the VM instance. This tutorial is created using the `tf-ent-2-10-cu113` . For the latest image versions, see [Choosing an operating system](/deep-learning-vm/docs/images#choosing_an_operating_system) in the Deep Learning VM Images documentation.```\nexport IMAGE_FAMILY=\"tf-ent-2-10-cu113\"\nexport ZONE=\"us-central1-b\"\nexport INSTANCE_NAME=\"model-prep\"\ngcloud compute instances create $INSTANCE_NAME \\\n --zone=$ZONE \\\n --image-family=$IMAGE_FAMILY \\\n --machine-type=n1-standard-8 \\\n --image-project=deeplearning-platform-release \\\n --maintenance-policy=TERMINATE \\\n --accelerator=\"type=nvidia-tesla-t4,count=1\" \\\n --metadata=\"install-nvidia-driver=True\"\n``` **Note:** You can create this instance in any [available zone](/compute/docs/gpus/gpu-regions-zones) that supports T4 GPUs. A single GPU is enough to compare the different TensorRT optimization modes.\n- Select a model. This tutorial uses the [ResNet model](https://github.com/tensorflow/tensorflow/tree/9489702e35b16a40a1accf3b8b5ed557efae10c7/tensorflow/python/keras/applications) . This ResNet model is trained on the ImageNet dataset that is in TensorFlow.To download the ResNet model to your VM instance, run the following command:```\nwget -q http://download.tensorflow.org/models/official/resnetv2_imagenet_frozen_graph.pb\n```Save the location of your ResNet model in the `$WORKDIR` variable. Replace `` with the working directory that contains the downloaded model.```\nexport WORKDIR=MODEL_LOCATION\n```\n## Running the inference speed testThis section covers the following procedures:- Setting up the ResNet model.\n- Running inference tests at different optimization modes.\n- Reviewing the results of the inference tests.\n### Overview of the test processTensorRT can improve the performance speed for inference workloads, however the most significant improvement comes from the quantization process.\nModel quantization is the process by which you reduce the precision of weights for a model. For example, if the initial weight of a model is FP32, you can reduce the precision to FP16, INT8, or even INT4. It is important to pick the right compromise between speed (precision of weights) and accuracy of a model. Luckily, TensorFlow includes functionality that does exactly this, measuring accuracy versus speed, or other metrics such as throughput, latency, node conversion rates, and total training time.\n **Note:** This test is limited to image recognition models at the moment, however it should not be too hard to implement a custom test based on this code.\n### Procedure\n- Set up the ResNet model. To set up the model, run the following commands:```\ngit clone https://github.com/tensorflow/models.git\ncd models\ngit checkout f0e10716160cd048618ccdd4b6e18336223a172f\ntouch research/__init__.py\ntouch research/tensorrt/__init__.py\ncp research/tensorrt/labellist.json .\ncp research/tensorrt/image.jpg ..\n```\n- Run the test. This command takes some time to finish.```\npython -m research.tensorrt.tensorrt \\\n --frozen_graph=$WORKDIR/resnetv2_imagenet_frozen_graph.pb \\\n --image_file=$WORKDIR/image.jpg \\\n --native --fp32 --fp16 --int8 \\\n --output_dir=$WORKDIR\n```Where:- `$WORKDIR`is the directory in which you downloaded the ResNet model.\n- The`--native`arguments are the different quantization modes to test.\n- Review the results. When the test completes, you can do a comparison of the inference results for each optimization mode.```\nPredictions:\nPrecision: native [u'seashore, coast, seacoast, sea-coast', u'promontory, headland, head, foreland', u'breakwater, groin, groyne, mole, bulwark, seawall, jetty',  u'lakeside, lakeshore', u'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus']\nPrecision: FP32 [u'seashore, coast, seacoast, sea-coast', u'promontory, headland, head, foreland', u'breakwater, groin, groyne, mole, bulwark, seawall, jetty', u'lakeside, lakeshore', u'sandbar, sand bar']\nPrecision: FP16 [u'seashore, coast, seacoast, sea-coast', u'promontory, headland, head, foreland', u'breakwater, groin, groyne, mole, bulwark, seawall, jetty', u'lakeside, lakeshore', u'sandbar, sand bar']\nPrecision: INT8 [u'seashore, coast, seacoast, sea-coast', u'promontory, headland, head, foreland', u'breakwater, groin, groyne, mole, bulwark, seawall, jetty', u'grey   whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus', u'lakeside, lakeshore']\n```To see the full results, run the following command:```\ncat $WORKDIR/log.txt\n``` From the results, you can see that FP32 and FP16 are identical. This means that if you are comfortable working with TensorRT you can definitely start using FP16 right away. INT8, shows slightly worse results.In addition, you can see that running the model with TensorRT5 shows the following results:- Using FP32 optimization, improves the throughput by 40% from 314 fps to 440 fps. At the same time latency decreases by approximately 30% making it 0.28 ms instead of 0.40 ms.\n- Using FP16 optimization, rather than native TensorFlow graph, increases the speed by 214% from 314 to 988 fps. At the same time latency decreases by 0.12 ms, almost a 3x decrease.\n- Using INT8, you can observe a speedup of 385% from 314 fps to 1524 fps with the latency decreasing to 0.08 ms.## Converting a custom model to TensorRTFor this conversion, you can use an INT8 model.- Download the model. To convert a custom model to a TensorRT graph, you need a saved model. To get a saved INT8 ResNet model, run the following command:```\nwget http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v2_fp32_savedmodel_NCHW.tar.gz\ntar -xzvf resnet_v2_fp32_savedmodel_NCHW.tar.gz\n```\n- Convert the model to the TensorRT graph by using TFTools. To convert the model using the TFTools, run the following command:```\ngit clone https://github.com/GoogleCloudPlatform/ml-on-gcp.git\ncd ml-on-gcp/dlvm/tools\npython ./convert_to_rt.py \\\n --input_model_dir=$WORKDIR/resnet_v2_fp32_savedmodel_NCHW/1538687196 \\\n --output_model_dir=$WORKDIR/resnet_v2_int8_NCHW/00001 \\\n --batch_size=128 \\\n --precision_mode=\"INT8\"\n```You now have an INT8 model in your `$WORKDIR/resnet_v2_int8_NCHW/00001` directory.To ensure that everything is set up properly, try to run an inference test.```\ntensorflow_model_server --model_base_path=$WORKDIR/resnet_v2_int8_NCHW/ --rest_api_port=8888\n```\n- Upload the model to Cloud Storage. This step is needed so that the model can be used from the multiple-zone cluster that is set up in the next section. To upload the model, complete the following steps:- Archive the model.```\ntar -zcvf model.tar.gz ./resnet_v2_int8_NCHW/\n```\n- Upload the archive. Replace `` with the path to your Cloud Storage bucket.```\nexport GCS_PATH=GCS_PATH\ngsutil cp model.tar.gz $GCS_PATH\n```If needed, you can get an INT8 frozen graph from the Cloud Storage at this URL:```\ngs://cloud-samples-data/dlvm/t4/model.tar.gz\n```## Setting up a multiple-zone cluster\n### Create the clusterNow that you have a model on the Cloud Storage platform, you can create a cluster.- Create an instance template. An instance template is a useful resource to creates new instances. See [Instance Templates](/compute/docs/instance-templates) . Replace `` with your project ID.```\nexport INSTANCE_TEMPLATE_NAME=\"tf-inference-template\"\nexport IMAGE_FAMILY=\"tf-ent-2-10-cu113\"\nexport PROJECT_NAME=YOUR_PROJECT_NAME\ngcloud beta compute --project=$PROJECT_NAME instance-templates create $INSTANCE_TEMPLATE_NAME \\\n  --machine-type=n1-standard-16 \\\n  --maintenance-policy=TERMINATE \\\n  --accelerator=type=nvidia-tesla-t4,count=4 \\\n  --min-cpu-platform=Intel\\ Skylake \\\n  --tags=http-server,https-server \\\n  --image-family=$IMAGE_FAMILY \\\n  --image-project=deeplearning-platform-release \\\n  --boot-disk-size=100GB \\\n  --boot-disk-type=pd-ssd \\\n  --boot-disk-device-name=$INSTANCE_TEMPLATE_NAME \\\n  --metadata startup-script-url=gs://cloud-samples-data/dlvm/t4/start_agent_and_inf_server_4.sh\n```- This instance template includes a startup script that is specified by the metadata parameter.- Run this startup script during instance creation on every instance that uses this template.\n- This startup script performs the following steps:- Installs a monitoring agent that monitors the GPU usage on the instance.\n- Downloads the model.\n- Starts the inference service.\n- In the startup script,`tf_serve.py`contains the inference logic. This example includes a very small python file based on the [TFServe package](https://github.com/GoogleCloudPlatform/ml-on-gcp/blob/master/dlvm/tools/tf_serve.py) .\n- To view the startup script, see [startup_inf_script.sh](https://github.com/GoogleCloudPlatform/ml-on-gcp/blob/master/dlvm/tools/scripts/start_agent_and_inf_server.sh) .\n- Create a managed instance group (MIG). This managed instance group is needed to set up multiple running instances in specific zones. The instances are created based on the instance template generated in the previous step.```\nexport INSTANCE_GROUP_NAME=\"deeplearning-instance-group\"\nexport INSTANCE_TEMPLATE_NAME=\"tf-inference-template\"\ngcloud compute instance-groups managed create $INSTANCE_GROUP_NAME \\\n --template $INSTANCE_TEMPLATE_NAME \\\n --base-instance-name deeplearning-instances \\\n --size 2 \\\n --zones us-central1-a,us-central1-b\n``` **Note:** `INSTANCE_TEMPLATE_NAME` is the name of the instance template that you created in the previous step.- You can create this instance in any [available zone](/compute/docs/gpus#gpus-list) that support T4 GPUs. Ensure that you have available [GPU quotas](/compute/quotas#gpus) in the zone.\n- The creation of the instance takes some time. You can watch the progress by running the following commands:```\nexport INSTANCE_GROUP_NAME=\"deeplearning-instance-group\"\n``````\ngcloud compute instance-groups managed list-instances $INSTANCE_GROUP_NAME --region us-central1\n``` \n- When the managed instance group is created, you should see an output that resembles the following: \n- Confirm that metrics are available on the Google Cloud Cloud Monitoring page.- In the Google Cloud console, go to the **Monitoring** page. [Go to Monitoring](https://console.cloud.google.com/monitoring) \n- If **Metrics Explorer** is shown in the navigation pane, click **Metrics Explorer** . Otherwise, select **Resources** and then select **Metrics Explorer** .\n- Search for `gpu_utilization` . \n- If data is coming in, you should see something like this: \n### Enable autoscaling\n- Enable autoscaling for the managed instance group.```\nexport INSTANCE_GROUP_NAME=\"deeplearning-instance-group\"\ngcloud compute instance-groups managed set-autoscaling $INSTANCE_GROUP_NAME \\\n --custom-metric-utilization metric=custom.googleapis.com/gpu_utilization,utilization-target-type=GAUGE,utilization-target=85 \\\n --max-num-replicas 4 \\\n --cool-down-period 360 \\\n --region us-central1\n```The `custom.googleapis.com/gpu_utilization` is the full path to our metric. The sample specifies level 85, this means that whenever GPU utilization reaches 85, the platform creates a new instance in our group.\n- Test the autoscaling. To test the autoscaling, you need to perform the following steps:- SSH to the instance. See [Connecting to Instances](/compute/docs/instances/connecting-to-instance) .\n- Use the `gpu-burn` tool to load your GPU to 100% utilization for 600 seconds:```\ngit clone https://github.com/GoogleCloudPlatform/ml-on-gcp.git\ncd ml-on-gcp/third_party/gpu-burn\ngit checkout c0b072aa09c360c17a065368294159a6cef59ddf\nmake\n./gpu_burn 600 > /dev/null &\n```\n- View the Cloud Monitoring page. Observe the autoscaling. The cluster scales up by adding one more instance. \n- In the Google Cloud console, go to the **Instance groups** page. [Go to Instance groups](https://console.cloud.google.com/compute/instanceGroups) \n- Click the `deeplearning-instance-group` managed instance group.\n- Click the **Monitoring** tab.At this point your autoscaling logic should be trying to spin as much instances as possible to reduce the load, without any luck: At this point you can stop burning instances, and observe how the system scales down.\n### Set up a load balancerLet's revisit what you have so far:- A trained model, optimized with TensorRT5 (INT8)\n- A managed group of instances. These instances have auto scaling enable based on the GPU utilization enabled\nNow you can create a load balancer in front of the instances.- Create health checks. Health checks are used to determine if a particular host on our backend can serve the traffic.```\nexport HEALTH_CHECK_NAME=\"http-basic-check\"\ngcloud compute health-checks create http $HEALTH_CHECK_NAME \\\n --request-path /v1/models/default \\\n --port 8888\n```\n- Create a backend service that includes an instance group and health check.- Create the health check.```\nexport HEALTH_CHECK_NAME=\"http-basic-check\"\nexport WEB_BACKED_SERVICE_NAME=\"tensorflow-backend\"\ngcloud compute backend-services create $WEB_BACKED_SERVICE_NAME \\\n --protocol HTTP \\\n --health-checks $HEALTH_CHECK_NAME \\\n --global\n```\n- Add the instance group to the new backend service.```\nexport INSTANCE_GROUP_NAME=\"deeplearning-instance-group\"\nexport WEB_BACKED_SERVICE_NAME=\"tensorflow-backend\"\ngcloud compute backend-services add-backend $WEB_BACKED_SERVICE_NAME \\\n --balancing-mode UTILIZATION \\\n --max-utilization 0.8 \\\n --capacity-scaler 1 \\\n --instance-group $INSTANCE_GROUP_NAME \\\n --instance-group-region us-central1 \\\n --global\n```\n- Set up forwarding URL. The load balancer needs to know which URL can be forwarded to the backends services.```\nexport WEB_BACKED_SERVICE_NAME=\"tensorflow-backend\"\nexport WEB_MAP_NAME=\"map-all\"\ngcloud compute url-maps create $WEB_MAP_NAME \\\n --default-service $WEB_BACKED_SERVICE_NAME\n```\n- Create the load balancer.```\nexport WEB_MAP_NAME=\"map-all\"\nexport LB_NAME=\"tf-lb\"\ngcloud compute target-http-proxies create $LB_NAME \\\n --url-map $WEB_MAP_NAME\n```\n- Add an external IP address to the load balancer.```\nexport IP4_NAME=\"lb-ip4\"\ngcloud compute addresses create $IP4_NAME \\\n --ip-version=IPV4 \\\n --network-tier=PREMIUM \\\n --global\n```\n- Find the IP address that is allocated.```\ngcloud compute addresses list\n```\n- Set up the forwarding rule that tells Google Cloud to forward all requests from the public IP address to the load balancer.```\nexport IP=$(gcloud compute addresses list | grep ${IP4_NAME} | awk '{print $2}')\nexport LB_NAME=\"tf-lb\"\nexport FORWARDING_RULE=\"lb-fwd-rule\"\ngcloud compute forwarding-rules create $FORWARDING_RULE \\\n --address $IP \\\n --global \\\n --load-balancing-scheme=EXTERNAL \\\n --network-tier=PREMIUM \\\n --target-http-proxy $LB_NAME \\\n --ports 80\n```After creating the global forwarding rules, it can take several minutes for your configuration to propagate.\n### Enable firewall\n- Check if you have firewall rules that allow connections from external sources to your VM instances.```\ngcloud compute firewall-rules list\n```\n- If you do not have firewall rules to allow these connections, you must create them. To create firewall rules, run the following commands:```\ngcloud compute firewall-rules create www-firewall-80 \\\n --target-tags http-server --allow tcp:80\ngcloud compute firewall-rules create www-firewall-8888 \\\n --target-tags http-server --allow tcp:8888\n```\n## Running an inference\n- You can use the following python script to convert images to a format that can uploaded to the server.```\nfrom PIL import Image\nimport numpy as np\nimport json\nimport codecs\nimg = Image.open(\"image.jpg\").resize((240, 240))\nimg_array=np.array(img)\nresult = {\n  \"instances\":[img_array.tolist()]\n  }\nfile_path=\"/tmp/out.json\"\nprint(json.dump(result, codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4))\n```\n- Run the inference.```\ncurl -X POST $IP/v1/models/default:predict -d @/tmp/out.json\n```\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.- Delete forwarding rules.```\ngcloud compute forwarding-rules delete $FORWARDING_RULE --global\n```\n- Delete the IPV4 address.```\ngcloud compute addresses delete $IP4_NAME --global\n```\n- Delete the load balancer.```\ngcloud compute target-http-proxies delete $LB_NAME\n```\n- Delete the forwarding URL.```\ngcloud compute url-maps delete $WEB_MAP_NAME\n```\n- Delete the backend service.```\ngcloud compute backend-services delete $WEB_BACKED_SERVICE_NAME --global\n```\n- Delete health checks.```\ngcloud compute health-checks delete $HEALTH_CHECK_NAME\n```\n- Delete the managed instance group.```\ngcloud compute instance-groups managed delete $INSTANCE_GROUP_NAME --region us-central1\n```\n- Delete the instance template.```\ngcloud beta compute --project=$PROJECT_NAME instance-templates delete $INSTANCE_TEMPLATE_NAME\n```\n- Delete the firewall rules.```\ngcloud compute firewall-rules delete www-firewall-80\n``````\ngcloud compute firewall-rules delete www-firewall-8888\n```", "guide": "Compute Engine"}