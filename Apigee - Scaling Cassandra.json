{"title": "Apigee - Scaling Cassandra", "url": "https://cloud.google.com/apigee/docs/api-platform/get-started/what-apigee", "abstract": "# Apigee - Scaling Cassandra\nYou are currently viewing version 1.3 of the Apigee hybrid documentation. **This version is end of life.** You should upgrade to a newer version. For more information, see [Supported versions](/apigee/docs/hybrid/supported-platforms#supported-versions) .\nThis topic discusses how to scale up Cassandra horizontally and vertically, and how to scale down Cassandra.\n", "content": "## \n Scaling Cassandra horizontally\nTo scale up Cassandra horizontally\n- Make sure that your`apigee-data`node pool has additional capacity, as needed, before scaling Cassandra. See also [Configure dedicated node pools](/apigee/docs/hybrid/v1.3/configure-dedicated-nodes) .\n- Set the value of the`cassandra.replicaCount`configuration property in your overrides file. For information about this property, see the [Configuration property reference](/apigee/docs/hybrid/v1.3/config-prop-ref#cassandra) . See also [Manage runtime plane components](/apigee/docs/hybrid/v1.3/customize-services) . **Note:** The default replication factor for all keyspaces is three. As a result, we recommend that you scale the replicas by a factor of three.\n- Apply the changes. For example:```\n$APIGEE_HOME/apigeectl apply --datastore -f overrides/overrides.yaml\n```## \n Scaling Cassandra vertically\nThis section explains how to scale the Cassandra pods vertically to accommodate higher CPU and memory requirements.\n### \n Overview\nFor an Apigee hybrid production deployment, we recommend that you create at least two separate node pools: one for stateful services (Cassandra) and one for stateless (runtime) services. For example, see [GKE production cluster requirements](/apigee/docs/hybrid/v1.3/gke-cluster-setup) .\nFor the stateful Cassandra node pool, we recommend starting with 8 CPU cores and 30 GB of memory. Once the node pool is provisioned, these settings cannot be changed. See also [Configure Cassandra for production](/apigee/docs/hybrid/v1.3/cassandra-production) .\nIf you need to scale up the Cassandra pods to accommodate higher CPU and memory requirements, follow the steps described in this topic.\n### \n Scaling up the Cassandra pods\nFollow these steps to increase the CPU and memory for the stateful node pool used for Cassandra:\n- Follow your Kubernetes platform's instructions to add a new node pool to the cluster. Supported  platforms are listed in the [installation instructions](/apigee/docs/hybrid/v1.3/install-before-begin) .\n- Verify that the new node pool is ready:```\nkubectl get nodes -l NODE_POOL_LABEL_NAME=NODE_POOL_LABEL_VALUE\n```Example command:```\nkubectl get nodes -l cloud.google.com/gke-nodepool=apigee-data-new\n```Example output:```\nNAME            STATUS ROLES AGE  VERSION\ngke-apigee-data-new-441387c2-2h5n Ready <none> 4m28s v1.14.10-gke.17\ngke-apigee-data-new-441387c2-6941 Ready <none> 4m28s v1.14.10-gke.17\ngke-apigee-data-new-441387c2-nhgc Ready <none> 4m29s v1.14.10-gke.17\n```\n- Update your overrides file to use the new node pool for Cassandra and  update the pod resources to the increased CPU count and memory size that you  wish to use. For example, for a GKE cluster, use a configuration similar to the following.  If you are on another Kubernetes platform, you need to adjust the`apigeeData.key`value  accordingly:```\nnodeSelector:\n requiredForScheduling: true\n apigeeData:\n key: \"NODE_POOL_LABEL_NAME\"\n value: \"NODE_POOL_LABEL_VALUE\"\ncassandra:\n resources:\n requests:\n  cpu: NODE_POOL_CPU_NUMBER\n  memory: NODE_POOL_MEMORY_SIZE\n```For example:```\nnodeSelector:\n requiredForScheduling: true\n apigeeData:\n key: \"cloud.google.com/gke-nodepool\"\n value: \"apigee-data-new\"\ncassandra:\n resources:\n requests:\n  cpu: 14\n  memory: 16Gi\n```\n- Apply the overrides file to the cluster:```\n$APIGEECTL_HOME/apigeectl apply -f ./overrides/overrides.yaml --datastore\n```\nWhen you complete these steps, the Cassandra pods will begin rolling over to the new node pool.\n## \n Scaling down Cassandra\n**CAUTION:** Scaling down cassandra clusters are more invasive than scaling up. Please proceed with caution when scaling down\nApigee hybrid employs a ring of Cassandra nodes as a [StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) . Cassandra provides persistent storage for certain Apigee entities on the runtime plane. For more information about Cassandra, see [About the runtime plane](/apigee/docs/hybrid/v1.3/what-is-hybrid#about-the-runtime-plane) .\nCassandra is a resource intensive service and should not be deployed on a pod with any other hybrid services. Depending on the load, you might want to scale the number of Cassandra nodes in the ring down in your cluster.\nThe general process for scaling down a Cassandra ring is:\n- Decommission one Cassandra node.\n- Update the`cassandra.replicaCount`property in`overrides.yaml`.\n- Apply the configuration update.\n- Repeat these steps for each node you want remove.\n- Delete the persistent volume claim or volume, depending on your cluster configuration.\n### \n What you need to know\n- Perform this task on one node at a time before proceeding to the  next node.\n- If any node other than the node to be  decommissioned is unhealthy, do not proceed. Kubernetes will not be  able to downscale the pods  from the cluster.\n- Always scale down or up by a factor of three nodes.\n### Prerequisites\nBefore you scale down the number of Cassandra nodes in the ring, validate if the cluster is healthy and all the nodes are up and running, as the following example shows:\n```\n kubectl get pods -n yourNamespace -l app=apigee-cassandra\nNAME     READY STATUS RESTARTS AGE\napigee-cassandra-default-0 1/1  Running 0   2h\napigee-cassandra-default-1 1/1  Running 0   2h\napigee-cassandra-default-2 1/1  Running 0   2h\napigee-cassandra-default-3 1/1  Running 0   16m\napigee-cassandra-default-4 1/1  Running 0   14m\napigee-cassandra-default-5 1/1  Running 0   13m\n```\n```\nkubectl -n yourNamespace exec -it apigee-cassandra-default-0 nodetool status\nDatacenter: us-east1\n====================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n-- Address  Load  Tokens  Owns (effective) Host ID        Rack\nUN 10.16.2.6 690.17 KiB 256   48.8%    b02089d1-0521-42e1-bbed-900656a58b68 ra-1\nUN 10.16.4.6 700.55 KiB 256   51.6%    dc6b7faf-6866-4044-9ac9-1269ebd85dab ra-1 to\nUN 10.16.11.11 144.36 KiB 256   48.3%    c7906366-6c98-4ff6-a4fd-17c596c33cf7 ra-1\nUN 10.16.1.11 767.03 KiB 256   49.8%    ddf221aa-80aa-497d-b73f-67e576ff1a23 ra-1\nUN 10.16.5.13 193.64 KiB 256   50.9%    2f01ac42-4b6a-4f9e-a4eb-4734c24def95 ra-1\nUN 10.16.8.15 132.42 KiB 256   50.6%    a27f93af-f8a0-4c88-839f-2d653596efc2 ra-1\n```\n**CAUTION:** If the cluster is not healthy or at least one node is not up and running, do not proceed with this process.\n### \n Decommission the Cassandra nodes\n- Decommission the Cassandra nodes from the cluster using the **nodetool** command.Always decommission the nodes with higher numbers in the pod name first. For example in a 6 node cluster start with  the apigee-cassandra-5 Cassandra node.```\nkubectl -n yourNamespace exec -it nodeName nodetool decommission\n```For example, this command decommissions `apigee-cassandra-5` , the node with the highest number value in the name:```\nkubectl -n apigee exec -it apigee-cassandra-5 nodetool decommission\n```\n- Wait for the decommission to complete, and verify that the cluster now has one less node. For example:```\nkubectl -n yourNamespace exec -it nodeName nodetool status\nDatacenter: us-east1\n====================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n-- Address  Load  Tokens  Owns (effective) Host ID        Rack\nUN 10.16.2.6 710.37 KiB 256   59.0%    b02089d1-0521-42e1-bbed-900656a58b68 ra-1\nUN 10.16.4.6 720.97 KiB 256   61.3%    dc6b7faf-6866-4044-9ac9-1269ebd85dab ra-1\nUN 10.16.1.11 777.11 KiB 256   58.9%    ddf221aa-80aa-497d-b73f-67e576ff1a23 ra-1\nUN 10.16.5.13 209.23 KiB 256   62.2%    2f01ac42-4b6a-4f9e-a4eb-4734c24def95 ra-1\nUN 10.16.8.15 143.23 KiB 256   58.6%    a27f93af-f8a0-4c88-839f-2d653596efc2 ra-1\n```\n- Update or add the`cassandra.replicaCount`property in your`overrides.yaml`file. For example, if the current node count is 6, change it to 5:```\ncassandra:\n replicaCount: 5 # (n-1 5 in this example)\n```\n- Apply the configuration change to your cluster:```\n./apigeectl apply --datastore\nnamespace/apigee unchanged\nsecret/ssl-cassandra unchanged\nstorageclass.storage.k8s.io/apigee-gcepd unchanged\nservice/apigee-cassandra unchanged\nstatefulset.apps/apigee-cassandra configured\n``` **Note:** you must update and apply the`cassandra.replicaCount`property to n-1 after decommissioning each node. For example if you are scaling down from six  nodes to three, you will need to change and apply`cassandra.replicaCount`three times.\n- Verify that all of the remaining Cassandra nodes are running:```\nkubectl get pods -n yourNamespace -l app=apigee-cassandra\nNAME     READY STATUS RESTARTS AGE\napigee-cassandra-default-0 1/1  Running 0   3h\napigee-cassandra-default-1 1/1  Running 0   3h\napigee-cassandra-default-2 1/1  Running 0   2h\napigee-cassandra-default-3 1/1  Running 0   25m\napigee-cassandra-default-4 1/1  Running 0   24m\n```\n- Repeat Steps 1-5 for each node that you wish to decommission.\n- When you are finished decommissioning nodes, verify that the`cassandra.replicaCount`value equals the number of  nodes returned by the`nodetool status`command.For example, if you scaled Cassandra down to three nodes:```\nkubectl -n yourNamespace exec -it apigee-cassandra-default-0 nodetool status\nDatacenter: us-east1\n====================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n-- Address  Load  Tokens  Owns (effective) Host ID        Rack\nUN 10.16.2.6 710.37 KiB 256   59.0%    b02089d1-0521-42e1-bbed-900656a58b68 ra-1\nUN 10.16.4.6 720.97 KiB 256   61.3%    dc6b7faf-6866-4044-9ac9-1269ebd85dab ra-1\nUN 10.16.5.13 209.23 KiB 256   62.2%    2f01ac42-4b6a-4f9e-a4eb-4734c24def95 ra-1\n```\n- After the Cassandra cluster is downsized, make sure to delete the pvc  (PersistentVolumeClaim) to make sure next scale up event does not  use the same Persistent volume and the data created earlier.Get the names of the pvcs:```\nkubectl get pvc -n yourNamespace\nNAME        STATUS VOLUME          CAPACITY ACCESS MODES STORAGECLASS AGE\ncassandra-data-apigee-cassandra-default-0 Bound pvc-f9c2a5b9-818c-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 7h\ncassandra-data-apigee-cassandra-default-1 Bound pvc-2956cb78-818d-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 7h\ncassandra-data-apigee-cassandra-default-2 Bound pvc-79de5407-8190-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 7h\ncassandra-data-apigee-cassandra-default-3 Bound pvc-d29ba265-81a2-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 5h\ncassandra-data-apigee-cassandra-default-4 Bound pvc-0675a0ff-81a3-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 5h\ncassandra-data-apigee-cassandra-default-5 Bound pvc-354afa95-81a3-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 5h\n```In this example the following pvcs correspond with the three decommissioned nodes:- `cassandra-data-apigee-cassandra-5`\n- `cassandra-data-apigee-cassandra-4`\n- `cassandra-data-apigee-cassandra-3`\n- Delete the pvcs:```\nkubectl -n yourNamespace delete pvc cassandra-data-apigee-cassandra-5\npersistentvolumeclaim \"cassandra-data-apigee-cassandra-5\" deleted\nkubectl -n yourNamespace delete pvc cassandra-data-apigee-cassandra-4\npersistentvolumeclaim \"cassandra-data-apigee-cassandra-4\" deleted\nkubectl -n yourNamespace delete pvc cassandra-data-apigee-cassandra-3\npersistentvolumeclaim \"cassandra-data-apigee-cassandra-3\" deleted\n```\n- Verify that the pvc was deleted:```\nkubectl get pvc -n yourNamespace\nNAME        STATUS VOLUME          CAPACITY ACCESS MODES STORAGECLASS AGE\ncassandra-data-apigee-cassandra-default-0 Bound pvc-f9c2a5b9-818c-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 7h\ncassandra-data-apigee-cassandra-default-1 Bound pvc-2956cb78-818d-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 7h\ncassandra-data-apigee-cassandra-default-2 Bound pvc-79de5407-8190-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 7h\ncassandra-data-apigee-cassandra-default-3 Bound pvc-d29ba265-81a2-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 5h\ncassandra-data-apigee-cassandra-default-4 Bound pvc-0675a0ff-81a3-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 5h\n```\n- If you are using Anthos installation, delete the Persistent volume from Anthos  Kubernetes cluster using the same sequence.Get the names of the persistent volumes:```\nkubectl get pv -n youNamespace\nNAME          CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM          STORAGECLASS REASON AGE\npvc-0675a0ff-81a3-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-default-4 apigee-gcepd   5h\npvc-2956cb78-818d-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-default-1 apigee-gcepd   7h\npvc-354afa95-81a3-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-default-5 apigee-gcepd   5h\npvc-79de5407-8190-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-default-2 apigee-gcepd   7h\npvc-d29ba265-81a2-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-default-3 apigee-gcepd   5h\npvc-f9c2a5b9-818c-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-default-0 apigee-gcepd   7h\n```In this example the following volumes correspond with the three decommissioned nodes:- **5:** `pvc-354afa95-81a3-11e9-8862-42010a8e014a`\n- **4:** `pvc-0675a0ff-81a3-11e9-8862-42010a8e014a`\n- **3:** `pvc-d29ba265-81a2-11e9-8862-42010a8e014a`\n- Delete the persistent volumes:```\nkubectl -n yourNamespace delete pv pvc-354afa95-81a3-11e9-8862-42010a8e014a\nkubectl -n yourNamespace delete pv pvc-0675a0ff-81a3-11e9-8862-42010a8e014a\nkubectl -n yourNamespace delete pv pvc-d29ba265-81a2-11e9-8862-42010a8e014a\n```\n- Verify that the persistent volumes were deleted:```\nkubectl get pv -n youNamespace\nNAME          CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM          STORAGECLASS REASON AGE\npvc-2956cb78-818d-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-default-1 apigee-gcepd   7h\npvc-79de5407-8190-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-default-2 apigee-gcepd   7h\npvc-f9c2a5b9-818c-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-default-0 apigee-gcepd   7h\n```", "guide": "Apigee"}