{"title": "Cloud Architecture Center - Security log analytics in Google Cloud", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Security log analytics in Google Cloud\nLast reviewed 2023-02-06 UTC\nThis guide shows security practitioners how to onboard Google Cloud logs to be used in security analytics. By performing security analytics, you help your organization prevent, detect, and respond to threats like malware, phishing, ransomware, and poorly configured assets.\nThis guide shows you how to do the following:\n- Enable the logs to be analyzed.\n- Route those logs to a single destination depending on your choice of security analytics tool, such as [Log Analytics](/logging/docs/log-analytics#analytics) , [BigQuery](/bigquery/docs) , [Chronicle](/chronicle/docs) , or a third-party security information and event management (SIEM) technology.\n- Analyze those logs to audit your cloud usage and detect potential threats to your data and workloads, using sample queries from the [Community Security Analytics (CSA)](https://github.com/GoogleCloudPlatform/security-analytics) project.\nThe information in this guide is part of Google Cloud [Autonomic Security Operations](https://cloud.google.com/solutions/security-analytics-and-operations) , which includes engineering-led transformation of detection and response practices and security analytics to improve your threat detection capabilities.\nIn this guide, logs provide the data source to be analyzed. However, you can apply the concepts from this guide to analysis of other complementary security-related data from Google Cloud, such as [security findingsfrom Security Command Center](/security-command-center/docs/concepts-vulnerabilities-findings) . Provided in Security Command Center Premium is a list of regularly-updated managed detectors that are designed to identify threats, vulnerabilities, and misconfigurations within your systems in near real-time. By analyzing these signals from Security Command Center and correlating them with logs ingested in your security analytics tool as described in this guide, you can achieve a broader perspective of potential security threats.\nThe following diagram shows how security data sources, security analytics tools, and CSA queries work together.\nThe diagram starts with the following security data sources: logs from Cloud Logging, asset changes from Cloud Asset Inventory, and security findings from Security Command Center. The diagram then shows these security data sources being routed into the security analytics tool of your choice: Log Analytics in Cloud Logging, BigQuery, Chronicle, or a third-party SIEM. Finally, the diagram shows using CSA queries with your analytics tool to analyze the collated security data.\n", "content": "## Security log analytics workflow\nThis section describe the steps to set up security log analytics in Google Cloud. The workflow consists of the three steps shown in the following diagram and described in the following paragraphs:- **Enable logs:** There are many security logs available in Google Cloud. Each log has different information that can be useful in answering specific security questions. Some logs like Admin Activity audit logs are enabled by default; others need to be manually enabled because they incur additional ingestion costs in Cloud Logging. Therefore, the first step in the workflow is to prioritize the security logs that are most relevant for your security analysis needs and to individually enable those specific logs.To help you evaluate logs in terms of the visibility and threat detection coverage they provide, this guide includes a [log scoping tool](#log_scoping_tool) . This tool maps each log to relevant threat tactics and techniques in the [MITRE ATT&CK\u00ae Matrix for Enterprise](https://attack.mitre.org/matrices/enterprise/cloud/) . The tool also maps [Event Threat Detection](/security-command-center/docs/concepts-event-threat-detection-overview) rules in Security Command Center to the logs on which they rely. You can use the log scoping tool to evaluate logs regardless of the analytics tool that you use.\n- **Route logs:** After identifying and enabling the logs to be analyzed, the next step is to route and aggregate the logs from your organization, including any contained folders, projects, and billing accounts. How you route logs depends on the analytics tool that you use.This guide describes common log routing destinations, and shows you how to use a Cloud Logging [aggregated sink](/logging/docs/export/aggregated_sinks) to route organization-wide logs into a Cloud Logging [log bucket](/logging/docs/routing/overview#buckets) or a BigQuery dataset depending on whether you choose to use Log Analytics or BigQuery for analytics.\n- **Analyze logs:** After you route the logs into an analytics tool, the next step is to perform an analysis of these logs to identify any potential security threats. How you analyze the logs depends on the analytics tool that you use. If you use Log Analytics or BigQuery, you can analyze the logs by using SQL queries. If you use Chronicle, you analyze the logs by using [YARA-L rules](/chronicle/docs/detection/yara-l-2-0-overview) . If you are using a third-party SIEM tool, you use the query language specified by that tool.In this guide, you'll find SQL queries that you can use to analyze the logs in either Log Analytics or BigQuery. The SQL queries provided in this guide come from the [Community Security Analytics (CSA)](https://github.com/GoogleCloudPlatform/security-analytics) project. CSA is an open-source set of foundational security analytics designed to provide you with a baseline of pre-built queries and rules that you can reuse to start analyzing your Google Cloud logs.\nThe following sections provide detailed information on how to set up and apply each step in the security logs analytics workflow.\n## Enable logs\nThe process of enabling logs involves the following steps:\n- Identify the logs you need by using the log scoping tool in this guide.\n- Record the log filter generated by the log scoping tool for use later when configuring the log sink.\n- Enable logging for each identified log type or Google Cloud service. Depending on the service, you might have to also enable the corresponding Data Access audit logs as detailed later in this section.\n### Identify logs using the log scoping tool\nTo help you identify the logs that meet your security and compliance needs, you can use the log scoping tool shown in this section. This tool provides an interactive table that lists valuable security-relevant logs across Google Cloud including Cloud Audit Logs, Access Transparency logs, network logs, and several platform logs. This tool maps each log type to the following areas:\n- [MITRE ATT&CK](https://attack.mitre.org/matrices/enterprise/cloud/) threat tactics and techniques that can be monitored with that log.\n- [CIS Google Cloud Computing Platform](https://www.cisecurity.org/benchmark/google_cloud_computing_platform/) compliance violations that can be detected in that log.\n- [Event Threat Detection](/security-command-center/docs/concepts-event-threat-detection-overview#rules) rules that rely on that log.\nThe log scoping tool also generates a log filter which appears immediately after the table. As you identify the logs that you need, select those logs in the tool to automatically update that log filter.\nThe following short procedures explain how to use the log scoping tool:\n- To select or remove a log in the log scoping tool, click the toggle next to the name of the log.\n- To select or remove all the logs, click the toggle next to the **Log\ntype** heading.\n- To see which MITRE ATT&CK techniques can be monitored by each log type, clickadd_circlenext to the **MITRE ATT&CK tactics and techniques** heading.\n### Record the log filter\nThe log filter that is automatically generated by the log scoping tool contains all of the logs that you have selected in the tool. You can use the filter as is or you can refine the log filter further depending on your requirements. For example, you can include (or exclude) resources only in one or more specific projects. After you have a log filter that meets your logging requirements, you need to save the filter for use when routing the logs. For instance, you can save the filter in a text editor or save it in an environment variable as follows:\n- In the \"Auto-generated log filter\" section that follows the tool, copy the code for the log filter.\n- Optional: Edit the copied code to refine the filter.\n- In [Cloud Shell](/shell/docs/launching-cloud-shell) , create a variable to save the log filter:```\nexport LOG_FILTER='LOG_FILTER'\n```Replace `` with the code for the log filter.\n### Enable service-specific platform logs\nFor each of the platform logs that you select in the log scoping tool, those logs must be enabled (typically at the resource level) on a service-by-service basis. For example, Cloud DNS logs are enabled at the VPC-network level. Likewise, VPC Flow Logs are enabled at the subnet level for all VMs in the subnet, and logs from Firewall Rules Logging are enabled at the individual firewall rule level.\nEach platform log has its own instructions on how to enable logging. However, you can use the log scoping tool to quickly open the relevant instructions for each platform log.\nTo learn how to enable logging for a specific platform log, do the following:\n- In the log scoping tool, locate the platform log that you want to enable.\n- In the **Enabled by default** column, click the **Enable** link that corresponds to that log. The link takes you to detailed instructions on how to enable logging for that service.\n### Enable the Data Access audit logs\nAs you can see in the log scoping tool, the Data Access audit logs from Cloud Audit Logs provide broad threat detection coverage. However, their volume can be quite large. Enabling these Data Access audit logs might therefore result in additional charges related to ingesting, storing, exporting, and processing these logs. This section both explains how to enable these logs and presents some best practices to help you with making the tradeoff between value and cost.\n**Note:** Data Access audit logs might contain personally identifiable information (PII) like caller identities and IP addresses. You must apply the appropriate access control and retention settings available in your analytics tool to secure your log data, retain that data only as long as needed, and then dispose of that data securely.\nData Access audit logs\u2014except for BigQuery\u2014are disabled by default. To configure Data Access audit logs for Google Cloud services other than BigQuery, you must explicitly enable them either by [using the Google Cloud console](/logging/docs/audit/configure-data-access#config-console) or by [using the Google Cloud CLI](/logging/docs/audit/configure-data-access#config-api) to edit Identity and Access Management (IAM) policy objects. When you enable Data Access audit logs, you can also configure which types of operations are recorded. There are three Data Access audit log types:\n- `ADMIN_READ`: Records operations that read metadata or configuration information.\n- `DATA_READ`: Records operations that read user-provided data.\n- `DATA_WRITE`: Records operations that write user-provided data.\nNote that you can't configure the recording of `ADMIN_WRITE` operations, which are operations that write metadata or configuration information. `ADMIN_WRITE` operations are included in Admin Activity audit logs from Cloud Audit Logs and therefore can't be disabled.\n**Best Practice** : Enable Data Access audit logs at the folder or organization level to ensure compliance across all child projects of that folder or organization. When you enable the audit logs at the folder or organization level, the audit policy applies to all existing and new projects in that folder or organization. That audit policy cannot be disabled at the project level.\nWhen enabling Data Access audit logs, the goal is to maximize their value in terms of security visibility while also limiting their cost and management overhead. To help you achieve that goal, we recommend that you do the following to filter out low-value, high-volume logs:\n- **Prioritize relevant services** such as services that host sensitive workloads, keys and data. For specific examples of services that you might want to prioritize over others, see [Example Data Access audit log configuration](#example_data_access_audit_config) .\n- **Prioritize relevant projects** such as projects that host production workloads as opposed to projects that host developer and staging environments. To filter out all logs from a particular project, add the following expression to your log filter for your sink. Replace with the ID of the project from which you want to filter out all logs:| Project        | Log filter expression     |\n|:--------------------------------------|:--------------------------------------|\n| Exclude all logs from a given project | NOT logName =~ \"^projects/PROJECT_ID\" |\n- **Prioritize a subset of data access operations** such as `ADMIN_READ` , `DATA_READ` , or `DATA_WRITE` for a minimal set of recorded operations. For example, some services like Cloud DNS write all three types of operations, but you can enable logging for only `ADMIN_READ` operations. After you have configured one of more of these three types of data access operations, you might want to exclude specific operations that are particularly high volume. You can exclude these high volume operations by modifying the sink's log filter. For example, you decide to enable full Data Access audit logging, including `DATA_READ` operations on some critical storage services. To exclude specific high-traffic data read operations in this situation, you can add the following recommended log filter expressions to your sink's log filter:| Service          | Log filter expression                             |\n|:--------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------|\n| Exclude high volume logs from Cloud Storage | NOT (resource.type=\"gcs_bucket\" AND (protoPayload.methodName=\"storage.buckets.get\" OR protoPayload.methodName=\"storage.buckets.list\")) |\n| Exclude high volume logs from Cloud SQL  | NOT (resource.type=\"cloudsql_database\" AND protoPayload.request.cmd=\"select\")               |\n- **Prioritize relevant resources** such as resources that host your most sensitive workloads and data. You can classify your resources based on the value of the data that they process, and their security risk such as whether they are externally accessible or not. Although Data Access audit logs are enabled per service, you can filter out specific resources or resource types through the log filter.\n- **Exclude specific principals** from having their data accesses recorded. For example, you can exempt your internal testing accounts from having their operations recorded. To learn more, see [Set exemptions](/logging/docs/audit/configure-data-access#config-console-exempt) in Data Access audit logs documentation.\n**Note:** In addition to using the sink's log filter to filter out the additional logs as discussed in this section, you might want to exclude these logs from being ingested into Cloud Logging for cost reasons. To prevent these logs from being ingested, you can apply the log filter expressions listed in this section as [exclusion filters](/logging/docs/routing/overview#exclusions) on the predefined `_Default` sink that routes logs (including Data Access audit logs) to the [_Default log bucket](/logging/docs/routing/overview#default-bucket) . Exclusion filters have the opposite effect of a log filter, which is an inclusion filter. Thus when configuring these expressions as exclusion filters, you need to remove the preceding `NOT` Boolean operator from the filter expressions that are shown in this section.\nThe following table provides a baseline Data Access audit log configuration that you can use for Google Cloud projects to limit log volumes while gaining valuable security visibility:\n| Tier         | Services                  | Data Access audit log types | MITRE ATT&CK tactics        |\n|:----------------------------------------|:------------------------------------------------------------------------------|:------------------------------|:-------------------------------------------------|\n| Authentication & authorization services | IAM Identity-Aware Proxy (IAP)1 Cloud KMS Secret Manager Resource Manager | ADMIN_READ DATA_READ   | Discovery Credential Access Privilege Escalation |\n| Storage services      | BigQuery (enabled by default) Cloud Storage1, 2        | DATA_READ DATA_WRITE   | Collection Exfiltration       |\n| Infrastructure services     | Compute Engine Organization Policy           | ADMIN_READ     | Discovery          |\nEnabling Data Access audit logs for IAP or Cloud Storage can generate large log volumes when there is high traffic to IAP-protected web resources or to Cloud Storage objects.\nEnabling Data Access audit logs for Cloud Storage might break the use of [authenticated browser downloads](/storage/docs/request-endpoints#cookieauth) for non-public objects. For more details and suggested workarounds to this issue, see the [Cloud Storage troubleshootingguide](/storage/docs/troubleshooting#trouble-download-storage-cloud) .\nIn the example configuration, notice how services are grouped in tiers of sensitivity based on their underlying data, metadata, or configuration. These tiers demonstrate the following recommended granularity of Data Access audit logging:\n- Authentication & authorization services: For this tier of services, we recommend auditing all data access operations. This level of auditing helps you monitor access to your sensitive keys, secrets, and IAM policies. Monitoring this access might help you detect MITRE ATT&CK tactics like [Discovery](https://attack.mitre.org/tactics/TA0007/) , [Credential Access](https://attack.mitre.org/tactics/TA0006/) , and [Privilege Escalation](https://attack.mitre.org/tactics/TA0004/) .\n- Storage services: For this tier of services, we recommend auditing data access operations that involve user-provided data. This level of auditing helps you monitor access to your valuable and sensitive data. Monitoring this access might help you detect MITRE ATT&CK tactics like [Collection](https://attack.mitre.org/tactics/TA0009/) and [Exfiltration](https://attack.mitre.org/tactics/TA0010/) against your data.\n- Infrastructure services: For this tier of services, we recommend auditing data access operations that involve metadata or configuration information. This level of auditing helps you monitor for scanning of infrastructure configuration. Monitoring this access might help you detect MITRE ATT&CK tactics like [Discovery](https://attack.mitre.org/tactics/TA0007/) against your workloads.## Route logs\nAfter the logs are identified and enabled, the next step is to route the logs to a single destination. The routing destination, path and complexity vary depending on the analytics tools that you use, as shown in the following diagram.\nThe diagram shows the following routing options:\n- If you use Log Analytics, you need an [aggregated sink](/logging/docs/export/aggregated_sinks) to aggregate the logs from across your Google Cloud organization into a single Cloud Logging bucket.\n- If you use BigQuery, you need an aggregated sink to aggregate the logs from across your Google Cloud organization into a single BigQuery dataset.\n- If you use Chronicle and this [predefined subset of logs](/chronicle/docs/ingestion/cloud/ingest-gcp-logs#export-logs) meets your security analysis needs, you can automatically aggregate these logs into your Chronicle account using the built-in Chronicle ingest. You can also view this predefined set of logs by looking at the **Exportable directly to Chronicle** column of the log scoping tool. For more information about exporting these predefined logs, see [Ingest Google Cloud logs to Chronicle](/chronicle/docs/ingestion/cloud/ingest-gcp-logs) .\n- If you use BigQuery or a third-party SIEM or want to export an expanded set of logs into Chronicle, the diagram shows that an additional step is needed between enabling the logs and analyzing them. This additional step consists of configuring an aggregated sink that routes the selected logs appropriately. If you're using BigQuery, this sink is all that you need to route the logs to BigQuery. If you're using a third-party SIEM, you need to have the sink aggregate the selected logs in Pub/Sub or Cloud Storage before the logs can be pulled into your analytics tool.\n**Note:** If you use Log Analytics (or BigQuery) and have already configured an aggregated sink to store logs in a central Logging bucket (or a central BigQuery dataset), you can skip this section of the guide and instead just update the sink with the log filter from the previous section. In the case of Log Analytics, make sure to upgrade your existing [log bucket to use Log Analytics](/logging/docs/buckets#upgrade-bucket) .\nThe routing options to Chronicle and a third-party SIEM aren't covered in this guide. However, the following sections provide the detailed steps to route logs to Log Analytics or BigQuery:\n- Set up a single destination\n- Create an aggregated log sink.\n- Grant access to the sink.\n- Configure read access to the destination.\n- Verify that the logs are routed to the destination.\n### Set up a single destination\n**Note:** You can skip this step if you use a Cloud Logging bucket that already exists in the Google Cloud project where you want to aggregate the logs. You can use the `_Default` bucket, but we recommend that you create a separate bucket for this use case.- Open the Google Cloud console in the Google Cloud project that you want to aggregate logs into. [Go to Google Cloud console](https://console.cloud.google.com/) \n- In a [Cloud Shell](/shell/docs/launching-cloud-shell) terminal, run the following `gcloud` command to create a log bucket:```\ngcloud logging buckets create BUCKET_NAME \\\n --location=BUCKET_LOCATION \\\n --project=PROJECT_ID\n```Replace the following:- ``: the ID of the Google Cloud project where the aggregated logs will be stored.\n- ``: the name of the new Logging bucket.\n- `` : the geographical location of the new Logging bucket. The supported locations are `global` , `us` , or `eu` . To learn more about these storage regions, refer to [Supported regions](/logging/docs/regionalized-logs#other) . If you don't specify a location, then the `global` region is used, which means that the logs could be physically located in any of the regions. **Note:** After you create your bucket, you can't change your bucket's region.\n- Verify that the bucket was created:```\ngcloud logging buckets list --project=PROJECT_ID\n```\n- (Optional) Set the retention period of the logs in the bucket. The following example extends the retention of logs stored in the bucket to 365 days:```\ngcloud logging buckets update BUCKET_NAME \\\n --location=BUCKET_LOCATION \\\n --project=PROJECT_ID \\\n --retention-days=365\n```\n- Upgrade your new bucket to use Log Analytics by [following these steps](/logging/docs/buckets#upgrade-bucket) .\n- Open the Google Cloud console in the Google Cloud project that you want to aggregate logs into. [Go to Google Cloud console](https://console.cloud.google.com/) \n- In a [Cloud Shell](/shell/docs/launching-cloud-shell) terminal, run the following `bq mk` command to create a dataset:```\nbq --location=DATASET_LOCATION mk \\\n --dataset \\\n --default_partition_expiration=PARTITION_EXPIRATION \\\n PROJECT_ID:DATASET_ID\n```Replace the following:- ``: the ID of the Google Cloud project where the aggregated logs will be stored.\n- ``: the ID of the new BigQuery dataset.\n- `` : the geographic location of the dataset. After a dataset is created, the location can't be changed. **Note:** If you choose **EU** or an EU-based region for the dataset location, your Core BigQuery Customer Data resides in the EU. Core BigQuery Customer Data is defined in the [ServiceSpecific Terms](/terms/service-terms#13-google-bigquery-service) .\n- `` : the default lifetime (in seconds) for the partitions in the partitioned tables that are created by the log sink. You configure the log sink in the next section. The log sink that you configure uses partitioned tables that are partitioned by day based on the log entry's timestamp. Partitions (including associated log entries) are deleted `` seconds after the partition's date. **Best Practice** : Set the [default partition expiration](/bigquery/docs/updating-datasets#partition-expiration) property of the dataset based on your log retention requirements so older logs age out and expire. You can do so during or after you create the dataset. This allows you to retain logs as long as needed, while limiting the total size of the log storage and associated cost. If you have more granular retention requirements based on the log type, you can override this property at the table levelthe log sink has started routing logs and has created their corresponding partitioned tables. For example, you might be required to keep Cloud Audit Logs data for three years, but VPC Flow Logs and Firewall Rules Logs need only be retained for 90 days. If you do not set a default partition expiration at the dataset level, and you do not set a partition expiration when the table is created, the partitions never expire\n### Create an aggregated log sink\nYou route your organization logs into your destination by creating an aggregated sink at the organization level. To include all the logs you selected in the log scoping tool, you configure the sink with the log filter generated by the log scoping tool.\n**Note:** Routing logs to this new destination doesn't mean that your logs are redirected to it. Instead, your logs are stored twice: once in their parent Google Cloud project and then again in the new destination. To avoid this duplicate storage of your logs, add an [exclusion filter](/logging/docs/routing/overview#excusions) to the `_Default` sink of every child Google Cloud project in your organization. To stop logs from being ingested into the `_Default` sinks of Google Cloud projects in your organization, [disable the _Default sink](/logging/docs/default-settings#disable-default-sink) in the default settings of your organization.\n- In a [Cloud Shell](/shell/docs/launching-cloud-shell) terminal, run the following `gcloud` command to create an aggregated sink at the organization level:```\ngcloud logging sinks create SINK_NAME \\\n logging.googleapis.com/projects/PROJECT_ID/locations/BUCKET_LOCATION/buckets/BUCKET_NAME \\\n --log-filter=\"LOG_FILTER\" \\\n --organization=ORGANIZATION_ID \\\n --include-children\n```Replace the following:- ``: the name of the sink that routes the logs.\n- ``: the ID of the Google Cloud project where the aggregated logs will be stored.\n- ``: the location of the Logging bucket that you created for log storage.\n- ``: the name of the Logging bucket that you created for log storage.\n- ``: the log filter that you saved from the log scoping tool.\n- ``: the resource ID for your organization.\nThe `--include-children` flag is important so that logs from all the Google Cloud projects within your organization are also included. For more information, see [Collate and route organization-level logs to supported destinations](/logging/docs/export/aggregated_sinks) .\n- Verify the sink was created:```\ngcloud logging sinks list --organization=ORGANIZATION_ID\n```\n- Get the name of the service account associated with the sink that you just created:```\ngcloud logging sinks describe SINK_NAME --organization=ORGANIZATION_ID\n```The output looks similar to the following:```\nwriterIdentity: serviceAccount:p1234567890-12345@logging-o1234567890.iam.gserviceaccount.com`\n```\n- Copy the entire string for `writerIdentity` starting with **serviceAccount:** . This identifier is the sink's service account. Until you grant this service account write access to the log bucket, log routing from this sink will fail. You grant write access to the sink's writer identity in the next section.\n- In a [Cloud Shell](/shell/docs/launching-cloud-shell) terminal, run the following `gcloud` command to create an aggregated sink at the organization level:```\ngcloud logging sinks create SINK_NAME \\\n bigquery.googleapis.com/projects/PROJECT_ID/datasets/DATASET_ID \\\n --log-filter=\"LOG_FILTER\" \\\n --organization=ORGANIZATION_ID \\\n --use-partitioned-tables \\\n --include-children\n```Replace the following:- ``: the name of the sink that routes the logs.\n- ``: the ID for the Google Cloud project you want to aggregate the logs into.\n- ``: the ID of the BigQuery dataset you created.\n- ``: the log filter that you saved from the log scoping tool.\n- ``: the resource ID for your organization.\nThe `--include-children` flag is important so that logs from all the Google Cloud projects within your organization are also included. For more information, see [Collate and route organization-level logs to supported destinations](/logging/docs/export/aggregated_sinks) .The `--use-partitioned-tables` flag is important so that data is partitioned by day based on the log entry's `timestamp` field. This simplifies querying of the data and helps reduce query costs by reducing the amount of data scanned by queries. Another benefit of partitioned tables is that you can [set a default partition expiration](/bigquery/docs/updating-datasets#partition-expiration) at the dataset level to meet your log retention requirements. You have already set a default partition expiration when you created the dataset destination in the previous section. You might also choose to [set a partition expiration](/bigquery/docs/managing-partitioned-tables#partition-expiration) at the individual table level, providing you with fine-grained data retention controls based on log type.\n- Verify the sink was created:```\ngcloud logging sinks list --organization=ORGANIZATION_ID\n```\n- Get the name of the service account associated with the sink that you just created:```\ngcloud logging sinks describe SINK_NAME --organization=ORGANIZATION_ID\n```The output looks similar to the following:```\nwriterIdentity: serviceAccount:p1234567890-12345@logging-o1234567890.iam.gserviceaccount.com`\n```\n- Copy the entire string for `writerIdentity` starting with **serviceAccount:** . This identifier is the sink's service account. Until you grant this service account write access to the BigQuery dataset, log routing from this sink will fail. You grant write access to the sink's writer identity in the next section.\n### Grant access to the sink\nAfter creating the log sink, you must grant your sink access to write to its destination, be it the Logging bucket or the BigQuery dataset.\n**Note:** To route logs to a resource protected by a [service perimeter](/vpc-service-controls/docs/service-perimeters#about-perimeters) , you must also add the service account for that sink to an access level and then assign it to the destination service perimeter. This isn't necessary for non-aggregated sinks. For details, see [VPC Service Controls: Cloud Logging](/vpc-service-controls/docs/supported-products#logging) .\nTo add the permissions to the sink's service account, follow these steps:- In the Google Cloud console, go to the IAM page: [Go to the IAM page](https://console.cloud.google.com/iam-admin/iam) \n- Make sure that you've selected the destination Google Cloud project that contains the Logging bucket you created for central log storage.\n- Click **Grant access** .\n- In the **New principals** field, enter the sink's service account without the `serviceAccount:` prefix. Recall that this identity comes from the `writerIdentity` field you retrieved in the previous section after you created the sink.\n- In the **Select a role** drop-down menu, select **Logs Bucket Writer** .\n- Click **Add IAM condition** to restrict the service account's access to only the log bucket you created.\n- Enter a **Title** and **Description** for the condition.\n- In the **Condition type** drop-down menu, select **Resource** > **Name** .\n- In the **Operator** drop-down menu, select **Ends with** .\n- In the **Value** field, enter the bucket's location and name as follows:```\nlocations/BUCKET_LOCATION/buckets/BUCKET_NAME\n```\n- Click **Save** to add the condition.\n- Click **Save** to set the permissions.\nTo add the permissions to the sink's service account, follow these steps:- In the Google Cloud console, go to BigQuery: [Go to BigQuery](https://bigquery.cloud.google.com) \n- Open the BigQuery dataset that you created for central log storage.\n- In the Dataset info tab, click the **Sharingkeyboard_arrow_down** drop-down menu, and then click **Permissions** .\n- In the Dataset Permissions side panel, click **Add Principal** .\n- In the **New principals** field, enter the sink's service account without the `serviceAccount:` prefix. Recall that this identity comes from the `writerIdentity` field you retrieved in the previous section after you created the sink.\n- In the **Role** drop-down menu, select **BigQuery Data Editor** .\n- Click **Save** .\nAfter you grant access to the sink, log entries begin to populate the sink destination: the Logging bucket or the BigQuery dataset.\n### Configure read access to the destination\nNow that your log sink routes logs from your entire organization into one single destination, you can search across all of these logs. Use IAM permissions to manage permissions and grant access as needed.\nTo grant access to view and query the logs in your new log bucket, follow these steps.- In the Google Cloud console, go to the IAM page: [Go to the IAM page](https://console.cloud.google.com/iam-admin/iam) Make sure you've selected the Google Cloud project you're using to aggregate the logs.\n- Click **Add** .\n- In the **New principal** field, add your email account.\n- In the **Select a role** drop-down menu, select **Logs Views Accessor** .This role provides the newly added principal with read access to all views for any buckets in the Google Cloud project. To limit a user's access, add a condition that lets the user read only from your new bucket only.- Click **Add condition** .\n- Enter a **Title** and **Description** for the condition.\n- In the **Condition type** drop-down menu, select **Resource** > **Name** .\n- In the **Operator** drop-down menu, select **Ends with** .\n- In the **Value** field, enter the bucket's location and name, and the default log view `_AllLogs` as follows:```\nlocations/BUCKET_LOCATION/buckets/BUCKET_NAME/views/_AllLogs\n``` **Note:** Cloud Logging automatically creates the `_AllLogs` view for every bucket, which shows all the logs in the bucket. For more granular control over which logs can be viewed and queried within that log bucket, you can create and use a [custom log view](/logging/docs/logs-views#create_view) instead of `_AllLogs` .\n- Click **Save** to add the condition.\n- Click **Save** to set the permissions.\nTo grant access to view and query the logs in your BigQuery dataset, follow the steps in the [Granting access to a dataset](/bigquery/docs/dataset-access-controls#granting_access_to_a_dataset) section of the BigQuery documentation.\n### Verify that the logs are routed to the destination\nWhen you route logs to a log bucket upgraded to Log Analytics, you can view and query all log entries through a single log view with a unified schema for all log types. Follow these steps to verify the logs are correctly routed.- In the Google Cloud console, go to Log Analytics page: [Go to Log Analytics](https://console.cloud.google.com/logs/analytics) Make sure you've selected the Google Cloud project you're using to aggregate the logs.\n- Click on **Log Views** tab.\n- Expand the log views under the log bucket that you have created (that is `` ) if it is not expanded already.\n- Select the default log view `_AllLogs` . You can now inspect the entire log schema in the right panel, as shown in the following screenshot: \n- Next to `_AllLogs` , click **Query** . This populates the **Query** editor with a SQL sample query to retrieve recently routed log entries.\n- Click **Run query** to view recently routed log entries.\nDepending on level of activity in Google Cloud projects in your organization, you might have to wait a few minutes until some logs get generated, and then routed to your log bucket.\nWhen you route logs to a BigQuery dataset, Cloud Logging creates BigQuery tables to hold the log entries as shown in the following screenshot:\n \nThe screenshot shows how Cloud Logging names each BigQuery table based on the name of the log to which a log entry belongs. For example, the `cloudaudit_googleapis_com_data_access` table that is selected in the screenshot contains Data Access audit logs whose log ID is `cloudaudit.googleapis.com%2Fdata_access` . In addition to being named based on the corresponding log entry, each table is also partitioned based on the timestamps for each log entry.\nDepending on level of activity in Google Cloud projects in your organization, you might have to wait a few minutes until some logs get generated, and then routed to your BigQuery dataset.\n **Note:** Both Admin Activity and Data Access logs are loaded into BigQuery with their `protoPayload` log entry field renamed to `protoPayload_auditlog` in BigQuery. For more information about schema conversions done by Cloud Logging before writing to BigQuery, see [Fields in exported audit logs](/logging/docs/export/bigquery#audit-logs) .\n## Analyze logs\nYou can run a broad range of queries against your audit and platform logs. The following list provides a set of sample security questions that you might want to ask of your own logs. For each question in this list, there are two versions of the corresponding CSA query: one for use with Log Analytics and one for use with BigQuery. Use the query version that matches the sink destination that you previously set up.\nBefore using any of the SQL queries below, replace `` with the ID of the Google Cloud project where you created the log bucket (that is `` `)` , and `` with the region and name of that log bucket (that is `` `.` `` ).\n [Go to Log Analytics](https://console.cloud.google.com/logs/analytics) \nBefore using any of the SQL queries below, replace `` with the ID of the Google Cloud project where you created the BigQuery dataset (that is `` `)` , and `` with the name of that dataset, that is `` .\n [Go to BigQuery](https://console.cloud.google.com/bigquery)\n- [Login and access questions](#login_and_access_questions) - [Any suspicious login attempt flagged by Google Workspace?](#1_02_suspicious_login_attempt) \n- [Any excessive login failures from any user identity?](#1_03_excessive_login_failures) \n- [Any access attempts violating VPC Service Controls?](#1_10_access_attempts_blocked_by_VPC_SC) \n- [Any access attempts violating Identity-Aware Proxy access controls?](#1_20_access_attempts_blocked_by_IAP) \n- [Permission changes questions](#permission_changes_questions) - [Any user added to highly-privileged groups?](#2_02_user_added_to_privileged_group) \n- [Any permissions granted over a service account?](#2_20_permissions_granted_over_SA) \n- [Any service accounts or keys created by non-approved identity?](#2_30_service_accounts_or_keys_created_by_non_approved_identity) \n- [Any user added to (or removed from) sensitive IAM policy?](#2_40_user_access_modified_in_IAP) \n- [Provisioning activity questions](#provisioning_activity_questions) - [Any changes made to logging settings?](#3_01_logging_settings_modified) \n- [Any VPC Flow Logs disabled?](#3_02_vpc_flows_logging_disabled) \n- [Any unusual number of firewall rules modified in the past week?](#3_11_unusual_number_of_firewall_rules_modified) \n- [Any VMs deleted in the past week?](#3_20_virtual_machines_deleted) \n- [Workload usage questions](#workload_usage_questions) - [Any unusually high API usage by any user identity in the past week?](#4_01_unusually_high_api_usage_by_user_identity) \n- [What is the autoscaling usage per day in the past month?](#4_11_autoscaling_usage_frequency_by_day) \n- [Data access questions](#data_access_questions) - [Which users most frequently accessed data in the past week?](#5_01_users_who_most_frequently_accessed_data) \n- [Which users accessed the data in the \"accounts\" table last month?](#5_04_users_who_accessed_data_in_table) \n- [What tables are most frequently accessed and by whom?](#5_05_tables_most_frequently_accessed) \n- [What are the top 10 queries against BigQuery in the past week?](#5_06_BQ_queries_top) \n- [What are the most common actions recorded in the data access log over the past month?](#5_20_top_data_access_actions) \n- [Network security questions](#network_security_questions) - [Any connections from a new IP address to a specific subnetwork?](#6_10_connection_from_new_IP) \n- [Any connections blocked by Google Cloud Armor?](#6_20_connections_blocked_by_cloud_armor) \n- [Any high-severity virus or malware detected by Cloud IDS?](#6_30_virus_or_malware_detected_by_cloud_IDS) \n- [What are the top Cloud DNS queried domains from your VPC network?](#6_40_DNS_top_queried_domains) \n### Login and access questions\nThese sample queries perform analysis to detect suspicious login attempts or initial access attempts to your Google Cloud environment.\n**Note:** Login activity is captured in Cloud Identity logs that are included in [Google Workspace Login Audit](https://cloud.google.com/logging/docs/audit/gsuite-audit-logging#2) . To analyze login activity and use some of the queries in this section, you need to enable Google Workspace data sharing with Google Cloud. To learn more about sharing Google Workspace audit logs with Google Cloud, see [View and manage audit logs for Google Workspace](/logging/docs/audit/configure-gsuite-audit-logs) .\nBy searching Cloud Identity logs that are part of [Google Workspace Login Audit](https://developers.google.com/admin-sdk/reports/v1/appendix/activity/login) , the following query detects suspicious login attempts flagged by Google Workspace. Such login attempts might be from the Google Cloud console, Admin console, or the gcloud CLI.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/1_02_suspicious_login_attempt.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 proto_payload.audit_log.authentication_info.principal_email,\u00a0 proto_payload.audit_log.request_metadata.caller_ip,\u00a0 proto_payload.audit_log.method_name, parameterFROM `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`,\u00a0 UNNEST(JSON_QUERY_ARRAY(proto_payload.audit_log.metadata.event[0].parameter)) AS parameterWHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 60 DAY)\u00a0 AND proto_payload.audit_log IS NOT NULL\u00a0 AND proto_payload.audit_log.service_name = \"login.googleapis.com\"\u00a0 AND proto_payload.audit_log.method_name = \"google.login.LoginService.loginSuccess\"\u00a0 AND JSON_VALUE(parameter.name) = \"is_suspicious\"\u00a0 AND JSON_VALUE(parameter.boolValue) = \"true\"\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/1_02_suspicious_login_attempt.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 protopayload_auditlog.authenticationInfo.principalEmail,\u00a0 protopayload_auditlog.requestMetadata.callerIp,\u00a0 protopayload_auditlog.methodNameFROM `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_data_access`,\u00a0 UNNEST(JSON_QUERY_ARRAY(protopayload_auditlog.metadataJson, '$.event[0].parameter')) AS parameterWHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 60 DAY)\u00a0 AND protopayload_auditlog.metadataJson IS NOT NULL\u00a0 AND protopayload_auditlog.serviceName = \"login.googleapis.com\"\u00a0 AND protopayload_auditlog.methodName = \"google.login.LoginService.loginSuccess\"\u00a0 AND JSON_VALUE(parameter, '$.name') = \"is_suspicious\"\u00a0 AND JSON_VALUE(parameter, '$.boolValue') = \"true\"\n```\nBy searching Cloud Identity logs that are part of [Google Workspace Login Audit](https://developers.google.com/admin-sdk/reports/v1/appendix/activity/login) , the following query detects users who have had three or more successive login failures within the last 24 hours.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/1_03_excessive_login_failures.sql) \n```\nSELECT\u00a0 proto_payload.audit_log.authentication_info.principal_email,\u00a0 MIN(timestamp) AS earliest,\u00a0 MAX(timestamp) AS latest,\u00a0 count(*) AS attemptsFROM `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)\u00a0 AND proto_payload.audit_log.service_name = \"login.googleapis.com\"\u00a0 AND proto_payload.audit_log.method_name = \"google.login.LoginService.loginFailure\"GROUP BY\u00a0 1HAVING\u00a0 attempts >= 3\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/1_03_excessive_login_failures.sql) \n```\nSELECT\u00a0 protopayload_auditlog.authenticationInfo.principalEmail,\u00a0 MIN(timestamp) AS earliest,\u00a0 MAX(timestamp) AS latest,\u00a0 count(*) AS attemptsFROM\u00a0`[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_data_access`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)\u00a0 AND protopayload_auditlog.serviceName=\"login.googleapis.com\"\u00a0 AND protopayload_auditlog.methodName=\"google.login.LoginService.loginFailure\"GROUP BY\u00a0 1HAVING\u00a0 attempts >= 3\n```\nBy analyzing Policy Denied audit logs from Cloud Audit Logs, the following query detects access attempts blocked by VPC Service Controls. Any query results might indicate potential malicious activity like access attempts from unauthorized networks using stolen credentials.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/1_10_access_attempts_blocked_by_VPC_SC.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 log_name,\u00a0 proto_payload.audit_log.authentication_info.principal_email,\u00a0 proto_payload.audit_log.request_metadata.caller_ip,\u00a0 proto_payload.audit_log.method_name,\u00a0 proto_payload.audit_log.service_name,\u00a0 JSON_VALUE(proto_payload.audit_log.metadata.violationReason) as violationReason, \u00a0 IF(JSON_VALUE(proto_payload.audit_log.metadata.ingressViolations) IS NULL, 'ingress', 'egress') AS violationType,\u00a0 COALESCE(\u00a0 \u00a0 JSON_VALUE(proto_payload.audit_log.metadata.ingressViolations[0].targetResource),\u00a0 \u00a0 JSON_VALUE(proto_payload.audit_log.metadata.egressViolations[0].targetResource)\u00a0 ) AS \u00a0targetResource,\u00a0 COALESCE(\u00a0 \u00a0 JSON_VALUE(proto_payload.audit_log.metadata.ingressViolations[0].servicePerimeter),\u00a0 \u00a0 JSON_VALUE(proto_payload.audit_log.metadata.egressViolations[0].servicePerimeter)\u00a0 ) AS \u00a0servicePerimeterFROM `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)\u00a0 AND proto_payload.audit_log IS NOT NULL\u00a0 AND JSON_VALUE(proto_payload.audit_log.metadata, '$.\"@type\"') = 'type.googleapis.com/google.cloud.audit.VpcServiceControlAuditMetadata'ORDER BY\u00a0 timestamp DESCLIMIT 1000\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/1_10_access_attempts_blocked_by_VPC_SC.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 protopayload_auditlog.authenticationInfo.principalEmail,\u00a0 protopayload_auditlog.requestMetadata.callerIp,\u00a0 protopayload_auditlog.methodName,\u00a0 protopayload_auditlog.serviceName,\u00a0 JSON_VALUE(protopayload_auditlog.metadataJson, '$.violationReason') as violationReason, \u00a0 IF(JSON_VALUE(protopayload_auditlog.metadataJson, '$.ingressViolations') IS NULL, 'ingress', 'egress') AS violationType,\u00a0 COALESCE(\u00a0 \u00a0 JSON_VALUE(protopayload_auditlog.metadataJson, '$.ingressViolations[0].targetResource'),\u00a0 \u00a0 JSON_VALUE(protopayload_auditlog.metadataJson, '$.egressViolations[0].targetResource')\u00a0 ) AS \u00a0targetResource,\u00a0 COALESCE(\u00a0 \u00a0 JSON_VALUE(protopayload_auditlog.metadataJson, '$.ingressViolations[0].servicePerimeter'),\u00a0 \u00a0 JSON_VALUE(protopayload_auditlog.metadataJson, '$.egressViolations[0].servicePerimeter')\u00a0 ) AS \u00a0servicePerimeterFROM\u00a0`[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_policy`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 400 DAY)\u00a0 AND JSON_VALUE(protopayload_auditlog.metadataJson, '$.\"@type\"') = 'type.googleapis.com/google.cloud.audit.VpcServiceControlAuditMetadata'ORDER BY\u00a0 timestamp DESCLIMIT 1000\n```\nBy analyzing external Application Load Balancer logs, the following query detects access attempts blocked by IAP. Any query results might indicate an initial access attempt or vulnerability exploit attempt.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/1_20_access_attempts_blocked_by_IAP.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 http_request.remote_ip,\u00a0 http_request.request_method,\u00a0 http_request.status,\u00a0 JSON_VALUE(resource.labels.backend_service_name) AS backend_service_name,\u00a0 http_request.request_urlFROM `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)\u00a0 AND resource.type=\"http_load_balancer\"\u00a0 AND JSON_VALUE(json_payload.statusDetails) = \"handled_by_identity_aware_proxy\"ORDER BY\u00a0 timestamp DESC\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/1_20_access_attempts_blocked_by_IAP.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 httpRequest.remoteIp,\u00a0 httpRequest.requestMethod,\u00a0 httpRequest.status,\u00a0 resource.labels.backend_service_name,\u00a0 httpRequest.requestUrl,FROM `[MY_PROJECT_ID].[MY_DATASET_ID].requests`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)\u00a0 AND resource.type=\"http_load_balancer\"\u00a0 AND jsonpayload_type_loadbalancerlogentry.statusdetails = \"handled_by_identity_aware_proxy\"ORDER BY\u00a0 timestamp DESC\n```### Permission changes questions\nThese sample queries perform analysis over administrator activity that changes permissions, including changes in IAM policies, groups and group memberships, service accounts, and any associated keys. Such permission changes might provide a high level of access to sensitive data or environments.\n**Note:** Group changes are captured in [Google Workspace Admin Audit](/logging/docs/audit/gsuite-audit-logging#3) . To analyze group changes activity and use some of the queries in this section, you need to enable Google Workspace data sharing with Google Cloud. To learn more about sharing Google Workspace audit logs with Google Cloud, see [View and manage audit logs for Google Workspace](/logging/docs/audit/configure-gsuite-audit-logs) .\nBy analyzing [Google Workspace Admin Audit](/logging/docs/audit/gsuite-audit-logging#3) audit logs, the following query detects users who have been added to any of the highly-privileged groups listed in the query. You use the regular expression in the query to define which groups (such as `admin@example.com` or `prod@example.com` ) to monitor. Any query results might indicate a malicious or accidental privilege escalation.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/2_02_user_added_to_privileged_group.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 proto_payload.audit_log.authentication_info.principal_email,\u00a0 proto_payload.audit_log.method_name,\u00a0 proto_payload.audit_log.resource_name,\u00a0 (SELECT JSON_VALUE(x.value)\u00a0 \u00a0FROM UNNEST(JSON_QUERY_ARRAY(proto_payload.audit_log.metadata.event[0].parameter)) AS x\u00a0 \u00a0WHERE JSON_VALUE(x.name) = \"USER_EMAIL\") AS user_email,\u00a0 (SELECT JSON_VALUE(x.value)\u00a0 \u00a0FROM UNNEST(JSON_QUERY_ARRAY(proto_payload.audit_log.metadata.event[0].parameter)) AS x\u00a0 \u00a0WHERE JSON_VALUE(x.name) = \"GROUP_EMAIL\") AS group_email,FROM `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 120 DAY)\u00a0 AND proto_payload.audit_log.service_name = \"admin.googleapis.com\"\u00a0 AND proto_payload.audit_log.method_name = \"google.admin.AdminService.addGroupMember\"\u00a0 AND EXISTS(\u00a0 \u00a0 SELECT * FROM UNNEST(JSON_QUERY_ARRAY(proto_payload.audit_log.metadata.event[0].parameter)) AS x\u00a0 \u00a0 WHERE\u00a0 \u00a0 \u00a0 JSON_VALUE(x.name) = \"GROUP_EMAIL\"\u00a0 \u00a0 \u00a0 AND REGEXP_CONTAINS(JSON_VALUE(x.value), r'(admin|prod).*') -- Update regexp with other sensitive groups if applicable\u00a0 )\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/2_02_user_added_to_privileged_group.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 protopayload_auditlog.authenticationInfo.principalEmail,\u00a0 protopayload_auditlog.methodName,\u00a0 protopayload_auditlog.resourceName,\u00a0 (SELECT JSON_VALUE(x, '$.value')\u00a0 \u00a0FROM UNNEST(JSON_QUERY_ARRAY(protopayload_auditlog.metadataJson, '$.event[0].parameter')) AS x\u00a0 \u00a0WHERE JSON_VALUE(x, '$.name') = \"USER_EMAIL\") AS userEmail,\u00a0 (SELECT JSON_VALUE(x, '$.value')\u00a0 \u00a0FROM UNNEST(JSON_QUERY_ARRAY(protopayload_auditlog.metadataJson, '$.event[0].parameter')) AS x\u00a0 \u00a0WHERE JSON_VALUE(x, '$.name') = \"GROUP_EMAIL\") AS groupEmail,FROM `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_activity`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 120 DAY)\u00a0 AND protopayload_auditlog.serviceName = \"admin.googleapis.com\"\u00a0 AND protopayload_auditlog.methodName = \"google.admin.AdminService.addGroupMember\"\u00a0 AND EXISTS(\u00a0 \u00a0 SELECT * FROM UNNEST(JSON_QUERY_ARRAY(protopayload_auditlog.metadataJson, '$.event[0].parameter')) AS x\u00a0 \u00a0 WHERE\u00a0 \u00a0 \u00a0 JSON_VALUE(x, '$.name') = 'GROUP_EMAIL'\u00a0 \u00a0 \u00a0 AND REGEXP_CONTAINS(JSON_VALUE(x, '$.value'), r'(admin|prod).*') -- Update regexp with other sensitive groups if applicable\u00a0 )\n```\nBy analyzing Admin Activity audit logs from Cloud Audit Logs, the following query detects any permissions that have been granted to any principal over a service account. Examples of permissions that might be granted are the ability to impersonate that service account or create service account keys. Any query results might indicate an instance of privilege escalation or a risk of credentials leakage.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/2_20_permissions_granted_over_SA.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 proto_payload.audit_log.authentication_info.principal_email as grantor,\u00a0 JSON_VALUE(bindingDelta.member) as grantee,\u00a0 JSON_VALUE(bindingDelta.role) as role,\u00a0 proto_payload.audit_log.resource_name,\u00a0 proto_payload.audit_log.method_nameFROM\u00a0 `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`,\u00a0 UNNEST(JSON_QUERY_ARRAY(proto_payload.audit_log.service_data.policyDelta.bindingDeltas)) AS bindingDeltaWHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 400 DAY)\u00a0 -- AND log_id = \"cloudaudit.googleapis.com/activity\"\u00a0 AND (\u00a0 \u00a0 (resource.type = \"service_account\"\u00a0 \u00a0 AND proto_payload.audit_log.method_name LIKE \"google.iam.admin.%.SetIAMPolicy\")\u00a0 \u00a0 OR\u00a0 \u00a0 (resource.type IN (\"project\", \"folder\", \"organization\")\u00a0 \u00a0 AND proto_payload.audit_log.method_name = \"SetIamPolicy\"\u00a0 \u00a0 AND JSON_VALUE(bindingDelta.role) LIKE \"roles/iam.serviceAccount%\")\u00a0 )\u00a0 AND JSON_VALUE(bindingDelta.action) = \"ADD\"\u00a0 -- Principal (grantee) exclusions\u00a0 AND JSON_VALUE(bindingDelta.member) NOT LIKE \"%@example.com\"ORDER BY\u00a0 timestamp DESC\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/2_20_permissions_granted_over_SA.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 protopayload_auditlog.authenticationInfo.principalEmail as grantor,\u00a0 bindingDelta.member as grantee,\u00a0 bindingDelta.role,\u00a0 protopayload_auditlog.resourceName,\u00a0 protopayload_auditlog.methodName,FROM\u00a0 `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_activity`,\u00a0 UNNEST(protopayload_auditlog.servicedata_v1_iam.policyDelta.bindingDeltas) AS bindingDeltaWHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 180 DAY)\u00a0 AND (\u00a0 \u00a0 (resource.type = \"service_account\"\u00a0 \u00a0 AND protopayload_auditlog.methodName LIKE \"google.iam.admin.%.SetIAMPolicy\")\u00a0 \u00a0 OR\u00a0 \u00a0 (resource.type IN (\"project\", \"folder\", \"organization\")\u00a0 \u00a0 AND protopayload_auditlog.methodName = \"SetIamPolicy\"\u00a0 \u00a0 AND bindingDelta.role LIKE \"roles/iam.serviceAccount%\")\u00a0 )\u00a0 AND bindingDelta.action = 'ADD'\u00a0 -- Principal (grantee) exclusions\u00a0 AND bindingDelta.member NOT LIKE \"%@example.com\"ORDER BY\u00a0 timestamp DESC\n```\nBy analyzing Admin Activity audit logs, the following query detects any service accounts or keys that have been manually created by a user. For example, you might follow a best practice to only allow service accounts to be created by an approved service account as part of an automated workflow. Therefore, any service account creation outside of that workflow is considered non-compliant and possibly malicious.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/2_30_service_accounts_or_keys_created_by_non_approved_identity.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 proto_payload.audit_log.authentication_info.principal_email,\u00a0 proto_payload.audit_log.method_name,\u00a0 proto_payload.audit_log.resource_name,\u00a0 JSON_VALUE(proto_payload.audit_log.response.email) as service_account_emailFROM\u00a0 `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)\u00a0 AND resource.type=\"service_account\"\u00a0 AND proto_payload.audit_log.method_name LIKE \"%CreateServiceAccount%\"\u00a0 AND proto_payload.audit_log.authentication_info.principal_email NOT LIKE \"%.gserviceaccount.com\"\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/2_30_service_accounts_or_keys_created_by_non_approved_identity.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 protopayload_auditlog.authenticationInfo.principalEmail,\u00a0 protopayload_auditlog.methodName,\u00a0 protopayload_auditlog.resourceName,\u00a0 JSON_VALUE(protopayload_auditlog.responseJson, \"$.email\") as serviceAccountEmailFROM\u00a0 `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_activity`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 180 DAY)\u00a0 AND resource.type=\"service_account\"\u00a0 AND protopayload_auditlog.methodName LIKE \"%CreateServiceAccount%\"\u00a0 AND protopayload_auditlog.authenticationInfo.principalEmail NOT LIKE \"%.gserviceaccount.com\"\n```\nBy searching Admin Activity audit logs, the following query detects any user or group access change for an IAP-secured resource such as a Compute Engine backend service. The following query searches all IAM policy updates for IAP resources involving the IAM role `roles/iap.httpsResourceAccessor` . This role provides permissions to access the HTTPS resource or the backend service. Any query results might indicate attempts to bypass the defenses of a backend service that might be exposed to the internet.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/2_40_user_access_modified_in_IAP.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 proto_payload.audit_log.authentication_info.principal_email,\u00a0 resource.type,\u00a0 proto_payload.audit_log.resource_name,\u00a0 JSON_VALUE(binding, '$.role') as role,\u00a0 JSON_VALUE_ARRAY(binding, '$.members') as membersFROM\u00a0 `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`,\u00a0 UNNEST(JSON_QUERY_ARRAY(proto_payload.audit_log.response, '$.bindings')) AS bindingWHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)\u00a0 -- AND log_id = \"cloudaudit.googleapis.com/activity\"\u00a0 AND proto_payload.audit_log.service_name = \"iap.googleapis.com\"\u00a0 AND proto_payload.audit_log.method_name LIKE \"%.IdentityAwareProxyAdminService.SetIamPolicy\"\u00a0 AND JSON_VALUE(binding, '$.role') = \"roles/iap.httpsResourceAccessor\"ORDER BY\u00a0 timestamp DESC\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/2_40_user_access_modified_in_IAP.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 protopayload_auditlog.authenticationInfo.principalEmail,\u00a0 resource.type,\u00a0 protopayload_auditlog.resourceName,\u00a0 JSON_VALUE(binding, '$.role') as role,\u00a0 JSON_VALUE_ARRAY(binding, '$.members') as membersFROM\u00a0 `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_activity`,\u00a0 UNNEST(JSON_QUERY_ARRAY(protopayload_auditlog.responseJson, '$.bindings')) AS bindingWHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 400 DAY)\u00a0 AND protopayload_auditlog.serviceName = \"iap.googleapis.com\"\u00a0 AND protopayload_auditlog.methodName LIKE \"%.IdentityAwareProxyAdminService.SetIamPolicy\"\u00a0 AND JSON_VALUE(binding, '$.role') = \"roles/iap.httpsResourceAccessor\"ORDER BY\u00a0 timestamp DESC\n```### Provisioning activity questions\nThese sample queries perform analysis to detect suspicious or anomalous admin activity like provisioning and configuring resources.\nBy searching Admin Activity audit logs, the following query detects any change made to logging settings. Monitoring logging settings helps you detect accidental or malicious disabling of audit logs and similar defense evasion techniques.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/3_01_logging_settings_modified.sql) \n```\nSELECT\u00a0 receive_timestamp, timestamp AS eventTimestamp,\u00a0 proto_payload.audit_log.request_metadata.caller_ip,\u00a0 proto_payload.audit_log.authentication_info.principal_email, \u00a0 proto_payload.audit_log.resource_name,\u00a0 proto_payload.audit_log.method_nameFROM \u00a0 `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 proto_payload.audit_log.service_name = \"logging.googleapis.com\"\u00a0 AND log_id = \"cloudaudit.googleapis.com/activity\"\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/3_01_logging_settings_modified.sql) \n```\nSELECT\u00a0 receiveTimestamp, timestamp AS eventTimestamp,\u00a0 protopayload_auditlog.requestMetadata.callerIp,\u00a0 protopayload_auditlog.authenticationInfo.principalEmail, \u00a0 protopayload_auditlog.resourceName,\u00a0 protopayload_auditlog.methodNameFROM `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_activity`WHERE\u00a0 protopayload_auditlog.serviceName = \"logging.googleapis.com\"\n```\nBy searching Admin Activity audit logs, the following query detects any subnet whose VPC Flow Logs were actively disabled . Monitoring VPC Flow Logs settings helps you detect accidental or malicious disabling of VPC Flow Logs and similar defense evasion techniques.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/3_02_vpc_flows_logging_disabled.sql) \n```\nSELECT\u00a0 receive_timestamp, timestamp AS eventTimestamp,\u00a0 proto_payload.audit_log.request_metadata.caller_ip,\u00a0 proto_payload.audit_log.authentication_info.principal_email, \u00a0 proto_payload.audit_log.resource_name,\u00a0 proto_payload.audit_log.method_nameFROM \u00a0 `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 proto_payload.audit_log.method_name = \"v1.compute.subnetworks.patch\" \u00a0 AND (\u00a0 \u00a0 JSON_VALUE(proto_payload.audit_log.request, \"$.logConfig.enable\") = \"false\"\u00a0 \u00a0 OR JSON_VALUE(proto_payload.audit_log.request, \"$.enableFlowLogs\") = \"false\"\u00a0 )\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/3_02_vpc_flows_logging_disabled.sql) \n```\nSELECT\u00a0 receiveTimestamp, timestamp AS eventTimestamp,\u00a0 protopayload_auditlog.requestMetadata.callerIp,\u00a0 protopayload_auditlog.authenticationInfo.principalEmail, \u00a0 protopayload_auditlog.resourceName,\u00a0 protopayload_auditlog.methodNameFROM `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_activity`WHEREprotopayload_auditlog.methodName = \"v1.compute.subnetworks.patch\" AND JSON_VALUE(protopayload_auditlog.requestJson, \"$.logConfig.enable\") = \"false\"\n```\nBy searching Admin Activity audit logs, the following query detects any unusually high number of firewall rules changes on any given day in the past week. To determine whether there is an outlier, the query performs statistical analysis over the daily counts of firewall rules changes. Averages and standard deviations are computed for each day by looking back at the preceding daily counts with a lookback window of 90 days. An outlier is considered when the daily count is more than two standard deviations above the mean. The query, including the standard deviation factor and the lookback windows, can all be configured to fit your cloud provisioning activity profile and to minimize false positives.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/3_11_unusual_number_of_firewall_rules_modified.sql) \n```\nSELECT\u00a0 *FROM (\u00a0 SELECT\u00a0 \u00a0 *,\u00a0 \u00a0 AVG(counter) OVER (\u00a0 \u00a0 \u00a0 ORDER BY day\u00a0 \u00a0 \u00a0 ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS avg,\u00a0 \u00a0 STDDEV(counter) OVER (\u00a0 \u00a0 \u00a0 ORDER BY day\u00a0 \u00a0 \u00a0 ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS stddev,\u00a0 \u00a0 COUNT(*) OVER (\u00a0 \u00a0 \u00a0 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS numSamples\u00a0 FROM (\u00a0 \u00a0 SELECT\u00a0 \u00a0 \u00a0 EXTRACT(DATE FROM timestamp) AS day,\u00a0 \u00a0 \u00a0 ARRAY_AGG(DISTINCT proto_payload.audit_log.method_name IGNORE NULLS) AS actions,\u00a0 \u00a0 \u00a0 ARRAY_AGG(DISTINCT proto_payload.audit_log.authentication_info.principal_email IGNORE NULLS) AS actors,\u00a0 \u00a0 \u00a0 COUNT(*) AS counter\u00a0 \u00a0 FROM `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`\u00a0 \u00a0 WHERE\u00a0 \u00a0 \u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 DAY)\u00a0 \u00a0 \u00a0 AND proto_payload.audit_log.method_name LIKE \"v1.compute.firewalls.%\"\u00a0 \u00a0 \u00a0 AND proto_payload.audit_log.method_name NOT IN (\"v1.compute.firewalls.list\", \"v1.compute.firewalls.get\")\u00a0 \u00a0 GROUP BY\u00a0 \u00a0 \u00a0 day\u00a0 ))WHERE\u00a0 counter > avg + 2 * stddev\u00a0 AND day >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY) ORDER BY\u00a0 counter DESC\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/3_11_unusual_number_of_firewall_rules_modified.sql) \n```\nSELECT\u00a0 *,\u00a0 AVG(counter) OVER (\u00a0 \u00a0 ORDER BY day\u00a0 \u00a0 ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS avg,\u00a0 STDDEV(counter) OVER (\u00a0 \u00a0 ORDER BY day\u00a0 \u00a0 ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS stddev,\u00a0 COUNT(*) OVER (\u00a0 \u00a0 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS numSamplesFROM (\u00a0 SELECT\u00a0 \u00a0 EXTRACT(DATE FROM timestamp) AS day,\u00a0 \u00a0 ARRAY_AGG(DISTINCT protopayload_auditlog.methodName IGNORE NULLS) AS actions,\u00a0 \u00a0 ARRAY_AGG(DISTINCT protopayload_auditlog.authenticationInfo.principalEmail IGNORE NULLS) AS actors,\u00a0 \u00a0 COUNT(*) AS counter\u00a0 FROM `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_activity`\u00a0 WHERE\u00a0 \u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 DAY)\u00a0 \u00a0 AND protopayload_auditlog.methodName LIKE \"v1.compute.firewalls.%\"\u00a0 \u00a0 AND protopayload_auditlog.methodName NOT IN (\"v1.compute.firewalls.list\", \"v1.compute.firewalls.get\")\u00a0 GROUP BY\u00a0 \u00a0 day)WHERE TRUEQUALIFY\u00a0 counter > avg + 2 * stddev\u00a0 AND day >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY) ORDER BY\u00a0 counter DESC\n```\nBy searching Admin Activity audit logs, the following query lists any Compute Engine instances deleted in the past week. This query can help you audit resource deletions and detect potential malicious activity.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/3_20_virtual_machines_deleted.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 JSON_VALUE(resource.labels.instance_id) AS instance_id,\u00a0 proto_payload.audit_log.authentication_info.principal_email, \u00a0 proto_payload.audit_log.resource_name,\u00a0 proto_payload.audit_log.method_nameFROM \u00a0 `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 resource.type = \"gce_instance\"\u00a0 AND proto_payload.audit_log.method_name = \"v1.compute.instances.delete\"\u00a0 AND operation.first IS TRUE\u00a0 AND timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)ORDER BY\u00a0 timestamp desc,\u00a0 instance_idLIMIT\u00a0 1000\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/3_20_virtual_machines_deleted.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 resource.labels.instance_id,\u00a0 protopayload_auditlog.authenticationInfo.principalEmail,\u00a0 protopayload_auditlog.resourceName,\u00a0 protopayload_auditlog.methodNameFROM `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_activity`WHERE\u00a0 resource.type = \"gce_instance\"\u00a0 AND protopayload_auditlog.methodName = \"v1.compute.instances.delete\"\u00a0 AND operation.first IS TRUE\u00a0 AND timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)ORDER BY\u00a0 timestamp desc,\u00a0 resource.labels.instance_idLIMIT\u00a0 1000\n```### Workload usage questions\nThese sample queries perform analysis to understand who and what is consuming your cloud workloads and APIs, and help you detect potential malicious behavior internally or externally.\nBy analyzing all Cloud Audit Logs, the following query detects unusually high API usage by any user identity on any given day in the past week. Such unusually high usage might be an indicator of potential API abuse, insider threat, or leaked credentials. To determine whether there is an outlier, this query performs statistical analysis over the daily count of actions per principal. Averages and standard deviations are computed for each day and for each principal by looking back at the preceding daily counts with a lookback window of 60 days. An outlier is considered when the daily count for a user is more than three standard deviations above their mean. The query, including the standard deviation factor and the lookback windows, are all configurable to fit your cloud provisioning activity profile and to minimize false positives.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/4_01_unusually_high_api_usage_by_user_identity.sql) \n```\nSELECT\u00a0 *FROM (\u00a0 SELECT\u00a0 \u00a0 *,\u00a0 \u00a0 AVG(counter) OVER (\u00a0 \u00a0 \u00a0 PARTITION BY principal_email\u00a0 \u00a0 \u00a0 ORDER BY day\u00a0 \u00a0 \u00a0 ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS avg,\u00a0 \u00a0 STDDEV(counter) OVER (\u00a0 \u00a0 \u00a0 PARTITION BY principal_email\u00a0 \u00a0 \u00a0 ORDER BY day\u00a0 \u00a0 \u00a0 ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS stddev,\u00a0 \u00a0 COUNT(*) OVER (\u00a0 \u00a0 \u00a0 PARTITION BY principal_email\u00a0 \u00a0 \u00a0 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS numSamples\u00a0 FROM (\u00a0 \u00a0 SELECT\u00a0 \u00a0 \u00a0 proto_payload.audit_log.authentication_info.principal_email,\u00a0 \u00a0 \u00a0 EXTRACT(DATE FROM timestamp) AS day,\u00a0 \u00a0 \u00a0 ARRAY_AGG(DISTINCT proto_payload.audit_log.method_name IGNORE NULLS) AS actions,\u00a0 \u00a0 \u00a0 COUNT(*) AS counter\u00a0 \u00a0 FROM `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`\u00a0 \u00a0 WHERE\u00a0 \u00a0 \u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 60 DAY)\u00a0 \u00a0 \u00a0 AND proto_payload.audit_log.authentication_info.principal_email IS NOT NULL\u00a0 \u00a0 \u00a0 AND proto_payload.audit_log.method_name NOT LIKE \"storage.%.get\"\u00a0 \u00a0 \u00a0 AND proto_payload.audit_log.method_name NOT LIKE \"v1.compute.%.list\"\u00a0 \u00a0 \u00a0 AND proto_payload.audit_log.method_name NOT LIKE \"beta.compute.%.list\"\u00a0 \u00a0 GROUP BY\u00a0 \u00a0 \u00a0 proto_payload.audit_log.authentication_info.principal_email,\u00a0 \u00a0 \u00a0 day\u00a0 ))WHERE\u00a0 counter > avg + 3 * stddev\u00a0 AND day >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY) ORDER BY\u00a0 counter DESC\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/4_01_unusually_high_api_usage_by_user_identity.sql) \n```\nSELECT\u00a0 *,\u00a0 AVG(counter) OVER (\u00a0 \u00a0 PARTITION BY principalEmail\u00a0 \u00a0 ORDER BY day\u00a0 \u00a0 ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS avg,\u00a0 STDDEV(counter) OVER (\u00a0 \u00a0 PARTITION BY principalEmail\u00a0 \u00a0 ORDER BY day\u00a0 \u00a0 ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS stddev,\u00a0 COUNT(*) OVER (\u00a0 \u00a0 PARTITION BY principalEmail\u00a0 \u00a0 RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS numSamplesFROM (\u00a0 SELECT\u00a0 \u00a0 protopayload_auditlog.authenticationInfo.principalEmail,\u00a0 \u00a0 EXTRACT(DATE FROM timestamp) AS day,\u00a0 \u00a0 ARRAY_AGG(DISTINCT protopayload_auditlog.methodName IGNORE NULLS) AS actions,\u00a0 \u00a0 COUNT(*) AS counter\u00a0 FROM `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_*`\u00a0 WHERE\u00a0 \u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 60 DAY)\u00a0 \u00a0 AND protopayload_auditlog.authenticationInfo.principalEmail IS NOT NULL\u00a0 \u00a0 AND protopayload_auditlog.methodName NOT LIKE \"storage.%.get\"\u00a0 \u00a0 AND protopayload_auditlog.methodName NOT LIKE \"v1.compute.%.list\"\u00a0 \u00a0 AND protopayload_auditlog.methodName NOT LIKE \"beta.compute.%.list\"\u00a0 GROUP BY\u00a0 \u00a0 protopayload_auditlog.authenticationInfo.principalEmail,\u00a0 \u00a0 day)WHERE TRUEQUALIFY\u00a0 counter > avg + 3 * stddev\u00a0 AND day >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY) ORDER BY\u00a0 counter DESC\n```\nBy analyzing Admin Activity audit logs, the following query reports the autoscaling usage by day for the last month. This query can be used to identify patterns or anomalies that warrant further security investigation.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/4_11_autoscaling_usage_frequency_by_day.sql) \n```\nSELECT\u00a0 TIMESTAMP_TRUNC(timestamp, DAY) AS day,\u00a0 proto_payload.audit_log.method_name,\u00a0 COUNT(*) AS counterFROM\u00a0 \u00a0`[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 resource.type = \"gce_instance_group_manager\"\u00a0 AND log_id = \"cloudaudit.googleapis.com/activity\"\u00a0 AND timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)GROUP BY\u00a0 1, 2ORDER BY\u00a0 1, 2\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/4_11_autoscaling_usage_frequency_by_day.sql) \n```\nSELECT\u00a0 TIMESTAMP_TRUNC(timestamp, DAY) AS day,\u00a0 protopayload_auditlog.methodName AS methodName,\u00a0 COUNT(*) AS counterFROM `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_activity`WHERE\u00a0 resource.type = \"gce_instance_group_manager\"\u00a0 AND timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)GROUP BY\u00a0 1, 2ORDER BY\u00a0 1, 2\n```### Data access questions\nThese sample queries perform analysis to understand who is accessing or modifying data in Google Cloud.\nThe following query uses the Data Access audit logs to find the user identities that most frequently accessed BigQuery tables data over the past week.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/5_01_users_who_most_frequently_accessed_data.sql) \n```\nSELECT\u00a0 proto_payload.audit_log.authentication_info.principal_email,\u00a0 COUNT(*) AS COUNTERFROM \u00a0 \u00a0`[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 (proto_payload.audit_log.method_name = \"google.cloud.bigquery.v2.JobService.InsertJob\" OR\u00a0 \u00a0proto_payload.audit_log.method_name = \"google.cloud.bigquery.v2.JobService.Query\")\u00a0 AND timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)\u00a0 AND log_id = \"cloudaudit.googleapis.com/data_access\"GROUP BY\u00a0 1ORDER BY\u00a0 2 desc, 1LIMIT\u00a0 100\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/5_01_users_who_most_frequently_accessed_data.sql) \n```\nSELECT\u00a0 protopayload_auditlog.authenticationInfo.principalEmail,\u00a0 COUNT(*) AS COUNTERFROM `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_data_access`WHERE\u00a0 (protopayload_auditlog.methodName = \"google.cloud.bigquery.v2.JobService.InsertJob\" OR\u00a0 \u00a0protopayload_auditlog.methodName = \"google.cloud.bigquery.v2.JobService.Query\")\u00a0 AND timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)GROUP BY\u00a0 1ORDER BY\u00a0 2 desc, 1LIMIT\u00a0 100\n```\nThe following query uses the Data Access audit logs to find the user identities that most frequently queried a given `accounts` table over the past month. Besides the `` and `` placeholders for your BigQuery export destination, the following query uses the `` and `` placeholders. You need to replace to the `` and `` placeholders in order to specify the target table whose access is being analyzed, such as the `accounts` table in this example.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/5_04_users_who_accessed_data_in_table.sql) \n```\nSELECT\u00a0 proto_payload.audit_log.authentication_info.principal_email,\u00a0 COUNT(*) AS COUNTERFROM \u00a0 `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`,\u00a0 UNNEST(proto_payload.audit_log.authorization_info) authorization_infoWHERE\u00a0 (proto_payload.audit_log.method_name = \"google.cloud.bigquery.v2.JobService.InsertJob\" OR\u00a0 \u00a0proto_payload.audit_log.method_name = \"google.cloud.bigquery.v2.JobService.Query\")\u00a0 AND authorization_info.permission = \"bigquery.tables.getData\"\u00a0 AND authorization_info.resource = \"projects/[PROJECT_ID]/datasets/[DATASET_ID]/tables/accounts\"\u00a0 AND timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)GROUP BY\u00a0 1ORDER BY\u00a0 2 desc, 1LIMIT\u00a0 100\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/5_04_users_who_accessed_data_in_table.sql) \n```\nSELECT\u00a0 protopayload_auditlog.authenticationInfo.principalEmail,\u00a0 COUNT(*) AS COUNTERFROM `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_data_access`,\u00a0 UNNEST(protopayload_auditlog.authorizationInfo) authorizationInfoWHERE\u00a0 (protopayload_auditlog.methodName = \"google.cloud.bigquery.v2.JobService.InsertJob\" OR\u00a0 \u00a0protopayload_auditlog.methodName = \"google.cloud.bigquery.v2.JobService.Query\")\u00a0 AND authorizationInfo.permission = \"bigquery.tables.getData\"\u00a0 AND authorizationInfo.resource = \"projects/[PROJECT_ID]/datasets/[DATASET_ID]/tables/accounts\"\u00a0 AND timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)GROUP BY\u00a0 1ORDER BY\u00a0 2 desc, 1LIMIT\u00a0 100\n```\nThe following query uses the Data Access audit logs to find the BigQuery tables with most frequently read and modified data over the past month. It displays the associated user identity along with breakdown of total number of times data was read versus modified.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/5_05_tables_most_frequently_accessed.sql) \n```\nSELECT\u00a0 proto_payload.audit_log.resource_name,\u00a0 proto_payload.audit_log.authentication_info.principal_email,\u00a0 COUNTIF(JSON_VALUE(proto_payload.audit_log.metadata, \"$.tableDataRead\") IS NOT NULL) AS dataReadEvents,\u00a0 COUNTIF(JSON_VALUE(proto_payload.audit_log.metadata, \"$.tableDataChange\") IS NOT NULL) AS dataChangeEvents,\u00a0 COUNT(*) AS totalEventsFROM \u00a0 `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 STARTS_WITH(resource.type, 'bigquery') IS TRUE\u00a0 AND (JSON_VALUE(proto_payload.audit_log.metadata, \"$.tableDataRead\") IS NOT NULL\u00a0 \u00a0 OR JSON_VALUE(proto_payload.audit_log.metadata, \"$.tableDataChange\") IS NOT NULL)\u00a0 AND timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)GROUP BY\u00a0 1, 2ORDER BY\u00a0 5 DESC, 1, 2LIMIT 1000\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/5_05_tables_most_frequently_accessed.sql) \n```\nSELECT\u00a0 protopayload_auditlog.resourceName,\u00a0 protopayload_auditlog.authenticationInfo.principalEmail,\u00a0 COUNTIF(JSON_EXTRACT(protopayload_auditlog.metadataJson, \"$.tableDataRead\") IS NOT NULL) AS dataReadEvents,\u00a0 COUNTIF(JSON_EXTRACT(protopayload_auditlog.metadataJson, \"$.tableDataChange\") IS NOT NULL) AS dataChangeEvents,\u00a0 COUNT(*) AS totalEventsFROM `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_data_access`WHERE\u00a0 STARTS_WITH(resource.type, 'bigquery') IS TRUE\u00a0 AND (JSON_EXTRACT(protopayload_auditlog.metadataJson, \"$.tableDataRead\") IS NOT NULL\u00a0 \u00a0 OR JSON_EXTRACT(protopayload_auditlog.metadataJson, \"$.tableDataChange\") IS NOT NULL)\u00a0 AND timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)GROUP BY\u00a0 1, 2ORDER BY\u00a0 5 DESC, 1, 2LIMIT 1000\n```\nThe following query uses the Data Access audit logs to find the most common queries over the past week. It also lists the corresponding users and the referenced tables.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/5_06_BQ_queries_top.sql) \n```\nSELECT\u00a0 COALESCE(\u00a0 \u00a0JSON_VALUE(proto_payload.audit_log.metadata, \"$.jobChange.job.jobConfig.queryConfig.query\"),\u00a0 \u00a0JSON_VALUE(proto_payload.audit_log.metadata, \"$.jobInsertion.job.jobConfig.queryConfig.query\")) as query,\u00a0 STRING_AGG(DISTINCT proto_payload.audit_log.authentication_info.principal_email, ',') as users,\u00a0 ANY_VALUE(COALESCE(\u00a0 \u00a0JSON_EXTRACT_ARRAY(proto_payload.audit_log.metadata, \"$.jobChange.job.jobStats.queryStats.referencedTables\"),\u00a0 \u00a0JSON_EXTRACT_ARRAY(proto_payload.audit_log.metadata, \"$.jobInsertion.job.jobStats.queryStats.referencedTables\"))) as tables,\u00a0 COUNT(*) AS counterFROM \u00a0 `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 (resource.type = 'bigquery_project' OR resource.type = 'bigquery_dataset')\u00a0 AND operation.last IS TRUE\u00a0 AND (JSON_VALUE(proto_payload.audit_log.metadata, \"$.jobChange\") IS NOT NULL\u00a0 \u00a0 OR JSON_VALUE(proto_payload.audit_log.metadata, \"$.jobInsertion\") IS NOT NULL)\u00a0 AND timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)GROUP BY\u00a0 queryORDER BY\u00a0 counter DESCLIMIT 10\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/5_06_BQ_queries_top.sql) \n```\nSELECT\u00a0 COALESCE(\u00a0 \u00a0JSON_EXTRACT_SCALAR(protopayload_auditlog.metadataJson, \"$.jobChange.job.jobConfig.queryConfig.query\"),\u00a0 \u00a0JSON_EXTRACT_SCALAR(protopayload_auditlog.metadataJson, \"$.jobInsertion.job.jobConfig.queryConfig.query\")) as query,\u00a0 STRING_AGG(DISTINCT protopayload_auditlog.authenticationInfo.principalEmail, ',') as users,\u00a0 ANY_VALUE(COALESCE(\u00a0 \u00a0JSON_EXTRACT_ARRAY(protopayload_auditlog.metadataJson, \"$.jobChange.job.jobStats.queryStats.referencedTables\"),\u00a0 \u00a0JSON_EXTRACT_ARRAY(protopayload_auditlog.metadataJson, \"$.jobInsertion.job.jobStats.queryStats.referencedTables\"))) as tables,\u00a0 COUNT(*) AS counterFROM `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_data_access`WHERE\u00a0 (resource.type = 'bigquery_project' OR resource.type = 'bigquery_dataset')\u00a0 AND operation.last IS TRUE\u00a0 AND (JSON_EXTRACT(protopayload_auditlog.metadataJson, \"$.jobChange\") IS NOT NULL\u00a0 \u00a0 OR JSON_EXTRACT(protopayload_auditlog.metadataJson, \"$.jobInsertion\") IS NOT NULL)\u00a0 AND timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)GROUP BY\u00a0 queryORDER BY\u00a0 counter DESCLIMIT 10\n```\nThe following query uses all logs from Cloud Audit Logs to find the 100 most frequent actions recorded over the past month.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/5_20_top_data_access_actions.sql) \n```\nSELECT\u00a0 proto_payload.audit_log.method_name,\u00a0 proto_payload.audit_log.service_name,\u00a0 resource.type,\u00a0 COUNT(*) AS counterFROM `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)\u00a0 AND log_id=\"cloudaudit.googleapis.com/data_access\"GROUP BY\u00a0 proto_payload.audit_log.method_name,\u00a0 proto_payload.audit_log.service_name,\u00a0 resource.typeORDER BY\u00a0 counter DESCLIMIT 100\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/5_20_top_data_access_actions.sql) \n```\nSELECT\u00a0 protopayload_auditlog.methodName,\u00a0 protopayload_auditlog.serviceName,\u00a0 resource.type,\u00a0 COUNT(*) AS counterFROM `[MY_PROJECT_ID].[MY_DATASET_ID].cloudaudit_googleapis_com_data_access`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)GROUP BY\u00a0 protopayload_auditlog.methodName,\u00a0 protopayload_auditlog.serviceName,\u00a0 resource.typeORDER BY\u00a0 counter DESCLIMIT 100\n```### Network security questions\nThese sample queries perform analysis over your network activity in Google Cloud.\nThe following query detects connections from any new source IP address to a given subnet by analyzing VPC Flow Logs. In this example, a source IP address is considered new if it was seen for the first time in the last 24 hours over a lookback window of 60 days. You might want to use and tune this query on a subnet that is in-scope for a particular compliance requirement like PCI.\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/6_10_connection_from_new_IP.sql) \n```\nSELECT\u00a0 JSON_VALUE(json_payload.connection.src_ip) as src_ip,\u00a0 -- TIMESTAMP supports up to 6 digits of fractional precision, so drop any more digits to avoid parse errors\u00a0 MIN(TIMESTAMP(REGEXP_REPLACE(JSON_VALUE(json_payload.start_time), r'\\.(\\d{0,6})\\d+(Z)?$', '.\\\\1\\\\2'))) AS firstInstance,\u00a0 MAX(TIMESTAMP(REGEXP_REPLACE(JSON_VALUE(json_payload.start_time), r'\\.(\\d{0,6})\\d+(Z)?$', '.\\\\1\\\\2'))) AS lastInstance,\u00a0 ARRAY_AGG(DISTINCT JSON_VALUE(resource.labels.subnetwork_name)) as subnetNames,\u00a0 ARRAY_AGG(DISTINCT JSON_VALUE(json_payload.dest_instance.vm_name)) as vmNames,\u00a0 COUNT(*) numSamplesFROM `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 60 DAY)\u00a0 AND JSON_VALUE(json_payload.reporter) = 'DEST'\u00a0 AND JSON_VALUE(resource.labels.subnetwork_name) IN ('prod-customer-data')GROUP BY\u00a0 src_ipHAVING firstInstance >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)ORDER BY\u00a0 lastInstance DESC,\u00a0 numSamples DESC\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/6_10_connection_from_new_IP.sql) \n```\nSELECT\u00a0 jsonPayload.connection.src_ip as src_ip,\u00a0 -- TIMESTAMP supports up to 6 digits of fractional precision, so drop any more digits to avoid parse errors\u00a0 MIN(TIMESTAMP(REGEXP_REPLACE(jsonPayload.start_time, r'\\.(\\d{0,6})\\d+(Z)?$', '.\\\\1\\\\2'))) AS firstInstance,\u00a0 MAX(TIMESTAMP(REGEXP_REPLACE(jsonPayload.start_time, r'\\.(\\d{0,6})\\d+(Z)?$', '.\\\\1\\\\2'))) AS lastInstance,\u00a0 ARRAY_AGG(DISTINCT resource.labels.subnetwork_name) as subnetNames,\u00a0 ARRAY_AGG(DISTINCT jsonPayload.dest_instance.vm_name) as vmNames,\u00a0 COUNT(*) numSamplesFROM `[MY_PROJECT_ID].[MY_DATASET_ID].compute_googleapis_com_vpc_flows`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 60 DAY)\u00a0 AND jsonPayload.reporter = 'DEST'\u00a0 AND resource.labels.subnetwork_name IN ('prod-customer-data')GROUP BY\u00a0 src_ipHAVING firstInstance >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)ORDER BY\u00a0 lastInstance DESC,\u00a0 numSamples DESC\n```\nThe following query helps detect potential exploit attempts by analyzing external Application Load Balancer logs to find any connection blocked by the security policy configured in Google Cloud Armor. This query assumes that you have a Google Cloud Armor security policy configured on your external Application Load Balancer. This query also assumes that you have enabled external Application Load Balancer logging as described in the instructions that are provided by the **Enable** link in the [log scoping tool](#log_scoping_tool) .\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/6_20_connections_blocked_by_cloud_armor.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 http_request.remote_ip,\u00a0 http_request.request_method,\u00a0 http_request.status,\u00a0 JSON_VALUE(json_payload.enforcedSecurityPolicy.name) AS security_policy_name,\u00a0 JSON_VALUE(resource.labels.backend_service_name) AS backend_service_name,\u00a0 http_request.request_url,FROM `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)\u00a0 AND resource.type=\"http_load_balancer\"\u00a0 AND JSON_VALUE(json_payload.statusDetails) = \"denied_by_security_policy\"ORDER BY\u00a0 timestamp DESC\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/6_20_connections_blocked_by_cloud_armor.sql) \n```\nSELECT\u00a0 timestamp,\u00a0 httpRequest.remoteIp,\u00a0 httpRequest.requestMethod,\u00a0 httpRequest.status,\u00a0 jsonpayload_type_loadbalancerlogentry.enforcedsecuritypolicy.name,\u00a0 resource.labels.backend_service_name,\u00a0 httpRequest.requestUrl,FROM `[MY_PROJECT_ID].[MY_DATASET_ID].requests`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)\u00a0 AND resource.type=\"http_load_balancer\"\u00a0 AND jsonpayload_type_loadbalancerlogentry.statusdetails = \"denied_by_security_policy\"ORDER BY\u00a0 timestamp DESC\n```\nThe following query shows any high-severity virus or malware detected by Cloud IDS by searching Cloud IDS Threat Logs. This query assumes that you have a [Cloud IDS endpoint configured](/intrusion-detection-system/docs/configuring-ids) .\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/6_30_virus_or_malware_detected_by_cloud_IDS.sql) \n```\nSELECT\u00a0 JSON_VALUE(json_payload.alert_time) AS alert_time,\u00a0 JSON_VALUE(json_payload.name) AS name,\u00a0 JSON_VALUE(json_payload.details) AS details,\u00a0 JSON_VALUE(json_payload.application) AS application,\u00a0 JSON_VALUE(json_payload.uri_or_filename) AS uri_or_filename,\u00a0 JSON_VALUE(json_payload.ip_protocol) AS ip_protocol,FROM `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)\u00a0 AND resource.type=\"ids.googleapis.com/Endpoint\"\u00a0 AND JSON_VALUE(json_payload.alert_severity) IN (\"HIGH\", \"CRITICAL\")\u00a0 AND JSON_VALUE(json_payload.type) = \"virus\"ORDER BY \u00a0 timestamp DESC\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/6_30_virus_or_malware_detected_by_cloud_IDS.sql) \n```\nSELECT\u00a0 jsonPayload.alert_time,\u00a0 jsonPayload.name,\u00a0 jsonPayload.details,\u00a0 jsonPayload.application,\u00a0 jsonPayload.uri_or_filename,\u00a0 jsonPayload.ip_protocolFROM `[MY_PROJECT_ID].[MY_DATASET_ID].ids_googleapis_com_threat`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)\u00a0 AND resource.type=\"ids.googleapis.com/Endpoint\"\u00a0 AND jsonPayload.alert_severity IN (\"HIGH\", \"CRITICAL\")\u00a0 AND jsonPayload.type = \"virus\"ORDER BY \u00a0 timestamp DESC\n```\nThe following query lists the top 10 Cloud DNS queried domains from your VPC network(s) over the last 60 days. This query assumes that you have enabled Cloud DNS logging for your VPC network(s) as described in the instructions that are provided by the **Enable** link in the [log scoping tool](#log_scoping_tool) .\n[View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/log_analytics/sql/6_40_DNS_top_queried_domains.sql) \n```\nSELECT\u00a0 JSON_VALUE(json_payload.queryName) AS query_name,\u00a0 COUNT(*) AS total_queriesFROM\u00a0 `[MY_PROJECT_ID].[MY_LOG_BUCKET_REGION].[MY_LOG_BUCKET_NAME]._AllLogs`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 60 DAY)\u00a0 AND log_id=\"dns.googleapis.com/dns_queries\"GROUP BY\u00a0 query_nameORDER BY\u00a0 total_queries DESCLIMIT\u00a0 10\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/security-analytics/blob/main/backends/bigquery/sql/6_40_DNS_top_queried_domains.sql) \n```\nSELECT\u00a0jsonPayload.queryname AS query_name,\u00a0COUNT(*) AS total_queriesFROM\u00a0`[MY_PROJECT_ID].[MY_DATASET_ID].dns_googleapis_com_dns_queries`WHERE\u00a0 timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 60 DAY)GROUP BY\u00a0query_nameORDER BY\u00a0total_queries DESCLIMIT\u00a010\n```\n## What's next\n- Look at the other export scenarios:- [Scenario \u2013 Export for compliance requirements](/architecture/exporting-stackdriver-logging-for-compliance-requirements) .\n- [Scenario \u2013 Export to Splunk](/architecture/exporting-stackdriver-logging-for-splunk) .\n- [Ingesting Google Cloud logs to Chronicle](/security/chronicle) .\n- [Exporting Google Cloud security data to your SIEM system](/community/tutorials/exporting-security-data-to-your-siem) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}