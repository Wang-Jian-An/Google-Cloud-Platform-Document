{"title": "Dataflow - Write from Dataflow to Cloud Storage", "url": "https://cloud.google.com/dataflow/docs/guides/write-to-cloud-storage", "abstract": "# Dataflow - Write from Dataflow to Cloud Storage\nThis document describes how to write text data from Dataflow to Cloud Storage by using the Apache Beam `TextIO` [I/O connector](https://beam.apache.org/documentation/io/connectors/) .\n**Note:** Depending on your scenario, consider using one of the [Google-provided Dataflow templates](/dataflow/docs/guides/templates/provided-templates) . Several of these templates output to Cloud Storage.\n", "content": "## Include the Google Cloud library dependency\nTo use the `TextIO` connector with Cloud Storage, include the following dependency. This library provides a schema handler for `\"gs://\"` filenames.\n```\n<dependency>\u00a0 <groupId>org.apache.beam</groupId>\u00a0 <artifactId>beam-sdks-java-io-google-cloud-platform</artifactId>\u00a0 <version>${beam.version}</version></dependency>\n```\n```\napache-beam[gcp]==VERSION\n```\n```\nimport _ \"github.com/apache/beam/sdks/v2/go/pkg/beam/io/filesystem/gcs\"\n```\nFor more information, see [Install the Apache Beam SDK](/dataflow/docs/guides/installing-beam-sdk) .\n## Parallelism\nParallelism is determined primarily by the number of shards. By default, the runner automatically sets this value. For most pipelines, using the default behavior is recommended. In this document, see [Best practices](#best-practices) .\n## Performance\nThe following table shows performance metrics for writing to Cloud Storage. The workloads were run on one `e2-standard2` worker, using the Apache Beam SDK 2.49.0 for Java. They did not use Runner v2.\n| 100 M records | 1 kB | 1 column | Throughput (bytes) | Throughput (elements)  |\n|:----------------------------------|:---------------------|:----------------------------|\n| Write        | 130 MBps    | 130,000 elements per second |\nThese metrics are based on simple batch pipelines. They are intended to compare performance between I/O connectors, and are not necessarily representative of real-world pipelines. Dataflow pipeline performance is complex, and is a function of VM type, the data being processed, the performance of external sources and sinks, and user code. Metrics are based on running the Java SDK, and aren't representative of the performance characteristics of other language SDKs. For more information, see [Beam IO Performance](https://beam.apache.org/performance/) .\n## Best practices\n- In general, avoid setting a specific number of shards. This allows the runner to select an appropriate value for your scale. If you tune the number of shards, we recommend writing between 100MB and 1GB per shard. However, the optimum value might depend on the workload.\n- Cloud Storage can scale to a very large number of requests per second. However, if your pipeline has large spikes in write volume, consider writing to multiple buckets, to avoid temporarily overloading any single Cloud Storage bucket.\n- In general, writing to Cloud Storage is more efficient when each write is larger (1 kb or greater). Writing small records to a large number of files can result in worse performance per byte.\n- When generating file names, consider using non-sequential file names, in order to distribute load. For more information, see [Use a naming convention that distributes load evenly across key ranges](/storage/docs/request-rate#naming-convention) .\n- When naming files, don't use the at sign ('@') followed by a number or an asterisk ('*'). For more information, see [\"@*\" and \"@N\" are reserved sharding specs](/dataflow/docs/guides/common-errors#reserved-sharding-spec) .## Example: Write text files to Cloud Storage\nThe following example creates a batch pipeline that writes text files using GZIP compression:\nTo authenticate to Dataflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BatchWriteStorage.java) \n```\nimport java.util.Arrays;import java.util.List;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.Compression;import org.apache.beam.sdk.io.TextIO;import org.apache.beam.sdk.options.Description;import org.apache.beam.sdk.options.PipelineOptions;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.Create;public class BatchWriteStorage {\u00a0 public interface Options extends PipelineOptions {\u00a0 \u00a0 @Description(\"The Cloud Storage bucket to write to\")\u00a0 \u00a0 String getBucketName();\u00a0 \u00a0 void setBucketName(String value);\u00a0 }\u00a0 // Write text data to Cloud Storage\u00a0 public static void main(String[] args) {\u00a0 \u00a0 final List<String> wordsList = Arrays.asList(\"1\", \"2\", \"3\", \"4\");\u00a0 \u00a0 var options = PipelineOptionsFactory.fromArgs(args).withValidation().as(Options.class);\u00a0 \u00a0 var pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 .apply(Create\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .of(wordsList))\u00a0 \u00a0 \u00a0 \u00a0 .apply(TextIO\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .write()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .to(options.getBucketName())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withSuffix(\".txt\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withCompression(Compression.GZIP)\u00a0 \u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 pipeline.run().waitUntilFinish();\u00a0 }}\n```\nIf the input `PCollection` is unbounded, you must define a window or a trigger on the collection, and then specify windowed writes by calling [TextIO.Write.withWindowedWrites](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/TextIO.Write.html#withWindowedWrites--) .\nTo authenticate to Dataflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataflow/snippets/batch_write_storage.py) \n```\nimport argparsefrom typing import Listimport apache_beam as beamfrom apache_beam.io.textio import WriteToTextfrom apache_beam.options.pipeline_options import PipelineOptionsfrom typing_extensions import Selfdef write_to_cloud_storage(argv : List[str] = None) -> None:\u00a0 \u00a0 # Parse the pipeline options passed into the application.\u00a0 \u00a0 class MyOptions(PipelineOptions):\u00a0 \u00a0 \u00a0 \u00a0 @classmethod\u00a0 \u00a0 \u00a0 \u00a0 # Define a custom pipeline option that specfies the Cloud Storage bucket.\u00a0 \u00a0 \u00a0 \u00a0 def _add_argparse_args(cls: Self, parser: argparse.ArgumentParser) -> None:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parser.add_argument(\"--output\", required=True)\u00a0 \u00a0 wordsList = [\"1\", \"2\", \"3\", \"4\"]\u00a0 \u00a0 options = MyOptions()\u00a0 \u00a0 with beam.Pipeline(options=options) as pipeline:\u00a0 \u00a0 \u00a0 \u00a0 (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"Create elements\" >> beam.Create(wordsList)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"Write Files\" >> WriteToText(options.output, file_name_suffix=\".txt\")\u00a0 \u00a0 \u00a0 \u00a0 )\n```\nFor the output path, specify a Cloud Storage path that includes the bucket name and a filename prefix. For example, if you specify `gs://my_bucket/output/file` , the `TextIO` connector writes to the Cloud Storage bucket named `my_bucket` , and the output files have the prefix `output/file*` .\nBy default, the `TextIO` connector shards the output files, using a naming convention like this: `<file-prefix>-00000-of-00001` . Optionally, you can specify a filename suffix and a compression scheme, as shown in the example.\nTo ensure idempotent writes, Dataflow writes to a temporary file and then copies the completed temporary file to the final file.\n## What's next\n- Read the [TextIO](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/TextIO.html) API documentation.\n- See the list of [Google-provided templates](/dataflow/docs/guides/templates/provided-templates) .", "guide": "Dataflow"}