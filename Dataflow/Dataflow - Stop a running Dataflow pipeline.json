{"title": "Dataflow - Stop a running Dataflow pipeline", "url": "https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline", "abstract": "# Dataflow - Stop a running Dataflow pipeline\nTo stop a Dataflow job, use either the [Google Cloud console](/dataflow/docs/guides/using-monitoring-intf) , [Cloud Shell](/shell/docs/using-cloud-shell) , a local terminal installed with the [Google Cloud CLI](/sdk/docs/install#installation_options) , or the [Dataflow REST API](/dataflow/docs/reference/rest) .\nYou can stop a Dataflow job in one of the following three ways:\n- **Cancel a job.** This method applies to both streaming pipelines and batch pipelines. Canceling a job stops the Dataflow service from processing any data, including buffered data. For more information, see [Cancel a job](#cancel) .\n- **Drain a job.** This method applies only to streaming pipelines. Draining a job enables the Dataflow service to finish processing the buffered data while simultaneously ceasing the ingestion of new data. For more information, see [Drain a job](#drain) .\n- **Force cancel a job.** This method applies to both streaming pipelines and batch pipelines. Force canceling a job immediately stops the Dataflow service from processing any data, including buffered data. Before force canceling, you must first attempt a regular cancel. Force canceling is only intended for jobs that have become stuck in the regular canceling process. For more information, see [Force cancel a job](#forcecancel) .\nWhen you cancel a job, you can't restart it. If you're not using Flex Templates, you can clone the cancelled pipeline and start a new job from the cloned pipeline.\nBefore stopping a streaming pipeline, consider creating a snapshot of the job. Dataflow snapshots save the state of a streaming pipeline, so you can start a new version of your Dataflow job without losing state. To learn more, see [Using Dataflow snapshots](/dataflow/docs/guides/using-snapshots) .\nIf you have a complex pipeline, consider creating a [template](/dataflow/docs/concepts/dataflow-templates) and running the job from the template.\nYou can't delete Dataflow jobs, but you can [archive](#archive) completed jobs. All completed jobs are deleted after a 30 day retention period.\n", "content": "## Cancel a Dataflow job\nWhen you cancel a job, the Dataflow service stops the job immediately.\nThe following actions occur when you cancel a job:\n- The Dataflow service halts all data ingestion and data processing.\n- The Dataflow service begins cleaning up the Google Cloud resources that are attached to your job.These resources might include shutting down Compute Engine worker instances and closing active connections to I/O sources or sinks.\n### Important information about canceling a job\n- Canceling a job immediately halts the processing of the pipeline.\n- You might lose in-flight data when you cancel a job. In-flight data refers to data that is already read but is still being processed by the pipeline.\n- Data that's written from the pipeline to an output sink before you cancelled the job might still be accessible on your output sink.\n- If data loss is not a concern, canceling your job ensures that the Google Cloud resources that are associated with your job are shut down as soon as possible.## Drain a Dataflow job\nWhen you drain a job, the Dataflow service finishes your job in its current state. If you want to prevent data loss as you bring down your streaming pipelines, the best option is to drain your job.\nThe following actions occur when you drain a job:\n- Your job stops ingesting new data from input sources soon after receiving the drain request (typically within a few minutes).\n- The Dataflow service preserves any existing resources, such as worker instances, to finish processing and writing any buffered data in your pipeline.\n- When all pending processing and write operations are complete, the Dataflow service shuts down Google Cloud resources that are associated with your job.\nTo drain your job, Dataflow stops reading new input, marks the source with an event timestamp at infinity, and then propagates infinity timestamps through the pipeline. Therefore, pipelines in the process of draining might have an infinite watermark.\n### Important information about draining a job\n- Draining a job is not supported for batch pipelines.\n- Your pipeline continues to incur the cost of maintaining any associated Google Cloud resources until all processing and writing is finished.\n- You can [update](/dataflow/docs/guides/updating-a-pipeline) a pipeline that is being drained. If your pipeline is stuck, updating the pipeline with code that fixes the error that is creating the problem enables a successful drain without data loss.\n- You can cancel a job that is currently draining.\n- Draining a job can take a significant amount of time to complete, such as when your pipeline has a large amount of buffered data.\n- If your streaming pipeline includes a [Splittable DoFn](https://beam.apache.org/documentation/programming-guide/#splittable-dofns) , you must truncate the result before running the drain option. For more information about truncating Splittable DoFns, see the [Apache Beam](https://beam.apache.org/documentation/programming-guide/#truncating-during-drain) documentation.\n- In some cases, a Dataflow job might be unable to complete the drain operation. You can consult the job logs to determine the root cause and take appropriate action.\n### Data retention\n- Dataflow streaming is tolerant to workers restarting and doesn't fail streaming jobs when errors occur. Instead, the Dataflow service retries until you take an action such as canceling or restarting the job. When you drain the job, Dataflow continues to retry, which can lead to stuck pipelines. In this situation, to enable a successful drain without data loss, update the pipeline with code that fixes the error that is creating the problem.\n- Dataflow doesn't acknowledge messages until the Dataflow service durably commits them. For example, with Kafka, you can view this process as a safe handoff of ownership of the message from Kafka to Dataflow, eliminating the risk of data loss.\n### Stuck jobs\n- Draining doesn't fix stuck pipelines. If data movement is blocked, the pipeline remains stuck after the drain command. To address a stuck pipeline, use the [update](/dataflow/docs/guides/updating-a-pipeline) command to update the pipeline with code that resolves the error that is creating the problem. You can also [cancel](#cancel) stuck jobs, but canceling jobs might result in data loss.\n### Timers\n- If your streaming pipeline code includes a looping timer, the job might be slow or unable to drain. Because draining doesn't finish until all timers complete, pipelines with infinite looping timers never finish draining.\n- Dataflow waits until all processing-time timers complete instead of firing them right away, which might result in slow drains.\n### Effects of draining a job\nWhen you drain a streaming pipeline, Dataflow immediately closes any in-process [windows](https://beam.apache.org/documentation/programming-guide/#windowing) and fires all [triggers](https://beam.apache.org/documentation/programming-guide/#triggers) .\nThe system doesn't wait for any outstanding time-based windows to finish in a drain operation.\nFor example, if your pipeline is ten minutes into a two-hour window when you drain the job, Dataflow doesn't wait for the remainder of the window to finish. It closes the window immediately with partial results. Dataflow causes open windows to close by advancing the data watermark to infinity. This functionality also works with custom data sources.\nWhen draining a pipeline that uses a custom data source class, Dataflow stops issuing requests for new data, advances the data watermark to infinity, and calls your source's `finalize()` method on the last checkpoint.\nDraining can result in partially filled windows. In that case, if you restart the drained pipeline, the same window might fire a second time, which can cause issues with your data. For example, in the following scenario, files might have conflicting names, and data might be overwritten:\nIf you drain a pipeline with hourly windowing at 12:34 PM, the 12:00 PM to 1:00 PM window closes with only the data that fired within the first 34 minutes of the window. The pipeline doesn't read new data after 12:34 PM.\nIf you then immediately restart the pipeline, the 12:00 PM to 1:00 PM window is triggered again, with only the data that was read from 12:35 PM to 1:00 PM. No duplicates are sent, but if a filename is repeated, data is overwritten.\nIn the Google Cloud console, you can view the details of your pipeline's transforms. The following diagram shows the effects of an in-process drain operation. Note that the watermark is advanced to the maximum value.\n**Figure 1.** A step view of a drain operation.\n## Force cancel a Dataflow job\nUse force cancel only when you're unable to cancel your job using other methods. Force cancel terminates your job without cleaning up all of the resources. If you use force cancel repeatedly, leaked resources might accumulate, and leaked resources use your quota.\nWhen you force cancel a job, the Dataflow service stops the job immediately, leaking any VMs the Dataflow job created. Regular cancel must be attempted at least 30 minutes before force canceling.\nThe following actions occur when you force cancel a job:\n- The Dataflow service halts all data ingestion and data processing.\n### Important information about force canceling a job\n- Force canceling a job immediately halts the processing of the pipeline.\n- Force canceling a job is only intended for jobs that have become stuck in the regular canceling process.\n- Any worker instances that the Dataflow job created are not necessarily released, which might result in leaked worker instances. Leaked worker instances don't contribute to job costs but they might use your quota. After the job cancellation completes, you can delete these resources.For Dataflow Prime jobs, you can't see or delete the leaked VMs. In most cases, these VMs don't create issues. However, if the leaked VMs cause problems, such as consuming your VM quota, contact support.## Stop a Dataflow job\nBefore stopping a job, you must understand the effects of [canceling](#cancel) , [draining](#drain) , or [force canceling](#forcecancel) a job.\n- Go to the Dataflow **Jobs** page. [Go to Jobs](https://console.cloud.google.com/dataflow) \n- Click the job that you want to stop.To stop a job, the status of the job must be **running** .\n- In the job details page, click **Stop** .\n- Do one of the following:- For a batch pipeline, click **Cancel** or **Force Cancel** .\n- For a streaming pipeline, click either **Cancel** , **Drain** or **Force Cancel** .\n- To confirm your choice, click **Stop Job** .\nTo either drain or cancel a Dataflow job, you can use the [gcloud dataflow jobs command](/sdk/gcloud/reference/dataflow/jobs/list) in the Cloud Shell or a local terminal installed with the gcloud CLI.- Log in to your shell.\n- List the job IDs for the Dataflow jobs that are currently running, and then note the job ID for the job that you want to stop:```\ngcloud dataflow jobs list\n```If the `--region` flag is not set, Dataflow jobs from all available regions are displayed.\n- Do one of the following:- To drain a streaming job:```\ngcloud dataflow jobs drain JOB_ID\n```Replace `` with the job ID that you copied earlier.\n- To cancel a batch or streaming job:```\ngcloud dataflow jobs cancel JOB_ID\n```Replace `` with the job ID that you copied earlier.\n- To force cancel a batch or streaming job:```\ngcloud dataflow jobs cancel JOB_ID --force\n```Replace `` with the job ID that you copied earlier.\nTo cancel or drain a job using the [Dataflow REST API](/dataflow/docs/reference/rest) , you can choose either [projects.locations.jobs.update](/dataflow/docs/reference/rest/v1b3/projects.locations.jobs/update) or [projects.jobs.update](/dataflow/docs/reference/rest/v1b3/projects.jobs/update) . In the request body, pass the required [job state](/dataflow/docs/reference/rest/v1b3/projects.jobs#Job.JobState) in the `requestedState` field of the job instance of the chosen API.\n **Important** : Using [projects.locations.jobs.update](/dataflow/docs/reference/rest/v1b3/projects.locations.jobs/update) is recommended, as [projects.jobs.update](/dataflow/docs/reference/rest/v1b3/projects.jobs/update) only allows updating the state of jobs running in `us-central1` .- To cancel the job, set the job state to `JOB_STATE_CANCELLED` .\n- To drain the job, set the job state to `JOB_STATE_DRAINED` .\n- To force cancel the job, set the job state to `JOB_STATE_CANCELLED` with the label `\"force_cancel_job\": \"true\"` . The request body is:```\n\u200b\u200b{\u00a0 \"requestedState\": \"JOB_STATE_CANCELLED\",\u00a0 \"labels\": {\u00a0 \u00a0 \"force_cancel_job\": \"true\"\u00a0 }}\n```## Detect Dataflow job completion\nTo detect when the job cancellation or draining has completed, use one of the following methods:\n- Use a workflow orchestration service such as [Cloud Composer](/composer/docs/concepts/overview) to monitor your Dataflow job.\n- Run the pipeline synchronously so that tasks are blocked until pipeline completion. For more information, see [Controlling execution modes](/dataflow/docs/guides/setting-pipeline-options#controlling_execution_modes) in Setting pipeline options.\n- Use the command-line tool in the [Google Cloud CLI](/sdk/docs) to poll the job status. To get a list of all the Dataflow jobs in your project, run the following command in your shell or terminal:```\ngcloud dataflow jobs list\n```The output shows the job ID, name, status ( `STATE` ), and other information for each job. For more information, see [Using the Dataflow command-line interface](/dataflow/docs/guides/using-command-line-intf) .\n**Note:** Terminated Dataflow jobs are purged after 30 days.\n## Archive Dataflow jobs\nWhen you archive a Dataflow job, the job is removed from the list of jobs in the Dataflow **Jobs** page in the console. The job is moved to an archived jobs list. You can only archive completed jobs, which includes jobs in the following states:\n- `JOB_STATE_CANCELLED`\n- `JOB_STATE_DRAINED`\n- `JOB_STATE_DONE`\n- `JOB_STATE_FAILED`\n- `JOB_STATE_UPDATED`\nFor more information, see [Detect Dataflow job completion](#job-completion) in this document. For troubleshooting information, see [Archive job errors](/dataflow/docs/guides/common-errors#archive-jobs) in \"Troubleshoot Dataflow errors.\"\nAll completed jobs are deleted after a 30 day retention period.\n### Archive a job\nFollow these steps to remove a completed job from the main jobs list on the Dataflow **Jobs** page.\n- In the Google Cloud console, go to the Dataflow **Jobs** page. [Go to Jobs](https://console.cloud.google.com/dataflow/jobs) A list of Dataflow jobs appears along with their status.\n- Select a job.\n- On the **Job Details** page, click **Archive** . If the job hasn't completed, the **Archive** option isn't available.\nTo archive jobs by using the API, use the [JobMetadata](/dataflow/docs/reference/rest/v1b3/projects.jobs#jobmetadata) field. In the `JobMetadata` field, for `userDisplayProperties` , use the key-value pair `\"archived\":\"true\"` .\nYour API request must also include the [updateMask](/dataflow/docs/reference/rest/v1b3/projects.jobs/update#query-parameters) query parameter.\n```\ncurl --request PUT \\\"https://dataflow.googleapis.com/v1b3/projects/PROJECT_ID/locations/REGION/jobs/JOB_ID/?updateMask=job_metadata.user_display_properties._archived\" \\\u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 -H \"Accept: application/json\" \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 --data'{\"job_metadata\":{\"userDisplayProperties\":{\"archived\":\"true\"}}}' \\\u00a0 --compressed\n```\nReplace the following:- ``: your project ID\n- ``: a [Dataflow region](/dataflow/docs/resources/locations) \n- ``: the ID of your Dataflow job\n### View and restore archived jobs\nFollow these steps to view archived jobs or to restore archived jobs to the main jobs list on the Dataflow **Jobs** page.\n- In the Google Cloud console, go to the Dataflow **Jobs** page. [Go to Jobs](https://console.cloud.google.com/dataflow/jobs) \n- Click the **Archived** toggle. A list of archived Dataflow jobs appears.\n- Select a job.\n- To restore the job to the main jobs list on the Dataflow **Jobs** page, on the **Job Details** page, click **Restore** .\nTo restore jobs by using the API, use the [JobMetadata](/dataflow/docs/reference/rest/v1b3/projects.jobs#jobmetadata) field. In the `JobMetadata` field, for `userDisplayProperties` , use the key-value pair `\"archived\":\"false\"` .\nYour API request must also include the [updateMask](/dataflow/docs/reference/rest/v1b3/projects.jobs/update#query-parameters) query parameter.\n```\ncurl --request PUT \\\"https://dataflow.googleapis.com/v1b3/projects/PROJECT_ID/locations/REGION/jobs/JOB_ID/?updateMask=job_metadata.user_display_properties._archived\" \\\u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 -H \"Accept: application/json\" \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 --data'{\"job_metadata\":{\"userDisplayProperties\":{\"archived\":\"false\"}}}' \\\u00a0 --compressed\n```\nReplace the following:- ``: your project ID\n- ``: a [Dataflow region](/dataflow/docs/resources/locations) \n- ``: the ID of your Dataflow job## What's next\n- Explore the [Dataflow command line](/dataflow/docs/guides/using-command-line-intf) .\n- Explore the [Dataflow REST API](/dataflow/docs/reference/rest) .\n- Explore the [Dataflow monitoring interface](/dataflow/docs/guides/using-monitoring-intf) in the Google Cloud console.\n- Learn more about [updating a pipeline](/dataflow/docs/guides/updating-a-pipeline) .", "guide": "Dataflow"}