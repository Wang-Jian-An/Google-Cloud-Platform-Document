{"title": "Dataflow - \u5728\u6d41\u8655\u7406\u6d41\u6c34\u7dda\u4e2d\u904b\u884c LLM", "url": "https://cloud.google.com/dataflow/docs/tutorials/streaming-llm?hl=zh-cn", "abstract": "# Dataflow - \u5728\u6d41\u8655\u7406\u6d41\u6c34\u7dda\u4e2d\u904b\u884c LLM\n\u672c\u6559\u7a0b\u4ecb\u7d39\u5982\u4f55\u4f7f\u7528 Apache Beam RunInference API \u5728\u6d41\u8655\u7406 Dataflow \u6d41\u6c34\u7dda\u4e2d\u904b\u884c\u5927\u8a9e\u8a00\u6a21\u578b (LLM)\u3002\n\u5982\u9700\u8a73\u7d30\u77ad\u89e3 RunInference API\uff0c\u8acb\u53c3\u95b1 Apache Beam \u6587\u6a94\u4e2d\u7684 [Beam ML \u7c21\u4ecb](https://beam.apache.org/documentation/ml/about-ml/) \u3002\n\u60a8 [\u53ef\u4ee5\u5728 GitHub \u4e0a](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/dataflow/run-inference) \u627e\u5230\u793a\u4f8b\u4ee3\u78bc\u3002", "content": "## \u76ee\u6a19\n- \u7232\u6a21\u578b\u7684\u8f38\u5165\u548c\u97ff\u61c9\u5275\u5efa Pub/Sub \u4e3b\u984c\u548c\u8a02\u95b1\u3002\n- \u4f7f\u7528 Vertex AI \u81ea\u5b9a\u7fa9\u4f5c\u696d\u5c07\u6a21\u578b\u52a0\u8f09\u5230 Cloud Storage \u4e2d\u3002\n- \u904b\u884c\u6d41\u6c34\u7dda\u3002\n- \u5411\u6a21\u578b\u63d0\u554f\u4e26\u7372\u5f97\u97ff\u61c9\u3002\n## \u8cbb\u7528\nTitles in dynamic includes are not used anywhere, and we should avoid paying to translate them\u5728\u672c\u6587\u6a94\u4e2d\uff0c\u60a8\u5c07\u4f7f\u7528 Google Cloud \u7684\u4ee5\u4e0b\u6536\u8cbb\u7d44\u4ef6\uff1a- [Dataflow](https://cloud.google.com/dataflow/pricing?hl=zh-cn) \n- [Cloud Storage](https://cloud.google.com/storage/pricing?hl=zh-cn) \n- [Pub/Sub](https://cloud.google.com/pubsub/pricing?hl=zh-cn) \n- [Vertex AI](https://cloud.google.com/vertex-ai/pricing?hl=zh-cn) \n\u60a8\u53ef\u4f7f\u7528 [\u50f9\u683c\u8a08\u7b97\u5668](https://cloud.google.com/products/calculator?hl=zh-cn) \u6839\u64da\u60a8\u7684\u9810\u8a08\u4f7f\u7528\u60c5\u6cc1\u4f86\u4f30\u7b97\u8cbb\u7528\u3002 \nTitles in dynamic includes are not used anywhere, and we should avoid paying to translate them\u5b8c\u6210\u672c\u6587\u6a94\u4e2d\u63cf\u8ff0\u7684\u4efb\u52d9\u5f8c\uff0c\u60a8\u53ef\u4ee5\u901a\u904e\u522a\u9664\u6240\u5275\u5efa\u7684\u8cc7\u6e90\u4f86\u907f\u514d\u7e7c\u7e8c\u8a08\u8cbb\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u6e05\u7406](#clean-up) \u3002## \u6e96\u5099\u5de5\u4f5c\n\u5728\u5177\u6709\u81f3\u5c11 5\u00a0GB \u53ef\u7528\u78c1\u76e4\u7a7a\u9593\u7684\u6a5f\u5668\u4e0a\u904b\u884c\u672c\u6559\u7a0b\u4ee5\u5b89\u88dd\u4f9d\u8cf4\u9805\u3002\n- Titles in dynamic includes are not used anywhere, and we should avoid paying to translate them\n- \u5411\u60a8\u7684 Compute Engine \u9ed8\u8a8d\u670d\u52d9\u8cec\u865f\u6388\u4e88\u89d2\u8272\u3002\u5c0d\u4ee5\u4e0b\u6bcf\u500b IAM \u89d2\u8272\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\u4e00\u6b21\uff1a- `roles/dataflow.admin`\n- `roles/dataflow.worker`\n- `roles/storage.admin`\n- `roles/pubsub.editor`\n- `roles/aiplatform.user`\n```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com\" --role=SERVICE_ACCOUNT_ROLE\n```\u66ff\u63db\u4ee5\u4e0b\u5167\u5bb9\uff1a- ``\uff1a\u60a8\u7684\u9805\u76ee ID\u3002\n- ``\uff1a\u60a8\u7684\u9805\u76ee\u7de8\u865f\u3002   \u5982\u9700\u67e5\u627e\u60a8\u7684\u9805\u76ee\u7de8\u865f\uff0c\u8acb\u4f7f\u7528 [gcloud projects describe \u547d\u4ee4](https://cloud.google.com/sdk/gcloud/reference/projects/describe?hl=zh-cn) \u3002\n- ``\uff1a\u6bcf\u500b\u89d2\u8272\u3002\n- \u8907\u88fd Google Cloud \u9805\u76ee ID\u3002\u60a8\u5728\u672c\u6559\u7a0b\u7684\u5f8c\u9762\u90e8\u5206\u9700\u8981\u7528\u5230\u6b64\u503c\u3002\n## \u5275\u5efa Google Cloud \u8cc7\u6e90\u672c\u90e8\u5206\u4ecb\u7d39\u5982\u4f55\u5275\u5efa\u4ee5\u4e0b\u8cc7\u6e90\uff1a- \u7528\u4f5c\u81e8\u6642\u5b58\u5132\u4f4d\u7f6e\u7684 Cloud Storage \u5b58\u5132\u6876\n- \u7528\u65bc\u6a21\u578b\u63d0\u793a\u7684 Pub/Sub \u4e3b\u984c\n- \u7528\u65bc\u6a21\u578b\u97ff\u61c9\u7684 Pub/Sub \u4e3b\u984c\u548c\u8a02\u95b1\n### \u5275\u5efa Cloud Storage \u5b58\u5132\u6876\u4f7f\u7528 gcloud CLI \u5275\u5efa Cloud Storage \u5b58\u5132\u6876\u3002\u6b64\u5b58\u5132\u6876\u7531 Dataflow \u6d41\u6c34\u7dda\u7528\u4f5c\u81e8\u6642\u5b58\u5132\u4f4d\u7f6e\u3002\n\u5982\u9700\u5275\u5efa\u5b58\u5132\u6876\uff0c\u8acb\u4f7f\u7528 [gcloud storage buckets create \u547d\u4ee4](https://cloud.google.com/sdk/gcloud/reference/storage/buckets/create?hl=zh-cn) \uff1a\n```\ngcloud storage buckets create gs://BUCKET_NAME --location=LOCATION\n```\n\u66ff\u63db\u4ee5\u4e0b\u5167\u5bb9\uff1a- \uff1a\u7b26\u5408 [\u5b58\u5132\u6876\u547d\u540d\u8981\u6c42](https://cloud.google.com/storage/docs/buckets?hl=zh-cn#naming) \u7684 Cloud Storage \u5b58\u5132\u6876\u7684\u540d\u7a31\u3002Cloud Storage \u5b58\u5132\u6876\u540d\u7a31\u5fc5\u9808\u662f\u5168\u5c40\u552f\u4e00\u7684\u3002\n- \uff1a\u5b58\u5132\u6876\u7684 [\u4f4d\u7f6e](https://cloud.google.com/storage/docs/locations?hl=zh-cn#available-locations) \u3002\n\u8907\u88fd\u5b58\u5132\u6876\u540d\u7a31\u3002\u60a8\u5728\u672c\u6559\u7a0b\u7684\u5f8c\u9762\u90e8\u5206\u9700\u8981\u7528\u5230\u6b64\u503c\u3002\n### \u5275\u5efa Pub/Sub \u4e3b\u984c\u548c\u8a02\u95b1\u5275\u5efa\u5169\u500b Pub/Sub \u4e3b\u984c\u548c\u4e00\u500b\u8a02\u95b1\u3002\u4e00\u500b\u4e3b\u984c\u7528\u65bc\u767c\u9001\u5230\u6a21\u578b\u7684\u8f38\u5165\u63d0\u793a\u3002\u53e6\u4e00\u500b\u4e3b\u984c\u53ca\u5176\u9644\u52a0\u8a02\u95b1\u7528\u65bc\u6a21\u578b\u7684\u97ff\u61c9\u3002- \u5982\u9700\u5275\u5efa\u4e3b\u984c\uff0c\u8acb\u904b\u884c [gcloud pubsub topics create \u547d\u4ee4](https://cloud.google.com/sdk/gcloud/reference/pubsub/topics/create?hl=zh-cn) \u5169\u6b21\uff0c\u91dd\u5c0d\u6bcf\u500b\u4e3b\u984c\u5404\u4e00\u6b21\uff1a```\ngcloud pubsub topics create PROMPTS_TOPIC_IDgcloud pubsub topics create RESPONSES_TOPIC_ID\n```\u66ff\u63db\u4ee5\u4e0b\u5167\u5bb9\uff1a- \uff1a\u8981\u767c\u9001\u5230\u6a21\u578b\u7684\u8f38\u5165\u63d0\u793a\u7684\u4e3b\u984c ID\uff0c\u4f8b\u5982`prompts`\n- \uff1a\u6a21\u578b\u97ff\u61c9\u7684\u4e3b\u984c ID\uff0c\u4f8b\u5982`responses`\n- \u5982\u9700\u5275\u5efa\u8a02\u95b1\u4e26\u5c07\u5176\u9644\u52a0\u5230\u97ff\u61c9\u4e3b\u984c\uff0c\u8acb\u4f7f\u7528 [gcloud pubsub subscriptions create \u547d\u4ee4](https://cloud.google.com/sdk/gcloud/reference/pubsub/subscriptions/create?hl=zh-cn) \uff1a```\ngcloud pubsub subscriptions create RESPONSES_SUBSCRIPTION_ID --topic=RESPONSES_TOPIC_ID\n```\u5c07 \u66ff\u63db\u7232\u6a21\u578b\u97ff\u61c9\u7684\u8a02\u95b1 ID\uff0c\u4f8b\u5982 `responses-subscription` \u3002\n\u8907\u88fd\u4e3b\u984c ID \u548c\u8a02\u95b1 ID\u3002\u60a8\u5728\u672c\u6559\u7a0b\u7684\u5f8c\u9762\u90e8\u5206\u9700\u8981\u7528\u5230\u9019\u4e9b\u503c\u3002## \u6e96\u5099\u74b0\u5883\u4e0b\u8f09\u4ee3\u78bc\u793a\u4f8b\uff0c\u7136\u5f8c\u8a2d\u7f6e\u60a8\u7684\u74b0\u5883\u4ee5\u904b\u884c\u6559\u7a0b\u3002\n [python-docs-samples GitHub \u4ee3\u78bc\u5eab](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/dataflow/run-inference) \u4e2d\u7684\u4ee3\u78bc\u793a\u4f8b\u63d0\u4f9b\u4e86\u904b\u884c\u6b64\u6d41\u6c34\u7dda\u6240\u9700\u7684\u4ee3\u78bc\u3002\u7576\u60a8\u6e96\u5099\u597d\u69cb\u5efa\u81ea\u5df1\u7684\u6d41\u6c34\u7dda\u6642\uff0c\u53ef\u4ee5\u4f7f\u7528\u6b64\u793a\u4f8b\u4ee3\u78bc\u4f5c\u7232\u6a21\u677f\u3002\n\u60a8\u53ef\u4ee5\u4f7f\u7528 [venv](https://docs.python.org/3/library/venv.html) \u5275\u5efa\u9694\u96e2\u7684 Python \u865b\u64ec\u74b0\u5883\u4f86\u904b\u884c\u6d41\u6c34\u7dda\u9805\u76ee\u3002\u85c9\u52a9\u865b\u64ec\u74b0\u5883\uff0c\u60a8\u53ef\u4ee5\u5c07\u4e00\u500b\u9805\u76ee\u7684\u4f9d\u8cf4\u9805\u8207\u5176\u4ed6\u9805\u76ee\u7684\u4f9d\u8cf4\u9805\u9694\u96e2\u958b\u4f86\u3002\u5982\u9700\u8a73\u7d30\u77ad\u89e3\u5982\u4f55\u5b89\u88dd Python \u4e26\u5275\u5efa\u865b\u64ec\u74b0\u5883\uff0c\u8acb\u53c3\u95b1 [\u8a2d\u7f6e Python \u958b\u767c\u74b0\u5883](https://cloud.google.com/python/docs/setup?hl=zh-cn) \u3002- \u4f7f\u7528 `git clone` \u547d\u4ee4\u514b\u9686 GitHub \u4ee3\u78bc\u5eab\uff1a```\ngit clone https://github.com/GoogleCloudPlatform/python-docs-samples.git\n```\n- \u5c0e\u822a\u5230 `run-inference` \u76ee\u9304\uff1a```\ncd python-docs-samples/dataflow/run-inference\n```\n- \u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f\u547d\u4ee4\u63d0\u793a\u7b26\uff0c\u8acb\u6aa2\u67e5\u7cfb\u7d71\u4e2d\u662f\u5426\u5df2\u904b\u884c Python 3 \u548c `pip` \uff1a```\npython --versionpython -m pip --version\n```\u5982\u679c\u9700\u8981\uff0c\u8acb [\u5b89\u88dd Python 3](https://cloud.google.com/python/docs/setup?hl=zh-cn#installing_python) \u3002\u5982\u679c\u60a8\u4f7f\u7528\u7684\u662f Cloud Shell\uff0c\u5247\u53ef\u4ee5\u8df3\u904e\u6b64\u6b65\u9a5f\uff0c\u56e0\u7232 Cloud Shell \u5df2\u7d93\u5b89\u88dd\u4e86 Python\u3002\n- \u5275\u5efa [Python \u865b\u64ec\u74b0\u5883](https://cloud.google.com/python/docs/setup?hl=zh-cn#installing_and_using_virtualenv) \uff1a```\npython -m venv /tmp/envsource /tmp/env/bin/activate\n```\n- \u5b89\u88dd\u4f9d\u8cf4\u9805\uff1a```\npip install -r requirements.txt --no-cache-dir\n```\n### \u6a21\u578b\u52a0\u8f09\u4ee3\u78bc\u793a\u4f8b\u672c\u6559\u7a0b\u4e2d\u7684\u6a21\u578b\u52a0\u8f09\u4ee3\u78bc\u6703\u5553\u52d5 Vertex AI \u81ea\u5b9a\u7fa9\u4f5c\u696d\uff0c\u4ee5\u5c07\u6a21\u578b\u7684 `state_dict` \u5c0d\u8c61\u52a0\u8f09\u5230 Cloud Storage \u4e2d\u3002\n\u8d77\u59cb\u6587\u4ef6\u5982\u4e0b\u6240\u793a\uff1a\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataflow/run-inference/download_model.py) \n```\n# Copyright 2023 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 \u00a0http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.\"\"\"Loads the state_dict for an LLM model into Cloud Storage.\"\"\"from __future__ import annotationsimport osimport torchfrom transformers import AutoModelForSeq2SeqLMdef run_local(model_name: str, state_dict_path: str) -> None:\u00a0 \u00a0 \"\"\"Loads the state dict and saves it into the desired path.\u00a0 \u00a0 If the `state_dict_path` is a Cloud Storage location starting\u00a0 \u00a0 with \"gs://\", this assumes Cloud Storage is mounted with\u00a0 \u00a0 Cloud Storage FUSE in `/gcs`. Vertex AI is set up like this.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 model_name: HuggingFace model name compatible with AutoModelForSeq2SeqLM.\u00a0 \u00a0 \u00a0 \u00a0 state_dict_path: File path to the model's state_dict, can be in Cloud Storage.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 print(f\"Loading model: {model_name}\")\u00a0 \u00a0 model = AutoModelForSeq2SeqLM.from_pretrained(\u00a0 \u00a0 \u00a0 \u00a0 model_name, torch_dtype=torch.bfloat16\u00a0 \u00a0 )\u00a0 \u00a0 print(f\"Model loaded, saving state dict to: {state_dict_path}\")\u00a0 \u00a0 # Assume Cloud Storage FUSE is mounted in `/gcs`.\u00a0 \u00a0 state_dict_path = state_dict_path.replace(\"gs://\", \"/gcs/\")\u00a0 \u00a0 directory = os.path.dirname(state_dict_path)\u00a0 \u00a0 if directory and not os.path.exists(directory):\u00a0 \u00a0 \u00a0 \u00a0 os.makedirs(os.path.dirname(state_dict_path), exist_ok=True)\u00a0 \u00a0 torch.save(model.state_dict(), state_dict_path)\u00a0 \u00a0 print(\"State dict saved successfully!\")def run_vertex_job(\u00a0 \u00a0 model_name: str,\u00a0 \u00a0 state_dict_path: str,\u00a0 \u00a0 job_name: str,\u00a0 \u00a0 project: str,\u00a0 \u00a0 bucket: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 machine_type: str = \"e2-highmem-2\",\u00a0 \u00a0 disk_size_gb: int = 100,) -> None:\u00a0 \u00a0 \"\"\"Launches a Vertex AI custom job to load the state dict.\u00a0 \u00a0 If the model is too large to fit into memory or disk, we can launch\u00a0 \u00a0 a Vertex AI custom job with a large enough VM for this to work.\u00a0 \u00a0 Depending on the model's size, it might require a different VM\u00a0 \u00a0 configuration. The model MUST fit into the VM's memory, and there\u00a0 \u00a0 must be enough disk space to stage the entire model while it gets\u00a0 \u00a0 copied to Cloud Storage.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 model_name: HuggingFace model name compatible with AutoModelForSeq2SeqLM.\u00a0 \u00a0 \u00a0 \u00a0 state_dict_path: File path to the model's state_dict, can be in Cloud Storage.\u00a0 \u00a0 \u00a0 \u00a0 job_name: Job display name in the Vertex AI console.\u00a0 \u00a0 \u00a0 \u00a0 project: Google Cloud Project ID.\u00a0 \u00a0 \u00a0 \u00a0 bucket: Cloud Storage bucket name, without the \"gs://\" prefix.\u00a0 \u00a0 \u00a0 \u00a0 location: Google Cloud regional location.\u00a0 \u00a0 \u00a0 \u00a0 machine_type: Machine type for the VM to run the job.\u00a0 \u00a0 \u00a0 \u00a0 disk_size_gb: Disk size in GB for the VM to run the job.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.init(project=project, staging_bucket=bucket, location=location)\u00a0 \u00a0 job = aiplatform.CustomJob.from_local_script(\u00a0 \u00a0 \u00a0 \u00a0 display_name=job_name,\u00a0 \u00a0 \u00a0 \u00a0 container_uri=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-13:latest\",\u00a0 \u00a0 \u00a0 \u00a0 script_path=\"download_model.py\",\u00a0 \u00a0 \u00a0 \u00a0 args=[\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"local\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f\"--model-name={model_name}\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f\"--state-dict-path={state_dict_path}\",\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 machine_type=machine_type,\u00a0 \u00a0 \u00a0 \u00a0 boot_disk_size_gb=disk_size_gb,\u00a0 \u00a0 \u00a0 \u00a0 requirements=[\"transformers\"],\u00a0 \u00a0 )\u00a0 \u00a0 job.run()if __name__ == \"__main__\":\u00a0 \u00a0 import argparse\u00a0 \u00a0 parser = argparse.ArgumentParser()\u00a0 \u00a0 subparsers = parser.add_subparsers(required=True)\u00a0 \u00a0 parser_local = subparsers.add_parser(\"local\")\u00a0 \u00a0 parser_local.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--model-name\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"HuggingFace model name compatible with AutoModelForSeq2SeqLM\",\u00a0 \u00a0 )\u00a0 \u00a0 parser_local.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--state-dict-path\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"File path to the model's state_dict, can be in Cloud Storage\",\u00a0 \u00a0 )\u00a0 \u00a0 parser_local.set_defaults(run=run_local)\u00a0 \u00a0 parser_vertex = subparsers.add_parser(\"vertex\")\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--model-name\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"HuggingFace model name compatible with AutoModelForSeq2SeqLM\",\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--state-dict-path\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"File path to the model's state_dict, can be in Cloud Storage\",\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--job-name\", required=True, help=\"Job display name in the Vertex AI console\"\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--project\", required=True, help=\"Google Cloud Project ID\"\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--bucket\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help='Cloud Storage bucket name, without the \"gs://\" prefix',\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--location\", default=\"us-central1\", help=\"Google Cloud regional location\"\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--machine-type\",\u00a0 \u00a0 \u00a0 \u00a0 default=\"e2-highmem-2\",\u00a0 \u00a0 \u00a0 \u00a0 help=\"Machine type for the VM to run the job\",\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--disk-size-gb\",\u00a0 \u00a0 \u00a0 \u00a0 type=int,\u00a0 \u00a0 \u00a0 \u00a0 default=100,\u00a0 \u00a0 \u00a0 \u00a0 help=\"Disk size in GB for the VM to run the job\",\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.set_defaults(run=run_vertex_job)\u00a0 \u00a0 args = parser.parse_args()\u00a0 \u00a0 kwargs = args.__dict__.copy()\u00a0 \u00a0 kwargs.pop(\"run\")\u00a0 \u00a0 args.run(**kwargs)\n```\n### \u6d41\u6c34\u7dda\u4ee3\u78bc\u793a\u4f8b\u672c\u6559\u7a0b\u4e2d\u7684\u6d41\u6c34\u7dda\u4ee3\u78bc\u6703\u90e8\u7f72\u57f7\u884c\u4ee5\u4e0b\u64cd\u4f5c\u7684 Dataflow \u6d41\u6c34\u7dda\uff1a- \u5f9e Pub/Sub \u8b80\u53d6\u63d0\u793a\u4e26\u5c07\u6587\u672c\u7de8\u78bc\u7232\u8a5e\u5143\u5f35\u91cf\u3002\n- \u904b\u884c`RunInference`\u8f49\u63db\u3002\n- \u5c07\u8f38\u51fa\u8a5e\u5143\u5f35\u91cf\u89e3\u78bc\u7232\u6587\u672c\u4e26\u5c07\u97ff\u61c9\u5beb\u5165 Pub/Sub\u3002\n\u8d77\u59cb\u6587\u4ef6\u5982\u4e0b\u6240\u793a\uff1a\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataflow/run-inference/main.py) \n```\n# Copyright 2023 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 \u00a0http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.\"\"\"Runs a streaming RunInference Language Model pipeline.\"\"\"from __future__ import annotationsimport loggingimport apache_beam as beamfrom apache_beam.ml.inference.base import PredictionResultfrom apache_beam.ml.inference.base import RunInferencefrom apache_beam.ml.inference.pytorch_inference import make_tensor_model_fnfrom apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensorfrom apache_beam.options.pipeline_options import PipelineOptionsimport torchfrom transformers import AutoConfigfrom transformers import AutoModelForSeq2SeqLMfrom transformers import AutoTokenizerfrom transformers.tokenization_utils import PreTrainedTokenizerMAX_RESPONSE_TOKENS = 256def to_tensors(input_text: str, tokenizer: PreTrainedTokenizer) -> torch.Tensor:\u00a0 \u00a0 \"\"\"Encodes input text into token tensors.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 input_text: Input text for the language model.\u00a0 \u00a0 \u00a0 \u00a0 tokenizer: Tokenizer for the language model.\u00a0 \u00a0 Returns: Tokenized input tokens.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 return tokenizer(input_text, return_tensors=\"pt\").input_ids[0]def decode_response(result: PredictionResult, tokenizer: PreTrainedTokenizer) -> str:\u00a0 \u00a0 \"\"\"Decodes output token tensors into text.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 result: Prediction results from the RunInference transform.\u00a0 \u00a0 \u00a0 \u00a0 tokenizer: Tokenizer for the language model.\u00a0 \u00a0 Returns: The model's response as text.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 output_tokens = result.inference\u00a0 \u00a0 return tokenizer.decode(output_tokens, skip_special_tokens=True)class AskModel(beam.PTransform):\u00a0 \u00a0 \"\"\"Asks an language model a prompt message and gets its responses.\u00a0 \u00a0 Attributes:\u00a0 \u00a0 \u00a0 \u00a0 model_name: HuggingFace model name compatible with AutoModelForSeq2SeqLM.\u00a0 \u00a0 \u00a0 \u00a0 state_dict_path: File path to the model's state_dict, can be in Cloud Storage.\u00a0 \u00a0 \u00a0 \u00a0 max_response_tokens: Maximum number of tokens for the model to generate.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 def __init__(\u00a0 \u00a0 \u00a0 \u00a0 self,\u00a0 \u00a0 \u00a0 \u00a0 model_name: str,\u00a0 \u00a0 \u00a0 \u00a0 state_dict_path: str,\u00a0 \u00a0 \u00a0 \u00a0 max_response_tokens: int = MAX_RESPONSE_TOKENS,\u00a0 \u00a0 ) -> None:\u00a0 \u00a0 \u00a0 \u00a0 self.model_handler = PytorchModelHandlerTensor(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 state_dict_path=state_dict_path,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_class=AutoModelForSeq2SeqLM.from_config,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_params={\"config\": AutoConfig.from_pretrained(model_name)},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 inference_fn=make_tensor_model_fn(\"generate\"),\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 self.tokenizer = AutoTokenizer.from_pretrained(model_name)\u00a0 \u00a0 \u00a0 \u00a0 self.max_response_tokens = max_response_tokens\u00a0 \u00a0 def expand(self, pcollection: beam.PCollection[str]) -> beam.PCollection[str]:\u00a0 \u00a0 \u00a0 \u00a0 return (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pcollection\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"To tensors\" >> beam.Map(to_tensors, self.tokenizer)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"RunInference\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 >> RunInference(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 self.model_handler,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 inference_args={\"max_new_tokens\": self.max_response_tokens},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"Get response\" >> beam.Map(decode_response, self.tokenizer)\u00a0 \u00a0 \u00a0 \u00a0 )if __name__ == \"__main__\":\u00a0 \u00a0 import argparse\u00a0 \u00a0 parser = argparse.ArgumentParser()\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--messages-topic\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"Pub/Sub topic for input text messages\",\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--responses-topic\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"Pub/Sub topic for output text responses\",\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--model-name\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"HuggingFace model name compatible with AutoModelForSeq2SeqLM\",\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--state-dict-path\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"File path to the model's state_dict, can be in Cloud Storage\",\u00a0 \u00a0 )\u00a0 \u00a0 args, beam_args = parser.parse_known_args()\u00a0 \u00a0 logging.getLogger().setLevel(logging.INFO)\u00a0 \u00a0 beam_options = PipelineOptions(\u00a0 \u00a0 \u00a0 \u00a0 beam_args,\u00a0 \u00a0 \u00a0 \u00a0 pickle_library=\"cloudpickle\",\u00a0 \u00a0 \u00a0 \u00a0 streaming=True,\u00a0 \u00a0 )\u00a0 \u00a0 simple_name = args.model_name.split(\"/\")[-1]\u00a0 \u00a0 pipeline = beam.Pipeline(options=beam_options)\u00a0 \u00a0 _ = (\u00a0 \u00a0 \u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 | \"Read from Pub/Sub\" >> beam.io.ReadFromPubSub(args.messages_topic)\u00a0 \u00a0 \u00a0 \u00a0 | \"Decode bytes\" >> beam.Map(lambda msg: msg.decode(\"utf-8\"))\u00a0 \u00a0 \u00a0 \u00a0 | f\"Ask {simple_name}\" >> AskModel(args.model_name, args.state_dict_path)\u00a0 \u00a0 \u00a0 \u00a0 | \"Encode bytes\" >> beam.Map(lambda msg: msg.encode(\"utf-8\"))\u00a0 \u00a0 \u00a0 \u00a0 | \"Write to Pub/Sub\" >> beam.io.WriteToPubSub(args.responses_topic)\u00a0 \u00a0 )\u00a0 \u00a0 pipeline.run()\n```## \u52a0\u8f09\u6a21\u578bLLM \u53ef\u80fd\u662f\u975e\u5e38\u5927\u7684\u6a21\u578b\u3002\u4f7f\u7528\u66f4\u591a\u53c3\u6578\u8a13\u7df4\u7684\u8f03\u5927\u7684\u6a21\u578b\u901a\u5e38\u53ef\u63d0\u4f9b\u66f4\u597d\u7684\u7d50\u679c\u3002\u4f46\u662f\uff0c\u8f03\u5927\u7684\u6a21\u578b\u9700\u8981\u66f4\u5927\u7684\u6a5f\u5668\u548c\u66f4\u591a\u5167\u5b58\u624d\u80fd\u904b\u884c\u3002\u8f03\u5927\u7684\u6a21\u578b\u5728 CPU \u4e0a\u904b\u884c\u7684\u901f\u5ea6\u53ef\u80fd\u4e5f\u8f03\u6162\u3002\n\u5728 Dataflow \u4e0a\u904b\u884c PyTorch \u6a21\u578b\u4e4b\u524d\uff0c\u60a8\u9700\u8981\u52a0\u8f09\u6a21\u578b\u7684 `state_dict` \u5c0d\u8c61\u3002\u6a21\u578b\u7684 [state_dict \u5c0d\u8c61](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html) \u7528\u65bc\u5b58\u5132\u6a21\u578b\u7684\u6b0a\u91cd\u3002\n\u5728\u4f7f\u7528 Apache Beam `RunInference` \u8f49\u63db\u7684 Dataflow \u6d41\u6c34\u7dda\u4e2d\uff0c\u6a21\u578b\u7684 `state_dict` \u5c0d\u8c61\u5fc5\u9808\u52a0\u8f09\u5230 Cloud Storage \u4e2d\u3002\u7528\u65bc\u5c07 `state_dict` \u5c0d\u8c61\u52a0\u8f09\u5230 Cloud Storage \u7684\u6a5f\u5668\u5fc5\u9808\u5177\u6709\u8db3\u5920\u7684\u5167\u5b58\u4f86\u52a0\u8f09\u6a21\u578b\u3002\u8a72\u6a5f\u5668\u9084\u9700\u8981\u5177\u6709\u5feb\u901f\u4e92\u806f\u7db2\u9023\u63a5\uff0c\u4ee5\u4e0b\u8f09\u6b0a\u91cd\u4e26\u5c07\u5176\u4e0a\u50b3\u5230 Cloud Storage\u3002\n\u4e0b\u8868\u986f\u793a\u4e86\u6bcf\u500b\u6a21\u578b\u7684\u53c3\u6578\u6578\u91cf\u4ee5\u53ca\u52a0\u8f09\u6bcf\u500b\u6a21\u578b\u6240\u9700\u7684\u6700\u5c0f\u5167\u5b58\u3002\n| \u6a21\u578b     | \u53c3\u6578 | \u6240\u9700\u7684\u5167\u5b58 |\n|:---------------------|:--------|:-------------|\n| google/flan-t5-small | 8000 \u842c | >\u00a0320 MB  |\n| google/flan-t5-base | 2.5 \u5104 | >\u00a01 GB  |\n| google/flan-t5-large | 7.8 \u5104 | >\u00a03.2 GB  |\n| google/flan-t5-xl | 30 \u5104 | >\u00a012 GB  |\n| google/flan-t5-xxl | 110 \u5104 | >\u00a044 GB  |\n| google/flan-ul2  | 200 \u5104 | >\u00a080 GB  |\n\u96d6\u7136\u60a8\u53ef\u4ee5\u5728\u672c\u5730\u52a0\u8f09\u8f03\u5c0f\u7684\u6a21\u578b\uff0c\u4f46\u672c\u6559\u7a0b\u4ecb\u7d39\u77ad\u5982\u4f55\u5553\u52d5 Vertex AI \u81ea\u5b9a\u7fa9\u4f5c\u696d\uff0c\u4ee5\u52a0\u8f09\u5177\u6709\u9069\u7576\u5927\u5c0f\u7684\u865b\u64ec\u6a5f\u7684\u6a21\u578b\u3002\n\u7531\u65bc LLM \u53ef\u80fd\u975e\u5e38\u5927\uff0c\u56e0\u6b64\u672c\u6559\u7a0b\u4e2d\u7684\u793a\u4f8b\u5c07 `state_dict` \u5c0d\u8c61\u4fdd\u5b58\u7232 `float16` \u683c\u5f0f\uff0c\u800c\u4e0d\u662f\u9ed8\u8a8d `float32` \u683c\u5f0f\u3002\u4f7f\u7528\u9019\u7a2e\u914d\u7f6e\u6642\uff0c\u6bcf\u500b\u53c3\u6578\u4f7f\u7528 16 \u4f4d\u800c\u4e0d\u662f 32 \u4f4d\uff0c\u5f9e\u800c\u4f7f `state_dict` \u5c0d\u8c61\u7684\u5927\u5c0f\u6e1b\u534a\u3002\u5927\u5c0f\u8f03\u5c0f\u53ef\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u7e2e\u77ed\u52a0\u8f09\u6a21\u578b\u6240\u9700\u7684\u6642\u9593\u3002\u4f46\u662f\uff0c\u8f49\u63db\u683c\u5f0f\u610f\u5473\u7740\u865b\u64ec\u6a5f\u5fc5\u9808\u5c07\u6a21\u578b\u548c `state_dict` \u5c0d\u8c61\u653e\u5165\u5167\u5b58\u4e2d\u3002\n\u4e0b\u8868\u986f\u793a\u4e86\u5728\u5c07 `state_dict` \u5c0d\u8c61\u4fdd\u5b58\u7232 `float16` \u683c\u5f0f\u5f8c\u52a0\u8f09\u6a21\u578b\u7684\u6700\u4f4e\u8981\u6c42\u3002\u8a72\u8868\u9084\u986f\u793a\u4e86\u4f7f\u7528 Vertex AI \u52a0\u8f09\u6a21\u578b\u7684\u5efa\u8b70\u6a5f\u5668\u985e\u578b\u3002Vertex AI \u7684\u6700\u5c0f\uff08\u9ed8\u8a8d\uff09\u78c1\u76e4\u5927\u5c0f\u7232 100 GB\uff0c\u4f46\u67d0\u4e9b\u6a21\u578b\u53ef\u80fd\u9700\u8981\u66f4\u5927\u7684\u78c1\u76e4\u3002\n| \u6a21\u578b\u540d\u7a31    | \u6240\u9700\u7684\u5167\u5b58 | \u6a5f\u5668\u985e\u578b  | \u865b\u64ec\u6a5f\u5167\u5b58 | \u865b\u64ec\u6a5f\u78c1\u76e4 |\n|:---------------------|:-------------|:--------------|:-------------|:-------------|\n| google/flan-t5-small | > 480\u00a0MB  | e2-standard-4 | 16\u00a0GB  | 100\u00a0GB  |\n| google/flan-t5-base | > 1.5\u00a0GB  | e2-standard-4 | 16\u00a0GB  | 100\u00a0GB  |\n| google/flan-t5-large | > 4.8\u00a0GB  | e2-standard-4 | 16\u00a0GB  | 100\u00a0GB  |\n| google/flan-t5-xl | > 18\u00a0GB  | e2-highmem-4 | 32\u00a0GB  | 100\u00a0GB  |\n| google/flan-t5-xxl | > 66\u00a0GB  | e2-highmem-16 | 128\u00a0GB  | 100\u00a0GB  |\n| google/flan-ul2  | > 120\u00a0GB  | e2-highmem-16 | 128\u00a0GB  | 150\u00a0GB  |\n\u4f7f\u7528 Vertex AI \u81ea\u5b9a\u7fa9\u4f5c\u696d\u5c07\u6a21\u578b\u7684 `state_dict` \u5c0d\u8c61\u52a0\u8f09\u5230 Cloud Storage \u4e2d\uff1a\n```\npython download_model.py vertex \\\u00a0 \u00a0 --model-name=\"MODEL_NAME\" \\\u00a0 \u00a0 --state-dict-path=\"gs://BUCKET_NAME/run-inference/MODEL_NAME.pt\" \\\u00a0 \u00a0 --job-name=\"Load MODEL_NAME\" \\\u00a0 \u00a0 --project=\"PROJECT_ID\" \\\u00a0 \u00a0 --bucket=\"BUCKET_NAME\" \\\u00a0 \u00a0 --location=\"LOCATION\" \\\u00a0 \u00a0 --machine-type=\"VERTEX_AI_MACHINE_TYPE\" \\\u00a0 \u00a0 --disk-size-gb=\"DISK_SIZE_GB\"\n```\n\u66ff\u63db\u4ee5\u4e0b\u5167\u5bb9\uff1a- \uff1a\u6a21\u578b\u7684\u540d\u7a31\uff0c\u4f8b\u5982`google/flan-t5-xl`\u3002\n- \uff1a\u8981\u5728\u5176\u4e2d\u904b\u884c Vertex AI \u81ea\u5b9a\u7fa9\u4f5c\u696d\u7684\u6a5f\u5668\u985e\u578b\uff0c\u4f8b\u5982`e2-highmem-4`\u3002\n- \uff1a\u865b\u64ec\u6a5f\u7684\u78c1\u76e4\u5927\u5c0f\uff08\u4ee5 GB \u7232\u55ae\u4f4d\uff09\u3002\u6700\u5c0f\u5927\u5c0f\u7232 100\u00a0GB\u3002\n\u52a0\u8f09\u6a21\u578b\u53ef\u80fd\u9700\u8981\u5e7e\u5206\u9418\u7684\u6642\u9593\uff0c\u5177\u9ad4\u53d6\u6c7a\u65bc\u6a21\u578b\u7684\u5927\u5c0f\u3002\u5982\u9700\u67e5\u770b\u72c0\u614b\uff0c\u8acb\u9032\u5165 Vertex AI **\u81ea\u5b9a\u7fa9\u4f5c\u696d** \u9801\u9762\u3002\n [\u6253\u958b\u201c\u81ea\u5b9a\u7fa9\u4f5c\u696d\u201d](https://console.cloud.google.com/vertex-ai/training/custom-jobs?hl=zh-cn) ## \u904b\u884c\u6d41\u6c34\u7dda\u5728\u52a0\u8f09\u6a21\u578b\u5f8c\uff0c\u60a8\u53ef\u4ee5\u904b\u884c Dataflow \u6d41\u6c34\u7dda\u3002\u7232\u904b\u884c\u8a72\u6d41\u6c34\u7dda\uff0c\u6bcf\u500b\u5de5\u4f5c\u5668\u4f7f\u7528\u7684\u6a21\u578b\u548c\u5167\u5b58\u90fd\u5fc5\u9808\u653e\u5165\u5167\u5b58\u4e2d\u3002\n\u4e0b\u8868\u986f\u793a\u4e86\u904b\u884c\u63a8\u7406\u6d41\u6c34\u7dda\u7684\u63a8\u85a6\u6a5f\u5668\u985e\u578b\u3002\n| \u6a21\u578b\u540d\u7a31    | \u6a5f\u5668\u985e\u578b  | \u865b\u64ec\u6a5f\u5167\u5b58 |\n|:---------------------|:--------------|:-------------|\n| google/flan-t5-small | n2-highmem-2 | 16\u00a0GB  |\n| google/flan-t5-base | n2-highmem-2 | 16\u00a0GB  |\n| google/flan-t5-large | n2-highmem-4 | 32\u00a0GB  |\n| google/flan-t5-xl | n2-highmem-4 | 32\u00a0GB  |\n| google/flan-t5-xxl | n2-highmem-8 | 64\u00a0GB  |\n| google/flan-ul2  | n2-highmem-16 | 128\u00a0GB  |\n\u904b\u884c\u6d41\u6c34\u7dda\uff1a\n```\npython main.py \\\u00a0 \u00a0 --messages-topic=\"projects/PROJECT_ID/topics/PROMPTS_TOPIC_ID\" \\\u00a0 \u00a0 --responses-topic=\"projects/PROJECT_ID/topics/RESPONSES_TOPIC_ID\" \\\u00a0 \u00a0 --model-name=\"MODEL_NAME\" \\\u00a0 \u00a0 --state-dict-path=\"gs://BUCKET_NAME/run-inference/MODEL_NAME.pt\" \\\u00a0 \u00a0 --runner=\"DataflowRunner\" \\\u00a0 \u00a0 --project=\"PROJECT_ID\" \\\u00a0 \u00a0 --temp_location=\"gs://BUCKET_NAME/temp\" \\\u00a0 \u00a0 --region=\"REGION\" \\\u00a0 \u00a0 --machine_type=\"DATAFLOW_MACHINE_TYPE\" \\\u00a0 \u00a0 --requirements_file=\"requirements.txt\" \\\u00a0 \u00a0 --requirements_cache=\"skip\" \\\u00a0 \u00a0 --experiments=\"use_sibling_sdk_workers\" \\\u00a0 \u00a0 --experiments=\"no_use_multiple_sdk_containers\"\n```\n\u66ff\u63db\u4ee5\u4e0b\u5167\u5bb9\uff1a- \uff1a\u9805\u76ee ID\n- \uff1a\u8981\u767c\u9001\u5230\u6a21\u578b\u7684\u8f38\u5165\u63d0\u793a\u7684\u4e3b\u984c ID\n- \uff1a\u6a21\u578b\u97ff\u61c9\u7684\u4e3b\u984c ID\n- \uff1a\u6a21\u578b\u7684\u540d\u7a31\uff0c\u4f8b\u5982`google/flan-t5-xl`\n- \uff1a\u5b58\u5132\u6876\u7684\u540d\u7a31\n- \uff1a\u8981\u5728\u5176\u4e2d\u90e8\u7f72\u4f5c\u696d\u7684\u5340\u57df\uff0c\u4f8b\u5982`us-central1`\n- \uff1a\u8981\u904b\u884c\u6d41\u6c34\u7dda\u7684\u865b\u64ec\u6a5f\uff0c\u4f8b\u5982`n2-highmem-4`\n\u5982\u9700\u78ba\u4fdd\u6a21\u578b\u50c5\u7232\u6bcf\u500b\u5de5\u4f5c\u5668\u52a0\u8f09\u4e00\u6b21\uff0c\u4e26\u4e14\u4e0d\u6703\u8017\u76e1\u5167\u5b58\uff0c\u8acb\u901a\u904e\u8a2d\u7f6e\u6d41\u6c34\u7dda\u9078\u9805 `--experiments=no_use_multiple_sdk_containers` \u5c07\u5de5\u4f5c\u5668\u914d\u7f6e\u7232\u4f7f\u7528\u55ae\u500b\u9032\u7a0b\u3002\u60a8\u7121\u9700\u9650\u5236\u7dda\u7a0b\u6578\uff0c\u56e0\u7232 `RunInference` \u8f49\u63db\u8207\u591a\u500b\u7dda\u7a0b\u5171\u4eab\u540c\u4e00\u6a21\u578b\u3002\n\u6b64\u793a\u4f8b\u4e2d\u7684\u6d41\u6c34\u7dda\u4f7f\u7528 CPU \u904b\u884c\u3002\u5c0d\u65bc\u8f03\u5927\u7684\u6a21\u578b\uff0c\u8655\u7406\u6bcf\u500b\u8acb\u6c42\u9700\u8981\u66f4\u591a\u6642\u9593\u3002\u5982\u679c\u60a8\u9700\u8981\u66f4\u5feb\u7684\u97ff\u61c9\uff0c\u53ef\u4ee5 [\u5553\u7528 GPU](https://cloud.google.com/dataflow/docs/concepts/gpu-support?hl=zh-cn) \u3002\n\u5982\u9700\u67e5\u770b\u6d41\u6c34\u7dda\u7684\u72c0\u614b\uff0c\u8acb\u9032\u5165 Dataflow **\u4f5c\u696d** \u9801\u9762\u3002\n [\u9032\u5165\u201c\u4f5c\u696d\u201d](https://console.cloud.google.com/dataflow/jobs?hl=zh-cn) ## \u5411\u6a21\u578b\u63d0\u554f\u6d41\u6c34\u7dda\u958b\u59cb\u904b\u884c\u5f8c\uff0c\u60a8\u53ef\u4ee5\u5411\u6a21\u578b\u63d0\u4f9b\u63d0\u793a\u4e26\u63a5\u6536\u97ff\u61c9\u3002- \u901a\u904e\u5411 Pub/Sub \u767c\u4f48\u6d88\u606f\u4f86\u767c\u9001\u63d0\u793a\u3002\u4f7f\u7528 [gcloud pubsub topics publish \u547d\u4ee4](https://cloud.google.com/sdk/gcloud/reference/pubsub/topics/publish?hl=zh-cn) \uff1a```\ngcloud pubsub topics publish PROMPTS_TOPIC_ID \\\u00a0 \u00a0 --message=\"PROMPT_TEXT\"\n```\u5c07 `` \u66ff\u63db\u7232\u5305\u542b\u60a8\u8981\u63d0\u4f9b\u7684\u63d0\u793a\u7684\u5b57\u7b26\u4e32\u3002\u4f7f\u7528\u82f1\u6587\u5f15\u865f\u5c07\u63d0\u793a\u62ec\u8d77\u4f86\u3002\u4f7f\u7528\u60a8\u81ea\u5df1\u7684\u63d0\u793a\uff0c\u6216\u5617\u8a66\u4ee5\u4e0b\u793a\u4f8b\u4e4b\u4e00\uff1a- `Translate to Spanish: My name is Luka`\n- `Complete this sentence: Once upon a time, there was a`\n- `Summarize the following text: Dataflow is a Google Cloud service that provides unified stream and batch data processing at scale. Use Dataflow to create data pipelines that read from one or more sources, transform the data, and write the data to a destination.`\n- \u5982\u9700\u7372\u5f97\u97ff\u61c9\uff0c\u8acb\u4f7f\u7528 [gcloud pubsub subscriptions pull \u547d\u4ee4](https://cloud.google.com/sdk/gcloud/reference/pubsub/subscriptions/pull?hl=zh-cn) \u3002\u6a21\u578b\u53ef\u80fd\u9700\u8981\u5e7e\u5206\u9418\u624d\u80fd\u751f\u6210\u97ff\u61c9\uff0c\u5177\u9ad4\u53d6\u6c7a\u65bc\u6a21\u578b\u7684\u5927\u5c0f\u3002\u6a21\u578b\u8d8a\u5927\uff0c\u90e8\u7f72\u548c\u751f\u6210\u97ff\u61c9\u6240\u9700\u7684\u6642\u9593\u5c31\u8d8a\u9577\u3002```\ngcloud pubsub subscriptions pull RESPONSES_SUBSCRIPTION_ID --auto-ack\n```\u5c07 `` \u66ff\u63db\u7232\u6a21\u578b\u97ff\u61c9\u7684\u8a02\u95b1 ID\u3002\n## \u6e05\u7406\u7232\u907f\u514d\u56e0\u672c\u6559\u7a0b\u4e2d\u4f7f\u7528\u7684\u8cc7\u6e90\u5c0e\u81f4\u60a8\u7684 Google Cloud \u8cec\u865f\u7522\u751f\u8cbb\u7528\uff0c\u8acb\u522a\u9664\u5305\u542b\u9019\u4e9b\u8cc7\u6e90\u7684\u9805\u76ee\uff0c\u6216\u8005\u4fdd\u7559\u9805\u76ee\u4f46\u522a\u9664\u5404\u500b\u8cc7\u6e90\u3002\n### \u522a\u9664\u9805\u76ee\n- Titles in dynamic includes are not used anywhere, and we should avoid paying to translate them\n- **\u8b66\u544a** \uff1a\u522a\u9664\u9805\u76ee\u6703\u7522\u751f\u4ee5\u4e0b\u5f71\u97ff- **\u9805\u76ee\u4e2d\u7684\u6240\u6709\u5167\u5bb9\u90fd\u6703\u88ab\u522a\u9664\u3002** \u5982\u679c\u60a8\u5c07\u73fe\u6709\u9805\u76ee\u7528\u65bc\u672c\u6587\u6a94\u4e2d\u7684\u4efb\u52d9\uff0c\u5247\u522a\u9664\u8a72\u9805\u76ee\u5f8c\uff0c\u9084\u5c07\u522a\u9664\u60a8\u5df2\u5728\u8a72\u9805\u76ee\u4e2d\u5b8c\u6210\u7684\u4efb\u4f55\u5176\u4ed6\u5de5\u4f5c\u3002\n- **\u81ea\u5b9a\u7fa9\u9805\u76ee ID \u4e1f\u5931\u3002** \u5275\u5efa\u6b64\u9805\u76ee\u6642\uff0c\u60a8\u53ef\u80fd\u5275\u5efa\u4e86\u8981\u5728\u5c07\u4f86\u4f7f\u7528\u7684\u81ea\u5b9a\u7fa9\u9805\u76ee ID\u3002\u8981\u4fdd\u7559\u4f7f\u7528\u8a72\u9805\u76ee ID \u7684\u7db2\u5740\uff08\u4f8b\u5982`appspot.com`\u7db2\u5740\uff09\uff0c\u8acb\u522a\u9664\u9805\u76ee\u5167\u7684\u6240\u9078\u8cc7\u6e90\uff0c\u800c\u4e0d\u662f\u522a\u9664\u6574\u500b\u9805\u76ee\u3002\n\u5982\u679c\u60a8\u6253\u7b97\u63a2\u7d22\u591a\u500b\u67b6\u69cb\u3001\u6559\u7a0b\u6216\u5feb\u901f\u5165\u9580\uff0c\u5247\u91cd\u8907\u4f7f\u7528\u9805\u76ee\u53ef\u4ee5\u5e6b\u52a9\u60a8\u907f\u514d\u8d85\u51fa\u9805\u76ee\u914d\u984d\u9650\u5236\u3002\n- \u522a\u9664 Google Cloud \u9805\u76ee\uff1a\n- ```\ngcloud projects delete PROJECT_ID\n```\n### \u522a\u9664\u5404\u500b\u8cc7\u6e90- \u9000\u51fa Python \u865b\u64ec\u74b0\u5883\uff1a```\ndeactivate\n```\n- \u505c\u6b62\u6d41\u6c34\u7dda\uff1a- \u5217\u51fa\u6b63\u5728\u904b\u884c\u7684 Dataflow \u4f5c\u696d\u7684 ID\uff0c\u7136\u5f8c\u8a18\u4e0b\u672c\u6559\u7a0b\u4f5c\u696d\u7684 ID\uff1a```\ngcloud dataflow jobs list --region=REGION --status=active\n```\n- \u53d6\u6d88\u4f5c\u696d\uff1a```\ngcloud dataflow jobs cancel JOB_ID --region=REGION\n```\n- \u522a\u9664\u5b58\u5132\u6876\u53ca\u5176\u4e2d\u7684\u6240\u6709\u5c0d\u8c61\uff1a```\ngcloud storage rm gs://BUCKET_NAME --recursive\n```\n- \u522a\u9664\u4e3b\u984c\u548c\u8a02\u95b1\uff1a```\ngcloud pubsub topics delete PROMPTS_TOPIC_IDgcloud pubsub topics delete RESPONSES_TOPIC_IDgcloud pubsub subscriptions delete RESPONSES_SUBSCRIPTION_ID\n```\n- \u64a4\u6d88\u6388\u4e88 Compute Engine \u9ed8\u8a8d\u670d\u52d9\u8cec\u865f\u7684\u89d2\u8272\u3002\u5c0d\u4ee5\u4e0b\u6bcf\u500b IAM \u89d2\u8272\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\u4e00\u6b21\uff1a- `roles/dataflow.admin`\n- `roles/dataflow.worker`\n- `roles/storage.admin`\n- `roles/pubsub.editor`\n- `roles/aiplatform.user`\n```\ngcloud projects remove-iam-policy-binding PROJECT_ID --member=serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com --role=SERVICE_ACCOUNT_ROLE\n```\n- \u53ef\u9078\uff1a\u64a4\u6d88\u60a8\u7684 Google \u8cec\u865f\u7684\u89d2\u8272\u3002```\ngcloud projects remove-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS\" --role=roles/iam.serviceAccountUser\n```\n- Titles in dynamic includes are not used anywhere, and we should avoid paying to translate them\n- \u53ef\u9078\uff1a\u64a4\u6d88\u60a8\u5275\u5efa\u7684\u8eab\u4efd\u9a57\u8b49\u6191\u64da\uff0c\u4e26\u522a\u9664\u672c\u5730\u6191\u64da\u6587\u4ef6\u3002```\ngcloud auth application-default revoke\n```\n- Titles in dynamic includes are not used anywhere, and we should avoid paying to translate them\n- \u53ef\u9078\uff1a\u5f9e gcloud CLI \u64a4\u6d88\u6191\u64da\u3002```\ngcloud auth revoke\n```## \u5f8c\u7e8c\u6b65\u9a5f\n- [\u63a2\u7d22 Dataflow ML](https://cloud.google.com/dataflow/docs/machine-learning?hl=zh-cn) \u3002\n- \u8a73\u7d30\u77ad\u89e3 [RunInference API](https://beam.apache.org/documentation/ml/about-ml/) \u3002\n- \u53c3\u95b1 Apache Beam [AI/\u6a5f\u5668\u5b78\u7fd2\u6d41\u6c34\u7dda](https://beam.apache.org/documentation/ml/overview/) \u6587\u6a94\uff0c\u6df1\u5165\u77ad\u89e3\u5982\u4f55\u5c07\u6a5f\u5668\u5b78\u7fd2\u8207 Apache Beam \u642d\u914d\u4f7f\u7528\u3002\n- \u5b8c\u6210 [\u5c07 RunInference \u7528\u65bc\u751f\u6210\u5f0f AI](https://cloud.google.com/dataflow/docs/notebooks/run_inference_generative_ai?hl=zh-cn) \u7b46\u8a18\u672c\u3002\n- \u63a2\u7d22\u6709\u95dc Google Cloud \u7684\u53c3\u8003\u67b6\u69cb\u3001\u5716\u8868\u548c\u6700\u4f73\u505a\u6cd5\u3002\u67e5\u770b\u6211\u5011\u7684 [Cloud Architecture Center](https://cloud.google.com/architecture?hl=zh-cn) \u3002", "guide": "Dataflow"}