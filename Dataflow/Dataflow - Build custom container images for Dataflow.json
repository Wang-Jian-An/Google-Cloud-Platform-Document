{"title": "Dataflow - Build custom container images for Dataflow", "url": "https://cloud.google.com/dataflow/docs/guides/build-container-image", "abstract": "# Dataflow - Build custom container images for Dataflow\n", "content": "## Requirements\nA custom container image for Dataflow must meet the following requirements:\n- The Apache Beam SDK and necessary dependencies are installed. We recommend starting with a default Apache Beam SDK image. For more information, see [Select a base image](#base-images) in this document.\n- The`/opt/apache/beam/boot`script must run as the last step during container startup. This script initializes the worker environment and starts the SDK worker process. This script is the default`ENTRYPOINT`in the Apache Beam SDK images. However, if you use a different base image, or if you override the default`ENTRYPOINT`, then you must run the script explicitly. For more information, see [Modify the container entrypoint](#custom-entrypoint) in this document.\n- Your container image must support the architecture of the worker VMs for your Dataflow job. If you plan to use the custom container on ARM VMs, we recommend building a multi-architecture image. For more information, see [Build a multi-architecture container image](/dataflow/docs/guides/multi-architecture-container) .## Before you begin\n- Verify that the version of the Apache Beam SDK installed supports [Runner v2](/dataflow/docs/runner-v2) and your language version. For more information, see [Install the Apache Beam SDK](/dataflow/docs/guides/installing-beam-sdk) .\n- To test your container image locally, you must have Docker installed. For more information, see [Get Docker](https://docs.docker.com/get-docker/) .\n- Create an [Artifact Registry](/artifact-registry/docs/repositories) repository. Specify Docker image format. You must have at least [Artifact Registry Writer access](/artifact-registry/docs/access-control#permissions) to the repository.To create a new repository, run the [gcloud artifacts repositories create command](/sdk/gcloud/reference/artifacts/repositories/create) :```\ngcloud artifacts repositories create REPOSITORY \\\u00a0 --repository-format=docker \\\u00a0 --location=REGION \\\u00a0 --async\n```Replace the following:- : a name for your repository. Repository names must be unique for each location in a project.\n- : the region to deploy your Dataflow job in. Select a Dataflow region close to where you run the commands. The value must be a valid region name. For more information about regions and locations, see [Dataflow locations](/dataflow/docs/resources/locations) .\nThis example uses the `--async` flag. The command returns immediately, without waiting for the operation to complete.\n- To configure Docker to authenticate requests for Artifact Registry, run the [gcloud auth configure-docker command](/sdk/gcloud/reference/auth/configure-docker) :```\ngcloud auth configure-docker REGION-docker.pkg.dev\n```The command updates your Docker configuration. You can now connect with Artifact Registry in your Google Cloud project to push images.## Select a base image\nWe recommend starting with an Apache Beam SDK image as the base container image. These images are released as part of Apache Beam releases to [Docker Hub](https://hub.docker.com/search?q=apache%2Fbeam&type=image) .\n### Use an Apache Beam base image\nTo use an Apache Beam SDK image as the base image, specify the container image in the `FROM` instruction and then add your own customizations.\nThis example uses Java 8 with the Apache Beam SDK version 2.54.0.\n```\nFROM apache/beam_java8_sdk:2.54.0\n# Make your customizations here, for example:\nENV FOO=/bar\nCOPY path/to/myfile ./\n```\nThe runtime version of the custom container must match the runtime that you will use to start the pipeline. For example, if you will start the pipeline from a local Java 11 environment, the `FROM` line must specify a Java 11 environment: `apache/beam_java11_sdk:...` .\nThis example uses Python 3.10 with the Apache Beam SDK version 2.54.0.\n```\nFROM apache/beam_python3.10_sdk:2.54.0\n# Make your customizations here, for example:\nENV FOO=/bar\nCOPY path/to/myfile ./\n```\nThe runtime version of the custom container must match the runtime that you will use to start the pipeline. For example, if you will start the pipeline from a local Python 3.10 environment, the `FROM` line must specify a Python 3.10 environment: `apache/beam_python3.10_sdk:...` .\nThis example uses Go with the Apache Beam SDK version 2.54.0.\n```\nFROM apache/beam_go_sdk:2.54.0\n# Make your customizations here, for example:\nENV FOO=/bar\nCOPY path/to/myfile ./\n```\n### Use a custom base image\nIf you want to use a different base image, or need to modify some aspect of the default Apache Beam images (such as OS version or patches), use a [multistage build](https://docs.docker.com/develop/develop-images/multistage-build/) process. Copy the necessary artifacts from a default Apache Beam base image.\nSet the `ENTRYPOINT` to run the `/opt/apache/beam/boot` script, which initializes the worker environment and starts the SDK worker process. If you don't set this entrypoint, the Dataflow workers don't start properly.\nThe following example shows a Dockerfile that copies files from the Apache Beam SDK:\n```\nFROM openjdk:8\n# Copy files from official SDK image, including script/dependencies.\nCOPY --from=apache/beam_java8_sdk:2.54.0 /opt/apache/beam /opt/apache/beam\n# Set the entrypoint to Apache Beam SDK launcher.\nENTRYPOINT [\"/opt/apache/beam/boot\"]\n```\n```\nFROM python:3.10-slim\n# Install SDK.\nRUN pip install --no-cache-dir apache-beam[gcp]==2.54.0\n# Verify that the image does not have conflicting dependencies.\nRUN pip check\n# Copy files from official SDK image, including script/dependencies.\nCOPY --from=apache/beam_python3.10_sdk:2.54.0 /opt/apache/beam /opt/apache/beam\n# Set the entrypoint to Apache Beam SDK launcher.\nENTRYPOINT [\"/opt/apache/beam/boot\"]\n```\nThis example assumes necessary dependencies (in this case, Python 3.10 and `pip` ) have been installed on the existing base image. Installing the Apache Beam SDK into the image ensures that the image has the necessary SDK dependencies and reduces the worker startup time.\n **Important:** The SDK version specified in the `RUN` and `COPY` instructions must match the version used to launch the pipeline.\n```\nFROM golang:latest\n# Copy files from official SDK image, including script/dependencies.\nCOPY --from=apache/beam_go_sdk:2.54.0 /opt/apache/beam /opt/apache/beam\n# Set the entrypoint to Apache Beam SDK launcher.\nENTRYPOINT [\"/opt/apache/beam/boot\"]\n```\n## Modify the container entrypoint\nIf your container runs a custom script during container startup, the script must end with running `/opt/apache/beam/boot` . Arguments passed by Dataflow during container startup must be passed to the default boot script. The following example shows a custom startup script that calls the default boot script:\n```\n#!/bin/bashecho \"This is my custom script\"# ...# Pass command arguments to the default boot script./opt/apache/beam/boot \"$@\"\n```\nIn your Dockerfile, set the `ENTRYPOINT` to call your script:\n```\nFROM apache/beam_java8_sdk:2.54.0\nCOPY script.sh path/to/my/script.sh\nENTRYPOINT [ \"path/to/my/script.sh\" ]\n```\n```\nFROM apache/beam_python3.10_sdk:2.54.0\nCOPY script.sh path/to/my/script.sh\nENTRYPOINT [ \"path/to/my/script.sh\" ]\n```\n```\nFROM apache/beam_go_sdk:2.54.0\nCOPY script.sh path/to/my/script.sh\nENTRYPOINT [ \"path/to/my/script.sh\" ]\n```\n## Build and push the image\nYou can use Cloud Build or Docker to build your container image and push it to an Artifact Registry repository.\nTo build the file and push it to your Artifact Registry repository, run the [gcloud builds submit command](/sdk/gcloud/reference/builds/submit) :\n```\n\u00a0 gcloud builds submit --tag REGION-docker.pkg.dev/PROJECT_ID/REPOSITORY/dataflow/FILE_NAME:TAG .\n```\n```\ndocker build . --tag REGION-docker.pkg.dev/PROJECT_ID/REPOSITORY/dataflow/FILE_NAME:TAGdocker push REGION-docker.pkg.dev/PROJECT_ID/REPOSITORY/dataflow/FILE_NAME:TAG\n```\nReplace the following:\n- ``: the [region](/dataflow/docs/resources/locations) to deploy your Dataflow job in. The value of the`REGION`variable must be a valid region name.\n- ``: the project name or username.\n- ``: the image repository name.\n- ``: the name of your Dockerfile.\n- ``: the image tag. Always specify a versioned container SHA or tag. Don't use the`:latest`tag or a mutable tag.## Pre-install Python dependencies\nThis section applies to Python pipelines.\nWhen you launch a Python Dataflow job, you can specify additional dependencies by using the `--requirements_file` or the `--extra_packages` option at runtime. For more information, see [Managing Python Pipeline Dependencies](https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/) . Additional dependencies are installed in each Dataflow worker container. When the job first starts and during autoscaling, the dependency installation often leads to high CPU usage and a long warm-up period on all newly started Dataflow workers.\nTo avoid repetitive dependency installations, you can pre-build a custom Python SDK container image with the dependencies pre-installed. You can perform this step at build time by using a Dockerfile, or at run time when you submit the job.\nWorkers create a new virtual Python environment when they start the container. For this reason, install dependencies into the default (global) Python environment instead of creating a virtual environment. If you activate a virtual environment in your container image, this environment might not be activate when the job starts. For more information, see [Common issues](#common_issues) .\n### Pre-install using a Dockerfile\nTo add extra dependencies directly to your Python custom container, use the following commands:\n```\nFROM apache/beam_python3.10_sdk:2.54.0\nCOPY requirements.txt .\n# Pre-install Python dependencies. For reproducibile builds,\n# supply all of the dependencies and their versions in a requirements.txt file.\nRUN pip install -r requirements.txt\n# You can also install individual dependencies.\nRUN pip install lxml\n# Pre-install other dependencies.\nRUN apt-get update \\\n && apt-get dist-upgrade \\\n && apt-get install -y --no-install-recommends ffmpeg\n```\nSubmit your job with the `--sdk_container_image` and the `--sdk_location` pipeline options. The `--sdk_location` option prevents the SDK from downloading when your job launches. The SDK is retrieved directly from the container image.\nThe following example runs the [wordcount example pipeline](/dataflow/docs/quickstarts/create-pipeline-python) :\n```\npython -m apache_beam.examples.wordcount \\\u00a0 --input=INPUT_FILE \\\u00a0 --output=OUTPUT_FILE \\\u00a0 --project=PROJECT_ID \\\u00a0 --region=REGION \\\u00a0 --temp_location=TEMP_LOCATION \\\u00a0 --runner=DataflowRunner \\\u00a0 --experiments=use_runner_v2 \\\u00a0 --sdk_container_image=IMAGE_URI\u00a0 --sdk_location=container\n```\nReplace the following:\n- : an input file for the pipeline\n- : a path to write output to\n- : the Google Cloud project ID\n- : the [region](/dataflow/docs/resources/locations) to deploy your Dataflow job in\n- : the Cloud Storage path for Dataflow to stage temporary job files\n- : the custom container image URI\n### Pre-build a container image when submitting the job\nPre-building a container image lets you to pre-install the pipeline dependencies before job startup. You don't need to build a custom container image.\nTo pre-build a container with additional Python dependencies when you submit a job, use the following pipeline options:\n- `--prebuild_sdk_container_engine=[cloud_build | local_docker]` . When this flag is set, Apache Beam generates a custom container and installs all of the dependencies specified by the `--requirements_file` and the `--extra_packages` options. This flag supports the following values:- `cloud_build`. Use [Cloud Build](/build/docs/overview) to build the container. The Cloud Build API must be enabled in your project.\n- `local_docker`. Use your local Docker installation to build the container.\n- `--docker_registry_push_url=` `` . Replace `` with an Artifact Registry folder.\n- `--sdk_location=container` . This option prevents the workers from downloading the SDK when your job launches. Instead, the SDK is retrieved directly from the container image.\nThe following example uses Cloud Build to pre-build the image:\n```\npython -m apache_beam.examples.wordcount \\\u00a0 --input=INPUT_FILE \\\u00a0 --output=OUTPUT_FILE \\\u00a0 --project=PROJECT_ID \\\u00a0 --region=REGION \\\u00a0 --temp_location=TEMP_LOCATION \\\u00a0 --runner=DataflowRunner \\\u00a0 --disk_size_gb=DISK_SIZE_GB \\\u00a0 --experiments=use_runner_v2 \\\u00a0 --requirements_file=./requirements.txt \\\u00a0 --prebuild_sdk_container_engine=cloud_build \\\u00a0 --docker_registry_push_url=IMAGE_PATH \\\u00a0 --sdk_location=container\n```\nThe pre-build feature requires the Apache Beam SDK for Python version 2.25.0 or later.\nThe SDK container image pre-building workflow uses the image passed using the `--sdk_container_image` pipeline option as the base image. If the option is not set, by default an Apache Beam image is used as the base image.\n**Note:** With Apache Beam SDK versions 2.38.0 and earlier, to specify the base image, use `--prebuild_sdk_container_base_image` .\nYou can reuse a prebuilt Python SDK container image in another job with the same dependencies and SDK version. To reuse the image, pass the prebuilt container image URL to the other job by using the `--sdk_container_image` pipeline option. Remove the dependency options `--requirements_file` , `--extra_packages` , and `--setup_file` .\nIf you don't plan to reuse the image, delete it after the job completes. You can delete the image with the gcloud CLI or in the Artifact Registry pages in the Google Cloud console.\nIf the image is stored in Artifact Registry, use the [artifacts docker images delete](/sdk/gcloud/reference/artifacts/docker/images/delete) command:\n```\n\u00a0 \u00a0gcloud artifacts docker images delete IMAGE --delete-tags\n```\n### Common issues\n- If your job has extra Python dependencies from a private PyPi mirror and can't be pulled by a remote Cloud Build job, try using the local docker option or try building your container using a Dockerfile.\n- If the Cloud Build job fails with `docker exit code 137` , the build job ran out of memory, potentially due to the size of the dependencies being installed. Use a larger Cloud Build worker machine type by passing `--cloud_build_machine_type=` `` , where is one of the following options:- `n1-highcpu-8`\n- `n1-highcpu-32`\n- `e2-highcpu-8`\n- `e2-highcpu-32`\nBy default, Cloud Build uses the machine type `e2-medium` .\n- In Apache Beam 2.44.0 and later, workers create a virtual environment when starting a custom container. If the container creates its own virtual environment to install dependencies, those dependencies are discarded. This behavior can cause errors such as the following:`ModuleNotFoundError: No module named '<dependency name>'`To avoid this issue, install dependencies into the default (global) Python environment. As a workaround, disable this behavior in Beam 2.48.0 and later by setting the following environment variable in your container image:`ENV RUN_PYTHON_SDK_IN_DEFAULT_ENVIRONMENT=1`## What's next\n- For more information about writing Dockerfiles, see [Best practices for writing Dockerfiles](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/) .\n- Learn how to [Run a Dataflow job in a custom container](/dataflow/docs/guides/run-custom-container) .", "guide": "Dataflow"}