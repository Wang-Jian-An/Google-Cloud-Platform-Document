{"title": "Dataflow - Use Apache Beam and Bigtable to enrich data", "url": "https://cloud.google.com/dataflow/docs/notebooks/bigtable_enrichment_transform?hl=zh-cn", "abstract": "# Dataflow - Use Apache Beam and Bigtable to enrich data\n| 0     | 1      |\n|:--------------------|:----------------------|\n| Run in Google Colab | View source on GitHub |\nThis notebook shows how to enrich data by using the Apache Beam [enrichment transform](https://beam.apache.org/documentation/transforms/python/elementwise/enrichment/) with [Bigtable](https://cloud.google.com/bigtable/docs/overview) . The enrichment transform is a turnkey transform in Apache Beam that lets you enrich data by using a key-value lookup. This transform has the following features:\n- The transform has a built-in Apache Beam handler that interacts with Bigtable to get data to use in the enrichment.\n- The enrichment transform uses client-side throttling to manage rate limiting the requests. The requests are exponentially backed off with a default retry strategy. You can configure rate limiting to suit your use case.\nThis notebook demonstrates the following ecommerce use case:\nA stream of online transaction from [Pub/Sub](https://cloud.google.com/pubsub/docs/guides) contains the following fields: `sale_id` , `product_id` , `customer_id` , `quantity` , and `price` . Additional customer demographic data is stored in a separate Bigtable cluster. The demographic data is used to enrich the event stream from Pub/Sub. Then, the enriched data is used to predict the next product to recommended to a customer.\n", "content": "## Before you begin\nSet up your environment and download dependencies.\n### Install Apache Beam\nTo use the enrichment transform with the built-in Bigtable handler, install the Apache Beam SDK version 2.54.0 or later.\n```\npip install torchpip install apache_beam[interactive,gcp]==2.54.0 --quiet\n```\n```\nimport datetimeimport jsonimport mathfrom typing import Anyfrom typing import Dictimport torchfrom google.cloud import pubsub_v1from google.cloud.bigtable import Clientfrom google.cloud.bigtable import column_familyimport apache_beam as beamimport apache_beam.runners.interactive.interactive_beam as ibfrom apache_beam.ml.inference.base import RunInferencefrom apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensorfrom apache_beam.options import pipeline_optionsfrom apache_beam.runners.interactive.interactive_runner import InteractiveRunnerfrom apache_beam.transforms.enrichment import Enrichmentfrom apache_beam.transforms.enrichment_handlers.bigtable import BigTableEnrichmentHandler\n```\n### Authenticate with Google Cloud\nThis notebook reads data from Pub/Sub and Bigtable. To use your Google Cloud account, authenticate this notebook.\n```\nfrom google.colab import authauth.authenticate_user()\n```\nReplace `<PROJECT_ID>` , `<INSTANCE_ID>` , and `<TABLE_ID>` with the appropriate values for your setup. These fields are used with Bigtable.\n```\nPROJECT_ID = \"<PROJECT_ID>\"INSTANCE_ID = \"<INSTANCE_ID>\"TABLE_ID = \"<TABLE_ID>\"\n```\n### Train the model\nCreate sample data by using the format `[product_id, quantity, price, customer_id, customer_location, recommend_product_id]` .\n```\ndata = [\u00a0 \u00a0 [3, 5, 127, 9, 'China', 7], [1, 6, 167, 5, 'Peru', 4], [5, 4, 91, 2, 'USA', 8], [7, 2, 52, 1, 'India', 4], [1, 8, 118, 3, 'UK', 8], [4, 6, 132, 8, 'Mexico', 2],\u00a0 \u00a0 [6, 3, 154, 6, 'Brazil', 3], [4, 7, 163, 1, 'India', 7], [5, 2, 80, 4, 'Egypt', 9], [9, 4, 107, 7, 'Bangladesh', 1], [2, 9, 192, 8, 'Mexico', 4], [4, 5, 116, 5, 'Peru', 8],\u00a0 \u00a0 [8, 1, 195, 1, 'India', 7], [8, 6, 153, 5, 'Peru', 1], [5, 3, 120, 6, 'Brazil', 2], [2, 7, 187, 7, 'Bangladesh', 4], [1, 8, 103, 6, 'Brazil', 8], [2, 9, 181, 1, 'India', 8],\u00a0 \u00a0 [6, 5, 166, 3, 'UK', 5], [3, 4, 115, 8, 'Mexico', 1], [4, 7, 170, 4, 'Egypt', 2], [9, 3, 141, 7, 'Bangladesh', 3], [9, 3, 157, 1, 'India', 2], [7, 6, 128, 9, 'China', 1],\u00a0 \u00a0 [1, 8, 102, 3, 'UK', 4], [5, 2, 107, 4, 'Egypt', 6], [6, 5, 164, 8, 'Mexico', 9], [4, 7, 188, 5, 'Peru', 1], [8, 1, 184, 1, 'India', 2], [8, 6, 198, 2, 'USA', 5],\u00a0 \u00a0 [5, 3, 105, 6, 'Brazil', 7], [2, 7, 162, 7, 'Bangladesh', 7], [1, 8, 133, 9, 'China', 3], [2, 9, 173, 1, 'India', 7], [6, 5, 183, 5, 'Peru', 8], [3, 4, 191, 3, 'UK', 6],\u00a0 \u00a0 [4, 7, 123, 2, 'USA', 5], [9, 3, 159, 8, 'Mexico', 2], [9, 3, 146, 4, 'Egypt', 8], [7, 6, 194, 1, 'India', 8], [3, 5, 112, 6, 'Brazil', 1], [4, 6, 101, 7, 'Bangladesh', 2],\u00a0 \u00a0 [8, 1, 192, 4, 'Egypt', 4], [7, 2, 196, 5, 'Peru', 6], [9, 4, 124, 9, 'China', 7], [3, 4, 129, 5, 'Peru', 6], [6, 3, 151, 8, 'Mexico', 9], [5, 7, 114, 7, 'Bangladesh', 4],\u00a0 \u00a0 [4, 7, 175, 6, 'Brazil', 5], [1, 8, 121, 1, 'India', 2], [4, 6, 187, 2, 'USA', 5], [6, 5, 144, 9, 'China', 9], [9, 4, 103, 5, 'Peru', 3], [5, 3, 84, 3, 'UK', 1],\u00a0 \u00a0 [3, 5, 193, 2, 'USA', 4], [4, 7, 135, 1, 'India', 1], [7, 6, 148, 8, 'Mexico', 8], [1, 6, 160, 5, 'Peru', 7], [8, 6, 155, 6, 'Brazil', 9], [5, 7, 183, 7, 'Bangladesh', 2],\u00a0 \u00a0 [2, 9, 125, 4, 'Egypt', 4], [6, 3, 111, 9, 'China', 9], [5, 2, 132, 3, 'UK', 3], [4, 5, 104, 7, 'Bangladesh', 7], [2, 7, 177, 8, 'Mexico', 7]]\n```\n```\ncountries_to_id = {'India': 1, 'USA': 2, 'UK': 3, 'Egypt': 4, 'Peru': 5,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0'Brazil': 6, 'Bangladesh': 7, 'Mexico': 8, 'China': 9}\n```\nPreprocess the data:\n- Convert the lists to tensors.\n- Separate the features from the expected prediction.\n```\nX = [torch.tensor(item[:4]+[countries_to_id[item[4]]], dtype=torch.float) for item in data]Y = [torch.tensor(item[-1], dtype=torch.float) for item in data]\n```\nDefine a simple model that has five input features and predicts a single value.\n```\ndef build_model(n_inputs, n_outputs):\u00a0 \"\"\"build_model builds and returns a model that takes\u00a0 `n_inputs` features and predicts `n_outputs` value\"\"\"\u00a0 return torch.nn.Sequential(\u00a0 \u00a0 \u00a0 torch.nn.Linear(n_inputs, 8),\u00a0 \u00a0 \u00a0 torch.nn.ReLU(),\u00a0 \u00a0 \u00a0 torch.nn.Linear(8, 16),\u00a0 \u00a0 \u00a0 torch.nn.ReLU(),\u00a0 \u00a0 \u00a0 torch.nn.Linear(16, n_outputs))\n```\nTrain the model.\n```\nmodel = build_model(n_inputs=5, n_outputs=1)loss_fn = torch.nn.MSELoss()optimizer = torch.optim.Adam(model.parameters())for epoch in range(1000):\u00a0 print(f'Epoch {epoch}: ---')\u00a0 optimizer.zero_grad()\u00a0 for i in range(len(X)):\u00a0 \u00a0 pred = model(X[i])\u00a0 \u00a0 loss = loss_fn(pred, Y[i])\u00a0 \u00a0 loss.backward()\u00a0 optimizer.step()\n```\nSave the model to the `STATE_DICT_PATH` variable.\n```\nSTATE_DICT_PATH = './model.pth'torch.save(model.state_dict(), STATE_DICT_PATH)\n```\n### Set up the Bigtable table\nCreate a sample Bigtable table for this notebook.\n```\n# Connect to the Bigtable instance. If you don't have admin access, then drop `admin=True`.client = Client(project=PROJECT_ID, admin=True)instance = client.instance(INSTANCE_ID)# Create a column family.column_family_id = 'demograph'max_versions_rule = column_family.MaxVersionsGCRule(2)column_families = {column_family_id: max_versions_rule}# Create a table.table = instance.table(TABLE_ID)# You need admin access to use `.exists()`. If you don't have the admin access, then# comment out the if-else block.if not table.exists():\u00a0 table.create(column_families=column_families)else:\u00a0 print(\"Table %s already exists in %s:%s\" % (TABLE_ID, PROJECT_ID, INSTANCE_ID))\n```\nAdd rows to the table for the enrichment example.\n```\n# Define column names for the table.customer_id = 'customer_id'customer_name = 'customer_name'customer_location = 'customer_location'# The following data is sample data to insert into Bigtable.customers = [\u00a0 {\u00a0 \u00a0 'customer_id': 1, 'customer_name': 'Sam', 'customer_location': 'India'\u00a0 },\u00a0 {\u00a0 \u00a0 'customer_id': 2, 'customer_name': 'John', 'customer_location': 'USA'\u00a0 },\u00a0 {\u00a0 \u00a0 'customer_id': 3, 'customer_name': 'Travis', 'customer_location': 'UK'\u00a0 },]for customer in customers:\u00a0 row_key = str(customer[customer_id]).encode()\u00a0 row = table.direct_row(row_key)\u00a0 row.set_cell(\u00a0 \u00a0 column_family_id,\u00a0 \u00a0 customer_id.encode(),\u00a0 \u00a0 str(customer[customer_id]),\u00a0 \u00a0 timestamp=datetime.datetime.utcnow())\u00a0 row.set_cell(\u00a0 \u00a0 column_family_id,\u00a0 \u00a0 customer_name.encode(),\u00a0 \u00a0 customer[customer_name],\u00a0 \u00a0 timestamp=datetime.datetime.utcnow())\u00a0 row.set_cell(\u00a0 \u00a0 column_family_id,\u00a0 \u00a0 customer_location.encode(),\u00a0 \u00a0 customer[customer_location],\u00a0 \u00a0 timestamp=datetime.datetime.utcnow())\u00a0 row.commit()\u00a0 print('Inserted row for key: %s' % customer[customer_id])\n```\n```\nInserted row for key: 1\nInserted row for key: 2\nInserted row for key: 3\n```\n### Publish messages to Pub/Sub\nUse the Pub/Sub Python client to publish messages.\n```\n# Replace <TOPIC_NAME> with the name of your Pub/Sub topic.TOPIC = \"<TOPIC_NAME>\"# Replace <SUBSCRIPTION_PATH> with the subscription for your topic.SUBSCRIPTION = \"<SUBSCRIPTION_PATH>\"\n```\n```\nmessages = [\u00a0 \u00a0 {'sale_id': i, 'customer_id': i, 'product_id': i, 'quantity': i, 'price': i*100}\u00a0 \u00a0 for i in range(1,4)\u00a0 ]publisher = pubsub_v1.PublisherClient()topic_name = publisher.topic_path(PROJECT_ID, TOPIC)for message in messages:\u00a0 data = json.dumps(message).encode('utf-8')\u00a0 publish_future = publisher.publish(topic_name, data)\n```\n## Use the Bigtable enrichment handler\nThe [BigTableEnrichmentHandler](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.enrichment_handlers.bigtable.html#apache_beam.transforms.enrichment_handlers.bigtable.BigTableEnrichmentHandler) is a built-in handler included in the Apache Beam SDK versions 2.54.0 and later.\nTo establish a client for the Bigtable enrichment handler, replace `<PROJECT_ID>` , `<INSTANCE_ID>` , and `<TABLE_ID>` with the appropriate values for those fields. The `row_key` variable is the field name from the input row that contains the row key to use when querying Bigtable.\nTo convert a `string` type to a `byte` type or a `byte` type to a `string` type from Bigtable, you can configure additional options, such as [app_profile_id](https://cloud.google.com/bigtable/docs/app-profiles) , [row_filter](https://cloud.google.com/python/docs/reference/bigtable/latest/row-filters) , and [encoding](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.enrichment_handlers.bigtable.html#apache_beam.transforms.enrichment_handlers.bigtable.BigTableEnrichmentHandler:%7E:text=for%20more%20details.-,encoding,-(str)%20%E2%80%93%20encoding) type.\nThe default `encoding` type is `utf-8` .\n```\nrow_key = 'customer_id'\n```\n```\nbigtable_handler = BigTableEnrichmentHandler(project_id=PROJECT_ID,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0instance_id=INSTANCE_ID,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0table_id=TABLE_ID,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0row_key=row_key)\n```\nThe `BigTableEnrichmentHandler` returns the latest value from the table without its associated timestamp for the `row_key` that you provide. If you want to fetch the `timestamp` associated with the `row_key` value, then pass `include_timestamp=True` to the handler.\n**Note:** When exceptions occur, by default, the logging severity is set to warning ( [ExceptionLevel.WARN](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.enrichment_handlers.bigtable.html#apache_beam.transforms.enrichment_handlers.bigtable.ExceptionLevel.WARN) ). To configure the severity to raise exceptions, set `exception_level` to [ExceptionLevel.RAISE](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.enrichment_handlers.bigtable.html#apache_beam.transforms.enrichment_handlers.bigtable.ExceptionLevel.RAISE) . To ignore exceptions, set `exception_level` to [ExceptionLevel.QUIET](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.enrichment_handlers.bigtable.html#apache_beam.transforms.enrichment_handlers.bigtable.ExceptionLevel.QUIET) .\n## Use the enrichment transform\nTo use the [enrichment transform](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.enrichment.html#apache_beam.transforms.enrichment.Enrichment) , the [EnrichmentHandler](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.enrichment.html#apache_beam.transforms.enrichment.EnrichmentSourceHandler) parameter is required. You can also use a configuration parameter to specify a `lambda` for a join function, a timeout, a throttler, and a repeater (retry strategy).\n- `join_fn`: A lambda function that takes dictionaries as input and returns an enriched row (`Callable[[Dict[str, Any], Dict[str, Any]], beam.Row]`). The enriched row specifies how to join the data fetched from the API. Defaults to a [cross-join](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.enrichment.html#apache_beam.transforms.enrichment.cross_join) .\n- `timeout`: The number of seconds to wait for the request to be completed by the API before timing out. Defaults to 30 seconds.\n- `throttler`: Specifies the throttling mechanism. The only supported option is default client-side adaptive throttling.\n- `repeater`: Specifies the retry strategy when errors like`TooManyRequests`and`TimeoutException`occur. Defaults to [ExponentialBackOffRepeater](https://beam.apache.org/releases/pydoc/current/apache_beam.io.requestresponse.html#apache_beam.io.requestresponse.ExponentialBackOffRepeater) .\nThe following example demonstrates the code needed to add this transform to your pipeline.\n```\nwith beam.Pipeline() as p:\u00a0 output = (p\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ...\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"Enrich with BigTable\" >> Enrichment(bigtable_handler, timeout=10)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"RunInference\" >> RunInference(model_handler)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ...\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\n```\nTo make a prediction, use the following fields: `product_id` , `quantity` , `price` , `customer_id` , and `customer_location` . Retrieve the value of the `customer_location` field from Bigtable.\nBecause the enrichment transform performs a [cross_join](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.enrichment.html#apache_beam.transforms.enrichment.cross_join) by default, design the custom join to enrich the input data. This design ensures that the join includes only the specified fields.\n```\ndef custom_join(left: Dict[str, Any], right: Dict[str, Any]):\u00a0 enriched = {}\u00a0 enriched['product_id'] = left['product_id']\u00a0 enriched['quantity'] = left['quantity']\u00a0 enriched['price'] = left['price']\u00a0 enriched['customer_id'] = left['customer_id']\u00a0 enriched['customer_location'] = right['demograph']['customer_location']\u00a0 return beam.Row(**enriched)\n```\n## Use the PyTorchModelHandlerTensor interface to run inference\nBecause the enrichment transform outputs data in the format `beam.Row` , to make it compatible with the [PyTorchModelHandlerTensor](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.pytorch_inference.html#apache_beam.ml.inference.pytorch_inference.PytorchModelHandlerTensor) interface, convert it to `torch.tensor` . Additionally, the enriched field `customer_location` is a `string` type, but the model requires a `float` type. Convert the `customer_location` field to a `float` type.\n```\ndef convert_row_to_tensor(element: beam.Row):\u00a0 row_dict = element._asdict()\u00a0 row_dict['customer_location'] = countries_to_id[row_dict['customer_location']]\u00a0 return torch.tensor(list(row_dict.values()), dtype=torch.float)\n```\nInitialize the model handler with the preprocessing function.\n```\nmodel_handler = PytorchModelHandlerTensor(state_dict_path=STATE_DICT_PATH,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_class=build_model,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_params={'n_inputs':5, 'n_outputs':1}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ).with_preprocess_fn(convert_row_to_tensor)\n```\nDefine a `DoFn` to format the output.\n```\nclass PostProcessor(beam.DoFn):\u00a0 def process(self, element, *args, **kwargs):\u00a0 \u00a0 print('Customer %d who bought product %d is recommended to buy product %d' % (element.example[3], element.example[0], math.ceil(element.inference[0])))\n```\n## Run the pipeline\nConfigure the pipeline to run in streaming mode.\n```\noptions = pipeline_options.PipelineOptions()options.view_as(pipeline_options.StandardOptions).streaming = True # Streaming mode is set True\n```\nPub/Sub sends the data in bytes. Convert the data to `beam.Row` objects by using a `DoFn` .\n```\nclass DecodeBytes(beam.DoFn):\u00a0 \"\"\"\u00a0 The DecodeBytes `DoFn` converts the data read from Pub/Sub to `beam.Row`.\u00a0 First, decode the encoded string. Convert the output to\u00a0 a `dict` with `json.loads()`, which is used to create a `beam.Row`.\u00a0 \"\"\"\u00a0 def process(self, element, *args, **kwargs):\u00a0 \u00a0 element_dict = json.loads(element.decode('utf-8'))\u00a0 \u00a0 yield beam.Row(**element_dict)\n```\nUse the following code to run the pipeline.\n**Note:** Because this pipeline is a streaming pipeline, you need to manually stop the cell. If you don't stop the cell, the pipeline continues to run.\n```\nwith beam.Pipeline(options=options) as p:\u00a0 _ = (p\u00a0 \u00a0 \u00a0 \u00a0| \"Read from Pub/Sub\" >> beam.io.ReadFromPubSub(subscription=SUBSCRIPTION)\u00a0 \u00a0 \u00a0 \u00a0| \"ConvertToRow\" >> beam.ParDo(DecodeBytes())\u00a0 \u00a0 \u00a0 \u00a0| \"Enrichment\" >> Enrichment(bigtable_handler, join_fn=custom_join)\u00a0 \u00a0 \u00a0 \u00a0| \"RunInference\" >> RunInference(model_handler)\u00a0 \u00a0 \u00a0 \u00a0| \"Format Output\" >> beam.ParDo(PostProcessor())\u00a0 \u00a0 \u00a0 \u00a0)\n```\n```\nCustomer 1 who bought product 1 is recommended to buy product 3\nCustomer 2 who bought product 2 is recommended to buy product 5\nCustomer 3 who bought product 3 is recommended to buy product 7\n```", "guide": "Dataflow"}