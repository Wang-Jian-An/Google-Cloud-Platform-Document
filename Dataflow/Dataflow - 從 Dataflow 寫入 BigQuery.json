{"title": "Dataflow - \u5f9e Dataflow \u5beb\u5165 BigQuery", "url": "https://cloud.google.com/dataflow/docs/guides/write-to-bigquery?hl=zh-cn", "abstract": "# Dataflow - \u5f9e Dataflow \u5beb\u5165 BigQuery\n\u672c\u6587\u6a94\u4ecb\u7d39\u5982\u4f55\u4f7f\u7528 Apache Beam [BigQuery I/O \u9023\u63a5\u5668](https://beam.apache.org/documentation/io/built-in/google-bigquery/) \u5c07\u6578\u64da\u5f9e Dataflow \u5beb\u5165 BigQuery\u3002\nApache Beam SDK \u4e2d\u63d0\u4f9b BigQuery I/O \u9023\u63a5\u5668\u3002\u6211\u5011\u5efa\u8b70\u4f7f\u7528\u6700\u65b0\u7684 SDK \u7248\u672c\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [Apache Beam 2.x SDK](https://cloud.google.com/dataflow/docs/support/sdk-version-support-status?hl=zh-cn#java) \u3002\n\u6211\u5011\u9084\u63d0\u4f9b\u4e86\u91dd\u5c0d Python \u7684 [\u8de8\u8a9e\u8a00\u652f\u6301](https://beam.apache.org/documentation/programming-guide/#multi-language-pipelines) \u3002\n**\u6ce8\u610f** \uff1a\u6839\u64da\u60a8\u7684\u5834\u666f\uff0c\u8acb\u8003\u616e\u4f7f\u7528 [Google \u63d0\u4f9b\u7684 Dataflow \u6a21\u677f](https://cloud.google.com/dataflow/docs/guides/templates/provided-templates?hl=zh-cn) \u4e4b\u4e00\u3002 \u5176\u4e2d\u4e00\u4e9b\u6a21\u677f\u6703\u5c07\u6578\u64da\u5beb\u5165 BigQuery\u3002\u5982\u679c\u60a8\u8981\u5c07\u6578\u64da\u5f9e Pub/Sub \u6ce8\u5165\u5230 BigQuery\uff0c\u8acb\u8003\u616e\u4f7f\u7528 Pub/Sub [BigQuery \u8a02\u95b1](https://cloud.google.com/pubsub/docs/bigquery?hl=zh-cn) \u3002\n", "content": "## \u6982\u89bd\nBigQuery I/O \u9023\u63a5\u5668\u652f\u6301\u4f7f\u7528\u4ee5\u4e0b\u65b9\u6cd5\u5c07\u6578\u64da\u5beb\u5165 BigQuery\uff1a\n- `STORAGE_WRITE_API`\u3002\u5728\u6b64\u6a21\u5f0f\u4e0b\uff0c\u9023\u63a5\u5668\u4f7f\u7528 [BigQuery Storage Write API](https://cloud.google.com/bigquery/docs/write-api?hl=zh-cn) \u76f4\u63a5\u5c07\u6578\u64da\u5beb\u5165 BigQuery \u5b58\u5132\u3002Storage Write API \u6703\u5c07\u6d41\u5f0f\u6ce8\u5165\u548c\u6279\u91cf\u52a0\u8f09\u6574\u5408\u5230\u4e00\u500b\u9ad8\u6027\u80fd API \u4e2d\u3002\u6b64\u6a21\u5f0f\u53ef\u4fdd\u8b49\u201c\u6b63\u597d\u4e00\u6b21\u201d\u8a9e\u7fa9\u3002\n- `STORAGE_API_AT_LEAST_ONCE`\u3002\u6b64\u6a21\u5f0f\u4e5f\u4f7f\u7528 Storage Write API\uff0c\u4f46\u63d0\u4f9b\u201c\u81f3\u5c11\u4e00\u6b21\u201d\u8a9e\u7fa9\u3002\u6b64\u6a21\u5f0f\u53ef\u7e2e\u77ed\u5927\u591a\u6578\u6d41\u6c34\u7dda\u7684\u5ef6\u9072\u6642\u9593\u3002\u4f46\u662f\uff0c\u4e5f\u53ef\u4ee5\u91cd\u8907\u5beb\u5165\u3002\n- `FILE_LOADS`\u3002\u5728\u6b64\u6a21\u5f0f\u4e0b\uff0c\u9023\u63a5\u5668\u5c07\u8f38\u5165\u6578\u64da\u5beb\u5165 Cloud Storage \u4e2d\u7684\u66ab\u5b58\u6587\u4ef6\u3002\u7136\u5f8c\uff0c\u9023\u63a5\u5668\u904b\u884c BigQuery [\u52a0\u8f09\u4f5c\u696d](https://cloud.google.com/bigquery/docs/batch-loading-data?hl=zh-cn) \u4ee5\u5c07\u6578\u64da\u52a0\u8f09\u5230 BigQuery \u4e2d\u3002 \u8a72\u6a21\u5f0f\u662f\u6709\u754c\u9650`PCollections`\u7684\u9ed8\u8a8d\u6a21\u5f0f\uff0c\u6700\u5e38\u898b\u65bc\u6279\u8655\u7406\u6d41\u6c34\u7dda\u4e2d\u3002\n- `STREAMING_INSERTS`\u3002\u5728\u6b64\u6a21\u5f0f\u4e2d\uff0c\u9023\u63a5\u5668\u4f7f\u7528 [\u820a\u7248\u6d41\u5f0f\u63d2\u5165 API](https://cloud.google.com/bigquery/docs/streaming-data-into-bigquery?hl=zh-cn) \u3002\u6b64\u6a21\u5f0f\u662f\u7121\u754c\u9650`PCollections`\u7684\u9ed8\u8a8d\u6a21\u5f0f\uff0c\u4f46\u4e0d\u5efa\u8b70\u7528\u65bc\u65b0\u9805\u76ee\u3002\n\u9078\u64c7\u5beb\u5165\u65b9\u6cd5\u6642\uff0c\u8acb\u8003\u616e\u4ee5\u4e0b\u5e7e\u9ede\uff1a\n- \u8acb\u8003\u616e\u4f7f\u7528`STORAGE_WRITE_API`\u6216`STORAGE_API_AT_LEAST_ONCE`\uff0c\u5c24\u5176\u662f\u5c0d\u65bc\u6d41\u5f0f\u8655\u7406\u6d41\u6c34\u7dda\u3002Storage Write API \u6bd4\u6587\u4ef6\u52a0\u8f09\u66f4\u9ad8\u6548\uff0c\u56e0\u7232\u5b83\u5c07\u6578\u64da\u76f4\u63a5\u5beb\u5165 BigQuery \u5b58\u5132\uff0c\u4e0d\u9700\u8981\u5728\u7576\u4e2d\u66ab\u5b58\u6587\u4ef6\u3002\u5b83\u652f\u6301\u6279\u8655\u7406\u548c\u6d41\u8655\u7406\u6d41\u6c34\u7dda\u3002\n- \u5982\u679c\u4f7f\u7528 [\u201c\u81f3\u5c11\u4e00\u6b21\u201d\u6d41\u8655\u7406\u6a21\u5f0f](https://cloud.google.com/dataflow/docs/guides/streaming-modes?hl=zh-cn) \u904b\u884c\u6d41\u6c34\u7dda\uff0c\u8acb\u5c07\u5beb\u5165\u6a21\u5f0f\u8a2d\u7f6e\u7232`STORAGE_API_AT_LEAST_ONCE`\u3002\u6b64\u8a2d\u7f6e\u66f4\u9ad8\u6548\uff0c\u4e26\u4e14\u8207\u201c\u81f3\u5c11\u4e00\u6b21\u201d\u6d41\u8655\u7406\u6a21\u5f0f\u7684\u8a9e\u7fa9\u76f8\u5339\u914d\u3002\n- \u6587\u4ef6\u52a0\u8f09\u548c Storage Write API \u5177\u6709\u4e0d\u540c\u7684 [\u914d\u984d\u548c\u9650\u5236](https://cloud.google.com/bigquery/quotas?hl=zh-cn) \u3002\n- \u52a0\u8f09\u4f5c\u696d\u4f7f\u7528\u5171\u4eab BigQuery \u69fd\u6c60\u6216\u9810\u7559\u69fd\u3002\u5982\u9700\u4f7f\u7528\u9810\u7559\u69fd\uff0c\u8acb\u5728\u9810\u7559\u5206\u914d\u985e\u578b\u7232`PIPELINE`\u7684\u9805\u76ee\u4e2d\u904b\u884c\u52a0\u8f09\u4f5c\u696d\u3002\u5982\u679c\u60a8\u4f7f\u7528\u5171\u4eab BigQuery \u69fd\u6c60\uff0c\u5247\u52a0\u8f09\u4f5c\u696d\u662f\u514d\u8cbb\u7684\u3002\u4f46\u662f\uff0cBigQuery \u4e0d\u4fdd\u8b49\u5171\u4eab\u6c60\u7684\u53ef\u7528\u5bb9\u91cf\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u9810\u7559\u7c21\u4ecb](https://cloud.google.com/bigquery/docs/reservations-intro?hl=zh-cn) \u3002## \u4e26\u884c\u6578\u91cf\n- \u5c0d\u65bc\u6d41\u5f0f\u8655\u7406\u6d41\u6c34\u7dda\u4e2d\u7684 `FILE_LOADS` \u548c `STORAGE_WRITE_API` \uff0c\u9023\u63a5\u5668\u6703\u5c07\u6578\u64da\u5206\u7247\u7232\u591a\u500b\u6587\u4ef6\u6216\u6d41\u3002\u901a\u5e38\uff0c\u6211\u5011\u5efa\u8b70\u8abf\u7528 [withAutoSharding](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#withAutoSharding--) \u4ee5\u5553\u7528\u81ea\u52d5\u5206\u7247\u3002\n- \u5c0d\u65bc\u6279\u8655\u7406\u6d41\u6c34\u7dda\u4e2d\u7684 `FILE_LOADS` \uff0c\u9023\u63a5\u5668\u6703\u5c07\u6578\u64da\u5beb\u5165\u5206\u5340\u6587\u4ef6\uff0c\u7136\u5f8c\u5206\u5340\u6587\u4ef6\u6703\u4e26\u884c\u52a0\u8f09\u5230 BigQuery \u4e2d\u3002\n- \u5c0d\u65bc\u6279\u8655\u7406\u6d41\u6c34\u7dda\u4e2d\u7684 `STORAGE_WRITE_API` \uff0c\u6bcf\u500b\u5de5\u4f5c\u5668\u90fd\u6703\u5275\u5efa\u4e00\u500b\u6216\u591a\u500b\u8981\u5beb\u5165 BigQuery \u7684\u6d41\uff08\u7531\u5206\u7247\u7e3d\u6578\u6c7a\u5b9a\uff09\u3002\n- \u5c0d\u65bc `STORAGE_API_AT_LEAST_ONCE` \uff0c\u6709\u55ae\u500b [\u9ed8\u8a8d\u5beb\u5165\u6d41](https://cloud.google.com/bigquery/docs/write-api?hl=zh-cn#default_stream) \u3002\u591a\u500b\u5de5\u4f5c\u5668\u9644\u52a0\u5230\u6b64\u5beb\u5165\u6d41\u3002## \u6027\u80fd\n\u4e0b\u8868\u986f\u793a\u4e86\u5404\u7a2e BigQuery I/O \u8b80\u53d6\u9078\u9805\u7684\u6027\u80fd\u6307\u6a19\u3002\u5de5\u4f5c\u8ca0\u8f09\u4f7f\u7528 Java \u7248 Apache Beam SDK 2.49.0 \u5728\u4e00\u500b `e2-standard2` \u5de5\u4f5c\u5668\u4e0a\u904b\u884c\u3002\u5b83\u5011\u672a\u4f7f\u7528 Runner v2\u3002\n| 1 \u5104\u689d\u8a18\u9304 | 1 KB | 1 \u5217 | \u541e\u5410\u91cf\uff08\u5b57\u7bc0\uff09 | \u541e\u5410\u91cf\uff08\u5143\u7d20\uff09  |\n|:---------------------------|:-----------------|:-------------------|\n| Storage Write    | 55 MBps   | \u6bcf\u79d2 54,000 \u500b\u5143\u7d20 |\n| Avro Load     | 78 MBps   | \u6bcf\u79d2 77,000 \u500b\u5143\u7d20 |\n| Json Load     | 54 MBps   | \u6bcf\u79d2 53,000 \u500b\u5143\u7d20 |\n\u9019\u4e9b\u6307\u6a19\u57fa\u65bc\u7c21\u55ae\u7684\u6279\u8655\u7406\u6d41\u6c34\u7dda\u3002\u5b83\u5011\u65e8\u5728\u6bd4\u8f03 I/O \u9023\u63a5\u5668\u4e4b\u9593\u7684\u6027\u80fd\uff0c\u4e0d\u4e00\u5b9a\u4ee3\u8868\u5be6\u969b\u6d41\u6c34\u7dda\u3002Dataflow \u6d41\u6c34\u7dda\u6027\u80fd\u5f88\u8907\u96dc\uff0c\u5b83\u53d7\u5230\u591a\u500b\u56e0\u7d20\u7684\u5f71\u97ff\uff0c\u5305\u62ec\u865b\u64ec\u6a5f\u985e\u578b\u3001\u6b63\u5728\u8655\u7406\u7684\u6578\u64da\u91cf\u3001\u5916\u90e8\u4f86\u6e90\u548c\u63a5\u6536\u5668\u7684\u6027\u80fd\u4ee5\u53ca\u7528\u6236\u4ee3\u78bc\u3002\u6307\u6a19\u57fa\u65bc\u904b\u884c Java SDK\uff0c\u4e0d\u4ee3\u8868\u5176\u4ed6\u8a9e\u8a00 SDK \u7684\u6027\u80fd\u7279\u5fb5\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [Beam IO \u6027\u80fd](https://beam.apache.org/performance/) \u3002\n## \u6700\u4f73\u5be6\u8e10\n### \u5e38\u898f\n- Storage Write API \u5177\u6709 [\u914d\u984d\u9650\u5236](https://cloud.google.com/bigquery/quotas?hl=zh-cn#storage-limits) \u3002\u5c0d\u65bc\u5927\u591a\u6578\u6d41\u6c34\u7dda\uff0c\u9023\u63a5\u5668\u6703\u8655\u7406\u9019\u4e9b\u9650\u5236\u3002\u4f46\u662f\uff0c\u67d0\u4e9b\u5834\u666f\u53ef\u80fd\u6703\u8017\u76e1\u53ef\u7528\u7684 Storage Write API \u6d41\u3002\u4f8b\u5982\uff0c\u5982\u679c\u6d41\u6c34\u7dda\u4f7f\u7528\u81ea\u52d5\u5206\u7247\u548c\u81ea\u52d5\u64f4\u7e2e\u4e26\u5177\u6709\u5927\u91cf\u76ee\u6a19\u4f4d\u7f6e\uff0c\u5247\u53ef\u80fd\u6703\u767c\u751f\u6b64\u554f\u984c\uff0c\u5c24\u5176\u662f\u5728\u5177\u6709\u591a\u8b8a\u5de5\u4f5c\u8ca0\u8f09\u7684\u9577\u6642\u9593\u904b\u884c\u7684\u4f5c\u696d\u4e2d\u3002\u5982\u679c\u767c\u751f\u6b64\u554f\u984c\uff0c\u8acb\u8003\u616e\u4f7f\u7528 `STORAGE_WRITE_API_AT_LEAST_ONCE` \u4f86\u907f\u514d\u6b64\u554f\u984c\u3002\n- \u4f7f\u7528 [Google Cloud \u6307\u6a19](https://cloud.google.com/monitoring/api/metrics_gcp?hl=zh-cn#gcp-bigquerystorage) \u4f86\u76e3\u63a7 Storage Write API \u914d\u984d\u7528\u91cf\u3002\n- \u4f7f\u7528\u6587\u4ef6\u52a0\u8f09\u6642\uff0cAvro \u7684\u6027\u80fd\u901a\u5e38\u512a\u65bc JSON\u3002\u5982\u9700\u4f7f\u7528 Avro\uff0c\u8acb\u8abf\u7528 [withAvroFormatFunction](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#withAvroFormatFunction-org.apache.beam.sdk.transforms.SerializableFunction-) \u3002\n- \u9ed8\u8a8d\u60c5\u6cc1\u4e0b\uff0c\u52a0\u8f09\u4f5c\u696d\u8207 Dataflow \u4f5c\u696d\u5728\u540c\u4e00\u9805\u76ee\u4e2d\u904b\u884c\u3002\u5982\u9700\u6307\u5b9a\u5176\u4ed6\u9805\u76ee\uff0c\u8acb\u8abf\u7528 [withLoadJobProjectId](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#withLoadJobProjectId-java.lang.String-) \u3002\n- \u4f7f\u7528 Java SDK \u6642\uff0c\u8acb\u8003\u616e\u5275\u5efa\u4e00\u500b\u8868\u793a BigQuery \u8868\u67b6\u69cb\u7684\u985e\u3002\u7136\u5f8c\uff0c\u5728\u6d41\u6c34\u7dda\u4e2d\u8abf\u7528 [useBeamSchema](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#useBeamSchema--) \uff0c\u4ee5\u4fbf\u5728 Apache Beam `Row` \u548c BigQuery `TableRow` \u985e\u578b\u4e4b\u9593\u81ea\u52d5\u8f49\u63db\u3002\u5982\u9700\u67e5\u770b\u67b6\u69cb\u985e\u7684\u793a\u4f8b\uff0c\u8acb\u53c3\u95b1 [ExampleModel.java](https://github.com/GoogleCloudPlatform/cloud-code-samples/blob/v1/java/java-dataflow-samples/read-pubsub-write-bigquery/src/main/java/com/cloudcode/dataflow/ExampleModel.java) \u3002\n- \u5982\u679c\u8981\u52a0\u8f09\u5177\u6709\u5305\u542b\u6578\u5343\u500b\u5b57\u6bb5\u7684\u8907\u96dc\u67b6\u69cb\u7684\u8868\uff0c\u8acb\u8003\u616e\u8abf\u7528 [withMaxBytesPerPartition](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#withMaxBytesPerPartition-long-) \u4f86\u7232\u6bcf\u500b\u52a0\u8f09\u4f5c\u696d\u8a2d\u7f6e\u8f03\u5c0f\u7684\u5927\u5c0f\u4e0a\u9650\u3002\n### \u6d41\u5f0f\u8655\u7406\n- \u5c0d\u65bc\u6d41\u5f0f\u8655\u7406\u6d41\u6c34\u7dda\uff0c\u6211\u5011\u5efa\u8b70\u4f7f\u7528 Storage Write API\uff08 `STORAGE_WRITE_API` \u6216 `STORAGE_API_AT_LEAST_ONCE` \uff09\u3002\n- \u6d41\u5f0f\u8655\u7406\u6d41\u6c34\u7dda\u53ef\u4ee5\u4f7f\u7528\u6587\u4ef6\u52a0\u8f09\uff0c\u4f46\u9019\u7a2e\u65b9\u6cd5\u6709\u4ee5\u4e0b\u7f3a\u9ede\uff1a- \u5b83\u9700\u8981\u4f7f\u7528 [\u6578\u64da\u9078\u53d6](https://cloud.google.com/dataflow/docs/concepts/beam-programming-model?hl=zh-cn#advanced_concepts) \u529f\u80fd\u624d\u80fd\u5beb\u5165\u6587\u4ef6\u3002\u60a8\u7121\u6cd5\u4f7f\u7528\u5168\u5c40\u7a97\u53e3\u3002\n- \u4f7f\u7528 [\u5171\u4eab\u69fd\u6c60](https://cloud.google.com/bigquery/pricing?hl=zh-cn#free) \u6642\uff0cBigQuery \u6703\u76e1\u529b\u800c\u7232\u5730\u52a0\u8f09\u6587\u4ef6\u3002\u5beb\u5165\u8a18\u9304\u8207\u8a18\u9304\u5728 BigQuery \u4e2d\u53ef\u7528\u4e4b\u9593\u53ef\u80fd\u5b58\u5728\u660e\u986f\u5ef6\u9072\u3002\n- \u5982\u679c\u52a0\u8f09\u4f5c\u696d\u5931\u6557\uff08\u4f8b\u5982\u7531\u65bc\u6578\u64da\u932f\u8aa4\u6216\u67b6\u69cb\u4e0d\u5339\u914d\uff09\uff0c\u5247\u6574\u500b\u6d41\u6c34\u7dda\u5c07\u6703\u5931\u6557\u3002\n- \u8acb\u8003\u616e\u5118\u53ef\u80fd\u4f7f\u7528 `STORAGE_WRITE_API_AT_LEAST_ONCE` \u3002\u9019\u53ef\u80fd\u6703\u5c0e\u81f4\u5c07\u91cd\u8907\u7684\u8a18\u9304\u5beb\u5165 BigQuery\uff0c\u4f46\u8cbb\u7528\u6bd4 `STORAGE_WRITE_API` \u4f4e\u3002\n- \u4e00\u822c\u800c\u8a00\uff0c\u8acb\u907f\u514d\u4f7f\u7528 `STREAMING_INSERTS` \u3002\u6d41\u5f0f\u63d2\u5165\u6bd4 Storage Write API \u8981\u8cb4\uff0c\u4e26\u4e14\u6027\u80fd\u66f4\u4f4e\u3002\n- \u6578\u64da\u5206\u7247\u53ef\u4ee5\u63d0\u9ad8\u6d41\u5f0f\u8655\u7406\u6d41\u6c34\u7dda\u7684\u6027\u80fd\u3002\u5c0d\u65bc\u5927\u591a\u6578\u6d41\u6c34\u7dda\u800c\u8a00\uff0c\u81ea\u52d5\u5206\u7247\u662f\u4e00\u500b\u5f88\u597d\u7684\u8d77\u9ede\u3002\u4f46\u662f\uff0c\u60a8\u53ef\u4ee5\u6309\u5982\u4e0b\u65b9\u5f0f\u8abf\u6574\u5206\u7247\uff1a- \u5c0d\u65bc`STORAGE_WRITE_API`\uff0c\u8acb\u8abf\u7528 [withNumStorageWriteApiStreams](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#withNumStorageWriteApiStreams-int-) \u4ee5\u8a2d\u7f6e\u5beb\u5165\u6d41\u7684\u6578\u91cf\u3002\n- \u5c0d\u65bc`FILE_LOADS`\uff0c\u8acb\u8abf\u7528 [withNumFileShards](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#withNumFileShards-int-) \u4ee5\u8a2d\u7f6e\u6587\u4ef6\u5206\u7247\u6578\u91cf\u3002\n- \u5982\u679c\u60a8\u4f7f\u7528\u6d41\u5f0f\u63d2\u5165\uff0c\u6211\u5011\u5efa\u8b70\u60a8\u5c07 [retryTransientErrors](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/InsertRetryPolicy.html#retryTransientErrors--) \u8a2d\u7f6e\u7232 [\u91cd\u8a66\u653f\u7b56](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#withFailedInsertRetryPolicy-org.apache.beam.sdk.io.gcp.bigquery.InsertRetryPolicy-) \u3002\n### \u8655\u7406\u884c\u7d1a\u932f\u8aa4\n\u672c\u90e8\u5206\u4ecb\u7d39\u77ad\u5982\u4f55\u8655\u7406\u53ef\u80fd\u5728\u884c\u7d1a\u5c64\u767c\u751f\u7684\u932f\u8aa4\uff0c\u4f8b\u5982\u8f38\u5165\u6578\u64da\u683c\u5f0f\u932f\u8aa4\u6216\u67b6\u69cb\u4e0d\u5339\u914d\u3002\n\u5c0d\u65bc Storage Write API\uff0c\u6240\u6709\u7121\u6cd5\u5beb\u5165\u7684\u884c\u90fd\u6703\u88ab\u653e\u5165\u55ae\u7368\u7684 `PCollection` \u4e2d\u3002\u5982\u9700\u7372\u53d6\u6b64\u96c6\u5408\uff0c\u8acb\u5c0d `WriteResult` \u5c0d\u8c61\u8abf\u7528 [getFailedStorageApiInserts](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/WriteResult.html#getFailedStorageApiInserts--) \u3002\u5982\u9700\u67e5\u770b\u6b64\u65b9\u6cd5\u7684\u793a\u4f8b\uff0c\u8acb\u53c3\u95b1 [\u5c07\u6578\u64da\u6d41\u5f0f\u63d2\u5165 BigQuery](#stream-data) \u3002\n\u6700\u597d\u5c07\u932f\u8aa4\u767c\u9001\u5230\u6b7b\u4fe1\u968a\u5217\u6216\u8868\uff0c\u4ee5\u4f9b\u65e5\u5f8c\u9032\u884c\u8655\u7406\u3002\u5982\u9700\u8a73\u7d30\u77ad\u89e3\u6b64\u6a21\u5f0f\uff0c\u8acb\u53c3\u95b1 [BigQueryIO \u6b7b\u4fe1\u6a21\u5f0f](https://beam.apache.org/documentation/patterns/bigqueryio/#bigqueryio-deadletter-pattern) \u3002\n\u5c0d\u65bc `FILE_LOADS` \uff0c\u5982\u679c\u5728\u52a0\u8f09\u6578\u64da\u6642\u767c\u751f\u932f\u8aa4\uff0c\u5247\u52a0\u8f09\u4f5c\u696d\u6703\u5931\u6557\uff0c\u4e26\u4e14\u6d41\u6c34\u7dda\u6703\u62cb\u51fa\u904b\u884c\u6642\u7570\u5e38\u3002\u60a8\u53ef\u4ee5\u5728 Dataflow \u65e5\u8a8c\u4e2d\u67e5\u770b\u932f\u8aa4\u6216\u67e5\u770b BigQuery \u4f5c\u696d\u6b77\u53f2\u8a18\u9304\u3002I/O \u9023\u63a5\u5668\u4e0d\u6703\u8fd4\u56de\u500b\u5225\u5931\u6557\u884c\u7684\u76f8\u95dc\u4fe1\u606f\u3002\n\u5982\u9700\u8a73\u7d30\u77ad\u89e3\u5982\u4f55\u6392\u67e5\u932f\u8aa4\uff0c\u8acb\u53c3\u95b1 [BigQuery \u9023\u63a5\u5668\u932f\u8aa4](https://cloud.google.com/dataflow/docs/guides/common-errors?hl=zh-cn#connector_errors) \u3002\n## \u793a\u4f8b\n### \u5beb\u5165\u73fe\u6709\u8868\n\u4ee5\u4e0b\u793a\u4f8b\u6703\u5275\u5efa\u4e00\u500b\u6279\u8655\u7406\u6d41\u6c34\u7dda\uff0c\u8a72\u6d41\u6c34\u7dda\u5c07 `PCollection<MyData>` \u5beb\u5165 BigQuery\uff0c\u5176\u4e2d `MyData` \u662f\u81ea\u5b9a\u7fa9\u6578\u64da\u985e\u578b\u3002\n[BigQueryIO.<T>write()](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.html#write--) \u65b9\u6cd5\u6703\u8fd4\u56de [BigQueryIO.Write<T>](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html) \u985e\u578b\uff0c\u7528\u65bc\u914d\u7f6e\u5beb\u5165\u64cd\u4f5c\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 Apache Beam \u6587\u6a94\u4e2d\u7684 [\u5beb\u5165\u8868](https://beam.apache.org/documentation/io/built-in/google-bigquery/#writing-to-a-table) \u3002\u6b64\u4ee3\u78bc\u793a\u4f8b\u6703\u5c07\u6578\u64da\u5beb\u5165\u73fe\u6709\u8868 ( `CREATE_NEVER` ) \u4e26\u5c07\u65b0\u884c\u9644\u52a0\u5230\u8868 ( `WRITE_APPEND` )\u3002\n\u5982\u9700\u5411 Dataflow \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002  \u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BigQueryWrite.java) \n```\nimport com.google.api.services.bigquery.model.TableRow;import java.util.Arrays;import java.util.List;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.coders.DefaultCoder;import org.apache.beam.sdk.extensions.avro.coders.AvroCoder;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.WriteDisposition;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.Create;public class BigQueryWrite {\u00a0 // A custom datatype for the source data.\u00a0 @DefaultCoder(AvroCoder.class)\u00a0 public static class MyData {\u00a0 \u00a0 public String name;\u00a0 \u00a0 public Long age;\u00a0 \u00a0 public MyData() {}\u00a0 \u00a0 public MyData(String name, Long age) {\u00a0 \u00a0 \u00a0 this.name = name;\u00a0 \u00a0 \u00a0 this.age = age;\u00a0 \u00a0 }\u00a0 }\u00a0 public static void main(String[] args) {\u00a0 \u00a0 // Example source data.\u00a0 \u00a0 final List<MyData> data = Arrays.asList(\u00a0 \u00a0 \u00a0 \u00a0 new MyData(\"Alice\", 40L),\u00a0 \u00a0 \u00a0 \u00a0 new MyData(\"Bob\", 30L),\u00a0 \u00a0 \u00a0 \u00a0 new MyData(\"Charlie\", 20L)\u00a0 \u00a0 );\u00a0 \u00a0 // Parse the pipeline options passed into the application. Example:\u00a0 \u00a0 // \u00a0 --projectId=$PROJECT_ID --datasetName=$DATASET_NAME --tableName=$TABLE_NAME\u00a0 \u00a0 // For more information, see https://beam.apache.org/documentation/programming-guide/#configuring-pipeline-options\u00a0 \u00a0 PipelineOptionsFactory.register(ExamplePipelineOptions.class);\u00a0 \u00a0 ExamplePipelineOptions options = PipelineOptionsFactory.fromArgs(args)\u00a0 \u00a0 \u00a0 \u00a0 .withValidation()\u00a0 \u00a0 \u00a0 \u00a0 .as(ExamplePipelineOptions.class);\u00a0 \u00a0 // Create a pipeline and apply transforms.\u00a0 \u00a0 Pipeline pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 // Create an in-memory PCollection of MyData objects.\u00a0 \u00a0 \u00a0 \u00a0 .apply(Create.of(data))\u00a0 \u00a0 \u00a0 \u00a0 // Write the data to an exiting BigQuery table.\u00a0 \u00a0 \u00a0 \u00a0 .apply(BigQueryIO.<MyData>write()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .to(String.format(\"%s:%s.%s\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getProjectId(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getDatasetName(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getTableName()))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withFormatFunction(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (MyData x) -> new TableRow().set(\"user_name\", x.name).set(\"age\", x.age))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withCreateDisposition(CreateDisposition.CREATE_NEVER)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withWriteDisposition(WriteDisposition.WRITE_APPEND)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withMethod(Write.Method.STORAGE_WRITE_API));\u00a0 \u00a0 pipeline.run().waitUntilFinish();\u00a0 }}\n```### \u5beb\u5165\u65b0\u8868\u6216\u73fe\u6709\u8868\n\u4ee5\u4e0b\u793a\u4f8b\u5728\u76ee\u6a19\u8868\u4e0d\u5b58\u5728\u6642\u901a\u904e\u5c07 [\u5275\u5efa\u8655\u7f6e\u65b9\u5f0f](https://beam.apache.org/documentation/io/built-in/google-bigquery/#create-disposition) \u8a2d\u7f6e\u7232 `CREATE_IF_NEEDED` \u4f86\u5275\u5efa\u65b0\u8868\u3002\u4f7f\u7528\u6b64\u9078\u9805\u6642\uff0c\u60a8\u5fc5\u9808\u63d0\u4f9b\u8868\u67b6\u69cb\u3002\u5982\u679c\u5275\u5efa\u65b0\u8868\uff0c\u5247\u9023\u63a5\u5668\u4f7f\u7528\u6b64\u67b6\u69cb\u3002\n\u5982\u9700\u5411 Dataflow \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002  \u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BigQueryWriteWithSchema.java) \n```\nimport com.google.api.services.bigquery.model.TableFieldSchema;import com.google.api.services.bigquery.model.TableRow;import com.google.api.services.bigquery.model.TableSchema;import java.util.Arrays;import java.util.List;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.coders.DefaultCoder;import org.apache.beam.sdk.extensions.avro.coders.AvroCoder;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.Create;public class BigQueryWriteWithSchema {\u00a0 // A custom datatype for the source data.\u00a0 @DefaultCoder(AvroCoder.class)\u00a0 public static class MyData {\u00a0 \u00a0 public String name;\u00a0 \u00a0 public Long age;\u00a0 \u00a0 public MyData() {}\u00a0 \u00a0 public MyData(String name, Long age) {\u00a0 \u00a0 \u00a0 this.name = name;\u00a0 \u00a0 \u00a0 this.age = age;\u00a0 \u00a0 }\u00a0 }\u00a0 public static void main(String[] args) {\u00a0 \u00a0 // Example source data.\u00a0 \u00a0 final List<MyData> data = Arrays.asList(\u00a0 \u00a0 \u00a0 \u00a0 new MyData(\"Alice\", 40L),\u00a0 \u00a0 \u00a0 \u00a0 new MyData(\"Bob\", 30L),\u00a0 \u00a0 \u00a0 \u00a0 new MyData(\"Charlie\", 20L)\u00a0 \u00a0 );\u00a0 \u00a0 // Define a table schema. A schema is required for write disposition CREATE_IF_NEEDED.\u00a0 \u00a0 TableSchema schema = new TableSchema()\u00a0 \u00a0 \u00a0 \u00a0 .setFields(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Arrays.asList(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new TableFieldSchema()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setName(\"user_name\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setType(\"STRING\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMode(\"REQUIRED\"),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new TableFieldSchema()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setName(\"age\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setType(\"INT64\") // Defaults to NULLABLE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 // Parse the pipeline options passed into the application. Example:\u00a0 \u00a0 // \u00a0 --projectId=$PROJECT_ID --datasetName=$DATASET_NAME --tableName=$TABLE_NAME\u00a0 \u00a0 // For more information, see https://beam.apache.org/documentation/programming-guide/#configuring-pipeline-options\u00a0 \u00a0 PipelineOptionsFactory.register(ExamplePipelineOptions.class);\u00a0 \u00a0 ExamplePipelineOptions options = PipelineOptionsFactory.fromArgs(args)\u00a0 \u00a0 \u00a0 \u00a0 .withValidation()\u00a0 \u00a0 \u00a0 \u00a0 .as(ExamplePipelineOptions.class);\u00a0 \u00a0 // Create a pipeline and apply transforms.\u00a0 \u00a0 Pipeline pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 // Create an in-memory PCollection of MyData objects.\u00a0 \u00a0 \u00a0 \u00a0 .apply(Create.of(data))\u00a0 \u00a0 \u00a0 \u00a0 // Write the data to a new or existing BigQuery table.\u00a0 \u00a0 \u00a0 \u00a0 .apply(BigQueryIO.<MyData>write()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .to(String.format(\"%s:%s.%s\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getProjectId(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getDatasetName(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getTableName()))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withFormatFunction(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (MyData x) -> new TableRow().set(\"user_name\", x.name).set(\"age\", x.age))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withSchema(schema)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withMethod(Write.Method.STORAGE_WRITE_API)\u00a0 \u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 pipeline.run().waitUntilFinish();\u00a0 }}\n```### \u5c07\u6578\u64da\u6d41\u5f0f\u63d2\u5165\u5230 BigQuery\n\u4ee5\u4e0b\u793a\u4f8b\u5c55\u793a\u77ad\u5982\u4f55\u901a\u904e\u5c07\u5beb\u5165\u6a21\u5f0f\u8a2d\u7f6e\u7232 `STORAGE_WRITE_API` \uff0c\u4f7f\u7528\u201c\u6b63\u597d\u4e00\u6b21\u201d\u8a9e\u7fa9\u4f86\u6d41\u5f0f\u63d2\u5165\u6578\u64da\n\u4e26\u975e\u6240\u6709\u6d41\u8655\u7406\u6d41\u6c34\u7dda\u90fd\u9700\u8981\u201c\u6b63\u597d\u4e00\u6b21\u201d\u8a9e\u7fa9\u3002\u4f8b\u5982\uff0c\u60a8\u53ef\u4ee5\u5f9e\u76ee\u6a19\u8868\u4e2d [\u624b\u52d5\u79fb\u9664\u91cd\u8907\u9805](https://cloud.google.com/bigquery/docs/streaming-data-into-bigquery?hl=zh-cn#manually_removing_duplicates) \u3002\u5982\u679c\u60a8\u7684\u5834\u666f\u53ef\u4ee5\u63a5\u53d7\u91cd\u8907\u8a18\u9304\u7684\u53ef\u80fd\u6027\uff0c\u8acb\u8003\u616e\u5c07 [\u5beb\u5165\u65b9\u6cd5](#write_method) \u8a2d\u7f6e\u7232 `STORAGE_API_AT_LEAST_ONCE` \u4f86\u4f7f\u7528\u201c\u81f3\u5c11\u4e00\u6b21\u201d\u8a9e\u7fa9\u3002\u6b64\u65b9\u6cd5\u901a\u5e38\u66f4\u9ad8\u6548\uff0c\u53ef\u7e2e\u77ed\u5927\u591a\u6578\u6d41\u6c34\u7dda\u7684\u5ef6\u9072\u6642\u9593\u3002\n\u5982\u9700\u5411 Dataflow \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002  \u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BigQueryStreamExactlyOnce.java) \n```\nimport com.google.api.services.bigquery.model.TableRow;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.PipelineResult;import org.apache.beam.sdk.coders.StringUtf8Coder;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.WriteDisposition;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.testing.TestStream;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.values.TimestampedValue;import org.apache.beam.sdk.values.TypeDescriptor;import org.apache.beam.sdk.values.TypeDescriptors;import org.joda.time.Duration;import org.joda.time.Instant;public class BigQueryStreamExactlyOnce {\u00a0 // Create a PTransform that sends simulated streaming data. In a real application, the data\u00a0 // source would be an external source, such as Pub/Sub.\u00a0 private static TestStream<String> createEventSource() {\u00a0 \u00a0 Instant startTime = new Instant(0);\u00a0 \u00a0 return TestStream.create(StringUtf8Coder.of())\u00a0 \u00a0 \u00a0 \u00a0 .advanceWatermarkTo(startTime)\u00a0 \u00a0 \u00a0 \u00a0 .addElements(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TimestampedValue.of(\"Alice,20\", startTime),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TimestampedValue.of(\"Bob,30\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 startTime.plus(Duration.standardSeconds(1))),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TimestampedValue.of(\"Charles,40\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 startTime.plus(Duration.standardSeconds(2))),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TimestampedValue.of(\"Dylan,Invalid value\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 startTime.plus(Duration.standardSeconds(2))))\u00a0 \u00a0 \u00a0 \u00a0 .advanceWatermarkToInfinity();\u00a0 }\u00a0 public static PipelineResult main(String[] args) {\u00a0 \u00a0 // Parse the pipeline options passed into the application. Example:\u00a0 \u00a0 // \u00a0 --projectId=$PROJECT_ID --datasetName=$DATASET_NAME --tableName=$TABLE_NAME\u00a0 \u00a0 // For more information, see https://beam.apache.org/documentation/programming-guide/#configuring-pipeline-options\u00a0 \u00a0 PipelineOptionsFactory.register(ExamplePipelineOptions.class);\u00a0 \u00a0 ExamplePipelineOptions options = PipelineOptionsFactory.fromArgs(args)\u00a0 \u00a0 \u00a0 \u00a0 .withValidation()\u00a0 \u00a0 \u00a0 \u00a0 .as(ExamplePipelineOptions.class);\u00a0 \u00a0 options.setStreaming(true);\u00a0 \u00a0 // Create a pipeline and apply transforms.\u00a0 \u00a0 Pipeline pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 // Add a streaming data source.\u00a0 \u00a0 \u00a0 \u00a0 .apply(createEventSource())\u00a0 \u00a0 \u00a0 \u00a0 // Map the event data into TableRow objects.\u00a0 \u00a0 \u00a0 \u00a0 .apply(MapElements\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .into(TypeDescriptor.of(TableRow.class))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via((String x) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String[] columns = x.split(\",\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return new TableRow().set(\"user_name\", columns[0]).set(\"age\", columns[1]);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }))\u00a0 \u00a0 \u00a0 \u00a0 // Write the rows to BigQuery\u00a0 \u00a0 \u00a0 \u00a0 .apply(BigQueryIO.writeTableRows()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .to(String.format(\"%s:%s.%s\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getProjectId(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getDatasetName(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getTableName()))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withCreateDisposition(CreateDisposition.CREATE_NEVER)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withWriteDisposition(WriteDisposition.WRITE_APPEND)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withMethod(Write.Method.STORAGE_WRITE_API)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // For exactly-once processing, set the triggering frequency.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withTriggeringFrequency(Duration.standardSeconds(5)))\u00a0 \u00a0 \u00a0 \u00a0 // Get the collection of write errors.\u00a0 \u00a0 \u00a0 \u00a0 .getFailedStorageApiInserts()\u00a0 \u00a0 \u00a0 \u00a0 .apply(MapElements.into(TypeDescriptors.strings())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Process each error. In production systems, it's useful to write the errors to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // another destination, such as a dead-letter table or queue.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 x -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"Failed insert: \" + x.getErrorMessage());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"Row: \" + x.getRow());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return \"\";\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }));\u00a0 \u00a0 return pipeline.run();\u00a0 }}\n```\n## \u5f8c\u7e8c\u6b65\u9a5f\n- \u5982\u9700\u8a73\u7d30\u77ad\u89e3 BigQuery I/O \u9023\u63a5\u5668\uff0c\u8acb\u53c3\u95b1 [Apache Beam \u6587\u6a94](https://beam.apache.org/documentation/io/built-in/google-bigquery) \u3002\n- \u77ad\u89e3\u5982\u4f55 [\u4f7f\u7528 Storage Write API \u5c07\u6578\u64da\u6d41\u5f0f\u63d2\u5165\u5230 BigQuery](https://cloud.google.com/blog/products/data-analytics/streaming-data-into-bigquery-using-storage-write-api?hl=zh-cn) \uff08\u535a\u6587\uff09\u3002", "guide": "Dataflow"}