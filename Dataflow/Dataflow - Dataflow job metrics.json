{"title": "Dataflow - Dataflow job metrics", "url": "https://cloud.google.com/dataflow/docs/guides/using-monitoring-intf", "abstract": "# Dataflow - Dataflow job metrics\nYou can view charts in the **Job metrics** tab of the Dataflow page in the Google Cloud console. Each metric is organized into the following dashboards:\nOverview metrics\n- [Autoscaling](#autoscaling) \n- [Throughput](#throughput) \n- [Worker error log count](#worker-error-logs) \nStreaming metrics (streaming pipelines only)\n- [Data freshness (with and without Streaming Engine)](#data_freshness_streaming) \n- [System latency (with and without Streaming Engine)](#system_latency_streaming) \n- [Backlog](#backlog) \n- [Processing (Streaming Engine only)](#processing_streaming) \n- [Parallelism (Streaming Engine only)](#parallelism) \n- [Persistence (Streaming Engine only)](#persistence_streaming) \n- [Duplicates (Streaming Engine only)](#duplicates) \n- [Timers (Streaming Engine only)](#timers) \nResource metrics\n- [CPU utilization](#cpu-use) \n- [Memory utilization](#memory-use) \nInput metrics\n- [Pub/Sub read, BigQuery read, and so on](#input-output) \nOutput metrics\n- [Pub/Sub write, BigQuery write, and so on](#input-output) \nFor more information about scenarios where you can use these metrics for debugging, see [Tools for debugging](/dataflow/docs/guides/troubleshoot-slow-jobs#debug-tools) in \"Troubleshoot slow or stuck jobs.\"\n", "content": "## Support and limitations\nWhen using the Dataflow metrics, be aware of the following details.\n- Sometimes job data is intermittently unavailable. When data is missing, gaps appear in the job monitoring charts.\n- Some of these charts are specific to streaming pipelines.\n- To write metrics data, a [user-managed service account](/dataflow/docs/concepts/security-and-permissions#user-managed) must have the IAM API permission `monitoring.timeSeries.create` . This permission is included with the [Dataflow Worker role](/dataflow/access-control#roles) .\n- The Dataflow service reports the reserved CPU time after jobs complete. For unbounded (streaming) jobs, reserved CPU time is only reported after jobs are cancelled or fail. Therefore, the job metrics don't include reserved CPU time for streaming jobs.## Access job metrics\n- [Sign in](https://console.cloud.google.com/) to the Google Cloud console.\n- Select your Google Cloud project.\n- Open the navigation menu and select **Dataflow** .\n- In the job list, click the name of your job. The **Job details** page opens.\n- Click the **Job metrics** tab.\nTo access additional information in the job metrics charts, click query_stats **Explore data** .\n## Use Cloud Monitoring\nDataflow is fully integrated with Cloud Monitoring. Use Cloud Monitoring for the following tasks:\n- [Create alerts](#create-alerts) when your job exceeds a user-defined threshold.\n- [Use Metrics Explorer](#metrics-explorer) to build queries and adjust the timespan of the metrics.\nFor instructions about creating alerts and using Metrics Explorer, see [Use Cloud Monitoring for Dataflow pipelines](/dataflow/docs/guides/using-cloud-monitoring) .\n### Create Cloud Monitoring alerts\nCloud Monitoring lets you create alerts when your Dataflow job exceeds a user-defined threshold. To create a Cloud Monitoring alert from a metric chart, click **Create alerting policy** .\nIf you're unable to see the monitoring graphs or create alerts, you might need additional [Monitoring permissions](/monitoring/access-control) .\n### View in Metrics Explorer\nYou can view the Dataflow metrics charts in [Metrics Explorer](/monitoring/charts/metrics-explorer) , where you can build queries and adjust the timespan of the metrics.\nTo view the Dataflow charts in Metrics Explorer, in the **Job metrics** view, open more_vert **More chart options** and click **View in Metrics Explorer** .\nWhen you adjust the timespan of the metrics, you can select a predefined duration or select a custom time interval to analyze your job.\nBy default, for streaming jobs and in-flight batch jobs, the display shows the previous six hours of metrics for that job. For stopped or completed streaming jobs, the default display shows the entire runtime of the job duration.\n### Dataflow I/O metrics\nYou can view the following Dataflow I/O metrics in [Metrics Explorer](/monitoring/charts/metrics-explorer) :\n- [job/pubsub/write_count](/monitoring/api/metrics_gcp#dataflow/job/pubsub/write_count) : Pub/Sub publish requests from [PubsubIO.Write](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/pubsub/PubsubIO.Write.html) in Dataflow jobs.\n- [job/pubsub/read_count](/monitoring/api/metrics_gcp#dataflow/job/pubsub/read_count) : Pub/Sub pull requests from [PubsubIO.Read](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/pubsub/PubsubIO.Read.html) in Dataflow jobs.\n- [job/bigquery/write_count](/monitoring/api/metrics_gcp#dataflow/job/bigquery/write_count) : BigQuery publish requests from [BigQueryIO.Write](https://beam.apache.org/releases/javadoc/2.27.0/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html) in Dataflow jobs.`job/bigquery/write_count`metrics are available in Python pipelines using the [WriteToBigQuery transform](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/gcp/bigquery.py) with`method='STREAMING_INSERTS'`enabled on Apache Beam v2.28.0 or later.\n- If your pipeline uses a BigQuery source or sink, to troubleshoot quota issues, use the [BigQuery Storage API metrics](/monitoring/api/metrics_gcp#gcp-bigquerystorage) .\nFor the complete list of Dataflow metrics, see the [Google Cloud metrics documentation](/monitoring/api/metrics_gcp#gcp-dataflow) .\n## Stage and worker metrics\nThe following sections provide details about the stage and worker metrics available in the monitoring interface.\n### Autoscaling\nThe Dataflow service automatically chooses the number of worker instances required to run your [autoscaling](/dataflow/service/dataflow-service-desc#autoscaling) job. The number of worker instances can change over time according to the job requirements.\nTo see the history of autoscaling changes, click the **More History** button. A table with information about the worker history of your pipeline displays.### Throughput\nThroughput is the volume of data that is processed at any point in time. This per-step metric is displayed as the number of elements per second. To see this metric in bytes per second, view the **Throughput (bytes/sec)** chart farther down the page.### Worker error log count\nThe **Worker error log count** shows you the rate of errors observed across all workers at any point in time.### Data freshness (with and without Streaming Engine)\nThe data freshness metric shows the difference in seconds between the timestamp on the data element and the time that the event is processed in your pipeline. The data element receives a timestamp when an event occurs on the element, such as a click event on a website or ingestion by Pub/Sub. The output watermark is the time that the data is processed.\nAt any time, the Dataflow job is processing multiple elements. The data points in the data freshness chart show the element with the largest delay relative to its event time. Therefore, the same line in the chart displays data for multiple elements. Each data point in the line displays data for the slowest element at that stage in the pipeline.\nIf some input data has not yet been processed, the output watermark might be delayed, which affects data freshness. A significant difference between the watermark time and the event time might indicate a [slow or stuck operation](/dataflow/docs/guides/common-errors#processing-stuck) .\nFor recently updated streaming jobs, job state and watermark information might be unavailable. The Update operation makes several changes that take a few minutes to propagate to the Dataflow monitoring interface. Try refreshing the monitoring interface 5 minutes after updating your job.\nFor more information, see [Watermarks and late data](https://beam.apache.org/documentation/programming-guide/#watermarks-and-late-data) in the Apache Beam documentation.\nThe dashboard includes the following two charts:\n- Data freshness by stages\n- Data freshnessIn the preceding image, the highlighted area shows a substantial difference between the event time and the output watermark time, indicating a slow operation.\nHigh data freshness metrics (for example, metrics indicating that data is less fresh) might be caused by:\n- **Performance Bottlenecks** : If your pipeline has stages with high [system latency](#system_latency_streaming) or [logs indicating stuck transforms](/dataflow/docs/guides/common-errors#processing-stuck) , the pipeline might have performance issues that could raise data freshness. Follow these guidelines for [troubleshooting slow pipelines](/dataflow/docs/guides/troubleshoot-slow-jobs) to investigate further.\n- **Data Source Bottlenecks** : If your data sources have growing backlogs, the event timestamps of your elements might diverge from the watermark as they wait to be processed. Large backlogs are often caused either by performance bottlenecks, or data source issues which are best detected by monitoring the sources used by your pipeline.- **Note** : Unordered sources such as Pub/Sub can produce stuck watermarks even while outputting at a high rate. This situation occurs because elements are not output in timestamp order, and the watermark is based on the minimum unprocessed timestamp.\n- **Frequent Retries** : If you see any errors indicating elements failing to process and getting retried, that older timestamps from retried elements might be raising data freshness. The [list of common Dataflow errors](/dataflow/docs/guides/common-errors) can help you troubleshoot.\n### System latency (with and without Streaming Engine)\nSystem latency is the current maximum number of seconds that an item of data has been processing or awaiting processing. This metric indicates how long an element waits inside any one source in the pipeline. The maximum duration is adjusted after processing. The following cases are additional considerations:\n- For multiple sources and sinks, system latency is the maximum amount of time that an element waits inside a source before it's written to all sinks.\n- Sometimes, a source does not provide a value for the time period for which an element waits inside the source. In addition, the element might not have metadata to define its event time. In this scenario, system latency is calculated from the time the pipeline first receives the element.\nThe dashboard includes the following two charts:\n- System latency by stages\n- System latency### Backlog\nThe **Backlog** dashboard provides information about elements waiting to be processed. The dashboard includes the following two charts:\n- Backlog seconds (Streaming Engine only)\n- Backlog bytes (with and without Streaming Engine)\nThe **Backlog seconds** chart shows an estimate of the amount of time in seconds needed to consume the current backlog if no new data arrives and throughput doesn't change. The estimated backlog time is calculated from both the throughput and the backlog bytes from the input source that still need to be processed. This metric is used by the [streaming autoscaling](/dataflow/docs/horizontal-autoscaling#streaming) feature to determine when to scale up or down.\nThe **Backlog bytes** chart shows the amount of known unprocessed input for a stage in bytes. This metric compares the remaining bytes to be consumed by each stage against upstream stages. For this metric to report accurately, each source ingested by the pipeline must be configured correctly. Built-in sources such as Pub/Sub and BigQuery are already supported out of the box, however, custom sources require some extra implementation. For more details, see [autoscaling for custom unbounded sources](/dataflow/docs/horizontal-autoscaling#streaming) .### Processing (Streaming Engine only)\nWhen you run an Apache Beam pipeline on the Dataflow service, pipeline tasks run on worker VMs. The **Processing** dashboard provides information about the amount of time tasks have been processing on the worker VMs. The dashboard includes the following two charts:\n- User processing latencies heatmap\n- User processing latencies by stage\nThe **User processing latencies heatmap** shows the maximum operation latencies over the 50th, 95th, and 99th percentile distributions. Use the heatmap to see whether any long-tail operations are causing high overall system latency or are negatively affecting overall data freshness.\nTo fix an upstream issue before it becomes a problem downstream, set an alerting policy for high latencies in the 50th percentile.\nThe **User processing latencies by stage** chart shows the 99th percentile for all tasks that workers are processing broken down by stage. If user code is causing a bottleneck, this chart shows which stage contains the bottleneck. You can use the following steps to debug the pipeline:\n- Use the chart to find a stage with an unusually high latency.\n- On the job details page, in the **Execution details** tab, for **Graph view** , select **Stage workflow** . In the **Stage workflow** graph, find the stage that has unusually high latency.\n- To find the associated user operations, in the graph, click the node for that stage.\n- To find additional details, navigate to [Cloud Profiler](/dataflow/docs/guides/profiling-a-pipeline) , and use Cloud Profiler to debug the stack trace at the correct time range. Look for the user operations that you identified in the previous step.### Parallelism (Streaming Engine only)\nThe **Parallel processing** chart shows the approximate number of keys in use for data processing for each stage. Dataflow scales based on the parallelism of a pipeline.\nWhen Dataflow runs a pipeline, the processing is distributed across multiple Compute Engine virtual machines (VMs), also known as workers. The Dataflow service automatically parallelizes and distributes the processing logic in your pipeline to the workers. Processing for any given key is serialized, so the total number of keys for a stage represents the maximum available parallelism at that stage.\nParallelism metrics can be useful for finding hot keys or bottlenecks for slow or stuck pipelines.### Persistence (Streaming Engine only)\nThe **Persistence** dashboard provides information about the rate at which persistent storage is written and read by a particular pipeline stage in bytes per second. The dashboard includes the following two charts:\n- Storage write\n- Storage read### Duplicates (Streaming Engine only)\nThe **Duplicates** chart shows the number of messages being processed by a particular stage that have been filtered out as duplicates. Dataflow supports many sources and sinks which guarantee `at least once` delivery. The downside of `at least once` delivery is that it can result in duplicates. Dataflow guarantees `exactly once` delivery, which means that duplicates are automatically filtered out. Downstream stages are saved from reprocessing the same elements, which ensures that state and outputs are not affected. The pipeline can be optimized for resources and performance by reducing the number of duplicates produced in each stage.### Timers (Streaming Engine only)\nThe **Timers** dashboard provides information about the number of timers pending and the number of timers already processed in a particular pipeline stage. Because windows rely on timers, this metric lets you track the progress of windows.\nThe dashboard includes the following two charts:\n- Timers pending by stage\n- Timers processing by stage\nThese charts show the rate at which windows are pending or processing at a specific point in time. The **Timers pending by stage** chart indicates how many windows are delayed due to bottlenecks. The **Timers processing by stage** chart indicates how many windows are collecting elements.\nThese charts display all job timers, so if timers are used elsewhere in your code, those timers also appear in these charts.\n### CPU utilization\nCPU utilization is the amount of CPU used divided by the amount of CPU available for processing. This per-worker metric is displayed as a percentage. The dashboard includes the following four charts:\n- CPU utilization (All workers)\n- CPU utilization (Stats)\n- CPU utilization (Top 4)\n- CPU utilization (Bottom 4)### Memory utilization\nMemory utilization is the estimated amount of memory used by the workers in bytes per second. The dashboard includes the following two charts:\n- Max worker memory utilization (estimated bytes per second)\n- Memory utilization (estimated bytes per second)\nThe **Max worker memory utilization** chart provides information about the workers that use the most memory in the Dataflow job at each point in time. If, at different points during a job, the worker using the maximum amount of memory changes, the same line in the chart displays data for multiple workers. Each data point in the line displays data for the worker using the maximum amount of memory at that time. The chart compares the estimated memory used by the worker to the memory limit in bytes.\nYou can use this chart to troubleshoot out-of-memory (OOM) issues. Worker out-of-memory crashes are not shown on this chart.\nThe **Memory utilization** chart shows an estimate of the memory used by all workers in the Dataflow job compared to the memory limit in bytes.\n## Input and Output Metrics\nIf your streaming Dataflow job reads or writes records using Pub/Sub, input metrics and output metrics display.\n**Note:** The **Input Metrics** and **Output Metrics** sections are displayed only if your job reads or writes data to a source or sink and you're viewing the new Job page. If you don't see these metrics, you must [update any existing streaming jobs](/dataflow/docs/guides/updating-a-pipeline) or launch new jobs using Pub/Sub.\nAll input metrics of the same type are combined, and all output metrics are also combined. For example, all Pub/Sub metrics are grouped in one section. Each metric type is organized into a separate section. To change which metrics are displayed, select the section on the left which best represents the metrics you're looking for. The following images show all the available sections.\nThe following two charts are displayed in both the **Input Metrics** and **Output Metrics** sections.### Requests per second\nRequests per sec is the rate of API requests to read or write data by the source or sink over time. If this rate drops to zero, or decreases significantly for an extended time period relative to expected behavior, then the pipeline might be blocked from performing certain operations. Also, there might be no data to read. In such a case, review the job steps that have a high system watermark. Also, examine the worker logs for errors or indications about slow processing.### Response errors per second by error type\nResponse errors per sec by error type is the rate of failed API requests to read or write data by the source or sink over time. If such errors occur frequently, these API requests might slow down processing. Such failed API requests must be investigated. To help troubleshoot these issues, review the general [I/O error code documentation](/dataflow/docs/guides/input-and-output-error-codes) . Also review any specific error code documentation used by the source or sink, such as the [Pub/Sub error codes](/pubsub/docs/reference/error-codes) .\n**Note:** The chart is empty until the first error occurs.", "guide": "Dataflow"}