{"title": "Dataflow - Create a Dataflow pipeline using Python", "url": "https://cloud.google.com/dataflow/docs/quickstarts/create-pipeline-python", "abstract": "# Dataflow - Create a Dataflow pipeline using Python\n# Create a Dataflow pipeline using Python\nIn this quickstart, you learn how to use the Apache Beam SDK for Python to build a program that defines a pipeline. Then, you run the pipeline by using a direct local runner or a cloud-based runner such as Dataflow. For an introduction to the WordCount pipeline, see the [How to use WordCount in Apache Beam](https://www.youtube.com/watch?v=RTIOW1fIhkM) video.To follow step-by-step guidance for this task directly in the Google Cloud console, click **Guide me** :\n [Guide me](https://console.cloud.google.com/?walkthrough_id=dataflow--quickstart-beam--quickstart-beam-python) ", "content": "## Before you begin\n- Grant roles to your Compute Engine default service account. Run the following command once  for each of the following IAM roles:- `roles/dataflow.admin`\n- `roles/dataflow.worker`\n- `roles/storage.objectAdmin`\n```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com\" --role=SERVICE_ACCOUNT_ROLE\n```- Replace``with your project ID.\n- Replace``with your project number.  To find your project number, see [Identify projects](/resource-manager/docs/creating-managing-projects#identifying_projects) or use the [gcloud projects describe](/sdk/gcloud/reference/projects/describe) command.\n- Replace``with each individual role.\n- Create a Cloud Storage bucket and configure it as follows:- Set the storage class to`S`(Standard).\n- Set the storage location to the following:`US`(United States).\n- Replace``with     a unique bucket name. Don't include sensitive information in the   bucket name because the bucket namespace is global and publicly visible.\n```\ngcloud storage buckets create gs://BUCKET_NAME --default-storage-class STANDARD --location US\n```\n- Copy the Google Cloud project ID and the Cloud Storage bucket name. You need these values later in this document.\n## Set up your environmentDataflow no longer supports pipelines using Python 2. For more information, see [Python 2 support on Google Cloud](/python/docs/python2-sunset#dataflow) page. For details about supported versions, see [Apache Beam runtime support](/dataflow/docs/support/beam-runtime-support) .\nIn this section, use the command prompt to set up an isolated Python virtual environment to run your pipeline project by using [venv](https://docs.python.org/3/library/venv.html) . This process lets you isolate the dependencies of one project from the dependencies of other projects.\nIf you don't have a command prompt readily available, you can use [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) . Cloud Shell already has the package manager for Python 3 installed, so you can skip to creating a [virtual environment](/python/docs/setup#installing_and_using_virtualenv) .\nTo install Python and then create a virtual environment, follow these steps:- Check that you have Python 3 and`pip`running in your system:```\npython --versionpython -m pip --version\n```\n- If required, install Python 3 and then set up a Python virtual environment: follow the instructions provided in theandsections of the [Setting up a Python development environment page](/python/docs/setup#installing_python) . If you're using Python 3.10 or later, you must also [enable Dataflow Runner v2](/dataflow/docs/runner-v2#python) . To use Runner v1, use Python 3.9 or earlier.\n **Note:** Cython is not required, but if it is installed, the version must be 0.28.1 or later. To check your Cython version, run`pip show cython`.\nAfter you complete the quickstart, you can deactivate the virtual environment by running `deactivate` .\n## Get the Apache Beam SDKThe Apache Beam SDK is an open source programming model for data pipelines. You define a pipeline with an Apache Beam program and then choose a runner, such as Dataflow, to run your pipeline.\nTo download and install the Apache Beam SDK, follow these steps:- Verify that you are in the Python virtual environment that you created in the preceding section. Ensure that the prompt starts with`<` `` `>`, where``is the name of the virtual environment.\n- Install the [Python wheel](https://pypi.org/project/wheel/) packaging standard:```\npip install wheel\n```\n- Install the latest version of the Apache Beam SDK for Python:\n- ```\npip install 'apache-beam[gcp]'\n```\n- On Microsoft Windows, use the following command:\n- ```\npip install apache-beam[gcp]\n```\n- Depending on the connection, your installation might take a while.\n## Run the pipeline locallyTo see how a pipeline runs locally, use a ready-made Python module for the `wordcount` example that is included with the `apache_beam` package.\nThe `wordcount` pipeline example does the following:- Takes a text file as input.This text file is located in a Cloud Storage bucket with the resource name `gs://dataflow-samples/shakespeare/kinglear.txt` .\n- Parses each line into words.\n- Performs a frequency count on the tokenized words.\nTo stage the `wordcount` pipeline locally, follow these steps:- From your local terminal, run the`wordcount`example:```\npython -m apache_beam.examples.wordcount \\\u00a0 --output outputs\n```\n- View the output of the pipeline:```\nmore outputs*\n```\n- To exit, press.Running the pipeline locally lets you test and debug your Apache Beam program. You can view the\n`wordcount.py`\nsource code on\n [Apache Beam GitHub](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount.py) \n.\n## Run the pipeline on the Dataflow service\nIn this section, run the\n`wordcount`\nexample pipeline from the\n`apache_beam`\npackage on the Dataflow service. This example specifies\n`DataflowRunner`\nas the parameter for\n`--runner`\n.\n- Run the pipeline:```\npython -m apache_beam.examples.wordcount \\\u00a0 \u00a0 --region DATAFLOW_REGION \\\u00a0 \u00a0 --input gs://dataflow-samples/shakespeare/kinglear.txt \\\u00a0 \u00a0 --output gs://BUCKET_NAME/results/outputs \\\u00a0 \u00a0 --runner DataflowRunner \\\u00a0 \u00a0 --project PROJECT_ID \\\u00a0 \u00a0 --temp_location gs://BUCKET_NAME/tmp/\n```Replace the following:- ``: the [region](/dataflow/docs/resources/locations) where you want to deploy the Dataflow job\u2014for example,`europe-west1`The `--region` flag overrides the default region that is   set in the metadata server, your local client, or environment   variables.\n- ``: the   Cloud Storage bucket name that you copied   earlier\n- ``: the Google Cloud project ID   that you copied earlier\n **Note:** To specify a [user-managed worker service account](/dataflow/docs/concepts/security-and-permissions#user-managed) , include the`--service_account_email` [pipeline option](/dataflow/docs/reference/pipeline-options) . User-managed worker service accounts are recommended for production workloads. If you don't specify a worker service account when you create a job, Dataflow uses the [Compute Engine default service account](/dataflow/docs/concepts/security-and-permissions#default-service-account) .\n## View your resultsWhen you run a pipeline using Dataflow, your results are stored in a Cloud Storage bucket. In this section, verify that the pipeline is running by using either the Google Cloud console or the local terminal.To view your results in Google Cloud console, follow these steps:- In the Google Cloud console, go to the Dataflow **Jobs** page. [Go to Jobs](https://console.cloud.google.com/dataflow) The **Jobs** page displays details of your `wordcount` job, including a status of **Running** at first, and then **Succeeded** .\n- Go to the Cloud Storage **Buckets** page. [Go to Buckets](https://console.cloud.google.com/storage/browser) \n- From the list of buckets in your project, click the storage bucket that you created earlier.In the `wordcount` directory, the output files that your job created are displayed.\nView the results from your terminal or by using Cloud Shell.- To list the output files, use the [gcloud storage ls command](/sdk/gcloud/reference/storage/ls) :```\ngcloud storage ls gs://BUCKET_NAME/results/outputs* --long\n```\n- Replace `` with the name of the Cloud Storage bucket used in the pipeline program.\n- To view the results in the output files, use the [gcloud storage cat command](/sdk/gcloud/reference/storage/cat) :```\ngcloud storage cat gs://BUCKET_NAME/results/outputs*\n```## Modify the pipeline code\nThe\n`wordcount`\npipeline in the previous examples distinguishes between uppercase and lowercase words. The following steps show how to modify the pipeline so that the\n`wordcount`\npipeline is not case-sensitive.\n- On your local machine, download the latest copy of the [wordcount code](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount.py) from the Apache Beam GitHub repository.\n- From the local terminal, run the pipeline:```\npython wordcount.py --output outputs\n```\n- View the results:```\nmore outputs*\n```\n- To exit, press.\n- In an editor of your choice, open the`wordcount.py`file.\n- Inside the`run`function, examine the pipeline steps:```\ncounts = (\u00a0 \u00a0 \u00a0 \u00a0 lines\u00a0 \u00a0 \u00a0 \u00a0 | 'Split' >> (beam.ParDo(WordExtractingDoFn()).with_output_types(str))\u00a0 \u00a0 \u00a0 \u00a0 | 'PairWithOne' >> beam.Map(lambda x: (x, 1))\u00a0 \u00a0 \u00a0 \u00a0 | 'GroupAndSum' >> beam.CombinePerKey(sum))\n```After `split` , the lines are split into words as strings.\n- To lowercase the strings, modify the line after`split`:```\ncounts = (\u00a0 \u00a0 \u00a0 \u00a0 lines\u00a0 \u00a0 \u00a0 \u00a0 | 'Split' >> (beam.ParDo(WordExtractingDoFn()).with_output_types(str))\u00a0 \u00a0 \u00a0 \u00a0 | 'lowercase' >> beam.Map(str.lower)\u00a0 \u00a0 \u00a0 \u00a0 | 'PairWithOne' >> beam.Map(lambda x: (x, 1))\u00a0 \u00a0 \u00a0 \u00a0 | 'GroupAndSum' >> beam.CombinePerKey(sum)) \n```This modification maps the`str.lower`function onto every word. This line is equivalent to`beam.Map(lambda word: str.lower(word))`.\n- Save the file and run the modified`wordcount`job:```\npython wordcount.py --output outputs\n```\n- View the results of the modified pipeline:```\nmore outputs*\n```\n- To exit, press.\n- Run the modified pipeline on the Dataflow service:```\npython wordcount.py \\\u00a0 \u00a0 --region DATAFLOW_REGION \\\u00a0 \u00a0 --input gs://dataflow-samples/shakespeare/kinglear.txt \\\u00a0 \u00a0 --output gs://BUCKET_NAME/results/outputs \\\u00a0 \u00a0 --runner DataflowRunner \\\u00a0 \u00a0 --project PROJECT_ID \\\u00a0 \u00a0 --temp_location gs://BUCKET_NAME/tmp/\n```Replace the following:- ``: the [region](/dataflow/docs/resources/locations) where you want to deploy the Dataflow job\n- ``: your   Cloud Storage bucket name\n- ``: you Google Cloud project ID\n## Clean upTo avoid incurring charges to your Google Cloud account for   the resources used on this page, delete the Google Cloud project with the   resources.\n **Note:** If you followed this quickstart in a new project, then you can [delete the project](/resource-manager/docs/creating-managing-projects#shutting_down_projects) .- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets](https://console.cloud.google.com/storage/browser) \n- Click the checkbox for the bucket that you want to delete.\n- To delete the bucket,  clickdelete **Delete** , and then follow the  instructions.\n- If you keep your project, revoke the roles that you granted to the Compute Engine default service account. Run the following command once for each of the following IAM roles:- `roles/dataflow.admin`\n- `roles/dataflow.worker`\n- `roles/storage.objectAdmin`\n```\ngcloud projects remove-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member=serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com \\\u00a0 \u00a0 --role=SERVICE_ACCOUNT_ROLE\n```\n- Optional: Revoke the authentication credentials that you created, and delete the local   credential file.```\ngcloud auth application-default revoke\n```\n- Optional: Revoke credentials from the gcloud CLI.```\ngcloud auth revoke\n```\n## What's next\n- [Read about the Apache Beam programming model](/dataflow/model/programming-model-beam) .\n- [Interactively develop a pipeline using an Apache Beam notebook](/dataflow/docs/guides/interactive-pipeline-development) .\n- [Learn how to design and create your own pipeline](/dataflow/pipelines/creating-a-pipeline-beam) .\n- [Work through the WordCount and Mobile Gaming examples](/dataflow/examples/examples-beam) .", "guide": "Dataflow"}