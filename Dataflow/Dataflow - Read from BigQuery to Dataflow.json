{"title": "Dataflow - Read from BigQuery to Dataflow", "url": "https://cloud.google.com/dataflow/docs/guides/read-from-bigquery", "abstract": "# Dataflow - Read from BigQuery to Dataflow\nThis document describes how to read data from BigQuery to Dataflow by using the Apache Beam [BigQuery I/O connector](https://beam.apache.org/documentation/io/built-in/google-bigquery/) .\n**Note:** Depending on your scenario, consider using one of the [Google-provided Dataflow templates](/dataflow/docs/guides/templates/provided-templates) . Several of these read from BigQuery.\n", "content": "## Overview\nThe BigQuery I/O connector supports two options for reading from BigQuery:\n- Direct table reads. This option is the fastest, because it uses the [BigQuery Storage Read API](/bigquery/docs/reference/storage) .\n- Export job. With this option, BigQuery runs an [export job](/bigquery/docs/exporting-data) that writes the table data to Cloud Storage. The connector then reads the exported data from Cloud Storage. This option is less efficient, because it requires the export step.\nExport jobs are the default option. To specify direct reads, call `withMethod(Method.DIRECT_READ)` .\nThe connector serializes the table data into a `PCollection` . Each element in the `PCollection` represents a single table row. The connector supports the following serialization methods:\n- [Read the data as Avro-formatted records](#read_avro-formatted_records) . Using this method, you provide a function that parses the Avro records into a custom data type.\n- [Read the data as TableRow objects](#read_tablerow_objects) . This method is convenient because it doesn't require a custom data type. However, it generally has lower performance than reading Avro-formatted records.## Parallelism\nParallelism in this connector depends on the read method:\n- Direct reads: The I/O connector produces a dynamic number of streams, based on the size of the export request. It reads these streams directly from BigQuery in parallel.\n- Export jobs: BigQuery determines how many files to write to Cloud Storage. The number of files depends on the query and the volume of data. The I/O connector reads the exported files in parallel.## Performance\nThe following table shows performance metrics for various BigQuery I/O read options. The workloads were run on one `e2-standard2` worker, using the Apache Beam SDK 2.49.0 for Java. They did not use Runner v2.\n| 100 M records | 1 kB | 1 column | Throughput (bytes) | Throughput (elements)  |\n|:----------------------------------|:---------------------|:---------------------------|\n| Storage Read      | 120 MBps    | 88,000 elements per second |\n| Avro Export      | 105 MBps    | 78,000 elements per second |\n| Json Export      | 110 MBps    | 81,000 elements per second |\nThese metrics are based on simple batch pipelines. They are intended to compare performance between I/O connectors, and are not necessarily representative of real-world pipelines. Dataflow pipeline performance is complex, and is a function of VM type, the data being processed, the performance of external sources and sinks, and user code. Metrics are based on running the Java SDK, and aren't representative of the performance characteristics of other language SDKs. For more information, see [Beam IO Performance](https://beam.apache.org/performance/) .\n## Best practices\n- In general, we recommend using direct table reads ( [Method.DIRECT_READ](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.TypedRead.Method.html) ). The Storage Read API is better suited to data pipelines than export jobs, because it doesn't need the intermediate step of exporting data.\n- If you use direct reads, you are charged for Storage Read API usage. See [Data extraction pricing](/bigquery/pricing#data_extraction_pricing) in the BigQuery pricing page.\n- There is no additional cost for export jobs. However, export jobs have [limits](/bigquery/quotas#export_jobs) . For large data movement, where timeliness is a priority and cost is adjustable, direct reads are recommended.\n- The Storage Read API has [quota limits](/bigquery/quotas#storage-limits) . Use [Google Cloud metrics](/monitoring/api/metrics_gcp#gcp-bigquerystorage) to monitor your quota usage.\n- When using the Storage Read API, you might see lease expiration and session timeout errors in the logs, such as:- `DEADLINE_EXCEEDED`\n- `Server Unresponsive`\n- `StatusCode.FAILED_PRECONDITION details = \"there was an error operating on 'projects/<projectID>/locations/<location>/sessions/<sessionID>/streams/<streamID>': session``\nThese errors can occur when an operation takes longer than the timeout, usually in pipelines that run for longer than 6 hours. To mitigate this issue, switch to file exports.\n- When using the Java SDK, consider creating a class that represents the schema of the BigQuery table. Then call [useBeamSchema](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#useBeamSchema--) in your pipeline to automatically convert between Apache Beam `Row` and BigQuery `TableRow` types. For an example of a schema class, see [ExampleModel.java](https://github.com/GoogleCloudPlatform/cloud-code-samples/blob/v1/java/java-dataflow-samples/read-pubsub-write-bigquery/src/main/java/com/cloudcode/dataflow/ExampleModel.java) .## Examples\nThe code examples in this section use direct table reads.\nTo use an export job instead, omit the call to `withMethod` or specify `Method.EXPORT` . Then set the `--tempLocation` [pipeline option](/dataflow/docs/reference/pipeline-options#basic_options) to specify a Cloud Storage bucket for the exported files.\nThese code examples assume the source table has the following columns:\n- `name`(string)\n- `age`(integer)\nSpecified as a [JSON schema file](/bigquery/docs/schemas#specifying_a_json_schema_file) :\n```\n[\u00a0 {\"name\":\"user_name\",\"type\":\"STRING\",\"mode\":\"REQUIRED\"},\u00a0 {\"name\":\"age\",\"type\":\"INTEGER\",\"mode\":\"REQUIRED\"}]\n```\n### Read Avro-formatted records\nTo read BigQuery data into Avro-formatted records, use the [read(SerializableFunction)](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.html#read-org.apache.beam.sdk.transforms.SerializableFunction-) method. This method takes an application-defined function that parses [SchemaAndRecord](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/SchemaAndRecord.html) objects and returns a custom data type. The output from the connector is a `PCollection` of your custom data type.\nThe following code reads a `PCollection<MyData>` from a BigQuery table, where `MyData` is an application-defined class.\nTo authenticate to Dataflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BigQueryReadAvro.java) \n```\nimport org.apache.avro.generic.GenericRecord;import org.apache.avro.util.Utf8;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.coders.DefaultCoder;import org.apache.beam.sdk.extensions.avro.coders.AvroCoder;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.TypedRead;import org.apache.beam.sdk.io.gcp.bigquery.SchemaAndRecord;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.transforms.SerializableFunction;import org.apache.beam.sdk.values.TypeDescriptor;public class BigQueryReadAvro {\u00a0 // A custom datatype to hold a record from the source table.\u00a0 @DefaultCoder(AvroCoder.class)\u00a0 public static class MyData {\u00a0 \u00a0 public String name;\u00a0 \u00a0 public Long age;\u00a0 \u00a0 // Function to convert Avro records to MyData instances.\u00a0 \u00a0 public static class FromSchemaAndRecord\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 implements SerializableFunction<SchemaAndRecord, MyData> {\u00a0 \u00a0 \u00a0 @Override public MyData apply(SchemaAndRecord elem) {\u00a0 \u00a0 \u00a0 \u00a0 MyData data = new MyData();\u00a0 \u00a0 \u00a0 \u00a0 GenericRecord record = elem.getRecord();\u00a0 \u00a0 \u00a0 \u00a0 data.name = ((Utf8) record.get(\"user_name\")).toString();\u00a0 \u00a0 \u00a0 \u00a0 data.age = (Long) record.get(\"age\");\u00a0 \u00a0 \u00a0 \u00a0 return data;\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\u00a0 public static void main(String[] args) {\u00a0 \u00a0 // Parse the pipeline options passed into the application. Example:\u00a0 \u00a0 // \u00a0 --projectId=$PROJECT_ID --datasetName=$DATASET_NAME --tableName=$TABLE_NAME\u00a0 \u00a0 // For more information, see https://beam.apache.org/documentation/programming-guide/#configuring-pipeline-options\u00a0 \u00a0 PipelineOptionsFactory.register(ExamplePipelineOptions.class);\u00a0 \u00a0 ExamplePipelineOptions options = PipelineOptionsFactory.fromArgs(args)\u00a0 \u00a0 \u00a0 \u00a0 .withValidation()\u00a0 \u00a0 \u00a0 \u00a0 .as(ExamplePipelineOptions.class);\u00a0 \u00a0 // Create a pipeline and apply transforms.\u00a0 \u00a0 Pipeline pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 // Read table data into Avro records, using an application-defined parsing function.\u00a0 \u00a0 \u00a0 \u00a0 .apply(BigQueryIO.read(new MyData.FromSchemaAndRecord())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .from(String.format(\"%s:%s.%s\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getProjectId(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getDatasetName(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getTableName()))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withMethod(TypedRead.Method.DIRECT_READ))\u00a0 \u00a0 \u00a0 \u00a0 // The output from the previous step is a PCollection<MyData>.\u00a0 \u00a0 \u00a0 \u00a0 .apply(MapElements\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .into(TypeDescriptor.of(MyData.class))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via((MyData x) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Name: %s, Age: %d%n\", x.name, x.age);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return x;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }));\u00a0 \u00a0 pipeline.run().waitUntilFinish();\u00a0 }}\n```\nThe `read` method takes a `SerializableFunction<SchemaAndRecord, T>` interface, which defines a function to convert from Avro records to a custom data class. In the previous code example, the `MyData.apply` method implements this conversion function. The example function parses the `name` and `age` fields from the Avro record and returns a `MyData` instance.\n**Note:** This code fails if the parsing function doesn't match the actual schema of the BigQuery table. Specifically, the named columns must exist, and the data types must be compatible. Also, if any columns in the table are nullable, your code must handle null values.\nTo specify which BigQuery table to read, call the `from` method, as shown in the previous example. For more information, see [Table names](https://beam.apache.org/documentation/io/built-in/google-bigquery/#table-names) in the BigQuery I/O connector documentation.\n### Read TableRow objects\nThe [readTableRows](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.html#readTableRows--) method reads BigQuery data into a `PCollection` of [TableRow](https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/java/latest/com/google/api/services/bigquery/model/TableRow.html) objects. Each `TableRow` is a map of key-value pairs that holds a single row of table data. Specify the BigQuery table to read by calling the `from` method.\nThe following code reads a `PCollection<TableRows>` from a BigQuery table.\nTo authenticate to Dataflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BiqQueryReadTableRows.java) \n```\nimport com.google.api.services.bigquery.model.TableRow;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.TypedRead.Method;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.values.TypeDescriptor;public class BiqQueryReadTableRows {\u00a0 public static void main(String[] args) {\u00a0 \u00a0 // Parse the pipeline options passed into the application. Example:\u00a0 \u00a0 // \u00a0 --projectId=$PROJECT_ID --datasetName=$DATASET_NAME --tableName=$TABLE_NAME\u00a0 \u00a0 // For more information, see https://beam.apache.org/documentation/programming-guide/#configuring-pipeline-options\u00a0 \u00a0 PipelineOptionsFactory.register(ExamplePipelineOptions.class);\u00a0 \u00a0 ExamplePipelineOptions options = PipelineOptionsFactory.fromArgs(args)\u00a0 \u00a0 \u00a0 \u00a0 .withValidation()\u00a0 \u00a0 \u00a0 \u00a0 .as(ExamplePipelineOptions.class);\u00a0 \u00a0 // Create a pipeline and apply transforms.\u00a0 \u00a0 Pipeline pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 // Read table data into TableRow objects.\u00a0 \u00a0 \u00a0 \u00a0 .apply(BigQueryIO.readTableRows()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .from(String.format(\"%s:%s.%s\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getProjectId(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getDatasetName(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getTableName()))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withMethod(Method.DIRECT_READ)\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 // The output from the previous step is a PCollection<TableRow>.\u00a0 \u00a0 \u00a0 \u00a0 .apply(MapElements\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .into(TypeDescriptor.of(TableRow.class))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Use TableRow to access individual fields in the row.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via((TableRow row) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 var name = (String) row.get(\"user_name\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 var age = (String) row.get(\"age\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Name: %s, Age: %s%n\", name, age);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return row;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }));\u00a0 \u00a0 pipeline.run().waitUntilFinish();\u00a0 }}\n```\nThis example also shows how to access the values from the `TableRow` dictionary. Integer values are encoded as strings to match BigQuery's exported JSON format.\n**Note:** This code fails if the parsing function doesn't match the actual schema of the BigQuery table. Specifically, the named columns must exist, and the data types must be compatible. Also, if any columns in the table are nullable, your code must handle null values.\n### Column projection and filtering\nWhen using direct reads ( `Method.DIRECT_READ` ), you can make the read operations more efficient by reducing how much data is read from BigQuery and sent over the network.\n- Column projection: Call`withSelectedFields`to read a subset of columns from the table. This allows efficient reads when tables contain many columns.\n- Row filtering: Call`withRowRestriction`to specify a predicate that filters data on the server side.\nFilter predicates must be deterministic, and aggregation is not supported.\n**Note:** These methods are not supported when reading from an export job ( `Method.EXPORT` ).\nThe following example projects the `\"user_name\"` and `\"age\"` columns, and filters out rows that don't match the predicate `\"age > 18\"` .\nTo authenticate to Dataflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BigQueryReadWithProjectionAndFiltering.java) \n```\nimport com.google.api.services.bigquery.model.TableRow;import java.util.Arrays;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.TypedRead;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.values.TypeDescriptor;public class BigQueryReadWithProjectionAndFiltering {\u00a0 public static void main(String[] args) {\u00a0 \u00a0 // Parse the pipeline options passed into the application. Example:\u00a0 \u00a0 // \u00a0 --projectId=$PROJECT_ID --datasetName=$DATASET_NAME --tableName=$TABLE_NAME\u00a0 \u00a0 // For more information, see https://beam.apache.org/documentation/programming-guide/#configuring-pipeline-options\u00a0 \u00a0 PipelineOptionsFactory.register(ExamplePipelineOptions.class);\u00a0 \u00a0 ExamplePipelineOptions options = PipelineOptionsFactory.fromArgs(args)\u00a0 \u00a0 \u00a0 \u00a0 .withValidation()\u00a0 \u00a0 \u00a0 \u00a0 .as(ExamplePipelineOptions.class);\u00a0 \u00a0 // Create a pipeline and apply transforms.\u00a0 \u00a0 Pipeline pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 .apply(BigQueryIO.readTableRows()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Read rows from a specified table.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .from(String.format(\"%s:%s.%s\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getProjectId(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getDatasetName(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getTableName()))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withMethod(TypedRead.Method.DIRECT_READ)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withSelectedFields(Arrays.asList(\"user_name\", \"age\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withRowRestriction(\"age > 18\")\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 // The output from the previous step is a PCollection<TableRow>.\u00a0 \u00a0 \u00a0 \u00a0 .apply(MapElements\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .into(TypeDescriptor.of(TableRow.class))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Use TableRow to access individual fields in the row.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via((TableRow row) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 var name = (String) row.get(\"user_name\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 var age = row.get(\"age\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Name: %s, Age: %s%n\", name, age);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return row;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }));\u00a0 \u00a0 pipeline.run().waitUntilFinish();\u00a0 }}\n```### Read from a query result\nThe previous examples show how to read rows from a table. You can also read from the result of a SQL query, by calling `fromQuery` . This approach moves some of the computational work into BigQuery. You can also use this method to read from a BigQuery view or materialized view, by running a query against the view.\nThe following example runs a query against a BigQuery public dataset and reads the results. After the pipeline runs, you can see the query job in your BigQuery job history.\nTo authenticate to Dataflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BigQueryReadFromQuery.java) \n```\nimport com.google.api.services.bigquery.model.TableRow;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.TypedRead;import org.apache.beam.sdk.options.PipelineOptions;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.values.TypeDescriptor;public class BigQueryReadFromQuery {\u00a0 public static void main(String[] args) {\u00a0 \u00a0 // The SQL query to run inside BigQuery.\u00a0 \u00a0 final String queryString =\u00a0 \u00a0 \u00a0 \u00a0 \"SELECT repo_name as repo, COUNT(*) as count \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"FROM `bigquery-public-data.github_repos.sample_commits` \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"GROUP BY repo_name\";\u00a0 \u00a0 // Parse the pipeline options passed into the application.\u00a0 \u00a0 // For more information, see https://beam.apache.org/documentation/programming-guide/#configuring-pipeline-options\u00a0 \u00a0 PipelineOptions options = PipelineOptionsFactory.fromArgs(args)\u00a0 \u00a0 \u00a0 \u00a0 .withValidation().create();\u00a0 \u00a0 // Create a pipeline and apply transforms.\u00a0 \u00a0 Pipeline pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 // Read the query results into TableRow objects.\u00a0 \u00a0 \u00a0 \u00a0 .apply(BigQueryIO.readTableRows()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .fromQuery(queryString)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .usingStandardSql()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withMethod(TypedRead.Method.DIRECT_READ))\u00a0 \u00a0 \u00a0 \u00a0 // The output from the previous step is a PCollection<TableRow>.\u00a0 \u00a0 \u00a0 \u00a0 .apply(MapElements\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .into(TypeDescriptor.of(TableRow.class))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via((TableRow row) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Repo: %s, commits: %s%n\", row.get(\"repo\"), row.get(\"count\"));\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return row;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }));\u00a0 \u00a0 pipeline.run().waitUntilFinish();\u00a0 }}\n```\n## What's next\n- Read the [BigQuery I/O connector](https://beam.apache.org/documentation/io/built-in/google-bigquery/) documentation.\n- See the list of [Google-provided templates](/dataflow/docs/guides/templates/provided-templates) .", "guide": "Dataflow"}