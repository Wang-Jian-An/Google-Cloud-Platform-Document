{"title": "Dataflow - Dataflow job graphs", "url": "https://cloud.google.com/dataflow/docs/guides/job-graph", "abstract": "# Dataflow - Dataflow job graphs\nWhen you select a specific Dataflow job, the monitoring interface provides a graphical representation of your pipeline: the **job graph** . The job graph page in the console also provides a job summary, a job log, and information about each step in the pipeline.\nA pipeline's job graph represents each transform in the pipeline as a box. Each box contains the transform name and information about the job status, which includes the following:\n- **Running** : the step is running.\n- **Queued** : the step in a [FlexRS job](/dataflow/docs/guides/flexrs#delayed_scheduling) is queued.\n- **Succeeded** : the step finished successfully.\n- **Stopped** : the step stopped because the job stopped.\n- **Unknown** : the step failed to report status.\n- **Failed** : the step failed to complete.", "content": "## Basic job graph\n## Composite transforms\nIn the job graph, [composite transforms](https://beam.apache.org/documentation/programming-guide/#composite-transforms) , transforms that contain multiple nested sub-transforms, are expandable. Expandable composite transforms are marked with an arrow in the graph. Click the arrow to expand the transform and view the sub-transforms within.\nIn your pipeline code, you might have invoked your composite transform as follows:\n```\nresult = transform.apply(input);\n```\nComposite transforms invoked in this manner omit the expected nesting and might thus appear expanded in the Dataflow Monitoring Interface. Your pipeline might also generate warnings or errors about stable unique names at pipeline execution time.\nTo avoid these issues, make sure you invoke your transforms using the [recommended format](/dataflow/model/transforms) :\n```\nresult = input.apply(transform);\n```\n## Transform names\nDataflow has a few different ways to obtain the transform name that's shown in the monitoring job graph.\n**Note:** Transform names are used in publicly-visible places, including the Dataflow monitoring interface, log files, and debugging tools. Don't use transform names that include personally identifiable information, such as usernames or organization names.\n- **Dataflow can use a name that you assign** when you apply your transform. The first  argument you supply to the`apply`method is your transform name.\n- **Dataflow can infer the transform name** , either from the class name (if you've built a  custom transform) or the name of your`DoFn`function object (if you're using a  core transform such as`ParDo`).\n- **Dataflow can use a name that you assign** when you apply your transform. You can set the  transform name by specifying the transform's`label`argument.\n- **Dataflow can infer the transform name** , either from the class name (if you've built a  custom transform) or the name of your`DoFn`function object (if you're using a  core transform such as`ParDo`).\n- **Dataflow can use a name that you assign** when you apply your transform. You can set the  transform name by specifying the`Scope`.\n- **Dataflow can infer the transform name** , either from the struct name if you're using a structural`DoFn`or from the function name if you're using a functional`DoFn`.## Understand the metrics\nThis section provides details about the metrics associated with the job graph.\n### Wall time\nWhen you click a step, the **Wall time** metric is displayed in the **Step info** panel. Wall time provides the total approximate time spent across all threads in all workers on the following actions:\n- Initializing the step\n- Processing data\n- Shuffling data\n- Ending the step\nFor composite steps, wall time tells you the sum of time spent in the component steps. This estimate can help you identify slow steps and diagnose which part of your pipeline is taking more time than required.### Side input metrics\nshow you how your [side input](https://beam.apache.org/documentation/programming-guide/#side-inputs) access patterns and algorithms affect your pipeline's performance. When your pipeline uses a side input, Dataflow writes the collection to a persistent layer, such as a disk, and your transforms read from this persistent collection. These reads and writes affect your job's run time.\n**Note:** To see side input metrics, you must use Apache Beam SDK for Java version 2.3.0 or later. Apache Beam SDK for Python version 2.8.0 or later. Side input metrics are only available in batch pipelines.\nThe Dataflow monitoring interface displays side input metrics when you select a transform that creates or consumes a side input collection. You can view the metrics in the **Side Input Metrics** section of the **Step info** panel.\nIf the selected transform creates a side input collection, the **Side Input Metrics** section displays the name of the collection, along with the following metrics:\n- **Time spent writing:** The time spent writing the side input collection.\n- **Bytes written:** The total number of bytes written to the side input collection.\n- **Time & bytes read from side input:** A table that contains additional metrics for all transforms that consume the side input collection, called.\nThe **Time & bytes read from side input** table contains the following information for each side input consumer:\n- **Side input consumer:** The transform name of the side input consumer.\n- **Time spent reading:** The time this consumer spent reading the side input collection.\n- **Bytes read:** The number of bytes this consumer read from the side input collection.\nIf your pipeline has a composite transform that creates a side input, [expand the composite transform](#composite-transforms) until you see the specific subtransform that creates the side input. Then, select that subtransform to view the **Side Input Metrics** section.\nFigure 4 shows side input metrics for a transform that creates a side input collection.\nIf the selected transform consumes one or more side inputs, the **Side Input Metrics** section displays the **Time & bytes read from side input** table. This table contains the following information for each side input collection:\n- **Side input collection:** The name of the side input collection.\n- **Time spent reading:** The time the transform spent reading this side input collection.\n- **Bytes read:** The number of bytes the transform read from this side input collection.\nIf your pipeline has a composite transform that reads a side input, [expand the composite transform](#composite-transforms) until you see the specific subtransform that reads the side input. Then, select that subtransform to view the **Side Input Metrics** section.\nFigure 5 shows side input metrics for a transform that reads from a side input collection.\nis a common side input performance issue. If your side input `PCollection` is too large, workers can't cache the entire collection in memory. As a result, the workers must repeatedly read from the persistent side input collection.\nIn figure 6, side input metrics show that the total bytes read from the side input collection are much larger than the collection's size, total bytes written.\nTo improve the performance of this pipeline, redesign your algorithm to avoid iterating or refetching the side input data. In this example, the pipeline creates the Cartesian product of two collections. The algorithm iterates through the entire side input collection for each element of the main collection. You can improve the access pattern of the pipeline by batching multiple elements of the main collection together. This change reduces the number of times workers must re-read the side input collection.\nAnother common performance issue can occur if your pipeline performs a join by applying a `ParDo` with one or more large side inputs. In this case, workers spend a large percentage of the processing time for the join operation reading from the side input collections.\nFigure 7 shows example side input metrics for this issue:\nTo improve the performance of this pipeline, use [CoGroupByKey](https://beam.apache.org/documentation/programming-guide/#cogroupbykey) instead of side inputs.", "guide": "Dataflow"}