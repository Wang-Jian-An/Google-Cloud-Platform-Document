{"title": "Dataflow - Create a streaming pipeline using a Dataflow template", "url": "https://cloud.google.com/dataflow/docs/quickstarts/create-streaming-pipeline-template", "abstract": "# Dataflow - Create a streaming pipeline using a Dataflow template\n# Create a streaming pipeline using a Dataflow template\nThis quickstart shows you how to create a streaming pipeline using a Google-provided Dataflow template. Specifically, this quickstart uses the **Pub/Sub to BigQuery** template as an example.\nThe Pub/Sub to BigQuery template is a streaming pipeline that can read JSON-formatted messages from a Pub/Sub topic and write them to a BigQuery table.To follow step-by-step guidance for this task directly in the Google Cloud console, click **Guide me** :\n [Guide me](https://console.cloud.google.com/home/dashboard?tutorial=dataflow--quickstart-templates) ", "content": "## Before you begin- Create a Cloud Storage bucket:\n- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets page](https://console.cloud.google.com/storage/browser) \n- Click **Create bucket** .\n- On the **Create a bucket** page, enter your bucket information. To go to the next step, click **Continue** .- For **Name your bucket** , enter a unique bucket name. Don't include sensitive   information in the bucket name, because the bucket namespace is global and publicly   visible.\n- For **Choose where to store your data** , do the following:- Select a **Location type** option.\n- Select a **Location** option.\n- For **Choose a default storage class for your data** , select the following: **Standard** .\n- For **Choose how to control access to objects** , select an **Access control** option.\n- For **Advanced settings (optional)** , specify  an [encryption method](/storage/docs/encryption) ,  a [retention policy](/storage/docs/bucket-lock) ,  or [bucket labels](/storage/docs/tags-and-labels#bucket-labels) .\n- Click **Create** .\n- Copy the following, as you need them in a later section:- Your Cloud Storage bucket name.\n- Your Google Cloud project ID.To find this ID, see [Identifying projects](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- To complete the steps in this quickstart, your user account must have the [Dataflow Admin role](/dataflow/docs/concepts/access-control#roles) and the [Service Account User](/iam/docs/service-accounts-actas) role. The [Compute Engine default service account](/compute/docs/access/service-accounts#default_service_account) must have the [Dataflow Worker role](/dataflow/docs/concepts/access-control#roles) . To add the required roles in the Google Cloud console:- Go to the **IAM** page. [Go to IAM](https://console.cloud.google.com/projectselector/iam-admin/iam?supportedpurview=project,folder,organizationId) \n- Select your project.\n- In the row containing your user account, clickedit **Edit principal** ,  and then clickadd **Add another role** .\n- In the drop-down list, select the role **Dataflow Admin** .\n- Repeat for the **Service Account User** role, and then click **Save** .\n- In the row containing the Compute Engine default service account, clickedit **Edit principal** ,  and then clickadd **Add another role** .\n- In the drop-down list, select the role **Dataflow Worker** .\n- Repeat for the **Pub/Sub Editor** and the **BigQuery Data Editor** roles, and then click **Save** .For more information about granting roles, see [Grant an IAM role by using the console](/iam/docs/grant-role-console) .\n- By default, each new project starts with a [default network](/vpc/docs/vpc#default-network) . If the default network for your project is [disabled](/vpc/docs/vpc#org-policies) or was deleted, you need to have a network in your project for which your user account has the [Compute Network User role](/compute/docs/access/iam#compute.networkUser) (`roles/compute.networkUser`).\n## Create a BigQuery dataset and tableCreate a BigQuery dataset and table with the appropriate schema for your Pub/Sub topic using the Google Cloud console.\nIn this example, the name of the dataset is `taxirides` and the name of the table is `realtime` . To create this dataset and table, follow these steps:- Go to the **BigQuery** page. [Go to BigQuery](https://console.cloud.google.com/bigquery) \n- In the **Explorer** panel, next to the project where you want to create the  dataset, clickmore_vert **View actions** , and then click **Create dataset** . **Note:** The default experience is the [Preview](https://cloud.google.com/products#product-launch-stages) Google Cloud console. If you clicked **Hide preview features** to go to the Google Cloud console, then perform the following step instead: In the navigation panel, in the **Resources** section, select your project.\n- On the **Create dataset** panel, follow these steps:\n- For **Dataset ID** , enter`taxirides`.  Dataset IDs are unique for each Google Cloud project.\n- For **Location type** , choose **Multi-region** , and then select **US (multiple regions in United States)** . Public datasets are stored in  the`US`multi-region [location](/bigquery/docs/dataset-locations) . For simplicity, place  your dataset in the same location.\n- Leave the other default settings, and then click **Create dataset** \n- In thepanel, expand your project.\n- Next to your`taxirides`dataset, clickmore_vert **View actions** , and then  click **Create table** . **Note:** The default experience is the [Preview](https://cloud.google.com/products#product-launch-stages) Google Cloud console. If you clicked **Hide preview features** to go to the  Google Cloud console, then perform the following step instead: In the navigation panel, in the **Resources** section, select the`taxirides`dataset you created.\n- On the **Create table** panel, follow these steps:\n- In the **Source** section, for **Create table from** , select **Empty table** .\n- In the **Destination** section, for **Table** , enter`realtime`.\n- In the **Schema** section, click the **Edit as text** toggle and paste   the following schema definition into the box:```\nride_id:string,point_idx:integer,latitude:float,longitude:float,timestamp:timestamp,meter_reading:float,meter_increment:float,ride_status:string,passenger_count:integer\n```\n- In the **Partition and cluster settings** section, for **Partitioning** , select the **timestamp** field.\n- Leave the other default settings in place and click **Create table** .\n## Run the pipelineRun a streaming pipeline using the Google-provided [Pub/Sub to BigQuery](/dataflow/docs/guides/templates/provided/pubsub-to-bigquery) template. The pipeline gets incoming data from the input topic.\n- Go to the Dataflow **Jobs** page. [Go to  Jobs](https://console.cloud.google.com/dataflow/jobs) \n- Click.\n- Enter`taxi-data`as the **Job name** for your  Dataflow job.\n- For **Dataflow template** , select the **Pub/Sub to\n BigQuery** template.\n- For **BigQuery output table** , enter the following:```\nPROJECT_ID:taxirides.realtime\n```Replace `` with the project ID of the project where you  created your BigQuery dataset.\n- Expand **Optional parameters** .\n- For **Input Pub/Sub topic** , click **Enter topic manually** .\n- In the dialog, for **Topic name** enter the following, and then click **Save** :```\nprojects/pubsub-public-data/topics/taxirides-realtime\n```This publicly available Pub/Sub topic is based on the [NYC Taxi &  Limousine Commission's open dataset](https://data.cityofnewyork.us/) . The following is a sample message from this topic, in  the JSON format:```\n{\u00a0 \"ride_id\": \"19c41fc4-e362-4be5-9d06-435a7dc9ba8e\",\u00a0 \"point_idx\": 217,\u00a0 \"latitude\": 40.75399,\u00a0 \"longitude\": -73.96302,\u00a0 \"timestamp\": \"2021-03-08T02:29:09.66644-05:00\",\u00a0 \"meter_reading\": 6.293821,\u00a0 \"meter_increment\": 0.029003782,\u00a0 \"ride_status\": \"enroute\",\u00a0 \"passenger_count\": 1}\n```\n- For **Temp location** , enter the following:```\ngs://BUCKET_NAME/temp/\n```Replace `` with the name of your Cloud Storage  bucket. The `temp` folder stores temporary files, like the  staged pipeline job.\n- If your project does not have a [default network](/vpc/docs/vpc#default-network) , enter a **Network** and a **Subnetwork** . For more information, see [Specify a network and subnetwork](/dataflow/docs/guides/specifying-networks#specifying_a_network_and_a_subnetwork) . **Note:** Unless specified through the`network`option, the Dataflow runner runs jobs in the`default`Virtual Private Cloud network. If your project does not have a default network and you don't specify a network, an error occurs. You might not have a default network if the default network was deleted or if [an organization policy constraint](/vpc/docs/vpc#org-policies) prevents the creation of the default network.\n- Click **Run job** .\n## View your results\nTo view the data written to your\n`realtime`\ntable, follow these steps:\n- Go to the **BigQuery** page. [Go toBigQuery](https://console.cloud.google.com/bigquery) \n- Click add_box **Compose a new query** . A new **Editor** tab opens.```\nSELECT * FROM `PROJECT_ID.taxirides.realtime`WHERE `timestamp` > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 DAY)LIMIT 1000\n```Replace `` with the project ID of the project where you created your BigQuery dataset. It can take up to a minute for data to start appearing in your table.\n- Click **Run** .The query returns rows that have been added to your table in the past 24 hours. You can also run queries using standard SQL.\n## Clean upTo avoid incurring charges to your Google Cloud account for   the resources used on this page, follow these steps.\n### Delete the project\nThe easiest way to eliminate billing is to delete the Google Cloud project that you created for the quickstart.\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Delete the individual resourcesIf you want to keep the Google Cloud project that you used in this quickstart, then delete the individual resources:- Go to the Dataflow **Jobs** page. [  Go to Jobs](https://console.cloud.google.com/dataflow/jobs) \n- Select your streaming job from the job list.\n- In the navigation, click **Stop** .\n- In the **Stop job** dialog, either [cancel](/dataflow/docs/guides/stopping-a-pipeline#cancel) or [drain](/dataflow/docs/guides/stopping-a-pipeline#drain) your  pipeline, and then click **Stop job** .\n- Go to the **BigQuery** page. [Go to  BigQuery](https://console.cloud.google.com/bigquery) \n- In the **Explorer** panel, expand your project.\n- Next to the dataset you want to delete, clickmore_vert **View actions** , and then  click **Open** .\n- In the details panel, click **Delete dataset** , and then follow the instructions.\n- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets](https://console.cloud.google.com/storage/browser) \n- Click the checkbox for the bucket that you want to delete.\n- To delete the bucket,  clickdelete **Delete** , and then follow the  instructions.\n## What's next\n- [Dataflow templates overview](/dataflow/docs/templates/overview) \n- [Creating classic templates](/dataflow/docs/templates/creating-templates) \n- [Running classic templates](/dataflow/docs/templates/running-templates) \n- [Google-provided templates](/dataflow/docs/templates/provided-templates)", "guide": "Dataflow"}