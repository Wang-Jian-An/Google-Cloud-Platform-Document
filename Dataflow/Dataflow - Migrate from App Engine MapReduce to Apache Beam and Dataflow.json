{"title": "Dataflow - Migrate from App Engine MapReduce to Apache Beam and Dataflow", "url": "https://cloud.google.com/dataflow/docs/guides/gae-mapreduce-migration", "abstract": "# Dataflow - Migrate from App Engine MapReduce to Apache Beam and Dataflow\nThis tutorial is intended for App Engine MapReduce users. It shows how to migrate from using App Engine MapReduce to Apache Beam and Dataflow.\n", "content": "## Why migrate?\nApp Engine MapReduce is a programming model for processing large amounts of data in a parallel and distributed fashion. It is useful for large, long-running tasks that cannot be handled within the scope of a single request, such as:\n- Analyzing application logs\n- Aggregating related data from external sources\n- Transforming data from one format to another\n- Exporting data for external analysis\nHowever, App Engine MapReduce is a community-maintained, [open source library](https://github.com/GoogleCloudPlatform/appengine-mapreduce/wiki/1-MapReduce) that is built on top of App Engine services and is no longer supported by Google.\nDataflow, on the other hand, is fully supported by Google, and provides extended functionality compared to App Engine MapReduce.\n### Migration cases\nThe following are some example cases where you could benefit from migrating from App Engine MapReduce to Apache Beam and Dataflow:\n- Store your Datastore database application data in a BigQuery data warehouse for analytical processing using SQL.\n- Use Dataflow as an alternative to App Engine MapReduce for maintenance and/or updates of your Datastore dataset.\n- Back up parts of your Datastore database to Cloud Storage.## What is Dataflow and Apache Beam?\nDataflow is a managed service for executing a wide variety of data processing patterns. Apache Beam is a unified programming model that provides SDKs for defining data processing workflows. Use Apache Beam to create complex pipelines for both batch and streaming and run them on Dataflow.\n## Getting started with Dataflow and Apache Beam\nTo get started, follow the quickstart of your choice:\n- [Using Java and Apache Maven](/dataflow/docs/quickstarts/create-pipeline-java) \n- [Using Python](/dataflow/docs/quickstarts/create-pipeline-python) \n- [Using Go](/dataflow/docs/quickstarts/create-pipeline-go) \n### Creating and running a pipeline\nWhen using App Engine MapReduce, you create data processing classes, add the MapReduce library, and once the job's specification and settings are defined, you create and start the job in one step using the static `start()` method on the appropriate job class.\nFor Map jobs, you create the `Input` and `Output` classes and the `Map` class that does the mapping. For App Engine MapReduce jobs, you create `Input` and `Output` classes, and define the `Mapper` and `Reducer` classes for data transformations.\nWith Apache Beam you do things slightly differently; you create a pipeline. You use input and output connectors to read and write from your data sources and sinks. You use predefined data transforms (or write your own) to implement your data transformations. Then, once your code is ready, you deploy your pipeline to the Dataflow service.\n### Converting App Engine MapReduce jobs to Apache Beam pipelines\nThe following table presents the Apache Beam equivalents of the **map** , **shuffle** , and **reduce** steps of the App Engine MapReduce model.\n| App Engine MapReduce | Apache Beam equivalent     |\n|:-----------------------|:----------------------------------------|\n| Map     | MapElements<InputT,OutputT>    |\n| Shuffle    | GroupByKey<K,V>       |\n| Reduce     | Combine.GroupedValues<K,InputT,OutputT> |\nA common practice is to use `Combine.PerKey<K,InputT,OutputT>` instead of `GroupByKey` and `CombineValues` .| App Engine MapReduce | Apache Beam equivalent |\n|:-----------------------|:-------------------------|\n| Map     | beam.Map     |\n| Shuffle    | beam.GroupByKey   |\n| Reduce     | beam.CombineValues  |\nA common practice is to use `beam.CombinePerKey` instead of `beam.GroupByKey` and `beam.CombineValues` .| App Engine MapReduce | Apache Beam equivalent |\n|:-----------------------|:-------------------------|\n| Map     | beam.ParDo    |\n| Shuffle    | beam.GroupByKey   |\n| Reduce     | beam.Combine    |\nThe following sample code demonstrates how to implement the App Engine MapReduce model in Apache Beam:\nTaken from\n [MinimalWordCount.java](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/MinimalWordCount.java) \n:\n```\np.apply(TextIO.read().from(\"gs://apache-beam-samples/shakespeare/*\"))\u00a0// Apply a ParDo that returns a PCollection\n```\nTaken from\n [wordcount_minimal.py](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount_minimal.py) \n:\n```\n# Read the text file[pattern] into a PCollection.lines = p | ReadFromText(known_args.input)# Count the occurrences of each word.counts = (\u00a0 \u00a0 lines\u00a0 \u00a0 | 'Split' >> (beam.FlatMap(lambda x: re.findall(r'[A-Za-z\\']+', x))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .with_output_types(unicode))\u00a0 \u00a0 | 'PairWithOne' >> beam.Map(lambda x: (x, 1))\u00a0 \u00a0 | 'GroupAndSum' >> beam.CombinePerKey(sum))# Format the counts into a PCollection of strings.output = counts | 'Format' >> beam.Map(lambda (w, c): '%s: %s' % (w, c))# Write the output using a \"Write\" transform that has side effects.# pylint: disable=expression-not-assignedoutput | WriteToText(known_args.output)\n```\nTaken from\n [minimal_wordcount.go](https://github.com/apache/beam/blob/master/sdks/go/examples/minimal_wordcount/minimal_wordcount.go) \n:\n```\n// beam.Init() is an initialization hook that must be called on startup.beam.Init()// Create the Pipeline object and root scope.p := beam.NewPipeline()s := p.Root()lines := textio.Read(s, \"gs://apache-beam-samples/shakespeare/kinglear.txt\")// Invoke a ParDo transform on our PCollection of text lines.// This ParDo invokes a DoFn (defined in-line) on each element that// tokenizes the text line into individual words. The ParDo returns a// PCollection of type string, where each element is an individual word in// Shakespeare's collected texts.words := beam.ParDo(s, func(line string, emit func(string)) {\u00a0 \u00a0 for _, word := range wordRE.FindAllString(line, -1) {\u00a0 \u00a0 \u00a0 \u00a0 emit(word)\u00a0 \u00a0 }}, lines)// Invoke the stats.Count transform on our PCollection of// individual words. The Count transform returns a new PCollection of// key/value pairs, where each key represents a unique word in the text.// The associated value is the occurrence count for that word.counted := stats.Count(s, words)// Use a ParDo to format our PCollection of word counts into a printable// string, suitable for writing to an output file. When each element// produces exactly one element, the DoFn can simply return it.formatted := beam.ParDo(s, func(w string, c int) string {\u00a0 \u00a0 return fmt.Sprintf(\"%s: %v\", w, c)}, counted)// Invoke textio.Write at the end of the pipeline to write// the contents of a PCollection (in this case, our PCollection of// formatted strings) to a text file.textio.Write(s, \"wordcounts.txt\", formatted)\n```\n## Additional Apache Beam and Dataflow benefits\nIf you choose to migrate your App Engine MapReduce jobs to Apache Beam pipelines, you will benefit from several features that Apache Beam and Dataflow have to offer.\n### Scheduling Cloud Dataflow jobs\nIf you are familiar with App Engine [task queues](/appengine/docs/standard/python/taskqueue) , you can [schedule your recurring jobs using Cron](/appengine/docs/standard/python/config/cron) . This [example](https://cloud.google.com/blog/products/bigquery/designing-etl-architecture-for-a-cloud-native-data-warehouse-on-google-cloud-platform) demonstrates how to use App Engine Cron to schedule your Apache Beam pipelines.\nThere are several additional ways to schedule execution of your pipeline. You can:\n- Use Dataflow [templates](/dataflow/docs/templates/overview) to stage your pipelines on Cloud Storage and execute them from a variety of environments.\n- Use [App Engine Cron service or Cloud Functions](https://cloud.google.com/blog/products/data-analytics/scheduling-dataflow-pipelines-using-app-engine-cron-service-or-cloud-functions) .\n- Use [Apache Airflow](https://airflow.incubator.apache.org/) .\n### Monitoring Cloud Dataflow jobs\nTo [monitor your App Engine MapReduce jobs](https://github.com/GoogleCloudPlatform/appengine-mapreduce/wiki/2.2-Monitoring-a-Job) , you depend on an `appspot.com` -hosted URL.\nWhen you execute your pipelines using the Dataflow managed service, you can monitor the pipelines using the convenient Dataflow web-based [monitoring user interface](/dataflow/pipelines/dataflow-monitoring-intf) . You can also use [Cloud Monitoring](/dataflow/docs/guides/using-cloud-monitoring) for additional information about your pipelines.\n### Reading and writing\nIn Apache Beam, App Engine MapReduce's [Readers and Writers](https://github.com/GoogleCloudPlatform/appengine-mapreduce/wiki/3.4-Readers-and-Writers) are called data sources and sinks, or [I/O connectors](https://beam.apache.org/documentation/io/built-in/) .\nApache Beam has many I/O connectors for several Google Cloud services, such as Bigtable, BigQuery, Datastore, Cloud SQL, and others. There are also I/O connectors created by Apache Beam contributors for non-Google services, such as Apache Cassandra and MongoDB.\n## What's next\n- [Learn about the Apache Beam Programming Model](/dataflow/model/programming-model-beam) \n- [Learn how to design, create, and test your pipeline](/dataflow/pipelines/creating-a-pipeline-beam) \n- [Work through the example walkthroughs](/dataflow/examples/examples-beam)", "guide": "Dataflow"}