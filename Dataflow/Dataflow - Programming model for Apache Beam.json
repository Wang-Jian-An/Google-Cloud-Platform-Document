{"title": "Dataflow - Programming model for Apache Beam", "url": "https://cloud.google.com/dataflow/docs/concepts/beam-programming-model", "abstract": "# Dataflow - Programming model for Apache Beam\nDataflow is based on the open-source Apache Beam project. This document describes the Apache Beam programming model.\n", "content": "## Overview\nApache Beam is an open source, unified model for defining both batch and streaming pipelines. The Apache Beam programming model simplifies the mechanics of large-scale data processing. Using one of the Apache Beam SDKs, you build a program that defines the pipeline. Then, you execute the pipeline on a specific platform such as Dataflow. This model lets you concentrate on the logical composition of your data processing job, rather than managing the orchestration of parallel processing.\nApache Beam insulates you from the low-level details of distributed processing, such as coordinating individual workers, sharding datasets, and other such tasks. Dataflow fully manages these low-level details.\nA is a graph of transformations that are applied to collections of data. In Apache Beam, a collection is called a `PCollection` , and a transform is called a `PTransform` . A `PCollection` can be bounded or unbounded. A `PCollection` has a known, fixed size, and can be processed using a batch pipeline. Unbounded `PCollections` must use a streaming pipeline, because the data is processed as it arrives.\nApache Beam provides connectors to read from and write to different systems, including Google Cloud services and third-party technologies such as Apache Kafka.\nThe following diagram shows an Apache Beam pipeline.\nYou can write `PTransforms` that perform arbitrary logic. The Apache Beam SDKs also provide a library of useful `PTransforms` out of the box, including the following:\n- Filter out all elements that don't satisfy a predicate.\n- Apply a 1-to-1 mapping function over each element.\n- Group elements by key.\n- Count the elements in a collection\n- Count the elements associated with each key in a key-value collection.\nTo run an Apache Beam pipeline using Dataflow, perform the following steps:\n- Use the Apache Beam SDK to define and build the pipeline. Alternatively, you can deploy a prebuilt pipeline by using a Dataflow template.\n- Use Dataflow to run the pipeline. Dataflow allocates a pool of VMs to run the job, deploys the code to the VMs, and orchestrates running the job.\n- Dataflow performs optimizations on the backend to make your pipeline run efficiently and take advantage of parallelization.\n- While a job is running and after it completes, use Dataflow management capabilities to monitor progress and troubleshoot.## Apache Beam concepts\nThis section contains summaries of fundamental concepts.\n### Basic concepts### Advanced concepts\n## What's next\n- To learn more about the basic concepts of building pipelines using the Apache Beam SDKs, see the [Apache Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/) in the Apache Beam documentation.\n- For more details about the Apache Beam capabilities supported by Dataflow, see the [Apache Beam capability matrix](https://beam.apache.org/documentation/runners/capability-matrix/) .", "guide": "Dataflow"}