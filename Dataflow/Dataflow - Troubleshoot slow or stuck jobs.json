{"title": "Dataflow - Troubleshoot slow or stuck jobs", "url": "https://cloud.google.com/dataflow/docs/guides/troubleshoot-slow-jobs", "abstract": "# Dataflow - Troubleshoot slow or stuck jobs\nThis page explains how to troubleshoot common causes of slow or stuck Dataflow streaming and batch jobs.\n", "content": "## Streaming\nIf you notice the following symptoms, your Dataflow streaming job might be running slowly or stuck:\n- The pipeline isn't reading data from the source. For example, Pub/Sub has a growing backlog.\n- The pipeline isn't writing data to the sink.\n- The [data freshness metric](/dataflow/docs/guides/using-monitoring-intf#data_freshness_streaming) is increasing.\n- The [system latency metric](/dataflow/docs/guides/using-monitoring-intf#system_latency_streaming) is increasing.\nUse the information in the following sections to identify and diagnose the problem.\n### Investigate repeated failures\nIn a streaming job, some failures are retried indefinitely. These retries prevent the pipeline from progressing. To identify repeated failures, check the worker logs for exceptions.\n- If the exception is with user code, debug and fix the issue in the code or in the data.\n- To prevent unexpected failures from stalling your pipeline, implement a dead-letter queue. For an example implementation, see [BigQuery patterns](https://beam.apache.org/documentation/patterns/bigqueryio/) in the Apache Beam documentation.\n- If the exception is an out of memory (OOM) error, see [Troubleshoot Dataflow out of memory errors](/dataflow/docs/guides/troubleshoot-oom) .\n- For other exceptions, see [Troubleshoot Dataflow errors](/dataflow/docs/guides/common-errors) .\n### Identify unhealthy workers\nIf the workers processing the streaming job are unhealthy, the job might be slow or appear stuck. To identify unhealthy workers:\n- Check for memory pressure by using the [memory utilization metrics](/dataflow/docs/guides/using-monitoring-intf#memory-use) and by looking for out of memory errors in the worker logs. For more information, see [Troubleshoot Dataflow out of memory errors](/dataflow/docs/guides/troubleshoot-oom) .\n- If you're using Streaming Engine, use the [persistence metrics](/dataflow/docs/guides/using-monitoring-intf#persistence_streaming) to identify bottlenecks with the disk input/output operations (IOPS).\n- Check the worker logs for other errors. For more information, see [Work with pipeline logs](/dataflow/docs/guides/logging) and [Troubleshoot Dataflow errors](/dataflow/docs/guides/common-errors) .\n### Identify stragglers\nA straggler is a work item that is slow relative to other work items in the stage. For information about identifying and fixing stragglers, see [Troubleshoot stragglers in streaming jobs](/dataflow/docs/guides/troubleshoot-streaming-stragglers) .\n### Troubleshoot insufficient parallelism\nFor scalability and efficiency, Dataflow runs the stages of your pipeline in parallel across multiple workers. The smallest unit of parallel processing in Dataflow is a key. Incoming messages for each fused stage are associated with a key. The key is defined in one of the following ways:\n- The key is implicitly defined by the properties of the source, such as Pub/Sub partitions.\n- The key is explicitly defined by aggregation logic in the pipeline, such as`GroupByKey`.\nIf the pipeline doesn't have enough keys for a given stage, it limits parallel processing. That stage might become a bottleneck.\nTo identify if pipeline slowness is caused by low parallelism, view the [CPU utilization metrics](/dataflow/docs/guides/using-monitoring-intf#cpu-use) . If CPU is low but evenly distributed across workers, your job might have insufficient parallelism. If your job is using Streaming Engine, to see if a stage has low parallelism, in the **Job Metrics** tab, view the [parallelism metrics](/dataflow/docs/guides/using-monitoring-intf#parallelism) . To mitigate this issue:\n- In the Google Cloud console, on the **Job info** page, use the [Autoscaling tab](/dataflow/docs/guides/autoscaling-metrics) to see if the job is having problems scaling up. If autoscaling is the problem, see [Troubleshoot Dataflow autoscaling](/dataflow/docs/guides/troubleshoot-autoscaling) .\n- Use the [job graph](/dataflow/docs/guides/job-graph) to check the steps in the stage. If the stage is reading from a source or writing to a sink, review the documentation for the service of the source or sink. Use the documentation to determine if that service is configured for sufficient scalability.- To gather more information, use the [input and output metrics](/dataflow/docs/guides/using-monitoring-intf#input-output) provided by Dataflow.\n- If you're using Kafka, check the number of Kafka partitions. For more information, see the [Apache Kafka](https://kafka.apache.org/) documentation.\n- If you're using a BigQuery sink, enable automatic sharding to improve parallelism. For more information, see [3x Dataflow Throughput with Auto Sharding for BigQuery](https://cloud.google.com/blog/products/data-analytics/3x-dataflow-throughput-auto-sharding-bigquery) .\nIf tasks are unevenly distributed across workers and worker utilization is very uneven, your pipeline might have a hot key. A hot key is a key that has far more elements to process compared to other keys. To resolve this issue, take one or more of the following steps:\n- Rekey your data. To output new key-value pairs, apply a`ParDo`transform. For more information, see the [Java ParDo transform page](https://beam.apache.org/documentation/transforms/java/elementwise/pardo/) or the [Python ParDo transform page](https://beam.apache.org/documentation/transforms/python/elementwise/pardo/) in the Apache Beam documentation.\n- Use`.withFanout`in your combine transforms. For more information, see the [Combine.PerKey](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/transforms/Combine.PerKey.html) class in the Java SDK or the [with_hot_key_fanout](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.core.html#apache_beam.transforms.core.CombinePerKey.with_hot_key_fanout) operation in the Python SDK.\n- If you have a Java pipeline that processes high-volume unbounded`PCollections`, we recommend that you do the following:- Use`Combine.Globally.withFanout`instead of`Combine.Globally`.\n- Use`Combine.PerKey.withHotKeyFanout`instead of`Count.PerKey`.\n### Check for insufficient quota\nMake sure you have sufficient quota for your source and sink. For example, if your pipeline reads input from Pub/Sub or BigQuery, your Google Cloud project might have insufficient quota. For more information about quota limits for these services, see [Pub/Sub quota](/pubsub/quotas) or [BigQuery quota](/bigquery/quotas) .\nIf your job is generating a high number of `429 (Rate Limit Exceeded)` errors, it might have insufficient quota. To check for errors, try the following steps:\n- Go to the [Google Cloud console](https://console.cloud.google.com/) .\n- In the navigation pane, click **APIs & services** .\n- In the menu, click **Library** .\n- Use the search box to search for **Pub/Sub** .\n- Click **Cloud Pub/Sub API** .\n- Click **Manage** .\n- In the **Traffic by response code** chart, look for`(4xx)`client error codes.\nYou can also use [Metrics Explorer](/monitoring/charts/metrics-explorer) to check quota usage. If your pipeline uses a BigQuery source or sink, to troubleshoot quota issues, use the [BigQuery Storage API metrics](/monitoring/api/metrics_gcp#gcp-bigquerystorage) . For example, to create a chart showing the BigQuery concurrent connection count, follow these steps:\n- In the Google Cloud console, select **Monitoring** : [Go to Monitoring](https://console.cloud.google.com/monitoring) \n- In the navigation pane, select **Metrics explorer** .\n- In the **Select a metric** pane, for **Metric** , filter to **BigQuery Project** > **Write** > **concurrent connection count** .\nFor instructions about viewing Pub/Sub metrics, see [Monitor quota usage](/pubsub/docs/monitoring#quota) in \"Monitor Pub/Sub in Cloud Monitoring.\" For instructions about viewing BigQuery metrics, see [View quota usage and limits](/bigquery/docs/monitoring-dashboard#view_quota_usage_and_limits) in \"Create dashboards, charts, and alerts.\"\n## Batch\nIf your batch job is slow or stuck, use the [Execution details](/dataflow/docs/concepts/execution-details) tab to find more information about the job and to identify the stage or worker that's causing a bottleneck.\n### Identify stragglers\nA straggler is a work item that is slow relative to other work items in the stage. For information about identifying and fixing stragglers, see [Troubleshoot stragglers in batch jobs](/dataflow/docs/guides/troubleshoot-batch-stragglers) .\n### Identify slow or stuck stages\nTo identify slow or stuck stages, use the [Stage progress](/dataflow/docs/concepts/execution-details#progress-batch) view. Longer bars indicate that the stage takes more time. Use this view to identify the slowest stages in your pipeline.\nAfter you find the bottleneck stage, you can take the following steps:\n- Identify the [lagging worker](#lagging-worker) within that stage.\n- If there are no lagging workers, identify the slowest step by using the [Stage info](/dataflow/docs/concepts/execution-details#stage-info) panel. Use this information to identify candidates for user code optimization.\n- To find parallelism bottlenecks, use [Dataflow monitoring metrics](#debug-tools) .\n### Identify a lagging worker\nTo identify a lagging worker for a specific stage, use the [Worker progress](/dataflow/docs/concepts/execution-details#worker-progress) view. This view shows whether all workers are processing work until the end of the stage, or if a single worker is stuck on a lagging task. If you find a lagging worker, take the following steps:\n- View the log files for that worker. For more information, see [Monitor and view pipeline logs](/dataflow/docs/guides/logging#MonitoringLogs) .\n- View the [CPU utilization metrics](/dataflow/docs/guides/using-monitoring-intf#cpu-use) and the [worker progress](/dataflow/docs/concepts/execution-details#worker-progress) details for lagging workers. If you see unusually high or low CPU utilization, in the log files for that worker, look for the following issues:- [A hot key ... was detected](/dataflow/docs/guides/common-errors#hot-key-detected) \n- [Processing stuck ... Operation ongoing](/dataflow/docs/guides/common-errors#processing-stuck) \n## Tools for debugging\nWhen you have a slow or stuck pipeline, the following tools can help you diagnose the problem.\n- To correlate incidents and identify bottlenecks, use [Cloud Monitoring for Dataflow](/dataflow/docs/guides/using-cloud-monitoring) .\n- To monitor pipeline performance, use [Cloud Profiler](/dataflow/docs/guides/profiling-a-pipeline) .\n- Some transforms are better suited to high-volume pipelines than others. Log messages can [identify a stuck user transform](/dataflow/docs/guides/common-errors#processing-stuck) in either batch or streaming pipelines.\n- To learn more about a stuck job, use [Dataflow job metrics](/dataflow/docs/guides/using-monitoring-intf) . The following list includes useful metrics:- The [Backlog bytes](/dataflow/docs/guides/using-monitoring-intf#backlog) metric (`backlog_bytes`) measures the amount of unprocessed input in bytes by stage. Use this metric to find a fused step that has no throughput. Similarly, the backlog elements metric (`backlog_elements`) measures the number of unprocessed input elements for a stage.\n- The [Processing parallelism keys](/dataflow/docs/guides/using-monitoring-intf#parallelism) (`processing_parallelism_keys`) metric measures the number of parallel processing keys for a particular stage of the pipeline over the last five minutes. Use this metric to investigate in the following ways:- Narrow the issue down to specific stages and confirm hot key warnings, such as [A hot key ... was detected](/dataflow/docs/guides/common-errors#hot-key-detected) .\n- Find throughput bottlenecks caused by insufficient parallelism. These bottlenecks can result in slow or stuck pipelines.\n- The [System lag](/dataflow/docs/guides/using-monitoring-intf#system_latency_streaming) metric (`system_lag`) and the per-stage system lag metric (`per_stage_system_lag`) measure the maximum amount of time an item of data has been processing or awaiting processing. Use these metrics to identify inefficient stages and bottlenecks from data sources.For additional metrics that aren't included in the Dataflow monitoring web interface, see the complete list of Dataflow metrics in [Google Cloud metrics](/monitoring/api/metrics_gcp#gcp-dataflow) .", "guide": "Dataflow"}