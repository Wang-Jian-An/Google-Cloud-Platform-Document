{"title": "Dataflow - \u5f9e BigQuery \u8b80\u53d6\u5230 Dataflow", "url": "https://cloud.google.com/dataflow/docs/guides/read-from-bigquery?hl=zh-cn", "abstract": "# Dataflow - \u5f9e BigQuery \u8b80\u53d6\u5230 Dataflow\n\u672c\u6587\u6a94\u4ecb\u7d39\u5982\u4f55\u4f7f\u7528 Apache Beam [BigQuery I/O \u9023\u63a5\u5668](https://beam.apache.org/documentation/io/built-in/google-bigquery/) \u5c07\u6578\u64da\u5f9e BigQuery \u8b80\u53d6\u5230 Dataflow\u3002\n**\u6ce8\u610f** \uff1a\u6839\u64da\u60a8\u7684\u5834\u666f\uff0c\u8acb\u8003\u616e\u4f7f\u7528 [Google \u63d0\u4f9b\u7684 Dataflow \u6a21\u677f](https://cloud.google.com/dataflow/docs/guides/templates/provided-templates?hl=zh-cn) \u4e4b\u4e00\u3002 \u5176\u4e2d\u4e00\u4e9b\u5f9e BigQuery \u4e2d\u8b80\u53d6\u3002\n", "content": "## \u6982\u89bd\nBigQuery I/O \u9023\u63a5\u5668\u652f\u6301\u5f9e BigQuery \u8b80\u53d6\u6578\u64da\u7684\u5169\u500b\u9078\u9805\uff1a\n- \u76f4\u63a5\u8868\u8b80\u53d6\u3002\u9019\u500b\u9078\u9805\u662f\u6700\u5feb\u7684\uff0c\u56e0\u7232\u5b83\u4f7f\u7528 [BigQuery Storage Read API](https://cloud.google.com/bigquery/docs/reference/storage?hl=zh-cn) \u3002\n- \u5c0e\u51fa\u4f5c\u696d\u3002\u5982\u679c\u4f7f\u7528\u6b64\u9078\u9805\uff0cBigQuery \u6703\u904b\u884c [\u5c0e\u51fa\u4f5c\u696d](https://cloud.google.com/bigquery/docs/exporting-data?hl=zh-cn) \uff0c\u4ee5\u5c07\u8868\u6578\u64da\u5beb\u5165 Cloud Storage\u3002\u7136\u5f8c\uff0c\u9023\u63a5\u5668\u6703\u5f9e Cloud Storage \u8b80\u53d6\u5c0e\u51fa\u7684\u6578\u64da\u3002\u6b64\u9078\u9805\u7684\u6548\u7387\u8f03\u4f4e\uff0c\u56e0\u7232\u5b83\u9700\u8981\u5c0e\u51fa\u6b65\u9a5f\u3002\n\u5c0e\u51fa\u4f5c\u696d\u662f\u9ed8\u8a8d\u9078\u9805\u3002\u5982\u9700\u6307\u5b9a\u76f4\u63a5\u8b80\u53d6\uff0c\u8acb\u8abf\u7528 `withMethod(Method.DIRECT_READ)` \u3002\n\u9023\u63a5\u5668\u5c07\u8868\u6578\u64da\u5e8f\u5217\u5316\u7232 `PCollection` \u3002 `PCollection` \u4e2d\u7684\u6bcf\u500b\u5143\u7d20\u8868\u793a\u4e00\u500b\u9336\u884c\u3002\u9023\u63a5\u5668\u652f\u6301\u4ee5\u4e0b\u5e8f\u5217\u5316\u65b9\u6cd5\uff1a\n- [\u4ee5 Avro \u8a18\u9304\u7684\u5f62\u5f0f\u8b80\u53d6\u6578\u64da](#read_avro-formatted_records) \u3002\u901a\u904e\u6b64\u65b9\u6cd5\uff0c\u60a8\u53ef\u4ee5\u63d0\u4f9b\u4e00\u500b\u51fd\u6578\uff0c\u7528\u65bc\u5c07 Avro \u8a18\u9304\u89e3\u6790\u7232\u81ea\u5b9a\u7fa9\u6578\u64da\u985e\u578b\u3002\n- [\u4ee5 TableRow \u5c0d\u8c61\u7684\u5f62\u5f0f\u8b80\u53d6\u6578\u64da](#read_tablerow_objects) \u3002\u9019\u7a2e\u65b9\u6cd5\u5f88\u65b9\u4fbf\uff0c\u56e0\u7232\u4e0d\u9700\u8981\u81ea\u5b9a\u7fa9\u6578\u64da\u985e\u578b\u3002\u4f46\u662f\uff0c\u5b83\u7684\u6027\u80fd\u901a\u5e38\u4f4e\u65bc\u8b80\u53d6 Avro \u683c\u5f0f\u7684\u8a18\u9304\u3002## \u4e26\u884c\u6578\u91cf\n\u6b64\u9023\u63a5\u5668\u4e2d\u7684\u4e26\u884c\u6027\u53d6\u6c7a\u65bc\u8b80\u53d6\u65b9\u6cd5\uff1a\n- \u76f4\u63a5\u8b80\u53d6\uff1aI/O \u9023\u63a5\u5668\u6703\u6839\u64da\u5c0e\u51fa\u8acb\u6c42\u7684\u5927\u5c0f\u751f\u6210\u52d5\u614b\u6578\u91cf\u7684\u6d41\u3002\u5b83\u76f4\u63a5\u5f9e BigQuery \u4e26\u884c\u8b80\u53d6\u9019\u4e9b\u6d41\u3002\n- \u5c0e\u51fa\u4f5c\u696d\uff1aBigQuery \u78ba\u5b9a\u8981\u5beb\u5165 Cloud Storage \u7684\u6587\u4ef6\u6578\u91cf\u3002\u6587\u4ef6\u6578\u53d6\u6c7a\u65bc\u67e5\u8a62\u548c\u6578\u64da\u91cf\u3002I/O \u9023\u63a5\u5668\u6703\u4e26\u884c\u8b80\u53d6\u5c0e\u51fa\u7684\u6587\u4ef6\u3002## \u6027\u80fd\n\u4e0b\u8868\u986f\u793a\u4e86\u5404\u7a2e BigQuery I/O \u8b80\u53d6\u9078\u9805\u7684\u6027\u80fd\u6307\u6a19\u3002\u5de5\u4f5c\u8ca0\u8f09\u4f7f\u7528 Java \u7248 Apache Beam SDK 2.49.0 \u5728\u4e00\u500b `e2-standard2` \u5de5\u4f5c\u5668\u4e0a\u904b\u884c\u3002\u5b83\u5011\u672a\u4f7f\u7528 Runner v2\u3002\n| 1 \u5104\u689d\u8a18\u9304 | 1 KB | 1 \u5217 | \u541e\u5410\u91cf\uff08\u5b57\u7bc0\uff09 | \u541e\u5410\u91cf\uff08\u5143\u7d20\uff09  |\n|:---------------------------|:-----------------|:-------------------|\n| Storage Read    | 120 MBps   | \u6bcf\u79d2 88,000 \u500b\u5143\u7d20 |\n| Avro Export    | 105 MBps   | \u6bcf\u79d2 78,000 \u500b\u5143\u7d20 |\n| Json Export    | 110 MBps   | \u6bcf\u79d2 81,000 \u500b\u5143\u7d20 |\n\u9019\u4e9b\u6307\u6a19\u57fa\u65bc\u7c21\u55ae\u7684\u6279\u8655\u7406\u6d41\u6c34\u7dda\u3002\u5b83\u5011\u65e8\u5728\u6bd4\u8f03 I/O \u9023\u63a5\u5668\u4e4b\u9593\u7684\u6027\u80fd\uff0c\u4e0d\u4e00\u5b9a\u4ee3\u8868\u5be6\u969b\u6d41\u6c34\u7dda\u3002Dataflow \u6d41\u6c34\u7dda\u6027\u80fd\u5f88\u8907\u96dc\uff0c\u5b83\u53d7\u5230\u591a\u500b\u56e0\u7d20\u7684\u5f71\u97ff\uff0c\u5305\u62ec\u865b\u64ec\u6a5f\u985e\u578b\u3001\u6b63\u5728\u8655\u7406\u7684\u6578\u64da\u91cf\u3001\u5916\u90e8\u4f86\u6e90\u548c\u63a5\u6536\u5668\u7684\u6027\u80fd\u4ee5\u53ca\u7528\u6236\u4ee3\u78bc\u3002\u6307\u6a19\u57fa\u65bc\u904b\u884c Java SDK\uff0c\u4e0d\u4ee3\u8868\u5176\u4ed6\u8a9e\u8a00 SDK \u7684\u6027\u80fd\u7279\u5fb5\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [Beam IO \u6027\u80fd](https://beam.apache.org/performance/) \u3002\n## \u6700\u4f73\u5be6\u8e10\n- \u901a\u5e38\uff0c\u6211\u5011\u5efa\u8b70\u4f7f\u7528\u76f4\u63a5\u8868\u8b80\u53d6 ( [Method.DIRECT_READ](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.TypedRead.Method.html) )\u3002Storage Read API \u6bd4\u5c0e\u51fa\u4f5c\u696d\u66f4\u9069\u5408\u6578\u64da\u6d41\u6c34\u7dda\uff0c\u56e0\u7232\u5b83\u4e0d\u9700\u8981\u5c0e\u51fa\u6578\u64da\u7684\u4e2d\u9593\u6b65\u9a5f\u3002\n- \u5982\u679c\u60a8\u4f7f\u7528\u76f4\u63a5\u8b80\u53d6\uff0c\u5247\u9700\u8981\u652f\u4ed8 Storage Read API \u7684\u4f7f\u7528\u8cbb\u3002\u8acb\u53c3\u95b1 BigQuery \u50f9\u683c\u9801\u9762\u4e2d\u7684 [\u6578\u64da\u63d0\u53d6\u50f9\u683c](https://cloud.google.com/bigquery/pricing?hl=zh-cn#data_extraction_pricing) \u3002\n- \u5c0e\u51fa\u4f5c\u696d\u4e0d\u7522\u751f\u984d\u5916\u8cbb\u7528\u3002\u4f46\u662f\uff0c\u5c0e\u51fa\u4f5c\u696d\u5b58\u5728\u4e00\u4e9b [\u9650\u5236](https://cloud.google.com/bigquery/quotas?hl=zh-cn#export_jobs) \u3002\u5982\u679c\u9700\u8981\u79fb\u52d5\u5927\u91cf\u6578\u64da\uff0c\u800c\u53ca\u6642\u6027\u6700\u7232\u91cd\u8981\u4e14\u8cbb\u7528\u53ef\u8abf\u6574\uff0c\u5247\u5efa\u8b70\u4f7f\u7528\u76f4\u63a5\u8b80\u53d6\u3002\n- Storage Read API \u5177\u6709 [\u914d\u984d\u9650\u5236](https://cloud.google.com/bigquery/quotas?hl=zh-cn#storage-limits) \u3002\u4f7f\u7528 [Google Cloud \u6307\u6a19](https://cloud.google.com/monitoring/api/metrics_gcp?hl=zh-cn#gcp-bigquerystorage) \u76e3\u63a7\u914d\u984d\u7528\u91cf\u3002\n- \u4f7f\u7528 Storage Read API \u6642\uff0c\u60a8\u53ef\u80fd\u6703\u5728\u65e5\u8a8c\u4e2d\u770b\u5230\u79df\u671f\u5230\u671f\u548c\u6703\u8a71\u8d85\u6642\u932f\u8aa4\uff0c\u4f8b\u5982\uff1a- `DEADLINE_EXCEEDED`\n- `Server Unresponsive`\n- `StatusCode.FAILED_PRECONDITION details = \"there was an error operating on 'projects/<projectID>/locations/<location>/sessions/<sessionID>/streams/<streamID>': session``\n\u7576\u64cd\u4f5c\u82b1\u8cbb\u7684\u6642\u9593\u8d85\u904e\u8d85\u6642\u503c\u6642\uff0c\u53ef\u80fd\u6703\u767c\u751f\u9019\u4e9b\u932f\u8aa4\uff0c\u901a\u5e38\u767c\u751f\u5728\u904b\u884c\u8d85\u904e 6 \u5c0f\u6642\u7684\u6d41\u6c34\u7dda\u4e2d\u3002\u7232\u4e86\u7de9\u89e3\u6b64\u554f\u984c\uff0c\u8acb\u6539\u7528\u6587\u4ef6\u5c0e\u51fa\u529f\u80fd\u3002\n- \u4f7f\u7528 Java SDK \u6642\uff0c\u8acb\u8003\u616e\u5275\u5efa\u4e00\u500b\u8868\u793a BigQuery \u8868\u67b6\u69cb\u7684\u985e\u3002\u7136\u5f8c\uff0c\u5728\u6d41\u6c34\u7dda\u4e2d\u8abf\u7528 [useBeamSchema](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#useBeamSchema--) \uff0c\u4ee5\u4fbf\u5728 Apache Beam `Row` \u548c BigQuery `TableRow` \u985e\u578b\u4e4b\u9593\u81ea\u52d5\u8f49\u63db\u3002\u5982\u9700\u67e5\u770b\u67b6\u69cb\u985e\u7684\u793a\u4f8b\uff0c\u8acb\u53c3\u95b1 [ExampleModel.java](https://github.com/GoogleCloudPlatform/cloud-code-samples/blob/v1/java/java-dataflow-samples/read-pubsub-write-bigquery/src/main/java/com/cloudcode/dataflow/ExampleModel.java) \u3002## \u793a\u4f8b\n\u672c\u90e8\u5206\u4e2d\u7684\u4ee3\u78bc\u793a\u4f8b\u4f7f\u7528\u76f4\u63a5\u8868\u8b80\u53d6\u3002\n\u5982\u9700\u6539\u7528\u5c0e\u51fa\u4f5c\u696d\uff0c\u8acb\u7701\u7565\u5c0d `withMethod` \u7684\u8abf\u7528\u6216\u6307\u5b9a `Method.EXPORT` \u3002\u7136\u5f8c\u8a2d\u7f6e `--tempLocation` [\u6d41\u6c34\u7dda\u9078\u9805](https://cloud.google.com/dataflow/docs/reference/pipeline-options?hl=zh-cn#basic_options) \uff0c\u7232\u5c0e\u51fa\u7684\u6587\u4ef6\u6307\u5b9a Cloud Storage \u5b58\u5132\u6876\u3002\n\u9019\u4e9b\u4ee3\u78bc\u793a\u4f8b\u5047\u5b9a\u6e90\u8868\u6709\u4ee5\u4e0b\u5217\uff1a\n- `name`\uff08\u5b57\u7b26\u4e32\uff09\n- `age`\uff08\u6574\u6578\uff09\n\u6307\u5b9a\u7232 [JSON \u67b6\u69cb\u6587\u4ef6](https://cloud.google.com/bigquery/docs/schemas?hl=zh-cn#specifying_a_json_schema_file) \uff1a\n```\n[\u00a0 {\"name\":\"user_name\",\"type\":\"STRING\",\"mode\":\"REQUIRED\"},\u00a0 {\"name\":\"age\",\"type\":\"INTEGER\",\"mode\":\"REQUIRED\"}]\n```\n### \u8b80\u53d6 Avro \u683c\u5f0f\u7684\u8a18\u9304\n\u8981\u5c07 BigQuery \u6578\u64da\u8b80\u53d6\u7232 Avro \u683c\u5f0f\u7684\u8a18\u9304\uff0c\u8acb\u4f7f\u7528 [read(SerializableFunction)](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.html#read-org.apache.beam.sdk.transforms.SerializableFunction-) \u65b9\u6cd5\u3002\u6b64\u65b9\u6cd5\u63a1\u7528\u61c9\u7528\u5b9a\u7fa9\u7684\u51fd\u6578\uff0c\u8a72\u51fd\u6578\u6703\u89e3\u6790 [SchemaAndRecord](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/SchemaAndRecord.html) \u5c0d\u8c61\u4e26\u8fd4\u56de\u81ea\u5b9a\u7fa9\u6578\u64da\u985e\u578b\u3002\u9023\u63a5\u5668\u7684\u8f38\u51fa\u662f\u81ea\u5b9a\u7fa9\u6578\u64da\u985e\u578b\u7684 `PCollection` \u3002\n\u4ee5\u4e0b\u4ee3\u78bc\u5f9e BigQuery \u8868\u4e2d\u8b80\u53d6 `PCollection<MyData>` \uff0c\u5176\u4e2d `MyData` \u662f\u61c9\u7528\u5b9a\u7fa9\u7684\u985e\u3002\n\u5982\u9700\u5411 Dataflow \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002  \u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BigQueryReadAvro.java) \n```\nimport org.apache.avro.generic.GenericRecord;import org.apache.avro.util.Utf8;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.coders.DefaultCoder;import org.apache.beam.sdk.extensions.avro.coders.AvroCoder;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.TypedRead;import org.apache.beam.sdk.io.gcp.bigquery.SchemaAndRecord;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.transforms.SerializableFunction;import org.apache.beam.sdk.values.TypeDescriptor;public class BigQueryReadAvro {\u00a0 // A custom datatype to hold a record from the source table.\u00a0 @DefaultCoder(AvroCoder.class)\u00a0 public static class MyData {\u00a0 \u00a0 public String name;\u00a0 \u00a0 public Long age;\u00a0 \u00a0 // Function to convert Avro records to MyData instances.\u00a0 \u00a0 public static class FromSchemaAndRecord\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 implements SerializableFunction<SchemaAndRecord, MyData> {\u00a0 \u00a0 \u00a0 @Override public MyData apply(SchemaAndRecord elem) {\u00a0 \u00a0 \u00a0 \u00a0 MyData data = new MyData();\u00a0 \u00a0 \u00a0 \u00a0 GenericRecord record = elem.getRecord();\u00a0 \u00a0 \u00a0 \u00a0 data.name = ((Utf8) record.get(\"user_name\")).toString();\u00a0 \u00a0 \u00a0 \u00a0 data.age = (Long) record.get(\"age\");\u00a0 \u00a0 \u00a0 \u00a0 return data;\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\u00a0 public static void main(String[] args) {\u00a0 \u00a0 // Parse the pipeline options passed into the application. Example:\u00a0 \u00a0 // \u00a0 --projectId=$PROJECT_ID --datasetName=$DATASET_NAME --tableName=$TABLE_NAME\u00a0 \u00a0 // For more information, see https://beam.apache.org/documentation/programming-guide/#configuring-pipeline-options\u00a0 \u00a0 PipelineOptionsFactory.register(ExamplePipelineOptions.class);\u00a0 \u00a0 ExamplePipelineOptions options = PipelineOptionsFactory.fromArgs(args)\u00a0 \u00a0 \u00a0 \u00a0 .withValidation()\u00a0 \u00a0 \u00a0 \u00a0 .as(ExamplePipelineOptions.class);\u00a0 \u00a0 // Create a pipeline and apply transforms.\u00a0 \u00a0 Pipeline pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 // Read table data into Avro records, using an application-defined parsing function.\u00a0 \u00a0 \u00a0 \u00a0 .apply(BigQueryIO.read(new MyData.FromSchemaAndRecord())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .from(String.format(\"%s:%s.%s\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getProjectId(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getDatasetName(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getTableName()))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withMethod(TypedRead.Method.DIRECT_READ))\u00a0 \u00a0 \u00a0 \u00a0 // The output from the previous step is a PCollection<MyData>.\u00a0 \u00a0 \u00a0 \u00a0 .apply(MapElements\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .into(TypeDescriptor.of(MyData.class))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via((MyData x) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Name: %s, Age: %d%n\", x.name, x.age);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return x;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }));\u00a0 \u00a0 pipeline.run().waitUntilFinish();\u00a0 }}\n```\n`read` \u65b9\u6cd5\u63a5\u53d7\u4e00\u500b `SerializableFunction<SchemaAndRecord, T>` \u63a5\u53e3\uff0c\u5176\u5b9a\u7fa9\u4e00\u500b\u5f9e Avro \u8a18\u9304\u8f49\u63db\u7232\u81ea\u5b9a\u7fa9\u6578\u64da\u985e\u7684\u51fd\u6578\u3002\u5728\u524d\u9762\u7684\u4ee3\u78bc\u793a\u4f8b\u4e2d\uff0c `MyData.apply` \u65b9\u6cd5\u5be6\u73fe\u4e86\u6b64\u8f49\u63db\u51fd\u6578\u3002\u793a\u4f8b\u51fd\u6578\u6703\u89e3\u6790 Avro \u8a18\u9304\u4e2d\u7684 `name` \u548c `age` \u5b57\u6bb5\uff0c\u4e26\u8fd4\u56de `MyData` \u5be6\u4f8b\u3002\n**\u6ce8\u610f** \uff1a\u5982\u679c\u89e3\u6790\u51fd\u6578\u8207 BigQuery \u8868\u7684\u5be6\u969b\u67b6\u69cb\u4e0d\u5339\u914d\uff0c\u5247\u6b64\u4ee3\u78bc\u6703\u5931\u6557\u3002\u5177\u9ad4\u800c\u8a00\uff0c\u547d\u540d\u5217\u5fc5\u9808\u5b58\u5728\uff0c\u4e14\u6578\u64da\u985e\u578b\u5fc5\u9808\u517c\u5bb9\u3002\u6b64\u5916\uff0c\u5982\u679c\u8868\u4e2d\u7684\u4efb\u4f55\u5217\u53ef\u7232 null\uff0c\u5247\u60a8\u7684\u4ee3\u78bc\u5fc5\u9808\u8655\u7406 null \u503c\u3002\n\u5982\u9700\u6307\u5b9a\u8981\u8b80\u53d6\u7684 BigQuery \u8868\uff0c\u8acb\u8abf\u7528 `from` \u65b9\u6cd5\uff0c\u5982\u4ee5\u4e0a\u793a\u4f8b\u6240\u793a\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 BigQuery I/O \u9023\u63a5\u5668\u6587\u6a94\u4e2d\u7684 [\u8868\u540d\u7a31](https://beam.apache.org/documentation/io/built-in/google-bigquery/#table-names) \u3002\n### \u8b80\u53d6 TableRow \u5c0d\u8c61\n[readTableRows](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.html#readTableRows--) \u65b9\u6cd5\u5c07 BigQuery \u6578\u64da\u8b80\u53d6\u5230 [TableRow](https://developers.google.com/resources/api-libraries/documentation/bigquery/v2/java/latest/com/google/api/services/bigquery/model/TableRow.html?hl=zh-cn) \u5c0d\u8c61\u7684 `PCollection` \u4e2d\u3002\u6bcf\u500b `TableRow` \u90fd\u662f\u4e00\u500b\u9375\u503c\u5c0d\u6620\u5c04\uff0c\u5305\u542b\u4e00\u884c\u8868\u6578\u64da\u3002\u901a\u904e\u8abf\u7528 `from` \u65b9\u6cd5\u6307\u5b9a\u8981\u8b80\u53d6\u7684 BigQuery \u8868\u3002\n\u4ee5\u4e0b\u4ee3\u78bc\u5f9e BigQuery \u8868\u4e2d\u8b80\u53d6 `PCollection<TableRows>` \u3002\n\u5982\u9700\u5411 Dataflow \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002  \u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BiqQueryReadTableRows.java) \n```\nimport com.google.api.services.bigquery.model.TableRow;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.TypedRead.Method;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.values.TypeDescriptor;public class BiqQueryReadTableRows {\u00a0 public static void main(String[] args) {\u00a0 \u00a0 // Parse the pipeline options passed into the application. Example:\u00a0 \u00a0 // \u00a0 --projectId=$PROJECT_ID --datasetName=$DATASET_NAME --tableName=$TABLE_NAME\u00a0 \u00a0 // For more information, see https://beam.apache.org/documentation/programming-guide/#configuring-pipeline-options\u00a0 \u00a0 PipelineOptionsFactory.register(ExamplePipelineOptions.class);\u00a0 \u00a0 ExamplePipelineOptions options = PipelineOptionsFactory.fromArgs(args)\u00a0 \u00a0 \u00a0 \u00a0 .withValidation()\u00a0 \u00a0 \u00a0 \u00a0 .as(ExamplePipelineOptions.class);\u00a0 \u00a0 // Create a pipeline and apply transforms.\u00a0 \u00a0 Pipeline pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 // Read table data into TableRow objects.\u00a0 \u00a0 \u00a0 \u00a0 .apply(BigQueryIO.readTableRows()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .from(String.format(\"%s:%s.%s\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getProjectId(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getDatasetName(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getTableName()))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withMethod(Method.DIRECT_READ)\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 // The output from the previous step is a PCollection<TableRow>.\u00a0 \u00a0 \u00a0 \u00a0 .apply(MapElements\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .into(TypeDescriptor.of(TableRow.class))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Use TableRow to access individual fields in the row.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via((TableRow row) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 var name = (String) row.get(\"user_name\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 var age = (String) row.get(\"age\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Name: %s, Age: %s%n\", name, age);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return row;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }));\u00a0 \u00a0 pipeline.run().waitUntilFinish();\u00a0 }}\n```\n\u6b64\u793a\u4f8b\u9084\u5c55\u793a\u77ad\u5982\u4f55\u8a2a\u554f `TableRow` \u5b57\u5178\u4e2d\u7684\u503c\u3002\u6574\u6578\u503c\u7de8\u78bc\u7232\u5b57\u7b26\u4e32\uff0c\u4ee5\u5339\u914d BigQuery \u5c0e\u51fa\u7684 JSON \u683c\u5f0f\u3002\n**\u6ce8\u610f** \uff1a\u5982\u679c\u89e3\u6790\u51fd\u6578\u8207 BigQuery \u8868\u7684\u5be6\u969b\u67b6\u69cb\u4e0d\u5339\u914d\uff0c\u5247\u6b64\u4ee3\u78bc\u6703\u5931\u6557\u3002\u5177\u9ad4\u800c\u8a00\uff0c\u547d\u540d\u5217\u5fc5\u9808\u5b58\u5728\uff0c\u4e14\u6578\u64da\u985e\u578b\u5fc5\u9808\u517c\u5bb9\u3002\u6b64\u5916\uff0c\u5982\u679c\u8868\u4e2d\u7684\u4efb\u4f55\u5217\u53ef\u7232 null\uff0c\u5247\u60a8\u7684\u4ee3\u78bc\u5fc5\u9808\u8655\u7406 null \u503c\u3002\n### \u5217\u6295\u5f71\u548c\u904e\u6ffe\n\u4f7f\u7528\u76f4\u63a5\u8b80\u53d6 ( `Method.DIRECT_READ` ) \u6642\uff0c\u901a\u904e\u6e1b\u5c11\u5f9e BigQuery \u8b80\u53d6\u6578\u64da\u548c\u901a\u904e\u7db2\u7d61\u767c\u9001\u7684\u6578\u64da\u91cf\uff0c\u53ef\u4ee5\u63d0\u9ad8\u8b80\u53d6\u64cd\u4f5c\u7684\u6548\u7387\u3002\n- \u5217\u6295\u5f71\uff1a\u8abf\u7528`withSelectedFields`\u53ef\u5f9e\u8868\u4e2d\u8b80\u53d6\u90e8\u5206\u5217\u3002\u9019\u6a23\u53ef\u4ee5\u5728\u8868\u4e2d\u5305\u542b\u8a31\u591a\u5217\u6642\u9ad8\u6548\u8b80\u53d6\u3002\n- \u884c\u904e\u6ffe\uff1a\u8abf\u7528`withRowRestriction`\u4ee5\u6307\u5b9a\u7528\u65bc\u5728\u670d\u52d9\u5668\u7aef\u904e\u6ffe\u6578\u64da\u7684\u8b02\u8a5e\u3002\n\u904e\u6ffe\u689d\u4ef6\u8b02\u8a5e\u5fc5\u9808\u662f\u78ba\u5b9a\u6027\u7684\uff0c\u4e26\u4e14\u4e0d\u652f\u6301\u805a\u5408\u3002\n**\u6ce8\u610f** \uff1a\u5f9e\u5c0e\u51fa\u4f5c\u696d ( `Method.EXPORT` ) \u8b80\u53d6\u6642\u4e0d\u652f\u6301\u9019\u4e9b\u65b9\u6cd5\u3002\n\u4ee5\u4e0b\u793a\u4f8b\u6295\u5f71 `\"user_name\"` \u548c `\"age\"` \u5217\uff0c\u4e26\u904e\u6ffe\u6389\u8207\u8b02\u8a5e `\"age > 18\"` \u4e0d\u5339\u914d\u7684\u884c\u3002\n\u5982\u9700\u5411 Dataflow \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002  \u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BigQueryReadWithProjectionAndFiltering.java) \n```\nimport com.google.api.services.bigquery.model.TableRow;import java.util.Arrays;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.TypedRead;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.values.TypeDescriptor;public class BigQueryReadWithProjectionAndFiltering {\u00a0 public static void main(String[] args) {\u00a0 \u00a0 // Parse the pipeline options passed into the application. Example:\u00a0 \u00a0 // \u00a0 --projectId=$PROJECT_ID --datasetName=$DATASET_NAME --tableName=$TABLE_NAME\u00a0 \u00a0 // For more information, see https://beam.apache.org/documentation/programming-guide/#configuring-pipeline-options\u00a0 \u00a0 PipelineOptionsFactory.register(ExamplePipelineOptions.class);\u00a0 \u00a0 ExamplePipelineOptions options = PipelineOptionsFactory.fromArgs(args)\u00a0 \u00a0 \u00a0 \u00a0 .withValidation()\u00a0 \u00a0 \u00a0 \u00a0 .as(ExamplePipelineOptions.class);\u00a0 \u00a0 // Create a pipeline and apply transforms.\u00a0 \u00a0 Pipeline pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 .apply(BigQueryIO.readTableRows()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Read rows from a specified table.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .from(String.format(\"%s:%s.%s\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getProjectId(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getDatasetName(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getTableName()))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withMethod(TypedRead.Method.DIRECT_READ)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withSelectedFields(Arrays.asList(\"user_name\", \"age\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withRowRestriction(\"age > 18\")\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 // The output from the previous step is a PCollection<TableRow>.\u00a0 \u00a0 \u00a0 \u00a0 .apply(MapElements\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .into(TypeDescriptor.of(TableRow.class))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Use TableRow to access individual fields in the row.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via((TableRow row) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 var name = (String) row.get(\"user_name\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 var age = row.get(\"age\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Name: %s, Age: %s%n\", name, age);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return row;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }));\u00a0 \u00a0 pipeline.run().waitUntilFinish();\u00a0 }}\n```### \u5f9e\u67e5\u8a62\u7d50\u679c\u4e2d\u8b80\u53d6\n\u524d\u9762\u7684\u793a\u4f8b\u5c55\u793a\u77ad\u5982\u4f55\u5f9e\u8868\u4e2d\u8b80\u53d6\u884c\u3002\u60a8\u9084\u53ef\u4ee5\u901a\u904e\u8abf\u7528 `fromQuery` \u5f9e SQL \u67e5\u8a62\u7d50\u679c\u4e2d\u8b80\u53d6\u3002\u6b64\u65b9\u6cd5\u5c07\u4e00\u4e9b\u8a08\u7b97\u5de5\u4f5c\u9077\u79fb\u5230 BigQuery\u3002\u60a8\u9084\u53ef\u4ee5\u4f7f\u7528\u6b64\u65b9\u6cd5\u5c0d BigQuery \u8996\u5716\u6216\u5177\u9ad4\u5316\u8996\u5716\u904b\u884c\u67e5\u8a62\u4f86\u5f9e\u4e2d\u8b80\u53d6\u3002\n\u4ee5\u4e0b\u793a\u4f8b\u5c0d BigQuery \u516c\u5171\u6578\u64da\u96c6\u904b\u884c\u67e5\u8a62\u4e26\u8b80\u53d6\u7d50\u679c\u3002\u6d41\u6c34\u7dda\u904b\u884c\u5f8c\uff0c\u60a8\u53ef\u4ee5\u5728 BigQuery \u4f5c\u696d\u6b77\u53f2\u8a18\u9304\u4e2d\u67e5\u770b\u67e5\u8a62\u4f5c\u696d\u3002\n\u5982\u9700\u5411 Dataflow \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002  \u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BigQueryReadFromQuery.java) \n```\nimport com.google.api.services.bigquery.model.TableRow;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.TypedRead;import org.apache.beam.sdk.options.PipelineOptions;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.values.TypeDescriptor;public class BigQueryReadFromQuery {\u00a0 public static void main(String[] args) {\u00a0 \u00a0 // The SQL query to run inside BigQuery.\u00a0 \u00a0 final String queryString =\u00a0 \u00a0 \u00a0 \u00a0 \"SELECT repo_name as repo, COUNT(*) as count \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"FROM `bigquery-public-data.github_repos.sample_commits` \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"GROUP BY repo_name\";\u00a0 \u00a0 // Parse the pipeline options passed into the application.\u00a0 \u00a0 // For more information, see https://beam.apache.org/documentation/programming-guide/#configuring-pipeline-options\u00a0 \u00a0 PipelineOptions options = PipelineOptionsFactory.fromArgs(args)\u00a0 \u00a0 \u00a0 \u00a0 .withValidation().create();\u00a0 \u00a0 // Create a pipeline and apply transforms.\u00a0 \u00a0 Pipeline pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 // Read the query results into TableRow objects.\u00a0 \u00a0 \u00a0 \u00a0 .apply(BigQueryIO.readTableRows()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .fromQuery(queryString)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .usingStandardSql()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withMethod(TypedRead.Method.DIRECT_READ))\u00a0 \u00a0 \u00a0 \u00a0 // The output from the previous step is a PCollection<TableRow>.\u00a0 \u00a0 \u00a0 \u00a0 .apply(MapElements\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .into(TypeDescriptor.of(TableRow.class))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via((TableRow row) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Repo: %s, commits: %s%n\", row.get(\"repo\"), row.get(\"count\"));\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return row;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }));\u00a0 \u00a0 pipeline.run().waitUntilFinish();\u00a0 }}\n```\n## \u5f8c\u7e8c\u6b65\u9a5f\n- \u95b1\u8b80 [BigQuery I/O \u9023\u63a5\u5668](https://beam.apache.org/documentation/io/built-in/google-bigquery/) \u6587\u6a94\u3002\n- \u53c3\u95b1 [Google \u63d0\u4f9b\u7684\u6a21\u677f](https://cloud.google.com/dataflow/docs/guides/templates/provided-templates?hl=zh-cn) \u5217\u8868\u3002", "guide": "Dataflow"}