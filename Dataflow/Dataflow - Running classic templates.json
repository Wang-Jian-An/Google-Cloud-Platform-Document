{"title": "Dataflow - Running classic templates", "url": "https://cloud.google.com/dataflow/docs/guides/templates/running-templates", "abstract": "# Dataflow - Running classic templates\nAfter you create and stage your Dataflow template, run the template with the Google Cloud console, REST API, or the Google Cloud CLI. You can deploy Dataflow template jobs from many environments, including App Engine standard environment, Cloud Functions, and other constrained environments.\n**Note:** In addition to the template file, templated pipelines rely on files that were staged and referenced at the time of template creation. If you move or remove the staged files, your pipeline job will fail.\n", "content": "## Use the Google Cloud consoleYou can use the Google Cloud console to run Google-provided and custom Dataflow templates.\n### Google-provided templatesTo run a Google-provided template:- Go to the Dataflow page in the Google Cloud console.\n- [Go to the Dataflow page](https://console.cloud.google.com/dataflow) \n- Click **CREATE JOB FROM TEMPLATE** .\n- Select the Google-provided template that you want to run from the **Dataflow template** drop-down menu.\n- Enter a job name in the **Job Name** field.\n- Enter your parameter values in the provided parameter fields. You  don't need the **Additional Parameters** section when you use a  Google-provided template.\n- Click **Run Job** .\n### Custom templatesTo run a custom template:- Go to the Dataflow page in the Google Cloud console.\n- [Go to the Dataflow page](https://console.cloud.google.com/dataflow) \n- Click **CREATE JOB FROM TEMPLATE** .\n- Select **Custom Template** from the **Dataflow template** drop-down menu.\n- Enter a job name in the **Job Name** field.\n- Enter the Cloud Storage path to your template file in the template Cloud Storage  path field.\n- If your template needs parameters, click **ADD PARAMETER** in  the **Additional parameters** section. Enter in the **Name** and **Value** of the parameter. Repeat  this step for each needed parameter.\n- Click **Run Job** .\n## Use the REST APITo run a template with a [REST API request](/dataflow/docs/reference/rest/v1b3/projects.locations.templates/launch) , send an HTTP POST request with your project ID. This request requires [authorization](/dataflow/docs/reference/rest/v1b3/projects.locations.templates/launch#authorization-scopes) .\nSee the REST API reference for [projects.locations.templates.launch](/dataflow/docs/reference/rest/v1b3/projects.locations.templates/launch) to learn more about the available parameters.\nIn the JSON request body, set [runtime environment values](/dataflow/docs/reference/rest/v1b3/RuntimeEnvironment) in the`environment`property, not the`parameters`property. The`environment`property defines the runtime environment for the job.\n### Create a custom template batch jobThis example [projects.locations.templates.launch](/dataflow/docs/reference/rest/v1b3/projects.locations.templates/launch) request creates a batch job from a template that reads a text file and writes an output text file. If the request is successful, the response body contains an instance of [LaunchTemplateResponse](/dataflow/docs/reference/rest/v1b3/LaunchTemplateResponse) .\nModify the following values:- Replace``with your project ID.\n- Replace``with the Dataflow [region](/dataflow/docs/resources/locations) of your choice.\n- Replace``with a job name of your choice.\n- Replace``with the name of your Cloud Storage  bucket.\n- Set`gcsPath`to the Cloud Storage location of the template  file.\n- Set`parameters`to your list of key-value pairs.\n- Set`tempLocation`to a location where you have write  permission. This value is required to run Google-provided templates.\n```\n POST https://dataflow.googleapis.com/v1b3/projects/YOUR_PROJECT_ID/locations/LOCATION/templates:launch?gcsPath=gs://YOUR_BUCKET_NAME/templates/TemplateName\n {\n  \"jobName\": \"JOB_NAME\",\n  \"parameters\": {\n   \"inputFile\" : \"gs://YOUR_BUCKET_NAME/input/my_input.txt\",\n   \"output\": \"gs://YOUR_BUCKET_NAME/output/my_output\"\n  },\n  \"environment\": {\n   \"tempLocation\": \"gs://YOUR_BUCKET_NAME/temp\",\n   \"zone\": \"us-central1-f\"\n  }\n }\n```\n### Create a custom template streaming jobThis example [projects.locations.templates.launch](/dataflow/docs/reference/rest/v1b3/projects.locations.templates/launch) request creates a streaming job from a classic template that reads from a Pub/Sub subscription and writes to a BigQuery table. If you want to launch a Flex Template, use [projects.locations.flexTemplates.launch](/dataflow/docs/reference/rest/v1b3/projects.locations.flexTemplates/launch) instead. The [example template](/dataflow/docs/guides/templates/provided/pubsub-subscription-to-bigquery) is a Google-provided template. You can modify the path in the template to point to a custom template. The same logic is used to launch Google-provided and custom templates. In this example, the BigQuery table must already exist with the appropriate schema. If successful, the response body contains an instance of [LaunchTemplateResponse](/dataflow/docs/reference/rest/v1b3/LaunchTemplateResponse) .\nModify the following values:- Replace``with your project ID.\n- Replace``with the Dataflow [region](/dataflow/docs/resources/locations) of your choice.\n- Replace``with a job name of your choice.\n- Replace``with the name of your Cloud  Storage bucket.\n- Replace``with the Cloud Storage location of the template file. The location should start with gs://\n- Set`parameters`to your list of key-value pairs. The parameters listed are specific to this template example.  If you're using a custom template, modify the parameters as needed. If you're using the example template, replace the following variables.- Replace``with your Pub/Sub subscription name.\n- Replace``with your BigQuery dataset, and replace``with your BigQuery table name.\n- Set`tempLocation`to a location where you have write  permission. This value is required to run Google-provided templates.\n```\n POST https://dataflow.googleapis.com/v1b3/projects/YOUR_PROJECT_ID/locations/LOCATION/templates:launch?gcsPath=GCS_PATH\n {\n  \"jobName\": \"JOB_NAME\",\n  \"parameters\": {\n   \"inputSubscription\": \"projects/YOUR_PROJECT_ID/subscriptions/YOUR_SUBSCRIPTION_NAME\",\n   \"outputTableSpec\": \"YOUR_PROJECT_ID:YOUR_DATASET.YOUR_TABLE_NAME\"\n  },\n  \"environment\": {\n   \"tempLocation\": \"gs://YOUR_BUCKET_NAME/temp\",\n   \"zone\": \"us-central1-f\"\n  }\n }\n```\n### Update a custom template streaming jobThis example [projects.locations.templates.launch](/dataflow/docs/reference/rest/v1b3/projects.locations.templates/launch) request shows you how to update a template streaming job. If you want to update a Flex Template, use [projects.locations.flexTemplates.launch](/dataflow/docs/reference/rest/v1b3/projects.locations.flexTemplates/launch) instead.- Run [Example 2: Creating a custom template streaming job](/dataflow/docs/guides/templates/running-templates#example-2-custom-template-streaming-job) to start a streaming template job.\n- Send the following HTTP POST request, with the following modified values:- Replace``with your project ID.\n- Replace``with the Dataflow [region](/dataflow/docs/resources/locations) of the job that you're updating.\n- Replace``with the exact name of the job that you want to update.\n- Replace``with the Cloud Storage location of the template file. The location should start with gs://\n- Set`parameters`to your list of key-value pairs. The parameters listed are specific to this template example.  If you're using a custom template, modify the parameters as needed. If you're using the example template, replace the following variables.- Replace``with your Pub/Sub subscription name.\n- Replace``with your BigQuery dataset, and replace``with your BigQuery table name.\n- Use the`environment`parameter to change environment settings, such as the machine type.  This example uses the n2-highmem-2 machine type, which has more memory and CPU per worker than the default machine type.\n```\n POST https://dataflow.googleapis.com/v1b3/projects/YOUR_PROJECT_ID/locations/LOCATION/templates:launch?gcsPath=GCS_PATH\n {\n  \"jobName\": \"JOB_NAME\",\n  \"parameters\": {\n   \"inputSubscription\": \"projects/YOUR_PROJECT_ID/subscriptions/YOUR_TOPIC_NAME\",\n   \"outputTableSpec\": \"YOUR_PROJECT_ID:YOUR_DATASET.YOUR_TABLE_NAME\"\n  },\n  \"environment\": {\n   \"machineType\": \"n2-highmem-2\"\n  },\n  \"update\": true\n }\n```\n- Access the [Dataflow monitoring interface](/dataflow/docs/guides/monitoring-overview#access-monitoring-interface) and verify that a new job with the same name was created. This job has the status **Updated** .\n### Use the Google API Client LibrariesConsider using the [Google API Client Libraries](https://developers.google.com/api-client-library/) to easily make calls to the Dataflow REST APIs. This sample script uses the [Google API Client Library for Python](https://developers.google.com/api-client-library/python/) .\nIn this example, you must set the following variables:- `project`: Set to your project ID.\n- `job`: Set to a unique job name of your choice.\n- `template`: Set to the Cloud Storage location of the template file.\n- `parameters`: Set to a dictionary with the template parameters.\nTo set the [region](/dataflow/docs/resources/locations) , include the [location](/dataflow/docs/reference/rest/v1b3/projects.locations.templates/launch#path-parameters) parameter.\n [  dataflow/run_template/main.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataflow/run_template/main.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataflow/run_template/main.py) \n```\nfrom googleapiclient.discovery import build# project = 'your-gcp-project'# job = 'unique-job-name'# template = 'gs://dataflow-templates/latest/Word_Count'# parameters = {# \u00a0 \u00a0 'inputFile': 'gs://dataflow-samples/shakespeare/kinglear.txt',# \u00a0 \u00a0 'output': 'gs://<your-gcs-bucket>/wordcount/outputs',# }dataflow = build(\"dataflow\", \"v1b3\")request = (\u00a0 \u00a0 dataflow.projects()\u00a0 \u00a0 .templates()\u00a0 \u00a0 .launch(\u00a0 \u00a0 \u00a0 \u00a0 projectId=project,\u00a0 \u00a0 \u00a0 \u00a0 gcsPath=template,\u00a0 \u00a0 \u00a0 \u00a0 body={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"jobName\": job,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"parameters\": parameters,\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ))response = request.execute()\n```\nFor more information about the available options, see the [projects.locations.templates.launch method](/dataflow/docs/reference/rest/v1b3/projects.locations.templates/launch) in the Dataflow REST API reference.## Use gcloud CLI **Note:** To use the gcloud CLI to run templates, you must have [Cloud SDK](/sdk/downloads) version 138.0.0 or higher.\nThe gcloud CLI can run either a custom or a [Google-provided](/dataflow/docs/templates/provided-templates) template using the `gcloud dataflow jobs run` command. Examples of running Google-provided templates are documented in the [Google-provided templates page](/dataflow/docs/templates/provided-templates) .\nFor the following custom template examples, set the following values:- Replace``with a job name of your choice.\n- Replace``with the name of your Cloud  Storage bucket.\n- Set`--gcs-location`to the Cloud Storage location of the template file.\n- Set`--parameters`to the comma-separated list of parameters  to pass to the job. Spaces between commas and values are not allowed.\n- To prevent VMs from accepting SSH keys that are stored in project  metadata, use the`additional-experiments`flag with the [block_project_ssh_keys](/dataflow/docs/reference/service-options) service option:`--additional-experiments=block_project_ssh_keys`.\n### Create a custom template batch jobThis example creates a batch job from a template that reads a text file and writes an output text file.\n```\n gcloud dataflow jobs run JOB_NAME \\\n  --gcs-location gs://YOUR_BUCKET_NAME/templates/MyTemplate \\\n  --parameters inputFile=gs://YOUR_BUCKET_NAME/input/my_input.txt,output=gs://YOUR_BUCKET_NAME/output/my_output\n```\nThe request returns a response with the following format.\n```\n id: 2016-10-11_17_10_59-1234530157620696789\n projectId: YOUR_PROJECT_ID\n type: JOB_TYPE_BATCH\n```\n### Create a custom template streaming jobThis example creates a streaming job from a template that reads from a Pub/Sub topic and writes to a BigQuery table. The BigQuery table must already exist with the appropriate schema.\n```\n gcloud dataflow jobs run JOB_NAME \\\n  --gcs-location gs://YOUR_BUCKET_NAME/templates/MyTemplate \\\n  --parameters topic=projects/project-identifier/topics/resource-name,table=my_project:my_dataset.my_table_name\n```\nThe request returns a response with the following format.\n```\n id: 2016-10-11_17_10_59-1234530157620696789\n projectId: YOUR_PROJECT_ID\n type: JOB_TYPE_STREAMING\n```\nFor a complete list of flags for the `gcloud dataflow jobs run` command, see the [gcloud CLI reference](/sdk/gcloud/reference/dataflow/jobs/run) .## Monitoring and TroubleshootingThe [Dataflow monitoring interface](/dataflow/docs/guides/using-monitoring-intf) lets you to monitor your Dataflow jobs. If a job fails, you can find troubleshooting tips, debugging strategies, and a catalog of common errors in the [Troubleshooting Your Pipeline](/dataflow/pipelines/troubleshooting-your-pipeline) guide.", "guide": "Dataflow"}