{"title": "Dataflow - Horizontal Autoscaling", "url": "https://cloud.google.com/dataflow/docs/horizontal-autoscaling", "abstract": "# Dataflow - Horizontal Autoscaling\nHorizontal Autoscaling enables Dataflow to choose the appropriate number of worker instances for your job, adding or removing workers as needed. Dataflow scales based on the average CPU utilization of the workers and on the parallelism of a pipeline. The parallelism of a pipeline is an estimate of the number of threads needed to most efficiently process data at any given time.\nHorizontal Autoscaling is supported in both batch and streaming pipelines.\n", "content": "## Batch autoscaling\nHorizontal Autoscaling is enabled by default on all batch pipelines. Dataflow automatically chooses the number of workers based on the estimated total amount of work in each stage of your pipeline. This estimate depends on the input size and the current throughput. Every 30 seconds, Dataflow re-evaluates the amount of work according to the execution progress. As the estimated total amount of work increases or decreases, Dataflow dynamically scales the number of workers up or down.\nThe number of workers is sublinear to the amount of work. For example, a job with twice the work has fewer than twice the workers.\nIf any of the following conditions occur, Dataflow either maintains or decreases the number of workers, in order to save idle resources:\n- The average worker CPU usage is lower than 5%.\n- Parallelism is limited due to unparallelizable work, such as un-splittable data caused by compressed files or I/O modules that don't split.\n- The degree of parallelism is fixed, for example when writing to existing files in Cloud Storage.\nTo set an upper bound on the number of workers, set the [--maxNumWorkers pipeline option](/dataflow/docs/reference/pipeline-options) . This flag is optional for batch jobs. The default value is `1000` .\n## Streaming autoscaling\nFor streaming jobs, Horizontal Autoscaling allows Dataflow to adaptively change the number of workers in response to changes in load and resource utilization.\nHorizontal Autoscaling is enabled by default for streaming jobs that use [Streaming Engine](/dataflow/docs/streaming-engine) . To enable Horizontal Autoscaling for streaming jobs that don't use Streaming Engine, set the following [pipeline options](/dataflow/docs/reference/pipeline-options) when you start your pipeline:\n```\n--autoscalingAlgorithm=THROUGHPUT_BASED\n--maxNumWorkers=MAX_WORKERS\n```\nReplace with the maximum number of worker instances.\n```\n--autoscaling_algorithm=THROUGHPUT_BASED\n--max_num_workers=MAX_WORKERS\n```\nReplace with the maximum number of worker instances.\n```\n--autoscaling_algorithm=THROUGHPUT_BASED\n--max_num_workers=MAX_WORKERS\n```\nReplace with the maximum number of worker instances.\n## Disable Horizontal Autoscaling\nTo disable Horizontal Autoscaling, set the following [pipeline option](/dataflow/docs/reference/pipeline-options) when you run the job.\n```\n--autoscalingAlgorithm=NONE\n```\nIf you disable Horizontal Autoscaling, then Dataflow sets the number of workers based on the `--numWorkers` option.\n```\n--autoscaling_algorithm=NONE\n```\nIf you disable Horizontal Autoscaling, then Dataflow sets the number of workers based on the `--num_workers` option.\n```\n--autoscaling_algorithm=NONE\n```\nIf you disable Horizontal Autoscaling, then Dataflow sets the number of workers based on the `--num_workers` option.\n## Custom sources\nIf you create a custom data source, you can potentially improve performance by implementing methods that provide more information to the Horizontal Autoscaling algorithm:\n- In your`BoundedSource`subclass, implement the method`getEstimatedSizeBytes`. The Dataflow service uses`getEstimatedSizeBytes`when calculating the initial number of workers to use for your pipeline.\n- In your`BoundedReader`subclass, implement the method`getFractionConsumed`. The Dataflow service uses`getFractionConsumed`to track read progress and converge on the correct number of workers to use during a read.\nThe source must inform the Dataflow service about backlog. Backlog is an estimate of the input in bytes that has not yet been processed by the source. To inform the service about backlog, implement either one of the following methods in your `UnboundedReader` class.- `getSplitBacklogBytes()`- Backlog for the current split of the source. The service aggregates backlog across all the splits.\n- `getTotalBacklogBytes()`- The global backlog across all the splits. In some cases the backlog is not available for each split and can only be calculated across all the splits. Only the first split (split ID '0') needs to provide total backlog.\nThe Apache Beam repository contains several [examples](https://github.com/apache/beam/blob/master/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaUnboundedReader.java) of custom sources that implement the `UnboundedReader` class.\n- In your`BoundedSource`subclass, implement the method`estimate_size`. The Dataflow service uses`estimate_size`when calculating the initial number of workers to use for your pipeline.\n- In your`RangeTracker`subclass, implement the method`fraction_consumed`. The Dataflow service uses`fraction_consumed`to track read progress and converge on the correct number of workers to use during a read.\n- In your`RangeTracker`, implement the method`GetProgress()`. The Dataflow service uses`GetProgress`to track read progress and converge on the correct number of workers to use during a read.## Limitations\n- In jobs running Dataflow Prime, Horizontal Autoscaling is deactivated during and up to 10 minutes after Vertical Autoscaling. For more information, see [Effect on Horizontal Autoscaling](/dataflow/docs/vertical-autoscaling#horizontal-autoscaling) .\n- For pipelines not using [Dataflow Shuffle](/dataflow/docs/shuffle-for-batch) , Dataflow might not be able to scale down the workers effectively because the workers might have shuffled data stored in local disks.\n- The [PeriodicImpulse](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/transforms/PeriodicImpulse.html) transform is not supported with streaming autoscaling. If your pipeline uses`PeriodicImpulse`, then Dataflow workers don't scale down as expected.## What's next\n- [Tune Horizontal Autoscaling for streaming pipelines](/dataflow/docs/guides/tune-horizontal-autoscaling) \n- [Monitor Dataflow autoscaling](/dataflow/docs/guides/autoscaling-metrics) \n- [Troubleshoot Dataflow autoscaling](/dataflow/docs/guides/troubleshoot-autoscaling)", "guide": "Dataflow"}