{"title": "Dataflow - Run an LLM in a streaming pipeline", "url": "https://cloud.google.com/dataflow/docs/tutorials/streaming-llm", "abstract": "# Dataflow - Run an LLM in a streaming pipeline\nThis tutorial shows how to run a large language model (LLM) in a streaming Dataflow pipeline by using the Apache Beam RunInference API.\nFor more information about the RunInference API, see [About Beam ML](https://beam.apache.org/documentation/ml/about-ml/) in the Apache Beam documentation.\nThe example code is [available on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/dataflow/run-inference) .", "content": "## Objectives\n- Create Pub/Sub topics and subscriptions for the model's input and responses.\n- Load the model into Cloud Storage by using a Vertex AI custom job.\n- Run the pipeline.\n- Ask the model a question and get a response.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Dataflow](/dataflow/pricing) \n- [Cloud Storage](/storage/pricing) \n- [Pub/Sub](/pubsub/pricing) \n- [Vertex AI](/vertex-ai/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you beginRun this tutorial on a machine that has at least 5\u00a0GB of free disk space to install the dependencies.- Grant roles to your Compute Engine default service account. Run the  following command once for each of the following IAM roles:- `roles/dataflow.admin`\n- `roles/dataflow.worker`\n- `roles/storage.admin`\n- `roles/pubsub.editor`\n- `roles/aiplatform.user`\n```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com\" --role=SERVICE_ACCOUNT_ROLE\n```Replace the following:- ``: your project ID.\n- ``: your project number.   To find your project number, use the [gcloud projects describe command](/sdk/gcloud/reference/projects/describe) .\n- ``: each individual role.\n- Copy the Google Cloud project ID. You need this value later in this tutorial.\n## Create the Google Cloud resourcesThis section explains how to create the following resources:- A Cloud Storage bucket to use as a temporary storage location\n- A Pub/Sub topic for the model's prompts\n- A Pub/Sub topic and subscription for the model's responses\n### Create a Cloud Storage bucketCreate a Cloud Storage bucket by using the gcloud CLI. This bucket is used as a temporary storage location by the Dataflow pipeline.\nTo create the bucket, use the [gcloud storage buckets create command](/sdk/gcloud/reference/storage/buckets/create) :\n```\ngcloud storage buckets create gs://BUCKET_NAME --location=LOCATION\n```\nReplace the following:- : a name for your Cloud Storage bucket that meets the [bucket naming requirements](/storage/docs/buckets#naming) . Cloud Storage bucket names must be globally unique.\n- : the [location](/storage/docs/locations#available-locations) for the bucket.\nCopy the bucket name. You need this value later in this tutorial.\n### Create Pub/Sub topics and subscriptionsCreate two Pub/Sub topics and one subscription. One topic is for the input prompts that you send to the model. The other topic and its attached subscription is for the model's responses.- To create the topics, run the [gcloud pubsub topics create command](/sdk/gcloud/reference/pubsub/topics/create) twice, once for each topic:```\ngcloud pubsub topics create PROMPTS_TOPIC_IDgcloud pubsub topics create RESPONSES_TOPIC_ID\n```Replace the following:- : the topic ID for the input prompts to send to the model, such as`prompts`\n- : the topic ID for the model's responses, such as`responses`\n- To create the subscription and attach it to your responses topic, use the [gcloud pubsub subscriptions create command](/sdk/gcloud/reference/pubsub/subscriptions/create) :```\ngcloud pubsub subscriptions create RESPONSES_SUBSCRIPTION_ID --topic=RESPONSES_TOPIC_ID\n```Replace with the subscription ID for the model's responses, such as `responses-subscription` .\nCopy the topic IDs and the subscription ID. You need these values later in this tutorial.## Prepare your environmentDownload the code samples and then set up your environment to run the tutorial.\nThe code samples in the [python-docs-samples GitHub repository](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/dataflow/run-inference) provide the code that you need to run this pipeline. When you are ready to build your own pipeline, you can use this sample code as a template.\nYou create an isolated Python virtual environment to run your pipeline project by using [venv](https://docs.python.org/3/library/venv.html) . A virtual environment lets you isolate the dependencies of one project from the dependencies of other projects. For more information about how to install Python and create a virtual environment, see [Setting up a Python development environment](/python/docs/setup) .- Use the `git clone` command to clone the GitHub repository:```\ngit clone https://github.com/GoogleCloudPlatform/python-docs-samples.git\n```\n- Navigate to the `run-inference` directory:```\ncd python-docs-samples/dataflow/run-inference\n```\n- If you're using a command prompt, check that you have Python 3 and `pip` running in your system:```\npython --versionpython -m pip --version\n```If required, [install Python 3](/python/docs/setup#installing_python) .If you're using Cloud Shell, you can skip this step because Cloud Shell already has Python installed.\n- Create a [Python virtual environment](/python/docs/setup#installing_and_using_virtualenv) :```\npython -m venv /tmp/envsource /tmp/env/bin/activate\n```\n- Install the dependencies:```\npip install -r requirements.txt --no-cache-dir\n```\n### Model loading code sampleThe model loading code in this tutorial launches a Vertex AI custom job that loads the model's `state_dict` object into Cloud Storage.\nThe starter file looks like the following:\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataflow/run-inference/download_model.py) \n```\n# Copyright 2023 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 \u00a0http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.\"\"\"Loads the state_dict for an LLM model into Cloud Storage.\"\"\"from __future__ import annotationsimport osimport torchfrom transformers import AutoModelForSeq2SeqLMdef run_local(model_name: str, state_dict_path: str) -> None:\u00a0 \u00a0 \"\"\"Loads the state dict and saves it into the desired path.\u00a0 \u00a0 If the `state_dict_path` is a Cloud Storage location starting\u00a0 \u00a0 with \"gs://\", this assumes Cloud Storage is mounted with\u00a0 \u00a0 Cloud Storage FUSE in `/gcs`. Vertex AI is set up like this.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 model_name: HuggingFace model name compatible with AutoModelForSeq2SeqLM.\u00a0 \u00a0 \u00a0 \u00a0 state_dict_path: File path to the model's state_dict, can be in Cloud Storage.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 print(f\"Loading model: {model_name}\")\u00a0 \u00a0 model = AutoModelForSeq2SeqLM.from_pretrained(\u00a0 \u00a0 \u00a0 \u00a0 model_name, torch_dtype=torch.bfloat16\u00a0 \u00a0 )\u00a0 \u00a0 print(f\"Model loaded, saving state dict to: {state_dict_path}\")\u00a0 \u00a0 # Assume Cloud Storage FUSE is mounted in `/gcs`.\u00a0 \u00a0 state_dict_path = state_dict_path.replace(\"gs://\", \"/gcs/\")\u00a0 \u00a0 directory = os.path.dirname(state_dict_path)\u00a0 \u00a0 if directory and not os.path.exists(directory):\u00a0 \u00a0 \u00a0 \u00a0 os.makedirs(os.path.dirname(state_dict_path), exist_ok=True)\u00a0 \u00a0 torch.save(model.state_dict(), state_dict_path)\u00a0 \u00a0 print(\"State dict saved successfully!\")def run_vertex_job(\u00a0 \u00a0 model_name: str,\u00a0 \u00a0 state_dict_path: str,\u00a0 \u00a0 job_name: str,\u00a0 \u00a0 project: str,\u00a0 \u00a0 bucket: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 machine_type: str = \"e2-highmem-2\",\u00a0 \u00a0 disk_size_gb: int = 100,) -> None:\u00a0 \u00a0 \"\"\"Launches a Vertex AI custom job to load the state dict.\u00a0 \u00a0 If the model is too large to fit into memory or disk, we can launch\u00a0 \u00a0 a Vertex AI custom job with a large enough VM for this to work.\u00a0 \u00a0 Depending on the model's size, it might require a different VM\u00a0 \u00a0 configuration. The model MUST fit into the VM's memory, and there\u00a0 \u00a0 must be enough disk space to stage the entire model while it gets\u00a0 \u00a0 copied to Cloud Storage.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 model_name: HuggingFace model name compatible with AutoModelForSeq2SeqLM.\u00a0 \u00a0 \u00a0 \u00a0 state_dict_path: File path to the model's state_dict, can be in Cloud Storage.\u00a0 \u00a0 \u00a0 \u00a0 job_name: Job display name in the Vertex AI console.\u00a0 \u00a0 \u00a0 \u00a0 project: Google Cloud Project ID.\u00a0 \u00a0 \u00a0 \u00a0 bucket: Cloud Storage bucket name, without the \"gs://\" prefix.\u00a0 \u00a0 \u00a0 \u00a0 location: Google Cloud regional location.\u00a0 \u00a0 \u00a0 \u00a0 machine_type: Machine type for the VM to run the job.\u00a0 \u00a0 \u00a0 \u00a0 disk_size_gb: Disk size in GB for the VM to run the job.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.init(project=project, staging_bucket=bucket, location=location)\u00a0 \u00a0 job = aiplatform.CustomJob.from_local_script(\u00a0 \u00a0 \u00a0 \u00a0 display_name=job_name,\u00a0 \u00a0 \u00a0 \u00a0 container_uri=\"us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-13:latest\",\u00a0 \u00a0 \u00a0 \u00a0 script_path=\"download_model.py\",\u00a0 \u00a0 \u00a0 \u00a0 args=[\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"local\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f\"--model-name={model_name}\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f\"--state-dict-path={state_dict_path}\",\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 machine_type=machine_type,\u00a0 \u00a0 \u00a0 \u00a0 boot_disk_size_gb=disk_size_gb,\u00a0 \u00a0 \u00a0 \u00a0 requirements=[\"transformers\"],\u00a0 \u00a0 )\u00a0 \u00a0 job.run()if __name__ == \"__main__\":\u00a0 \u00a0 import argparse\u00a0 \u00a0 parser = argparse.ArgumentParser()\u00a0 \u00a0 subparsers = parser.add_subparsers(required=True)\u00a0 \u00a0 parser_local = subparsers.add_parser(\"local\")\u00a0 \u00a0 parser_local.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--model-name\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"HuggingFace model name compatible with AutoModelForSeq2SeqLM\",\u00a0 \u00a0 )\u00a0 \u00a0 parser_local.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--state-dict-path\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"File path to the model's state_dict, can be in Cloud Storage\",\u00a0 \u00a0 )\u00a0 \u00a0 parser_local.set_defaults(run=run_local)\u00a0 \u00a0 parser_vertex = subparsers.add_parser(\"vertex\")\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--model-name\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"HuggingFace model name compatible with AutoModelForSeq2SeqLM\",\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--state-dict-path\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"File path to the model's state_dict, can be in Cloud Storage\",\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--job-name\", required=True, help=\"Job display name in the Vertex AI console\"\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--project\", required=True, help=\"Google Cloud Project ID\"\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--bucket\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help='Cloud Storage bucket name, without the \"gs://\" prefix',\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--location\", default=\"us-central1\", help=\"Google Cloud regional location\"\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--machine-type\",\u00a0 \u00a0 \u00a0 \u00a0 default=\"e2-highmem-2\",\u00a0 \u00a0 \u00a0 \u00a0 help=\"Machine type for the VM to run the job\",\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--disk-size-gb\",\u00a0 \u00a0 \u00a0 \u00a0 type=int,\u00a0 \u00a0 \u00a0 \u00a0 default=100,\u00a0 \u00a0 \u00a0 \u00a0 help=\"Disk size in GB for the VM to run the job\",\u00a0 \u00a0 )\u00a0 \u00a0 parser_vertex.set_defaults(run=run_vertex_job)\u00a0 \u00a0 args = parser.parse_args()\u00a0 \u00a0 kwargs = args.__dict__.copy()\u00a0 \u00a0 kwargs.pop(\"run\")\u00a0 \u00a0 args.run(**kwargs)\n```\n### Pipeline code sampleThe pipeline code in this tutorial deploys a Dataflow pipeline that does the following things:- Reads a prompt from Pub/Sub and encodes the text into token tensors.\n- Runs the`RunInference`transform.\n- Decodes the output token tensors into text and writes the response to Pub/Sub.\nThe starter file looks like the following:\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataflow/run-inference/main.py) \n```\n# Copyright 2023 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 \u00a0http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.\"\"\"Runs a streaming RunInference Language Model pipeline.\"\"\"from __future__ import annotationsimport loggingimport apache_beam as beamfrom apache_beam.ml.inference.base import PredictionResultfrom apache_beam.ml.inference.base import RunInferencefrom apache_beam.ml.inference.pytorch_inference import make_tensor_model_fnfrom apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensorfrom apache_beam.options.pipeline_options import PipelineOptionsimport torchfrom transformers import AutoConfigfrom transformers import AutoModelForSeq2SeqLMfrom transformers import AutoTokenizerfrom transformers.tokenization_utils import PreTrainedTokenizerMAX_RESPONSE_TOKENS = 256def to_tensors(input_text: str, tokenizer: PreTrainedTokenizer) -> torch.Tensor:\u00a0 \u00a0 \"\"\"Encodes input text into token tensors.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 input_text: Input text for the language model.\u00a0 \u00a0 \u00a0 \u00a0 tokenizer: Tokenizer for the language model.\u00a0 \u00a0 Returns: Tokenized input tokens.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 return tokenizer(input_text, return_tensors=\"pt\").input_ids[0]def decode_response(result: PredictionResult, tokenizer: PreTrainedTokenizer) -> str:\u00a0 \u00a0 \"\"\"Decodes output token tensors into text.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 result: Prediction results from the RunInference transform.\u00a0 \u00a0 \u00a0 \u00a0 tokenizer: Tokenizer for the language model.\u00a0 \u00a0 Returns: The model's response as text.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 output_tokens = result.inference\u00a0 \u00a0 return tokenizer.decode(output_tokens, skip_special_tokens=True)class AskModel(beam.PTransform):\u00a0 \u00a0 \"\"\"Asks an language model a prompt message and gets its responses.\u00a0 \u00a0 Attributes:\u00a0 \u00a0 \u00a0 \u00a0 model_name: HuggingFace model name compatible with AutoModelForSeq2SeqLM.\u00a0 \u00a0 \u00a0 \u00a0 state_dict_path: File path to the model's state_dict, can be in Cloud Storage.\u00a0 \u00a0 \u00a0 \u00a0 max_response_tokens: Maximum number of tokens for the model to generate.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 def __init__(\u00a0 \u00a0 \u00a0 \u00a0 self,\u00a0 \u00a0 \u00a0 \u00a0 model_name: str,\u00a0 \u00a0 \u00a0 \u00a0 state_dict_path: str,\u00a0 \u00a0 \u00a0 \u00a0 max_response_tokens: int = MAX_RESPONSE_TOKENS,\u00a0 \u00a0 ) -> None:\u00a0 \u00a0 \u00a0 \u00a0 self.model_handler = PytorchModelHandlerTensor(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 state_dict_path=state_dict_path,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_class=AutoModelForSeq2SeqLM.from_config,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_params={\"config\": AutoConfig.from_pretrained(model_name)},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 inference_fn=make_tensor_model_fn(\"generate\"),\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 self.tokenizer = AutoTokenizer.from_pretrained(model_name)\u00a0 \u00a0 \u00a0 \u00a0 self.max_response_tokens = max_response_tokens\u00a0 \u00a0 def expand(self, pcollection: beam.PCollection[str]) -> beam.PCollection[str]:\u00a0 \u00a0 \u00a0 \u00a0 return (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pcollection\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"To tensors\" >> beam.Map(to_tensors, self.tokenizer)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"RunInference\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 >> RunInference(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 self.model_handler,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 inference_args={\"max_new_tokens\": self.max_response_tokens},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"Get response\" >> beam.Map(decode_response, self.tokenizer)\u00a0 \u00a0 \u00a0 \u00a0 )if __name__ == \"__main__\":\u00a0 \u00a0 import argparse\u00a0 \u00a0 parser = argparse.ArgumentParser()\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--messages-topic\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"Pub/Sub topic for input text messages\",\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--responses-topic\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"Pub/Sub topic for output text responses\",\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--model-name\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"HuggingFace model name compatible with AutoModelForSeq2SeqLM\",\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--state-dict-path\",\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"File path to the model's state_dict, can be in Cloud Storage\",\u00a0 \u00a0 )\u00a0 \u00a0 args, beam_args = parser.parse_known_args()\u00a0 \u00a0 logging.getLogger().setLevel(logging.INFO)\u00a0 \u00a0 beam_options = PipelineOptions(\u00a0 \u00a0 \u00a0 \u00a0 beam_args,\u00a0 \u00a0 \u00a0 \u00a0 pickle_library=\"cloudpickle\",\u00a0 \u00a0 \u00a0 \u00a0 streaming=True,\u00a0 \u00a0 )\u00a0 \u00a0 simple_name = args.model_name.split(\"/\")[-1]\u00a0 \u00a0 pipeline = beam.Pipeline(options=beam_options)\u00a0 \u00a0 _ = (\u00a0 \u00a0 \u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 | \"Read from Pub/Sub\" >> beam.io.ReadFromPubSub(args.messages_topic)\u00a0 \u00a0 \u00a0 \u00a0 | \"Decode bytes\" >> beam.Map(lambda msg: msg.decode(\"utf-8\"))\u00a0 \u00a0 \u00a0 \u00a0 | f\"Ask {simple_name}\" >> AskModel(args.model_name, args.state_dict_path)\u00a0 \u00a0 \u00a0 \u00a0 | \"Encode bytes\" >> beam.Map(lambda msg: msg.encode(\"utf-8\"))\u00a0 \u00a0 \u00a0 \u00a0 | \"Write to Pub/Sub\" >> beam.io.WriteToPubSub(args.responses_topic)\u00a0 \u00a0 )\u00a0 \u00a0 pipeline.run()\n```## Load the modelLLMs can be very large models. Larger models that are trained with more parameters generally give better results. However, larger models require a bigger machine and more memory to run. Larger models can also be slower to run on CPUs.\nBefore you run a PyTorch model on Dataflow, you need to load the model's `state_dict` object. A model's [state_dict object](https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html) stores the weights for the model.\nIn a Dataflow pipeline that uses the Apache Beam `RunInference` transform, the model's `state_dict` object must be loaded to Cloud Storage. The machine that you use to load the `state_dict` object to Cloud Storage needs to have enough memory to load the model. The machine also needs a fast internet connection to download the weights and to upload them to Cloud Storage.\nThe following table shows the number of parameters for each model and the minimum memory that's needed to load each model.\n| Model    | Parameters | Memory needed |\n|:---------------------|:-------------|:----------------|\n| google/flan-t5-small | 80 million | >\u00a0320 MB  |\n| google/flan-t5-base | 250 million | >\u00a01 GB   |\n| google/flan-t5-large | 780 million | >\u00a03.2 GB  |\n| google/flan-t5-xl | 3 billion | >\u00a012 GB   |\n| google/flan-t5-xxl | 11 billion | >\u00a044 GB   |\n| google/flan-ul2  | 20 billion | >\u00a080 GB   |\nAlthough you can load a smaller model locally, this tutorial shows how to launch a Vertex AI custom job that loads the model with an appropriately sized VM.\nBecause LLMs can be so large, the example in this tutorial saves the `state_dict` object as `float16` format instead of the default `float32` format. With this configuration, each parameter uses 16 bits instead of 32 bits, making the `state_dict` object half the size. A smaller size minimizes the time that's needed to load the model. However, converting the format means that the VM has to fit both the model and the `state_dict` object into memory.\nThe following table shows the minimum requirements to load a model after the `state_dict` object is saved as `float16` format. The table also shows the suggested machine types to load a model by using Vertex AI. The minimum (and default) disk size for Vertex AI is 100 GB, but some models might require a larger disk.\n| Model name   | Memory needed | Machine type | VM memory | VM disk |\n|:---------------------|:----------------|:---------------|:------------|:----------|\n| google/flan-t5-small | > 480\u00a0MB  | e2-standard-4 | 16\u00a0GB  | 100\u00a0GB |\n| google/flan-t5-base | > 1.5\u00a0GB  | e2-standard-4 | 16\u00a0GB  | 100\u00a0GB |\n| google/flan-t5-large | > 4.8\u00a0GB  | e2-standard-4 | 16\u00a0GB  | 100\u00a0GB |\n| google/flan-t5-xl | > 18\u00a0GB   | e2-highmem-4 | 32\u00a0GB  | 100\u00a0GB |\n| google/flan-t5-xxl | > 66\u00a0GB   | e2-highmem-16 | 128\u00a0GB  | 100\u00a0GB |\n| google/flan-ul2  | > 120\u00a0GB  | e2-highmem-16 | 128\u00a0GB  | 150\u00a0GB |\nLoad the model's `state_dict` object into Cloud Storage by using a Vertex AI custom job:\n```\npython download_model.py vertex \\\u00a0 \u00a0 --model-name=\"MODEL_NAME\" \\\u00a0 \u00a0 --state-dict-path=\"gs://BUCKET_NAME/run-inference/MODEL_NAME.pt\" \\\u00a0 \u00a0 --job-name=\"Load MODEL_NAME\" \\\u00a0 \u00a0 --project=\"PROJECT_ID\" \\\u00a0 \u00a0 --bucket=\"BUCKET_NAME\" \\\u00a0 \u00a0 --location=\"LOCATION\" \\\u00a0 \u00a0 --machine-type=\"VERTEX_AI_MACHINE_TYPE\" \\\u00a0 \u00a0 --disk-size-gb=\"DISK_SIZE_GB\"\n```\nReplace the following:- : the name of the model, such as`google/flan-t5-xl`.\n- : the type of machine to run the Vertex AI custom job on, such as`e2-highmem-4`.\n- : the disk size for the VM, in GB. The minimum size is 100\u00a0GB.\nDepending on the size of the model, it might take a few minutes to load the model. To view the status, go to the Vertex AI **Custom jobs** page.\n [Go to Custom jobs](https://console.cloud.google.com/vertex-ai/training/custom-jobs) ## Run the pipelineAfter you load the model, you run the Dataflow pipeline. To run the pipeline, both the model and the memory used by each worker must fit into memory.\nThe following table shows the recommended machine types to run an inference pipeline.\n| Model name   | Machine type | VM memory |\n|:---------------------|:---------------|:------------|\n| google/flan-t5-small | n2-highmem-2 | 16\u00a0GB  |\n| google/flan-t5-base | n2-highmem-2 | 16\u00a0GB  |\n| google/flan-t5-large | n2-highmem-4 | 32\u00a0GB  |\n| google/flan-t5-xl | n2-highmem-4 | 32\u00a0GB  |\n| google/flan-t5-xxl | n2-highmem-8 | 64\u00a0GB  |\n| google/flan-ul2  | n2-highmem-16 | 128\u00a0GB  |\nRun the pipeline:\n```\npython main.py \\\u00a0 \u00a0 --messages-topic=\"projects/PROJECT_ID/topics/PROMPTS_TOPIC_ID\" \\\u00a0 \u00a0 --responses-topic=\"projects/PROJECT_ID/topics/RESPONSES_TOPIC_ID\" \\\u00a0 \u00a0 --model-name=\"MODEL_NAME\" \\\u00a0 \u00a0 --state-dict-path=\"gs://BUCKET_NAME/run-inference/MODEL_NAME.pt\" \\\u00a0 \u00a0 --runner=\"DataflowRunner\" \\\u00a0 \u00a0 --project=\"PROJECT_ID\" \\\u00a0 \u00a0 --temp_location=\"gs://BUCKET_NAME/temp\" \\\u00a0 \u00a0 --region=\"REGION\" \\\u00a0 \u00a0 --machine_type=\"DATAFLOW_MACHINE_TYPE\" \\\u00a0 \u00a0 --requirements_file=\"requirements.txt\" \\\u00a0 \u00a0 --requirements_cache=\"skip\" \\\u00a0 \u00a0 --experiments=\"use_sibling_sdk_workers\" \\\u00a0 \u00a0 --experiments=\"no_use_multiple_sdk_containers\"\n```\nReplace the following:- : the project ID\n- : the topic ID for the input prompts to send to the model\n- : the topic ID for the model's responses\n- : the name of the model, such as`google/flan-t5-xl`\n- : the name of the bucket\n- : the region to deploy the job in, such as`us-central1`\n- : the VM to run the pipeline on, such as`n2-highmem-4`\nTo ensure that the model is loaded only once per worker and doesn't run out of memory, you configure workers to use a single process by setting the pipeline option `--experiments=no_use_multiple_sdk_containers` . You don't have to limit the number of threads because the `RunInference` transform shares the same model with multiple threads.\nThe pipeline in this example runs with CPUs. For a larger model, more time is required to process each request. You can [enable GPUs](/dataflow/docs/concepts/gpu-support) if you need faster responses.\nTo view the status of the pipeline, go to the Dataflow **Jobs** page.\n [Go to Jobs](https://console.cloud.google.com/dataflow/jobs) ## Ask the model a questionAfter the pipeline starts running, you provide a prompt to the model and receive a response.- Send your prompt by publishing a message to Pub/Sub. Use the [gcloud pubsub topics publish command](/sdk/gcloud/reference/pubsub/topics/publish) :```\ngcloud pubsub topics publish PROMPTS_TOPIC_ID \\\u00a0 \u00a0 --message=\"PROMPT_TEXT\"\n```Replace `` with a string that contains the prompt that you want to provide. Surround the prompt with quotation marks.Use your own prompt, or try one of the following examples:- `Translate to Spanish: My name is Luka`\n- `Complete this sentence: Once upon a time, there was a`\n- `Summarize the following text: Dataflow is a Google Cloud service that provides unified stream and batch data processing at scale. Use Dataflow to create data pipelines that read from one or more sources, transform the data, and write the data to a destination.`\n- To get the response, use the [gcloud pubsub subscriptions pull command](/sdk/gcloud/reference/pubsub/subscriptions/pull) .Depending on the size of the model, it might take a few minutes for the model to generate a response. Larger models take longer to deploy and to generate a response.```\ngcloud pubsub subscriptions pull RESPONSES_SUBSCRIPTION_ID --auto-ack\n```Replace `` with the subscription ID for the model's responses.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- Delete a Google Cloud project:\n- ```\ngcloud projects delete PROJECT_ID\n```\n### Delete individual resources\n- Exit the Python virtual environment:```\ndeactivate\n```\n- Stop the pipeline:- List the job IDs for the Dataflow jobs that are   running, and then note the job ID for the tutorial's job:```\ngcloud dataflow jobs list --region=REGION --status=active\n```\n- Cancel the job:```\ngcloud dataflow jobs cancel JOB_ID --region=REGION\n```\n- Delete the bucket and anything inside of it:```\ngcloud storage rm gs://BUCKET_NAME --recursive\n```\n- Delete the topics and the subscription:```\ngcloud pubsub topics delete PROMPTS_TOPIC_IDgcloud pubsub topics delete RESPONSES_TOPIC_IDgcloud pubsub subscriptions delete RESPONSES_SUBSCRIPTION_ID\n```\n- Revoke the roles that you granted to the  Compute Engine default service account. Run the following command once  for each of the following IAM roles:- `roles/dataflow.admin`\n- `roles/dataflow.worker`\n- `roles/storage.admin`\n- `roles/pubsub.editor`\n- `roles/aiplatform.user`\n```\ngcloud projects remove-iam-policy-binding PROJECT_ID --member=serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com --role=SERVICE_ACCOUNT_ROLE\n```\n- Optional: Revoke roles from your Google Account.```\ngcloud projects remove-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS\" --role=roles/iam.serviceAccountUser\n```\n- Optional: Revoke the authentication credentials that you created, and delete the local   credential file.```\ngcloud auth application-default revoke\n```\n- Optional: Revoke credentials from the gcloud CLI.```\ngcloud auth revoke\n```\n## What's next\n- [Explore Dataflow ML](/dataflow/docs/machine-learning) .\n- Learn more about the [RunInference API](https://beam.apache.org/documentation/ml/about-ml/) .\n- Get in-depth information about using ML with Apache Beam in the Apache Beam [AI/ML pipelines](https://beam.apache.org/documentation/ml/overview/) documentation.\n- Work through the notebook [Use RunInference for Generative AI](/dataflow/docs/notebooks/run_inference_generative_ai) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Dataflow"}