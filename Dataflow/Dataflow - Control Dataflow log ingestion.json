{"title": "Dataflow - Control Dataflow log ingestion", "url": "https://cloud.google.com/dataflow/docs/guides/filter-logs", "abstract": "# Dataflow - Control Dataflow log ingestion\n[Exclusion filters](/logging/docs/routing/overview#exclusions) let you control the volume of Dataflow logs ingested by Cloud Logging while still making verbose logging available for debugging. You can use exclusion filters to exclude matching log entries from being ingested by Cloud Logging or from being routed to the destination of the [sink](/logging/docs/routing/overview#sinks) . Create exclusion filters by using the [Logging query language](/logging/docs/view/logging-query-language) . Logging query language lets you specify a subset of all log entries in your selected Google Cloud resource, such as a project or a folder.\nBy using exclusion filters, you can reduce the Cloud Logging costs incurred by Dataflow log ingestion. For more information about log ingestion pricing for Cloud Logging, see the [Cloud Logging pricing summary](/stackdriver/pricing) . For more details about how exclusion filters work and their limitations, see [Exclusion filters](/logging/docs/routing/overview#exclusions) in the Cloud Logging documentation.\nDataflow jobs emit multiple [log types](/logging#log-types) . This page demonstrates how to filter Dataflow job logs and worker logs.\n", "content": "## Create log exclusion filters\nThis example creates an exclusion filter on the [_Default Cloud Logging sink](/logging/docs/routing/overview#sinks) . The filter excludes all `DEFAULT` , `DEBUG` , `INFO` , and `NOTICE` severity Dataflow logs from being ingested into Cloud Logging. `WARNING` , `ERROR` , `CRITICAL` , `ALERT` , and `EMERGENCY` severity logs are still captured. For more information about supported log levels, see [LogSeverity](/logging/docs/reference/v2/rest/v2/LogEntry#logseverity) .\n### Before you begin\n### Permissions\nAs you get started, ensure the following:\n- You have a Google Cloud project with logs that you can see in the [Logs Explorer](/logging/docs/view/logs-explorer-summary) .\n- You have one of the following IAM roles for the source Google Cloud project from which you're routing logs.- **Owner** (`roles/owner`)\n- **Logging Admin** (`roles/logging.admin`)\n- **Logs Configuration Writer** (`roles/logging.configWriter`)\nThe permissions contained in these roles let you create, delete, or modify sinks. For information on setting IAM roles, see the Logging [Access control guide](/logging/docs/access-control) .\n- You have a resource in a [supported destination](#supported-destinations) or can create one.You need to create the routing destination before the sink, through either Google Cloud CLI, Google Cloud console, or the Google Cloud APIs. You can create the destination in any Google Cloud project in any organization. Before you create the destination, make sure the service account from the sink has [permissions to write to the destination](#dest-auth) .\n### Add an exclusion filter\nThe following steps demonstrate how to add a Cloud Logging exclusion filter to your Dataflow logs. This exclusion filter selects all Dataflow log entries with the severity `DEFAULT` , `DEBUG` , `INFO` , and `NOTICE` from jobs that have a Dataflow job name that does not end in the string `debug` . The filter excludes these logs from ingestion into the `Default` Cloud Logging bucket.\n- In the Google Cloud console, go to the **Logs Router** page: [Go to Logs Router](https://console.cloud.google.com/logs/router) \n- Find the row with the `_Default` sink, expand the more_vert **Actions** option, and then click **Edit sink** .\n- In **Choose logs to filter out of sink** , for **Build an exclusion filter** , click add **Add exclusion** .\n- Enter a name for your exclusion filter.\n- In the **Build an exclusion filter** section, paste the following text into the box:```\nresource.type=\"dataflow_step\" ANDlabels.\"dataflow.googleapis.com/job_name\"!~\".*debug\" ANDseverity=(DEFAULT OR DEBUG OR INFO OR NOTICE)\n```- The first line selects all log entries generated by the Dataflow service.\n- The second line selects all log entries where the`job_name`field does not end with the string`debug`.\n- The third line selects all log entries with the severity`DEFAULT`,`DEBUG`,`INFO`, or`NOTICE`.\n- Click **Update sink** .## Test your exclusion filter\nYou can verify that the filter is working correctly by running a sample Dataflow job and then viewing the logs.\nAfter your job starts running, to view job logs, complete the following steps:\n- In the Google Cloud console, go to the Dataflow **Jobs** page. [Go to Jobs](https://console.cloud.google.com/dataflow/jobs) A list of Dataflow jobs appears along with their status.\n- Select a job.\n- On the **Job details** page, in the **Logs** panel, click **Show** .\n- Verify that no logs appear in the **Job logs** panel and that no `DEFAULT` , `DEBUG` , `INFO` , or `NOTICE` logs appear in the **Worker logs** panel.## Bypass the exclusion filter\nThe Dataflow job name ( `job_name` ) is used to provide a bypass mechanism for scenarios where the generated Dataflow logs need to be captured. You can use this bypass to rerun a failed job and capture all the log information.\nThe filter created in this scenario retains all log entries when the `job_name` field ends with the string `debug` . When you want to bypass the exclusion filter and display all logs for a Dataflow job, append `debug` to the job name. For example, to bypass the exclusion filter, you could use the job name `dataflow-job-debug` .\n## Compare log counts\nIf you want to compare the volume of logs ingested with and without the exclusion filter, run one job with `debug` appended to the job name and one without. Use the system-defined, logs-based metric [Log bytes](/monitoring/api/metrics_gcp#logging/byte_count) to view and compare the ingestion data. For more information about viewing ingestion data, see [View ingestion data in Metrics Explorer](/stackdriver/estimating-bills#metric-exp-usage) .\n## Create an external destination\nOptionally, after you create the exclusion filter, you can create an additional Cloud Logging sink. Use this sink to redirect the complete set of Dataflow logs into a [supported external destination](/logging/docs/routing/overview#destinations) , such as BigQuery, Pub/Sub, or Splunk.\nIn this scenario, the external logs aren't stored in Logs Explorer but are available in the external destination. Using an external destination gives you more control over the costs incurred by storing logs in Logs Explorer.\nFor steps detailing how to control how Cloud Logging routes logs, see [Configure and manage sinks](/logging/docs/export/configure_export_v2) . To capture all Dataflow logs in an external destination, in the **Choose logs to include in sink** panel, in the **Build inclusion filter** field, enter the following filter expression:\n```\nresource.type=\"dataflow_step\"\n```\nTo find log entries that you routed from Cloud Logging to supported destinations, see [View logs in sink destinations](/logging/docs/export/using_exported_logs) .\n## Track Dataflow log messages by severity\nExclusion filters do not apply to [user-defined logs-based metrics](/logging/docs/logs-based-metrics#user-metrics) . These metrics count the number of log entries that match a given filter or record particular values within the matching log entries. To track counts of Dataflow log messages based on severity, you can create a logs-based metric for the Dataflow logs. The logs are tracked even when the log messages are excluded from ingestion.\nYou're billed for user-defined logs-based metrics. For pricing information, see [Chargeable metrics](/stackdriver/pricing#metrics-chargeable) .\nTo configure user-defined logs-based metrics, see [Create a counter metric](/logging/docs/logs-based-metrics/counter-metrics#create_a_counter_metric) . To track the Dataflow logs, in the **Filter selection** section, in the **Build filter** box, enter the following text:\n```\nresource.type=\"dataflow_step\"\n```", "guide": "Dataflow"}