{"title": "Dataflow - Streaming with Pub/Sub", "url": "https://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub", "abstract": "# Dataflow - Streaming with Pub/Sub\nThis page provides a conceptual overview of Dataflow's integration with [Pub/Sub](/pubsub/docs/overview) . The overview describes some optimizations that are available in the Dataflow runner's implementation of the Pub/Sub I/O connector. Pub/Sub is a scalable, durable event ingestion and delivery system. Dataflow complements Pub/Sub's scalable, at-least-once delivery model with message deduplication, exactly-once processing, and generation of a data watermark from timestamped events. To use Dataflow, write your pipeline using the Apache Beam SDK and then execute the pipeline code on the Dataflow service.\n**Note:** Pub/Sub dead-letter topics and exponential backoff delay retry policies are not supported by Dataflow. Refer to the [Unsupported Pub/Sub features](#unsupported-features) section below for alternative solutions.\nBefore you begin, learn about the basic concepts of Apache Beam and streaming pipelines. Read the following resources for more information:\n- Intro to Apache Beam concepts such as [PCollections, triggers, windows, and watermarks](/dataflow/docs/concepts/beam-programming-model) \n- After Lambda: Exactly-once processing in Dataflow [Part 1](https://cloud.google.com/blog/products/gcp/after-lambda-exactly-once-processing-in-google-cloud-dataflow-part-1) and [Part 3: Sources and Sinks](https://cloud.google.com/blog/products/gcp/after-lambda-exactly-once-processing-in-cloud-dataflow-part-3-sources-and-sinks) \n- Streaming: The world beyond batch: [101](https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101) and [102](https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102) \n- Apache Beam [programming guide](https://beam.apache.org/documentation/programming-guide/) \n**Warning:** Do not use a single Pub/Sub subscription for multiple pipelines. The data from a single subscription gets divided nondeterministically between the entities consuming that data; therefore, if you have two pipelines reading from a single subscription, each pipeline receives part of the data in a nondeterministic manner which might cause duplication of messages, watermark lag, and inefficient autoscaling. Instead, create a separate subscription for each pipeline. Alternatively, reading from a Pub/Sub topic automatically creates a separate subscription for each pipeline.\n", "content": "## Building streaming pipelines with Pub/Sub\nTo get the benefits of [Dataflow's integration with Pub/Sub](#integration-features) , you can build your streaming pipelines in any of the following ways:\n- Use existing streaming pipeline example code from the Apache Beam GitHub repo, such as [streaming word extraction](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/complete/StreamingWordExtract.java) (Java), [streaming wordcount](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/streaming_wordcount.py) (Python), and [streaming_wordcap](https://github.com/apache/beam/blob/master/sdks/go/examples/streaming_wordcap/wordcap.go) (Go).\n- Write a new pipeline using the Apache Beam API reference ( [Java](https://beam.apache.org/releases/javadoc/current) , [Python](https://beam.apache.org/releases/pydoc/current) , or [Go](https://pkg.go.dev/github.com/apache/beam/sdks/v2/go/pkg/beam) ).\n- Use Google-provided Dataflow templates and the corresponding template source code in Java.Google provides a set of Dataflow [templates](/dataflow/docs/guides/templates/provided-templates#streaming-templates) that offer a UI-based way to start Pub/Sub stream processing pipelines. If you use Java, you can also use the [source code](https://github.com/GoogleCloudPlatform/DataflowTemplates) of these templates as a starting point to create a custom pipeline.The following streaming templates export Pub/Sub data to different destinations:- [Pub/Sub Subscription to BigQuery](/dataflow/docs/guides/templates/provided/pubsub-subscription-to-bigquery) \n- [Pub/Sub to Pub/Sub relay](/dataflow/docs/guides/templates/provided/pubsub-to-pubsub) \n- [Pub/Sub to Cloud Storage Avro](/dataflow/docs/guides/templates/provided/pubsub-to-avro) \n- [Pub/Sub to Cloud Storage Text](/dataflow/docs/guides/templates/provided/pubsub-topic-to-text) \n- [Cloud Storage Text to Pub/Sub (Stream)](/dataflow/docs/guides/templates/provided/text-to-pubsub-stream) \nThe following batch template imports a stream of data into a Pub/Sub topic:- [Cloud Storage Text to Pub/Sub (Batch)](/dataflow/docs/guides/templates/provided/cloud-storage-to-bigquery) \n- Follow the [Pub/Sub quickstart for stream processing with Dataflow](/pubsub/docs/stream-messages-dataflow) to run a simple pipeline.## Pub/Sub and Dataflow integration features\nApache Beam provides a reference I/O source implementation ( `PubsubIO` ) for Pub/Sub ( [Java](https://github.com/apache/beam/tree/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/pubsub) , [Python](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/gcp/pubsub.py) , and [Go](https://github.com/apache/beam/blob/master/sdks/go/pkg/beam/io/pubsubio/pubsubio.go) ). This I/O source implementation is used by non-Dataflow runners, such as the Apache Spark runner, Apache Flink runner, and the direct runner.\nThe Dataflow runner uses a different, private implementation of `PubsubIO` (for [Java](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/pubsub/PubsubIO.html) , [Python](https://beam.apache.org/releases/pydoc/current/apache_beam.io.gcp.pubsub.html) , and [Go](https://pkg.go.dev/github.com/apache/beam/sdks/v2/go/pkg/beam/io/pubsubio) ). This implementation takes advantage of Google Cloud-internal APIs and services to offer three main advantages: low latency watermarks, high watermark accuracy (and therefore data completeness), and efficient deduplication (exactly-once message processing).\n**Note:** [Pub/Sub exactly-once delivery](/pubsub/docs/exactly-once-delivery) is a separate feature and is not used for Dataflow exactly-once processing. Enabling Pub/Sub exactly-once delivery with Dataflow jobs is not recommended because it reduces pipeline performance.\nThe Apache Beam I/O connectors let you interact with Dataflow by using controlled sources and sinks. The Dataflow runner's implementation of `PubsubIO` automatically acknowledges messages after they are successfully processed by the first fused stage and side-effects of that processing are written to persistent storage. See the [fusion documentation](/dataflow/docs/pipeline-lifecycle#fusion_optimization) for more details. Messages are therefore only acknowledged when Dataflow can guarantee that there is no data loss if some component crashes or a connection is lost.\n### Low latency watermarks\nDataflow has access to Pub/Sub's private API that provides the age of the oldest unacknowledged message in a subscription, with lower latency than is available in Cloud Monitoring. For comparison, the Pub/Sub backlog metrics that are available in Cloud Monitoring are typically delayed by two to three minutes, but the metrics are delayed only by approximately ten seconds for Dataflow. This makes it possible for Dataflow to advance pipeline watermarks and emit windowed computation results sooner.\n### High watermark accuracy\nAnother important problem solved natively by the Dataflow integration with Pub/Sub is the need for a robust watermark for windows defined in event time. The event time is a timestamp specified by the publisher application as an attribute of a Pub/Sub message, rather than the [publish_time](/pubsub/docs/reference/rest/v1/PubsubMessage) field set on a message by the Pub/Sub service itself. Because Pub/Sub computes backlog statistics only with respect to the service-assigned (or processing time) timestamps, estimating the event time watermark requires a separate mechanism.\nTo solve this problem, if the user elects to use custom event timestamps, the Dataflow service creates a second tracking subscription. This tracking subscription is used to inspect the event times of the messages in the backlog of the base subscription, and estimate the event time backlog. See the StackOverflow page that covers [how Dataflow computes Pub/Sub watermarks](https://stackoverflow.com/questions/42169004/what-is-the-watermark-heuristic-for-pubsubio-running-on-gcd) for more information.\n### Efficient deduplication\nMessage deduplication is required for exactly-once message processing, and you can use the [Apache Beam programming model](/dataflow/docs/concepts/beam-programming-model) to achieve exactly-once processing of Pub/Sub message streams. Dataflow deduplicates messages with respect to the Pub/Sub message ID. As a result, all processing logic can assume that the messages are already unique with respect to the Pub/Sub message ID. The efficient, incremental aggregation mechanism to accomplish this is abstracted in the `PubsubIO` API.\nIf `PubsubIO` is configured to use the Pub/Sub message attribute for deduplication instead of the message ID, Dataflow deduplicates messages published to Pub/Sub within 10 minutes of each other. The standard sorting APIs of the Dataflow service allow you to use ordered processing with Dataflow. Alternatively, to use ordering with Pub/Sub, see [Message Ordering](/pubsub/docs/ordering) .\n## Unsupported Pub/Sub features\n### Dead-letter topics and exponential backoff delay retry policies\nPub/Sub dead-letter topics and exponential backoff delay retry policies are not fully supported by Dataflow. Instead, implement these patterns explicitly within the pipeline. Two examples of dead-letter patterns are provided in the [retail application](https://github.com/GoogleCloudPlatform/dataflow-sample-applications/tree/master/retail/retail-java-applications#queuing-unprocessable-data-for-further-analysis) and the [Pub/Sub to BigQuery template](/dataflow/docs/guides/templates/provided/pubsub-subscription-to-bigquery) .\nThere are two reasons why dead-letter topics and exponential backoff delay retry policies do not work with Dataflow.\nFirst, Dataflow NACK messages (that is, send a negative acknowledgement) to Pub/Sub when pipeline code fails. Instead, Dataflow retries message processing indefinitely, while continually extending the acknowledgment deadline for the message. However, the Dataflow backend might NACK messages for various internal reasons, so it is possible for messages to be delivered to the dead-letter topic even when there have been no failures in the pipeline code.\nSecond, Dataflow might acknowledge messages before the pipeline fully processes the data. Specifically, Dataflow acknowledges messages after they are successfully processed by the first fused stage and side-effects of that processing have been written to persistent storage. If the pipeline has multiple fused stages and failures occur at any point after the first stage, the messages are already acknowledged.\n### Pub/Sub exactly-once delivery\nBecause Dataflow has its own [exactly-once processing](/dataflow/docs/concepts/exactly-once) , using [Pub/Sub exactly-once delivery](/pubsub/docs/exactly-once-delivery) with Dataflow is not recommended. Enabling Pub/Sub exactly-once delivery reduces pipeline performance, because it limits the messages available for parallel processing.\n## What's next\n- [Stream Processing with Pub/Sub and Dataflow: Qwik Start](https://www.cloudskillsboost.google/focuses/18457?parent=catalog) (self-paced lab)\n- [Stream from Pub/Sub to BigQuery](/dataflow/docs/tutorials/dataflow-stream-to-bigquery)", "guide": "Dataflow"}