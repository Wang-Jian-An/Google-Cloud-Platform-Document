{"title": "Dataflow - Dataflow security and permissions", "url": "https://cloud.google.com/dataflow/docs/concepts/security-and-permissions", "abstract": "# Dataflow - Dataflow security and permissions\nDataflow pipelines can be run locally (to perform tests on small datasets), or on managed Google Cloud resources using the [Dataflow managed service](/dataflow/service/dataflow-service-desc) . Whether running locally or in the cloud, your pipeline and its workers use a permissions system to maintain secure access to pipeline files and resources. Dataflow permissions are assigned according to the role that's used to access pipeline resources. This document explains the following concepts:\n- Upgrading Dataflow VMs.\n- Roles and permissions required for running local and Google Cloud pipelines.\n- Roles and permissions required for accessing pipeline resources across projects.\n- Types of data used in a Dataflow service and in data security.", "content": "## Before you begin\nRead about Google Cloud project identifiers in the [Platform Overview](/docs/overview#projects) . These identifiers include the project name, project ID, and project number.\n## Upgrade and patch Dataflow VMs\nDataflow uses [Container-Optimized OS](/container-optimized-os/docs/concepts/security) . Hence, the security processes of Container-Optimized OS also apply to Dataflow.\nBatch pipelines are time-bound and don't require maintenance. When a new batch pipeline starts, the latest Dataflow image is used.\nFor streaming pipelines, if a security patch is immediately required, Google Cloud notifies you by using [security bulletins](https://cloud.google.com/support/bulletins) . For streaming pipelines, we recommend that you use the [--update option](/dataflow/docs/guides/updating-a-pipeline) to restart your job with the latest Dataflow image.\nDataflow container images are available in the [Google Cloud console](https://console.cloud.google.com/gcr/images/cloud-dataflow/GLOBAL/v1beta3) .\n## Security and permissions for local pipelines\nWhen you run locally, your Apache Beam pipeline runs as the Google Cloud account that you [configured with the Google Cloud CLI executable](/sdk/gcloud/reference/config/set) . Hence, locally run Apache Beam SDK operations and your Google Cloud account have access to the same files and resources.\nTo list the Google Cloud account that you selected as your default, run the `gcloud config list` command.\n**Note:** Local pipelines can output data to local destinations, such as local files, or to cloud destinations, such as Cloud Storage or BigQuery. If your locally run pipeline writes files to cloud-based resources such as Cloud Storage, it uses your Google Cloud account credentials and the [Google Cloud project that you configured](/sdk/gcloud/reference/config/set) as the Google Cloud CLI default. For instructions about how to authenticate with your Google Cloud account credentials, see the quickstart for the language you're using: [Java quickstart](/dataflow/docs/quickstarts/create-pipeline-java) , [Python quickstart](/dataflow/docs/quickstarts/create-pipeline-python) , or [Go quickstart](/dataflow/docs/quickstarts/create-pipeline-go) .\n## Security and permissions for pipelines on Google Cloud\nWhen you run your pipeline, Dataflow uses two service accounts to manage security and permissions:\n- **The Dataflow service account.** The Dataflow service uses the Dataflow service account as part of the job creation request, such as to check project quota and to create worker instances on your behalf. Dataflow also uses the Dataflow service account during job execution to manage the job. This account is also known as the Dataflow service agent.\n- **The worker service account.** Worker instances use the worker service account to access input and output resources after you submit your job. By default, workers use the [Compute Engine default service account](/compute/docs/access/service-accounts#default_service_account) associated with your project as the worker service account. For the worker service account to be able to create, run, and examine a job, it must have the following roles:- `roles/dataflow.admin`\n- `roles/dataflow.worker`In addition, when your Apache Beam pipelines access Google Cloud resources, you need to grant the required roles to your Dataflow project's worker service account. The worker service account needs to be able to access the resources while running the Dataflow job. For example, if your job writes to BigQuery, your service account must also have at least the `roles/bigquery.dataEditor` role. Examples of resources include:\n- [Cloud Storage buckets](#accessing_gcs) \n- [BigQuery datasets](#accessing_bigquery) \n- [Pub/Sub topics and subscriptions](#accessing_pubsub) \n- [Firestore datasets](#accessing_firestore) \n[](None) Finally, to impersonate the service account, your user account must have the following role: `iam.serviceAccounts.actAs` . Depending on other project permissions, your user account might also need the `roles/dataflow.developer` role.\nTo add the required roles in your project, follow these steps.\n- In the Google Cloud console, go to the **IAM** page. [Go to IAM](https://console.cloud.google.com/projectselector/iam-admin/iam?supportedpurview=project,folder,organizationId) \n- Select your project.\n- In the row containing your user account, click edit **Edit principal** , and then click add **Add another role** .\n- In the drop-down list, select the role **Service Account User** .\n- In the row containing the Compute Engine default service account, click edit **Edit principal** , and then click add **Add another role** .\n- In the drop-down list, select the role **Dataflow Worker** .\n- Repeat for the **Dataflow Admin** and any roles required by resources used in your job, and then click **Save** .For more information about granting roles, see [Grant an IAM role by using the console](/iam/docs/grant-role-console) .\n- Grant the `roles/iam.serviceAccountUser` role to your user account. Run the following command:```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS --role=roles/iam.serviceAccountUser\n```- Replace``with your project ID.\n- Replace``with the email address for the user account.\n- Grant roles to your Compute Engine default service account. Run the following command once for each of the following IAM roles: `roles/dataflow.admin` , `roles/dataflow.worker` , and any roles required by resources used in your job.```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com\" --role=SERVICE_ACCOUNT_ROLE\n```- Replace``with your project ID.\n- Replace``with your project number. To find your project number, see [Identify projects](/resource-manager/docs/creating-managing-projects#identifying_projects) or use the [gcloud projects describe](/sdk/gcloud/reference/projects/describe) command.\n- Replace``with each individual role.\n### Dataflow service account\nAll projects that have used the resource `Dataflow Job` have a **Dataflow Service Account** , also known as the [Dataflow service agent](/iam/docs/service-agents) , which has the following email:\n```\nservice-PROJECT_NUMBER@dataflow-service-producer-prod.iam.gserviceaccount.com\n```\nThis service account is created and managed by Google and assigned to your project automatically upon first usage of the resource `Dataflow Job` .\nAs part of running the Dataflow pipeline, the Dataflow service manipulates resources on your behalf. For example, it creates additional VMs. When you run your pipeline on the [Dataflow service](/dataflow/service/dataflow-service-desc) , the service uses this service account.\nThis account is assigned the Dataflow Service Agent role on the project. It has the necessary permissions to run a Dataflow job in the project, including starting Compute Engine workers. This account is used exclusively by the [Dataflow service](/dataflow/service/dataflow-service-desc) and is specific to your project.\nYou can review the permissions of the Dataflow service account in the Google Cloud console or the [Google Cloud CLI](/dataflow/docs/guides/using-command-line-intf#running_the_available_commands) .\n- Go to the **Roles** page. [Go to Roles](https://console.cloud.google.com/iam-admin/roles) \n- If applicable, select your project.\n- In the list, click the title **Cloud Dataflow Service Agent** . A page opens that lists the permissions assigned to the Dataflow service account.\nView the permissions of the Dataflow service account:\n```\ngcloud iam roles describe roles/dataflow.serviceAgent\n```\nBecause Google Cloud services expect to have read and write access to the project and its resources, it's recommended that you don't change the default permissions automatically established for your project. If a Dataflow service account loses permissions to a project, Dataflow cannot launch VMs or perform other management tasks.\nIf you remove the permissions for the service account from the Identity and Access Management (IAM) policy, the account remains present, because it's owned by the Dataflow service.\n**Best practice:** Create a bucket owned by your project to use as the staging bucket for Dataflow. The default bucket permissions allow Dataflow to use the bucket to stage the executable files of the pipeline.\n[](None)\n### Worker service account\nCompute Engine instances execute Apache Beam SDK operations in the cloud. These workers use the worker service account of your project to access the files and other resources associated with the pipeline. The worker service account is used as the identity for all worker VMs, and all requests that originate from the VM use the worker service account. This service account is also used to interact with resources such as Cloud Storage buckets and Pub/Sub topics.\nFor the worker service account to be able to create, run, and examine a job, it must have the following roles:\n- `roles/dataflow.admin`\n- `roles/dataflow.worker`By default, workers use the [Compute Engine default service account](/compute/docs/access/service-accounts#default_service_account) of your project as the worker service account. This service account has the following email:\n```\nPROJECT_NUMBER-compute@developer.gserviceaccount.com\n```\nThis service account is automatically created when you enable the Compute Engine API for your project from the [API Library](https://console.cloud.google.com/project/_/apiui/apis/library) in the Google Cloud console.\nThe Compute Engine default service account has broad access to the resources of your project, which helps you to get started with Dataflow. However, for production workloads, we recommend that you create a new service account with only the roles and permissions that you need.\n**Note:** In the past, Dataflow users were able to deploy applications that authenticated as the Compute Engine default service account, even if they didn't have permission to impersonate the Compute Engine default service account. This legacy behavior still affects some organizations. For more information, see [Requiringimpersonation permissions when attaching service accounts to resources](/iam/docs/service-accounts-actas) .\n[](None)\nIf you want to create and use resources with fine-grained access control, you can create a user-managed service account. Use this account as the worker service account.\n- If you don't have a user-managed service account, [create a service account](/iam/docs/creating-managing-service-accounts#creating_a_service_account) .\n- Set the required IAM roles for your service account.- For the service account to be able to create, run, and examine a job, it must have the roles`roles/dataflow.admin`and`roles/dataflow.worker`, or a custom IAM role with the required permissions for those roles. For a list of the required permissions, see [Roles](/dataflow/docs/concepts/access-control#roles) .\n- Your service account might also need additional roles to use Google Cloud resources as required by your job, such as BigQuery, Pub/Sub, or Cloud Storage. For example, if your job reads from BigQuery, your service account must also have at least the`roles/bigquery.dataViewer`role.\n- Ensure that your user-managed service account has read and write access to the staging and temporary locations specified in the Dataflow job.\n- To impersonate the service account, your user account must have the`iam.serviceAccounts.actAs`permission.\n- Grant the following roles to the [Dataflow Service Account](#df-service-account) ( `service-` `` `@dataflow-service-producer-prod.iam.gserviceaccount.com` ) and to the [Compute Engine Service Agent](/compute/docs/access/service-accounts#compute_engine_service_account) ( `service-` `` `@compute-system.iam.gserviceaccount.com` ). Both of these accounts are Google-managed service accounts on the user-managed service account. These accounts are in the same project as the user-managed service account, even if the Dataflow job runs in a different project.- [Service Account Token Creator role](/iam/docs/service-accounts#token-creator-role) (`iam.serviceAccountTokenCreator`)\n- [Service Account User role](/iam/docs/service-accounts#user-role) (`iam.serviceAccountUser`)\nFor instructions demonstrating how to grant roles to service accounts, see the [Grant a single role](/iam/docs/manage-access-service-accounts#grant-single-role) section in the Manage access to service accounts page.\n- When you run your pipeline job, specify your service account.\nUse the `--serviceAccount` option and specify your service account when you run your pipeline job from the command line: `--serviceAccount=` `` `@` `` `.iam.gserviceaccount.com`\nUse the `--service-account-email` option and specify your service account when you run your pipeline job as a Flex template: `--service-account-email=` `` `@` `` `.iam.gserviceaccount.com`Use the `--service_account_email` option and specify your service account when you run your pipeline job: `--service_account_email=` `` `@` `` `.iam.gserviceaccount.com`Use the `--service_account_email` option and specify your service account when you run your pipeline job: `--service_account_email=` `` `@` `` `.iam.gserviceaccount.com`You can get a list of the service accounts associated with your project from the [Permissions page](https://console.cloud.google.com/projectselector/permissions/projectpermissions) in the Google Cloud console.\nThe user-managed service account can be in the same project as your job, or in a different project. If the service account and the job are in different projects, you must [configure the service account](/iam/docs/attach-service-accounts#attaching-different-project) before you run the job.\n## Access Google Cloud resources\nYour Apache Beam pipelines can access Google Cloud resources, either in the same Google Cloud project or in other projects. These resources include:\n- [Artifact Registry repositories](#access-ar) \n- [Cloud Storage buckets](#accessing_gcs) \n- [BigQuery datasets](#accessing_bigquery) \n- [Pub/Sub topics and subscriptions](#accessing_pubsub) \n- [Firestore datasets](#accessing_firestore) \nTo ensure that your Apache Beam pipeline can access these resources, you need to use the resources' respective access control mechanisms to explicitly grant access to your Dataflow project [worker service account](#worker-service-account) .\nIf you use Assured Workloads features with Dataflow, such as [EU Regions and Support with Sovereignty Controls](/assured-workloads/docs/concept-platform-controls#eu-sovereignty-controls) , all Cloud Storage, BigQuery, Pub/Sub, I/O connectors, and other resources that your pipeline accesses must be located in your organization's [Assured Workloads project or folder](/assured-workloads/docs/eu-sovereign-controls-restrictions-limitations) .\nIf you're using a user-managed worker service account, or accessing resources in other projects, then additional action might be needed. The following examples assume that the Compute Engine default service account is used, but you can also use a user-managed worker service account.\n### Access Artifact Registry repositories\nWhen you [use custom containers with Dataflow](/dataflow/docs/guides/using-custom-containers) , you might upload artifacts to an Artifact Registry repository.\nTo use Artifact Registry with Dataflow, you must grant at least [Artifact Registry Writer access](/artifact-registry/docs/access-control#permissions) ( `role/artifactregistry.writer` ) to the [worker service account](#worker-service-account) that runs the Dataflow job.\nAll repository content is encrypted using either Google-managed or customer-managed encryption keys. Artifact Registry uses Google-managed encryption keys by default and no configuration is required for this option.\n### Access Cloud Storage buckets\nTo grant your Dataflow project access to a Cloud Storage bucket, make the bucket accessible to your Dataflow project [worker service account](#worker-service-account) . At a minimum, your service account needs read and write permissions to both the bucket and its contents. You can use [IAM permissions for Cloud Storage](/storage/docs/access-control/using-iam-permissions) to grant the required access.\nTo give your worker service account the necessary permissions to read from and write to a bucket, use the [gcloud storage buckets add-iam-policy-binding](/sdk/gcloud/reference/storage/buckets/add-iam-policy-binding) command. This command adds your Dataflow project service account to a [bucket-level policy](/storage/docs/access-control/using-iam-permissions#bucket-add) .\n```\ngcloud storage buckets add-iam-policy-binding gs://BUCKET_NAME --member=\"serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com\" --role=SERVICE_ACCOUNT_ROLE\n```\nReplace the following:\n- : the name of your Cloud Storage bucket\n- : your Dataflow project number. To find your project number, see [Identify projects](/resource-manager/docs/creating-managing-projects#identifying_projects) or use the [gcloud projects describe](/sdk/gcloud/reference/projects/describe) command.\n- : the IAM role\nIf your service account needs to have full control over storage objects, including listing, creating, viewing, and deleting objects and managed folders, grant your service account the Storage Object Admin ( `roles/storage.objectAdmin` ) role.\nTo retrieve a list of the Cloud Storage buckets in a Google Cloud project, use the [gcloud storage buckets list](/sdk/gcloud/reference/storage/buckets/list) command:\n```\ngcloud storage buckets list --project= PROJECT_ID\n```\nReplace with the ID of the project.\nUnless you're restricted by organizational policies that limit resource sharing, you can access a bucket that resides in a different project than your Dataflow pipeline. For more information about domain restrictions, see [Restricting identities by domain](/resource-manager/docs/organization-policy/restricting-domains) .\nIf you don't have a bucket, [create a new bucket](/storage/docs/creating-buckets) . Then, give your worker service account the necessary permissions to read from and write to the bucket.\nYou can also set bucket permissions from the Google Cloud console. For more information, see [Setting bucket permissions](/storage/docs/cloud-console#_bucketpermission) .\nCloud Storage offers two systems for granting users access to your buckets and objects: IAM and Access Control Lists (ACLs). In most cases, IAM is the recommended method for controlling access to your resources.\n- IAM controls permissioning throughout Google Cloud and lets you grant permissions at the bucket and project levels. For a list of IAM roles that are associated with Cloud Storage and the permissions that are contained in each role, see [IAM roles for Cloud Storage](/storage/docs/access-control/iam-roles) . If you need more control over permissions, [create a custom role](/iam/docs/creating-custom-roles) .\n- If you [use ACLs to control access](/storage/docs/access-control) , ensure that your worker service account permissions are consistent with your IAM settings. Due to the inconsistency between IAM and ACL policies, the Cloud Storage bucket might become inaccessible to your Dataflow jobs when the Cloud Storage bucket is migrated from fine-grained access to uniform bucket-level access. For more information, see [Common error guidance](/dataflow/docs/guides/common-errors#staged-package-inaccessible) .\n### Access BigQuery datasets\nYou can use the `BigQueryIO` API to access BigQuery datasets, in the same project where you're using Dataflow or in a different project. For the BigQuery source and sink to operate properly, the following two accounts must have access to any BigQuery datasets that your Dataflow job reads from or writes to:\n- The Google Cloud account that you use to run the Dataflow job.\n- The [worker service account](#worker-service-account) that runs the Dataflow job.\nYou might need to configure BigQuery to explicitly grant access to these accounts. See [BigQuery Access Control](/bigquery/access-control) for more information on granting access to BigQuery datasets using either the [BigQuery page](/bigquery/bigquery-web-ui) or the [BigQuery API](/bigquery/docs/reference/v2/datasets/update) .\nAmong the required BigQuery permissions, the `bigquery.datasets.get` IAM permission is required by the pipeline to access a BigQuery dataset. Typically, most BigQuery IAM roles include the `bigquery.datasets.get` permission, but the `roles/bigquery.jobUser` role is an exception.\nFor example, if your Google Cloud account is `cloudysanfrancisco@gmail.com` and the project number of the project where you execute the Dataflow job is `123456789` , the following accounts must all be granted access to the BigQuery datasets used: `cloudysanfrancisco@gmail.com` , and `123456789-compute@developer.gserviceaccount.com` .\n### Access Pub/Sub topics and subscriptions\nTo access a Pub/Sub topic or subscription, use the [Identity and Access Management](/pubsub/access_control) features of Pub/Sub to set up permissions for the [worker service account](#worker-service-account) .\nPermissions from the following [Pub/Sub roles](/pubsub/docs/access-control#roles) are relevant:\n- `roles/pubsub.subscriber`is **required** to consume data.\n- `roles/pubsub.editor`is **required** to create a  Pub/Sub subscription.\n- `roles/pubsub.viewer`is **recommended** , so that  Dataflow can query the configurations of  topics and subscriptions. This configuration has two benefits:- Dataflow can check for [unsupported settings](/dataflow/docs/concepts/streaming-with-cloud-pubsub#unsupported-features) on subscriptions that might not work as expected.\n- If the subscription does not use the default [ack deadline](/pubsub/docs/subscriber) of 10 seconds, performance improves. Dataflow repeatedly  extends the ack deadline for a message while it's being processed by the  pipeline. Without`pubsub.viewer`permissions, Dataflow  is unable to query the ack deadline, and therefore must assume a default  deadline. This configuration causes Dataflow to issue more [modifyAckDeadline](/pubsub/docs/reference/rest/v1/projects.subscriptions/modifyAckDeadline) requests than necessary.\n- If VPC Service Controls is enabled on the project that owns the   subscription or topic, IP address-based ingress rules don't allow   Dataflow to query the configurations. In this case, an   ingress rule based on the worker service account is required.\nFor more information and some code examples that demonstrate how to use the Identity and Access Management features of Pub/Sub, see [Sample use case: cross-project communication](/pubsub/access_control#sample_use_case_cross-project_communication) .\n### Access Firestore\nTo access a Firestore database (in Native mode or Datastore mode), add your Dataflow worker service account (for example, `` `-compute@developer.gserviceaccount.com` ) as editor of the project that owns the database, or use a more restrictive [Datastore role](/datastore/docs/access/iam#iam_roles) like `roles/datastore.viewer` . Also, enable the Firestore API in both projects from the [API Library](https://console.cloud.google.com/project/_/apiui/apis/library) in the Google Cloud console.\n### Access images for projects with a trusted image policy\nIf you have a [trusted image policy](/compute/docs/images/restricting-image-access) set up for your project and your boot image is located in another project, ensure that the trusted image policy is configured to have access to the image. For example, if you're running a templated Dataflow job, ensure that the policy file includes access to the `dataflow-service-producer-prod` project. This Google Cloud project contains the images for template jobs.\n## Data access and security\nThe Dataflow service works with two kinds of data:\n- **End-user data.** This data is processed by a Dataflow pipeline. A typical pipeline reads data from one or more sources, implements transformations of the data, and writes the results to one or more sinks. All the sources and sinks are storage services that are not directly managed by Dataflow.\n- **Operational data.** This data includes all the metadata that is required for managing a Dataflow pipeline. This data includes both user-provided metadata such as a job name or pipeline options and also system-generated metadata such as a job ID.\nThe Dataflow service uses several security mechanisms to keep your data secure and private. These mechanisms apply to the following scenarios:\n- Submitting a pipeline to the service\n- Evaluating a pipeline\n- Requesting access to telemetry and metrics during and after a pipeline execution\n- Using a Dataflow service such as Shuffle or Streaming Engine\n### Data locality\n**Note:** We recommend that you **always** specify a region when you run a pipeline. Support for default regions would be discontinued in a future release.\nAll the core data processing for the Dataflow service happens in the region that is specified in the pipeline code. If a region is not specified, the default region `us-central1` is used. If you specify that option in the pipeline code, the pipeline job can optionally read and write from sources and sinks in other regions. However, the actual data processing occurs only in the region that is specified to run the Dataflow VMs.\nPipeline logic is evaluated on individual worker VM instances. You can specify the zone in which these instances and the private network over which they communicate are located. Ancillary computations for the platform depend on metadata such as Cloud Storage locations or file sizes.\nDataflow is a regional service. For more information about data locality and regions, see [Dataflow regions](/dataflow/docs/concepts/regional-endpoints#data_locality) .\n### Data in a pipeline submission\nThe IAM permissions for your Google Cloud project control access to the Dataflow service. Any principals who are given editor or owner rights to your project can submit pipelines to the service. To submit pipelines, you must authenticate using the Google Cloud CLI. After you're authenticated, your pipelines are submitted using the HTTPS protocol. For instructions about how to authenticate with your Google Cloud account credentials, see the [quickstart](/dataflow/docs/quickstarts) for the language that you're using.\n### Data in a pipeline evaluation\nAs part of evaluating a pipeline, temporary data might be generated and stored locally in the worker VM instances or in Cloud Storage. Temporary data is encrypted at rest and does not persist after a pipeline evaluation concludes. Such data can also be stored in the Shuffle service or Streaming Engine service (if you have opted for the service) in the same region as specified in the Dataflow pipeline.\nBy default, Compute Engine VMs are deleted when the Dataflow job completes, regardless of whether the job succeeds or fails. Consequently, the associated [Persistent Disk](/compute/docs/disks) , and any intermediate data that might be stored on it, is deleted. The intermediate data stored in Cloud Storage can be found in sublocations of the Cloud Storage path that you provide as your `--stagingLocation` or `--tempLocation` . If you're writing output to a Cloud Storage file, temporary files might be created in the output location before the Write operation is finalized.By default, Compute Engine VMs are deleted when the Dataflow job completes, regardless of whether the job succeeds or fails. Consequently, the associated [Persistent Disk](/compute/docs/disks) , and any intermediate data that might be stored on it, is deleted. The intermediate data stored in Cloud Storage can be found in sublocations of the Cloud Storage path that you provide as your `--staging_location` or `--temp_location` . If you're writing output to a Cloud Storage file, temporary files might be created in the output location before the Write operation is finalized.By default, Compute Engine VMs are deleted when the Dataflow job completes, regardless of whether the job succeeds or fails. Consequently, the associated [Persistent Disk](/compute/docs/disks) , and any intermediate data that might be stored on it, is deleted. The intermediate data stored in Cloud Storage can be found in sublocations of the Cloud Storage path that you provide as your `--staging_location` or `--temp_location` . If you're writing output to a Cloud Storage file, temporary files might be created in the output location before the Write operation is finalized.\n### Data in pipeline logs and telemetry\nInformation stored in [Cloud Logging](/logging/docs/region-support) is primarily generated by the code in your Dataflow program. The Dataflow service might also generate warning and error data in Cloud Logging, but this data is the only intermediate data that the service adds to logs. Cloud Logging is a global service.\nTelemetry data and associated metrics are encrypted at rest, and access to this data is controlled by your Google Cloud project's read permissions.\n### Data in Dataflow services\nIf you use Dataflow Shuffle or Dataflow Streaming for your pipeline, don't specify the zone pipeline options. Instead, specify the region and set the value to one of the regions where Shuffle or Streaming is available. Dataflow auto-selects the zone in the region that you specify. The end-user data in transit stays within the worker VMs and in the same zone. These Dataflow jobs can still read and write to sources and sinks that are outside the VM zone. The data in transit can also be sent to Dataflow Shuffle or Dataflow Streaming services, however the data always remains in the region specified in the pipeline code.\n### Recommended practice\nWe recommend that you use the security mechanisms available in the underlying cloud resources of your pipeline. These mechanisms include the data security capabilities of data sources and sinks such as BigQuery and Cloud Storage. It's also best not to mix different trust levels in a single project.", "guide": "Dataflow"}