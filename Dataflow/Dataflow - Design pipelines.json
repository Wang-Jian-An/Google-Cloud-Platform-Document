{"title": "Dataflow - Design pipelines", "url": "https://cloud.google.com/dataflow/docs/design-considerations", "abstract": "# Dataflow - Design pipelines\nThis document highlights some things to consider when designing your pipeline.\n", "content": "## Questions to consider\nIn designing your pipeline, think about the following questions:\n- Where is your pipeline's input data stored? How many sets of input data do you have?\n- What does your data look like?\n- What do you want to do with your data?\n- Where should your pipeline's output data go?\n- Does your Dataflow job use [Assured Workloads](/assured-workloads/docs/concept-platform-controls) ?## Consider using Dataflow templates\nAlternatively to building a pipeline (by writing Apache Beam code), you can use a [template](/dataflow/docs/concepts/dataflow-templates) . Templates allow for reusability. Templates let you customize each job by changing specific pipeline parameters. Then, anyone you provide permissions to can use the template to deploy the pipeline.\nFor example, a Developer can create a job from a template, and a Data Scientist in the organization can deploy that template at a later time.\nThere are many [Google-provided templates](/dataflow/docs/guides/templates/provided-templates) you can use, or you can [create your own](/dataflow/docs/guides/templates/using-flex-templates) .\n## Structure your Apache Beam user code\nOften in creating pipelines, you'll use the generic parallel processing Apache Beam transform, [ParDo](https://beam.apache.org/documentation/programming-guide/#pardo) . When you apply a `ParDo` transform, you provide user code in the form of a `DoFn` object. `DoFn` is an Apache Beam SDK class that defines a distributed processing function.\nYou can think of your `DoFn` code as small, independent entities: there can potentially be many instances running on different machines, each with no knowledge of the others. As such, we recommend creating (functions that do not depend on hidden or external state, that have no observable side effects, and are deterministic), because they are ideal for the parallel and distributed nature of `DoFn` s.\nThe pure function model is not strictly rigid, however; state information or external initialization data can be valid for `DoFn` and other function objects, as long as your code does not depend on things that the Dataflow service does not guarantee. When structuring your `ParDo` transforms and creating your `DoFn` s, keep the following guidelines in mind:\n- The Dataflow service guarantees that every element in your input`PCollection`is processed by a`DoFn`instance.\n- The Dataflow service does not guarantee how many times a`DoFn`will be invoked.\n- The Dataflow service does not guarantee exactly how the distributed elements are grouped\u2014that is, it does not guarantee which (if any) elements are processed together.\n- The Dataflow service does not guarantee the exact number of`DoFn`instances that will be created over the course of a pipeline.\n- The Dataflow service is fault-tolerant, and may retry your code multiple times in the case of worker issues. The Dataflow service may create backup copies of your code, and can have issues with manual side effects (such as if your code relies upon or creates temporary files with non-unique names).\n- The Dataflow service serializes element processing per`DoFn`instance. Your code does not need to be strictly thread-safe; however, any state shared between multiple`DoFn`instances must be thread-safe.## Assured Workloads\nAssured Workloads helps enforce security and compliance requirements for Google Cloud customers. For example, [EU Regions and Support with Sovereignty Controls](/assured-workloads/docs/concept-platform-controls#eu-sovereignty-controls) helps enforce data residency and data sovereignty guarantees for EU-based customers. To provide these features, some Dataflow features are restricted or limited. If you use Assured Workloads with Dataflow, all of the resources that your pipeline accesses must be located in your organization's [Assured Workloads project or folder](/assured-workloads/docs/eu-sovereign-controls-restrictions-limitations) . These resources include:\n- Cloud Storage buckets\n- BigQuery datasets\n- Pub/Sub topics and subscriptions\n- Firestore datasets\n- I/O connectors\nIn Dataflow, user-specified data keys used in key-based operations are not protected by [CMEK encryption](/dataflow/docs/guides/customer-managed-encryption-keys) . If these keys contain personally identifiable information (PII), you need to [hash or otherwise transform the keys](/dataflow/docs/guides/customer-managed-encryption-keys#encryption_of_pipeline_state_artifacts) before they enter the Dataflow pipeline.\n## Share data across pipelines\nThere is no Dataflow-specific cross pipeline communication mechanism for sharing data or processing context between pipelines. You can use durable storage like Cloud Storage or an in-memory cache like App Engine to share data between pipeline instances.\n## Schedule jobs\nYou can automate pipeline execution in the following ways:\n- Using Cloud Scheduler\n- Using Apache Airflow's [Dataflow Operator](https://airflow.apache.org/integration.html#cloud-dataflow) , one of several [Google Cloud Operators](/composer/docs/how-to/using/writing-dags#operators) in a [Cloud Composer workflow](/composer/docs/how-to/using/writing-dags) \n- Running custom (cron) job processes on Compute Engine## Learn more\nThe following Apache Beam articles describe how to structure your pipeline, how to choose which transforms to apply to your data, and what to consider in choosing your pipeline's input and output methods.\n- [Design your pipeline](https://beam.apache.org/documentation/pipelines/design-your-pipeline/) .\n- [Create your pipeline](https://beam.apache.org/documentation/pipelines/create-your-pipeline/) .\nFor more information about building your user code, see the [requirements for user-provided functions](https://beam.apache.org/documentation/programming-guide/#requirements-for-writing-user-code-for-beam-transforms) .", "guide": "Dataflow"}