{"title": "Dataflow - Write from Dataflow to Bigtable", "url": "https://cloud.google.com/dataflow/docs/guides/write-to-bigtable", "abstract": "# Dataflow - Write from Dataflow to Bigtable\nTo write data from Dataflow to Bigtable, use the Apache Beam [Bigtable I/O connector](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigtable/package-summary.html) .\n**Note:** Depending on your scenario, consider using one of the [Google-provided Dataflow templates](/dataflow/docs/guides/templates/provided-templates) . Several of these write to Bigtable.\n", "content": "## Parallelism\nParallelism is controlled by the number of [nodes](/bigtable/docs/instances-clusters-nodes#nodes) in the Bigtable cluster. Each node manages one or more key ranges, although key ranges can move between nodes as part of [load balancing](/bigtable/docs/overview#load-balancing) . For more information, see [Understand performance](/bigtable/docs/performance) in the Bigtable documentation.\nYou are charged for the number of nodes in your instance's clusters. See [Bigtable pricing](/bigtable/pricing) .\n## Performance\nThe following table shows performance metrics for Bigtable I/O write operations. The workloads were run on one `e2-standard2` worker, using the Apache Beam SDK 2.48.0 for Java. They did not use Runner v2.\n| 100M record | 1kB | 1 column | Throughput (bytes) | Throughput (elements)  |\n|:-------------------------------|:---------------------|:---------------------------|\n| Write       | 65 MBps    | 60,000 elements per second |\nThese metrics are based on simple batch pipelines. They are intended to compare performance between I/O connectors, and are not necessarily representative of real-world pipelines. Dataflow pipeline performance is complex, and is a function of VM type, the data being processed, the performance of external sources and sinks, and user code. Metrics are based on running the Java SDK, and aren't representative of the performance characteristics of other language SDKs. For more information, see [Beam IO Performance](https://beam.apache.org/performance/) .\n## Best practices\n- In general, avoid using transactions. Transactions aren't guaranteed to be idempotent, and Dataflow might invoke them multiple times due to retries, causing unexpected values.\n- A single Dataflow worker might process data for many key ranges, leading to inefficient writes to Bigtable. Using `GroupByKey` to group data by Bigtable key can significantly improve write performance.\n- If you write large datasets to Bigtable, consider calling [withFlowControl](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigtable/BigtableIO.Write.html#withFlowControl-boolean-) . This setting automatically rate-limits traffic to Bigtable, to ensure the Bigtable servers have enough resources available to serve data.## What's next\n- Read the [Bigtable I/O connector](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigtable/package-summary.html) documentation.\n- See the list of [Google-provided templates](/dataflow/docs/guides/templates/provided-templates) .", "guide": "Dataflow"}