{"title": "Dataflow - Run ML inference with multiple differently-trained models", "url": "https://cloud.google.com/dataflow/docs/notebooks/per_key_models?hl=zh-cn", "abstract": "# Dataflow - Run ML inference with multiple differently-trained models\n| 0     | 1      |\n|:--------------------|:----------------------|\n| Run in Google Colab | View source on GitHub |\nRunning inference with multiple differently-trained models performing the same task is useful in many scenarios, including the following examples:\n- You want to compare the performance of multiple different models.\n- You have models trained on different datasets that you want to use conditionally based on additional metadata.\nIn Apache Beam, the recommended way to run inference is to use the `RunInference` transform. By using a `KeyedModelHandler` , you can efficiently run inference with O(100s) of models without having to manage memory yourself.\nThis notebook demonstrates how to use a `KeyedModelHandler` to run inference in an Apache Beam pipeline with multiple different models on a per-key basis. This notebook uses pretrained pipelines from Hugging Face. Before continuing with this notebook, it is recommended that you walk through the [Use RunInference in Apache Beam](https://colab.sandbox.google.com/github/apache/beam/blob/master/examples/notebooks/beam-ml/run_inference_pytorch_tensorflow_sklearn.ipynb) notebook.\n", "content": "## Install dependencies\nInstall both Apache Beam and the dependencies needed by Hugging Face.\n```\n!pip install apache_beam[gcp]>=2.51.0 --quiet!pip install torch --quiet!pip install transformers --quiet# To use the newly installed versions, restart the runtime.exit()\n```\n```\nfrom typing import Dictfrom typing import Iterablefrom typing import Tuplefrom transformers import pipelineimport apache_beam as beamfrom apache_beam.ml.inference.base import KeyedModelHandlerfrom apache_beam.ml.inference.base import KeyModelMappingfrom apache_beam.ml.inference.base import PredictionResultfrom apache_beam.ml.inference.huggingface_inference import HuggingFacePipelineModelHandlerfrom apache_beam.ml.inference.base import RunInference\n```\n## Define the model configurations\nA model handler is the Apache Beam method used to define the configuration needed to load and invoke models. Because this example uses two models, we define two model handlers, one for each model. Because both models are incapsulated within Hugging Face pipelines, we use the model handler `HuggingFacePipelineModelHandler` .\nFor this example, load the models using Hugging Face, and then run them against an example. The models produce different outputs.\n```\ndistilbert_mh = HuggingFacePipelineModelHandler('text-classification', model=\"distilbert-base-uncased-finetuned-sst-2-english\")roberta_mh = HuggingFacePipelineModelHandler('text-classification', model=\"roberta-large-mnli\")distilbert_pipe = pipeline('text-classification', model=\"distilbert-base-uncased-finetuned-sst-2-english\")roberta_large_pipe = pipeline(model=\"roberta-large-mnli\")\n```\n```\nDownloading (\u2026)lve/main/config.json: 0%|   | 0.00/629 [00:00<?, ?B/s]\nDownloading model.safetensors: 0%|   | 0.00/268M [00:00<?, ?B/s]\nDownloading (\u2026)okenizer_config.json: 0%|   | 0.00/48.0 [00:00<?, ?B/s]\nDownloading (\u2026)solve/main/vocab.txt: 0%|   | 0.00/232k [00:00<?, ?B/s]\nDownloading (\u2026)lve/main/config.json: 0%|   | 0.00/688 [00:00<?, ?B/s]\nDownloading model.safetensors: 0%|   | 0.00/1.43G [00:00<?, ?B/s]\nDownloading (\u2026)olve/main/vocab.json: 0%|   | 0.00/899k [00:00<?, ?B/s]\nDownloading (\u2026)olve/main/merges.txt: 0%|   | 0.00/456k [00:00<?, ?B/s]\nDownloading (\u2026)/main/tokenizer.json: 0%|   | 0.00/1.36M [00:00<?, ?B/s]\n```\n```\ndistilbert_pipe(\"This restaurant is awesome\")\n```\n```\n[{'label': 'POSITIVE', 'score': 0.9998743534088135}]\n```\n```\nroberta_large_pipe(\"This restaurant is awesome\")\n```\n```\n[{'label': 'NEUTRAL', 'score': 0.7313134670257568}]\n```\n## Define the examples\nDefine examples to input into the pipeline. The examples include the correct classifications.\n```\nexamples = [\u00a0 \u00a0 (\"This restaurant is awesome\", \"positive\"),\u00a0 \u00a0 (\"This restaurant is bad\", \"negative\"),\u00a0 \u00a0 (\"I feel fine\", \"neutral\"),\u00a0 \u00a0 (\"I love chocolate\", \"positive\"),]\n```\nTo feed the examples into RunInference, you need distinct keys that can map to the model. In this case, to make it possible to extract the actual sentiment of the example later, define keys in the form `<model_name>-<actual_sentiment>` .\n```\nclass FormatExamples(beam.DoFn):\u00a0 \"\"\"\u00a0 Map each example to a tuple of ('<model_name>-<actual_sentiment>', 'example').\u00a0 Use these keys to map our elements to the correct models.\u00a0 \"\"\"\u00a0 def process(self, element: Tuple[str, str]) -> Iterable[Tuple[str, str]]:\u00a0 \u00a0 yield (f'distilbert-{element[1]}', element[0])\u00a0 \u00a0 yield (f'roberta-{element[1]}', element[0])\n```\nUse the formatted keys to define a `KeyedModelHandler` that maps keys to the `ModelHandler` used for those keys. The `KeyedModelHandler` method lets you define an optional `max_models_per_worker_hint` , which limits the number of models that can be held in a single worker process at one time. If your worker might run out of memory, use this option. For more information about managing memory, see [Use a keyed ModelHandler](https://beam.apache.org/documentation/ml/about-ml/#use-a-keyed-modelhandler-object) .\n```\nper_key_mhs = [\u00a0 \u00a0 KeyModelMapping(['distilbert-positive', 'distilbert-neutral', 'distilbert-negative'], distilbert_mh),\u00a0 \u00a0 KeyModelMapping(['roberta-positive', 'roberta-neutral', 'roberta-negative'], roberta_mh)]mh = KeyedModelHandler(per_key_mhs, max_models_per_worker_hint=2)\n```\n## Postprocess the results\nThe `RunInference` transform returns a tuple that contains the following objects:\n- the original key\n- a`PredictionResult`object containing the original example and the inference Use those outputs to extract the relevant data. Then, to compare each model's prediction, group this data by the original example.\n```\nclass ExtractResults(beam.DoFn):\u00a0 \"\"\"\u00a0 Extract the relevant data from the PredictionResult object.\u00a0 \"\"\"\u00a0 def process(self, element: Tuple[str, PredictionResult]) -> Iterable[Tuple[str, Dict[str, str]]]:\u00a0 \u00a0 actual_sentiment = element[0].split('-')[1]\u00a0 \u00a0 model = element[0].split('-')[0]\u00a0 \u00a0 result = element[1]\u00a0 \u00a0 example = result.example\u00a0 \u00a0 predicted_sentiment = result.inference[0]['label']\u00a0 \u00a0 yield (example, {'model': model, 'actual_sentiment': actual_sentiment, 'predicted_sentiment': predicted_sentiment})\n```\nFinally, print the results produced by each model.\n```\nclass PrintResults(beam.DoFn):\u00a0 \"\"\"\u00a0 Print the results produced by each model along with the actual sentiment.\u00a0 \"\"\"\u00a0 def process(self, element: Tuple[str, Iterable[Dict[str, str]]]):\u00a0 \u00a0 example = element[0]\u00a0 \u00a0 actual_sentiment = element[1][0]['actual_sentiment']\u00a0 \u00a0 predicted_sentiment_1 = element[1][0]['predicted_sentiment']\u00a0 \u00a0 model_1 = element[1][0]['model']\u00a0 \u00a0 predicted_sentiment_2 = element[1][1]['predicted_sentiment']\u00a0 \u00a0 model_2 = element[1][1]['model']\u00a0 \u00a0 if model_1 == 'distilbert':\u00a0 \u00a0 \u00a0 distilbert_prediction = predicted_sentiment_1\u00a0 \u00a0 \u00a0 roberta_prediction = predicted_sentiment_2\u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 roberta_prediction = predicted_sentiment_1\u00a0 \u00a0 \u00a0 distilbert_prediction = predicted_sentiment_2\u00a0 \u00a0 print(f'Example: {example}\\nActual Sentiment: {actual_sentiment}\\n'\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f'Distilbert Prediction: {distilbert_prediction}\\n'\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f'Roberta Prediction: {roberta_prediction}\\n------------')\n```\n## Run the pipeline\nTo run a single Apache Beam pipeline, combine the previous steps.\n```\nwith beam.Pipeline() as beam_pipeline:\u00a0 formatted_examples = (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 beam_pipeline\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"ReadExamples\" >> beam.Create(examples)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"FormatExamples\" >> beam.ParDo(FormatExamples()))\u00a0 inferences = (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 formatted_examples\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"Run Inference\" >> RunInference(mh)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"ExtractResults\" >> beam.ParDo(ExtractResults())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"GroupByExample\" >> beam.GroupByKey()\u00a0 )\u00a0 inferences | beam.ParDo(PrintResults())\n```\n```\nExample: This restaurant is awesome\nActual Sentiment: positive\nDistilbert Prediction: POSITIVE\nRoberta Prediction: NEUTRAL\n-----------Example: This restaurant is bad\nActual Sentiment: negative\nDistilbert Prediction: NEGATIVE\nRoberta Prediction: NEUTRAL\n-----------Example: I love chocolate\nActual Sentiment: positive\nDistilbert Prediction: POSITIVE\nRoberta Prediction: NEUTRAL\n-----------Example: I feel fine\nActual Sentiment: neutral\nDistilbert Prediction: POSITIVE\nRoberta Prediction: ENTAILMENT\n-----------\n```", "guide": "Dataflow"}