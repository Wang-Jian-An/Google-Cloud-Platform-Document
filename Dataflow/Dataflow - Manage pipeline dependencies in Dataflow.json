{"title": "Dataflow - Manage pipeline dependencies in Dataflow", "url": "https://cloud.google.com/dataflow/docs/guides/manage-dependencies", "abstract": "# Dataflow - Manage pipeline dependencies in Dataflow\nMany Apache Beam pipelines can run using the default Dataflow runtime environments. However, some data processing use cases benefit from using additional libraries or classes. In these cases, you might need to manage your pipeline dependencies.\nThe following list provides some reasons you might need to manage your pipeline dependencies:\n- The dependencies provided by the default runtime environment are insufficient for your use case.\n- The default dependencies either have version collisions or have classes and libraries that are incompatible with your pipeline code.\n- You need to pin to specific library versions for your pipeline.\n- You have a Python pipeline that needs to run with a consistent set of dependencies.\nHow you manage dependencies depends on whether your pipeline uses [Java](#java) , [Python](#python) , or [Go](#go) .\n", "content": "## Java\nIncompatible classes and libraries can cause Java dependency issues. If your pipeline contains user-specific code and settings, the code can't contain mixed versions of libraries.\n### Java dependency issues\nWhen your pipeline has Java dependency issues, one of the following errors might occur:\n- `NoClassDefFoundError`: This error occurs when an entire class is not available during runtime.\n- `NoSuchMethodError`: This error occurs when the class in the classpath uses a version that doesn't contain the correct method or when the method signature changed.\n- `NoSuchFieldError`: This error occurs when the class in the classpath uses a version that doesn't have a field required during runtime.\n- `FATAL ERROR`: This error occurs when a built-in dependency can't be loaded properly. When using an uber JAR file (shaded), don't include libraries that use signatures in the same JAR file, such as Conscrypt.\n### Dependency management\nTo simplify dependency management for Java pipelines, Apache Beam uses [Bill of Materials (BOM)](/java/docs/bom) artifacts. The BOM helps dependency management tools select compatible dependency combinations. For more information, see [Apache Beam SDK for Java dependencies](https://beam.apache.org/documentation/sdks/java-dependencies/) in the Apache Beam documentation.\nTo use a BOM with your pipeline and to explicitly add other dependencies to the dependency list, add the following information to the `pom.xml` file for the SDK artifact. To import the correct libraries BOM, use `beam-sdks-java-io-google-cloud-platform-bom` .\n```\n<dependencyManagement>\u00a0 <dependencies>\u00a0 \u00a0 <dependency>\u00a0 \u00a0 \u00a0 <groupId>org.apache.beam</groupId>\u00a0 \u00a0 \u00a0 <artifactId>beam-sdks-java-google-cloud-platform-bom</artifactId>\u00a0 \u00a0 \u00a0 <version>LATEST</version>\u00a0 \u00a0 \u00a0 <type>pom</type>\u00a0 \u00a0 \u00a0 <scope>import</scope>\u00a0 \u00a0 </dependency>\u00a0 </dependencies></dependencyManagement><dependencies>\u00a0 <dependency>\u00a0 \u00a0 <groupId>org.apache.beam</groupId>\u00a0 \u00a0 <artifactId>beam-sdks-java-core</artifactId>\u00a0 </dependency>\u00a0 <dependency>\u00a0 \u00a0 <groupId>org.apache.beam</groupId>\u00a0 \u00a0 <artifactId>beam-runners-google-cloud-dataflow-java</artifactId>\u00a0 </dependency>\u00a0 <dependency>\u00a0 \u00a0 <groupId>org.apache.beam</groupId>\u00a0 \u00a0 <artifactId>beam-sdks-java-io-google-cloud-platform</artifactId>\u00a0 </dependency></dependencies>\n```\nThe `beam-sdks-java-core` artifact contains only the core SDK. You need to explicitly add other dependencies, such as I/O and runners, to the dependency list.\n## Python\nWhen you run Dataflow jobs by using the Apache Beam Python SDK, dependency management is useful in the following scenarios:\n- Your pipeline uses public packages from the [Python Package Index](https://pypi.python.org/) (PiPy), and you want to make these packages available remotely.\n- You want to create a reproducible environment.\n- To reduce startup time, you want to avoid dependency installation on the workers at runtime.\n### Define Python pipeline dependencies\nAlthough you can use a single Python script or notebook to write an Apache Beam pipeline, in the Python ecosystem, software is often distributed as packages. To make your pipeline easier to maintain, when your pipeline code spans multiple files, group the pipeline files as a Python package.\n- Define the dependencies of the pipeline in the`setup.py`file of your package.\n- Stage the package to the workers using the`--setup_file`pipeline option.\nWhen the remote workers start, they install your package. For an example, see [juliaset](https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples/complete/juliaset) in the Apache Beam GitHub.\nTo structure your pipeline as a Python package, follow these steps:\n- Create a `setup.py` file for your project. In the `setup.py` file, include the [install_requires](https://packaging.python.org/en/latest/guides/distributing-packages-using-setuptools/#install-requires) argument to specify the minimal set of dependencies for your pipeline. The following example shows a basic `setup.py` file.```\nimport setuptoolssetuptools.setup(\u00a0 name='PACKAGE_NAME',\u00a0 version='PACKAGE_VERSION',\u00a0 install_requires=[],\u00a0 packages=setuptools.find_packages(),)\n```\n- Add the `setup.py` file, the main workflow file, and a directory with the rest of the files to the root directory of your project. This file grouping is the Python package for your pipeline. The file structure looks like the following example:```\nroot_dir/\n package_name/\n my_pipeline_launcher.py\n my_custom_transforms.py\n ...other files...\n setup.py\n main.py\n```\n- To run your pipeline, install the package in the submission environment. Use the `--setup_file` pipeline option to stage the package to the workers. For example:```\npython -m pip install -e .python main.py --runner DataflowRunner --setup_file ./setup.py \u00a0<...other options...>\n```\nThese steps simplify pipeline code maintenance, particularly when the code grows in size and complexity. For other ways to specify dependencies, see [Managing Python pipeline dependencies](https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/) in the Apache Beam documentation.\n### Use custom containers to control the runtime environment\nTo run a pipeline with the Apache Beam Python SDK, Dataflow workers need a Python environment that contains an interpreter, the Apache Beam SDK, and the pipeline dependencies. Docker container images provide the appropriate environment for running your pipeline code.\nStock container images are released with each version of the Apache Beam SDK, and these images include the Apache Beam SDK dependencies. For more information, see [Apache Beam SDK for Python dependencies](https://beam.apache.org/documentation/sdks/python-dependencies/) in the Apache Beam documentation.\nWhen your pipeline requires a dependency that isn't included in the default container image, the dependency must be installed at runtime. Installing packages at runtime can have the following consequences:\n- Worker startup time increases due to dependency resolution, download, and installation.\n- The pipeline requires a connection to the internet to run.\n- Non-determinism occurs due to software releases in dependencies.\nTo avoid these issues, supply the runtime environment in a custom Docker container image. Using a custom Docker container image that has the pipeline dependencies preinstalled has the following benefits:\n- Ensures that the pipeline runtime environment has the same set of dependencies every time you launch your Dataflow job.\n- Lets you control the runtime environment of your pipeline.\n- Avoids potentially time-consuming dependency resolution at startup.\nWhen you use custom container images, consider the following guidance:\n- Avoid using the tag`:latest`with your custom images. Tag your builds with a date, version, or a unique identifier. This step lets you revert to a known working configuration if needed.\n- Use a [launch environment that is compatible](#control-launch-environment) with your container image. For more guidance about using custom containers, see [Build a container image](/dataflow/docs/guides/build-container-image) .\nFor details about pre-installing Python dependencies, see [Pre-install Python dependencies](/dataflow/docs/guides/build-container-image#preinstall_using_a_dockerfile) .\n### Control the launch environment with Dataflow Templates\nIf your pipeline requires additional dependencies, you might need to install them in both the runtime environment and the launch environment. The launch environment runs the production version of the pipeline. Because the launch environment must be compatible with the runtime environment, use the same versions of dependencies in both environments.\nTo have a containerized, reproducible launch environment, use Dataflow Flex Templates. For more information, see [Build and run a Flex Template](/dataflow/docs/guides/templates/using-flex-templates) . When using Flex Templates, consider the following factors:\n- If you configure the pipeline as a package, install the package in your template Dockerfile. To configure the Flex Template, specify`FLEX_TEMPLATE_PYTHON_SETUP_FILE`. For more information, see [Set required Dockerfile environment variables](/dataflow/docs/guides/templates/configuring-flex-templates#env-variables) .\n- If you use a custom container image with your pipeline, supply it when you launch your template. For more information, see [Use a custom container for dependencies](/dataflow/docs/guides/templates/configuring-flex-templates#use_a_custom_container_for_dependencies) .\n- To build your Dataflow Flex Template Docker image, use the same custom container image as the base image. For more information, see [Use custom container images](/dataflow/docs/guides/templates/configuring-flex-templates#use_custom_container_images) .\nThis construction makes your launch environment both reproducible and compatible with your runtime environment.\nFor an example that follows this approach, see the [Flex Template for a pipeline with dependencies and a custom container](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/dataflow/flex-templates/pipeline_with_dependencies) tutorial in GitHub.\nFor more information, see [Make the launch environment compatible with the runtime environment](https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/#make-the-launch-environment-compatible-with-the-runtime-environment) and [Control the dependencies the pipeline uses](https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/#pipeline-environments) in the Apache Beam documentation.\n## Go\nWhen you run Dataflow jobs by using the Apache Beam Go SDK, [Go Modules](https://pkg.go.dev/cmd/go#hdr-Modules__module_versions__and_more) are used to manage dependencies. The following file contains the default compil and runtime dependencies used by your pipeline:\n```\nhttps://raw.githubusercontent.com/apache/beam/vVERSION_NUMBER/sdks/go.sum\n```\nReplace with the SDK version that you're using.\nFor information about managing dependencies for your Go pipeline, see [Managing dependencies](https://go.dev/doc/modules/managing-dependencies) in the Go documentation.", "guide": "Dataflow"}