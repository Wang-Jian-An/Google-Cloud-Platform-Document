{"title": "Dataflow - Stream from Pub/Sub to BigQuery", "url": "https://cloud.google.com/dataflow/docs/tutorials/dataflow-stream-to-bigquery", "abstract": "# Dataflow - Stream from Pub/Sub to BigQuery\nThis tutorial uses the [Pub/Sub Subscription to BigQuery](/dataflow/docs/guides/templates/provided/pubsub-subscription-to-bigquery) template to create and run a [Dataflow template](/dataflow/docs/guides/templates/overview) job using the Google Cloud console or Google Cloud CLI. The tutorial walks you through a streaming pipeline example that reads JSON-encoded messages from [Pub/Sub](/pubsub/docs) , uses a User-Defined Function (UDF) to extend the Google-provided streaming template, transforms message data with the Apache Beam SDK, and writes the results to a [BigQuery](/bigquery/docs) table.\nThe benefit of this workflow is that you can use UDFs to extend your Google-provided streaming template. If you need to pull data from Pub/Sub and output it to BigQuery but don't need to extend the template, for a simpler workflow, use the [Pub/Sub to BigQuery subscription](/pubsub/docs/bigquery) feature.\nStreaming analytics and data integration pipelines use Pub/Sub to ingest and distribute data. Pub/Sub enables you to create systems of event producers and consumers, called **publishers** and **subscribers** . Publishers send events to the Pub/Sub service asynchronously, and Pub/Sub delivers the events to all services that need to react to them.\nDataflow is a fully-managed service for transforming and enriching data in stream (real-time) and batch modes. It provides a simplified pipeline development environment that uses the Apache Beam SDK to transform incoming data and then output the transformed data.\nIf you want to write messages to BigQuery directly, without configuring Dataflow to provide data transformation, use the [Pub/Sub to BigQuery subscription](/pubsub/docs/bigquery) feature.", "content": "## Objectives\n- Create a Pub/Sub topic.\n- Create a BigQuery dataset with a table and schema.\n- Use a Google-provided streaming template to stream data from your Pub/Sub subscription to BigQuery by using Dataflow.\n- Create a User-Defined Function (UDF) to extend the Google-provided streaming template.\n## CostsIn this document, you use the following billable components of Google Cloud:- Dataflow\n- Pub/Sub\n- Cloud Storage\n- Cloud Scheduler\n- BigQuery\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you beginThis section shows you how to select a project, enable APIs, and grant the appropriate roles to your user account and to the [worker service account](/dataflow/docs/concepts/security-and-permissions#worker-service-account) .\n- To complete the steps in this tutorial, your user account must have the [Service Account User](/iam/docs/service-accounts-actas) role. The [Compute Engine default service account](/compute/docs/access/service-accounts#default_service_account) must have the following roles: [Dataflow Worker](/dataflow/docs/concepts/access-control#roles) , [Dataflow Admin](/dataflow/docs/concepts/access-control#roles) , Pub/Sub Editor, Storage Object Admin, and BigQuery Data Editor. To add the required roles in the Google Cloud console:- In the Google Cloud console, go to the **IAM** page. [Go to IAM](https://console.cloud.google.com/projectselector/iam-admin/iam?supportedpurview=project,folder,organizationId) \n- Select your project.\n- In the row containing your user account, clickedit **Edit principal** , and then clickadd **Add another role** .\n- In the drop-down list, select the role **Service Account User** .\n- In the row containing the Compute Engine default service account, clickedit **Edit principal** , and then clickadd **Add another role** .\n- In the drop-down list, select the role **Dataflow Worker** .\n- Repeat for the **Dataflow Admin** , the **Pub/Sub Editor** , the **Storage Object Admin** , and the **BigQuery Data Editor** roles, and then click **Save** .For more information about granting roles, see [Grant an IAM role by using the console](/iam/docs/grant-role-console) .- Grant roles to your Compute Engine default service account. Run the following command once for each of the following IAM roles:- `roles/dataflow.admin`\n- `roles/dataflow.worker`\n- `roles/storage.admin`\n- `roles/pubsub.editor`\n- `roles/bigquery.dataEditor`\n```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com\" --role=SERVICE_ACCOUNT_ROLE\n```Replace the following:- ``: your project ID.\n- ``: your project number.  To find your project number, use the [gcloud projects describe command](/sdk/gcloud/reference/projects/describe) .\n- ``: each individual role.## Create the example source and sinkThis section explains how to create the following:- A streaming source of data using Pub/Sub\n- A dataset to load the data into BigQuery\n### Create a Cloud Storage bucketBegin by creating a Cloud Storage bucket using the Google Cloud console or Google Cloud CLI. The Dataflow pipeline uses this bucket as a temporary storage location.\n- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets](https://console.cloud.google.com/storage) \n- Click **Create bucket** .\n- On the **Create a bucket** page, for **Name your bucket** , enter a name that meets the [bucket naming requirements](/storage/docs/buckets#naming) . Cloud Storage bucket names must be globally unique. Don't select the other options.\n- Click **Create** .\nUse the [gcloud storage buckets create command](/sdk/gcloud/reference/storage/buckets/create) :\n```\ngcloud storage buckets create gs://BUCKET_NAME\n```\nReplace `` with a name for your Cloud Storage bucket that meets the [bucket naming requirements](/storage/docs/buckets#naming) . Cloud Storage bucket names must be globally unique.\n### Create a Pub/Sub topic and subscriptionCreate a Pub/Sub topic and then create a subscription to that topic.\nTo create a topic, complete the following steps.\n- In the Google Cloud console, go to the Pub/Sub **Topics** page. [Go to Topics](https://console.cloud.google.com/cloudpubsub/topic) \n- Click **Create topic** .\n- In the **Topic ID** field, enter an ID for your topic. For  information about how to name a topic, see [Guidelines to name a topic or a subscription](/pubsub/docs/pubsub-basics#resource_names) .\n- Retain the option **Add a default subscription** .  Don't select the other options.\n- Click **Create topic** .\nTo create a topic, run the [gcloud pubsub topics create](/sdk/gcloud/reference/pubsub/topics/create) command. For information about how to name a subscription, see [Guidelines to name a topic or a subscription](/pubsub/docs/pubsub-basics#resource_names) .\n```\ngcloud pubsub topics create TOPIC_ID\n```\nReplace `` with a name for your Pub/Sub topic.\nTo create a subscription to your topic, run the [gcloud pubsub subscriptions create](/sdk/gcloud/reference/pubsub/subscriptions/create) command:\n```\ngcloud pubsub subscriptions create --topic TOPIC_ID SUBSCRIPTION_ID\n```\nReplace `` with a name for your Pub/Sub subscription.\n### Create and run Cloud Scheduler jobsCreate and run two Cloud Scheduler jobs, one that publishes positive ratings and a second that publishes negative ratings to your Pub/Sub topic.\nCreate a Cloud Scheduler job for positive ratings.- Visit the **Cloud Scheduler** page in the console. [Go to Cloud Scheduler](https://console.cloud.google.com/cloudscheduler) \n- Click the **Create a job** button.\n- Enter the name `positive-ratings-publisher` .\n- Select a Dataflow [region](/dataflow/docs/resources/locations) close to where you run the commands in this tutorial. The value of the `REGION` variable must be a valid region name. For more information about regions and locations, see [Dataflow locations](/dataflow/docs/resources/locations) .\n- Specify the **frequency** for your job, using the [unix-cron](http://man7.org/linux/man-pages/man5/crontab.5.html) format: `* * * * *`See [Configuring Cron Job Schedules](/scheduler/docs/configuring/cron-job-schedules) for more information.\n- Select your time zone.\n- Click **Continue** .\n- In the **Target** list, select **Pub/Sub** .\n- Select your **Topic** name from the list.\n- Add the following **Message** string to be sent to your target: `{\"url\": \"https://beam.apache.org/\", \"review\": \"positive\"}`\n- Click **Create** .\nYou now have a cron job that sends a message with a positive rating to your Pub/Sub topic every minute. Your Cloud Function subscribes to that topic.\nCreate a Cloud Scheduler job for negative ratings.- On the **Cloud Scheduler** page in the console, click the **Create a job** button.\n- Enter the name `negative-ratings-publisher` .\n- Select a region for your job to run in.\n- Specify the **frequency** for your job, using the [unix-cron](http://man7.org/linux/man-pages/man5/crontab.5.html) format: `*/2 * * * *`See [Configuring Cron Job Schedules](/scheduler/docs/configuring/cron-job-schedules) for more information.\n- Select your time zone.\n- Click **Continue** .\n- In the **Target** list, select **Pub/Sub** .\n- Select your **Topic** name from the list.\n- Add the following **Message** string to be sent to your target: `{\"url\": \"https://beam.apache.org/\", \"review\": \"negative\"}`\n- Click **Create** .\nYou now have a cron job that sends a message with a negative rating to your Pub/Sub topic every two minutes. Your Cloud Function subscribes to that topic.- To create a Cloud Scheduler job for this tutorial, use the [gcloud scheduler jobs create](/sdk/gcloud/reference/scheduler/jobs/create/pubsub) command. This step creates a publisher for \"positive ratings\" that publishes one message per minute.```\ngcloud scheduler jobs create pubsub positive-ratings-publisher \\\u00a0 --schedule=\"* * * * *\" \\\u00a0 --location=DATAFLOW_REGION \\\u00a0 --topic=\"TOPIC_ID\" \\\u00a0 --message-body='{\"url\": \"https://beam.apache.org/\", \"review\": \"positive\"}'\n```Replace `` with the [region](/dataflow/docs/resources/locations) to deploy your Dataflow job in. Select a Dataflow region close to where you run the commands in this tutorial. The value of the `REGION` variable must be a valid region name.\n- To start the Cloud Scheduler job, use the [gcloud scheduler jobs run](/sdk/gcloud/reference/scheduler/jobs/run) command.```\ngcloud scheduler jobs run --location=DATAFLOW_REGION positive-ratings-publisher\n```\n- Create and run another similar publisher for \"negative ratings\" that publishes one message every two minutes. This step creates a publisher for \"negative ratings\" that publishes one message every two minutes.```\ngcloud scheduler jobs create pubsub negative-ratings-publisher \\\u00a0 --schedule=\"*/2 * * * *\" \\\u00a0 --location=DATAFLOW_REGION \u00a0\\\u00a0 --topic=\"TOPIC_ID\" \\\u00a0 --message-body='{\"url\": \"https://beam.apache.org/\", \"review\": \"negative\"}'\n```\n- Start the second Cloud Scheduler job.```\ngcloud scheduler jobs run --location=DATAFLOW_REGION negative-ratings-publisher\n```### Create a BigQuery datasetCreate a BigQuery dataset and table with the appropriate schema for your Pub/Sub topic.\nCreate a BigQuery dataset.- Open the BigQuery page in the Google Cloud console. [Go to the BigQuery page](https://console.cloud.google.com/bigquery) \n- In the **Explorer** panel, select the project where you want to create the dataset.\n- Expand the more_vert **Actions** option and click **Create dataset** .\n- On the **Create dataset** page:- For **Dataset ID** , enter`tutorial_dataset`.\n- For **Data location** , choose a geographic [location](/bigquery/docs/locations) for the dataset. After a dataset is created, the location can't be changed. **Note:** If you choose`EU`or an EU-based region for the dataset location, your Core Dataflow Customer Data resides in the EU. Core Dataflow Customer Data is defined in the [Service Specific Terms](/terms/service-terms#13-google-bigquery-service) .\n- Don't select the other options.\n- Click **Create dataset** .\nCreate a BigQuery table with a schema.- In the **Explorer** panel, expand your project and select your `tutorial_dataset` dataset.\n- Expand the more_vert **Actions** option and click **Open** .\n- In the details panel, click **Create table** add_box .\n- On the **Create table** page, in the **Source** section, select **Emptytable.** \n- On the **Create table** page, in the **Destination** section:- Verify that **Dataset name** is set to`tutorial_dataset`.\n- In the **Table name** field, enter`tutorial`.\n- Verify that **Table type** is set to **Native table** .\n- In the **Schema** section, enter the [schema](/bigquery/docs/schemas) definition. Enable **Edit as text** and enter the following table schema as a JSON array.```\n[\u00a0 {\u00a0 \u00a0 \"mode\": \"NULLABLE\",\u00a0 \u00a0 \"name\": \"url\",\u00a0 \u00a0 \"type\": \"STRING\"\u00a0 },\u00a0 {\u00a0 \u00a0 \"mode\": \"NULLABLE\",\u00a0 \u00a0 \"name\": \"review\",\u00a0 \u00a0 \"type\": \"STRING\"\u00a0 }]\n```\n- For **Partition and cluster settings** leave the default value: `No partitioning` .\n- In the **Advanced options** section, for **Encryption** leave the default value: `Google-managed key` . By default, Dataflow [encrypts customer content stored at rest](/security/encryption/default-encryption) .\n- Click **Create table** .\nUse the [bq mk](/bigquery/docs/reference/bq-cli-reference#bq_mk) command to create the dataset.\n```\nbq --location=DATAFLOW_REGION mk \\PROJECT_ID:tutorial_dataset\n```\nReplace `` with the project ID of your project.\nUse the `bq mk` command with the `--table` or `-t` flag to create a table in your dataset.\n```\nbq mk \\\u00a0 \u00a0 --table \\\u00a0 \u00a0 PROJECT_ID:tutorial_dataset.tutorial \\\u00a0 \u00a0 url:STRING,review:STRING\n```## Create a User-Defined Function (UDF)You can optionally create a JavaScript UDF to extend the Google-provided Pub/Sub Subscription to BigQuery template. UDFs let you define data transformations not present in the template and inject them into the template.\n **Note:** Dataflow UDFs are written in JavaScript and must be executable in [Nashorn JavaScript engine](https://www.oracle.com/technical-resources/articles/java/jf14-nashorn.html) .\nThe following UDF validates the URLs of the incoming ratings. Ratings with no URLs or wrong URLs are forwarded to a different output table suffixed with `_error_records` , also known as a dead-letter table, in the same project and dataset. [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataflow/extensible-templates/dataflow_udf_transform.js) \n```\n/**\u00a0* User-defined function (UDF) to transform events\u00a0* as part of a Dataflow template job.\u00a0*\u00a0* @param {string} inJson input Pub/Sub JSON message (stringified)\u00a0*/\u00a0function process(inJson) {\u00a0 \u00a0 const obj = JSON.parse(inJson);\u00a0 \u00a0 const includePubsubMessage = obj.data && obj.attributes;\u00a0 \u00a0 const data = includePubsubMessage ? obj.data : obj;\u00a0 \u00a0 if (!data.hasOwnProperty('url')) {\u00a0 \u00a0 \u00a0 throw new Error(\"No url found\");\u00a0 \u00a0 } else if (data.url !== \"https://beam.apache.org/\") {\u00a0 \u00a0 \u00a0 throw new Error(\"Unrecognized url\");\u00a0 \u00a0 }\u00a0 \u00a0 return JSON.stringify(obj);\u00a0 }\n```\nSave this JavaScript snippet to the Cloud Storage bucket created earlier.## Run the pipelineRun a streaming pipeline using the Google-provided Pub/Sub Subscription to BigQuery template. The pipeline gets incoming data from the Pub/Sub topic and outputs the data to your BigQuery dataset.\n- In the Google Cloud console, go to the Dataflow **Jobs** page. [Go to Jobs](https://console.cloud.google.com/dataflow/jobs) \n- Click **Create job from template** .\n- Enter a **Job name** for your Dataflow job.\n- For **Regional endpoint** , select a region for your Dataflow job.\n- For **Dataflow template** , select the **Pub/Sub Subscription toBigQuery** template.\n- For **BigQuery output table** , enter the following:```\nPROJECT_ID:tutorial_dataset.tutorial\n```\n- For **Pub/Sub input subscription** , enter the following:```\nprojects/PROJECT_ID/subscriptions/SUBSCRIPTION_ID\n```Replace `` with the project ID of the project where you created your BigQuery dataset and `` with the name of your Pub/Sub subscription.\n- For **Temporary location** , enter the following:```\ngs://BUCKET_NAME/temp/\n```Replace `` with the name of your Cloud Storage bucket. The `temp` folder stores temporary files, like the staged pipeline job.\n- Optional: to include a UDF for the job, expand **Optional parameters** .- For **JavaScript UDF path in Cloud Storage** , enter the following:```\ngs://BUCKET_NAME/dataflow_udf_transform.js\n```\n- For **JavaScript UDF name** , enter the following:```\nprocess\n```\n- Click **Run job** .\nTo check if the template can forward messages to a dead-letter table, publish some ratings with no URLs or wrong URLs.- Go to the Pub/Sub **Topics** page.\n- Click your .\n- Go to the **Messages** section.\n- Click **Publish message** .\n- Enter some ratings with no URLs or wrong URLs in **Message body** . For example:```\n{\"url\": \"https://beam.apache.org/documentation/sdks/java/\", \"review\": \"positive\"}\n```\n- Click **Publish** .\n **Note:** To use the Google Cloud CLI to run classic templates, you must have [gcloud CLI](/sdk/docs/install) version 138.0.0 or later.\nTo run the template in your shell or terminal, use the [gcloud dataflow jobs run](/sdk/gcloud/reference/dataflow/jobs/run) command.\n```\ngcloud dataflow jobs run JOB_NAME \\\u00a0 \u00a0 --gcs-location gs://dataflow-templates-DATAFLOW_REGION/latest/PubSub_Subscription_to_BigQuery \\\u00a0 \u00a0 --region DATAFLOW_REGION \\\u00a0 \u00a0 --staging-location gs://BUCKET_NAME/temp \\\u00a0 \u00a0 --parameters \\inputSubscription=projects/PROJECT_ID/subscriptions/SUBSCRIPTION_ID,\\outputTableSpec=PROJECT_ID:tutorial_dataset.tutorial\n```\nReplace `` with a unique name of your choice.\nOptionally, to run the template with the UDF, use the following command:\n```\ngcloud dataflow jobs run JOB_NAME \\\u00a0 \u00a0 --gcs-location gs://dataflow-templates-DATAFLOW_REGION/latest/PubSub_Subscription_to_BigQuery \\\u00a0 \u00a0 --region DATAFLOW_REGION \\\u00a0 \u00a0 --staging-location gs://BUCKET_NAME/temp \\\u00a0 \u00a0 --parameters \\inputSubscription=projects/PROJECT_ID/subscriptions/SUBSCRIPTION_ID,\\outputTableSpec=PROJECT_ID:tutorial_dataset.tutorial,\\javascriptTextTransformGcsPath=gs://BUCKET_NAME/dataflow_udf_transform.js,\\javascriptTextTransformFunctionName=process\n```\nTo check if the template can forward messages to a dead-letter table, publish some ratings with no URLs or wrong URLs. For example:\n```\ngcloud pubsub topics publish TOPIC_ID \\\u00a0 --message='{\"url\": \"https://beam.apache.org/documentation/sdks/java/\", \"review\": \"positive\"}'\n```## View your resultsView the data written to your BigQuery tables.\n- In the Google Cloud console, go to the **BigQuery** page.  [Go to the BigQuery page](https://console.cloud.google.com/bigquery) \n- In the query editor, run the following query:```\nSELECT * FROM `PROJECT_ID.tutorial_dataset.tutorial`LIMIT 1000\n```It can take up to a minute for data to start appearing in your table.The query returns rows that have been added to your table in the past 24 hours. You can also run queries using standard SQL.If you expect some error records to be written to your dead-letter table, in the query, use the table name `tutorial_error_records` . For example:```\nSELECT * FROM `PROJECT_ID.tutorial_dataset.tutorial_error_records`LIMIT 1000\n```\nCheck the results in BigQuery by running the following query:\n```\nbq query --use_legacy_sql=false 'SELECT * FROM `'\"PROJECT_ID.tutorial_dataset.tutorial\"'`'\n```\nWhile this pipeline is running, you can see new rows appended into the BigQuery table every minute.\nIf you expect some error records to be written to your dead-letter table, in the query, use the table name `tutorial_error_records` . For example:\n```\nSELECT * FROM `PROJECT_ID.tutorial_dataset.tutorial_error_records`LIMIT 1000\n```## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the projectThe easiest way to eliminate billing is to delete the Google Cloud project that you created for the tutorial.\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- Delete a Google Cloud project:\n- ```\ngcloud projects delete PROJECT_ID\n```\n### Delete the individual resourcesIf you want to reuse the project later, you can keep the project but delete the resources that you created during the tutorial.\n- In the Google Cloud console, go to the Dataflow **Jobs** page. [Go to Jobs](https://console.cloud.google.com/dataflow/jobs) \n- Click the job that you want to stop.To stop a job, the status of the job must be **running** .\n- In the job details page, click **Stop** .\n- Click **Cancel** .\n- To confirm your choice, click **Stop Job** .\nTo cancel your Dataflow job, use the [gcloud dataflow jobs](/sdk/gcloud/reference/dataflow/jobs/list) command.\n```\ngcloud dataflow jobs list \\\u00a0 --filter 'NAME=JOB_NAME AND STATE=Running' \\\u00a0 --format 'value(JOB_ID)' \\\u00a0 --region \"DATAFLOW_REGION\" \\\u00a0 | xargs gcloud dataflow jobs cancel --region \"DATAFLOW_REGION\"\n```\n- Delete the Cloud Scheduler jobs.- Go to the **Cloud Scheduler** page in the Google Cloud console. [Go to Cloud Scheduler](https://console.cloud.google.com/cloudscheduler) \n- Select your jobs.\n- Click the **Delete** button at the top of the page and confirm your delete.\n- Delete the Pub/Sub topic and subscription.- Go to the Pub/Sub **Topics** page in the Google Cloud console. [Go to Topics](https://console.cloud.google.com/cloudpubsub/topicList) \n- Select the topic that you created.\n- Click **Delete** to permanently delete the topic.\n- Go to the Pub/Sub **Subscriptions** page in the Google Cloud console. [Go to Subscriptions](https://console.cloud.google.com/cloudpubsub/subscription/list) \n- Select the subscription created with your topic.\n- Click **Delete** to permanently delete the subscription.\n- Delete the BigQuery table and dataset.- In the Google Cloud console, go to the **BigQuery** page. [Go toBigQuery](https://console.cloud.google.com/bigquery) \n- In the **Explorer** panel, expand your project.\n- Next to the dataset you want to delete, click more_vert **View actions** , and then click **delete** .\n- Delete the Cloud Storage bucket.- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets](https://console.cloud.google.com/storage) \n- Select the bucket that you want to delete, click delete **Delete** , and then follow the instructions.- To delete the Cloud Scheduler jobs, use the [gcloud scheduler jobs delete](/sdk/gcloud/reference/scheduler/jobs/delete) command.```\ngcloud scheduler jobs delete negative-ratings-publisher --location=DATAFLOW_REGION\n``````\ngcloud scheduler jobs delete positive-ratings-publisher --location=DATAFLOW_REGION\n```\n- To delete the Pub/Sub subscription and topic, use the [gcloud pubsub subscriptions delete](/sdk/gcloud/reference/pubsub/subscriptions/delete) and the [gcloud pubsub topics delete](/sdk/gcloud/reference/pubsub/topics/delete) commands.```\ngcloud pubsub subscriptions delete SUBSCRIPTION_IDgcloud pubsub topics delete TOPIC_ID\n```\n- To delete the BigQuery table, use the [bq rm](/bigquery/docs/reference/bq-cli-reference) command.```\nbq rm -f -t PROJECT_ID:tutorial_dataset.tutorial\n```\n- Delete the BigQuery dataset. The dataset alone does not incur any charges. **Caution:** The following command also deletes all tables in the dataset. The tables and data cannot be recovered.```\nbq rm -r -f -d PROJECT_ID:tutorial_dataset\n```\n- To delete the Cloud Storage bucket, use the [gcloud storage rm command](/sdk/gcloud/reference/storage/rm) . The bucket alone does not incur any charges. **Caution:** The following command also deletes all objects in the bucket. These objects cannot be recovered.```\ngcloud storage rm gs://BUCKET_NAME --recursive\n```If you keep your project, revoke the roles that you granted to the Compute Engine default service account.- In the Google Cloud console, go to the **IAM** page.\n [ Go to IAM](https://console.cloud.google.com/projectselector/iam-admin/iam?supportedpurview=project,folder,organizationId) - Select a project, folder, or organization.\n- Find the row containing the principal whose access you want to revoke. In that row, click edit **Edit principal** .\n- Click the **Delete** delete button for each role you want to revoke, and then click **Save** .\n- If you keep your project, revoke the roles that you granted to the Compute Engine default service account. Run the following command one time for each of the following IAM roles:- `roles/dataflow.admin`\n- `roles/dataflow.worker`\n- `roles/storage.admin`\n- `roles/pubsub.editor`\n- `roles/bigquery.dataEditor`\n```\n\u00a0 gcloud projects remove-iam-policy-binding <var>PROJECT_ID</var> \\\u00a0 --member=serviceAccount:<var>PROJECT_NUMBER</var>-compute@developer.gserviceaccount.com \\\u00a0 --role=<var>ROLE</var>\n```\n- Optional: Revoke the authentication credentials that you created, and delete the local   credential file.```\ngcloud auth application-default revoke\n```\n- Optional: Revoke credentials from the gcloud CLI.```\ngcloud auth revoke\n```## What's next\n- [Extend your Dataflow template with UDFs](/dataflow/docs/guides/templates/create-template-udf) .\n- Learn more about using [Dataflow templates](/dataflow/docs/guides/templates/overview) .\n- View [all the Google-provided templates](/dataflow/docs/guides/templates/provided-templates) .\n- Read about using Pub/Sub to [create and use topics](/pubsub/docs/create-topic) and how to create a [pull subscription](/pubsub/docs/create-subscription) .\n- Learn more about using [Cloud Scheduler to schedule and run cron jobs](/scheduler/docs/schedule-run-cron-job) .\n- Read about using BigQuery to [create datasets](/bigquery/docs/datasets) .\n- Learn about [Pub/Sub subscriptions](/pubsub/docs/subscriber) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Dataflow"}