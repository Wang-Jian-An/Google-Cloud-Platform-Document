{"title": "Dataflow - Use Eventarc to manage Dataflow jobs", "url": "https://cloud.google.com/dataflow/docs/guides/job-notifications-using-eventarc", "abstract": "# Dataflow - Use Eventarc to manage Dataflow jobs\nThis document describes how to create event-driven workflows triggered by state changes in your Dataflow jobs.\nFor example, your workflow might:\n- Send an alert to an on-call engineer if a critical job fails.\n- Notify users when a batch job completes, or start another Dataflow job.\n- Clean up resources used by a job, such as Cloud Storage buckets.", "content": "## Overview\n[Eventarc](/eventarc/docs) is a Google Cloud service that can listen to events from other services and route them to various destinations.\nWhen you run a Dataflow job, the job transitions through various states, such as `JOB_STATE_QUEUED` , `JOB_STATE_RUNNING` , and `JOB_STATE_DONE` . Dataflow integration with Eventarc lets you trigger an action when a job changes state.\nBecause Eventarc is a managed service, you don't need to provision or manage the underlying infrastructure.\n## Before you beginTo use the Eventarc API, your project must have enough [quota](/eventarc/docs/quotas) . Also, the service account associated with the Eventarc trigger must have the [appropriate permissions](/eventarc/docs/all-roles-permissions) .\n## Choose an event destination\nChoose an [event destination](/eventarc/docs/overview#targets) to receive the event. The destination determines the next step in your workflow.\nFor example:\n- To send an SMS alert, you might use Cloud Functions to create a standalone HTTP trigger.\n- For a more complex workflow, you might use Workflows.\n- If your Dataflow pipeline is part of a larger solution that runs on Google Kubernetes Engine, the trigger can route the event to a GKE service running in your cluster.\nFor more information about this style of architecture, see [Event-driven architectures](/eventarc/docs/event-driven-architectures) in the Eventarc documentation.\n## Create a trigger\nTo create an Eventarc trigger for Dataflow job state changes, refer to one of the following documents:\n- [Route Dataflow events to Cloud Run](/eventarc/docs/run/route-trigger-dataflow) \n- [Route Dataflow events to Google Kubernetes Engine](/eventarc/docs/gke/route-trigger-dataflow) \n- [Route Dataflow events to Workflows](/eventarc/docs/workflows/route-trigger-dataflow) \n- [Create a trigger for Cloud Functions](/eventarc/docs/functions/create-triggers) \nOptionally, you can filter events by Dataflow job ID. For example, you can select job IDs that match a regular expression. For more information, see [Understand path patterns](/eventarc/docs/path-patterns) .\n## Process events\nThe event data describes the Dataflow job at the time the event was triggered. The payload is similar to the [Job](/dataflow/docs/reference/rest/v1b3/projects.jobs#Job) resource type, with the `steps` , `pipeline_description` , and `transform_name_mapping` fields omitted. Also, depending on the job state, some fields might not be present.\nThe following shows an example payload:\n```\n{\u00a0 \"id\":\"2023-04-13_16_28_37-12345678\",\u00a0 \"projectId\":\"my-project\",\u00a0 \"name\":\"job1\",\u00a0 \"currentState\":\"JOB_STATE_QUEUED\",\u00a0 \"currentStateTime\":\"2023-04-13T23:28:37.437622Z\",\u00a0 \"createTime\":\"2023-04-13T23:28:37.437622Z\",\u00a0 \"location\":\"us-central1\",\u00a0 \"startTime\":\"2023-04-13T23:28:37.437622Z\"}\n```\nFor more information about job states, see the following topics:\n- [JobState](/dataflow/docs/reference/rest/v1b3/projects.jobs#jobstate) \n- [Life of a Dataflow job](/dataflow/docs/guides/pipeline-workflows#life-of-a-job) ## What's next\n- [Design Dataflow pipeline workflows](/dataflow/docs/guides/pipeline-workflows) .\n- Read the [Eventarc documentation](/eventarc/docs/overview) .", "guide": "Dataflow"}