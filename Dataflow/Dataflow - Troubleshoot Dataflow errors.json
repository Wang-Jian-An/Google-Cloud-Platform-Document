{"title": "Dataflow - Troubleshoot Dataflow errors", "url": "https://cloud.google.com/dataflow/docs/guides/common-errors", "abstract": "# Dataflow - Troubleshoot Dataflow errors\nIf you run into problems with your Dataflow pipeline or job, this page lists error messages that you might see and provides suggestions for how to fix each error.\nErrors in the log types `dataflow.googleapis.com/worker-startup` , `dataflow.googleapis.com/harness-startup` , and `dataflow.googleapis.com/kubelet` indicate configuration problems with a job. They can also indicate conditions that prevent the normal logging path from functioning.\nYour pipeline might throw exceptions while processing data. Some of these errors are transient, for example when temporary difficulty accessing an external service occurs. Some of these errors are permanent, such as errors caused by corrupt or unparseable input data, or null pointers during computation.\nDataflow processes elements in arbitrary bundles and retries the complete bundle when an error is thrown for any element in that bundle. When running in batch mode, bundles including a failing item are retried four times. The pipeline fails completely when a single bundle fails four times. When running in streaming mode, a bundle including a failing item is retried indefinitely, which might cause your pipeline to permanently stall.\nExceptions in user code, for example, your `DoFn` instances, are reported in the [Dataflow monitoring interface](/dataflow/pipelines/dataflow-monitoring-intf) . If you run your pipeline with `BlockingDataflowPipelineRunner` , you also see error messages printed in your console or terminal window.\nConsider guarding against errors in your code by adding exception handlers. For example, if you want to drop elements that fail some custom input validation done in a `ParDo` , use a try/catch block within your `ParDo` to handle the exception and log and drop the element. For production workloads, implement an unprocessed message pattern. To track the error count, you use [aggregation transforms](https://beam.apache.org/documentation/transforms/java/overview/#aggregation) .\n", "content": "## Missing log files\nIf you don't see any logs for your jobs, remove any exclusion filters containing `resource.type=\"dataflow_step\"` from all of your Cloud Logging **Log Router** sinks.\n[Go to Log Router](https://console.cloud.google.com/logs/router)\nFor more details about removing your logs exclusions, refer to the [Removing exclusions](/logging/docs/exclusions#stopping-resource) guide.\n## Pipeline errors\nThe following sections contain common pipeline errors that you might encounter and steps for resolving or troubleshooting the errors.\n### Some Cloud APIs need to be enabled\nWhen you try to run a Dataflow job, the following error occurs:\n```\nSome Cloud APIs need to be enabled for your project in order for Cloud Dataflow to run this job.\n```\nThis issue occurs because some required APIs are not enabled in your project.\nTo resolve this issue and run a Dataflow job, enable the following Google Cloud APIs in your project:\n- Compute Engine API (Compute Engine)\n- Cloud Logging API\n- Cloud Storage\n- Cloud Storage JSON API\n- BigQuery API\n- Pub/Sub\n- Datastore API\nFor detailed instructions, see the [Getting Started section on enabling Google Cloud APIs](/dataflow/getting-started#APIs) .\n### \"@*\" and \"@N\" are reserved sharding specs\nWhen you try to run a job, the following error appears in the log files, and the job fails:\n```\nWorkflow failed. Causes: \"@*\" and \"@N\" are reserved sharding specs. Filepattern must not contain any of them.\n```\nThis error occurs if the filename for your Cloud Storage path for temporary files ( `tempLocation` or `temp_location` ) has an at sign (@) followed by a number or by an asterisk (*).\nTo resolve this issue, change the filename so that the at sign is followed by a supported character.\n### Bad request\nWhen you run a Dataflow job, [Cloud Monitoring](/dataflow/docs/guides/using-cloud-monitoring) logs display a series of warnings similar to the following:\n```\nUnable to update setup work item STEP_ID error: generic::invalid_argument: Http(400) Bad Request\nUpdate range task returned 'invalid argument'. Assuming lost lease for work with id LEASE_ID\nwith expiration time: TIMESTAMP, now: TIMESTAMP. Full status: generic::invalid_argument: Http(400) Bad Request\n```\nBad request warnings occur if worker state information is stale or out of sync due to processing delays. Often, your Dataflow job succeeds despite the bad request warnings. If that is the case, ignore the warnings.\n### Cannot read and write in different locations\nWhen you run a Dataflow job, you might see the following error in the log files:\n```\nmessage:Cannot read and write in different locations: source: SOURCE_REGION, destination: DESTINATION_REGION,reason:invalid\n```\nThis error occurs when the source and destination are in different regions. It can also occur when the staging location and destination are in different regions. For example, if the job reads from Pub/Sub and then writes to a Cloud Storage `temp` bucket before writing to a BigQuery table, the Cloud Storage `temp` bucket and the BigQuery table must be in the same region.\nMulti-region locations are considered different than single-region locations, even if the single region falls within the scope of the multi-region location. For example, `us (multiple regions in the United States)` and `us-central1` are different regions.\nTo resolve this issue, have your destination, source, and staging locations in the same region. Cloud Storage bucket locations can't be changed, so you might need to create a new Cloud Storage bucket in the correct region.\n### Connection timed out\nWhen you run a Dataflow job, you might see the following error in the log files:\n```\norg.springframework.web.client.ResourceAccessException: I/O error on GET request for CONNECTION_PATH: Connection timed out (Connection timed out); nested exception is java.net.ConnectException: Connection timed out (Connection timed out)\n```\nThis issue occurs when the Dataflow workers fail to establish or maintain a connection with the data source or destination.\nTo resolve the issue, follow these troubleshooting steps:\n- Verify that the data source is running.\n- Verify that the destination is running.\n- Review the [connection parameters](https://beam.apache.org/documentation/pipelines/create-your-pipeline/#reading-data-into-your-pipeline) used in the Dataflow pipeline configuration.\n- Verify that performance issues aren't affecting the source or destination.\n- Make sure that [firewall rules](/dataflow/docs/guides/routes-firewall) aren't blocking the connection.\n### No such object\nWhen you run your Dataflow jobs, you might see the following error in the log files:\n```\n..., 'server': 'UploadServer', 'status': '404'}>, <content <No such object:...\n```\nThese errors typically occur when some of your running Dataflow jobs use the same `temp_location` to stage temporary job files created when the pipeline runs. When multiple concurrent jobs share the same `temp_location` , these jobs might step on the temporary data of each other, and a race condition might occur. To avoid this issue, it's recommended that you use a unique `temp_location` for each job.\n### Dataflow is unable to determine backlog\nWhen running a streaming pipeline from Pub/Sub, the following warning occurs:\n```\nDataflow is unable to determine the backlog for Pub/Sub subscription\n```\nWhen a Dataflow pipeline pulls data from Pub/Sub, Dataflow needs to repeatedly request information from Pub/Sub. This information includes the amount of backlog on the subscription and the age of the oldest unacknowledged message. Occasionally, Dataflow is unable to retrieve this information from Pub/Sub because of internal system issues, which may cause a transient accumulation of backlog.\nFor more information, see [Streaming With Cloud Pub/Sub](/dataflow/docs/concepts/streaming-with-cloud-pubsub) .\n### DEADLINE_EXCEEDED or Server Unresponsive\nWhen you run your jobs, you might encounter RPC timeout exceptions or one of the following errors:\n```\nDEADLINE_EXCEEDED\n```\nOr:\n```\nServer Unresponsive\n```\nThese errors typically occur for one of the following reasons:\n- **The Virtual Private Cloud (VPC) network used for your job might be missing afirewall rule** . The firewall rule needs to enable all TCP traffic among VMs in the VPC network you specified in your pipeline options. For more information, see [Firewall rules for Dataflow](/dataflow/docs/guides/routes-firewall#firewall_rules) .In some cases, the workers aren't able to communicate with each other. When you run a Dataflow job that doesn't use Dataflow Shuffle or Streaming Engine, workers need to communicate with each other using TCP ports `12345` and `12346` within the VPC network. In this scenario, the error includes the worker harness name and the TCP port that's blocked. The error looks like one of the following examples:```\nDEADLINE_EXCEEDED: (g)RPC timed out when SOURCE_WORKER_HARNESS\ntalking to DESTINATION_WORKER_HARNESS:12346.\n``````\nRpc to WORKER_HARNESS:12345 completed with error UNAVAILABLE: failed to connect to all addresses\nServer unresponsive (ping error: Deadline Exceeded, UNKNOWN: Deadline Exceeded...)\n```To resolve this issue, use the `gcloud compute firewall-rules create` [rules](/sdk/gcloud/reference/compute/firewall-rules/create#--rules) flag to allow network traffic to ports `12345` and `12346` . The following example demonstrates the Google Cloud CLI command:```\ngcloud compute firewall-rules create FIREWALL_RULE_NAME \\\n --network NETWORK \\\n --action allow \\\n --direction IN \\\n --target-tags dataflow \\\n --source-tags dataflow \\\n --priority 0 \\\n --rules tcp:12345-12346\n```Replace the following:- ``: the name of your firewall rule\n- ``: the name of your network\n- **Your job is shuffle-bound** .To resolve this issue, make one or more of the following changes.\n- If the job is not using the service-based shuffle, switch to using the service-based Dataflow Shuffle by setting`--experiments=shuffle_mode=service`. For details and availability, see [Dataflow Shuffle](/dataflow/docs/shuffle-for-batch) .\n- . Try setting`--numWorkers`with a higher value when you run your pipeline.\n- Try setting`--diskSizeGb`with a higher value when you run your pipeline.\n- . Try setting`--workerDiskType=\"compute.googleapis.com/projects/` `` `/zones/` `` `/diskTypes/pd-ssd\"`when you run your pipeline.\n- If the job is not using the service-based shuffle, switch to using the service-based Dataflow Shuffle by setting`--experiments=shuffle_mode=service`. For details and availability, see [Dataflow Shuffle](/dataflow/docs/shuffle-for-batch) .\n- . Try setting`--num_workers`with a higher value when you run your pipeline.\n- Try setting`--disk_size_gb`with a higher value when you run your pipeline.\n- . Try setting`--worker_disk_type=\"compute.googleapis.com/projects/` `` `/zones/` `` `/diskTypes/pd-ssd\"`when you run your pipeline.\n- If the job is not using the service-based shuffle, switch to using the service-based Dataflow Shuffle by setting`--experiments=shuffle_mode=service`. For details and availability, see [Dataflow Shuffle](/dataflow/docs/shuffle-for-batch) .\n- . Try setting`--num_workers`with a higher value when you run your pipeline.\n- Try setting`--disk_size_gb`with a higher value when you run your pipeline.\n- . Try setting`--disk_type=\"compute.googleapis.com/projects/` `` `/zones/` `` `/diskTypes/pd-ssd\"`when you run your pipeline.### Encoding errors, IOExceptions, or unexpected behavior in user code\nThe Apache Beam SDKs and the Dataflow workers depend on common third-party components. These components import additional dependencies. Version collisions can result in unexpected behavior in the service. Also, some libraries aren't forward-compatible. You might need to pin to the listed versions that are in scope during execution. [SDK and Worker Dependencies](/dataflow/docs/concepts/sdk-worker-dependencies) contains a list of dependencies and their required versions.\n### Error running LookupEffectiveGuestPolicies\nWhen you run a Dataflow job, you might see the following error in the log files:\n```\nOSConfigAgent Error policies.go:49: Error running LookupEffectiveGuestPolicies:\nerror calling LookupEffectiveGuestPolicies: code: \"Unauthenticated\",\nmessage: \"Request is missing required authentication credential.\nExpected OAuth 2 access token, login cookie or other valid authentication credential.\n```\nThis error occurs if [OS configuration management](/compute/docs/os-configuration-management) is enabled for the entire project.\nTo resolve this issue, disable [VM Manager](/compute/docs/vm-manager) policies that apply to the entire project. If disabling VM Manager policies for the entire project isn't possible, you can safely ignore this error and filter it out of log monitoring tools.\n### Exhausted resource pool\nWhen you create a Google Cloud resource, you might see the following error for an exhausted resource pool:\n```\nERROR: ZONE_RESOURCE_POOL_EXHAUSTED\n```\nThis error occurs for temporary stock-out conditions for a specific resource in a specific zone.\nTo resolve the issue, you can either wait or create the same resource in another zone. As a best practice, we recommend that you distribute your resources across [multiple zones and regions](/compute/docs/regions-zones#choosing_a_region_and_zone) to tolerate outages.\n### A fatal error has been detected by the Java Runtime Environment\nThe following error occurs during worker startup:\n```\nA fatal error has been detected by the Java Runtime Environment\n```\nThis error occurs if the pipeline is using Java Native Interface (JNI) to run non-Java code and that code or the JNI bindings contain an error.\n### googclient_deliveryattempt attribute key error\nYour Dataflow job fails with one of the following errors:\n```\nThe request contains an attribute key that is not valid (key=googclient_deliveryattempt). Attribute keys must be non-empty and must not begin with 'goog' (case-insensitive).\n```\nOr:\n```\nInvalid extensions name: googclient_deliveryattempt\n```\nThis error occurs when your Dataflow job has the following characteristics:\n- The Dataflow job uses Streaming Engine.\n- The pipeline has a Pub/Sub sink.\n- The pipeline uses a [pull subscription](/pubsub/docs/pull) .\n- The pipeline uses one of the [Pub/Sub service APIs](/pubsub/docs/pull#service_apis) to publish messages instead of using the built-in Pub/Sub I/O sink.\n- Pub/Sub is using the Java or C# [client library](/pubsub/docs/pull#client_libraries) .\n- The Pub/Sub subscription has a [dead-letter topic](/pubsub/docs/handling-failures#dead_letter_topic) .\nThis error occurs because when you use the Pub/Sub Java or C# client library and a dead-letter topic for a subscription is enabled, the delivery attempts are in the `googclient_deliveryattempt` message attribute instead of the `delivery_attempt` field. For more information, see [Track delivery attempts](/pubsub/docs/handling-failures#track-delivery-attempts) in the \"Handle message failures\" page.\nTo workaround this issue, make one or more of the following changes.\n- [Disable Streaming Engine](/dataflow/docs/streaming-engine#use) .\n- Use the built-in [Apache Beam PubSubIO connector](https://beam.apache.org/documentation/io/connectors/) instead of the Pub/Sub service API.\n- Use a different [type of Pub/Sub subscription](/pubsub/docs/subscription-overview#push_pull) .\n- [Remove the dead-letter topic](/pubsub/docs/handling-failures#removing_a_dead_letter_topic) .\n- Don't use the Java or C# client library with your Pub/Sub pull subscription. For other options, see [Client library code samples](/pubsub/docs/pull#client_library_code_samples) .\n- In your pipeline code, when attribute keys start with`goog`, erase the message attributes before publishing the messages.\n### A hot key ... was detected\nThe following error occurs:\n```\nA hot key HOT_KEY_NAME was detected in...\n```\nThese errors occur if your data contains a hot key. A hot key is a key with enough elements to negatively affect pipeline performance. These keys limit the ability of Dataflow to process elements in parallel, which increases execution time.\nTo print the human-readable key to the logs when a hot key is detected in the pipeline, use the [hot key pipeline option](/dataflow/docs/reference/pipeline-options#debugging) .\nTo resolve this issue, check that your data is evenly distributed. If a key has disproportionately many values, consider the following courses of action:\n- Rekey your data. Apply a [ParDo](https://beam.apache.org/documentation/programming-guide/#pardo) transform to output new key-value pairs.\n- For Java jobs, use the [Combine.PerKey.withHotKeyFanout](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/transforms/Combine.PerKey.html) transform.\n- For Python jobs, use the [CombinePerKey.with_hot_key_fanout](https://beam.apache.org/releases/pydoc/current/apache_beam.transforms.core.html#apache_beam.transforms.core.CombinePerKey.with_hot_key_fanout) transform.\n- Enable [Dataflow Shuffle](/dataflow/docs/shuffle-for-batch) .\nTo view hot keys in the Dataflow monitoring interface, see [Troubleshoot stragglers in batch jobs](/dataflow/docs/guides/troubleshoot-batch-stragglers) .\n### Invalid table specification in Data Catalog\nWhen you use Dataflow SQL to create Dataflow SQL jobs, your job might fail with the following error in the log files:\n```\nInvalid table specification in Data Catalog: Could not resolve table in Data Catalog\n```\nThis error occurs if the Dataflow service account doesn't have access to the Data Catalog API.\nTo resolve this issue, [enable the Data Catalog API](/apis/docs/enable-disable-apis#enabling_apis) in the Google Cloud [project](/resource-manager/docs/cloud-platform-resource-hierarchy#projects) that you're using to write and run queries.\nAlternately, assign the `roles/datacatalog.viewer` role to the [Dataflow service account](/dataflow/docs/concepts/security-and-permissions#security_and_permissions_for_pipelines_on) .\n### The job graph is too large\nYour job might fail with the following error:\n```\nThe job graph is too large. Please try again with a smaller job graph,\nor split your job into two or more smaller jobs.\n```\nThis error occurs if the graph size of your job exceeds 10\u00a0MB. Certain conditions in your pipeline can cause the job graph to exceed the limit. Common conditions include:\n- A`Create`transform that includes a large amount of in-memory data.\n- A large`DoFn`instance that is serialized for transmission to remote workers.\n- A`DoFn`as an anonymous inner class instance that (possibly inadvertently) pulls in a large amount of data to be serialized.\n- A directed acyclic graph (DAG) is being used as part of a programmatic loop that is enumerating a large list.\nTo avoid these conditions, consider restructuring your pipeline.\n### Key Commit Too Large\nWhen running a streaming job, the following error appears in the worker log files:\n```\nKeyCommitTooLargeException\n```\nThis error occurs in streaming scenarios if a very large amount of data is grouped without using a `Combine` transform, or if a large amount of data is produced from a single input element.\nTo reduce the possibility of encountering this error, use the following strategies:\n- Ensure that processing a single element cannot result in outputs or state modifications exceeding the limit.\n- If multiple elements were grouped by a key, consider increasing the key space to reduce the elements grouped per key.\n- If elements for a key are emitted at a high frequency over a short time, that might result in many GB of events for that key in windows. Rewrite the pipeline to detect keys like this and only emit an output indicating the key was frequently present in that window.\n- Use sublinear space`Combine`transforms for commutative and associate operations. Don't use a combiner if it doesn't reduce space. For example, combiner for strings that just appends strings together is worse than not using combiner.\n### Rejecting message over 7168K\nWhen you run a Dataflow job created from a template, the job might fail with the following error:\n```\nError: CommitWork failed: status: APPLICATION_ERROR(3): Pubsub publish requests are limited to 10MB, rejecting message over 7168K (size MESSAGE_SIZE) to avoid exceeding limit with byte64 request encoding.\n```\nThis error occurs when messages written to a dead-letter queue exceed the size limit of 7168\u00a0K. As a workaround, enable [Streaming Engine](/dataflow/docs/streaming-engine) , which has a higher size limit. To enable Streaming Engine, use the following [pipeline option](/dataflow/docs/reference/pipeline-options#streaming_pipeline_management) .\n`--enableStreamingEngine=true`\n`--enable_streaming_engine=true`\n### Request Entity Too Large\nWhen you submit your job, one of the following errors appears in your console or terminal window:\n```\n413 Request Entity Too Large\nThe size of serialized JSON representation of the pipeline exceeds the allowable limit\nFailed to create a workflow job: Invalid JSON payload received\nFailed to create a workflow job: Request payload exceeds the allowable limit\n```\nWhen you encounter an error about the JSON payload when submitting your job, the JSON representation of your pipeline exceeds the maximum 20\u00a0MB request size.\nThe size of your job is tied to the JSON representation of the pipeline. A larger pipeline means a larger request. Dataflow has a limitation that caps requests at 20\u00a0MB.\nTo estimate the size of the JSON request of your pipeline, run your pipeline with the following option:\n`--dataflowJobFile=` ``\n`--dataflow_job_file=` ``\nOutputting your job as JSON is not supported in Go.\nThis command writes a JSON representation of your job to a file. The size of the serialized file is a good estimate of the size of the request. The actual size is slightly larger due to some additional information included the request.\nCertain conditions in your pipeline can cause the JSON representation to exceed the limit. Common conditions include:\n- A`Create`transform that includes a large amount of in-memory data.\n- A large`DoFn`instance that is serialized for transmission to remote workers.\n- A`DoFn`as an anonymous inner class instance that (possibly inadvertently) pulls in a large amount of data to be serialized.\nTo avoid these conditions, consider restructuring your pipeline.\n### SDK pipeline options or staging file list exceeds size limit\nWhen running a pipeline, one of the following errors occurs:\n```\nSDK pipeline options or staging file list exceeds size limit.\nPlease keep their length under 256K Bytes each and 512K Bytes in total.\n```\nOr:\n```\nValue for field 'resource.properties.metadata' is too large: maximum size\n```\nThese errors occur if the pipeline couldn't be started due to Compute Engine metadata limits being exceeded. These limits can't be changed. Dataflow uses Compute Engine metadata for pipeline options. The limit is documented in the Compute Engine custom metadata [limitations](/compute/docs/metadata/setting-custom-metadata#limitations) .\nThe following scenarios can cause the JSON representation to exceed the limit:\n- There are too many JAR files to stage.\n- The`sdkPipelineOptions`request field is too large.\nTo estimate the size of the JSON request of your pipeline, run your pipeline with the following option:\n`--dataflowJobFile=` ``\n`--dataflow_job_file=` ``\nOutputting your job as JSON is not supported in Go.\nThe size of the output file from this command must be less than 256\u00a0KB. The 512\u00a0KB in the error message refers to the total size of the output file and the custom metadata options for the Compute Engine VM instance.\nYou can get a rough estimate of the custom metadata option for VM instance from running Dataflow jobs in the project. Choose any running Dataflow job. Take a VM instance, and then navigate to the Compute Engine VM instance details page for that VM to check for the custom metadata section. The total length of the custom metadata and the file should be less than 512\u00a0KB. An accurate estimate for the failed job is not possible, because the VMs are not spun up for failed jobs.\nIf your JAR list is hitting the 256-KB limit, review it and reduce any unnecessary JAR files. If it's still too large, try running the Dataflow job by using an uber JAR. For an example that demonstrates how to create and use uber JAR, see [Build and deploy an Uber JAR](/functions/docs/concepts/java-deploy#build_and_deploy_an_uber_jar) .\nIf the `sdkPipelineOptions` request field is too large, include the following option when you run your pipeline. The pipeline option is the same for Java, Python, and Go.\n```\n--experiments=no_display_data_on_gce_metadata\n```\n### Shuffle key too large\nThe following error appears in the worker log files:\n```\nShuffle key too large\n```\nThis error occurs if the serialized key emitted to a particular (Co-)GroupByKey is too large after the corresponding coder is applied. Dataflow has a limit for serialized shuffle keys.\nTo resolve this issue, reduce the size of the keys or use more space-efficient coders.\n### Total number of BoundedSource objects ... is larger than the allowable limit\nOne of the following errors might occur when running jobs with Java:\n```\nTotal number of BoundedSource objects generated by splitIntoBundles() operation is larger than the allowable limit\n```\nOr:\n```\nTotal size of the BoundedSource objects generated by splitIntoBundles() operation is larger than the allowable limit\n```\nThis error might occur if you're reading from a very large number of files by using `TextIO` , `AvroIO` , `BigQueryIO` through EXPORT, or some other file-based source. The particular limit depends on the details of your source, but it is on the order of tens of thousands of files in one pipeline. For example, embedding schema in `AvroIO.Read` allows fewer files.\nThis error might also occur if you created a custom data source for your pipeline and the `splitIntoBundles` method of your source returned a list of `BoundedSource` objects which takes up more than 20\u00a0MB when serialized.\nThe allowable limit for the total size of the `BoundedSource` objects generated by the `splitIntoBundles()` operation of your custom source is 20\u00a0MB.\nTo work around this limitation, make one of the following changes:- Enable [Runner V2](/dataflow/docs/runner-v2) . Runner v2 converts sources into splittable DoFns that don't have this source split limit.\n- Modify your custom `BoundedSource` subclass so that the total size of the generated `BoundedSource` objects is smaller han the 20-MB limit. For example, your source might generate fewer splits initially, and rely on [Dynamic Work Rebalancing](/dataflow/docs/dynamic-work-rebalancing) to further split inputs on demand.\n### NameError\nWhen you execute your pipeline using the Dataflow service, the following error occurs:\n```\nNameError\n```\nThis error does not occur when you execute locally, such as when you execute using the `DirectRunner` .\nThis error occurs if your `DoFn` s are using values in the global namespace that are not available on the Dataflow worker.\nBy default, global imports, functions, and variables defined in the main session are not saved during the serialization of a Dataflow job.\nTo resolve this issue, use one of the following methods. If your `DoFn` s are defined in the main file and reference imports and functions in the global namespace, set the `--save_main_session` pipeline option to `True` . This change pickles the state of the global namespace to and loads it on the Dataflow worker.\nIf you have objects in your global namespace that can't be pickled, a pickling error occurs. If the error is regarding a module that should be available in the Python distribution, import the module locally, where it's used.\nFor example, instead of:\n```\nimport re\n\u2026\ndef myfunc():\n # use re module\n```\nuse:\n```\ndef myfunc():\n import re\n # use re module\n```\nAlternatively, if your `DoFn` s span multiple files, use a different approach to packaging your workflow and [managing dependencies](https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/) .\n### Processing stuck or operation ongoing\nIf Dataflow spends more time executing a `DoFn` than the time specified in without returning, the following message is displayed.\nEither of the two following log messages, depending on the version:\n`Processing stuck in step` `` `for at least` ``\n`Operation ongoing in bundle` `` `for at least` `` `without outputting or completing: at` ```Operation ongoing for over` `` `in state` `` `in step` `` `without returning. Current Traceback:` ```Operation ongoing in transform` `` `for at least` `` `without outputting or completing in state` ``\nThis behavior has two possible causes:\n- Your`DoFn`code is slow, or waiting for some slow external operation to complete.\n- Your`DoFn`code might be stuck, deadlocked, or abnormally slow to finish processing.\nTo determine which is the case, expand the [Cloud Monitoring](/dataflow/docs/guides/using-cloud-monitoring) log entry to see a stack trace. Look for messages that indicate that the `DoFn` code is stuck or otherwise encountering issues. If no messages are present, the issue might be the execution speed of the `DoFn` code. Consider using [Cloud Profiler](/dataflow/docs/guides/profiling-a-pipeline) or other tool to investigate the performance of your code.\nIf your pipeline is built on the Java VM (using either Java or Scala), you can investigate the cause of your stuck code. Take a full thread dump of the whole JVM (not just the stuck thread) by following these steps:\n- Make note of the worker name from the log entry.\n- In the Compute Engine section of the Google Cloud console, find the Compute Engine instance with the worker name you noted.\n- Use SSH to connect to the instance with that name.\n- Run the following command:```\ncurl http://localhost:8081/threadz\n```\n### Pub/Sub quota errors\nWhen running a streaming pipeline from Pub/Sub, the following errors occur:\n```\n429 (rateLimitExceeded)\n```\nOr:\n```\nRequest was throttled due to user QPS limit being reached\n```\nThese errors occur if your project has insufficient [Pub/Sub quota](/pubsub/quotas) .\nTo find out if your project has insufficient quota, follow these steps to check for client errors:\n- Go to the [Google Cloud console](https://console.cloud.google.com/) .\n- In the menu on the left, select **APIs & services** .\n- In the **Search Box** , search for **Cloud Pub/Sub** .\n- Click the **Usage** tab.\n- Check **Response Codes** and look for`(4xx)`client error codes.\n### Request is prohibited by organization's policy\nWhen running a pipeline, the following error occurs:\n```\nError trying to get gs://BUCKET_NAME/FOLDER/FILE:\n{\"code\":403,\"errors\":[{\"domain\":\"global\",\"message\":\"Request is prohibited by organization's policy\",\"reason\":\"forbidden\"}],\n\"message\":\"Request is prohibited by organization's policy\"}\n```\nThis error occurs if the Cloud Storage bucket is outside of your [service perimeter](/vpc-service-controls/docs/overview) .\nTo resolve this issue, create an [egress rule](/vpc-service-controls/docs/ingress-egress-rules) that allows access to the bucket outside of the service perimeter.\n### Staged package...is inaccessible\nJobs that used to succeed might fail with the following error:\n```\nStaged package...is inaccessible\n```\nTo resolve this issue:\n- Verify that the Cloud Storage bucket used for staging does not have [TTL settings](/storage/docs/lifecycle) that cause staged packages to be deleted.\n- Verify that the worker service account of your Dataflow project has the permission to access the Cloud Storage bucket used for staging. Gaps in permission can be due to any of the following reasons:- The Cloud Storage bucket used for staging is present in a different project.\n- The Cloud Storage bucket used for staging was migrated from fine-grained access to [uniform bucket-level access](/storage/docs/uniform-bucket-level-access) . Due to the inconsistency between IAM and ACL policies, migrating the staging bucket to uniform bucket-level access disallows ACLs for Cloud Storage resources. ACLs include the permissions held by the worker service account of your Dataflow project over the staging bucket.For more information, see [Accessing Cloud Storage buckets across Google Cloud projects](/dataflow/docs/concepts/security-and-permissions#accessing_buckets_across_projects) .\n### A work item failed 4 times\nThe following error occurs when a job fails:\n```\na work item failed 4 times\n```\nThis error occurs if a single operation causes the worker code to fail four times. Dataflow fails the job, and this message is displayed.\nYou can't configure this failure threshold. For more details, refer to [pipeline error and exception handling](/dataflow/docs/pipeline-lifecycle#error_and_exception_handling) .\nTo resolve this issue, look in the [Cloud Monitoring](/dataflow/docs/guides/using-cloud-monitoring) logs of the job for the four individual failures. Look for **Error-level** or **Fatal-level** log entries in the worker logs that show exceptions or errors. The exception or error should appear at least four times. If the logs only contain generic timeout errors related to accessing external resources, such as MongoDB, verify that the worker service account has permission to access the subnetwork of the resource.\n### Timeout in Polling Result File\nThe following occurs when a job fails:\n```\nTimeout in polling result file: PATH. Possible causes are:\n1. Your launch takes too long time to finish. Please check the logs on stackdriver.\n2. Service account SERVICE_ACCOUNT may not have enough permissions to pull\ncontainer image IMAGE_PATH or create new objects in PATH.\n3. Transient errors occurred, please try again.\n```\nThe issue is often related to how the Python dependencies are being installed by using the `requirements.txt` file. The Apache Beam stager downloads the source of all dependencies from PyPi, including the sources of transitive dependencies. Then, the `wheel` compilation happens implicitly during the `pip` download command for some of the Python packages that are dependencies of `apache-beam` . A timeout issue might occur because of the `requirements.txt` file.\nFor more information, see [the Apache Arrow team's bug tracking this issue](https://github.com/apache/arrow/issues/25105) . The [suggested workaround](/dataflow/docs/guides/troubleshoot-templates#python-requirements) is to install `apache-beam` directly in the Dockerfile. This way, the timeout for the `requirements.txt` file is not applied.\n## Archive job errors\nThe following sections contain common errors that you might encounter when you try to [archive a Dataflow job](/dataflow/docs/guides/stopping-a-pipeline#archive) by using the API.\n### No value is provided\nWhen you try to archive a Dataflow job by using the API, the following error might occur:\n```\nThe field mask specifies an update for the field job_metadata.user_display_properties.archived in job JOB_ID, but no value is provided. To update a field, please provide a field for the respective value.\n```\nThis error occurs for one of the following reasons:\n- The path specified for the `updateMask` field doesn't follow the correct format. This issue can occur due to typos.\n- The [JobMetadata](/dataflow/docs/reference/rest/v1b3/projects.jobs#jobmetadata) isn't correctly specified. In the `JobMetadata` field, for `userDisplayProperties` , use the key-value pair `\"archived\":\"true\"` .\nTo resolve this error, verify that the command that you pass to the API matches the required format. For more details, see [Archive a job](/dataflow/docs/guides/stopping-a-pipeline#archive-jobs) .\n### The API does not recognize the value\nWhen you try to archive a Dataflow job by using the API, the following error might occur:\n```\nThe API does not recognize the value VALUE for the field job_metadata.user_display_properties.archived for job JOB_ID. REASON: Archived display property can only be set to 'true' or 'false'\n```\nThis error occurs when the value provided in the archive jobs key-value pair isn't a supported value. The supported values for the archive jobs key-value pair are `\"archived\":\"true\"` and `\"archived\":\"false\"` .\nTo resolve this error, verify that the command that you pass to the API matches the required format. For more details, see [Archive a job](/dataflow/docs/guides/stopping-a-pipeline#archive-jobs) .\n### Cannot update both state and mask\nWhen you try to archive a Dataflow job by using the API, the following error might occur:\n```\nCannot update both state and mask.\n```\nThis error occurs when you try to update both the [job state](/dataflow/docs/reference/rpc/google.dataflow.v1beta3#google.dataflow.v1beta3.JobState) and the archive status in the same API call. You can't make updates to both the job state and the [updateMask](/dataflow/docs/reference/rest/v1b3/projects.jobs/update#query-parameters) query parameter in the same API call.\nTo resolve this error, update the job state in a separate API call. Make updates to the job state before updating the job archive status.\n### Workflow modification failed\nWhen you try to archive a Dataflow job by using the API, the following error might occur:\n```\nWorkflow modification failed.\n```\nThis error usually occurs when you try to archive a job that is running.\nTo resolve this error, wait until the job completes before archiving it. Completed jobs have one of the following [job states](/dataflow/docs/reference/rpc/google.dataflow.v1beta3#google.dataflow.v1beta3.JobState) :\n- `JOB_STATE_CANCELLED`\n- `JOB_STATE_DRAINED`\n- `JOB_STATE_DONE`\n- `JOB_STATE_FAILED`\n- `JOB_STATE_UPDATED`\nFor more information, see [Detect Dataflow job completion](/dataflow/docs/guides/stopping-a-pipeline#job-completion) .\n## Container image errors\nThe following sections contain common errors that you might encounter when using custom containers and steps for resolving or troubleshooting the errors. The errors are typically prefixed with the following message:\n```\nUnable to pull container image due to error: DETAILED_ERROR_MESSAGE\n```\n### Permission \"containeranalysis.occurrences.list\" denied\nThe following error appears in your log files:\n```\nError getting old patchz discovery occurrences: generic::permission_denied: permission \"containeranalysis.occurrences.list\" denied for project \"PROJECT_ID\", entity ID \"\" [region=\"REGION\" projectNum=PROJECT_NUMBER projectID=\"PROJECT_ID\"]\n```\nthe Container Analysis API is required for vulnerability scanning.\n**Note:** This API is automatically enabled by the [Container Scanning](/artifact-analysis/docs/enable-container-scanning) API.\nFor more information, see [OS scanning overview](/artifact-analysis/docs/os-overview) and [Configuring access control](/artifact-analysis/docs/ca-access-control) in the Artifact Analysis documentation.\n### Error syncing pod ... failed to \"StartContainer\"\nThe following error occurs during worker startup:\n```\nError syncing pod POD_ID, skipping: [failed to \"StartContainer\" for CONTAINER_NAME with CrashLoopBackOff: \"back-off 5m0s restarting failed container=CONTAINER_NAME pod=POD_NAME].\n```\nA pod is a colocated group of Docker containers running on a Dataflow worker. This error occurs when one of the Docker containers in the pod fails to start. If the failure is not recoverable, the Dataflow worker isn't able to start, and Dataflow batch jobs eventually fail with errors like the following:\n```\nThe Dataflow job appears to be stuck because no worker activity has been seen in the last 1h.\n```\nThis error typically occurs when one of the containers is continuously crashing during startup.\n**Note:** If you see a `Dataflow job appears to be stuck` error when a single worker is repeatedly started and then stopped after a few minutes, the issue is likely a networking issue. For more information, see [Single worker is repeatedly started and stopped](/dataflow/docs/guides/troubleshoot-networking#one-worker-repeatedly-stopped) .\nTo understand the root cause, look for the logs captured immediately prior to the failure. To analyze the logs, use the [Logs Explorer](/logging/docs/view/logs-explorer-interface) . In the Logs Explorer, limit the log files to log entries emitted from the worker with container startup errors. To limit the log entries, complete the following steps:\n- In the Logs Explorer, find the`Error syncing pod`log entry.\n- To see the labels associated with the log entry, expand the log entry.\n- Click the label associated with the`resource_name`, and then click **Show\nmatching entries** .In the Logs Explorer, the Dataflow logs are organized into several log streams. The `Error syncing pod` message is emitted in the log named `kubelet` . However, the logs from the failing container could be in a different log stream. Each container has a name. Use the following table to determine which log stream might contain logs relevant to the failing container.\n| Container name      | Log names    |\n|:--------------------------------------|:-------------------------|\n| sdk, sdk0, sdk1, sdk-0-0, and similar | docker     |\n| harness        | harness, harness-startup |\n| python, java-batch, java-streaming | worker-startup, worker |\n| artifact        | artifact     |\nWhen you query the Logs Explorer, make sure that the query either includes the relevant log names [in the query builder interface](/logging/docs/view/building-queries#query-builder-menus) or does not have restrictions on the log name.\nAfter you select the relevant logs, the query result might look like the following example:\n```\nresource.type=\"dataflow_step\"\nresource.labels.job_id=\"2022-06-29_08_02_54-JOB_ID\"\nlabels.\"compute.googleapis.com/resource_name\"=\"testpipeline-jenkins-0629-DATE-cyhg-harness-8crw\"\nlogName=(\"projects/apache-beam-testing/logs/dataflow.googleapis.com%2Fdocker\"\nOR\n\"projects/apache-beam-testing/logs/dataflow.googleapis.com%2Fworker-startup\"\nOR\n\"projects/apache-beam-testing/logs/dataflow.googleapis.com%2Fworker\")\n```\nBecause the logs reporting the symptom of the container failure are sometimes reported as `INFO` , include `INFO` logs in your analysis.\nTypical causes of container failures include the following:\n- Your Python pipeline has additional dependencies that are installed at runtime, and the installation is unsuccessful. You might see errors like`pip install failed with error`. This issue might occur due to conflicting requirements, or due to a restricted networking configuration that prevents a Dataflow worker from pulling an external dependency from a public repository over the internet.\n- A worker fails in the middle of the pipeline run due to an out of memory error. You might see an error like one of the following:- `java.lang.OutOfMemoryError: Java heap space`\n- `Shutting down JVM after 8 consecutive periods of measured GC thrashing. Memory is used/total/max = 24453/42043/42043 MB, GC last/max = 58.97/99.89 %, #pushbacks=82, gc thrashing=true. Heap dump not written.`\nTo debug an out of memory issue, see [Troubleshoot Dataflow out of memory errors](/dataflow/docs/guides/troubleshoot-oom) .\n- Dataflow is unable to pull the container image. For more information, see [Image pull request failed with error](#error-pulling-container-image) .\n- The container used is not compatible with the worker VM's CPU architecture. In the harness startup logs, you might see an error like the following: `exec /opt/apache/beam/boot: exec format error` . To check the container image's architecture, run `docker image inspect $IMAGE:$TAG` and look for the `Architecture` key word. If it says `Error: No such image: $IMAGE:$TAG` , you might need to pull the image first by running `docker pull $IMAGE:$TAG` . For information on building multi-architecture images, see [Build a multi-architecture container image](/dataflow/docs/guides/multi-architecture-container) .\nAfter you identify the error causing the container to fail, try to address the error, and then resubmit the pipeline.\n### Image pull request failed with error\nDuring worker startup, one of the following errors appears in the worker or job logs:\n```\nImage pull request failed with error\n```\n```\npull access denied for IMAGE_NAME\n```\n```\nmanifest for IMAGE_NAME not found: manifest unknown: Failed to fetch\n```\n```\nGet IMAGE_NAME: Service Unavailable\n```\nThese errors occur if a worker is unable to start up because the worker can't pull a Docker container image. This issue happens in the following scenarios:\n- The custom SDK container image URL is incorrect\n- The worker lacks credential or network access to the remote image\nTo resolve this issue:\n- If you're using a custom container image with your job, verify that your image URL is correct and has a valid tag or digest. The Dataflow workers also need access to the image.\n- Verify that public images can be pulled locally by running`docker pull $image`from an unauthenticated machine.\nFor private images or private workers:\n- If you're using Container Registry to host your container image, it is recommended that you use Artifact Registry instead. Effective May 15, 2023, Container Registry is deprecated. If you use Container Registry, you can [transition to Artifact Registry](/artifact-registry/docs/transition/transition-from-gcr) . If your images are in a different project than the one used to run your Google Cloud job, [configure access control](/container-registry/docs/access-control) for the default Google Cloud service account.\n- If using shared Virtual Private Cloud (VPC), make sure that workers [can access](/dataflow/docs/guides/routes-firewall) the custom container repository host.\n- Use`ssh`to connect with a running job worker VM and run`docker pull $image`to directly confirm that the worker is configured properly.\nIf workers fail several times in a row due to this error and work has started on a job, the job can fail with an error similar to the following message:\n```\nJob appears to be stuck.\n```\nIf you remove access to the image while the job is running, either by removing the image itself or revoking the Dataflow worker Service Account Credentials or internet access to access images, Dataflow only logs errors. Dataflow doesn't fail the job. Dataflow also avoids failing long-running streaming pipelines to avoid losing pipeline state.\nOther possible errors can arise from repository quota issues or outages. If you experience issues exceeding the [Docker Hub quota](https://docs.docker.com/docker-hub/download-rate-limit/) for pulling public images or general third-party repository outages, consider using [Artifact Registry](/artifact-registry/docs/overview) as the image repository.\n### SystemError: unknown opcode\nYour Python custom container pipeline might fail with the following error immediately after job submission:\n```\nSystemError: unknown opcode\n```\nIn addition, the stack trace might include\n```\napache_beam/internal/pickler.py\n```\nTo resolve this issue, verify that the Python version that you're using locally matches the version in the container image up to the major and minor version. The difference in the patch version, such as 3.6.7 versus 3.6.8, does not create compatibility issues. The difference in minor version, such as 3.6.8 versus 3.8.2, can cause pipeline failures.\n## Worker errors\nThe following sections contain common worker errors that you might encounter and steps for resolving or troubleshooting the errors.\n### Call from Java worker harness to Python DoFn fails with error\nIf a call from the Java worker harness to a Python `DoFn` fails, a relevant error message is displayed.\nTo investigate the error, expand the [Cloud Monitoring](/dataflow/docs/guides/using-cloud-monitoring) error log entry and look at the error message and traceback. It shows you which code failed so you can correct it if necessary. If you believe that the error is a bug in Apache Beam or Dataflow, [report the bug](/dataflow/docs/support/getting-support) .\n### EOFError: marshal data too short\nThe following error appears in the worker logs:\n```\nEOFError: marshal data too short\n```\nThis error sometimes occurs when Python pipeline workers run out of disk space.\nTo resolve this issue, see [No space left on device](#no-space-left) .\n### Failed to attach disk\nWhen you try to launch a Dataflow job that uses [C3 VMs](/compute/docs/general-purpose-machines#c3_series) with Persistent Disk, the job fails with one or both of the following errors:\n```\nFailed to attach disk(s), status: generic::invalid_argument: One or more operations had an error\n```\n```\nCan not allocate sha384 (reason: -2), Spectre V2 : WARNING: Unprivileged eBPF is enabled with eIBRS on...\n```\nThese errors occur when you use C3 VMs with an unsupported Persistent Disk type. For more information, see [Supported disk types for C3](/compute/docs/general-purpose-machines#c3_disks) .\nTo use C3 VMs with your Dataflow job, choose the `pd-ssd` worker disk type. For more information, see [Worker-level options](/dataflow/docs/reference/pipeline-options#worker-level_options) .\n`--workerDiskType=pd-ssd`\n`--worker_disk_type=pd-ssd`\n`disk_type=pd-ssd`\n### No space left on device\nWhen a job runs out of disk space, the following error might appear in the worker logs:\n```\nNo space left on device\n```\nThis error can occur for one of the following reasons:\n- The worker persistent storage runs out of free space, which can occur for one of the following reasons:- A job downloads large dependencies at runtime\n- A job uses large custom containers\n- A job writes many temporary data to local disk\n- When using [Dataflow Shuffle](/dataflow/docs/shuffle-for-batch) , Dataflow sets [lower default disk size](/dataflow/docs/guides/deploying-a-pipeline#disk-considerations) . As a result, this error might occur with jobs moving from worker-based shuffle.\n- The worker boot disk fills up because it's logging more than 50 entries per second.\nTo resolve this issue, follow these troubleshooting steps:\nTo see disk resources associated with a single worker, look up VM instance details for worker VMs associated with your job. Part of the disk space is consumed by the operating system, binaries, logs, and containers.\nTo increase persistent disk or boot disk space, adjust the [disk size pipeline option](/dataflow/docs/reference/pipeline-options#worker-level_options) .\nTrack disk space usage on the worker VM instances by using Cloud Monitoring. See [Receive worker VM metrics from the Monitoring agent](/dataflow/docs/guides/using-cloud-monitoring#receive_worker_vm_metrics_from_the_agent) for instructions explaining how to set this up.\nLook for boot disk space issues by [Viewing serial port output](/compute/docs/instances/viewing-serial-port-output#viewing_serial_port_output) on the worker VM instances and looking for messages like:\n```\nFailed to open system journal: No space left on device\n```\nIf you have many worker VM instances, you can create a script to run `gcloud compute instances get-serial-port-output` on all of them at once. You can review that output instead.\n### Python pipeline fails after one hour of worker inactivity\nWhen using the Apache Beam SDK for Python with Dataflow Runner V2 on worker machines with many CPU cores, use Apache Beam SDK 2.35.0 or later. If your job uses a custom container, use Apache Beam SDK 2.46.0 or later.\nConsider pre-building your Python container. This step can improve VM startup times and horizontal autoscaling performance. To use this feature, enable the Cloud Build API on your project and submit your pipeline with the following parameter:\n`\u2011\u2011prebuild_sdk_container_engine=cloud_build` .\nFor more information, see [Dataflow Runner V2](/dataflow/docs/runner-v2) .\nYou can also [use a custom container image](/dataflow/docs/guides/using-custom-containers) with all dependencies preinstalled.\n### Startup of the worker pool in zone failed to bring up any of the desired workers\nThe following error occurs:\n```\nStartup of the worker pool in zone ZONE_NAME failed to bring up any of the desired NUMBER workers.\nThe project quota may have been exceeded or access control policies may be preventing the operation;\nreview the Cloud Logging 'VM Instance' log for diagnostics.\n```\nThis error occurs for one of the following reasons:\n- You have exceeded one of the Compute Engine quotas that Dataflow worker creation relies on.\n- Your organization has [constraints](/resource-manager/docs/organization-policy/using-constraints) in place that prohibit some aspect of the VM instance creation process, like the account being used, or the zone being targeted.\nTo resolve this issue, follow these troubleshooting steps:\n**Review the VM Instance log**\n- Go to the [Cloud Logging viewer](https://console.cloud.google.com/logs/viewer) \n- In the **Audited Resource** drop-down list, select **VM Instance** .\n- In the **All logs** drop-down list, select **compute.googleapis.com/activity_log** .\n- Scan the log for any entries related to VM instance creation failure.\n**Check your usage of Compute Engine quotas**\n- To view Compute Engine resource usage compared to [Dataflow quotas](/dataflow/quotas#compute-engine-quotas) for the zone you're targeting, run the following command:`gcloud compute regions describe [REGION]`\n- Review the results for the following resources to see if any are exceeding quota:- CPUS\n- DISKS_TOTAL_GB\n- IN_USE_ADDRESSES\n- INSTANCE_GROUPS\n- INSTANCES\n- REGIONAL_INSTANCE_GROUP_MANAGERS\n- If needed, [request a quota change](/compute/quotas#request_quotas) .\n**Review your organization policy constraints**\n- Go to the [Organization policies page](https://console.cloud.google.com/iam-admin/orgpolicies/list) \n- Review the constraints for any that might limit VM instance creation for either the account you're using (by default, the [Dataflow service account](/dataflow/docs/concepts/security-and-permissions#cloud_dataflow_service_account) ) or in the zone that you're targeting.\n- If you have a policy that restricts the use of external IP addresses, turn off external IP addresses for this job. For more information about turning off external IP addresses, see [Configure internet access and firewall rules](/dataflow/docs/guides/routes-firewall#turn_off_external_ip_address) .\n### Timed out waiting for an update from the worker\nWhen a Dataflow job fails, the following error occurs:\n```\nRoot cause: Timed out waiting for an update from the worker. For more information, see https://cloud.google.com/dataflow/docs/guides/common-errors#worker-lost-contact.\n```\nSometimes, this error occurs when the worker runs out of memory or swap space. To resolve this issue, as a first step, try running the job again. If the job still fails and the same error occurs, try using a worker with more memory and disk space. For example, add the following pipeline startup option:\n`--worker_machine_type=m1-ultramem-40 --disk_size_gb=500`\nChanging the worker type could affect billed cost. For more information, see [Troubleshoot Dataflow out of memory errors](/dataflow/docs/guides/troubleshoot-oom) .\nThis error can also occur when your data contains a hot key. In this scenario, CPU utilization is high on some workers during most of the duration of the job. However, the number of workers does not reach the maximum allowed. For more information about hot keys and possible solutions, see [Writing Dataflow pipelines with scalability in mind](https://cloud.google.com/blog/products/gcp/writing-dataflow-pipelines-with-scalability-in-mind) .\nFor additional solutions to this issue, see [A hot key ... was detected](#hot-key-detected) .\nIf your Python code calls C/C++ code by using the [Python extension mechanism](https://docs.python.org/3/extending/extending.html) , check whether the extension code releases the Python Global Interpreter Lock (GIL) in computationally intensive parts of code that don't access Python state. The libraries that facilitate interactions with extensions like [Cython](https://cython.readthedocs.io/en/latest/src/userguide/external_C_code.html#nogil) , and [PyBind](https://pybind11.readthedocs.io/en/stable/advanced/misc.html#global-interpreter-lock-gil) have primitives to control GIL status. You can also manually release the GIL and reacquire it before returning control to the Python interpreter by using the `Py_BEGIN_ALLOW_THREADS` and `Py_END_ALLOW_THREADS` macros. For more information, see [Thread State and the Global Interpreter Lock](https://docs.python.org/3/c-api/init.html#thread-state-and-the-global-interpreter-lock) in the Python documentation.\nIn Python pipelines, in the default configuration, Dataflow assumes that each Python process running on the workers efficiently uses one vCPU core. If the pipeline code bypasses the GIL limitations, such as by using libraries that are implemented in C++, processing elements might use resources from more than one vCPU core, and the workers might not get enough CPU resources. To work around this issue, [reduce the number of threads](/dataflow/docs/guides/troubleshoot-oom#reduce-threads) on the workers.\n### Java dependency issues\nIncompatible classes and libraries can cause Java dependency issues. When your pipeline has Java dependency issues, one of the following errors might occur:\n- `NoClassDefFoundError`: This error occurs when an entire class is not available during runtime.\n- `NoSuchMethodError`: This error occurs when the class in the classpath uses a version that doesn't contain the correct method or when the method signature changed.\n- `NoSuchFieldError`: This error occurs when the class in the classpath uses a version that doesn't have a field required during runtime.\n- `FATAL ERROR in native method`: This error occurs when a built-in dependency can't be loaded properly. When using uber JAR (shaded), don't include libraries that use signatures (such as Conscrypt) in the same JAR.\nIf your pipeline contains user-specific code and settings, the code can't contain mixed versions of libraries. If you're using a dependency management library, we recommend that you use [Google Cloud Libraries BOM](/java/docs/bom) .\nIf you're using the Apache Beam SDK, to import the correct libraries BOM, use `beam-sdks-java-io-google-cloud-platform-bom` :\n```\n<dependencyManagement>\u00a0 <dependencies>\u00a0 \u00a0 <dependency>\u00a0 \u00a0 \u00a0 <groupId>org.apache.beam</groupId>\u00a0 \u00a0 \u00a0 <artifactId>beam-sdks-java-google-cloud-platform-bom</artifactId>\u00a0 \u00a0 \u00a0 <version>BEAM_VERSION</version>\u00a0 \u00a0 \u00a0 <type>pom</type>\u00a0 \u00a0 \u00a0 <scope>import</scope>\u00a0 \u00a0 </dependency>\u00a0 </dependencies></dependencyManagement>\n```\n```\ndependencies {\u00a0 \u00a0 implementation(platform(\"org.apache.beam:beam-sdks-java-google-cloud-platform-bom:BEAM_VERSION\"))}\n```\nFor more information, see [Manage pipeline dependencies in Dataflow](/dataflow/docs/guides/manage-dependencies#java) .\n### InaccessibleObjectException in JDK 17 and later\nWhen you run pipelines with the Java Platform, Standard Edition Development Kit (JDK) versions 17 and later, the following error might appear in the worker log files:\n```\nUnable to make protected METHOD accessible:\n module java.MODULE does not \"opens java.MODULE\" to ...\n```\nThis issue occurs because starting in Java version 9, open module Java virtual machine (JVM) options are needed to access JDK internals. In Java 16 and later versions, open module JVM options are always required to access JDK internals.\nTo resolve this issue, when you pass modules to your Dataflow pipeline to open, use the format `` `/` `` `=` `` `(,` `` `)*` with the `jdkAddOpenModules` pipeline option. This format allows access to the necessary library.\nFor example, if the error is `module java.base does not \"opens java.lang\" to unnamed module @...` , then include the following pipeline option when you run your pipeline:\n```\n--jdkAddOpenModules=java.base/java.lang=ALL-UNNAMED\n```\nFor more information, see the [DataflowPipelineOptions](https://beam.apache.org/documentation/sdks/javadoc/current/index.html?org/apache/beam/runners/dataflow/options/DataflowPipelineOptions.html) class documentation.\n## BigQuery connector errors\nThe following sections contain common BigQuery connector errors that you might encounter and steps for resolving or troubleshooting the errors.\n### quotaExceeded\nWhen using the BigQuery connector to write to BigQuery using streaming inserts, write throughput is lower than expected, and the following error might occur:\n```\nquotaExceeded\n```\nSlow throughput might be due to your pipeline exceeding the available BigQuery streaming insert quota. If so, quota related error messages from BigQuery appear in the Dataflow worker logs (look for `quotaExceeded` errors).\nIf you see `quotaExceeded` errors, to resolve this issue:\n- When using the Apache Beam SDK for Java, set the BigQuery sink option`ignoreInsertIds()`.\n- When using the Apache Beam SDK for Python, use the`ignore_insert_ids`option.\nThese settings make you eligible for a one GB per sec, per-project BigQuery streaming insert throughput. For more information on caveats related to automatic message deduplication, see the [BigQuery documentation](/bigquery/streaming-data-into-bigquery#disabling_best_effort_de-duplication) . To increase the BigQuery streaming insert quota higher than one GBps, [submit a request through the Google Cloud console](https://console.cloud.google.com/iam-admin/quotas) .\nIf you don't see quota related errors in worker logs, the issue might be that default bundling or batching related parameters don't provide adequate parallelism for your pipeline to scale. You can adjust several Dataflow BigQuery connector related configurations to achieve the expected performance when writing to BigQuery using streaming inserts. For example, for Apache Beam SDK for Java, adjust `numStreamingKeys` to match the maximum number of workers and consider increasing `insertBundleParallelism` to configure BigQuery connector to write to BigQuery using more parallel threads.\nFor configurations available in the Apache Beam SDK for Java, see [BigQueryPipelineOptions](https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryOptions.java) , and for configurations available in the Apache Beam SDK for Python, see the [WriteToBigQuery transform](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/gcp/bigquery.py) .\n### rateLimitExceeded\nWhen using the BigQuery connector, the following error occurs:\n```\nrateLimitExceeded\n```\nThis error occurs if BigQuery too many [API requests](https://console.cloud.google.com/apis/api/bigquery.googleapis.com/quotas) are sent during a short duration. BigQuery has short term quota limits. It's possible for your Dataflow pipeline to temporarily exceed such a quota. In this scenario, [API requests](https://console.cloud.google.com/apis/api/bigquery.googleapis.com/quotas) from your Dataflow pipeline to BigQuery might fail, which could result in `rateLimitExceeded` errors in worker logs.\nDataflow retries such failures, so you can safely ignore these errors. If you believe that your pipeline is affected by `rateLimitExceeded` errors, contact [Cloud Customer Care](/support) .\n[](None)  [](None)\n## Miscellaneous errors\nThe following sections contain miscellaneous errors that you might encounter and steps for resolving or troubleshooting the errors.\n### No such object: pipeline.pb\nWhen listing jobs using the [JOB_VIEW_ALL](/dataflow/docs/reference/rest/v1b3/JobView) option, the following error occurs:\n```\nNo such object: BUCKET_NAME/PATH/pipeline.pb\n```\nThis error can occur if you delete the `pipeline.pb` file from the staging files for the job.\n## Recommendations\nFor guidance on recommendations generated by Dataflow Insights, see [Insights](/dataflow/docs/guides/using-dataflow-insights#insights) .", "guide": "Dataflow"}