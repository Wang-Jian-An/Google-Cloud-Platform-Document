{"title": "Dataflow - Use Dataflow snapshots", "url": "https://cloud.google.com/dataflow/docs/guides/using-snapshots", "abstract": "# Dataflow - Use Dataflow snapshots\nDataflow snapshots save the state of a streaming pipeline, which lets you start a new version of your Dataflow job without losing state. Snapshots are useful for backup and recovery, testing and rolling back updates to streaming pipelines, and other similar scenarios.\nYou can create a Dataflow snapshot of any running streaming job. Note that any new job you create from a snapshot uses [Streaming Engine](/dataflow/docs/streaming-engine) . You can also use a Dataflow snapshot to migrate your existing pipeline over to the more efficient and scalable Streaming Engine with minimal downtime.\nThis guide explains how to create snapshots, manage snapshots, and create jobs from snapshots.\n", "content": "## Before you begin\n## Create a snapshot\n- In the Google Cloud console, go to the Dataflow **Jobs** page. [Go to Jobs](https://console.cloud.google.com/dataflow) A list of Dataflow jobs appears along with their status.   If you don't see any streaming jobs, you need to run a new streaming   job. For an example of a streaming job, see the [Using   Templates quickstart](/dataflow/docs/quickstarts/create-streaming-pipeline-template) .\n- Select a job.\n- In the menu bar on the **Job details** page, click **Create snapshot** .\n- In the **Create a snapshot** dialog, select one of the following   options:- **Without data sources** : select this option to create a    snapshot of your Dataflow job state only.\n- **With data sources** : select this option to create a snapshot    of both your Dataflow job state along with a snapshot    of your Pub/Sub source.\n- Click **Create** .\nCreate a snapshot:\n```\ngcloud dataflow snapshots create \\\n --job-id=JOB_ID \\\n --snapshot-ttl=DURATION \\\n --snapshot-sources=true \\\n --region=REGION\n```\nReplace the following:- ``: your streaming job ID\n- ``: the amount of time (in days)   before the snapshot expires, after which no more jobs can be created   from the snapshot. The`snapshot-ttl`flag is optional, so   if it is not specified, the snapshot expires in 7 days. Specify the   value in the following format:`5d`. The maximum duration   you can specify is 30 days (`30d`).\n- ``: the region where your streaming job   is running\nThe `snapshot-sources` flag specifies whether to snapshot the  Pub/Sub sources along with the Dataflow  snapshot. If `true` , Pub/Sub sources are  automatically snapshotted and the Pub/Sub snapshot IDs are  shown in the output response. After running the [create](/sdk/gcloud/reference/dataflow/snapshots/create) command, check on the snapshot status by running either the [list](/sdk/gcloud/reference/dataflow/snapshots/list) or the [describe](/sdk/gcloud/reference/dataflow/snapshots/describe) command.\nThe following apply when creating Dataflow snapshots:\n- Dataflow snapshots incur a charge on [disk usage](https://cloud.google.com/dataflow/pricing#snapshot_pricing) .\n- Snapshots are created in the same region as the job.\n- If the job's worker location is different from the job's region, snapshot creation fails. See the [Dataflow regions](/dataflow/docs/concepts/regional-endpoints) guide.\n- You can only take snapshots of non-Streaming Engine jobs if the jobs were started or updated after February 1, 2021.\n- Pub/Sub snapshots created with Dataflow snapshots are managed by the Pub/Sub service and [incur a charge](https://cloud.google.com/pubsub/pricing) .\n- A Pub/Sub snapshot expires no later than 7 days from the time of its creation. Its exact lifetime is determined at creation by the existing backlog in the source subscription. Specifically, the lifetime of the Pub/Sub snapshot is`7 days - (age of oldest unacked message in the subscription)`. For example, consider a subscription whose oldest unacknowledged message is 3 days old. If a Pub/Sub snapshot is created from this subscription, the snapshot, which always captures this 3-day-old backlog as long as the snapshot exists, expires in 4 days. See [Pub/Sub snapshot reference](/pubsub/docs/reference/rest/v1/Snapshot) .\n- During the snapshot operation, your Dataflow job pauses and resumes after the snapshot is ready. The time needed depends on the size of the pipeline state. For example, the time needed to take snapshots on Streaming Engine jobs are generally shorter than non-Streaming Engine jobs.\n- You can [cancel](/dataflow/docs/guides/stopping-a-pipeline#cancel) the job while a snapshot is in progress, which then cancels the snapshot.\n- You cannot [update](/dataflow/docs/guides/updating-a-pipeline) or [drain](/dataflow/docs/guides/stopping-a-pipeline#drain) the job while a snapshot is in progress. You must wait until the job has resumed from the snapshot process before you can update or drain the job.## Use the snapshots page\nAfter you create a snapshot, you can use the **Snapshots** page in the Google Cloud console to view and manage the snapshots for your project.\nClicking on a snapshot opens the **Snapshot details** page. You can view additional metadata about the snapshot as well as a link to the source job and any Pub/Sub snapshots.\n**Note:** The snapshot disk size might not be available in the page for up to 30 minutes after the snapshot has been created. Also, the disk size can vary for the first 24 hours after the snapshot is ready because of periodic data compression operations happening in the background.\n## Delete a snapshot\n**Caution:** Deleting a snapshot while the snapshot is in progress halts the current progress and cancels the snapshot.\nDeleting a snapshot is a way you can stop the snapshot process and resume the job. Also, deleting Dataflow snapshots does not automatically delete the associated Pub/Sub snapshots.\n- In the Google Cloud console, go to the Dataflow **Snapshots** page. [Go to Snapshots](https://console.cloud.google.com/dataflow/snapshots) \n- Select the snapshot and click **Delete** .\n- In the **Delete snapshot** dialog, click **Delete** to   confirm.\nDelete a snapshot:\n```\ngcloud dataflow snapshots delete SNAPSHOT_ID \\\n --region=REGION\n```\nReplace the following:- ``: your snapshot ID\n- ``: the region where your snapshot   exists\nFor more information, see the [delete](/sdk/gcloud/reference/dataflow/snapshots/delete) command reference.\n## Create a job from a snapshot\nAfter you create a snapshot, you can restore your Dataflow job's state by creating a new job from that snapshot.\nTo create a new job from a snapshot, use both the `--createFromSnapshot` and `--enableStreamingEngine` flags.- In your shell or terminal, create a new job from a snapshot. For   example:```\nmvn -Pdataflow-runner compile exec:java \\\u00a0 \u00a0 -Dexec.mainClass=MAIN_CLASS \\\u00a0 \u00a0 -Dexec.args=\"--project=PROJECT_ID \\\u00a0 \u00a0 --stagingLocation=gs://STORAGE_BUCKET/staging/ \\\u00a0 \u00a0 --inputFile=gs://apache-beam-samples/shakespeare/* \\\u00a0 \u00a0 --output=gs://STORAGE_BUCKET/output \\\u00a0 \u00a0 --runner=DataflowRunner \\\u00a0 \u00a0 --enableStreamingEngine \\\u00a0 \u00a0 --createFromSnapshot=SNAPSHOT_ID \\\u00a0 \u00a0 --region=REGION\"\n```Replace the following:- ``or``:   For Java pipelines, the location   of the main class that contains your pipeline code.   For Python pipelines, the location   of the module that contains your pipeline code. For   example, when using the Wordcount example, the value is`org.apache.beam.examples.WordCount`.\n- ``: your Google Cloud project ID\n- ``: the Cloud Storage   bucket you use for temporary job assets and the final output\n- ``: the snapshot ID of the snapshot   from which you want to create a new job\n- ``: the location where you want to run   the new Dataflow jobDataflow snapshots require the Apache Beam SDK for  Python, version 2.29.0 or later.\nTo create a new job from a snapshot, use both the `--createFromSnapshot` and `--enableStreamingEngine` flags.- In your shell or terminal, create a new job from a snapshot. For  example:```\npython -m MODULE \\\u00a0 \u00a0 --project PROJECT_ID \\\u00a0 \u00a0 --temp_location gs://STORAGE_BUCKET/tmp/ \\\u00a0 \u00a0 --input gs://apache-beam-samples/shakespeare/* \\\u00a0 \u00a0 --output gs://STORAGE_BUCKET/output \\\u00a0 \u00a0 --runner DataflowRunner \\\u00a0 \u00a0 --enable_streaming_engine \\\u00a0 \u00a0 --create_from_snapshot=SNAPSHOT_ID \\\u00a0 \u00a0 --region REGION \\\u00a0 \u00a0 --streaming\n```Replace the following:- ``or``:   For Java pipelines, the location   of the main class that contains your pipeline code.   For Python pipelines, the location   of the module that contains your pipeline code. For   example, when using the Wordcount example, the value is`org.apache.beam.examples.WordCount`.\n- ``: your Google Cloud project ID\n- ``: the Cloud Storage   bucket you use for temporary job assets and the final output\n- ``: the snapshot ID of the snapshot   from which you want to create a new job\n- ``: the location where you want to run   the new Dataflow job\nThe following apply when creating jobs from Dataflow snapshots:\n- Jobs created from snapshots must run in the same region where the snapshot is stored.\n- If a Dataflow snapshot includes snapshots of Pub/Sub sources, jobs created from a Dataflow snapshot automatically [seek](/pubsub/docs/replay-overview) to those Pub/Sub snapshots as sources. You must specify the same Pub/Sub topics used by the source job when creating jobs from that Dataflow snapshot. **Note:** Before you create a new Dataflow job, ensure the Pub/Sub snapshots are not expired, otherwise the job stops responding.\n- If a Dataflow snapshot doesn't include snapshots of Pub/Sub sources and the source job uses a Pub/Sub source, you must specify a Pub/Sub topic when creating jobs from that Dataflow snapshot.\n- New jobs created from a snapshot are still subjected to an [update compatibility check](/dataflow/docs/guides/updating-a-pipeline#CCheck) .## Known limitations\nThe following limitations apply to Dataflow snapshots:\n- You cannot create jobs from snapshots using templates or the Dataflow SQL editor.\n- The snapshot expiration timeframe can only be set through the Google Cloud CLI.\n- Dataflow snapshots support only Pub/Sub source snapshots.\n- Sink snapshots are not supported. For example, you cannot create a BigQuery snapshot when creating a Dataflow snapshot.## Troubleshooting\nThis section provides instructions for troubleshooting common issues found when interacting with Dataflow snapshots.\nBefore reaching out for support, ensure that you have ruled out problems related to the known limitations and in the following troubleshooting sections.\n### Snapshot creation request is rejected\nAfter a snapshot creation request is submitted, either from the Google Cloud console or the gcloud CLI, Dataflow service performs a precondition check and returns any error messages. The snapshot creation request can be rejected for various reasons that are specified in the error messages\u2014for example, if a job type is unsupported or a region is unavailable.\nIf the request is rejected because the job is too old, you must update your job before you request a snapshot.\n### Snapshot creation failed\nSnapshot creation might fail for several reasons. For example, the source job was canceled or the project does not have the correct permissions to create Pub/Sub snapshots. The [job-message](/dataflow/docs/guides/logging#navigating_to) logs of the job contains error messages from the snapshot creation. If the snapshot creation fails, then the source job resumes.\n### Create a job from snapshot failed\nWhen you create a job from a snapshot, ensure that the snapshot exists and is not expired. The new job must run on [Streaming Engine](/dataflow/docs/streaming-engine) .\nFor common job creation issues, refer to Dataflow [troubleshooting guide](/dataflow/docs/guides/troubleshooting-your-pipeline) . In particular, new jobs created from snapshots are subjected to an [update compatibility check](/dataflow/docs/guides/updating-a-pipeline#Check) where the new job is required to be compatible with the snapshotted source job.\n### Job created from snapshot makes little progress\nThe [job-message](/dataflow/docs/guides/logging#navigating_to) logs of the job contains error messages for job creation. For example, you might see that the job can't find the Pub/Sub snapshots. In this case, verify that the Pub/Sub snapshots exist and are not expired. Pub/Sub snapshots expire as soon as the oldest message in a snapshot is older than seven days. Expired Pub/Sub snapshots might be removed by Pub/Sub service automatically.\nFor jobs created from Dataflow snapshots that include Pub/Sub source snapshots, the new job might have large Pub/Sub backlogs to process. [Streaming autoscaling](/dataflow/docs/guides/deploying-a-pipeline#autotuning-features) might help the new job to clear the backlog faster.\nThe snapshotted source job might already be in an unhealthy state before the snapshot was taken. Understanding why the source job is unhealthy might help resolve issues of the new job. For common job debugging tips, refer to Dataflow [troubleshooting guide](/dataflow/docs/guides/troubleshooting-your-pipeline) .", "guide": "Dataflow"}