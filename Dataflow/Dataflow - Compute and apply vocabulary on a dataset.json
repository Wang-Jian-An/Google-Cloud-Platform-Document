{"title": "Dataflow - Compute and apply vocabulary on a dataset", "url": "https://cloud.google.com/dataflow/docs/notebooks/compute_and_apply_vocab?hl=zh-cn", "abstract": "# Dataflow - Compute and apply vocabulary on a dataset\n| 0     | 1      |\n|:--------------------|:----------------------|\n| Run in Google Colab | View source on GitHub |\nThe [ComputeAndApplyVocabulary](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.transforms.tft.html#apache_beam.ml.transforms.tft.ComputeAndApplyVocabulary) data processing transform computes a unique vocabulary from a dataset and then maps each word or token to a distinct integer index. Use this transform to change textual data into numerical representations for machine learning (ML) tasks.\nWhen you train ML models that use text data, generating a vocabulary on the incoming dataset is an important preprocessing step. By mapping words to numerical indices, the vocabulary reduces the complexity and dimensionality of dataset. This step allows ML models to process the same words in a consistent way.\nThis notebook shows how to use `MLTransform` to complete the following tasks:\n- Use`write`mode to generate a vocabulary on the input text and assign an index value to each token.\n- Use`read`mode to use the generated vocabulary and assign an index to a different dataset.\n`MLTransform` uses the `ComputeAndApplyVocabulary` transform, which is implemented by using `tensorflow_transform` to generate the vocabulary.\nFor more information about using `MLTransform` , see [Preprocess data with MLTransform](https://beam.apache.org/documentation/ml/preprocess-data/) in the Apache Beam documentation\n", "content": "## Install the required modules\nTo use `ComputeAndVocabulary` with `MLTransfrom` , install `tensorflow_transform` and the Apache Beam SDK version 2.53.0 or later.\n```\n\u00a0pip install apache_beam>=2.53.0 --quiet\u00a0pip install tensorflow-transform --quiet\n```\n```\nimport osimport tempfileimport apache_beam as beamfrom apache_beam.ml.transforms.base import MLTransformfrom apache_beam.ml.transforms.tft import ComputeAndApplyVocabulary\n```\n## Use the artifact location\nIn `write` mode, the artifact location is used to store artifacts, such as the vocabulary file generated by `ComputeAndApplyVocabulary` .\n**Note:** The artifact location must be empty. If it isn't empty, a `RuntimeError` occurs.\nIn `read` mode, `MLTransform` fetches artifacts from the specified artifact location. Pass the same artifact location that you used in `write` mode. Otherwise, a `RuntimeError` occurs or `MLTransform` produces unexpected results in `read` mode.\n```\nartifact_location = tempfile.mkdtemp(prefix='compute_and_apply_vocab_')artifact_location_with_frequency_threshold = tempfile.mkdtemp(prefix='compute_and_apply_vocab_frequency_threshold_')\n```\n```\ndocuments = [\u00a0 \u00a0 {\"feature\": \"the quick brown fox jumps over the lazy dog\"},\u00a0 \u00a0 {\"feature\": \"the five boxing wizards jump quickly in the sky\"},\u00a0 \u00a0 {\"feature\": \"dogs are running in the park\"},\u00a0 \u00a0 {\"feature\": \"the quick brown fox\"}]\n```\nIn this example, in `write` mode, `MLTransform` uses `ComputeAndApplyVocabulary` to generate vocabulary on the incoming dataset. The incoming text data is split into tokens. Each token is assigned an unique index.\nThe generated vocabulary is stored in an artifact location that you can use on a different dataset in `read` mode.\n```\nwith beam.Pipeline() as pipeline:\u00a0 data_pcoll = pipeline | \"CreateData\" >> beam.Create(documents)\u00a0 # Compute and apply vocabulary by using MLTransform.\u00a0 transformed_pcoll = (\u00a0 \u00a0 \u00a0 data_pcoll\u00a0 \u00a0 \u00a0 | \"MLTransform\" >> MLTransform(write_artifact_location=artifact_location).with_transform(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ComputeAndApplyVocabulary(columns=['feature'], split_string_by_delimiter=' ', vocab_filename='vocab_index'))\u00a0 \u00a0 \u00a0 )\u00a0 transformed_pcoll | \"Print\" >> beam.Map(print)\n```\n```\nRow(feature=array([ 0, 1, 4, 3, 12, 10, 0, 11, 16]))\nRow(feature=array([ 0, 14, 17, 5, 13, 8, 2, 0, 6]))\nRow(feature=array([15, 18, 7, 2, 0, 9]))\nRow(feature=array([0, 1, 4, 3]))\n```\n## Understand and visualize vocabulary\nWhen working with text data in machine learning, one common step is the generation of a vocabulary index. `MLTransform` completes this step by using the `ComputeAndApplyVocabulary` transformation. Each unique word in your text data is assigned a specific index. This index is then used to represent the text in a numerical format, which is needed for machine learning algorithms.\nIn this example, the `ComputeAndApplyVocabulary` transformation is applied to the `feature` column. A vocabulary index is created for each unique word found in this column.\nTo visualize and understand this generated vocabulary, use the `ArtifactsFetcher` class. This class allows you to retrieve the vocabulary list from your specified location. When you have this list, you can see the index associated with each word in your vocabulary. This index corresponds to the numerical representation used in the transformation output of `ComputeAndApplyVocabulary` .\nExamine this vocabulary index to understand how your text data is being processed and represented numerically. This understanding is useful for debugging and improving machine learning models that rely on text data.\n```\nfrom apache_beam.ml.transforms.utils import ArtifactsFetcherartifact_fetcher = ArtifactsFetcher(artifact_location)vocab_list = artifact_fetcher.get_vocab_list(vocab_filename='vocab_index_feature')for i in range(len(vocab_list)):\u00a0 print(f'{i}: {vocab_list[i]}')\n```\n```\n0: the\n1: quick\n2: in\n3: fox\n4: brown\n5: wizards\n6: sky\n7: running\n8: quickly\n9: park\n10: over\n11: lazy\n12: jumps\n13: jump\n14: five\n15: dogs\n16: dog\n17: boxing\n18: are\n```\n## Set the frequency threshold\nThe `frequency_threshold` parameter identifies the elements that appear frequently in the dataset. This parameter limits the generated vocabulary to elements with an absolute frequency greater than or equal to the specified threshold. If you don't specify the parameter, the entire vocabulary is generated.\nIf the frequency of a vocabulary item is less than the threshold, it's assigned a default value. You can use the `default_value` parameter to set this value. Otherwise, it defaults to `-1` .\n```\nwith beam.Pipeline() as pipeline:\u00a0 data_pcoll = pipeline | \"CreateData\" >> beam.Create(documents)\u00a0 # Compute and apply vocabulary by using MLTransform.\u00a0 transformed_pcoll = (\u00a0 \u00a0 \u00a0 data_pcoll\u00a0 \u00a0 \u00a0 | \"MLTransform\" >> MLTransform(write_artifact_location=artifact_location_with_frequency_threshold).with_transform(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ComputeAndApplyVocabulary(columns=['feature'], split_string_by_delimiter=' ', frequency_threshold=2, vocab_filename='vocab_index'))\u00a0 \u00a0 \u00a0 )\u00a0 transformed_pcoll | \"Print\" >> beam.Map(print)\n```\n```\nRow(feature=array([ 0, 1, 4, 3, -1, -1, 0, -1, -1]))\nRow(feature=array([ 0, -1, -1, -1, -1, -1, 2, 0, -1]))\nRow(feature=array([-1, -1, -1, 2, 0, -1]))\nRow(feature=array([0, 1, 4, 3]))\n```\nIn the output, if the frequency of the token is less than the specified frequency, it's assigned to a `default_value` of `-1` . For the other tokens, a vocabulary file is generated.\n```\nfrom apache_beam.ml.transforms.utils import ArtifactsFetcherartifact_fetcher = ArtifactsFetcher(artifact_location_with_frequency_threshold)vocab_list = artifact_fetcher.get_vocab_list(vocab_filename='vocab_index_feature')for i in range(len(vocab_list)):\u00a0 print(f'{i}: {vocab_list[i]}')\n```\n```\n0: the\n1: quick\n2: in\n3: fox\n4: brown\n```\n## Use MLTransform for inference workloads\nWhen `MLTransform` is in `write` mode, it produces artifacts, such as vocabulary files for `ComputeAndApplyVocabulary` . These artifacts allow you to apply the same vocabulary, and any other preprocessing transforms, when you train your model and serve it in production, or when you test its accuracy.\nWhen `MLTransform` is used `read` mode, it uses the previously generated vocabulary files to map the incoming text data. If the incoming vocabulary isn't found in the generated vocabulary, then the incoming vocabulary is mapped to a `default_value` provided during `write` mode. In this case, the `default_value` is `-1` .\nWhen `MLTransform` is in `write` mode, it produces artifacts, such as vocabulary files for `ComputeAndApplyVocabulary` .\n```\ntest_documents = [\u00a0 \u00a0 {'feature': 'wizards are flying in the sky'},\u00a0 \u00a0 {'feature': 'I love dogs'}]with beam.Pipeline() as pipeline:\u00a0 data_pcoll = pipeline | \"CreateData\" >> beam.Create(test_documents)\u00a0 # Compute and apply vocabulary by using MLTransform.\u00a0 transformed_pcoll = (\u00a0 \u00a0 \u00a0 data_pcoll\u00a0 \u00a0 \u00a0 | \"MLTransform\" >> MLTransform(read_artifact_location=artifact_location))\u00a0 transformed_pcoll | \"Print\" >> beam.Map(print)\n```\n```\nRow(feature=array([ 5, 18, -1, 2, 0, 6]))\nRow(feature=array([-1, -1, 15]))\n```\nWhen you specify `read_artifact_location` , you don't have to pass any transforms to `MLTransform` . Instead, `MLTransform` saves the artifacts and the transforms produced in the location specified by `write_artifact_location` .", "guide": "Dataflow"}