{"title": "Dataflow - Configure internet access and firewall rules", "url": "https://cloud.google.com/dataflow/docs/guides/routes-firewall", "abstract": "# Dataflow - Configure internet access and firewall rules\nThis document explains how to configure Dataflow virtual machine (VM) instances for internet access, create network tags, and define firewall rules for the network associated with your Dataflow jobs.\nThis document requires that you have basic knowledge of Google Cloud networks. To define a network for your Dataflow job, see [Specify your network and subnetwork](/dataflow/docs/guides/specifying-networks) .\n**Note:** The `default` network has configurations that allow Dataflow jobs to run. However, other services might also use this network. Make sure your changes to the `default` network are compatible with all of your services. Alternatively, create a separate network for Dataflow.\n", "content": "## Internet access for Dataflow\nDataflow worker virtual machines (VMs) must reach Google Cloud APIs and services. Depending on your use case, your VMs may also need access to resources outside Google Cloud. Use one of the following methods to configure internet access for Dataflow:\n- [Configure worker VMs](/vpc/docs/access-apis-external-ip) with an external IP address so that they meet the [internet access requirements](/vpc/docs/vpc#internet_access_reqs) .\n- Configure [Private Google Access](/vpc/docs/configure-private-google-access) . With Private Google Access, VMs that have only internal IP addresses can access IP addresses for Google Cloud and services.\n- Configure a [Private Service Connect endpoint IP address](/vpc/docs/configure-private-service-connect-apis) to access Google Cloud APIs and services.\n- Configure a NAT solution, such as [Cloud NAT](/nat/docs/overview) . This option is for running jobs that access APIs and services outside of Google Cloud that require internet access. For example, Python SDK jobs might require access to the Python Package Index (PyPI) to download pipelines dependencies. In this case, you must either configure worker VMs with external IP addresses or use Cloud NAT. You can also supply Python pipeline dependencies during job submission. For example, you can [use custom containers](/dataflow/docs/guides/using-custom-containers) to supply Python pipeline dependencies, which removes the need to access PyPI at runtime.For more information, see [Managing Python Pipeline Dependencies](https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/) in the Apache Beam documentation.\n### Turn off external IP address\nBy default, the Dataflow service assigns workers both external and internal IP addresses. When you turn off external IP addresses, the Dataflow pipeline can access resources only in the following places:\n- Another instance in the same VPC network\n- A [Shared VPC network](/vpc/docs/shared-vpc) \n- A network with [VPC Network Peering](/vpc/docs/vpc-peering) enabledWithout external IP addresses, you can still perform administrative and monitoring tasks. You can access your workers by using SSH through the options listed in the preceding list. However, the pipeline cannot access the internet, and internet hosts cannot access your Dataflow workers.\nNot using external IP addresses helps to better secure your data processing infrastructure. Additionally, you also lower the number of external IP addresses that you consume against your [Google Cloudproject quota](/vpc/docs/quota) .\nIf you turn off external IP addresses, your Dataflow jobs cannot access APIs and services outside of Google Cloud that require internet access.\nFor information on setting up internet access for jobs with internal IP addresses, read the previous section.\nTo turn off external IP addresses, do one of the following:\n- Enable [Private Google Access](/vpc/docs/private-google-access) for your network or subnetwork.\n- In the parameters of your Dataflow job, specify`--usePublicIps=false`and`--network=` ``or`--subnetwork=` ``.Depending on your choice, replace one of the following:- : the name of your Compute Engine network\n- : the name of your Compute Engine subnetwork- To stage all Python package dependencies, follow the Apache Beam [pipeline dependencies instructions](https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/) .\n- Enable [Private Google Access](/vpc/docs/private-google-access) for your network or subnetwork.\n- In the parameters of your Dataflow job, specify`--no_use_public_ips`and`--network=` ``or`--subnetwork=` ``.\n- Depending on your choice, replace one of the following:- : the name of your Compute Engine network\n- : the name of your Compute Engine subnetwork\n- Enable [Private Google Access](/vpc/docs/private-google-access) for your network or subnetwork.\n- In the parameters of your Dataflow job, specify`--no_use_public_ips`and`--network=` ``or`--subnetwork=` ``.\n- Depending on your choice, replace one of the following:- : the name of your Compute Engine network\n- : the name of your Compute Engine subnetwork\n## Network tags for Dataflow\n[Network tags](/vpc/docs/add-remove-network-tags) are text attributes that you can attach to Compute Engine VMs. Network tags let you make VPC network firewall rules and certain custom static routes applicable to specific VM instances. Dataflow supports adding network tags to all worker VMs that run a particular Dataflow job.\nEven if you do not use the network parameter, Dataflow always adds the default network tag `dataflow` to every worker VM it creates.\n### Enable network tags\nYou can specify the network tags only when you run the Dataflow job template to create a job. After a job starts, you cannot add more network tags to the job. To apply additional network tags to a job, you must recreate your job template with the required network tags.\nAdd the following to your pipeline code, whether you run in Java or Python:\n```\n--experiments=use_network_tags=TAG-NAME\n```\nReplace with the names of your tags. If you add more than one tag, separate each tag with a semicolon ( `;` ), as shown in the following format: .\nEven if you do not use this parameter, Dataflow always adds the network tag `dataflow` to every worker VM it creates.\n### Enable network tags for Flex Template VMs\nWhen using Flex Templates, to enable network tags for Dataflow worker VMs, use the `--additional-experiments` option as shown in the following example:\n```\n--additional-experiments=use_network_tags=TAG-NAME\n```\nTo enable network tags for both worker VMs and launcher VMs, you need to use the following two options:\n```\n--additional-experiments=use_network_tags=TAG-NAME--additional-experiments=use_network_tags_for_flex_templates=TAG-NAME\n```\nReplace with the names of your tags. If you add more than one tag, separate each tag with a semicolon ( `;` ), as shown in the following format: .\nAfter you enable the network tags, the tags are parsed and attached to the VMs.\n**Note:** Specifying the network tag also adds the default network tag `Dataflow` to Flex Template Launcher VMs.\nSee the [limits](/vpc/docs/add-remove-network-tags#restrictions) applicable for network tags.\n## Firewall rules for Dataflow\nFirewall rules let you allow or deny traffic to and from your VMs. If your Dataflow jobs use [Dataflow Shuffle](/dataflow/docs/shuffle-for-batch) or [Streaming Engine](/dataflow/docs/streaming-engine) , then you don't need to configure any firewall rules. Otherwise, you must configure firewalls rules so that Dataflow VMs can send and receive network traffic on TCP port `12345` for streaming jobs and on TCP port `12346` for batch jobs. A project owner, editor, or security administrator must create necessary firewall rules in the VPC network used by your Dataflow VMs.\nBefore you configure firewall rules for Dataflow, read the following documents:\n- [VPC firewall rules overview](/vpc/docs/firewalls) and [Using firewall rules](/vpc/docs/using-firewalls) \n- [Hierarchical firewall policies overview](/vpc/docs/firewall-policies) and [Using hierarchical firewall policies](/vpc/docs/using-firewall-policies) \nWhen you create firewall rules for Dataflow, specify the Dataflow network tags. Otherwise, the firewall rules apply to all VMs in the VPC network.\nWhere applicable, hierarchical firewall policies are evaluated first and these rules preempt VPC firewall rules. If the Dataflow job is in a project which is part of a folder or organization where hierarchical firewall policies are used, the `compute.orgFirewallPolicyAdmin` role is required to make policy modifications.\nIf you did not create custom network tags when you ran the pipeline code, Dataflow VMs use the default `dataflow` tag. In the absence of custom network tags, create the firewall rules with the default `dataflow` tag.\nIf you created custom network tags when you ran the pipeline code, Dataflow VMs use those tags. Create the firewall rules with the custom tags.\nSome VPC networks, such as the automatically created `default` network, include a `default-allow-internal` rule that meets the firewall requirement for Dataflow.\n### Example firewall ingress rule\nThe ingress firewall rule permits Dataflow VMs to receive packets from each other. You must always create ingress allow firewall rules or traffic is always blocked, even if egress rules allow such traffic.\nIn the following example, a firewall ingress rule is created for Dataflow, where all worker VMs have the default network tag `dataflow` . A project owner, editor, or security admin can use the following `gcloud` command to create an ingress allow rule that permits traffic on TCP ports `12345` and `12346` from VMs with the network tag `dataflow` to other VMs with the same tag:\n```\ngcloud compute firewall-rules create FIREWALL_RULE_NAME_INGRESS \\\u00a0 \u00a0 --action=allow \\\u00a0 \u00a0 --direction=ingress \\\u00a0 \u00a0 --network=NETWORK \u00a0\\\u00a0 \u00a0 --target-tags=CUSTOM_TAG \\\u00a0 \u00a0 --source-tags=CUSTOM_TAG \\\u00a0 \u00a0 --priority=PRIORITY_NUM \\\u00a0 \u00a0 --rules tcp:12345-12346\n```\nReplace the following:\n- `` : a name for the firewall rule\n- `` : the name of the network that your worker VMs use\n- `` : a comma-delimited list of network tagsThe following is a list of guidelines for using network tags:- If you omit `--target-tags` , the rule applies to all VMs in the VPC network.\n- If you omit `--source-tags` and all other source specifications, traffic from any source is allowed.\n- If you have not specified custom network tags and you want the rule to be specific to Dataflow VMs, use `dataflow` as the network tag.\n- If you have specified custom network tags and you want the rule to be specific to Dataflow VMs, use your custom network tags.\n- `` : the priority of the firewall ruleLower numbers have higher priorities and 0 is the highest priority.\n### Example firewall egress rule\nThe egress firewall rule permits Dataflow VMs to send packets to each other. If you've created any egress deny firewall rules, you might need to create custom egress allow firewall rules in your VPC network.\nIn this example, a firewall egress rule is created for Dataflow, where all worker VMs have the default network tag of `dataflow` . A project owner, editor, or security admin can use the following `gcloud` command to create an egress allow rule that permits traffic from TCP ports `12345` and `12346` on VMs with the network tag `dataflow` to other VMs with the same tag:\n```\ngcloud compute firewall-rules create FIREWALL_RULE_NAME_EGRESS \\\u00a0 \u00a0 --network=NETWORK \\\u00a0 \u00a0 --action=allow \\\u00a0 \u00a0 --direction=egress \\\u00a0 \u00a0 --target-tags=CUSTOM_TAG \\\u00a0 \u00a0 --destination-ranges=DESTINATION-RANGES\\\u00a0 \u00a0 --priority=PRIORITY_NUM \u00a0\\\u00a0 \u00a0 --rules tcp:12345-12346\n```\nReplace the following:\n- `` : a name for the firewall rule\n- `` : the name of the network that your worker VMs use\n- `` : a comma-delimited list of network tagsThe following is a list of guidelines for using network tags:- If you omit `--target-tags` , the rule applies to all VMs in the VPC network.\n- If you omit `--source-tags` and all other source specifications, traffic from any source is allowed.\n- If you have not specified custom network tags and you want the rule to be specific to Dataflow VMs, use `dataflow` as the network tag.\n- If you have specified custom network tags and you want the rule to be specific to Dataflow VMs, use your custom network tags.\n- `` : a comma-delimited list of CIDRsInclude the selected subnetwork's primary IP address range.\n- `` : the priority of the firewall ruleLower numbers have higher priorities and 0 is the highest priority.\nFor specific TCP ports used by Dataflow, you can [view the project container manifest](/deployment-manager/docs/deployments/viewing-manifest#view_a_manifest) . The container manifest explicitly specifies the ports in order to map host ports into the container.\n## SSH access to worker VMs\nDataflow does not require SSH; however, SSH is useful for troubleshooting.\nIf your worker VM has an external IP address, you can [connect tothe VM](/compute/docs/instances/connecting-to-instance) through either the Google Cloud console or by using the Google Cloud CLI. To connect using SSH, you must have a firewall rule that allows incoming connections on TCP port 22 from at least the IP address of the system on which you're running `gcloud` or the system running the web browser you use to access the Google Cloud console.\nYou can view network configuration and activity by opening an [SSH session on one of your workers](https://cloud.google.com/compute/docs/tutorials/service-account-ssh#sa_ssh) and running `iproute2` . For more information, see the [iproute2 page](https://wiki.linuxfoundation.org/networking/iproute2) .\nIf you need to connect to a worker VM that only has an internal IP address, see [Choose a connection option for internal-only VMs](/compute/docs/connect/ssh-internal-ip) .", "guide": "Dataflow"}