{"title": "Dataflow - Apache Beam RunInference for PyTorch", "url": "https://cloud.google.com/dataflow/docs/notebooks/run_inference_pytorch?hl=zh-cn", "abstract": "# Dataflow - Apache Beam RunInference for PyTorch\n| 0     | 1      |\n|:--------------------|:----------------------|\n| Run in Google Colab | View source on GitHub |\nThis notebook demonstrates the use of the RunInference transform for PyTorch. Apache Beam includes implementations of the [ModelHandler](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.ModelHandler) class for [users of PyTorch](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.pytorch_inference.html) . For more information about using RunInference, see [Get started with AI/ML pipelines](https://beam.apache.org/documentation/ml/overview/) in the Apache Beam documentation.\nThis notebook illustrates common RunInference patterns, such as:\n- Using a database with RunInference.\n- Postprocessing results after using RunInference.\n- Inference with multiple models in the same pipeline.\nThe linear regression models used in these samples are trained on data that correspondes to the 5 and 10 times tables; that is, `y = 5x` and `y = 10x` respectively.\n", "content": "## Dependencies\nThe RunInference library is available in Apache Beam versions 2.40 and later.\nTo use Pytorch RunInference API, you need to install the PyTorch module. To install PyTorch, use `pip` :\n```\npip install apache_beam[gcp,dataframe] --quiet\n```\n```\n%pip install torch --quiet\n```\n```\nimport argparseimport csvimport jsonimport osimport torchfrom typing import Tupleimport apache_beam as beamimport numpyfrom apache_beam.io.gcp.bigquery import ReadFromBigQueryfrom apache_beam.ml.inference.base import KeyedModelHandlerfrom apache_beam.ml.inference.base import PredictionResultfrom apache_beam.ml.inference.base import RunInferencefrom apache_beam.dataframe.convert import to_pcollectionfrom apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensorfrom apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerKeyedTensorfrom apache_beam.options.pipeline_options import PipelineOptionsimport warningswarnings.filterwarnings('ignore')\n```\n```\nfrom google.colab import authauth.authenticate_user()\n```\n```\n# Constantsproject = \"<your GCP project>\"bucket = \"<your GCP bucket>\"# To avoid warnings, set the project.os.environ['GOOGLE_CLOUD_PROJECT'] = projectsave_model_dir_multiply_five = 'five_times_table_torch.pt'save_model_dir_multiply_ten = 'ten_times_table_torch.pt'\n```\n## Create data and PyTorch models for the RunInference transform\nCreate linear regression models, prepare train and test data, and train models.\n### Create a linear regression model in PyTorch\nUse the following code to create a linear regression model.\n```\nclass LinearRegression(torch.nn.Module):\u00a0 \u00a0 def __init__(self, input_dim=1, output_dim=1):\u00a0 \u00a0 \u00a0 \u00a0 super().__init__()\u00a0 \u00a0 \u00a0 \u00a0 self.linear = torch.nn.Linear(input_dim, output_dim) \u00a0\u00a0 \u00a0 def forward(self, x):\u00a0 \u00a0 \u00a0 \u00a0 out = self.linear(x)\u00a0 \u00a0 \u00a0 \u00a0 return out\n```\n### Prepare train and test data for an example model\nThis example model is a 5 times table.\n- `x`contains values in the range from 0 to 99.\n- `y`is a list of 5 *`x`.\n- `value_to_predict`includes values outside of the training data.\n```\nx = numpy.arange(0, 100, dtype=numpy.float32).reshape(-1, 1)y = (x * 5).reshape(-1, 1)value_to_predict = numpy.array([20, 40, 60, 90], dtype=numpy.float32).reshape(-1, 1)\n```\n### Train the linear regression mode on 5 times data\nUse the following code to train your linear regression model on the 5 times table.\n```\nfive_times_model = LinearRegression()optimizer = torch.optim.Adam(five_times_model.parameters())loss_fn = torch.nn.L1Loss()\"\"\"Train the five_times_model\"\"\"epochs = 10000tensor_x = torch.from_numpy(x)tensor_y = torch.from_numpy(y)for epoch in range(epochs):\u00a0 \u00a0 y_pred = five_times_model(tensor_x)\u00a0 \u00a0 loss = loss_fn(y_pred, tensor_y)\u00a0 \u00a0 five_times_model.zero_grad()\u00a0 \u00a0 loss.backward()\u00a0 \u00a0 optimizer.step()\n```\nSave the model using `torch.save()` , and then confirm that the saved model file exists.\n```\ntorch.save(five_times_model.state_dict(), save_model_dir_multiply_five)print(os.path.exists(save_model_dir_multiply_five)) # Verify that the model is saved.\n```\n```\nTrue\n```\n### Prepare train and test data for a 10 times model\nThis example model is a 10 times table.\n- `x`contains values in the range from 0 to 99.\n- `y`is a list of 10 *`x`.\n```\nx = numpy.arange(0, 100, dtype=numpy.float32).reshape(-1, 1)y = (x * 10).reshape(-1, 1)\n```\n### Train the linear regression model on 10 times data\nUse the following to train your linear regression model on the 10 times table.\n```\nten_times_model = LinearRegression()optimizer = torch.optim.Adam(ten_times_model.parameters())loss_fn = torch.nn.L1Loss()epochs = 10000tensor_x = torch.from_numpy(x)tensor_y = torch.from_numpy(y)for epoch in range(epochs):\u00a0 \u00a0 y_pred = ten_times_model(tensor_x)\u00a0 \u00a0 loss = loss_fn(y_pred, tensor_y)\u00a0 \u00a0 ten_times_model.zero_grad()\u00a0 \u00a0 loss.backward()\u00a0 \u00a0 optimizer.step()\n```\nSave the model using `torch.save()` .\n```\ntorch.save(ten_times_model.state_dict(), save_model_dir_multiply_ten)print(os.path.exists(save_model_dir_multiply_ten)) # verify if the model is saved\n```\n```\nTrue\n```\n## Pattern 1: RunInference for predictions\nThis pattern demonstrates how to use RunInference for predictions.\n### Use RunInference within the pipeline\n- Create a PyTorch model handler object by passing required arguments such as`state_dict_path`,`model_class`, and`model_params`to the`PytorchModelHandlerTensor`class.\n- Pass the`PytorchModelHandlerTensor`object to the RunInference transform to perform predictions on unkeyed data.\n```\ntorch_five_times_model_handler = PytorchModelHandlerTensor(\u00a0 \u00a0 state_dict_path=save_model_dir_multiply_five,\u00a0 \u00a0 model_class=LinearRegression,\u00a0 \u00a0 model_params={'input_dim': 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'output_dim': 1}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )pipeline = beam.Pipeline()with pipeline as p:\u00a0 \u00a0 \u00a0 (\u00a0 \u00a0 \u00a0 p \u00a0 \u00a0 \u00a0 | \"ReadInputData\" >> beam.Create(value_to_predict)\u00a0 \u00a0 \u00a0 | \"ConvertNumpyToTensor\" >> beam.Map(torch.Tensor)\u00a0 \u00a0 \u00a0 | \"RunInferenceTorch\" >> RunInference(torch_five_times_model_handler)\u00a0 \u00a0 \u00a0 | beam.Map(print)\u00a0 \u00a0 \u00a0 )\n```\n```\nPredictionResult(example=tensor([20.]), inference=tensor([102.0095], grad_fn=<UnbindBackward0>))\nPredictionResult(example=tensor([40.]), inference=tensor([201.2056], grad_fn=<UnbindBackward0>))\nPredictionResult(example=tensor([60.]), inference=tensor([300.4017], grad_fn=<UnbindBackward0>))\nPredictionResult(example=tensor([90.]), inference=tensor([449.1959], grad_fn=<UnbindBackward0>))\n```\n## Pattern 2: Postprocess RunInference results\nThis pattern demonstrates how to postprocess the RunInference results.\nAdd a `PredictionProcessor` to the pipeline after `RunInference` . `PredictionProcessor` processes the output of the `RunInference` transform.\n```\nclass PredictionProcessor(beam.DoFn):\u00a0 \"\"\"\u00a0 A processor to format the output of the RunInference transform.\u00a0 \"\"\"\u00a0 def process(\u00a0 \u00a0 \u00a0 self,\u00a0 \u00a0 \u00a0 element: PredictionResult):\u00a0 \u00a0 input_value = element.example\u00a0 \u00a0 output_value = element.inference\u00a0 \u00a0 yield (f\"input is {input_value.item()} output is {output_value.item()}\")pipeline = beam.Pipeline()with pipeline as p:\u00a0 \u00a0 (\u00a0 \u00a0 p\u00a0 \u00a0 | \"ReadInputData\" >> beam.Create(value_to_predict)\u00a0 \u00a0 | \"ConvertNumpyToTensor\" >> beam.Map(torch.Tensor)\u00a0 \u00a0 | \"RunInferenceTorch\" >> RunInference(torch_five_times_model_handler)\u00a0 \u00a0 | \"PostProcessPredictions\" >> beam.ParDo(PredictionProcessor())\u00a0 \u00a0 | beam.Map(print)\u00a0 \u00a0 )\n```\n```\ninput is 20.0 output is 102.00947570800781\ninput is 40.0 output is 201.20559692382812\ninput is 60.0 output is 300.4017028808594\ninput is 90.0 output is 449.1958923339844\n```\n## Pattern 3: Attach a key\nThis pattern demonstrates how attach a key to allow your model to handle keyed data.\n### Modify the model handler and post processor\nModify the pipeline to read from sources like CSV files and BigQuery.\nIn this step, you take the following actions:\n- To handle keyed data, wrap the`PytorchModelHandlerTensor`object around`KeyedModelHandler`.\n- Add a map transform that converts a table row into`Tuple[str, float]`.\n- Add a map transform that converts`Tuple[str, float]`to`Tuple[str, torch.Tensor]`.\n- Modify the post-inference processor to output results with the key.\n```\nclass PredictionWithKeyProcessor(beam.DoFn):\u00a0 \u00a0 def __init__(self):\u00a0 \u00a0 \u00a0 \u00a0 beam.DoFn.__init__(self)\u00a0 \u00a0 def process(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 self,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 element: Tuple[str, PredictionResult]):\u00a0 \u00a0 \u00a0 \u00a0 key = element[0]\u00a0 \u00a0 \u00a0 \u00a0 input_value = element[1].example\u00a0 \u00a0 \u00a0 \u00a0 output_value = element[1].inference\u00a0 \u00a0 \u00a0 \u00a0 yield (f\"key: {key}, input: {input_value.item()} output: {output_value.item()}\" )\n```\n### Create a source with attached key\nThis section shows how to create either a BigQuery or a CSV source with an attached key.\nTo install the Google Cloud BigQuery API, use `pip` :\n```\n%pip install --upgrade google-cloud-bigquery --quiet\n```\nCreate a table in BigQuery using the following snippet, which has two columns. The first column holds the key and the second column holds the test value. To use BiqQuery, you need a Google Cloud account with the BigQuery API enabled.\n```\ngcloud config set project $project\n```\n```\nUpdated property [core/project].\n```\n```\nfrom google.cloud import bigqueryclient = bigquery.Client(project=project)# Make sure the dataset_id is unique in your project.dataset_id = '{project}.maths'.format(project=project)dataset = bigquery.Dataset(dataset_id)# Modify the location based on your project configuration.dataset.location = 'US'dataset = client.create_dataset(dataset, exists_ok=True)# Table name in the BigQuery dataset.table_name = 'maths_problems_1'query = \"\"\"\u00a0 \u00a0 CREATE OR REPLACE TABLE\u00a0 \u00a0 \u00a0 {project}.maths.{table} ( key STRING OPTIONS(description=\"A unique key for the maths problem\"),\u00a0 \u00a0 value FLOAT64 OPTIONS(description=\"Our maths problem\" ) );\u00a0 \u00a0 INSERT INTO maths.{table}\u00a0 \u00a0 VALUES\u00a0 \u00a0 \u00a0 (\"first_question\", 105.00),\u00a0 \u00a0 \u00a0 (\"second_question\", 108.00),\u00a0 \u00a0 \u00a0 (\"third_question\", 1000.00),\u00a0 \u00a0 \u00a0 (\"fourth_question\", 1013.00)\"\"\".format(project=project, table=table_name)create_job = client.query(query)create_job.result()\n```\n```\n<google.cloud.bigquery.table._EmptyRowIterator at 0x7f5694206650>\n```\nTo read keyed data, use BigQuery as the pipeline source.\n```\npipeline_options = PipelineOptions().from_dictionary({'temp_location':f'gs://{bucket}/tmp',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 })pipeline = beam.Pipeline(options=pipeline_options)keyed_torch_five_times_model_handler = KeyedModelHandler(torch_five_times_model_handler)table_name = 'maths_problems_1'table_spec = f'{project}:maths.{table_name}'with pipeline as p:\u00a0 \u00a0 \u00a0 (\u00a0 \u00a0 \u00a0 p\u00a0 \u00a0 \u00a0 | \"ReadFromBQ\" >> beam.io.ReadFromBigQuery(table=table_spec) \u00a0 \u00a0 \u00a0 | \"PreprocessData\" >> beam.Map(lambda x: (x['key'], x['value']))\u00a0 \u00a0 \u00a0 | \"ConvertNumpyToTensor\" >> beam.Map(lambda x: (x[0], torch.Tensor([x[1]])))\u00a0 \u00a0 \u00a0 | \"RunInferenceTorch\" >> RunInference(keyed_torch_five_times_model_handler)\u00a0 \u00a0 \u00a0 | \"PostProcessPredictions\" >> beam.ParDo(PredictionWithKeyProcessor())\u00a0 \u00a0 \u00a0 | beam.Map(print)\u00a0 \u00a0 \u00a0 )\n```\n```\nkey: third_question, input: 1000.0 output: 4962.61962890625\nkey: second_question, input: 108.0 output: 538.472412109375\nkey: first_question, input: 105.0 output: 523.5929565429688\nkey: fourth_question, input: 1013.0 output: 5027.0966796875\n```\nCreate a CSV file with two columns: one named `key` that holds the keys, and a second named `value` that holds the test values.\n```\n# creates a CSV file with the values.csv_values = [(\"first_question\", 105.00),\u00a0 \u00a0 \u00a0 (\"second_question\", 108.00),\u00a0 \u00a0 \u00a0 (\"third_question\", 1000.00),\u00a0 \u00a0 \u00a0 (\"fourth_question\", 1013.00)]input_csv_file = \"./maths_problem.csv\"with open(input_csv_file, 'w') as f:\u00a0 writer = csv.writer(f)\u00a0 writer.writerow(['key', 'value'])\u00a0 for row in csv_values:\u00a0 \u00a0 writer.writerow(row)assert os.path.exists(input_csv_file) == True\n```\n```\npipeline_options = PipelineOptions().from_dictionary({'temp_location':f'gs://{bucket}/tmp',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 })pipeline = beam.Pipeline(options=pipeline_options)keyed_torch_five_times_model_handler = KeyedModelHandler(torch_five_times_model_handler)with pipeline as p:\u00a0 df = p | beam.dataframe.io.read_csv(input_csv_file)\u00a0 pc = to_pcollection(df)\u00a0 (pc\u00a0 \u00a0 | \"ConvertNumpyToTensor\" >> beam.Map(lambda x: (x[0], torch.Tensor([x[1]])))\u00a0 \u00a0 | \"RunInferenceTorch\" >> RunInference(keyed_torch_five_times_model_handler)\u00a0 \u00a0 | \"PostProcessPredictions\" >> beam.ParDo(PredictionWithKeyProcessor())\u00a0 \u00a0 | beam.Map(print)\u00a0 \u00a0 )\n```\n```\nkey: first_question, input: 105.0 output: 523.5929565429688\nkey: second_question, input: 108.0 output: 538.472412109375\nkey: third_question, input: 1000.0 output: 4962.61962890625\nkey: fourth_question, input: 1013.0 output: 5027.0966796875\n```\n## Pattern 4: Inference with multiple models in the same pipeline\nThis pattern demonstrates how use inference with multiple models in the same pipeline.\n### Multiple models in parallel\nThis section demonstrates how use inference with multiple models in parallel.\nCreate a torch model handler for the 10 times model using `PytorchModelHandlerTensor` .\n```\ntorch_ten_times_model_handler = PytorchModelHandlerTensor(state_dict_path=save_model_dir_multiply_ten,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_class=LinearRegression,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_params={'input_dim': 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'output_dim': 1}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )keyed_torch_ten_times_model_handler = KeyedModelHandler(torch_ten_times_model_handler)\n```\nIn this section, the same data is run through two different models: the one that we use to multiply by 5 and a new model that learns to multiply by 10.\n```\npipeline_options = PipelineOptions().from_dictionary(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {'temp_location':f'gs://{bucket}/tmp'})pipeline = beam.Pipeline(options=pipeline_options)read_from_bq = beam.io.ReadFromBigQuery(table=table_spec)with pipeline as p:\u00a0 multiply_five = (\u00a0 \u00a0 \u00a0 p \u00a0 \u00a0 \u00a0 | \u00a0read_from_bq\u00a0 \u00a0 \u00a0 | \"CreateMultiplyFiveTuple\" >> beam.Map(lambda x: ('{} {}'.format(x['key'], '* 5'), x['value']))\u00a0 \u00a0 \u00a0 | \"ConvertNumpyToTensorFiveTuple\" >> beam.Map(lambda x: (x[0], torch.Tensor([x[1]])))\u00a0 \u00a0 \u00a0 | \"RunInferenceTorchFiveTuple\" >> RunInference(keyed_torch_five_times_model_handler)\u00a0 )\u00a0 multiply_ten = (\u00a0 \u00a0 \u00a0 p \u00a0 \u00a0 \u00a0 | read_from_bq \u00a0 \u00a0 \u00a0 | \"CreateMultiplyTenTuple\" >> beam.Map(lambda x: ('{} {}'.format(x['key'], '* 10'), x['value']))\u00a0 \u00a0 \u00a0 | \"ConvertNumpyToTensorTenTuple\" >> beam.Map(lambda x: (x[0], torch.Tensor([x[1]])))\u00a0 \u00a0 \u00a0 | \"RunInferenceTorchTenTuple\" >> RunInference(keyed_torch_ten_times_model_handler)\u00a0 )\u00a0 inference_result = ((multiply_five, multiply_ten) | beam.Flatten() \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.ParDo(PredictionWithKeyProcessor()))\u00a0 inference_result | beam.Map(print)\n```\n```\nkey: third_question * 10, input: 1000.0 output: 9889.59765625\nkey: second_question * 10, input: 108.0 output: 1075.4891357421875\nkey: first_question * 10, input: 105.0 output: 1045.84521484375\nkey: fourth_question * 10, input: 1013.0 output: 10018.0546875\nkey: third_question * 5, input: 1000.0 output: 4962.61962890625\nkey: second_question * 5, input: 108.0 output: 538.472412109375\nkey: first_question * 5, input: 105.0 output: 523.5929565429688\nkey: fourth_question * 5, input: 1013.0 output: 5027.0966796875\n```\n### Multiple models in sequence\nThis section demonstrates how use inference with multiple models in sequence.\nIn a sequential pattern, data is sent to one or more models in sequence, with the output from each model chaining to the next model. The following list demonstrates the sequence used in this section.\n- Read the data from BigQuery.\n- Map the data.\n- Use RunInference with the multiply by 5 model.\n- Process the results.\n- Use RunInference with the multiply by 10 model.\n- Process the results.\n```\ndef process_interim_inference(element):\u00a0 \u00a0 key, prediction_result = element\u00a0 \u00a0 input_value = prediction_result.example\u00a0 \u00a0 inference = prediction_result.inference\u00a0 \u00a0 formatted_input_value = 'original input is `{} {}`'.format(key, input_value)\u00a0 \u00a0 return formatted_input_value, inferencepipeline_options = PipelineOptions().from_dictionary(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {'temp_location':f'gs://{bucket}/tmp'})pipeline = beam.Pipeline(options=pipeline_options)with pipeline as p:\u00a0 multiply_five = (\u00a0 \u00a0 \u00a0 p \u00a0 \u00a0 \u00a0 | beam.io.ReadFromBigQuery(table=table_spec) \u00a0 \u00a0 \u00a0 | \"CreateMultiplyFiveTuple\" >> beam.Map(lambda x: (x['key'], x['value']))\u00a0 \u00a0 \u00a0 | \"ConvertNumpyToTensorFiveTuple\" >> beam.Map(lambda x: (x[0], torch.Tensor([x[1]])))\u00a0 \u00a0 \u00a0 | \"RunInferenceTorchFiveTuple\" >> RunInference(keyed_torch_five_times_model_handler)\u00a0 )\u00a0 inference_result = (\u00a0 \u00a0 multiply_five \u00a0 \u00a0 \u00a0 | \"ExtractResult\" >> beam.Map(process_interim_inference) \u00a0 \u00a0 \u00a0 | \"RunInferenceTorchTenTuple\" >> RunInference(keyed_torch_ten_times_model_handler)\u00a0 \u00a0 \u00a0 | beam.ParDo(PredictionWithKeyProcessor())\u00a0 \u00a0 )\u00a0 inference_result | beam.Map(print)\n```\n```\nkey: original input is `third_question tensor([1000.])`, input: 4962.61962890625 output: 49045.37890625\nkey: original input is `second_question tensor([108.])`, input: 538.472412109375 output: 5329.11083984375\nkey: original input is `first_question tensor([105.])`, input: 523.5929565429688 output: 5182.08251953125\nkey: original input is `fourth_question tensor([1013.])`, input: 5027.0966796875 output: 49682.49609375\n```", "guide": "Dataflow"}