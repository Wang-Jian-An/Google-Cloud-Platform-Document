{"title": "Dataflow - Specify a network and subnetwork", "url": "https://cloud.google.com/dataflow/docs/guides/specifying-networks", "abstract": "# Dataflow - Specify a network and subnetwork\nThis document explains how to specify a network or a subnetwork or both options when you run Dataflow jobs.\nThis document requires that you know how to create Google Cloud networks and subnetworks. This document also requires your familiarity with the network terms discussed in the next section.\nThe `default` network has configurations that allow Dataflow jobs to run. However, other services might also use this network. Ensure that your changes to the `default` network are compatible with all of your services. Alternatively, create a separate network for Dataflow.\nFor more information about how to troubleshoot networking issues, see [Troubleshoot Dataflow networking issues](/dataflow/docs/guides/troubleshoot-networking) .\n", "content": "## Google Cloud network terminology\n- **VPC network.** A VPC network, sometimes called a , provides connectivity for resources in a project.To learn more about VPC, see [VPC network overview](/vpc/docs/vpc) .\n- **Shared VPC network.** A Shared VPC network is one that exists in a separate project, called a , within your organization. If a [Shared VPC Admin](/vpc/docs/shared-vpc#iam_roles_required_for_shared_vpc) has defined you as a [Service Project Admin](/vpc/docs/shared-vpc#svc_proj_admins) , you have permission to use at least some of the subnetworks in networks of the host project.To learn more about Shared VPC, see [Shared VPC overview](/vpc/docs/shared-vpc) .\n- **VPC Service Controls.** Dataflow VPC Service Controls help secure your pipeline's resources and services.To learn more about VPC Service Controls, see [VPC Service Controls overview](/vpc-service-controls/docs/overview) . To learn about the limitations when using Dataflow with VPC Service Controls, see [supported products and limitations](/vpc-service-controls/docs/supported-products) .## Network and subnetwork for a Dataflow job\nWhen you create a Dataflow job, you can specify a network, a subnetwork, or both options.\nConsider the following guidelines:\n- If you are unsure about which parameter to use, specify only the subnetwork parameter. The network parameter is then implicitly specified for you.\n- If you omit both the subnetwork and network parameters, Google Cloud assumes you intend to use an [auto mode VPC network](/vpc/docs/vpc#subnet-ranges) named `default` . If you don't have a network named `default` in your project, you must specify an alternate network or subnetwork.\n### Guidelines for specifying a network parameter\n- You can select an [auto mode VPC network](/vpc/docs/create-modify-vpc-networks#create-auto-network) in your project with the network parameter.\n- You can specify a network using only its name and not the complete URL.\n- You can only use the network parameter to select a Shared VPC network if both of the following conditions are true:- The Shared VPC network that you select is an auto mode VPC network.\n- You are a Service Project Admin with [project-level permissions](/vpc/docs/shared-vpc#svc_proj_admins) to the whole Shared VPC host project. A Shared VPC Admin has granted you the [Compute Network User role](/compute/docs/access/iam#compute.networkUser) for the whole host project, so you are able to use all of its networks and subnetworks.\n- For all other cases, you must specify a subnetwork.\n### Guidelines for specifying a subnetwork parameter\n- If you specify a subnetwork, Dataflow chooses the network for you. Therefore, when specifying a subnetwork, you can omit the network parameter.\n- To select a specific subnetwork in a network, use the subnetwork parameter.\n- Specify a subnetwork using either a complete URL or an abbreviated path. If the subnetwork is located in a Shared VPC network, you must use the complete URL.\n- You must select a subnetwork in the same region as the zone where you run your Dataflow workers. For example, you must specify the subnetwork parameter in the following situations:- The subnetwork you specify is in a custom mode VPC network.\n- You are a Service Project Admin with [subnet-level permissions](/vpc/docs/shared-vpc#svc_proj_admins) to a specific subnetwork in a Shared VPC host project.\n- The subnetwork size only limits the number of instances by number of available IP addresses. This sizing does not have impact on Dataflow VPC Service Controls performance.\n### Guidelines for specifying a subnetwork parameter for Shared VPC\n- When specifying the subnetwork URL for Shared VPC, ensure that is the project in which the VPC is hosted.\n- If the subnetwork is located in a Shared VPC network, you must use the complete URL.\n- Make sure the Shared VPC subnetwork is shared with the [Dataflow service account](/dataflow/docs/concepts/security-and-permissions#df-service-account) and has the [Compute Network User role](/compute/docs/access/iam#compute.networkUser) assigned on the specified subnet. The Compute Network User role must be assigned to the Dataflow service account in the host project.- In the Google Cloud console, go to the **Shared VPC** page and search for the subnet. In the **Shared with** column, you can see whether the VPC subnetwork is shared with the Dataflow service account.\n- If the network is not shared, the following error message appears: [Error: Message: Required 'compute.subnetworks.get' permission](/dataflow/docs/guides/troubleshoot-permissions#shared-VPC) .\n## Example network and subnetwork specifications\nExample of a complete URL that specifies a subnetwork:\n```\nhttps://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION_NAME/subnetworks/SUBNETWORK_NAME\n```\nReplace the following:\n- ``: the host project ID\n- ``: the [region](/dataflow/docs/resources/locations) of your Dataflow job\n- ``: the name of your Compute Engine subnetwork\nThe following is an example URL, where the host project ID is `my-cloud-project` , the region is `us-central1` , and the subnetwork name is `mysubnetwork` :\n```\n https://www.googleapis.com/compute/v1/projects/my-cloud-project/regions/us-central1/subnetworks/mysubnetwork\n```\nThe following is an example of a short form that specifies a subnetwork:\n```\nregions/REGION_NAME/subnetworks/SUBNETWORK_NAME\n```\nReplace the following:\n- ``: the [region](/dataflow/docs/resources/locations) of your Dataflow job\n- ``: the name of your Compute Engine subnetwork## Run your pipeline with the network specified\nIf you want to use a network other than the default network created by Google Cloud, in most cases, you need to [specify the subnetwork](#specify-subnet) . The network is automatically inferred from the subnetwork that you specify. For more information, see [Guidelines for specifying a network parameter](#network_parameter) in this document.\nThe following example shows how to run your pipeline from the command line or by using the REST API. The example specifies a network.\n```\nmvn compile exec:java \\\u00a0 \u00a0 -Dexec.mainClass=INPUT_PATH \\\u00a0 \u00a0 -Dexec.args=\"--project=HOST_PROJECT_ID \\\u00a0 \u00a0 \u00a0 \u00a0 --stagingLocation=gs://STORAGE_BUCKET/staging/ \\\u00a0 \u00a0 \u00a0 \u00a0 --output=gs://STORAGE_BUCKET/output \\\u00a0 \u00a0 \u00a0 \u00a0 --region=REGION \\\u00a0 \u00a0 \u00a0 \u00a0 --runner=DataflowRunner \\\u00a0 \u00a0 \u00a0 \u00a0 --network=NETWORK_NAME\"\n```\n```\npython -m INPUT_PATH \\\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --region=REGION \\\u00a0 \u00a0 --runner DataflowRunner \\\u00a0 \u00a0 --staging_location gs://STORAGE_BUCKET/staging \\\u00a0 \u00a0 --temp_location gs://STORAGE_BUCKET/temp \\\u00a0 \u00a0 --output gs://STORAGE_BUCKET/output \\\u00a0 \u00a0 --network NETWORK_NAME\n```\n```\nwordcount\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --region HOST_GCP_REGION \\\u00a0 \u00a0 --runner dataflow \\\u00a0 \u00a0 --staging_location gs://STORAGE_BUCKET/staging \\\u00a0 \u00a0 --temp_location gs://STORAGE_BUCKET/temp \\\u00a0 \u00a0 --input INPUT_PATH \\\u00a0 \u00a0 --output gs://STORAGE_BUCKET/output \\\u00a0 \u00a0 --network NETWORK_NAME\n```\nIf you're [running a Dataflow template](/dataflow/docs/templates/executing-templates#using-the-rest-api) by using the [REST API](/dataflow/docs/reference/rest/v1b3/projects.templates/launch) , add `network` or `subnetwork` , or both, to the `environment` object.\n```\nPOST https://dataflow.googleapis.com/v1b3/projects/[YOUR_PROJECT_ID]/templates:launch?gcsPath=gs://dataflow-templates/wordcount/template_file{\u00a0 \u00a0 \"jobName\": \"JOB_NAME\",\u00a0 \u00a0 \"parameters\": {\u00a0 \u00a0 \u00a0 \u00a0\"inputFile\" : \"INPUT_PATH\",\u00a0 \u00a0 \u00a0 \u00a0\"output\": \"gs://STORAGE_BUCKET/output\"\u00a0 \u00a0 },\u00a0 \u00a0 \"environment\": {\u00a0 \u00a0 \u00a0 \u00a0\"tempLocation\": \"gs://STORAGE_BUCKET/temp\",\u00a0 \u00a0 \u00a0 \u00a0\"network\": \"NETWORK_NAME\",\u00a0 \u00a0 \u00a0 \u00a0\"zone\": \"us-central1-f\"\u00a0 \u00a0 }}\n```\nReplace the following:\n- ``: the name of your Dataflow job (API only)\n- ``: the path to your source\n- ``: the host project ID\n- ``: a [Dataflow region](/dataflow/docs/resources/locations) , like`us-central1`\n- ``: the storage bucket\n- ``: the name of your Compute Engine network## Run your pipeline with the subnetwork specified\nIf you are a Service Project Admin who only has permission to use specific subnetworks in a Shared VPC network, you **must** specify the `subnetwork` parameter with a subnetwork that you have permission to use.\nThe following example shows how to run your pipeline from the command line or by using the REST API. The example specifies a subnetwork. You can also specify the network.\n```\nmvn compile exec:java \\\u00a0 \u00a0 -Dexec.mainClass=INPUT_PATH \\\u00a0 \u00a0 -Dexec.args=\"--project=HOST_PROJECT_ID \\\u00a0 \u00a0 \u00a0 \u00a0 --stagingLocation=gs://STORAGE_BUCKET/staging/ \\\u00a0 \u00a0 \u00a0 \u00a0 --output=gs://STORAGE_BUCKET/output \\\u00a0 \u00a0 \u00a0 \u00a0 --region=REGION \\\u00a0 \u00a0 \u00a0 \u00a0 --runner=DataflowRunner \\\u00a0 \u00a0 \u00a0 \u00a0 --subnetwork=https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK_NAME\"\n```\n```\npython -m INPUT_PATH \\\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --region=REGION \\\u00a0 \u00a0 --runner DataflowRunner \\\u00a0 \u00a0 --staging_location gs://STORAGE_BUCKET/staging \\\u00a0 \u00a0 --temp_location gs://STORAGE_BUCKET/temp \\\u00a0 \u00a0 --output gs://STORAGE_BUCKET/output \\\u00a0 \u00a0 --subnetwork https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK_NAME\n```\n```\nwordcount\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --region HOST_GCP_REGION \\\u00a0 \u00a0 --runner dataflow \\\u00a0 \u00a0 --staging_location gs://STORAGE_BUCKET/staging \\\u00a0 \u00a0 --temp_location gs://STORAGE_BUCKET/temp \\\u00a0 \u00a0 --input INPUT_PATH \\\u00a0 \u00a0 --output gs://STORAGE_BUCKET/output \\\u00a0 \u00a0 --subnetwork https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK_NAME\n```\nIf you're [running a Dataflow template](/dataflow/docs/templates/executing-templates#using-the-rest-api) using the [REST API](/dataflow/docs/reference/rest/v1b3/projects.templates/launch) , add `network` or `subnetwork` , or both, to the `environment` object.\n```\nPOST https://dataflow.googleapis.com/v1b3/projects/[YOUR_PROJECT_ID]/templates:launch?gcsPath=gs://dataflow-templates/wordcount/template_file{\u00a0 \u00a0 \"jobName\": \"JOB_NAME\",\u00a0 \u00a0 \"parameters\": {\u00a0 \u00a0 \u00a0 \u00a0\"inputFile\" : \"INPUT_PATH\",\u00a0 \u00a0 \u00a0 \u00a0\"output\": \"gs://STORAGE_BUCKET/output\"\u00a0 \u00a0 },\u00a0 \u00a0 \"environment\": {\u00a0 \u00a0 \u00a0 \u00a0\"tempLocation\": \"gs://STORAGE_BUCKET/temp\",\u00a0 \u00a0 \u00a0 \u00a0\"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/HOST_PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK_NAME\",\u00a0 \u00a0 \u00a0 \u00a0\"zone\": \"us-central1-f\"\u00a0 \u00a0 }}\n```\nReplace the following:\n- ``: the name of your Dataflow job (API only)\n- ``: the path to your source\n- ``: the host project ID\n- ``: a [Dataflow region](/dataflow/docs/resources/locations) , like`us-central1`\n- ``: the storage bucket\n- ``: the name of your Compute Engine subnetwork## Turn off an external IP address\nTo turn off an external IP address, see [Configure internet access and firewall rules](/dataflow/docs/guides/routes-firewall) .", "guide": "Dataflow"}