{"title": "Dataflow - Create a Dataflow pipeline using Go", "url": "https://cloud.google.com/dataflow/docs/quickstarts/create-pipeline-go", "abstract": "# Dataflow - Create a Dataflow pipeline using Go\n# Create a Dataflow pipeline using Go\nThis page shows you how to use the Apache Beam SDK for Go to build a program that defines a pipeline. Then, you run the pipeline locally and on the Dataflow service. For an introduction to the WordCount pipeline, see the [How to use WordCount in Apache Beam](https://www.youtube.com/watch?v=RTIOW1fIhkM) video.", "content": "## Before you begin- Grant roles to your Compute Engine default service account. Run the following command once  for each of the following IAM roles:- `roles/dataflow.admin`\n- `roles/dataflow.worker`\n- `roles/storage.objectAdmin`\n```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com\" --role=SERVICE_ACCOUNT_ROLE\n```- Replace``with your project ID.\n- Replace``with your project number.  To find your project number, see [Identify projects](/resource-manager/docs/creating-managing-projects#identifying_projects) or use the [gcloud projects describe](/sdk/gcloud/reference/projects/describe) command.\n- Replace``with each individual role.\n- Create a Cloud Storage bucket and configure it as follows:- Set the storage class to`S`(Standard).\n- Set the storage location to the following:`US`(United States).\n- Replace``with     a unique bucket name. Don't include sensitive information in the   bucket name because the bucket namespace is global and publicly visible.\n```\ngcloud storage buckets create gs://BUCKET_NAME --default-storage-class STANDARD --location US\n```\n- Copy the Google Cloud project ID and the Cloud Storage bucket name.  You need these values later in this quickstart.## Set up your development environmentThe Apache Beam SDK is an open source programming model for data pipelines. You define a pipeline with an Apache Beam program and then choose a runner, such as Dataflow, to run your pipeline.\nWe recommend that you use the latest version of Go when working with the Apache Beam SDK for Go. If you don't have the latest version of Go installed, use Go's [Download and install guide](https://go.dev/doc/install) to download and install Go for your specific operating system.\nTo verify the version of Go that you have installed, run the following command in your local terminal:\n```\ngo version\n```## Run the Beam wordcount exampleThe Apache Beam SDK for Go includes a [wordcount pipeline example](https://github.com/apache/beam/blob/master/sdks/go/examples/wordcount/wordcount.go) . The `wordcount` example does the following:- Reads a text file as input. By default, it reads a text file located in a Cloud Storage bucket with the resource name`gs://dataflow-samples/shakespeare/kinglear.txt`.\n- Parses each line into words.\n- Performs a frequency count on the tokenized words.\nTo run the latest version of the Beam `wordcount` example on your local machine, use the following command. The `input` flag specifies the file to read, and the `output` flag specifies the filename for the frequency count output.\n```\ngo run github.com/apache/beam/sdks/v2/go/examples/wordcount@latest \\\u00a0 --input gs://dataflow-samples/shakespeare/kinglear.txt \\\u00a0 --output outputs\n```\nAfter the pipeline completes, view the output results:\n```\nmore outputs*\n```\nTo exit, press .## Modify the pipeline codeThe Beam `wordcount` pipeline distinguishes between uppercase and lowercase words. The following steps show how to create your own Go module, modify the `wordcount` pipeline so that the pipeline is not case-sensitive, and run it on Dataflow.\n### Create a Go moduleTo make changes to the pipeline code, follow these steps.- Create a directory for your Go module in a location of your choice:```\nmkdir wordcountcd wordcount\n```\n- Create a Go module. For this example, use `example/dataflow` as the module path.```\ngo mod init example/dataflow\n```\n- Download the latest copy of the [wordcount code](https://github.com/apache/beam/blob/master/sdks/go/examples/wordcount/wordcount.go) from the Apache Beam GitHub repository. Put this file into the `wordcount` directory you created.\n- If you are using a non-Linux operating system, you must get the [Go unix package](https://pkg.go.dev/golang.org/x/sys/unix) . This package is required to run pipelines on the Dataflow service.```\ngo get -u golang.org/x/sys/unix\n```\n- Ensure that the `go.mod` file matches the module's source code:```\ngo mod tidy\n```\n### Run the unmodified pipelineVerify the unmodified `wordcount` pipeline runs locally.- From the terminal, build and run the pipeline locally:```\n\u00a0go run wordcount.go --input gs://dataflow-samples/shakespeare/kinglear.txt \\\u00a0 \u00a0 \u00a0--output outputs\n```\n- View the output results:```\n\u00a0more outputs*\n```\n- To exit, press .\n### Change the pipeline codeTo change the pipeline so that it is not case-sensitive, modify the code to apply the [strings.ToLower](https://pkg.go.dev/strings#ToLower) function to all words.- In an editor of your choice, open the `wordcount.go` file.\n- Examine the `init` block (comments have been removed for clarity):```\n\u00a0func init() {\u00a0 \u00a0register.DoFn3x0[context.Context, string, func(string)](&extractFn{})\u00a0 \u00a0register.Function2x1(formatFn)\u00a0 \u00a0register.Emitter1[string]()\u00a0}\n```\n- Add a new line to register the `strings.ToLower` function:```\n\u00a0func init() {\u00a0 \u00a0register.DoFn3x0[context.Context, string, func(string)](&extractFn{})\u00a0 \u00a0register.Function2x1(formatFn)\u00a0 \u00a0register.Emitter1[string]()\u00a0 \u00a0register.Function1x1(strings.ToLower)\u00a0}\n```\n- Examine the `CountWords` function:```\n\u00a0func CountWords(s beam.Scope, lines beam.PCollection) beam.PCollection {\u00a0 \u00a0s = s.Scope(\"CountWords\")\u00a0 \u00a0// Convert lines of text into individual words.\u00a0 \u00a0col := beam.ParDo(s, &extractFn{SmallWordLength: *smallWordLength}, lines)\u00a0 \u00a0// Count the number of times each word occurs.\u00a0 \u00a0return stats.Count(s, col)\u00a0}\n```\n- To lowercase the words, add a [ParDo](https://pkg.go.dev/github.com/apache/beam/sdks/go/pkg/beam#ParDo) that applies `strings.ToLower` to every word:```\n\u00a0func CountWords(s beam.Scope, lines beam.PCollection) beam.PCollection {\u00a0 \u00a0s = s.Scope(\"CountWords\")\u00a0 \u00a0// Convert lines of text into individual words.\u00a0 \u00a0col := beam.ParDo(s, &extractFn{SmallWordLength: *smallWordLength}, lines)\u00a0 \u00a0// Map all letters to lowercase.\u00a0 \u00a0lowercaseWords := beam.ParDo(s, strings.ToLower, col)\u00a0 \u00a0// Count the number of times each word occurs.\u00a0 \u00a0return stats.Count(s, lowercaseWords)\u00a0}\n```\n- Save the file.\n### Run the updated pipeline locallyRun your updated `wordcount` pipeline locally and verify the output has changed.- Build and run the modified `wordcount` pipeline:```\n\u00a0go run wordcount.go --input gs://dataflow-samples/shakespeare/kinglear.txt \\\u00a0 \u00a0 \u00a0--output outputs\n```\n- View the output results of the modified pipeline. All words should be lowercase.```\n\u00a0more outputs*\n```\n- To exit, press .\n [](None) \n### Run the pipeline on the Dataflow serviceTo run the updated `wordcount` example on the Dataflow service, use the following command:\n```\ngo run wordcount.go --input gs://dataflow-samples/shakespeare/kinglear.txt \\\u00a0 \u00a0 --output gs://BUCKET_NAME/results/outputs \\\u00a0 \u00a0 --runner dataflow \\\u00a0 \u00a0 --project PROJECT_ID \\\u00a0 \u00a0 --region DATAFLOW_REGION \\\u00a0 \u00a0 --staging_location gs://BUCKET_NAME/binaries/\n```\nReplace the following:- `` : the Cloud Storage bucket name.\n- `` : the Google Cloud project ID.\n- `` : the [region](/dataflow/docs/resources/locations) where you want to deploy the Dataflow job. For example, `europe-west1` . For a list of available locations, see [Dataflow locations](/dataflow/docs/resources/locations) . The `--region` flag overrides the default region that is set in the metadata server, your local client, or environment variables.\n **Note:** To specify a [user-managed worker service account](/dataflow/docs/concepts/security-and-permissions#user-managed) , include the `--service_account_email` [pipeline option](/dataflow/docs/reference/pipeline-options) . User-managed worker service accounts are recommended for production workloads. If you don't specify a worker service account when you create a job, Dataflow uses the [Compute Engine default service account](/dataflow/docs/concepts/security-and-permissions#default-service-account) .## View your resultsYou can see a list of your Dataflow jobs in Google Cloud console. In the Google Cloud console, go to the Dataflow **Jobs** page.\n [Go to Jobs](https://console.cloud.google.com/dataflow) \nThe **Jobs** page displays details of your `wordcount` job, including a status of **Running** at first, and then **Succeeded** .\nWhen you run a pipeline using Dataflow, your results are stored in a Cloud Storage bucket. View the output results by using either the Google Cloud console or the local terminal.\nTo view your results in the Google Cloud console, go to the Cloud Storage **Buckets** page.\n [Go to Buckets](https://console.cloud.google.com/storage/browser) \nFrom the list of buckets in your project, click the storage bucket that you created earlier. The output files that your job created are displayed in the `results` directory.\nView the results from your terminal or by using Cloud Shell.- To list the output files, use the [gcloud storage ls command](/sdk/gcloud/reference/storage/ls) :```\ngcloud storage ls gs://BUCKET_NAME/results/outputs* --long\n```Replace `` with the name of the specified output Cloud Storage bucket.\n- To view the results in the output files, use the [gcloud storage cat command](/sdk/gcloud/reference/storage/cat) :```\ngcloud storage cat gs://BUCKET_NAME/results/outputs*\n```\n## Clean upTo avoid incurring charges to your Google Cloud account for   the resources used on this page, delete the Google Cloud project with the   resources.\n **Note:** If you followed this quickstart in a new project, then you can [delete the project](/resource-manager/docs/creating-managing-projects#shutting_down_projects) .- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets](https://console.cloud.google.com/storage/browser) \n- Click the checkbox for the bucket that you want to delete.\n- To delete the bucket,  clickdelete **Delete** , and then follow the  instructions.\n- If you keep your project, revoke the roles that you granted to the Compute Engine default service account. Run the following command once for each of the following IAM roles:- `roles/dataflow.admin`\n- `roles/dataflow.worker`\n- `roles/storage.objectAdmin`\n```\ngcloud projects remove-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member=serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com \\\u00a0 \u00a0 --role=SERVICE_ACCOUNT_ROLE\n```\n- Optional: Revoke the authentication credentials that you created, and delete the local   credential file.```\ngcloud auth application-default revoke\n```\n- Optional: Revoke credentials from the gcloud CLI.```\ngcloud auth revoke\n```\n## What's next\n- [Programming model for Apache Beam](/dataflow/docs/concepts/beam-programming-model) \n- [Setting pipeline options](/dataflow/docs/guides/setting-pipeline-options) \n- [Pipeline options reference](/dataflow/docs/reference/pipeline-options) \n- [Deploying a pipeline](/dataflow/docs/guides/deploying-a-pipeline)", "guide": "Dataflow"}