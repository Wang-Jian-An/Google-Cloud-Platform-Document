{"title": "Dataflow - About Dataflow ML", "url": "https://cloud.google.com/dataflow/docs/machine-learning/ml-about", "abstract": "# Dataflow - About Dataflow ML\nYou can use Dataflow ML's scale data processing abilities for [prediction and inference pipelines](#prediction) and for [data preparation for training](#data-prep) .", "content": "## Requirements and limitations\n- Dataflow ML supports batch and streaming pipelines.\n- The`RunInference`API is supported in Apache Beam 2.40.0 and later versions.\n- The`MLTransform`API is supported in Apache Beam 2.53.0 and later versions.\n- Model handlers are available for PyTorch, scikit-learn, TensorFlow, ONNX, and TensorRT. For unsupported frameworks, you can use a custom model handler.## Data preparation for training\n- Use the `MLTransform` feature to prepare your data for training ML models. For more information, see [Preprocess data with MLTransform](/dataflow/docs/machine-learning/ml-preprocess-data) .\n- Use Dataflow with ML-OPS frameworks, such as [Kubeflow Pipelines](https://www.kubeflow.org/docs/components/pipelines/v1/introduction/) (KFP) or [TensorFlow Extended](https://www.tensorflow.org/tfx) (TFX). To learn more, see [Dataflow ML in ML workflows](/dataflow/docs/machine-learning/ml-data) .## Prediction and inference pipelines\nDataflow ML combines the power of Dataflow with Apache Beam's [RunInference API](https://beam.apache.org/documentation/ml/about-ml/) . With the `RunInference` API, you define the model's characteristics and properties and pass that configuration to the `RunInference` transform. This feature allows users to run the model within their Dataflow pipelines without needing to know the model's implementation details. You can choose the framework that best suits your data, such as TensorFlow and PyTorch.\n## Run multiple models in a pipeline\nUse the `RunInference` transform to add multiple inference models to your Dataflow pipeline. For more information, including code details, see [Multi-model pipelines](https://beam.apache.org/documentation/ml/about-ml/#multi-model-pipelines) in the Apache Beam documentation.\n## Use GPUs with Dataflow\nFor batch or streaming pipelines that require the use of accelerators, you can run Dataflow pipelines on NVIDIA GPU devices. For more information, see [Run a Dataflow pipeline with GPUs](/dataflow/docs/gpu/use-gpus) .\n## Troubleshoot Dataflow ML\nThis section provides troubleshooting strategies and links that you might find helpful when using Dataflow ML.\n### Stack expects each tensor to be equal size\nIf you provide images of different sizes or word embeddings of different lengths when using the `RunInference` API, the following error might occur:\n```\nFile \"/beam/sdks/python/apache_beam/ml/inference/pytorch_inference.py\", line 232, in run_inference batched_tensors = torch.stack(key_to_tensor_list[key]) RuntimeError: stack expects each tensor to be equal size, but got [12] at entry 0 and [10] at entry 1 [while running 'PyTorchRunInference/ParDo(_RunInferenceDoFn)']\n```\nThis error occurs because the `RunInference` API can't batch tensor elements of different sizes. For workarounds, see [Unable to batch tensor elements](https://beam.apache.org/documentation/ml/about-ml/#unable-to-batch-tensor-elements) in the Apache Beam documentation.\n## What's next\n- Explore the [use case notebooks](https://github.com/apache/beam/tree/master/examples/notebooks/beam-ml) .\n- Get in-depth information about using ML with Apache Beam in Apache Beam's [AI/ML pipelines](https://beam.apache.org/documentation/ml/overview/) documentation.\n- Learn more about the [RunInference API](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.html#apache_beam.ml.inference.RunInference) .\n- Learn about the [metrics](https://beam.apache.org/documentation/ml/runinference-metrics/) that you can use to monitor your`RunInference`transform.", "guide": "Dataflow"}