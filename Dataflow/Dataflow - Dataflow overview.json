{"title": "Dataflow - Dataflow overview", "url": "https://cloud.google.com/dataflow/docs/overview", "abstract": "# Dataflow - Dataflow overview\nDataflow is a Google Cloud service that provides unified stream and batch data processing at scale. Use Dataflow to create data pipelines that read from one or more sources, transform the data, and write the data to a destination.\nTypical use cases for Dataflow include the following:\n- Data movement: Ingesting data or replicating data across subsystems.\n- [ETL](/learn/what-is-etl) (extract-transform-load) workflows that ingest data into a data warehouse such as BigQuery.\n- Powering BI dashboards.\n- Applying ML in real time to streaming data.\n- Processing sensor data or log data at scale.\nDataflow uses the same programming model for both batch and stream analytics. Streaming pipelines can achieve very low latency. You can ingest, process, and analyze fluctuating volumes of real-time data. By default, Dataflow guarantees [exactly-once processing](/dataflow/docs/concepts/exactly-once) of every record. For streaming pipelines that can tolerate duplicates, you can often reduce cost and improve latency by enabling [at-least-once mode](/dataflow/docs/guides/streaming-modes) .\n", "content": "## Advantages of Dataflow\nThis section describes some of the advantages of using Dataflow.\n### Managed\nDataflow is a fully managed service. That means Google manages all of the resources needed to run Dataflow. When you run a Dataflow job, the Dataflow service allocates a pool of worker VMs to execute the pipeline. You don't need to provision or manage these VMs. When the job completes or is cancelled, Dataflow automatically deletes the VMs. You're billed for the compute resources that your job uses. For more information about costs, see [Dataflow pricing](/dataflow/pricing) .\n### Scalable\nDataflow is designed to support batch and streaming pipelines at large scale. Data is processed in parallel, so the work is distributed across multiple VMs.\nDataflow can autoscale by provisioning extra worker VMs, or by shutting down some worker VMs if fewer are needed. It also optimizes the work, based on the characteristics of the pipeline. For example, Dataflow can [dynamically rebalance work](/dataflow/docs/dynamic-work-rebalancing) among the VMs, so that parallel work completes more efficiently.\n### Portable\nDataflow is built on the open source [Apache Beam](https://beam.apache.org/) project. Apache Beam lets you write pipelines using a language-specific SDK. Apache Beam supports Java, Python, and Go SDKs, as well as [multi-language pipelines](https://beam.apache.org/documentation/programming-guide/#multi-language-pipelines) .\nDataflow executes Apache Beam pipelines. If you decide later to run your pipeline on a different platform, such as Apache Flink or Apache Spark, you can do so without rewriting the pipeline code.\n### Flexible\nYou can use Dataflow for relatively simple pipelines, such as moving data. However, it's also suitable for more advanced applications, such as real-time streaming analytics. A solution built on Dataflow can grow with your needs as you move from batch to streaming or encounter more advanced use cases.\nDataflow supports several different ways to create and execute pipelines, depending on your needs:\n- Write code using the Apache Beam SDKs.\n- Deploy a [Dataflow template](/dataflow/docs/concepts/dataflow-templates) . Templates let you run predefined pipelines. For example, a developer can create a template, and then a data scientist can deploy it on demand.Google also provides a [library](/dataflow/docs/guides/templates/provided-templates) of templates for common scenarios. You can deploy these templates without knowing any Apache Beam programming concepts.\n- Use [JupyterLab notebooks](/dataflow/docs/guides/interactive-pipeline-development) to develop and run pipelines iteratively.\n### Observable\nYou can monitor the status of your Dataflow jobs through the [Dataflow monitoring interface](/dataflow/docs/guides/monitoring-overview) in the Google Cloud console. The monitoring interface includes a graphical representation of your pipeline, showing the progress and [execution details](/dataflow/docs/concepts/execution-details) of each pipeline stage. The monitoring interface makes it easier to spot problems such as bottlenecks or high latency. You can also [profile](/dataflow/docs/guides/profiling-a-pipeline) your Dataflow jobs to monitor CPU usage and memory allocation.\n## How it works\nDataflow uses a data pipeline model, where data moves through a series of stages. Stages can include reading data from a source, transforming and aggregating the data, and writing the results to a destination.\nPipelines can range from very simple to more complex processing. For example, a pipeline might do the following:\n- Move data as-is to a destination.\n- Transform data to be more useable by the target system.\n- Aggregate, process, and enrich data for analysis.\n- Join data with other data.\nA pipeline that is defined in Apache Beam does not specify the pipeline is executed. Running the pipeline is the job of a [runner](https://beam.apache.org/documentation/basics/#runner) . The purpose of a runner is to run an Apache Beam pipeline on a specific platform. Apache Beam supports multiple runners, including a [Dataflow runner](https://beam.apache.org/documentation/runners/dataflow/) .\nTo use Dataflow with your Apache Beam pipelines, specify the Dataflow runner. The runner uploads your executable code and dependencies to a Cloud Storage bucket and creates a Dataflow . Dataflow then allocates a pool of VMs to execute the pipeline.\nThe following diagram shows a typical ETL and BI solution using Dataflow and other Google Cloud services:\nThis diagram shows the following stages:\n- Pub/Sub ingests data from an external system.\n- Dataflow reads the data from Pub/Sub and writes it to BigQuery. During this stage, Dataflow might transform or aggregate the data.\n- BigQuery acts as a data warehouse, allowing data analysts to run ad hoc queries on the data.\n- Looker provides real-time BI insights from the data stored in BigQuery.\nFor basic data movement scenarios, you might run a Google-provided template. Some templates support user-defined functions (UDFs) written in JavaScript. UDFs let you add custom processing logic to a template. For more complex pipelines, start with the Apache Beam SDK.\n## What's next\n- For more information about Apache Beam, see [Programming model for Apache Beam](/dataflow/docs/concepts/beam-programming-model) .\n- [Install the Apache Beam SDK](/dataflow/docs/guides/installing-beam-sdk) .\n- Create your first pipeline by following the [Java quickstart](/dataflow/docs/quickstarts/create-pipeline-java) , [Python quickstart](/dataflow/docs/quickstarts/create-pipeline-python) , or [Go quickstart](/dataflow/docs/quickstarts/create-pipeline-go) .\n- Learn about Dataflow templates by [creating a streaming pipeline using a Dataflow template](/dataflow/docs/quickstarts/create-streaming-pipeline-template) .", "guide": "Dataflow"}