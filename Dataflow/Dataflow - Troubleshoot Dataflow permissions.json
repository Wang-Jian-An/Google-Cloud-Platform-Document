{"title": "Dataflow - Troubleshoot Dataflow permissions", "url": "https://cloud.google.com/dataflow/docs/guides/troubleshoot-permissions", "abstract": "# Dataflow - Troubleshoot Dataflow permissions\nThis page shows you how to investigate and resolve issues with [Dataflow permissions](/dataflow/docs/concepts/security-and-permissions) .\nTo successfully run Dataflow jobs, your user account and the Dataflow service accounts must have the necessary access to resources. For a list of required roles and steps for granting these roles, see [Security and permissions for pipelines on Google Cloud](/dataflow/docs/concepts/security-and-permissions#permissions) on the Dataflow security and permissions page.\nIn addition, when your Apache Beam pipelines access Google Cloud resources, your Dataflow project worker service account needs access to the resources. For a list of roles that your worker service account might need, see [Example role assignment](/dataflow/docs/concepts/access-control#example) .\nWhen one or more roles required for running a job is missing, an error might appear in the job logs or in the worker logs. For instructions explaining how to find errors when a job fails, see [Find information about pipeline failures](/dataflow/docs/guides/troubleshooting-your-pipeline#Workflow) .\nTo resolve permissions issues, you need to understand which permission is missing and which account needs to have that permission. To understand which permission is missing, look at the permission listed in the error message and find the role that contains that permission. Often, but not always, you need to assign the relevant role to the [Dataflow worker service account](/dataflow/docs/concepts/security-and-permissions#worker-service-account) .\nTo add permissions, your user account needs to be allowed to manage access. For more information, see [Manage access to service accounts](/iam/docs/manage-access-service-accounts) and [Manage access to other resources](/iam/docs/manage-access-other-resources) .\n", "content": "## User does not have write access to project\nWhen you try to run a Dataflow job, the job fails and you see an error similar to the following:\n```\nPERMISSION_DENIED: (Could not create workflow; user does not have write access to project: $PROJECT_ID Causes: (...): Permission 'dataflow.jobs.create' denied on project: '$PROJECT_ID'\n```\nThis error occurs when your user account doesn't have the `roles/dataflow.developer` role.\nTo resolve this issue, grant your user account the `roles/dataflow.developer` role. In addition, make sure that your user account has the `roles/iam.serviceAccountUser` role. For more information, see [Grant a single role](/iam/docs/manage-access-service-accounts#grant-single-role) in the Identity and Access Management documentation.\n## User does not have sufficient permissions on project\nWhen you try to cancel a Dataflow job, you see an error similar to the following:\n```\nCould not cancel workflow; user does not have sufficient permissions on project:PROJECT_ID, or the job does not exist in the project. Causes: (...): Permission 'dataflow.jobs.cancel' denied on project: 'PROJECT_ID' Please ensure you have permission to access the job\n```\nSimilar errors might occur when trying to drain or update a job.\nThis error occurs for one of the following reasons:\n- Your user account doesn't have the`roles/dataflow.developer`role. To resolve this issue, grant your user account the`roles/dataflow.developer`role. In addition, make sure that your user account has the`roles/iam.serviceAccountUser`role. For more information, see [Grant a single role](/iam/docs/manage-access-service-accounts#grant-single-role) in the Identity and Access Management documentation.\n- The job ID is incorrect. It might contain a typo, or you might be using the job name to [cancel the job](https://cloud.google.com/sdk/gcloud/reference/dataflow/jobs/cancel) instead of the job ID.## Permissions verification for worker service account failed\nWhen you try to run a Dataflow job, you see an error similar to the following:\n```\nWorkflow failed. Causes: Permissions verification for controller service account failed. All permissions in IAM role roles/dataflow.worker should be granted to controller service account PROJECT_NUMBER-compute@developer.gserviceaccount.com.\n```\nThis error occurs when the worker service account doesn't have the `roles/dataflow.worker` role.\nTo resolve this issue, grant the worker service account the `roles/dataflow.worker` role. For more information, see [Grant a single role](/iam/docs/manage-access-service-accounts#grant-single-role) in the Identity and Access Management documentation.\n## Pipeline validation failed\nBefore a new Dataflow job launches, Dataflow performs validation checks on the pipeline. When the validation checks find problems with the pipeline, to save time and compute resources, Dataflow fails the job submission early. In the [job logs](/dataflow/docs/guides/logging#MonitoringLogs) , Dataflow includes log messages that contain the validation findings and instructions for resolving the issues.\nWhen the pipeline validation check finds permission issues, you might see the following error in the job logs:\n```\n[The preflight pipeline validation failed for job JOB_ID.] Missing permissions\nPERMISSION when accessing RESOURCE_PATH as Dataflow worker service account WORKER_SERVICE_ACCOUNT.\n```\nIf permissions are missing for more than one resource, the job logs contain multiple permission error messages.\nBefore attempting to resubmit your Dataflow job, fix the permission issues. The following resources provide information about modifying roles and permissions.\n- If you need to find a role that gives a specific permission, refer to the [IAM permissions reference](/iam/docs/permissions-reference) .\n- To grant a role to a principal for a project, see [Grant an IAM role by using the Google Cloud console](/iam/docs/grant-role-console) .\n- To add a missing permission for a project, IAM administrators can use the [gcloud projects add-iam-policy-binding command](/sdk/gcloud/reference/projects/add-iam-policy-binding) :```\ngcloud projects add-iam-policy-binding PROJECT_ID \\\n --member=PRINCIPAL --role=ROLE\n```\n- To grant or change specific roles, see [Grant or revoke roles](/iam/docs/granting-changing-revoking-access#single-role) .\n- For information about required roles and permissions for the Dataflow worker service account, see [Worker service account](/dataflow/docs/concepts/security-and-permissions#worker-service-account) in the Dataflow security and permissions page.\nIf you want to override the pipeline validation and launch your job with validation errors, use the following pipeline option:\n```\n--experiment=enable_ppv_effect=false\n```\n## There was a problem refreshing your credentials\nWhen you try to run a Dataflow job, you see an error similar to the following:\n```\nWorkflow failed. Causes: There was a problem refreshing your credentials.\nPlease check: 1. The Dataflow API is enabled for your project.\n2. Make sure both the Dataflow service account and the controller service account have sufficient permissions.\nIf you are not specifying a controller service account, ensure the default Compute Engine service account PROJECT_NUMBER-compute@developer.gserviceaccount.com exists and has sufficient permissions.\nIf you have deleted the default Compute Engine service account, you must specify a controller service account\n```\nThis error occurs when the worker service account doesn't have the `roles/dataflow.worker` role or when the Dataflow API isn't enabled.\nFirst verify that the worker service account the `roles/dataflow.worker` role. If needed, grant the `roles/dataflow.worker` to the worker service account. For more information, see [Grant a single role](/iam/docs/manage-access-service-accounts#grant-single-role) in the Identity and Access Management documentation.\nTo enable the Dataflow API, see [Enabling an API in your Google Cloud project](/endpoints/docs/openapi/enable-api) .\n## Required 'compute.subnetworks.get' permission\nWhen you try to run a Dataflow job on a Shared VPC network, you see an error similar to the following:\n```\nRequired 'compute.subnetworks.get' permission for 'projects/project-id/regions/region/subnetworks/subnet-name' HTTP Code: 403\n```\nShared VPC lets you export subnets from a [VPC network](/vpc/docs/vpc) in a to other in the same [organization](/resource-manager/docs/creating-managing-organization) . Instances in the service projects can have network connections in the shared subnets of the host project. For more information, see [Shared VPC overview](/vpc/docs/shared-vpc) .\nTo resolve this issue, first verify that the service project is attached to the host project. For more information, see [Attach service projects](/vpc/docs/provisioning-shared-vpc) in the Provisioning Shared VPC page.\nNext, grant the following roles to the Compute Engine service account of the host project, the Dataflow worker service account of the service project, and the service account used to submit the job:\n- `roles/dataflow.admin`\n- `roles/dataflow.serviceAgent`\n- [roles/compute.networkUser](/compute/docs/access/iam#compute.networkUser) \n- [roles/storage.objectViewer](/storage/docs/access-control/iam-roles) \nFor more information, see [Grant a single role](/iam/docs/manage-access-service-accounts#grant-single-role) in the Identity and Access Management documentation.\n## Dataflow runner does not have access to bucket\nWhen you try to list objects in a Cloud Storage bucket, the Dataflow job fails, and you see an error similar to the following:\n```\n\"dataflow-runner@project-id.iam.gserviceaccount.com\" does not have `storage.objects.list` access to the Google Cloud Storage Bucket\n```\nThis error occurs when the worker service account doesn't have the `roles/storage.objectViewer` role.\nTo resolve this issue, grant your user account account the `roles/storage.objectViewer` role. For more information, see [Grant a single role](/iam/docs/manage-access-service-accounts#grant-single-role) in the Identity and Access Management documentation.\n## Cloud KMS key permission denied on resource\nWhen you're [using customer-managed encryption keys](/dataflow/docs/guides/customer-managed-encryption-keys) and try to create a Dataflow job, the job fails, and you see an error similar to the following:\n```\nCloud KMS key permission 'cloudkms.cryptoKeyVersions.useToEncrypt' denied on resource\n'projects/project-id/locations/location/keyRings/keyRingName/cryptoKeys/keyname' (or it may not exist). cannot be validated.\nPlease confirm the full key path is used (starts with projects) and that there are no typos.\n```\nThis error occurs when the worker service account and the Dataflow service account don't have the `roles/cloudkms.cryptoKeyEncrypterDecrypter` [role](/kms/docs/reference/permissions-and-roles#predefined_roles) .\nTo resolve this issue, grant the `roles/cloudkms.cryptoKeyEncrypterDecrypter` role to the worker service account and to the Dataflow service account. For more information, see [Granting Encrypter/Decrypter permissions](/dataflow/docs/guides/customer-managed-encryption-keys#granting_encrypterdecrypter_permissions) in the Using customer-managed encryption keys page.\n## Permission denied on resource\nWhen you try to create a pipeline, the pipeline fails with the following error:\n```\nPermission 'datapipelines.pipelines.create' denied on resource '//datapipelines.googleapis.com/projects/PROJECT_ID/locations/REGION' (or it may not exist).\n```\nThis error occurs if the worker service account of your project doesn't have access to the files and other resources associated with the pipeline.\nTo resolve this issue, assign the following roles to the worker service account:\n- `roles/dataflow.admin`\n- `roles/dataflow.worker`\nFor more information, see [Worker service account](/dataflow/docs/concepts/security-and-permissions#worker-service-account) in \"Dataflow security and permissions.\"\n## Workflow failed\nWhen you're [using customer-managed encryption keys](/dataflow/docs/guides/customer-managed-encryption-keys) and try to create a Dataflow job, the job fails with the following error:\n```\nWorkflow failed\n```\nThis error can occur for the following reasons:\n- The key and the Dataflow job aren't in the same region, or a multi-regional key is used. Global and multi-regional keys are not supported. The region for your CMEK and the [region](/dataflow/docs/resources/locations) for your Dataflow job must be the same.\n- The key name is not specified correctly. The key might not exist, or the name might have a typo.## Cloud KMS key can't protect resources for this job\nWhen you're running a Dataflow job and trying to enable a [customer-managed encryption key](/dataflow/docs/guides/customer-managed-encryption-keys) , the job fails, and you see an error similar to the following:\n```\nCloud KMS key can't protect resources for this job. Please make sure the KMS key's region matches the Dataflow region\n```\nThis error can occur for the following reasons:\n- The key and the Dataflow job aren't in the same region, or a multi-regional key is used. Global and multi-regional keys are not supported. The region for your CMEK and the [region](/dataflow/docs/resources/locations) for your Dataflow job must be the same.\n- The [dataflowKMSKey](/dataflow/docs/reference/pipeline-options#security_and_networking) parameter is not specified correctly.## Vertical Autoscaling not working\nWhen you're using [Vertical Autoscaling](/dataflow/docs/vertical-autoscaling) , the job doesn't automatically scale vertically, and the following error appears in the job logs:\n```\n{\"level\":\"error\",\"ts\":1708815877.1246133,\"caller\":\"exporter/exporter.go:232\",\"msg\":\"failed to get response from UAS: %v\",\"error\":\"rpc error: code = PermissionDenied desc = The caller does not have permission\",\"stacktrace\":\"google3/autoscaler/vitor/external/go/exporter/exporter.receiver\\n\\tautoscaler/vitor/external/go/exporter/exporter.go:232\"}\n```\nThis error occurs when the worker service account doesn't have the Dataflow Worker ( `roles/dataflow.worker` ) role.\nTo resolve this issue, grant the worker service account the `roles/dataflow.worker` role. For more information, see [Grant a single role](/iam/docs/manage-access-service-accounts#grant-single-role) in the Identity and Access Management documentation.\nIf you're using a custom role for the worker service account, add the following permissions to the custom role:\n- `autoscaling.sites.readRecommendations`\n- `autoscaling.sites.writeMetrics`\n- `autoscaling.sites.writeState`", "guide": "Dataflow"}