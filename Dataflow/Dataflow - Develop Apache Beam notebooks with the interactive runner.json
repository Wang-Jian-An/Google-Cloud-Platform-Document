{"title": "Dataflow - Develop Apache Beam notebooks with the interactive runner", "url": "https://cloud.google.com/dataflow/docs/guides/interactive-pipeline-development", "abstract": "# Dataflow - Develop Apache Beam notebooks with the interactive runner\nUse the Apache Beam interactive runner with JupyterLab notebooks to complete the following tasks:\n- Iteratively develop pipelines.\n- Inspect your pipeline graph.\n- Parse individual`PCollections`in a read-eval-print-loop (REPL) workflow.\nThese Apache Beam notebooks are made available through [Vertex AI Workbench user-managed notebooks](/vertex-ai/docs/workbench/user-managed) , a service that hosts notebook virtual machines pre-installed with the latest data science and machine learning frameworks.\nThis guide focuses on the functionality introduced by Apache Beam notebooks, but doesn't show how to build a notebook. For more information about Apache Beam, see the [Apache Beam programming guide](https://beam.apache.org/documentation/programming-guide/) .\n", "content": "## Support and limitations\n- Apache Beam notebooks only support Python.\n- Apache Beam pipeline segments running in these notebooks are run in a test environment, and not against a production Apache Beam runner. To launch the notebooks on the Dataflow service, export the pipelines created in your Apache Beam notebook. For more details, see [Launch Dataflow jobs from a pipeline created in your notebook](#launch-jobs-from-pipeline) .## Before you begin\nBefore creating your Apache Beam notebook instance, enable additional APIs for pipelines that use other services, such as Pub/Sub.\nIf not specified, the notebook instance is executed by the [default Compute Engine service account](/compute/docs/access/service-accounts#default_service_account) with the IAM project editor role. If the project explicitly limits the roles of the service account, make sure it still has enough authorization to run your notebooks. For example, reading from a Pub/Sub topic implicitly creates a subscription, and your service account needs an IAM Pub/Sub editor role. By contrast, reading from a Pub/Sub subscription only requires an IAM Pub/Sub subscriber role.\nWhen you finish this guide, to avoid continued billing, delete the resources you created. For more details, see [Cleaning up](#clean-up) .\n## Launch an Apache Beam notebook instance\n- In the Google Cloud console, go to the Dataflow **Workbench** page. [Go to workbench](https://console.cloud.google.com/dataflow/workbench) \n- Make sure that you're on the **User-managed notebooks** tab.\n- In the toolbar, click **add Create new** .\n- In the **Environment** section, for **Environment** , select **Apache Beam** .\n- Optional: If you want to run notebooks on a GPU, in the **Machine type** section, select a machine type that supports GPUs and then select **Install NVIDIA GPU driver automatically for me** . For more information, see [GPU platforms](/compute/docs/gpus) .\n- In the **Networking** section, select a subnetwork for the notebook VM.\n- Optional: If you want to set up a custom notebook instance, see [Create a user-managed notebooks instance with specific properties](/vertex-ai/docs/workbench/user-managed/create-new#create-with-options) .\n- Click **Create** . Dataflow Workbench creates a new Apache Beam notebook instance.\n- After the notebook instance is created, the **Open JupyterLab** link becomes active. Click **Open JupyterLab** .\n### Optional: Install dependencies\nApache Beam notebooks already come with Apache Beam and Google Cloud connector dependencies installed. If your pipeline contains custom connectors or custom `PTransforms` that depend on third-party libraries, install them after you create a notebook instance. For more information, see [Install dependencies](/vertex-ai/docs/workbench/user-managed/dependencies) in the user-managed notebooks documentation.\n## Get started with Apache Beam notebooks\nAfter opening a user-managed notebooks instance, example notebooks are available in the **Examples** folder. The following notebooks are available:\n- Word Count\n- Streaming Word Count\n- Streaming NYC Taxi Ride Data\n- Apache Beam SQL in notebooks with comparisons to pipelines\n- Apache Beam SQL in notebooks with the Dataflow Runner\n- Apache Beam SQL in notebooks\n- Dataflow Word Count\n- Interactive Flink at Scale\n- RunInference\n- Use GPUs with Apache Beam\n- Visualize Data\nYou can find additional tutorials explaining the fundamentals of Apache Beam in the **Tutorials** folder. The following tutorials are available:\n- Basic Operations\n- Element Wise Operations\n- Aggregations\n- Windows\n- I/O Operations\n- Streaming\n- Final Exercises\nThese notebooks include explanatory text and commented code blocks to help you understand Apache Beam concepts and API usage. The tutorials also provide hands-on exercises for you to practice concepts.\nThe following sections use example code from the Streaming Word Count notebook. The code snippets in this guide and what is found in the Streaming Word Count notebook might have minor discrepancies.\n### Create a notebook instance\nNavigate to **File > New > Notebook** and select a kernel that is Apache Beam 2.22 or later.\nApache Beam notebooks are built against the master branch of the Apache Beam SDK. This means that the latest version of the kernel shown in the notebooks UI might be ahead of the most recently released version of the SDK.\nApache Beam is installed on your notebook instance, so include the `interactive_runner` and `interactive_beam` modules in your notebook.\n```\nimport apache_beam as beamfrom apache_beam.runners.interactive.interactive_runner import InteractiveRunnerimport apache_beam.runners.interactive.interactive_beam as ib\n```\nIf your notebook uses other Google APIs, add the following import statements:\n```\nfrom apache_beam.options import pipeline_optionsfrom apache_beam.options.pipeline_options import GoogleCloudOptionsimport google.auth\n```\n### Set interactivity options\nThe following line sets the amount of time the InteractiveRunner records data from an unbounded source. In this example, the duration is set to 10 minutes.\n```\nib.options.recording_duration = '10m'\n```\nYou can also change the recording size limit (in bytes) for an unbounded source by using the `recording_size_limit` property.\n```\n# Set the recording size limit to 1 GB.ib.options.recording_size_limit = 1e9\n```\n**Note:** When changing either the recording duration or size, call [stop](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/interactive/interactive_beam.py#L265) , [clear](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/interactive/interactive_beam.py#L249) , and [record](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/interactive/interactive_beam.py#L274) after changing values to allow new data to be recorded for the updated duration or size limit.\nFor additional interactive options, see the [interactive_beam.options class](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/interactive/interactive_beam.py#L53) .\n### Create your pipeline\nInitialize the pipeline using an `InteractiveRunner` object.\n```\noptions = pipeline_options.PipelineOptions()# Set the pipeline mode to stream the data from Pub/Sub.options.view_as(pipeline_options.StandardOptions).streaming = True# Set the project to the default project in your current Google Cloud environment.# The project is used to create a subscription to the Pub/Sub topic._, options.view_as(GoogleCloudOptions).project = google.auth.default()p = beam.Pipeline(InteractiveRunner(), options=options)\n```\n### Read and visualize the data\nThe following example shows an Apache Beam pipeline that creates a subscription to the given Pub/Sub topic and reads from the subscription.\n```\nwords = p\u00a0 \u00a0 | \"read\" >> beam.io.ReadFromPubSub(topic=\"projects/pubsub-public-data/topics/shakespeare-kinglear\")\n```\nThe pipeline counts the words by windows from the source. It creates fixed windowing with each window being 10 seconds in duration.\n```\nwindowed_words = (words\u00a0 \u00a0| \"window\" >> beam.WindowInto(beam.window.FixedWindows(10)))\n```\nAfter the data is windowed, the words are counted by window.\n```\nwindowed_word_counts = (windowed_words\u00a0 \u00a0| \"count\" >> beam.combiners.Count.PerElement())\n```\nThe `show()` method visualizes the resulting PCollection in the notebook.\n```\nib.show(windowed_word_counts, include_window_info=True)\n```\nYou can scope the result set back from `show()` by setting two optional parameters: `n` and `duration` .\n- Set`n`to limit the result set to show at most`n`number of elements, such as 20. If`n`is not set, the default behavior is to list the most recent elements captured until the source recording is over.\n- Set`duration`to limit the result set to a specified number of seconds worth of data starting from the beginning of the source recording. If`duration`isn't set, the default behavior is to list all elements until the recording is over.\nIf both optional parameters are set, `show()` stops whenever either threshold is met. In the following example, `show()` returns at most 20 elements that are computed based on the first 30 seconds worth of data from the recorded sources.\n```\nib.show(windowed_word_counts, include_window_info=True, n=20, duration=30)\n```\nTo display visualizations of your data, pass `visualize_data=True` into the `show()` method. You can apply multiple filters to your visualizations. The following visualization allows you to filter by label and axis:\nTo ensure replayability while prototyping streaming pipelines, the `show()` method calls reuse the captured data by default. To change this behavior and have the `show()` method always fetch new data, set `interactive_beam.options.enable_capture_replay = False` . Also, if you add a second unbounded source to your notebook, the data from the previous unbounded source is discarded.\nAnother useful visualization in Apache Beam notebooks is a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) . The following example first converts the words to lowercase and then computes the frequency of each word.\n```\nwindowed_lower_word_counts = (windowed_words\u00a0 \u00a0| beam.Map(lambda word: word.lower())\u00a0 \u00a0| \"count\" >> beam.combiners.Count.PerElement())\n```\nThe `collect()` method provides the output in a Pandas DataFrame.\n```\nib.collect(windowed_lower_word_counts, include_window_info=True)\n```\nEditing and rerunning a cell is a common practice in notebook development. When you edit and rerun a cell in a Apache Beam notebook, the cell doesn't undo the intended action of the code in the original cell. For example, if a cell adds a `PTransform` to a pipeline, rerunning that cell adds an additional `PTransform` to the pipeline. If you want to clear the state, restart the kernel, and then rerun the cells.\nYou might find it distracting to introspect the data of a `PCollection` by constantly calling `show()` and `collect()` , especially when the output takes up a lot of the space on your screen and makes it hard to navigate through the notebook. You might also want to compare multiple `PCollections` side by side to validate if a transform works as intended. For example, when one `PCollection` goes through a transform and produces the other. For these use cases, the Interactive Beam inspector is a convenient solution.\nInteractive Beam inspector is provided as a JupyterLab extension [apache-beam-jupyterlab-sidepanel](https://www.npmjs.com/package/apache-beam-jupyterlab-sidepanel) preinstalled in the Apache Beam notebook. With the extension, you can interactively inspect the state of pipelines and data associated with each `PCollection` without explicitly invoking `show()` or `collect()` .\nThere are 3 ways to open the inspector:\n- Click `Interactive Beam` on the top menu bar of JupyterLab. In the dropdown, locate `Open Inspector` , and click it to open the inspector. \n- Use the launcher page. If there is no launcher page opened, click `File` -> `New Launcher` to open it. On the launcher page, locate `Interactive Beam` and click `Open Inspector` to open the inspector. \n- Use the command palette. On the JupyterLab menu bar, click `View` > `Activate Command Palette` . In the dialog, search for `Interactive Beam` to list all options of the extension. Click `Open Inspector` to open the inspector. \nWhen the inspector is about to open:\n- If there is exactly one notebook open, the inspector automatically connects to it.\n- If no notebook is open, a dialog appears that lets you select a kernel.\n- If multiple notebooks are open, a dialog appears that lets you select the notebook session. \nIt's recommended to open at least one notebook and select a kernel for it before opening the inspector. If you open an inspector with a kernel before opening any notebook, later when you open a notebook to connect to the inspector, you have to select the `Interactive Beam Inspector Session` from `Use Kernel from Preferred Session` . An inspector and a notebook are connected when they share the same session, not different sessions created from the same kernel. Selecting the same kernel from `Start Preferred Kernel` creates a new session that is independent from existing sessions of opened notebooks or inspectors.\nYou can open multiple inspectors for an opened notebook and arrange the inspectors by dragging and dropping their tabs freely in the workspace.\nThe inspector page automatically refreshes when you run cells in the notebook. The page lists pipelines and `PCollections` defined in the connected notebook. `PCollections` are organized by the pipelines they belong to, and you can collapse them by clicking the header pipeline.\nFor the items in the pipelines and `PCollections` list, on click, the inspector renders corresponding visualizations on the right side:\n- If it's a `PCollection` , the inspector renders its data (dynamically if the data is still coming in for unbounded `PCollections` ) with additional widgets to tune the visualization after clicking the `APPLY` button. Because the inspector and the opened notebook share the same kernel session, they block each other from running. For example, if the notebook is busy running code, the inspector does not update until the notebook completes that execution. Conversely, if you want to run code immediately in your notebook while the inspector is visualizing a `PCollection` dynamically, you have to click the `STOP` button to stop the visualization and preemptively release the kernel to the notebook.\n- If it's a pipeline, the inspector displays the pipeline graph. \nYou might notice anonymous pipelines. Those pipelines have `PCollections` that you can access, but they are no longer referenced by the main session. For example:\n```\np = beam.Pipeline()pcoll = p | beam.Create([1, 2, 3])p = beam.Pipeline()\n```\nThe previous example creates an empty pipeline `p` and an anonymous pipeline that contains one `PCollection` `pcoll` . You can access the anonymous pipeline by using `pcoll.pipeline` .\nYou can toggle the pipeline and `PCollection` list to save space for big visualizations.\nIn addition to visualizations, you can also inspect the recording status for one or all pipelines in your notebook instance by calling [describe](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/runners/interactive/interactive_beam.py#L230) .\n```\n# Return the recording status of a specific pipeline. Leave the parameter list empty to return# the recording status of all pipelines.ib.recordings.describe(p)\n```\nThe `describe()` method provides the following details:\n- Total size (in bytes) of all of the recordings for the pipeline on disk\n- Start time of when the background recording job started (in seconds from Unix epoch)\n- Current pipeline status of the background recording job\n- Python variable for the pipeline\n### Launch Dataflow jobs from a pipeline created in your notebook\n- Optional: Before using your notebook to run Dataflow jobs, restart the kernel, rerun all cells, and verify the output. If you skip this step, hidden states in the notebook might affect the job graph in the pipeline object.\n- [Enable the Dataflow API](https://console.cloud.google.com/apis/library/dataflow.googleapis.com) .\n- Add the following import statement:```\nfrom apache_beam.runners import DataflowRunner\n```\n- Pass in your [pipelineoptions](/dataflow/docs/guides/specifying-exec-params) .```\n# Set up Apache Beam pipeline options.options = pipeline_options.PipelineOptions()# Set the project to the default project in your current Google Cloud# environment._, options.view_as(GoogleCloudOptions).project = google.auth.default()# Set the Google Cloud region to run Dataflow.options.view_as(GoogleCloudOptions).region = 'us-central1'# Choose a Cloud Storage location.dataflow_gcs_location = 'gs://<change me>/dataflow'# Set the staging location. This location is used to stage the# Dataflow pipeline and SDK binary.options.view_as(GoogleCloudOptions).staging_location = '%s/staging' % dataflow_gcs_location# Set the temporary location. This location is used to store temporary files# or intermediate results before outputting to the sink.options.view_as(GoogleCloudOptions).temp_location = '%s/temp' % dataflow_gcs_location# If and only if you are using Apache Beam SDK built from source code, set# the SDK location. This is used by Dataflow to locate the SDK# needed to run the pipeline.options.view_as(pipeline_options.SetupOptions).sdk_location = (\u00a0 \u00a0 '/root/apache-beam-custom/packages/beam/sdks/python/dist/apache-beam-%s0.tar.gz' %\u00a0 \u00a0 beam.version.__version__)\n```You can adjust the parameter values. For example, you can change the `region` value from `us-central1` .\n- Run the pipeline with `DataflowRunner` . This step runs your job on the Dataflow service.```\nrunner = DataflowRunner()runner.run_pipeline(p, options=options)\n````p` is a pipeline object from [Creating yourpipeline](#creating_your_pipeline) .\nFor an example about how to perform this conversion on an interactive notebook, see the Dataflow Word Count notebook in your notebook instance.\nAlternatively, you can export your notebook as an executable script, modify the generated `.py` file using the previous steps, and then [deploy yourpipeline](/dataflow/docs/guides/deploying-a-pipeline) to the Dataflow service.\n## Save your notebook\nNotebooks you create are saved locally in your running notebook instance. If you [reset](/compute/docs/instances/stop-start-instance#resetting_an_instance) or shut down the notebook instance during development, those new notebooks are persisted as long as they are created under the `/home/jupyter` directory. However, if a notebook instance is deleted, those notebooks are also deleted.\nTo keep your notebooks for future use, download them locally to your workstation, [save them to GitHub](/vertex-ai/docs/workbench/user-managed/save-to-github) , or export them to a different file format.\n### Save your notebook to additional persistent disks\nIf you want to keep your work such as notebooks and scripts throughout various notebook instances, store them in Persistent Disk.\n- Create or attach a [Persistent Disk](/compute/docs/disks/add-persistent-disk) . Follow the instructions to use `ssh` to connect to the VM of the notebook instance and issue commands in the opened Cloud Shell. **Warning:** Do not issue commands from terminals inside of the notebook instance.\n- Note the directory where the Persistent Disk is mounted, for example, `/mnt/myDisk` .\n- Edit the VM details of the notebook instance to add an entry to the `Custom metadata` : key - `container-custom-params` ; value - `-v /mnt/myDisk:/mnt/myDisk` . \n- Click **Save** .\n- To update these changes, reset the notebook instance. \n- After the reset, click **Open JupyterLab** . It might take time for the JupyterLab UI to become available. After the UI appears, open a terminal and run the following command: `ls -al /mnt` The `/mnt/myDisk` directory should be listed. \nNow you can save your work to the `/mnt/myDisk` directory. Even if the notebook instance is deleted, the Persistent Disk exists in your project. You can then attach this Persistent Disk to other notebook instances.\n## Clean up\nAfter you finish using your Apache Beam notebook instance, clean up the resources you created on Google Cloud by [shutting down the notebook instance](/vertex-ai/docs/workbench/user-managed/shut-down) .\n## What's next\n- Learn about [advanced features](/dataflow/docs/guides/notebook-advanced) that you can use with your Apache Beam notebooks. Advanced features include the following workflows:- [Use Interactive FlinkRunner on notebook-managed clusters](/dataflow/docs/guides/notebook-advanced#flinkrunner) \n- [Use Beam SQL and beam_sql magic](/dataflow/docs/guides/notebook-advanced#beam-sql) \n- [Accelerate using JIT compiler and GPU](/dataflow/docs/guides/notebook-advanced#jit-compiler) \n- [Build a custom container](/dataflow/docs/guides/notebook-advanced#custom-container) \n- [Disable external IP addresses](/dataflow/docs/guides/notebook-advanced#external-ip)", "guide": "Dataflow"}