{"title": "Dataflow - Creating classic Dataflow templates", "url": "https://cloud.google.com/dataflow/docs/guides/templates/creating-templates", "abstract": "# Dataflow - Creating classic Dataflow templates\nIn this document, you learn how to create a custom classic template from your Dataflow pipeline code. Classic templates package existing Dataflow pipelines to create reusable templates that you can customize for each job by changing specific pipeline parameters. Rather than writing the template, you use a command to generate the template from an existing pipeline.\nThe following is a brief overview of the process. Details of this process are provided in subsequent sections.\n- In your pipeline code, use the [ValueProvider](/dataflow/docs/guides/templates/creating-templates#about-runtime-parameters-and-the-valueprovider-interface) interface for all pipeline options that you want to set or use at runtime. Use`DoFn`objects that accept runtime parameters.\n- Extend your template with additional metadata so that custom parameters are validated when the classic template is run. Examples of such metadata include the name of your custom classic template and optional parameters.\n- Check if the pipeline I/O connectors support`ValueProvider`objects, and make changes as required.\n- Create and stage the custom classic template.\n- Run the custom classic template.\nTo learn about the different kinds of Dataflow templates, their benefits, and when to choose a classic template, see [Dataflow templates](/dataflow/docs/concepts/dataflow-templates) .\n", "content": "## Required permissions for running a classic template\nThe permissions that you need to run the Dataflow classic template depend on where you run the template, and whether your source and sink for the pipeline are in another project.\nFor more information about running Dataflow pipelines either locally or by using Google Cloud, see [Dataflow security and permissions](/dataflow/docs/concepts/security-and-permissions) .\nFor a list of Dataflow roles and permissions, see [Dataflow access control](/dataflow/docs/concepts/access-control) .\n## Limitations\n- The following [pipeline option](/dataflow/docs/reference/pipeline-options#resource_utilization) isn't supported with classic templates. If you need to control the number of worker harness threads, use [Flex Templates](/dataflow/docs/guides/templates/using-flex-templates) .\n```\nnumberOfWorkerHarnessThreads\u00a0 \n``````\nnumber_of_worker_harness_threads\u00a0 \n```\n- The Dataflow runner doesn't support the`ValueProvider`options for Pub/Sub topics and subscription parameters. If you require Pub/Sub options in your runtime parameters, use Flex Templates.## About runtime parameters and the ValueProvider interfaceThe `ValueProvider` interface allows pipelines to accept runtime parameters. Apache Beam provides three types of `ValueProvider` objects.\n| Name     | Description                                                                                                                                                                                               |\n|:---------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| RuntimeValueProvider | RuntimeValueProvider is the default ValueProvider type. RuntimeValueProvider allows your pipeline to accept a value that is only available during pipeline execution. The value is not available during pipeline construction, so you can't use the value to change your pipeline's workflow graph. You can use isAccessible() to check if the value of a ValueProvider is available. If you call get() before pipeline execution, Apache Beam returns an error: Value only available at runtime, but accessed from a non-runtime context. Use RuntimeValueProvider when you do not know the value ahead of time. To change the parameter values at runtime, do not set values for the parameters in the template. Set the values for the parameters when you create jobs from the template. |\n| StaticValueProvider | StaticValueProvider lets you provide a static value to your pipeline. The value is available during pipeline construction, so you can use the value to change your pipeline's workflow graph. Use StaticValueProvider when you know the value ahead of time. See the StaticValueProvider section for examples.                                                                                                                     |\n| NestedValueProvider | NestedValueProvider lets you compute a value from another ValueProvider object. NestedValueProvider wraps a ValueProvider, and the type of the wrapped ValueProvider determines whether the value is accessible during pipeline construction. Use NestedValueProvider when you want to use the value to compute another value at runtime. See the NestedValueProvider section for examples.                                                                                                  |\n## Use runtime parameters in your pipeline codeThis section walks through how to use `ValueProvider` , `StaticValueProvider` , and `NestedValueProvider` .\n **Note:** If you don't use the`ValueProvider`interface in your classic template, the pipeline options that you want to set or use at runtime might be ignored.\n### Use ValueProvider in your pipeline optionsUse `ValueProvider` for all pipeline options that you want to set or use at runtime.\nFor example, the following `WordCount` code snippet does not support runtime parameters. The code adds an input file option, creates a pipeline, and reads lines from the input file:```\n\u00a0 public interface WordCountOptions extends PipelineOptions {\u00a0 \u00a0 @Description(\"Path of the file to read from\")\u00a0 \u00a0 @Default.String(\"gs://dataflow-samples/shakespeare/kinglear.txt\")\u00a0 \u00a0 String getInputFile();\u00a0 \u00a0 void setInputFile(String value);\u00a0 }\u00a0 public static void main(String[] args) {\u00a0 \u00a0 WordCountOptions options =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PipelineOptionsFactory.fromArgs(args).withValidation()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .as(WordCountOptions.class);\u00a0 \u00a0 Pipeline p = Pipeline.create(options);\u00a0 \u00a0 p.apply(\"ReadLines\", TextIO.read().from(options.getInputFile()));\u00a0 \u00a0 ...\n``````\n\u00a0 class WordcountOptions(PipelineOptions):\u00a0 \u00a0 @classmethod\u00a0 \u00a0 def _add_argparse_args(cls, parser):\u00a0 \u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '--input',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 default='gs://dataflow-samples/shakespeare/kinglear.txt',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 help='Path of the file to read from')\u00a0 \u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '--output',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 help='Output file to write results to.')\u00a0 pipeline_options = PipelineOptions(['--output', 'some/output_path'])\u00a0 p = beam.Pipeline(options=pipeline_options)\u00a0 wordcount_options = pipeline_options.view_as(WordcountOptions)\u00a0 lines = p | 'read' >> ReadFromText(wordcount_options.input)\n```\nTo add runtime parameter support, modify the input file option to use `ValueProvider` .Use `ValueProvider<String>` instead of `String` for the type of the input file option.\n```\n\u00a0 public interface WordCountOptions extends PipelineOptions {\u00a0 \u00a0 @Description(\"Path of the file to read from\")\u00a0 \u00a0 @Default.String(\"gs://dataflow-samples/shakespeare/kinglear.txt\")\u00a0 \u00a0 ValueProvider<String> getInputFile();\u00a0 \u00a0 void setInputFile(ValueProvider<String> value);\u00a0 }\u00a0 public static void main(String[] args) {\u00a0 \u00a0 WordCountOptions options =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PipelineOptionsFactory.fromArgs(args).withValidation()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .as(WordCountOptions.class);\u00a0 \u00a0 Pipeline p = Pipeline.create(options);\u00a0 \u00a0 p.apply(\"ReadLines\", TextIO.read().from(options.getInputFile()));\u00a0 \u00a0 ...\n```Replace `add_argument` with `add_value_provider_argument` .\n```\n\u00a0class WordcountOptions(PipelineOptions):\u00a0 \u00a0 @classmethod\u00a0 \u00a0 def _add_argparse_args(cls, parser):\u00a0 \u00a0 \u00a0 # Use add_value_provider_argument for arguments to be templatable\u00a0 \u00a0 \u00a0 # Use add_argument as usual for non-templatable arguments\u00a0 \u00a0 \u00a0 parser.add_value_provider_argument(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '--input',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 default='gs://dataflow-samples/shakespeare/kinglear.txt',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 help='Path of the file to read from')\u00a0 \u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '--output',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 help='Output file to write results to.')\u00a0 pipeline_options = PipelineOptions(['--output', 'some/output_path'])\u00a0 p = beam.Pipeline(options=pipeline_options)\u00a0 wordcount_options = pipeline_options.view_as(WordcountOptions)\u00a0 lines = p | 'read' >> ReadFromText(wordcount_options.input)\n```\n### Use ValueProvider in your functionsTo use runtime parameter values in your own functions, update the functions to use `ValueProvider` parameters.\nThe following example contains an integer `ValueProvider` option, and a simple function that adds an integer. The function depends on the `ValueProvider` integer. During execution, the pipeline applies `MySumFn` to every integer in a `PCollection` that contains `[1, 2, 3]` . If the runtime value is 10, the resulting `PCollection` contains `[11, 12, 13]` .```\n\u00a0 public interface SumIntOptions extends PipelineOptions {\u00a0 \u00a0 \u00a0 // New runtime parameter, specified by the --int\u00a0 \u00a0 \u00a0 // option at runtime.\u00a0 \u00a0 \u00a0 ValueProvider<Integer> getInt();\u00a0 \u00a0 \u00a0 void setInt(ValueProvider<Integer> value);\u00a0 }\u00a0 class MySumFn extends DoFn<Integer, Integer> {\u00a0 \u00a0 \u00a0 ValueProvider<Integer> mySumInteger;\u00a0 \u00a0 \u00a0 MySumFn(ValueProvider<Integer> sumInt) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Store the value provider\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.mySumInteger = sumInt;\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 @ProcessElement\u00a0 \u00a0 \u00a0 public void processElement(ProcessContext c) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Get the value of the value provider and add it to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0// the element's value.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0c.output(c.element() + mySumInteger.get());\u00a0 \u00a0 \u00a0 }\u00a0 }\u00a0 public static void main(String[] args) {\u00a0 \u00a0 SumIntOptions options =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PipelineOptionsFactory.fromArgs(args).withValidation()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .as(SumIntOptions.class);\u00a0 \u00a0 Pipeline p = Pipeline.create(options);\u00a0 \u00a0 p.apply(Create.of(1, 2, 3))\u00a0 \u00a0 \u00a0 // Get the value provider and pass it to MySumFn\u00a0 \u00a0 \u00a0.apply(ParDo.of(new MySumFn(options.getInt())))\u00a0 \u00a0 \u00a0.apply(\"ToString\", MapElements.into(TypeDescriptors.strings()).via(x -> x.toString()))\u00a0 \u00a0 \u00a0.apply(\"OutputNums\", TextIO.write().to(\"numvalues\"));\u00a0 \u00a0 p.run();\u00a0 }\n``````\n\u00a0 import apache_beam as beam\u00a0 from apache_beam.options.pipeline_options import PipelineOptions\u00a0 from apache_beam.options.value_provider import StaticValueProvider\u00a0 from apache_beam.io import WriteToText\u00a0 class UserOptions(PipelineOptions):\u00a0 \u00a0 @classmethod\u00a0 \u00a0 def _add_argparse_args(cls, parser):\u00a0 \u00a0 \u00a0 parser.add_value_provider_argument('--templated_int', type=int)\u00a0 class MySumFn(beam.DoFn):\u00a0 \u00a0 def __init__(self, templated_int):\u00a0 \u00a0 \u00a0 self.templated_int = templated_int\u00a0 \u00a0 def process(self, an_int):\u00a0 \u00a0 \u00a0 yield self.templated_int.get() + an_int\u00a0 pipeline_options = PipelineOptions()\u00a0 p = beam.Pipeline(options=pipeline_options)\u00a0 user_options = pipeline_options.view_as(UserOptions)\u00a0 sum = (p\u00a0 \u00a0 \u00a0 \u00a0 \u00a0| 'ReadCollection' >> beam.io.ReadFromText(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0'gs://some/integer_collection')\u00a0 \u00a0 \u00a0 \u00a0 \u00a0| 'StringToInt' >> beam.Map(lambda w: int(w))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0| 'AddGivenInt' >> beam.ParDo(MySumFn(user_options.templated_int))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0| 'WriteResultingCollection' >> WriteToText('some/output_path'))\n```### Use StaticValueProviderTo provide a static value to your pipeline, use `StaticValueProvider` .\nThis example uses `MySumFn` , which is a `DoFn` that takes a `ValueProvider<Integer>` . If you know the  value of the parameter ahead of time, you can use `StaticValueProvider` to specify your static value as a `ValueProvider` .\nThis code gets the value at pipeline runtime:\n```\n\u00a0 .apply(ParDo.of(new MySumFn(options.getInt())))\n```\nInstead, you can use `StaticValueProvider` with a static value:\n```\n\u00a0 .apply(ParDo.of(new MySumFn(StaticValueProvider.of(10))))\n```This code gets the value at pipeline runtime:\n```\n\u00a0 beam.ParDo(MySumFn(user_options.templated_int))\n```\nInstead, you can use `StaticValueProvider` with a static value:\n```\n\u00a0 beam.ParDo(MySumFn(StaticValueProvider(int,10)))\n```\nYou can also use `StaticValueProvider` when you implement an I/O module that supports both regular parameters and runtime parameters. `StaticValueProvider` reduces the code duplication from implementing two similar methods.The source code for this example is from Apache Beam's [TextIO.java on GitHub](https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/TextIO.java) .\n```\n\u00a0 // Create a StaticValueProvider<String> from a regular String parameter\u00a0 // value, and then call .from() with this new StaticValueProvider.\u00a0 public Read from(String filepattern) {\u00a0 \u00a0 checkNotNull(filepattern, \"Filepattern cannot be empty.\");\u00a0 \u00a0 return from(StaticValueProvider.of(filepattern));\u00a0 }\u00a0 // This method takes a ValueProvider parameter.\u00a0 public Read from(ValueProvider<String> filepattern) {\u00a0 \u00a0 checkNotNull(filepattern, \"Filepattern cannot be empty.\");\u00a0 \u00a0 return toBuilder().setFilepattern(filepattern).build();\u00a0 }\n```In this example, there is a single constructor that accepts both a `string` or a `ValueProvider` argument. If the  argument is a `string` , it is converted to a `StaticValueProvider` .\n```\nclass Read():\u00a0 def __init__(self, filepattern):\u00a0 \u00a0 if isinstance(filepattern, str):\u00a0 \u00a0 \u00a0 # Create a StaticValueProvider from a regular string parameter\u00a0 \u00a0 \u00a0 filepattern = StaticValueProvider(str, filepattern)\u00a0 \u00a0 self.filepattern = filepattern\n```\n### Use NestedStaticValueProviderTo compute a value from another `ValueProvider` object, use `NestedValueProvider` .\n`NestedValueProvider` takes a `ValueProvider` and a `SerializableFunction` translator as input. When you call `.get()` on a `NestedValueProvider` , the translator  creates a new value based on the `ValueProvider` value. This  translation lets you use a `ValueProvider` value to create  the final value that you want.\n **Note:** `NestedValueProvider`accepts only one value input. You can't use a`NestedValueProvider`to combine two different values.\nIn the following example, the user provides the filename `file.txt` . The  transform prepends the path `gs://directory_name/` to  the filename. Calling `.get()` returns `gs://directory_name/file.txt` .```\n\u00a0 public interface WriteIntsOptions extends PipelineOptions {\u00a0 \u00a0 \u00a0 // New runtime parameter, specified by the --fileName\u00a0 \u00a0 \u00a0 // option at runtime.\u00a0 \u00a0 \u00a0 ValueProvider<String> getFileName();\u00a0 \u00a0 \u00a0 void setFileName(ValueProvider<String> value);\u00a0 }\u00a0 public static void main(String[] args) {\u00a0 \u00a0 \u00a0WriteIntsOptions options =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PipelineOptionsFactory.fromArgs(args).withValidation()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .as(WriteIntsOptions.class);\u00a0 \u00a0 Pipeline p = Pipeline.create(options);\u00a0 \u00a0 p.apply(Create.of(1, 2, 3))\u00a0 \u00a0 \u00a0// Write to the computed complete file path.\u00a0 \u00a0 \u00a0.apply(\"OutputNums\", TextIO.write().to(NestedValueProvider.of(\u00a0 \u00a0 \u00a0 \u00a0 options.getFileName(),\u00a0 \u00a0 \u00a0 \u00a0 new SerializableFunction<String, String>() {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 @Override\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 public String apply(String file) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return \"gs://directoryname/\" + file;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 })));\u00a0 \u00a0 p.run();\u00a0 }\n```\n## Use metadata in your pipeline codeThis content is for both Classic and Flex templates\nYou can extend your template with additional metadata so that custom parameters are validated when the template is run. If you want to create metadata for your template, follow these steps:- Create a JSON-formatted file named`` `_metadata`using the  parameters in [Metadata parameters](#metadataparameters) and the format in [Example metadata file](#example-metadata-file) . Replace``with the name of your template.Ensure the metadata file does not have a filename extension. For example, if your template  name is `myTemplate` , then its metadata file must be `myTemplate_metadata` .\n- Store the metadata file in Cloud Storage in the same folder as the template.\n### Metadata parameters| Parameter key  | Unnamed: 1 | Required | Description of the value                                                                       |\n|:--------------------|:-------------|:-----------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| name    | nan   | Yes  | The name of your template.                                                                       |\n| description   | nan   | No   | A short paragraph of text describing the template.                                                                 |\n| streaming   | nan   | No   | If true, this template supports streaming. The default value is false.                                                            |\n| supportsAtLeastOnce | nan   | No   | If true, this template supports at-least-once processing. The default value is false. Set this parameter to true if the template is designed to work with at-least-once streaming mode.                               |\n| supportsExactlyOnce | nan   | No   | If true, this template supports exactly-once processing. The default value is true.                                                        |\n| parameters   | nan   | No   | An array of additional parameters that the template uses. An empty array is used by default.                                                      |\n| parameters   | name   | Yes  | The name of the parameter that is used in your template.                                                               |\n| parameters   | label  | Yes  | A human readable string that is used in the Google Cloud console to label the parameter.                                                       |\n| parameters   | helpText  | Yes  | A short paragraph of text that describes the parameter.                                                                |\n| parameters   | isOptional | No   | false if the parameter is required and true if the parameter is optional. Unless set with a value, isOptional defaults to false. If you do not include this parameter key for your metadata, the metadata becomes a required parameter.                   |\n| parameters   | regexes  | No   | An array of POSIX-egrep regular expressions in string form that is used to validate the value of the parameter. For example, [\"^[a-zA-Z][a-zA-Z0-9]+\"] is a single regular expression that validates that the value starts with a letter and then has one or more characters. An empty array is used by default. |\n### Example metadata fileThe Dataflow service uses the following metadata to validate the [WordCount template's](https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/master/v1/src/main/java/com/google/cloud/teleport/templates/WordCount.java) custom parameters:\n```\n{\u00a0 \"description\": \"An example pipeline that counts words in the input file.\",\u00a0 \"name\": \"Word Count\",\u00a0 \"streaming\": false,\u00a0 \"parameters\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"regexes\": [\u00a0 \u00a0 \u00a0 \u00a0 \"^gs:\\\\/\\\\/[^\\\\n\\\\r]+$\"\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"name\": \"inputFile\",\u00a0 \u00a0 \u00a0 \"helpText\": \"Path of the file pattern glob to read from - for example, gs://dataflow-samples/shakespeare/kinglear.txt\",\u00a0 \u00a0 \u00a0 \"label\": \"Input Cloud Storage file(s)\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"regexes\": [\u00a0 \u00a0 \u00a0 \u00a0 \"^gs:\\\\/\\\\/[^\\\\n\\\\r]+$\"\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"name\": \"output\",\u00a0 \u00a0 \u00a0 \"helpText\": \"Path and filename prefix for writing output files - for example, gs://MyBucket/counts\",\u00a0 \u00a0 \u00a0 \"label\": \"Output Cloud Storage file(s)\"\u00a0 \u00a0 }\u00a0 ]}\n```The Dataflow service uses the following metadata to validate the [WordCount template's](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount.py) custom parameters:\n```\n{\u00a0 \"description\": \"An example pipeline that counts words in the input file.\",\u00a0 \"name\": \"Word Count\",\u00a0 \"streaming\": false,\u00a0 \"parameters\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"regexes\": [\u00a0 \u00a0 \u00a0 \u00a0 \"^gs:\\\\/\\\\/[^\\\\n\\\\r]+$\"\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"name\": \"input\",\u00a0 \u00a0 \u00a0 \"helpText\": \"Path of the file pattern glob to read from - for example, gs://dataflow-samples/shakespeare/kinglear.txt\",\u00a0 \u00a0 \u00a0 \"label\": \"Input Cloud Storage file(s)\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"regexes\": [\u00a0 \u00a0 \u00a0 \u00a0 \"^gs:\\\\/\\\\/[^\\\\n\\\\r]+$\"\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"name\": \"output\",\u00a0 \u00a0 \u00a0 \"helpText\": \"Path and filename prefix for writing output files - for example, gs://MyBucket/counts\",\u00a0 \u00a0 \u00a0 \"label\": \"Output Cloud Storage file(s)\"\u00a0 \u00a0 }\u00a0 ]}\n```\nYou can download metadata files for the Google-provided templates from the  Dataflow [template directory](https://console.cloud.google.com/storage/browser/dataflow-templates/latest) .\n## Supported pipeline I/O connectors and ValueProviderSome I/O connectors contain methods that accept `ValueProvider` objects. To determine support for a specific  connector and method, see the [API reference documentation](https://beam.apache.org/documentation/sdks/javadoc/current/) for the I/O connector. Supported methods have an overload with a `ValueProvider` . If a method does not have an overload, the method does not  support runtime parameters. The following I/O connectors have at  least partial `ValueProvider` support:- File-based IOs:`TextIO`,`AvroIO`,`FileIO`,`TFRecordIO`,`XmlIO`\n- `BigQueryIO` ***** \n- `BigtableIO`(requires SDK 2.3.0 or later)\n- `PubSubIO`\n- `SpannerIO`\n *** Note:** If you want to run a batch pipeline that reads from BigQuery, you must use`.withTemplateCompatibility()`on all BigQuery reads.Some I/O connectors contain methods that accept `ValueProvider` objects. To determine support for I/O connectors  and their methods, see the [API reference documentation](https://beam.apache.org/documentation/sdks/pydoc/current/) for the connector. The following I/O connectors accept runtime parameters:- File-based IOs:`textio`,`avroio`,`tfrecordio`## Create and stage a classic templateAfter you write your pipeline, you must create and stage your template file. When you create and stage a template, the staging location contains additional files that are necessary to run your template. If you delete the staging location, the template fails to run. The Dataflow job does not run immediately after you stage the template. To run a custom template-based Dataflow job, you can use the [Google Cloud console](/dataflow/docs/guides/templates/running-templates#custom-templates) , the [Dataflow REST API](/dataflow/docs/guides/templates/running-templates#using-the-rest-api) , or the [gcloud CLI](/dataflow/docs/guides/templates/running-templates#using-gcloud) .\n **Note:** Creating and staging a template requires authentication. For  instructions about how to create a service account and a service account key, see the  quickstart for the language you are using: [Java quickstart](/dataflow/docs/quickstarts/create-pipeline-java) , [Python quickstart](/dataflow/docs/quickstarts/create-pipeline-python) , or [Go quickstart](/dataflow/docs/quickstarts/create-pipeline-go) .\nThe following example shows how to stage a template file:This Maven command creates and stages a template at the Cloud Storage location  specified with `--templateLocation` .\n **Note:** If you use the Apache Beam SDK for Java 2.15.0 or later, you must specify`--region`.\n```\n mvn compile exec:java \\\n  -Dexec.mainClass=com.example.myclass \\\n  -Dexec.args=\"--runner=DataflowRunner \\\n     --project=PROJECT_ID \\\n     --stagingLocation=gs://BUCKET_NAME/staging \\\n     --templateLocation=gs://BUCKET_NAME/templates/TEMPLATE_NAME \\\n     --region=REGION\" \\\n  -P dataflow-runner\n \n```\nVerify that the `templateLocation` path is correct. Replace the following:- ``: your Java class\n- ``: your project ID\n- ``: the name of your Cloud Storage   bucket\n- ``: the name of your template\n- ``: the [region](/dataflow/docs/resources/locations) to deploy your  Dataflow job in\nThis Python command creates and stages a template at the Cloud Storage  location specified with `--template_location` .\n **Note:** If you use the Apache Beam SDK for Python 2.15.0 or later, you must specify`--region`.\n```\n python -m examples.mymodule \\\n --runner DataflowRunner \\\n --project PROJECT_ID \\\n --staging_location gs://BUCKET_NAME/staging \\\n --template_location gs://BUCKET_NAME/templates/TEMPLATE_NAME \\\n --region REGION\n```\nVerify that the `template_location` path is correct. Replace the following:- ``: your Python module\n- ``: your project ID\n- ``: the name of your Cloud Storage   bucket\n- ``: the name of your template\n- ``: the [region](/dataflow/docs/resources/locations) to deploy your  Dataflow job in\nAfter you create and stage your template, your next step is to [run the template](/dataflow/docs/templates/running-templates) .", "guide": "Dataflow"}