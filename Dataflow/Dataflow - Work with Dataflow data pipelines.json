{"title": "Dataflow - Work with Dataflow data pipelines", "url": "https://cloud.google.com/dataflow/docs/guides/data-pipelines", "abstract": "# Dataflow - Work with Dataflow data pipelines\n**Note:** You can report Dataflow data pipelines issues and request new features at [google-data-pipelines-feedback](https://groups.google.com/d/forum/google-data-pipelines-feedback) .\"\n", "content": "## Overview\nYou can use Dataflow data pipelines for the following tasks:\n- Create recurrent job schedules.\n- Understand where resources are spent over multiple job executions.\n- Define and manage data freshness objectives.\n- Drill down into individual pipeline stages to fix and optimize your pipelines.\nFor API documentation, see the [Data Pipelines reference](/dataflow/docs/reference/data-pipelines/rest) .\n## Features\n- Create a recurring batch pipeline to run a batch job on a schedule.\n- Create a recurring incremental batch pipeline to run a batch job against the latest version of input data.\n- Use the pipeline summary scorecard to view the aggregated capacity usage and resource consumption of a pipeline.\n- View the data freshness of a streaming pipeline. This metric, which evolves over time, can be tied to an alert that notifies you when freshness falls lower than a specified objective.\n- Use pipeline metric graphs to compare batch pipeline jobs and find anomalies.## Limitations\n- Regional availability: You can create data pipelines in [available Cloud Scheduler regions](/about/locations#region) .\n- Quota:- Default number of pipelines per project: 500\n- Default number of pipelines per organization: 2500The organization level quota is disabled by default. You can opt-in to organization level quotas, and if you do so, each organization can have at most 2500 pipelines by default.\n **Note:** If you want to increase the quota, contact [Google Cloud Support](/support) . However, the limit value is subject to the [Cloud Scheduler](/scheduler/quotas) and [Dataflow](/dataflow/quotas) quotas and limits.\n- Labels: You can't use [user-defined labels](/resource-manager/docs/creating-managing-labels) to label Dataflow data pipelines. However, when you use the [additionalUserLabels](/dataflow/docs/reference/rest/v1b3/RuntimeEnvironment) field, those values are passed through to your Dataflow job. For more information about how labels apply to individual Dataflow jobs, see [Pipeline options](/dataflow/docs/reference/pipeline-options) .## Types of data pipelines\nDataflow has two data pipeline types, streaming and batch. Both types of pipeline run jobs that are defined in Dataflow [templates](/dataflow/docs/concepts/dataflow-templates#templated-dataflow-jobs) .\n## Incremental batch pipelines\nYou can use datetime placeholders to specify an incremental input file format for a batch pipeline.\n- Placeholders for year, month, date, hour, minute, and second can be used, and must follow the [strftime()](https://www.cplusplus.com/reference/ctime/strftime/) format. Placeholders are preceded by the percentage symbol (%).\n- Parameter formatting is not verified during pipeline creation.- **Example:** If you specify \"gs://bucket/Y\" as the parameterized input path, it's evaluated as \"gs://bucket/Y\", because \"Y\" without a preceding \"%\" does not map to the`strftime()`format.At each scheduled batch pipeline execution time, the placeholder portion of the input path is evaluated to the current (or [time-shifted](#using_time_shift_parameters) ) datetime. Date values are evaluated using the current date in the time zone of the scheduled job. If the evaluated path matches the path of an input file, the file is picked up for processing by the batch pipeline at the scheduled time.\n- **Example:** A batch pipeline is scheduled to repeat at the start of each hour PST. If you parameterize the input path as`gs://` `` `/%Y-%m-%d/` `` `%H_%M.csv`, on April 15, 2021, 6PM PST, the input path is evaluated to`gs://` `` `/2021-04-15/` `` `18_00.csv`.\n### Use time shift parameters\nYou can use + or - minute or hour time shift parameters. To support matching an input path with an evaluated datetime that is shifted before or after the current datetime of the pipeline schedule, enclose these parameters in curly braces. Use the format `{[+|-][0-9]+[m|h]}` . The batch pipeline continues to repeat at its scheduled time, but the input path is evaluated with the specified time offset.\n- **Example:** A batch pipeline is scheduled to repeat at the start of each hour PST. If you parameterize the input path as`gs://` `` `/%Y-%m-%d/` `` `%H_%M.csv{-2h}`, on April 15, 2021, 6PM PST, the input path is evaluated to`gs://` `` `/2021-04-15/` `` `16_00.csv`.## Data pipeline roles\nFor Dataflow data pipeline operations to succeed, you need the necessary IAM roles, as follows:\n- You need the appropriate role to perform operations:- [Datapipelines.admin](/iam/docs/understanding-roles#datapipelines.admin) : Can perform all data pipeline operations\n- [Datapipelines.viewer](/iam/docs/understanding-roles#datapipelines.viewer) : Can view data pipelines and jobs\n- [Datapipelines.invoker](/iam/docs/understanding-roles#datapipelines.invoker) : Can invoke a data pipeline job run (this role can be enabled using the API)\n- The service account used by Cloud Scheduler needs to have the [roles/iam.serviceAccountUser](/compute/docs/access/iam#iam.serviceAccountUser) role, whether the service account is user-specified or the default Compute Engine service account. For more information, see [Data pipeline roles](#data_pipeline_roles) .\n- You need to be able to act as the service account used by Cloud Scheduler and Dataflow by being granted the [roles/iam.serviceAccountUser](/compute/docs/access/iam#iam.serviceAccountUser) role on that account. If you don't select a service account for Cloud Scheduler and Dataflow, the [default Compute Engine service account](/compute/docs/access/service-accounts#default_service_account) is used.\n**Note:** If you encounter a `NOT_FOUND` error status from Cloud Scheduler with the log entry `type.googleapis.com/google.cloud.scheduler.logging.AttemptFinished` , it often indicates missing permissions on the service accounts for either Cloud Scheduler or Dataflow. Verify and correct these permissions to resolve data pipeline issues.\n## Create a data pipeline\nYou can create a Dataflow data pipeline in two ways:\n- [Import a job](#import_a_job) , or\n- [Create a data pipeline](#create_a_data_pipeline) \n**The data pipelines setup page:** When you first access the Dataflow pipelines feature in the Google Cloud console, a setup page opens. Enable the listed APIs to create data pipelines.\n### Import a job\nYou can import a Dataflow batch or streaming job that is based on a [classic or flex template](/dataflow/docs/concepts/dataflow-templates#templated-dataflow-jobs) and make it a data pipeline.\n- In the Google Cloud console, go to the Dataflow **Jobs** page. [Go to Jobs](https://console.cloud.google.com/dataflow/jobs) \n- Select a completed job, then on the **Job Details** page, select **+Import as a pipeline** .\n- On the **Create pipeline from template** page, the parameters are populated with the options of the imported job.\n- For a batch job, in the **Schedule your pipeline** section, provide a recurrence schedule. Providing an email account address for the Cloud Scheduler, which is used to schedule batch runs, is optional. If it's not specified, the [default Compute Engine service account](/compute/docs/access/service-accounts#default_service_account) is used.\n### Create a data pipeline\n- In the Google Cloud console, go to the Dataflow **Data pipelines** page. [Go to Data pipelines](https://console.cloud.google.com/dataflow/pipelines) \n- Select **+Create data pipeline** .\n- On the **Create pipeline from template** page, provide a pipeline name, and fill in the other template selection and parameter fields.\n- For a batch job, in the **Schedule your pipeline** section, provide a recurrence schedule. Providing an email account address for the Cloud Scheduler, which is used to schedule batch runs, is optional. If a value is not specified, the [default Compute Engine service account](/compute/docs/access/service-accounts#default_service_account) is used.## Create a batch data pipeline\nTo create this sample batch data pipeline, you must have access to the following resources in your project:\n- A [Cloud Storage bucket](/storage/docs/creating-buckets) to store input and output files\n- A [BigQuery dataset](/bigquery/docs/datasets) to create a table.\nThis example pipeline uses the [Cloud Storage Text to BigQuery](/dataflow/docs/guides/templates/provided/cloud-storage-to-bigquery) batch pipeline template. This template reads files in CSV format from Cloud Storage, runs a transform, then inserts values into a BigQuery table with three columns.\n- Create the following files on your local drive:- A `bq_three_column_table.json` file that contains the following schema of the destination BigQuery table.```\n{\u00a0 \"BigQuery Schema\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"name\": \"col1\",\u00a0 \u00a0 \u00a0 \"type\": \"STRING\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"name\": \"col2\",\u00a0 \u00a0 \u00a0 \"type\": \"STRING\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"name\": \"col3\",\u00a0 \u00a0 \u00a0 \"type\": \"INT64\"\u00a0 \u00a0 }\u00a0 ]}\n```\n- A `split_csv_3cols.js` JavaScript file, which implements a simple transformation on the input data before insertion into BigQuery.```\nfunction transform(line) {\u00a0 \u00a0 var values = line.split(',');\u00a0 \u00a0 var obj = new Object();\u00a0 \u00a0 obj.col1 = values[0];\u00a0 \u00a0 obj.col2 = values[1];\u00a0 \u00a0 obj.col3 = values[2];\u00a0 \u00a0 var jsonString = JSON.stringify(obj);\u00a0 \u00a0 return jsonString;}\n```\n- A `file01.csv` CSV file with several records that are inserted into the BigQuery table.```\nb8e5087a,74,27531\n7a52c051,4a,25846\n672de80f,cd,76981\n111b92bf,2e,104653\nff658424,f0,149364\ne6c17c75,84,38840\n833f5a69,8f,76892\nd8c833ff,7d,201386\n7d3da7fb,d5,81919\n3836d29b,70,181524\nca66e6e5,d7,172076\nc8475eb6,03,247282\n558294df,f3,155392\n737b82a8,c7,235523\n82c8f5dc,35,468039\n57ab17f9,5e,480350\ncbcdaf84,bd,354127\n52b55391,eb,423078\n825b8863,62,88160\n26f16d4f,fd,397783\n```\n- Use the [gcloud storage cp command](/sdk/gcloud/reference/storage/cp) to copy the files to folders in a Cloud Storage bucket in your project, as follows:- Copy `bq_three_column_table.json` and `split_csv_3cols.js` to `gs://` `` `/text_to_bigquery/````\ngcloud storage cp bq_three_column_table.json gs://BUCKET_ID/text_to_bigquery/gcloud storage cp split_csv_3cols.js gs://BUCKET_ID/text_to_bigquery/\n```\n- Copy `file01.csv` to `gs://` `` `/inputs/````\ngcloud storage cp file01.csv gs://BUCKET_ID/inputs/\n```\n- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets](https://console.cloud.google.com/storage/browser) \n- To create a `tmp` folder in your Cloud Storage bucket, select your folder name to open the **Bucket details page** , then click **Create folder** . \n- In the Google Cloud console, go to the Dataflow **Data pipelines** page. [Go to Data pipelines](https://console.cloud.google.com/dataflow/pipelines) \n- Select **Create data pipeline** . Enter or select the following items on the **Create pipeline from template** page:- For **Pipeline name** , enter`text_to_bq_batch_data_pipeline`.\n- For **Regional endpoint** , select a Compute Engine [region](/compute/docs/regions-zones#available) . The source and destination regions must match. Therefore, your Cloud Storage bucket and BigQuery table must be in the same region.\n- For **Dataflow template** , in **Process Data in Bulk (batch)** , select **Text Files on Cloud Storage to BigQuery** . **Note:** Don't select the streaming pipeline with the same name in **Process Data Continuously (stream)** .\n- For **Schedule your pipeline** , select a schedule, such as **Hourly** at minute **25** , in your timezone. You can edit the schedule after you submit the pipeline. Providing an email account address for the Cloud Scheduler, which is used to schedule batch runs, is optional. If it's not specified, the [default Compute Engine service account](/compute/docs/access/service-accounts#default_service_account) is used.\n- In **Required parameters** , enter the following:- For **JavaScript UDF path in Cloud Storage** :```\ngs://BUCKET_ID/text_to_bigquery/split_csv_3cols.js\n```\n- For **JSON path** :```\nBUCKET_ID/text_to_bigquery/bq_three_column_table.json\n```\n- For **JavaScript UDF name** :`transform`\n- For **BigQuery output table** :```\nPROJECT_ID:DATASET_ID.three_column_table\n```\n- For **Cloud Storage input path** :```\nBUCKET_ID/inputs/file01.csv\n```\n- For **Temporary BigQuery directory** :```\nBUCKET_ID/tmp\n```\n- For **Temporary location** :```\nBUCKET_ID/tmp\n```\n- Click **Create pipeline** .\n- Confirm pipeline and template information and view current and previous history from the **Pipeline details** page. You can edit the data pipeline schedule from the Pipeline info panel on the **Pipeline details** page. ![Edit button next to the pipeline schedule.](/dataflow/images/pipeline-info.png){: width=\"375\"}\nYou can also run a batch pipeline on demand using the **Run** button in the Dataflow Pipelines console.\n## Create a sample streaming data pipeline\nYou can create a sample streaming data pipeline by following the [sample batch pipeline instructions](#create_a_batch_data_pipeline) , with the following differences:\n- For **Pipeline schedule** , don't specify a schedule for a streaming data pipeline. The Dataflow streaming job is started immediately.\n- For **Dataflow template** , in **Process Data Continuously (stream)** , select **Text Files on Cloud Storage to BigQuery** .\n- For **Worker machine type** , the pipeline processes the initial set of files matching the`gs://` `` `/inputs/file01.csv`pattern and any additional files matching this pattern that you upload to the`inputs/`folder. If the size of CSV files exceeds several GB, to avoid possible out-of-memory errors, select a machine type with higher memory than the default`n1-standard-4`machine type, such as`n1-highmem-8`.## Troubleshooting\nThis section shows you how to resolve issues with Dataflow data pipelines.\n### Data pipeline job fails to launch\nWhen you use data pipelines to create a recurring job schedule, your Dataflow job might not launch, and a `503` status error appears in the Cloud Scheduler log files.\nThis issue occurs when Dataflow is temporarily unable to run the job.\nTo work around this issue, configure Cloud Scheduler to retry the job. Because the issue is temporary, when the job is retried, it might succeed. For more information about setting retry values in Cloud Scheduler, see [Create a job](/scheduler/docs/creating#create) .\n### Investigate pipeline objectives violations\nThe following sections describe how to investigate pipelines that don't meet performance objectives.\nFor an initial analysis of the health of your pipeline, on the **Pipeline info** page in the Google Cloud console, use the **Individual job status** and **Thread time per step** graphs. These graphs are located in the pipeline status panel.\n**Example investigation:**\n- You have a recurring batch pipeline that runs every hour at 3 minutes past the hour. Each job normally runs for approximately 9 minutes. You have an objective for all jobs to complete in less than 10 minutes.\n- The job status graph shows that a job ran for more than 10 minutes.\n- In the **Update/Execution** history table, find the job that ran during the hour of interest. Click through to the Dataflow job details page. On that page, find the longer running stage, and then look in the logs for possible errors to determine the cause of the delay.For an initial analysis of the health of your pipeline, on the **Pipeline Details** page, in the **Pipeline info** tab, use the data freshness graph. This graph is located in the pipeline status panel.\n**Example investigation:**\n- You have a streaming pipeline that normally produces an output with a [data freshness](/dataflow/docs/guides/using-monitoring-intf#data_freshness_streaming_pipelines_only) of 20 seconds.\n- You set an objective of having a 30-second data freshness guarantee. When you review the data freshness graph, you notice that between 9 and 10 AM, data freshness jumped to almost 40 seconds.\n- Switch to the **Pipeline metrics** tab, then view the CPU Utilization and Memory Utilization graphs for further analysis.", "guide": "Dataflow"}