{"title": "Dataflow - Processing Landsat satellite images with GPUs", "url": "https://cloud.google.com/dataflow/docs/tutorials/satellite-images-gpus", "abstract": "# Dataflow - Processing Landsat satellite images with GPUs\n**Note:** The following considerations apply to this GA offering:- Jobs that use GPUs incur charges as specified in the Dataflow [pricing page](/dataflow/pricing) .\n- To use GPUs, your Dataflow job must use [Dataflow Runner v2](/dataflow/docs/runner-v2) .This tutorial shows you how to use GPUs on Dataflow to process Landsat 8 satellite images and render them as JPEG files. The tutorial is based on the example [Processing Landsat satellite images with GPUs](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/dataflow/gpu-examples/tensorflow-landsat) .", "content": "## Objectives\n- Build a Docker image for Dataflow that has TensorFlow with GPU support.\n- Run a Dataflow job with GPUs.\n## CostsThis tutorial uses billable components of Google Cloud, including:- Cloud Storage\n- Dataflow\n- Artifact Registry\nUse the [pricing calculator](/products/calculator) to generate a cost estimate based on your projected usage.## Before you begin\n- Grant roles to your Compute Engine default service account. Run the following command once  for each of the following IAM roles: `roles/dataflow.admin` , `roles/dataflow.worker` , `roles/bigquery.dataEditor` , `roles/pubsub.editor` , `roles/storage.objectAdmin` ,  and `roles/artifactregistry.reader` .```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com\" --role=SERVICE_ACCOUNT_ROLE\n```- Replace``with your project ID.\n- Replace``with your project number.  To find your project number, see [Identify projects](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- Replace``with each individual role.\n- To store the output JPEG image files from this tutorial, create a  Cloud Storage bucket:- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets page](https://console.cloud.google.com/storage/browser) \n- Click **Create bucket** .\n- On the **Create a bucket** page, enter your bucket information. To go to the next step, click **Continue** .- For **Name your bucket** , enter a unique bucket name. Don't include sensitive   information in the bucket name, because the bucket namespace is global and publicly   visible.\n- For **Choose where to store your data** , do the following:- Select a **Location type** option.\n- Select a **Location** option.\n- For **Choose a default storage class for your data** , select the following: **Standard** .\n- For **Choose how to control access to objects** , select an **Access control** option.\n- For **Advanced settings (optional)** , specify  an [encryption method](/storage/docs/encryption) ,  a [retention policy](/storage/docs/bucket-lock) ,  or [bucket labels](/storage/docs/tags-and-labels#bucket-labels) .\n- Click **Create** .\n## Prepare your working environmentDownload the starter files, and then create your Artifact Registry repository.\n### Download the starter filesDownload the starter files and then change directories.- Clone the `python-docs-samples` repository.```\ngit clone https://github.com/GoogleCloudPlatform/python-docs-samples.git\n```\n- Navigate to the [sample codedirectory](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/master/dataflow/gpu-examples/tensorflow-landsat) .```\ncd python-docs-samples/dataflow/gpu-examples/tensorflow-landsat\n```\n### Configure Artifact RegistryCreate an Artifact Registry repository so that you can upload artifacts. Each repository can contain artifacts for a single supported format.\nAll repository content is encrypted using either Google-managed or customer-managed encryption keys. Artifact Registry uses Google-managed encryption keys by default and no configuration is required for this option.\nYou must have at least [Artifact Registry Writer access](/artifact-registry/docs/access-control#permissions) to the repository.\nRun the following command to create a new repository. The command uses the `--async` flag and returns immediately, without waiting for the operation in progress to complete.\n```\ngcloud artifacts repositories create REPOSITORY \\\u00a0 \u00a0 --repository-format=docker \\\u00a0 \u00a0 --location=LOCATION \\\u00a0 \u00a0 --async\n```\nReplace with a name for your repository. For each repository location in a project, repository names must be unique.\nBefore you can push or pull images, configure Docker to authenticate requests for Artifact Registry. To set up authentication to Docker repositories, run the following command:\n```\ngcloud auth configure-docker LOCATION-docker.pkg.dev\n```\nThe command updates your Docker configuration. You can now connect with Artifact Registry in your Google Cloud project to push images.## Build the Docker imageCloud Build allows you to build a Docker image using a Dockerfile and save it into Artifact Registry, where the image is accessible to other Google Cloud products.\nBuild the container image by using the [build.yaml](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/dataflow/gpu-examples/tensorflow-landsat/build.yaml) config file.\n```\ngcloud builds submit --config build.yaml\n```## Run the Dataflow job with GPUsThe following code block demonstrates how to launch this Dataflow pipeline with GPUs.\nWe run the Dataflow pipeline using the [run.yaml](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/dataflow/gpu-examples/tensorflow-landsat/run.yaml) config file.\n **Note:** We launch the job using the container image to make sure the job launches with the same Python version as the workers and with all the dependencies installed.\n```\nexport PROJECT=PROJECT_NAMEexport BUCKET=BUCKET_NAMEexport JOB_NAME=\"satellite-images-$(date +%Y%m%d-%H%M%S)\"export OUTPUT_PATH=\"gs://$BUCKET/samples/dataflow/landsat/output-images/\"export REGION=\"us-central1\"export GPU_TYPE=\"nvidia-tesla-t4\"gcloud builds submit \\\u00a0 \u00a0 --config run.yaml \\\u00a0 \u00a0 --substitutions _JOB_NAME=$JOB_NAME,_OUTPUT_PATH=$OUTPUT_PATH,_REGION=$REGION,_GPU_TYPE=$GPU_TYPE \\\u00a0 \u00a0 --no-source\n```\nReplace the following:- : the Google Cloud project name\n- : the Cloud Storage bucket name (without the`gs://`prefix)\nAfter you run this pipeline, wait for the command to finish. If you exit your shell, you might lose the environment variables that you've set.\nTo avoid sharing the GPU between multiple worker processes, this sample uses a machine type with 1 vCPU. The memory requirements of the pipeline are addressed by using 13\u00a0GB of extended memory. For more information, read [GPUs and worker parallelism](/dataflow/docs/gpu/develop-with-gpus#parallelism) .## View your resultsThe pipeline in [tensorflow-landsat/main.py](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/dataflow/gpu-examples/tensorflow-landsat/main.py) processes Landsat 8 satellite images and renders them as JPEG files. Use the following steps to view these files.- List the output JPEG files with details by using [gsutil](https://cloud.google.com/storage/docs/gsutil) .```\ngsutil ls -lh \"gs://$BUCKET/samples/dataflow/landsat/\"\n```\n- Copy the files into your local directory.```\nmkdir outputsgsutil -m cp \"gs://$BUCKET/samples/dataflow/landsat/*\" outputs/\n```\n- Open these image files with the image viewer of your choice.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the project\nThe easiest way to eliminate billing is to delete the project that you created for the tutorial.\nTo delete the project:\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Look at a [minimal GPU-enabled TensorFlow example](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/master/dataflow/gpu-examples/tensorflow-minimal) \n- Look at a [minimal GPU-enabled PyTorch example](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/master/dataflow/gpu-examples/pytorch-minimal) \n- Learn more about [GPU support onDataflow](/dataflow/docs/gpu/gpu-support) .\n- Look through tasks for [Using GPUs](/dataflow/docs/gpu/use-gpus) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Dataflow"}