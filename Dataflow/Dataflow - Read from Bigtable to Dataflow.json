{"title": "Dataflow - Read from Bigtable to Dataflow", "url": "https://cloud.google.com/dataflow/docs/guides/read-from-bigtable", "abstract": "# Dataflow - Read from Bigtable to Dataflow\nTo read data from Bigtable to Dataflow, use the Apache Beam [Bigtable I/O connector](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigtable/package-summary.html) .\n**Note:** Depending on your scenario, consider using one of the [Google-provided Dataflow templates](/dataflow/docs/guides/templates/provided-templates) . Several of these read from Bigtable.\n", "content": "## Parallelism\nParallelism is controlled by the number of [nodes](/bigtable/docs/instances-clusters-nodes#nodes) in the Bigtable cluster. Each node manages one or more key ranges, although key ranges can move between nodes as part of [load balancing](/bigtable/docs/overview#load-balancing) . For more information, see [Reads and performance](/bigtable/docs/reads#performance) in the Bigtable documentation.\nYou are charged for the number of nodes in your instance's clusters. See [Bigtable pricing](/bigtable/pricing) .\n## Performance\nThe following table shows performance metrics for Bigtable read operations. The workloads were run on one `e2-standard2` worker, using the Apache Beam SDK 2.48.0 for Java. They did not use Runner v2.\n| 100 M records | 1 kB | 1 column | Throughput (bytes) | Throughput (elements)  |\n|:----------------------------------|:---------------------|:----------------------------|\n| Read        | 180 MBps    | 170,000 elements per second |\nThese metrics are based on simple batch pipelines. They are intended to compare performance between I/O connectors, and are not necessarily representative of real-world pipelines. Dataflow pipeline performance is complex, and is a function of VM type, the data being processed, the performance of external sources and sinks, and user code. Metrics are based on running the Java SDK, and aren't representative of the performance characteristics of other language SDKs. For more information, see [Beam IO Performance](https://beam.apache.org/performance/) .\n## Best practices\n- For new pipelines, use the [BigtableIO](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigtable/BigtableIO.html) connector, not `CloudBigtableIO` .\n- Create separate [app profiles](/bigtable/docs/app-profiles) for each type of pipeline. App profiles enable better metrics for differentiating traffic between pipelines, both for support and for tracking usage.\n- Monitor the Bigtable nodes. If you experience performance bottlenecks, check whether resources such as CPU utilization are constrained within Bigtable. For more information, see [Monitoring](/bigtable/docs/monitoring-instance) .\n- In general, the default timeouts are well tuned for most pipelines. If a streaming pipeline appears to get stuck reading from Bigtable, try calling [withAttemptTimeout](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigtable/BigtableIO.Read.html#withAttemptTimeout-org.joda.time.Duration-) to adjust the attempt timeout.\n- Consider enabling [Bigtable autoscaling](/bigtable/docs/autoscaling) , or resize the Bigtable cluster to scale with the size of your Dataflow jobs.\n- Consider setting [maxNumWorkers](/dataflow/docs/reference/pipeline-options#resource_utilization) on the Dataflow job to limit load on the Bigtable cluster.\n- If significant processing is done on a Bigtable element before a shuffle, calls to Bigtable might time out. In that case, you can call [withMaxBufferElementCount](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigtable/BigtableIO.Read.html#withMaxBufferElementCount-java.lang.Integer-) to buffer elements. This method converts the read operation from streaming to paginated, which avoids the issue.\n- If you use a single Bigtable cluster for both streaming and batch pipelines, and the performance degrades on the Bigtable side, consider setting up replication on the cluster. Then separate the batch and streaming pipelines, so that they read from different replicas. For more information, see [Replication overview](/bigtable/docs/replication-overview) .## What's next\n- Read the [Bigtable I/O connector](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigtable/package-summary.html) documentation.\n- See the list of [Google-provided templates](/dataflow/docs/guides/templates/provided-templates) .", "guide": "Dataflow"}