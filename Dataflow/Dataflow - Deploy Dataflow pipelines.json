{"title": "Dataflow - Deploy Dataflow pipelines", "url": "https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline", "abstract": "# Dataflow - Deploy Dataflow pipelines\nThis document provides an overview of pipeline deployment and highlights some of the operations you can perform on a deployed pipeline.\n**Note:** This page has undergone content movement. The sections below contain links to content that used to appear on this page that has moved elsewhere.\n", "content": "## Run your pipeline\nAfter you [create](https://beam.apache.org/documentation/pipelines/create-your-pipeline/) and [test](https://beam.apache.org/documentation/pipelines/test-your-pipeline/) your Apache Beam pipeline, run your pipeline. You can run your pipeline locally, which lets you test and debug your Apache Beam pipeline, or on Dataflow, a data processing system available for running Apache Beam pipelines.\n### Run locally\nRun your pipeline locally:\nThe following example code, taken from the quickstart, shows how to run the WordCount  pipeline locally. To learn more, see how to [run your Java pipeline locally](/dataflow/docs/quickstarts/create-pipeline-java#run-the-pipeline-locally) .\nIn your terminal, run the following command:\n```\n mvn compile exec:java \\\n  -Dexec.mainClass=org.apache.beam.examples.WordCount \\\n  -Dexec.args=\"--output=counts\"\n \n```The following example code, taken from the quickstart, shows how to run the WordCount  pipeline locally. To learn more, see how to [run your Python pipeline locally](/dataflow/docs/quickstarts/create-pipeline-python#run-the-pipeline-locally) .\nIn your terminal, run the following command:\n```\npython -m apache_beam.examples.wordcount \\ --output outputs\n```\n **Note:** This example demonstrates how to run the pipeline using a module file. For  different Python implementations, including how to run Python code contained in a script, see [Command line and environment](https://docs.python.org/3/using/cmdline.html) in the Python documentation.\nThe following example code, taken from the quickstart, shows how to run the WordCount  pipeline locally. To learn more, see how to [run your Go pipeline locally](/dataflow/docs/quickstarts/create-pipeline-go#run_the_unmodified_pipeline) .\nIn your terminal, run the following command:\n```\n go run wordcount.go --input gs://dataflow-samples/shakespeare/kinglear.txt \\ --output outputs\n \n```\nLearn how to run your pipeline locally, on your machine, [using the  direct runner](https://beam.apache.org/documentation/runners/direct/) .\n### Run on Dataflow\nRun your pipeline on Dataflow:\nThe following example code, taken from the quickstart, shows how to run the WordCount  pipeline on Dataflow. To learn more, see how to [run your Java pipeline on Dataflow](/dataflow/docs/quickstarts/create-pipeline-java#run-the-pipeline-on-the-dataflow-service) .\nIn your terminal, run the following command (from your `word-count-beam` directory):\n```\n mvn -Pdataflow-runner compile exec:java \\\n -Dexec.mainClass=org.apache.beam.examples.WordCount \\\n -Dexec.args=\"--project=PROJECT_ID \\\n --gcpTempLocation=gs://BUCKET_NAME/temp/ \\\n --output=gs://BUCKET_NAME/output \\\n --runner=DataflowRunner \\\n --region=REGION\"\n \n```\nReplace the following:- ``: your Google Cloud project ID\n- ``: the name of your Cloud Storage bucket\n- ``: a [Dataflow region](/dataflow/docs/resources/locations) , like`us-central1`\nThe following example code, taken from the quickstart, shows how to run the WordCount  pipeline on Dataflow. To learn more, see how to [run your Python pipeline on Dataflow](/dataflow/docs/quickstarts/create-pipeline-python#run-the-pipeline-on-the-dataflow-service) .\nIn your terminal, run the following command:\n```\npython -m apache_beam.examples.wordcount \\\u00a0 \u00a0 --region DATAFLOW_REGION \\\u00a0 \u00a0 --input gs://dataflow-samples/shakespeare/kinglear.txt \\\u00a0 \u00a0 --output gs://STORAGE_BUCKET/results/outputs \\\u00a0 \u00a0 --runner DataflowRunner \\\u00a0 \u00a0 --project PROJECT_ID \\\u00a0 \u00a0 --temp_location gs://STORAGE_BUCKET/tmp/\n```\nReplace the following:- ``: the [region](/dataflow/docs/resources/locations) where you want to deploy the Dataflow job\u2014for example,`europe-west1`The `--region` flag overrides the default region that is   set in the metadata server, your local client, or environment   variables.\n- ``: the   Cloud Storage name that [you copied  earlier](#Bucket) \n- ``: the Google Cloud project ID   that [you copied earlier](#Project) \nThe following example code, taken from the quickstart, shows how to run the WordCount pipeline on Dataflow. To learn more, see how to [run your Go pipeline on Dataflow](/dataflow/docs/quickstarts/create-pipeline-go#run_the_pipeline_on_the_service) .\nIn your terminal, run the following command:\n```\n\u00a0 posix-terminal go run wordcount.go --input gs://dataflow-samples/shakespeare/kinglear.txt \\\u00a0 \u00a0 --output gs://STORAGE_BUCKET/results/outputs \\\u00a0 \u00a0 --runner dataflow \\\u00a0 \u00a0 --project PROJECT_ID \\\u00a0 \u00a0 --region DATAFLOW_REGION \\\u00a0 \u00a0 --staging_location gs://STORAGE_BUCKET/binaries/\u00a0 \n```\nReplace the following:- ``: The Cloud Storage bucket name.\n- ``: the Google Cloud project ID.\n- ``: The [region](/dataflow/docs/resources/locations) where you want to deploy the Dataflow job. For example,`europe-west1`.   For a list of available locations, see [Dataflow locations](/dataflow/docs/resources/locations) .   Note that the`--region`flag overrides the default region that is set in the metadata   server, your local client, or environment variables.\nLearn how to run your pipeline on the Dataflow service, [using the Dataflow runner](https://beam.apache.org/documentation/runners/dataflow/) .\nWhen you run your pipeline on Dataflow, Dataflow turns your Apache Beam pipeline code into a Dataflow job. Dataflow fully manages Google Cloud services for you, such as [Compute Engine](/compute) and [Cloud Storage](/storage) to run your Dataflow job, and automatically spins up and tears down necessary resources. You can learn more about how Dataflow turns your Apache Beam code into a Dataflow job in [Pipeline lifecycle](/dataflow/docs/pipeline-lifecycle) .\n## Pipeline validation\nWhen you run your pipeline on Dataflow, before the job launches, Dataflow performs validation tests on the pipeline. When a validation test finds problems with the pipeline, Dataflow fails the job submission early. In the [job logs](/dataflow/docs/guides/logging#MonitoringLogs) , Dataflow includes messages with the following text. Each message also includes details about the validation findings and instructions for resolving the issue.\n```\nThe preflight pipeline validation failed for job JOB_ID.\n```\nWhich validation tests run depends on the resources and services that your Dataflow job uses.\n- If the [Service Usage API](/service-usage/docs/reference/rest) is enabled for your project, the pipeline validation tests check whether the  services needed to run your Dataflow job are enabled.\n- If the [Cloud Resource Manager API](/resource-manager/reference/rest) is enabled for your project, the pipeline validation tests check whether  you have the project-level configurations needed to run your  Dataflow job.\nFor more information about enabling services, see [Enabling and disabling services](/service-usage/docs/enable-disable) .\nFor information about how to resolve permission issues caught during pipeline validation, see [Pipeline validation failed](/dataflow/docs/guides/troubleshoot-permissions#validation-failed) .\nIf you want to override the pipeline validation and launch your job with validation errors, use the following pipeline [service option](/dataflow/docs/reference/service-options) :\n```\n--dataflowServiceOptions=enable_preflight_validation=false\n``````\n--dataflow_service_options=enable_preflight_validation=false\n``````\n--dataflow_service_options=enable_preflight_validation=false\n```\n## Set pipeline options\nYou can control some aspects of how Dataflow runs your job by setting [pipeline options](/dataflow/docs/guides/setting-pipeline-options) in your Apache Beam pipeline code. For example, you can use pipeline options to set whether your pipeline runs on worker virtual machines, on the Dataflow service backend, or locally.\n## Manage pipeline dependencies\nMany Apache Beam pipelines can run using the default Dataflow runtime environments. However, some data processing use cases benefit from using additional libraries or classes. In these cases, you might need to manage your pipeline dependencies. For more information about managing dependencies, see [Manage pipeline dependencies in Dataflow](/dataflow/docs/guides/manage-dependencies) .\n## Monitor your job\nDataflow provides visibility into your jobs through tools like the [Dataflow monitoring interface](/dataflow/pipelines/dataflow-monitoring-intf) and the [Dataflow command-line interface](/dataflow/pipelines/dataflow-command-line-intf) .\n## Access worker VMs\nYou can view the VM instances for a given pipeline by using the Google Cloud console. From there, you can use SSH to access each instance. However, after your job either completes or fails, the Dataflow service automatically shuts down and cleans up the VM instances.\n## Job optimizations\nIn addition to managing Google Cloud resources, Dataflow automatically performs and optimizes many aspects of distributed parallel processing for you.\n### Parallelization and distribution\nDataflow automatically partitions your data and distributes your worker code to Compute Engine instances for parallel processing. For more information, see [parallelization and distribution](/dataflow/docs/pipeline-lifecycle#parallelization_and_distribution) .\n### Fusion and combine optimizations\nDataflow uses your pipeline code to create  an execution graph that represents your pipeline's `PCollection` s and transforms,  and optimizes the graph for the most efficient performance and resource usage.  Dataflow also automatically optimizes potentially costly operations, such as data  aggregations. For more information, see [Fusion optimization](/dataflow/docs/pipeline-lifecycle#fusion_optimization) and [Combine optimization](/dataflow/docs/pipeline-lifecycle#combine_optimization) .\n### Automatic tuning features\nThe Dataflow service includes several features  that provide on-the-fly adjustment of resource allocation and data partitioning. These features  help Dataflow execute your job as quickly and efficiently as possible. These  features include the following:\n- [Horizontal Autoscaling](/dataflow/docs/horizontal-autoscaling) \n- [Vertical Autoscaling](/dataflow/docs/vertical-autoscaling) \n- [Dynamic work rebalancing](/dataflow/docs/dynamic-work-rebalancing) \n**Note:** With the introduction of Vertical Autoscaling, the feature that was previously  called Autoscaling has been renamed to Horizontal Autoscaling.\n### Streaming Engine\nBy default, the Dataflow pipeline runner executes the steps of your streaming pipeline entirely on worker virtual machines, consuming worker CPU, memory, and Persistent Disk storage. Dataflow's Streaming Engine moves pipeline execution out of the worker VMs and into the Dataflow service backend. For more information, see [Streaming Engine](/dataflow/docs/streaming-engine) .\n### Dataflow Flexible Resource Scheduling\nDataflow FlexRS reduces batch processing costs by using [advanced scheduling techniques](/dataflow/docs/guides/flexrs#delayed_scheduling) , the [Dataflow Shuffle](/dataflow/docs/shuffle-for-batch) service, and a combination of [preemptible virtual machine (VM) instances](/compute/docs/instances/preemptible) and regular VMs. By running preemptible VMs and regular VMs in parallel, Dataflow improves the user experience if Compute Engine stops preemptible VM instances during a system event. FlexRS helps to ensure that the pipeline continues to make progress and that you do not lose previous work when [Compute Engine preempts](/compute/docs/instances/preemptible#what_is_a_preemptible_instance) your preemptible VMs. For more information about FlexRS, see [Using Flexible Resource Scheduling in Dataflow](/dataflow/docs/guides/flexrs) .\n### Dataflow Shielded VM\nStarting on June 1, 2022, the Dataflow service uses [Shielded VM](/security/shielded-cloud/shielded-vm) for all workers. To learn more about Shielded VM capabilities, see [Shielded VM](/security/shielded-cloud/shielded-vm) .", "guide": "Dataflow"}