{"title": "Dataflow - Configure Flex Templates", "url": "https://cloud.google.com/dataflow/docs/guides/templates/configuring-flex-templates", "abstract": "# Dataflow - Configure Flex Templates\nThis page documents various Dataflow Flex Template configuration options, including:\n- [Permissions](#permissions) \n- [Dockerfile environment variables](#env-variables) \n- [Package dependencies](#dependencies) \n- [Docker images](#images) \n- [Pipeline options](#specify-options) \n- [Staging and temp locations](#locations) \nTo configure a sample Flex Template, see the [Flex Template tutorial](/dataflow/docs/guides/templates/using-flex-templates) .\n", "content": "## Understand Flex Template permissions\nWhen you're working with Flex Templates, you need three sets of permissions:\n- Permissions to create resources\n- Permissions to build a Flex Template\n- Permissions to run a Flex Template\n### Permissions to create resources\nTo develop and run a Flex Template pipeline, you need to create various resources (for example, a staging bucket). For one-time resource creation tasks, you can use the [basic Owner role](/iam/docs/understanding-roles#basic) .\n### Permissions to build a Flex Template\nAs the developer of a Flex Template, you need to build the template to make it available to users. Building involves uploading a template spec to a Cloud Storage bucket and provisioning a Docker image with the code and dependencies needed to run the pipeline. To build a Flex Template, you need read and write access to Cloud Storage and [Artifact Registry Writer access](/artifact-registry/docs/access-control#permissions) to your Artifact Registry repository. You can grant these permissions by assigning the following roles:\n- Storage Admin (`roles/storage.admin`)\n- Cloud Build Editor (`roles/cloudbuild.builds.editor`)\n- Artifact Registry Writer (`roles/artifactregistry.writer`)\n### Permissions to run a Flex Template\nWhen you run a Flex Template, Dataflow creates a job for you. To create the job, the Dataflow service account needs the following permission:\n- `dataflow.serviceAgent`\nWhen you first use Dataflow, the service assigns this role for you, so you don't need to grant this permission.\nBy default, the Compute Engine service account is used for launcher VMs and worker VMs. The service account needs the following roles and abilities:\n- Storage Object Admin (`roles/storage.objectAdmin`)\n- Viewer (`roles/viewer`)\n- Dataflow Worker (`roles/dataflow.worker`)\n- Read and write access to the staging bucket\n- Read access to the Flex Template image\nTo grant read and write access to the staging bucket, you can use the role Storage Object Admin ( `roles/storage.objectAdmin` ). For more information, see [IAM roles for Cloud Storage](/storage/docs/access-control/iam-roles) .\nTo grant read access to the Flex Template image, you can use the role Storage Object Viewer ( `roles/storage.objectViewer` ). For more information, see [Configuring access control](/artifact-registry/docs/access-control) .\n## Set required Dockerfile environment variables\nIf you want to create your own Dockerfile for a Flex Template job, specify the following environment variables:\nSpecify `FLEX_TEMPLATE_JAVA_MAIN_CLASS` and `FLEX_TEMPLATE_JAVA_CLASSPATH` in your Dockerfile.\n| ENV       | Description                 | Required |\n|:------------------------------|:----------------------------------------------------------------------------|:-----------|\n| FLEX_TEMPLATE_JAVA_MAIN_CLASS | Specifies which Java class to run in order to launch the Flex Template.  | YES  |\n| FLEX_TEMPLATE_JAVA_CLASSPATH | Specifies the location of class files.          | YES  |\n| FLEX_TEMPLATE_JAVA_OPTIONS | Specifies the Java options to be passed while launching the Flex Template. | NO   |\nSpecify `FLEX_TEMPLATE_PYTHON_PY_FILE` in your Dockerfile.\nTo manage pipeline dependencies, set variables in your Dockerfile, such as the following:- `FLEX_TEMPLATE_PYTHON_REQUIREMENTS_FILE`\n- `FLEX_TEMPLATE_PYTHON_PY_OPTIONS`\n- `FLEX_TEMPLATE_PYTHON_SETUP_FILE`\n- `FLEX_TEMPLATE_PYTHON_EXTRA_PACKAGES`\nFor example, the following environment variables are set in the [Streaming in Python Flex Template tutorial](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/dataflow/flex-templates/streaming_beam/Dockerfile) in GitHub:\n```\nENV FLEX_TEMPLATE_PYTHON_REQUIREMENTS_FILE=\"${WORKDIR}/requirements.txt\"ENV FLEX_TEMPLATE_PYTHON_PY_FILE=\"${WORKDIR}/streaming_beam.py\"\n```\n| ENV         | Description                                   | Required |\n|:---------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------|\n| FLEX_TEMPLATE_PYTHON_PY_FILE   | Specifies which Python file to run to launch the Flex Template.                      | YES  |\n| FLEX_TEMPLATE_PYTHON_REQUIREMENTS_FILE | Specifies the requirements file with pipeline dependencies. For more information, see PyPI dependencies in the Apache Beam documentation.   | NO   |\n| FLEX_TEMPLATE_PYTHON_SETUP_FILE  | Specifies the path to the pipeline package `setup.py` file. For more information, see Multiple File Dependencies in the Apache Beam documentation. | NO   |\n| FLEX_TEMPLATE_PYTHON_EXTRA_PACKAGES | Specifies the packages that are not available publicly. For information on how using extra packages, read Local or non-PyPI Dependencies.   | NO   |\n| FLEX_TEMPLATE_PYTHON_PY_OPTIONS  | Specifies the Python options to be passed while launching the Flex Template.                   | NO   |\n## Package dependencies\nWhen a Dataflow Python pipeline uses additional dependencies, you might need to configure the Flex Template to install additional dependencies on Dataflow worker VMs.\nWhen you run a Python Dataflow job that uses Flex Templates in an environment that restricts access to the internet, you must prepackage the dependencies when you create the template.\nUse one of the following options to prepackage Python dependencies.\n### Use a requirements file and prepackage the dependencies with the template\nIf you are using your own Dockerfile to define the Flex Template image, follow these steps:\n- Create a `requirements.txt` file that lists your pipeline dependencies.```\nCOPY requirements.txt /template/ENV FLEX_TEMPLATE_PYTHON_REQUIREMENTS_FILE=\"/template/requirements.txt\"\n```\n- Install the dependencies in the Flex Template image.```\nRUN pip install --no-cache-dir -r $FLEX_TEMPLATE_PYTHON_REQUIREMENTS_FILE\n```\n- Download the dependencies into the local requirements cache, which is staged to the Dataflow workers when the template launches.```\nRUN pip download --no-cache-dir --dest /tmp/dataflow-requirements-cache -r $FLEX_TEMPLATE_PYTHON_REQUIREMENTS_FILE\n```\nThe following is a code sample that pre-downloads dependencies.\n[  dataflow/flex-templates/streaming_beam/Dockerfile ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataflow/flex-templates/streaming_beam/Dockerfile) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataflow/flex-templates/streaming_beam/Dockerfile)\n```\n# Copyright 2020 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.FROM gcr.io/dataflow-templates-base/python3-template-launcher-base# Configure the Template to launch the pipeline with a --requirements_file option.# See: https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/#pypi-dependenciesENV FLEX_TEMPLATE_PYTHON_REQUIREMENTS_FILE=\"/template/requirements.txt\"ENV FLEX_TEMPLATE_PYTHON_PY_FILE=\"/template/streaming_beam.py\"COPY . /templateRUN apt-get update \\\u00a0 \u00a0 # Install any apt packages if required by your template pipeline.\u00a0 \u00a0 && apt-get install -y libffi-dev git \\\u00a0 \u00a0 && rm -rf /var/lib/apt/lists/* \\\u00a0 \u00a0 # Upgrade pip and install the requirements.\u00a0 \u00a0 && pip install --no-cache-dir --upgrade pip \\\u00a0 \u00a0 # Install dependencies from requirements file in the launch environment.\u00a0 \u00a0 && pip install --no-cache-dir -r $FLEX_TEMPLATE_PYTHON_REQUIREMENTS_FILE \\\u00a0 \u00a0 # When FLEX_TEMPLATE_PYTHON_REQUIREMENTS_FILE \u00a0option is used,\u00a0 \u00a0 # then during Template launch Beam downloads dependencies\u00a0 \u00a0 # into a local requirements cache folder and stages the cache to workers.\u00a0 \u00a0 # To speed up Flex Template launch, pre-download the requirements cache\u00a0 \u00a0 # when creating the Template.\u00a0 \u00a0 && pip download --no-cache-dir --dest /tmp/dataflow-requirements-cache -r $FLEX_TEMPLATE_PYTHON_REQUIREMENTS_FILE# Set this if using Beam 2.37.0 or earlier SDK to speed up job submission.ENV PIP_NO_DEPS=TrueENTRYPOINT [\"/opt/google/dataflow/python_template_launcher\"]\n```\n### Use a custom container that preinstalls all dependencies\nThis option is preferred for pipelines that run in environments without internet access.\nFollow these steps to use a custom container:\n- Build a custom container that [preinstalls necessary dependencies](/dataflow/docs/guides/build-container-image#preinstall_using_a_dockerfile) .\n- Preinstall the same dependencies in the Flex Template Dockerfile. Don't use the `FLEX_TEMPLATE_PYTHON_REQUIREMENTS_FILE` option.A modified `dataflow/flex-templates/streaming_beam/Dockerfile` might look like the following example:```\nFROM gcr.io/dataflow-templates-base/python3-template-launcher-baseENV FLEX_TEMPLATE_PYTHON_PY_FILE=\"/template/streaming_beam.py\"COPY . /templateRUN pip install --no-cache-dir -r /template/requirements.txt\n```Alternatively, to reduce the number of images to maintain, use your [custom container image as a base image for the Flex Template](#use_custom_container_images) .\n- If you use the Apache Beam SDK version 2.49.0 or earlier, add the `--sdk_location=container` pipeline option in your pipeline launcher. This option tells your pipeline to use the SDK from your custom container instead of downloading the SDK.```\noptions = PipelineOptions(beam_args, save_main_session=True, streaming=True, sdk_location=\"container\")\n```\n- Set the `sdk_container_image` parameter in the `flex-template run` command. For example:```\ngcloud dataflow flex-template run $JOB_NAME \\\u00a0 \u00a0--region=$REGION \\\u00a0 \u00a0--template-file-gcs-location=$TEMPLATE_PATH \\\u00a0 \u00a0--parameters=sdk_container_image=$CUSTOM_CONTAINER_IMAGE \\\u00a0 \u00a0--additional-experiments=use_runner_v2\n```For more information, see [Use custom containers in Dataflow](/dataflow/docs/guides/using-custom-containers) .\n### Structure the pipeline as a package\nIf you structure your pipeline as a package, use the `FLEX_TEMPLATE_PYTHON_SETUP_FILE` option. For more information about structuring your pipeline as a package, see [Multiple file dependencies](https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/#multiple-file-dependencies) in the Apache Beam documentation.\nIf you use your own Dockerfile to define the Flex Template image, install the package in the Dockerfile.\nYour Flex Template Dockerfile might include the following:\n```\n\u00a0 COPY setup.py .\u00a0 COPY main.py .\u00a0 COPY package_name package_name\u00a0 RUN pip install -e .\u00a0 ENV FLEX_TEMPLATE_PYTHON_SETUP_FILE=\"${WORKDIR}/setup.py\"\u00a0 ENV FLEX_TEMPLATE_PYTHON_PY_FILE=\"${WORKDIR}/main.py\"\n```\nYou can use this method and a custom container image to preinstall dependencies in the runtime environment.\nFor an example that follows this approach, see the [Flex Template for a pipeline with dependencies and a custom container image](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/dataflow/flex-templates/pipeline_with_dependencies) tutorial in GitHub.\n## Choose a base image\nYou can use a Google-provided [base image](https://docs.docker.com/glossary/#base_image) to package your template container images by using Docker. Choose the most recent tag from the [Flex Templates base images](/dataflow/docs/reference/flex-templates-base-images) . It's recommended to use a specific image tag instead of `latest` .\nSpecify the base image in the following format:\n```\ngcr.io/dataflow-templates-base/IMAGE_NAME:TAG\n```\nReplace the following:\n- ``: a Google-provided base image\n- ``: a version name for the base image, found in the [Flex Templates base imagesreference](/dataflow/docs/reference/flex-templates-base-images) \n### Use custom container images\nIf your pipeline uses a [custom container image](/dataflow/docs/guides/using-custom-containers) , we recommend using the custom image as a base image for your Flex Template Docker image. To do so, copy the Flex Template launcher binary from the Google-provided [template base image](/dataflow/docs/reference/flex-templates-base-images) onto your custom image.\nAn example `Dockerfile` for an image that can be used both as Custom SDK container image and as a Flex Template, might look like the following:\n```\nFROM gcr.io/dataflow-templates-base/IMAGE_NAME:TAG as template_launcherFROM apache/beam_python3.10_sdk:2.54.0# RUN <...Make image customizations here...># See: https://cloud.google.com/dataflow/docs/guides/build-container-image# Configure the Flex Template here.COPY --from=template_launcher /opt/google/dataflow/python_template_launcher /opt/google/dataflow/python_template_launcherCOPY my_pipeline.py /template/ENV FLEX_TEMPLATE_PYTHON_PY_FILE=\"/template/my_pipeline.py\"\n```\nReplace the following:\n- ``: a Google-provided base image. For example:`python311-template-launcher-base`.\n- ``: a version tag for the base image found in the [Flex Templates base imagesreference](/dataflow/docs/reference/flex-templates-base-images) . For better stability and troubleshooting, avoid using`latest`. Instead, pin to a specific version tag.\nFor an example that follows this approach, see the [Flex Template for a pipeline with dependencies and a custom container image](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/dataflow/flex-templates/pipeline_with_dependencies) tutorial.\n### Use an image from a private registry\nYou can build a Flex Template image stored in a private [Docker registry](https://docs.docker.com/registry/) , if the private registry uses HTTPS and has a valid certificate.\nTo use an image from a private registry, specify the path to the image and a username and password for the registry. The username and password must be stored in [Secret Manager](/secret-manager/docs) . You can provide the secret in one of the following formats:\n- `projects/` `` `/secrets/` `` `/versions/` ``\n- `projects/` `` `/secrets/` ``\nIf you use the second format, because it doesn't specify the version, Dataflow uses the latest version.\nIf the registry uses a self-signed certificate, you also need to specify the path to the self-signed certificate in Cloud Storage.\nThe following table describes the gcloud CLI options that you can use to configure a private registry.\n| Parameter       | Description                                                   |\n|:------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| image        | The address of the registry. For example: gcp.repository.example.com:9082/registry/example/image:latest.                           |\n| image-repository-username-secret-id | The Secret Manager secret ID for the username to authenticate to the private registry. For example: projects/example-project/secrets/username-secret.                |\n| image-repository-password-secret-id | The Secret Manager secret ID for the password to authenticate to the private registry. For example: projects/example-project/secrets/password-secret/versions/latest.            |\n| image-repository-cert-path   | The full Cloud Storage URL for a self-signed certificate for the private registry. This value is only required if the registry uses a self-signed certificate. For example: gs://example-bucket/self-signed.crt. |\nHere's an example Google Cloud CLI command that builds a Flex Template using an image in a private registry with a self-signed certificate.\n```\ngcloud dataflow flex-template build gs://example-bucket/custom-pipeline-private-repo.json--sdk-language=JAVA--image=\"gcp.repository.example.com:9082/registry/example/image:latest\"--image-repository-username-secret-id=\"projects/example-project/secrets/username-secret\"--image-repository-password-secret-id=\"projects/example-project/secrets/password-secret/versions/latest\"--image-repository-cert-path=\"gs://example-bucket/self-signed.crt\"--metadata-file=metadata.json\n```\nTo build your own Flex Template, you need to replace the example values, and you might need to specify different or additional options. To learn more, see the following resources:\n- [gcloud dataflow flex-template build](/sdk/gcloud/reference/dataflow/flex-template/build) \n- [Using Flex Templates](/dataflow/docs/guides/templates/using-flex-templates) ## Specify pipeline options\nFor information about pipeline options that are directly supported by Flex Templates, see [Pipeline options](/dataflow/docs/reference/pipeline-options) .\nYou can also use any Apache Beam pipeline options indirectly. If you're using a [metadata.json file](#metadata) for your Flex Template job, include these pipeline options in the file. This metadata file must follow the format in [TemplateMetadata](https://developers.google.com/resources/api-libraries/documentation/dataflow/v1b3/java/latest/com/google/api/services/dataflow/model/TemplateMetadata.html) .\nOtherwise, when you launch the Flex Template job, pass these pipeline options by using the parameters field.\nInclude pipeline options by using the [parameters](/dataflow/docs/reference/rest/v1b3/projects.locations.flexTemplates/launch#launchflextemplateparameter) field.\nInclude pipeline options by using the [parameters](/sdk/gcloud/reference/dataflow/flex-template/run#--parameters) flag.\nWhen passing parameters of `List` or `Map` type, you might need to define parameters in a YAML file and use the [flags-file](/sdk/gcloud/reference#--flags-file) . For an example of this approach, view the [\"Create a file with parameters...\" step in this solution](/architecture/building-a-vision-analytics-solution#running_the_dataflow_pipeline_for_a_set_of_vision_features) .\nWhen using Flex Templates, you can configure some pipeline options during pipeline initialization, but other pipeline options can't be changed. If the command line arguments required by the Flex Template are overwritten, the job might ignore, override, or discard the pipeline options passed by the template launcher. The job might fail to launch, or a job that doesn't use the Flex Template might launch. For more information, see [Failed to read the job file](/dataflow/docs/guides/troubleshoot-templates#read-job-file) .\nDuring pipeline initialization, don't change the following [pipeline options](/dataflow/docs/reference/pipeline-options) :\n- `runner`\n- `project`\n- `jobName`\n- `templateLocation`\n- `region`\n- `runner`\n- `project`\n- `job_name`\n- `template_location`\n- `region`\n- `runner`\n- `project`\n- `job_name`\n- `template_location`\n- `region`\n### Block project SSH keys from VMs that use metadata-based SSH keys\nYou can prevent VMs from accepting SSH keys that are stored in project metadata by blocking project SSH keys from VMs. Use the `additional-experiments` flag with the `block_project_ssh_keys` service option:\n```\n--additional-experiments=block_project_ssh_keys\n```\nFor more information, see [Dataflow service options](/dataflow/docs/reference/service-options) .\n## Metadata\nYou can extend your template with additional metadata so that custom parameters are validated when the template is run. If you want to create metadata for your template, follow these steps:\n- Create a`metadata.json`file using the parameters in [Metadata parameters](#metadataparameters) .To view an example, see [Example metadata file](#example-metadata-file) .\n- Store the metadata file in Cloud Storage in the same folder as the template.\n### Metadata parameters| Parameter key  | Unnamed: 1 | Required | Description of the value                                                                       |\n|:--------------------|:-------------|:-----------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| name    | nan   | Yes  | The name of your template.                                                                       |\n| description   | nan   | No   | A short paragraph of text describing the template.                                                                 |\n| streaming   | nan   | No   | If true, this template supports streaming. The default value is false.                                                            |\n| supportsAtLeastOnce | nan   | No   | If true, this template supports at-least-once processing. The default value is false. Set this parameter to true if the template is designed to work with at-least-once streaming mode.                               |\n| supportsExactlyOnce | nan   | No   | If true, this template supports exactly-once processing. The default value is true.                                                        |\n| parameters   | nan   | No   | An array of additional parameters that the template uses. An empty array is used by default.                                                      |\n| parameters   | name   | Yes  | The name of the parameter that is used in your template.                                                               |\n| parameters   | label  | Yes  | A human readable string that is used in the Google Cloud console to label the parameter.                                                       |\n| parameters   | helpText  | Yes  | A short paragraph of text that describes the parameter.                                                                |\n| parameters   | isOptional | No   | false if the parameter is required and true if the parameter is optional. Unless set with a value, isOptional defaults to false. If you do not include this parameter key for your metadata, the metadata becomes a required parameter.                   |\n| parameters   | regexes  | No   | An array of POSIX-egrep regular expressions in string form that is used to validate the value of the parameter. For example, [\"^[a-zA-Z][a-zA-Z0-9]+\"] is a single regular expression that validates that the value starts with a letter and then has one or more characters. An empty array is used by default. |\n### Example metadata file```\n{\u00a0 \"name\": \"Streaming Beam SQL\",\u00a0 \"description\": \"An Apache Beam streaming pipeline that reads JSON encoded messages from Pub/Sub, uses Beam SQL to transform the message data, and writes the results to a BigQuery\",\u00a0 \"parameters\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"name\": \"inputSubscription\",\u00a0 \u00a0 \u00a0 \"label\": \"Pub/Sub input subscription.\",\u00a0 \u00a0 \u00a0 \"helpText\": \"Pub/Sub subscription to read from.\",\u00a0 \u00a0 \u00a0 \"regexes\": [\u00a0 \u00a0 \u00a0 \u00a0 \"[a-zA-Z][-_.~+%a-zA-Z0-9]{2,}\"\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"name\": \"outputTable\",\u00a0 \u00a0 \u00a0 \"label\": \"BigQuery output table\",\u00a0 \u00a0 \u00a0 \"helpText\": \"BigQuery table spec to write to, in the form 'project:dataset.table'.\",\u00a0 \u00a0 \u00a0 \"isOptional\": true,\u00a0 \u00a0 \u00a0 \"regexes\": [\u00a0 \u00a0 \u00a0 \u00a0 \"[^:]+:[^.]+[.].+\"\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 }\u00a0 ]}\n``````\n{\u00a0 \"name\": \"Streaming beam Python flex template\",\u00a0 \"description\": \"Streaming beam example for python flex template.\",\u00a0 \"parameters\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"name\": \"input_subscription\",\u00a0 \u00a0 \u00a0 \"label\": \"Input PubSub subscription.\",\u00a0 \u00a0 \u00a0 \"helpText\": \"Name of the input PubSub subscription to consume from.\",\u00a0 \u00a0 \u00a0 \"regexes\": [\u00a0 \u00a0 \u00a0 \u00a0 \"projects/[^/]+/subscriptions/[a-zA-Z][-_.~+%a-zA-Z0-9]{2,}\"\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"name\": \"output_table\",\u00a0 \u00a0 \u00a0 \"label\": \"BigQuery output table name.\",\u00a0 \u00a0 \u00a0 \"helpText\": \"Name of the BigQuery output table name.\",\u00a0 \u00a0 \u00a0 \"isOptional\": true,\u00a0 \u00a0 \u00a0 \"regexes\": [\u00a0 \u00a0 \u00a0 \u00a0 \"([^:]+:)?[^.]+[.].+\"\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 }\u00a0 ]}\n```\nYou can download metadata files for the Google-provided templates from the  Dataflow [template directory](https://console.cloud.google.com/storage/browser/dataflow-templates/latest) .\n**Note:** Changing launch parameters in the `metadata.json` file might cause the job to ignore, override, or discard the pipeline options passed by the template launcher. The job might fail to launch, or a job that doesn't use the Flex Template might launch. For more information, see [Specify pipeline options](/dataflow/docs/guides/templates/configuring-flex-templates#specify-options) and [Failed to read the job file](/dataflow/docs/guides/troubleshoot-templates#read-job-file) .\n## Understand staging location and temp location\nThe Google Cloud CLI provides `--staging-location` and `--temp-location` options when you [run a flex template](/sdk/gcloud/reference/beta/dataflow/flex-template/run) . Similarly, the Dataflow REST API provides `stagingLocation` and `tempLocation` fields for [FlexTemplateRuntimeEnvironment](/dataflow/docs/reference/rest/v1b3/projects.locations.flexTemplates/launch#flextemplateruntimeenvironment) .\nFor Flex Templates, the staging location is the Cloud Storage URL that files are written to during the staging step of launching a template. Dataflow reads these staged files to create the template graph. The temp location is the Cloud Storage URL that temporary files are written to during the execution step.\n## Update a Flex Template job\nThe following example request shows you how to update a template streaming job by using the [projects.locations.flexTemplates.launch](/dataflow/docs/reference/rest/v1b3/projects.locations.flexTemplates/launch) method. If you want to use the gcloud CLI, see [Update an existing pipeline](/dataflow/docs/guides/updating-a-pipeline) .\nIf you want to update a classic template, use [projects.locations.templates.launch](/dataflow/docs/reference/rest/v1b3/projects.locations.templates/launch) instead.\n- Follow the steps to create a streaming job from a Flex Template. Send the following HTTP POST request with the modified values:```\nPOST https://dataflow.googleapis.com/v1b3/projects/PROJECT_ID/locations/REGION/flexTemplates:launch{\u00a0 \u00a0 \"launchParameter\": {\u00a0 \u00a0 \u00a0 \"update\": true\u00a0 \u00a0 \u00a0 \"jobName\": \"JOB_NAME\",\u00a0 \u00a0 \u00a0 \"parameters\": {\u00a0 \u00a0 \u00a0 \u00a0 \"input_subscription\": \"projects/PROJECT_ID/subscriptions/SUBSCRIPTION_NAME\",\u00a0 \u00a0 \u00a0 \u00a0 \"output_table\": \"PROJECT_ID:DATASET.TABLE_NAME\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \"containerSpecGcsPath\": \"STORAGE_PATH\"\u00a0 \u00a0 },}\n```- Replace``with your project ID.\n- Replace``with the Dataflow [region](/dataflow/docs/resources/locations) of the job that you're updating.\n- Replace``with the exact name of the job that you want to update.\n- Set`parameters`to your list of key-value pairs. The parameters listed are specific to this template example. If you're using a custom template, modify the parameters as needed. If you're using the example template, replace the following variables.- Replace``with your Pub/Sub subscription name.\n- Replace``with your with your BigQuery dataset name.\n- Replace``with your with your BigQuery table name.\n- Replace``with the Cloud Storage location of the template file. The location should start with`gs://`.\n- Use the `environment` parameter to change environment settings. For more information, see [FlexTemplateRuntimeEnvironment](/dataflow/docs/reference/rest/v1b3/projects.locations.flexTemplates/launch#FlexTemplateRuntimeEnvironment) .\n- Optional: To send your request using curl (Linux, macOS, or Cloud Shell), save the request to a JSON file, and then run the following command:```\ncurl -X POST -d \"@FILE_PATH\" -H \"Content-Type: application/json\" -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \u00a0https://dataflow.googleapis.com/v1b3/projects/PROJECT_ID/locations/REGION/flexTemplates:launch\n```Replace with the path to the JSON file that contains the request body.\n- Use the [Dataflow monitoring interface](/dataflow/docs/guides/monitoring-overview#access-monitoring-interface) to verify that a new job with the same name was created. This job has the status **Updated** .## Limitations\nThe following limitations apply to Flex Templates jobs:\n- You must use a Google-provided base image to package your containers using Docker. For a list of applicable images, see [Flex Template base images](/dataflow/docs/reference/flex-templates-base-images) .\n- The program that constructs the pipeline must exit after`run`is called in order for the pipeline to start.\n- `waitUntilFinish`(Java) and`wait_until_finish`(Python) are not supported.## What's next\n- To know more about Classic Templates, Flex Templates, and their use-case scenarios, see [Dataflow templates](/dataflow/docs/concepts/dataflow-templates) .\n- For Flex Templates troubleshooting information, see [Troubleshoot Flex Template timeouts](/dataflow/docs/guides/troubleshoot-templates) .\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Dataflow"}