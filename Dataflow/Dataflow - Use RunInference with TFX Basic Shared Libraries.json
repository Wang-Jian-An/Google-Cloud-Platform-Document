{"title": "Dataflow - Use RunInference with TFX Basic Shared Libraries", "url": "https://cloud.google.com/dataflow/docs/notebooks/run_inference_tensorflow_with_tfx?hl=zh-cn", "abstract": "# Dataflow - Use RunInference with TFX Basic Shared Libraries\n| 0     | 1      |\n|:--------------------|:----------------------|\n| Run in Google Colab | View source on GitHub |\nThis notebook demonstrates how to use the Apache Beam [RunInference](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.RunInference) transform with TensorFlow and [TFX Basic Shared Libraries](https://github.com/tensorflow/tfx-bsl) ( `tfx-bsl` ).\nUse this approach when your trained model uses a `tf.Example` input. If you have `numpy` or `tf.Tensor` inputs, see the [Apache Beam RunInference with TensorFlow](https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/beam-ml/run_inference_tensorflow.ipynb) notebook, which shows how to use the built-in TensorFlow model handlers.\nThe Apache Beam RunInference transform accepts a model handler generated from [tfx-bsl](https://github.com/tensorflow/tfx-bsl) by using `CreateModelHandler` .\nThis notebook demonstrates how to complete the following tasks:\n- Import`tfx-bsl`.\n- Build a simple TensorFlow model.\n- Set up example data.\n- Use the`tfx-bsl`model handler with the example data, and get a prediction inside an Apache Beam pipeline.\nFor more information about using RunInference, see [Get started with AI/ML pipelines](https://beam.apache.org/documentation/ml/overview/) in the Apache Beam documentation.\n", "content": "## Before you begin\nSet up your environment and download dependencies.\n### Import tfx-bsl\nFirst, import `tfx-bsl` . Creating a model handler is supported in `tfx-bsl` versions 1.10 and later.\n```\npip install tfx_bsl==1.10.0 --quietpip install protobuf --quietpip install apache_beam --quiet\n```\n### Authenticate with Google Cloud\nThis notebook relies on saving your model to Google Cloud. To use your Google Cloud account, authenticate this notebook.\n```\nfrom google.colab import authauth.authenticate_user()\n```\n### Import dependencies and set up your bucket\nUse the following code to import dependencies and to set up your Google Cloud Storage bucket.\nReplace `PROJECT_ID` and `BUCKET_NAME` with the ID of your project and the name of your bucket.\n**Important:** If an error occurs, restart your runtime.\n```\nimport argparseimport tensorflow as tffrom tensorflow import kerasfrom tensorflow_serving.apis import prediction_log_pb2import apache_beam as beamfrom apache_beam.ml.inference.base import RunInferenceimport tfx_bslfrom tfx_bsl.public.beam.run_inference import CreateModelHandlerfrom tfx_bsl.public import tfxiofrom tfx_bsl.public.proto import model_spec_pb2from tensorflow_metadata.proto.v0 import schema_pb2import numpyfrom typing import Dict, Text, Any, Tuple, Listfrom apache_beam.options.pipeline_options import PipelineOptionsproject = \"PROJECT_ID\"bucket = \"BUCKET_NAME\"save_model_dir_multiply = f'gs://{bucket}/tfx-inference/model/multiply_five/v1/'\n```\n## Create and test a simple model\nThis section creates and tests a model that predicts the 5 times multiplication table.\n### Create the model\nCreate training data, and then build a linear regression model.\n```\n# Create training data that represents the 5 times multiplication table for the numbers 0 to 99.# x is the data and y is the labels.x = numpy.arange(0, 100) \u00a0 # Examplesy = x * 5 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0# Labels# Build a simple linear regression model.# Note that the model has a shape of (1) for its input layer and expects a single int64 value.input_layer = keras.layers.Input(shape=(1), dtype=tf.float32, name='x')output_layer= keras.layers.Dense(1)(input_layer)model = keras.Model(input_layer, output_layer)model.compile(optimizer=tf.optimizers.Adam(), loss='mean_absolute_error')model.summary()\n```\n```\nModel: \"model\"\n_________________________________________________________________\n Layer (type)    Output Shape    Param # \n=================================================================\n x (InputLayer)    [(None, 1)]    0   \n                 \n dense (Dense)    (None, 1)     2   \n                 \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n_________________________________________________________________\n```\n### Test the model\nThis step tests the model that you created.\n```\nmodel.fit(x, y, epochs=500, verbose=0)test_examples =[20, 40, 60, 90]value_to_predict = numpy.array(test_examples, dtype=numpy.float32)predictions = model.predict(value_to_predict)print('Test Examples ' + str(test_examples))print('Predictions ' + str(predictions))\n```\n```\n1/1 [==============================] - 0s 94ms/step\nTest Examples [20, 40, 60, 90]\nPredictions [[ 9.201942]\n [16.40566 ]\n [23.609379]\n [34.41496 ]]\n```\n## RunInference with Tensorflow using tfx-bsl\nIn versions 1.10.0 and later of `tfx-bsl` , you can create a TensorFlow `ModelHandler` to use with Apache Beam.\n### Populate the data in a TensorFlow proto\nTensorflow data uses protos. If you are loading from a file, helpers exist for this step. Because this example uses generated data, this code populates a proto.\n```\n# This example shows a proto that converts the samples and labels into# tensors usable by TensorFlow.class ExampleProcessor:\u00a0 \u00a0 def create_example_with_label(self, feature: numpy.float32,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0label: numpy.float32)-> tf.train.Example:\u00a0 \u00a0 \u00a0 \u00a0 return tf.train.Example(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 features=tf.train.Features(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 feature={'x': self.create_feature(feature),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0'y' : self.create_feature(label)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }))\u00a0 \u00a0 def create_example(self, feature: numpy.float32):\u00a0 \u00a0 \u00a0 \u00a0 return tf.train.Example(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 features=tf.train.Features(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 feature={'x' : self.create_feature(feature)})\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 def create_feature(self, element: numpy.float32):\u00a0 \u00a0 \u00a0 \u00a0 return tf.train.Feature(float_list=tf.train.FloatList(value=[element]))# Create a labeled example file for the 5 times table.example_five_times_table = 'example_five_times_table.tfrecord'with tf.io.TFRecordWriter(example_five_times_table) as writer:\u00a0 for i in zip(x, y):\u00a0 \u00a0 example = ExampleProcessor().create_example_with_label(\u00a0 \u00a0 \u00a0 \u00a0 feature=i[0], label=i[1])\u00a0 \u00a0 writer.write(example.SerializeToString())# Create a file containing the values to predict.predict_values_five_times_table = 'predict_values_five_times_table.tfrecord'with tf.io.TFRecordWriter(predict_values_five_times_table) as writer:\u00a0 for i in value_to_predict:\u00a0 \u00a0 example = ExampleProcessor().create_example(feature=i)\u00a0 \u00a0 writer.write(example.SerializeToString())\n```\n### Fit the model\nThis step builds a model. Because RunInference requires pretrained models, this segment builds a usable model.\n```\nRAW_DATA_TRAIN_SPEC = {'x': tf.io.FixedLenFeature([], tf.float32),'y': tf.io.FixedLenFeature([], tf.float32)}dataset = tf.data.TFRecordDataset(example_five_times_table)dataset = dataset.map(lambda e : tf.io.parse_example(e, RAW_DATA_TRAIN_SPEC))dataset = dataset.map(lambda t : (t['x'], t['y']))dataset = dataset.batch(100)dataset = dataset.repeat()model.fit(dataset, epochs=5000, steps_per_epoch=1, verbose=0)\n```\n```\n<keras.callbacks.History at 0x7f6960074c70>\n```\n### Save the model\nThis step shows how to save your model.\n```\nRAW_DATA_PREDICT_SPEC = {'x': tf.io.FixedLenFeature([], tf.float32),}# tf.function compiles the function into a callable TensorFlow graph.# RunInference relies on calling a TensorFlow graph as a model.# Note: Use the input signature type tf.string, because it's supported by# tfx-bsl ModelHandlers.@tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string , name='examples')])def serve_tf_examples_fn(serialized_tf_examples):\u00a0 \"\"\"Returns the output to be used in the serving signature.\"\"\"\u00a0 features = tf.io.parse_example(serialized_tf_examples, RAW_DATA_PREDICT_SPEC)\u00a0 return model(features, training=False)signature = {'serving_default': serve_tf_examples_fn}# Signatures define the input and output types for a computation. The optional# save signatures argument controls which methods in obj are available to# programs that consume SavedModels, such as serving APIs.# See https://www.tensorflow.org/api_docs/python/tf/saved_model/savetf.keras.models.save_model(model, save_model_dir_multiply, signatures=signature)\n```\n## Run the pipeline\nUse the following code to run the pipeline.\n- `FormatOutput`demonstrates how to extract values from the output protos.\n- `CreateModelHandler`demonstrates the model handler that needs to be passed into the Apache Beam RunInference API.\n```\nfrom tfx_bsl.public.beam.run_inference import CreateModelHandlerclass FormatOutput(beam.DoFn):\u00a0 \u00a0 def process(self, element: prediction_log_pb2.PredictionLog):\u00a0 \u00a0 \u00a0 \u00a0 predict_log = element.predict_log\u00a0 \u00a0 \u00a0 \u00a0 input_value = tf.train.Example.FromString(predict_log.request.inputs['examples'].string_val[0])\u00a0 \u00a0 \u00a0 \u00a0 input_float_value = input_value.features.feature['x'].float_list.value[0]\u00a0 \u00a0 \u00a0 \u00a0 output_value = predict_log.response.outputs\u00a0 \u00a0 \u00a0 \u00a0 output_float_value = output_value['output_0'].float_val[0]\u00a0 \u00a0 \u00a0 \u00a0 yield (f\"example is {input_float_value:.2f} prediction is {output_float_value:.2f}\")tfexample_beam_record = tfx_bsl.public.tfxio.TFExampleRecord(file_pattern=predict_values_five_times_table)saved_model_spec = model_spec_pb2.SavedModelSpec(model_path=save_model_dir_multiply)inference_spec_type = model_spec_pb2.InferenceSpecType(saved_model_spec=saved_model_spec)model_handler = CreateModelHandler(inference_spec_type)with beam.Pipeline() as p:\u00a0 \u00a0 _ = (p | tfexample_beam_record.RawRecordBeamSource()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| RunInference(model_handler)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.ParDo(FormatOutput())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.Map(print)\u00a0 \u00a0 \u00a0 \u00a0 )\n```\n```\nWARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\nWARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tfx_bsl/beam/run_inference.py:615: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.saved_model.load` instead.\nWARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\nexample is 20.00 prediction is 104.36\nexample is 40.00 prediction is 202.62\nexample is 60.00 prediction is 300.87\nexample is 90.00 prediction is 448.26\n```\n## Use KeyedModelHandler with tfx-bsl\nBy default, the `ModelHandler` does not expect a key.\n- If you know that keys are associated with your examples, use`beam.KeyedModelHandler`to wrap the model handler.\n- If you don't know whether keys are associated with your examples, use`beam.MaybeKeyedModelHandler`.\nIn addition to demonstrating how to use a keyed model handler, this step demonstrates how to use `tfx-bsl` examples.\n```\nfrom apache_beam.ml.inference.base import KeyedModelHandlerfrom google.protobuf import text_formatimport tensorflow as tfclass FormatOutputKeyed(FormatOutput):\u00a0 # To simplify, inherit from FormatOutput.\u00a0 def process(self, tuple_in: Tuple):\u00a0 \u00a0 key, element = tuple_in\u00a0 \u00a0 output = super().process(element)\u00a0 \u00a0 yield ' : '.join([key, next(output)])def make_example(num):\u00a0 # Return keyed values in the form of (key num, example).\u00a0 key = f'key {num}'\u00a0 tf_proto = text_format.Parse(\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 features {\u00a0 \u00a0 \u00a0 feature {key: \"x\" value { float_list { value: %f } } }\u00a0 \u00a0 }\u00a0 \u00a0 \"\"\"% num, tf.train.Example())\u00a0 return (key, tf_proto)# Make a list of examples of type tf.train.Example.examples = [\u00a0 \u00a0 make_example(5.0),\u00a0 \u00a0 make_example(50.0),\u00a0 \u00a0 make_example(40.0),\u00a0 \u00a0 make_example(100.0)]tfexample_beam_record = tfx_bsl.public.tfxio.TFExampleRecord(file_pattern=predict_values_five_times_table)saved_model_spec = model_spec_pb2.SavedModelSpec(model_path=save_model_dir_multiply)inference_spec_type = model_spec_pb2.InferenceSpecType(saved_model_spec=saved_model_spec)keyed_model_handler = KeyedModelHandler(CreateModelHandler(inference_spec_type))with beam.Pipeline() as p:\u00a0 \u00a0 _ = (p | 'CreateExamples' >> beam.Create(examples)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| RunInference(keyed_model_handler)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.ParDo(FormatOutputKeyed())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.Map(print)\u00a0 \u00a0 \u00a0 \u00a0 )\n```\n```\nkey 5.0 : example is 5.00 prediction is 30.67\nkey 50.0 : example is 50.00 prediction is 251.75\nkey 40.0 : example is 40.00 prediction is 202.62\nkey 100.0 : example is 100.00 prediction is 497.38\n```", "guide": "Dataflow"}