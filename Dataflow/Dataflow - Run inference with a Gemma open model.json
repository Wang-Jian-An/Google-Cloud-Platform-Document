{"title": "Dataflow - Run inference with a Gemma open model", "url": "https://cloud.google.com/dataflow/docs/notebooks/run_inference_gemma?hl=zh-cn", "abstract": "# Dataflow - Run inference with a Gemma open model\n| 0     | 1      |\n|:--------------------|:----------------------|\n| Run in Google Colab | View source on GitHub |\nGemma is a family of lightweight, state-of-the art open models built from research and technology used to create the Gemini models. You can use Gemma models in your Apache Beam inference pipelines with the `RunInference` transform.\nThis notebook demonstrates how to load the preconfigured Gemma 2B model and then use it in your Apache Beam inference pipeline. The pipeline runs examples by using a built-in model handler and a custom inference function.\nFor more information about using RunInference, see [Get started with AI/ML pipelines](https://beam.apache.org/documentation/ml/overview/) in the Apache Beam documentation.\n", "content": "## Requirements\nServing and using Gemma models requires a substantial amount of RAM. To run this example, we recommend that you use a notebook instance with GPUs. At a mimumum, use a machine that has the T4 GPU type. This configuration provides sufficient memory for running inference with a saved model.\n**Note:** When you complete this workflow in Google Colab, if you don't have Colab Enterprise, you might run into resource constraints.\n## Before you begin\n- To use a fine-tuned version of the model, follow the steps in [Gemma fine-tuning](https://ai.google.dev/gemma/docs/lora_tuning) .\n- For testing this workflow, we recommend using the instruction tuned model in your Apache Beam workflow. For example, if you use the Gemma 2B model in your pipeline, when you load the model, change the`GemmaCausalLM.from_preset()`argument from`gemma_2b_en`to`gemma_instruct_2b_en`. For more information, see [Create a model](https://ai.google.dev/gemma/docs/get_started#create_a_model) in \"Get started with Gemma using KerasNLP\". For a list of models, see [Gemma models](https://www.kaggle.com/models/keras/gemma) .## Install Dependencies\nTo use the `RunInference` transform with the built-in TensorFlow model handler, install Apache Beam version 2.46.0 or later. The model class is contained in the Keras natural language processing (NLP) package versions 0.8.0 and later.\n```\n!pip install -q -U protobuf!pip install -q -U apache_beam[gcp]!pip install -q -U keras_nlp>=0.8.0!pip install -q -U keras>3# To use the newly installed versions, restart the runtime.exit()\n```\n## Authenticate with Kaggle\nThe pipeline defined below automatically pulls the model weights from Kaggle. Please go to [https://www.kaggle.com/models/keras/gemma](https://www.kaggle.com/models/keras/gemma) and accept the terms of usage for Gemma models, then generate an API token using the instructions at [https://www.kaggle.com/docs/api](https://www.kaggle.com/docs/api) and provide your username and token here.\n```\nimport kagglehubkagglehub.login()\n```\n```\nVBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggle\u2026\nKaggle credentials set.\nKaggle credentials successfully validated.\n```\n## Import dependencies and provide a model preset\nUse the following code to import dependencies.\nReplace the `model_preset` variable with the name of the Gemma preset to use. For example, if you want to use the default English weights, set the preset to \"gemma_2b_en\". For this demo, we wil use the instruction-tuned preset \"gemma_instruct_2b_en\". We also optionally use keras to run the model at half-precision to reduce GPU memory usage.\n```\nimport numpy as npimport apache_beam as beamimport keras_nlpimport kerasfrom apache_beam.ml.inference import utilsfrom apache_beam.ml.inference.base import RunInferencefrom apache_beam.ml.inference.tensorflow_inference import TFModelHandlerNumpyfrom apache_beam.options.pipeline_options import PipelineOptionsmodel_preset = \"gemma_instruct_2b_en\"# Optionally set the model to run at half-precision# (recommended for smaller GPUs)keras.config.set_floatx(\"bfloat16\")\n```\n## Run the pipeline\nTo run the pipeline, use a custom model handler.\n### Provide a custom model handler\nTo simplify model loading, this notebook defines a custom model handler that will load the model by pulling the model weights directly from Kaggle presets. Implementing `load_model()` , `validate_inference_args()` , and `share_model_across_processes()` allows us to customize the behavior of the handler. The Keras implementation of the Gemma models has a `generate()` method that generates text based on a prompt. Using this function in `run_inference()` routes the prompts properly.\n```\n# Define `GemmaModelHandler` to load the model and perform the inference.from apache_beam.ml.inference.base import ModelHandlerfrom apache_beam.ml.inference.base import PredictionResultfrom typing import Anyfrom typing import Dictfrom typing import Iterablefrom typing import Optionalfrom typing import Sequencefrom keras_nlp.src.models.gemma.gemma_causal_lm import GemmaCausalLMclass GemmaModelHandler(ModelHandler[str,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0PredictionResult,GemmaCausalLM\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0]):\u00a0 \u00a0 def __init__(\u00a0 \u00a0 \u00a0 \u00a0 self,\u00a0 \u00a0 \u00a0 \u00a0 model_name: str = \"gemma_2b_en\",\u00a0 \u00a0 ):\u00a0 \u00a0 \u00a0 \u00a0 \"\"\" Implementation of the ModelHandler interface for Gemma using text as input.\u00a0 \u00a0 \u00a0 \u00a0 Example Usage::\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pcoll | RunInference(GemmaModelHandler())\u00a0 \u00a0 \u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name: The Gemma model preset. Default is gemma_2b_instruct_en.\u00a0 \u00a0 \u00a0 \u00a0 \"\"\"\u00a0 \u00a0 \u00a0 \u00a0 self._model_name = model_name\u00a0 \u00a0 \u00a0 \u00a0 self._env_vars = {}\u00a0 \u00a0 def share_model_across_processes(self) \u00a0-> bool:\u00a0 \u00a0 \u00a0 \u00a0 return True\u00a0 \u00a0 def load_model(self) -> GemmaCausalLM:\u00a0 \u00a0 \u00a0 \u00a0 \"\"\"Loads and initializes a model for processing.\"\"\"\u00a0 \u00a0 \u00a0 \u00a0 return keras_nlp.models.GemmaCausalLM.from_preset(self._model_name)\u00a0 \u00a0 def validate_inference_args(self, inference_args: Optional[Dict[str, Any]]):\u00a0 \u00a0 \u00a0 \u00a0 \"\"\"Validates the inference arguments.\"\"\"\u00a0 \u00a0 \u00a0 \u00a0 for key, value in inference_args.items():\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if key != \"max_length\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 raise ValueError(f\"Invalid inference argument: {key}\")\u00a0 \u00a0 def run_inference(\u00a0 \u00a0 \u00a0 \u00a0 self,\u00a0 \u00a0 \u00a0 \u00a0 batch: Sequence[str],\u00a0 \u00a0 \u00a0 \u00a0 model: GemmaCausalLM,\u00a0 \u00a0 \u00a0 \u00a0 inference_args: Optional[Dict[str, Any]] = None\u00a0 \u00a0 ) -> Iterable[PredictionResult]:\u00a0 \u00a0 \u00a0 \u00a0 \"\"\"Runs inferences on a batch of text strings.\u00a0 \u00a0 \u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 batch: A sequence of examples as text strings.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 inference_args: Any additional arguments for an inference.\u00a0 \u00a0 \u00a0 \u00a0 Returns:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 An Iterable of type PredictionResult.\u00a0 \u00a0 \u00a0 \u00a0 \"\"\"\u00a0 \u00a0 \u00a0 \u00a0 # Loop each text string, and use a tuple to store the inference results.\u00a0 \u00a0 \u00a0 \u00a0 predictions = []\u00a0 \u00a0 \u00a0 \u00a0 for one_text in batch:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 result = model.generate(one_text, **inference_args)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 predictions.append(result)\u00a0 \u00a0 \u00a0 \u00a0 return utils._convert_to_result(batch, predictions, self._model_name)\n```\n### Execute the pipeline\nUse the following code to run the pipeline. The code includes the path to the trained TensorFlow model. This cell can take a few minutes to run, because the model is downloaded and then loaded onto the worker. This delay is a one-time cost per worker.\nThe `max_length` argument determines how long the response from Gemma is. The response includes your input, so the response length includes your input and the output. For longer prompts, use a larger maximum length. Longer lengths require more time to generate.\n**Note:** When the pipeline completes, the memory used to load the model in the pipeline isn't freed automatically. As a result, if you run the pipeline more than once, your pipeline might fail with an out of memory (OOM) error.\n```\nclass FormatOutput(beam.DoFn):\u00a0 def process(self, element, *args, **kwargs):\u00a0 \u00a0 yield \"Input: {input}, Output: {output}\".format(input=element.example, output=element.inference)# Instantiate a NumPy array of string prompts for the model.examples = np.array([\"Tell me the sentiment of the phrase 'I like pizza': \"])# Specify the model handler, providing a path and the custom inference function.model_handler = GemmaModelHandler(model_preset)with beam.Pipeline() as p:\u00a0 _ = (p | beam.Create(examples) # Create a PCollection of the prompts.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0| RunInference(model_handler, inference_args={'max_length': 32}) # Send the prompts to the model and get responses.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.ParDo(FormatOutput()) # Format the output.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.Map(print) # Print the formatted output.\u00a0 )\n```\n```\nWARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\nInput: Tell me the sentiment of the phrase 'I like pizza': , Output: Tell me the sentiment of the phrase 'I like pizza': \nThe sentiment of the phrase \"I like pizza\" is positive. It expresses a personal\n```", "guide": "Dataflow"}