{"title": "Dataflow - Use RunInference for Generative AI", "url": "https://cloud.google.com/dataflow/docs/notebooks/run_inference_generative_ai?hl=zh-cn", "abstract": "# Dataflow - Use RunInference for Generative AI\n| 0     | 1      |\n|:--------------------|:----------------------|\n| Run in Google Colab | View source on GitHub |\nThis notebook shows how to use the Apache Beam [RunInference](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.RunInference) transform for generative AI tasks. It uses a large language model (LLM) from the [Hugging Face Model Hub](https://huggingface.co/models) .\nThis notebook demonstrates the following steps:\n- Load and save a model from the Hugging Face Model Hub.\n- Use the PyTorch model handler for RunInference.\nFor more information about using RunInference, see [Get started with AI/ML pipelines](https://beam.apache.org/documentation/ml/overview/) in the Apache Beam documentation.\n", "content": "## Install the Apache Beam SDK and dependencies\nUse the following code to install the Apache Beam Python SDK, PyTorch, and Transformers.\n```\npip install apache_beam[gcp]==2.48.0pip install torchpip install transformers\n```\nUse the following code to import dependencies\n**Important:** If an error occurs, restart your runtime.\n```\nimport osimport apache_beam as beamfrom apache_beam.options.pipeline_options import PipelineOptionsfrom apache_beam.ml.inference.base import PredictionResultfrom apache_beam.ml.inference.base import RunInferencefrom apache_beam.ml.inference.pytorch_inference import make_tensor_model_fnfrom apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensorimport torchfrom transformers import AutoConfigfrom transformers import AutoModelForSeq2SeqLMfrom transformers import AutoTokenizerfrom transformers.tokenization_utils import PreTrainedTokenizerMAX_RESPONSE_TOKENS = 256model_name = \"google/flan-t5-small\"state_dict_path = \"saved_model\"\n```\n## Download and save the model\nThis notebook uses the [auto classes](https://huggingface.co/docs/transformers/model_doc/auto) from Hugging Face to instantly load the model in memory. Later, the model is saved to the path defined previously.\n```\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\u00a0 \u00a0 \u00a0 \u00a0 model_name, torch_dtype=torch.bfloat16\u00a0 \u00a0 )directory = os.path.dirname(state_dict_path)torch.save(model.state_dict(), state_dict_path)\n```\n## Define utility functions\nThe input and output for the [google/flan-t5-small](https://huggingface.co/google/flan-t5-small) model are token tensors. These utility functions are used for the conversion of text to token tensors and then back to text.\n```\ndef to_tensors(input_text: str, tokenizer) -> torch.Tensor:\u00a0 \u00a0 \"\"\"Encodes input text into token tensors.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 input_text: Input text for the LLM model.\u00a0 \u00a0 \u00a0 \u00a0 tokenizer: Tokenizer for the LLM model.\u00a0 \u00a0 Returns: Tokenized input tokens.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 return tokenizer(input_text, return_tensors=\"pt\").input_ids[0]def from_tensors(result: PredictionResult, tokenizer) -> str:\u00a0 \u00a0 \"\"\"Decodes output token tensors into text.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 result: Prediction results from the RunInference transform.\u00a0 \u00a0 \u00a0 \u00a0 tokenizer: Tokenizer for the LLM model.\u00a0 \u00a0 Returns: The model's response as text.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 output_tokens = result.inference\u00a0 \u00a0 return tokenizer.decode(output_tokens, skip_special_tokens=True)\n```\n```\n# Load the tokenizer.tokenizer = AutoTokenizer.from_pretrained(model_name)# Create an instance of the PyTorch model handler.model_handler = PytorchModelHandlerTensor(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 state_dict_path=state_dict_path,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_class=AutoModelForSeq2SeqLM.from_config,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_params={\"config\": AutoConfig.from_pretrained(model_name)},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 inference_fn=make_tensor_model_fn(\"generate\"),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\n```\n## Run the Pipeline\n```\nexample = [\"translate English to Spanish: We are in New York City.\"]pipeline = beam.Pipeline(options=PipelineOptions(save_main_session=True,pickle_library=\"cloudpickle\"))with pipeline as p:\u00a0 _ = (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 p\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"Create Examples\" >> beam.Create(example)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"To tensors\" >> beam.Map(to_tensors, tokenizer)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"RunInference\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 >> RunInference(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_handler,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 inference_args={\"max_new_tokens\": MAX_RESPONSE_TOKENS},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"From tensors\" >> beam.Map(from_tensors, tokenizer)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \"Print\" >> beam.Map(print)\u00a0 \u00a0 \u00a0 )\n```\n```\nEstamos en Nueva York City.\n```", "guide": "Dataflow"}