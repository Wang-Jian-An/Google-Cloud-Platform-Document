{"title": "Dataflow - Write data from Kafka to BigQuery with Dataflow", "url": "https://cloud.google.com/dataflow/docs/kafka-dataflow", "abstract": "# Dataflow - Write data from Kafka to BigQuery with Dataflow\nThis document provides high-level guidance on creating and deploying a Dataflow pipeline that streams from Apache Kafka to BigQuery.\n[Apache Kafka](https://kafka.apache.org/) is an open source platform for streaming events. Kafka is commonly used in distributed architectures to enable communication between loosely coupled components. You can use Dataflow to read events from Kafka, process them, and write the results to a BigQuery table for further analysis.\nGoogle provides a [Dataflow template](/dataflow/docs/guides/templates/provided/kafka-to-bigquery) that configures a Kafka-to-BigQuery pipeline. The template uses the [BigQueryIO](https://beam.apache.org/documentation/io/built-in/google-bigquery/) connector provided in the Apache Beam SDK.\nTo use this template, you perform the following steps:\n- Deploy Kafka, either in Google Cloud or elsewhere.\n- Configure networking.\n- Set Identity and Access Management (IAM) permissions.\n- Write a function to transform the event data.\n- Create the BigQuery output table.\n- Deploy the Dataflow template.", "content": "## Deploy Kafka\nWithin Google Cloud, you can deploy a Kafka cluster on Compute Engine virtual machine (VM) instances or use a third-party managed Kafka service. For more information about deployment options on Google Cloud, see [What is Apache Kafka?](/learn/what-is-apache-kafka) . You can find third-party Kafka solutions on the [Google Cloud Marketplace](https://console.cloud.google.com/marketplace/browse?q=kafka) .\nAlternatively, you might have an existing Kafka cluster that resides outside of Google Cloud. For example, you might have an existing workload that is deployed on-premises or in another public cloud.\n## Configure networking\nBy default, Dataflow launches instances within your default Virtual Private Cloud (VPC) network. Depending on your Kafka configuration, you might need to configure a different network and subnet for Dataflow. For more information, see [Specify a network and subnetwork](/dataflow/docs/guides/specifying-networks) in the Dataflow documentation. When configuring your network, create [firewall rules](/vpc/docs/using-firewalls) that allow the Dataflow worker machines to reach the Kafka brokers.\nIf you are using VPC Service Controls, then place the Kafka cluster within the VPC Service Controls perimeter, or else [extend the perimeters to the authorized VPN or Cloud Interconnect](/vpc-service-controls/docs/overview#hybrid_access) .\n### Connect to an external cluster\nIf your Kafka cluster is deployed outside of Google Cloud, you must create a network connection between Dataflow and the Kafka cluster. There are several networking options with different tradeoffs:\n- Connect using a shared RFC 1918 address space.- [Dedicated Interconnect](/network-connectivity/docs/interconnect/concepts/dedicated-overview) \n- [IPsec virtual private network (VPN)](/network-connectivity/docs/vpn/concepts/overview) \n- Reach your externally hosted Kafka cluster through public IP addresses.- Public internet\n- [Direct peering](/network-connectivity/docs/direct-peering/direct-peering) \n- [Carrier peering](/network-connectivity/docs/carrier-peering) Dedicated Interconnect is the best option for predictable performance and reliability, but it can take longer to set up because third parties must provision the new circuits. With a public IP\u2013based topology, you can get started quickly because little networking work needs to be done.\nThe next two sections describe these options in more detail.\nBoth Dedicated Interconnect and IPsec VPN give you direct access to RFC 1918 IP addresses in your Virtual Private Cloud (VPC), which can simplify your Kafka configuration. If you're using a VPN\u2013based topology, consider setting up a [high-throughput VPN](/solutions/building-high-throughput-vpns) .\nBy default, Dataflow launches instances on your default [VPC network](/vpc/docs/vpc) . In a private network topology with [routes explicitly defined in Cloud Router](/network-connectivity/docs/router/concepts/overview) that connect subnetworks in Google Cloud to that Kafka cluster, you need more control over where to locate your Dataflow instances. You can use Dataflow to configure the `network` and `subnetwork` [execution parameters](/dataflow/pipelines/specifying-exec-params#setting-other-cloud-pipeline-options) .\nMake sure that the corresponding subnetwork has enough IP addresses available for Dataflow to launch instances on as it attempts to scale out. Also, when you create a separate network for launching your Dataflow instances, ensure that you have a firewall rule that enables TCP traffic among all virtual machines in the project. The default network already has this firewall rule configured.\nThis architecture uses Transport Layer Security ( [TLS](https://wikipedia.org/wiki/Transport_Layer_Security) ) to secure traffic between external clients and Kafka, and uses plaintext for inter-broker communication. When the Kafka listener binds to a network interface that is used for both internal and external communication, configuring the listener is straightforward. However, in many scenarios, the externally advertised addresses of the Kafka brokers in the cluster differ from the internal network interfaces that Kafka uses. In such scenarios, you can use the `advertised.listeners` property:\n```\n# Configure protocol map\nlistener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:SSL\n# Use plaintext for inter-broker communication\ninter.broker.listener.name=INTERNAL\n# Specify that Kafka listeners should bind to all local interfaces\nlisteners=INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:9093\n# Separately, specify externally visible address\nadvertised.listeners=INTERNAL://kafkabroker-n.mydomain.com:9092,EXTERNAL://kafkabroker-n.mydomain.com:9093\n```\nExternal clients connect using port 9093 through an \"SSL\" channel, and internal clients connect using port 9092 through a plaintext channel. When you specify an address under `advertised.listeners` , use DNS names ( `kafkabroker-n.mydomain.com` , in this sample) that resolve to the same instance for both external and internal traffic. Using public IP addresses might not work because the addresses might fail to resolve for internal traffic.\n## Set IAM permissions\nDataflow jobs use two IAM service accounts:\n- The Dataflow service uses a [Dataflow service account](/dataflow/docs/concepts/security-and-permissions#service_account) to manipulate Google Cloud resources, such as creating VMs.\n- The Dataflow worker VMs use a [worker service account](/dataflow/docs/concepts/security-and-permissions#worker_service_account) to access your pipeline's files and other resources. This service account needs write access to the BigQuery output table. It also needs access to any other resources that the pipeline job references.\nEnsure that these two service accounts have appropriate roles. For more information, see [Dataflow security and permissions](/dataflow/docs/concepts/security-and-permissions) .\n## Transform the data for BigQuery\nThe Kafka-to-BigQuery template creates a pipeline that reads events from one or more Kafka topics and writes them into a BigQuery table. Optionally, you can provide a JavaScript user-defined function (UDF) that transforms the event data before it is written to BigQuery.\nThe output from the pipeline must be JSON-formatted data that matches the schema of the output table. If the Kafka event data is already in JSON format, then you can create a BigQuery table with a matching schema and pass the events directly to BigQuery. Otherwise, author a UDF that takes the event data as input and returns JSON data that matches your BigQuery table.\nFor example, suppose the event data contains two fields:\n- `name`(string)\n- `customer_id`(integer)\nThe output from the Dataflow pipeline might look like the following:\n```\n{ \"name\": \"Alice\", \"customer_id\": 1234 }\n```\nAssuming the event data is not already in JSON format, you would write a UDF that transforms the data, as follows:\n```\n// UDFfunction process(eventData) {\u00a0 var name;\u00a0 var customer_id;\u00a0 // TODO Parse the event data to extract the name and customer_id fields.\u00a0 // Return a JSON payload.\u00a0 return JSON.stringify({ name: name, customer_id: customer_id });}\n```\nThe UDF can perform additional processing on the event data, such as filtering events, removing personal identifiable information (PII), or enriching the data with additional fields.\nFor more information on writing a UDF for the template, see [Extend your Dataflow template with UDFs](/blog/topics/developers-practitioners/extend-your-dataflow-template-with-udfs) . Upload the JavaScript file to Cloud Storage.\n## Create the BigQuery output table\nCreate the BigQuery output table before you run the template. The table schema must be compatible with the JSON output from the pipeline. For each property in the JSON payload, the pipeline writes the value to the BigQuery table column of the same name. Any missing properties in the JSON are interpreted as NULL values.\nUsing the previous example, the BigQuery table would have the following columns:\n| Column name | Data type |\n|:--------------|:------------|\n| name   | STRING  |\n| customer_id | INTEGER  |\nYou can use the [CREATE TABLE](/bigquery/docs/reference/standard-sql/data-definition-language#create_table_statement) SQL statement to create the table:\n```\nCREATE TABLE my_dataset.kafka_events (name STRING, customer_id INTEGER);\n```\nAlternatively, you can specify the table schema by using a JSON definition file. For more information, see [Specifying a schema](/bigquery/docs/schemas) in the BigQuery documentation.\n## Run the Dataflow job\nAfter you create the BigQuery table, run the Dataflow template.\nTo create the Dataflow job by using the Google Cloud console, perform the following steps:- Go to the Dataflow page in the Google Cloud console.\n- Click **Create job from template** .\n- In the **Job Name** field, enter a job name.\n- For **Regional endpoint** , select a region.\n- Select the \"Kafka to BigQuery\" template.\n- Under **Required parameters** , enter the name of the BigQuery output table. The table must already exist and have a valid schema.\n- Click **Show optional parameters** and enter values for at least the following parameters:- The Kafka topic to read the input from.\n- The list of Kafka bootstrap servers, separated by commas.\n- A service account email.\nEnter additional parameters as needed. In particular, you might need to specify the following:- Networking: To use a VPC network other than the default network, [specify the network and subnet](/dataflow/docs/guides/specifying-networks) .\n- UDF: To use a JavaScript UDF, specify the Cloud Storage location of the script and the name of the JavaScript function to invoke.\nTo create the Dataflow job by using the Google Cloud CLI, run the following command:\n```\ngcloud dataflow flex-template run JOB_NAME \\--template-file-gcs-location gs://dataflow-templates/latest/flex/Kafka_to_BigQuery \\--region LOCATION \\--parameters inputTopics=KAFKA_TOPICS \\--parameters bootstrapServers=BOOTSTRAP_SERVERS \\--parameters outputTableSpec=OUTPUT_TABLE \\--parameters serviceAccount=IAM_SERVICE_ACCOUNT \\--parameters javascriptTextTransformGcsPath=UDF_SCRIPT_PATH \\--parameters javascriptTextTransformFunctionName=UDF_FUNCTION_NAME \\--network VPC_NETWORK_NAME \\--subnetwork SUBNET_NAME\n```\nReplace the following variables:- . A job name of your choice.\n- . The region in which to run the job. For more information about regions and locations, see [Dataflow locations](/dataflow/docs/resources/locations) .\n- . A comma-separated list of Kafka topics to read.\n- . A comma-separated list of Kafka bootstrap servers. Example:`127:9092,127.0.0.1:9093`.\n- . The BigQuery output table, specified as:.. Example:`my_project:dataset1.table1`.\n- . Optional. The email address of the service account to run the job as.\n- . Optional. The Cloud Storage path to a JavaScript file that contains a UDF. Example:`gs://your-bucket/your-function.js`.\n- . Optional. The name of the JavaScript function to call as the UDF.\n- . Optional. The network to which workers will be assigned.\n- . Optional. The subnetwork to which workers will be assigned.## Data types\nThis section describes how to handle various data types in the BigQuery table schema.\nInternally, the JSON messages are converted to [TableRow](https://www.javadoc.io/doc/com.google.apis/google-api-services-bigquery/latest/com/google/api/services/bigquery/model/TableRow.html) objects, and the `TableRow` field values are translated to BigQuery types.\n### Scalar types\nThe following example creates a BigQuery table with different scalar data types, including string, numeric, Boolean, date/time, interval, and geography types:\n```\nCREATE TABLE \u00a0my_dataset.kafka_events (\u00a0 \u00a0 string_col STRING,\u00a0 \u00a0 integer_col INT64,\u00a0 \u00a0 float_col FLOAT64,\u00a0 \u00a0 decimal_col DECIMAL,\u00a0 \u00a0 bool_col BOOL,\u00a0 \u00a0 date_col DATE,\u00a0 \u00a0 dt_col DATETIME,\u00a0 \u00a0 ts_col TIMESTAMP,\u00a0 \u00a0 interval_col INTERVAL,\u00a0 \u00a0 geo_col GEOGRAPHY);\n```\nHere is a JSON payload with compatible fields:\n```\n{\u00a0 \"string_col\": \"string_val\",\u00a0 \"integer_col\": 10,\u00a0 \"float_col\": 3.142,\u00a0 \"decimal_col\": 5.2E11,\u00a0 \"bool_col\": true,\u00a0 \"date_col\": \"2022-07-01\",\u00a0 \"dt_col\": \"2022-07-01 12:00:00.00\",\u00a0 \"ts_col\": \"2022-07-01T12:00:00.00Z\",\u00a0 \"interval_col\": \"0-13 370 48:61:61\",\u00a0 \"geo_col\": \"POINT(1 2)\"}\n```\nNotes:\n- For a`TIMESTAMP`column, you can use the JavaScript`Date.toJSON`method to format the value.\n- For the`GEOGRAPHY`column, you can specify the geography using well-known text (WKT) or GeoJSON, formatted as a string. For more information, see [Loading geospatial data](/bigquery/docs/geospatial-data#loading_geospatial_data) .\nFor more information about data types in BigQuery, see [Data types](/bigquery/docs/reference/standard-sql/data-types) .\n### Arrays\nYou can store an array in BigQuery by using the [ARRAY](/bigquery/docs/reference/standard-sql/data-types#array_type) data type. In the following example, the JSON payload contains a property named `scores` whose value is a JSON array:\n```\n{\"name\":\"Emily\",\"scores\":[10,7,10,9]}\n```\nThe following `CREATE TABLE` SQL statement creates a BigQuery table with a compatible schema:\n```\nCREATE TABLE my_dataset.kafka_events (name STRING, scores ARRAY<INTEGER>);\n```\nThe resulting table looks like the following:\n```\n+-------+-------------+| name \u00a0| \u00a0 scores \u00a0 \u00a0|+-------+-------------+| Emily | [10,7,10,9] |+-------+-------------+\n```\n### Structures\nThe [STRUCT](/bigquery/docs/reference/standard-sql/data-types#struct_type) data type in BigQuery contains an ordered list of named fields. You can use a `STRUCT` to hold JSON objects that follow a consistent schema.\nIn the following example, the JSON payload contains a property named `val` whose value is a JSON object:\n```\n{\"name\":\"Emily\",\"val\":{\"a\":\"yes\",\"b\":\"no\"}}\n```\nThe following `CREATE TABLE` SQL statement creates a BigQuery table with a compatible schema:\n```\nCREATE TABLE my_dataset.kafka_events (name STRING, val STRUCT<a STRING, b STRING>);\n```\nThe resulting table looks like the following:\n```\n+-------+----------------------+| name \u00a0| \u00a0 \u00a0 \u00a0 \u00a0 val \u00a0 \u00a0 \u00a0 \u00a0 \u00a0|+-------+----------------------+| Emily | {\"a\":\"yes\",\"b\":\"no\"} |+-------+----------------------+\n```\n### Semi-structured event data\nIf the Kafka event data does not follow a strict schema, consider storing it in BigQuery as a [JSON data type](/bigquery/docs/reference/standard-sql/data-types#json_type) (Preview). By storing JSON data as a `JSON` data type, you don't need to define the event schema upfront. After data ingestion, you can query the output table by using the field access (dot notation) and array access operators.\nFirst, create a table with a `JSON` column:\n```\n-- Create the BigQuery tableCREATE TABLE my_dataset.kafka_events (event_data JSON);\n```\nThen define a JavaScript UDF that wraps the event payload inside a JSON object:\n```\n// UDFfunction process(eventData) {\u00a0 var json;\u00a0 // TODO Convert the event data to JSON.\u00a0 return JSON.stringify({ \"event_data\": json });}\n```\nAfter the data is written to BigQuery, you can query the individual fields by using the field access operator. For example, the following query returns the value of the `name` field for each record:\n```\nSELECT event_data.name FROM my_dataset1.kafka_events;\n```\nFor more information about using JSON in BigQuery, see [Working with JSON data in Google Standard SQL](/bigquery/docs/reference/standard-sql/json-data) .\n## Errors and logging\nYou might experience errors from running the pipeline, or errors while handling individual Kafka events.\nFor more information about handling pipeline errors, see [Pipeline troubleshooting and debugging](/dataflow/docs/guides/troubleshooting-your-pipeline) .\nIf the job runs successfully but an error occurs when processing an individual Kafka event, the pipeline job writes an error record to a table in BigQuery. The job itself doesn't fail, and the event-level error does not appear as an error in the Dataflow job log.\nThe pipeline job automatically creates the table to hold error records. By default, the name of the table is \" _error_records\", where is the name of the output table. For example, if the output table is named `kafka_events` , the error table is named `kafka_events_error_records` . You can specify a different name by setting the `outputDeadletterTable` template parameter:\n```\noutputDeadletterTable=my_project:dataset1.errors_table\n```\nPossible errors include:\n- Serialization errors, including badly-formatted JSON.\n- Type conversion errors, caused by a mismatch in the table schema and the JSON data.\n- Extra fields in the JSON data that are not present in the table schema.\nExample error messages:\n| ('Type of error', 'Serialization error', 'Type conversion error', 'Unknown field') | ('Event data', '\"Hello world\"', '{\"name\":\"Emily\",\"customer_id\":\"abc\"}', '{\"name\":\"Zoe\",\"age\":34}') | ('errorMessage', 'Failed to serialize json to table row: \"Hello world\"', '{ \"errors\" : [ { \"debugInfo\" : \"\", \"location\" : \"age\", \"message\" : \"Cannot convert value to integer (bad value): abc\", \"reason\" : \"invalid\" } ], \"index\" : 0 }', '{ \"errors\" : [ { \"debugInfo\" : \"\", \"location\" : \"age\", \"message\" : \"no such field: customer_id.\", \"reason\" : \"invalid\" } ], \"index\" : 0 }') |\n|--------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n## Next steps\n- Learn more about [Dataflow templates](/dataflow/docs/concepts/dataflow-templates) .\n- [Get started with BigQuery](/bigquery/docs/quickstarts) .", "guide": "Dataflow"}