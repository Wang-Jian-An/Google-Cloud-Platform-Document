{"title": "Dataflow - Troubleshoot your Dataflow GPU job", "url": "https://cloud.google.com/dataflow/docs/gpu/troubleshoot-gpus", "abstract": "# Dataflow - Troubleshoot your Dataflow GPU job\n**Note:** The following considerations apply to this GA offering:- Jobs that use GPUs incur charges as specified in the Dataflow [pricing page](/dataflow/pricing) .\n- To use GPUs, your Dataflow job must use [Dataflow Runner v2](/dataflow/docs/runner-v2) .\nIf you encounter problems running your Dataflow job with GPUs, follow these steps:\n- Follow the workflow in [Best practices for working with Dataflow GPUs](/dataflow/docs/gpu/develop-with-gpus) to ensure that your pipeline is configured correctly.\n- Confirm that your Dataflow job is using GPUs. See [Verify your Dataflow job](/dataflow/docs/gpu/use-gpus#verify) in \"Run a pipeline with GPUs.\"\n- [Debug with a standalone VM](#debug-vm) .\n- If the problem persists, follow the rest of the troubleshooting steps on this page.", "content": "## Debug with a standalone VM\nWhile you're designing and iterating on a container image that works for you, it can be faster to reduce the feedback loop by trying out your container image on a standalone VM.\nYou can debug your custom container on a standalone VM with GPUs by creating a Compute Engine VM running GPUs on Container-Optimized OS, installing drivers, and starting your container as follows.\n- Create a VM instance.```\ngcloud compute instances create INSTANCE_NAME \\\u00a0 --project \"PROJECT\" \\\u00a0 --image-family cos-stable \\\u00a0 --image-project=cos-cloud \u00a0\\\u00a0 --zone=us-central1-f \\\u00a0 --accelerator type=nvidia-tesla-t4,count=1 \\\u00a0 --maintenance-policy TERMINATE \\\u00a0 --restart-on-failure \u00a0\\\u00a0 --boot-disk-size=200G \\\u00a0 --scopes=cloud-platform\n```\n- Use `ssh` to connect to the VM.```\ngcloud compute ssh INSTANCE_NAME --project \"PROJECT\"\n```\n- Install the GPU drivers. After connecting to the VM by using `ssh` , run the following commands on the VM:```\n# Run these commands on the virtual machinecos-extensions install gpusudo mount --bind /var/lib/nvidia /var/lib/nvidiasudo mount -o remount,exec /var/lib/nvidia/var/lib/nvidia/bin/nvidia-smi\n```\n- Launch your custom container.Apache Beam SDK containers use the `/opt/apache/beam/boot` entrypoint. For debugging purposes you can launch your container manually with a different entrypoint:```\ndocker-credential-gcr configure-dockerdocker run --rm \\\u00a0 -it \\\u00a0 --entrypoint=/bin/bash \\\u00a0 --volume /var/lib/nvidia/lib64:/usr/local/nvidia/lib64 \\\u00a0 --volume /var/lib/nvidia/bin:/usr/local/nvidia/bin \\\u00a0 --privileged \\\u00a0 IMAGE\n```Replace with the Artifact Registry path for your Docker image.\n- Verify that the GPU libraries installed in your container can access the GPU devices.If you're using TensorFlow, you can print available devices in Python interpreter with the following:```\n>>> import tensorflow as tf>>> print(tf.config.list_physical_devices(\"GPU\"))\n```If you're using PyTorch, you can inspect available devices in Python interpreter with the following:```\n>>> import torch>>> print(torch.cuda.is_available())>>> print(torch.cuda.device_count())>>> print(torch.cuda.get_device_name(0))\n```\nTo iterate on your pipeline, you can launch your pipeline on Direct Runner. You can also launch pipelines on Dataflow Runner from this environment.\n## Workers don't start\nIf your job is stuck and the Dataflow workers never start processing data, it's likely that you have a problem related to using a custom container with Dataflow. For more details, read the [custom containers troubleshooting guide](/dataflow/docs/guides/troubleshoot-custom-container) .\nIf you're a Python user, verify that the following conditions are met:\n- The Python interpreter minor version in your container image is the same version as you use when launching your pipeline. If there's a mismatch, you might see errors like [SystemError: unknown opcode](/dataflow/docs/guides/common-errors#custom-container-python-version) with a stack trace involving`apache_beam/internal/pickler.py`.\n- If you're using the Apache Beam SDK 2.29.0 or earlier,`pip`must be accessible on the image in`/usr/local/bin/pip`.\nWe recommend that you reduce the customizations to a minimal working configuration the first time you use a custom image. Use the sample custom container images provided in the examples on this page. Make sure you can run a straightforward Dataflow pipeline with this container image without requesting GPUs. Then, iterate on the solution.\nVerify that workers have sufficient disk space to download your container image. Adjust disk size if necessary. Large images take longer to download, which increases worker startup time.\n## Job fails immediately at startup\nIf you encounter the [ZONE_RESOURCE_POOL_EXHAUSTED](/compute/docs/troubleshooting/troubleshooting-vm-creation#resource_availability) or [ZONE_RESOURCE_POOL_EXHAUSTED_WITH_DETAILS](/compute/docs/troubleshooting/troubleshooting-vm-creation#resource_availability) errors, you can take the following steps:\n- Don't specify the worker zone so that Dataflow selects the optimal zone for you.\n- Launch the pipeline in a different zone or with a different accelerator type.## Job fails at runtime\nIf the job fails at runtime, check for out of memory (OOM) errors on the worker machine and on the GPU. GPU OOM errors may manifest as `cudaErrorMemoryAllocation out of memory` errors in worker logs. If you're using TensorFlow, verify that you use only one TensorFlow process to access one GPU device. For more information, read [GPUs and worker parallelism](/dataflow/docs/concepts/gpu-support#gpus_and_worker_parallelism) .\n## No GPU usage\nIf your pipeline runs successfully, but GPUs are not used, verify the following:\n- NVIDIA libraries installed in the container image match the requirements of pipeline user code and libraries that it uses.\n- Installed NVIDIA libraries in container images are accessible as shared libraries.\nIf the devices are not available, you might be using an incompatible software configuration. For example, if you're using TensorFlow, verify that you have a [compatible combination](https://www.tensorflow.org/install/source#gpu) of TensorFlow, cuDNN version, and CUDA Toolkit version.\nTo verify the image configuration, consider running a straightforward pipeline that just checks that GPUs are available and accessible to the workers.\n## What's next\n- [Getting started: Running GPUs on Container-OptimizedOS](/container-optimized-os/docs/how-to/run-gpus#getting_started_running_gpus_on) .\n- [Container-Optimized OS toolbox](/container-optimized-os/docs/how-to/toolbox) .\n- [Service account access scopes](/compute/docs/access/service-accounts#accesscopesiam) .", "guide": "Dataflow"}