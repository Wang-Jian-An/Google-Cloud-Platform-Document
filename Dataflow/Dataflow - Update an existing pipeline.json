{"title": "Dataflow - Update an existing pipeline", "url": "https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline", "abstract": "# Dataflow - Update an existing pipeline\nUpdating batch pipelines isn't supported.\nThis document describes how to update an ongoing streaming job. You might want to update your existing Dataflow job for the following reasons:\n- You want to enhance or otherwise improve your pipeline code.\n- You want to fix bugs in your pipeline code.\n- You want to update your pipeline to handle changes in data format, or to account for version or other changes in your data source.\n- You want to patch a security vulnerability related to [Container-Optimized OS](/container-optimized-os/docs/concepts/security) for all the Dataflow workers.\n- You want to scale a streaming Apache Beam pipeline to use a different number of workers.\nYou can update jobs in two ways:\n- [In-flight job update](#in-flight-updates) : For streaming jobs that use [Streaming Engine](/dataflow/docs/streaming-engine) , you can update the`min-num-workers`and`max-num-workers`job options without stopping the job or changing the job ID.\n- [Replacement job](#Launching) : To run updated pipeline code or to update job options that in-flight job updates don't support, launch a new job that replaces the existing job. To verify whether a replacement job is valid, before launching the new job, [validate its job graph](#validate) .\nWhen you update your job, the Dataflow service performs a [compatibility check](#CCheck) between your currently running job and your potential replacement job. The compatibility check ensures that things like intermediate state information and buffered data can be transferred from your prior job to your replacement job.\nYou can also use the built-in logging infrastructure of the Apache Beam SDK to log information when you update your job. For more information, see [Work with pipeline logs](/dataflow/docs/guides/logging) . To identify problems with the pipeline code, use the [DEBUG logging level](/dataflow/docs/guides/logging#SettingLevels) .\n- For instructions for updating streaming jobs that use classic templates, see [Update a custom template streaming job](/dataflow/docs/guides/templates/running-templates#update) .\n- For instructions for updating streaming jobs that use Flex Templates, either follow the gcloud CLI instruction on this page, or see [Update a Flex Template job](/dataflow/docs/guides/templates/configuring-flex-templates#update) .", "content": "## In-flight job option update\nFor a streaming job that uses [Streaming Engine](/dataflow/docs/streaming-engine) , you can update the following job options without stopping the job or changing the job ID:\n- `min-num-workers`: the minimum number of Compute Engine instances.\n- `max-num-workers`: the maximum number of Compute Engine instances.\nFor other job updates, you must the current job with the updated job. For more information, see [Launch a replacement job](/dataflow/docs/guides/updating-a-pipeline#Launching) .\n### Perform an in-flight update\nTo perform an in-flight job option update, perform the following steps.\nUse the `gcloud dataflow jobs update-options` command:\n```\ngcloud dataflow jobs update-options \\\u00a0 --region=REGION \\\u00a0 --min-num-workers=MINIMUM_WORKERS \\\u00a0 --max-num-workers=MAXIMUM_WORKERS \\\u00a0 JOB_ID\n```\nReplace the following:- : the ID of the job's region\n- : the ID of the job to update\nYou can also update `--min-num-workers` and `--max-num-workers` individually.\nUse the [projects.locations.jobs.update](/dataflow/docs/reference/rest/v1b3/projects.locations.jobs/update) method:\n```\nPUT https://dataflow.googleapis.com/v1b3/projects/PROJECT_ID/locations/REGION/jobs/JOB_ID?updateMask=runtime_updatable_params.max_num_workers,runtime_updatable_params.min_num_workers{\u00a0 \"runtime_updatable_params\": {\u00a0 \u00a0 \"min_num_workers\": MINIMUM_WORKERS,\u00a0 \u00a0 \"max_num_workers\": MAXIMUM_WORKERS\u00a0 }}\n```\nReplace the following:- : the Google Cloud project ID of the Dataflow job\n- : the ID of the job's region\n- : the ID of the job to update\n- : the minimum number of Compute Engine instances\n- : the maximum number of Compute Engine instances\nYou can also update `min_num_workers` and `max_num_workers` individually. Specify which parameters to update in the `updateMask` query parameter, and include the updated values in the `runtimeUpdatableParams` field of the request body. The following example updates `min_num_workers` :\n```\nPUT https://dataflow.googleapis.com/v1b3/projects/my_project/locations/us-central1/jobs/job1?updateMask=runtime_updatable_params.min_num_workers{\u00a0 \"runtime_updatable_params\": {\u00a0 \u00a0 \"min_num_workers\": 5\u00a0 }}\n```\nA job must be in the running state to be eligible for in-flight updates. An error occurs if the job has not started or is already cancelled. Similarly, if you launch a replacement job, wait for it to begin running before sending any in-flight updates to the new job.\nAfter you submit an update request, we recommend waiting for the request to complete before sending another update. View the [job logs](/dataflow/docs/guides/logging#MonitoringLogs) to see when the request completes.\n## Validate a replacement job\nTo verify whether a replacement job is valid, before you launch the new job, validate its job graph. In Dataflow, a [job graph](/dataflow/docs/guides/job-graph) is a graphical representation of a pipeline. By validating the job graph, you reduce the risk of the pipeline encountering errors or pipeline failures after the update. In addition, you can validate updates without needing to stop the original job, so that job doesn't experience any downtime.\nTo validate your job graph, follow the steps to [launch a replacement job](#Launching) . Include the `graph_validate_only` [Dataflow service option](/dataflow/docs/reference/service-options) in the update command.\n- Pass the`--update`option.\n- Set the`--jobName`option in`PipelineOptions`to the same name as the job that you want to update.\n- Set the`--region`option to the same region as the region of the job that you want to update.\n- Include the`--dataflowServiceOptions=graph_validate_only`service option.\n- If any transform names in your pipeline have changed, you must supply a [transform mapping](#Mapping) and pass it using the`--transformNameMapping`option.\n- If you're submitting a replacement job that uses a later version of the Apache Beam SDK, set`--updateCompatibilityVersion`to the Apache Beam SDK version used in the original job.\n- Pass the`--update`option.\n- Set the`--job_name`option in`PipelineOptions`to the same name as the job that you want to update.\n- Set the`--region`option to the same region as the region of the job that you want to update.\n- Include the`--dataflow_service_options=graph_validate_only`service option.\n- If any transform names in your pipeline have changed, you must supply a [transform mapping](#Mapping) and pass it using the`--transform_name_mapping`option.\n- If you're submitting a replacement job that uses a later version of the Apache Beam SDK, set`--updateCompatibilityVersion`to the Apache Beam SDK version used in the original job.\n- Pass the`--update`option.\n- Set the`--job_name`option to the same name as the job that you want to update.\n- Set the`--region`option to the same region as the region of the job that you want to update.\n- Include the`--dataflow_service_options=graph_validate_only`service option.\n- If any transform names in your pipeline have changed, you must supply a [transform mapping](#Mapping) and pass it using the`--transform_name_mapping`option.\nTo validate the job graph for a Flex Template job, use the [gcloud dataflow flex-template run](/sdk/gcloud/reference/dataflow/flex-template/run) command with the `additional-experiments` option:- Pass the`--update`option.\n- Set theto the same name as the job that you want to update.\n- Set the`--region`option to the same region as the region of the job that you want to update.\n- Include the`--additional-experiments=graph_validate_only`option.\n- If any transform names in your pipeline have changed, you must supply a [transform mapping](#Mapping) and pass it using the`--transform-name-mappings`option.\nFor example:\n```\ngcloud dataflow flex-template run JOB_NAME --additional-experiments=graph_validate_only\n```\nReplace with the name of the job that you want to update.\nUse the `additionalExperiments` field in the [FlexTemplateRuntimeEnvironment](/dataflow/docs/reference/rest/v1b3/projects.locations.flexTemplates/launch#FlexTemplateRuntimeEnvironment) (Flex templates) or [RuntimeEnvironment](/dataflow/docs/reference/rest/v1b3/RuntimeEnvironment) object.\n```\n{\u00a0 additionalExperiments : [\"graph_validate_only\"]\u00a0 ...}\n```\nThe `graph_validate_only` service option only validates pipeline updates. Don't use this option when creating or launching pipelines. To update your pipeline, [launch a replacement job](#Launching) without the `graph_validate_only` service option.\nWhen the job graph validation succeeds, the job state and the job logs show the following statuses:\n- The [job state](/dataflow/docs/reference/rpc/google.dataflow.v1beta3#google.dataflow.v1beta3.JobState) is`JOB_STATE_DONE`.\n- In the Google Cloud console, the **Job status** is`Succeeded`.\n- The following message appears in the [job logs](/dataflow/docs/guides/logging#MonitoringLogs) :```\nWorkflow job: JOB_ID succeeded validation. Marking graph_validate_only job as Done.\n```\nWhen the job graph validation fails, the job state and the job logs show the following statuses:\n- The [job state](/dataflow/docs/reference/rpc/google.dataflow.v1beta3#google.dataflow.v1beta3.JobState) is`JOB_STATE_FAILED`.\n- In the Google Cloud console, the **Job status** is`Failed`.\n- A message appears in the [job logs](/dataflow/docs/guides/logging#MonitoringLogs) describing the incompatibility error. The message content depends on the error.## Launch a replacement job\nYou might replace an existing job for the following reasons:\n- To run updated pipeline code.\n- To update job options that don't support [in-flight updates](#in-flight-updates) .\nTo verify whether a replacement job is valid, before you launch the new job, [validate its job graph](#validate) .\nWhen you launch a replacement job, set the following pipeline options to perform the update process in addition to the regular options of the job:\n- Pass the`--update`option.\n- Set the`--jobName`option in`PipelineOptions`to the same name as the job that you want to update.\n- Set the`--region`option to the same region as the region of the job that you want to update.\n- If any transform names in your pipeline have changed, you must supply a [transform mapping](#Mapping) and pass it using the`--transformNameMapping`option.\n- If you're submitting a replacement job that uses a later version of the Apache Beam SDK, set`--updateCompatibilityVersion`to the Apache Beam SDK version used in the original job.\n- Pass the`--update`option.\n- Set the`--job_name`option in`PipelineOptions`to the same name as the job that you want to update.\n- Set the`--region`option to the same region as the region of the job that you want to update.\n- If any transform names in your pipeline have changed, you must supply a [transform mapping](#Mapping) and pass it using the`--transform_name_mapping`option.\n- If you're submitting a replacement job that uses a later version of the Apache Beam SDK, set`--updateCompatibilityVersion`to the Apache Beam SDK version used in the original job.\n- Pass the`--update`option.\n- Set the`--job_name`option to the same name as the job that you want to update.\n- Set the`--region`option to the same region as the region of the job that you want to update.\n- If any transform names in your pipeline have changed, you must supply a [transform mapping](#Mapping) and pass it using the`--transform_name_mapping`option.\nTo update a Flex Template job by using the gcloud CLI, use the [gcloud dataflow flex-template run](/sdk/gcloud/reference/dataflow/flex-template/run) command. Updating other jobs by using the gcloud CLI isn't supported.- Pass the`--update`option.\n- Set theto the same name as the job that you want to update.\n- Set the`--region`option to the same region as the region of the job that you want to update.\n- If any transform names in your pipeline have changed, you must supply a [transform mapping](#Mapping) and pass it using the`--transform-name-mappings`option.\nThese instructions show how to update non-template jobs by using the REST API. To use the REST API to update a classic template job, see [Update a custom template streaming job](/dataflow/docs/guides/templates/running-templates#example-3:-updating-a-custom-template-streaming-job) . To use the REST API to update a Flex Template job, see [Update a Flex Template job](/dataflow/docs/guides/templates/configuring-flex-templates#update) .- Fetch the [job](/dataflow/docs/reference/rest/v1b3/projects.jobs#resource:-job) resource for the job that you want to replace by using the [projects.locations.jobs.get](/dataflow/docs/reference/rest/v1b3/projects.locations.jobs/get) method. Include the [view](/dataflow/docs/reference/rest/v1b3/projects.locations.jobs/get#query-parameters) query parameter with the value [JOB_VIEW_DESCRIPTION](/dataflow/docs/reference/rest/v1b3/JobView) . Including `JOB_VIEW_DESCRIPTION` limits the amount of data in the response so that your subsequent request doesn't exceed size limits. If you need more detailed job information, use the value `JOB_VIEW_ALL` .```\nGET https://dataflow.googleapis.com/v1b3/projects/PROJECT_ID/locations/REGION/jobs/JOB_ID?view=JOB_VIEW_DESCRIPTION\n```Replace the following values:- : the Google Cloud project ID of the Dataflow job\n- : the region of the job that you want to update\n- : the job ID of the job that you want to update\n- To update the job, use the [projects.locations.jobs.create](/dataflow/docs/reference/rest/v1b3/projects.locations.jobs/create) method. In the request body, use the `job` resource that you fetched.```\nPOST https://dataflow.googleapis.com/v1b3/projects/PROJECT_ID/locations/REGION/jobs{\u00a0 \"id\": JOB_ID,\u00a0 \"replaceJobId\": JOB_ID,\u00a0 \"name\": JOB_NAME,\u00a0 \"type\": \"JOB_TYPE_STREAMING\",\u00a0 \"transformNameMapping\": {\u00a0 \u00a0 string: string,\u00a0 \u00a0 ...\u00a0 },}\n```Replace the following:- : the same job ID as the ID of the job that you want to update.\n- : the same job name as the name of the job that you want to update.\nIf any transform names in your pipeline have changed, you must supply a [transform mapping](#Mapping) and pass it using the `transformNameMapping` field.\n- Optional: To send your request using curl (Linux, macOS, or Cloud Shell), save the request to a JSON file, and then run the following command:```\ncurl -X POST -d \"@FILE_PATH\" -H \"Content-Type: application/json\" -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \u00a0https://dataflow.googleapis.com/v1b3/projects/PROJECT_ID/locations/REGION/jobs\n```Replace with the path to the JSON file that contains the request body.\n**Caution:** To facilitate the mapping between transforms in your prior pipeline and your replacement pipeline, give explicit names to every transform in your pipelines. By default, Dataflow generates a warning if you don't explicitly name your transforms. You can increase this warning to an error.\n### Specify your replacement job name\nWhen you launch your replacement job, the value you pass for the `--jobName` option must match exactly the name of the job you want to replace.\nWhen you launch your replacement job, the value you pass for the `--job_name` option must match exactly the name of the job you want to replace.\nWhen you launch your replacement job, the value you pass for the `--job_name` option must match exactly the name of the job you want to replace.\nWhen you launch your replacement job, the must match exactly the name of the job you want to replace.\nSet the value of the `replaceJobId` field to the same job ID as the job that you want to update. To find the correct job name value, select your prior job in the [Dataflow Monitoring Interface](/dataflow/pipelines/dataflow-monitoring-intf) . Then, in the **Job info** side panel, find the **Job ID** field.\nTo find the correct job name value, select your prior job in the [DataflowMonitoring Interface](/dataflow/docs/guides/monitoring-overview) . Then, in the **Job info** side panel, find the **Job name** field:Alternatively, query a list of existing jobs by using the [Dataflow Command-line Interface](/dataflow/pipelines/dataflow-command-line-intf) . Enter the command `gcloud dataflow jobs list` into your shell or terminal window to obtain a list of Dataflow jobs in your Google Cloud project, and find the `NAME` field for the job you want to replace:\n```\nJOB_ID         NAME      TYPE  CREATION_TIME  STATE REGION\n2020-12-28_12_01_09-yourdataflowjobid  ps-topic     Streaming 2020-12-28 20:01:10 Running us-central1\n```\n### Create a transform mapping\nIf your replacement pipeline changes any transform names from the names in your prior pipeline, the Dataflow service requires a **transformmapping** . The transform mapping maps the named transforms in your prior pipeline code to names in your replacement pipeline code.\nPass the mapping by using the `--transformNameMapping` command-line option, using the following general format:\n```\n--transformNameMapping= . \n{\"oldTransform1\":\"newTransform1\",\"oldTransform2\":\"newTransform2\",...}\n```\nYou only need to provide mapping entries in `--transformNameMapping` for transform names that have changed between your prior pipeline and your replacement pipeline.\nWhen you run with `--transformNameMapping` , you might need to escape the quotations as appropriate for your shell. For example, in Bash:\n```\n--transformNameMapping='{\"oldTransform1\":\"newTransform1\",...}'\n```\nPass the mapping by using the `--transform_name_mapping` command-line option, using the following general format:\n```\n--transform_name_mapping= .\n{\"oldTransform1\":\"newTransform1\",\"oldTransform2\":\"newTransform2\",...}\n```\nYou only need to provide mapping entries in `--transform_name_mapping` for transform names that have changed between your prior pipeline and your replacement pipeline.\nWhen you run with `--transform_name_mapping` , you might need to escape the quotations as appropriate for your shell. For example, in Bash:\n```\n--transform_name_mapping='{\"oldTransform1\":\"newTransform1\",...}'\n```\nPass the mapping by using the `--transform_name_mapping` command-line option, using the following general format:\n```\n--transform_name_mapping= .\n{\"oldTransform1\":\"newTransform1\",\"oldTransform2\":\"newTransform2\",...}\n```\nYou only need to provide mapping entries in `--transform_name_mapping` for transform names that have changed between your prior pipeline and your replacement pipeline.\nWhen you run with `--transform_name_mapping` , you might need to escape the quotations as appropriate for your shell. For example, in Bash:\n```\n--transform_name_mapping='{\"oldTransform1\":\"newTransform1\",...}'\n```\nPass the mapping by using the `--transform-name-mappings` option, using the following general format:\n```\n--transform-name-mappings= .\n{\"oldTransform1\":\"newTransform1\",\"oldTransform2\":\"newTransform2\",...}\n```\nYou only need to provide mapping entries in `--transform-name-mappings` for transform names that have changed between your prior pipeline and your replacement pipeline.\nWhen you run with `--transform-name-mappings` , you might need to escape the quotations as appropriate for your shell. For example, in Bash:\n```\n--transform-name-mappings='{\"oldTransform1\":\"newTransform1\",...}'\n```\nPass the mapping by using the `transformNameMapping` field, using the following general format:\n```\n\"transformNameMapping\": {\u00a0 oldTransform1: newTransform1,\u00a0 oldTransform2: newTransform2,\u00a0 ...}\n```\nYou only need to provide mapping entries in `transformNameMapping` for transform names that have changed between your prior pipeline and your replacement pipeline.\nThe **transform name** in each instance in the map is the name that you supplied when you applied the transform in your pipeline. For example:\n```\n\u00a0 .apply(\"FormatResults\", ParDo\u00a0 \u00a0 .of(new DoFn&lt;KV&lt;String, Long&gt;&gt;, String>() {\u00a0 \u00a0 \u00a0 ...\u00a0 \u00a0 \u00a0}\u00a0 }))\n```\n```\n\u00a0 | 'FormatResults' >> beam.ParDo(MyDoFn())\n```\n```\n\u00a0 // In Go, this is always the package-qualified name of the DoFn itself.\u00a0 // For example, if the FormatResults DoFn is in the main package, its name\u00a0 // is \"main.FormatResults\".\u00a0 beam.ParDo(s, FormatResults, results)\n```\nYou can also get the transform names for your prior job by examining the execution graph of the job in the [Dataflow Monitoring Interface](/dataflow/pipelines/dataflow-monitoring-intf) :\nTransform names are hierarchical, based on the transform hierarchy in your pipeline. If your pipeline has a [composite transform](https://beam.apache.org/documentation/programming-guide/#composite-transforms) , the nested transforms are named in terms of their containing transform. For example, suppose that your pipeline contains a composite transform named `CountWidgets` , which contains an inner transform named `Parse` . The full name of your transform is `CountWidgets/Parse` , and you must specify that full name in your transform mapping.\nIf your new pipeline maps a composite transform to a different name, all nested transforms are also automatically renamed. You must specify the changed names for the inner transforms in your transform mapping.\nIf your replacement pipeline uses a different transform hierarchy than your prior pipeline, you must explicitly declare the mapping. You might have a different transform hierarchy because you refactored your composite transforms, or your pipeline depends on a composite transform from a library that changed.\nFor example, your prior pipeline applied a composite transform, `CountWidgets` , which contained an inner transform named `Parse` . The replacement pipeline refactors `CountWidgets` , and nests `Parse` inside another transform named `Scan` . For your update to succeed, you must explicitly map the complete transform name in the prior pipeline ( `CountWidgets/Parse` ) to the transform name in the new pipeline ( `CountWidgets/Scan/Parse` ):\n```\n--transformNameMapping={\"CountWidgets/Parse\":\"CountWidgets/Scan/Parse\"}\n```\nIf you delete a transform entirely in your replacement pipeline, you must provide a null mapping. Suppose that your replacement pipeline removes the `CountWidgets/Parse` transform entirely:\n```\n--transformNameMapping={\"CountWidgets/Parse\":\"\"}\n```\n```\n--transform_name_mapping={\"CountWidgets/Parse\":\"CountWidgets/Scan/Parse\"}\n```\nIf you delete a transform entirely in your replacement pipeline, you must provide a null mapping. Suppose that your replacement pipeline removes the `CountWidgets/Parse` transform entirely:\n```\n--transform_name_mapping={\"CountWidgets/Parse\":\"\"}\n```\n```\n--transform_name_mapping={\"CountWidgets/main.Parse\":\"CountWidgets/Scan/main.Parse\"}\n```\nIf you delete a transform entirely in your replacement pipeline, you must provide a null mapping. Suppose that your replacement pipeline removes the `CountWidgets/Parse` transform entirely:\n```\n--transform_name_mapping={\"CountWidgets/main.Parse\":\"\"}\n```\n```\n--transform-name-mappings={\"CountWidgets/Parse\":\"CountWidgets/Scan/Parse\"}\n```\nIf you delete a transform entirely in your replacement pipeline, you must provide a null mapping. Suppose that your replacement pipeline removes the `CountWidgets/Parse` transform entirely:\n```\n--transform-name-mappings={\"CountWidgets/main.Parse\":\"\"}\n```\n```\n\"transformNameMapping\": {\u00a0 CountWidgets/Parse: CountWidgets/Scan/Parse}\n```\nIf you delete a transform entirely in your replacement pipeline, you must provide a null mapping. Suppose that your replacement pipeline removes the `CountWidgets/Parse` transform entirely:\n```\n\"transformNameMapping\": {\u00a0 CountWidgets/main.Parse: null}\n```\n### The effects of replacing a job\nWhen you replace an existing job, a new job runs your updated pipeline code. The Dataflow service retains the job name but runs the replacement job with an updated **Job ID** . This process might cause downtime while the existing job stops, the compatibility check runs, and the new job starts.\nThe replacement job preserves the following items:\n- [Intermediate state data](#state-data) from the prior job. In-memory caches aren't saved.\n- Buffered data records or [metadata currently \"in-flight\"](#in-flight-data) from the prior job. For example, some records in your pipeline might be buffered while waiting for a [window](https://beam.apache.org/documentation/programming-guide/#windowing) to resolve.\n- [In-flight job option updates](#in-flight-updates) that you applied to the prior job.Intermediate state data from the prior job is preserved. State data doesn't include in-memory caches. If you want to preserve in-memory cache data when updating your pipeline, as a workaround, refactor your pipeline to convert caches to [state data](https://beam.apache.org/documentation/programming-guide/#types-of-state) or to [side inputs](https://beam.apache.org/documentation/programming-guide/#side-inputs) . For more information about using side inputs, see [Side input patterns](https://beam.apache.org/documentation/patterns/side-inputs/) in the Apache Beam documentation.\nStreaming pipelines have size limits for `ValueState` and for side inputs. As a result, if you have large caches that you want to preserve, you might need to use external storage, such as Memorystore or Bigtable.\n\"In-flight\" data is still processed by the transforms in your new pipeline. However, additional transforms that you add in your replacement pipeline code might or might not take effect, depending on where the records are buffered. In this example, your existing pipeline has the following transforms:\n```\n p.apply(\"Read\", ReadStrings())\n .apply(\"Format\", FormatStrings());\n``````\n p | 'Read' >> beam.io.ReadFromPubSub(subscription=known_args.input_subscription)\n | 'Format' >> FormatStrings()\n``````\n beam.ParDo(s, ReadStrings)\n beam.ParDo(s, FormatStrings)\n```\nYou can replace your job with new pipeline code, as follows:\n```\n p.apply(\"Read\", ReadStrings())\n .apply(\"Remove\", RemoveStringsStartingWithA())\n .apply(\"Format\", FormatStrings());\n``````\n p | 'Read' >> beam.io.ReadFromPubSub(subscription=known_args.input_subscription)\n | 'Remove' >> RemoveStringsStartingWithA()\n | 'Format' >> FormatStrings()\n``````\n beam.ParDo(s, ReadStrings)\n beam.ParDo(s, RemoveStringsStartingWithA)\n beam.ParDo(s, FormatStrings)\n```\nEven though you add a transform to filter out strings that begin with the letter \"A\", the next transform ( `FormatStrings` ) might still see buffered or in-flight strings that begin with \"A\" that were transferred over from the prior job.\nYou can change [windowing](https://beam.apache.org/documentation/programming-guide/#windowing) and [trigger](https://beam.apache.org/documentation/programming-guide/#triggers) strategies for the `PCollection` elements in your replacement pipeline, but use caution. Changing the windowing or trigger strategies doesn't affect data that is already buffered or otherwise in-flight.\nWe recommend that you attempt only smaller changes to your pipeline's windowing, such as changing the duration of fixed- or sliding-time windows. Making major changes to windowing or triggers, like changing the windowing algorithm, might have unpredictable results on your pipeline output.\n### Job compatibility check\nWhen you launch your replacement job, the Dataflow service performs a compatibility check between your replacement job and your prior job. If the **compatibility check passes** , your prior job is stopped. Your replacement job then launches on the Dataflow service while retaining the same job name. If the **compatibility check fails** , your prior job continues running on the Dataflow service and your replacement job returns an error.\nDue to a limitation, you must use blocking execution to see failed update attempt errors in your console or terminal. The current workaround consists of the following steps:- Use [pipeline.run().waitUntilFinish()](https://beam.apache.org/documentation/sdks/javadoc/current/index.html?org/apache/beam/runners/dataflow/DataflowPipelineJob.html) in your pipeline code.\n- Run your replacement pipeline program with the`--update`option.\n- Wait for the replacement job to successfully pass the compatibility check.\n- Exit the blocking runner process by typing`Ctrl+C`.\nAlternately, you can monitor the state of your replacement job in the [Dataflow Monitoring Interface](/dataflow/pipelines/dataflow-monitoring-intf) . If your job has started successfully, it also passed the compatibility check.\nDue to a limitation, you must use blocking execution to see failed update attempt errors in your console or terminal. The current workaround consists of the following steps:- Use [pipeline.run().wait_until_finish()](https://beam.apache.org/releases/pydoc/current/apache_beam.runners.runner.html?highlight=wait_until_finish#apache_beam.runners.runner.PipelineResult.wait_until_finish) in your pipeline code.\n- Run your replacement pipeline program with the`--update`option.\n- Wait for the replacement job to successfully pass the compatibility check.\n- Exit the blocking runner process by typing`Ctrl+C`.\nAlternately, you can monitor the state of your replacement job in the [Dataflow Monitoring Interface](/dataflow/pipelines/dataflow-monitoring-intf) . If your job has started successfully, it also passed the compatibility check.\nDue to a limitation, you must use blocking execution to see failed update attempt errors in your console or terminal. Specifically, you must specify non-blocking execution by using the `--execute_async` or `--async` flags. The current workaround consists of the following steps:- Run your replacement pipeline program with the`--update`option and without the`--execute_async`or`--async`flags.\n- Wait for the replacement job to successfully pass the compatibility check.\n- Exit the blocking runner process by typing`Ctrl+C`.\nDue to a limitation, you must use blocking execution to see failed update attempt errors in your console or terminal. The current workaround consists of the following steps:- For Java pipelines, use [pipeline.run().waitUntilFinish()](https://beam.apache.org/documentation/sdks/javadoc/current/index.html?org/apache/beam/runners/dataflow/DataflowPipelineJob.html) in your pipeline code. For Python pipelines, use [pipeline.run().wait_until_finish()](https://beam.apache.org/releases/pydoc/current/apache_beam.runners.runner.html?highlight=wait_until_finish#apache_beam.runners.runner.PipelineResult.wait_until_finish) in your pipeline code. For Go pipelines, follow the steps in the Go tab.\n- Run your replacement pipeline program with the`--update`option.\n- Wait for the replacement job to successfully pass the compatibility check.\n- Exit the blocking runner process by typing`Ctrl+C`.\nDue to a limitation, you must use blocking execution to see failed update attempt errors in your console or terminal. The current workaround consists of the following steps:- For Java pipelines, use [pipeline.run().waitUntilFinish()](https://beam.apache.org/documentation/sdks/javadoc/current/index.html?org/apache/beam/runners/dataflow/DataflowPipelineJob.html) in your pipeline code. For Python pipelines, use [pipeline.run().wait_until_finish()](https://beam.apache.org/releases/pydoc/current/apache_beam.runners.runner.html?highlight=wait_until_finish#apache_beam.runners.runner.PipelineResult.wait_until_finish) in your pipeline code. For Go pipelines, follow the steps in the Go tab.\n- Run your replacement pipeline program with the`replaceJobId`field.\n- Wait for the replacement job to successfully pass the compatibility check.\n- Exit the blocking runner process by typing`Ctrl+C`.\nThe compatibility check uses the provided transform mapping to ensure that Dataflow can transfer intermediate state data from the steps in your prior job to your replacement job. The compatibility check also ensures that the `PCollection` s in your pipeline are using the same [Coders](https://beam.apache.org/documentation/programming-guide/#data-encoding-and-type-safety) . Changing a `Coder` can cause the compatibility check to fail because any in-flight data or buffered records might not be correctly serialized in the replacement pipeline.\nCertain differences between your prior pipeline and your replacement pipeline can cause the compatibility to check to fail. These differences include:\n- **Changing the pipeline graph without providing a mapping.** When you update a job, Dataflow attempts to match the transforms in your prior job to the transforms in the replacement job. This matching process helps Dataflow transfer intermediate state data for each step. If you rename or remove any steps, you must provide a [transform mapping](#Mapping) so that Dataflow can match state data accordingly.\n- **Changing the side inputs for a step.** Adding [side inputs](https://beam.apache.org/documentation/programming-guide/#side-inputs) to or removing them from a transform in your replacement pipeline causes the compatibility check to fail.\n- **Changing the Coder for a step.** When you update a job, Dataflow preserves any currently buffered data records and handles them in the replacement job. For example, buffered data might occur while [windowing](https://beam.apache.org/documentation/programming-guide/#windowing) is resolving. If the replacement job uses different or incompatible [data encoding](https://beam.apache.org/documentation/programming-guide/#data-encoding-and-type-safety) , Dataflow is not able to serialize or deserialize these records.\n**Caution:** The Dataflow service can't guarantee that changing a coder in your  prior pipeline to an incompatible coder will cause the compatibility check to fail. It's  recommended that you **do not** attempt to make backwards-incompatible changes to `Coder` s when updating your pipeline. If your pipeline update succeeds but you  encounter issues or errors in the resulting data, ensure that your replacement pipeline uses  data encoding that is compatible with your prior job.\n- **Removing a \"stateful\" operation from your pipeline.** If you remove stateful operations from your pipeline, your replacement job might fail the compatibility check. Dataflow can [fuse](/dataflow/service/dataflow-service-desc#Optimization) multiple steps together for efficiency. If you remove a state-dependent operation from within a fused step, the check fails. Stateful operations include:- Transforms that produce or consume side inputs.\n- I/O reads.\n- Transforms that use keyed state.\n- Transforms that have window merging.\n- **Changing stateful DoFn variables.** For ongoing streaming jobs, if your pipeline includes stateful `DoFn` s, changing the stateful `DoFn` variables might cause the pipeline to fail.\n- **Attempting to run your replacement job in a different geographic zone.** Run your replacement job in the same zone in which you ran your prior job.## Updating schemas\nApache Beam allows `PCollection` s to have schemas with named fields, in which case explicit Coders are not needed. If the field names and types for a given schema are unchanged (including nested fields), then that schema does not cause the update check to fail. However, the update might still be blocked if other segments of the new pipeline are incompatible.\n### Evolve schemas\nOften it's necessary to evolve a `PCollection` 's schema due to evolving business requirements. The Dataflow service allows making the following changes to a schema when updating pipeline:\n- Adding one or more new fields to a schema, including nested fields.\n- Making a required (non-nullable) field type optional (nullable).\nRemoving fields, changing field names, or changing field types isn't permitted during update.\n## Pass additional data into an existing ParDo operation\nYou can pass additional (out-of-band) data into an existing ParDo operation by using one of the following methods, depending on your use case:\n- Serialize information as fields in your`DoFn`subclass.\n- Any variables referenced by the methods in an anonymous`DoFn`are automatically serialized.\n- Compute data inside`DoFn.startBundle()`.\n- Pass in data using`ParDo.withSideInputs`.\nFor more information, see the following pages:\n- [Apache Beam programming guide: ParDo](https://beam.apache.org/documentation/programming-guide/#pardo) , specifically the sections about creating a DoFn and side inputs.\n- [Apache Beam SDK for Java reference: ParDo](https://beam.apache.org/releases/javadoc/current/index.html?org/apache/beam/sdk/transforms/ParDo.html)", "guide": "Dataflow"}