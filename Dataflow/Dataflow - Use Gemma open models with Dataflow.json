{"title": "Dataflow - Use Gemma open models with Dataflow", "url": "https://cloud.google.com/dataflow/docs/machine-learning/gemma", "abstract": "# Dataflow - Use Gemma open models with Dataflow\nGemma is a family of lightweight, state-of-the art open models built from research and technology used to create the Gemini models. You can use Gemma models in your Apache Beam inference pipelines. The term means that a model's pretrained parameters, or weights, are released. Details such as the original dataset, model architecture, and training code aren't provided.\n- For a list of available models and the details about each model, see the [Gemma models overview](https://ai.google.dev/gemma/docs/) .\n- To learn how to download and use models, see [Get started with Gemma using KerasNLP](https://ai.google.dev/gemma/docs/get_started) .\n- To download a model, see [Gemma models](https://www.kaggle.com/models/keras/gemma) .", "content": "## Use cases\nYou can use Gemma models with Dataflow for [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) . With Dataflow and the Gemma models, you can process events, such as customer reviews, as they arrive. Run the reviews through the model to analyze them, and then generate recommendations. By combining Gemma with Apache Beam, you can seamlessly complete this workflow.\n## Support and limitations\nGemma open models are supported with Apache Beam and Dataflow with the following requirements:\n- Available for batch and streaming pipelines that use the Apache Beam Python SDK versions 2.46.0 and later.\n- Dataflow jobs must use [Runner v2](/dataflow/docs/runner-v2) .\n- Dataflow jobs must use [GPUs](/dataflow/docs/gpu/gpu-support) . For a list of GPU types supported with Dataflow, see [Availability](/dataflow/docs/gpu/gpu-support#availability) . The T4 and L4 GPU types are recommended.\n- The model must be downloaded and saved in the`.keras`file format.\n- The [TensorFlow model handler](https://beam.apache.org/documentation/ml/about-ml/#tensorflow) is recommended but not required.## Prerequisites\n- Access Gemma models through [Kaggle](https://www.kaggle.com/models/keras/gemma) .\n- Complete the [consent form](https://www.kaggle.com/models/google/gemma/license/consent) and accept the terms and conditions.\n- Download the Gemma model. Save it in the`.keras`file format in a location that your Dataflow job can access, such as a Cloud Storage bucket. When you specify a value for the model path variable, use the path to this storage location.\n- To run your job on Dataflow, create a custom container image. This step makes it possible to run the pipeline with GPUs on the Dataflow service. For more information, see [Build a custom container image](/dataflow/docs/gpu/use-gpus#custom-container) in \"Run a pipeline with GPUs.\"## Use Gemma in your pipeline\nTo use a Gemma model in your Apache Beam pipeline, follow these steps.\n- In your Apache Beam code, after you import your pipeline dependencies, include a path to your saved model:```\nmodel_path = \"MODEL_PATH\"\n```Replace `` with the path where you saved the downloaded model. For example, if you save your model to a Cloud Storage bucket, the path has the format `gs://` `` `/` `` `.keras` .\n- The Keras implementation of the Gemma models has a `generate()` method that generates text based on a prompt. To pass elements to the `generate()` method, use a custom inference function.```\ndef gemma_inference_function(model, batch, inference_args, model_id):\u00a0 vectorized_batch = np.stack(batch, axis=0)\u00a0 # The only inference_arg expected here is a max_length parameter to\u00a0 # determine how many words are included in the output.\u00a0 predictions = model.generate(vectorized_batch, **inference_args)\u00a0 return utils._convert_to_result(batch, predictions, model_id)\n```\n- Run your pipeline, specifying the path to the trained model. This example uses a TensorFlow model handler.```\nclass FormatOutput(beam.DoFn):\u00a0 def process(self, element, *args, **kwargs):\u00a0 \u00a0 yield \"Input: {input}, Output: {output}\".format(input=element.example, output=element.inference)# Instantiate a NumPy array of string prompts for the model.examples = np.array([\"Tell me the sentiment of the phrase 'I like pizza': \"])# Specify the model handler, providing a path and the custom inference function.model_handler = TFModelHandlerNumpy(model_path, inference_fn=gemma_inference_function)with beam.Pipeline() as p:\u00a0 _ = (p | beam.Create(examples) # Create a PCollection of the prompts.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0| RunInference(model_handler, inference_args={'max_length': 32}) # Send the prompts to the model and get responses.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.ParDo(FormatOutput()) # Format the output.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.Map(print) # Print the formatted output.\u00a0 )\n```## What's next\n- [Run inference with a Gemma open model](/dataflow/docs/machine-learning/gemma-run-inference) .\n- [Run a pipeline with GPUs](/dataflow/docs/gpu/use-gpus) .\n- [Tune your model](https://ai.google.dev/gemma/docs/lora_tuning) .", "guide": "Dataflow"}