{"title": "Dataflow - Streaming pipelines", "url": "https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines", "abstract": "# Dataflow - Streaming pipelines\nUnbounded [PCollections](/dataflow/docs/concepts/beam-programming-model#basic_concepts) , or unbounded , represent data in streaming pipelines. An unbounded collection contains data from a continuously updating data source such as [Pub/Sub](/pubsub/docs) .\nYou cannot use only a key to group elements in an unbounded collection. There might be infinitely many elements for a given key in streaming data because the data source constantly adds new elements. You can use [windows](#windows) , [watermarks](#watermarks) , and [triggers](#triggers) to aggregate elements in unbounded collections.\nThe concept of windows also applies to bounded PCollections that represent data in batch pipelines. For information on windowing in batch pipelines, see the Apache Beam documentation for [Windowing with bounded PCollections](https://beam.apache.org/documentation/programming-guide/#windowing-bounded-collections) .\nIf a Dataflow pipeline has a bounded data source, that is, a source that does not contain continuously updating data, and the pipeline is switched to streaming mode using the `--streaming` flag, when the bounded source is fully consumed, the pipeline stops running.\n", "content": "## Use streaming mode\nTo run a pipeline in streaming mode, set the `--streaming` flag in the [command line](/dataflow/pipelines/specifying-exec-params#setting-pipelineoptions-from-command-line-arguments) when you run your pipeline. You can also set the streaming mode [programmatically](/dataflow/pipelines/specifying-exec-params#configuring-pipelineoptions-for-execution-on-the-cloud-dataflow-service) when you construct your pipeline.\nBatch sources are not supported in streaming mode.\nWhen you update your pipeline with a larger pool of workers, your streaming job might not upscale as expected. For streaming jobs that don't use [Streaming Engine](/dataflow/docs/streaming-engine) , you cannot scale beyond the original number of workers and Persistent Disk resources allocated at the start of your original job. When you [update](/dataflow/pipelines/updating-a-pipeline) a Dataflow job and specify a larger number of workers in the new job, you can only specify a number of workers equal to the maximum number of workers that you specified for your original job.\nSpecify the maximum number of workers by using the following flags:\n`--maxNumWorkers`\n`--max_num_workers`\n`--max_num_workers`\n## Windows and windowing functions\ndivide unbounded collections into logical components, or . Windowing functions group unbounded collections by the timestamps of the individual elements. Each window contains a finite number of elements.\nYou set the following windows with the [Apache Beam SDK](https://beam.apache.org/documentation/programming-guide/#windowing) or [Dataflow SQL streaming extensions](/dataflow/docs/reference/sql/streaming-extensions) :\n- [Tumbling windows](#tumbling-windows) (called [fixed windows](https://beam.apache.org/documentation/programming-guide/#fixed-time-windows) in Apache Beam)\n- [Hopping windows](#hopping-windows) (called [sliding windows](https://beam.apache.org/documentation/programming-guide/#sliding-time-windows) in Apache Beam)\n- [Session windows](#session-windows) \n### Tumbling windows\nA tumbling window represents a consistent, disjoint time interval in the data stream.\nFor example, if you set to a thirty-second tumbling window, the elements with timestamp values [0:00:00-0:00:30) are in the first window. Elements with timestamp values [0:00:30-0:01:00) are in the second window.\nThe following image illustrates how elements are divided into thirty-second tumbling windows.### Hopping windows\nA hopping window represents a consistent time interval in the data stream. Hopping windows can overlap, whereas tumbling windows are disjoint.\nFor example, a hopping window can start every thirty seconds and capture one minute of data. The frequency with which hopping windows begin is called the . This example has a one-minute window and thirty-second period.\nThe following image illustrates how elements are divided into one-minute hopping windows with a thirty-second period.\nTo take running averages of data, use hopping windows. You can use one-minute hopping windows with a thirty-second period to compute a one-minute running average every thirty seconds.\n### Session windows\nA session window contains elements within a of another element. The gap duration is an interval between new data in a data stream. If data arrives after the gap duration, the data is assigned to a new window.\nFor example, session windows can divide a data stream representing user mouse activity. This data stream might have long periods of idle time interspersed with many clicks. A session window can contain the data generated by the clicks.\nSession windowing assigns different windows to each data key. Tumbling and hopping windows contain all elements in the specified time interval, regardless of data keys.\nThe following image visualizes how elements are divided into session windows.\n## Watermarks\nA is a threshold that indicates when Dataflow expects all of the data in a window to have arrived. If the watermark has progressed past the end of the window and new data arrives with a timestamp within the window, the data is considered . For more information, see [Watermarks and late data](https://beam.apache.org/documentation/programming-guide/#watermarks-and-late-data) in the Apache Beam documentation.\nDataflow tracks watermarks because of the following reasons:\n- Data is not guaranteed to arrive in time order or at predictable intervals.\n- Data events are not guaranteed to appear in pipelines in the same order that they were generated.\nThe data source determines the watermark. You can [allow late data](https://beam.apache.org/documentation/programming-guide/#managing-late-data) with the Apache Beam SDK. Dataflow SQL does not process late data.\n## Triggers\ndetermine when to emit aggregated results as data arrives. By default, results are emitted when the [watermark](#watermarks) passes the end of the window.\nYou can use the Apache Beam SDK to create or modify triggers for each collection in a streaming pipeline. You cannot set triggers with Dataflow SQL.\nThe Apache Beam SDK can [set triggers](https://beam.apache.org/documentation/programming-guide/#triggers) that operate on any combination of the following conditions:\n- Event time, as indicated by the timestamp on each data element.\n- Processing time, which is the time that the data element is processed at any given stage in the pipeline.\n- The number of data elements in a collection.## What's next\nFor a deep dive into the design of streaming SQL, see [One SQL to Rule Them All](https://arxiv.org/abs/1905.12133) .", "guide": "Dataflow"}