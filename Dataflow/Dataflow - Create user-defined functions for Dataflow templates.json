{"title": "Dataflow - Create user-defined functions for Dataflow templates", "url": "https://cloud.google.com/dataflow/docs/guides/templates/create-template-udf", "abstract": "# Dataflow - Create user-defined functions for Dataflow templates\nSome [Google-provided Dataflow templates](/dataflow/docs/guides/templates/provided-templates) support user-defined functions (UDFs) written in JavaScript. UDFs let you extend the functionality of a template without modifying the template code. To use a UDF, you store the JavaScript code file in Cloud Storage and specify the location as a template parameter. The template calls the UDF for each input element. Inside the function, you can transform the element or perform other custom logic, and then return the result back to the template.\nFor example, you might use a UDF to:\n- Reformat the input data to match a target schema.\n- Redact sensitive data.\n- Filter some elements from the output.\nThis document provides an overview of template UDFs and best practices for writing and testing a UDF. For a list of example UDFs, see the [DataflowTemplates GitHub repo](https://github.com/GoogleCloudPlatform/DataflowTemplates/tree/main/v2/common/src/main/resources/udf-samples) .\n", "content": "## Overview\nA template UDF is a JavaScript function. The input to the function is a single data element, serialized as a JSON string. The function returns a serialized JSON string as output.\nThe data format depends on the template. For example, in the [Pub/Sub Subscription to BigQuery](/dataflow/docs/guides/templates/provided/pubsub-subscription-to-bigquery) template, the input is the Pub/Sub message data serialized as a JSON object, and the output is a serialized JSON object representing a BigQuery table row. For more information, see the [documentation](/dataflow/docs/guides/templates/provided-templates) for each template.\nThe following code shows a no-op UDF that you can start from:\n```\n/*\u00a0* @param {string} inJson input JSON message (stringified)\u00a0* @return {?string} outJson output JSON message (stringified)\u00a0*/function process(inJson) {\u00a0 const obj = JSON.parse(inJson);\u00a0 // Example data transformations:\u00a0 // Add a field: obj.newField = 1;\u00a0 // Modify a field: obj.existingField = '';\u00a0 // Filter a record: return null;\u00a0 return JSON.stringify(obj);}\n```\n## Run a template with a UDF\nTo run a template with a UDF, you specify the Cloud Storage location of the JavaScript file and the name of the function as template parameters.\n**Note:** The relevant template parameter names depend on the template. See the documentation for each template.\nWith some Google-provided templates, you can also create the UDF directly in the Google Cloud console, as follows:\n- Go to the Dataflow page in the Google Cloud console. [Go to the Dataflow page](https://console.cloud.google.com/dataflow) \n- Click **Create job from template** .\n- Select the Google-provided template that you want to run.\n- Expand **Optional parameters** . If the template supports UDFs, it has a parameter for the Cloud Storage location of the UDF and another parameter for the function name.\n- Next to the template parameter, click **Create UDF** . **Note:** This button is not available for all templates. If this button is not visible, then create the UDF in a local file and upload the file to Cloud Storage.\n- In the **Select or Create a User-Defined Function (UDF)** panel:- Enter a filename. Example:`my_udf.js`.\n- Select a Cloud Storage folder. Example:`gs://your-bucket/your-folder`.\n- Use the inline code editor to write the function. The editor is pre-populated with boilerplate code that you can use as a starting point.\n- Click **Create UDF** .The Google Cloud console saves the UDF file and populates the Cloud Storage location.\n- Enter your function's name in the corresponding field.\n## Error handling\nTypically, when an error occurs during UDF execution, the error is written to a dead-letter location. The details depend on the template. For example, the [Pub/Sub Subscription to BigQuery](/dataflow/docs/guides/templates/provided/pubsub-subscription-to-bigquery) template creates an `_error_records` table and writes errors there. Runtime UDF errors can occur because of syntax errors or uncaught exceptions. To check for syntax errors, [test your UDF](#test_a_udf) locally.\nYou can programmatically throw an exception for an element that shouldn't be processed. In the case, the element is written to the dead-letter location, if the template supports one. For an example that shows this approach, see [Route events](#route_events) .\n## Test a UDF\nThe UDF JavaScript code runs on [Nashorn JavaScript engine](https://github.com/openjdk/nashorn) . We recommend testing your UDF on the Nashorn engine before deploying it.\nThe Nashorn engine does not exactly match the Node.js implementation of JavaScript. A common pitfall is using `console.log()` or `Number.isNaN()` , neither of which is defined in the Nashorn engine.\nYou can test your UDF on Nashorn engine by using Cloud Shell, which has JDK 11 pre-installed. Launch Nashorn in interactive mode as follows:\n```\njjs\n```\nIn the Nashorn interactive shell, perform the following steps:\n- Call`load`to load your UDF JavaScript file.\n- Define an input JSON object depending on your pipeline's expected messages.\n- Use the`JSON.stringify`function to serialize the input to a JSON string.\n- Call your UDF function to process the JSON string.\n- Call`JSON.parse`to deserialize the output.\n- Verify the result.\nExample:\n```\n> load('my_udf.js')> var input = {\"name\":\"user1\"}> var output = process(JSON.stringify(input))> print(output)\n```\n## Example use cases\nThis section describes some common patterns for UDFs, based on real-world use cases.\n### Enrich events\nUse a UDF to enrich events with new fields for more contextual information.\nExample:\n```\n\u00a0function process(inJson) {\u00a0 const data = JSON.parse(inJson);\u00a0 // Add new field to track data source\u00a0 data.source = \"source1\";\u00a0 return JSON.stringify(data);}\n```\n### Transform events\nUse a UDF to transform the entire event format depending on what your destination expects.\nThe following example reverts a Cloud Logging log entry ( [LogEntry](/logging/docs/reference/v2/rest/v2/LogEntry) ) to the original log string when available. (Depending on the log source, the original log string is sometimes populated in the `textPayload` field.) You might use this pattern to send the raw logs in their original format, instead of sending the entire `LogEntry` from Cloud Logging.\n```\n\u00a0function process(inJson) {\u00a0 const data = JSON.parse(inJson);\u00a0 if (data.textPayload) {\u00a0 \u00a0 return data.textPayload; // Return string value, and skip JSON.stringify\u00a0 }\u00a0return JSON.stringify(obj);}\n```\n### Redact or remove event data\nUse a UDF to redact or remove a part of the event.\nThe following example redacts the field name `sensitiveField` by replacing its value, and removes the field named `redundantField` entirely.\n```\n\u00a0function process(inJson) {\u00a0 const data = JSON.parse(inJson);\u00a0 // Normalize existing field values\u00a0 data.source = (data.source && data.source.toLowerCase()) || \"unknown\";\u00a0 // Redact existing field values\u00a0 if (data.sensitiveField) {\u00a0 \u00a0 data.sensitiveField = \"REDACTED\";\u00a0 }\u00a0 // Remove existing fields\u00a0 if (data.redundantField) {\u00a0 \u00a0 delete(data.redundantField);\u00a0 }\u00a0 return JSON.stringify(data);}\n```\n### Route events\nUse a UDF to route events to separate destinations in the downstream sink.\nThe following example, based on the [Pub/Sub to Splunk](/dataflow/docs/guides/templates/provided/pubsub-to-splunk) template, routes each event to the correct Splunk index. It calls a user-defined local function to map events to indexes.\n```\nfunction process(inJson) {\u00a0 const obj = JSON.parse(inJson);\u00a0 \u00a0 // Set index programmatically for data segregation in Splunk\u00a0 obj._metadata = {\u00a0 \u00a0 index: splunkIndexLookup(obj)\u00a0 }\u00a0 return JSON.stringify(obj);} \u00a0\n```\nThe next example routes unrecognized events to the dead-letter queue, assuming the template supports a dead-letter queue. (For example, see the [Pub/Sub to JDBC](/dataflow/docs/guides/templates/provided/pubsub-to-jdbc) template.) You might use this pattern to filter out unexpected entries before writing to the destination.\n```\n\u00a0function process(inJson) {\u00a0 const data = JSON.parse(inJson);\u00a0 // Route unrecognized events to the deadletter topic\u00a0 if (!data.hasOwnProperty('severity')) {\u00a0 \u00a0 throw new Error(\"Unrecognized event. eventId='\" + data.Id + \"'\");\u00a0 }\u00a0 return JSON.stringify(data);\n```\n### Filter events\nUse a UDF to filter undesired or unrecognized events from the output.\nThe following example drops events where `data.severity` equals `\"DEBUG\"` .\n```\n\u00a0function process(inJson) {\u00a0 const data = JSON.parse(inJson);\u00a0 // Drop events with certain field values\u00a0 if (data.severity == \"DEBUG\") {\u00a0 \u00a0 return null;\u00a0 }\u00a0 return JSON.stringify(data);}\n```\n## What's next\n- [Google-provided templates](/dataflow/docs/guides/templates/provided-templates) \n- [Build and run a Flex Template](/dataflow/docs/guides/templates/using-flex-templates) \n- [Running classic templates](/dataflow/docs/guides/templates/running-templates) \n- [Extend your Dataflow template with UDFs](/blog/topics/developers-practitioners/extend-your-dataflow-template-with-udfs) (blog post)", "guide": "Dataflow"}