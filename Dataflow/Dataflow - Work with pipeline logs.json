{"title": "Dataflow - Work with pipeline logs", "url": "https://cloud.google.com/dataflow/docs/guides/logging", "abstract": "# Dataflow - Work with pipeline logs\nYou can use the Apache Beam SDK's built-in logging infrastructure to log information when running your pipeline. You can use the [Google Cloud console](https://console.cloud.google.com/) to monitor logging information during and after your pipeline runs.\nCustomers who are subject to the requirements of the Health Insurance Portability and Accountability Act (known as HIPAA), note that Dataflow is eligible to be included on business associate agreements (BAA) with Google. If you would like your pipelines to be eligible for the Google Cloud BAA agreement, ensure that your pipelines only use services listed on the [HIPAA compliance](/security/compliance/hipaa) page.\n", "content": "## Add log messages to your pipeline\nThe Apache Beam SDK for Java recommends that you log worker messages through the open source [SimpleLogging Facade for Java (SLF4J) library](http://www.slf4j.org/) . The Apache Beam SDK for Java implements the required logging infrastructure so that your Java code only needs to import the SLF4J API. Then, it instantiates a Logger to enable message logging within your pipeline code.\nFor pre-existing code and/or libraries, the Apache Beam SDK for Java sets up additional logging infrastructure. Log messages produced by the following logging libraries for Java are captured:- [Apache/Jakarta Commons Logging](http://commons.apache.org/logging/) \n- [Java Logging API](https://docs.oracle.com/javase/7/docs/technotes/guides/logging/) \n- [Log4j](http://logging.apache.org/log4j/1.2/) \n- [Log4j2](http://logging.apache.org/log4j/2.x/) \n- [SLF4J](http://www.slf4j.org/) \nThe Apache Beam SDK for Python provides the `logging` library package, which allows the pipeline workers to output log messages. To use the library functions, you must import the library:\n```\nimport logging\n```\nThe Apache Beam SDK for Go provides the `log` library package, which allows the pipeline workers to output log messages. To use the library functions, you must import the library:\n```\nimport \"github.com/apache/beam/sdks/v2/go/pkg/beam/log\"\n```\n### Worker log message code example\nThe following example uses SLF4J for Dataflow logging. To learn more about configuring SLF4J for Dataflow logging, see the [Java Tips](https://cwiki.apache.org/confluence/display/BEAM/Java+Tips) article.\nThe Apache Beam [WordCount](https://github.com/apache/beam/blob/master/examples/java/src/main/java/org/apache/beam/examples/WordCount.java) example can be modified to output a log message when the word \"love\" is found in a line of the processed text. The added code is indicated in **bold** in the following example (surrounding code is included for context).\n```\n\u00a0package org.apache.beam.examples;\u00a0// Import SLF4J packages.\u00a0import org.slf4j.Logger;\u00a0import org.slf4j.LoggerFactory;\u00a0...\u00a0public class WordCount {\u00a0 \u00a0...\u00a0 \u00a0static class ExtractWordsFn extends DoFn<String, String> {\u00a0 \u00a0 \u00a0// Instantiate Logger.\u00a0 \u00a0 \u00a0// Suggestion: As shown, specify the class name of the containing class\u00a0 \u00a0 \u00a0// (WordCount).\u00a0 \u00a0 \u00a0private static final Logger LOG = LoggerFactory.getLogger(WordCount.class);\u00a0 \u00a0 \u00a0...\u00a0 \u00a0 \u00a0@ProcessElement\u00a0 \u00a0 \u00a0public void processElement(ProcessContext c) {\u00a0 \u00a0 \u00a0 \u00a0...\u00a0 \u00a0 \u00a0 \u00a0// Output each word encountered into the output PCollection.\u00a0 \u00a0 \u00a0 \u00a0for (String word : words) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0if (!word.isEmpty()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0c.output(word);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0// Log INFO messages when the word \"love\" is found.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0if(word.toLowerCase().equals(\"love\")) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0LOG.info(\"Found \" + word.toLowerCase());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0}\u00a0 \u00a0 \u00a0 \u00a0}\u00a0 \u00a0 \u00a0}\u00a0 \u00a0}\u00a0... // Remaining WordCount example code ...\n```The Apache Beam [wordcount.py](https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/wordcount.py) example can be modified to output a log message when the word \"love\" is found in a line of the processed text.\n```\n# import Python logging module.import loggingclass ExtractWordsFn(beam.DoFn):\u00a0 def process(self, element):\u00a0 \u00a0 words = re.findall(r'[A-Za-z\\']+', element)\u00a0 \u00a0 for word in words:\u00a0 \u00a0 \u00a0 yield word\u00a0 \u00a0 \u00a0 if word.lower() == 'love':\u00a0 \u00a0 \u00a0 \u00a0 # Log using the root logger at info or higher levels\u00a0 \u00a0 \u00a0 \u00a0 logging.info('Found : %s', word.lower())# Remaining WordCount example code ...\n```The Apache Beam [wordcount.go](https://github.com/apache/beam/blob/master/sdks/go/examples/wordcount/wordcount.go) example can be modified to output a log message when the word \"love\" is found in a line of the processed text.\n```\nfunc (f *extractFn) ProcessElement(ctx context.Context, line string, emit func(string)) {\u00a0 \u00a0 for _, word := range wordRE.FindAllString(line, -1) {\u00a0 \u00a0 \u00a0 \u00a0 // increment the counter for small words if length of words is\u00a0 \u00a0 \u00a0 \u00a0 // less than small_word_length\u00a0 \u00a0 \u00a0 \u00a0 if strings.ToLower(word) == \"love\" {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Infof(ctx, \"Found : %s\", strings.ToLower(word))\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 emit(word)\u00a0 \u00a0 }}// Remaining Wordcount example\n```\nIf the modified WordCount pipeline is run locally using the default with the output sent to a local file ( `--output=./local-wordcounts` ), console output includes the added log messages:\n```\nINFO: Executing pipeline using the DirectRunner.\n...\nFeb 11, 2015 1:13:22 PM org.apache.beam.examples.WordCount$ExtractWordsFn processElement\nINFO: Found love\nFeb 11, 2015 1:13:22 PM org.apache.beam.examples.WordCount$ExtractWordsFn processElement\nINFO: Found love\nFeb 11, 2015 1:13:22 PM org.apache.beam.examples.WordCount$ExtractWordsFn processElement\nINFO: Found love\n...\nINFO: Pipeline execution complete.\n```\nBy default, only log lines marked `INFO` and higher will be sent to Cloud Logging. If you want to change this behavior, see [Setting Pipeline Worker Log Levels](#SettingLevels) .If the modified WordCount pipeline is run locally using the default with the output sent to a local file ( `--output=./local-wordcounts` ), console output includes the added log messages:\n```\nINFO:root:Found : love\nINFO:root:Found : love\nINFO:root:Found : love\n```\nBy default, only log lines marked `INFO` and higher will be sent to Cloud Logging.If the modified WordCount pipeline is run locally using the default with the output sent to a local file ( `--output=./local-wordcounts` ), console output includes the added log messages:\n```\n2022/05/26 11:36:44 Found : love\n2022/05/26 11:36:44 Found : love\n2022/05/26 11:36:44 Found : love\n```\nBy default, only log lines marked `INFO` and higher will be sent to Cloud Logging.\n## Control log volume\nYou might also reduce the volume of logs generated by changing the pipeline [log levels](/dataflow/docs/guides/logging#SettingLevels) . If you don't want to continue ingesting some or all of your Dataflow logs, add a Logging exclusion to [exclude Dataflow logs](/logging/docs/exclusions#dataflow-exclusion-filter) . Then, export the logs to a different destination such as BigQuery, Cloud Storage, or Pub/Sub. For more information, see [Control Dataflow log ingestion](/dataflow/docs/guides/filter-logs) .\nAlthough Cloud Logging provides you with the ability to exclude logs from being ingested, you might want to consider keeping system logs (labels.\"dataflow.googleapis.com/log_type\"=\"system\") and supportability logs (labels.\"dataflow.googleapis.com/log_type\"=\"supportability\"). Using these logs can help you and Cloud Support troubleshoot and identify issues with your applications.\n## Logging limit and throttling\nWorker log messages are limited to 15,000\u00a0messages\u00a0every\u00a030\u00a0seconds, per worker. If this limit is reached, a single worker log message is added saying that logging is throttled:\n```\nThrottling logger worker. It used up its 30s quota for logs in only 12.345s\n```\n## Log storage and retention\nOperational logs are stored in the [_Default](/logging/docs/routing/overview#default-bucket) log bucket. The logging API service name is `dataflow.googleapis.com` . For more information about the Google Cloud monitored resource types and services used in Cloud Logging, see [Monitored resources and services](/logging/docs/api/v2/resource-list) .\nFor details about how long log entries are retained by Logging, see the retention information in [Quotas and limits: Logs retention periods](/logging/quotas#logs_retention_periods) .\nFor information about viewing operational logs, see [Monitor and view pipeline logs](#MonitoringLogs) .\n## Monitor and view pipeline logs\nWhen you run your pipeline on the [Dataflow service](/dataflow/service/dataflow-service-desc) , you can use the Dataflow [monitoring interface](/dataflow/pipelines/dataflow-monitoring-intf) to view logs emitted by your pipeline.\n### Dataflow worker log example\nThe modified WordCount pipeline can be run in the cloud with the following options:\n```\n--project=WordCountExample--output=gs://<bucket-name>/counts--runner=DataflowRunner--tempLocation=gs://<bucket-name>/temp--stagingLocation=gs://<bucket-name>/binaries\n``````\n--project=WordCountExample--output=gs://<bucket-name>/counts--runner=DataflowRunner--staging_location=gs://<bucket-name>/binaries\n``````\n--project=WordCountExample--output=gs://<bucket-name>/counts--runner=DataflowRunner--staging_location=gs://<bucket-name>/binaries\n```\nBecause the WordCount cloud pipeline uses blocking execution, console messages are output during pipeline execution. After the job starts, a link to the Google Cloud console page is output to the console, followed by the pipeline job ID:\n```\nINFO: To access the Dataflow monitoring console, please navigate to\nhttps://console.developers.google.com/dataflow/job/2017-04-13_13_58_10-6217777367720337669\nSubmitted job: 2017-04-13_13_58_10-6217777367720337669\n```\nThe console URL leads to the Dataflow [monitoring interface](/dataflow/pipelines/dataflow-monitoring-intf) with a summary page for the submitted job. It shows a dynamic execution graph on the left, with summary information on the right. Click on the bottom panel to expand the logs panel.\nThe logs panel defaults to showing **Job Logs** that report the status of the job as a whole. You can filter the messages that appear in the logs panel by clicking **Info** and **Filter logs** .\nSelecting a pipeline step in the graph changes the view to **Step Logs** generated by your code and the generated code running in the pipeline step.\nTo get back to **Job Logs** , clear the step by clicking outside the graph or using the **Deselect step** button in the right side panel.\nClicking the external link button from the logs panel navigates to **Logging** with a menu to select different log types.\nLogging also includes other infrastructure logs for your pipeline. For more details on how to explore your logs, refer to the [Logs explorer](/logging/docs/view/logs-explorer-interface) guide.\nHere's a summary of the different log types available for viewing from the **Logging** page:\n- **job-message** logs contain job-level messages that various components of Dataflow generate. Examples include the autoscaling configuration, when workers start up or shut down, progress on the job step, and job errors. Worker-level errors that originate from crashing user code and that are present in **worker** logs also propagate up to the **job-message** logs.\n- **worker** logs are produced by Dataflow workers. Workers do most of the pipeline work (for example, applying your`ParDo`s to data). **Worker** logs contain messages logged by your code and Dataflow.\n- **worker-startup** logs are present on most Dataflow jobs and can capture messages related to the startup process. The startup process includes downloading the jars of the job from Cloud Storage, then starting the workers. If there is a problem starting workers, these logs are a good place to look.\n- **shuffler** logs contain messages from workers that consolidate the results of parallel pipeline operations.\n- **docker** and **kubelet** logs contain messages related to these public technologies, which are used on Dataflow workers.\n- **nvidia-mps** logs contain messages about [NVIDIA Multi-Process Service (MPS) operations](/dataflow/docs/gpu/use-nvidia-mps) .\n### Set pipeline worker log levels\nThe default SLF4J logging level set on workers by the Apache Beam SDK for Java is `INFO` . All log messages of `INFO` or higher ( `INFO` , `WARN` , `ERROR` ) will be emitted. You can set a different default log level to support lower SLF4J logging levels ( `TRACE` or `DEBUG` ) or set different log levels for different packages of classes in your code.\nThe following pipeline options are provided to let you set worker log levels from the command line or programmatically:- `--defaultSdkHarnessLogLevel=<level>`: use this option to set all loggers at the  specified default level. For example, the following command-line option will override the  default Dataflow`INFO`log level, and set it to`DEBUG`:`--defaultSdkHarnessLogLevel=DEBUG`\n- `--sdkHarnessLogLevelOverrides={\"<package or class>\":\"<level>\"}`: use this option  to set the logging level for specified packages or classes. For example, to override the  default pipeline log level for the`org.apache.beam.runners.dataflow`package, and set it to`TRACE`:`--sdkHarnessLogLevelOverrides='{\"org.apache.beam.runners.dataflow\":\"TRACE\"}'`To make multiple overrides, provide a JSON map:(`--sdkHarnessLogLevelOverrides={\"<package/class>\":\"<level>\",\"<package/class>\":\"<level>\",...}`).\n- The`defaultSdkHarnessLogLevel`and`sdkHarnessLogLevelOverrides`pipeline options aren't  supported with pipelines that use the Apache Beam SDK versions 2.50.0 and earlier without Runner v2.  In that case, use the`--defaultWorkerLogLevel=<level>`and`--workerLogLevelOverrides={\"<package or class>\":\"<level>\"}`pipeline options. To make multiple overrides, provide a JSON map:(`--workerLogLevelOverrides={\"<package/class>\":\"<level>\",\"<package/class>\":\"<level>\",...}`)\nThe following example programmatically sets pipeline logging options with default values that can be overridden from the command line:\n```\n\u00a0PipelineOptions options = ...\u00a0SdkHarnessOptions loggingOptions = options.as(SdkHarnessOptions.class);\u00a0// Overrides the default log level on the worker to emit logs at TRACE or higher.\u00a0loggingOptions.setDefaultSdkHarnessLogLevel(LogLevel.TRACE);\u00a0// Overrides the Foo class and \"org.apache.beam.runners.dataflow\" package to emit logs at WARN or higher.\u00a0loggingOptions.getSdkHarnessLogLevelOverrides()\u00a0 \u00a0 \u00a0.addOverrideForClass(Foo.class, LogLevel.WARN)\u00a0 \u00a0 \u00a0.addOverrideForPackage(Package.getPackage(\"org.apache.beam.runners.dataflow\"), LogLevel.WARN);\n```Note: this feature is available in the Apache Beam SDK for Python 2.41.0 and later versions. It does not currently support multi-language transforms.\nThe default logging level set on workers by the Apache Beam SDK for Python is `INFO` . All log messages of `INFO` or higher ( `INFO` , `WARNING` , `ERROR` , `CRITICAL` ) will be emitted. You can set a different default log level to support lower logging levels ( `DEBUG` ) or set different log levels for different modules in your code.\nTwo pipeline options are provided to let you set worker log levels from the command line or programmatically:- `--default_sdk_harness_log_level=<level>`: use this option to set all loggers at the  specified default level. For example, the following command-line option overrides the  default Dataflow`INFO`log level, and sets it to`DEBUG`:`--default_sdk_harness_log_level=DEBUG`\n- `--sdk_harness_log_level_overrides={\\\"<module>\\\":\\\"<level>\\\"}`: use this option  to set the logging level for specified modules. For example, to override the  default pipeline log level for the`apache_beam.runners.dataflow`module,  and set it to`DEBUG`:`--sdk_harness_log_level_overrides={\\\"apache_beam.runners.dataflow\\\":\\\"DEBUG\\\"}`To make multiple overrides, provide a JSON map:(`--sdk_harness_log_level_overrides={\\\"<module>\\\":\\\"<level>\\\",\\\"<module>\\\":\\\"<level>\\\",...}`).\nThe following example uses the [WorkerOptions](https://beam.apache.org/releases/pydoc/current/apache_beam.options.pipeline_options.html#apache_beam.options.pipeline_options.WorkerOptions) class to programmatically set pipeline logging options that can be overridden from the command line:\n```\n\u00a0 from apache_beam.options.pipeline_options import PipelineOptions, WorkerOptions\u00a0 pipeline_args = [\u00a0 \u00a0 '--project=PROJECT_NAME',\u00a0 \u00a0 '--job_name=JOB_NAME',\u00a0 \u00a0 '--staging_location=gs://STORAGE_BUCKET/staging/',\u00a0 \u00a0 '--temp_location=gs://STORAGE_BUCKET/tmp/',\u00a0 \u00a0 '--region=DATAFLOW_REGION',\u00a0 \u00a0 '--runner=DataflowRunner'\u00a0 ]\u00a0 pipeline_options = PipelineOptions(pipeline_args)\u00a0 worker_options = pipeline_options.view_as(WorkerOptions)\u00a0 worker_options.default_sdk_harness_log_level = 'WARNING'\u00a0 # Note: In Apache Beam SDK 2.42.0 and earlier versions, use ['{\"apache_beam.runners.dataflow\":\"WARNING\"}']\u00a0 worker_options.sdk_harness_log_level_overrides = {\"apache_beam.runners.dataflow\":\"WARNING\"}\u00a0 # Pass in pipeline options during pipeline creation.\u00a0 with beam.Pipeline(options=pipeline_options) as pipeline:\n```\nReplace the following:- ``: the name of the project\n- ``: the name of the job\n- ``: the Cloud Storage name\n- ``: the [region](/dataflow/docs/resources/locations) where you want to deploy the Dataflow jobThe `--region` flag overrides the default region that is set in the metadata server, your local client, or environment variables.\nThis feature is not available in the Apache Beam SDK for Go.\n### View the log of launched BigQuery jobs\nWhen using BigQuery in your Dataflow pipeline, [BigQuery jobs](/bigquery/docs/jobs-overview) are launched to perform various actions on your behalf. These actions might include loading data, exporting data, and so on. For troubleshooting and monitoring purposes, the Dataflow monitoring interface has additional information on these BigQuery jobs available in the **Logs** panel.\n**Note:** To view BigQuery jobs information in the **Logs** panel, your Dataflow job must use [BigQueryIO.Read](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Read.html) to read data from BigQuery, or use the [FILE_LOADS](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.Method.html#FILE_LOADS) insertion method to write data.\nThe BigQuery jobs information displayed in the **Logs** panel is stored and loaded from a BigQuery system table. A [billing cost](/bigquery/pricing#queries) is incurred when the underlying BigQuery table is queried.\nTo view the BigQuery jobs information, your pipeline must use Apache Beam 2.24.0 or later.\nTo list the BigQuery jobs, open the **BigQuery Jobs** tab and select the location of the BigQuery jobs. Next, click **Load BigQuery Jobs** and confirm the dialog. After the query completes, the jobs list is displayed.\n**Note:** BigQuery jobs run in the [same location](/bigquery/docs/locations) as the dataset they read from or write to.\nBasic information about each job is provided including job ID, type, duration, and so on.\nFor more detailed information on a specific job, click **Command line** in the **More Info** column.\nIn the modal window for the command line, copy the [bq jobs describe](/sdk/gcloud/reference/alpha/bq/jobs/describe) command and run it locally or in Cloud Shell.\n```\ngcloud alpha bq jobs describe BIGQUERY_JOB_ID\n```\nThe `bq jobs describe` command outputs [JobStatistics](/bigquery/docs/reference/rest/v2/Job#jobstatistics) , which provide further details that are useful when diagnosing a slow or stuck BigQuery job.\nAlternatively, when you use [BigQueryIO](https://beam.apache.org/documentation/io/built-in/google-bigquery/) with a SQL query, a query job is issued. To see the SQL query used by the job, click **View query** in the **More Info** column.", "guide": "Dataflow"}