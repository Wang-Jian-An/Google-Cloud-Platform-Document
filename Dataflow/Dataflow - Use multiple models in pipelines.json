{"title": "Dataflow - Use multiple models in pipelines", "url": "https://cloud.google.com/dataflow/docs/machine-learning/ml-multi-model", "abstract": "# Dataflow - Use multiple models in pipelines\nYou can use the `RunInference` API to build pipelines that contain multiple models. Multi-model pipelines are useful for tasks such as A/B testing and building ensembles to solve business problems that require more than one ML model.\n", "content": "## Use multiple models\nThe following code examples show how to use the `RunInference` transform to add multiple models to your pipeline.\nWhen you build pipelines with multiple models, you can use one of two patterns:\n- **A/B branch pattern:** One portion of the input data goes to one model, and the rest of the data goes to a second model.\n- **Sequence pattern:** The input data traverses two models, one after the other.\n### A/B pattern\nThe following code shows how to add an A/B pattern to your pipeline with the `RunInference` transform.\n```\nwith pipeline as p:\u00a0 \u00a0data = p | 'Read' >> beam.ReadFromSource('a_source')\u00a0 \u00a0model_a_predictions = data | RunInference(MODEL_HANDLER_A)\u00a0 \u00a0model_b_predictions = data | RunInference(MODEL_HANDLER_B)\n```\nand are the model handler setup code.\nThe following diagram provides a visual presentation of this process.### Sequence pattern\nThe following code shows how to add a sequence pattern to your pipeline with the `RunInference` transform.\n```\nwith pipeline as p:\u00a0 \u00a0data = p | 'Read' >> beam.ReadFromSource('A_SOURCE')\u00a0 \u00a0model_a_predictions = data | RunInference(MODEL_HANDLER_A)\u00a0 \u00a0model_b_predictions = model_a_predictions | beam.Map(some_post_processing) | RunInference(MODEL_HANDLER_B)\n```\nand are the model handler setup code.\nThe following diagram provides a visual presentation of this process.\n## Map models to keys\nYou can load multiple models and map them to keys by using a [keyed model handler](https://beam.apache.org/documentation/ml/about-ml/#use-a-keyed-modelhandler-object) . Mapping models to keys makes it possible to use different models in the same `RunInference` transform. The following example uses a keyed model handler that loads one model by using and a second model by using . The pipeline uses the model associated with to run inference on examples associated with . The model associated with runs inference on examples associated with and .\n```\nfrom apache_beam.ml.inference.base import KeyedModelHandlerkeyed_model_handler = KeyedModelHandler([\u00a0 KeyModelMapping(['KEY_1'], PytorchModelHandlerTensor(CONFIG_1)),\u00a0 KeyModelMapping(['KEY_2', 'KEY_3'], PytorchModelHandlerTensor(CONFIG_2))])with pipeline as p:\u00a0 \u00a0data = p | beam.Create([\u00a0 \u00a0 \u00a0 ('KEY_1', torch.tensor([[1,2,3],[4,5,6],...])),\u00a0 \u00a0 \u00a0 ('KEY_2', torch.tensor([[1,2,3],[4,5,6],...])),\u00a0 \u00a0 \u00a0 ('KEY_3', torch.tensor([[1,2,3],[4,5,6],...])),\u00a0 \u00a0])\u00a0 \u00a0predictions = data | RunInference(keyed_model_handler)\n```\nFor a more detailed example, see [Run ML inference with multiple differently-trained models](/dataflow/docs/notebooks/per_key_models) .\n### Manage memory\nWhen you load multiple models at the same time, you might encounter out of memory errors (OOMs). When you use a keyed model handler, Apache Beam doesn't automatically limit the number of models loaded into memory. When the models don't all fit into memory, an out of memory error occurs, and the pipeline fails.\nTo avoid this issue, use the `max_models_per_worker_hint` parameter to limit the number of models that are loaded into memory at the same time. The following example uses a keyed model handler with the `max_models_per_worker_hint` parameter. Because the `max_models_per_worker_hint` parameter value is set to `2` , the pipeline loads a maximum of two models on each SDK worker process at the same time.\n```\nmhs = [\u00a0 KeyModelMapping(['KEY_1'], PytorchModelHandlerTensor(CONFIG_1)),\u00a0 KeyModelMapping(['KEY_2', 'KEY_3'], PytorchModelHandlerTensor(CONFIG_2)),\u00a0 KeyModelMapping(['KEY_4'], PytorchModelHandlerTensor(CONFIG_3)),\u00a0 KeyModelMapping(['KEY_5', 'KEY_5', 'KEY_6'], PytorchModelHandlerTensor(CONFIG_4)),]keyed_model_handler = KeyedModelHandler(mhs, max_models_per_worker_hint=2)\n```\nWhen designing your pipeline, make sure the workers have enough memory for both the models and the pipeline transforms. Because the memory used by the models might not be released immediately, to avoid OOMs, include an additional memory buffer.\nIf you have many models and use a low value with the `max_models_per_worker_hint` parameter, you might encounter memory thrashing. Memory thrashing occurs when excessive execution time is used to swap models in and out of memory. To avoid this issue, include a [GroupByKey](https://beam.apache.org/documentation/transforms/python/aggregation/groupbykey/) transform in the pipeline before the inference step. The `GroupByKey` transform ensures that elements with the same key and model are located on the same worker.\n## Learn more\n- Read about [Multi-model pipelines](https://beam.apache.org/documentation/ml/multi-model-pipelines/) in the Apache Beam documentation\n- [Run ML inference with multiple differently-trained models](/dataflow/docs/notebooks/per_key_models) .\n- Run an [interactive notebook in Colab](https://colab.sandbox.google.com/github/apache/beam/blob/master/examples/notebooks/beam-ml/run_inference_multi_model.ipynb) .", "guide": "Dataflow"}