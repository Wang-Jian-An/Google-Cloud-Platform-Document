{"title": "Dataflow - TensorFlow Model Analysis in Beam", "url": "https://cloud.google.com/dataflow/docs/notebooks/tfma_beam?hl=zh-cn", "abstract": "# Dataflow - TensorFlow Model Analysis in Beam\n| 0     | 1      |\n|:--------------------|:----------------------|\n| Run in Google Colab | View source on GitHub |\n[TensorFlow Model Analysis (TFMA)](https://www.tensorflow.org/tfx/guide/tfma) is a library for performing model evaluation across different slices of data. TFMA performs its computations in a distributed manner over large quantities of data by using Apache Beam.\nThis example notebook shows how you can use TFMA to investigate and visualize the performance of a model as part of your Apache Beam pipeline by creating and comparing two models. This example uses [ExtractEvaluateAndWriteResults](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/ExtractEvaluateAndWriteResults) , which is a `PTransform` that performs extraction and evaluation and writes results all in one step.\nTFMA enables scalable and flexible execution of your evaluation pipeline. For additional information about TFMA, see the [TFMA basic notebook](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic) , which provides an in-depth look at TFMA capabilities.\n", "content": "## Install Jupyter extensions\nIf you are running this example in a local Jupyter notebook, before running Jupyter, you must install these Jupyter extensions in the environment.\n```\njupyter nbextension enable --py widgetsnbextension --sys-prefix jupyter nbextension install --py --symlink tensorflow_model_analysis --sys-prefix jupyter nbextension enable --py tensorflow_model_analysis --sys-prefix \n```\n## Install TFMA\nInstalling TFMA pulls in all of the dependencies. The installation takes about a minute.\n```\n# Upgrade pip to the latest version, and then install TFMA.!pip install -U pip!pip install tensorflow-model-analysis# To use the newly installed version of pip, restart the runtime.exit()\n```\n```\n# This configuration was tested in Colab with TensorFlow 2.11, TFMA 0.43, and Apache Beam 2.44.# The setup is also compatible with the current release.import sys# Confirm that you're using Python 3.assert sys.version_info.major==3, 'This notebook must be run using Python 3.'import tensorflow as tfprint('TF version: {}'.format(tf.__version__))import apache_beam as beamprint('Beam version: {}'.format(beam.__version__))import tensorflow_model_analysis as tfmaprint('TFMA version: {}'.format(tfma.__version__))import tensorflow_datasets as tfdsprint('TFDS version: {}'.format(tfds.__version__))\n```\n**Note:** Before proceeding, verify that the output does not have errors. If errors occur, re-run the installation, and restart your kernel.\n## Preprocess data\nThis section includes the steps for preprocessing your data.\n### Create a diamond price prediction model\nThis example uses the [TFDS diamonds dataset](https://www.tensorflow.org/datasets/catalog/diamonds) to train a linear regression model that predicts the price of a diamond. This dataset contains various physical attributes of the diamonds, such as the weight (carat), cut quality, color, and clarity, as well as the price of 53,940 diamonds. The model's performance is evaluated using metrics such as mean squared error and mean absolute error.\nTo simulate a scenario where a model's performance improves over time as new data is added to the dataset, first use half of the diamond dataset to train a model called v1. Then, use additional data to train a second model called v2. These steps demonstrate the use of TFMA when comparing the performance of two models for the same task.\n```\n# Load the data from TFDS and then split the dataset into parts to create train, test, and validation datasets.(ds_train_v1, ds_test, ds_val), info = tfds.load('diamonds', split=['train[:40%]', 'train[80%:90%]', 'train[90%:]'], as_supervised=True, with_info=True)\n```\n```\nimport numpy as np# Load the numerical training data to use for normalization.def extract_numerical_features(item):\u00a0 carat = item['carat']\u00a0 depth = item['depth']\u00a0 table = item['table']\u00a0 x = item['x']\u00a0 y = item['y']\u00a0 z = item['z']\u00a0 return [carat, depth, table, x, y, z]def get_train_data(ds_train):\u00a0 train_data = []\u00a0 for item, label in ds_train:\u00a0 \u00a0 features = extract_numerical_features(item)\u00a0 \u00a0 train_data.append(features)\u00a0 train_data = np.array(train_data)\u00a0 return train_data\n```\n```\ntrain_data_v1 = get_train_data(ds_train_v1)\n```\n```\n# Define the length of the features.NUMERICAL_FEATURES = 6NUM_FEATURES = (NUMERICAL_FEATURES +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 info.features['features']['color'].num_classes +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 info.features['features']['cut'].num_classes +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 info.features['features']['clarity'].num_classes)\n```\n```\n# To transform the input data into a feature vector and label, select the input and output for the model.def transform_data(item, label):\u00a0 numerical_features = extract_numerical_features(item)\u00a0 # Categorical features are encoded using one-hot encoding.\u00a0 color = tf.one_hot(item['color'], info.features['features']['color'].num_classes)\u00a0 cut = tf.one_hot(item['cut'], info.features['features']['cut'].num_classes)\u00a0 clarity = tf.one_hot(item['clarity'], info.features['features']['clarity'].num_classes)\u00a0 # Create the output tensor.\u00a0 output = tf.concat([tf.stack(numerical_features, axis=0), color, cut, clarity], 0)\u00a0 return output, [label]\n```\n```\nds_train_v1 = ds_train_v1.map(transform_data)ds_test = ds_test.map(transform_data)ds_val = ds_val.map(transform_data)\n```\n```\n# To prepare the data for training, structure it in batches.BATCH_SIZE = 32ds_train_v1 = ds_train_v1.batch(BATCH_SIZE)ds_test = ds_test.batch(BATCH_SIZE)\n```\n### Create TFRecords\nTFMA and Apache Beam need to read the dataset used during evaluation from a file. Create a `TFRecords` file that contains the validation dataset.\n```\nmkdir data\n```\n```\n# Write the validation record to a file, which is used by TFMA.tfrecord_file = 'data/val_data.tfrecord'with tf.io.TFRecordWriter(tfrecord_file) as file_writer:\u00a0 for x, y in ds_val:\u00a0 \u00a0 record_bytes = tf.train.Example(features=tf.train.Features(feature={\u00a0 \u00a0 \u00a0 \u00a0 \"inputs\": tf.train.Feature(float_list=tf.train.FloatList(value=x)),\u00a0 \u00a0 \u00a0 \u00a0 \"output\": tf.train.Feature(float_list=tf.train.FloatList(value=[y])),\u00a0 \u00a0 })).SerializeToString()\u00a0 \u00a0 file_writer.write(record_bytes)\n```\n## Define and train one model\nTrain a linear regression model that predicts the price of a diamond. The model is a neural network with one hidden layer. The model also uses a normalization layer to scale all of the numerical features between 0 and 1.\n```\ndef construct_model(model_name, train_data):\u00a0 inputs = tf.keras.Input(shape=(NUM_FEATURES,), name='inputs')\u00a0 # Normalize the numerical features.\u00a0 normalization_layer = tf.keras.layers.Normalization()\u00a0 # Fit the normalization layer to the training data.\u00a0 normalization_layer.adapt(train_data)\u00a0 # Split the input between numerical and categorical input.\u00a0 input_numerical = tf.gather(inputs, indices=[*range(NUMERICAL_FEATURES)], axis=1)\u00a0 input_normalized = normalization_layer(input_numerical)\u00a0 input_one_hot = tf.gather(inputs, indices=[*range(NUMERICAL_FEATURES, NUM_FEATURES)], axis=1)\u00a0 # Define one hidden layer with 8 neurons.\u00a0 x = tf.keras.layers.Dense(8, activation='relu')(tf.concat([input_normalized, input_one_hot], 1))\u00a0 outputs = tf.keras.layers.Dense(1, name='output')(x)\u00a0 model = tf.keras.Model(inputs=inputs, outputs=outputs, name=model_name)\u00a0 model.compile(\u00a0 \u00a0 optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\u00a0 \u00a0 loss='mean_absolute_error')\u00a0 return model\n```\n```\nmodel_v1 = construct_model('model_v1', train_data_v1)\n```\n```\n# Train the model.history = model_v1.fit(\u00a0 \u00a0 ds_train_v1,\u00a0 \u00a0 validation_data=ds_test,\u00a0 \u00a0 epochs=5,\u00a0 \u00a0 verbose=1)\n```\n```\n# Save the model to disk.model_path_v1 = 'saved_model_v1'model_v1.save(model_path_v1)\n```\n## Evaluate the model\nWith the trained model, you can use TFMA to analyze the performance. First, define the evaluation configuration. This example uses the most common metrics used for a linear regression model: mean squared error and mean absolute error. For more information about the supported evaluation parameters, see [TFMA metrics and plots](https://www.tensorflow.org/tfx/model_analysis/metrics) .\n```\nfrom google.protobuf import text_format# Define the TFMA evaluation configuration.eval_config = text_format.Parse(\"\"\"\u00a0 \n## Model information\u00a0 model_specs {\u00a0 \u00a0 # For keras and serving models, you need to add a `label_key`.\u00a0 \u00a0 label_key: \"output\"\u00a0 }\u00a0 \n## This post-training metric information is merged with any built-in\u00a0 \n## metrics from training.\u00a0 metrics_specs {\u00a0 \u00a0 metrics { class_name: \"ExampleCount\" }\u00a0 \u00a0 metrics { class_name: \"MeanAbsoluteError\" }\u00a0 \u00a0 metrics { class_name: \"MeanSquaredError\" }\u00a0 \u00a0 metrics { class_name: \"MeanPrediction\" }\u00a0 }\u00a0 slicing_specs {}\"\"\", tfma.EvalConfig())\n```\nNext, use the [ExtractEvaluateAndWriteResults](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/ExtractEvaluateAndWriteResults) `PTransform` , which performs extraction and evaluation and writes results. To use this `PTransform` directly in your Apache Beam pipeline, use [TFXIO](https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio) to combine it with reading in your `TFRecords` .\n```\nfrom tfx_bsl.public import tfxiooutput_path = 'evaluation_results'eval_shared_model = tfma.default_eval_shared_model(\u00a0 \u00a0 eval_saved_model_path=model_path_v1, eval_config=eval_config)tfx_io = tfxio.TFExampleRecord(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 file_pattern=tfrecord_file,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 raw_record_column_name=tfma.ARROW_INPUT_COLUMN)# Run Evaluation.with beam.Pipeline() as pipeline:\u00a0 \u00a0 _ = (\u00a0 \u00a0 \u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 | 'ReadData' >> tfx_io.BeamSource()\u00a0 \u00a0 \u00a0 \u00a0 | 'EvalModel' >> tfma.ExtractEvaluateAndWriteResults(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0eval_shared_model=eval_shared_model,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0eval_config=eval_config,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0output_path=output_path))\n```\n```\n# Visualize the results.result = tfma.load_eval_result(output_path=output_path)tfma.view.render_slicing_metrics(result)\n```\nThe following image shows an example of a visualisation when evaluating one model:\n## Compare multiple models\nYou can compare the performance of multiple models to select the best candidate to use in production. With Apache Beam, you can evaluate and compare multiple models in one step.\n### Train a second model\nFor this use case, train a second model on the full dataset.\n```\n# Preprocess the data.ds_train_v2 = tfds.load('diamonds', split=['train[:80%]'], as_supervised=True)[0]train_data_v2 = get_train_data(ds_train_v2)ds_train_v2 = ds_train_v2.map(transform_data)ds_train_v2 = ds_train_v2.batch(BATCH_SIZE)\n```\n```\n# Define and train the model.model_v2 = construct_model('model_v2', train_data_v2)history = model_v2.fit(\u00a0 \u00a0 ds_train_v2,\u00a0 \u00a0 validation_data=ds_test,\u00a0 \u00a0 epochs=5,\u00a0 \u00a0 verbose=1)\n```\n```\n# Save the model to a file.model_path_v2 = 'saved_model_v2'model_v2.save(model_path_v2)\n```\n### Evaluate the model\nThe following code demonstrates how to compare the two models and then visualize the results.\n```\n# Define the TFMA evaluation configuration, including two model specs for the two models being compared.eval_config_compare = text_format.Parse(\"\"\"\u00a0 \n## Model information\u00a0 model_specs {\u00a0 \u00a0 name: \"model_v1\"\u00a0 \u00a0 # For keras (and serving models), add a `label_key`.\u00a0 \u00a0 label_key: \"output\"\u00a0 \u00a0 is_baseline: true\u00a0 }\u00a0 model_specs {\u00a0 \u00a0 name: \"model_v2\"\u00a0 \u00a0 # For keras (and serving models), add a `label_key`.\u00a0 \u00a0 label_key: \"output\"\u00a0 }\u00a0 \n## This post-training metric information is merged with any built-in\u00a0 \n## metrics from training.\u00a0 metrics_specs {\u00a0 \u00a0 metrics { class_name: \"ExampleCount\" }\u00a0 \u00a0 metrics { class_name: \"MeanAbsoluteError\" }\u00a0 \u00a0 metrics { class_name: \"MeanSquaredError\" }\u00a0 \u00a0 metrics { class_name: \"MeanPrediction\" }\u00a0 }\u00a0 slicing_specs {}\"\"\", tfma.EvalConfig())\n```\n```\nfrom tfx_bsl.public import tfxiooutput_path_compare = 'evaluation_results_compare'eval_shared_models = [\u00a0 tfma.default_eval_shared_model(\u00a0 \u00a0 \u00a0 model_name='model_v1',\u00a0 \u00a0 \u00a0 eval_saved_model_path=model_path_v1,\u00a0 \u00a0 \u00a0 eval_config=eval_config_compare),\u00a0 tfma.default_eval_shared_model(\u00a0 \u00a0 \u00a0 model_name='model_v2',\u00a0 \u00a0 \u00a0 eval_saved_model_path=model_path_v2,\u00a0 \u00a0 \u00a0 eval_config=eval_config_compare),]tfx_io = tfxio.TFExampleRecord(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 file_pattern=tfrecord_file,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 raw_record_column_name=tfma.ARROW_INPUT_COLUMN)# Run the evaluation.with beam.Pipeline() as pipeline:\u00a0 \u00a0 _ = (\u00a0 \u00a0 \u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 | 'ReadData' >> tfx_io.BeamSource()\u00a0 \u00a0 \u00a0 \u00a0 | 'EvalModel' >> tfma.ExtractEvaluateAndWriteResults(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0eval_shared_model=eval_shared_models,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0eval_config=eval_config_compare,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0output_path=output_path_compare))\n```\nUse the following code to create a visualization of the results. By default, the visualisation displays one time series, which is the evolution of the size of the validation set. To add more interesting visualisations, you can select **add metric series** , and choose to visualise the loss and mean absolute error.\n```\n# Visualize the results.results = tfma.load_eval_results(output_paths=output_path_compare)tfma.view.render_time_series(results)\n```\nThe following image displays an example of a visualisation that evaluates multiple models:", "guide": "Dataflow"}