{"title": "Dataflow - Use data sampling to observe pipeline data", "url": "https://cloud.google.com/dataflow/docs/guides/data-sampling", "abstract": "# Dataflow - Use data sampling to observe pipeline data\nData sampling lets you observe the data at each step of a Dataflow pipeline. This information can help you to debug problems with your pipeline, by showing the actual inputs and outputs in a running or completed job.\nUses for data sampling include the following:\n- During development, see what elements are produced throughout the pipeline.\n- If a pipeline throws an exception, view the elements that are correlated with that exception.\n- When debugging, view the outputs of transforms to ensure the output is correct.\n- Understand the behavior of a pipeline without needing to examine the pipeline code.\n- View the sampled elements at a later time, after the job finishes, or compare the sampled data with a previous run.", "content": "## Overview\nDataflow can sample pipeline data in the following ways:\n- **Periodic sampling** . With this type of sampling, Dataflow collects samples as the job runs. You can use the sampled data to check whether your pipeline processes elements as expected, and to diagnose runtime issues such as hot keys or incorrect output. For more information, see [Use periodic data sampling](#periodic-sampling) in this document.\n- **Exception sampling** . With this type of sampling, Dataflow collects samples if a pipeline throws an exception. You can use the samples to see the data that was being processed when the exception occurred. Exception sampling is enabled by default and can be disabled. For more information, see [Use exception sampling](#exception-sampling) in this document.\nDataflow writes the sampled elements to the Cloud Storage path specified by the [temp_location](/dataflow/docs/guides/setting-pipeline-options) pipeline option. You can view the sampled data in the Google Cloud console, or examine the raw data files in Cloud Storage. The files persist in Cloud Storage until you delete them.\nData sampling is executed by the Dataflow workers. Sampling is best-effort. Samples might be dropped if transient errors occur.\n## Requirements\nTo use data sampling, you must enable Runner v2. For more information, see [Enable Dataflow Runner v2](/dataflow/docs/runner-v2#enable) .\nTo view the sampled data in the Google Cloud console, you need the following [Identity and Access Management permissions](/iam/docs/permissions-reference) :\n- `storage.buckets.get`\n- `storage.objects.get`\n- `storage.objects.list`\nPeriodic sampling requires the following Apache Beam SDK:\n- Apache Beam Java SDK 2.47.0 or later\n- Apache Beam Python SDK 2.46.0 or later\n- Apache Beam Go SDK 2.53.0 or later\nException sampling requires the following Apache Beam SDK:\n- Apache Beam Java SDK 2.51.0 or later\n- Apache Beam Python SDK 2.51.0 or later\n- The Apache Beam Go SDK does not support exception sampling.\nStarting with these SDKs, Dataflow enables exception sampling for all jobs by default.\n## Use periodic data sampling\nThis section describes how to sample pipeline data continuously as a job runs.\n### Enable periodic data sampling\nPeriodic sampling is disabled by default. To enable it, set the following pipeline option:\n```\n--experiments=enable_data_sampling\n```\n```\n--experiments=enable_data_sampling\n```\n```\n--experiments=enable_data_sampling\n```\nYou can set the option programmatically or by using the command line. For more information, see [Set experimental pipeline options](/dataflow/docs/guides/setting-pipeline-options#experimental) .\nWhen running a Dataflow template, use the `additional-experiments` flag to enable data sampling:\n```\n--additional-experiments=enable_data_sampling\n```\nWhen periodic sampling is enabled, Dataflow collects samples from each `PCollection` in the job graph. The sampling rate is approximately one sample every 30 seconds.\nDepending on the volume of data, periodic data sampling can add significant performance overhead. Therefore, we recommend that you only enable periodic sampling during testing, and disable it for production workloads.\n### View sampled data\nTo view the sampled data in the Google Cloud console, perform the following steps:\n- In the Google Cloud console, go to the Dataflow **Jobs** page. [Go to Jobs](https://console.cloud.google.com/dataflow/jobs) \n- Select a job.\n- Click on the bottom panel to expand the logs panel.\n- Click the **Data Sampling** tab.\n- In the **Step** field, select a pipeline step. You can also select a step in the job graph.\n- In the **Collection** field, choose a `PCollection` .\nIf Dataflow has collected samples for that `PCollection` , the sampled data appears in the tab. For each sample, the tab displays the creation date and the output element. The output element is a serialized representation of the collection element, including the element data, timestamp, and window and pane information.\nThe following examples show sampled elements.\n```\nTimestampedValueInGlobalWindow{value=KV{way, [21]},\ntimestamp=294247-01-09T04:00:54.775Z, pane=PaneInfo{isFirst=true, isLast=true,\ntiming=ON_TIME, index=0, onTimeIndex=0}}\n```\n```\n(('THE', 1), MIN_TIMESTAMP, (GloblWindow,), PaneInfo(first: True, last: True,\ntiming: UNKNOWN, index: 0, nonspeculative_index: 0))\n```\n```\nKV<THE,1> [@1708122738999:[[*]]:{3 true true 0 0}]\n```\nThe following image shows how the sampled data appears in the Google Cloud console.\n## Use exception sampling\nIf your pipeline throws an unhandled exception, you can view both the exception and the input element that is correlated with that exception. Exception sampling is enabled by default when you use a supported [Apache Beam SDK](#requirements) .\n### View exceptions\nTo view an exception, perform the following steps:\n- In the Google Cloud console, go to the Dataflow **Jobs** page. [Go to Jobs](https://console.cloud.google.com/dataflow/jobs) \n- Select a job.\n- To expand the **Logs** panel, click **Toggle panel** on the **Logs** panel.\n- Click the **Data Sampling** tab.\n- In the **Step** field, select a pipeline step. You can also select a step in the job graph.\n- In the **Collection** field, choose a `PCollection` .The **Exception** column contains the exception details. There is no output element for an exception. Instead, the **Output element** column contains the message `Failed to process input element:` `` , where is the correlated input element.\n- To view the input sample and the exception details in a new window, click open_in_new **Open in new** .\nThe following image shows how an exception appears in the Google Cloud console.### Disable exception sampling\nTo disable exception sampling, set the following pipeline option:\n```\n--experiments=disable_always_on_exception_sampling\n```\n```\n--experiments=disable_always_on_exception_sampling\n```\nYou can set the option programmatically or by using the command line. For more information, see [Set experimental pipeline options](/dataflow/docs/guides/setting-pipeline-options#experimental) .\nWhen running a Dataflow template, use the `additional-experiments` flag to disable exception sampling:\n```\n--additional-experiments=disable_always_on_exception_sampling\n```\n## Security considerations\nDataflow writes the sampled data to a Cloud Storage bucket that you create and manage. Use the security features of Cloud Storage to safeguard the security of your data. In particular, consider the following additional security measures:\n- Use a [customer-managed encryption key](/storage/docs/encryption/customer-managed-keys) (CMEK) to encrypt the Cloud Storage bucket. For more information about choosing an encryption option, see [Choose the right encryption for your needs](/kms/docs/key-management-service#choose) .\n- Set a time to live (TTL) on the Cloud Storage bucket, so that data files are automatically deleted after a period of time. For more information, see [Set the lifecycle configuration for a bucket](/storage/docs/managing-lifecycles#set) .\n- Use the principle of least privilege when assigning IAM permissions to the Cloud Storage bucket.\nYou can also obfuscate individual fields in your `PCollection` data type, so that the raw value does not appear in the sampled data:\n- Python: Override the`__repr__`or`__str__`method.\n- Java: Override the`toString`method.\nHowever, you can't obfuscate the inputs and outputs from I/O connectors, unless you modify the connector source code to do so.\n## Billing\nWhen Dataflow performs data sampling, you are charged for the Cloud Storage data storage and for the read and write operations on Cloud Storage. For more information, see [Cloud Storage pricing](/storage/pricing) .\nEach Dataflow worker writes samples in batches, incurring one read operation and one write operation per batch.\n## Troubleshooting\nThis section contains information about common issues when using data sampling.\n### Permissions error\nIf you don't have permission to view the samples, the Google Cloud console shows the following error:\n```\nYou don't have permission to view a data sample.\n```\nTo resolve this error, check that you have the [required IAM permissions](#requirements) . If the error still occurs, you might be subject to an [IAM deny policy](/iam/docs/deny-overview) .\n### I don't see any samples\nIf you don't see any samples, check the following:\n- Ensure that data sampling is enabled by setting the`enable_data_sampling`option. See [Enable data sampling](#enable-periodic-sampling) .\n- Ensure that you are using [Runner v2](/dataflow/docs/runner-v2) \n- Make sure the workers have started. Sampling does not start until the workers start.\n- Make sure the job and workers are in a healthy state.\n- Double check the project's [Cloud Storage quotas](/storage/quotas) . If you exceed your Cloud Storage quota limits, then Dataflow cannot write the sample data.## What's next\n- [Develop and test Dataflow pipelines](/dataflow/docs/guides/develop-and-test-pipelines) \n- [Use the Dataflow monitoring interface](/dataflow/docs/guides/monitoring-overview)", "guide": "Dataflow"}