{"title": "Dataflow - Use customer-managed encryption keys", "url": "https://cloud.google.com/dataflow/docs/guides/customer-managed-encryption-keys", "abstract": "# Dataflow - Use customer-managed encryption keys\nThis page describes how to use a Cloud Key Management Service (Cloud KMS) encryption key with Dataflow. A customer-managed encryption key (CMEK) enables encryption of data at rest with a key that you can control through Cloud KMS. You can create a batch or streaming pipeline that is protected with a CMEK or access CMEK-protected data in sources and sinks.\n[](None)\nYou can also use [Cloud EKM](/kms/docs/ekm) or [Cloud HSM](/kms/docs/hsm) keys. When you use CMEK in Dataflow, your projects can consume Cloud KMS cryptographic requests quotas. For example, Dataflow pipelines can consume these quotas when your pipeline accesses CMEK-protected data in sources and sinks or when the state of a CMEK-encrypted pipeline is retrieved. For more information, see the [Encryption of pipeline state locations](#encryption_of_pipeline_state_locations) section in this page. Encryption and decryption operations using CMEK keys affect Cloud KMS quotas only if you use hardware (Cloud HSM) or external (Cloud EKM) keys. For more information, see [Cloud KMS quotas](/kms/quotas) .\nWith Cloud External Key Manager (Cloud EKM), you can use keys that you manage within a [supported external key management partner](/kms/docs/ekm#supported_partners) to protect data within Google Cloud. You can protect data at rest in [supported CMEK integration services](/kms/docs/ekm#supported_services) or by calling the Dataflow API directly.\nFor more information, see [encryption options on Google Cloud](/security/encryption/default-encryption) .\n", "content": "## Support and limitations\n- Cloud KMS is supported in the following Apache Beam SDK versions:- Java SDK versions 2.13.0 and later\n- Python SDK versions 2.13.0 and later\n- Go SDK versions 2.40.0 and later\n- Cloud KMS with Dataflow supports [regional keys](/kms/docs/locations#location_types) . If you [override the worker region or zone of the pipeline](/dataflow/docs/concepts/regional-endpoints#workeroverride) to use a region other than the one associated with your keys, regional keys don't work.\n- The region for your CMEK and the [region](/dataflow/docs/resources/locations) for your Dataflow job must be the same.\n- Multi-region and global locations are not supported. You can't use global and multi-regional keys with Dataflow pipelines.## Encryption of pipeline state artifacts\nData that a Dataflow pipeline reads from user-specified data sources is encrypted, except for the data keys that you specify for key-based transforms in streaming jobs.\nFor batch jobs, all data, including data keys that you specify for key-based transforms, is always protected by CMEK encryption.\nFor streaming jobs created after March 7, 2024, all user data is encrypted with CMEK.\nFor streaming jobs created before March 7, 2024, data keys used in key-based operations, such as windowing, grouping, and joining, are not protected by CMEK encryption. To enable this encryption for your jobs, [drain or cancel the job](/dataflow/docs/guides/stopping-a-pipeline) , and then restart it.\nJob metadata is not encrypted with Cloud KMS keys. Job metadata includes the following:\n- User-supplied data, such as Job Names, Job Parameter values, and Pipeline Graph\n- System-generated data, such as Job IDs and IP addresses of workers## Encryption of pipeline state locations\nThe following storage locations are protected with Cloud KMS keys:\n- Persistent Disks attached to Dataflow workers and used for Persistent Disk-based shuffle and streaming state storage.\n- Dataflow [Shuffle](/dataflow/docs/shuffle-for-batch) state for batch pipelines.\n- Cloud Storage buckets that store temporary export or import data. Dataflow only supports default keys set by the user on the bucket level.\n- Cloud Storage buckets used to store binary files containing pipeline code. Dataflow only supports default keys set by the user on the bucket level.\n- Cloud Storage buckets used to store sampled pipeline data, when [data sampling](/dataflow/docs/guides/data-sampling) is enabled.\n- Dataflow Streaming Engine state for streaming pipelines.## External keys\nYou can use [Cloud External Key Manager (Cloud EKM)](/kms/docs/ekm) to encrypt data within Google Cloud using external keys that you manage.\nWhen you use a Cloud EKM key, Google has no control over the availability of your externally managed key. If the key becomes unavailable during the job or pipeline creation period, your job or pipeline is canceled.\nFor more considerations when using external keys, see [Cloud External Key Manager](/kms/docs/ekm#considerations) .\n## Before you begin\n- Verify that you have the Apache Beam SDK for Java 2.13.0 or later, the Apache Beam SDK for Python 2.13.0 or later, or the Apache Beam SDK for Go 2.40.0 or later.For more information, see [Installing the Apache Beam SDK](/dataflow/docs/guides/installing-beam-sdk) .\n- Decide whether you're going to run Dataflow and Cloud KMS in the same Google Cloud project or in different projects. This page uses the following convention:- ``is the project ID of the project that is running Dataflow.\n- ``is the project number of the project that is running Dataflow.\n- ``is the project ID of the project that is running Cloud KMS.\nFor information about Google Cloud project IDs and project numbers, see [Identifying projects](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- On the Google Cloud project that you want to run Cloud KMS:- [Enable the Cloud KMS API](https://console.cloud.google.com/flows/enableapi?apiid=cloudkms.googleapis.com&redirect=https://console.cloud.google.com) .\n- Create a key ring and a key as described in [Creating symmetric keys](/kms/docs/creating-keys) . Cloud KMS and Dataflow are both regionalized services. The region for your CMEK and the [region](/dataflow/docs/resources/locations) of your Dataflow job must be the same. Don't use global or multi-regional keys with your Dataflow pipelines. Instead, use regional keys.\n## Grant Encrypter/Decrypter permissions\n**Note:** If you use the Google Cloud console and the **Create job from template** page, a prompt appears to grant the encrypt and decrypt permissions to your Compute Engine service account and Dataflow service account. If you follow that prompt to grant the permissions, you can skip this section, which describes how to manually set these permissions.\n- Assign the `Cloud KMS CryptoKey Encrypter/Decrypter` [role](/kms/docs/reference/permissions-and-roles#predefined_roles) to the **Dataflow service account** . This permission grants your Dataflow service account the permission to encrypt and decrypt with the CMEK you specify. If you use the Google Cloud console and the **Create job from template** page, this permission is granted automatically and you can skip this step.Use the Google Cloud CLI to assign the role:```\ngcloud projects add-iam-policy-binding KMS_PROJECT_ID \\--member serviceAccount:service-PROJECT_NUMBER@dataflow-service-producer-prod.iam.gserviceaccount.com \\--role roles/cloudkms.cryptoKeyEncrypterDecrypter\n```Replace `` with the ID of your Google Cloud project that is running Cloud KMS, and replace `` with the project number (not project ID) of your Google Cloud project that is running the Dataflow resources.\n- Assign the `Cloud KMS CryptoKey Encrypter/Decrypter` [role](/kms/docs/reference/permissions-and-roles#predefined_roles) to the **Compute Engine service account** . This permission grants your Compute Engine service account the permission to encrypt and decrypt with the CMEK you specify.Use the Google Cloud CLI to assign the role:```\ngcloud projects add-iam-policy-binding KMS_PROJECT_ID \\--member serviceAccount:service-PROJECT_NUMBER@compute-system.iam.gserviceaccount.com \\--role roles/cloudkms.cryptoKeyEncrypterDecrypter\n```Replace `` with the ID of your Google Cloud project that is running Cloud KMS, and replace `` with the project number (not project ID) of your Google Cloud project that is running the Compute Engine resources.## Create a pipeline protected by Cloud KMS\nWhen you create a batch or streaming pipeline, you can select a Cloud KMS key to encrypt the pipeline state. The pipeline state is the data that is stored by Dataflow in temporary storage.\n### Command-line interface\nTo create a new pipeline with pipeline state that is protected by a Cloud KMS key, add the relevant flag to the pipeline parameters. The following example demonstrates [running a word count pipeline](/dataflow/docs/quickstarts/create-pipeline-python#run-wordcount-on-the-cloud-dataflow-service) with Cloud KMS.\nDataflow does not support creating default Cloud Storage paths for temporary files when using a Cloud KMS key. Specifying `gcpTempLocation` is required.\n```\nmvn compile exec:java -Dexec.mainClass=org.apache.beam.examples.WordCount \\\u00a0 -Dexec.args=\"--inputFile=gs://dataflow-samples/shakespeare/kinglear.txt \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0--output=gs://STORAGE_BUCKET/counts \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0--runner=DataflowRunner --project=PROJECT_ID \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0--gcpTempLocation=gs://STORAGE_BUCKET/tmp \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0--dataflowKmsKey=KMS_KEY\"\u00a0 -Pdataflow-runner\n```\nDataflow does not support creating default Cloud Storage paths for temporary files when using a Cloud KMS key. Specifying `gcpTempLocation` is required.\n```\npython -m apache_beam.examples.wordcount \\\u00a0 --input gs://dataflow-samples/shakespeare/kinglear.txt \\\u00a0 --output gs://STORAGE_BUCKET/counts \\\u00a0 --runner DataflowRunner \\\u00a0 --region HOST_GCP_REGION \\\u00a0 --project PROJECT_ID \\\u00a0 --temp_location gs://STORAGE_BUCKET/tmp/ \\\u00a0 --dataflow_kms_key=KMS_KEY\n```\nDataflow does not support creating default Cloud Storage paths for temporary files when using a Cloud KMS key. Specifying `gcpTempLocation` is required.\n```\nwordcount\u00a0 --project HOST_PROJECT_ID \\\u00a0 --region HOST_GCP_REGION \\\u00a0 --runner dataflow \\\u00a0 --staging_location gs://STORAGE_BUCKET/staging \\\u00a0 --temp_location gs://STORAGE_BUCKET/temp \\\u00a0 --input gs://dataflow-samples/shakespeare/kinglear.txt \\\u00a0 --output gs://STORAGE_BUCKET/output \\\u00a0 --dataflow_kms_key=KMS_KEY\n```\n### Google Cloud console\n- Open the Dataflow monitoring interface. [Go to the Dataflow Web Interface](https://console.cloud.google.com/dataflow) \n- Select **Create job from template** .\n- In the **Encryption** section, select **Customer-managed key** .**Note:** The drop-down menu **Select a customer-managed key** only shows keys with the regional scope global or the region you selected in the **Regional endpoint** drop-down menu. In order to minimize Cloud KMS operation latency and improve system availability, we recommend choosing regional keys.\nThe first time you attempt to run a job with a particular Cloud KMS key, your **Compute Engine service account** or **Dataflow service account** might not have been granted the permissions to encrypt and decrypt using that key. In this case, a warning message appears to prompt you to grant the permission to your service account.\n## Verify Cloud KMS key usage\nYou can verify whether your pipeline uses a Cloud KMS key using the Google Cloud console or the Google Cloud CLI.\n- Open the Dataflow monitoring interface. [Go to the Dataflow Web Interface](https://console.cloud.google.com/dataflow) \n- To view job details, select your Dataflow job.\n- In the **Job info** side panel, to see the key type, check the **Encryption type** field.- For Encryption type: \"Google-Managed key\"\n- For Encryption type: \"Customer-Managed key\"\nRun the [describe](/sdk/gcloud/reference/projects/describe) command using the gcloud CLI:\n`gcloud dataflow jobs describe` ``\nSearch for the line that contains `serviceKmsKeyName` . This information shows that a Cloud KMS key was used for Dataflow pipeline state encryption.\nYou can verify Cloud KMS key usage for encrypting sources and sinks by using the Google Cloud console pages and tools of those sources and sinks, including Pub/Sub, Cloud Storage, and BigQuery. You can also verify Cloud KMS key usage through viewing your [Cloud KMS audit logs](/kms/docs/audit-logging#viewing_logs) .\n## Disable or destroy the key\nIf for any reason you may need to disable or destroy the key, you can use the Google Cloud console. Both disable and destroy operations cancel the jobs using that key. This operation is permanent.\nIf you're using Cloud EKM, disable or destroy the key in your external key manager.\nIf you're using the Streaming Engine option, taking a [snapshot](https://cloud.google.com/dataflow/docs/guides/using-snapshots) of the job before disabling the key is recommended.\n## Remove Dataflow access to the Cloud KMS key\nYou can remove Dataflow access to the Cloud KMS key by using the following steps:\n- Revoke`Cloud KMS CryptoKey Encrypter/Decrypter` [role](/kms/docs/reference/permissions-and-roles#predefined_roles) to the **Dataflow service account** using the [Google Cloud console](/iam/docs/granting-changing-revoking-access#revoke_access) or the [gcloud CLI](/iam/docs/granting-changing-revoking-access#updating-gcloud) .\n- Revoke`Cloud KMS CryptoKey Encrypter/Decrypter` [role](/kms/docs/reference/permissions-and-roles#predefined_roles) to the **Compute Engine service account** using the [Google Cloud console](/iam/docs/granting-changing-revoking-access#revoke_access) or the [gcloud CLI](/iam/docs/granting-changing-revoking-access#updating-gcloud) .\n- Optionally, you can also [destroy the key version material](/kms/docs/destroy-restore) to further prevent Dataflow and other services from accessing the pipeline state.\nAlthough you can destroy the , you [cannot delete keys and key rings](/kms/docs/object-hierarchy#lifetime) . Key rings and keys don't have billable costs or quota limitations, so their continued existence doesn't affect costs or production limits.\nDataflow jobs periodically validate whether the **Dataflow service account** can successfully use the given Cloud KMS key. If an encrypt or decrypt request fails, the Dataflow service halts all data ingestion and processing as soon as possible. Dataflow immediately begins cleaning up the Google Cloud resources attached to your job.\n## Use sources and sinks that are protected with Cloud KMS keys\nDataflow can access Google Cloud sources and sinks that are protected by Cloud KMS keys. If you're not creating new objects, you don't need to specify the Cloud KMS key of those sources and sinks. If your Dataflow pipeline might create new objects in a sink, you must define pipeline parameters. These parameters specify the Cloud KMS keys for that sink and pass this Cloud KMS key to appropriate I/O connector methods.\nFor Dataflow pipeline sources and sinks that don't support CMEK managed by Cloud KMS, the Dataflow CMEK settings are irrelevant.\n### Cloud KMS key permissions\nWhen accessing services that are protected with Cloud KMS keys, verify that you have assigned the `Cloud KMS CryptoKey Encrypter/Decrypter` [role](/kms/docs/reference/permissions-and-roles#predefined_roles) to that service. The accounts are of the following form:\n- [Cloud Storage](/storage/docs/encryption/using-customer-managed-keys#prereqs) :`service-{project_number}@gs-project-accounts.iam.gserviceaccount.com`\n- [BigQuery](/bigquery/docs/customer-managed-encryption#grant_permission) :`bq-{project_number}@bigquery-encryption.iam.gserviceaccount.com`\n- [Pub/Sub](/pubsub/docs/cmek) :`service-{project_number}@gcp-sa-pubsub.iam.gserviceaccount.com`\n### Cloud Storage\nIf you want to protect the temporary and staging buckets that you specified with the `TempLocation` / `temp_location` and `stagingLocation` / `staging_location` pipeline parameters, see [setting up CMEK-protected Cloud Storage buckets](/storage/docs/encryption/using-customer-managed-keys) .\n### BigQuery\nUse the `with_kms_key()` method on return values from `BigQueryIO.readTableRows()` , `BigQueryIO.read()` , `BigQueryIO.writeTableRows()` , and `BigQueryIO.write()` .\nYou can find an example in the [Apache Beam GitHub repository](https://github.com/apache/beam/blob/49403c00499f7f87c2bf9a4c63dec8ebd68d640d/sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryKmsKeyIT.java#L91-L100) .\nUse the `kms_key` argument in [BigQuerySource](https://beam.apache.org/releases/pydoc/current/apache_beam.io.gcp.bigquery.html?highlight=bigquery#apache_beam.io.gcp.bigquery.BigQuerySource) and [BigQuerySink](https://beam.apache.org/releases/pydoc/current/_modules/apache_beam/io/gcp/bigquery.html#BigQuerySink) .\nYou can find an example in the [Apache Beam GitHub repository](https://github.com/apache/beam/blob/006b7419de5c707912da315f3f66450a72c95a36/sdks/python/apache_beam/io/gcp/big_query_query_to_table_pipeline.py#L64-L73) .\nBigQuery IOs don't support using the kms key in Go.\n### Pub/Sub\nDataflow handles access to CMEK-protected topics by using your topic CMEK configuration.\nTo read from and write to CMEK-protected Pub/Sub topics, see [Pub/Sub instructions for using CMEK](/pubsub/docs/cmek) .\n## Audit logging for Cloud KMS key usage\nDataflow enables Cloud KMS to use Cloud Audit Logs for logging [key operations](/kms/docs/audit-logging#audited_operations) , such as encrypt and decrypt. Dataflow provides the job ID as context to a Cloud KMS caller. This ID lets you track each instance a specific Cloud KMS key is used for a Dataflow job.\nCloud Audit Logs maintains [audit logs](/logging/docs/audit) for each Google Cloud project, folder, and organization. You have several options for [viewing yourCloud KMS audit logs](/kms/docs/audit-logging#viewing_logs) .\nCloud KMS writes **Admin Activity** audit logs for your Dataflow jobs with CMEK encryption. These logs record operations that modify the configuration or metadata of a resource. You can't disable Admin Activity audit logs.\nIf explicitly enabled, Cloud KMS writes **Data Access** audit logs for your Dataflow jobs with CMEK encryption. Data Access audit logs contain API calls that read the configuration or metadata of resources. These logs also contain user-driven API calls that create, modify, or read user-provided resource data. For instructions on enabling some or all of your Data Access audit logs, go to [Configuring data access Logs](/logging/docs/audit/configure-data-access) .\n## Pricing\nYou can use Cloud KMS encryption keys with Dataflow in all [Dataflow regions](/dataflow/docs/resources/locations) where Cloud KMS is available.\nThis integration does not incur additional costs beyond the key operations, which are billed to your Google Cloud project. Each time the Dataflow service account uses your Cloud KMS key, the operation is billed at the rate of Cloud KMS key operations.\nFor more information, see [Cloud KMS pricing details](https://cloud.google.com/kms/pricing) .\n## Troubleshooting\nUse the suggestions in this section to troubleshoot errors.\n### Cloud KMS cannot be validated\nYour workflow might fail with the following error:\n```\nWorkflow failed. Causes: Cloud KMS key <key-name> cannot be validated.\n```\nTo fix this issue, verify that you have passed the full key path. It looks like `projects/<project-id>/locations/<gcp-region>/keyRings/<key-ring-name>/cryptoKeys/<key-name>` . Look for possible typos in the key path.\n### Cloud KMS key permission denied\nYour workflow might fail with the following error:\n```\nWorkflow failed. Causes: Cloud KMS key Permission 'cloudkms.cryptoKeyVersions.useToEncrypt' denied on resource\n'projects/<project-id>/locations/<gcp-region>/keyRings/<key-ring-name>/cryptoKeys/<key-name>' (or it may not exist). cannot be validated.\n```\nTo fix this issue, verify that the [project ID](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects) mentioned in the key path is correct. Also, check that you have the [permission](https://cloud.google.com/kms/docs/reference/permissions-and-roles#resource_hierarchy) to use the key.\n### Cloud KMS key location doesn't match Dataflow job location\nYour workflow might fail with the following error:\n```\nWorkflow failed. Causes: Cloud KMS key projects/<project-id>/locations/<gcp-region>/keyRings/<key-ring-name>/cryptoKeys/<key-name>\ncan't protect resources for this job. Make sure the region of the KMS key matches the Dataflow region.\n```\nTo fix this issue, if you're using a regional key, verify that the Cloud KMS key is in the same region as the Dataflow job.", "guide": "Dataflow"}