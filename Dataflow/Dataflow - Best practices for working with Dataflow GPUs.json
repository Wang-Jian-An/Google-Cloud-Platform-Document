{"title": "Dataflow - Best practices for working with Dataflow GPUs", "url": "https://cloud.google.com/dataflow/docs/gpu/develop-with-gpus", "abstract": "# Dataflow - Best practices for working with Dataflow GPUs\nThis page describes best practices for building pipelines using GPUs.\nFor information and examples about how to enable GPUs in your Dataflow jobs, see [Run a pipeline with GPUs](/dataflow/docs/gpu/use-gpus) and [Processing Landsat satellite images with GPUs](/dataflow/docs/samples/satellite-images-gpus) .\n", "content": "## Prerequisites for using GPUs in Dataflow- To use GPUs with your Dataflow job, you must use Runner v2.\n- Dataflow runs user code in worker VMs inside a Docker container.  These worker VMs run [Container-Optimized OS](/container-optimized-os/docs) .  For Dataflow jobs to use GPUs, you need the following prerequisites:- GPU drivers are installed on worker VMs and accessible to the Docker  container. For more information, see [Install GPU drivers](/dataflow/docs/gpu/use-gpus#drivers) .\n- GPU libraries required by your pipeline, such as [NVIDIA CUDA-X libraries](https://developer.nvidia.com/gpu-accelerated-libraries) or the [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) ,  are installed in the custom container image. For more information, see [Configure your container image](/dataflow/docs/gpu/use-gpus#container-image) .- Because GPU containers are typically large, to avoid [running out of disk space](/dataflow/docs/guides/common-errors#no-space-left) , increase the default [boot disk size](/dataflow/docs/reference/pipeline-options#worker-level_options) to 50 gigabytes or more.\n## Considerations\nWhen designing both your test and production environments, consider the following factors.\n### Local development\nUsing Apache Beam with NVIDIA GPUs lets you create large-scale data processing pipelines that handle preprocessing and inference. When you're using GPUs for local development, consider the following information:\n- Oftentimes, the data processing workflows use additional libraries that you need to install in the launch environment and in the execution environment on Dataflow workers. This configuration adds steps to the development workflow for [configuring pipeline requirements](https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/) or for [using custom containers in Dataflow](/dataflow/docs/guides/using-custom-containers) . You might want a local development environment that mimics the production environment as closely as possible.\n- If you're using a library that implicitly uses NVIDIA GPUs and your code doesn't require any changes to support GPUs, you don't need to change your development workflow to configure pipeline requirements or to build custom containers.\n- Some libraries don't switch transparently between the CPU and GPU usage, and hence require specific builds and different code paths. To replicate the code-run-code development lifecycle for this scenario, additional steps are required.\n- When running local experiments, replicate the environment of the Dataflow worker as closely as possible. Depending on the library, you might need a machine with a GPU and the required GPU libraries installed. This type of machine might not be available in your local environment. You can emulate the Dataflow runner environment using a container running on a GPU-equipped Google Cloud virtual machine.\n- It's unlikely to have a pipeline composed entirely of transformations that require a GPU. A typical pipeline has an ingestion stage that uses one of the many sources provided by Apache Beam. That stage is followed by data manipulation or shaping transforms, which then feed into a GPU transform.\n### Machine types specifications\nFor details about machine type support for each GPU model, see [GPU platforms](/compute/docs/gpus) . GPUs that are supported with N1 machine types also support the [custom N1 machine types](/compute/docs/instances/creating-instance-with-custom-machine-type#n1_custom_machine_types) .\nThe type and number of GPUs define the upper bound restrictions on the available amounts of vCPU and memory that workers can have. To find the corresponding restrictions, see [Availability](/dataflow/docs/gpu/gpu-support#availability) .\nSpecifying a higher number of CPUs or memory might require that you specify a higher number of GPUs.\nFor more details, read [GPUs onCompute Engine](/compute/docs/gpus#restrictions) .\n### GPUs and worker parallelism\nFor Python pipelines using the [Dataflow Runner v2](/dataflow/docs/runner-v2) architecture, Dataflow launches one Apache Beam SDK process per VM core. Each SDK process runs in its own Docker container and in turn spawns many threads, each of which processes incoming data.\nGPUs use multiple process architecture, and GPUs in Dataflow workers are visible to all processes and threads.\nIf you're running multiple SDK processes on a shared GPU, you can improve GPU efficiency and utilization by enabling the NVIDIA Multi-Process Service (MPS). MPS improves worker parallelism and overall throughput for GPU pipelines, especially for workloads with low GPU resource usage. For more information, see [Improve performance on a shared GPU by using NVIDIA MPS](/dataflow/docs/gpu/use-nvidia-mps) .\nTo avoid GPU memory oversubscription, you might need to manage GPU access. If you're using TensorFlow, either of the following suggestions can help you avoid GPU memory oversubscription:\n- Configure the Dataflow workers to start only one containerized Python process, regardless of the worker vCPU count. To make this configuration, when launching your job, use the following [pipeline options](/dataflow/docs/reference/pipeline-options) :- `--experiments=no_use_multiple_sdk_containers`\n- `--number_of_worker_harness_threads`\nFor more information about how many threads to use, see [Reduce the number of threads](/dataflow/docs/guides/troubleshoot-oom#reduce-threads) .\n- [Enable MPS](/dataflow/docs/gpu/use-nvidia-mps#enable) .## Workflow\nThe following two-stage workflow shows how to build a pipeline using GPUs. This flow takes care of GPU and non-GPU related issues separately and shortens the feedback loop.\n- Create a pipelineCreate a pipeline that can run on Dataflow. Replace the transforms that require GPUs with the transforms that don't use GPUs, but are functionally the same:- Create all transformations that surround the GPU usage, such as data ingestion and manipulation.\n- Create a stub for the GPU transform with a simple pass-through or schema change.\n- Test locallyTest the GPU portion of the pipeline code in the environment that mimics the Dataflow worker execution environment. The following steps describe one of the methods to run this test:- Create a [Docker](https://www.docker.com/) image with all necessary libraries.\n- Start development of the GPU code.\n- Begin the code-run-code cycle using a Google Cloud virtual machine with the Docker image. To rule out library incompatibilities, run the GPU code in a local Python process separately from an Apache Beam pipeline. Then, run the entire pipeline on the direct runner, or launch the pipeline on Dataflow.\n## Use a VM running container-optimized operating system\nFor a minimum environment, use a container-optimized virtual machine (VM). For more information, see [Create a VM with attached GPUs](/compute/docs/gpus/create-vm-with-gpus) .\nThe general flow is:\n- Create a VM.\n- Connect to the VM and run the following commands:```\nsudo cos-extensions install gpu -- -version latestsudo mount --bind /var/lib/nvidia /var/lib/nvidiasudo mount -o remount,exec /var/lib/nvidia\n```\n- Confirm that GPUs are available:```\n./nvidia-smi\n```\n- Start a Docker container with GPU drivers from the VM mounted as volumes. For example:```\nsudo docker run --rm -it --entrypoint /bin/bash--volume /var/lib/nvidia/lib64:/usr/local/nvidia/lib64--volume /var/lib/nvidia/bin:/usr/local/nvidia/bin--privileged gcr.io/bigdatapivot/image_process_example:latest\n```\nFor a sample Dockerfile, see [Build a custom container image](/dataflow/docs/gpu/use-gpus#custom-container) . Add all the dependencies that you need for your pipeline to the Dockerfile.\nFor more information about using a Docker image that is pre-configured for GPU usage, see [Use an existing image configured for GPU usage](/dataflow/docs/gpu/use-gpus#existing-image) .\n### Tools for working with container-optimized systems\n- To configure Docker CLI to use `docker-credential-gcr` as a credential helper for the default set of Google Container Registries (GCR), use:```\nsudo docker-credential-gcr configure-docker\n```For more information about setting up Docker credentials, see [docker-credential-gcr](https://github.com/GoogleCloudPlatform/docker-credential-gcr) .\n- To copy files, such as pipeline code, to or from a VM, use `toolbox` . This technique is useful when using a Custom-Optimized image. For example:```\ntoolbox /google-cloud-sdk/bin/gsutil cp gs://bucket/gpu/image_process/* /media/root/home/<userid>/opencv/\n```For more information, see [Debugging node issues using toolbox](/container-optimized-os/docs/how-to/toolbox) .## What's next\n- Learn more about [Dataflow support for GPUs](/dataflow/docs/gpu/gpu-support) .\n- Learn more about how to [run a pipeline by using GPUs](/dataflow/docs/gpu/use-gpus) .\n- Work through [Processing Landsat satellite images withGPUs](/dataflow/docs/samples/satellite-images-gpus) .", "guide": "Dataflow"}