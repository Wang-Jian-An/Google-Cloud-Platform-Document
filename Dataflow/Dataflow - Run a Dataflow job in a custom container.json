{"title": "Dataflow - Run a Dataflow job in a custom container", "url": "https://cloud.google.com/dataflow/docs/guides/run-custom-container", "abstract": "# Dataflow - Run a Dataflow job in a custom container\nThis document describes how to run a Dataflow pipeline using a custom container.\nFor information about creating the container image, see [Build custom container images for Dataflow](/dataflow/docs/guides/build-container-image) .\nWhen you run your pipeline, launch the pipeline using the Apache Beam SDK with the same version and language version as the SDK on your custom container image. This step avoids unexpected errors from incompatible dependencies or SDKs.\n", "content": "## Test locally\nBefore you run your pipeline in Dataflow, it's a good idea to test the container image locally, which allows for more rapid testing and debugging.\nTo learn more about Apache Beam-specific usage, see the Apache Beam guide for [Running pipelines with custom container images](https://beam.apache.org/documentation/runtime/environments/#running-pipelines) .\n### Basic testing with PortableRunner\nTo verify that remote container images can be pulled and can run a simple pipeline, use the Apache Beam `PortableRunner` . When you use the `PortableRunner` , job submission occurs in the local environment, and the `DoFn` execution happens in the Docker environment.\nWhen you use GPUs, the Docker container might not have access to the GPUs. To test your container with GPUs, use the [direct runner](#direct-runner) and follow the steps for testing a container image on a standalone VM with GPUs in the [Debug with a standalone VM](/dataflow/docs/gpu/troubleshoot-gpus#debug-vm) section of the \"Use GPUs\" page.\nThe following runs an example pipeline:\n```\nmvn compile exec:java -Dexec.mainClass=com.example.package.MyClassWithMain \\\u00a0 \u00a0 -Dexec.args=\"--runner=PortableRunner \\\u00a0 \u00a0 --jobEndpoint=REGION \\\u00a0 \u00a0 --defaultEnvironmentType=DOCKER \\\u00a0 \u00a0 --defaultEnvironmentConfig=IMAGE_URI \\\u00a0 \u00a0 --inputFile=INPUT_FILE \\\u00a0 \u00a0 --output=OUTPUT_FILE\"\n```\n```\npython path/to/my/pipeline.py \\\u00a0 --runner=PortableRunner \\\u00a0 --job_endpoint=REGION \\\u00a0 --environment_type=DOCKER \\\u00a0 --environment_config=IMAGE_URI \\\u00a0 --input=INPUT_FILE \\\u00a0 --output=OUTPUT_FILE\n```\n```\ngo path/to/my/pipeline.go \\\u00a0 --runner=PortableRunner \\\u00a0 --job_endpoint=REGION \\\u00a0 --environment_type=DOCKER \\\u00a0 --environment_config=IMAGE_URI \\\u00a0 --input=INPUT_FILE \\\u00a0 --output=OUTPUT_FILE\n```\nReplace the following:\n- ``: the job service region to use, in the form of address and port. For example:`localhost:3000`. Use`embed`to run an in-process job service.\n- ``: the custom container image URI.\n- ``: an input file that can be read as a text file. This file must be accessible by the SDK harnesscontainer image, either preloaded on the container image or a remote file.\n- ``: a path to write output to. This path is either a remote path or a local path on the container.\nWhen the pipeline successfully completes, review the console logs to verify that the pipeline completed successfully and that the **remote image** , specified by `IMAGE_URI` , is used.\nAfter running the pipeline, files saved to the container are not in your local file system, and the container is stopped. You can copy files from the stopped container file system by using [docker cp](https://docs.docker.com/engine/reference/commandline/cp/) .\nAlternatively:\n- Provide outputs to a remote file system like Cloud Storage. You might need to manually configure access for testing purposes, including for credential files or [Application Default Credentials](/docs/authentication/provide-credentials-adc) .\n- For quick debugging, add temporary [logging](/dataflow/docs/guides/logging) .\n### Use the Direct Runner\nFor more in-depth local testing of the container image and your pipeline, use the Apache Beam [Direct Runner](https://beam.apache.org/documentation/runners/direct/) .\nYou can verify your pipeline separately from the container by testing in a local environment matching the container image, or by launching the pipeline on a running container.\n```\ndocker run -it --entrypoint \"/bin/bash\" IMAGE_URI...# On docker container:root@4f041a451ef3:/# \u00a0mvn compile exec:java -Dexec.mainClass=com.example.package.MyClassWithMain ...\n```\n```\ndocker run -it --entrypoint \"/bin/bash\" IMAGE_URI...# On docker container:root@4f041a451ef3:/# \u00a0python path/to/my/pipeline.py ...\n```\n```\ndocker run -it --entrypoint \"/bin/bash\" IMAGE_URI...# On docker container:root@4f041a451ef3:/# \u00a0go path/to/my/pipeline.go ...\n```\nReplace `` with the custom container image URI.\nThe examples assume any pipeline files, including the pipeline itself, are on the custom container, have been mounted from a local file system, or are remote and accessible by Apache Beam and the container. For example, to use Maven ( `mvn` ) to run the previous Java example, Maven and its dependencies must be staged on the container. For more information, see [Storage](https://docs.docker.com/storage/) and [docker run](https://docs.docker.com/engine/reference/run/) in the Docker documentation.\nThe goal for testing on the Direct Runner is to test your pipeline in the custom container environment, not to test running your container with its default `ENTRYPOINT` . Modify the `ENTRYPOINT` (for example, `docker run --entrypoint ...` ) to either directly run your pipeline or to allow manually running commands on the container.\nIf you rely on a specific configuration that is based on running the container on Compute Engine, you can run the container directly on a Compute Engine VM. For more information, see [Containers on Compute Engine](/compute/docs/containers) .\n## Launch the Dataflow job\nWhen launching the Apache Beam pipeline on Dataflow, specify the path to the container image. Don't use the `:latest` tag with your custom images. Tag your builds with a date or a unique identifier. If something goes wrong, using this type of tag might make it possible to revert the pipeline execution to a previously known working configuration and allow for an inspection of changes.\nUse `--sdkContainerImage` to specify an SDK container image for your Java runtime.\nUse `--experiments=use_runner_v2` to enable Runner v2.\nIf using SDK version **2.30.0 or later** , use the pipeline option `--sdk_container_image` to specify an SDK container image.\nFor earlier versions of the SDK, use the pipeline option `--worker_harness_container_image` to specify the location of container image to use for the worker harness.\nCustom containers are only supported for Dataflow Runner v2. If you're launching a batch Python pipeline, set the `--experiments=use_runner_v2` flag. If you're launching a streaming Python pipeline, specifying the experiment isn't necessary, because streaming Python pipelines use Runner v2 by default.\nIf using SDK version **2.40.0 or later** , use the pipeline option `--sdk_container_image` to specify an SDK container image.\nFor earlier versions of the SDK, use the pipeline option `--worker_harness_container_image` to specify the location of container image to use for the worker harness.\nCustom containers are supported on all versions of the Go SDK because they use Dataflow Runner v2 by default.\nThe following example demonstrates how to launch the batch [WordCount example](https://beam.apache.org/get-started/wordcount-example/) with a custom container.\n```\nmvn compile exec:java -Dexec.mainClass=org.apache.beam.examples.WordCount \\\u00a0 \u00a0-Dexec.args=\"--runner=DataflowRunner \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --inputFile=INPUT_FILE \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --output=OUTPUT_FILE \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --project=PROJECT_ID \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --region=REGION \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --gcpTempLocation=TEMP_LOCATION \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --diskSizeGb=DISK_SIZE_GB \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --experiments=use_runner_v2 \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --sdkContainerImage=IMAGE_URI\"\n```\nUsing the Apache Beam SDK for Python version 2.30.0 or later:\n```\npython -m apache_beam.examples.wordcount \\\u00a0 --input=INPUT_FILE \\\u00a0 --output=OUTPUT_FILE \\\u00a0 --project=PROJECT_ID \\\u00a0 --region=REGION \\\u00a0 --temp_location=TEMP_LOCATION \\\u00a0 --runner=DataflowRunner \\\u00a0 --disk_size_gb=DISK_SIZE_GB \\\u00a0 --experiments=use_runner_v2 \\\u00a0 --sdk_container_image=IMAGE_URI\n```\n```\nwordcount --input gs://dataflow-samples/shakespeare/kinglear.txt \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --output gs://<your-gcs-bucket>/counts \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --runner dataflow \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --project your-gcp-project \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --region your-gcp-region \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --temp_location gs://<your-gcs-bucket>/tmp/ \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --staging_location gs://<your-gcs-bucket>/binaries/ \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --sdk_container_image=IMAGE_URI\n```\nReplace the following:\n- ``: the Cloud Storage input path read by Dataflow when running the example.\n- ``: the Cloud Storage output path written to by the example pipeline. This file contains the word counts.\n- ``: the ID of your Google Cloud project.\n- ``: the region to deploy your Dataflow job in.\n- ``: the Cloud Storage path for Dataflow to stage temporary job files created during the execution of the pipeline.\n- ``: Optional. If your container is large, consider increasing default [boot disk size](/dataflow/docs/reference/pipeline-options#worker-level_options) to avoid [running out of disk space](/dataflow/docs/guides/common-errors#no-space-left) .\n- ``: the SDK custom container image URI. Always use a versioned container SHA or tag. Don't use the`:latest`tag or a mutable tag.", "guide": "Dataflow"}