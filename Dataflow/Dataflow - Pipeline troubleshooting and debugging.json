{"title": "Dataflow - Pipeline troubleshooting and debugging", "url": "https://cloud.google.com/dataflow/docs/guides/troubleshooting-your-pipeline", "abstract": "# Dataflow - Pipeline troubleshooting and debugging\nThis page provides troubleshooting tips and debugging strategies that you might find helpful if you're having trouble building or running your Dataflow pipeline. This information can help you detect a pipeline failure, determine the reason behind a failed pipeline run, and suggest some courses of action to correct the problem.\nThe following diagram shows the Dataflow troubleshooting workflow described in this page.\nDataflow provides real-time feedback about your job, and there is a basic set of steps you can use to check the error messages, logs, and for conditions such as your job's progress having stalled.\nFor guidance about common errors you might encounter when running your Dataflow job, see [Troubleshoot Dataflowerrors](/dataflow/docs/guides/common-errors) . To monitor and troubleshoot pipeline performance, see [Monitor pipeline performance](/dataflow/docs/guides/profiling-a-pipeline) .\n", "content": "## Best practices for pipelines\nThe following are the best practices for Java, Python, and Go pipelines.\n- For batch jobs, we recommend that you set a [time to live (TTL)](/storage/docs/lifecycle) for the temporary location.\n- Before setting up TTL and as a general best practice, ensure that you set both the staging location and the temporary location to different [locations](/dataflow/docs/guides/setting-pipeline-options#setting_required_options) .\n- Do not delete the objects in the staging location as these objects are reused.\n- If a job completes or is stopped and the temporary objects are not cleaned up, manually remove these files from the Cloud Storage bucket that is used as a temporary location.\nBoth the temporary and staging locations have a prefix of `<job_name>.<time>` .- Ensure that you set both the staging location and the temporary location to different [locations](/dataflow/docs/guides/setting-pipeline-options#setting_required_options) .\n- If required, delete the objects in the staging location after a job completes or stops. Also, staged objects are not reused in Python pipelines.\n- If a job ends and the temporary objects are not cleaned up, manually remove these files from the Cloud Storage bucket that is used as a temporary location.\n- For batch jobs, we recommend that you set a [time to live (TTL)](/storage/docs/lifecycle) for both the temporary and the staging locations.\n- Both the temporary and staging locations have a prefix of `<job_name>.<time>` .\n- Ensure that you set both the staging location and the temporary location to different [locations](/dataflow/docs/guides/setting-pipeline-options#setting_required_options) .\n- If required, delete the objects in the staging location after a job completes or stops. Also, staged objects are not reused in Go pipelines.\n- If a job ends and the temporary objects are not cleaned up, manually remove these files from the Cloud Storage bucket that is used as a temporary location.\n- For batch jobs, we recommend that you set a [time to live (TTL)](/storage/docs/lifecycle) for both the temporary and the staging locations.## Check your pipeline's status\nYou can detect any errors in your pipeline runs by using the [Dataflow monitoring interface](/dataflow/pipelines/dataflow-monitoring-intf) .\n- Go to the [Google Cloud console](https://console.cloud.google.com/) .\n- Select your Google Cloud project from the project list.\n- In the navigation menu, under **Big Data** , click **Dataflow** . A list of running jobs appears in the right-hand pane.\n- Select the pipeline job you want to view. You can see the jobs' status at a glance in the **Status** field: \"Running,\" \"Succeeded,\" or \"Failed.\"\n## Find information about pipeline failures\nIf one of your pipeline jobs fails, you can select the job to view more detailed information about errors and run results. When you select a job, you can view key charts for your pipeline, the execution graph, the **Job info** panel, and the **Logs** panel with **Job logs** , **Worker logs** , **Diagnostics** , and **Recommendations** tabs.\n### Check job error messages\nTo view the **Job Logs** generated by your pipeline code and the Dataflow service, in the **Logs** panel, click **Show** .\nYou can filter the messages that appear in **Job logs** by clicking **Info** and **Filter** . To only display error messages, click **Info** and select **Error** .\nTo expand an error message, click the expandable section .\nAlternatively, you can click the [Diagnostics tab](/dataflow/docs/guides/monitoring-overview#diagnostics) . This tab shows where errors occurred along the chosen timeline, a count of all logged errors, and possible recommendations for your pipeline.### View step logs for your job\nWhen you select a step in your pipeline graph, the logs panel toggles from displaying **Job Logs** generated by the Dataflow service to showing logs from the Compute Engine instances running your pipeline step.\n[Cloud Logging](/logging) combines all of the collected logs from your project's Compute Engine instances in one location. See [Logging pipeline messages](/dataflow/pipelines/logging) for more information about using Dataflow's various logging capabilities.\n## Handle automated pipeline rejection\nIn some cases, the Dataflow service identifies that your pipeline might trigger known SDK [issues](/dataflow/docs/resources/release-notes-service) . To prevent pipelines that are likely to encounter issues from being submitted, Dataflow automatically rejects your pipeline and displays the following message:\n```\nThe workflow was automatically rejected by the service because it might trigger an\nidentified bug in the SDK (details below). If you think this identification is\nin error, and would like to override this automated rejection, please re-submit\nthis workflow with the following override flag: [OVERRIDE FLAG].\nBug details: [BUG DETAILS].\nContact Google Cloud Support for further help.\nPlease use this identifier in your communication: [BUG ID].\n```\nAfter reading the caveats in the linked bug details, if you want to try to run your pipeline anyway, you can override the automated rejection. Add the flag `--experiments=<override-flag>` and resubmit your pipeline.\n## Determine the cause of a pipeline failure\nTypically, a failed Apache Beam pipeline run can be attributed to one of the following causes:\n- **Graph or pipeline construction errors.** These errors occur when Dataflow runs into a problem building the graph of steps that compose your pipeline, as described by your Apache Beam pipeline.\n- **Errors in job validation.** The Dataflow service validates any pipeline job you launch. Errors in the validation process can prevent your job from being successfully created or executed. Validation errors can include problems with your Google Cloud project's Cloud Storage bucket, or with your project's permissions.\n- **Exceptions in worker code.** These errors occur when there are errors or bugs in the user-provided code that Dataflow distributes to parallel workers, such as the`DoFn`instances of a`ParDo`transform.\n- **Errors caused by transient failures in other Google Cloud services.** Your pipeline might fail because of a temporary outage or other problem in the Google Cloud services upon which Dataflow depends, such as Compute Engine or Cloud Storage.\n### Detect graph or pipeline construction errors\nA graph construction error can occur when Dataflow is building the execution graph for your pipeline from the code in your Dataflow program. During graph construction time, Dataflow checks for illegal operations.\nIf Dataflow detects an error in graph construction, keep in mind that **no job is created** on the Dataflow service. Thus, you don't see any feedback in the Dataflow monitoring interface. Instead, an error message similar to the following appears in the console or terminal window where you ran your Apache Beam pipeline:\nFor example, if your pipeline attempts to perform an aggregation like `GroupByKey` on a globally windowed, non-triggered, unbounded `PCollection` , an error message similar to the following appears:\n```\n...\n... Exception in thread \"main\" java.lang.IllegalStateException:\n... GroupByKey cannot be applied to non-bounded PCollection in the GlobalWindow without a trigger.\n... Use a Window.into or Window.triggering transform prior to GroupByKey\n...\n```For example, if your pipeline uses [type hints](https://beam.apache.org/documentation/sdks/python-type-safety/) and the argument type in one of the transforms is not as expected, an error message  similar to the following occurs:\n```\n... in <module> run()\n... in run | beam.Map('count', lambda (word, ones): (word, sum(ones))))\n... in __or__ return self.pipeline.apply(ptransform, self)\n... in apply transform.type_check_inputs(pvalueish)\n... in type_check_inputs self.type_check_inputs_or_outputs(pvalueish, 'input')\n... in type_check_inputs_or_outputs pvalue_.element_type))\ngoogle.cloud.dataflow.typehints.decorators.TypeCheckError: Input type hint violation at group: expected Tuple[TypeVariable[K], TypeVariable[V]], got <type 'str'>\n```For example, if your pipeline uses a `DoFn` that doesn't take in any  inputs, an error message similar to the following occurs:\n```\n... panic: Method ProcessElement in DoFn main.extractFn is missing all inputs. A main input is required.\n... Full error:\n...  inserting ParDo in scope root/CountWords\n...  graph.AsDoFn: for Fn named main.extractFn\n... ProcessElement method has no main inputs\n... goroutine 1 [running]:\n... github.com/apache/beam/sdks/v2/go/pkg/beam.MustN(...)\n... (more stacktrace)\n```\nShould you encounter such an error, check your pipeline code to ensure that your pipeline's operations are legal.\n### Detect errors in Dataflow job validation\nOnce the Dataflow service has received your pipeline's graph, the service attempts to validate your job. This validation includes the following:\n- Making sure the service can access your job's associated Cloud Storage buckets for file staging and temporary output.\n- Checking for the required permissions in your Google Cloud project.\n- Making sure the service can access input and output sources, such as files.\nIf your job fails the validation process, an error message appears in the Dataflow monitoring interface, as well as in your console or terminal window if you are using blocking execution. The error message looks similar to the following:\n```\nINFO: To access the Dataflow monitoring console, please navigate to\n https://console.developers.google.com/project/google.com%3Aclouddfe/dataflow/job/2016-03-08_18_59_25-16868399470801620798\nSubmitted job: 2016-03-08_18_59_25-16868399470801620798\n...\n... Starting 3 workers...\n... Executing operation BigQuery-Read+AnonymousParDo+BigQuery-Write\n... Executing BigQuery import job \"dataflow_job_16868399470801619475\".\n... Stopping worker pool...\n... Workflow failed. Causes: ...BigQuery-Read+AnonymousParDo+BigQuery-Write failed.\nCauses: ... BigQuery getting table \"non_existent_table\" from dataset \"cws_demo\" in project \"my_project\" failed.\nMessage: Not found: Table x:cws_demo.non_existent_table HTTP Code: 404\n... Worker pool stopped.\n... com.google.cloud.dataflow.sdk.runners.BlockingDataflowPipelineRunner run\nINFO: Job finished with status FAILED\nException in thread \"main\" com.google.cloud.dataflow.sdk.runners.DataflowJobExecutionException:\n Job 2016-03-08_18_59_25-16868399470801620798 failed with status FAILED\n at com.google.cloud.dataflow.sdk.runners.DataflowRunner.run(DataflowRunner.java:155)\n at com.google.cloud.dataflow.sdk.runners.DataflowRunner.run(DataflowRunner.java:56)\n at com.google.cloud.dataflow.sdk.Pipeline.run(Pipeline.java:180)\n at com.google.cloud.dataflow.integration.BigQueryCopyTableExample.main(BigQueryCopyTableExample.java:74)\n``````\nINFO:root:Created job with id: [2016-03-08_14_12_01-2117248033993412477]\n... Checking required Cloud APIs are enabled.\n... Job 2016-03-08_14_12_01-2117248033993412477 is in state JOB_STATE_RUNNING.\n... Combiner lifting skipped for step group: GroupByKey not followed by a combiner.\n... Expanding GroupByKey operations into optimizable parts.\n... Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\n... Annotating graph with Autotuner information.\n... Fusing adjacent ParDo, Read, Write, and Flatten operations\n... Fusing consumer split into read\n...\n... Starting 1 workers...\n...\n... Executing operation read+split+pair_with_one+group/Reify+group/Write\n... Executing failure step failure14\n... Workflow failed.\nCauses: ... read+split+pair_with_one+group/Reify+group/Write failed.\nCauses: ... Unable to view metadata for files: gs://dataflow-samples/shakespeare/missing.txt.\n... Cleaning up.\n... Tearing down pending resources...\nINFO:root:Job 2016-03-08_14_12_01-2117248033993412477 is in state JOB_STATE_FAILED.\n```The job validation described in this section is not currently supported for Go. Errors due to these issues appear as worker exceptions.\n### Detect an exception in worker code\nWhile your job is running, you might encounter errors or exceptions in your worker code. These errors generally mean that the `DoFn` s in your pipeline code have generated unhandled exceptions, which result in failed tasks in your Dataflow job.\nExceptions in user code (for example, your `DoFn` instances) are reported in the [Dataflow monitoring interface](/dataflow/pipelines/dataflow-monitoring-intf) . If you run your pipeline with blocking execution, error messages are printed in your console or terminal window, such as the following:\n```\nINFO: To access the Dataflow monitoring console, please navigate to https://console.developers.google.com/project/example_project/dataflow/job/2017-05-23_14_02_46-1117850763061203461\nSubmitted job: 2017-05-23_14_02_46-1117850763061203461\n...\n... To cancel the job using the 'gcloud' tool, run: gcloud beta dataflow jobs --project=example_project cancel 2017-05-23_14_02_46-1117850763061203461\n... Autoscaling is enabled for job 2017-05-23_14_02_46-1117850763061203461.\n... The number of workers will be between 1 and 15.\n... Autoscaling was automatically enabled for job 2017-05-23_14_02_46-1117850763061203461.\n...\n... Executing operation BigQueryIO.Write/BatchLoads/Create/Read(CreateSource)+BigQueryIO.Write/BatchLoads/GetTempFilePrefix+BigQueryIO.Write/BatchLoads/TempFilePrefixView/BatchViewOverrides.GroupByWindowHashAsKeyAndWindowAsSortKey/ParDo(UseWindowHashAsKeyAndWindowAsSortKey)+BigQueryIO.Write/BatchLoads/TempFilePrefixView/Combine.GloballyAsSingletonView/Combine.globally(Singleton)/WithKeys/AddKeys/Map/ParMultiDo(Anonymous)+BigQueryIO.Write/BatchLoads/TempFilePrefixView/Combine.GloballyAsSingletonView/Combine.globally(Singleton)/Combine.perKey(Singleton)/GroupByKey/Reify+BigQueryIO.Write/BatchLoads/TempFilePrefixView/Combine.GloballyAsSingletonView/Combine.globally(Singleton)/Combine.perKey(Singleton)/GroupByKey/Write+BigQueryIO.Write/BatchLoads/TempFilePrefixView/BatchViewOverrides.GroupByWindowHashAsKeyAndWindowAsSortKey/BatchViewOverrides.GroupByKeyAndSortValuesOnly/Write\n... Workers have started successfully.\n...\n... org.apache.beam.runners.dataflow.util.MonitoringUtil$LoggingHandler process SEVERE: 2017-05-23T21:06:33.711Z: (c14bab21d699a182): java.lang.RuntimeException: org.apache.beam.sdk.util.UserCodeException: java.lang.ArithmeticException: / by zero\n  at com.google.cloud.dataflow.worker.runners.worker.GroupAlsoByWindowsParDoFn$1.output(GroupAlsoByWindowsParDoFn.java:146)\n  at com.google.cloud.dataflow.worker.runners.worker.GroupAlsoByWindowFnRunner$1.outputWindowedValue(GroupAlsoByWindowFnRunner.java:104)\n  at com.google.cloud.dataflow.worker.util.BatchGroupAlsoByWindowAndCombineFn.closeWindow(BatchGroupAlsoByWindowAndCombineFn.java:191)\n...\n... Cleaning up.\n... Stopping worker pool...\n... Worker pool stopped.\n```\n **Note:** The Dataflow service retries failed tasks up to 4 times in batch mode, and an unlimited number of times in streaming mode. In batch mode, your job fails; in streaming mode, it might stall indefinitely.```\nINFO:root:Job 2016-03-08_14_21_32-8974754969325215880 is in state JOB_STATE_RUNNING.\n...\nINFO:root:... Expanding GroupByKey operations into optimizable parts.\nINFO:root:... Lifting ValueCombiningMappingFns into MergeBucketsMappingFns\nINFO:root:... Annotating graph with Autotuner information.\nINFO:root:... Fusing adjacent ParDo, Read, Write, and Flatten operations\n...\nINFO:root:...: Starting 1 workers...\nINFO:root:...: Executing operation group/Create\nINFO:root:...: Value \"group/Session\" materialized.\nINFO:root:...: Executing operation read+split+pair_with_one+group/Reify+group/Write\nINFO:root:Job 2016-03-08_14_21_32-8974754969325215880 is in state JOB_STATE_RUNNING.\nINFO:root:...: ...: Workers have started successfully.\nINFO:root:Job 2016-03-08_14_21_32-8974754969325215880 is in state JOB_STATE_RUNNING.\nINFO:root:...: Traceback (most recent call last):\n File \".../dataflow_worker/batchworker.py\", line 384, in do_work self.current_executor.execute(work_item.map_task)\n ...\n File \".../apache_beam/examples/wordcount.runfiles/py/apache_beam/examples/wordcount.py\", line 73, in <lambda>\nValueError: invalid literal for int() with base 10: 'www'\n```\n **Note:** The Dataflow service retries failed tasks up to 4 times in batch mode, and an unlimited number of times in streaming mode. In batch mode, your job fails; in streaming, it might stall indefinitely.```\n... 2022-05-26T18:32:52.752315397Zprocess bundle failed for instruction\n...  process_bundle-4031463614776698457-2 using plan s02-6 : while executing\n...  Process for Plan[s02-6] failed: Oh no! This is an error message!\n```\n **Note:** The Dataflow service retries failed tasks up to 4 times in batch mode, and an unlimited number of times in streaming mode. In batch mode, your job fails; in streaming, it might stall indefinitely.\nConsider guarding against errors in your code by adding exception handlers. For example, if you'd like to drop elements that fail some custom input validation done in a `ParDo` , handle the exception within your `DoFn` and drop the element.\nYou can also track failing elements in a few different ways:\n- You can [log](/dataflow/pipelines/logging) the failing elements and check the output using Cloud Logging.\n- You can check the Dataflow worker and worker startup logs for warnings or errors by following the instructions in [Viewing logs](/dataflow/docs/guides/logging#viewing_logs) .\n- You can have your`ParDo`write the failing elements to an [additional output](https://beam.apache.org/documentation/programming-guide/#additional-outputs) for later inspection.\nTo track the properties of an executing pipeline, you can use the `Metrics` class, as shown in the following example:\n```\nfinal Counter counter = Metrics.counter(\"stats\", \"even-items\");PCollection<Integer> input = pipeline.apply(...);...input.apply(ParDo.of(new DoFn<Integer, Integer>() {\u00a0 @ProcessElement\u00a0 public void processElement(ProcessContext c) {\u00a0 \u00a0 if (c.element() % 2 == 0) {\u00a0 \u00a0 \u00a0 counter.inc();\u00a0 \u00a0 }});\n``````\nclass FilterTextFn(beam.DoFn):\u00a0 \u00a0 \u00a0 \"\"\"A DoFn that filters for a specific key based on a regex.\"\"\"\u00a0 \u00a0 \u00a0 def __init__(self, pattern):\u00a0 \u00a0 \u00a0 \u00a0 self.pattern = pattern\u00a0 \u00a0 \u00a0 \u00a0 # A custom metric can track values in your pipeline as it runs. Create\u00a0 \u00a0 \u00a0 \u00a0 # custom metrics to count unmatched words, and know the distribution of\u00a0 \u00a0 \u00a0 \u00a0 # word lengths in the input PCollection.\u00a0 \u00a0 \u00a0 \u00a0 self.word_len_dist = Metrics.distribution(self.__class__,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'word_len_dist')\u00a0 \u00a0 \u00a0 \u00a0 self.unmatched_words = Metrics.counter(self.__class__,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0'unmatched_words')\u00a0 \u00a0 \u00a0 def process(self, element):\u00a0 \u00a0 \u00a0 \u00a0 word = element\u00a0 \u00a0 \u00a0 \u00a0 self.word_len_dist.update(len(word))\u00a0 \u00a0 \u00a0 \u00a0 if re.match(self.pattern, word):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 yield element\u00a0 \u00a0 \u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 self.unmatched_words.inc()\u00a0 \u00a0 filtered_words = (\u00a0 \u00a0 \u00a0 \u00a0 words | 'FilterText' >> beam.ParDo(FilterTextFn('s.*')))\n``````\nfunc addMetricDoFnToPipeline(s beam.Scope, input beam.PCollection) beam.PCollection {\u00a0 \u00a0 return beam.ParDo(s, &MyMetricsDoFn{}, input)}func executePipelineAndGetMetrics(ctx context.Context, p *beam.Pipeline) (metrics.QueryResults, error) {\u00a0 \u00a0 pr, err := beam.Run(ctx, runner, p)\u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 return metrics.QueryResults{}, err\u00a0 \u00a0 }\u00a0 \u00a0 // Request the metric called \"counter1\" in namespace called \"namespace\"\u00a0 \u00a0 ms := pr.Metrics().Query(func(r beam.MetricResult) bool {\u00a0 \u00a0 \u00a0 \u00a0 return r.Namespace() == \"namespace\" && r.Name() == \"counter1\"\u00a0 \u00a0 })\u00a0 \u00a0 // Print the metric value - there should be only one line because there is\u00a0 \u00a0 // only one metric called \"counter1\" in the namespace called \"namespace\"\u00a0 \u00a0 for _, c := range ms.Counters() {\u00a0 \u00a0 \u00a0 \u00a0 fmt.Println(c.Namespace(), \"-\", c.Name(), \":\", c.Committed)\u00a0 \u00a0 }\u00a0 \u00a0 return ms, nil}type MyMetricsDoFn struct {\u00a0 \u00a0 counter beam.Counter}func init() {\u00a0 \u00a0 beam.RegisterType(reflect.TypeOf((*MyMetricsDoFn)(nil)))}func (fn *MyMetricsDoFn) Setup() {\u00a0 \u00a0 // While metrics can be defined in package scope or dynamically\u00a0 \u00a0 // it's most efficient to include them in the DoFn.\u00a0 \u00a0 fn.counter = beam.NewCounter(\"namespace\", \"counter1\")}func (fn *MyMetricsDoFn) ProcessElement(ctx context.Context, v beam.V, emit func(beam.V)) {\u00a0 \u00a0 // count the elements\u00a0 \u00a0 fn.counter.Inc(ctx, 1)\u00a0 \u00a0 emit(v)}\n```\n## Troubleshoot slow-running pipelines or lack of output\nSee [Troubleshoot slow and stuck jobs](/dataflow/docs/guides/troubleshoot-slow-jobs) .\n## Common errors and courses of action\nWhen you know the error that caused the pipeline failure, see the [Troubleshoot Dataflow errors](/dataflow/docs/guides/common-errors) page for error troubleshooting guidance.", "guide": "Dataflow"}