{"title": "Dataflow - Create an ecommerce streaming pipeline", "url": "https://cloud.google.com/dataflow/docs/tutorials/ecommerce-retail-pipeline", "abstract": "# Dataflow - Create an ecommerce streaming pipeline\nIn this tutorial, you create a Dataflow streaming pipeline that transforms ecommerce data from Pub/Sub topics and subscriptions and outputs the data to BigQuery and Bigtable. This tutorial requires [Gradle](https://gradle.org/) .\nThe tutorial provides an end-to-end ecommerce sample application that streams data from a webstore to BigQuery and Bigtable. The sample application illustrates common use cases and best practices for implementing streaming data analytics and real-time artificial intelligence (AI). Use this tutorial to learn how to respond dynamically to customer actions in order to analyze and react to events in real-time. This tutorial describes how to store, analyze, and visualize event data to get more insight into customer behavior.\nThe sample application is [available on GitHub](https://github.com/GoogleCloudPlatform/dataflow-sample-applications/tree/master/retail/retail-java-applications) . To run this tutorial using Terraform, follow the steps provided with the sample application on GitHub.", "content": "## Objectives\n- Validate incoming data and apply corrections to it where possible.\n- Analyze clickstream data to keep a count of the number of views per product in a given time period. Store this information in a low-latency store. The application can then use the data to providemessages to customers on the website.\n- Use transaction data to inform inventory ordering:- Analyze transaction data to calculate the total number of sales for each item, both by store and globally, for a given period.\n- Analyze inventory data to calculate the incoming inventory for each item.\n- Pass this data to inventory systems on a continuous basis so it can be used for inventory purchasing decisions.\n- Validate incoming data and apply corrections to it where possible. Write any uncorrectable data to a dead-letter queue for additional analysis and processing. Make a metric that represents the percentage of incoming data that gets sent to the dead-letter queue available for monitoring and alerting.\n- Process all incoming data into a standard format and store it in a data warehouse to use for future analysis and visualization.\n- Denormalize transaction data for in-store sales so that it can include information like the latitude and longitude of the store location. Provide the store information through a slowly changing table in BigQuery, using the store ID as a key.\n### DataThe application processes the following types of data:- Clickstream data being sent by online systems to Pub/Sub.\n- Transaction data being sent by on-premises or software as a service (SaaS) systems to Pub/Sub.\n- Stock data being sent by on-premises or SaaS systems to Pub/Sub.\n **Note:** All data is sent as JSON. This format is normal for clickstream data but not necessarily for transaction or stock data.\n### Task patternsThe application contains the following task patterns common to pipelines built with the Apache Beam SDK for Java:- [Apache Beam schemas to work with structured data](/dataflow/docs/tutorials/ecommerce-java#structured-data) \n- [JsonToRow to convert JSON data](/dataflow/docs/tutorials/ecommerce-java#json-to-row) \n- [The AutoValue code generator to generate plain old Java objects (POJOs)](/dataflow/docs/tutorials/ecommerce-java#use-autovalue) \n- [Queuing unprocessable data for further analysis](/dataflow/docs/tutorials/ecommerce-java#queue-unprocessable-data) \n- [Serial data validation transforms](/dataflow/docs/tutorials/ecommerce-java#data-validation-transforms) \n- [DoFn.StartBundle to micro-batch calls to external services](/dataflow/docs/tutorials/ecommerce-java#micro-batch-calls) \n- [Side-input patterns](/dataflow/docs/tutorials/ecommerce-java#side-input-pattern) \n## CostsIn this document, you use the following billable components of Google Cloud:- BigQuery\n- Bigtable\n- Cloud Scheduler\n- Compute Engine\n- Dataflow\n- Pub/Sub\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- Create a user-managed worker service account for your new pipeline and grant the necessary roles to the service account.- To create the service account, run the [gcloud iam service-accounts create](/sdk/gcloud/reference/iam/service-accounts/create) command:```\ngcloud iam service-accounts create retailpipeline \\\u00a0 \u00a0 --description=\"Retail app data pipeline worker service account\" \\\u00a0 \u00a0 --display-name=\"Retail app data pipeline access\"\n```\n- Grant roles to the service account. Run the following command once  for each of the following IAM roles:- `roles/dataflow.admin`\n- `roles/dataflow.worker`\n- `roles/pubsub.editor`\n- `roles/bigquery.dataEditor`\n- `roles/bigtable.admin`\n```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"serviceAccount:retailpipeline@PROJECT_ID.iam.gserviceaccount.com\" --role=SERVICE_ACCOUNT_ROLE\n```Replace `` with each individual role.\n- Grant your Google Account a role that lets you create access tokens for the service account:```\ngcloud iam service-accounts add-iam-policy-binding retailpipeline@PROJECT_ID.iam.gserviceaccount.com --member=\"user:EMAIL_ADDRESS\" --role=roles/iam.serviceAccountTokenCreator\n```\n- If needed, download and install [Gradle](https://gradle.org/install/) .\n## Create the example sources and sinksThis section explains how to create the following:- A Cloud Storage bucket to use as a temporary storage location\n- Streaming data sources using Pub/Sub\n- Datasets to load the data into BigQuery\n- A Bigtable instance\n### Create a Cloud Storage bucketBegin by creating a Cloud Storage bucket. This bucket is used as a temporary storage location by the Dataflow pipeline.\nUse the [gcloud storage buckets create command](/sdk/gcloud/reference/storage/buckets/create) :\n```\ngcloud storage buckets create gs://BUCKET_NAME --location=LOCATION\n```\nReplace the following:- : a name for your Cloud Storage bucket that meets the [bucket naming requirements](/storage/docs/buckets#naming) . Cloud Storage bucket names must be globally unique.\n- : the [location](/storage/docs/locations#available-locations) for the bucket.\n### Create Pub/Sub topics and subscriptionsCreate four Pub/Sub topics and then create three subscriptions.\nTo create your topics, run the [gcloud pubsub topics create](/sdk/gcloud/reference/pubsub/topics/create) command once for each topic. For information about how to name a subscription, see [Guidelines to name a topic or a subscription](/pubsub/docs/pubsub-basics#resource_names) .\n```\ngcloud pubsub topics create TOPIC_NAME\n```\nReplace with the following values, running the command four times, once for each topic:- `Clickstream-inbound`\n- `Transactions-inbound`\n- `Inventory-inbound`\n- `Inventory-outbound`\nTo create a subscription to your topic, run the [gcloud pubsub subscriptions create](/sdk/gcloud/reference/pubsub/subscriptions/create) command once for each subscription:- Create a `Clickstream-inbound-sub` subscription:```\ngcloud pubsub subscriptions create --topic Clickstream-inbound Clickstream-inbound-sub\n```\n- Create a `Transactions-inbound-sub` subscription:```\ngcloud pubsub subscriptions create --topic Transactions-inbound Transactions-inbound-sub\n```\n- Create an `Inventory-inbound-sub` subscription:```\ngcloud pubsub subscriptions create --topic Inventory-inbound Inventory-inbound-sub\n```\n### Create BigQuery datasets and tableCreate a BigQuery dataset and a [partitioned table](/bigquery/docs/partitioned-tables) with the appropriate schema for your Pub/Sub topic.- Use the [bq mk](/bigquery/docs/reference/bq-cli-reference#bq_mk) command to create the first dataset.```\nbq --location=US mk \\PROJECT_ID:Retail_Store\n```\n- Create the second dataset.```\nbq --location=US mk \\PROJECT_ID:Retail_Store_Aggregations\n```\n- Use the [CREATE TABLE](/bigquery/docs/reference/standard-sql/data-definition-language#create_table_statement) SQL statement to create a table with a schema and test data. The test data has one store with an ID value of `1` . The slow update side input pattern uses this table.```\nbq query --use_legacy_sql=false \\\u00a0 'CREATE TABLE\u00a0 \u00a0 Retail_Store.Store_Locations\u00a0 \u00a0 (\u00a0 \u00a0 \u00a0 id INT64,\u00a0 \u00a0 \u00a0 city STRING,\u00a0 \u00a0 \u00a0 state STRING,\u00a0 \u00a0 \u00a0 zip INT64\u00a0 \u00a0 );\u00a0 INSERT INTO Retail_Store.Store_Locations\u00a0 VALUES (1, \"a_city\", \"a_state\",00000);'\n```\n### Create a Bigtable instance and tableCreate a Bigtable instance and table. For more information about creating Bigtable instances, see [Create an instance](/bigtable/docs/creating-instance) .- If needed, run the following command to install the [cbt CLI](/bigtable/docs/cbt-reference) :```\ngcloud components install cbt\n```\n- Use the [bigtable instances create command](/sdk/gcloud/reference/bigtable/instances/create) to create an instance:```\ngcloud bigtable instances create aggregate-tables \\\u00a0 \u00a0 --display-name=aggregate-tables \\\u00a0 \u00a0 --cluster-config=id=aggregate-tables-c1,zone=CLUSTER_ZONE,nodes=1\n```Replace with the [zone](/bigtable/docs/locations) where the cluster runs.\n- Use the [cbt createtable command](/bigtable/docs/cbt-reference) to create a table:```\ncbt -instance=aggregate-tables createtable PageView5MinAggregates\n```\n- Use the following command to add a column family to the table:```\ncbt -instance=aggregate-tables createfamily PageView5MinAggregates pageViewAgg\n```\n## Run the pipelineUse [Gradle](https://gradle.org/) to run a streaming pipeline. To view the Java code that the pipeline is using, see [RetailDataProcessingPipeline.java](https://github.com/GoogleCloudPlatform/dataflow-sample-applications/blob/master/retail/retail-java-applications/data-engineering-dept/pipelines/src/main/java/com/google/dataflow/sample/retail/pipeline/RetailDataProcessingPipeline.java) .- Use the `git clone` command to clone the GitHub repository:```\ngit clone https://github.com/GoogleCloudPlatform/dataflow-sample-applications.git\n```\n- Switch to the application directory:```\ncd dataflow-sample-applications/retail/retail-java-applications\n```\n- To test the pipeline, in your shell or terminal, run the following command using Gradle:```\n./gradlew :data-engineering-dept:pipelines:test --tests RetailDataProcessingPipelineSimpleSmokeTest --info --rerun-tasks\n```\n- To run the pipeline, run the following command using Gradle:```\n./gradlew tasks executeOnDataflow -Dexec.args=\" \\--project=PROJECT_ID \\--tempLocation=gs://BUCKET_NAME/temp/ \\--runner=DataflowRunner \\--region=REGION \\--clickStreamPubSubSubscription=projects/PROJECT_ID/subscriptions/Clickstream-inbound-sub \\--transactionsPubSubSubscription=projects/PROJECT_ID/subscriptions/Transactions-inbound-sub \\--inventoryPubSubSubscriptions=projects/PROJECT_ID/subscriptions/Inventory-inbound-sub \\--aggregateStockPubSubOutputTopic=projects/PROJECT_ID/topics/Inventory-outbound \\--dataWarehouseOutputProject=PROJECT_ID\"\n```\nSee the [pipeline source code](https://github.com/GoogleCloudPlatform/dataflow-sample-applications/blob/master/retail/retail-java-applications/data-engineering-dept/pipelines/src/main/java/com/google/dataflow/sample/retail/pipeline/RetailDataProcessingPipeline.java) on GitHub.## Create and run Cloud Scheduler jobsCreate and run three Cloud Scheduler jobs, one that publishes clickstream data, one for inventory data, and one for transaction data. This step generates sample data for the pipeline.- To create a Cloud Scheduler job for this tutorial, use the [gcloud scheduler jobs create](/sdk/gcloud/reference/scheduler/jobs/create/pubsub) command. This step creates a publisher for clickstream data that publishes one message per minute.```\ngcloud scheduler jobs create pubsub clickstream \\\u00a0 --schedule=\"* * * * *\" \\\u00a0 --location=LOCATION \\\u00a0 --topic=\"Clickstream-inbound\" \\\u00a0 --message-body='{\"uid\":464670,\"sessionId\":null,\"returning\":false,\"lat\":39.669082,\"lng\":-80.312306,\"agent\":\"Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148;\",\"event\":\"add-to-cart\",\"transaction\":false,\"timestamp\":1660091197071,\"ecommerce\":{\"items\":[{\"item_name\":\"Donut Friday Scented T-Shirt\",\"item_id\":\"67890\",\"price\":33.75,\"item_brand\":\"Google\",\"item_category\":\"Apparel\",\"item_category_2\":\"Mens\",\"item_category_3\":\"Shirts\",\"item_category_4\":\"Tshirts\",\"item_variant\":\"Black\",\"item_list_name\":\"Search Results\",\"item_list_id\":\"SR123\",\"index\":1,\"quantity\":2}]},\"user_id\":74378,\"client_id\":\"52393559\",\"page_previous\":\"P_3\",\"page\":\"P_3\",\"event_datetime\":\"2022-08-10 12:26:37\"}'\n```\n- To start the Cloud Scheduler job, use the [gcloud scheduler jobs run](/sdk/gcloud/reference/scheduler/jobs/run) command.```\ngcloud scheduler jobs run --location=LOCATION clickstream\n```\n- Create and run another similar publisher for inventory data that publishes one message every two minutes.```\ngcloud scheduler jobs create pubsub inventory \\\u00a0 --schedule=\"*/2 * * * *\" \\\u00a0 --location=LOCATION \u00a0\\\u00a0 --topic=\"Inventory-inbound\" \\\u00a0 --message-body='{\"count\":1,\"sku\":0,\"aisleId\":0,\"product_name\":null,\"departmentId\":0,\"price\":null,\"recipeId\":null,\"image\":null,\"timestamp\":1660149636076,\"store_id\":1,\"product_id\":10050}'\n```\n- Start the second Cloud Scheduler job.```\ngcloud scheduler jobs run --location=LOCATION inventory\n```\n- Create and run a third publisher for transaction data that publishes one message every two minutes.```\ngcloud scheduler jobs create pubsub transactions \\\u00a0 --schedule=\"*/2 * * * *\" \\\u00a0 --location=LOCATION \u00a0\\\u00a0 --topic=\"Transactions-inbound\" \\\u00a0 --message-body='{\"order_number\":\"b8be9222-990d-11ea-9c05-42010af00081\",\"user_id\":998685,\"store_id\":1,\"returning\":false,\"time_of_sale\":0,\"department_id\":0,\"product_id\":4,\"product_count\":1,\"price\":25.0,\"order_id\":0,\"order_dow\":0,\"order_hour_of_day\":0,\"order_woy\":0,\"days_since_prior_order\":null,\"product_name\":null,\"product_sku\":0,\"image\":null,\"timestamp\":1660157951000,\"ecommerce\":{\"items\":[{\"item_name\":\"Donut Friday Scented T-Shirt\",\"item_id\":\"67890\",\"price\":33.75,\"item_brand\":\"Google\",\"item_category\":\"Apparel\",\"item_category_2\":\"Mens\",\"item_category_3\":\"Shirts\",\"item_category_4\":\"Tshirts\",\"item_variant\":\"Black\",\"item_list_name\":\"Search Results\",\"item_list_id\":\"SR123\",\"index\":1,\"quantity\":2}]},\"client_id\":\"1686224283\",\"page_previous\":null,\"page\":null,\"event_datetime\":\"2022-08-10 06:59:11\"}'\n```\n- Start the third Cloud Scheduler job.```\ngcloud scheduler jobs run --location=LOCATION transactions\n```\n## View your resultsView data written to your BigQuery tables. Check the results in BigQuery by running the following queries. While this pipeline is running, you can see new rows appended to the BigQuery tables every minute.\nYou might need to wait for the tables to populate with data.\n```\nbq query --use_legacy_sql=false 'SELECT * FROM `'\"PROJECT_ID.Retail_Store.clean_inventory_data\"'`'\n```\n```\nbq query --use_legacy_sql=false 'SELECT * FROM `'\"PROJECT_ID.Retail_Store.clean_transaction_data\"'`'\n```## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the projectThe easiest way to eliminate billing is to delete the Google Cloud project that you created for the tutorial.- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Delete the individual resourcesIf you want to reuse the project, then delete the resources that you created for the tutorial.\n- To delete the Cloud Scheduler jobs, use the [gcloud scheduler jobs delete](/sdk/gcloud/reference/scheduler/jobs/delete) command.```\n\u00a0gcloud scheduler jobs delete transactions --location=LOCATION\n``````\n\u00a0gcloud scheduler jobs delete inventory --location=LOCATION\n``````\n\u00a0gcloud scheduler jobs delete clickstream --location=LOCATION\n```\n- To delete the Pub/Sub subscriptions and topics, use the [gcloud pubsub subscriptions delete](/sdk/gcloud/reference/pubsub/subscriptions/delete) and the [gcloud pubsub topics delete](/sdk/gcloud/reference/pubsub/topics/delete) commands.```\ngcloud pubsub subscriptions delete SUBSCRIPTION_NAMEgcloud pubsub topics delete TOPIC_NAME\n```\n- To delete the BigQuery table, use the [bq rm](/bigquery/docs/reference/bq-cli-reference) command.```\nbq rm -f -t PROJECT_ID:Retail_Store.Store_Locations\n```\n- Delete the BigQuery datasets. The dataset alone does not incur any charges. **Caution:** The following command also deletes all tables in the dataset. The tables and data cannot be recovered.```\nbq rm -r -f -d PROJECT_ID:Retail_Store\n``````\nbq rm -r -f -d PROJECT_ID:Retail_Store_Aggregations\n```\n- To delete the Bigtable instance, use the `cbt deleteinstance` command. The bucket alone does not incur any charges.```\ncbt deleteinstance aggregate-tables\n```\n- To delete the Cloud Storage bucket, use the [gcloud storage rm command](/sdk/gcloud/reference/storage/rm) . The bucket alone does not incur any charges. **Caution:** The following command also deletes all objects in the bucket. These objects cannot be recovered.```\ngcloud storage rm gs://BUCKET_NAME --recursive\n```\n- Revoke the roles that you granted to the user-managed worker service account.  Run the following command once for each of the following IAM roles:- `roles/dataflow.admin`\n- `roles/dataflow.worker`\n- `roles/pubsub.editor`\n- `roles/bigquery.dataEditor`\n- `roles/bigtable.admin`\n```\ngcloud projects remove-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member=serviceAccount:retailpipeline@PROJECT_ID.iam.gserviceaccount.com \\\u00a0 \u00a0 --role=ROLE\n```\n- Optional: Revoke the authentication credentials that you created, and delete the local   credential file.```\ngcloud auth application-default revoke\n```\n- Optional: Revoke credentials from the gcloud CLI.```\ngcloud auth revoke\n```\n## What's next\n- View [the sample application on GitHub](https://github.com/GoogleCloudPlatform/dataflow-sample-applications/tree/master/retail/retail-java-applications) .\n- Read the related blog post [Learn Beam patterns with Clickstream processing of Google Tag Manager data](https://cloud.google.com/blog/products/data-analytics/learn-beam-patterns-with-clickstream-processing-of-google-tag-manager-data) .\n- Read about using Pub/Sub to [create and use topics](/pubsub/docs/create-topic) and to [Use subscriptions](/pubsub/docs/subscriber) .\n- Read about using BigQuery to [create datasets](/bigquery/docs/datasets) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Dataflow"}