{"title": "Dataflow - Use custom containers with C++ libraries", "url": "https://cloud.google.com/dataflow/docs/hpc-ep/hpc-tutorial", "abstract": "# Dataflow - Use custom containers with C++ libraries\nIn this tutorial, you create a pipeline that uses custom containers with C++ libraries to run a Dataflow HPC highly parallel workflow. Use this tutorial to learn how to use Dataflow and Apache Beam to run grid computing applications that require data to be distributed to functions running on many cores.\nThe tutorial demonstrates how to run the pipeline first by using the [Direct Runner](https://beam.apache.org/documentation/runners/direct/) and then by using the [Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/) . By running the pipeline locally, you can test the pipeline before deploying it.\nThis example uses [Cython](https://cython.org/) bindings and functions from the [GMP library](https://gmplib.org/) . Regardless of the library or binding tool that you use, you can apply the same principles to your pipeline.\nThe example code is [available on GitHub](https://github.com/GoogleCloudPlatform/dataflow-sample-applications/tree/master/beam-cpp-example) .", "content": "## Objectives\n- Create a pipeline that uses custom containers with C++ libraries.\n- Build a Docker container image using a Dockerfile.\n- Package the code and dependencies into a Docker container.\n- Run the pipeline locally to test it.\n- Run the pipeline in a distributed environment.\n## CostsIn this document, you use the following billable components of Google Cloud:- Artifact Registry\n- Cloud Build\n- Cloud Storage\n- Compute Engine\n- Dataflow\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- Create a user-managed worker service account for your new pipeline and grant the necessary roles to the service account.- To create the service account, run the [gcloud iam service-accounts create](/sdk/gcloud/reference/iam/service-accounts/create) command:```\ngcloud iam service-accounts create parallelpipeline \\\u00a0 \u00a0 --description=\"Highly parallel pipeline worker service account\" \\\u00a0 \u00a0 --display-name=\"Highly parallel data pipeline access\"\n```\n- Grant roles to the service account. Run the following command once  for each of the following IAM roles:- `roles/dataflow.admin`\n- `roles/dataflow.worker`\n- `roles/storage.objectAdmin`\n- `roles/artifactregistry.reader`\n```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"serviceAccount:parallelpipeline@PROJECT_ID.iam.gserviceaccount.com\" --role=SERVICE_ACCOUNT_ROLE\n```Replace `` with each individual role.\n- Grant your Google Account a role that lets you create access tokens for the service account:```\ngcloud iam service-accounts add-iam-policy-binding parallelpipeline@PROJECT_ID.iam.gserviceaccount.com --member=\"user:EMAIL_ADDRESS\" --role=roles/iam.serviceAccountTokenCreator\n```## Download the code sample and change directoriesDownload the code sample and then change directories. The code samples in the GitHub repository provide all the code that you need to run this pipeline. When you are ready to build your own pipeline, you can use this sample code as a template.\nClone the [beam-cpp-example repository](https://github.com/GoogleCloudPlatform/dataflow-sample-applications/tree/master/beam-cpp-example) .- Use the `git clone` command to clone the GitHub repository:```\ngit clone https://github.com/GoogleCloudPlatform/dataflow-sample-applications.git\n```\n- Switch to the application directory:```\ncd dataflow-sample-applications/beam-cpp-example\n```\n### Pipeline codeYou can customize the pipeline code from this tutorial. This pipeline completes the following tasks:- Dynamically produces all integers in an input range.\n- Runs the integers through a C++ function and filters bad values.\n- Writes the bad values to a side channel.\n- Counts the occurrence of each stopping time and normalizes the results.\n- Prints the output, formatting and writing the results to a text file.\n- Creates a`PCollection`with a single element.\n- Processes the single element with a`map`function and passes the frequency`PCollection`as a side input.\n- Processes the`PCollection`and produces a single output.\nThe starter file looks like the following:\n [View on GitHub](https://github.com/GoogleCloudPlatform/dataflow-sample-applications/blob/HEAD/beam-cpp-example/pipeline.py) \n```\n## Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. \u00a0See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the \"License\"); you may not use this file except in compliance with# the License. \u00a0You may obtain a copy of the License at\n## \u00a0 \u00a0http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.#import argparseimport loggingimport osimport sysdef run(argv):\u00a0 # Import here to avoid __main__ session pickling issues.\u00a0 import io\u00a0 import itertools\u00a0 import matplotlib.pyplot as plt\u00a0 import collatz\u00a0 import apache_beam as beam\u00a0 from apache_beam.io import restriction_trackers\u00a0 from apache_beam.options.pipeline_options import PipelineOptions\u00a0 class RangeSdf(beam.DoFn, beam.RestrictionProvider):\u00a0 \u00a0 \"\"\"An SDF producing all the integers in the input range.\u00a0 \u00a0 This is preferable to beam.Create(range(...)) as it produces the integers\u00a0 \u00a0 dynamically rather than materializing them up front. \u00a0It is an SDF to do\u00a0 \u00a0 so with perfect dynamic sharding.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 def initial_restriction(self, desired_range):\u00a0 \u00a0 \u00a0 start, stop = desired_range\u00a0 \u00a0 \u00a0 return restriction_trackers.OffsetRange(start, stop)\u00a0 \u00a0 def restriction_size(self, _, restriction):\u00a0 \u00a0 \u00a0 return restriction.size()\u00a0 \u00a0 def create_tracker(self, restriction):\u00a0 \u00a0 \u00a0 return restriction_trackers.OffsetRestrictionTracker(restriction)\u00a0 \u00a0 def process(self, _, active_range=beam.DoFn.RestrictionParam()):\u00a0 \u00a0 \u00a0 for i in itertools.count(active_range.current_restriction().start):\u00a0 \u00a0 \u00a0 \u00a0 if active_range.try_claim(i):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 yield i\u00a0 \u00a0 \u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break\u00a0 class GenerateIntegers(beam.PTransform):\u00a0 \u00a0 def __init__(self, start, stop):\u00a0 \u00a0 \u00a0 self._start = start\u00a0 \u00a0 \u00a0 self._stop = stop\u00a0 \u00a0 def expand(self, p):\u00a0 \u00a0 \u00a0 return (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 p\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | beam.Create([(self._start, self._stop + 1)])\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | beam.ParDo(RangeSdf()))\u00a0 parser = argparse.ArgumentParser()\u00a0 parser.add_argument('--start', dest='start', type=int, default=1)\u00a0 parser.add_argument('--stop', dest='stop', type=int, default=10000)\u00a0 parser.add_argument('--output', default='./out.png')\u00a0 known_args, pipeline_args = parser.parse_known_args(argv)\u00a0 # Store this as a local to avoid capturing the full known_args.\u00a0 output_path = known_args.output\u00a0 with beam.Pipeline(options=PipelineOptions(pipeline_args)) as p:\u00a0 \u00a0 # Generate the integers from start to stop (inclusive).\u00a0 \u00a0 integers = p | GenerateIntegers(known_args.start, known_args.stop)\u00a0 \u00a0 # Run them through our C++ function, filtering bad records.\u00a0 \u00a0 # Requires apache beam 2.34 or later.\u00a0 \u00a0 stopping_times, bad_values = (\u00a0 \u00a0 \u00a0 \u00a0 integers\u00a0 \u00a0 \u00a0 \u00a0 | beam.Map(collatz.total_stopping_time).with_exception_handling(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 use_subprocess=True))\u00a0 \u00a0 # Write the bad values to a side channel.\u00a0 \u00a0 bad_values | 'WriteBadValues' >> beam.io.WriteToText(\u00a0 \u00a0 \u00a0 \u00a0 os.path.splitext(output_path)[0] + '-bad.txt')\u00a0 \u00a0 # Count the occurrence of each stopping time and normalize.\u00a0 \u00a0 total = known_args.stop - known_args.start + 1\u00a0 \u00a0 frequencies = (\u00a0 \u00a0 \u00a0 \u00a0 stopping_times\u00a0 \u00a0 \u00a0 \u00a0 | 'Aggregate' >> (beam.Map(lambda x: (x, 1)) | beam.CombinePerKey(sum))\u00a0 \u00a0 \u00a0 \u00a0 | 'Normalize' >> beam.MapTuple(lambda x, count: (x, count / total)))\u00a0 \u00a0 if known_args.stop <= 10:\u00a0 \u00a0 \u00a0 # Print out the results for debugging.\u00a0 \u00a0 \u00a0 frequencies | beam.Map(print)\u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 # Format and write them to a text file.\u00a0 \u00a0 \u00a0 (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frequencies\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | 'Format' >> beam.MapTuple(lambda count, freq: f'{count}, {freq}')\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | beam.io.WriteToText(os.path.splitext(output_path)[0] + '.txt'))\u00a0 \u00a0 # Define some helper functions.\u00a0 \u00a0 def make_scatter_plot(xy):\u00a0 \u00a0 \u00a0 x, y = zip(*xy)\u00a0 \u00a0 \u00a0 plt.plot(x, y, '.')\u00a0 \u00a0 \u00a0 png_bytes = io.BytesIO()\u00a0 \u00a0 \u00a0 plt.savefig(png_bytes, format='png')\u00a0 \u00a0 \u00a0 png_bytes.seek(0)\u00a0 \u00a0 \u00a0 return png_bytes.read()\u00a0 \u00a0 def write_to_path(path, content):\u00a0 \u00a0 \u00a0 \"\"\"Most Beam IOs write multiple elements to some kind of a container\u00a0 \u00a0 \u00a0 file (e.g. strings to lines of a text file, avro records to an avro file,\u00a0 \u00a0 \u00a0 etc.) \u00a0This function writes each element to its own file, given by path.\u00a0 \u00a0 \u00a0 \"\"\"\u00a0 \u00a0 \u00a0 # Write to a temporary path and to a rename for fault tolerence.\u00a0 \u00a0 \u00a0 tmp_path = path + '.tmp'\u00a0 \u00a0 \u00a0 fs = beam.io.filesystems.FileSystems.get_filesystem(path)\u00a0 \u00a0 \u00a0 with fs.create(tmp_path) as fout:\u00a0 \u00a0 \u00a0 \u00a0 fout.write(content)\u00a0 \u00a0 \u00a0 fs.rename([tmp_path], [path])\u00a0 \u00a0 (\u00a0 \u00a0 \u00a0 \u00a0 p\u00a0 \u00a0 \u00a0 \u00a0 # Create a PCollection with a single element.\u00a0 \u00a0 \u00a0 \u00a0 | 'CreateSingleton' >> beam.Create([None])\u00a0 \u00a0 \u00a0 \u00a0 # Process the single element with a Map function, passing the frequency\u00a0 \u00a0 \u00a0 \u00a0 # PCollection as a side input.\u00a0 \u00a0 \u00a0 \u00a0 # This will cause the normally distributed frequency PCollection to be\u00a0 \u00a0 \u00a0 \u00a0 # colocated and processed as a single unit, producing a single output.\u00a0 \u00a0 \u00a0 \u00a0 | 'MakePlot' >> beam.Map(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 lambda _,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 data: make_scatter_plot(data),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 data=beam.pvalue.AsList(frequencies))\u00a0 \u00a0 \u00a0 \u00a0 # Pair this with the desired filename.\u00a0 \u00a0 \u00a0 \u00a0 |\u00a0 \u00a0 \u00a0 \u00a0 'PairWithFilename' >> beam.Map(lambda content: (output_path, content))\u00a0 \u00a0 \u00a0 \u00a0 # And actually write it out, using MapTuple to split the tuple into args.\u00a0 \u00a0 \u00a0 \u00a0 | 'WriteToOutput' >> beam.MapTuple(write_to_path))if __name__ == '__main__':\u00a0 logging.getLogger().setLevel(logging.INFO)\u00a0 run(sys.argv)\n```## Set up your development environment\n- Use the [Apache Beam SDK](/dataflow/docs/guides/installing-beam-sdk#python) for Python.\n- Install the GMP library:```\napt-get install libgmp3-dev\n```\n- To install the dependencies, use the `requirements.txt` file.```\npip install -r requirements.txt\n```\n- To build the Python bindings, run the following command.```\npython setup.py build_ext --inplace\n```\nYou can customize the `requirements.txt` file from this tutorial. The starter file includes the following dependencies:\n [View on GitHub](https://github.com/GoogleCloudPlatform/dataflow-sample-applications/blob/HEAD/beam-cpp-example/requirements.txt) \n```\n## \u00a0 \u00a0Licensed to the Apache Software Foundation (ASF) under one or more# \u00a0 \u00a0contributor license agreements. \u00a0See the NOTICE file distributed with# \u00a0 \u00a0this work for additional information regarding copyright ownership.# \u00a0 \u00a0The ASF licenses this file to You under the Apache License, Version 2.0# \u00a0 \u00a0(the \"License\"); you may not use this file except in compliance with# \u00a0 \u00a0the License. \u00a0You may obtain a copy of the License at\n## \u00a0 \u00a0 \u00a0 http://www.apache.org/licenses/LICENSE-2.0\n## \u00a0 \u00a0Unless required by applicable law or agreed to in writing, software# \u00a0 \u00a0distributed under the License is distributed on an \"AS IS\" BASIS,# \u00a0 \u00a0WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# \u00a0 \u00a0See the License for the specific language governing permissions and# \u00a0 \u00a0limitations under the License.#apache-beam[gcp]==2.46.0cython==0.29.24pyparsing==2.4.2matplotlib==3.4.3\n```## Run the pipeline locallyRunning the pipeline locally is useful for testing. By running the pipeline locally, you can confirm that the pipeline runs and behaves as expected before you deploy the pipeline to a distributed environment.\nYou can run the pipeline locally by using the following command. This command outputs an image named `out.png` .\n```\npython pipeline.py\n```## Create the Google Cloud resourcesThis section explains how to create the following resources:- A Cloud Storage bucket to use as a temporary storage location and an output location.\n- A Docker container to package the pipeline code and dependencies.\n### Create a Cloud Storage bucketBegin by creating a Cloud Storage bucket using Google Cloud CLI. This bucket is used as a temporary storage location by the Dataflow pipeline.\nTo create the bucket, use the [gcloud storage buckets create command](/sdk/gcloud/reference/storage/buckets/create) :\n```\ngcloud storage buckets create gs://BUCKET_NAME --location=LOCATION\n```\nReplace the following:- : a name for your Cloud Storage bucket that meets the [bucket naming requirements](/storage/docs/buckets#naming) . Cloud Storage bucket names must be globally unique.\n- : the [location](/storage/docs/locations#available-locations) for the bucket.\n### Create and build a container imageYou can customize the Dockerfile from this tutorial. The starter file looks like the following:\n **Note:** You might need to update the Python version in the Dockerfile when you run the pipeline in a distributed environment. When you use a custom container image, the Python interpreter minor version in your image must match the version used at pipeline construction time.\n [View on GitHub](https://github.com/GoogleCloudPlatform/dataflow-sample-applications/blob/HEAD/beam-cpp-example/Dockerfile) \n```\nFROM apache/beam_python3.9_sdk:2.46.0# Install a C++ library.RUN apt-get updateRUN apt-get install -y libgmp3-dev# Install Python dependencies.COPY requirements.txt requirements.txtRUN pip install -r requirements.txt# Install the code and some python bindings.COPY pipeline.py pipeline.pyCOPY collatz.pyx collatz.pyxCOPY setup.py setup.pyRUN python setup.py install\n```\nThis Dockerfile contains the `FROM` , `COPY` , and `RUN` commands, which you can read about in the [Dockerfile reference](https://docs.docker.com/engine/reference/builder/) .- To upload artifacts, create an Artifact Registry repository. Each repository can contain artifacts for a single supported format.All repository content is encrypted using either Google-managed or customer-managed encryption keys. Artifact Registry uses Google-managed encryption keys by default and no configuration is required for this option.You must have at least [Artifact Registry Writer access](/artifact-registry/docs/access-control#permissions) to the repository.Run the following command to create a new repository. The command uses the `--async` flag and returns immediately, without waiting for the operation in progress to complete.```\ngcloud artifacts repositories create REPOSITORY \\\u00a0 \u00a0--repository-format=docker \\\u00a0 \u00a0--location=LOCATION \\\u00a0 \u00a0--async\n```Replace `` with a name for your repository. For each  repository location in a project, repository names must be unique.\n- Create the Dockerfile.For packages to be part of the Apache Beam container, you must specify them as part of the `requirements.txt` file. Ensure that you don't specify `apache-beam` as part of the `requirements.txt` file. The Apache Beam container already has `apache-beam` . **Note:** Dependencies installed with the Dockerfile are only available when launching the pipeline. Job dependencies must be included in the requirements file. As in the example, Apache Beam does not need to be in this file, and it is encouraged to keep it separate for faster launches.\n- Before you can push or pull images, configure Docker to authenticate requests for Artifact Registry. To set up authentication to Docker repositories, run the following command:```\ngcloud auth configure-docker LOCATION-docker.pkg.dev\n```The command updates your Docker configuration. You can now connect with Artifact Registry in your Google Cloud project to push images.\n- Build the [Docker](https://docs.docker.com/) image using your [Dockerfile](/build/docs/build-push-docker-image#build_using_dockerfile) with Cloud Build.Update path in the following command to match the Dockerfile that you created. This command builds the file and pushes it to your Artifact Registry repository.```\ngcloud builds submit --tag LOCATION-docker.pkg.dev/PROJECT_ID/REPOSITORY/dataflow/cpp_beam_container:latest .\n```\n## Package the code and dependencies in a Docker container\n- To run this pipeline in a distributed environment, package the code and dependencies into a docker container.```\ndocker build . -t cpp_beam_container\n```\n- After you package the code and dependencies, you can run the pipeline locally to test it.```\npython pipeline.py \\\u00a0 \u00a0--runner=PortableRunner \\\u00a0 \u00a0--job_endpoint=embed \\\u00a0 \u00a0--environment_type=DOCKER \\\u00a0 \u00a0--environment_config=\"docker.io/library/cpp_beam_container\"\n```This command writes the output inside the Docker image. To view the output, run the pipeline with the `--output` , and write the output to a Cloud Storage bucket. For example, run the following command.```\npython pipeline.py \\\u00a0 \u00a0--runner=PortableRunner \\\u00a0 \u00a0--job_endpoint=embed \\\u00a0 \u00a0--environment_type=DOCKER \\\u00a0 \u00a0--environment_config=\"docker.io/library/cpp_beam_container\" \\\u00a0 \u00a0--output=gs://BUCKET_NAME/out.png\n```\n## Run the pipelineYou can now run the Apache Beam pipeline in Dataflow by referring to the file with the pipeline code and passing the [parameters](/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options) required by the pipeline.\nIn your shell or terminal, run the pipeline with the Dataflow Runner.\n```\npython pipeline.py \\\u00a0 \u00a0 --runner=DataflowRunner \\\u00a0 \u00a0 --project=PROJECT_ID \\\u00a0 \u00a0 --region=REGION \\\u00a0 \u00a0 --temp_location=gs://BUCKET_NAME/tmp \\\u00a0 \u00a0 --sdk_container_image=\"LOCATION-docker.pkg.dev/PROJECT_ID/REPOSITORY/dataflow/cpp_beam_container:latest\" \\\u00a0 \u00a0 --experiment=use_runner_v2 \\\u00a0 \u00a0 --output=gs://BUCKET_NAME/out.png\n```\nAfter you execute the command to run the pipeline, the Dataflow returns a Job ID with the job status **Queued** . It might take several minutes before the job status reaches **Running** and you can access the [job graph](/dataflow/docs/guides/job-graph) .## View your resultsView data written to your Cloud Storage bucket. Use the [gcloud storage ls command](/sdk/gcloud/reference/storage/ls) to list the contents at the top level of your bucket:\n```\ngcloud storage ls gs://BUCKET_NAME\n```\nIf successful, the command returns a message similar to:\n```\ngs://BUCKET_NAME/out.png\n```## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the projectThe easiest way to eliminate billing is to delete the Google Cloud project that you created for the tutorial.- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Delete the individual resourcesIf you want to reuse the project, then delete the resources that you created for the tutorial.\n- Delete the Artifact Registry repository.```\ngcloud artifacts repositories delete REPOSITORY \\\u00a0 \u00a0--location=LOCATION --async\n```\n- Delete the Cloud Storage bucket. This bucket alone does not incur any charges. **Caution:** The following command also deletes all objects in the bucket. These objects cannot be recovered.```\ngcloud storage rm gs://BUCKET_NAME --recursive\n```\n- Revoke the roles that you granted to the user-managed worker service account.  Run the following command once for each of the following IAM roles:- `roles/dataflow.admin`\n- `roles/dataflow.worker`\n- `roles/storage.objectAdmin`\n- `roles/artifactregistry.reader`\n```\ngcloud projects remove-iam-policy-binding PROJECT_ID \\\u00a0 --member=serviceAccount:parallelpipeline@PROJECT_ID.iam.gserviceaccount.com \\\u00a0 --role=SERVICE_ACCOUNT_ROLE\n```\n- Optional: Revoke the authentication credentials that you created, and delete the local   credential file.```\ngcloud auth application-default revoke\n```\n- Optional: Revoke credentials from the gcloud CLI.```\ngcloud auth revoke\n```\n## What's next\n- View [the sample application on GitHub](https://github.com/GoogleCloudPlatform/dataflow-sample-applications/tree/master/beam-cpp-example) .\n- [Use custom containers in Dataflow](/dataflow/docs/guides/using-custom-containers) .\n- Learn more about using [container environments](https://beam.apache.org/documentation/runtime/environments/) with Apache Beam.\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Dataflow"}