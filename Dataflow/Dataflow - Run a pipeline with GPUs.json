{"title": "Dataflow - Run a pipeline with GPUs", "url": "https://cloud.google.com/dataflow/docs/gpu/use-gpus", "abstract": "# Dataflow - Run a pipeline with GPUs\n**Note:** The following considerations apply to this GA offering:- Jobs that use GPUs incur charges as specified in the Dataflow [pricing page](/dataflow/pricing) .\n- To use GPUs, your Dataflow job must use [Dataflow Runner v2](/dataflow/docs/runner-v2) .\nThis page explains how to run an Apache Beam pipeline on Dataflow with GPUs. Jobs that use GPUs incur charges as specified in the Dataflow [pricing page](/dataflow/pricing) .\nFor more information about using GPUs with Dataflow, see [Dataflow support for GPUs](/dataflow/docs/gpu/gpu-support) . For more information about the developer workflow for building pipelines using GPUs, see [About GPUs with Dataflow](/dataflow/docs/gpu/develop-with-gpus) .\n", "content": "## Use Apache Beam notebooks\nIf you already have a pipeline that you want to run with GPUs on Dataflow, you can skip this section.\nApache Beam notebooks offer a convenient way to prototype and iteratively develop your pipeline with GPUs without setting up a development environment. To get started, read the [Developing with Apache Beam notebooks](/dataflow/docs/guides/interactive-pipeline-development) guide, launch an Apache Beam notebooks instance, and follow the example notebook **Use GPUs with Apache Beam** .\n## Provision GPU quota\nGPU devices are subject to the quota availability of your Google Cloud project. [Request GPU quota](/compute/quotas#gpu_quota) in the region of your choice.\n## Install GPU drivers\nTo install NVIDIA drivers on the Dataflow workers, append `install-nvidia-driver` to the [worker_accelerator service option](/dataflow/docs/reference/service-options) .\nWhen you specify the `install-nvidia-driver` option, Dataflow installs NVIDIA drivers onto the Dataflow workers by using the [cos-extensions](/container-optimized-os/docs/how-to/run-gpus#install) utility provided by Container-Optimized OS. By specifying `install-nvidia-driver` , you agree to accept the NVIDIA license agreement.\nBinaries and libraries provided by the NVIDIA driver installer are mounted into the container that runs pipeline user code at `/usr/local/nvidia/` .\nThe [GPU driver version](/container-optimized-os/docs/how-to/run-gpus#identifying_gpu_driver_version) depends on the Container-Optimized OS version used by Dataflow. To find the GPU driver version for a given Dataflow job, in the [Dataflow Step Logs](/dataflow/docs/guides/logging) of your job, search for `GPU driver` .\n**Caution:** Don't install GPU drivers in your custom container images, because those drivers might conflict with the driver version used by Dataflow.\n## Build a custom container image\nTo interact with the GPUs, you might need additional NVIDIA software, such as [GPU-accelerated libraries](https://developer.nvidia.com/gpu-accelerated-libraries) and the [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) . Supply these libraries in the Docker container running user code.\nTo customize the container image, supply an image that fulfills the Apache Beam SDK container image contract and has the necessary GPU libraries.\nTo provide a custom container image, use [Dataflow Runner v2](/dataflow/docs/runner-v2) and supply the container image by using the `sdk_container_image` pipeline option. If you're using Apache Beam version 2.29.0 or earlier, use the `worker_harness_container_image` pipeline option. For more information, see [Use custom containers](/dataflow/docs/guides/using-custom-containers) .\nTo build a custom container image, use one of the following two approaches:\n- [Use an existing image configured for GPU usage](#existing-image) \n- [Use an Apache Beam container image](#container-image) \n### Use an existing image configured for GPU usage\nYou can build a Docker image that fulfills the Apache Beam SDK container contract from an existing base image that is preconfigured for GPU usage. For example, [TensorFlow Docker images](https://www.tensorflow.org/install/docker) , and [NVIDIA container images](https://ngc.nvidia.com/catalog/containers/) are preconfigured for GPU usage.\nA sample Dockerfile that builds on TensorFlow Docker image with Python 3.6 looks like the following example:\n```\nARG BASE=tensorflow/tensorflow:2.5.0-gpu\nFROM $BASE\n# Check that the chosen base image provides the expected version of Python interpreter.\nARG PY_VERSION=3.6\nRUN [[ $PY_VERSION == `python -c 'import sys; print(\"%s.%s\" % sys.version_info[0:2])'` ]] \\\n || { echo \"Could not find Python interpreter or Python version is different from ${PY_VERSION}\"; exit 1; }\nRUN pip install --upgrade pip \\\n && pip install --no-cache-dir apache-beam[gcp]==2.29.0 \\\n # Verify that there are no conflicting dependencies.\n && pip check\n# Copy the Apache Beam worker dependencies from the Beam Python 3.6 SDK image.\nCOPY --from=apache/beam_python3.6_sdk:2.29.0 /opt/apache/beam /opt/apache/beam\n# Apache Beam worker expects pip at /usr/local/bin/pip by default.\n# Some images have pip in a different location. If necessary, make a symlink.\n# This line can be omitted in Beam 2.30.0 and later versions.\nRUN [[ `which pip` == \"/usr/local/bin/pip\" ]] || ln -s `which pip` /usr/local/bin/pip\n# Set the entrypoint to Apache Beam SDK worker launcher.\nENTRYPOINT [ \"/opt/apache/beam/boot\" ]\n```\nWhen you use [TensorFlow Docker images](https://www.tensorflow.org/install/docker) , use TensorFlow 2.5.0 or later. Earlier TensorFlow Docker images install the `tensorflow-gpu` package instead of the `tensorflow` package. The distinction is not important after TensorFlow 2.1.0 release, but several downstream packages, such as `tfx` , require the `tensorflow` package.\nLarge container sizes slow down the worker startup time. This performance change might occur when you use containers like [Deep Learning Containers](/deep-learning-containers/docs/choosing-container) .\nIf you have strict requirements for the Python version, you can build your image from an NVIDIA base image that has the necessary GPU libraries. Then, install the Python interpreter.\nThe following example demonstrates how to select an NVIDIA image that doesn't include the Python interpreter from the [CUDA container image catalog](https://ngc.nvidia.com/catalog/containers/nvidia:cuda) . Adjust the example to install the needed version of Python 3 and pip. The example uses TensorFlow. Therefore, when choosing an image, the CUDA and cuDNN versions in the base image satisfy the [requirements for the TensorFlow version](https://www.tensorflow.org/install/source#gpu) .\nA sample Dockerfile looks like the following:\n```\n# Select an NVIDIA base image with needed GPU stack from https://ngc.nvidia.com/catalog/containers/nvidia:cuda\nFROM nvidia/cuda:11.0.3-cudnn8-runtime-ubuntu20.04\nRUN \\\n # Add Deadsnakes repository that has a variety of Python packages for Ubuntu.\n # See: https://launchpad.net/~deadsnakes/+archive/ubuntu/ppa\n apt-key adv --keyserver keyserver.ubuntu.com --recv-keys F23C5A6CF475977595C89F51BA6932366A755776 \\\n && echo \"deb http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal main\" >> /etc/apt/sources.list.d/custom.list \\\n && echo \"deb-src http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal main\" >> /etc/apt/sources.list.d/custom.list \\\n && apt-get update \\\n && apt-get install -y curl \\\n  python3.8 \\\n  # With python3.8 package, distutils need to be installed separately.\n  python3-distutils \\\n && rm -rf /var/lib/apt/lists/* \\\n && update-alternatives --install /usr/bin/python python /usr/bin/python3.8 10 \\\n && curl https://bootstrap.pypa.io/get-pip.py | python \\\n && pip install --upgrade pip \\\n # Install Apache Beam and Python packages that will interact with GPUs.\n && pip install --no-cache-dir apache-beam[gcp]==2.29.0 tensorflow==2.4.0 \\\n # Verify that there are no conflicting dependencies.\n && pip check\n# Copy the Apache Beam worker dependencies from the Beam Python 3.8 SDK image.\nCOPY --from=apache/beam_python3.8_sdk:2.29.0 /opt/apache/beam /opt/apache/beam\n# Set the entrypoint to Apache Beam SDK worker launcher.\nENTRYPOINT [ \"/opt/apache/beam/boot\" ]\n```\nOn some OS distributions, it might be difficult to install specific Python versions by using the OS package manager. In this case, install the Python interpreter with tools like Miniconda or `pyenv` .\nA sample Dockerfile looks like the following:\n```\nFROM nvidia/cuda:11.0.3-cudnn8-runtime-ubuntu20.04\n# The Python version of the Dockerfile must match the Python version you use\n# to launch the Dataflow job.\nARG PYTHON_VERSION=3.8\n# Update PATH so we find our new Conda and Python installations.\nENV PATH=/opt/python/bin:/opt/conda/bin:$PATH\nRUN apt-get update \\\n && apt-get install -y wget \\\n && rm -rf /var/lib/apt/lists/* \\\n # The NVIDIA image doesn't come with Python pre-installed.\n # We use Miniconda to install the Python version of our choice.\n && wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \\\n && bash Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda \\\n && rm Miniconda3-latest-Linux-x86_64.sh \\\n # Create a new Python environment with needed version, and install pip.\n && conda create -y -p /opt/python python=$PYTHON_VERSION pip \\\n # Remove unused Conda packages, install necessary Python packages via pip\n # to avoid mixing packages from pip and Conda.\n && conda clean -y --all --force-pkgs-dirs \\\n && pip install --upgrade pip \\\n # Install Apache Beam and Python packages that will interact with GPUs.\n && pip install --no-cache-dir apache-beam[gcp]==2.29.0 tensorflow==2.4.0 \\\n # Verify that there are no conflicting dependencies.\n && pip check \\\n # Apache Beam worker expects pip at /usr/local/bin/pip by default.\n # You can omit this line when using Beam 2.30.0 and later versions.\n && ln -s $(which pip) /usr/local/bin/pip\n# Copy the Apache Beam worker dependencies from the Apache Beam SDK for Python 3.8 image.\nCOPY --from=apache/beam_python3.8_sdk:2.29.0 /opt/apache/beam /opt/apache/beam\n# Set the entrypoint to Apache Beam SDK worker launcher.\nENTRYPOINT [ \"/opt/apache/beam/boot\" ]\n```\n**Caution:** We recommend that you install all Python packages in custom container images using `pip install` and not use `conda install` . Installing Python packages from both pip and Conda repositories might cause interoperability issues, especially for Python packages that include compiled code. In addition, Apache Beam does not fully support Conda environments. For more information, see [Using Pip in a Conda Environment](https://www.anaconda.com/blog/using-pip-in-a-conda-environment) .\n### Use an Apache Beam container image\nYou can configure a container image for GPU usage without using preconfigured images. This approach is only recommended when preconfigured images don't work for you. To set up your own container image, you need to select compatible libraries and configure their execution environment.\nA sample Dockerfile looks like the following:\n```\nFROM apache/beam_python3.7_sdk:2.24.0\nENV INSTALLER_DIR=\"/tmp/installer_dir\"\n# The base image has TensorFlow 2.2.0, which requires CUDA 10.1 and cuDNN 7.6.\n# You can download cuDNN from NVIDIA website\n# https://developer.nvidia.com/cudnn\nCOPY cudnn-10.1-linux-x64-v7.6.0.64.tgz $INSTALLER_DIR/cudnn.tgz\nRUN \\\n # Download CUDA toolkit.\n wget -q -O $INSTALLER_DIR/cuda.run https://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda_10.1.243_418.87.00_linux.run && \\\n # Install CUDA toolkit. Print logs upon failure.\n sh $INSTALLER_DIR/cuda.run --toolkit --silent || (egrep '^\\[ERROR\\]' /var/log/cuda-installer.log && exit 1) && \\\n # Install cuDNN.\n mkdir $INSTALLER_DIR/cudnn && \\\n tar xvfz $INSTALLER_DIR/cudnn.tgz -C $INSTALLER_DIR/cudnn && \\\n cp $INSTALLER_DIR/cudnn/cuda/include/cudnn*.h /usr/local/cuda/include && \\\n cp $INSTALLER_DIR/cudnn/cuda/lib64/libcudnn* /usr/local/cuda/lib64 && \\\n chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn* && \\\n rm -rf $INSTALLER_DIR\n# A volume with GPU drivers will be mounted at runtime at /usr/local/nvidia.\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/nvidia/lib64:/usr/local/cuda/lib64\n```\nDriver libraries in `/usr/local/nvidia/lib64` must be discoverable in the container as shared libraries. To make the driver libraries discoverable, configure the `LD_LIBRARY_PATH` environment variable.\nIf you use TensorFlow, you must choose a compatible combination of CUDA Toolkit and cuDNN versions. For more information, read [Software requirements](https://www.tensorflow.org/install/gpu#software_requirements) and [Tested build configurations](https://www.tensorflow.org/install/source#tested_build_configurations) .\n## Select the type and number of GPUs for Dataflow workers\nTo configure the type and number of GPUs to attach to Dataflow workers, use the [worker_accelerator service option](/dataflow/docs/reference/service-options) . Select the type and number of GPUs based on your use case and how you plan to use the GPUs in your pipeline.\nFor a list of GPU types that are supported with Dataflow, see [Dataflow support for GPUs](/dataflow/docs/gpu/gpu-support#availability) .\n## Run your job with GPUs\nThe considerations for running a Dataflow job with GPUs include the following:\n- Because GPU containers are typically large, to avoid [running out of disk space](/dataflow/docs/guides/common-errors#no-space-left) , do the following:- Increase the default [boot disk size](/dataflow/docs/reference/pipeline-options#worker-level_options) to 50 gigabytes or more.\n- Consider how many processes concurrently use the same GPU on a worker VM. Then, decide whether you want to limit the GPU to a single process or let multiple processes use the GPU.- If one Apache Beam SDK process can use most of the available GPU memory, for example by loading a large model onto a GPU, you might want to configure workers to use a single process by setting the pipeline option`--experiments=no_use_multiple_sdk_containers`. Alternatively, use workers with one vCPU by using a [custom machine type](/compute/docs/instances/creating-instance-with-custom-machine-type) , such as`n1-custom-1-NUMBER_OF_MB`or`n1-custom-1-NUMBER_OF_MB-ext`, for [extended memory](/compute/docs/instances/creating-instance-with-custom-machine-type#extendedmemory) . For more information, see [Use a machine type with more memory per vCPU](/dataflow/docs/guides/troubleshoot-oom#memory-per-vcpu) .\n- If the GPU is shared by multiple processes, enable concurrent processing on a shared GPU by using the [NVIDIA Multi-Processing Service (MPS)](/dataflow/docs/gpu/use-nvidia-mps) .\nFor background information, see [GPUs and worker parallelism](/dataflow/docs/gpu/develop-with-gpus#parallelism) .\n**Caution:** The Apache Beam version and Python interpreter minor version in the pipeline launch environment and in your container image must be the same. Verify the versions in your local environment. Alternatively, start the pipeline from the container created from your image or create a Flex template that launches the pipeline using your custom image. For more information, see: - - [Use custom containers in Dataflow](/dataflow/docs/guides/using-custom-containers) - - [Configure Flex Templates](/dataflow/docs/guides/templates/configuring-flex-templates#using_custom_container_images) To run a Dataflow job with GPUs, use the following command:\n```\npython PIPELINE \\\u00a0 --runner \"DataflowRunner\" \\\u00a0 --project \"PROJECT\" \\\u00a0 --temp_location \"gs://BUCKET/tmp\" \\\u00a0 --region \"REGION\" \\\u00a0 --worker_harness_container_image \"IMAGE\" \\\u00a0 --disk_size_gb \"DISK_SIZE_GB\" \\\u00a0 --dataflow_service_options \"worker_accelerator=type:GPU_TYPE;count:GPU_COUNT;install-nvidia-driver\" \\\u00a0 --experiments \"use_runner_v2\"\n```\nReplace the following:\n- : your pipeline source code file\n- : the Google Cloud project name\n- : the Cloud Storage bucket\n- : a Dataflow region, for example,`us-central1`. Select a `REGION` that has zones that [support the `GPU_TYPE`](/dataflow/docs/gpu/gpu-support#availability). Dataflow automatically assigns workers to a zone with GPUs in this region.\n- : the Artifact Registry path for your Docker image\n- : Size of the boot disk for each worker VM, for example,`50`\n- : an available [GPU type](/dataflow/docs/gpu/gpu-support#availability) , for example,`nvidia-tesla-t4`.\n- : number of GPUs to attach to each worker VM, for example,`1`## Verify your Dataflow job\nTo confirm that the job uses worker VMs with GPUs, follow these steps:\n- Verify that Dataflow workers for the job have started.\n- While a job is running, find a worker VM associated with the job.- In the **Search Products and Resources** prompt, paste the **Job ID** .\n- Select the Compute Engine VM instance associated with the job.You can also find list of all running instances in the Compute Engine console.\n- In the Google Cloud console, go to the **VM instances** page. [Go to VM instances](https://console.cloud.google.com/compute/instances) \n- Click **VM instance details** .\n- Verify that details page has a **GPUs** section and that your GPUs are attached.\nIf your job didn't launch with GPUs, check that the `worker_accelerator` service option is configured properly and visible in the Dataflow monitoring interface in **dataflow_service_options** . The order of tokens in the accelerator metadata is important.\nFor example, a `dataflow_service_options` pipeline option in the Dataflow monitoring interface might look like the following:\n```\n['worker_accelerator=type:nvidia-tesla-t4;count:1;install-nvidia-driver', ...]\n```\n## View GPU utilization\nTo see GPU utilization on the worker VMs, follow these steps:\n- In the Google Cloud console, go to **Monitoring** or use the following button: [Go to Monitoring](https://console.cloud.google.com/monitoring) \n- In the Monitoring navigation pane, click **Metrics Explorer** .\n- For **Resource Type** , specify `Dataflow Job` . For the metric, specify `GPU utilization` or `GPU memory utilization` , depending on which metric you want to monitor.\nFor more information, see [Metrics Explorer](/monitoring/charts/metrics-explorer) .\n## Enable NVIDIA Multi-Processing Service\nOn Python pipelines that run on workers with more than one vCPU, you can improve concurrency for GPU operations by enabling the NVIDIA Multi-Process Service (MPS). For more information and steps to use MPS, see [Improve performance on a shared GPU by using NVIDIA MPS](/dataflow/docs/gpu/use-nvidia-mps) .\n## Use GPUs with Dataflow Prime\n[Dataflow Prime](/dataflow/docs/guides/enable-dataflow-prime) lets you request accelerators for a specific step of your pipeline. To use GPUs with Dataflow Prime, don't use the `--dataflow-service_options=worker_accelerator` pipeline option. Instead, request the GPUs with the `accelerator` resource hint. For more information, see [Use resource hints](/dataflow/docs/guides/right-fitting#using_resource_hints) .\n## Troubleshoot your Dataflow job\nIf you run into problems running your Dataflow job with GPUs, see [Troubleshoot your Dataflow GPU job](/dataflow/docs/gpu/troubleshoot-gpus) .\n## What's next\n- Learn more about [GPU support on Dataflow](/dataflow/docs/gpu/gpu-support) .\n- Run your machine learning inference pipeline with the [NVIDIA L4 GPU type](/dataflow/docs/gpu/use-l4-gpus) .\n- Work through [Processing Landsat satellite images with GPUs](/dataflow/docs/samples/satellite-images-gpus) .", "guide": "Dataflow"}