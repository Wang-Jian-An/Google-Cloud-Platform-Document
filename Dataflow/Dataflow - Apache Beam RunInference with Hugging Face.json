{"title": "Dataflow - Apache Beam RunInference with Hugging Face", "url": "https://cloud.google.com/dataflow/docs/notebooks/run_inference_huggingface?hl=zh-cn", "abstract": "# Dataflow - Apache Beam RunInference with Hugging Face\n| 0     | 1      |\n|:--------------------|:----------------------|\n| Run in Google Colab | View source on GitHub |\nThis notebook shows how to use models from [Hugging Face](https://huggingface.co/) and [Hugging Face pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) in Apache Beam pipelines that uses the [RunInference](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.RunInference) transform.\nApache Beam has built-in support for Hugging Face model handlers. Hugging Face has three model handlers:\n- Use the [HuggingFacePipelineModelHandler](https://github.com/apache/beam/blob/926774dd02be5eacbe899ee5eceab23afb30abca/sdks/python/apache_beam/ml/inference/huggingface_inference.py#L567) model handler to run inference with [Hugging Face pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines#pipelines) .\n- Use the [HuggingFaceModelHandlerKeyedTensor](https://github.com/apache/beam/blob/926774dd02be5eacbe899ee5eceab23afb30abca/sdks/python/apache_beam/ml/inference/huggingface_inference.py#L208) model handler to run inference with models that uses keyed tensors as inputs. For example, you might use this model handler with language modeling tasks.\n- Use the [HuggingFaceModelHandlerTensor](https://github.com/apache/beam/blob/926774dd02be5eacbe899ee5eceab23afb30abca/sdks/python/apache_beam/ml/inference/huggingface_inference.py#L392) model handler to run inference with models that uses tensor inputs, such as`tf.Tensor`or`torch.Tensor`.\nFor more information about using RunInference, see [Get started with AI/ML pipelines](https://beam.apache.org/documentation/ml/overview/) in the Apache Beam documentation.\n", "content": "## Install dependencies\nInstall both Apache Beam and the required dependencies for Hugging Face.\n```\npip install torch --quietpip install tensorflow --quietpip install transformers==4.30.0 --quietpip install apache-beam[gcp]>=2.50 --quiet\n```\n```\nfrom typing import Dictfrom typing import Iterablefrom typing import Tupleimport tensorflow as tfimport torchfrom transformers import AutoTokenizerfrom transformers import TFAutoModelForMaskedLMimport apache_beam as beamfrom apache_beam.ml.inference.base import KeyedModelHandlerfrom apache_beam.ml.inference.base import PredictionResultfrom apache_beam.ml.inference.base import RunInferencefrom apache_beam.ml.inference.huggingface_inference import HuggingFacePipelineModelHandlerfrom apache_beam.ml.inference.huggingface_inference import HuggingFaceModelHandlerKeyedTensorfrom apache_beam.ml.inference.huggingface_inference import HuggingFaceModelHandlerTensorfrom apache_beam.ml.inference.huggingface_inference import PipelineTask\n```\n## Use RunInference with Hugging Face pipelines\nYou can use [Hugging Face pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines#pipelines) with `RunInference` by using the `HuggingFacePipelineModelHandler` model handler. Similar to the Hugging Face pipelines, to instantiate the model handler, the model handler needs either the pipeline `task` or the `model` that defines the task. To pass any optional arguments to load the pipeline, use `load_pipeline_args` . To pass the optional arguments for inference, use `inference_args` .\nYou can define the pipeline task in one of the following two ways:\n- In the form of string, for example`\"translation\"`. This option is similar to how the pipeline task is defined when using Hugging Face.\n- In the form of a [PipelineTask](https://github.com/apache/beam/blob/ac936b0b89a92d836af59f3fc04f5733ad6819b3/sdks/python/apache_beam/ml/inference/huggingface_inference.py#L75) enum object defined in Apache Beam, such as`PipelineTask.Translation`.\n### Create a model handler\nThis example demonstrates a task that translates text from English to Spanish.\n```\nmodel_handler = HuggingFacePipelineModelHandler(\u00a0 \u00a0 task=PipelineTask.Translation_XX_to_YY,\u00a0 \u00a0 model = \"google/flan-t5-small\",\u00a0 \u00a0 load_pipeline_args={'framework': 'pt'},\u00a0 \u00a0 inference_args={'max_length': 200})\n```\n### Define the input examples\nUse this code to define the input examples.\n```\ntext = [\"translate English to Spanish: How are you doing?\",\u00a0 \u00a0 \u00a0 \u00a0 \"translate English to Spanish: This is the Apache Beam project.\"]\n```\n### Postprocess the results\nThe output from the `RunInference` transform is a `PredictionResult` object. Use that output to extract inferences, and then format and print the results.\n```\nclass FormatOutput(beam.DoFn):\u00a0 \"\"\"\u00a0 Extract the results from PredictionResult and print the results.\u00a0 \"\"\"\u00a0 def process(self, element):\u00a0 \u00a0 example = element.example\u00a0 \u00a0 translated_text = element.inference[0]['translation_text']\u00a0 \u00a0 print(f'Example: {example}')\u00a0 \u00a0 print(f'Translated text: {translated_text}')\u00a0 \u00a0 print('-' * 80)\n```\n### Run the pipeline\nUse the following code to run the pipeline.\n```\nwith beam.Pipeline() as beam_pipeline:\u00a0 examples = (\u00a0 \u00a0 \u00a0 beam_pipeline\u00a0 \u00a0 \u00a0 | \"CreateExamples\" >> beam.Create(text)\u00a0 )\u00a0 inferences = (\u00a0 \u00a0 \u00a0 examples\u00a0 \u00a0 \u00a0 | \"RunInference\" >> RunInference(model_handler)\u00a0 \u00a0 \u00a0 | \"Print\" >> beam.ParDo(FormatOutput())\u00a0 )\n```\n```\nExample: translate English to Spanish: How are you doing?\nTranslated text: C\u00f3mo est\u00e1 acerca?\n-------------------------------------------------------------------------------Example: translate English to Spanish: This is the Apache Beam project.\nTranslated text: Esto es el proyecto Apache Beam.\n-------------------------------------------------------------------------------\n```\n## RunInference with a pretrained model from Hugging Face Hub\nTo use pretrained models directly from [Hugging Face Hub](https://huggingface.co/docs/hub/models) , use either the `HuggingFaceModelHandlerTensor` model handler or the `HuggingFaceModelHandlerKeyedTensor` model handler. Which model handler you use depends on your input type:\n- Use`HuggingFaceModelHandlerKeyedTensor`to run inference with models that uses keyed tensors as inputs.\n- Use`HuggingFaceModelHandlerTensor`to run inference with models that uses tensor inputs, such as`tf.Tensor`or`torch.Tensor`.\nWhen you construct your pipeline, you might also need to use the following items:\n- Use the`load_model_args`to provide optional arguments to load the model.\n- Use the`inference_args`argument to do the inference.\n- For TensorFlow models, specify the`framework='tf'`.\n- For PyTorch models, specify the`framework='pt'`.\nThe following language modeling task predicts the masked word in a sentence.\n### Create a model handler\nThis example shows a masked language modeling task. These models take keyed tensors as inputs.\n```\nmodel_handler = HuggingFaceModelHandlerKeyedTensor(\u00a0 \u00a0 model_uri=\"stevhliu/my_awesome_eli5_mlm_model\",\u00a0 \u00a0 model_class=TFAutoModelForMaskedLM,\u00a0 \u00a0 framework='tf',\u00a0 \u00a0 load_model_args={'from_pt': True},\u00a0 \u00a0 max_batch_size=1)\n```\n### Define the input examples\nUse this code to define the input examples.\n```\ntext = ['The capital of France is Paris .',\u00a0 \u00a0 'It is raining cats and dogs .',\u00a0 \u00a0 'He looked up and saw the sun and stars .',\u00a0 \u00a0 'Today is Monday and tomorrow is Tuesday .',\u00a0 \u00a0 'There are 5 coconuts on this palm tree .']\n```\n### Preprocess the input\nEdit the given input to replace the last word with a `<mask>` . Then, tokenize the input for doing inference.\n```\ndef add_mask_to_last_word(text: str) -> Tuple[str, str]:\u00a0 \"\"\"Replace the last word of sentence with <mask> and return\u00a0 the original sentence and the masked sentence.\"\"\"\u00a0 text_list = text.split()\u00a0 masked = ' '.join(text_list[:-2] + ['<mask>' + text_list[-1]])\u00a0 return text, maskedtokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_eli5_mlm_model\")def tokenize_sentence(\u00a0 \u00a0 text_and_mask: Tuple[str, str],\u00a0 \u00a0 tokenizer) -> Tuple[str, Dict[str, tf.Tensor]]:\u00a0 \"\"\"Convert string examples to tensors.\"\"\"\u00a0 text, masked_text = text_and_mask\u00a0 tokenized_sentence = tokenizer.encode_plus(\u00a0 \u00a0 \u00a0 masked_text, return_tensors=\"tf\")\u00a0 # Workaround to manually remove batch dim until we have the feature to\u00a0 # add optional batching flag.\u00a0 # TODO(https://github.com/apache/beam/issues/21863): Remove when optional\u00a0 # batching flag added\u00a0 return text, {\u00a0 \u00a0 \u00a0 k: tf.squeeze(v)\u00a0 \u00a0 \u00a0 for k, v in dict(tokenized_sentence).items()\u00a0 }\n```\n### Postprocess the results\nExtract the result from the `PredictionResult` object. Then, format the output to print the actual sentence and the word predicted for the last word in the sentence.\n```\nclass PostProcessor(beam.DoFn):\u00a0 \"\"\"Processes the PredictionResult to get the predicted word.\u00a0 The logits are the output of the BERT Model. To get the word with the highest\u00a0 probability of being the masked word, take the argmax.\u00a0 \"\"\"\u00a0 def __init__(self, tokenizer):\u00a0 \u00a0 super().__init__()\u00a0 \u00a0 self.tokenizer = tokenizer\u00a0 def process(self, element: Tuple[str, PredictionResult]) -> Iterable[str]:\u00a0 \u00a0 text, prediction_result = element\u00a0 \u00a0 inputs = prediction_result.example\u00a0 \u00a0 logits = prediction_result.inference['logits']\u00a0 \u00a0 mask_token_index = tf.where(inputs[\"input_ids\"] == self.tokenizer.mask_token_id)[0]\u00a0 \u00a0 predicted_token_id = tf.math.argmax(logits[mask_token_index[0]], axis=-1)\u00a0 \u00a0 decoded_word = self.tokenizer.decode(predicted_token_id)\u00a0 \u00a0 print(f\"Actual Sentence: {text}\\nPredicted last word: {decoded_word}\")\u00a0 \u00a0 print('-' * 80)\n```\n### Run the pipeline\nUse the following code to run the pipeline.\n```\nwith beam.Pipeline() as beam_pipeline:\u00a0 tokenized_examples = (\u00a0 \u00a0 \u00a0 beam_pipeline\u00a0 \u00a0 \u00a0 | \"CreateExamples\" >> beam.Create(text)\u00a0 \u00a0 \u00a0 | 'AddMask' >> beam.Map(add_mask_to_last_word)\u00a0 \u00a0 \u00a0 | 'TokenizeSentence' >>\u00a0 \u00a0 \u00a0 beam.Map(lambda x: tokenize_sentence(x, tokenizer)))\u00a0 result = (\u00a0 \u00a0 \u00a0 tokenized_examples\u00a0 \u00a0 \u00a0 | \"RunInference\" >> RunInference(KeyedModelHandler(model_handler))\u00a0 \u00a0 \u00a0 | \"PostProcess\" >> beam.ParDo(PostProcessor(tokenizer))\u00a0 )\n```\n```\nActual Sentence: The capital of France is Paris .\nPredicted last word: Paris\n-------------------------------------------------------------------------------Actual Sentence: It is raining cats and dogs .\nPredicted last word: dogs\n-------------------------------------------------------------------------------Actual Sentence: He looked up and saw the sun and stars .\nPredicted last word: stars\n-------------------------------------------------------------------------------Actual Sentence: Today is Monday and tomorrow is Tuesday .\nPredicted last word: Tuesday\n-------------------------------------------------------------------------------Actual Sentence: There are 5 coconuts on this palm tree .\nPredicted last word: tree\n-------------------------------------------------------------------------------\n```", "guide": "Dataflow"}