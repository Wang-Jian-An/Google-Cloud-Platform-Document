{"title": "Dataflow - Use Cloud Monitoring for Dataflow pipelines", "url": "https://cloud.google.com/dataflow/docs/guides/using-cloud-monitoring", "abstract": "# Dataflow - Use Cloud Monitoring for Dataflow pipelines\nCloud Monitoring provides powerful logging and diagnostics. Dataflow integration with Monitoring lets you access Dataflow job metrics such as job status, element counts, system lag (for streaming jobs), and user counters from the Monitoring dashboards. You can also use Monitoring alerts to notify you of various conditions, such as long streaming system lag or failed jobs.\n", "content": "## Before you begin\nFollow the [Java quickstart](/dataflow/docs/quickstarts/create-pipeline-java) , [Python quickstart](/dataflow/docs/quickstarts/create-pipeline-python) , or [Go quickstart](/dataflow/docs/quickstarts/create-pipeline-go) to set up your Dataflow project. Then, [construct and run your pipeline](/dataflow/pipelines/constructing-your-pipeline) .\n**Note:** If your [metrics scope](/monitoring/settings#concept-scope) is configured with a [VPC Service Controls](/vpc-service-controls) Service Perimeter for the Monitoring API, some metrics aren't available for pipeline Compute Engine workers outside the perimeter. To learn more about Service Perimeters, see the [VPC Service Controls Service Perimeter configuration documentation](/vpc-service-controls/docs/service-perimeters) .\nTo see logs in Metrics Explorer, the [worker service account](/dataflow/docs/concepts/security-and-permissions#worker-service-account) must have the `roles/monitoring.metricWriter` role.\n## Custom metrics\nAny metric that you define in your Apache Beam pipeline is reported by Dataflow to Monitoring as a custom metric. Apache Beam has [three types of pipeline metrics](https://beam.apache.org/documentation/programming-guide/#metrics) : `Counter` , `Distribution` , and `Gauge` .\n- Dataflow reports`Counter`and`Distribution`metrics to Monitoring.\n- `Distribution`is reported as four submetrics suffixed with`_MAX`,`_MIN`,`_MEAN`, and`_COUNT`.\n- Dataflow doesn't support creating a histogram from`Distribution`metrics.\n- Dataflow reports incremental updates to Monitoring approximately every 30 seconds.\n- To avoid conflicts, all Dataflow custom metrics are exported as a`double`data type.\n- For simplicity, all Dataflow custom metrics are exported as a `GAUGE` [metric kind](https://cloud.google.com/monitoring/api/v3/kinds-and-types#metric-kinds) . You can monitor the delta over a time window for a `GAUGE` metric, as shown in the following [Monitoring Query Language](/monitoring/mql) example:```\n fetch dataflow_job\n | metric 'dataflow.googleapis.com/job/user_counter'\n | filter (metric.job_id == '[JobID]')\n | delta 1m\n | group_by 1m, [value_user_counter_mean: mean(value.user_counter)]\n | every 1m\n | group_by [metric.ptransform, metric.metric_name],\n [value_user_counter_mean_aggregate: aggregate(value_user_counter_mean)]\n```\n- The Dataflow custom metrics appear in Monitoring as `dataflow.googleapis.com/job/user_counter` with the labels `metric_name:` `` and `ptransform:` `` .\n- For backward compatibility, Dataflow also reports custom metrics to Monitoring as `custom.googleapis.com/dataflow/` `` .\n- The Dataflow custom metrics are subject to the limitation of [cardinality](/monitoring/api/v3/metric-model#cardinality) in Monitoring.\n- Each project has a limit of 100 Dataflow custom metrics. These metrics are published as `custom.googleapis.com/dataflow/` `` .\nCustom metrics that are reported to Monitoring incur charges based on the [Cloud Monitoring pricing](/stackdriver/pricing#monitoring-costs) .\n## Use Metrics Explorer\nUse Monitoring to explore Dataflow metrics. Follow the steps in this section to observe the standard metrics that are provided for each of your Apache Beam pipelines. For more information about using Metrics Explorer, see [Create charts with Metrics Explorer](/monitoring/charts/metrics-explorer) .\n- In the Google Cloud console, select **Monitoring** : [Go to Monitoring](https://console.cloud.google.com/monitoring) \n- In the navigation pane, select **Metrics explorer** .\n- In the **Select a metric** pane, enter `Dataflow Job` in the filter.\n- From the list that appears, select a metric to observe for one of your jobs.\nWhen running Dataflow jobs, you might also want to monitor metrics for your sources and sinks. For example, you might want to monitor BigQuery Storage API metrics. For more information, see [Create dashboards, charts, and alerts](/bigquery/docs/monitoring-dashboard) and the complete list of [metrics from the BigQuery Data Transfer Service](/monitoring/api/metrics_gcp#gcp-bigquerystorage) .\n## Create alerting policies and dashboards\nMonitoring provides access to Dataflow-related metrics. Create dashboards to chart the time series of metrics, and create alerting policies that notify you when metrics reach specified values.\n### Create groups of resources\nTo make it easier to set alerts and build dashboards, create resource groups that include multiple Apache Beam pipelines.\n- In the Google Cloud console, select **Monitoring** : [Go to Monitoring](https://console.cloud.google.com/monitoring) \n- In the navigation pane, select **Groups** .\n- Click **Create group** .\n- Enter a name for your group.\n- Add filter criteria that define the Dataflow resources included in the group. For example, one of your filter criteria can be the name prefix of your pipelines. \n- After the group is created, you can see the basic metrics related to resources in that group.\n### Create alerting policies for Dataflow metrics\nMonitoring lets you create alerts and receive notifications when a metric crosses a specified threshold. For example, you can receive a notification when system lag of a streaming pipeline increases above a predefined value.\n- In the Google Cloud console, select **Monitoring** : [Go to Monitoring](https://console.cloud.google.com/monitoring) \n- In the navigation pane, select **Alerting** .\n- Click **Create policy** .\n- On the **Create new alerting policy** page, define the alerting conditions and notification channels. For example, to set an alert on the system lag for the `WindowedWordCount` Apache Beam pipeline group, complete the following steps:- Click **Select a metric** .\n- In the **Select a metric** field, enter`Dataflow Job`.\n- For **Metric Categories** , select **Job** .\n- For **Metrics** , select **System lag** .\nEvery time an alert is triggered, an incident and a corresponding event are created. If you specified a notification mechanism in the alert, such as email or SMS, you also receive a notification.\n### Build custom monitoring dashboards\nYou can build Monitoring dashboards with the most relevant Dataflow-related charts. To add a chart to a dashboard, follow these steps:\n- In the Google Cloud console, select **Monitoring** : [Go to Monitoring](https://console.cloud.google.com/monitoring) \n- In the navigation pane, select **Dashboards** .\n- Click **Create dashboard** .\n- Click **Add widget** .\n- In the **Add widget** window, for **Data** , select **Metric** .\n- In the **Select a metric** pane, for **Metric** , enter `Dataflow Job` .\n- Select a metric category and a metric.\nYou can add as many charts to the dashboard as you like.\n### Receive worker VM metrics from the Monitoring agent\nYou can use the Monitoring to monitor persistent disk, CPU, network, and process metrics. When you run your pipeline, from your Dataflow worker VM instances, enable the [Monitoring agent](/monitoring/agent) . See the list of [available Monitoring agent metrics](/monitoring/api/metrics_agent) .\nTo enable the Monitoring agent, use the `--experiments=enable_stackdriver_agent_metrics` option when running your pipeline. The [worker service account](/dataflow/docs/concepts/security-and-permissions#worker-service-account) must have the `roles/monitoring.metricWriter` role.\nTo disable the Monitoring agent without stopping your pipeline, update your pipeline by [launching a replacement job](/dataflow/pipelines/updating-a-pipeline#launching-your-replacement-job) without the `--experiments=enable_stackdriver_agent_metrics` parameter.\n## Storage and retention\nInformation about completed or cancelled Dataflow jobs is stored for 30 days.\nOperational logs are stored in the [_Default log bucket](/logging/docs/routing/overview#default-bucket) . The logging API service name is `dataflow.googleapis.com` . For more information about the Google Cloud monitored resource types and services used in Cloud Logging, see [Monitored resources and services](/logging/docs/api/v2/resource-list) .\nFor details about how long log entries are retained by Logging, see the retention information in [Quotas and limits: Logs retention periods](/logging/quotas#logs_retention_periods) .\nFor information about viewing operational logs, see [Monitor and view pipeline logs](/dataflow/docs/guides/logging#MonitoringLogs) .\n## What's next\nTo learn more, consider exploring these other resources:\n- [Monitoring documentation](/monitoring/docs) \n- [Use the Dataflow monitoring interface](/dataflow/docs/guides/monitoring-overview)", "guide": "Dataflow"}