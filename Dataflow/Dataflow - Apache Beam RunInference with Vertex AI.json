{"title": "Dataflow - Apache Beam RunInference with Vertex AI", "url": "https://cloud.google.com/dataflow/docs/notebooks/run_inference_vertex_ai?hl=zh-cn", "abstract": "# Dataflow - Apache Beam RunInference with Vertex AI\n| 0     | 1      |\n|:--------------------|:----------------------|\n| Run in Google Colab | View source on GitHub |\nThis notebook shows how to use the Apache Beam [RunInference](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.RunInference) transform for image classification with [Vertex AI](https://cloud.google.com/vertex-ai) . Apache Beam has built-in support for sending requests to a remotely deployed Vertex AI endpoint by using the [VertexAIModelHandlerJSON](https://github.com/apache/beam/blob/395c4d15bb74351b0aa020dc7463de8d85766e07/sdks/python/apache_beam/ml/inference/vertex_ai_inference.py#L61) class. The input for this class is a JSON-serializable type, like a list.\nWhen you use remote inference with Vertex AI, consider the following factors:\n- Public endpoints have a maximum request size of 1.5 MB. If you want to send larger requests, you must configure a [private endpoint](https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints) and run your pipeline within the same VPC network. You might want to send larger requests if you send batches of requests.\n- Inputs to the Vertex AI model handler must be JSON serializable. If the inputs aren't JSON serializable, the request to the endpoint fails.\n- Hosting a model on Vertex AI and deploying it to an endpoint incurs cost from Google Cloud.\nThis notebook demonstrates the following steps:\n- Configure access to a public Vertex AI endpoint.\n- Set up example data.\n- Run those examples with the built-in model handlers and get a prediction inside an Apache Beam pipeline.\nFor more information about using RunInference, see [Get started with AI/ML pipelines](https://beam.apache.org/documentation/ml/overview/) in the Apache Beam documentation.\n", "content": "## Before you begin\nSet up your environment and download dependencies.\n### Prerequisites\nTo run this notebook, first follow the steps for training a custom model in the Vertex AI [\"Hello Custom Training\"](https://cloud.google.com/vertex-ai/docs/tutorials/image-recognition-custom) tutorial. At minimum, you need to have a trained image classification model deployed to an endpoint within your Google Cloud project.\n### Install Apache Beam\nTo use RunInference with the built-in Vertex AI model handler, install the Apache Beam SDK version 2.50.0 or later.\n**Note:** The Apache Beam 2.50.0 SDK is currently in the release process and is not available. This notebook uses a release candidate build.\n```\n!pip install protobuf --quiet!pip install apache_beam[gcp,interactive]==2.50.0 --quiet# Enforce shapely < 2.0.0 to avoid an issue with google.aiplatform!pip install shapely==1.7.1 --quiet# To use the newly installed versions, restart the runtime.exit()\n```\n### Authenticate with Google Cloud\nThis notebook relies on having a Vertex AI endpoint deployed to Google Cloud. To use your Google Cloud account, authenticate this notebook.\n```\nfrom google.colab import authauth.authenticate_user()\n```\n### Import dependencies and set up your bucket\nUse the following code to import dependencies and to set up your Google Cloud Storage bucket.\nReplace `PROJECT_ID` , `LOCATION_NAME` , and `ENDPOINT_ID` with the ID of your project, the GCP region where your model is deployed, and the ID of your Vertex AI endpoint.\n### Query your Endpoint\nVerify that your model is deployed to your Vertex AI endpoint. If you encounter errors, make sure that your endpoint is live and accessible from your current account.\n```\naiplatform.init(project=project, location=location)endpoint = aiplatform.Endpoint(endpoint_name=endpoint_id)# To get more metadata, remove [0].display_nameendpoint.list_models()[0].display_name\n```\n```\n'hello_custom'\n```\n### Preprocess an example image\nPreprocess an input to match what your model expects. Use the following code to complete these steps:\n- Test the model by using an image of sunflowers.\n- Trim the image to 128x128 pixels to match the model's expectations.\n- Output the image as a list.\n```\nIMG_WIDTH = 128IMG_URL = \"https://storage.googleapis.com/apache-beam-ml/testing/inputs/vertex_images/sunflowers/1008566138_6927679c8a.jpg\"def download_image(image_url: str) -> bytes:\u00a0 return requests.get(image_url).contentdef preprocess_image(data: bytes) -> List[float]:\u00a0 \"\"\"Preprocess the image, resize it, and normalize it, and then\u00a0 convert it to a list.\u00a0 \"\"\"\u00a0 image = tf.io.decode_jpeg(data, channels=3)\u00a0 image = tf.image.resize_with_pad(image, IMG_WIDTH, IMG_WIDTH)\u00a0 image = image / 255\u00a0 return image.numpy().tolist()img = download_image(IMG_URL)image = Image.open(BytesIO(img)).convert('RGB')fig = plt.figure()plt.axis('off')plt.imshow(image)\n```\n```\n<matplotlib.image.AxesImage at 0x7e7f3463ac80>\n```\n## Run the pipeline\nUse the following code to run the pipeline. The pipeline gets an image classification and a probability from the Vertex AI endpoint.\n```\n# Column labels for the output probabilities.COLUMNS = ['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']class PostProcessor(beam.DoFn):\u00a0 def process(self, element: PredictionResult) -> Iterable[str]:\u00a0 \u00a0 prediction_vals = element.inference\u00a0 \u00a0 index = prediction_vals.index(max(prediction_vals))\u00a0 \u00a0 yield str(COLUMNS[index]) + \" (\" + str(max(prediction_vals)) + \")\"model_handler = VertexAIModelHandlerJSON(endpoint_id=endpoint_id, project=project, location=location).with_preprocess_fn(preprocess_image).with_preprocess_fn(download_image)with beam.Pipeline() as p:\u00a0 \u00a0 _ = (p | beam.Create([IMG_URL])\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| RunInference(model_handler)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.ParDo(PostProcessor())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.Map(print)\u00a0 \u00a0 \u00a0 \u00a0 )\n```\n```\nsunflowers (0.993382215)\n```\n## Use a keyed model handler\nTo use a keyed model handler, use `KeyedModelHandler` with Vertex AI by using `VertexAIModelHandlerJSON` .\nBy default, the `ModelHandler` does not expect a key.\n- If you know that keys are associated with your examples, use`beam.KeyedModelHandler`to wrap the model handler.\n- If you don't know whether keys are associated with your examples, use`beam.MaybeKeyedModelHandler`.\n```\nclass PostProcessorKeyed(beam.DoFn):\u00a0 def process(self, element: Tuple[str, PredictionResult]) -> Iterable[str]:\u00a0 \u00a0 img_name, prediction_result = element\u00a0 \u00a0 prediction_vals = prediction_result.inference\u00a0 \u00a0 index = prediction_vals.index(max(prediction_vals))\u00a0 \u00a0 yield img_name + \": \" + str(COLUMNS[index]) + \" (\" + str(\u00a0 \u00a0 \u00a0 \u00a0 max(prediction_vals)) + \")\"keyed_model_handler = KeyedModelHandler(VertexAIModelHandlerJSON(endpoint_id=endpoint_id, project=project, location=location))with beam.Pipeline() as p:\u00a0 \u00a0 _ = (p | 'CreateExamples' >> beam.Create([IMG_URL])\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.Map(lambda img_name: (img_name, download_image(img_name)))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.MapTuple(lambda img_name, img: (img_name, preprocess_image(img)))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| RunInference(keyed_model_handler)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.ParDo(PostProcessorKeyed())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0| beam.Map(print)\u00a0 \u00a0 \u00a0 \u00a0 )\n```\n```\nhttps://storage.googleapis.com/apache-beam-ml/testing/inputs/vertex_images/sunflowers/1008566138_6927679c8a.jpg: sunflowers (0.993382215)\n```", "guide": "Dataflow"}