{"title": "Dataflow - Use Apache Beam notebook advanced features", "url": "https://cloud.google.com/dataflow/docs/guides/notebook-advanced", "abstract": "# Dataflow - Use Apache Beam notebook advanced features\nUsing the Apache Beam interactive runner with JupyterLab notebooks lets you iteratively develop pipelines, inspect your pipeline graph, and parse individual PCollections in a read-eval-print-loop (REPL) workflow. For a tutorial that demonstrates how to use the Apache Beam interactive runner with JupyterLab notebooks, see [Develop with Apache Beam notebooks](/dataflow/docs/guides/interactive-pipeline-development) .\nThis page provides details about advanced features that you can use with your Apache Beam notebook.\n", "content": "## Interactive FlinkRunner on notebook-managed clusters\nTo work with production-sized data interactively from the notebook, you can use the `FlinkRunner` with some generic pipeline options to tell the notebook session to manage a long-lasting Dataproc cluster and to run your Apache Beam pipelines distributedly.\n### Prerequisites\nTo use this feature:\n- Enable the Dataproc API.\n- Grant an admin or editor role to the service account that runs the notebook instance for Dataproc.\n- Use a notebook kernel with the Apache Beam SDK version 2.40.0 or later.\n### Configuration\nAt a minimum, you need the following setup:\n```\n# Set a Cloud Storage bucket to cache source recording and PCollections.# By default, the cache is on the notebook instance itself, but that does not# apply to the distributed execution scenario.ib.options.cache_root = 'gs://<BUCKET_NAME>/flink'# Define an InteractiveRunner that uses the FlinkRunner under the hood.interactive_flink_runner = InteractiveRunner(underlying_runner=FlinkRunner())options = PipelineOptions()# Instruct the notebook that Google Cloud is used to run the FlinkRunner.cloud_options = options.view_as(GoogleCloudOptions)cloud_options.project = 'PROJECT_ID'\n```\n### Explicit provision (optional)\nYou can add the following options.\n```\n# Change this if the pipeline needs to run in a different region# than the default, 'us-central1'. For example, to set it to 'us-west1':cloud_options.region = 'us-west1'# Explicitly provision the notebook-managed cluster.worker_options = options.view_as(WorkerOptions)# Provision 40 workers to run the pipeline.worker_options.num_workers=40# Use the default subnetwork.worker_options.subnetwork='default'# Choose the machine type for the workers.worker_options.machine_type='n1-highmem-8'# When working with non-official Apache Beam releases, such as Apache Beam built from source# code, configure the environment to use a compatible released SDK container.# If needed, build a custom container and use it. For more information, see:# https://beam.apache.org/documentation/runtime/environments/options.view_as(PortableOptions).environment_config = 'apache/beam_python3.7_sdk:2.41.0 or LOCATION.pkg.dev/PROJECT_ID/REPOSITORY/your_custom_container'\n```\n### Usage\n```\n# The parallelism is applied to each step, so if your pipeline has 10 steps, you# end up having 10 * 10 = 100 tasks scheduled, which can be run in parallel.options.view_as(FlinkRunnerOptions).parallelism = 10p_word_count = beam.Pipeline(interactive_flink_runner, options=options)word_counts = (\u00a0 \u00a0 p_word_count\u00a0 \u00a0 | 'read' >> ReadWordsFromText('gs://apache-beam-samples/shakespeare/kinglear.txt')\u00a0 \u00a0 | 'count' >> beam.combiners.Count.PerElement())# The notebook session automatically starts and manages a cluster to run# your pipelines with the FlinkRunner.ib.show(word_counts)# Interactively adjust the parallelism.options.view_as(FlinkRunnerOptions).parallelism = 150# The BigQuery read needs a Cloud Storage bucket as a temporary location.options.view_as(GoogleCloudOptions).temp_location = ib.options.cache_rootp_bq = beam.Pipeline(runner=interactive_flink_runner, options=options)delays_by_airline = (\u00a0 \u00a0 p_bq\u00a0 \u00a0 | 'Read Dataset from BigQuery' >> beam.io.ReadFromBigQuery(\u00a0 \u00a0 \u00a0 \u00a0 project=project, use_standard_sql=True,\u00a0 \u00a0 \u00a0 \u00a0 query=('SELECT airline, arrival_delay '\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0'FROM `bigquery-samples.airline_ontime_data.flights` '\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0'WHERE date >= \"2010-01-01\"'))\u00a0 \u00a0 | 'Rebalance Data to TM Slots' >> beam.Reshuffle(num_buckets=1000)\u00a0 \u00a0 | 'Extract Delay Info' >> beam.Map(\u00a0 \u00a0 \u00a0 \u00a0 lambda e: (e['airline'], e['arrival_delay'] > 0))\u00a0 \u00a0 | 'Filter Delayed' >> beam.Filter(lambda e: e[1])\u00a0 \u00a0 | 'Count Delayed Flights Per Airline' >> beam.combiners.Count.PerKey())# This step reuses the existing cluster.ib.collect(delays_by_airline)# Describe the cluster running the pipelines.# You can access the Flink dashboard from the printed link.ib.clusters.describe()# Cleans up all long-lasting clusters managed by the notebook session.ib.clusters.cleanup(force=True)\n```\n### Notebook-managed clusters\n- By default, if you don't provide any pipeline options, Interactive Apache Beam **always reuses** the most recently used cluster to run a pipeline with the`FlinkRunner`.- To avoid this behavior, for example, to run another pipeline in the same notebook session with a FlinkRunner not hosted by the notebook, run`ib.clusters.set_default_cluster(None)`.\n- When instantiating a new pipeline that uses a project, region, and provisioning configuration that map to an existing Dataproc cluster, Dataflow also reuses the cluster, though it might not use the most recently used cluster.\n- However, whenever a provisioning change is given, such as when resizing a cluster, a new cluster is created to actuate the desired change. If you intend to resize a cluster, to avoid exhausting cloud resources, clean up unnecessary clusters by using`ib.clusters.cleanup(pipeline)`.\n- When a Flink`master_url`is specified, if it belongs to a cluster that is managed by the notebook session, Dataflow reuses the managed cluster.- If the`master_url`is unknown to the notebook session, it means that a user-self-hosted`FlinkRunner`is desired. The notebook doesn't do anything implicitly.\n### Troubleshooting\nThis section provides information to help you troubleshoot and debug the Interactive FlinkRunner on notebook-managed clusters.\nFor simplicity, the Flink network buffer configuration is not exposed for configuration.\nIf your job graph is too complicated or your parallelism is set too high, the cardinality of steps multiplied by parallelism might be too big, cause too many tasks to be scheduled in parallel, and fail the execution.\nUse the following tips to improve the velocity of interactive runs:\n- Only assign the`PCollection`that you want to inspect to a variable.\n- Inspect`PCollections`one by one.\n- Use reshuffle after high fanout transforms.\n- Adjust parallelism based on the data size. Sometimes smaller is faster.Check the Flink dashboard for the running job. You might see a step where hundreds of tasks have finished and only one remains, because in-flight data resides on a single machine and is not shuffled.\nAlways use reshuffle after a high fanout transform, such as when:\n- Reading rows from a file\n- Reading rows from a BigQuery table\nWithout reshuffle, fanout data is always run on the same worker, and you can't take advantage of parallelism.\nAs a rule of thumb, the Flink cluster has about the number of vCPUs multiplied by the number of worker slots. For example, if you have 40 n1-highmem-8 workers, the Flink cluster has at most 320 slots, or 8 multiplied by 40.\nIdeally, the worker can manage a job that reads, maps, and combines with parallelism set in the hundreds, which schedules thousands of tasks in parallel.\nStreaming pipelines are not currently compatible with the interactive Flink on notebook-managed cluster feature.\n## Beam SQL and beam_sql magic\n[Beam SQL](https://beam.apache.org/documentation/dsls/sql/overview/) allows you to query bounded and unbounded `PCollections` with SQL statements. If you're working in an Apache Beam notebook, you can use the IPython [custom magic](https://ipython.readthedocs.io/en/stable/config/custommagics.html) `beam_sql` to speed up your pipeline development.\nYou can check the `beam_sql` magic usage with the `-h` or `--help` option:\nYou can create a `PCollection` from constant values:\nYou can join multiple `PCollections` :\nYou can launch a Dataflow job with the `-r DataflowRunner` or `--runner DataflowRunner` option:\nTo learn more, see the example notebook [Apache Beam SQL in notebooks](/dataflow/docs/guides/interactive-pipeline-development#get_started_with_notebooks) .\n## Accelerate using JIT compiler and GPU\nYou can use libraries such as [numba](https://numba.pydata.org/) and [GPUs](/dataflow/docs/gpu/use-gpus) to accelerate your Python code and Apache Beam pipelines. In the Apache Beam notebook instance created with an `nvidia-tesla-t4` GPU, to run on GPUs, compile your Python code with `numba.cuda.jit` . Optionally, to speed up the execution on CPUs, compile your Python code into machine code with `numba.jit` or `numba.njit` .\nThe following example creates a `DoFn` that processes on GPUs:\n```\nclass Sampler(beam.DoFn):\u00a0 \u00a0 def __init__(self, blocks=80, threads_per_block=64):\u00a0 \u00a0 \u00a0 \u00a0 # Uses only 1 cuda grid with below config.\u00a0 \u00a0 \u00a0 \u00a0 self.blocks = blocks\u00a0 \u00a0 \u00a0 \u00a0 self.threads_per_block = threads_per_block\u00a0 \u00a0 def setup(self):\u00a0 \u00a0 \u00a0 \u00a0 import numpy as np\u00a0 \u00a0 \u00a0 \u00a0 # An array on host as the prototype of arrays on GPU to\u00a0 \u00a0 \u00a0 \u00a0 # hold accumulated sub count of points in the circle.\u00a0 \u00a0 \u00a0 \u00a0 self.h_acc = np.zeros(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 self.threads_per_block * self.blocks, dtype=np.float32)\u00a0 \u00a0 def process(self, element: Tuple[int, int]):\u00a0 \u00a0 \u00a0 \u00a0 from numba import cuda\u00a0 \u00a0 \u00a0 \u00a0 from numba.cuda.random import create_xoroshiro128p_states\u00a0 \u00a0 \u00a0 \u00a0 from numba.cuda.random import xoroshiro128p_uniform_float32\u00a0 \u00a0 \u00a0 \u00a0 @cuda.jit\u00a0 \u00a0 \u00a0 \u00a0 def gpu_monte_carlo_pi_sampler(rng_states, sub_sample_size, acc):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\"\"Uses GPU to sample random values and accumulates the sub count\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 of values within a circle of radius 1.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\"\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pos = cuda.grid(1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if pos < acc.shape[0]:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sub_acc = 0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for i in range(sub_sample_size):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 x = xoroshiro128p_uniform_float32(rng_states, pos)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 y = xoroshiro128p_uniform_float32(rng_states, pos)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if (x * x + y * y) <= 1.0:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sub_acc += 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 acc[pos] = sub_acc\u00a0 \u00a0 \u00a0 \u00a0 rng_seed, sample_size = element\u00a0 \u00a0 \u00a0 \u00a0 d_acc = cuda.to_device(self.h_acc)\u00a0 \u00a0 \u00a0 \u00a0 sample_size_per_thread = sample_size // self.h_acc.shape[0]\u00a0 \u00a0 \u00a0 \u00a0 rng_states = create_xoroshiro128p_states(self.h_acc.shape[0], seed=rng_seed)\u00a0 \u00a0 \u00a0 \u00a0 gpu_monte_carlo_pi_sampler[self.blocks, self.threads_per_block](\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rng_states, sample_size_per_thread, d_acc)\u00a0 \u00a0 \u00a0 \u00a0 yield d_acc.copy_to_host()\n```\nThe following image demonstrates the notebook running on a GPU:\nMore details can be found in the example notebook [Use GPUs with Apache Beam](/dataflow/docs/guides/interactive-pipeline-development#get_started_with_notebooks) .\n## Build a custom container\nIn most cases, if your pipeline doesn't require additional Python dependencies or executables, Apache Beam can automatically use its official container images to run your user-defined code. These images come with many common Python modules, and you don't have to build or explicitly specify them.\nIn some cases, you might have extra Python dependencies or even non-Python dependencies. In these scenarios, you can build a custom container and make it available to the Flink cluster to run. The following list provides the advantages of using a custom container:\n- Faster setup time for consecutive and interactive executions\n- Stable configurations and dependencies\n- More flexibility: you can set up more than Python dependencies\nThe container build process might be tedious, but you can do everything in the notebook using the following usage pattern.\n### Create a local workspace\nFirst, create a local work directory under the Jupyter home directory.\n```\n!mkdir -p /home/jupyter/.flink\n```\n### Prepare Python dependencies\nNext, install all the extra Python dependencies that you might use, and export them into a requirements file.\n```\n%pip install dep_a%pip install dep_b...\n```\nYou can explicitly create a requirements file by using the `%%writefile` notebook magic.\n```\n%%writefile /home/jupyter/.flink/requirements.txtdep_adep_b...\n```\nAlternatively, you can freeze all local dependencies into a requirements file. This option might introduce unintended dependencies.\n```\n%pip freeze > /home/jupyter/.flink/requirements.txt\n```\n### Prepare your non-Python dependencies\nCopy all non-Python dependencies into the workspace. If you don't have any non-Python dependencies, skip this step.\n```\n!cp /path/to/your-dep /home/jupyter/.flink/your-dep...\n```\n### Create a Dockerfile\nCreate a Dockerfile with the `%%writefile` notebook magic. For example:\n```\n%%writefile /home/jupyter/.flink/DockerfileFROM apache/beam_python3.7_sdk:2.40.0COPY \u00a0requirements.txt /tmp/requirements.txtCOPY \u00a0your_dep /tmp/your_dep...RUN python -m pip install -r /tmp/requirements.txt\n```\nThe example container uses the image of the Apache Beam SDK version 2.40.0 with Python 3.7 as the base, adds a `your_dep` file, and installs the extra Python dependencies. Use this Dockerfile as a template, and edit it for your use case.\nIn your Apache Beam pipelines, when referring to non-Python dependencies, use their `COPY` destinations. For example, `/tmp/your_dep` is the file path of the `your_dep` file.\n### Build a container image in Artifact Registry by using Cloud Build\n- Enable the Cloud Build and Artifact Registry services, if not already enabled.```\n!gcloud services enable cloudbuild.googleapis.com!gcloud services enable artifactregistry.googleapis.com\n```\n- Create an Artifact Registry repository so that you can upload artifacts. Each repository can contain artifacts for a single supported format.All repository content is encrypted using either Google-managed or customer-managed encryption keys. Artifact Registry uses Google-managed encryption keys by default and no configuration is required for this option.You must have at least [Artifact Registry Writer access](/artifact-registry/docs/access-control#permissions) to the repository.Run the following command to create a new repository. The command uses the `--async` flag and returns immediately, without waiting for the operation in progress to complete.```\ngcloud artifacts repositories create REPOSITORY \\--repository-format=docker \\--location=LOCATION \\--async\n```Replace the following values:- : a name for your repository. For each repository location in a project, repository names must be unique.\n- : the [location](/artifact-registry/docs/repositories/repo-locations) for your repository.\n- Before you can push or pull images, configure Docker to authenticate requests for Artifact Registry. To set up authentication to Docker repositories, run the following command:```\ngcloud auth configure-docker LOCATION-docker.pkg.dev\n```The command updates your Docker configuration. You can now connect with Artifact Registry in your Google Cloud project to push images.\n- Use Cloud Build to build the container image, and save it to Artifact Registry.```\n!cd /home/jupyter/.flink \\&& gcloud builds submit \\\u00a0--tag LOCATION.pkg.dev/PROJECT_ID/REPOSITORY/flink:latest \\\u00a0--timeout=20m\n```Replace `` with the project ID of your project.\n**Warning:** Don't build the container image on the notebook instance itself, because it isn't a consistent build environment.\n### Use custom containers\nDepending on the runner, you can use custom containers for different purposes.\nFor general Apache Beam container usage, see:\n- [Container environments](https://beam.apache.org/documentation/runtime/environments/) \n- [Managing Python Pipeline Dependencies](https://beam.apache.org/documentation/sdks/python-pipeline-dependencies/#custom-containers) \nFor Dataflow container usage, see:\n- [Use custom containers in Dataflow](https://cloud.google.com/dataflow/docs/guides/using-custom-containers) ## Disable external IP addresses\nWhen creating an Apache Beam notebook instance, to increase security, disable external IP addresses. Because notebook instances need to download some public internet resources, such as [Artifact Registry](/artifact-registry/docs/overview) , you need to first [create a new VPC network without an external IP address](/nat/docs/gce-example#create-vpc-network) . Then, [create a Cloud NAT gateway for this VPC network](/nat/docs/gce-example#create-nat) . For more information about Cloud NAT, see the [Cloud NAT documentation](/nat/docs/overview) . Use the VPC network and Cloud NAT gateway to access the necessary public internet resources without enabling external IP addresses.", "guide": "Dataflow"}