{"title": "Dataflow - Apache Beam RunInference for scikit-learn", "url": "https://cloud.google.com/dataflow/docs/notebooks/run_inference_sklearn?hl=zh-cn", "abstract": "# Dataflow - Apache Beam RunInference for scikit-learn\n| 0     | 1      |\n|:--------------------|:----------------------|\n| Run in Google Colab | View source on GitHub |\nThis notebook demonstrates the use of the RunInference transform for [scikit-learn](https://scikit-learn.org/) , also called sklearn. Apache Beam [RunInference](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.RunInference) has implementations of the [ModelHandler](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.base.html#apache_beam.ml.inference.base.ModelHandler) class prebuilt for scikit-learn. For more information about using RunInference, see [Get started with AI/ML pipelines](https://beam.apache.org/documentation/ml/overview/) in the Apache Beam documentation.\nYou can choose the appropriate model handler based on your input data type:\n- [NumPy model handler](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.sklearn_inference.html#apache_beam.ml.inference.sklearn_inference.SklearnModelHandlerNumpy) \n- [Pandas DataFrame model handler](https://beam.apache.org/releases/pydoc/current/apache_beam.ml.inference.sklearn_inference.html#apache_beam.ml.inference.sklearn_inference.SklearnModelHandlerNumpy) \nWith RunInference, these model handlers manage batching, vectorization, and prediction optimization for your scikit-learn pipeline or model.\nThis notebook demonstrates the following common RunInference patterns:\n- Generate predictions.\n- Postprocess results after RunInference.\n- Run inference with multiple models in the same pipeline.\nThe linear regression models used in these samples are trained on data that correspondes to the 5 and 10 times tables; that is, `y = 5x` and `y = 10x` respectively.\n", "content": "## Before you begin\nComplete the following setup steps:\n- Install dependencies for Apache Beam.\n- Authenticate with Google Cloud.\n- Specify your project and bucket. You use the project and bucket to save and load models.\n```\npip install google-api-core --quietpip install google-cloud-pubsub google-cloud-bigquery-storage --quietpip install apache-beam[gcp,dataframe] --quiet\n```\n## About scikit-learn versions\n`scikit-learn` is a build-dependency of Apache Beam. If you need to install a different version of sklearn , use `%pip install scikit-learn==<version>`\n```\nfrom google.colab import authauth.authenticate_user()\n```\n```\nimport picklefrom sklearn import linear_modelfrom typing import Tupleimport numpy as npimport apache_beam as beamfrom apache_beam.ml.inference.sklearn_inference import ModelFileTypefrom apache_beam.ml.inference.sklearn_inference import SklearnModelHandlerNumpyfrom apache_beam.ml.inference.base import KeyedModelHandlerfrom apache_beam.ml.inference.base import PredictionResultfrom apache_beam.ml.inference.base import RunInferencefrom apache_beam.options.pipeline_options import PipelineOptions# NOTE: If an error occurs, restart your runtime.\n```\n```\nimport os# Constantsproject = \"<PROJECT_ID>\"bucket = \"<BUCKET_NAME>\" # To avoid warnings, set the project.os.environ['GOOGLE_CLOUD_PROJECT'] = project\n```\n## Create the data and the scikit-learn model\nThis section demonstrates the following steps:\n- Create the data to train the scikit-learn linear regression model.\n- Train the linear regression model.\n- Save the scikit-learn model using`pickle`.\nIn this example, you create two models, one with the 5 times model and a second with the 10 times model.\n```\n# Input data to train the sklearn model for the 5 times table.x = np.arange(0, 100, dtype=np.float32).reshape(-1, 1)y = (x * 5).reshape(-1, 1)def train_and_save_model(x, y, model_file_name):\u00a0 regression = linear_model.LinearRegression()\u00a0 regression.fit(x,y)\u00a0 with open(model_file_name, 'wb') as f:\u00a0 \u00a0 \u00a0 pickle.dump(regression, f)five_times_model_filename = 'sklearn_5x_model.pkl'train_and_save_model(x, y, five_times_model_filename)# Change y to be 10 times, and output a 10 times table.ten_times_model_filename = 'sklearn_10x_model.pkl'train_and_save_model(x, y, ten_times_model_filename)y = (x * 10).reshape(-1, 1)train_and_save_model(x, y, 'sklearn_10x_model.pkl')\n```\n### Create a scikit-learn RunInference pipeline\nThis section demonstrates how to do the following:\n- Define a scikit-learn model handler that accepts an`array_like`object as input.\n- Read the data from BigQuery.\n- Use the scikit-learn trained model and the scikit-learn RunInference transform on unkeyed data.\n```\n%pip install --upgrade google-cloud-bigquery --quiet\n```\n```\ngcloud config set project $project\n```\n```\nUpdated property [core/project].\n```\n```\n# Populated BigQuery tablefrom google.cloud import bigqueryclient = bigquery.Client(project=project)# Make sure the dataset_id is unique in your project.dataset_id = '{project}.maths'.format(project=project)dataset = bigquery.Dataset(dataset_id)# Modify the location based on your project configuration.dataset.location = 'US'dataset = client.create_dataset(dataset, exists_ok=True)# Table name in the BigQuery dataset.table_name = 'maths_problems_1'query = \"\"\"\u00a0 \u00a0 CREATE OR REPLACE TABLE\u00a0 \u00a0 \u00a0 {project}.maths.{table} ( key STRING OPTIONS(description=\"A unique key for the maths problem\"),\u00a0 \u00a0 value FLOAT64 OPTIONS(description=\"Our maths problem\" ) );\u00a0 \u00a0 INSERT INTO maths.{table}\u00a0 \u00a0 VALUES\u00a0 \u00a0 \u00a0 (\"first_example\", 105.00),\u00a0 \u00a0 \u00a0 (\"second_example\", 108.00),\u00a0 \u00a0 \u00a0 (\"third_example\", 1000.00),\u00a0 \u00a0 \u00a0 (\"fourth_example\", 1013.00)\"\"\".format(project=project, table=table_name)create_job = client.query(query)create_job.result()\n```\n```\n<google.cloud.bigquery.table._EmptyRowIterator at 0x7f97abb4e850>\n```\n```\nsklearn_model_handler = SklearnModelHandlerNumpy(model_uri=five_times_model_filename) pipeline_options = PipelineOptions().from_dictionary(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {'temp_location':f'gs://{bucket}/tmp'})# Define the BigQuery table specification.table_name = 'maths_problems_1'table_spec = f'{project}:maths.{table_name}'with beam.Pipeline(options=pipeline_options) as p:\u00a0 (\u00a0 \u00a0 \u00a0 p \u00a0 \u00a0 \u00a0 | \"ReadFromBQ\" >> beam.io.ReadFromBigQuery(table=table_spec)\u00a0 \u00a0 \u00a0 | \"ExtractInputs\" >> beam.Map(lambda x: [x['value']]) \u00a0 \u00a0 \u00a0 | \"RunInferenceSklearn\" >> RunInference(model_handler=sklearn_model_handler)\u00a0 \u00a0 \u00a0 | beam.Map(print)\u00a0 )\n```\n```\nPredictionResult(example=[1000.0], inference=array([5000.]))\nPredictionResult(example=[1013.0], inference=array([5065.]))\nPredictionResult(example=[108.0], inference=array([540.]))\nPredictionResult(example=[105.0], inference=array([525.]))\n```\n### Use sklearn RunInference on keyed inputs\nThis section demonstrates how to do the following:\n- Wrap the`SklearnModelHandlerNumpy`object around`KeyedModelHandler`to handle keyed data.\n- Read the data from BigQuery.\n- Use the sklearn trained model and the sklearn RunInference transform on a keyed data.\n```\nsklearn_model_handler = SklearnModelHandlerNumpy(model_uri=five_times_model_filename) keyed_sklearn_model_handler = KeyedModelHandler(sklearn_model_handler)pipeline_options = PipelineOptions().from_dictionary(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {'temp_location':f'gs://{bucket}/tmp'})with beam.Pipeline(options=pipeline_options) as p:\u00a0 (\u00a0 p \u00a0 | \"ReadFromBQ\" >> beam.io.ReadFromBigQuery(table=table_spec)\u00a0 | \"ExtractInputs\" >> beam.Map(lambda x: (x['key'], [x['value']])) \u00a0 | \"RunInferenceSklearn\" >> RunInference(model_handler=keyed_sklearn_model_handler)\u00a0 | beam.Map(print)\u00a0 )\n```\n```\n('third_example', PredictionResult(example=[1000.0], inference=array([5000.])))\n('fourth_example', PredictionResult(example=[1013.0], inference=array([5065.])))\n('second_example', PredictionResult(example=[108.0], inference=array([540.])))\n('first_example', PredictionResult(example=[105.0], inference=array([525.])))\n```\n## Run multiple models\nThis code creates a pipeline that takes two RunInference transforms with different models and then combines the output.\n```\nfrom typing import Tupledef format_output(run_inference_output) -> str:\u00a0 \"\"\"Takes input from RunInference for scikit-learn and extracts the output.\"\"\"\u00a0 key, prediction_result = run_inference_output\u00a0 example = prediction_result.example[0]\u00a0 prediction = prediction_result.inference[0]\u00a0 return f\"key = {key}, example = {example} -> predictions {prediction}\"five_times_model_handler = KeyedModelHandler(\u00a0 \u00a0 SklearnModelHandlerNumpy(model_uri=five_times_model_filename))ten_times_model_handler = KeyedModelHandler(\u00a0 \u00a0 SklearnModelHandlerNumpy(model_uri=ten_times_model_filename))pipeline_options = PipelineOptions().from_dictionary(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {'temp_location':f'gs://{bucket}/tmp'})with beam.Pipeline(options=pipeline_options) as p:\u00a0 inputs = (p \u00a0 \u00a0 | \"ReadFromBQ\" >> beam.io.ReadFromBigQuery(table=table_spec))\u00a0 five_times = (inputs\u00a0 \u00a0 | \"Extract For 5\" >> beam.Map(lambda x: ('{} {}'.format(x['key'], '* 5'), [x['value']]))\u00a0 \u00a0 | \"5 times\" >> RunInference(model_handler = five_times_model_handler))\u00a0 ten_times = (inputs\u00a0 \u00a0 | \"Extract For 10\" >> beam.Map(lambda x: ('{} {}'.format(x['key'], '* 10'), [x['value']]))\u00a0 \u00a0 | \"10 times\" >> RunInference(model_handler = ten_times_model_handler))\u00a0 _ = ((five_times, ten_times) | \"Flattened\" >> beam.Flatten()\u00a0 \u00a0 | \"format output\" >> beam.Map(format_output)\u00a0 \u00a0 | \"Print\" >> beam.Map(print))\n```\n```\nkey = third_example * 10, example = 1000.0 -> predictions 10000.0\nkey = fourth_example * 10, example = 1013.0 -> predictions 10130.0\nkey = second_example * 10, example = 108.0 -> predictions 1080.0\nkey = first_example * 10, example = 105.0 -> predictions 1050.0\nkey = third_example * 5, example = 1000.0 -> predictions 5000.0\nkey = fourth_example * 5, example = 1013.0 -> predictions 5065.0\nkey = second_example * 5, example = 108.0 -> predictions 540.0\nkey = first_example * 5, example = 105.0 -> predictions 525.0\n```", "guide": "Dataflow"}