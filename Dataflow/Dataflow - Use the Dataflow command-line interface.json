{"title": "Dataflow - Use the Dataflow command-line interface", "url": "https://cloud.google.com/dataflow/docs/guides/using-command-line-intf", "abstract": "# Dataflow - Use the Dataflow command-line interface\nWhen you run your pipeline using the [Dataflow-managed service](/dataflow/service/dataflow-service-desc) , you can obtain information about your Dataflow job by using the Dataflow command-line interface. The Dataflow command-line interface is part of the command-line tool in [Google Cloud CLI](/sdk/docs) .\nIf you'd rather view and interact with your Dataflow jobs using Google Cloud console, use the [Dataflow monitoring interface](/dataflow/pipelines/dataflow-monitoring-intf) .\n", "content": "## Installing the Dataflow command-line component\nTo use the Dataflow command-line interface from your local terminal, install and configure [Google Cloud CLI](/sdk/docs/install-sdk) .\nFor Cloud Shell, the Dataflow command-line interface is automatically available.\n## Running the commands\nYou interact with the Dataflow command-line interface by running the available commands. To see the list of available Dataflow commands, type the following command into your shell or terminal:\n```\n gcloud dataflow --help\n```\nAs seen in the output, the Dataflow command has the following four groups: `flex-template` , `jobs` , `snapshots` and `sql` .\n**Note:** For a complete list of all available Dataflow commands and associated documentation, see the Dataflow [ command-line reference documentation for Google Cloud CLI](/sdk/gcloud/reference/dataflow) .\n### Flex Template commands\nThe `flex-template` sub-command group enables you to work with Dataflow Flex Templates. The following operations are supported:\n- `build`: Builds a Flex Template file from the specified parameters.\n- `run`: Runs a job from the specified path.\nTo run a template, you must create a template specification file that is stored in a Cloud Storage bucket. The template specification file contains all the necessary information to run the job, such as the SDK information and metadata. Additionally, the `metadata.json` file contains information about the template such as the name, description, and input parameters. After creating the template specification file, you can then build the Flex Template by using either Java or Python.\nFor information about creating and running a Flex Template using the Google Cloud CLI, see the tutorial [Build and run Flex Templates](/dataflow/docs/guides/templates/using-flex-templates) .\n### Jobs commands\nThe `jobs` sub-command group enables you to work with Dataflow jobs in your project. The following operations are supported:\n- `cancel`: Cancels all jobs that match the command-line arguments.\n- `describe`: Outputs the Job object resulting from the Get API.\n- `drain`: Drains all jobs that match the command line arguments.\n- `list`: Lists all jobs in a particular project, optionally filtered by region.\n- `run`: Runs a job from the specified path.\n- `show`: Shows a short description of the given job.\nTo get a list of all the Dataflow jobs in your project, run the following command in your shell or terminal :\n```\ngcloud dataflow jobs list\n```\nThe command returns a list of your current jobs. The following is a sample output:\n```\n ID          NAME         TYPE CREATION_TIME  STATE REGION\n 2015-06-03_16_39_22-4020553808241078833 wordcount-janedoe-0603233849   Batch 2015-06-03 16:39:22 Done us-central1\n 2015-06-03_16_38_28-4363652261786938862 wordcount-johndoe-0603233820   Batch 2015-06-03 16:38:28 Done us-central1\n 2015-05-21_16_24_11-17823098268333533078 bigquerytornadoes-johndoe-0521232402 Batch 2015-05-21 16:24:11 Done europe-west1\n 2015-05-21_13_38_06-16409850040969261121 bigquerytornadoes-johndoe-0521203801 Batch 2015-05-21 13:38:06 Done us-central1\n 2015-05-21_13_17_18-18349574013243942260 bigquerytornadoes-johndoe-0521201710 Batch 2015-05-21 13:17:18 Done europe-west1\n 2015-05-21_12_49_37-9791290545307959963 wordcount-johndoe-0521194928   Batch 2015-05-21 12:49:37 Done us-central1\n 2015-05-20_15_54_51-15905022415025455887 wordcount-johndoe-0520225444   Batch 2015-05-20 15:54:51 Failed us-central1\n 2015-05-20_15_47_02-14774624590029708464 wordcount-johndoe-0520224637   Batch 2015-05-20 15:47:02 Done us-central1\n```\nUsing the job `ID` displayed for each job, you can run the `describe` command to display more information about a job.\n```\ngcloud dataflow jobs describe JOB_ID\n```\nReplace with the job `ID` of one of the Dataflow jobs from your project.\nFor example, if you run the command for job ID `2015-02-09_11_39_40-15635991037808002875` , the following is a sample output:\n```\ncreateTime: '2015-02-09T19:39:41.140Z'\ncurrentState: JOB_STATE_DONE\ncurrentStateTime: '2015-02-09T19:56:39.510Z'\nid: 2015-02-09_11_39_40-15635991037808002875\nname: tfidf-bchambers-0209193926\nprojectId: google.com:clouddfe\ntype: JOB_TYPE_BATCH\n```\nTo format the result into JSON, run the command with the `--format=json` option:\n```\ngcloud --format=json dataflow jobs describe JOB_ID\n```\nReplace with the job `ID` of one of the Dataflow jobs from your project.\nThe following sample output is formatted as JSON:\n```\n{\n \"createTime\": \"2015-02-09T19:39:41.140Z\",\n \"currentState\": \"JOB_STATE_DONE\",\n \"currentStateTime\": \"2015-02-09T19:56:39.510Z\",\n \"id\": \"2015-02-09_11_39_40-15635991037808002875\",\n \"name\": \"tfidf-bchambers-0209193926\",\n \"projectId\": \"google.com:clouddfe\",\n \"type\": \"JOB_TYPE_BATCH\"\n}\n```\n### Snapshots commands\nThe `snapshots` sub-command group enables you to work with Dataflow snapshots. The following operations are supported:\n- `create`: Creates a snapshot for a Dataflow job.\n- `delete`: Deletes a Dataflow snapshot.\n- `describe`: Describes a Dataflow snapshot.\n- `list`: Lists all Dataflow snapshots in a project in the specified region, optionally filtered by job ID.\nFor more information about using snapshots in Dataflow, see [Using Dataflow snapshots](/dataflow/docs/guides/using-snapshots) .\n### SQL commands\nThe `sql` sub-command group enables you to work with Dataflow SQL. The `gcloud Dataflow sql query` command accepts and runs a user-specified SQL query on Dataflow.\nFor example, to run a simple SQL query on a Dataflow job that reads from a BigQuery dataset and writes to another BigQuery dataset, run the following in your shell or terminal:\n```\ngcloud dataflow sql query 'SELECT word FROM\nbigquery.table.PROJECT_ID.input_dataset.input_table\nwhere count > 3'\n --job-name=JOB_NAME \\\n --region=us-west1 \\\n --bigquery-dataset=OUTPUT_DATASET \\\n --bigquery-table=OUTPUT_TABLE\n```\nReplace the following:\n- : a globally unique name for your project\n- : a name for your Dataflow job\n- : a name for the output dataset\n- : a name for the output table\nStarting a Dataflow SQL job might take several minutes. You cannot update the job after creating it. Dataflow SQL jobs use autoscaling and Dataflow automatically chooses the execution mode to be either batch or streaming. You cannot control this behavior for Dataflow SQL jobs. To stop Dataflow SQL jobs, use the [cancel](/sdk/gcloud/reference/dataflow/jobs/cancel) command. Stopping a Dataflow SQL job with [drain](/sdk/gcloud/reference/dataflow/jobs/drain) is not supported.\nFor more information about using SQL commands for Dataflow, see the [Dataflow SQL reference](/dataflow/docs/reference/sql) and [gcloud Dataflow sql query](/sdk/gcloud/reference/dataflow/sql/query) documentation.\n## Using commands with regions\nThe Dataflow command-line interface supports [regions](/dataflow/docs/concepts/regional-endpoints) starting in gcloud CLI version 176. Use the `--region` option with any command to specify the region that manages your job.\nFor example, `gcloud dataflow jobs list` lists jobs from all regions, but `gcloud dataflow jobs list --region=europe-west1` only lists jobs managed from `europe-west1` .\n**Note:** The`--region`option is required to obtain job information from an endpoint. If you don't specify a region,`us-central1`is used by default.", "guide": "Dataflow"}