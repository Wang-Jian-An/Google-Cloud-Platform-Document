{"title": "Dataflow - Improve performance on a shared GPU by using NVIDIA MPS", "url": "https://cloud.google.com/dataflow/docs/gpu/use-nvidia-mps", "abstract": "# Dataflow - Improve performance on a shared GPU by using NVIDIA MPS\nIf you run multiple SDK processes on a shared Dataflow GPU, you can improve GPU efficiency and utilization by enabling the NVIDIA Multi-Process Service (MPS). MPS supports concurrent processing on a GPU by enabling processes to share CUDA contexts and scheduling resources. MPS can reduce context-switching costs, increase parallelism, and reduce storage requirements.\nTarget workflows are Python pipelines that run on workers with more than one vCPU.\nMPS is an NVIDIA technology that implements the CUDA API, an NVIDIA platform that supports general-purpose GPU computing. For more information, see the [NVIDIA Multi-Process Service user guide](https://docs.nvidia.com/deploy/mps/index.html) .\n", "content": "## Benefits\n- Improves parallel processing and overall throughput for GPU pipelines, especially for workloads with low GPU resource usage.\n- Improves GPU utilization, which might reduce your costs.## Support and limitations\n- MPS is supported only on Dataflow workers that use a single GPU.\n- The pipeline can't use pipeline options that restrict parallelism.\n- Avoid exceeding the available GPU memory, especially for use cases that involve loading large machine learning models. Balance the number of vCPUs and SDK processes with the available GPU memory that these processes need.\n- MPS doesn't affect the concurrency of non-GPU operations.\n- Dataflow Prime doesn't support MPS.## Enable MPS\nWhen you [run a pipeline with GPUs](/dataflow/docs/gpu/use-gpus) , enable MPS by doing the following:\n- In the pipeline option`--dataflow_service_options`, append`use_nvidia_mps`to the`worker_accelerator`parameter.\n- Set the`count`to 1.\n- Don't use the pipeline option`--experiments=no_use_multiple_sdk_containers`.\nThe pipeline option `--dataflow_service_options` looks like the following:\n```\n--dataflow_service_options=\"worker_accelerator=type:GPU_TYPE;count:1;install-nvidia-driver;use_nvidia_mps\"\n```\nIf you use TensorFlow and enable MPS, do the following:\n- [Enable dynamic memory allocation](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) on the GPU. Use either of the following TensorFlow options:- Turn on memory growth by calling`tf.config.experimental.set_memory_growth(gpu, True)`.\n- Set the environmental variable`TF_FORCE_GPU_ALLOW_GROWTH`to true.\n- Use logical devices with appropriate memory limits.\n- For optimal performance, enforce the use of the GPU when possible by using [soft device placement](https://www.tensorflow.org/api_docs/python/tf/config/set_soft_device_placement) or [manual placement](https://www.tensorflow.org/guide/gpu#manual_device_placement) .## What's next\n- To review more best practices, see [GPUs and worker parallelism](/dataflow/docs/gpu/develop-with-gpus#parallelism) .", "guide": "Dataflow"}