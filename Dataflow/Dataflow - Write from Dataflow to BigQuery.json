{"title": "Dataflow - Write from Dataflow to BigQuery", "url": "https://cloud.google.com/dataflow/docs/guides/write-to-bigquery", "abstract": "# Dataflow - Write from Dataflow to BigQuery\nThis document describes how to write data from Dataflow to BigQuery by using the Apache Beam [BigQuery I/O connector](https://beam.apache.org/documentation/io/built-in/google-bigquery/) .\nThe BigQuery I/O connector is available in the Apache Beam SDK. We recommend using the latest SDK version. For more information, see [Apache Beam 2.x SDKs](https://cloud.google.com/dataflow/docs/support/sdk-version-support-status#java) .\n[Cross-language support](https://beam.apache.org/documentation/programming-guide/#multi-language-pipelines) for Python is also available.\n**Note:** Depending on your scenario, consider using one of the [Google-provided Dataflow templates](/dataflow/docs/guides/templates/provided-templates) . Several of these write to BigQuery. If you are ingesting from Pub/Sub into BigQuery, consider using a Pub/Sub [BigQuery subscription](/pubsub/docs/bigquery) .\n", "content": "## Overview\nThe BigQuery I/O connector supports the following methods for writing to BigQuery:\n- `STORAGE_WRITE_API`. In this mode, the connector performs direct writes to BigQuery storage, using the [BigQuery Storage Write API](/bigquery/docs/write-api) . The Storage Write API combines streaming ingestion and batch loading into a single high-performance API. This mode guarantees exactly-once semantics.\n- `STORAGE_API_AT_LEAST_ONCE`. This mode also uses the Storage Write API, but provides at-least-once semantics. This mode results in lower latency for most pipelines. However, duplicate writes are possible.\n- `FILE_LOADS`. In this mode, the connector writes the input data to staging files in Cloud Storage. Then it runs a BigQuery [load job](/bigquery/docs/batch-loading-data) to load the data into BigQuery. The mode is the default for bounded`PCollections`, which are most commonly found in batch pipelines.\n- `STREAMING_INSERTS`. In this mode, the connector uses the [legacy streaming API](/bigquery/docs/streaming-data-into-bigquery) . This mode is the default for unbounded`PCollections`, but is not recommended for new projects.\nWhen choosing a write method, consider the following points:\n- Consider using`STORAGE_WRITE_API`or`STORAGE_API_AT_LEAST_ONCE`, especially for streaming pipelines. The Storage Write API is more efficient than file loads, because it writes the data directly to BigQuery storage, without intermediate staging files. It supports both batch and streaming pipelines.\n- If you run the pipeline using [at-least-once streaming mode](/dataflow/docs/guides/streaming-modes) , set the write mode to`STORAGE_API_AT_LEAST_ONCE`. This setting is more efficient and matches the semantics of at-least-once streaming mode.\n- File loads and Storage Write API have different [quotas and limits](/bigquery/quotas) .\n- Load jobs use either the shared BigQuery slot pool or reserved slots. To use reserved slots, run the load job in a project with a reservation assignment of type`PIPELINE`. Load jobs are free if you use the shared BigQuery slot pool. However, BigQuery does not make guarantees about the available capacity of the shared pool. For more information, see [Introduction to reservations](/bigquery/docs/reservations-intro) .## Parallelism\n- For `FILE_LOADS` and `STORAGE_WRITE_API` in streaming pipelines, the connector shards the data to a number of files or streams. In general, we recommend calling [withAutoSharding](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#withAutoSharding--) to enable auto-sharding.\n- For `FILE_LOADS` in batch pipelines, the connector writes data to partitioned files, which are then loaded into BigQuery in parallel.\n- For `STORAGE_WRITE_API` in batch pipelines, each worker creates one or more streams to write to BigQuery, determined by the total number of shards.\n- For `STORAGE_API_AT_LEAST_ONCE` , there is a single [default write stream](/bigquery/docs/write-api#default_stream) . Multiple workers append to this stream.## Performance\nThe following table shows performance metrics for various BigQuery I/O read options. The workloads were run on one `e2-standard2` worker, using the Apache Beam SDK 2.49.0 for Java. They did not use Runner v2.\n| 100 M records | 1 kB | 1 column | Throughput (bytes) | Throughput (elements)  |\n|:----------------------------------|:---------------------|:---------------------------|\n| Storage Write      | 55 MBps    | 54,000 elements per second |\n| Avro Load       | 78 MBps    | 77,000 elements per second |\n| Json Load       | 54 MBps    | 53,000 elements per second |\nThese metrics are based on simple batch pipelines. They are intended to compare performance between I/O connectors, and are not necessarily representative of real-world pipelines. Dataflow pipeline performance is complex, and is a function of VM type, the data being processed, the performance of external sources and sinks, and user code. Metrics are based on running the Java SDK, and aren't representative of the performance characteristics of other language SDKs. For more information, see [Beam IO Performance](https://beam.apache.org/performance/) .\n## Best practices\n### General\n- The Storage Write API has [quota limits](/bigquery/quotas#storage-limits) . The connector handles these limits for most pipelines. However, some scenarios can exhaust the available Storage Write API streams. For example, this issue might happen in a pipeline that uses auto-sharding and autoscaling with a large number of destinations, especially in long-running jobs with highly variable workloads. If this problem occurs, consider using `STORAGE_WRITE_API_AT_LEAST_ONCE` , which avoids the issue.\n- Use [Google Cloud metrics](/monitoring/api/metrics_gcp#gcp-bigquerystorage) to monitor your Storage Write API quota usage.\n- When using file loads, Avro typically outperforms JSON. To use Avro, call [withAvroFormatFunction](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#withAvroFormatFunction-org.apache.beam.sdk.transforms.SerializableFunction-) .\n- By default, load jobs run in the same project as the Dataflow job. To specify a different project, call [withLoadJobProjectId](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#withLoadJobProjectId-java.lang.String-) .\n- When using the Java SDK, consider creating a class that represents the schema of the BigQuery table. Then call [useBeamSchema](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#useBeamSchema--) in your pipeline to automatically convert between Apache Beam `Row` and BigQuery `TableRow` types. For an example of a schema class, see [ExampleModel.java](https://github.com/GoogleCloudPlatform/cloud-code-samples/blob/v1/java/java-dataflow-samples/read-pubsub-write-bigquery/src/main/java/com/cloudcode/dataflow/ExampleModel.java) .\n- If you load tables with complex schemas containing thousands of fields, consider calling [withMaxBytesPerPartition](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#withMaxBytesPerPartition-long-) to set a smaller maximum size for each load job.\n### Streaming\n- For streaming pipelines, we recommend using the Storage Write API ( `STORAGE_WRITE_API` or `STORAGE_API_AT_LEAST_ONCE` ).\n- A streaming pipeline can use file loads, but this approach has disadvantages:- It requires [windowing](/dataflow/docs/concepts/beam-programming-model#advanced_concepts) in order to write the files. You can't use the global window.\n- BigQuery loads files on a best-effort basis when using the [shared slot pool](/bigquery/pricing#free) . There can be a significant delay between when a record is written and when it's available in BigQuery.\n- If a load job fails \u2014 for example, due to bad data or a schema mismatch \u2014 the entire pipeline fails.\n- Consider using `STORAGE_WRITE_API_AT_LEAST_ONCE` when possible. It can result in duplicate records being written to BigQuery, but is less expensive than `STORAGE_WRITE_API` .\n- In general, avoid using `STREAMING_INSERTS` . Streaming inserts are more expensive than Storage Write API, and don't perform as well.\n- Data sharding can improve performance in streaming pipelines. For most pipelines, auto-sharding is a good starting point. However, you can tune sharding as follows:- For`STORAGE_WRITE_API`, call [withNumStorageWriteApiStreams](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#withNumStorageWriteApiStreams-int-) to set the number of write streams.\n- For`FILE_LOADS`, call [withNumFileShards](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#withNumFileShards-int-) to set the number of file shards.\n- If you use streaming inserts, we recommend setting [retryTransientErrors](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/InsertRetryPolicy.html#retryTransientErrors--) as the [retry policy](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html#withFailedInsertRetryPolicy-org.apache.beam.sdk.io.gcp.bigquery.InsertRetryPolicy-) .\n### Handle row-level errors\nThis section describes how to handle errors that might happen at the row level, for example because of badly formed input data or schema mismatches.\nFor Storage Write API, any rows that can't be written are placed into a separate `PCollection` . To get this collection, call [getFailedStorageApiInserts](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/WriteResult.html#getFailedStorageApiInserts--) on the `WriteResult` object. For an example of this approach, see [Stream data to BigQuery](#stream-data) .\nIt's a good practice to send the errors to a dead-letter queue or table, for later processing. For more information about this pattern, see [BigQueryIO dead letter pattern](https://beam.apache.org/documentation/patterns/bigqueryio/#bigqueryio-deadletter-pattern) .\nFor `FILE_LOADS` , if an error occurs while loading the data, the load job fails and the pipeline throws a runtime exception. You can view the error in the Dataflow logs or look at the BigQuery job history. The I/O connector does not return information about individual failed rows.\nFor more information about troubleshooting errors, see [BigQuery connector errors](/dataflow/docs/guides/common-errors#connector_errors) .\n## Examples\n### Write to an existing table\nThe following example creates a batch pipeline that writes a `PCollection<MyData>` to BigQuery, where `MyData` is a custom datatype.\nThe [BigQueryIO.<T>write()](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.html#write--) method returns a [BigQueryIO.Write<T>](https://beam.apache.org/releases/javadoc/current/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.Write.html) type, which is used to configure the write operation. For more information, see [Writing to a table](https://beam.apache.org/documentation/io/built-in/google-bigquery/#writing-to-a-table) in the Apache Beam documentation. This code example writes to an existing table ( `CREATE_NEVER` ) and appends the new rows to the table ( `WRITE_APPEND` ).\nTo authenticate to Dataflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BigQueryWrite.java) \n```\nimport com.google.api.services.bigquery.model.TableRow;import java.util.Arrays;import java.util.List;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.coders.DefaultCoder;import org.apache.beam.sdk.extensions.avro.coders.AvroCoder;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.WriteDisposition;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.Create;public class BigQueryWrite {\u00a0 // A custom datatype for the source data.\u00a0 @DefaultCoder(AvroCoder.class)\u00a0 public static class MyData {\u00a0 \u00a0 public String name;\u00a0 \u00a0 public Long age;\u00a0 \u00a0 public MyData() {}\u00a0 \u00a0 public MyData(String name, Long age) {\u00a0 \u00a0 \u00a0 this.name = name;\u00a0 \u00a0 \u00a0 this.age = age;\u00a0 \u00a0 }\u00a0 }\u00a0 public static void main(String[] args) {\u00a0 \u00a0 // Example source data.\u00a0 \u00a0 final List<MyData> data = Arrays.asList(\u00a0 \u00a0 \u00a0 \u00a0 new MyData(\"Alice\", 40L),\u00a0 \u00a0 \u00a0 \u00a0 new MyData(\"Bob\", 30L),\u00a0 \u00a0 \u00a0 \u00a0 new MyData(\"Charlie\", 20L)\u00a0 \u00a0 );\u00a0 \u00a0 // Parse the pipeline options passed into the application. Example:\u00a0 \u00a0 // \u00a0 --projectId=$PROJECT_ID --datasetName=$DATASET_NAME --tableName=$TABLE_NAME\u00a0 \u00a0 // For more information, see https://beam.apache.org/documentation/programming-guide/#configuring-pipeline-options\u00a0 \u00a0 PipelineOptionsFactory.register(ExamplePipelineOptions.class);\u00a0 \u00a0 ExamplePipelineOptions options = PipelineOptionsFactory.fromArgs(args)\u00a0 \u00a0 \u00a0 \u00a0 .withValidation()\u00a0 \u00a0 \u00a0 \u00a0 .as(ExamplePipelineOptions.class);\u00a0 \u00a0 // Create a pipeline and apply transforms.\u00a0 \u00a0 Pipeline pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 // Create an in-memory PCollection of MyData objects.\u00a0 \u00a0 \u00a0 \u00a0 .apply(Create.of(data))\u00a0 \u00a0 \u00a0 \u00a0 // Write the data to an exiting BigQuery table.\u00a0 \u00a0 \u00a0 \u00a0 .apply(BigQueryIO.<MyData>write()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .to(String.format(\"%s:%s.%s\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getProjectId(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getDatasetName(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getTableName()))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withFormatFunction(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (MyData x) -> new TableRow().set(\"user_name\", x.name).set(\"age\", x.age))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withCreateDisposition(CreateDisposition.CREATE_NEVER)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withWriteDisposition(WriteDisposition.WRITE_APPEND)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withMethod(Write.Method.STORAGE_WRITE_API));\u00a0 \u00a0 pipeline.run().waitUntilFinish();\u00a0 }}\n```### Write to a new or existing table\nThe following example creates a new table if the destination table does not exist, by setting the [create disposition](https://beam.apache.org/documentation/io/built-in/google-bigquery/#create-disposition) to `CREATE_IF_NEEDED` . When you use this option, you must provide a table schema. The connector uses this schema if it creates a new table.\nTo authenticate to Dataflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BigQueryWriteWithSchema.java) \n```\nimport com.google.api.services.bigquery.model.TableFieldSchema;import com.google.api.services.bigquery.model.TableRow;import com.google.api.services.bigquery.model.TableSchema;import java.util.Arrays;import java.util.List;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.coders.DefaultCoder;import org.apache.beam.sdk.extensions.avro.coders.AvroCoder;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.transforms.Create;public class BigQueryWriteWithSchema {\u00a0 // A custom datatype for the source data.\u00a0 @DefaultCoder(AvroCoder.class)\u00a0 public static class MyData {\u00a0 \u00a0 public String name;\u00a0 \u00a0 public Long age;\u00a0 \u00a0 public MyData() {}\u00a0 \u00a0 public MyData(String name, Long age) {\u00a0 \u00a0 \u00a0 this.name = name;\u00a0 \u00a0 \u00a0 this.age = age;\u00a0 \u00a0 }\u00a0 }\u00a0 public static void main(String[] args) {\u00a0 \u00a0 // Example source data.\u00a0 \u00a0 final List<MyData> data = Arrays.asList(\u00a0 \u00a0 \u00a0 \u00a0 new MyData(\"Alice\", 40L),\u00a0 \u00a0 \u00a0 \u00a0 new MyData(\"Bob\", 30L),\u00a0 \u00a0 \u00a0 \u00a0 new MyData(\"Charlie\", 20L)\u00a0 \u00a0 );\u00a0 \u00a0 // Define a table schema. A schema is required for write disposition CREATE_IF_NEEDED.\u00a0 \u00a0 TableSchema schema = new TableSchema()\u00a0 \u00a0 \u00a0 \u00a0 .setFields(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Arrays.asList(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new TableFieldSchema()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setName(\"user_name\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setType(\"STRING\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMode(\"REQUIRED\"),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new TableFieldSchema()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setName(\"age\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setType(\"INT64\") // Defaults to NULLABLE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 // Parse the pipeline options passed into the application. Example:\u00a0 \u00a0 // \u00a0 --projectId=$PROJECT_ID --datasetName=$DATASET_NAME --tableName=$TABLE_NAME\u00a0 \u00a0 // For more information, see https://beam.apache.org/documentation/programming-guide/#configuring-pipeline-options\u00a0 \u00a0 PipelineOptionsFactory.register(ExamplePipelineOptions.class);\u00a0 \u00a0 ExamplePipelineOptions options = PipelineOptionsFactory.fromArgs(args)\u00a0 \u00a0 \u00a0 \u00a0 .withValidation()\u00a0 \u00a0 \u00a0 \u00a0 .as(ExamplePipelineOptions.class);\u00a0 \u00a0 // Create a pipeline and apply transforms.\u00a0 \u00a0 Pipeline pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 // Create an in-memory PCollection of MyData objects.\u00a0 \u00a0 \u00a0 \u00a0 .apply(Create.of(data))\u00a0 \u00a0 \u00a0 \u00a0 // Write the data to a new or existing BigQuery table.\u00a0 \u00a0 \u00a0 \u00a0 .apply(BigQueryIO.<MyData>write()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .to(String.format(\"%s:%s.%s\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getProjectId(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getDatasetName(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getTableName()))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withFormatFunction(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (MyData x) -> new TableRow().set(\"user_name\", x.name).set(\"age\", x.age))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withCreateDisposition(CreateDisposition.CREATE_IF_NEEDED)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withSchema(schema)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withMethod(Write.Method.STORAGE_WRITE_API)\u00a0 \u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 pipeline.run().waitUntilFinish();\u00a0 }}\n```### Stream data to BigQuery\nThe following example shows how to stream data using exactly-once semantics, by setting the write mode to `STORAGE_WRITE_API`\nNot all streaming pipelines require exactly-once semantics. For example, you might be able to [manually remove duplicates](/bigquery/docs/streaming-data-into-bigquery#manually_removing_duplicates) from the destination table. If the possibility of duplicate records is acceptable for your scenario, consider using at-least-once semantics by setting the [write method](#write_method) to `STORAGE_API_AT_LEAST_ONCE` . This method is generally more efficient and results in lower latency for most pipelines.\nTo authenticate to Dataflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataflow/snippets/src/main/java/com/example/dataflow/BigQueryStreamExactlyOnce.java) \n```\nimport com.google.api.services.bigquery.model.TableRow;import org.apache.beam.sdk.Pipeline;import org.apache.beam.sdk.PipelineResult;import org.apache.beam.sdk.coders.StringUtf8Coder;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.CreateDisposition;import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO.Write.WriteDisposition;import org.apache.beam.sdk.options.PipelineOptionsFactory;import org.apache.beam.sdk.testing.TestStream;import org.apache.beam.sdk.transforms.MapElements;import org.apache.beam.sdk.values.TimestampedValue;import org.apache.beam.sdk.values.TypeDescriptor;import org.apache.beam.sdk.values.TypeDescriptors;import org.joda.time.Duration;import org.joda.time.Instant;public class BigQueryStreamExactlyOnce {\u00a0 // Create a PTransform that sends simulated streaming data. In a real application, the data\u00a0 // source would be an external source, such as Pub/Sub.\u00a0 private static TestStream<String> createEventSource() {\u00a0 \u00a0 Instant startTime = new Instant(0);\u00a0 \u00a0 return TestStream.create(StringUtf8Coder.of())\u00a0 \u00a0 \u00a0 \u00a0 .advanceWatermarkTo(startTime)\u00a0 \u00a0 \u00a0 \u00a0 .addElements(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TimestampedValue.of(\"Alice,20\", startTime),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TimestampedValue.of(\"Bob,30\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 startTime.plus(Duration.standardSeconds(1))),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TimestampedValue.of(\"Charles,40\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 startTime.plus(Duration.standardSeconds(2))),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TimestampedValue.of(\"Dylan,Invalid value\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 startTime.plus(Duration.standardSeconds(2))))\u00a0 \u00a0 \u00a0 \u00a0 .advanceWatermarkToInfinity();\u00a0 }\u00a0 public static PipelineResult main(String[] args) {\u00a0 \u00a0 // Parse the pipeline options passed into the application. Example:\u00a0 \u00a0 // \u00a0 --projectId=$PROJECT_ID --datasetName=$DATASET_NAME --tableName=$TABLE_NAME\u00a0 \u00a0 // For more information, see https://beam.apache.org/documentation/programming-guide/#configuring-pipeline-options\u00a0 \u00a0 PipelineOptionsFactory.register(ExamplePipelineOptions.class);\u00a0 \u00a0 ExamplePipelineOptions options = PipelineOptionsFactory.fromArgs(args)\u00a0 \u00a0 \u00a0 \u00a0 .withValidation()\u00a0 \u00a0 \u00a0 \u00a0 .as(ExamplePipelineOptions.class);\u00a0 \u00a0 options.setStreaming(true);\u00a0 \u00a0 // Create a pipeline and apply transforms.\u00a0 \u00a0 Pipeline pipeline = Pipeline.create(options);\u00a0 \u00a0 pipeline\u00a0 \u00a0 \u00a0 \u00a0 // Add a streaming data source.\u00a0 \u00a0 \u00a0 \u00a0 .apply(createEventSource())\u00a0 \u00a0 \u00a0 \u00a0 // Map the event data into TableRow objects.\u00a0 \u00a0 \u00a0 \u00a0 .apply(MapElements\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .into(TypeDescriptor.of(TableRow.class))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via((String x) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String[] columns = x.split(\",\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return new TableRow().set(\"user_name\", columns[0]).set(\"age\", columns[1]);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }))\u00a0 \u00a0 \u00a0 \u00a0 // Write the rows to BigQuery\u00a0 \u00a0 \u00a0 \u00a0 .apply(BigQueryIO.writeTableRows()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .to(String.format(\"%s:%s.%s\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getProjectId(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getDatasetName(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 options.getTableName()))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withCreateDisposition(CreateDisposition.CREATE_NEVER)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withWriteDisposition(WriteDisposition.WRITE_APPEND)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withMethod(Write.Method.STORAGE_WRITE_API)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // For exactly-once processing, set the triggering frequency.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .withTriggeringFrequency(Duration.standardSeconds(5)))\u00a0 \u00a0 \u00a0 \u00a0 // Get the collection of write errors.\u00a0 \u00a0 \u00a0 \u00a0 .getFailedStorageApiInserts()\u00a0 \u00a0 \u00a0 \u00a0 .apply(MapElements.into(TypeDescriptors.strings())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Process each error. In production systems, it's useful to write the errors to\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // another destination, such as a dead-letter table or queue.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 x -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"Failed insert: \" + x.getErrorMessage());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"Row: \" + x.getRow());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return \"\";\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }));\u00a0 \u00a0 return pipeline.run();\u00a0 }}\n```\n## What's next\n- Learn more about the BigQuery I/O connector in the [Apache Beam documentation](https://beam.apache.org/documentation/io/built-in/google-bigquery) .\n- Read about [Streaming data into BigQuery using Storage Write API](https://cloud.google.com/blog/products/data-analytics/streaming-data-into-bigquery-using-storage-write-api) (blog post).", "guide": "Dataflow"}