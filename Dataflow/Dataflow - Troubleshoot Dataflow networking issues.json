{"title": "Dataflow - Troubleshoot Dataflow networking issues", "url": "https://cloud.google.com/dataflow/docs/guides/troubleshoot-networking", "abstract": "# Dataflow - Troubleshoot Dataflow networking issues\nThis page shows you how to resolve issues with Dataflow networking.\n", "content": "## Network interface must specify a subnet if the network resource is in custom subnet mode\nThe following error occurs when you run a Dataflow job:\n```\nWorkflow failed. Causes: Invalid Error: Message: Invalid value for field\n'resource.properties.networkInterfaces[0].subnetwork': ''. Network interface\nmust specify a subnet if the network resource is in custom subnet mode. HTTP\nCode: 400\n```\nThis issue occurs if the VPC network named `default` was converted from an auto mode VPC network to a custom mode VPC network.\nTo resolve this issue, specify the subnetwork parameter when using a custom mode VPC network. For more information, see [Specify a network and subnetwork](/dataflow/docs/guides/specifying-networks) .\n## Cross-project references for this resource are not allowed\nThe following error occurs when you run a Dataflow job on a Shared VPC network:\n```\nInvalid value for field 'resource.properties.networkInterfaces[0].subnetwork':\n'https://www.googleapis.com/compute/v1/projects/PROJECT/regions/REGION/subnetworks/SUBNETWORK'.\nCross-project references for this resource are not allowed.\n```\nThis issue occurs if you specify a subnetwork in a Shared VPC network, but the service project isn't attached to the Shared VPC host project.\nTo resolve this issue, a Shared VPC Admin must [attach the service project to the host project](/vpc/docs/provisioning-shared-vpc#create-shared) .\n## Network or subnetwork is not accessible to Dataflow service account or does not exist\nOne of the following errors occurs when you try to run a Dataflow job. The job fails.\n```\nWorkflow failed. Causes: Network default is not accessible to Dataflow Service\naccount or does not exist\n```\n```\nWorkflow failed. Causes: Subnetwork SUBNETWORK is not\naccessible to Dataflow Service account or does not exist\n```\nThis issue can occur for the following reasons:\n- You omit both the subnetwork and network parameters when you create the Dataflow job, but an auto mode VPC network named`default`doesn't exist in your project. You might not have a default network if the default network was deleted or if an organization policy constraint prevents you from creating a default network.\n- The subnetwork is missing.\n- The subnetwork parameter is specified incorrectly.\n- The required permissions for the Dataflow service account are missing.\nTo resolve this issue, follow the [guidelines for specifying a network and subnetwork](/dataflow/docs/guides/specifying-networks#specifying_a_network_and_a_subnetwork) .\n## RPC timed out or failed to connect on ports 12345 or 12346\nOne of the following errors occurs when you run a Dataflow job that doesn't use Streaming Engine or Dataflow Shuffle. The job gets stuck or fails.\nFor streaming jobs:\n```\nRpc to WORKER_HARNESS:12345 completed with error\nUNAVAILABLE: failed to connect to all addresses; last error : UNKNOWN:\nipv4:WORKER_IP_ADDRESS:12345: Failed to connect to remote\nhost: FD Shutdown\n```\nFor batch jobs:\n```\n(g)RPC timed out when SOURCE_WORKER_HARNESS talking to\nDESTINATION_WORKER_HARNESS:12346.\n```\nThis issue occurs if a firewall rule that allows network traffic on TCP ports `12345` and `12346` is missing. When the job uses multiple workers, the workers aren't able to communicate with each other.\nTo resolve this issue, see the troubleshooting steps in [DEADLINE_EXCEEDED or Server Unresponsive](/dataflow/docs/guides/common-errors#tsg-rpc-timeout) .\n## Single worker is repeatedly started and stopped\nThe following issue occurs when you launch a Dataflow job. On the Dataflow job's **Job metrics** page, the **CPU utilization (All Workers)** chart shows that a worker is repeatedly started and then stopped after a few minutes. Only one worker is available at a given time.\nThe following error occurs:\n```\nThe Dataflow job appears to be stuck because no worker activity has been seen\nin the last 1h. Please check the worker logs in Stackdriver Logging.\n```\n**Note:** If you see a `Dataflow job appears to be stuck` error and you also see an `Error syncing pod` error, the issue is likely a container failure. For more information, see [Error syncing pod ... failed to \"StartContainer\"](/dataflow/docs/guides/common-errors#error-syncing-pod) .\nNo worker logs are created.\nIn the job logs, multiple messages similar to the following might appear:\n```\nAutoscaling: Raised the number of workers to 1 based on the rate of progress in\nthe currently running stage(s).\n```\nThis issue occurs if the VPC network doesn't have a default route to the internet and a default route to the subnetwork.\nTo resolve this issue, [add default routes](/vpc/docs/using-routes#adding_and_removing_routes) to your VPC network. For more information, see [Internet access for Dataflow](/dataflow/docs/guides/routes-firewall#internet_access_for) .\n## Subnetwork does not have Private Google Access\nThe following error occurs when you launch a Dataflow job in which external IP addresses are disabled:\n```\nWorkflow failed. Causes: Subnetwork SUBNETWORK on project\nPROJECT_ID network NETWORK in\nregion REGION does not have Private Google Access, which\nis required for usage of private IP addresses by the Dataflow workers.\n```\nThis issue occurs if you [turn off external IP addresses](/dataflow/docs/guides/routes-firewall#turn_off_external_ip_address) without enabling Private Google Access.\nTo resolve this issue, [enable Private Google Access](/vpc/docs/configure-private-google-access) for the subnetwork that the Dataflow job uses.", "guide": "Dataflow"}