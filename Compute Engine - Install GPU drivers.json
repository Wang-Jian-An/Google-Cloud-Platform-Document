{"title": "Compute Engine - Install GPU drivers", "url": "https://cloud.google.com/compute/docs/instances", "abstract": "# Compute Engine - Install GPU drivers\nAfter you create a virtual machine (VM) instance with one or more GPUs, your system requires NVIDIA device drivers so that your applications can access the device. Make sure your virtual machine (VM) instances have enough free disk space. You should choose at least 40\u00a0GB for the boot disk when creating the new VM.\nTo install the drivers, you have two options to choose from:\n- If you need GPUs for hardware accelerated 3D graphics such as remote desktop or gaming, see [Install drivers for NVIDIA RTX Virtual Workstations (vWS)](/compute/docs/gpus/install-grid-drivers) .\n- For other workloads, follow the instructions in this document to install the NVIDIA driver. **Pro Tip:** Alternatively, you can skip this setup by creating VMs with [ Deep Learning VMimages](/compute/docs/gpus/create-vm-with-gpus#dlvm-image) . Deep Learning VM images have NVIDIA drivers pre-installed, and also include other machine learning applications such as TensorFlow and PyTorch.", "content": "## NVIDIA driver, CUDA toolkit, and CUDA runtime versions\nThere are different versioned components of drivers and runtime that might be needed in your environment. These include the following components:\n- NVIDIA driver\n- CUDA toolkit\n- CUDA runtime\nWhen installing these components, you have the ability to configure your environment to suit your needs. For example, if you have an earlier version of Tensorflow that works best with an earlier version of the CUDA toolkit, but the GPU that you want to use requires a later version of the NVIDIA driver, then you can install an earlier version of a CUDA toolkit along with a later version of the NVIDIA driver.\nHowever, you must make sure that your NVIDIA driver and CUDA toolkit versions are compatible. For CUDA toolkit and NVIDIA driver compatibility, see the NVIDIA documentation about [CUDA compatibility](https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver) .\n## Required NVIDIA driver versions\nFor NVIDIA GPUs running on Compute Engine, the following NVIDIA driver versions are recommended.\n| GPU model       | Linux            | Windows      |\n|:------------------------------------|:---------------------------------------------------|:-----------------------------|\n| NVIDIA H100       | R525: 525.125.06 or later R535: 535.86.10 or later | nan       |\n| NVIDIA L4       | 525.60.13 or later         | 528.89      |\n| NVIDIA A100, T4, P4, P100, and V100 | 470.57.02 or later         | 471.41 or later    |\n| NVIDIA K80 (End of support)   | 410.79 - latest R470 version      | 426.00 - latest R470 version |\nFor K80 GPUs, NVIDIA has announced that the [R470 driver](https://docs.nvidia.com/datacenter/tesla/index.html#r470-driver-release-notes) branch will be the final driver version to receive debug support. To review this update, see [NVIDIA Software Support Matrix](https://docs.nvidia.com/datacenter/tesla/drivers/index.html#cuda-arch-matrix) .\n## Install GPU drivers on VMs\nOne way to install the NVIDIA driver on most VMs is to install the [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive) .\n**Note:** With the exception of Windows, these instructions do not work on VMs that have [Secure Boot](/security/shielded-cloud/shielded-vm#secure-boot) enabled. For VMs that have Secure Boot enabled, see [Installing GPU drivers (Secure Boot VMs)](#secure-boot) .\nTo install the NVIDIA toolkit, complete the following steps:\n- Select a [CUDA toolkit](https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver) that supports the [minimum driver](#minimum-driver) that you need.\n- [Connect to the VM](/compute/docs/instances/connecting-to-instance) where you want to install the driver.\n- On your VM, download and install the CUDA toolkit. The installation package and guide for the a recommended toolkit is found in the following table. Before you install the toolkit, make sure you complete the pre-installation steps found in the installation guide.| GPU model            | Minimum recommended CUDA toolkit version      | Installation instructions for minimum version        |\n|:--------------------------------------------------------|:-------------------------------------------------------------|:--------------------------------------------------------------------------|\n| NVIDIA H100            | Linux: CUDA Toolkit 12.2 Update 1 Windows: N/A    | Linux: CUDA 12.2 installation guide Windows: N/A       |\n| NVIDIA L4            | Linux: CUDA Toolkit 12.1 Windows: CUDA Toolkit 12.2 Update 2 | Linux: CUDA 12.1 installation guide Windows: CUDA 12.2 installation guide |\n| NVIDIA A100 NVIDIA T4 NVIDIA V100 NVIDIA P100 NVIDIA P4 | Linux: CUDA Toolkit 11.4 Windows: CUDA Toolkit 11.4   | Linux: CUDA 11.4 installation guide Windows: CUDA 11.4 installation guide |## Install GPU drivers by using installation script\nYou can use the following scripts to automate the installation process. To review these scripts, see the [GitHub repository](https://github.com/GoogleCloudPlatform/compute-gpu-installation) .- This script won't work on Linux VMs that have [Secure Boot](/security/shielded-cloud/shielded-vm#secure-boot) enabled. For Linux VMs that have Secure Boot enabled, see [Installing GPU drivers (Secure Boot VMs)](#secure-boot) .\n- If you have version 2.38.0 or later of the [Ops Agent](/stackdriver/docs/solutions/agents/ops-agent/install-index) collecting GPU metrics on your VM, you must stop the agent before you can install or upgrade your GPU drivers using this installation script.After you have completed the installation or upgrade of the GPU driver, you must then reboot the VM.To stop the Ops Agent, run the following command:```\nsudo systemctl stop google-cloud-ops-agent\n```\n**Supported operating systems** \nThe Linux installation script was tested on the following operating systems:- CentOS 7 and 8\n- Debian 10 and 11\n- Red Hat Enterprise Linux (RHEL) 7, 8, and 9\n- Rocky Linux 8 and 9\n- Ubuntu 20 and 22\nIf you use this script on other operating systems, the installation will fail. For Linux VMs, this script installs only the NVIDIA driver.- Ensure that Python 3 is installed on your operating system.\n- Download the installation script.```\ncurl https://raw.githubusercontent.com/GoogleCloudPlatform/compute-gpu-installation/main/linux/install_gpu_driver.py --output install_gpu_driver.py\n```\n- Run the installation script.```\nsudo python3 install_gpu_driver.py\n```The script takes some time to run. It might restart your VM. If the VM restarts, run the script again to continue the installation.\n- Verify the installation. See [Verify the GPU driver install](#verify-driver-install) .\nThis installation script can be used on VMs that have secure boot enabled.- For Windows VMs that use a G2 machine series, this script installs only the NVIDIA driver.\n- For other machine types, the script installs the NVIDIA driver and CUDA toolkit.\nOpen a PowerShell terminal as an administrator, then complete the following steps:- If you are using Windows Server 2016, set the Transport Layer Security (TLS) version to 1.2.```\n[Net.ServicePointManager]::SecurityProtocol = 'Tls12'\n```\n- Download the script.```\nInvoke-WebRequest https://github.com/GoogleCloudPlatform/compute-gpu-installation/raw/main/windows/install_gpu_driver.ps1 -OutFile C:\\install_gpu_driver.ps1\n```\n- Run the script.```\nC:\\install_gpu_driver.ps1\n```The script takes some time to run. No command prompts are given during the installation process. Once the script exits, the driver is installed.This script installs the drivers in the following default location on your VM: `C:\\Program Files\\NVIDIA Corporation\\` .\n- Verify the installation. See [Verify the GPU driver install](#verify-driver-install) .## Install GPU drivers (Secure Boot VMs)\nThese instructions are for installing GPU drivers on Linux VMs that use [Secure Boot](/security/shielded-cloud/shielded-vm#secure-boot) .\nIf you are using either a Windows VM or a Linux VM that doesn't use Secure Boot, review one of the following instructions instead:\n- [Install GPU drivers on VMs](/compute/docs/gpus/install-drivers-gpu#no-secure-boot) \n- [Install GPU drivers by using installation script](/compute/docs/gpus/install-drivers-gpu#install-script) \nInstallation of the driver on a Secure Boot VM is different for Linux VMs, because these VMs require all kernel modules to be signed by the key trusted by the system.\nThese instructions are only available for Secure boot Linux VMs that run on Ubuntu 18.04, 20.04, and 22.04 operating systems. Support for more Linux operating systems is in progress.\nTo install GPU drivers on your Ubuntu VMs that use Secure Boot, complete the following steps:\n- [Connect to the VM](/compute/docs/instances/connecting-to-instance) where you want to install the driver.\n- Update the repository.```\n sudo apt-get update\n```\n- Search for the most recent NVIDIA kernel module package or the version you want. This package contains NVIDIA kernel modules signed by the Ubuntu key. If you want to find an earlier version, change the number for the tail parameter to get an earlier version. For example, specify `tail -n 2` .\nFor Ubuntu PRO and LTS, run the following command:\n```\nNVIDIA_DRIVER_VERSION=$(sudo apt-cache search 'linux-modules-nvidia-[0-9]+-gcp$' | awk '{print $1}' | sort | tail -n 1 | head -n 1 | awk -F\"-\" '{print $4}')\n```\nFor Ubuntu PRO FIPS, run the following commands:- Enable Ubuntu FIPS updates.```\nsudo ua enable fips-updates\n```\n- Shutdown and reboot```\nsudo shutdown -r now\n```\n- Get the latest package.```\nNVIDIA_DRIVER_VERSION=$(sudo apt-cache search 'linux-modules-nvidia-[0-9]+-gcp-fips$' | awk '{print $1}' | sort | tail -n 1 | head -n 1 | awk -F\"-\" '{print $4}')\n```\nYou can check the picked driver version by running `echo $NVIDIA_DRIVER_VERSION` . The output is a version string like `455` .\n- Install the kernel module package and corresponding NVIDIA driver. **Note:** Installing the package might upgrade your kernel.```\n sudo apt install linux-modules-nvidia-${NVIDIA_DRIVER_VERSION}-gcp nvidia-driver-${NVIDIA_DRIVER_VERSION}\n```If the command failed with the `package not found error` , the latest NVIDIA driver might be missing from the repository. Retry the previous step and select an earlier driver version by changing the tail number.\n- [Verify](/compute/docs/gpus/install-drivers-gpu#verify-linux) that the NVIDIA driver is installed. You might need to reboot the VM.\n- If you rebooted the system to verify the NVIDIA version. After the reboot, you need to reset the `NVIDIA_DRIVER_VERSION` variable by rerunning the command that you used in step 3.\n- Configure APT to use the NVIDIA package repository.- To help APT pick the correct dependency, pin the repositories as follows:```\nsudo tee /etc/apt/preferences.d/cuda-repository-pin-600 > /dev/null <<EOL\nPackage: nsight-compute\nPin: origin *ubuntu.com*\nPin-Priority: -1\nPackage: nsight-systems\nPin: origin *ubuntu.com*\nPin-Priority: -1\nPackage: nvidia-modprobe\nPin: release l=NVIDIA CUDA\nPin-Priority: 600\nPackage: nvidia-settings\nPin: release l=NVIDIA CUDA\nPin-Priority: 600\nPackage: *\nPin: release l=NVIDIA CUDA\nPin-Priority: 100\nEOL\n```\n- Install `software-properties-common` . This is required if you are using Ubuntu minimal images.```\n sudo apt install software-properties-common\n \n```\n- Set the Ubuntu version.\nFor Ubuntu 18.04, run the following command:\n```\nexport UBUNTU_VERSION=ubuntu1804/x86_64\n```\nFor Ubuntu 20.04, run the following command:\n```\nexport UBUNTU_VERSION=ubuntu2004/x86_64\n```\nFor Ubuntu 22.04, run the following command:\n```\nexport UBUNTU_VERSION=ubuntu2204/x86_64\n```\n- Download the `cuda-keyring` package.```\nwget https://developer.download.nvidia.com/compute/cuda/repos/$UBUNTU_VERSION/cuda-keyring_1.0-1_all.deb\n```\n- Install the `cuda-keyring` package.```\nsudo dpkg -i cuda-keyring_1.0-1_all.deb\n```\n- Add the NVIDIA repository.```\nsudo add-apt-repository \"deb https://developer.download.nvidia.com/compute/cuda/repos/$UBUNTU_VERSION/ /\"\n```If prompted, select the default action to keep your current version.\n- Find the compatible CUDA driver version.The following script determines the latest CUDA driver version that is compatible with the NVIDIA driver we just installed:```\n CUDA_DRIVER_VERSION=$(apt-cache madison cuda-drivers | awk '{print $3}' | sort -r | while read line; do\n  if dpkg --compare-versions $(dpkg-query -f='${Version}\\n' -W nvidia-driver-${NVIDIA_DRIVER_VERSION}) ge $line ; then\n  echo \"$line\"\n  break\n  fi\n done)\n```You can check the CUDA driver version by running `echo $CUDA_DRIVER_VERSION` . The output is a version string like `455.32.00-1` .\n- Install CUDA drivers with the version identified from the previous step.```\n sudo apt install cuda-drivers-${NVIDIA_DRIVER_VERSION}=${CUDA_DRIVER_VERSION} cuda-drivers=${CUDA_DRIVER_VERSION}\n```\n- Optional: Hold back `dkms` packages.After enabling Secure Boot, all kernel modules must be signed to be loaded. Kernel modules built by `dkms` don't work on the VM because they aren't properly signed by default. This is an optional step, but it can help prevent you from accidentally installing other `dkms` packages in the future.To hold `dkms` packages, run the following command:```\n sudo apt-get remove dkms && sudo apt-mark hold dkms\n```\n- Install CUDA toolkit and runtime.Pick the suitable CUDA version. The following script determines the latest CUDA version that is compatible with the CUDA driver we just installed:```\n CUDA_VERSION=$(apt-cache showpkg cuda-drivers | grep -o 'cuda-runtime-[0-9][0-9]-[0-9],cuda-drivers [0-9\\\\.]*' | while read line; do\n  if dpkg --compare-versions ${CUDA_DRIVER_VERSION} ge $(echo $line | grep -Eo '[[:digit:]]+\\.[[:digit:]]+') ; then\n  echo $(echo $line | grep -Eo '[[:digit:]]+-[[:digit:]]')\n  break\n  fi\n done)\n```You can check the CUDA version by running `echo $CUDA_VERSION` . The output is a version string like `11-1` .\n- Install the CUDA package.```\n sudo apt install cuda-${CUDA_VERSION}\n```\n- Verify the CUDA installation.```\n sudo nvidia-smi\n /usr/local/cuda/bin/nvcc --version\n```The first command prints the GPU information. The second command prints the installed CUDA compiler version.## Verify the GPU driver install\nAfter completing the driver installation steps, verify that the driver installed and initialized properly.\n[Connect to the Linux instance](/compute/docs/instances/connecting-to-instance) and use the `nvidia-smi` command to verify that the driver is running properly.\n```\nsudo nvidia-smi\n```\nThe output is similar to the following:\n```\nTue Mar 21 19:50:15 2023\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 530.30.02 Driver Version: 530.30.02 CUDA Version: 12.1  |\n|-------------------------------+----------------------+----------------------+\n| GPU Name  Persistence-M| Bus-Id  Disp.A | Volatile Uncorr. ECC |\n| Fan Temp Perf Pwr:Usage/Cap|   Memory-Usage | GPU-Util Compute M. |\n|        |      |    MIG M. |\n|===============================+======================+======================|\n| 0 NVIDIA L4   Off | 00000000:00:03.0 Off |     0 |\n| N/A 63C P0 30W / 75W |  0MiB / 23034MiB |  8%  Default |\n|        |      |     N/A |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes:                 |\n| GPU GI CI  PID Type Process name     GPU Memory |\n|  ID ID             Usage  |\n|=============================================================================|\n| No running processes found             |\n+-----------------------------------------------------------------------------+\n```\nIf this command fails, review the following:- Check if there is any GPU attached to the VM.Use the following command to check for any NVIDIA PCI devices:`sudo lspci | grep -i \"nvidia\"` .\n- Check that the driver kernel version and the VM kernel version are the same.- To check the VM kernel version, run`uname -r`.\n- To check the driver kernel version, run`sudo apt-cache show linux-modules-nvidia-` `` `-gcp`.\nIf the versions don't match, reboot the VM to the new kernel version.\n **Note:** If you used the automatic installation script, you can also use the command `python3 install_gpu_driver.py verify` to automatically compile and run sample scripts that use the CUDA framework.\n [Connect to the Windows Server instance](/compute/docs/instances/connecting-to-windows) and open a PowerShell terminal, then run the following command to verify that the driver is running properly.\n```\nnvidia-smi\n```\nThe output is similar to the following:\n```\n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 537.13     Driver Version: 537.13  CUDA Version: 12.2  |\n|-----------------------------------------+----------------------+----------------------+\n| GPU Name      TCC/WDDM | Bus-Id  Disp.A | Volatile Uncorr. ECC |\n| Fan Temp Perf   Pwr:Usage/Cap |   Memory-Usage | GPU-Util Compute M. |\n|           |      |    MIG M. |\n|=========================================+======================+======================|\n| 0 NVIDIA L4     WDDM | 00000000:00:03.0 Off |     0 |\n| N/A 66C P8    17W / 72W | 128MiB / 23034MiB |  0%  Default |\n|           |      |     N/A |\n+-----------------------------------------+----------------------+----------------------+\n+---------------------------------------------------------------------------------------+\n| Processes:                   |\n| GPU GI CI  PID Type Process name       GPU Memory |\n|  ID ID                Usage  |\n|=======================================================================================|\n| 0 N/A N/A  4888 C+G ...CBS_cw5n1h2txyewy\\TextInputHost.exe N/A  |\n| 0 N/A N/A  5180 C+G ....Search_cw5n1h2txyewy\\SearchApp.exe N/A  |\n+---------------------------------------------------------------------------------------+\n```\n## What's next?\n- To monitor GPU performance, see [Monitor GPU performance](/compute/docs/gpus/monitor-gpus) .\n- To handle GPU host maintenance, see [Handle GPU host maintenance events](/compute/docs/gpus/gpu-host-maintenance) .\n- To improve network performance, see [Use higher network bandwidth](/compute/docs/gpus/optimize-gpus) .\n- To troubleshoot GPU VMs, see [Troubleshoot GPU VMs](/compute/docs/troubleshooting/troubleshooting-gpus) .", "guide": "Compute Engine"}