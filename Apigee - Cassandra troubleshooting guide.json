{"title": "Apigee - Cassandra troubleshooting guide", "url": "https://cloud.google.com/apigee/docs/api-platform/get-started/what-apigee", "abstract": "# Apigee - Cassandra troubleshooting guide\nYou are currently viewing version 1.1 of the Apigee hybrid documentation. **This version is end of life.** You should upgrade to a newer version. For more information, see [Supported versions](/apigee/docs/hybrid/supported-platforms#supported-versions) .\nThis topic discusses steps you can take to troubleshoot and fix problems with the [Cassandra](/apigee/docs/hybrid/v1.1/what-is-hybrid#cassandra-datastore) datastore. Cassandra is a persistent datastore that runs in the `cassandra` component of the [hybrid runtime architecture](/apigee/docs/hybrid/v1.1/what-is-hybrid#about-the-runtime-plane) . See also [Runtime service configuration overview](/apigee/docs/hybrid/v1.1/service-config) .\n", "content": "## \n Cassandra pods are stuck in the Pending state\n### \n Symptom\nWhen starting up, the Cassandra pods remain in the **Pending** state.\n### \n Error message\nWhen you use `kubectl` to view the pod states, you see that one or more Cassandra pods are stuck in the `Pending` state. The `Pending` state indicates that Kubernetes is unable to schedule the pod on a node: the pod cannot be created. For example:\n```\nkubectl get pods -n namespace\nNAME          READY STATUS  RESTARTS AGE\nadah-resources-install-4762w    0/4  Completed 0   10m\napigee-cassandra-0      0/1  Pending  0   10m\n...\n```\n### \n Possible causes\nA pod stuck in the Pending state can have multiple causes. For example:\n| Cause     | Description             |\n|:-----------------------|:---------------------------------------------------------------|\n| Insufficient resources | There is not enough CPU or memory available to create the pod. |\n| Volume not created  | The pod is waiting for the persistent volume to be created. |\n### \n Diagnosis\nUse `kubectl` to describe the pod to determine the source of the error. For example:\n```\nkubectl -n namespace describe pods pod_name\n```\nFor example:\n```\nkubectl -n apigee describe pods apigee-cassandra-0\n```\nThe output may show one of these possible problems:\n- If the problem is insufficient resources, you will see a Warning message that indicates  insufficient CPU or memory.\n- If the error message indicates that the pod has unbound immediate PersistentVolumeClaims (PVC),  it means the pod is not able to create its [ Persistent volume](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) .\n### \n Resolution\nModify the Cassandra node pool so that it has sufficient CPU and memory resources.  See [  Resizing a node pool](https://cloud.google.com/kubernetes-engine/docs/how-to/node-pools#resizing_a_node_pool) for details.\nIf you determine a persistent volume issue, describe the PersistentVolumeClaim (PVC) to determine why it is not being created:\n- List the PVCs in the cluster:```\nkubectl -n namespace get pvc\nNAME        STATUS VOLUME          CAPACITY ACCESS MODES STORAGECLASS AGE\ncassandra-data-apigee-cassandra-0 Bound pvc-b247faae-0a2b-11ea-867b-42010a80006e 10Gi  RWO   standard  15m\n...\n```\n- Describe the PVC for the pod that is failing. For example, the following command  describes the PVC bound to the pod`apigee-cassandra-0`:```\nkubectl apigee describe pvc cassandra-data-apigee-cassandra-0\nEvents:\n Type  Reason    Age    From       Message\n ----  ------    ----    ----       ------ Warning ProvisioningFailed 3m (x143 over 5h) persistentvolume-controller storageclass.storage.k8s.io \"apigee-sc\" not found\n```Note that in this example, the StorageClass named `apigee-sc` does not exist. To  resolve this problem, create the missing StorageClass in the cluster, as explained in [  Change the default StorageClass](/apigee/docs/hybrid/v1.1/cassandra-config) .\nSee also [ Debugging Pods](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller#debugging-pods) .\n## \n Cassandra pods are stuck in the CrashLoopBackoff state\n### \n Symptom\nWhen starting up, the Cassandra pods remain in the **CrashLoopBackoff** state.\n### \n Error message\nWhen you use `kubectl` to view the pod states, you see that one or more Cassandra pods are in the `CrashLoopBackoff` state. This state indicates that Kubernetes is unable to create the pod. For example:\n```\nkubectl get pods -n namespace\nNAME          READY STATUS   RESTARTS AGE\nadah-resources-install-4762w    0/4  Completed   0   10m\napigee-cassandra-0      0/1  CrashLoopBackoff   0   10m\n...\n```\n### \n Possible causes\nA pod stuck in the `CrashLoopBackoff` state can have multiple causes. For example:\n| Cause           | Description                                                                                         |\n|:----------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Data center differs from previous data center | This error indicates that the Cassandra pod has a persistent volume that has data from a previous cluster, and the new pods are not able to join the old cluster. This usually happens when stale persistent volumes persist from the previous Cassandra cluster on the same Kubernetes node. This problem can occur if you delete and recreate Cassandra in the cluster. |\n| Truststore directory not found    | This error indicates that the Cassandra pod is not able to create a TLS connection. This usually happens when the provided keys and certificates are invalid, missing, or have other issues.                                            |\n### \n Diagnosis\nCheck the [Cassandra error log](/apigee/docs/hybrid/v1.1/cassandra-logs) to determine the cause of the problem.\n- List the pods to get the ID of the Cassandra pod that is failing:```\nkubectl get pods -n namespace\n```\n- Check the failing pod's log:```\nkubectl logs pod_id -n namespace\n```\n### \n Resolution\nLook for the following clues in the pod's log:\nIf you see this log message:\n```\nCannot start node if snitch's data center (us-east1) differs from previous data center\n```\n- Check if there are any stale or old PVC in the cluster and delete them.\n- If this is a fresh install, delete all the PVCs and re-try the setup. For example:```\nkubectl -n namespace get pvc\nkubectl -n namespace delete pvc cassandra-data-apigee-cassandra-0\n```If you see this log message:\n```\nCaused by: java.io.FileNotFoundException: /apigee/cassandra/ssl/truststore.p12\n(No such file or directory)\n```\nVerify the key and certificates if provided in your overrides file are correct and valid. For example:\n```\ncassandra:\n sslRootCAPath: path_to_root_ca-file\n sslCertPath: path-to-tls-cert-file\n sslKeyPath: path-to-tls-key-file\n```\n## Node failure\n### \n Symptom\nWhen starting up, the Cassandra pods remain in the Pending state. This problem can indicate an underlying node failure.\n### \n Diagnosis\n- Determine which Cassandra pods are not running:```\n$ kubectl get pods -n your_namespace\u00a0 \u00a0 NAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0READY \u00a0 STATUS \u00a0 \u00a0RESTARTS \u00a0 AGE\u00a0 \u00a0 cassandra-0 \u00a0 0/1 \u00a0 \u00a0 Pending \u00a0 0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a013s\u00a0 \u00a0 cassandra-1 \u00a0 1/1 \u00a0 \u00a0 Running \u00a0 0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a08d\u00a0 \u00a0 cassandra-2 \u00a0 1/1 \u00a0 \u00a0 Running \u00a0 0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a08d\n```\n- Check the worker nodes. If one is in the **NotReady** state, then  that is the node that has failed:```\nkubectl get nodes -n your_namespaceNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0STATUS \u00a0 ROLES \u00a0 \u00a0AGE \u00a0 VERSIONip-10-30-1-190.ec2.internal \u00a0 Ready \u00a0 \u00a0<none> \u00a0 8d \u00a0 \u00a0v1.13.2ip-10-30-1-22.ec2.internal \u00a0 \u00a0Ready \u00a0 \u00a0master \u00a0 8d \u00a0 \u00a0v1.13.2ip-10-30-1-36.ec2.internal \u00a0 \u00a0NotReady <none> \u00a0 8d \u00a0 \u00a0v1.13.2ip-10-30-2-214.ec2.internal \u00a0 Ready \u00a0 \u00a0<none> \u00a0 8d \u00a0 \u00a0v1.13.2ip-10-30-2-252.ec2.internal \u00a0 Ready \u00a0 \u00a0<none> \u00a0 8d \u00a0 \u00a0v1.13.2ip-10-30-2-47.ec2.internal \u00a0 \u00a0Ready \u00a0 \u00a0<none> \u00a0 8d \u00a0 \u00a0v1.13.2ip-10-30-3-11.ec2.internal \u00a0 \u00a0Ready \u00a0 \u00a0<none> \u00a0 8d \u00a0 \u00a0v1.13.2ip-10-30-3-152.ec2.internal \u00a0 Ready \u00a0 \u00a0<none> \u00a0 8d \u00a0 \u00a0v1.13.2ip-10-30-3-5.ec2.internal \u00a0 \u00a0 Ready \u00a0 \u00a0<none> \u00a0 8d \u00a0 \u00a0v1.13.2\n```\n### \n Resolution\n- Remove the dead Cassandra pod from the cluster.```\n$ kubectl exec -it apigee-cassandra-0 -- nodetool status\n$ kubectl exec -it apigee-cassandra-0 -- nodetool removenode deadnode_hostID\n```\n- Remove the VolumeClaim from the dead node to prevent the  Cassandra pod from attempting to come up on the dead node because  of the affinity:```\nkubectl get pvc -n your_namespace\nkubectl delete pvc volumeClaim_name -n your_namespace\n```\n- Update the volume template and create PersistentVolume for the  newly added node. The following is an example volume template:```\napiVersion: v1kind: PersistentVolumemetadata:\u00a0 name: cassandra-data-3spec:\u00a0 capacity:\u00a0 \u00a0 storage: 100Gi\u00a0 accessModes:\u00a0 - ReadWriteOnce\u00a0 persistentVolumeReclaimPolicy: Retain\u00a0 storageClassName: local-storage\u00a0 local:\u00a0 \u00a0 path: /apigee/data\u00a0 nodeAffinity:\u00a0 \u00a0 \"required\":\u00a0 \u00a0 \u00a0 \"nodeSelectorTerms\":\u00a0 \u00a0 \u00a0 - \"matchExpressions\":\u00a0 \u00a0 \u00a0 \u00a0 - \"key\": \"kubernetes.io/hostname\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"operator\": \"In\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"values\": [\"ip-10-30-1-36.ec2.internal\"]\n```\n- Replace the values with the new hostname/IP and apply the template:```\nkubectl apply -f volume-template.yaml\n```", "guide": "Apigee"}