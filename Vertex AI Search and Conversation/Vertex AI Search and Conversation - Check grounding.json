{"title": "Vertex AI Search and Conversation - Check grounding", "url": "https://cloud.google.com/generative-ai-app-builder/docs/check-grounding?hl=zh-cn", "abstract": "# Vertex AI Search and Conversation - Check grounding\n**Note:** This feature is a Preview offering, subject to the \"Pre-GA Offerings Terms\" of the [GCP Service Specific Terms](https://cloud.google.com/terms/service-terms) . Pre-GA products and features may have limited support, and changes to pre-GA products and features may not be compatible with other pre-GA versions. For more information, see the [launch stage descriptions](https://cloud.google.com/products#product-launch-stages) . Further, by using this feature, you agree to the [Generative AI Preview terms and conditions](https://cloud.google.com/trustedtester/aitos) (\"Preview Terms\"). For this feature, you can process personal data as outlined in the [Cloud Data Processing Addendum](https://cloud.google.com/terms/data-processing-terms) , subject to applicable restrictions and obligations in the Agreement (as defined in the Preview Terms).\nThis page describes how to use the `CheckGrounding` API to obtain a grounding score for an answer candidate.\nThe `CheckGrounding` API determines how grounded a piece of text, called an answer candidate, is in a given set of reference texts, called facts. The `CheckGrounding` API returns an overall score of 0 to 1, indicating how grounded the answer candidate is, along with citations to the appropriate given facts for each claim in the answer candidate.\nPerfect grounding requires that every claim in the candidate must be attributed to one or more of the given facts. In other words, the claim is wholly entailed by the facts. If the claim is only partially entailed, it is not considered grounded. For example, the claim \"Google was founded by Larry Page and Sergey Brin in 1975\" is only partially correct\u2014the names of the founders are correct but the date is wrong\u2014and as such the whole claim is considered ungrounded. In this version of the `CheckGrounding` API, a sentence is considered a single claim.\nYou can use the `CheckGrounding` API for checking any piece of text. It could be a human-generated blurb or a machine-generated response. A typical use case would be to check an LLM-generated response with respect to a given set of facts. Among other things, the citations generated by the API would help distinguish hallucinated claim in the response from grounded claims.\n", "content": "## Before you begin\nThis feature is Preview with allowlist.\nIf you want to use the `CheckGrounding` API, contact your Google account team and ask to be added to the allowlist.\n## Terms defined and explained\nBefore you use the `CheckGrounding` API, it helps to understand the inputs and outputs, and how to structure your grounding facts for best results.\n### Input data\nThe `CheckGrounding` API requires the following inputs in the request.\n- **Answer candidate:** An answer candidate can be any piece of text whose grounding you want to check. For example, in the context of Vertex AI Search, the answer candidate might be the generated search summary that answers a query. The API would then determine how grounded the summary is in the input facts.\n- **Facts:** A set of text segments to be used as references for grounding. A set of metadata attributes (key-value pairs) can be supplied with each text segment. For example, \"Author\" and \"Title\" are typical attribute keys.The service supports up to 200 facts, each with a maximum of 10k characters.Google recommends against supplying one very large fact that contains all of the information. Instead, you can get better results by breaking large facts into smaller facts and supplying appropriate attributes for the smaller facts. For example, you can break up a large fact by title, author, or URL, and supply this information in attributes.\n### Output data\nThe `CheckGrounding` API returns the following for an answer candidate:\n- **Grounding score:** The grounding score is a number from 0 to 1 that indicates how grounded an answer candidate is in the provided set of facts. It loosely approximates the fraction of claims in the answer candidate that were found to be grounded in one or more of the given facts.\n- **Cited chunks:** Cited chunks are portions of the input facts that support the answer candidate.\n- **Claims and citations:** The claims and citations connect a span (typically a sentence) of the answer candidate to one or more of the cited chunks the corroborate the claims in the span.\n### Examples of facts\nThe following are a couple of examples of facts and their attributes. These examples are to help you understand the grounding response and the [formatof the curl command](#procedure) .\n- **Fact 0** - Text: `\"Titanic is a 1997 American epic romantic disaster movie. It was directed, written, and co-produced by James Cameron. The movie is about the 1912 sinking of the RMS Titanic. It stars Kate Winslet and Leonardo DiCaprio. The movie was released on December 19, 1997. It received positive critical reviews. The movie won 11 Academy Awards, and was nominated for fourteen total Academy Awards.\"`\n- Attributes: `{\"Author\": \"Simple Wikipedia\"}`\n- **Fact 1** - Text: `\"James Cameron's \"Titanic\" is an epic, action-packed romance set against the ill-fated maiden voyage of the R.M.S. Titanic; the pride and joy of the White Star Line and, at the time, the largest moving object ever built. She was the most luxurious liner of her era -- the \"ship of dreams\" -- which ultimately carried over 1,500 people to their death in the ice cold waters of the North Atlantic in the early hours of April 15, 1912.\"`\n- Attributes: `{\"Author\": \"Rotten Tomatoes\"}`\n### Examples of answer candidates and grounding responses\nThe following table shows examples of answer candidates and the outputs, based on the example facts.\n| Answer Candidate                            | Check Grounding Response                                                                                                                                                        |\n|:-----------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Titanic was directed by James Cameron. It was released in 1997.                | Grounding score: 0.994006 Cited chunks: ...It was directed, written, and co-produced by James Cameron... (from Fact 0) ...The movie was released on December 19, 1997... (from Fact 0) Claims and citations: Span: Titanic was directed by James Cameron. Citations: [0] Span: It was released in 1997. Citations: [1]                                                                               |\n| Titanic was directed by James Cameron. It was based on the sinking of the RMS Titanic that led to the death of 1500 people. | Grounding score: 0.9586579015 Cited chunks: ...It was directed, written, and co-produced by James Cameron... (from Fact 0) ...The movie is about the 1912 sinking of the RMS Titanic... (from Fact 0) ...She was the most luxurious liner of her era -- the \"ship of dreams\" -- which ultimately carried over 1,500 people to their death in the ice cold waters of the North Atlantic in the early hours of April 15, 1912... (from Fact 1) Claims and citations: Span: Titanic was directed by James Cameron. Citations: [0] Span: It was based on the sinking of the RMS Titanic that led to the death of 1500 people. Citations: [1, 2] |\n| Titanic was directed by James Cameron. It starred Brad Pitt and Kate Winslet            | Grounding score: 0.5448985022999999 Cited chunks: ...It was directed, written, and co-produced by James Cameron... (from Fact 0) Claims and citations: Span: Titanic was directed by James Cameron. Citations: [0] Span: It starred Brad Pitt and Kate Winslet Citations: [] Note: Even though Kate Winslet starred in the movie, because the claim \"It starred Brad Pitt and Kate Winslet\" is not wholly true, it gets no citations.                                                   |\n**Note:** The raw input and output from the API calls don't look exactly like this. For example, the span is indicated using an integer range, which indexes into the answer candidate. See [Obtain a grounding score for an answercandidate](#procedure) .\n## Obtain a grounding score for an answer candidate\nTo find out how grounded an answer candidate is in a set of facts, you use the `CheckGrounding` method.\n- Prepare your set of facts. For more information and examples, see [Termsdefined and explained](#terms-defined) .\n- Run the following curl command:```\ncurl -X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\\"https://discoveryengine.googleapis.com/v1alpha/projects/PROJECT_ID/locations/global/groundingConfigs/default_grounding_config:checkGrounding\" \\-d '{\"answerCandidate\": \"CANDIDATE\",\"facts\": [\u00a0{\u00a0 \"factText\": \"TEXT_0\",\u00a0 \"attributes\": {\"ATTRIBUTE_A\": \"VALUE_A0\",\"ATTRIBUTE_B\": \"VALUE_B0\"}\u00a0},\u00a0{\u00a0 \"factText\": \"TEXT_1\",\u00a0 \"attributes\": {\"ATTRIBUTE_A\": \"VALUE_A1\",\"ATTRIBUTE_B\": \"VALUE_B1\"}\u00a0},\u00a0{\u00a0 \"factText\": \"TEXT_2\",\u00a0 \"attributes\": {\"ATTRIBUTE_A\": \"VALUE_A2\",\"ATTRIBUTE_B\": \"VALUE_B2\"}\u00a0},]}'\n```Replace the following:- `` : the project number or ID of your Google Cloud project.\n- `` : the answer candidate for which you want to get a grounding score\u2014for example, `Titanic was directed by James Cameron. It was released in 1997.`\n- `` : the text segment to be used for grounding\u2014for example, `Titanic is a 1997 American epic... Academy Awards.` (See the full text in [Examples of facts](#fact-examples) .)\n- `` : the name of a metadata attribute associated with the fact\u2014for example, `author` or `title` .\n- `` : the value for the attribute\u2014for example, `Simple Wikipedia` or `Titanic (1997 film)` .", "guide": "Vertex AI Search and Conversation"}