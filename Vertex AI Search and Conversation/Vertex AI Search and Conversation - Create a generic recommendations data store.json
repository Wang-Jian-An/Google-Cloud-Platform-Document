{"title": "Vertex AI Search and Conversation - Create a generic recommendations data store", "url": "https://cloud.google.com/generative-ai-app-builder/docs/create-data-store-recommendations?hl=zh-cn", "abstract": "# Vertex AI Search and Conversation - Create a generic recommendations data store\n**Note:** This feature is a Preview offering, subject to the \"Pre-GA Offerings Terms\" of the [GCP Service Specific Terms](https://cloud.google.com/terms/service-terms) . Pre-GA products and features may have limited support, and changes to pre-GA products and features may not be compatible with other pre-GA versions. For more information, see the [launch stage descriptions](https://cloud.google.com/products#product-launch-stages) . Further, by using this feature, you agree to the [Generative AI Preview terms and conditions](https://cloud.google.com/trustedtester/aitos) (\"Preview Terms\"). For this feature, you can process personal data as outlined in the [Cloud Data Processing Addendum](https://cloud.google.com/terms/data-processing-terms) , subject to applicable restrictions and obligations in the Agreement (as defined in the Preview Terms).\nTo create a data store and ingest data for generic recommendations, go to the section for which source you plan to use:\n- [Website URLs](#website) \n- [BigQuery](#bigquery) \n- [Cloud Storage](#cloud-storage) \n- [Upload structured JSON data with the API](#api-json) ", "content": "## Website URLs\nTo use the Google Cloud console to create a a data store and index data from a website, follow these steps:- In the Google Cloud console, go to the **Search and Conversation** page. [Search and Conversation](https://console.cloud.google.com/gen-app-builder/) \n- In the navigation menu, click **Data stores** .\n- Click **New data store** .\n- On the **Select a data source** page, select **Website URLs** .\n- Choose whether to turn on **Advanced website indexing** for this data store. This option can't be turned off later.Advanced website indexing provides additional features such as search summarization, search with follow-ups, and extractive answers. Advanced website indexing incurs additional cost, and requires that you verify domain ownership for any website that you index. For more information, see [Advanced website indexing](/generative-ai-app-builder/docs/about-advanced-features#advanced-website-indexing) and [Pricing](/generative-ai-app-builder/pricing) .\n- In the **Sites to include** field, specify the URLs of the websites that you want to index. Include one URL per line, without comma separators.\n- Optional: In the **Sites to exclude** field, enter websites that you want to exclude from your app.\n- Click **Continue** .\n- Enter a name for your data store.\n- Select a location for your data store. Advanced website indexing must be turned on to select a location.\n- Click **Create** . Vertex AI Search and Conversation creates your data store and displays your data stores on the **Data stores** page.\n- To view information about your data store, click the name of your data store in the **Name** column. Your data store page appears.If you turned on **Advanced website indexing** , a warning appears prompting you to verify your domain ownership. If you have a quota shortfall (the number of pages in the websites that you specified exceeds the \"Number of documents per project\" [quota](/generative-ai-app-builder/quotas) for your project), an additional warning appears prompting you to upgrade your quota. The following steps show you how to verify domain ownership and upgrade your quota.\n- To verify your domain ownership, follow these steps:- Click **Verify in Google Search console** . The **Welcome to the Google\nSearch Console** page appears.\n- Follow the on-screen instructions to verify a domain or a URL prefix, depending on whether you are verifying an entire domain or a URL prefix that is part of a domain. For more information, see [Verify your siteownership](https://support.google.com/webmasters/answer/9008080) in the Search Console Help.\n- When you have finished the domain verification workflow, go back to the [Search and Conversation](https://console.cloud.google.com/gen-app-builder/) page and click **Data stores** in the navigation menu.\n- Click the name of your data store in the **Name** column. Your data store page appears.\n- Click **Refresh status** to update the values in the **Status** column. The **Status** column for your website indicates that indexing is in progress.\n- Repeat the domain verification steps for every website that requires domain verification until all of them begin indexing. When the **Status** column for a URL shows **Indexed** , [advanced website indexing](/generative-ai-app-builder/docs/about-advanced-features#advanced-website-indexing) features are available for that URL or URL pattern.\n- To upgrade your quota, follow these steps:- Click **Upgrade quota** . The **Discovery Engine API** pane appears, with the **Quotas** tab selected.\n- Follow the instructions at [Request a higher quotalimit](https://cloud.google.com/docs/quota_detail/view_manage#requesting_higher_quota) in the Google Cloud documentation. The quota to increase is **Number of documents** .\n- After submitting your request for a higher quota limit, go back to the [Search and Conversation](https://console.cloud.google.com/gen-app-builder/) page and click **Data stores** in the navigation menu.\n- Click the name of your data store in the **Name** column. The **Status** column indicates that indexing is in progress for the websites that had surpassed the quota. When the **Status** column for a URL shows **Indexed** , [advanced website indexing](/generative-ai-app-builder/docs/about-advanced-features#advanced-website-indexing) features are available for that URL or URL pattern.\n### Next steps\n- To attach your data store to an app, create an app and select your data store following the steps in [Create a Recommendations app](/generative-ai-app-builder/docs/create-engine-personalize) .\n- To preview how your recommendations appear after your app and data store are set up, see [Get recommendations](/generative-ai-app-builder/docs/preview-recommendations) .## BigQuery\nTo ingest data from BigQuery, use the following steps to create a data store and ingest data using either the Google Cloud console or the API.\nBefore importing your data, review [Prepare data for ingesting](/generative-ai-app-builder/docs/prepare-data) .\nTo use the Google Cloud console to ingest data from BigQuery, follow these steps:- In the Google Cloud console, go to the **Search and Conversation** page. [Search and Conversation](https://console.cloud.google.com/gen-app-builder/) \n- Go to the **Data Stores** page.\n- Click **New data store** .\n- On the **Type** page, select **BigQuery** .\n- In the **BigQuery path** field, click **Browse** , select a table that you have [prepared for ingesting](/generative-ai-app-builder/docs/prepare-data) , and then click **Select** . Alternatively, enter the table location directly in the **BigQuery path** field.\n- Select what kind of data you are importing.\n- Click **Continue** .\n- Choose a region for your data store.\n- Enter a name for your data store.\n- Click **Create** .\n- To confirm that your data store was created, go to the **Data stores** page and click your data store name to see details about it on its **Data** page.\n- To check the status of your ingestion, go to the **Data stores** page and click your data store name to see details about it on its **Data** page. When the status column on the **Activity** tab changes from **In progress** to **Import completed** , the ingestion is complete.Depending on the size of your data, ingestion can take several minutes or several hours.\nTo use the command line to create a data store and import data from BigQuery, follow these steps:- Create a data store. **Note:** This step is experimental. If you want a GA procedure, create a data store by following steps 1-4 of the Console instructions.```\ncurl -X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\-H \"X-Goog-User-Project: PROJECT_ID\" \\\"https://discoveryengine.googleapis.com/v1alpha/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores?dataStoreId=DATA_STORE_ID\" \\-d '{\u00a0 \"displayName\": \"DISPLAY_NAME\",\u00a0 \"industryVertical\": \"GENERIC\",\u00a0 \"solutionTypes\": [\"SOLUTION_TYPE_RECOMMENDATION\"]}'\n```- : The ID of your project.\n- : The ID of the data store. The ID can contain only lowercase letters, digits, underscores, and hyphens.\n- : The display name of the data store. This might be displayed in the Google Cloud console.\n- Optional: If you're uploading structured data with your own schema, you can provide the schema. When you provide the schema, you typically get better results. Otherwise, the schema is auto-detected. For more information, see [Schemas: auto-detecting versus providing your own](/generative-ai-app-builder/docs/provide-schema) .```\ncurl -X PATCH \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\\"https://discoveryengine.googleapis.com/v1/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/schemas/default_schema\" \\-d '{\u00a0 \"structSchema\": JSON_SCHEMA_OBJECT}'\n```- : The ID of your project.\n- : The ID of the data store. The ID can contain only lowercase letters, digits, underscores, and hyphens.\n- : Your JSON schema as a JSON object\u2014for example:```\n{\u00a0 \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\u00a0 \"type\": \"object\",\u00a0 \"properties\": {\u00a0 \u00a0 \"title\": {\u00a0 \u00a0 \u00a0 \"type\": \"string\",\u00a0 \u00a0 \u00a0 \"keyPropertyMapping\": \"title\"\u00a0 \u00a0 },\u00a0 \u00a0 \"categories\": {\u00a0 \u00a0 \u00a0 \"type\": \"array\",\u00a0 \u00a0 \u00a0 \"items\": {\u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"string\",\u00a0 \u00a0 \u00a0 \u00a0 \"keyPropertyMapping\": \"category\"\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 },\u00a0 \u00a0 \"uri\": {\u00a0 \u00a0 \u00a0 \"type\": \"string\",\u00a0 \u00a0 \u00a0 \"keyPropertyMapping\": \"uri\"\u00a0 \u00a0 }\u00a0 }}\n```\n- Import data from BigQuery.If you defined a schema, make sure the data conforms to that schema.```\ncurl -X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\\"https://discoveryengine.googleapis.com/v1/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/branches/0/documents:import\" \\-d '{\u00a0 \"bigquerySource\": {\u00a0 \u00a0 \"projectId\": \"PROJECT_ID\",\u00a0 \u00a0 \"datasetId\":\"DATASET_ID\",\u00a0 \u00a0 \"tableId\": \"TABLE_ID\",\u00a0 \u00a0 \"dataSchema\": \"DATA_SCHEMA\",\u00a0 },\u00a0 \"reconciliationMode\": \"RECONCILIATION_MODE\",\u00a0 \"autoGenerateIds\": \"AUTO_GENERATE_IDS\",\u00a0 \"idField\": \"ID_FIELD\",\u00a0 \"errorConfig\": {\u00a0 \u00a0 \"gcsPrefix\": \"ERROR_DIRECTORY\"\u00a0 }}'\n```- : The ID of your project.\n- : The ID of the data store. The ID can contain only lowercase letters, digits, underscores, and hyphens.\n- : The ID of the BigQuery dataset.\n- : The ID of the BigQuery table.- If the BigQuery table is not under, you need to give the service account`service-<project number>@gcp-sa-discoveryengine.iam.gserviceaccount.com`\"BigQuery Data Viewer\" permission for the BigQuery table. For example, if you are importing a BigQuery table from source project \"123\" to destination project \"456\", give`service-456@gcp-sa-discoveryengine.iam.gserviceaccount.com`permissions for the BigQuery table under project \"123\".\n- : Optional. Values are`document`and`custom`. The default is`document`.- `document`: the BigQuery table that you use must conform to the default BigQuery schema provided in [Prepare data for ingesting](/generative-ai-app-builder/docs/prepare-data#unstructured-with-metadata-bq) . You can define the ID of each document yourself, while wrapping all the data in the jsonData string.\n- `custom`: Any BigQuery table schema is accepted, and Recommendations automatically generates the IDs for each document that is imported.\n- : Optional. A Cloud Storage directory for error information about the import\u2014for example,`gs://<your-gcs-bucket>/directory/import_errors`. Google recommends leaving this field empty to let Recommendations automatically create a temporary directory.\n- : Optional. Values are`FULL`and`INCREMENTAL`. Default is`INCREMENTAL`. Specifying`INCREMENTAL`causes an incremental refresh of data from BigQuery to your data store. This does an upsert operation, which adds new documents and replaces existing documents with updated documents with the same ID. Specifying`FULL`causes a full rebase of the documents in your data store. In other words, new and updated documents are added to your data store, and documents that are not in BigQuery are removed from your data store. The`FULL`mode is helpful if you want to automatically delete documents that you no longer need.\n- : Optional. Specifies whether to automatically generate document IDs. If set to `true` , document IDs are generated based on a hash of the payload. Note that generated document IDs might not remain consistent over multiple imports. If you auto-generate IDs over multiple imports, Google highly recommends setting `reconciliationMode` to `FULL` to maintain consistent document IDs.Specify `autoGenerateIds` only when `bigquerySource.dataSchema` is set to `custom` . Otherwise an `INVALID_ARGUMENT` error is returned. If you don't specify `autoGenerateIds` or set it to `false` , you must specify `idField` . Otherwise the documents fail to import.\n- : Optional. Specifies which fields are the document IDs. For BigQuery source files, `idField` indicates the name of the column in the BigQuery table that contains the document IDs.Specify `idField` only when: (1) `bigquerySource.dataSchema` is set to `custom` , and (2) `auto_generate_ids` is set to `false` or is unspecified. Otherwise an `INVALID_ARGUMENT` error is returned.The value of the BigQuery column name must be of string type, must be between 1 and 63 characters, and must conform to [RFC-1034](https://tools.ietf.org/html/rfc1034) . Otherwise, the documents fail to import.\nFor more information, see the [Vertex AI Search and Conversation C# API reference documentation](/dotnet/docs/reference/Google.Cloud.DiscoveryEngine.V1Beta/latest) .\nTo authenticate to Vertex AI Search and Conversation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nThis sample ingests unstructured data from either BigQuery or Cloud Storage into an existing data store. [View on GitHub](https://github.com/googleapis/google-cloud-dotnet/blob/HEAD/apis/Google.Cloud.DiscoveryEngine.V1/Google.Cloud.DiscoveryEngine.V1.GeneratedSnippets/DocumentServiceClient.ImportDocumentsRequestObjectSnippet.g.cs) \n```\nusing Google.Cloud.DiscoveryEngine.V1;using Google.LongRunning;public sealed partial class GeneratedDocumentServiceClientSnippets{\u00a0 \u00a0 /// <summary>Snippet for ImportDocuments</summary>\u00a0 \u00a0 /// <remarks>\u00a0 \u00a0 /// This snippet has been automatically generated and should be regarded as a code template only.\u00a0 \u00a0 /// It will require modifications to work:\u00a0 \u00a0 /// - It may require correct/in-range values for request initialization.\u00a0 \u00a0 /// - It may require specifying regional endpoints when creating the service client as shown in\u00a0 \u00a0 /// \u00a0 https://cloud.google.com/dotnet/docs/reference/help/client-configuration#endpoint.\u00a0 \u00a0 /// </remarks>\u00a0 \u00a0 public void ImportDocumentsRequestObject()\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 // Create client\u00a0 \u00a0 \u00a0 \u00a0 DocumentServiceClient documentServiceClient = DocumentServiceClient.Create();\u00a0 \u00a0 \u00a0 \u00a0 // Initialize request argument(s)\u00a0 \u00a0 \u00a0 \u00a0 ImportDocumentsRequest request = new ImportDocumentsRequest\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ParentAsBranchName = BranchName.FromProjectLocationDataStoreBranch(\"[PROJECT]\", \"[LOCATION]\", \"[DATA_STORE]\", \"[BRANCH]\"),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InlineSource = new ImportDocumentsRequest.Types.InlineSource(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ErrorConfig = new ImportErrorConfig(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ReconciliationMode = ImportDocumentsRequest.Types.ReconciliationMode.Unspecified,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutoGenerateIds = false,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 IdField = \"\",\u00a0 \u00a0 \u00a0 \u00a0 };\u00a0 \u00a0 \u00a0 \u00a0 // Make the request\u00a0 \u00a0 \u00a0 \u00a0 Operation<ImportDocumentsResponse, ImportDocumentsMetadata> response = documentServiceClient.ImportDocuments(request);\u00a0 \u00a0 \u00a0 \u00a0 // Poll until the returned long-running operation is complete\u00a0 \u00a0 \u00a0 \u00a0 Operation<ImportDocumentsResponse, ImportDocumentsMetadata> completedResponse = response.PollUntilCompleted();\u00a0 \u00a0 \u00a0 \u00a0 // Retrieve the operation result\u00a0 \u00a0 \u00a0 \u00a0 ImportDocumentsResponse result = completedResponse.Result;\u00a0 \u00a0 \u00a0 \u00a0 // Or get the name of the operation\u00a0 \u00a0 \u00a0 \u00a0 string operationName = response.Name;\u00a0 \u00a0 \u00a0 \u00a0 // This name can be stored, then the long-running operation retrieved later by name\u00a0 \u00a0 \u00a0 \u00a0 Operation<ImportDocumentsResponse, ImportDocumentsMetadata> retrievedResponse = documentServiceClient.PollOnceImportDocuments(operationName);\u00a0 \u00a0 \u00a0 \u00a0 // Check if the retrieved long-running operation has completed\u00a0 \u00a0 \u00a0 \u00a0 if (retrievedResponse.IsCompleted)\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // If it has completed, then access the result\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ImportDocumentsResponse retrievedResult = retrievedResponse.Result;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }}\n```For more information, see the [Vertex AI Search and Conversation Go API reference documentation](/go/docs/reference/cloud.google.com/go/discoveryengine/latest/apiv1beta) .\nTo authenticate to Vertex AI Search and Conversation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nThis sample ingests unstructured data from either BigQuery or Cloud Storage into an existing data store. [View on GitHub](https://github.com/googleapis/google-cloud-go/blob/HEAD/internal/generated/snippets/discoveryengine/apiv1/DocumentClient/ImportDocuments/main.go) \n```\npackage mainimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 discoveryengine \"cloud.google.com/go/discoveryengine/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 discoveryenginepb \"cloud.google.com/go/discoveryengine/apiv1/discoveryenginepb\")func main() {\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // This snippet has been automatically generated and should be regarded as a code template only.\u00a0 \u00a0 \u00a0 \u00a0 // It will require modifications to work:\u00a0 \u00a0 \u00a0 \u00a0 // - It may require correct/in-range values for request initialization.\u00a0 \u00a0 \u00a0 \u00a0 // - It may require specifying regional endpoints when creating the service client as shown in:\u00a0 \u00a0 \u00a0 \u00a0 // \u00a0 https://pkg.go.dev/cloud.google.com/go#hdr-Client_Options\u00a0 \u00a0 \u00a0 \u00a0 c, err := discoveryengine.NewDocumentClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // TODO: Handle error.\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer c.Close()\u00a0 \u00a0 \u00a0 \u00a0 req := &discoveryenginepb.ImportDocumentsRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // TODO: Fill request struct fields.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // See https://pkg.go.dev/cloud.google.com/go/discoveryengine/apiv1/discoveryenginepb#ImportDocumentsRequest.\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 op, err := c.ImportDocuments(ctx, req)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // TODO: Handle error.\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 resp, err := op.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // TODO: Handle error.\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // TODO: Use resp.\u00a0 \u00a0 \u00a0 \u00a0 _ = resp}\n```For more information, see the [Vertex AI Search and Conversation Java API reference documentation](/java/docs/reference/google-cloud-discoveryengine/latest/overview) .\nTo authenticate to Vertex AI Search and Conversation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nThis sample ingests unstructured data from either BigQuery or Cloud Storage into an existing data store. [View on GitHub](https://github.com/googleapis/google-cloud-java/blob/HEAD/java-discoveryengine/samples/snippets/generated/com/google/cloud/discoveryengine/v1/documentservice/importdocuments/SyncImportDocuments.java) \n```\nimport com.google.cloud.discoveryengine.v1.BranchName;import com.google.cloud.discoveryengine.v1.DocumentServiceClient;import com.google.cloud.discoveryengine.v1.ImportDocumentsRequest;import com.google.cloud.discoveryengine.v1.ImportDocumentsResponse;import com.google.cloud.discoveryengine.v1.ImportErrorConfig;public class SyncImportDocuments {\u00a0 public static void main(String[] args) throws Exception {\u00a0 \u00a0 syncImportDocuments();\u00a0 }\u00a0 public static void syncImportDocuments() throws Exception {\u00a0 \u00a0 // This snippet has been automatically generated and should be regarded as a code template only.\u00a0 \u00a0 // It will require modifications to work:\u00a0 \u00a0 // - It may require correct/in-range values for request initialization.\u00a0 \u00a0 // - It may require specifying regional endpoints when creating the service client as shown in\u00a0 \u00a0 // https://cloud.google.com/java/docs/setup#configure_endpoints_for_the_client_library\u00a0 \u00a0 try (DocumentServiceClient documentServiceClient = DocumentServiceClient.create()) {\u00a0 \u00a0 \u00a0 ImportDocumentsRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ImportDocumentsRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setParent(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BranchName.ofProjectLocationDataStoreBranchName(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"[PROJECT]\", \"[LOCATION]\", \"[DATA_STORE]\", \"[BRANCH]\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .toString())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setErrorConfig(ImportErrorConfig.newBuilder().build())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAutoGenerateIds(true)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setIdField(\"idField1629396127\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 ImportDocumentsResponse response = documentServiceClient.importDocumentsAsync(request).get();\u00a0 \u00a0 }\u00a0 }}\n```For more information, see the [Vertex AI Search and Conversation Node.js API reference documentation](/nodejs/docs/reference/discoveryengine/latest) .\nTo authenticate to Vertex AI Search and Conversation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nThis sample ingests unstructured data from either BigQuery or Cloud Storage into an existing data store. [View on GitHub](https://github.com/googleapis/google-cloud-node/blob/HEAD/packages/google-cloud-discoveryengine/samples/generated/v1/document_service.import_documents.js) \n```\n/**\u00a0* This snippet has been automatically generated and should be regarded as a code template only.\u00a0* It will require modifications to work.\u00a0* It may require correct/in-range values for request initialization.\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*//**\u00a0* \u00a0The Inline source for the input content for documents.\u00a0*/// const inlineSource = {}/**\u00a0* \u00a0Cloud Storage location for the input content.\u00a0*/// const gcsSource = {}/**\u00a0* \u00a0BigQuery input source.\u00a0*/// const bigquerySource = {}/**\u00a0* \u00a0Required. The parent branch resource name, such as\u00a0* \u00a0`projects/{project}/locations/{location}/collections/{collection}/dataStores/{data_store}/branches/{branch}`.\u00a0* \u00a0Requires create/update permission.\u00a0*/// const parent = 'abc123'/**\u00a0* \u00a0The desired location of errors incurred during the Import.\u00a0*/// const errorConfig = {}/**\u00a0* \u00a0The mode of reconciliation between existing documents and the documents to\u00a0* \u00a0be imported. Defaults to\u00a0* \u00a0ReconciliationMode.INCREMENTAL google.cloud.discoveryengine.v1.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL.\u00a0*/// const reconciliationMode = {}/**\u00a0* \u00a0Whether to automatically generate IDs for the documents if absent.\u00a0* \u00a0If set to `true`,\u00a0* \u00a0Document.id google.cloud.discoveryengine.v1.Document.id s are\u00a0* \u00a0automatically generated based on the hash of the payload, where IDs may not\u00a0* \u00a0be consistent during multiple imports. In which case\u00a0* \u00a0ReconciliationMode.FULL google.cloud.discoveryengine.v1.ImportDocumentsRequest.ReconciliationMode.FULL \u00a0* \u00a0is highly recommended to avoid duplicate contents. If unset or set to\u00a0* \u00a0`false`, Document.id google.cloud.discoveryengine.v1.Document.id s have\u00a0* \u00a0to be specified using\u00a0* \u00a0id_field google.cloud.discoveryengine.v1.ImportDocumentsRequest.id_field,\u00a0* \u00a0otherwise, documents without IDs fail to be imported.\u00a0* \u00a0Only set this field when using\u00a0* \u00a0GcsSource google.cloud.discoveryengine.v1.GcsSource \u00a0or\u00a0* \u00a0BigQuerySource google.cloud.discoveryengine.v1.BigQuerySource, and when\u00a0* \u00a0GcsSource.data_schema google.cloud.discoveryengine.v1.GcsSource.data_schema \u00a0* \u00a0or\u00a0* \u00a0BigQuerySource.data_schema google.cloud.discoveryengine.v1.BigQuerySource.data_schema \u00a0* \u00a0is `custom` or `csv`. Otherwise, an INVALID_ARGUMENT error is thrown.\u00a0*/// const autoGenerateIds = true/**\u00a0* \u00a0The field in the Cloud Storage and BigQuery sources that indicates the\u00a0* \u00a0unique IDs of the documents.\u00a0* \u00a0For GcsSource google.cloud.discoveryengine.v1.GcsSource \u00a0it is the key of\u00a0* \u00a0the JSON field. For instance, `my_id` for JSON `{\"my_id\": \"some_uuid\"}`.\u00a0* \u00a0For BigQuerySource google.cloud.discoveryengine.v1.BigQuerySource \u00a0it is\u00a0* \u00a0the column name of the BigQuery table where the unique ids are stored.\u00a0* \u00a0The values of the JSON field or the BigQuery column are used as the\u00a0* \u00a0Document.id google.cloud.discoveryengine.v1.Document.id s. The JSON field\u00a0* \u00a0or the BigQuery column must be of string type, and the values must be set\u00a0* \u00a0as valid strings conform to RFC-1034 (https://tools.ietf.org/html/rfc1034)\u00a0* \u00a0with 1-63 characters. Otherwise, documents without valid IDs fail to be\u00a0* \u00a0imported.\u00a0* \u00a0Only set this field when using\u00a0* \u00a0GcsSource google.cloud.discoveryengine.v1.GcsSource \u00a0or\u00a0* \u00a0BigQuerySource google.cloud.discoveryengine.v1.BigQuerySource, and when\u00a0* \u00a0GcsSource.data_schema google.cloud.discoveryengine.v1.GcsSource.data_schema \u00a0* \u00a0or\u00a0* \u00a0BigQuerySource.data_schema google.cloud.discoveryengine.v1.BigQuerySource.data_schema \u00a0* \u00a0is `custom`. And only set this field when\u00a0* \u00a0auto_generate_ids google.cloud.discoveryengine.v1.ImportDocumentsRequest.auto_generate_ids \u00a0* \u00a0is unset or set as `false`. Otherwise, an INVALID_ARGUMENT error is thrown.\u00a0* \u00a0If it is unset, a default value `_id` is used when importing from the\u00a0* \u00a0allowed data sources.\u00a0*/// const idField = 'abc123'// Imports the Discoveryengine libraryconst {DocumentServiceClient} = require('@google-cloud/discoveryengine').v1;// Instantiates a clientconst discoveryengineClient = new DocumentServiceClient();async function callImportDocuments() {\u00a0 // Construct request\u00a0 const request = {\u00a0 \u00a0 parent,\u00a0 };\u00a0 // Run request\u00a0 const [operation] = await discoveryengineClient.importDocuments(request);\u00a0 const [response] = await operation.promise();\u00a0 console.log(response);}callImportDocuments();\n```For more information, see the [Vertex AI Search and Conversation Python API reference documentation](/python/docs/reference/discoveryengine/latest) .\nTo authenticate to Vertex AI Search and Conversation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nThis sample ingests unstructured data from either BigQuery or Cloud Storage into an existing data store. [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/discoveryengine/import_documents_sample.py) \n```\nfrom typing import Optionalfrom google.api_core.client_options import ClientOptionsfrom google.cloud import discoveryengine# TODO(developer): Uncomment these variables before running the sample.# project_id = \"YOUR_PROJECT_ID\"# location = \"YOUR_LOCATION\" # Values: \"global\"# data_store_id = \"YOUR_DATA_STORE_ID\"# Must specify either `gcs_uri` or (`bigquery_dataset` and `bigquery_table`)# Format: `gs://bucket/directory/object.json` or `gs://bucket/directory/*.json`# gcs_uri = \"YOUR_GCS_PATH\"# bigquery_dataset = \"YOUR_BIGQUERY_DATASET\"# bigquery_table = \"YOUR_BIGQUERY_TABLE\"def import_documents_sample(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 data_store_id: str,\u00a0 \u00a0 gcs_uri: Optional[str] = None,\u00a0 \u00a0 bigquery_dataset: Optional[str] = None,\u00a0 \u00a0 bigquery_table: Optional[str] = None,) -> str:\u00a0 \u00a0 # \u00a0For more information, refer to:\u00a0 \u00a0 # https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store\u00a0 \u00a0 client_options = (\u00a0 \u00a0 \u00a0 \u00a0 ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\u00a0 \u00a0 \u00a0 \u00a0 if location != \"global\"\u00a0 \u00a0 \u00a0 \u00a0 else None\u00a0 \u00a0 )\u00a0 \u00a0 # Create a client\u00a0 \u00a0 client = discoveryengine.DocumentServiceClient(client_options=client_options)\u00a0 \u00a0 # The full resource name of the search engine branch.\u00a0 \u00a0 # e.g. projects/{project}/locations/{location}/dataStores/{data_store_id}/branches/{branch}\u00a0 \u00a0 parent = client.branch_path(\u00a0 \u00a0 \u00a0 \u00a0 project=project_id,\u00a0 \u00a0 \u00a0 \u00a0 location=location,\u00a0 \u00a0 \u00a0 \u00a0 data_store=data_store_id,\u00a0 \u00a0 \u00a0 \u00a0 branch=\"default_branch\",\u00a0 \u00a0 )\u00a0 \u00a0 if gcs_uri:\u00a0 \u00a0 \u00a0 \u00a0 request = discoveryengine.ImportDocumentsRequest(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parent=parent,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 gcs_source=discoveryengine.GcsSource(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 input_uris=[gcs_uri], data_schema=\"custom\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Options: `FULL`, `INCREMENTAL`\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 \u00a0 request = discoveryengine.ImportDocumentsRequest(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parent=parent,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 bigquery_source=discoveryengine.BigQuerySource(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 project_id=project_id,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 dataset_id=bigquery_dataset,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 table_id=bigquery_table,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 data_schema=\"custom\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Options: `FULL`, `INCREMENTAL`\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 # Make the request\u00a0 \u00a0 operation = client.import_documents(request=request)\u00a0 \u00a0 print(f\"Waiting for operation to complete: {operation.operation.name}\")\u00a0 \u00a0 response = operation.result()\u00a0 \u00a0 # Once the operation is complete,\u00a0 \u00a0 # get information from operation metadata\u00a0 \u00a0 metadata = discoveryengine.ImportDocumentsMetadata(operation.metadata)\u00a0 \u00a0 # Handle the response\u00a0 \u00a0 print(response)\u00a0 \u00a0 print(metadata)\u00a0 \u00a0 return operation.operation.name\n```For more information, see the [Vertex AI Search and Conversation Ruby API reference documentation](/ruby/docs/reference/google-cloud-discovery_engine-v1beta/latest) .\nTo authenticate to Vertex AI Search and Conversation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nThis sample ingests unstructured data from either BigQuery or Cloud Storage into an existing data store. [View on GitHub](https://github.com/googleapis/google-cloud-ruby/blob/HEAD/google-cloud-discovery_engine-v1/snippets/document_service/import_documents.rb) \n```\nrequire \"google/cloud/discovery_engine/v1\"\n### Snippet for the import_documents call in the DocumentService service\n## This snippet has been automatically generated and should be regarded as a code# template only. It will require modifications to work:# - It may require correct/in-range values for request initialization.# - It may require specifying regional endpoints when creating the service# client as shown in https://cloud.google.com/ruby/docs/reference.\n## This is an auto-generated example demonstrating basic usage of# Google::Cloud::DiscoveryEngine::V1::DocumentService::Client#import_documents.#def import_documents\u00a0 # Create a client object. The client can be reused for multiple calls.\u00a0 client = Google::Cloud::DiscoveryEngine::V1::DocumentService::Client.new\u00a0 # Create a request. To set request fields, pass in keyword arguments.\u00a0 request = Google::Cloud::DiscoveryEngine::V1::ImportDocumentsRequest.new\u00a0 # Call the import_documents method.\u00a0 result = client.import_documents request\u00a0 # The returned object is of type Gapic::Operation. You can use it to\u00a0 # check the status of an operation, cancel it, or wait for results.\u00a0 # Here is how to wait for a response.\u00a0 result.wait_until_done! timeout: 60\u00a0 if result.response?\u00a0 \u00a0 p result.response\u00a0 else\u00a0 \u00a0 puts \"No response received.\"\u00a0 endend\n```### Next steps\n- To attach your data store to an app, create an app and select your data store following the steps in [Create a Recommendations app](/generative-ai-app-builder/docs/create-engine-personalize) .\n- To preview how your recommendations appear after your app and data store are set up, see [Get recommendations](/generative-ai-app-builder/docs/preview-recommendations) .## Cloud Storage\nTo ingest data from Cloud Storage, use the following steps to create a data store and ingest data using either the Google Cloud console or the API.\nBefore importing your data, review [Prepare data for ingesting](/generative-ai-app-builder/docs/prepare-data#unstructured) .\nTo use the console to ingest data from a Cloud Storage bucket, follow these steps:- In the Google Cloud console, go to the **Search and Conversation** page. [Search and Conversation](https://console.cloud.google.com/gen-app-builder/) \n- Go to the **Data Stores** page.\n- Click **New data store** .\n- On the **Type** page, select **Cloud Storage** .\n- In the **Select a folder or file you want to import** section, select **Folder** or **File** .\n- Click **Browse** and choose the data you have [prepared for ingesting](/generative-ai-app-builder/docs/prepare-data) , and then click **Select** . Alternatively, enter the location directly in the **gs://** field.\n- Select what kind of data you are importing.\n- Click **Continue** .\n- Choose a region for your data store.\n- Enter a name for your data store.\n- Click **Create** .\n- To confirm that your data store was created, go to the **Data stores** page and click your data store name to see details about it on its **Data** page.\n- To check the status of your ingestion, go to the **Data stores** page and click your data store name to see details about it on its **Data** page. When the status column on the **Activity** tab changes from **In progress** to **Import completed** , the ingestion is complete.Depending on the size of your data, ingestion can take several minutes or several hours.\nTo use the command line to create a data store and ingest data from Cloud Storage, follow these steps:- Create a data store. **Note:** This step is experimental. If you want a GA procedure, create a data store by following steps 1-4 of the Console instructions.```\ncurl -X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\-H \"X-Goog-User-Project: PROJECT_ID\" \\\"https://discoveryengine.googleapis.com/v1alpha/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores?dataStoreId=DATA_STORE_ID\" \\-d '{\u00a0 \"displayName\": \"DISPLAY_NAME\",\u00a0 \"industryVertical\": \"GENERIC\",\u00a0 \"solutionTypes\": [\"SOLUTION_TYPE_RECOMMENDATION\"],\u00a0 \"contentConfig\": \"CONTENT_REQUIRED\"}'\n```- : The ID of your project.\n- : The ID of the data store. The ID can contain only lowercase letters, digits, underscores, and hyphens.\n- : The display name of the data store. This might be displayed in the Google Cloud console.\n- Import data from Cloud Storage.```\n\u00a0 curl -X POST \\\u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 \"https://discoveryengine.googleapis.com/v1/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/branches/0/documents:import\" \\\u00a0 -d '{\u00a0 \u00a0 \"gcsSource\": {\u00a0 \u00a0 \u00a0 \"inputUris\": [\"INPUT_FILE_PATTERN_1\", \"INPUT_FILE_PATTERN_2\"],\u00a0 \u00a0 \u00a0 \"dataSchema\": \"DATA_SCHEMA\",\u00a0 \u00a0 },\u00a0 \u00a0 \"reconciliationMode\": \"RECONCILIATION_MODE\",\u00a0 \u00a0 \"autoGenerateIds\": \"AUTO_GENERATE_IDS\",\u00a0 \u00a0 \"idField\": \"ID_FIELD\",\u00a0 \u00a0 \"errorConfig\": {\u00a0 \u00a0 \u00a0 \"gcsPrefix\": \"ERROR_DIRECTORY\"\u00a0 \u00a0 }\u00a0 }'\n```- : The ID of your project.\n- : The ID of the data store. The ID can contain only lowercase letters, digits, underscores, and hyphens.\n- : A file pattern in Cloud Storage containing your documents.For structured data, or for unstructured data with metadata for  unstructured documents, an example of the input file pattern is `gs://<your-gcs-bucket>/directory/object.json` , or a pattern  matching one or more files, such as `gs://<your-gcs-bucket>/directory/*.json` .For unstructured documents, an example is `gs://<your-gcs-bucket>/directory/*.pdf` . Each file that is matched  by the pattern becomes a document.If `<your-gcs-bucket>` is not under , you  need to give the service account `service-<project  number>@gcp-sa-discoveryengine.iam.gserviceaccount.com` \"Storage  Object Viewer\" permissions for the Cloud Storage bucket. For  example, if you are importing a Cloud Storage bucket from  source project \"123\" to destination project \"456\", give `service-456@gcp-sa-discoveryengine.iam.gserviceaccount.com` permissions on the Cloud Storage bucket under project \"123\".\n- : Optional. Values are `document` , `custom` , `csv` , and `content` . The default is `document` .- `document` : Upload unstructured data with metadata for unstructured documents. Each line of the file has to follow one of the following formats. You can define the ID of each document:- `{ \"id\": \"<your-id>\", \"jsonData\": \"<JSON string>\", \"content\": { \"mimeType\": \"<application/pdf or text/html>\", \"uri\": \"gs://<your-gcs-bucket>/directory/filename.pdf\" } }`\n- `{ \"id\": \"<your-id>\", \"structData\": <JSON object>, \"content\": { \"mimeType\": \"<application/pdf or text/html>\", \"uri\": \"gs://<your-gcs-bucket>/directory/filename.pdf\" } }`\n- `custom` : Upload JSON for structured documents. The data is organized according to a schema. You can specify the schema; otherwise it is auto-detected. You can put the JSON string of the document in a consistent format directly in each line, and Recommendations automatically generates the IDs for each document imported.\n- `content` : Upload unstructured documents (PDF, HTML, DOC, TXT, PPTX). The ID of each document is automatically generated as the first 128 bits of SHA256(GCS_URI) encoded as a hex string. You can specify multiple input file patterns as long as the matched files don't exceed the 100K files limit.\n- `csv` : Include a header row in your CSV file, with each header mapped to a document field. Specify the path to the CSV file using the `inputUris` field.\n- : Optional. A Cloud Storage directory for error information about the import\u2014for example, `gs://<your-gcs-bucket>/directory/import_errors` . Google recommends leaving this field empty to let Recommendations automatically create a temporary directory.\n- : Optional. Values are `FULL` and `INCREMENTAL` . Default is `INCREMENTAL` . Specifying `INCREMENTAL` causes an incremental refresh of data from Cloud Storage to your data store. This does an upsert operation, which adds new documents and replaces existing documents with updated documents with the same ID. Specifying `FULL` causes a full rebase of the documents in your data store. In other words, new and updated documents are added to your data store, and documents that are not in Cloud Storage are removed from your data store. The `FULL` mode is helpful if you want to automatically delete documents that you no longer need.\n- : Optional. Specifies whether to automatically generate document IDs. If set to `true` , document IDs are generated based on a hash of the payload. Note that generated document IDs might not remain consistent over multiple imports. If you auto-generate IDs over multiple imports, Google highly recommends setting `reconciliationMode` to `FULL` to maintain consistent document IDs.Specify `autoGenerateIds` only when `gcsSource.dataSchema` is set to `custom` or `csv` . Otherwise an `INVALID_ARGUMENT` error is returned. If you don't specify `autoGenerateIds` or set it to `false` , you must specify `idField` . Otherwise the documents fail to import.\n- : Optional. Specifies which fields are the document IDs. For Cloud Storage source documents, `idField` specifies the name in the JSON fields that are document IDs. For example, if `{\"my_id\":\"some_uuid\"}` is the document ID field in one of your documents, specify `\"idField\":\"my_id\"` . This identifies all JSON fields with the name `\"my_id\"` as document IDs.Specify this field only when: (1) `gcsSource.dataSchema` is set to `custom` or `csv` , and (2) `auto_generate_ids` is set to `false` or is unspecified. Otherwise an `INVALID_ARGUMENT` error is returned.Note that the value of the Cloud Storage JSON field must be of string type, must be between 1-63 characters, and must conform to [RFC-1034](https://tools.ietf.org/html/rfc1034) . Otherwise, the documents fail to import.Note that the JSON field name specified by `id_field` must be of string type, must be between 1 and 63 characters, and must conform to [RFC-1034](https://tools.ietf.org/html/rfc1034) . Otherwise, the documents fail to import.For more information, see the [Vertex AI Search and Conversation C# API reference documentation](/dotnet/docs/reference/Google.Cloud.DiscoveryEngine.V1Beta/latest) .\nTo authenticate to Vertex AI Search and Conversation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nThis sample ingests unstructured data from either BigQuery or Cloud Storage into an existing data store. [View on GitHub](https://github.com/googleapis/google-cloud-dotnet/blob/HEAD/apis/Google.Cloud.DiscoveryEngine.V1/Google.Cloud.DiscoveryEngine.V1.GeneratedSnippets/DocumentServiceClient.ImportDocumentsRequestObjectSnippet.g.cs) \n```\nusing Google.Cloud.DiscoveryEngine.V1;using Google.LongRunning;public sealed partial class GeneratedDocumentServiceClientSnippets{\u00a0 \u00a0 /// <summary>Snippet for ImportDocuments</summary>\u00a0 \u00a0 /// <remarks>\u00a0 \u00a0 /// This snippet has been automatically generated and should be regarded as a code template only.\u00a0 \u00a0 /// It will require modifications to work:\u00a0 \u00a0 /// - It may require correct/in-range values for request initialization.\u00a0 \u00a0 /// - It may require specifying regional endpoints when creating the service client as shown in\u00a0 \u00a0 /// \u00a0 https://cloud.google.com/dotnet/docs/reference/help/client-configuration#endpoint.\u00a0 \u00a0 /// </remarks>\u00a0 \u00a0 public void ImportDocumentsRequestObject()\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 // Create client\u00a0 \u00a0 \u00a0 \u00a0 DocumentServiceClient documentServiceClient = DocumentServiceClient.Create();\u00a0 \u00a0 \u00a0 \u00a0 // Initialize request argument(s)\u00a0 \u00a0 \u00a0 \u00a0 ImportDocumentsRequest request = new ImportDocumentsRequest\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ParentAsBranchName = BranchName.FromProjectLocationDataStoreBranch(\"[PROJECT]\", \"[LOCATION]\", \"[DATA_STORE]\", \"[BRANCH]\"),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InlineSource = new ImportDocumentsRequest.Types.InlineSource(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ErrorConfig = new ImportErrorConfig(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ReconciliationMode = ImportDocumentsRequest.Types.ReconciliationMode.Unspecified,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutoGenerateIds = false,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 IdField = \"\",\u00a0 \u00a0 \u00a0 \u00a0 };\u00a0 \u00a0 \u00a0 \u00a0 // Make the request\u00a0 \u00a0 \u00a0 \u00a0 Operation<ImportDocumentsResponse, ImportDocumentsMetadata> response = documentServiceClient.ImportDocuments(request);\u00a0 \u00a0 \u00a0 \u00a0 // Poll until the returned long-running operation is complete\u00a0 \u00a0 \u00a0 \u00a0 Operation<ImportDocumentsResponse, ImportDocumentsMetadata> completedResponse = response.PollUntilCompleted();\u00a0 \u00a0 \u00a0 \u00a0 // Retrieve the operation result\u00a0 \u00a0 \u00a0 \u00a0 ImportDocumentsResponse result = completedResponse.Result;\u00a0 \u00a0 \u00a0 \u00a0 // Or get the name of the operation\u00a0 \u00a0 \u00a0 \u00a0 string operationName = response.Name;\u00a0 \u00a0 \u00a0 \u00a0 // This name can be stored, then the long-running operation retrieved later by name\u00a0 \u00a0 \u00a0 \u00a0 Operation<ImportDocumentsResponse, ImportDocumentsMetadata> retrievedResponse = documentServiceClient.PollOnceImportDocuments(operationName);\u00a0 \u00a0 \u00a0 \u00a0 // Check if the retrieved long-running operation has completed\u00a0 \u00a0 \u00a0 \u00a0 if (retrievedResponse.IsCompleted)\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // If it has completed, then access the result\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ImportDocumentsResponse retrievedResult = retrievedResponse.Result;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }}\n```For more information, see the [Vertex AI Search and Conversation Go API reference documentation](/go/docs/reference/cloud.google.com/go/discoveryengine/latest/apiv1beta) .\nTo authenticate to Vertex AI Search and Conversation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nThis sample ingests unstructured data from either BigQuery or Cloud Storage into an existing data store. [View on GitHub](https://github.com/googleapis/google-cloud-go/blob/HEAD/internal/generated/snippets/discoveryengine/apiv1/DocumentClient/ImportDocuments/main.go) \n```\npackage mainimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 discoveryengine \"cloud.google.com/go/discoveryengine/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 discoveryenginepb \"cloud.google.com/go/discoveryengine/apiv1/discoveryenginepb\")func main() {\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // This snippet has been automatically generated and should be regarded as a code template only.\u00a0 \u00a0 \u00a0 \u00a0 // It will require modifications to work:\u00a0 \u00a0 \u00a0 \u00a0 // - It may require correct/in-range values for request initialization.\u00a0 \u00a0 \u00a0 \u00a0 // - It may require specifying regional endpoints when creating the service client as shown in:\u00a0 \u00a0 \u00a0 \u00a0 // \u00a0 https://pkg.go.dev/cloud.google.com/go#hdr-Client_Options\u00a0 \u00a0 \u00a0 \u00a0 c, err := discoveryengine.NewDocumentClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // TODO: Handle error.\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer c.Close()\u00a0 \u00a0 \u00a0 \u00a0 req := &discoveryenginepb.ImportDocumentsRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // TODO: Fill request struct fields.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // See https://pkg.go.dev/cloud.google.com/go/discoveryengine/apiv1/discoveryenginepb#ImportDocumentsRequest.\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 op, err := c.ImportDocuments(ctx, req)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // TODO: Handle error.\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 resp, err := op.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // TODO: Handle error.\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // TODO: Use resp.\u00a0 \u00a0 \u00a0 \u00a0 _ = resp}\n```For more information, see the [Vertex AI Search and Conversation Java API reference documentation](/java/docs/reference/google-cloud-discoveryengine/latest/overview) .\nTo authenticate to Vertex AI Search and Conversation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nThis sample ingests unstructured data from either BigQuery or Cloud Storage into an existing data store. [View on GitHub](https://github.com/googleapis/google-cloud-java/blob/HEAD/java-discoveryengine/samples/snippets/generated/com/google/cloud/discoveryengine/v1/documentservice/importdocuments/SyncImportDocuments.java) \n```\nimport com.google.cloud.discoveryengine.v1.BranchName;import com.google.cloud.discoveryengine.v1.DocumentServiceClient;import com.google.cloud.discoveryengine.v1.ImportDocumentsRequest;import com.google.cloud.discoveryengine.v1.ImportDocumentsResponse;import com.google.cloud.discoveryengine.v1.ImportErrorConfig;public class SyncImportDocuments {\u00a0 public static void main(String[] args) throws Exception {\u00a0 \u00a0 syncImportDocuments();\u00a0 }\u00a0 public static void syncImportDocuments() throws Exception {\u00a0 \u00a0 // This snippet has been automatically generated and should be regarded as a code template only.\u00a0 \u00a0 // It will require modifications to work:\u00a0 \u00a0 // - It may require correct/in-range values for request initialization.\u00a0 \u00a0 // - It may require specifying regional endpoints when creating the service client as shown in\u00a0 \u00a0 // https://cloud.google.com/java/docs/setup#configure_endpoints_for_the_client_library\u00a0 \u00a0 try (DocumentServiceClient documentServiceClient = DocumentServiceClient.create()) {\u00a0 \u00a0 \u00a0 ImportDocumentsRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ImportDocumentsRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setParent(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BranchName.ofProjectLocationDataStoreBranchName(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"[PROJECT]\", \"[LOCATION]\", \"[DATA_STORE]\", \"[BRANCH]\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .toString())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setErrorConfig(ImportErrorConfig.newBuilder().build())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAutoGenerateIds(true)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setIdField(\"idField1629396127\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 ImportDocumentsResponse response = documentServiceClient.importDocumentsAsync(request).get();\u00a0 \u00a0 }\u00a0 }}\n```For more information, see the [Vertex AI Search and Conversation Node.js API reference documentation](/nodejs/docs/reference/discoveryengine/latest) .\nTo authenticate to Vertex AI Search and Conversation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nThis sample ingests unstructured data from either BigQuery or Cloud Storage into an existing data store. [View on GitHub](https://github.com/googleapis/google-cloud-node/blob/HEAD/packages/google-cloud-discoveryengine/samples/generated/v1/document_service.import_documents.js) \n```\n/**\u00a0* This snippet has been automatically generated and should be regarded as a code template only.\u00a0* It will require modifications to work.\u00a0* It may require correct/in-range values for request initialization.\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*//**\u00a0* \u00a0The Inline source for the input content for documents.\u00a0*/// const inlineSource = {}/**\u00a0* \u00a0Cloud Storage location for the input content.\u00a0*/// const gcsSource = {}/**\u00a0* \u00a0BigQuery input source.\u00a0*/// const bigquerySource = {}/**\u00a0* \u00a0Required. The parent branch resource name, such as\u00a0* \u00a0`projects/{project}/locations/{location}/collections/{collection}/dataStores/{data_store}/branches/{branch}`.\u00a0* \u00a0Requires create/update permission.\u00a0*/// const parent = 'abc123'/**\u00a0* \u00a0The desired location of errors incurred during the Import.\u00a0*/// const errorConfig = {}/**\u00a0* \u00a0The mode of reconciliation between existing documents and the documents to\u00a0* \u00a0be imported. Defaults to\u00a0* \u00a0ReconciliationMode.INCREMENTAL google.cloud.discoveryengine.v1.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL.\u00a0*/// const reconciliationMode = {}/**\u00a0* \u00a0Whether to automatically generate IDs for the documents if absent.\u00a0* \u00a0If set to `true`,\u00a0* \u00a0Document.id google.cloud.discoveryengine.v1.Document.id s are\u00a0* \u00a0automatically generated based on the hash of the payload, where IDs may not\u00a0* \u00a0be consistent during multiple imports. In which case\u00a0* \u00a0ReconciliationMode.FULL google.cloud.discoveryengine.v1.ImportDocumentsRequest.ReconciliationMode.FULL \u00a0* \u00a0is highly recommended to avoid duplicate contents. If unset or set to\u00a0* \u00a0`false`, Document.id google.cloud.discoveryengine.v1.Document.id s have\u00a0* \u00a0to be specified using\u00a0* \u00a0id_field google.cloud.discoveryengine.v1.ImportDocumentsRequest.id_field,\u00a0* \u00a0otherwise, documents without IDs fail to be imported.\u00a0* \u00a0Only set this field when using\u00a0* \u00a0GcsSource google.cloud.discoveryengine.v1.GcsSource \u00a0or\u00a0* \u00a0BigQuerySource google.cloud.discoveryengine.v1.BigQuerySource, and when\u00a0* \u00a0GcsSource.data_schema google.cloud.discoveryengine.v1.GcsSource.data_schema \u00a0* \u00a0or\u00a0* \u00a0BigQuerySource.data_schema google.cloud.discoveryengine.v1.BigQuerySource.data_schema \u00a0* \u00a0is `custom` or `csv`. Otherwise, an INVALID_ARGUMENT error is thrown.\u00a0*/// const autoGenerateIds = true/**\u00a0* \u00a0The field in the Cloud Storage and BigQuery sources that indicates the\u00a0* \u00a0unique IDs of the documents.\u00a0* \u00a0For GcsSource google.cloud.discoveryengine.v1.GcsSource \u00a0it is the key of\u00a0* \u00a0the JSON field. For instance, `my_id` for JSON `{\"my_id\": \"some_uuid\"}`.\u00a0* \u00a0For BigQuerySource google.cloud.discoveryengine.v1.BigQuerySource \u00a0it is\u00a0* \u00a0the column name of the BigQuery table where the unique ids are stored.\u00a0* \u00a0The values of the JSON field or the BigQuery column are used as the\u00a0* \u00a0Document.id google.cloud.discoveryengine.v1.Document.id s. The JSON field\u00a0* \u00a0or the BigQuery column must be of string type, and the values must be set\u00a0* \u00a0as valid strings conform to RFC-1034 (https://tools.ietf.org/html/rfc1034)\u00a0* \u00a0with 1-63 characters. Otherwise, documents without valid IDs fail to be\u00a0* \u00a0imported.\u00a0* \u00a0Only set this field when using\u00a0* \u00a0GcsSource google.cloud.discoveryengine.v1.GcsSource \u00a0or\u00a0* \u00a0BigQuerySource google.cloud.discoveryengine.v1.BigQuerySource, and when\u00a0* \u00a0GcsSource.data_schema google.cloud.discoveryengine.v1.GcsSource.data_schema \u00a0* \u00a0or\u00a0* \u00a0BigQuerySource.data_schema google.cloud.discoveryengine.v1.BigQuerySource.data_schema \u00a0* \u00a0is `custom`. And only set this field when\u00a0* \u00a0auto_generate_ids google.cloud.discoveryengine.v1.ImportDocumentsRequest.auto_generate_ids \u00a0* \u00a0is unset or set as `false`. Otherwise, an INVALID_ARGUMENT error is thrown.\u00a0* \u00a0If it is unset, a default value `_id` is used when importing from the\u00a0* \u00a0allowed data sources.\u00a0*/// const idField = 'abc123'// Imports the Discoveryengine libraryconst {DocumentServiceClient} = require('@google-cloud/discoveryengine').v1;// Instantiates a clientconst discoveryengineClient = new DocumentServiceClient();async function callImportDocuments() {\u00a0 // Construct request\u00a0 const request = {\u00a0 \u00a0 parent,\u00a0 };\u00a0 // Run request\u00a0 const [operation] = await discoveryengineClient.importDocuments(request);\u00a0 const [response] = await operation.promise();\u00a0 console.log(response);}callImportDocuments();\n```For more information, see the [Vertex AI Search and Conversation Python API reference documentation](/python/docs/reference/discoveryengine/latest) .\nTo authenticate to Vertex AI Search and Conversation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nThis sample ingests unstructured data from either BigQuery or Cloud Storage into an existing data store. [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/discoveryengine/import_documents_sample.py) \n```\nfrom typing import Optionalfrom google.api_core.client_options import ClientOptionsfrom google.cloud import discoveryengine# TODO(developer): Uncomment these variables before running the sample.# project_id = \"YOUR_PROJECT_ID\"# location = \"YOUR_LOCATION\" # Values: \"global\"# data_store_id = \"YOUR_DATA_STORE_ID\"# Must specify either `gcs_uri` or (`bigquery_dataset` and `bigquery_table`)# Format: `gs://bucket/directory/object.json` or `gs://bucket/directory/*.json`# gcs_uri = \"YOUR_GCS_PATH\"# bigquery_dataset = \"YOUR_BIGQUERY_DATASET\"# bigquery_table = \"YOUR_BIGQUERY_TABLE\"def import_documents_sample(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 data_store_id: str,\u00a0 \u00a0 gcs_uri: Optional[str] = None,\u00a0 \u00a0 bigquery_dataset: Optional[str] = None,\u00a0 \u00a0 bigquery_table: Optional[str] = None,) -> str:\u00a0 \u00a0 # \u00a0For more information, refer to:\u00a0 \u00a0 # https://cloud.google.com/generative-ai-app-builder/docs/locations#specify_a_multi-region_for_your_data_store\u00a0 \u00a0 client_options = (\u00a0 \u00a0 \u00a0 \u00a0 ClientOptions(api_endpoint=f\"{location}-discoveryengine.googleapis.com\")\u00a0 \u00a0 \u00a0 \u00a0 if location != \"global\"\u00a0 \u00a0 \u00a0 \u00a0 else None\u00a0 \u00a0 )\u00a0 \u00a0 # Create a client\u00a0 \u00a0 client = discoveryengine.DocumentServiceClient(client_options=client_options)\u00a0 \u00a0 # The full resource name of the search engine branch.\u00a0 \u00a0 # e.g. projects/{project}/locations/{location}/dataStores/{data_store_id}/branches/{branch}\u00a0 \u00a0 parent = client.branch_path(\u00a0 \u00a0 \u00a0 \u00a0 project=project_id,\u00a0 \u00a0 \u00a0 \u00a0 location=location,\u00a0 \u00a0 \u00a0 \u00a0 data_store=data_store_id,\u00a0 \u00a0 \u00a0 \u00a0 branch=\"default_branch\",\u00a0 \u00a0 )\u00a0 \u00a0 if gcs_uri:\u00a0 \u00a0 \u00a0 \u00a0 request = discoveryengine.ImportDocumentsRequest(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parent=parent,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 gcs_source=discoveryengine.GcsSource(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 input_uris=[gcs_uri], data_schema=\"custom\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Options: `FULL`, `INCREMENTAL`\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 \u00a0 request = discoveryengine.ImportDocumentsRequest(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parent=parent,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 bigquery_source=discoveryengine.BigQuerySource(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 project_id=project_id,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 dataset_id=bigquery_dataset,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 table_id=bigquery_table,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 data_schema=\"custom\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Options: `FULL`, `INCREMENTAL`\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 reconciliation_mode=discoveryengine.ImportDocumentsRequest.ReconciliationMode.INCREMENTAL,\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 # Make the request\u00a0 \u00a0 operation = client.import_documents(request=request)\u00a0 \u00a0 print(f\"Waiting for operation to complete: {operation.operation.name}\")\u00a0 \u00a0 response = operation.result()\u00a0 \u00a0 # Once the operation is complete,\u00a0 \u00a0 # get information from operation metadata\u00a0 \u00a0 metadata = discoveryengine.ImportDocumentsMetadata(operation.metadata)\u00a0 \u00a0 # Handle the response\u00a0 \u00a0 print(response)\u00a0 \u00a0 print(metadata)\u00a0 \u00a0 return operation.operation.name\n```For more information, see the [Vertex AI Search and Conversation Ruby API reference documentation](/ruby/docs/reference/google-cloud-discovery_engine-v1beta/latest) .\nTo authenticate to Vertex AI Search and Conversation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nThis sample ingests unstructured data from either BigQuery or Cloud Storage into an existing data store. [View on GitHub](https://github.com/googleapis/google-cloud-ruby/blob/HEAD/google-cloud-discovery_engine-v1/snippets/document_service/import_documents.rb) \n```\nrequire \"google/cloud/discovery_engine/v1\"\n### Snippet for the import_documents call in the DocumentService service\n## This snippet has been automatically generated and should be regarded as a code# template only. It will require modifications to work:# - It may require correct/in-range values for request initialization.# - It may require specifying regional endpoints when creating the service# client as shown in https://cloud.google.com/ruby/docs/reference.\n## This is an auto-generated example demonstrating basic usage of# Google::Cloud::DiscoveryEngine::V1::DocumentService::Client#import_documents.#def import_documents\u00a0 # Create a client object. The client can be reused for multiple calls.\u00a0 client = Google::Cloud::DiscoveryEngine::V1::DocumentService::Client.new\u00a0 # Create a request. To set request fields, pass in keyword arguments.\u00a0 request = Google::Cloud::DiscoveryEngine::V1::ImportDocumentsRequest.new\u00a0 # Call the import_documents method.\u00a0 result = client.import_documents request\u00a0 # The returned object is of type Gapic::Operation. You can use it to\u00a0 # check the status of an operation, cancel it, or wait for results.\u00a0 # Here is how to wait for a response.\u00a0 result.wait_until_done! timeout: 60\u00a0 if result.response?\u00a0 \u00a0 p result.response\u00a0 else\u00a0 \u00a0 puts \"No response received.\"\u00a0 endend\n```### Next steps\n- To attach your data store to an app, create an app and select your data store following the steps in [Create a Recommendations app](/generative-ai-app-builder/docs/create-engine-personalize) .\n- To preview how your recommendations appear after your app and data store are set up, see [Get recommendations](/generative-ai-app-builder/docs/preview-recommendations) .## Upload structured JSON data with the API\nTo directly upload a JSON document or object using the API, follow these steps.\nBefore importing your data, [Prepare data for ingesting](/generative-ai-app-builder/docs/prepare-data) .\nTo use the command line to create a data store and import structured JSON data, follow these steps:- Create a data store.```\ncurl -X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\-H \"X-Goog-User-Project: PROJECT_ID\" \\\"https://discoveryengine.googleapis.com/v1alpha/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores?dataStoreId=DATA_STORE_ID\" \\-d '{\u00a0 \"displayName\": \"DISPLAY_NAME\",\u00a0 \"industryVertical\": \"GENERIC\",\u00a0 \"solutionTypes\": [\"SOLUTION_TYPE_RECOMMENDATION\"]}'\n```- : The ID of your project.\n- : The ID of the data store. The ID can contain only lowercase letters, digits, underscores, and hyphens.\n- : The display name of the data store. This might be displayed in the Google Cloud console.\n- Optional: Provide your own schema. When you provide a schema, you typically get better results. For more information, see [Schemas: auto-detecting versus providing your own](/generative-ai-app-builder/docs/provide-schema) .```\ncurl -X PATCH \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\\"https://discoveryengine.googleapis.com/v1beta/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/schemas/default_schema\" \\-d '{\u00a0 \"structSchema\": JSON_SCHEMA_OBJECT}'\n```- : The ID of your project.\n- : The ID of the data store. The ID can contain only lowercase letters, digits, underscores, and hyphens.\n- : Your JSON schema as a JSON object\u2014for example:```\n{\u00a0 \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\u00a0 \"type\": \"object\",\u00a0 \"properties\": {\u00a0 \u00a0 \"title\": {\u00a0 \u00a0 \u00a0 \"type\": \"string\",\u00a0 \u00a0 \u00a0 \"keyPropertyMapping\": \"title\"\u00a0 \u00a0 },\u00a0 \u00a0 \"categories\": {\u00a0 \u00a0 \u00a0 \"type\": \"array\",\u00a0 \u00a0 \u00a0 \"items\": {\u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"string\",\u00a0 \u00a0 \u00a0 \u00a0 \"keyPropertyMapping\": \"category\"\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 },\u00a0 \u00a0 \"uri\": {\u00a0 \u00a0 \u00a0 \"type\": \"string\",\u00a0 \u00a0 \u00a0 \"keyPropertyMapping\": \"uri\"\u00a0 \u00a0 }\u00a0 }}\n```\n- Import structured data that conforms to the defined schema.There are a few approaches that you can use to upload data, including:- Upload a JSON document.```\ncurl -X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\\"https://discoveryengine.googleapis.com/v1beta/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/branches/0/documents?documentId=DOCUMENT_ID\" \\-d '{\u00a0 \"jsonData\": \"JSON_DOCUMENT_STRING\"}'\n```- : The JSON document as a single string. This must conform to the JSON schema that you provided in the previous step\u2014for example:```\n{ \\\"title\\\": \\\"test title\\\", \\\"categories\\\": [\\\"cat_1\\\", \\\"cat_2\\\"], \\\"uri\\\": \\\"test uri\\\"}\n```\n- Upload a JSON object.```\ncurl -X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\\"https://discoveryengine.googleapis.com/v1beta/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/branches/0/documents?documentId=DOCUMENT_ID\" \\-d '{\u00a0 \"structData\": JSON_DOCUMENT_OBJECT}'\n```- : The JSON document as a JSON object. This must conform to the JSON schema that you provided in the previous step\u2014for example:```\n{\u00a0 \"title\": \"test title\",\u00a0 \"categories\": [\u00a0 \u00a0 \"cat_1\",\u00a0 \u00a0 \"cat_2\"\u00a0 ],\u00a0 \"uri\": \"test uri\"}\n```\n- Update with a JSON document.```\ncurl -X PATCH \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\\"https://discoveryengine.googleapis.com/v1beta/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/branches/0/documents/DOCUMENT_ID\" \\-d '{\u00a0 \"jsonData\": \"JSON_DOCUMENT_STRING\"}'\n```\n- Update with a JSON object.```\ncurl -X PATCH \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\\"https://discoveryengine.googleapis.com/v1beta/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/branches/0/documents/DOCUMENT_ID\" \\-d '{\u00a0 \"structData\": JSON_DOCUMENT_OBJECT}'\n```### Next steps\n- To attach your data store to an app, create an app and select your data store following the steps in [Create a Recommendations app](/generative-ai-app-builder/docs/create-engine-personalize) .\n- To preview how your recommendations appear after your app and data store are set up, see [Get recommendations](/generative-ai-app-builder/docs/preview-recommendations) .## Create a data store using Terraform\nYou can use Terraform to create an empty data store. After the empty data store is created, you can ingest data into the data store using the Google Cloud console or API commands.\nTo learn how to apply or remove a Terraform configuration, see [Basic Terraform commands](/docs/terraform/basic-commands) .\nTo create an empty data store using Terraform, see [google_discovery_engine_data_store](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/discovery_engine_data_store) .\n## What's next\n- [Create a Recommendations app](/generative-ai-app-builder/docs/create-engine-personalize) \n- [Get recommendations](/generative-ai-app-builder/docs/preview-recommendations)", "guide": "Vertex AI Search and Conversation"}