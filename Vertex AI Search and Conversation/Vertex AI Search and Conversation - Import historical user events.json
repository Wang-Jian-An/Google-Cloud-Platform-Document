{"title": "Vertex AI Search and Conversation - Import historical user events", "url": "https://cloud.google.com/generative-ai-app-builder/docs/import-user-events?hl=zh-cn", "abstract": "# Vertex AI Search and Conversation - Import historical user events\nThis page describes how to import user event data from past events in bulk. User events are required for media recommendations. If you don't use media recommendations, importing user events isn't required. However, it is recommended for media search apps (Preview with allowlist).\nFor the user event types that you can import, see the `eventType` field of the [userEvents](/generative-ai-app-builder/docs/reference/rest/v1beta/projects.locations.dataStores.userEvents) object. By importing user events, you can improve the quality of your recommendations as well as the ranking of your search results. Search results with higher click-through rates are boosted, while results with lower click-through rates are buried. Don't import user events for documents that you have not yet imported.\nIf an event has documents listed in `userEvents.Documents` , then Vertex AI Search automatically joins the events with that document. Joining allows Vertex AI Search to attribute events such as clicks and views to the correct document in a search result or recommendation. For generic use case apps, joining occurs , that is, user events are joined to documents after the batch import is complete (generally happening within minutes). For media apps, joining occurs , Vertex AI Search starts joining user events to documents while the batch import is still in progress. For how to view unjoined event metrics in the console, see [View aggregated user event information](/generative-ai-app-builder/docs/manage-user-events#view-events) .\nTo import user events in real-time, see [Record real-time user events](/generative-ai-app-builder/docs/record-user-events) .\nYou can import historical events in the following ways:\n- [From Cloud Storage](#import-user-events-from-cloud-storage) \n- [From BigQuery](#import-user-events-from-big-query) \n- [As local JSON data](#import-user-events-inline) \nFor the JSON representation for user events, see [userEvents](/generative-ai-app-builder/docs/reference/rest/v1beta/projects.locations.dataStores.userEvents) in the API reference.\n", "content": "## Before you begin\nBefore you import user events:\n- Create a data store and an app.\n- Review [About media user events](/generative-ai-app-builder/docs/user-events) for user event requirements.## Import historical user events from Cloud Storage\nIf you have a media app, you can import user events using the Google Cloud console or the API. For other app types, you can only import using the API.\nTo import user events for media apps using the console, follow these steps:- In the Google Cloud console, go to the **Search and Conversation** page. [Search and Conversation](https://console.cloud.google.com/gen-app-builder/engines) \n- On the **Apps** page, click your media app.\n- Go to the **Data** page.For media recommendations apps, the **Requirements** tab displays the media documents and user events that you need to import.\n- Click the **Events** tab. If you have already imported any events, information about them is displayed on this tab.\n- Click **Import events** .\n- Select **Cloud Storage** as your data source.\n- Enter or select the Cloud Storage location of your user events.\n- Click **Import** .\n- The **Activity** tab displays the status of your import.\n- If import errors occur, in the **Activity** tab:- Expand the error in the **Details** column and click **View details** to see more information in the **Activity log details** pane.\n- In the **Activity log details** pane, click **View full error logs** to see the error in Logs Explorer.\nTo import historical user events in bulk from Cloud Storage, follow these steps:- Create one or more data files for the input parameters for the import. Use the [gcsSource](/generative-ai-app-builder/docs/reference/rest/v1beta/GcsSource) object to point to your Cloud Storage bucket.```\n{\"gcsSource\": {\u00a0 \"inputUris\": [\"INPUT_FILE_1\", \"INPUT_FILE_2\"],\u00a0 \"dataSchema\": \"user_event\"\u00a0 },\u00a0 \"errorConfig\":{\u00a0 \u00a0 \u00a0 \"gcsPrefix\":\"ERROR_DIRECTORY\"\u00a0 }}\n```- : A file in Cloud Storage containing your user event data. Make sure each user event is on its own single line, with no line breaks. For the JSON representation for user events, see [userEvents](/generative-ai-app-builder/docs/reference/rest/v1beta/projects.locations.dataStores.userEvents) in the API reference. The input file fields must be in this format:`gs://<bucket>/<path-to-file>/`.\n- : Optional. A Cloud Storage directory for error information about the import\u2014for example,`gs://<your-gcs-bucket>/directory/import_errors`. Google recommends leaving this field empty to let Vertex AI Search and Conversation automatically create a temporary directory.\n- Import your events by making a POST request to the [userEvents:import](/generative-ai-app-builder/docs/reference/rest/v1beta/projects.locations.dataStores.userEvents/import) method, providing the name of the data file.```\nexport GOOGLE_APPLICATION_CREDENTIALS=/tmp/my-key.jsoncurl -X POST \\\u00a0 \u00a0 -v \\\u00a0 \u00a0 -H \"Content-Type: application/json; charset=utf-8\" \\\u00a0 \u00a0 -H \"Authorization: Bearer \"$(gcloud auth print-access-token)\"\" \\\u00a0 \u00a0 --data @DATA_FILE.json \\\"https://discoveryengine.googleapis.com/v1beta/projects/PROJECT_NUMBER/locations/global/dataStores/DATA_STORE_ID/userEvents:import\"\n```## Import historical user events from BigQuery\nWhen creating tables for user events in BigQuery, use the schemas documented in [About media user events](/generative-ai-app-builder/docs/user-events) .\n### Set up BigQuery access\nIf your BigQuery dataset is in a different project than your Vertex AI Search or Recommendations project, follow these steps to set up access to BigQuery.\n- Open the [IAM & Admin](https://console.cloud.google.com/iam-admin/iam) page in the Google Cloud console.\n- Select your Vertex AI Search or Recommendations project.\n- Select the **Include Google-provided role grants** checkbox.\n- Find the service account with the name **Discovery Engine Service Account** .\n- If you have not previously initiated an import operation with Discovery Engine, this service account might not be listed. If you don't see this service account, return to the import task and initiate the import. When it fails due to permission errors, return here and complete this task. The Discovery Engine service account will be listed.\n- Copy the identifier for the service account, which looks like an email address\u2014for example,`service-525@gcp-sa-discoveryengine.iam.gserviceaccount.com`.\n- Switch to your BigQuery project (on the same **IAM & Admin** page) and click **Grant Access** .\n- For **New principals** , enter the identifier for the service account and select the **BigQuery > BigQuery Data Viewer** role.\n- Click **Save** .\nFor more information about BigQuery access, see [Controlling access to datasets](/bigquery/docs/dataset-access-controls) in the BigQuery documentation.\n### Import events from BigQuery\nIf you have a media app, you can import user events using the Google Cloud console or the API. For other app types, you can only import using the API.\nTo import user events for media apps using the console, follow these steps:- In the Google Cloud console, go to the **Search and Conversation** page. [Search and Conversation](https://console.cloud.google.com/gen-app-builder/engines) \n- On the **Apps** page, click your media app.\n- Go to the **Data** page.For media recommendations apps, the **Requirements** tab displays the media documents and user events that you need to import.\n- Click the **Events** tab. If you have already imported any events, information about them is displayed on this tab.\n- Click **Import events** .\n- Select **BigQuery** as your data source.\n- Enter or select the BigQuery path for your user events.\n- Click **Import** .\n- The **Activity** tab displays the status of your import.\n- If import errors occur, in the **Activity** tab:- Expand the error in the **Details** column and click **View details** to see more information in the **Activity log details** pane.\n- In the **Activity log details** pane, click **View full error logs** to see the error in Logs Explorer.\nTo import historical user events in bulk from Cloud Storage, follow these steps:- Import your user events by making a POST request to the [userEvents:import](/generative-ai-app-builder/docs/reference/rest/v1beta/projects.locations.dataStores.userEvents/import) method, providing the name of the BigQuery project, dataset ID, and table ID.When importing your events, use the value `user_event` for `dataSchema` .```\nexport GOOGLE_APPLICATION_CREDENTIALS=/tmp/my-key.jsoncurl \\\u00a0 -v \\\u00a0 -X POST \\\u00a0 -H \"Content-Type: application/json; charset=utf-8\" \\\u00a0 -H \"Authorization: Bearer \"$(gcloud auth print-access-token)\"\" \\\u00a0 \"https://discoveryengine.googleapis.com/v1beta/projects/[PROJECT_NUMBER]/locations/global/dataStores/DATA_STORE_ID/userEvents:import\" \\\u00a0 --data '{\u00a0 \u00a0 \u00a0 \"bigquerySource\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"projectId\":\"PROJECT_ID\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"datasetId\": \"DATASET_ID\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"tableId\": \"TABLE_ID\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"dataSchema\": \"user_event\"\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }'\n```## Import historical user events as local JSON data\nYou can import user events by including local JSON data for the events in your call to the [userEvents:import](/generative-ai-app-builder/docs/reference/rest/v1beta/projects.locations.dataStores.userEvents/import) method.\nTo put user event data into a JSON file and specify the file in an API request, follow these instructions:\n- Create a JSON file containing your user event data. For the JSON representation for user events, see [userEvents](/generative-ai-app-builder/docs/reference/rest/v1beta/projects.locations.dataStores.userEvents) in the API reference documentation.```\n{\u00a0 \"userEventInlineSource\": {\u00a0 \u00a0 \"userEvents\": [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 USER_EVENT_1\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 USER_EVENT_2\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 ]\u00a0 }}\n```\n- Import your events by making a POST request to the [userEvents:import](/generative-ai-app-builder/docs/reference/rest/v1beta/projects.locations.dataStores.userEvents/import) method, providing the name of the data file.```\ncurl -X POST \\\u00a0 \u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 \u00a0 -H \"Content-Type: application/json; charset=utf-8\" \\\u00a0 \u00a0 --data @[JSON_FILE] \\\u00a0 \"https://discoveryengine.googleapis.com/v1beta/projects/PROJECT_NUMBER/locations/global/dataStores/DATA_STORE_ID/userEvents:import\"\n```", "guide": "Vertex AI Search and Conversation"}