{"title": "Vertex AI Search and Conversation - Improve search results with search tuning", "url": "https://cloud.google.com/generative-ai-app-builder/docs/tune-search?hl=zh-cn", "abstract": "# Vertex AI Search and Conversation - Improve search results with search tuning\n**Note:** This feature is a Preview offering, subject to the \"Pre-GA Offerings Terms\" of the [GCP Service Specific Terms](https://cloud.google.com/terms/service-terms) . Pre-GA products and features may have limited support, and changes to pre-GA products and features may not be compatible with other pre-GA versions. For more information, see the [launch stage descriptions](https://cloud.google.com/products#product-launch-stages) . Further, by using this feature, you agree to the [Generative AI Preview terms and conditions](https://cloud.google.com/trustedtester/aitos) (\"Preview Terms\"). For this feature, you can process personal data as outlined in the [Cloud Data Processing Addendum](https://cloud.google.com/terms/data-processing-terms) , subject to applicable restrictions and obligations in the Agreement (as defined in the Preview Terms).\nTuned search can give you better quality results than the default search.\nSearch tuning is particularly valuable if you have industry-specific or company-specific queries that are less well addressed by general LLMs. It can be used to further train the search model.\n", "content": "## About training data\nTo tune a search model, you start by putting together training data.\nThe training data should contain queries that you expect that your end users will ask and snippets of text, 250 to 500 words long, that contain relevant information needed to answer the queries. A query can be associated with multiple snippets as long as each snippet contains information that answers the query.\nThe training data should also contain snippets of text that are not paired with queries, but are otherwise similar to the answers in style and length. These snippets without associated queries provide random negatives to tune the model. Google recommends that you provide at least 10,000 of these snippets.\nHere is some terminology to describe the training data that you'll need to provide:\n- **Training queries:** Queries that you anticipate your end users asking. Make sure to focus on queries with specific domain or company terminology.Provide at least 100.\n- **Extractive segments:** Snippets (typically multiple paragraphs) must be taken verbatim from the documents in the data store. All the documents together in the data store are referred to as \"the corpus\".You must provide two types of extractive segments:- Segments that contain relevant information needed to answer the training queries. These are segments that have positive matching with queries.\n- Segments that are not associated with any training queries. These segments are used as random negatives in the model tuning.\nA sentence or two is not sufficiently long to be an extractive segment; the segment needs to contain enough context for training. For example, in response to a query like \"who founded Google,\" a short extract like \"Larry Page\" is insufficient. For examples of sufficiently long segments, see the following table of examples.Provide at least one extractive segment per query and at least 10,000 additional extractive segments.\n- **Relevance scores:** Relevance scores are non-negative integers that estimate how relevant the extractive segment is to the query. You provide a score value for each query and extractive segment pair. A score of 0 means that the extractive segment isn't relevant to the query at all. A score greater than zero indicates some relevance. For simple scoring, Google recommends 1 for all relevant segments and 0 for non-relevant ones. Alternatively, if you want to rank relevance, you can assign relevance scores of 0 to 10 (for example), with 10 for the most relevant segments and 0 for segments that are not at all relevant.Provide at least 100 relevant scores and, optionally, additional non-relevant scores.## Examples of query and extractive segment pairs\nThe following table provides some examples of query and extractive segment pairs. These general examples have been taken from Wikipedia. However, for useful tuning, you'll want to supply documents from proprietary data sets that contain information specific to your business and that are less easily found on the web.\nThe last pair in this table is an example of a zero-score pair, where the answer is not relevant to the query.\n| Training query            | Extractive segment                                                                                                                                                                                                               | Score |\n|:-----------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------:|\n| who founded Google?          | Google was founded on September 4, 1998, by American computer scientists Larry Page and Sergey Brin while they were PhD students at Stanford University in California.Together they own about 14% of its publicly listed shares and control 56% of its stockholder voting power through super-voting stock. The company went public via an initial public offering (IPO) in 2004. In 2015, Google was reorganized as a wholly owned subsidiary of Alphabet Inc. Google is Alphabet's largest subsidiary and is a holding company for Alphabet's internet properties and interests. Sundar Pichai was appointed CEO of Google on October 24, 2015, replacing Larry Page, who became the CEO of Alphabet. On December 3, 2019, Pichai also became the CEO of Alphabet. [...] On the list of most valuable brands, Google is ranked second by Forbes and fourth by Interbrand. |  1 |\n| where is blood pumped after it leaves the right ventricle? | Oxygenated blood leaves the lungs through pulmonary veins, which return it to the left part of the heart, completing the pulmonary cycle. This blood then enters the left atrium, which pumps it through the mitral valve into the left ventricle. From the left ventricle, the blood passes through the aortic valve to the aorta. The blood is then distributed to the body through the systemic circulation before returning again to the pulmonary circulation. Arteries Main article: Pulmonary artery From the right ventricle, blood is pumped through the semilunar pulmonary valve into the left and right main pulmonary artery (one for each lung), which branch into smaller pulmonary arteries that spread throughout the lungs. [...] Cardiac shunt is an unnatural connection between parts of the heart that leads to blood flow that bypasses the lungs. |  1 |\n| where is the bowling hall of fame located?     | The World Bowling Writers ( WBW ) International Bowling Hall of Fame was established in 1993 and is located in the International Bowling Museum and Hall of Fame , on the International Bowling Campus in Arlington , Texas. History The International Bowling Museum and Hall of Fame was located at 11 Stadium Plaza, St. Louis, Missouri, USA, and shared the same building with the St. Louis Cardinals Hall of Fame Museum, until November 8, 2008. It moved to Arlington and reopened in early 2010. In 2012, the WBW was merged with the International Bowling Media Association. After the merger, the WBW Hall of Fame inductees became part of the IBMA Luby Hall of Fame. officers of the World Bowling Writers, which formed the Hall's Board.][...] The man and woman who receive the most votes are elected.             |  1 |\n| why is the sky blue?          | A \"Hello, World!\" program is generally a simple computer program which outputs (or displays) to the screen (often the console) a message similar to \"Hello, World!\" while ignoring any user input. A small piece of code in most general-purpose programming languages, this program is used to illustrate a language's basic syntax. A \"Hello, World!\" program is often the first written by a student of a new programming language, but such a program can also be used as a check to ensure that the computer software intended to compile or run source code is correctly installed, and that its operator understands how to use it. [...] The C-language version was preceded by Kernighan's own 1972 A Tutorial Introduction to the Language B, where the first known version of the program is found in an example used to illustrate external variables   |  0 |\n## About testing\nAfter training, the tuned search is tested to determine if tuning improved the results. You can explicitly provide the queries that you want tested. If you don't provide test queries, then Vertex AI Search uses 20% of the training queries as test queries.\n## Training files\nThe training data needs to be uploaded in three (optionally four) specific files:\n- A [corpus file](#corpus) that contains the extractive segments\n- A [query file](#query) that contains only the queries\n- A [training labels file](#training) that connects queries with segments and contains the relevance scores\n- Optional: A [test labels file](#test) that is similar to the training labels files but is used for evaluating the tuned model instead of training it\nThe three training files (corpus file, query file, and training labels file) and the (optional) test labels file need to be in Cloud Storage. The paths of the files are defined by fields in the `trainCustomMethod` call.\n### Corpus file\nThe corpus file contains extractive segments: segments that contain information to answer the queries in the query file and many additional segments to be used as random negatives when tuning the model. You should have at least 100 segments that contain query answers; queries can be answered by multiple segments. You should also have at least 10,000 random segments.\nIf the documents in your data store contain fewer than 500 words, you can use whole documents as segments. Otherwise, programmatically create random segments of 250\u2013500 words from the documents in your data store and add those to the corpus file.\nThe corpus file is a JSONL (JSON lines) file where each line has the fields `_id` and `text` with string values. The maximum size of the file is 250,000 lines.\nFor example:\n```\n\u00a0 {\"_id\": \"doc1\", \"text\": \"Google was founded on September 4, 1998, by American computer scientists Larry Page and Sergey Brin while they were PhD students at Stanford University in California. Together they own about 14% of its publicly listed shares and control 56% of its stockholder voting power through super-voting stock. The company went public via an initial public offering (IPO) in 2004. In 2015, Google was reorganized as a wholly owned subsidiary of Alphabet Inc. Google is Alphabet's largest subsidiary and is a holding company for Alphabet's internet properties and interests. Sundar Pichai was appointed CEO of Google on October 24, 2015, replacing Larry Page, who became the CEO of Alphabet. On December 3, 2019, Pichai also became the CEO of Alphabet. [...] On the list of most valuable brands, Google is 105 ranked second by Forbes and fourth by Interbrand.\"}\u00a0 {\"_id\": \"doc2\", \"text\": \"Oxygenated blood leaves the lungs through pulmonary veins, which return it to the left part of the heart, completing the pulmonary cycle. This blood then enters the left atrium, which pumps it through the mitral valve into the left ventricle. From the left ventricle, the blood passes through the aortic valve to the aorta. The blood is then distributed to the body through the systemic circulation before returning again to the pulmonary circulation. Arteries Main article: Pulmonary artery From the right ventricle, blood is pumped through the semilunar pulmonary valve into the left and right main pulmonary artery (one for each lung), which branch into smaller pulmonary arteries that spread throughout the lungs. [...] Cardiac shunt is an unnatural connection between parts of the heart that leads to blood flow that bypasses the lungs.\"}\u00a0 {\"_id\": \"doc3\", \"text\": \"The World Bowling Writers ( WBW ) International Bowling Hall of Fame was established in 1993 and is located in the International Bowling Museum and Hall of Fame , on the International Bowling Campus in Arlington , Texas. History The International Bowling Museum and Hall of Fame was located at 11 Stadium Plaza, St. Louis, Missouri, USA, and shared the same building with the St. Louis Cardinals Hall of Fame Museum, until November 8, 2008. It moved to Arlington and reopened in early 2010. In 2012, the WBW was merged with the International Bowling Media Association. After the merger, the WBW Hall of Fame inductees became part of the IBMA Luby Hall of Fame. \u00a0officers of the World Bowling Writers, which formed the Hall's Board.][...] The man and woman who receive the most votes are elected.\"}\u00a0 {\"_id\": \"doc4\", \"text\": \"A \\\"Hello, World!\\\" program is generally a simple computer program which outputs (or displays) to the screen (often the console) a message similar to \"Hello, World!\" while ignoring any user input. A small piece of code in most general-purpose programming languages, this program is used to illustrate a language's basic syntax. A \"Hello, World!\" program is often the first written by a student of a new programming language, but such a program can also be used as a check to ensure that the computer software intended to compile or run source code is correctly installed, and that its operator understands how to use it. \u00a0[...] The C-language version was preceded by Kernighan's own 1972 A Tutorial Introduction to the Language B, where the first known version of the program is found in an example used to illustrate external variables.\"}\n```\n### Query file\nThe query file contains the example queries that will be used for tuning the model. Each query should have one or more corresponding extractive segments in the corpus file. You should provide at least 100 positive match queries. You can also provide non-relevant queries: these are queries that correspond to extractive segments with a relevance score of zero.\nThe query file is in JSONL format and has the same fields as the corpus file.\nFor example:\n```\n\u00a0 {\"_id\": \"query1\", \"text\": \"who founded Google?\"}\u00a0 {\"_id\": \"query2\", \"text\": \"where is blood pumped after it leaves the right ventricle?\"}\u00a0 {\"_id\": \"query3\", \"text\": \"where is the bowling hall of fame located?\"}\u00a0 {\"_id\": \"query4\", \"text\": \"why is the sky blue?\"}\n```\n### Training labels\nThe training labels file connects the queries with the extractive segments and scores each query and segment pair.\nIf the test labels file is not present, then 20% of the queries in the training labels file are reserved for evaluating the tuned model after training.\nThe file contains the ID of a query and the ID of its matching (or non-matching) extractive segment and a score for the relevance of the segment to the query. There must be at least one line per query; if a query is answered by two segments, then there are two lines for that query. `Score` is a non-negative integer value. Any score greater than zero indicates that the document is related to the query. Larger numbers indicate a greater level of relevance. If the score is omitted, the default value is 1.\nThe training labels file is a TSV (tab-separated values) file with a header. The file must have the columns `query-id` , `corpus-id` and `score` . The `query-id` is a string that matches the `_id` key from the query file, and the `corpus-id` is a string that matches the `_id` in the corpus file.\nFor example:\n```\n\u00a0 query-id \u00a0corpus-id \u00a0 score\u00a0 query1 \u00a0 \u00a0doc1 \u00a0 \u00a01\u00a0 query2 \u00a0 \u00a0doc2 \u00a0 \u00a01\u00a0 query3 \u00a0 \u00a0doc3 \u00a0 \u00a01\u00a0 query3 \u00a0 \u00a0doc9 \u00a0 \u00a01\u00a0 query4 \u00a0 \u00a0doc4 \u00a0 \u00a00\n```\n**Note:** Ensure that the separators in this file are tab characters ( `\\t` ). Some browsers and text editors may replace tabs with a sequence of spaces.\n### Test labels\nLike the training labels file, this optional file contains the IDs of the query and extractive segment and relevance scores. It contains fewer and different queries than in the training labels file. If present, the query and extractive segment pairs in the file are used to evaluate the tuning. If the test labels file is not present, then query and extractive segment pairs from the training labels file are used for evaluating.\nThis file has the same format as the training labels file.\nFor example:\n```\n\u00a0 query-id \u00a0corpus-id \u00a0 score\u00a0 query200 \u00a0 \u00a0doc200 \u00a0 \u00a01\u00a0 query201 \u00a0 \u00a0doc201 \u00a0 \u00a01\u00a0 query202 \u00a0 \u00a0doc202 \u00a0 \u00a01\u00a0 query203 \u00a0 \u00a0doc203 \u00a0 \u00a01\n```\n## Tune search\nTo tune a data store for search with your own training data follow these steps.\n**Note:** For large data stores, search tuning can take a couple of days.\n- Prepare your training data (and optionally, your test data) files. Use the formats described in [Training files](#training-files) .\n- Place the files into a [Cloud Storage](/storage/docs) bucket.\n- Upload the files from the Cloud Storage bucket into Vertex AI Search by running the following curl command:```\ncurl -X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\\"https://discoveryengine.googleapis.com/v1alpha/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID:trainCustomModel\" \\-d '{\"gcsTrainingInput\": {\u00a0\"corpusDataPath\": \"CORPUS_JSONL_GCS_PATH\",\u00a0\"queryDataPath\": \"QUERY_JSONL_GCS_PATH\",\u00a0\"trainDataPath\": \"TRAIN_JSONL_GCS_PATH\",\u00a0\"testDataPath\": \"TEST_JSONL_GCS_PATH\"},\"modelType\": \"search-tuning\"}'\n```Replace the following:- `` : The project number or ID of your Google Cloud project.\n- `` : The ID of the data store that you want to tune.\n- `` : The [corpus JSONL file](#corpus) path in Cloud Storage\u2014for example, `gs://my-bucket/corpus.jsonl` .\n- `` : The [query JSONL file](#query) path in Cloud Storage\u2014for example, `gs://my-bucket/query.jsonl` .\n- `` : The [training labels TSV file](#training) path in Cloud Storage\u2014for example, `gs://my-bucket/train.tsv` .\n- `` : An optional field. If you have a [testlabels TSV file](#test) , specify its path Cloud Storage\u2014for example, `gs://my-bucket/test.tsv` . If you don't have a test labels file, don't provide the field name or path in the curl command.\nFor general information about this method, see [trainCustomModel](/generative-ai-app-builder/docs/reference/rest/v1alpha/projects.locations.collections.dataStores/trainCustomModel) .Tuning begins automatically after the data files are uploaded.\n- Make note of the `name` value returned by the `trainCustomModel` method, and follow the instructions in [Get detailsabout a long-running operation](/generative-ai-app-builder/docs/long-running-operations#get-details) to see when the search-tuning operation is complete.## Test tuned search and use it for individual search queries\nAfter tuning is complete, you can test it out in search queries by setting the `enableSearchAdaptor` field in the query calls. Setting the `enableSearchAdaptor` field to `true` indicates that the tuned version of search is used for that query.\nTo assess the effect of tuning, you can make queries with the `enableSearchAdaptor` field set to `true` and then `false` and compare the results.\nTo make search queries that use the tuned model:\n- In the query method call, set the `enableSearchAdaptor` field in the `customFineTuningSpec` field to `true` .For example:```\ncurl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\\"https://discoveryengine.googleapis.com/v1alpha/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/servingConfigs/default_search:search\" \\-d '{\"query\": \"QUERY\",\"customFineTuningSpec\": { \"enableSearchAdaptor\": true }}'\n```\nFor detailed information about search queries, see [Get searchresults](/generative-ai-app-builder/docs/preview-search-results) and the [servingConfigs.search](/generative-ai-app-builder/docs/reference/rest/v1alpha/projects.locations.dataStores.servingConfigs/search) method.\n## Turn on tuned search\nAfter you have tested the tuned search and decided that you want to use it for all search queries, you can make it the default search model.\nThis means that you don't need to specify the `customFineTuningSpec` field in the search query as in the preceding procedure.\nTo used the tuned version of search by default for all search queries, follow these steps:\n- To use the tuned search to be the default model, run the following curl command:```\ncurl -X PATCH \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\-H \"X-Goog-User-Project: PROJECT_ID\" \\\"https://discoveryengine.googleapis.com/v1alpha/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/servingConfigs/default_search?updateMask=customFineTuningSpec.enableSearchAdaptor\" \\-d '{\"customFineTuningSpec\": {\u00a0\"enableSearchAdaptor\": true}}'\n```For general information about this method, see [servingConfigs.patch](/generative-ai-app-builder/docs/reference/rest/v1alpha/projects.locations.collections.dataStores.servingConfigs/patch) .## Turn off tuned search\nIf you no longer want to use the tuned version of search\u2014for example, if you find the results no better, or worse, than before tuning, then you can disable the tuned search.\nTo stop using the tuned search, run a curl call similar to the preceding one, but set `enableSearchAdaptor` to `false` :\n- Run the following curl command:```\ncurl -X PATCH \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\-H \"X-Goog-User-Project: PROJECT_ID\" \\\"https://discoveryengine.googleapis.com/v1alpha/projects/PROJECT_ID/locations/global/collections/default_collection/dataStores/DATA_STORE_ID/servingConfigs/default_search?updateMask=customFineTuningSpec.enableSearchAdaptor\" \\-d '{\"customFineTuningSpec\": {\u00a0\"enableSearchAdaptor\": false}}'\n```Replace the following:- `` : The project number or ID of your Google Cloud project.\n- `` : The ID of the data store that you want to tune.\nFor general information about this method, see [servingConfigs.patch](/generative-ai-app-builder/docs/reference/rest/v1alpha/projects.locations.collections.dataStores.servingConfigs/patch) .", "guide": "Vertex AI Search and Conversation"}