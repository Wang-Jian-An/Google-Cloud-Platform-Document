{"title": "Apigee - View metrics", "url": "https://cloud.google.com/apigee/docs/api-platform/get-started/what-apigee", "abstract": "# Apigee - View metrics\nYou are currently viewing version 1.1 of the Apigee hybrid documentation. **This version is end of life.** You should upgrade to a newer version. For more information, see [Supported versions](/apigee/docs/hybrid/supported-platforms#supported-versions) .\nThis topic explains how to view Apigee hybrid metrics in a [Stackdriver](https://cloud.google.com/monitoring/kubernetes-engine/) dashboard.\n", "content": "## About Stackdriver\nFor more information about metrics, dashboards, and [Stackdriver](https://cloud.google.com/monitoring/kubernetes-engine/) see:\n- [Metrics Explorer documentation](https://cloud.google.com/monitoring/charts/metrics-explorer) \n- [Introduction to alerting](https://cloud.google.com/monitoring/alerts/) \n- [Creating charts](https://cloud.google.com/monitoring/charts/) ## Enabling hybrid metrics\nBefore hybrid metrics can be sent to [Stackdriver](https://cloud.google.com/monitoring/kubernetes-engine/) , you must first enable metrics collection. See [Configure metrics collection](/apigee/docs/hybrid/v1.1/metrics-enable) for this procedure.\n## About hybrid metric names and labels\nWhen enabled, hybrid automatically populates Stackdriver metrics. The domain name prefix of the metrics created by hybrid is:\n```\napigee.googleapis.com/\n```\nFor example, the `/proxy/request_count` metric contains the total number of requests received by an API proxy. The metric name in Stackdriver is therefore:\n```\napigee.googleapis.com/proxy/request_count\n```\nStackdriver lets you [filter](https://cloud.google.com/monitoring/charts/metrics-selector#filter-option) and [group](https://cloud.google.com/monitoring/charts/metrics-selector#groupby-option) metrics data based on labels. Some labels are predefined, and others are added explicitly by hybrid. The [Available metrics](#available_metrics) section below lists all of the available hybrid metrics and any labels added specifically for a metric that you can use for filtering and grouping.\n## Viewing metrics\n- Open the [Monitoring Metrics Explorer](https://console.cloud.google.com/monitoring/metrics-explorer) in a browser. Alternatively, if you're already in the  Stackdriver console, select **Metrics explorer** .\n- In **Find resource type and metric** , locate and select the  metric you want to examine. Choose a specific metric listed in [Available metrics](#available_metrics) , or search for a metric. For example,  search for `proxy/latencies` :\n- Select the desired metric.\n- Apply filters. Filter choices for each metric are listed in [Available metrics](#available-metrics) . For example, for the`proxy_latencies`metric, filter choices are: **org=org_name** .\n- Stackdriver displays the chart for the selected metric.\n- Click **Save** .## Creating a dashboard\n[Dashboards](https://cloud.google.com/monitoring/dashboards) are one way for you to view and analyze metric data that is important to you. Stackdriver provides predefined dashboards for the resources and services that you use, and you can also create custom dashboards.\nYou use a [chart](https://cloud.google.com/monitoring/dashboards) to display an Apigee metric in your custom dashboard. With custom dashboards, you have complete control over the charts that are displayed and their configuration. For more information on creating charts, see [Creating charts](https://cloud.google.com/monitoring/charts) .\nThe following example shows how to create a dashboard in Stackdriver and then to add charts to view metrics data:\n- Open the [Monitoring Metrics Explorer](https://console.cloud.google.com/monitoring/metrics-explorer) in a browser and then select **Dashboards** .\n- Select **+ Create Dashboard** .\n- Give the dashboard a name. For example: **Hybrid Proxy Request Traffic** \n- Click **Confirm** .\n- For each chart that you want to add to your dashboard, follow these steps:- In the dashboard, select **Add chart** .\n- Select the desired metric as described above in [Viewing metrics](#view_metrics) .\n- Complete the dialog to define your chart.\n- Click **Save** . Stackdriver displays data for the selected metric.\n## Available metrics\nThe following tables list metrics for analyzing proxy traffic.\n### Proxy, target, and server traffic metrics\nThe [Prometheus](https://github.com/kubernetes/kops/tree/master/addons/prometheus-operator) service collects and processes metrics (as described in [Metrics collection](/apigee/docs/hybrid/v1.1/metrics-collection) ) for proxy, target, and server traffic.\nThe following table describes the metrics and labels that Prometheus uses. These labels are used in the metrics log entries.\n| Metric name    | Label           | Use                                                                              |\n|:-------------------------|:-------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| /proxy/request_count  | method           | The total number of API proxy requests received.                                                                   |\n| /proxy/response_count | method response_code        | The total number of API proxy responses received.                                                                   |\n| /proxy/latencies   | method           | Total number of milliseconds it took to respond to a call. This time includes the Apigee API proxy overhead and your target server time.                                             |\n| /target/request_count | method target_type target_endpoint    | The total number of requests sent to the proxy's target.                                                                 |\n| /target/response_count | method response_code target_type target_endpoint | The total number of responses received from the proxy's target.                                                               |\n| /target/latencies  | method response_code target_type target_endpoint | Total number of milliseconds it took to respond to a call. This time does not include the Apigee API proxy overhead.                                                  |\n| /policy/latencies  | policy_name          | The total number of milliseconds that this named policy took to execute.                                                             |\n| /server/fault_count  | source           | The total number of faults for the server application. For example, the application could be apigee-runtime, apigee-synchronizer, or apigee-udca. Use the pod_name label to filter results by application.                            |\n| /server/nio    | state           | The number of open sockets.                                                                        |\n| /server/num_threads  | nan            | The number of active non-daemon threads in the server.                                                                  |\n| /server/request_count | method type          | The total number of requests received by the server application. For example, the application could be apigee-runtime, apigee-synchronizer, or apigee-udca. Use the pod_name label to filter results by application.                          |\n| /server/response_count | method response_code type      | Total number of responses sent by the server application. For example, the application could be apigee-runtime, apigee-synchronizer, or apigee-udca. Use the pod_name label to filter results by application.                           |\n| /server/latencies  | method response_code type      | Latency is the latency in millisecs introduced by the server application. For example, the application could be apigee-runtime, apigee-synchronizer, or apigee-udca. Use the pod_name label to filter results by application.                       |\n| /upstream/request_count | method type          | The number of requests sent by the server application to its upstream application. For example, for the apigee-synchronizer, the control plane is upstream. So upstream/request_count for apigee-synchronizer is a metric that indicates the requests that apigee-synchronizer made to the control plane.    |\n| /upstream/response_count | method response_code type      | The number of responses received by the server application from its upstream application. For example, for the apigee-synchronizer, the control plane is upstream. So upstream/response_count for apigee-synchronizer is a metric that indicates the requests that apigee-synchronizer received from the control plane. |\n| /upstream/latencies  | method response_code type      | The latency incurred at the upstream server application in milliseconds. For example, for the apigee-synchronizer, the control plane is upstream. So upstream/latencies for apigee-synchronizer is a metric that indicates the latency from the control plane.               |\n### UDCA metrics\nThe [Prometheus](https://github.com/kubernetes/kops/tree/master/addons/prometheus-operator) service collects and processes metrics (as described in [Metrics collection](/apigee/docs/hybrid/v1.1/metrics-collection) ) for the UDCA service just as it does for other hybrid services.\nThe following table describes the metrics and labels that Prometheus uses in the UDCA metrics data. These labels are used in the metrics log entries.\n| Metric name      | Label       | Use                                                                                              |\n|:-----------------------------------|:------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| /udca/server/local_file_oldest_ts | dataset state    | The timestamp, in milliseconds since the start of the Unix Epoch, for the oldest file in the dataset. This is computed every 60 seconds and does not reflect the state in real time. If the UDCA is up to date and there are no files waiting to be uploaded when this metric is computed, then this value will be 0. If this value keeps increasing, then old files are still on disk. |\n| /udca/server/local_file_latest_ts | dataset state     | The timestamp, in milliseconds since the start of the Unix Epoch, for the latest file on disk by state. This is computed every 60 seconds and does not reflect the state in real time. If the UDCA is up to date and there are no files waiting to be uploaded when this metric is computed, then this value will be 0.                 |\n| /udca/server/local_file_count  | dataset state     | A count of the number of files on disk in the data collection pod. Ideally, the value will be close to 0. A consistent high value indicates that files are not being uploaded or that the UDCA is not able to upload them fast enough. This value is computed every 60 seconds and does not reflect the state of the UDCA in real time.             |\n| /udca/server/total_latencies  | dataset      | The time interval, in seconds, between the data file being created and the data file being successfully uploaded. Buckets will be 100ms, 250ms, 500ms, 1s, 2s, 4s, 8s, 16s, 32s, and 64s. Histogram for total latency from file creation time to successful upload time.                            |\n| /udca/server/upload_latencies  | dataset      | The total time, in seconds, that UDCA spent uploading a data file. Buckets will be 100ms, 250ms, 500ms, 1s, 2s, 4s, 8s, 16s, 32s, and 64s. The metrics will display a histogram for total upload latency, including all upstream calls.                                     |\n| /udca/upstream/http_error_count | service dataset response_code | The total count of HTTP errors that UDCA encountered. This metric is useful to help determine which part of the UDCA external dependencies are failing and for what reason. These errors can arise for various services (getDataLocation, Cloud storage, Token generator) and for various datasets (such as api and trace) with various response codes.        |\n| /udca/upstream/http_latencies  | service dataset    | The upstream latency of services, in seconds. Buckets will be 100ms, 250ms, 500ms, 1s, 2s, 4s, 8s, 16s, 32s, and 64s. Histogram for latency from upstream services.                                                      |\n| /udca/upstream/uploaded_file_sizes | dataset      | The size of the file being uploaded to the Apigee services, in bytes. Buckets will be 1KB, 10KB, 100KB, 1MB, 10MB, 100MB, and 1GB. Histogram for file size by dataset, organization and environment.                                             |\n| /udca/upstream/uploaded_file_count | dataset      | A count of the files that UDCA uploaded to the Apigee services. Note that: The event dataset value should keep growing. The api dataset value should keep growing if org/env has constant traffic. The trace dataset value should increase when you use the Apigee trace tools to debug or inspect your requests.                  |\n| /udca/disk/used_bytes    | dataset state     | The space occupied by the data files on the data collection pod's disk, in bytes. An increase in this value over time: ready_to_upload implies agent is lagging behind. failed implies files are piling up on disk and not being uploaded. This value is computed every 60 seconds.                         |\n| /udca/server/pruned_file_count  | dataset state     | Count of files which have been deleted because their Time To Life (TTL) was beyond a set threshold. The dataset can include API, trace, and others, and state can be UPLOADED, FAILED, or DISCARDED.                                             |\n| /udca/server/retry_cache_size  | dataset      | A count of the number of files, by dataset, that UDCA is retrying to upload. After 3 retries for each file, UDCA moves the file to the /failed subdirectory and removes it from this cache. An increase in this value over time implies that the cache is not being cleared, which happens when files are moved to the /failed subdirectory after 3 retries.        |\n### Cassandra metrics\nThe [Prometheus](https://github.com/kubernetes/kops/tree/master/addons/prometheus-operator) service collects and processes metrics (as described in [Metrics collection](/apigee/docs/hybrid/v1.1/metrics-collection) ) for Cassandra just as it does for other hybrid services.\nThe following table describes the metrics and labels that Prometheus uses in the Cassandra metrics data. These labels are used in the metrics log entries.\n| Metric name (excluding domain)    | Label  | Use                  |\n|:-------------------------------------------|:-----------|:--------------------------------------------------------------------------|\n| /cassandra/process_max_fds     | nan  | Maximum number of open file descriptors.         |\n| /cassandra/process_open_fds    | nan  | Open file descriptors.             |\n| /cassandra/jvm_memory_pool_bytes_max  | pool  | JVM maximum memory usage for the pool.         |\n| /cassandra/jvm_memory_pool_bytes_init  | pol  | JVM initial memory usage for the pool.         |\n| /cassandra/jvm_memory_bytes_max   | area  | JVM heap maximum memory usage.           |\n| /cassandra/process_cpu_seconds_total  | nan  | User and system CPU time spent in seconds.        |\n| /cassandra/jvm_memory_bytes_used   | area  | JVM heap memory usage.             |\n| /cassandra/compaction_pendingtasks   | unit  | Outstanding compactions for Cassandra sstables. See Compaction for more. |\n| /cassandra/jvm_memory_bytes_init   | area  | JVM heap initial memory usage.           |\n| /cassandra/jvm_memory_pool_bytes_used  | pool  | JVM pool memory usage.             |\n| /cassandra/jvm_memory_pool_bytes_committed | pool  | JVM pool committed memory usage.           |\n| /cassandra/clientrequest_latency   | scope unit | Read request latency in the 75th percentile range in microseconds.  |\n| /cassandra/jvm_memory_bytes_committed  | area  | JVM heap committed memory usage.           |", "guide": "Apigee"}