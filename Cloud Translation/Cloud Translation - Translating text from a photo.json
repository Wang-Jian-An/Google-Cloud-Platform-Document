{"title": "Cloud Translation - Translating text from a photo", "url": "https://cloud.google.com/translate/docs/hybrid-glossaries-tutorial", "abstract": "# Cloud Translation - Translating text from a photo\nThis page shows how to detect text in an image, how to personalize translations, and how to generate synthetic speech from text. This tutorial uses Cloud Vision to detect text in an image file. Then, this tutorial shows how to use Cloud Translation to provide a custom translation of the detected text. Finally, this tutorial uses Text-to-Speech to provide machine dictation of the translated text.\n", "content": "## Objectives\n- Pass text recognized by the Cloud Vision API to the Cloud Translation API.\n- Create and use Cloud Translation glossaries to personalize Cloud Translation API translations.\n- Create an audio representation of translated text using the Text-to-Speech API.## Costs\nEach Google Cloud API uses a separate pricing structure.\nFor pricing details, refer to the [Cloud Vision pricing guide](/vision/pricing) , the [Cloud Translation pricing guide](/translate/pricing) , and the [Text-to-Speech pricing guide](/text-to-speech/pricing) .\n## Before you begin\nMake sure that you have:\n- A project in the [Google Cloud console](https://console.cloud.google.com/) with the Vision API, the Cloud Translation API, and the Text-to-Speech API [enabled](/apis/docs/getting-started#enabling_apis) \n- A basic familiarity with [Python](https://www.python.org/) or [NodeJS](https://nodejs.org/) programming## Setting up client librariesThis tutorial uses [Vision](/vision/docs/reference/libraries) , [Translation](/translate/docs/reference/libraries) , and [Text-to-Speech](/text-to-speech/docs/reference/libraries) client libraries.\nTo install the relevant client libraries, run the following commands from the terminal.```\n pip install --upgrade google-cloud-vision\n pip install --upgrade google-cloud-translate\n pip install --upgrade google-cloud-texttospeech\n \n``````\n npm install --save @google-cloud/vision\n npm install --save @google-cloud/translate\n npm install --save @google-cloud/text-to-speech\n \n```\n## Setting up permissions for glossary creationCreating Translation glossaries requires using a service account key with [\"Cloud Translation API Editor\"](/iam/docs/understanding-roles#cloud-translation-roles) permissions.\nTo set up a service account key with Cloud Translation API Editor permissions, do the following:- Create a service account:- In the Google Cloud console, go to the **Service Accounts** page. [Go toService Accounts](https://console.cloud.google.com/iam-admin/serviceaccounts) \n- Select your project.\n- Click add **Create Service Account** .\n- In the **Service account name** field, enter a name. The Google Cloud console fills in the **Service account ID** field based on this name.\n- Optional: In the **Service account description** field, enter a description for the service account.\n- Click **Create and continue** .\n- Click the **Select a role** field and select **Cloud Translation\u00a0> Cloud Translation API Editor** \n- Click **Done** to finish creating the service account.Do not close your browser window. You will use it in the next step.\n- Download a JSON key for the service account you just created:- In the Google Cloud console, click the email address for the service account that you created.\n- Click **Keys** .\n- Click **Add key** , then click **Create new key** .\n- Click **Create** . A JSON key file is downloaded to your computer.Make sure to store the key file securely, because it can be used to authenticate as your service account. You can move and rename this file however you would like.\n- Click **Close** .\n- In your terminal, set the variable using the following command. Replace with the path to the downloaded JSON file containing your new service account key.\n```\nexport GOOGLE_APPLICATION_CREDENTIALS=path_to_key\n``````\nset GOOGLE_APPLICATION_CREDENTIALS=path_to_key\n```## Importing librariesThis tutorial uses the following system imports and client library imports.Before trying this sample, follow the Python setup instructions in the [Cloud Translation quickstart using   client libraries](/translate/docs/quickstart-client-libraries) .       For more information, see the [Cloud Translation Python API reference documentation](/python/docs/reference/translate/latest) .\nTo authenticate to Cloud Translation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/translate/samples/snippets/hybrid_glossaries/hybrid_tutorial.py) \n```\nimport htmlimport os# Imports the Google Cloud client librariesfrom google.api_core.exceptions import AlreadyExistsfrom google.cloud import texttospeechfrom google.cloud import translate_v3beta1 as translatefrom google.cloud import vision\n```Before trying this sample, follow the Node.js setup instructions in the [Cloud Translation quickstart using   client libraries](/translate/docs/quickstart-client-libraries) .       For more information, see the [Cloud Translation Node.js API reference documentation](/nodejs/docs/reference/translate/latest) .\nTo authenticate to Cloud Translation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/translate/hybridGlossaries.js) \n```\n// Imports the Google Cloud client libraryconst textToSpeech = require('@google-cloud/text-to-speech');const translate = require('@google-cloud/translate').v3beta1;const vision = require('@google-cloud/vision');// Import other required librariesconst fs = require('fs');//const escape = require('escape-html');const util = require('util');\n```\n## Setting your project IDYou must associate a [Google Cloud project](#prerequisites) with each request to a Google Cloud API. Designate your [Google Cloud project](#prerequisites) by setting the `GCLOUD_PROJECT` environment variable from the terminal.\nIn the following command, replace with your Google Cloud project ID. Run the following command from the terminal.```\nexport GCLOUD_PROJECT=project-id\n``````\nset GCLOUD_PROJECT=project-id\n```\n## Using Vision to detect text from an imageUse the Vision API to detect and extract text from an image. The Vision API uses [Optical Character Recognition (OCR)](/vision/docs/ocr) to support two text-detection features: detection of dense text, or [DOCUMENT_TEXT_DETECTION](/vision/docs/features-list#text-detection) , and sparse text detection, or [TEXT_DETECTION](/vision/docs/features-list#document-text-detection-dense------------------text--handwriting) .\nThe following code shows how to use the Vision API [DOCUMENT_TEXT_DETECTION](/vision/docs/features-list#text-detection) feature to detect text in a photo with dense text.Before trying this sample, follow the Python setup instructions in the [Cloud Translation quickstart using   client libraries](/translate/docs/quickstart-client-libraries) .       For more information, see the [Cloud Translation Python API reference documentation](/python/docs/reference/translate/latest) .\nTo authenticate to Cloud Translation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/translate/samples/snippets/hybrid_glossaries/hybrid_tutorial.py) \n```\ndef pic_to_text(infile: str) -> str:\u00a0 \u00a0 \"\"\"Detects text in an image file\u00a0 \u00a0 Args:\u00a0 \u00a0 infile: path to image file\u00a0 \u00a0 Returns:\u00a0 \u00a0 String of text detected in image\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 # Instantiates a client\u00a0 \u00a0 client = vision.ImageAnnotatorClient()\u00a0 \u00a0 # Opens the input image file\u00a0 \u00a0 with open(infile, \"rb\") as image_file:\u00a0 \u00a0 \u00a0 \u00a0 content = image_file.read()\u00a0 \u00a0 image = vision.Image(content=content)\u00a0 \u00a0 # For dense text, use document_text_detection\u00a0 \u00a0 # For less dense text, use text_detection\u00a0 \u00a0 response = client.document_text_detection(image=image)\u00a0 \u00a0 text = response.full_text_annotation.text\u00a0 \u00a0 print(f\"Detected text: {text}\")\u00a0 \u00a0 return text\n```Before trying this sample, follow the Node.js setup instructions in the [Cloud Translation quickstart using   client libraries](/translate/docs/quickstart-client-libraries) .       For more information, see the [Cloud Translation Node.js API reference documentation](/nodejs/docs/reference/translate/latest) .\nTo authenticate to Cloud Translation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/translate/hybridGlossaries.js) \n```\n/**\u00a0* Detects text in an image file\u00a0*\u00a0* ARGS\u00a0* inputFile: path to image file\u00a0* RETURNS\u00a0* string of text detected in the input image\u00a0**/async function picToText(inputFile) {\u00a0 // Creates a client\u00a0 const client = new vision.ImageAnnotatorClient();\u00a0 // Performs text detection on the local file\u00a0 const [result] = await client.textDetection(inputFile);\u00a0 return result.fullTextAnnotation.text;}\n```\n## Using Translation with glossariesAfter extracting text from an image, use [Translation glossaries](/translate/docs/glossary) to personalize the translation of the extracted text. Glossaries provide pre-defined translations that override the Cloud Translation API translations of designated terms.\nGlossary use cases include:- **Product names:** For example, 'Google Home' must translate to 'Google Home'.\n- **Ambiguous words:** For example, the word 'bat' can mean a piece of sports equipment or an animal. If you know that you are translating words about sports, you might want to use a glossary to feed the Cloud Translation API the sports translation of 'bat', not the animal translation.\n- **Borrowed words:** For example, 'bouillabaisse' in French translates to 'bouillabaisse' in English; the English language borrowed the word 'bouillabaisse' from the French language. An English speaker lacking French cultural context might not know that bouillabaisse is a French fish stew dish. Glossaries can override a translation so that 'bouillabaisse' in French translates to 'fish stew' in English.\n### Making a glossary fileThe Cloud Translation API accepts TSV, CSV, or TMX glossary files. This tutorial uses a CSV file uploaded to Cloud Storage to define sets of equivalent terms.\nTo make a glossary CSV file:- **Designate the language of a column** using either [ISO-639](https://wikipedia.org/wiki/ISO_639) or [BCP-47](https://tools.ietf.org/html/bcp47) language codes in the first row of the CSV file.```\nfr,en,\n```\n- **List pairs of equivalent terms** in each row of the CSV file. Separate terms with commas. The following example defines the English translation for several culinary French words.```\nfr,en,\nch\u00e8vre,goat cheese,\ncr\u00e8me brul\u00e9e,cr\u00e8me brul\u00e9e,\nbouillabaisse,fish stew,\nsteak frites,steak with french fries,\n```\n- **Define variants of a word** . The Cloud Translation API is case-sensitive and sensitive to special characters such as accented words. Ensure that your glossary handles variations on a word by explicitly defining different spellings of the word.```\nfr,en,\nchevre,goat cheese,\nChevre,Goat cheese,\nch\u00e8vre,goat cheese,\nCh\u00e8vre,Goat cheese,\ncr\u00e8me brul\u00e9e,cr\u00e8me brul\u00e9e,\nCr\u00e8me brul\u00e9e,Cr\u00e8me brul\u00e9e,\nCr\u00e8me Brul\u00e9e,Cr\u00e8me Brul\u00e9e,\nbouillabaisse,fish stew,\nBouillabaisse,Fish stew,\nsteak frites,steak with french fries,\nSteak frites,Steak with french fries,\nSteak Frites,Steak with French Fries,\n```\n- [Upload](/storage/docs/uploading-objects) the glossary to a [Cloud Storage bucket](/storage/docs/creating-buckets) . For the purposes of this tutorial, you do not need to upload a glossary file to a Cloud Storage bucket nor do you need to create a Cloud Storage bucket. Instead, use the publicly-available glossary file created for this tutorial to avoid incurring any Cloud Storage costs. Send the URI of a glossary file in Cloud Storage to the Cloud Translation API to create a glossary resource. The URI of the publicly-available glossary file for this tutorial is [gs://cloud-samples-data/translation/bistro_glossary.csv](https://storage.cloud.google.com/cloud-samples-data/translation/bistro_glossary.csv) . To download the glossary, click on the above URI link, but do not open it in a new tab.\n### Creating a glossary resourceIn order to use a glossary, you must create a glossary resource with the Cloud Translation API. To create a glossary resource, send the URI of a glossary file in Cloud Storage to the Cloud Translation API.\nMake sure that you are using a service account key with [\"Cloud Translation API Editor\"](#setting_up_permissions_for_glossary_creation) permissions and make sure that you have [set your project ID from the terminal](#setting_your_project_id) .\nThe following function creates a glossary resource. With this glossary resource, you can personalize the translation request in the next step of this tutorial.Before trying this sample, follow the Python setup instructions in the [Cloud Translation quickstart using   client libraries](/translate/docs/quickstart-client-libraries) .       For more information, see the [Cloud Translation Python API reference documentation](/python/docs/reference/translate/latest) .\nTo authenticate to Cloud Translation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/translate/samples/snippets/hybrid_glossaries/hybrid_tutorial.py) \n```\ndef create_glossary(\u00a0 \u00a0 languages: list,\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 glossary_name: str,\u00a0 \u00a0 glossary_uri: str,) -> str:\u00a0 \u00a0 \"\"\"Creates a GCP glossary resource\u00a0 \u00a0 Assumes you've already manually uploaded a glossary to Cloud Storage\u00a0 \u00a0 Args:\u00a0 \u00a0 languages: list of languages in the glossary\u00a0 \u00a0 project_id: GCP project id\u00a0 \u00a0 glossary_name: name you want to give this glossary resource\u00a0 \u00a0 glossary_uri: the uri of the glossary you uploaded to Cloud Storage\u00a0 \u00a0 Returns:\u00a0 \u00a0 name of the created or existing glossary\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 # Instantiates a client\u00a0 \u00a0 client = translate.TranslationServiceClient()\u00a0 \u00a0 # Designates the data center location that you want to use\u00a0 \u00a0 location = \"us-central1\"\u00a0 \u00a0 # Set glossary resource name\u00a0 \u00a0 name = client.glossary_path(project_id, location, glossary_name)\u00a0 \u00a0 # Set language codes\u00a0 \u00a0 language_codes_set = translate.Glossary.LanguageCodesSet(language_codes=languages)\u00a0 \u00a0 gcs_source = translate.GcsSource(input_uri=glossary_uri)\u00a0 \u00a0 input_config = translate.GlossaryInputConfig(gcs_source=gcs_source)\u00a0 \u00a0 # Set glossary resource information\u00a0 \u00a0 glossary = translate.Glossary(\u00a0 \u00a0 \u00a0 \u00a0 name=name, language_codes_set=language_codes_set, input_config=input_config\u00a0 \u00a0 )\u00a0 \u00a0 parent = f\"projects/{project_id}/locations/{location}\"\u00a0 \u00a0 # Create glossary resource\u00a0 \u00a0 # Handle exception for case in which a glossary\u00a0 \u00a0 # \u00a0with glossary_name already exists\u00a0 \u00a0 try:\u00a0 \u00a0 \u00a0 \u00a0 operation = client.create_glossary(parent=parent, glossary=glossary)\u00a0 \u00a0 \u00a0 \u00a0 operation.result(timeout=90)\u00a0 \u00a0 \u00a0 \u00a0 print(\"Created glossary \" + glossary_name + \".\")\u00a0 \u00a0 except AlreadyExists:\u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"The glossary \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + glossary_name\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" already exists. No new glossary was created.\"\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 return glossary_name\n```Before trying this sample, follow the Node.js setup instructions in the [Cloud Translation quickstart using   client libraries](/translate/docs/quickstart-client-libraries) .       For more information, see the [Cloud Translation Node.js API reference documentation](/nodejs/docs/reference/translate/latest) .\nTo authenticate to Cloud Translation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/translate/hybridGlossaries.js) \n```\n/** Creates a GCP glossary resource\u00a0* Assumes you've already manually uploaded a glossary to Cloud Storage\u00a0*\u00a0* ARGS\u00a0* languages: list of languages in the glossary\u00a0* projectId: GCP project id\u00a0* glossaryName: name you want to give this glossary resource\u00a0* glossaryUri: the uri of the glossary you uploaded to Cloud Storage\u00a0* RETURNS\u00a0* nothing\u00a0**/async function createGlossary(\u00a0 languages,\u00a0 projectId,\u00a0 glossaryName,\u00a0 glossaryUri) {\u00a0 // Instantiates a client\u00a0 const translationClient = await new translate.TranslationServiceClient();\u00a0 // Construct glossary\u00a0 const glossary = {\u00a0 \u00a0 languageCodesSet: {\u00a0 \u00a0 \u00a0 languageCodes: languages,\u00a0 \u00a0 },\u00a0 \u00a0 inputConfig: {\u00a0 \u00a0 \u00a0 gcsSource: {\u00a0 \u00a0 \u00a0 \u00a0 inputUri: glossaryUri,\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 \u00a0 name: translationClient.glossaryPath(\u00a0 \u00a0 \u00a0 projectId,\u00a0 \u00a0 \u00a0 'us-central1',\u00a0 \u00a0 \u00a0 glossaryName\u00a0 \u00a0 ),\u00a0 };\u00a0 // Construct request\u00a0 const request = {\u00a0 \u00a0 parent: translationClient.locationPath(projectId, 'us-central1'),\u00a0 \u00a0 glossary: glossary,\u00a0 };\u00a0 // Create glossary using a long-running operation.\u00a0 try {\u00a0 \u00a0 const [operation] = await translationClient.createGlossary(request);\u00a0 \u00a0 // Wait for operation to complete.\u00a0 \u00a0 await operation.promise();\u00a0 \u00a0 console.log('Created glossary ' + glossaryName + '.');\u00a0 } catch (AlreadyExists) {\u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 'The glossary ' +\u00a0 \u00a0 \u00a0 \u00a0 glossaryName +\u00a0 \u00a0 \u00a0 \u00a0 ' already exists. No new glossary was created.'\u00a0 \u00a0 );\u00a0 }}\n```\n### Translating with glossariesOnce you create a glossary resource, you can use the glossary resource to personalize translations of text that you send to the Cloud Translation API.\nThe following function uses your previously-created glossary resource to personalize the translation of text.Before trying this sample, follow the Python setup instructions in the [Cloud Translation quickstart using   client libraries](/translate/docs/quickstart-client-libraries) .       For more information, see the [Cloud Translation Python API reference documentation](/python/docs/reference/translate/latest) .\nTo authenticate to Cloud Translation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/translate/samples/snippets/hybrid_glossaries/hybrid_tutorial.py) \n```\ndef translate_text(\u00a0 \u00a0 text: str,\u00a0 \u00a0 source_language_code: str,\u00a0 \u00a0 target_language_code: str,\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 glossary_name: str,) -> str:\u00a0 \u00a0 \"\"\"Translates text to a given language using a glossary\u00a0 \u00a0 Args:\u00a0 \u00a0 text: String of text to translate\u00a0 \u00a0 source_language_code: language of input text\u00a0 \u00a0 target_language_code: language of output text\u00a0 \u00a0 project_id: GCP project id\u00a0 \u00a0 glossary_name: name you gave your project's glossary\u00a0 \u00a0 \u00a0 \u00a0 resource when you created it\u00a0 \u00a0 Return:\u00a0 \u00a0 String of translated text\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 # Instantiates a client\u00a0 \u00a0 client = translate.TranslationServiceClient()\u00a0 \u00a0 # Designates the data center location that you want to use\u00a0 \u00a0 location = \"us-central1\"\u00a0 \u00a0 glossary = client.glossary_path(project_id, location, glossary_name)\u00a0 \u00a0 glossary_config = translate.TranslateTextGlossaryConfig(glossary=glossary)\u00a0 \u00a0 parent = f\"projects/{project_id}/locations/{location}\"\u00a0 \u00a0 result = client.translate_text(\u00a0 \u00a0 \u00a0 \u00a0 request={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"parent\": parent,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"contents\": [text],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"mime_type\": \"text/plain\", \u00a0# mime types: text/plain, text/html\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"source_language_code\": source_language_code,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"target_language_code\": target_language_code,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"glossary_config\": glossary_config,\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 )\u00a0 \u00a0 # Extract translated text from API response\u00a0 \u00a0 return result.glossary_translations[0].translated_text\n```Before trying this sample, follow the Node.js setup instructions in the [Cloud Translation quickstart using   client libraries](/translate/docs/quickstart-client-libraries) .       For more information, see the [Cloud Translation Node.js API reference documentation](/nodejs/docs/reference/translate/latest) .\nTo authenticate to Cloud Translation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/translate/hybridGlossaries.js) \n```\n/**\u00a0* Translates text to a given language using a glossary\u00a0*\u00a0* ARGS\u00a0* text: String of text to translate\u00a0* sourceLanguageCode: language of input text\u00a0* targetLanguageCode: language of output text\u00a0* projectId: GCP project id\u00a0* glossaryName: name you gave your project's glossary\u00a0* \u00a0 \u00a0 resource when you created it\u00a0* RETURNS\u00a0* String of translated text\u00a0**/async function translateText(\u00a0 text,\u00a0 sourceLanguageCode,\u00a0 targetLanguageCode,\u00a0 projectId,\u00a0 glossaryName) {\u00a0 // Instantiates a client\u00a0 const translationClient = new translate.TranslationServiceClient();\u00a0 const glossary = translationClient.glossaryPath(\u00a0 \u00a0 projectId,\u00a0 \u00a0 'us-central1',\u00a0 \u00a0 glossaryName\u00a0 );\u00a0 const glossaryConfig = {\u00a0 \u00a0 glossary: glossary,\u00a0 };\u00a0 // Construct request\u00a0 const request = {\u00a0 \u00a0 parent: translationClient.locationPath(projectId, 'us-central1'),\u00a0 \u00a0 contents: [text],\u00a0 \u00a0 mimeType: 'text/plain', // mime types: text/plain, text/html\u00a0 \u00a0 sourceLanguageCode: sourceLanguageCode,\u00a0 \u00a0 targetLanguageCode: targetLanguageCode,\u00a0 \u00a0 glossaryConfig: glossaryConfig,\u00a0 };\u00a0 // Run request\u00a0 const [response] = await translationClient.translateText(request);\u00a0 // Extract the string of translated text\u00a0 return response.glossaryTranslations[0].translatedText;}\n```\n## Using Text-to-Speech with Speech Synthesis Markup LanguageNow that you have personalized a translation of image-detected text, you are ready to use the Text-to-Speech API. The Text-to-Speech API can create synthetic audio of your translated text.\nThe Text-to-Speech API generates synthetic audio from either a string of plain text or a string of text marked up with [Speech Synthesis Markup Language (SSML)](/text-to-speech/docs/ssml) . SSML is a markup language which supports annotating text with [SSML tags](/text-to-speech/docs/ssml#support-for-ssml-elements) . You can use SSML tags to influence how the Text-to-Speech API [formats synthetic speech creation](/text-to-speech/docs/ssml-tutorial) .\nThe following function converts a string of SSML to an MP3 file of synthetic speech.Before trying this sample, follow the Python setup instructions in the [Cloud Translation quickstart using   client libraries](/translate/docs/quickstart-client-libraries) .       For more information, see the [Cloud Translation Python API reference documentation](/python/docs/reference/translate/latest) .\nTo authenticate to Cloud Translation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/translate/samples/snippets/hybrid_glossaries/hybrid_tutorial.py) \n```\ndef text_to_speech(text: str, outfile: str) -> str:\u00a0 \u00a0 \"\"\"Converts plaintext to SSML and\u00a0 \u00a0 generates synthetic audio from SSML\u00a0 \u00a0 Args:\u00a0 \u00a0 text: text to synthesize\u00a0 \u00a0 outfile: filename to use to store synthetic audio\u00a0 \u00a0 Returns:\u00a0 \u00a0 String of synthesized audio\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 # Replace special characters with HTML Ampersand Character Codes\u00a0 \u00a0 # These Codes prevent the API from confusing text with\u00a0 \u00a0 # SSML commands\u00a0 \u00a0 # For example, '<' --> '&lt;' and '&' --> '&amp;'\u00a0 \u00a0 escaped_lines = html.escape(text)\u00a0 \u00a0 # Convert plaintext to SSML in order to wait two seconds\u00a0 \u00a0 # \u00a0 between each line in synthetic speech\u00a0 \u00a0 ssml = \"<speak>{}</speak>\".format(\u00a0 \u00a0 \u00a0 \u00a0 escaped_lines.replace(\"\\n\", '\\n<break time=\"2s\"/>')\u00a0 \u00a0 )\u00a0 \u00a0 # Instantiates a client\u00a0 \u00a0 client = texttospeech.TextToSpeechClient()\u00a0 \u00a0 # Sets the text input to be synthesized\u00a0 \u00a0 synthesis_input = texttospeech.SynthesisInput(ssml=ssml)\u00a0 \u00a0 # Builds the voice request, selects the language code (\"en-US\") and\u00a0 \u00a0 # the SSML voice gender (\"MALE\")\u00a0 \u00a0 voice = texttospeech.VoiceSelectionParams(\u00a0 \u00a0 \u00a0 \u00a0 language_code=\"en-US\", ssml_gender=texttospeech.SsmlVoiceGender.MALE\u00a0 \u00a0 )\u00a0 \u00a0 # Selects the type of audio file to return\u00a0 \u00a0 audio_config = texttospeech.AudioConfig(\u00a0 \u00a0 \u00a0 \u00a0 audio_encoding=texttospeech.AudioEncoding.MP3\u00a0 \u00a0 )\u00a0 \u00a0 # Performs the text-to-speech request on the text input with the selected\u00a0 \u00a0 # voice parameters and audio file type\u00a0 \u00a0 request = texttospeech.SynthesizeSpeechRequest(\u00a0 \u00a0 \u00a0 \u00a0 input=synthesis_input, voice=voice, audio_config=audio_config\u00a0 \u00a0 )\u00a0 \u00a0 response = client.synthesize_speech(request=request)\u00a0 \u00a0 # Writes the synthetic audio to the output file.\u00a0 \u00a0 with open(outfile, \"wb\") as out:\u00a0 \u00a0 \u00a0 \u00a0 out.write(response.audio_content)\u00a0 \u00a0 \u00a0 \u00a0 print(\"Audio content written to file \" + outfile)\n```Before trying this sample, follow the Node.js setup instructions in the [Cloud Translation quickstart using   client libraries](/translate/docs/quickstart-client-libraries) .       For more information, see the [Cloud Translation Node.js API reference documentation](/nodejs/docs/reference/translate/latest) .\nTo authenticate to Cloud Translation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/translate/hybridGlossaries.js) \n```\n/**\u00a0* Generates synthetic audio from plaintext tagged with SSML.\u00a0*\u00a0* Given the name of a text file and an output file name, this function\u00a0* tags the text in the text file with SSML. This function then\u00a0* calls the Text-to-Speech API. The API returns a synthetic audio\u00a0* version of the text, formatted according to the SSML commands. This\u00a0* function saves the synthetic audio to the designated output file.\u00a0*\u00a0* ARGS\u00a0* text: String of plaintext\u00a0* outFile: String name of file under which to save audio output\u00a0* RETURNS\u00a0* nothing\u00a0*\u00a0*/async function syntheticAudio(text, outFile) {\u00a0 // Replace special characters with HTML Ampersand Character Codes\u00a0 // These codes prevent the API from confusing text with SSML tags\u00a0 // For example, '<' --> '&lt;' and '&' --> '&amp;'\u00a0 let escapedLines = text.replace(/&/g, '&amp;');\u00a0 escapedLines = escapedLines.replace(/\"/g, '&quot;');\u00a0 escapedLines = escapedLines.replace(/</g, '&lt;');\u00a0 escapedLines = escapedLines.replace(/>/g, '&gt;');\u00a0 // Convert plaintext to SSML\u00a0 // Tag SSML so that there is a 2 second pause between each address\u00a0 const expandedNewline = escapedLines.replace(/\\n/g, '\\n<break time=\"2s\"/>');\u00a0 const ssmlText = '<speak>' + expandedNewline + '</speak>';\u00a0 // Creates a client\u00a0 const client = new textToSpeech.TextToSpeechClient();\u00a0 // Constructs the request\u00a0 const request = {\u00a0 \u00a0 // Select the text to synthesize\u00a0 \u00a0 input: {ssml: ssmlText},\u00a0 \u00a0 // Select the language and SSML Voice Gender (optional)\u00a0 \u00a0 voice: {languageCode: 'en-US', ssmlGender: 'MALE'},\u00a0 \u00a0 // Select the type of audio encoding\u00a0 \u00a0 audioConfig: {audioEncoding: 'MP3'},\u00a0 };\u00a0 // Performs the Text-to-Speech request\u00a0 const [response] = await client.synthesizeSpeech(request);\u00a0 // Write the binary audio content to a local file\u00a0 const writeFile = util.promisify(fs.writeFile);\u00a0 await writeFile(outFile, response.audioContent, 'binary');\u00a0 console.log('Audio content written to file ' + outFile);}\n```\n## Putting it all togetherIn the previous steps, you defined functions in `hybrid_glossaries.py` that use Vision, Translation, and Text-to-Speech. Now, you are ready to use these functions to generate synthetic speech of translated text from the following photo.The following code calls functions defined in `hybrid_glossaries.py` to:- create a Cloud Translation API glossary resource\n- use the Vision API to detect text in the above image\n- perform a Cloud Translation API glossary translation of the detected text\n- generate Text-to-Speech synthetic speech of the translated text\nBefore trying this sample, follow the Python setup instructions in the [Cloud Translation quickstart using   client libraries](/translate/docs/quickstart-client-libraries) .       For more information, see the [Cloud Translation Python API reference documentation](/python/docs/reference/translate/latest) .\nTo authenticate to Cloud Translation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/translate/samples/snippets/hybrid_glossaries/hybrid_tutorial.py) \n```\ndef main() -> None:\u00a0 \u00a0 \"\"\"This method is called when the tutorial is run in the Google Cloud\u00a0 \u00a0 Translation API. It creates a glossary, translates text to\u00a0 \u00a0 French, and speaks the translated text.\u00a0 \u00a0 Args:\u00a0 \u00a0 None\u00a0 \u00a0 Returns:\u00a0 \u00a0 None\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 # Photo from which to extract text\u00a0 \u00a0 infile = \"resources/example.png\"\u00a0 \u00a0 # Name of file that will hold synthetic speech\u00a0 \u00a0 outfile = \"resources/example.mp3\"\u00a0 \u00a0 # Defines the languages in the glossary\u00a0 \u00a0 # This list must match the languages in the glossary\u00a0 \u00a0 # \u00a0 Here, the glossary includes French and English\u00a0 \u00a0 glossary_langs = [\"fr\", \"en\"]\u00a0 \u00a0 # Name that will be assigned to your project's glossary resource\u00a0 \u00a0 glossary_name = \"bistro-glossary\"\u00a0 \u00a0 # uri of .csv file uploaded to Cloud Storage\u00a0 \u00a0 glossary_uri = \"gs://cloud-samples-data/translation/bistro_glossary.csv\"\u00a0 \u00a0 created_glossary_name = create_glossary(\u00a0 \u00a0 \u00a0 \u00a0 glossary_langs, PROJECT_ID, glossary_name, glossary_uri\u00a0 \u00a0 )\u00a0 \u00a0 # photo -> detected text\u00a0 \u00a0 text_to_translate = pic_to_text(infile)\u00a0 \u00a0 # detected text -> translated text\u00a0 \u00a0 text_to_speak = translate_text(\u00a0 \u00a0 \u00a0 \u00a0 text_to_translate, \"fr\", \"en\", PROJECT_ID, created_glossary_name\u00a0 \u00a0 )\u00a0 \u00a0 # translated text -> synthetic audio\u00a0 \u00a0 text_to_speech(text_to_speak, outfile)\n```Before trying this sample, follow the Node.js setup instructions in the [Cloud Translation quickstart using   client libraries](/translate/docs/quickstart-client-libraries) .       For more information, see the [Cloud Translation Node.js API reference documentation](/nodejs/docs/reference/translate/latest) .\nTo authenticate to Cloud Translation, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/translate/hybridGlossaries.js) \n```\nawait createGlossary(glossaryLangs, projectId, glossaryName, glossaryUri);const text = await picToText(inFile);const translatedText = await translateText(\u00a0 text,\u00a0 'fr',\u00a0 'en',\u00a0 projectId,\u00a0 glossaryName);syntheticAudio(translatedText, outFile);\n```\n### Running the codeTo run the code, enter the following command in terminal in the [directory](#downloading_the_code_samples) where your code is located:\n```\npython hybrid_tutorial.py\n \n``````\n node hybridGlossaries.js\n \n```The following output appears:\n```\nCreated glossary bistro-glossary.\nAudio content written to file resources/example.mp3\n```\nAfter running the code, navigate into the directory from the directory. Check the resources directory for an `example.mp3` file.\nListen to the following audio clip to check that your `example.mp3` file sounds the same.\n ## Troubleshooting error messages\n- ```\n403 IAM permission 'cloudtranslate.glossaries.create' denied.\n```Using a [service account key without \"Cloud Translation API Editor\" permissions](#setting_up_permissions_for_glossary_creation) raises this exception.\n- ```\nKeyError: 'GCLOUD_PROJECT'\n```Not setting your [GCLOUD_PROJECT variable](#setting_your_project_id) generates this error.\n- ```\n400 Invalid resource name project id\n```Either using a glossary name which contains characters other than lowercase letters, digits, periods, a colon, or hyphens, or using a [service account key without \"Cloud Translation API Editor\" permissions](#setting_up_permissions_for_glossary_creation) raises this exception.\n- ```\nFile filename was not found.\n```Setting the [GOOGLE_APPLICATION_CREDENTIALS variable](#setting_up_permissions_for_glossary_creation) to an invalid filepath raises this exception.\n- ```\nCould not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application\n```Not [setting the GOOGLE_APPLICATION_CREDENTIALS variable](#setting_up_permissions_for_glossary_creation) raises this exception.\n- ```\nForbidden: 403 POST API has not been used or is disabled\n```Calling the Cloud Translation API, the Cloud Vision API, or the Text-to-Speech API without [enabling their APIs](/apis/docs/getting-started#enabling_apis) generates this warning.\n- ```\nAttributeError: 'module' object has no attribute 'escape'\n```Python 2.7.10 or earlier is not compatible with `HTML` . To fix this error, use a [Python virtual environment](https://docs.python.org/3/library/venv.html) . The virtual environment will use the newest version of Python.\n- ```\nUnicodeEncodeError\n```Python 2.7.10 or earlier is not compatible with `HTML` . To fix this error, use a [Python virtual environment](https://docs.python.org/3/library/venv.html) . The virtual environment will use the newest version of Python.\n## Cleaning upUse the [Google Cloud console](https://console.cloud.google.com/) to delete your project if you do not need it. Deleting your project prevents incurring additional charges to your Cloud Billing account for the resources used in this tutorial.\n### Deleting your project\n- In the [Google Cloud console](https://console.cloud.google.com/) , go to the Projects page.\n- In the project list, select the project you want to delete and click **Delete** .\n- In the dialog box, type the project ID, and click **Shut down** to delete the project.\n## What's nextCongratulations! You just used Vision OCR to detect text in an image. Then, you created a Translation glossary and performed a translated with that glossary. Afterwards, you used Text-to-Speech to generate synthetic audio of the translated text.\nTo build on your knowledge of Vision, Translation, and Text-to-Speech:- Make your own glossary. Learn how to [create a Cloud Storage bucket](/storage/docs/creating-buckets) and to [upload your glossary CSV file to the bucket](/storage/docs/uploading-objects) .\n- Experiment with other ways to use [Translation glossaries](/translate/docs/glossary) .\n- Learn how to [use Cloud Storage with Cloud Vision OCR](/functions/docs/tutorials/ocr) .\n- Learn more about how to [use SSML with Text-to-Speech](/text-to-speech/docs/ssml-tutorial) .\n- Learn how to use the Vision API [imageContext field](/vision/docs/handwriting#specify_the_language_optional) to pass along additional context about a photo when using Vision OCR.\n- Explore [community tutorials](/community/tutorials) .", "guide": "Cloud Translation"}