{"title": "Google Kubernetes Engine (GKE) - Troubleshoot load balancing in GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview", "abstract": "# Google Kubernetes Engine (GKE) - Troubleshoot load balancing in GKE\nThis page shows you how to resolve issues related to load balancing in Google Kubernetes Engine (GKE) clusters using Service, Ingress, or Gateway resources.\n[Cloud Customer Care](/kubernetes-engine/docs/getting-support)\n#", "content": "## BackendConfig not found\nThis error occurs when a BackendConfig for a Service port is specified in the Service annotation, but the actual BackendConfig resource couldn't be found.\nTo evaluate a Kubernetes event, run the following command:\n```\nkubectl get event\n```\nThe following example output indicates your BackendConfig was not found:\n```\nKIND ... SOURCE\nIngress ... loadbalancer-controller\nMESSAGE\nError during sync: error getting BackendConfig for port 80 on service \"default/my-service\":\nno BackendConfig for service port exists\n```\nTo resolve this issue, ensure you have not created the BackendConfig resource in the wrong namespace or misspelled its reference in the Service annotation.\n### Ingress security policy not found\nAfter the Ingress object is created, if the security policy isn't properly associated with the LoadBalancer Service, evaluate the Kubernetes event to see if there is a configuration mistake. If your BackendConfig specifies a security policy that does not exist, a warning event is periodically emitted.\nTo evaluate a Kubernetes event, run the following command:\n```\nkubectl get event\n```\nThe following example output indicates your security policy was not found:\n```\nKIND ... SOURCE\nIngress ... loadbalancer-controller\nMESSAGE\nError during sync: The given security policy \"my-policy\" does not exist.\n```\nTo resolve this issue, specify the correct security policy name in your BackendConfig.\n### Addressing 500 series errors with NEGs during workload scaling in GKE\n**Symptom:**\nWhen you use GKE provisioned NEGs for load balancing, you might experience 502 or 503 errors for the services during the workload scale down. 502 errors occur when Pods are terminated before existing connections close, while the 503 errors occur when traffic is directed to deleted Pods.\nThis issue can affect clusters if you are using GKE managed load balancing products that use NEGs, including Gateway, Ingress, and standalone NEGs. If you frequently scale your workloads, your cluster is at a higher risk of being affected.\n**Diagnosis:**\nRemoving a Pod in Kubernetes without draining its endpoint and removing it from the NEG first leads to 500 series errors. To avoid issues during Pod termination, you must consider the order of operations. The following images display scenarios when `BackendService Drain Timeout` is unset and `BackendService Drain Timeout` is set with `BackendConfig` .\n**Note:** The following resolution is not effective in resolving other 500 series errors resulting from Pod or node preemptions in GKE, particularly in the case of GKE with Spot VMs and preemptible VMs.\n**Scenario 1** : `BackendService Drain Timeout` is unset.\nThe following image displays a scenario where the `BackendService Drain Timeout` is unset.\n**Scenario 2** : `BackendService Drain Timeout` is set.\nThe following image displays a scenario where the `BackendService Drain Timeout` is set.\nThe exact time the 500 series errors occur depends on the following factors:\n- **NEG API detach latency** : The NEG API detach latency represents the current time taken for the detach operation to finalize in Google Cloud. This is influenced by a variety of factors outside Kubernetes, including the type of load balancer and the specific zone.\n- **Drain latency** : Drain latency represents the time taken for the load balancer to start directing traffic away from a particular part of your system. Once drain is initiated, the load balancer stops sending new requests to the endpoint, however there is still a latency in triggering drain (Drain Latency) which can cause temporary 503 errors if the Pod no longer exists.\n- **Health check configuration** : More sensitive health check thresholds mitigate the duration of 503 errors as it can signal the load balancer to stop sending requests to endpoints even if the detach operation has not finished.\n- **Termination grace period** : The termination grace period determines the maximum amount of time a Pod is given to exit. However, a Pod can exit before the termination grace period completes. If a Pod takes longer than this period, the Pod is forced to exit at the end of this period. This is a setting on the Pod and needs to be configured in the workload definition.\n**Potential resolution:**\nTo prevent those 5XX errors, apply the following settings. The timeout values are suggestive and you might need to adjust them for your specific application. The following section guides you through the [customization](#customize_timeouts) process.\nThe following image displays how to keep the Pod alive with `preStopHook` :\nTo avoid 500 series errors, perform the following steps:\n- Set the `BackendService Drain Timeout` for your service to 1 minute. **Note:** If your average request time is more than 30 seconds, see the [Customize timeouts](#customize_timeouts) to customize the `BackendService Drain Timeout` .- For Ingress Users, see [set the timeout on theBackendConfig](/kubernetes-engine/docs/how-to/ingress-configuration#draining_timeout) .\n- For Gateway Users, see [configure the timeout on theGCPBackendPolicy](/kubernetes-engine/docs/how-to/configure-gateway-resources#configure_connection_draining) .\n- For those managing their BackendServices directly when using Standalone NEGs, see [set the timeout directly on the Backend Service](/load-balancing/docs/enabling-connection-draining#enabling_connection_draining) .\n- Extend the `terminationGracePeriod` on the Pod.Set the `terminationGracePeriodSeconds` on the Pod to 3.5 minutes. When combined with the recommended settings, this allows your Pods a 30 to 45 second window for a graceful shutdown after the Pod's endpoint has been removed from the NEG. If you require more time for the graceful shutdown, you can extend the grace period or follow the instructions mentioned in the [Customize timeouts](#customize_timeouts) section.The following `BackendConfig` manifest specifies a connection draining timeout of 210 seconds (3.5 minutes):```\napiVersion: v1kind: BackendConfigmetadata:\u00a0 name: my-backendconfigspec:\u00a0 terminationGracePeriodSeconds: 210\u00a0 containers:\u00a0 - name: my-app\u00a0 \u00a0 image: my-app-image:latest\u00a0 \u00a0 ports:\u00a0 \u00a0 - containerPort: 80\n```\n- Apply a `preStopHook` to all containers.Apply a `preStopHook` that will ensure the Pod is alive for 120 seconds longer while the Pod's endpoint is drained in the load balancer and the endpoint is removed from the NEG. **Note:** Apply the `preStopHook` to every container in your Pod. Without this, containers without the hook will exit as soon as the Pod is deleted. If you use a tool that injects containers, ensure that the injected containers will have the required `preStopHook` .```\napiVersion: v1kind: Podmetadata:\u00a0 name: my-podspec:\u00a0 containers:\u00a0 - name: my-app\u00a0 \u00a0 # Container configuration details...\u00a0 \u00a0 lifecycle:\u00a0 \u00a0 \u00a0 preStop:\u00a0 \u00a0 \u00a0 \u00a0 exec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command: [\"/bin/sh\", \"-c\", \"sleep 120s\"]\n```To ensure Pod continuity and prevent 500 series errors, the Pod must be alive until the endpoint is removed from the NEG. Specially to prevent 502 and 503 errors, consider implementing a combination of timeouts and a `preStopHook` .\n**Note:** If the health check configuration is set to detect failures immediately, it may lead to load balancers ceasing to send requests sooner, potentially reducing the occurrence of 503 errors. However, note that 503 errors may still occur during the duration of the unhealthy threshold. It's essential to understand that configuring health checks won't prevent 502 errors, which occur when existing connections are prematurely terminated.\nTo keep the Pod alive longer during the shutdown process, add a `preStopHook` to the Pod. Run the `preStopHook` before a Pod is signaled to exit, so the `preStopHook` can be used to keep the Pod around until its corresponding endpoint is removed from the NEG.\nTo extend the duration that a Pod remains active during the shutdown process, insert a `preStopHook` into the Pod configuration as follows:\n```\nspec:\u00a0 containers:\u00a0 - name: my-app\u00a0 \u00a0 ...\u00a0 \u00a0 lifecycle:\u00a0 \u00a0 \u00a0 preStopHook:\u00a0 \u00a0 \u00a0 \u00a0 exec:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command: [\"/bin/sh\", \"-c\", \"sleep <latency time>\"]\n```\n**Note:** In multi-container Pods, ensure that you add the `preStopHook` to each container to keep all containers active. Without this, only containers with the hook runs, and the rest stops. If you're using a tool that adds containers, make sure those added containers also have the necessary `preStopHook` .\nYou can configure timeouts and related settings to manage the graceful shutdown of Pods during workload scale downs. You can adjust timeouts based on specific use cases. We recommend that you start with longer timeouts and reduce the duration as necessary. You can customize the timeouts by configuring timeout-related parameters and the `preStopHook` in the following ways:\n**Backend Service Drain Timeout**\nThe `Backend Service Drain Timeout` parameter is unset by default and has no effect. If you set the `Backend Service Drain Timeout` parameter and activate it, the load balancer stops routing new requests to the endpoint and waits the timeout before terminating existing connections.\nYou can [set the Backend Service Drain Timeout](#step1) parameter by using the `BackendConfig` with Ingress, the `GCPBackendPolicy` with Gateway or manually on the `BackendService` with standalone NEGs. The timeout should be 1.5 to 2 times longer than the time it takes to process a request. This ensures if a request came in right before the drain was initiated, it will complete before the timeout completes. Setting the `Backend Service Drain Timeout` parameter to a value greater than 0 helps mitigate 503 errors because no new requests are sent to endpoints scheduled for removal. For this timeout to be effective, you must use it in conjunction with the `preStopHook` to ensure that the Pod remains active while the drain occurs. Without this combination, existing requests that didn't complete will receive a 502 error.\n**preStopHook time**\nThe `preStopHook` must delay Pod shut down sufficiently for both Drain latency and backend service drain timeout to complete, ensuring proper connection drainage and endpoint removal from the NEG before the Pod is shut down.\nFor optimal results, ensure your `preStopHook` execution time is less than or equal to the sum of the `Backend Service Drain Timeout` and Drain Latency.\nThis can be calculated using the formula:\n`preStopHook >= Backend Service Drain Timeout + Drain Latency`\nWe recommend setting the Drain Latency to 1 minute. If 500 errors persist, estimate the total occurrence duration and add double that time to the latency. This ensures that your Pod has enough time to drain gracefully before being removed from the service. You can adjust this value if it's too long for your specific use case.\nAlternatively, you can estimate the timing by examining the deletion timestamp from the Pod and the timestamp when the endpoint was removed from the NEG in the Cloud Audit Logs.\n**Termination Grace Period parameter**\nYou must configure the `terminationGracePeriod` parameter to allow sufficient time for the `preStopHook` to finish and for the Pod to complete a graceful shutdown.\nBy default, when not explicitly set, the `terminationGracePeriod` is 30 seconds. You can calculate the optimal `terminationGracePeriod` using the formula:\n`terminationGracePeriod >= preStopHook Time + Pod shutdown time`\nTo define `terminationGracePeriod` within the Pod's configuration as follows:\n```\n\u00a0 spec:\u00a0 \u00a0 terminationGracePeriodSeconds: <terminationGracePeriod>\u00a0 \u00a0 containers:\u00a0 \u00a0 - name: my-app\u00a0 \u00a0 \u00a0 ...\u00a0 \u00a0 ...\n```\n### NEG not found when creating an Internal Ingress resource\nThe following error might occur when you create an internal Ingress in GKE:\n```\nError syncing: error running backend syncing routine: googleapi: Error 404: The resource 'projects/PROJECT_ID/zones/ZONE/networkEndpointGroups/NEG' was not found, notFound\n```\nThis error occurs because Ingress for internal Application Load Balancers requires Network Endpoint Groups (NEGs) as backends.\nIn Shared VPC environments or clusters with Network Policies enabled, add the annotation `cloud.google.com/neg: '{\"ingress\": true}'` to the Service manifest.\n### 504 Gateway Timeout: upstream request timeout\nThe following error might occur when you access a Service from an internal Ingress in GKE:\n```\nHTTP/1.1 504 Gateway Timeout\ncontent-length: 24\ncontent-type: text/plain\nupsteam request timeout\n```\nThis error occurs because traffic sent to internal Application Load Balancers are proxied by [envoy proxies](/load-balancing/docs/proxy-only-subnets) in the proxy-only subnet range.\nTo allow traffic from the proxy-only subnet range, [create a firewall rule](/kubernetes-engine/docs/how-to/internal-load-balance-ingress#create_a_firewall_rule) on the `targetPort` of the Service.\n### Error 400: Invalid value for field 'resource.target'\nThe following error might occur when you access a Service from an internal Ingress in GKE:\n```\nError syncing:LB_NAME does not exist: googleapi: Error 400: Invalid value for field 'resource.target': 'https://www.googleapis.com/compute/v1/projects/PROJECT_NAME/regions/REGION_NAME/targetHttpProxies/LB_NAME. A reserved and active subnetwork is required in the same region and VPC as the forwarding rule.\n```\nTo resolve this issue, create a [proxy-only subnet](/kubernetes-engine/docs/how-to/internal-load-balance-ingress#prepare-environment) .\n### Error during sync: error running load balancer syncing routine: loadbalancer does not exist\nOne of the following errors might occur when the GKE control plane upgrades or when you modify an Ingress object:\n```\n\"Error during sync: error running load balancer syncing routine: loadbalancer\nINGRESS_NAME does not exist: invalid ingress frontend configuration, please\ncheck your usage of the 'kubernetes.io/ingress.allow-http' annotation.\"\n```\nOr:\n```\nError during sync: error running load balancer syncing routine: loadbalancer LOAD_BALANCER_NAME does not exist:\ngoogleapi: Error 400: Invalid value for field 'resource.IPAddress':'INGRESS_VIP'. Specified IP address is in-use and would result in a conflict., invalid\n```\nTo resolve these issues, try the following steps:\n- Add the`hosts`field in the`tls`section of the Ingress manifest, then delete the Ingress. Wait five minutes for GKE to delete the unused Ingress resources. Then, recreate the Ingress. For more information, see [The hosts field of an Ingress object](/kubernetes-engine/docs/how-to/ingress-multi-ssl#the_hosts_field_of_an_ingress_object) .\n- Revert the changes you made to the Ingress. Then, add a certificate using an [annotation or Kubernetes Secret](/kubernetes-engine/docs/how-to/ingress-multi-ssl#specifying_certificates_for_your_ingress) .## External Ingress produces HTTP 502 errors\nUse the following guidance to troubleshoot HTTP 502 errors with external Ingress resources:\n- [Enable logs](/load-balancing/docs/https/https-logging-monitoring#enabling_logging_on_an_existing_backend_service) for each backend service associated with each GKE Service that is referenced by the Ingress.\n- Use [status details](/load-balancing/docs/https/https-logging-monitoring#failure-messages) to identify causes for HTTP 502 responses. Status details that indicate the HTTP 502 response originated from the backend require troubleshooting within the serving Pods, not the load balancer.\n### Unmanaged instance groups\nYou might experience HTTP 502 errors with external Ingress resources if your external Ingress uses unmanaged instance group backends. This issue occurs when **all** of the following conditions are met:\n- The cluster has a large total number of nodes among all node pools.\n- The serving Pods for one or more Services that are referenced by the Ingress are located on only a few nodes.\n- Services referenced by the Ingress use`externalTrafficPolicy: Local`.\nTo determine if your external Ingress uses unmanaged instance group backends, do the following:\n- Go to the **Ingress** page in the Google Cloud console. [Go to Ingress](https://console.cloud.google.com/kubernetes/ingresses) \n- Click the name of your external Ingress.\n- Click the name of the **Load balancer** . The **Load balancing details** page displays.\n- Check the table in the **Backend services** section to determine if your external Ingress uses NEGs or instance groups.\nTo resolve this issue, use one of the following solutions:\n- Use a VPC-native cluster.\n- Use`externalTrafficPolicy: Cluster`for each Service referenced by the external Ingress. This solution causes you to lose the original client IP address in the packet's sources.\n- Use the`node.kubernetes.io/exclude-from-external-load-balancers=true`annotation. Add the annotation to the nodes or node pools that do not run any serving Pod for any Service referenced by any external Ingress or`LoadBalancer`Service in your cluster.## Use load balancer logs to troubleshoot\nYou can use [internal passthrough Network Load Balancer logs](/load-balancing/docs/internal/internal-logging-monitoring#ilblb-logging) and [external passthrough Network Load Balancer logs](/load-balancing/docs/network/networklb-monitoring#netlb-logging) to troubleshoot issues with load balancers and correlate traffic from load balancers to GKE resources.\nLogs are aggregated per-connection and exported in near real time. Logs are generated for each GKE node involved in the data path of a LoadBalancer Service, for both ingress and egress traffic. Log entries include additional fields for GKE resources, such as: - Cluster name - Cluster location - Service name - Service namespace - Pod name - Pod namespace\n### Pricing\nThere are no additional charges for using logs. Based on how you ingest logs, standard pricing for [Cloud Logging](/stackdriver/pricing#google-clouds-operations-suite-pricing) , BigQuery, or Pub/Sub apply. Enabling logs has no effect on the performance of the load balancer.\n## Use diagnostic tools to troubleshoot\nThe `check-gke-ingress` diagnostic tool inspects Ingress resources for common misconfigurations. You can use the `check-gke-ingress` tool in the following ways:\n- Run the [gcpdiag command-line tool](https://gcpdiag.dev/docs/running/) on your cluster. Ingress results appear in the check rule`gke/ERR/2023_004`section.\n- Use the`check-gke-ingress`tool alone or as a kubectl plugin by following the instructions in [check-gke-ingress](https://github.com/kubernetes/ingress-gce/tree/master/cmd/check-gke-ingress) .## What's next\n[Cloud Customer Care](/kubernetes-engine/docs/getting-support)", "guide": "Google Kubernetes Engine (GKE)"}