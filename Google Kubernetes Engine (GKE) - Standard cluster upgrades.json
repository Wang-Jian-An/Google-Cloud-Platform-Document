{"title": "Google Kubernetes Engine (GKE) - Standard cluster upgrades", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview", "abstract": "# Google Kubernetes Engine (GKE) - Standard cluster upgrades\nThis page discusses how automatic and manual upgrades work on Google Kubernetes Engine (GKE) Standard clusters, including links to more information about related tasks and settings. You can use this information to keep your clusters updated for stability and security with minimal disruptions to your workloads.\nFor information on how cluster upgrades work for Autopilot, see [Autopilot cluster upgrades](/kubernetes-engine/docs/concepts/cluster-upgrades-autopilot) .\n", "content": "## How cluster and node pool upgrades work\nThis section discusses what happens in your cluster during automatic or manual upgrades. For auto-upgrades, GKE initiates the auto-upgrade. GKE observes automatic and manual upgrades across all GKE clusters, and intervenes if problems are observed.\nTo upgrade a cluster, GKE updates the version the control plane and nodes are running. Clusters are upgraded to either a newer minor version (for example, 1.24 to 1.25) or newer patch version (for example, 1.24.2-gke.100 to 1.24.5-gke.200). For more information, see [GKE versioning and support](/kubernetes-engine/versioning) .\n**Note:** A cluster's control plane and nodes do not necessarily run the same version at all times, however they must adhere to the [Kubernetes version skew support policy](https://kubernetes.io/docs/setup/release/version-skew-policy/) . In this topic [cluster upgrade](#cluster_upgrades) and are used interchangeably, and are differentiated from [node upgrades](#node_pool_upgrades) . To learn more about how versions work, see [Versioning](/kubernetes-engine/versioning) .\nIf you enroll your cluster in a [release channel](/kubernetes-engine/docs/concepts/release-channels) , nodes run the same version of GKE as the cluster, except during a brief period (typically a few days, depending on the current release) between completing the cluster's control plane upgrade and starting the node pool upgrade, or if the control plane was manually upgraded. Check the [release notes](/kubernetes-engine/docs/release-notes) for more information.\n### Cluster upgrades\nThis section discusses what to expect when GKE auto-upgrades your cluster or you [initiate a manualupgrade](/kubernetes-engine/docs/how-to/upgrading-a-cluster) .\n- **Zonalclusters** have only a single control plane. During the upgrade, your workloads continue to run, but you cannot deploy new workloads, modify existing workloads, or make other changes to the cluster's configuration until the upgrade is complete.\n- **Regionalclusters** have multiple replicas of the control plane, and only one replica is upgraded at a time, in an undefined order. During the upgrade, the cluster remains highly available, and each control plane replica is unavailable only while it is being upgraded.\n**Note:** Cluster control planes are always [upgraded](/kubernetes-engine/upgrades#automatic_cp_upgrades) on a regular basis, regardless of whether your cluster is enrolled in a [release channel](/kubernetes-engine/docs/concepts/release-channels) or not.\nIf you configure a [maintenance window or exclusion](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions) , it is honored if possible.\n### Node pool upgrades\nThis section discusses what to expect when GKE auto-upgrades your node pool or you initiate a manual node pool upgrade.\nGKE automatically upgrades one node pool at a time in a cluster. Alternatively, you can manually upgrade one or more node pools in parallel. By default, nodes within a node pool are upgraded one at a time in an arbitrary order. In a [node pool spread across multiplezones](/kubernetes-engine/docs/concepts/node-pools#multiple-zones) , upgrades take place zone-by-zone. Within a zone, the nodes will be upgraded in an undefined order.\nWith GKE node pool upgrades, you can choose between [twoconfigurable, built-in upgrade strategies](#node-pool-upgrade-strategies) where you can tune the upgrade process based on your cluster environment's needs. To learn more about surge and blue-green upgrade strategies, see [Upgrade strategies](#node-pool-upgrade-strategies) .\nDuring a node pool upgrade, you can't make changes to the cluster configuration unless you [cancel theupgrade](/kubernetes-engine/docs/how-to/upgrading-a-cluster#cancel) .\nGKE honors [maintenance windows andexclusions](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions) during automatic upgrades when possible. Manual upgrades bypass your configured maintenance windows and exclusions.\n**Note:** Cluster Autoscaler scale-up events may still occur during a node pool upgrade. For a multi-zone node pool, a node may be created running the older node version if the zone's nodes have not yet been upgraded.\nDuring a node pool upgrade, how the nodes are upgraded depends on the [node poolupgrade strategy](#node-pool-upgrade-strategies) and [how you configureit](/kubernetes-engine/docs/how-to/node-pool-upgrade-strategies) . However, the basic steps remain consistent. To upgrade a node, GKE removes Pods from the node so that it can be upgraded.\nWhen a node is upgraded, the following happens with the Pods:\n- The node is cordoned so that Kubernetes does not schedule new Pods on it.\n- The node is then drained, meaning that the Pods are removed. For surge upgrades, GKE respects the Pod's [PodDisruptionBudget](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) and [GracefulTerminationPeriod](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination) settings for up to one hour. With blue-green upgrades, this can be extended if you configure a longer soaking time.\n- The control plane reschedules Pods managed by controllers onto other nodes. Pods that cannot be rescheduled stay in the Pending phase until they can be rescheduled.\nThe node pool upgrade process may take up to a few hours depending on the upgrade strategy, the number of nodes, and their workload configurations.\nConfigurations that can cause a node upgrade to take longer to complete include:\n- A high value of [terminationGracePeriodSeconds](https://v1-25.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#podtemplatespec-v1-core) in a Pod's configuration.\n- A conservative [Pod Disruption Budget](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#how-disruption-budgets-work) .\n- [Node affinity](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity) interactions.\n- Attached [PersistentVolumes](/kubernetes-engine/docs/concepts/persistent-volumes) .GKE offers built-in configurable strategies which determine how the node pool is upgraded. To learn more about types of changes that use a node upgrade strategy, see [When GKE uses surgeupgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#when-surge-upgrades-are-used) and [When GKE uses blue-greenupgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#when-blue-green-upgrades-are-used) .\nBy default, the [surge upgrade strategy](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#surge) is used for node pool upgrades. Surge upgrades use a rolling method to upgrade nodes. This strategy is best for applications that can handle incremental, non-disruptive changes. With this strategy, nodes are upgraded in a rolling window. With the settings you can change how many nodes can be upgraded at once, and how disruptive the upgrades can be, finding the optimal balance of speed and disruption for your environment's needs.\nThe alternative approach is [blue-green upgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#blue-green-upgrade-strategy) , where two sets of environments (the original and new environments) are maintained at once, making rolling back as easy as possible. Blue-green is more resource intensive and better for applications that are more sensitive to changes. With this strategy, workloads are gradually migrated from the original \"blue\" environment to the new \"green\" environment, and given soak time to validate them with the new configuration. If needed, the workloads can be quickly rolled back to the existing \"blue\" environment.\nTo learn more about how the node upgrade strategies work, see [Node upgrade strategies](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies) .\nSurge upgrades create extra nodes if `maxSurge` is set to more than 0, and blue-green upgrades temporarily double the number of nodes in a node pool. This requires additional resources, which is subject to [Compute Engine quota](/compute/quotas) , [resourceavailability](/compute/resource-usage#quotas_and_resource_availability) , and [reservation](/kubernetes-engine/docs/how-to/consuming-reservations) capacity. If your node pool doesn't have sufficient resources, upgrades can take longer or fail.\nTo learn more about how to ensure your project has enough resources for node upgrades, and what to do if your environment is resource-constrained, see [Ensure resources for nodeupgrades](/kubernetes-engine/docs/how-to/node-upgrades-quota) .\n## Upgrading automatically\nWhen you create a Standard cluster, by default, auto-upgrade is enabled on the cluster and its node pools.\nGKE is responsible for [securing your cluster's controlplane](/kubernetes-engine/docs/concepts/control-plane-security) , and upgrades your clusters when a new GKE version is [selected forauto-upgrade](#auto-upgrade-version-selection) . Infrastructure security is high priority for GKE, and as such control planes are upgraded on a regular basis, and cannot be disabled. However, you can apply [maintenancewindows and exclusions](#maintenance-window) to temporarily suspend upgrades for control planes and nodes.\nAs part of the [GKE shared responsibility model](/kubernetes-engine/docs/concepts/shared-responsibility) , you are responsible for securing your nodes, containers, and Pods. Node auto-upgrade is enabled by default. Although it is , you can [disable node auto-upgrade](/kubernetes-engine/docs/how-to/node-auto-upgrades#disable) . Opting out of node auto-upgrades does not block your cluster's control plane upgrade. If you opt out of node auto-upgrades you are responsible for ensuring that the cluster's nodes run a version compatible with the cluster's version, and that the version adheres to the [Kubernetes version skew support policy](https://kubernetes.io/docs/setup/release/version-skew-policy/) .\nFor more control over when an auto-upgrade can occur (or must not occur), you can configure [maintenance windows and exclusions](#maintenance-window) .\nA cluster's node pools can be no more than two minor versions behind the control plane version, to maintain compatibility with the cluster API. The node pool version also determines the versions of software packages installed on each node. It is recommended to keep node pools updated to the cluster version.\nIf you enroll your cluster in a [release channel](/kubernetes-engine/docs/concepts/release-channels) , nodes always run the same version of GKE as the cluster itself, except during a brief period (typically a few days, depending on the current release)\u00a0between completing the cluster's control plane upgrade and beginning to upgrade a given node pool. Check the [release notes](/kubernetes-engine/docs/release-notes) for more information.\n### How versions are selected for auto-upgrade\nNew GKE versions are released [regularly](/kubernetes-engine/docs/release-notes) , but a version is not selected for auto-upgrade right away. When a GKE version has accumulated enough cluster usage to prove stability over time, GKE selects it as an auto-upgrade target for clusters running a subset of older versions.\nNew auto-upgrade targets are announced in the release notes. Until an available version is selected for auto-upgrade, you can [upgrade to it manually](#upgrading_manually) . Occasionally, a version is selected for cluster auto-upgrade and node auto-upgrade during different weeks.\nSoon after a new minor version becomes generally available, the oldest available minor version typically becomes unsupported. Clusters running minor versions that become unsupported are automatically upgraded to the next minor version.\nWithin a minor version (such as v1.14.x), clusters can be automatically upgraded to a new patch release.\n[Release channels](/kubernetes-engine/docs/concepts/release-channels) allow you to control your cluster and node pool version based on a version's stability rather than managing the version directly.\n**Note:** Node auto-upgrade is not available for [Alpha clusters](/kubernetes-engine/docs/concepts/alpha-clusters) . Also, alpha clusters cannot be enrolled in release channels.\nTo ensure the stability and reliability of clusters on new versions, GKE follows certain practices during version rollouts.\nThese practices include, but are not limited to:\n- GKE gradually rolls out changes across Google Cloud regions and zones.\n- GKE gradually rolls out [patch versions](/kubernetes-engine/versioning#versioning_scheme) across [release channels](/kubernetes-engine/docs/concepts/release-channels) . A patch is given soak time in the Rapid release channel, then the Regular release channel, before being promoted to the Stable release channel once it has accumulated usage and continued to demonstrate stability. If an issue is found with a patch version during the soaking time on a release channel, that version is not promoted to the next channel and the issue is fixed on a newer patch version.\n- GKE gradually rolls out [minor versions](/kubernetes-engine/versioning#versioning_scheme) , following a similar soaking process to patch versions. Minor versions have longer soaking periods as they introduce more significant changes.\n- GKE may delay automatic upgrades when a new version impacts a group of clusters. For example, [GKE pausesautomatic upgrades](/kubernetes-engine/docs/deprecations#auto-upgrade-pause) for clusters that it detects are exposed to a deprecated API or feature that will be removed in the next minor version.\n- GKE might delay the rollout of new versions during peak times (for example, major holidays) to ensure business continuity.\n### Configuring when auto-upgrades can occur\nBy default, auto-upgrades can occur at any time to preserve infrastructure security. Auto-upgrades are minimally disruptive, especially for regional clusters. However, some workloads may require finer-grained control. You can configure [maintenance windows and exclusions](/kubernetes-engine/docs/how-to/maintenance-windows-and-exclusions) to manage when auto-upgrades can and must not occur.\n**Note:** If you configure [maintenance windows and exclusions](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions) , the upgrade does not occur until the current time is within a maintenance window. If a maintenance window expires before the upgrade completes, an attempt is made to pause it. During the next occurrence maintenance window, an attempt is made to resume the upgrade.\n## Upgrading manually\nYou can request to [manually upgrade your cluster or its node pools](/kubernetes-engine/docs/how-to/upgrading-a-cluster) to an available and compatible version at any time. Manual upgrades bypass any configured maintenance windows and maintenance exclusions.\n**Note:** You cannot upgrade your cluster more than one minor version at a time. For example, you can upgrade a cluster from version 1.12.x to 1.13.x, but not directly from 1.11.x to 1.13.x. For more information, see [Versioning and upgrades](/kubernetes-engine/versioning-and-upgrades) .\nWhen you [manually upgrade a cluster](/kubernetes-engine/docs/how-to/upgrading-a-cluster) , its availability depends on whether the cluster is regional or not:\n- **For zonal clusters** , the control plane is unavailable while it is being upgraded. For the most part, workloads run normally but cannot be modified during the upgrade.\n- **For regional clusters** , one replica of the control plane is unavailable at a time while it is upgraded, but the cluster remains highly available during the upgrade.\nYou can [manually initiate a node upgrade](/kubernetes-engine/docs/how-to/node-auto-upgrades) to a version compatible with the control plane.\n## How GKE responds to auto-upgrade failure\nNode pool auto-upgrades can fail because of issues with the underlying Compute Engine instances, or because of issues with Kubernetes. For example, auto-upgrades fail in the following situations:\n- Your configured`maxSurge`setting exceeds your Compute Engine resource quota.\n- New surge nodes didn't register with the cluster control plane.\n- Nodes took too long to drain, or took too long to delete.\nWhen issues occur with individual node upgrades, GKE retries the upgrade a few times, with an increasing interval between retries. If nodes in the node pool fail to upgrade, GKE does not roll back the upgraded nodes. Instead, GKE tries the node pool auto-upgrade again until all the nodes are successfully upgraded.\nIf your node upgrades fail because your surge node requests exceed your Compute Engine quota, GKE reduces the number of concurrent surge nodes to attempt to meet the quota and continue the upgrade.\n**Note:** GKE maintains the existing node capacity during a surge upgrade, as long as `maxSurge` is more than `0` and `maxUnavailable=0` . Your workloads continue to run even when the node upgrade fails.\n## Receiving upgrade notifications\nGKE publishes notifications about events relevant to your cluster, such as version upgrades and security bulletins, to [Pub/Sub](/pubsub/docs/overview) , providing you with a channel to receive information from GKE about your clusters.\nFor more information, see [Receiving cluster notifications](/kubernetes-engine/docs/how-to/cluster-notifications) .\n## Check upgrade logs\nGKE logs control plane and node pool upgrade events to Cloud Logging by default. Upgrade events log provides visibility into the upgrade process, and includes valuable information for troubleshooting if needed.\n### Control plane upgrade logs\nCluster upgrade events can be queried using the following filter:\n```\nresource.type=\"gke_cluster\"\nprotoPayload.metadata.operationType=~\"(UPDATE_CLUSTER|UPGRADE_MASTER)\"\nresource.labels.cluster_name=\"CLUSTER_NAME\"\n```\nThese logs are recorded as structured logging formats. You can use the following fields for the details of the upgrade events:\n| Field          | Description                                                                                             |\n|:--------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| protoPayload.metadata.operationType   | There are two types of cluster upgrade events: UPGRADE_MASTER and UPDATE_CLUSTER. UPGRADE_MASTER changes the Kubernetes control plane version. UPDATE_CLUSTER means an update not changing the Kubernetes control plane version. Both cluster upgrade types can cause the loss of control plane availability for zonal clusters. To learn more, see How cluster and node pool upgrades work. |\n| protoPayload.methodName      | This field shows which API triggered the cluster upgrade. google.container.v1.ClusterManager.UpdateCluster: manual control plane upgrade google.container.internal.ClusterManagerInternal.UpdateClusterInternal: automatic control plane upgrade google.container.v1.ClusterManager.PatchCluster: cluster configuration change.                |\n| protoPayload.metadata.previousMasterVersion | This field is used only for the MASTER_UPGRADE operation type, and contains the previous control plane version used before the upgrade.                                                              |\n| protoPayload.metadata.currentMasterVersion | This field is used only for the MASTER_UPGRADE operation type, and contains the new control plane version number used after the upgrade.                                                              |\n### Node pool upgrade logs\nUse the following query to view node pool upgrade events:\n```\nresource.type=\"gke_nodepool\"\nprotoPayload.metadata.operationType=\"UPGRADE_NODES\"\nresource.labels.cluster_name=\"CLUSTER_NAME\"\n```\nUse the following field for details about the upgrade event:\n`protoPayload.methodName` field shows whether the upgrade was triggered manually or triggered automatically as follows.\n- `google.container.v1.ClusterManager.UpdateNodePool`: [manual node pool upgrade](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrade_nodes) \n- `google.container.internal.ClusterManagerInternal.UpdateClusterInternal`: [automatic node pool upgrade](/kubernetes-engine/docs/how-to/node-auto-upgrades) ## Component upgrades\nGKE runs system workloads on worker nodes to support specific capabilities for clusters. For example, the `gke-metadata-server` system workload supports [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) . GKE is [responsible](/kubernetes-engine/docs/concepts/shared-responsibility#googles_responsibilities) for the health of these workloads. To learn more about these components, refer to the documentation for the associated capabilities.\nWhen new features or fixes become available for a component, GKE indicates the patch version in which they are included. To obtain the latest version of a component, refer to the associated documentation or [releasenotes](/kubernetes-engine/docs/release-notes) for instructions on upgrading your control plane or nodes to the appropriate version.\n## What's next\n- Learn more [about cluster configuration choices](/kubernetes-engine/docs/concepts/types-of-clusters) .\n- Learn more about [upgrading a cluster or its nodes](/kubernetes-engine/docs/how-to/upgrading-a-cluster) .\n- Configure [maintenance windows and exclusions](/kubernetes-engine/docs/how-to/maintenance-windows-and-exclusions) .\n- Learn about managing automatic cluster upgrades across environments with [rollout sequencing](/kubernetes-engine/docs/concepts/about-rollout-sequencing) .\n- [Best practices for upgrading clusters](/kubernetes-engine/docs/best-practices/upgrading-clusters) .\n- Watch [GKE cluster upgrades: Best practices for GKE cluster stability, security, and performance](https://www.youtube.com/watch?v=RFp61C3YEXw)", "guide": "Google Kubernetes Engine (GKE)"}