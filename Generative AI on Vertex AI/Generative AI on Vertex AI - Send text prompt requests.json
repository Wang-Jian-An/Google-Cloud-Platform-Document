{"title": "Generative AI on Vertex AI - Send text prompt requests", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/text/test-text-prompts", "abstract": "# Generative AI on Vertex AI - Send text prompt requests\nVertex AI lets you test prompts by using Vertex AI Studio in the Google Cloud console, the Vertex AI API, and the Vertex AI SDK for Python. This page shows you how to test text prompts by using any of these interfaces.\nTo learn more about prompt design for text, see [Design text prompts](/vertex-ai/generative-ai/docs/text/text-prompts) .\n", "content": "## Test text prompts\nTo test text prompts, choose one of the following methods.\nTo test a text prompt by using the Vertex AI API, send a POST request to the publisher model endpoint.\nBefore using any of the request data, make the following replacements:- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : A prompt is a natural language request submitted to a language model to receive a response back. Prompts can contain questions, instructions, contextual information, examples, and text for the model to complete or continue. (Don't add quotes around the prompt here.)\n- : The temperature is used for sampling during response generation, which occurs when`topP`and`topK`are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of`0`means that the highest probability tokens are always selected. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.If the model returns a response that's too generic, too short, or the model gives a fallback response, try increasing the temperature.\n- : Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.Specify a lower value for shorter responses and a higher value for potentially longer responses.\n- : Top-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K) to least probable until the sum of their probabilities equals the top-P value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-P value is`0.5`, then the model will select either A or B as the next token by using temperature and excludes C as a candidate.Specify a lower value for less random responses and a higher value for more random responses.\n- : Top-K changes how the model selects tokens for output. A top-K of`1`means the next selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-K of`3`means that the next token is selected from among the three most probable tokens by using temperature.For each token selection step, the top-K tokens with the highest probabilities are sampled. Then tokens are further filtered based on top-P with the final token selected using temperature sampling.Specify a lower value for less random responses and a higher value for more random responses.\nHTTP method and URL:\n```\nPOST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/text-bison:predict\n```\nRequest JSON body:\n```\n{\n \"instances\": [ { \"prompt\": \"PROMPT\"}\n ],\n \"parameters\": {\n \"temperature\": TEMPERATURE,\n \"maxOutputTokens\": MAX_OUTPUT_TOKENS,\n \"topP\": TOP_P,\n \"topK\": TOP_K\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/text-bison:predict\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/text-bison:predict\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following.```\nMODEL_ID=\"text-bison\"PROJECT_ID=PROJECT_IDcurl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:predict -d \\$'{\u00a0 \"instances\": [\u00a0 \u00a0 { \"prompt\": \"Give me ten interview questions for the role of program manager.\" }\u00a0 ],\u00a0 \"parameters\": {\u00a0 \u00a0 \"temperature\": 0.2,\u00a0 \u00a0 \"maxOutputTokens\": 256,\u00a0 \u00a0 \"topK\": 40,\u00a0 \u00a0 \"topP\": 0.95\u00a0 }}'\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/ideation.py) \n```\nimport vertexaifrom vertexai.language_models import TextGenerationModeldef interview(\u00a0 \u00a0 temperature: float,\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,) -> str:\u00a0 \u00a0 \"\"\"Ideation example with a Large Language Model\"\"\"\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 # TODO developer - override these parameters as needed:\u00a0 \u00a0 parameters = {\u00a0 \u00a0 \u00a0 \u00a0 \"temperature\": temperature, \u00a0# Temperature controls the degree of randomness in token selection.\u00a0 \u00a0 \u00a0 \u00a0 \"max_output_tokens\": 256, \u00a0# Token limit determines the maximum amount of text output.\u00a0 \u00a0 \u00a0 \u00a0 \"top_p\": 0.8, \u00a0# Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\u00a0 \u00a0 \u00a0 \u00a0 \"top_k\": 40, \u00a0# A top_k of 1 means the selected token is the most probable among all tokens.\u00a0 \u00a0 }\u00a0 \u00a0 model = TextGenerationModel.from_pretrained(\"text-bison@002\")\u00a0 \u00a0 response = model.predict(\u00a0 \u00a0 \u00a0 \u00a0 \"Give me ten interview questions for the role of program manager.\",\u00a0 \u00a0 \u00a0 \u00a0 **parameters,\u00a0 \u00a0 )\u00a0 \u00a0 print(f\"Response from Model: {response.text}\")\u00a0 \u00a0 return response.text\n```Before trying this sample, follow the Go setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Go API reference documentation](/go/docs/reference/cloud.google.com/go/aiplatform/latest/apiv1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/aiplatform/text-predict/text.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 aiplatform \"cloud.google.com/go/aiplatform/apiv1beta1\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/aiplatform/apiv1beta1/aiplatformpb\"\u00a0 \u00a0 \u00a0 \u00a0 \"google.golang.org/api/option\"\u00a0 \u00a0 \u00a0 \u00a0 \"google.golang.org/protobuf/types/known/structpb\")// textPredict generates text from prompt and configurations provided.func textPredict(w io.Writer, prompt, projectID, location, publisher, model string, parameters map[string]interface{}) error {\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 apiEndpoint := fmt.Sprintf(\"%s-aiplatform.googleapis.com:443\", location)\u00a0 \u00a0 \u00a0 \u00a0 client, err := aiplatform.NewPredictionClient(ctx, option.WithEndpoint(apiEndpoint))\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"unable to create prediction client: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 // PredictRequest requires an endpoint, instances, and parameters\u00a0 \u00a0 \u00a0 \u00a0 // Endpoint\u00a0 \u00a0 \u00a0 \u00a0 base := fmt.Sprintf(\"projects/%s/locations/%s/publishers/%s/models\", projectID, location, publisher)\u00a0 \u00a0 \u00a0 \u00a0 url := fmt.Sprintf(\"%s/%s\", base, model)\u00a0 \u00a0 \u00a0 \u00a0 // Instances: the prompt to use with the text model\u00a0 \u00a0 \u00a0 \u00a0 promptValue, err := structpb.NewValue(map[string]interface{}{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"prompt\": prompt,\u00a0 \u00a0 \u00a0 \u00a0 })\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"unable to convert prompt to Value: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Parameters: the model configuration parameters\u00a0 \u00a0 \u00a0 \u00a0 parametersValue, err := structpb.NewValue(parameters)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"unable to convert parameters to Value: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // PredictRequest: create the model prediction request\u00a0 \u00a0 \u00a0 \u00a0 req := &aiplatformpb.PredictRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Endpoint: \u00a0 url,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Instances: \u00a0[]*structpb.Value{promptValue},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Parameters: parametersValue,\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // PredictResponse: receive the response from the model\u00a0 \u00a0 \u00a0 \u00a0 resp, err := client.Predict(ctx, req)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"error in prediction: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"text-prediction response: %v\", resp.Predictions[0])\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/PredictTextPromptSample.java) \n```\nimport com.google.cloud.aiplatform.v1beta1.EndpointName;import com.google.cloud.aiplatform.v1beta1.PredictResponse;import com.google.cloud.aiplatform.v1beta1.PredictionServiceClient;import com.google.cloud.aiplatform.v1beta1.PredictionServiceSettings;import com.google.protobuf.Value;import com.google.protobuf.util.JsonFormat;import java.io.IOException;import java.util.ArrayList;import java.util.List;public class PredictTextPromptSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 // Details of designing text prompts for supported large language models:\u00a0 \u00a0 // https://cloud.google.com/vertex-ai/docs/generative-ai/text/text-overview\u00a0 \u00a0 String instance =\u00a0 \u00a0 \u00a0 \u00a0 \"{ \\\"prompt\\\": \" + \"\\\"Give me ten interview questions for the role of program manager.\\\"}\";\u00a0 \u00a0 String parameters =\u00a0 \u00a0 \u00a0 \u00a0 \"{\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" \u00a0\\\"temperature\\\": 0.2,\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" \u00a0\\\"maxOutputTokens\\\": 256,\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" \u00a0\\\"topP\\\": 0.95,\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" \u00a0\\\"topK\\\": 40\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"}\";\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 String publisher = \"google\";\u00a0 \u00a0 String model = \"text-bison@001\";\u00a0 \u00a0 predictTextPrompt(instance, parameters, project, location, publisher, model);\u00a0 }\u00a0 // Get a text prompt from a supported text model\u00a0 public static void predictTextPrompt(\u00a0 \u00a0 \u00a0 String instance,\u00a0 \u00a0 \u00a0 String parameters,\u00a0 \u00a0 \u00a0 String project,\u00a0 \u00a0 \u00a0 String location,\u00a0 \u00a0 \u00a0 String publisher,\u00a0 \u00a0 \u00a0 String model)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 String endpoint = String.format(\"%s-aiplatform.googleapis.com:443\", location);\u00a0 \u00a0 PredictionServiceSettings predictionServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceSettings.newBuilder().setEndpoint(endpoint).build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests.\u00a0 \u00a0 try (PredictionServiceClient predictionServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceClient.create(predictionServiceSettings)) {\u00a0 \u00a0 \u00a0 final EndpointName endpointName =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 EndpointName.ofProjectLocationPublisherModelName(project, location, publisher, model);\u00a0 \u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 \u00a0 // once, and can be reused for multiple requests.\u00a0 \u00a0 \u00a0 Value.Builder instanceValue = Value.newBuilder();\u00a0 \u00a0 \u00a0 JsonFormat.parser().merge(instance, instanceValue);\u00a0 \u00a0 \u00a0 List<Value> instances = new ArrayList<>();\u00a0 \u00a0 \u00a0 instances.add(instanceValue.build());\u00a0 \u00a0 \u00a0 // Use Value.Builder to convert instance to a dynamically typed value that can be\u00a0 \u00a0 \u00a0 // processed by the service.\u00a0 \u00a0 \u00a0 Value.Builder parameterValueBuilder = Value.newBuilder();\u00a0 \u00a0 \u00a0 JsonFormat.parser().merge(parameters, parameterValueBuilder);\u00a0 \u00a0 \u00a0 Value parameterValue = parameterValueBuilder.build();\u00a0 \u00a0 \u00a0 PredictResponse predictResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 predictionServiceClient.predict(endpointName, instances, parameterValue);\u00a0 \u00a0 \u00a0 System.out.println(\"Predict Response\");\u00a0 \u00a0 \u00a0 System.out.println(predictResponse);\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/predict-text-prompt.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';const aiplatform = require('@google-cloud/aiplatform');// Imports the Google Cloud Prediction service clientconst {PredictionServiceClient} = aiplatform.v1;// Import the helper module for converting arbitrary protobuf.Value objects.const {helpers} = aiplatform;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};const publisher = 'google';const model = 'text-bison@001';// Instantiates a clientconst predictionServiceClient = new PredictionServiceClient(clientOptions);async function callPredict() {\u00a0 // Configure the parent resource\u00a0 const endpoint = `projects/${project}/locations/${location}/publishers/${publisher}/models/${model}`;\u00a0 const prompt = {\u00a0 \u00a0 prompt:\u00a0 \u00a0 \u00a0 'Give me ten interview questions for the role of program manager.',\u00a0 };\u00a0 const instanceValue = helpers.toValue(prompt);\u00a0 const instances = [instanceValue];\u00a0 const parameter = {\u00a0 \u00a0 temperature: 0.2,\u00a0 \u00a0 maxOutputTokens: 256,\u00a0 \u00a0 topP: 0.95,\u00a0 \u00a0 topK: 40,\u00a0 };\u00a0 const parameters = helpers.toValue(parameter);\u00a0 const request = {\u00a0 \u00a0 endpoint,\u00a0 \u00a0 instances,\u00a0 \u00a0 parameters,\u00a0 };\u00a0 // Predict request\u00a0 const response = await predictionServiceClient.predict(request);\u00a0 console.log('Get text prompt response');\u00a0 console.log(response);}callPredict();\n```Before trying this sample, follow the C# setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI C# API reference documentation](/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/dotnet-docs-samples/blob/HEAD/aiplatform/api/AIPlatform.Samples/PredictTextPrompt.cs) \n```\nusing Google.Cloud.AIPlatform.V1;using System;using System.Collections.Generic;using System.Linq;using Value = Google.Protobuf.WellKnownTypes.Value;public class PredictTextPromptSample{\u00a0 \u00a0 public string PredictTextPrompt(\u00a0 \u00a0 \u00a0 \u00a0 string projectId = \"your-project-id\",\u00a0 \u00a0 \u00a0 \u00a0 string locationId = \"us-central1\",\u00a0 \u00a0 \u00a0 \u00a0 string publisher = \"google\",\u00a0 \u00a0 \u00a0 \u00a0 string model = \"text-bison@001\"\u00a0 \u00a0 )\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 // Initialize client that will be used to send requests.\u00a0 \u00a0 \u00a0 \u00a0 // This client only needs to be created\u00a0 \u00a0 \u00a0 \u00a0 // once, and can be reused for multiple requests.\u00a0 \u00a0 \u00a0 \u00a0 var client = new PredictionServiceClientBuilder\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Endpoint = $\"{locationId}-aiplatform.googleapis.com\"\u00a0 \u00a0 \u00a0 \u00a0 }.Build();\u00a0 \u00a0 \u00a0 \u00a0 // Configure the parent resource\u00a0 \u00a0 \u00a0 \u00a0 var endpoint = EndpointName.FromProjectLocationPublisherModel(projectId, locationId, publisher, model);\u00a0 \u00a0 \u00a0 \u00a0 // Initialize request argument(s)\u00a0 \u00a0 \u00a0 \u00a0 var prompt = \"Give me ten interview questions for the role of program manager.\";\u00a0 \u00a0 \u00a0 \u00a0 var instanceValue = Value.ForStruct(new()\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Fields =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [\"prompt\"] = Value.ForString(prompt)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 });\u00a0 \u00a0 \u00a0 \u00a0 var instances = new List<Value>\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 instanceValue\u00a0 \u00a0 \u00a0 \u00a0 };\u00a0 \u00a0 \u00a0 \u00a0 var parameters = Value.ForStruct(new()\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Fields =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"temperature\", new Value { NumberValue = 0.2 } },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"maxOutputTokens\", new Value { NumberValue = 256 } },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"topP\", new Value { NumberValue = 0.95 } },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 { \"topK\", new Value { NumberValue = 40 } }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 });\u00a0 \u00a0 \u00a0 \u00a0 // Make the request\u00a0 \u00a0 \u00a0 \u00a0 var response = client.Predict(endpoint, instances, parameters);\u00a0 \u00a0 \u00a0 \u00a0 // Parse and return the content.\u00a0 \u00a0 \u00a0 \u00a0 var content = response.Predictions.First().StructValue.Fields[\"content\"].StringValue;\u00a0 \u00a0 \u00a0 \u00a0 Console.WriteLine($\"Content: {content}\");\u00a0 \u00a0 \u00a0 \u00a0 return content;\u00a0 \u00a0 }}\n```Before trying this sample, follow the Ruby setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Ruby API reference documentation](/ruby/docs/reference/google-cloud-ai_platform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/googleapis/google-cloud-ruby/blob/HEAD/google-cloud-ai_platform/samples/predict_text_prompt.rb) \n```\nrequire \"google/cloud/ai_platform/v1\"\n### Vertex AI Predict Text Prompt\n## @param project_id [String] Your Google Cloud project (e.g. \"my-project\")# @param location_id [String] Your Processor Location (e.g. \"us-central1\")# @param publisher [String] The Model Publisher (e.g. \"google\")# @param model [String] The Model Identifier (e.g. \"text-bison@001\")#def predict_text_prompt project_id:, location_id:, publisher:, model:\u00a0 # Create the Vertex AI client.\u00a0 client = ::Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\u00a0 \u00a0 config.endpoint = \"#{location_id}-aiplatform.googleapis.com\"\u00a0 end\u00a0 # Build the resource name from the project.\u00a0 endpoint = client.endpoint_path(\u00a0 \u00a0 project: project_id,\u00a0 \u00a0 location: location_id,\u00a0 \u00a0 publisher: publisher,\u00a0 \u00a0 model: model\u00a0 )\u00a0 prompt = \"Give me ten interview questions for the role of program manager.\"\u00a0 # Initialize the request arguments\u00a0 instance = Google::Protobuf::Value.new(\u00a0 \u00a0 struct_value: Google::Protobuf::Struct.new(\u00a0 \u00a0 \u00a0 fields: {\u00a0 \u00a0 \u00a0 \u00a0 \"prompt\" => Google::Protobuf::Value.new(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 string_value: prompt\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 )\u00a0 )\u00a0 instances = [instance]\u00a0 parameters = Google::Protobuf::Value.new(\u00a0 \u00a0 struct_value: Google::Protobuf::Struct.new(\u00a0 \u00a0 \u00a0 fields: {\u00a0 \u00a0 \u00a0 \u00a0 \"temperature\" => Google::Protobuf::Value.new(number_value: 0.2),\u00a0 \u00a0 \u00a0 \u00a0 \"maxOutputTokens\" => Google::Protobuf::Value.new(number_value: 256),\u00a0 \u00a0 \u00a0 \u00a0 \"topP\" => Google::Protobuf::Value.new(number_value: 0.95),\u00a0 \u00a0 \u00a0 \u00a0 \"topK\" => Google::Protobuf::Value.new(number_value: 40)\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 )\u00a0 )\u00a0 # Make the prediction request\u00a0 response = client.predict endpoint: endpoint, instances: instances, parameters: parameters\u00a0 # Handle the prediction response\u00a0 puts \"Predict Response\"\u00a0 puts responseend\n```To test a text prompt by using Vertex AI Studio in the Google Cloud console, perform the following steps:- In the Vertex AI section of the Google Cloud console, go to  the **Vertex AI Studio** page. [Go to Vertex AI Studio](https://console.cloud.google.com/vertex-ai/generative/language) \n- Click the **Get started** tab.\n- Clickadd **Text prompt** .\n- Select the method for inputting your prompt:- **Freeform** is recommended for zero-shot prompts or copy-pasting  few-shot prompts.\n- **Structured** is recommended for designing few-shot prompts in  Vertex AI Studio.\nEnter your prompt in the **Prompt** text field.The structured method for inputting prompts separates the components   of a prompt into different fields:- **Context** : Enter instructions for the task that you want the   model to perform and include any contextual information for the model   to reference.\n- **Examples** : For few-shot prompts, add input-output examples that   that exhibit the behavioral patterns for the model to imitate. Adding   a prefix for example input and output is optional. If you choose to   add prefixes, they should be consistent across all examples.\n- **Test** : In the **Input** field, enter the input of the prompt   that you want to get a response for. Adding a prefix for the test   input and output is optional. If your examples have prefixes, the test   should have the same prefixes.\n- Configure the model and parameters:- **Model** : Select a`text-bison`or`gemini-1.0-pro`model.\n- **Temperature** : Use the slider or textbox to enter a value for  temperature.The temperature is used for sampling during response generation, which occurs when`topP`and`topK`are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of`0`means that the highest probability tokens are always selected. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.If the model returns a response that's too generic, too short, or the model gives a fallback response, try increasing the temperature.\n- **Token limit** : Use the slider or textbox to enter a value for the  max output limit.Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.Specify a lower value for shorter responses and a higher value for potentially longer responses.\n- **Top-K** : Use the slider or textbox to enter a value for top-K.Top-K changes how the model selects tokens for output. A top-K of`1`means the next selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-K of`3`means that the next token is selected from among the three most probable tokens by using temperature.For each token selection step, the top-K tokens with the highest probabilities are sampled. Then tokens are further filtered based on top-P with the final token selected using temperature sampling.Specify a lower value for less random responses and a higher value for more random responses.\n- **Top-P** : Use the slider or textbox to enter a value for top-P.  Tokens are selected from most probable to the least until the sum of their  probabilities equals the value of top-P. For the least variable results,  set top-P to`0`.\n- Click **Submit** .\n- Optional: To save your prompt to **My prompts** , clicksave_alt **Save** .\n- Optional: To get the Python code or a curl command for your prompt, clickcode **View code** .\n## Stream response from text model\nTo view sample code requests and responses using the REST API, see [Examples using the REST API](/vertex-ai/generative-ai/docs/learn/streaming#rest) .\nTo view sample code requests and responses using the Vertex AI SDK for Python, see [Examples using Vertex AI SDK for Python](/vertex-ai/generative-ai/docs/learn/streaming#sdk) .\n## What's next\n- Learn how to [send Gemini chat prompt requests](/vertex-ai/generative-ai/docs/multimodal/send-chat-prompts-gemini) .\n- Learn how to [test chat prompts](/vertex-ai/generative-ai/docs/chat/test-chat-prompts) .\n- Learn how to [tune a foundation model](/vertex-ai/generative-ai/docs/models/tune-models) .\n- Learn about [responsible AI best practices and Vertex AI's safety filters](/vertex-ai/generative-ai/docs/learn/responsible-ai) .", "guide": "Generative AI on Vertex AI"}