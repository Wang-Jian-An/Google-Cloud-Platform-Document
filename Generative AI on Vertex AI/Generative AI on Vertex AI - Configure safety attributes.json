{"title": "Generative AI on Vertex AI - Configure safety attributes", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-attributes", "abstract": "# Generative AI on Vertex AI - Configure safety attributes\n", "content": "## Safety attribute confidence scoring\nContent processed through the Vertex AI Gemini API is assessed against a list of safety attributes, which include \"harmful categories\" and topics that can be considered sensitive. Those safety attributes are denoted in the following table:\n## Safety attribute definitions\n| Safety Attribute | Definition                   |\n|:-------------------|:-------------------------------------------------------------------------------------|\n| Hate Speech  | Negative or harmful comments targeting identity and/or protected attributes.   |\n| Harassment   | Malicious, intimidating, bullying, or abusive comments targeting another individual. |\n| Sexually Explicit | Contains references to sexual acts or other lewd content.       |\n| Dangerous Content | Promotes or enables access to harmful goods, services, and activities.    |\n## Safety attribute probabilities\nEach safety attribute has an associated confidence score between 0.0 and 1.0, rounded to one decimal place, which reflects the likelihood of the input or response belonging to a given category.\nThe confidence score in the following table is returned with a safety-confidence level:\n| Probability | Description           |\n|:--------------|:------------------------------------------------------|\n| NEGLIGIBLE | Content has a negligible probability of being unsafe. |\n| LOW   | Content has a low probability of being unsafe.  |\n| MEDIUM  | Content has a medium probability of being unsafe.  |\n| HIGH   | Content has a high probability of being unsafe.  |\n## Safety attribute severity\nEach of the four safety attributes is assigned a safety rating (severity level) and a severity score ranging from 0.0 to 1.0, rounded to one decimal place. The ratings and scores in the following table reflect the predicted severity of the content belonging to a given category:\n| Severity | Description                  |\n|:-----------|:----------------------------------------------------------------------------------|\n| NEGLIGIBLE | Content severity is predicted as negligible in respect to Google's safety policy. |\n| LOW  | Content severity is predicted as low in respect to Google's safety policy.  |\n| MEDIUM  | Content severity is predicted as medium in respect to Google's safety policy.  |\n| HIGH  | Content severity is predicted as high in respect to Google's safety policy.  |\n## Safety settings\nSafety settings are part of the request you send to the API service. It can be adjusted for each request you make to the API. The following table describes the block settings you can adjust for each category. For example, if you set the block setting to **Block few** for the **Dangerous Content category** , everything that has a high probability of being dangerous content is blocked. But anything with a lower probability is allowed. If not set, the default block setting is **Block some** .\n| Threshold (Studio) | Threshold (API)     | Threshold (Description)         |\n|:---------------------|:---------------------------------|:---------------------------------------------------------|\n| nan     | BLOCK_NONE (Restricted)   | Always show regardless of probability of unsafe content. |\n| Block few   | BLOCK_ONLY_HIGH     | Block when high probability of unsafe content.   |\n| Block some (Default) | BLOCK_MEDIUM_AND_ABOVE (Default) | Block when medium or high probability of unsafe content. |\n| Block most   | BLOCK_LOW_AND_ABOVE    | Block when medium or high probability of unsafe content. |\n| nan     | HARM_BLOCK_THRESHOLD_UNSPECIFIED | Threshold is unspecified, block using default threshold. |\nYou can change these settings for each request that you make to the text service. See the [HarmBlockThreshold API reference](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.types.SafetySetting.HarmBlockThreshold) for details.\n**Note:** The \"BLOCK_NONE\" setting is a restricted field that is not available to all users in Generally Available model versions. Refer to the remainder of this document for more guidance.\n## How to remove automated response blocking for select safety attributes\nThe \"BLOCK_NONE\" safety setting removes automated response blocking (for the safety attributes described under Safety Settings) and allow you to configure your own safety guidelines with the scores that are returned. In order to access the \"BLOCK_NONE\" setting, you have two options:\n(1) You might apply for the allowlist through the [Gemini safety filter allowlist form](https://docs.google.com/forms/d/e/1FAIpQLSeeaIrARA2Hdcri4upSNpS-OHnBEgzavVUpDhcVdWC_Qku_KQ/viewform?usp=header_link) , or\n(2) You might switch your account type to monthly invoiced billing with [the GCP invoiced billing reference](https://cloud.google.com/billing/docs/how-to/invoiced-billing) .\n## Key differences between Gemini and other model families\nWhile the same safety classifiers are applied to Gemini and PaLM, the number of safety attributes returned in the API might vary across different model families. The blocking logic (ie. confidence threshold) is based on rigorous evaluation against each model. Therefore, a safety setting that is applied to one model may not perfectly match the behavior of a safety setting applied to a different model. If this is a concern, we recommend that you configure your own blocking logic with raw severity scores and raw confidence scores, applying the same scoring thresholds across models.\n## Configure thresholds\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/gemini_safety_config_example.py) \n```\nfrom vertexai import generative_modelsdef generate_text(project_id: str, location: str, image: str) -> str:\u00a0 \u00a0 # Initialize Vertex AI\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 # Load the model\u00a0 \u00a0 model = generative_models.GenerativeModel(\"gemini-1.0-pro-vision\")\u00a0 \u00a0 # Generation config\u00a0 \u00a0 config = {\"max_output_tokens\": 2048, \"temperature\": 0.4, \"top_p\": 1, \"top_k\": 32}\u00a0 \u00a0 # Safety config\u00a0 \u00a0 safety_config = {\u00a0 \u00a0 \u00a0 \u00a0 generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\u00a0 \u00a0 \u00a0 \u00a0 generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\u00a0 \u00a0 }\u00a0 \u00a0 # Generate content\u00a0 \u00a0 responses = model.generate_content(\u00a0 \u00a0 \u00a0 \u00a0 [image, \"Add your prompt here\"],\u00a0 \u00a0 \u00a0 \u00a0 generation_config=config,\u00a0 \u00a0 \u00a0 \u00a0 stream=True,\u00a0 \u00a0 \u00a0 \u00a0 safety_settings=safety_config,\u00a0 \u00a0 )\u00a0 \u00a0 text_responses = []\u00a0 \u00a0 for response in responses:\u00a0 \u00a0 \u00a0 \u00a0 print(response.text)\u00a0 \u00a0 \u00a0 \u00a0 text_responses.append(response.text)\u00a0 \u00a0 return \"\".join(text_responses)\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/generative-ai/snippets/safetySettings.js) \n```\nconst {\u00a0 VertexAI,\u00a0 HarmCategory,\u00a0 HarmBlockThreshold,} = require('@google-cloud/vertexai');/**\u00a0* TODO(developer): Update these variables before running the sample.\u00a0*/async function setSafetySettings(\u00a0 projectId = 'PROJECT_ID',\u00a0 location = 'us-central1',\u00a0 model = 'gemini-1.0-pro') {\u00a0 // Initialize Vertex with your Cloud project and location\u00a0 const vertexAI = new VertexAI({project: projectId, location: location});\u00a0 // Instantiate the model\u00a0 const generativeModel = vertexAI.getGenerativeModel({\u00a0 \u00a0 model: model,\u00a0 \u00a0 // The following parameters are optional\u00a0 \u00a0 // They can also be passed to individual content generation requests\u00a0 \u00a0 safety_settings: [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\u00a0 \u00a0 \u00a0 \u00a0 threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ],\u00a0 \u00a0 generation_config: {max_output_tokens: 256},\u00a0 });\u00a0 const request = {\u00a0 \u00a0 contents: [{role: 'user', parts: [{text: 'Tell me something dangerous.'}]}],\u00a0 };\u00a0 console.log('Prompt:');\u00a0 console.log(request.contents[0].parts[0].text);\u00a0 console.log('Streaming Response Text:');\u00a0 // Create the response stream\u00a0 const responseStream = await generativeModel.generateContentStream(request);\u00a0 // Log the text response as it streams\u00a0 for await (const item of responseStream.stream) {\u00a0 \u00a0 if (item.candidates[0].finishReason === 'SAFETY') {\u00a0 \u00a0 \u00a0 console.log('This response stream terminated due to safety concerns.');\u00a0 \u00a0 \u00a0 break;\u00a0 \u00a0 } else {\u00a0 \u00a0 \u00a0 process.stdout.write(item.candidates[0].content.parts[0].text);\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/vertexai/snippets/src/main/java/vertexai/gemini/WithSafetySettings.java) \n```\nimport com.google.cloud.vertexai.VertexAI;import com.google.cloud.vertexai.api.Candidate;import com.google.cloud.vertexai.api.GenerateContentResponse;import com.google.cloud.vertexai.api.GenerationConfig;import com.google.cloud.vertexai.api.HarmCategory;import com.google.cloud.vertexai.api.SafetySetting;import com.google.cloud.vertexai.generativeai.GenerativeModel;import java.util.Arrays;import java.util.List;public class WithSafetySettings {\u00a0 public static void main(String[] args) throws Exception {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String projectId = \"your-google-cloud-project-id\";\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 String modelName = \"gemini-1.0-pro-vision\";\u00a0 \u00a0 String textPrompt = \"your-text-here\";\u00a0 \u00a0 String output = safetyCheck(projectId, location, modelName, textPrompt);\u00a0 \u00a0 System.out.println(output);\u00a0 }\u00a0 // Use safety settings to avoid harmful questions and content generation.\u00a0 public static String safetyCheck(String projectId, String location, String modelName,\u00a0 \u00a0 \u00a0 String textPrompt) throws Exception {\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs\u00a0 \u00a0 // to be created once, and can be reused for multiple requests.\u00a0 \u00a0 try (VertexAI vertexAI = new VertexAI(projectId, location)) {\u00a0 \u00a0 \u00a0 StringBuilder output = new StringBuilder();\u00a0 \u00a0 \u00a0 GenerationConfig generationConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 GenerationConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMaxOutputTokens(2048)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTemperature(0.4F)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTopK(32)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTopP(1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 GenerativeModel model = new GenerativeModel(modelName, generationConfig, vertexAI);\u00a0 \u00a0 \u00a0 List<SafetySetting> safetySettings = Arrays.asList(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 SafetySetting.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setCategory(HarmCategory.HARM_CATEGORY_HATE_SPEECH)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setThreshold(SafetySetting.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 SafetySetting.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setCategory(HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setThreshold(SafetySetting.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build()\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 GenerateContentResponse response = model.generateContent(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 textPrompt,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 safetySettings\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 output.append(response).append(\"\\n\");\u00a0 \u00a0 \u00a0 // Verifies if the above content has been blocked for safety reasons.\u00a0 \u00a0 \u00a0 boolean blockedForSafetyReason = response.getCandidatesList()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .stream()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .anyMatch(candidate -> candidate.getFinishReason() == Candidate.FinishReason.SAFETY);\u00a0 \u00a0 \u00a0 output.append(\"Blocked for safety reasons?: \").append(blockedForSafetyReason);\u00a0 \u00a0 \u00a0 return output.toString();\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Go setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Go API reference documentation](/go/docs/reference/cloud.google.com/go/aiplatform/latest/apiv1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/vertexai/safety-settings-multimodal/safety-settings-multimodal.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 \"mime\"\u00a0 \u00a0 \u00a0 \u00a0 \"path/filepath\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/vertexai/genai\")// generateMultimodalContent generates a response into w, based upon the prompt// and image provided.func generateMultimodalContent(w io.Writer, prompt, image, projectID, location, modelName string) error {\u00a0 \u00a0 \u00a0 \u00a0 // prompt := \"describe this image.\"\u00a0 \u00a0 \u00a0 \u00a0 // location := \"us-central1\"\u00a0 \u00a0 \u00a0 \u00a0 // model := \"gemini-1.0-pro-vision\"\u00a0 \u00a0 \u00a0 \u00a0 // image := \"gs://cloud-samples-data/generative-ai/image/320px-Felis_catus-cat_on_snow.jpg\"\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 client, err := genai.NewClient(ctx, projectID, location)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"unable to create client: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 model := client.GenerativeModel(modelName)\u00a0 \u00a0 \u00a0 \u00a0 model.SetTemperature(0.4)\u00a0 \u00a0 \u00a0 \u00a0 // configure the safety settings thresholds\u00a0 \u00a0 \u00a0 \u00a0 model.SafetySettings = []*genai.SafetySetting{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Category: \u00a0genai.HarmCategoryHarassment,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Threshold: genai.HarmBlockLowAndAbove,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Category: \u00a0genai.HarmCategoryDangerousContent,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Threshold: genai.HarmBlockLowAndAbove,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Given an image file URL, prepare image file as genai.Part\u00a0 \u00a0 \u00a0 \u00a0 img := genai.FileData{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MIMEType: mime.TypeByExtension(filepath.Ext(image)),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 FileURI: \u00a0image,\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 res, err := model.GenerateContent(ctx, img, genai.Text(prompt))\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"unable to generate contents: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"generated response: %s\\n\", res.Candidates[0].Content.Parts[0])\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```Before trying this sample, follow the C# setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI C# API reference documentation](/dotnet/docs/reference/Google.Cloud.AIPlatform.V1/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/dotnet-docs-samples/blob/HEAD/aiplatform/api/AIPlatform.Samples/WithSafetySettings.cs) \n```\nusing Google.Api.Gax.Grpc;using Google.Cloud.AIPlatform.V1;using System.Collections.Generic;using System.Text;using System.Threading.Tasks;using static Google.Cloud.AIPlatform.V1.SafetySetting.Types;public class WithSafetySettings{\u00a0 \u00a0 public async Task<string> GenerateContent(\u00a0 \u00a0 \u00a0 \u00a0 string projectId = \"your-project-id\",\u00a0 \u00a0 \u00a0 \u00a0 string location = \"us-central1\",\u00a0 \u00a0 \u00a0 \u00a0 string publisher = \"google\",\u00a0 \u00a0 \u00a0 \u00a0 string model = \"gemini-1.0-pro-vision\"\u00a0 \u00a0 )\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 // Create client\u00a0 \u00a0 \u00a0 \u00a0 var predictionServiceClient = new PredictionServiceClientBuilder\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Endpoint = $\"{location}-aiplatform.googleapis.com\"\u00a0 \u00a0 \u00a0 \u00a0 }.Build();\u00a0 \u00a0 \u00a0 \u00a0 // Prompt\u00a0 \u00a0 \u00a0 \u00a0 string prompt = \"Hello!\";\u00a0 \u00a0 \u00a0 \u00a0 // Initialize request argument(s)\u00a0 \u00a0 \u00a0 \u00a0 var content = new Content\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Role = \"USER\"\u00a0 \u00a0 \u00a0 \u00a0 };\u00a0 \u00a0 \u00a0 \u00a0 content.Parts.AddRange(new List<Part>()\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Text = prompt\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 });\u00a0 \u00a0 \u00a0 \u00a0 var safetySettings = new List<SafetySetting>()\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Category = HarmCategory.HateSpeech,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Threshold = HarmBlockThreshold.BlockLowAndAbove\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Category = HarmCategory.DangerousContent,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Threshold = HarmBlockThreshold.BlockMediumAndAbove\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 };\u00a0 \u00a0 \u00a0 \u00a0 var generateContentRequest = new GenerateContentRequest\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Model = $\"projects/{projectId}/locations/{location}/publishers/{publisher}/models/{model}\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 GenerationConfig = new GenerationConfig\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Temperature = 0.4f,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TopP = 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TopK = 32,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MaxOutputTokens = 2048\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 };\u00a0 \u00a0 \u00a0 \u00a0 generateContentRequest.Contents.Add(content);\u00a0 \u00a0 \u00a0 \u00a0 generateContentRequest.SafetySettings.AddRange(safetySettings);\u00a0 \u00a0 \u00a0 \u00a0 // Make the request, returning a streaming response\u00a0 \u00a0 \u00a0 \u00a0 using PredictionServiceClient.StreamGenerateContentStream response = predictionServiceClient.StreamGenerateContent(generateContentRequest);\u00a0 \u00a0 \u00a0 \u00a0 StringBuilder fullText = new();\u00a0 \u00a0 \u00a0 \u00a0 // Read streaming responses from server until complete\u00a0 \u00a0 \u00a0 \u00a0 AsyncResponseStream<GenerateContentResponse> responseStream = response.GetResponseStream();\u00a0 \u00a0 \u00a0 \u00a0 await foreach (GenerateContentResponse responseItem in responseStream)\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Check if the content has been blocked for safety reasons.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 bool blockForSafetyReason = responseItem.Candidates[0].FinishReason == Candidate.Types.FinishReason.Safety;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if (blockForSafetyReason)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fullText.Append(\"Blocked for safety reasons\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 else\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fullText.Append(responseItem.Candidates[0].Content.Parts[0].Text);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 return fullText.ToString();\u00a0 \u00a0 }}\n```Before using any of the request data, make the following replacements:- : The region to process the request. Available options include the following:\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The model ID of the multimodal model  that you want to use. The options are:- `gemini-1.0-pro`\n- `gemini-1.0-pro-vision`\n- : The role in a conversation associated with the content. Specifying a role is required even in singleturn use cases. Acceptable values include the following:- `USER`: Specifies content that's sent by you.\n- `MODEL`: Specifies the model's response.- : The text instructions to include in the prompt.\n- : The safety category to configure a threshold for. Acceptable values include the following:\n- : The threshold for blocking responses that could belong to the specified safety category based on probability. Acceptable values include the following:`BLOCK_LOW_AND_ABOVE`blocks the most while`BLOCK_ONLY_HIGH`blocks the least.\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:streamGenerateContent\n```\nRequest JSON body:\n```\n{\n \"contents\": {\n \"role\": \"ROLE\",\n \"parts\": { \"text\": \"TEXT\" }\n },\n \"safety_settings\": {\n \"category\": \"SAFETY_CATEGORY\",\n \"threshold\": \"THRESHOLD\"\n },\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:streamGenerateContent\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:streamGenerateContent\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following.```\nLOCATION=\"us-central1\"MODEL_ID=\"gemini-1.0-pro\"PROJECT_ID=\"test-project\"curl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:streamGenerateContent -d \\$'{\u00a0 \"contents\": {\u00a0 \u00a0 \"role\": \"user\",\u00a0 \u00a0 \"parts\": { \"text\": \"Hello!\" }\u00a0 },\u00a0 \"safety_settings\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\u00a0 \u00a0 \u00a0 \"threshold\": \"BLOCK_NONE\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\u00a0 \u00a0 \u00a0 \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"category\": \"HARM_CATEGORY_HARASSMENT\",\u00a0 \u00a0 \u00a0 \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\u00a0 \u00a0 \u00a0 \"threshold\": \"BLOCK_ONLY_HIGH\"\u00a0 \u00a0 }\u00a0 ]}'\n```\n- In the Vertex AI section of the Google Cloud console, go to the **Vertex AI Studio** page. [Go to Vertex AI Studio](https://console.cloud.google.com/vertex-ai/generative/language) \n- Under **Create a new prompt** , click any of the buttons to open the prompt design page.\n- Click **Safety settings** .The **Safety settings** dialog window opens.\n- For each safety attribute, configure the desired threshold value.\n- Click **Save** .\n## What's next\n- Learn more about [responsible AI](/vertex-ai/generative-ai/docs/learn/responsible-ai) .\n- Learn about [data governance](/vertex-ai/generative-ai/docs/data-governance) .", "guide": "Generative AI on Vertex AI"}