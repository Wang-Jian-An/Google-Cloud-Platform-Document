{"title": "Generative AI on Vertex AI - RLHF model tuning", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/tuning/rlhf-tuning", "abstract": "# Generative AI on Vertex AI - RLHF model tuning\n**Preview** Reinforcement Learning from Human Feedback (RLHF) is a Preview offering, subject to the Pre-GA Offerings Terms of the [GCP Service Specific Terms](https://cloud.google.com/terms/service-terms) . Pre-GA products and features may have limited support, and changes to pre-GA products and features may not be compatible with other pre-GA versions. For more information, see the [launch stage descriptions](https://cloud.google.com/products#product-launch-stages) . Further, by using these features, you agree to the Generative AI Preview [ terms and conditions ](https://cloud.google.com/trustedtester/aitos) (Preview Terms).For these features, you can process personal data as outlined in the Cloud Data Processing Addendum, subject to applicable restrictions and obligations in the Agreement (as defined in the Preview Terms).\nReinforcement learning from human feedback (RLHF) uses preferences specified by humans to optimize a language model. By using human feedback to tune your models, you can make the models better align with human preferences and reduce undesired outcomes in scenarios where people have complex intuitions about a task. For example, RLHF can help with an ambiguous task, such as how to write a poem about the ocean, by offering a human two poems about the ocean and letting that person choose their preferred one.\n", "content": "## Models that support RLHF tuning\nThe following text models support RLHF tuning:\n- The text generation foundation model,`text-bison@002`. For more information, see [Text generation model](/vertex-ai/generative-ai/docs/model-reference/text) .\n- The chat generation foundation model,`chat-bison@002`. For more information, see [Chat generation model](/vertex-ai/generative-ai/docs/model-reference/text-chat) .\n- The`t5-small`,`t5-large`,`t5-xl`, and`t5-xxl`Flan text-to-text transfer transformer (Flan-T5) models. Flan-T5 models can be fine-tuned to perform tasks such as text classification, language translation, and question answering. For more information, see [Flan-T5 checkpoints](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) .\nCode models don't support RLHF tuning.\n## Workflow for RLHF model tuning\nThe RLHF model tuning workflow on Vertex AI includes the following steps:\n- Prepare your human preference dataset.\n- Prepare your prompt dataset.\n- Upload your datasets to Cloud Storage bucket. They don't need to be in the same Cloud Storage bucket.\n- Create a RLHF model tuning job.\nAfter model tuning completes, the tuned model is deployed to a Vertex AI endpoint. The name of the endpoint is the same as the name of the tuned model. Tuned models are available to select in Vertex AI Studio when you want to create a new prompt.\nTo learn about tuning a text model by using RLHF tuning, see [Tune text models by using RLHF tuning](/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf) .\n## RLHF tuning region settings\nRLHF tuning supports the following two regions:\n- `us-central1`- If you choose this region, then 8 Nvidia A100 80GB GPUs are used.\n- `europe-west4`- If you choose this region, then 64 cores of the TPU v3 pod are used.\nThe region you choose is where Vertex AI tunes the model and then uploads the tuned model.\n## What's next\n- Learn about [supervised tuning](/vertex-ai/generative-ai/docs/tuning/supervised-tuning) .\n- Learn about [model distillation](/vertex-ai/generative-ai/docs/tuning/model-distillation) .", "guide": "Generative AI on Vertex AI"}