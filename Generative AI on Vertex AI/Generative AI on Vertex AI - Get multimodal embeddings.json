{"title": "Generative AI on Vertex AI - Get multimodal embeddings", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings", "abstract": "# Generative AI on Vertex AI - Get multimodal embeddings\nThe multimodal embeddings model generates 1408-dimension vectors* based on the input you provide, which can include a combination of image, text, and video data. The embedding vectors can then be used for subsequent tasks like image classification or video content moderation.\nThe image embedding vector and text embedding vector are in the same semantic space with the same dimensionality. Consequently, these vectors can be used interchangeably for use cases like searching image by text, or searching video by image.\nFor text-only embedding use cases, we recommend using the Vertex AI text-embeddings API instead. For example, the text-embeddings API might be better for text-based semantic search, clustering, long-form document analysis, and other text retrieval or question-answering use cases. For more information, see [Get text embeddings](/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) .\n* Default value.\n#", "content": "## Use cases\nImage and text:\n- **Image classification** : Takes an image as input and predicts one or more classes (labels).\n- **Image search** : Search relevant or similar images.\n- **Recommendations** : Generate product or ad recommendations based on images.\nImage, text, and video:\n- **Recommendations** : Generate product or advertisement recommendations based on videos (similarity search).\n- **Video content search** - **Using semantic search** : Take a text as an input, and return a set of ranked frames matching the query.\n- **Using similarity search** :- Take a video as an input, and return a set of videos matching the query.\n- Take an image as an input, and return a set of videos matching the query.\n- **Video classification** : Takes a video as input and predicts one or more classes.## Supported models\nYou can get multimodal embeddings by using the following model:\n- `multimodalembedding`## Best practices\nConsider the following input aspects when using the multimodal embeddings model:\n- **Text in images** - The model can distinguish text in images, similar to optical character recognition (OCR). If you need to distinguish between a description of the image content and the text within an image, consider using prompt engineering to specify your target content. For example: instead of just \"cat\", specify \"picture of a cat\" or \"the text 'cat'\", depending on your use case.| 0    | 1           |\n|:-----------------|:------------------------------------------|\n| the text 'cat' | nan          |\n| picture of a cat | Image credit: Manja Vitolic on Unsplash. |\n- **Embedding similarities** - The dot product of embeddings isn't a calibrated probability. The dot product is a similarity metric and might have different score distributions for different use cases. Consequently, avoid using a fixed value threshold to measure quality. Instead, use ranking approaches for retrieval, or use sigmoid for classification.## API usage\n### API limits\nThe following limits apply when you use the `multimodalembedding` model for text and image embeddings:\n| Limit             | Value and description                                                                                |\n|:------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| nan             | Text and image data                                                                                |\n| Maximum number of API requests per minute per project | 120                                                                                    |\n| Maximum text length         | 32 tokens (~32 words) The maximum text length is 32 tokens (approximately 32 words). If the input exceeds 32 tokens, the model internally shortens the input to this length.                                          |\n| Language            | English                                                                                   |\n| Image formats           | BMP, GIF, JPG, PNG                                                                                |\n| Image size           | Base64-encoded images: 20 MB (when transcoded to PNG) Cloud Storage images: 20MB (original file format) The maximum image size accepted is 20 MB. To avoid increased network latency, use smaller images. Additionally, the model resizes images to 512 x 512 pixel resolution. Consequently, you don't need to provide higher resolution images. |\n| nan             | Video data                                                                                  |\n| Audio supported          | N/A - The model doesn't consider audio content when generating video embeddings                                                                 |\n| Video formats           | AVI, FLV, MKV, MOV, MP4, MPEG, MPG, WEBM, and WMV                                                                         |\n| Maximum video length (Cloud Storage)     | No limit. However, only 2 minutes of content can be analyzed at a time.                                                                   |\n### Before you begin\n- Certain tasks in Vertex AI require that you use additional  Google Cloud products besides Vertex AI. For example, in most  cases, you must use Cloud Storage and Artifact Registry when you  create a custom training pipeline. You might need to perform additional setup  tasks to use other Google Cloud products.\n- Set up authentication for your environment.Select the tab for how you plan to use the samples on this page:\nTo use the Java samples on this page from a local development environment, install and initialize the gcloud CLI, and then set up Application Default Credentials with your user credentials.- [Install](/sdk/docs/install) the Google Cloud CLI.\n- To [initialize](/sdk/docs/initializing) the gcloud CLI, run the following command:```\ngcloud init\n```\n- Update and install`gcloud`components:```\ngcloud components updategcloud components install beta\n```\n- Create local authentication credentials for your Google Account:```\ngcloud auth application-default login\n```\nFor more information, see [ Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) in the Google Cloud authentication documentation.To use the Node.js samples on this page from a local development environment, install and initialize the gcloud CLI, and then set up Application Default Credentials with your user credentials.- [Install](/sdk/docs/install) the Google Cloud CLI.\n- To [initialize](/sdk/docs/initializing) the gcloud CLI, run the following command:```\ngcloud init\n```\n- Update and install`gcloud`components:```\ngcloud components updategcloud components install beta\n```\n- Create local authentication credentials for your Google Account:```\ngcloud auth application-default login\n```\nFor more information, see [ Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) in the Google Cloud authentication documentation.To use the Python samples on this page from a local development environment, install and initialize the gcloud CLI, and then set up Application Default Credentials with your user credentials.- [Install](/sdk/docs/install) the Google Cloud CLI.\n- To [initialize](/sdk/docs/initializing) the gcloud CLI, run the following command:```\ngcloud init\n```\n- Update and install`gcloud`components:```\ngcloud components updategcloud components install beta\n```\n- Create local authentication credentials for your Google Account:```\ngcloud auth application-default login\n```\nFor more information, see [ Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) in the Google Cloud authentication documentation.To use the REST API samples on this page in a local development environment, you use the credentials you provide to the gcloud CLI.- [Install](/sdk/docs/install) the Google Cloud CLI.\n- To [initialize](/sdk/docs/initializing) the gcloud CLI, run the following command:```\ngcloud init\n```\n- Update and install`gcloud`components:```\ngcloud components updategcloud components install beta\n```\n- To use the Python SDK, follow instructions at [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/install-sdk) .  For more information, see the [Vertex AI SDK for Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n- Optional. Review [pricing](/vertex-ai/generative-ai/pricing) for this feature. Pricing for embeddings depends on the type of data you send (such as image or text), and also depends on the mode you use for certain data types (such as Video Plus, Video Standard, or Video Essential).\n### Locations\nA location is a [region](/about/locations) you can specify in a request to control where data is stored at rest. For a list of available regions, see [Generative AI on Vertex AI locations](/vertex-ai/generative-ai/docs/learn/locations-genai) .\n### Error messages\n```\ngoogle.api_core.exceptions.ResourceExhausted: 429 Quota exceeded foraiplatform.googleapis.com/online_prediction_requests_per_base_model with basemodel: multimodalembedding. Please submit a quota increase request.\n```\nIf this is the first time you receive this error, use the Google Cloud console to [request a quota increase](/docs/quotas/view-manage#requesting_higher_quota) for your project. Use the following filters before requesting your increase:\n- `Service ID: aiplatform.googleapis.com`\n- `metric: aiplatform.googleapis.com/online_prediction_requests_per_base_model`\n- `base_model:multimodalembedding`\n[Go toQuotas](https://console.cloud.google.com/iam-admin/quotas?service=aiplatform.googleapis.com&metric=aiplatform.googleapis.com/online_prediction_requests_per_base_model&base_model=multimodalembedding)\nIf you have already sent a quota increase request, wait before sending another request. If you need to further increase the quota, repeat the quota increase request with your justification for a sustained quota request.\n### Specify lower-dimension embeddings\nBy default an embedding request returns a 1408 float vector for a data type. You can also specify lower-dimension embeddings (128, 256, or 512 float vectors) for text and image data. This option lets you optimize for latency and storage or quality based on how you plan to use the embeddings. Lower-dimension embeddings provide decreased storage needs and lower latency for subsequent embedding tasks (like search or recommendation), while higher-dimension embeddings offer greater accuracy for the same tasks.\nLow-dimension can be accessed by adding the `parameters.dimension` field. The parameter accepts one of the following values: `128` , `256` , `512` or `1408` . The response includes the embedding of that dimension.\nBefore using any of the request data, make the following replacements:- : Your project's region. For example,`us-central1`,`europe-west2`, or`asia-northeast3`. For a list of available regions, see [Generative AI on Vertex AI locations](/vertex-ai/generative-ai/docs/learn/locations-genai) .\n- : Your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The Cloud Storage URI of the target video to get embeddings for. For example,`gs://my-bucket/embeddings/supermarket-img.png`.\n- : The target text to get embeddings for. For example,`a cat`.\n- : The number of embedding dimensions. Lower values offer decreased latency when using these embeddings for subsequent tasks, while higher values offer better accuracy. Available values:`128`,`256`,`512`, and`1408`(default).\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\n```\nRequest JSON body:\n```\n{\n \"instances\": [ {\n  \"image\": {\n  \"gcsUri\": \"IMAGE_URI\"\n  },\n  \"text\": \"TEXT\"\n }\n ],\n \"parameters\": {\n \"dimension\": EMBEDDING_DIMENSION\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\" | Select-Object -Expand Content\n```\n## Send an embedding request (image and text)\n**Note:** For text-only embedding use cases, we recommend using the Vertex AI text-embeddings API instead. For example, the text-embeddings API might be better for text-based semantic search, clustering, long-form document analysis, and other text retrieval or question-answering use cases. For more information, see [Get text embeddings](/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) .\nUse the following code samples to send an embedding request with image and text data. The samples show how to send a request with both data types, but you can also use the service with an individual data type.\n### Get text and image embeddings\nFor more information about `multimodalembedding` model requests, see the [multimodalembedding model API reference](/vertex-ai/generative-ai/docs/model-reference/multimodal-embeddings) .\nBefore using any of the request data, make the following replacements:- : Your project's region. For example,`us-central1`,`europe-west2`, or`asia-northeast3`. For a list of available regions, see [Generative AI on Vertex AI locations](/vertex-ai/generative-ai/docs/learn/locations-genai) .\n- : Your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The target text to get embeddings for. For example,`a cat`.\n- : The target image to get embeddings for. The image must be specified as a [base64-encoded](/vertex-ai/generative-ai/docs/image/base64-encode) byte string.\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\n```\nRequest JSON body:\n```\n{\n \"instances\": [ {\n  \"text\": \"TEXT\",\n  \"image\": {\n  \"bytesBase64Encoded\": \"B64_ENCODED_IMG\"\n  }\n }\n ]\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\" | Select-Object -Expand Content\n```The embedding the model returns is a 1408 float vector. The following sample response is shortened for space.\n```\n{\n \"predictions\": [ {\n  \"textEmbedding\": [  0.010477379,\n  -0.00399621,\n  0.00576670747,\n  [...]\n  -0.00823613815,\n  -0.0169572588,\n  -0.00472954148\n  ],\n  \"imageEmbedding\": [  0.00262696808,\n  -0.00198890246,\n  0.0152047109,\n  -0.0103145819,\n  [...]\n  0.0324628279,\n  0.0284924973,\n  0.011650892,\n  -0.00452344026\n  ]\n }\n ],\n \"deployedModelId\": \"DEPLOYED_MODEL_ID\"\n}\n```\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/multimodal_embedding_image.py) \n```\nfrom typing import Optionalimport vertexaifrom vertexai.vision_models import (\u00a0 \u00a0 Image,\u00a0 \u00a0 MultiModalEmbeddingModel,\u00a0 \u00a0 MultiModalEmbeddingResponse,)def get_image_embeddings(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 image_path: str,\u00a0 \u00a0 contextual_text: Optional[str] = None,) -> MultiModalEmbeddingResponse:\u00a0 \u00a0 \"\"\"Example of how to generate multimodal embeddings from image and text.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 project_id: Google Cloud Project ID, used to initialize vertexai\u00a0 \u00a0 \u00a0 \u00a0 location: Google Cloud Region, used to initialize vertexai\u00a0 \u00a0 \u00a0 \u00a0 image_path: Path to image (local or Google Cloud Storage) to generate embeddings for.\u00a0 \u00a0 \u00a0 \u00a0 contextual_text: Text to generate embeddings for.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding\")\u00a0 \u00a0 image = Image.load_from_file(image_path)\u00a0 \u00a0 embeddings = model.get_embeddings(\u00a0 \u00a0 \u00a0 \u00a0 image=image,\u00a0 \u00a0 \u00a0 \u00a0 contextual_text=contextual_text,\u00a0 \u00a0 )\u00a0 \u00a0 print(f\"Image Embedding: {embeddings.image_embedding}\")\u00a0 \u00a0 print(f\"Text Embedding: {embeddings.text_embedding}\")\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/predict-image-from-image-and-text.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// const bastImagePath = \"YOUR_BASED_IMAGE_PATH\"// const textPrompt = 'YOUR_TEXT_PROMPT';const aiplatform = require('@google-cloud/aiplatform');// Imports the Google Cloud Prediction service clientconst {PredictionServiceClient} = aiplatform.v1;// Import the helper module for converting arbitrary protobuf.Value objects.const {helpers} = aiplatform;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};const publisher = 'google';const model = 'multimodalembedding@001';// Instantiates a clientconst predictionServiceClient = new PredictionServiceClient(clientOptions);async function predictImageFromImageAndText() {\u00a0 // Configure the parent resource\u00a0 const endpoint = `projects/${project}/locations/${location}/publishers/${publisher}/models/${model}`;\u00a0 const fs = require('fs');\u00a0 const imageFile = fs.readFileSync(baseImagePath);\u00a0 // Convert the image data to a Buffer and base64 encode it.\u00a0 const encodedImage = Buffer.from(imageFile).toString('base64');\u00a0 const prompt = {\u00a0 \u00a0 text: textPrompt,\u00a0 \u00a0 image: {\u00a0 \u00a0 \u00a0 bytesBase64Encoded: encodedImage,\u00a0 \u00a0 },\u00a0 };\u00a0 const instanceValue = helpers.toValue(prompt);\u00a0 const instances = [instanceValue];\u00a0 const parameter = {\u00a0 \u00a0 sampleCount: 1,\u00a0 };\u00a0 const parameters = helpers.toValue(parameter);\u00a0 const request = {\u00a0 \u00a0 endpoint,\u00a0 \u00a0 instances,\u00a0 \u00a0 parameters,\u00a0 };\u00a0 // Predict request\u00a0 const [response] = await predictionServiceClient.predict(request);\u00a0 console.log('Get image embedding response');\u00a0 const predictions = response.predictions;\u00a0 console.log('\\tPredictions :');\u00a0 for (const prediction of predictions) {\u00a0 \u00a0 console.log(`\\t\\tPrediction : ${JSON.stringify(prediction)}`);\u00a0 }}await predictImageFromImageAndText();\n```Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/PredictImageFromImageAndTextSample.java) \n```\nimport com.google.cloud.aiplatform.v1beta1.EndpointName;import com.google.cloud.aiplatform.v1beta1.PredictResponse;import com.google.cloud.aiplatform.v1beta1.PredictionServiceClient;import com.google.cloud.aiplatform.v1beta1.PredictionServiceSettings;import com.google.gson.Gson;import com.google.gson.JsonObject;import com.google.protobuf.InvalidProtocolBufferException;import com.google.protobuf.Value;import com.google.protobuf.util.JsonFormat;import java.io.IOException;import java.nio.charset.StandardCharsets;import java.nio.file.Files;import java.nio.file.Paths;import java.util.ArrayList;import java.util.Base64;import java.util.HashMap;import java.util.List;import java.util.Map;public class PredictImageFromImageAndTextSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace this variable before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String textPrompt = \"YOUR_TEXT_PROMPT\";\u00a0 \u00a0 String baseImagePath = \"YOUR_BASE_IMAGE_PATH\";\u00a0 \u00a0 // Learn how to use text prompts to update an image:\u00a0 \u00a0 // https://cloud.google.com/vertex-ai/docs/generative-ai/image/edit-images\u00a0 \u00a0 Map<String, Object> parameters = new HashMap<String, Object>();\u00a0 \u00a0 parameters.put(\"sampleCount\", 1);\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 String publisher = \"google\";\u00a0 \u00a0 String model = \"multimodalembedding@001\";\u00a0 \u00a0 predictImageFromImageAndText(\u00a0 \u00a0 \u00a0 \u00a0 project, location, publisher, model, textPrompt, baseImagePath, parameters);\u00a0 }\u00a0 // Update images using text prompts\u00a0 public static void predictImageFromImageAndText(\u00a0 \u00a0 \u00a0 String project,\u00a0 \u00a0 \u00a0 String location,\u00a0 \u00a0 \u00a0 String publisher,\u00a0 \u00a0 \u00a0 String model,\u00a0 \u00a0 \u00a0 String textPrompt,\u00a0 \u00a0 \u00a0 String baseImagePath,\u00a0 \u00a0 \u00a0 Map<String, Object> parameters)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 final String endpoint = String.format(\"%s-aiplatform.googleapis.com:443\", location);\u00a0 \u00a0 final PredictionServiceSettings predictionServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceSettings.newBuilder().setEndpoint(endpoint).build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests.\u00a0 \u00a0 try (PredictionServiceClient predictionServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceClient.create(predictionServiceSettings)) {\u00a0 \u00a0 \u00a0 final EndpointName endpointName =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 EndpointName.ofProjectLocationPublisherModelName(project, location, publisher, model);\u00a0 \u00a0 \u00a0 // Convert the image to Base64\u00a0 \u00a0 \u00a0 byte[] imageData = Base64.getEncoder().encode(Files.readAllBytes(Paths.get(baseImagePath)));\u00a0 \u00a0 \u00a0 String encodedImage = new String(imageData, StandardCharsets.UTF_8);\u00a0 \u00a0 \u00a0 JsonObject jsonInstance = new JsonObject();\u00a0 \u00a0 \u00a0 jsonInstance.addProperty(\"text\", textPrompt);\u00a0 \u00a0 \u00a0 JsonObject jsonImage = new JsonObject();\u00a0 \u00a0 \u00a0 jsonImage.addProperty(\"bytesBase64Encoded\", encodedImage);\u00a0 \u00a0 \u00a0 jsonInstance.add(\"image\", jsonImage);\u00a0 \u00a0 \u00a0 Value instanceValue = stringToValue(jsonInstance.toString());\u00a0 \u00a0 \u00a0 List<Value> instances = new ArrayList<>();\u00a0 \u00a0 \u00a0 instances.add(instanceValue);\u00a0 \u00a0 \u00a0 Gson gson = new Gson();\u00a0 \u00a0 \u00a0 String gsonString = gson.toJson(parameters);\u00a0 \u00a0 \u00a0 Value parameterValue = stringToValue(gsonString);\u00a0 \u00a0 \u00a0 PredictResponse predictResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 predictionServiceClient.predict(endpointName, instances, parameterValue);\u00a0 \u00a0 \u00a0 System.out.println(\"Predict Response\");\u00a0 \u00a0 \u00a0 System.out.println(predictResponse);\u00a0 \u00a0 \u00a0 for (Value prediction : predictResponse.getPredictionsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\tPrediction: %s\\n\", prediction);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\u00a0 // Convert a Json string to a protobuf.Value\u00a0 static Value stringToValue(String value) throws InvalidProtocolBufferException {\u00a0 \u00a0 Value.Builder builder = Value.newBuilder();\u00a0 \u00a0 JsonFormat.parser().merge(value, builder);\u00a0 \u00a0 return builder.build();\u00a0 }}\n```\n## Send an embedding request (video, image, or text)\nWhen sending an embedding request you can specify an input video alone, or you can specify a combination of video, image, and text data.\n### Video embedding modes\nThere are three modes you can use with video embeddings: Essential, Standard, or Plus. The mode corresponds to the density of the embeddings generated, which can be specified by the `interval_sec` config in the request. For each video interval with `interval_sec` length, an embedding is generated. The minimal video interval length is 4 seconds. Interval lengths greater than 120 seconds might negatively affect the quality of the generated embeddings.\nPricing for video embedding depends on the mode you use. For more information, see [pricing](/vertex-ai/generative-ai/pricing) .\nThe following table summarizes the three modes you can use for video embeddings:\n| Mode  | Maximum number of embeddings per minute | Video embedding interval (minimum value)  |\n|:----------|------------------------------------------:|:---------------------------------------------|\n| Essential |           4 | 15 This corresponds to: intervalSec >= 15 |\n| Standard |           8 | 8 This corresponds to: 8 <= intervalSec < 15 |\n| Plus  |          15 | 4 This corresponds to: 4 <= intervalSec < 8 |\n### Video embeddings best practices\nConsider the following when you send video embedding requests:\n- To generate a single embedding for the first two minutes of an input video of any length, use the following `videoSegmentConfig` setting:`request.json` :```\n// other request body content\"videoSegmentConfig\": {\u00a0 \"intervalSec\": 120}// other request body content\n```\n- To generate embedding for a video with a length greater than two minutes, you can send multiple requests that specify the start and end times in the `videoSegmentConfig` :`request1.json` :```\n// other request body content\"videoSegmentConfig\": {\u00a0 \"startOffsetSec\": 0,\u00a0 \"endOffsetSec\": 120}// other request body content\n````request2.json` :```\n// other request body content\"videoSegmentConfig\": {\u00a0 \"startOffsetSec\": 120,\u00a0 \"endOffsetSec\": 240}// other request body content\n```\n### Get video embeddings\nUse the following sample to get embeddings for video content alone.\nFor more information about `multimodalembedding` model requests, see the [multimodalembedding model API reference](/vertex-ai/generative-ai/docs/model-reference/multimodal-embeddings) .\nThe following example uses a video located in Cloud Storage. You can also use the `video.bytesBase64Encoded` field to provide a [base64-encoded](/vertex-ai/generative-ai/docs/image/base64-encode) string representation of the video.\nBefore using any of the request data, make the following replacements:- : Your project's region. For example,`us-central1`,`europe-west2`, or`asia-northeast3`. For a list of available regions, see [Generative AI on Vertex AI locations](/vertex-ai/generative-ai/docs/learn/locations-genai) .\n- : Your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The Cloud Storage URI of the target video to get embeddings for. For example,`gs://my-bucket/embeddings/supermarket-video.mp4`.\n- `videoSegmentConfig`(,,). Optional. The specific video segments (in seconds) the embeddings are generated for.The value you set for`videoSegmentConfig.intervalSec`affects  the pricing tier you are charged at. For more information, see  the [video embedding modes](/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings#video-modes) section and [pricing](/vertex-ai/generative-ai/pricing) page.For example:```\n[...]\n\"videoSegmentConfig\": {\n \"startOffsetSec\": 10,\n \"endOffsetSec\": 60,\n \"intervalSec\": 10\n}\n[...]\n```Using this config specifies video data from 10 seconds to 60 seconds and generates embeddings  for the following 10 second video intervals: [10, 20), [20, 30), [30, 40), [40, 50), [50, 60).  This video interval ( `\"intervalSec\": 10` ) falls in the [Standard video embedding mode](/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings#video-modes) , and the user  is charged at the [Standard mode pricing rate](/vertex-ai/generative-ai/pricing) .If you omit `videoSegmentConfig` , the service uses the following default values: `\"videoSegmentConfig\": { \"startOffsetSec\": 0, \"endOffsetSec\": 120, \"intervalSec\": 16 }` .  This video interval ( `\"intervalSec\": 16` ) falls in the [Essential video embedding mode](/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings#video-modes) , and the user  is charged at the [Essential mode pricing rate](/vertex-ai/generative-ai/pricing) .\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\n```\nRequest JSON body:\n```\n{\n \"instances\": [ {\n  \"video\": {\n  \"gcsUri\": \"VIDEO_URI\",\n  \"videoSegmentConfig\": {\n   \"startOffsetSec\": START_SECOND,\n   \"endOffsetSec\": END_SECOND,\n   \"intervalSec\": INTERVAL_SECONDS\n  }\n  }\n }\n ]\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\" | Select-Object -Expand Content\n```The embedding the model returns is a 1408 float vector. The following sample responses are shortened for space.\n **Response (7 second video, no videoSegmentConfig specified):** \n```\n{\n \"predictions\": [ {\n  \"videoEmbeddings\": [  {\n   \"endOffsetSec\": 7,\n   \"embedding\": [   -0.0045467657,\n   0.0258095954,\n   0.0146885719,\n   0.00945400633,\n   [...]\n   -0.0023291884,\n   -0.00493789,\n   0.00975185353,\n   0.0168156829\n   ],\n   \"startOffsetSec\": 0\n  }\n  ]\n }\n ],\n \"deployedModelId\": \"DEPLOYED_MODEL_ID\"\n}\n```\n **Response (59 second video, with the following video segment config: \"videoSegmentConfig\": { \"startOffsetSec\": 0, \"endOffsetSec\": 60, \"intervalSec\": 10 }):** \n```\n{\n \"predictions\": [ {\n  \"videoEmbeddings\": [  {\n   \"endOffsetSec\": 10,\n   \"startOffsetSec\": 0,\n   \"embedding\": [   -0.00683252793,\n   0.0390476175,\n   [...]\n   0.00657121744,\n   0.013023301\n   ]\n  },\n  {\n   \"startOffsetSec\": 10,\n   \"endOffsetSec\": 20,\n   \"embedding\": [   -0.0104404651,\n   0.0357737206,\n   [...]\n   0.00509833824,\n   0.0131902946\n   ]\n  },\n  {\n   \"startOffsetSec\": 20,\n   \"embedding\": [   -0.0113538112,\n   0.0305239167,\n   [...]\n   -0.00195809244,\n   0.00941874553\n   ],\n   \"endOffsetSec\": 30\n  },\n  {\n   \"embedding\": [   -0.00299320649,\n   0.0322436653,\n   [...]\n   -0.00993082579,\n   0.00968887936\n   ],\n   \"startOffsetSec\": 30,\n   \"endOffsetSec\": 40\n  },\n  {\n   \"endOffsetSec\": 50,\n   \"startOffsetSec\": 40,\n   \"embedding\": [   -0.00591270532,\n   0.0368893594,\n   [...]\n   -0.00219071587,\n   0.0042470959\n   ]\n  },\n  {\n   \"embedding\": [   -0.00458270218,\n   0.0368121453,\n   [...]\n   -0.00317760976,\n   0.00595594104\n   ],\n   \"endOffsetSec\": 59,\n   \"startOffsetSec\": 50\n  }\n  ]\n }\n ],\n \"deployedModelId\": \"DEPLOYED_MODEL_ID\"\n}\n```\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/multimodal_embedding_video.py) \n```\nfrom typing import Optionalimport vertexaifrom vertexai.vision_models import (\u00a0 \u00a0 MultiModalEmbeddingModel,\u00a0 \u00a0 MultiModalEmbeddingResponse,\u00a0 \u00a0 Video,\u00a0 \u00a0 VideoSegmentConfig,)def get_video_embeddings(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 video_path: str,\u00a0 \u00a0 contextual_text: Optional[str] = None,\u00a0 \u00a0 dimension: Optional[int] = 1408,\u00a0 \u00a0 video_segment_config: Optional[VideoSegmentConfig] = None,) -> MultiModalEmbeddingResponse:\u00a0 \u00a0 \"\"\"Example of how to generate multimodal embeddings from video and text.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 project_id: Google Cloud Project ID, used to initialize vertexai\u00a0 \u00a0 \u00a0 \u00a0 location: Google Cloud Region, used to initialize vertexai\u00a0 \u00a0 \u00a0 \u00a0 video_path: Path to video (local or Google Cloud Storage) to generate embeddings for.\u00a0 \u00a0 \u00a0 \u00a0 contextual_text: Text to generate embeddings for.\u00a0 \u00a0 \u00a0 \u00a0 dimension: Dimension for the returned embeddings.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings#low-dimension\u00a0 \u00a0 \u00a0 \u00a0 video_segment_config: Define specific segments to generate embeddings for.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings#video-best-practices\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding\")\u00a0 \u00a0 video = Video.load_from_file(video_path)\u00a0 \u00a0 embeddings = model.get_embeddings(\u00a0 \u00a0 \u00a0 \u00a0 video=video,\u00a0 \u00a0 \u00a0 \u00a0 video_segment_config=video_segment_config,\u00a0 \u00a0 \u00a0 \u00a0 contextual_text=contextual_text,\u00a0 \u00a0 \u00a0 \u00a0 dimension=dimension,\u00a0 \u00a0 )\u00a0 \u00a0 # Video Embeddings are segmented based on the video_segment_config.\u00a0 \u00a0 print(\"Video Embeddings:\")\u00a0 \u00a0 for video_embedding in embeddings.video_embeddings:\u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f\"Video Segment: {video_embedding.start_offset_sec} - {video_embedding.end_offset_sec}\"\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Embedding: {video_embedding.embedding}\")\u00a0 \u00a0 print(f\"Text Embedding: {embeddings.text_embedding}\")\n```### Get image, text, and video embeddings\nUse the following sample to get embeddings for video, text, and image content.\nFor more information about `multimodalembedding` model requests, see the [multimodalembedding model API reference](/vertex-ai/generative-ai/docs/model-reference/multimodal-embeddings) .\nThe following example uses image, text, and video data. You can use any combination of these data types in your request body.\nAdditionally, this sample uses a video located in Cloud Storage. You can also use the `video.bytesBase64Encoded` field to provide a [base64-encoded](/vertex-ai/generative-ai/docs/image/base64-encode) string representation of the video.\nBefore using any of the request data, make the following replacements:- : Your project's region. For example,`us-central1`,`europe-west2`, or`asia-northeast3`. For a list of available regions, see [Generative AI on Vertex AI locations](/vertex-ai/generative-ai/docs/learn/locations-genai) .\n- : Your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The target text to get embeddings for. For example,`a cat`.\n- : The Cloud Storage URI of the target video to get embeddings for. For example,`gs://my-bucket/embeddings/supermarket-img.png`.\n- : The Cloud Storage URI of the target video to get embeddings for. For example,`gs://my-bucket/embeddings/supermarket-video.mp4`.\n- `videoSegmentConfig`(,,). Optional. The specific video segments (in seconds) the embeddings are generated for.The value you set for`videoSegmentConfig.intervalSec`affects  the pricing tier you are charged at. For more information, see  the [video embedding modes](/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings#video-modes) section and [pricing](/vertex-ai/generative-ai/pricing) page.For example:```\n[...]\n\"videoSegmentConfig\": {\n \"startOffsetSec\": 10,\n \"endOffsetSec\": 60,\n \"intervalSec\": 10\n}\n[...]\n```Using this config specifies video data from 10 seconds to 60 seconds and generates embeddings  for the following 10 second video intervals: [10, 20), [20, 30), [30, 40), [40, 50), [50, 60).  This video interval ( `\"intervalSec\": 10` ) falls in the [Standard video embedding mode](/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings#video-modes) , and the user  is charged at the [Standard mode pricing rate](/vertex-ai/generative-ai/pricing) .If you omit `videoSegmentConfig` , the service uses the following default values: `\"videoSegmentConfig\": { \"startOffsetSec\": 0, \"endOffsetSec\": 120, \"intervalSec\": 16 }` .  This video interval ( `\"intervalSec\": 16` ) falls in the [Essential video embedding mode](/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings#video-modes) , and the user  is charged at the [Essential mode pricing rate](/vertex-ai/generative-ai/pricing) .\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\n```\nRequest JSON body:\n```\n{\n \"instances\": [ {\n  \"text\": \"TEXT\",\n  \"image\": {\n  \"gcsUri\": \"IMAGE_URI\"\n  },\n  \"video\": {\n  \"gcsUri\": \"VIDEO_URI\",\n  \"videoSegmentConfig\": {\n   \"startOffsetSec\": START_SECOND,\n   \"endOffsetSec\": END_SECOND,\n   \"intervalSec\": INTERVAL_SECONDS\n  }\n  }\n }\n ]\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\" | Select-Object -Expand Content\n```The embedding the model returns is a 1408 float vector. The following sample response is shortened for space.\n```\n{\n \"predictions\": [ {\n  \"textEmbedding\": [  0.0105433334,\n  -0.00302835181,\n  0.00656806398,\n  0.00603460241,\n  [...]\n  0.00445805816,\n  0.0139605571,\n  -0.00170318608,\n  -0.00490092579\n  ],\n  \"videoEmbeddings\": [  {\n   \"startOffsetSec\": 0,\n   \"endOffsetSec\": 7,\n   \"embedding\": [   -0.00673126569,\n   0.0248149596,\n   0.0128901172,\n   0.0107588246,\n   [...]\n   -0.00180952181,\n   -0.0054573305,\n   0.0117037306,\n   0.0169312079\n   ]\n  }\n  ],\n  \"imageEmbedding\": [  -0.00728622358,\n  0.031021487,\n  -0.00206603738,\n  0.0273937676,\n  [...]\n  -0.00204976718,\n  0.00321615417,\n  0.0121978866,\n  0.0193375275\n  ]\n }\n ],\n \"deployedModelId\": \"DEPLOYED_MODEL_ID\"\n}\n```\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/multimodal_embedding_image_video_text.py) \n```\nfrom typing import Optionalimport vertexaifrom vertexai.vision_models import (\u00a0 \u00a0 Image,\u00a0 \u00a0 MultiModalEmbeddingModel,\u00a0 \u00a0 MultiModalEmbeddingResponse,\u00a0 \u00a0 Video,\u00a0 \u00a0 VideoSegmentConfig,)def get_image_video_text_embeddings(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 image_path: str,\u00a0 \u00a0 video_path: str,\u00a0 \u00a0 contextual_text: Optional[str] = None,\u00a0 \u00a0 dimension: Optional[int] = 1408,\u00a0 \u00a0 video_segment_config: Optional[VideoSegmentConfig] = None,) -> MultiModalEmbeddingResponse:\u00a0 \u00a0 \"\"\"Example of how to generate multimodal embeddings from image, video, and text.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 project_id: Google Cloud Project ID, used to initialize vertexai\u00a0 \u00a0 \u00a0 \u00a0 location: Google Cloud Region, used to initialize vertexai\u00a0 \u00a0 \u00a0 \u00a0 image_path: Path to image (local or Google Cloud Storage) to generate embeddings for.\u00a0 \u00a0 \u00a0 \u00a0 video_path: Path to video (local or Google Cloud Storage) to generate embeddings for.\u00a0 \u00a0 \u00a0 \u00a0 contextual_text: Text to generate embeddings for.\u00a0 \u00a0 \u00a0 \u00a0 dimension: Dimension for the returned embeddings.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings#low-dimension\u00a0 \u00a0 \u00a0 \u00a0 video_segment_config: Define specific segments to generate embeddings for.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings#video-best-practices\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding\")\u00a0 \u00a0 image = Image.load_from_file(image_path)\u00a0 \u00a0 video = Video.load_from_file(video_path)\u00a0 \u00a0 embeddings = model.get_embeddings(\u00a0 \u00a0 \u00a0 \u00a0 image=image,\u00a0 \u00a0 \u00a0 \u00a0 video=video,\u00a0 \u00a0 \u00a0 \u00a0 video_segment_config=video_segment_config,\u00a0 \u00a0 \u00a0 \u00a0 contextual_text=contextual_text,\u00a0 \u00a0 \u00a0 \u00a0 dimension=dimension,\u00a0 \u00a0 )\u00a0 \u00a0 print(f\"Image Embedding: {embeddings.image_embedding}\")\u00a0 \u00a0 # Video Embeddings are segmented based on the video_segment_config.\u00a0 \u00a0 print(\"Video Embeddings:\")\u00a0 \u00a0 for video_embedding in embeddings.video_embeddings:\u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f\"Video Segment: {video_embedding.start_offset_sec} - {video_embedding.end_offset_sec}\"\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Embedding: {video_embedding.embedding}\")\u00a0 \u00a0 print(f\"Text Embedding: {embeddings.text_embedding}\")\n```\n## What's next\n- Read the blog [\"What is Multimodal Search: 'LLMs with vision' changebusinesses\"](https://cloud.google.com/blog/products/ai-machine-learning/multimodal-generative-ai-search) .\n- For information about text-only use cases (text-based semantic search, clustering, long-form document analysis, and other text retrieval or question-answering use cases), read [Get text embeddings](/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) .\n- View all Vertex AI image generative AI offerings in the [Imagen on Vertex AI overview](/vertex-ai/generative-ai/docs/image/overview) .\n- Explore more pretrained models in [Model Garden](/vertex-ai/docs/start/explore-models) .\n- Learn about [responsible AI best practices and safety filters inVertex AI](/vertex-ai/generative-ai/docs/learn/responsible-ai) .", "guide": "Generative AI on Vertex AI"}