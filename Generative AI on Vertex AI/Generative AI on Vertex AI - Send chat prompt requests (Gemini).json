{"title": "Generative AI on Vertex AI - Send chat prompt requests (Gemini)", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-chat-prompts-gemini", "abstract": "# Generative AI on Vertex AI - Send chat prompt requests (Gemini)\nThis page shows you how to send chat prompts to the Gemini 1.0 Pro ( `gemini-1.0-pro` ) model by using the Google Cloud console, REST API, and supported SDKs. Gemini 1.0 Pro supports prompts with text-only input, including natural language tasks, multi-turn text and code chat, and code generation. It can output text and code.\nThe Gemini 1.0 Pro foundation model is a large language model that excels at understanding and generating language. You can interact with Gemini Pro using a single-turn prompt and response or chat with it in a multi-turn, continuous conversation, even for code understanding and generation.\nFor a list of languages supported by Gemini 1.0 Pro, see model information [Language support](/vertex-ai/generative-ai/docs/learn/models#language-support) .\nTo explore this model in the console, select the `gemini-1.0-pro` model card in the Model Garden.\n[Go to the Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-1.0-pro)\nIf you're looking for a way to use Gemini directly from your mobile and web apps, check out the [Google AI SDKs](https://ai.google.dev/docs) for Android, Swift, and web.\n", "content": "## Use cases\nGemini 1.0 Pro supports text and code generation from a text prompt, including but not limited to the following use cases:\n- **Summarization:** Create a shorter version of a document that incorporates pertinent information from the original text. For example, you might want to summarize a chapter from a textbook. Or, you could create a succinct product description from a long paragraph that describes the product in detail.\n- **Question answering:** Provide answers to questions in text. For example, you might automate the creation of a Frequently Asked Questions (FAQ) document from knowledge base content.\n- **Classification:** Assign a label to provided text. For example, a label might be applied to text that describes how grammatically correct it is.\n- **Sentiment analysis:** This is a form of classification that identifies the sentiment of text. The sentiment is turned into a label that's applied to the text. For example, the sentiment of text might be polarities like positive or negative, or sentiments like anger or happiness.\n- **Entity extraction:** Extract a piece of information from text. For example, you can extract the name of a movie from the text of an article.\n- **Content creation:** Generate texts by specifying a set of requirements and background. For example, you might want to draft an email under a given context using a certain tone.\n- **Code generation:** Generate code based on a description. For example, you can ask the model to write a function that checks whether a year is a leap year.\n- **Multi-turn chat:** Prompts that include previous messages as context for generating new responses.\nTo learn more about how to design prompts for various uses, see the following pages:\n- [Text prompts](/vertex-ai/generative-ai/docs/text/text-prompts) \n- [Chat prompts](/vertex-ai/generative-ai/docs/chat/chat-prompts) \n- [Code chat prompts](/vertex-ai/generative-ai/docs/code/code-chat-prompts) \n- [Code generation prompts](/vertex-ai/generative-ai/docs/code/code-generation-prompts) ## Send chat prompts\nFor testing and iterating on chat prompts, we recommend using the Google Cloud console. To send prompts programmatically to the model, you can use the REST API, Vertex AI SDK for Python, or one of the other supported libraries and SDKs shown in the following tabs.\nTo learn how to install or update the Vertex AI SDK for Python, see [ Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk#install-vertex-ai-python-sdk) . For more information, see the [Vertex AI SDK for Python API reference documentation.](/python/docs/reference/aiplatform/latest) \n### Streaming and non-streaming responsesYou can choose whether the model generates a streaming response or a non-streaming response. Streaming involves receiving responses to prompts as they are generated. That is, as soon as the model generates output tokens, the output tokens are sent. A non-streaming response to prompts is sent only after all of the output tokens are generated.For a streaming response, use the `stream` parameter in [generate_content](/python/docs/reference/aiplatform/latest/vertexai.preview.generative_models#content) .```\n response = model.generate_content(contents=[...], stream = True)\n \n```For a non-streaming response, remove the parameter, or set the parameter to `False` .\n### Sample code [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/gemini_chat_example.py) \n```\nimport vertexaifrom vertexai.generative_models import GenerativeModel, ChatSession# TODO(developer): Update and un-comment below lines# project_id = \"PROJECT_ID\"# location = \"us-central1\"vertexai.init(project=project_id, location=location)model = GenerativeModel(\"gemini-1.0-pro\")chat = model.start_chat()def get_chat_response(chat: ChatSession, prompt: str) -> str:\u00a0 \u00a0 text_response = []\u00a0 \u00a0 responses = chat.send_message(prompt, stream=True)\u00a0 \u00a0 for chunk in responses:\u00a0 \u00a0 \u00a0 \u00a0 text_response.append(chunk.text)\u00a0 \u00a0 return \"\".join(text_response)prompt = \"Hello.\"print(get_chat_response(chat, prompt))prompt = \"What are all the colors in a rainbow?\"print(get_chat_response(chat, prompt))prompt = \"Why does it appear when it rains?\"print(get_chat_response(chat, prompt))\n```Before trying this sample, follow the Node.js setup instructions in the [Generative AI quickstart using the Node.js SDK](/nodejs/docs/reference/vertexai/latest#before-you-begin) . For more information, see the [Node.js SDK for Gemini reference documentation](/nodejs/docs/reference/vertexai/latest) .To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n### Streaming and non-streaming responsesYou can choose whether the model generates a streaming response or a non-streaming response. Streaming involves receiving responses to prompts as they are generated. That is, as soon as the model generates output tokens, the output tokens are sent. A non-streaming response to prompts is sent only after all of the output tokens are generated.For a streaming response, use the [generateContentStream](/nodejs/docs/reference/vertexai/latest#streaming-content-generation) method.```\n const streamingResp = await generativeModel.generateContentStream(request);\n \n```For a non-streaming response, use the [generateContent](/nodejs/docs/reference/vertexai/latest#content-generation-non-streaming) method.```\n const streamingResp = await generativeModel.generateContent(request);\n \n```\n### Sample code [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/generative-ai/snippets/streamChat.js) \n```\nconst {VertexAI} = require('@google-cloud/vertexai');/**\u00a0* TODO(developer): Update these variables before running the sample.\u00a0*/async function createStreamChat(\u00a0 projectId = 'PROJECT_ID',\u00a0 location = 'us-central1',\u00a0 model = 'gemini-1.0-pro') {\u00a0 // Initialize Vertex with your Cloud project and location\u00a0 const vertexAI = new VertexAI({project: projectId, location: location});\u00a0 // Instantiate the model\u00a0 const generativeModel = vertexAI.getGenerativeModel({\u00a0 \u00a0 model: model,\u00a0 });\u00a0 const chat = generativeModel.startChat({});\u00a0 const chatInput1 = 'How can I learn more about that?';\u00a0 console.log(`User: ${chatInput1}`);\u00a0 const result1 = await chat.sendMessageStream(chatInput1);\u00a0 for await (const item of result1.stream) {\u00a0 \u00a0 console.log(item.candidates[0].content.parts[0].text);\u00a0 }}\n```Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart](/vertex-ai/docs/start/client-libraries) . For more information, see the [ Vertex AI Java SDK for Gemini reference documentation](/java/docs/reference/google-cloud-vertexai/latest/overview) .To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment.](/docs/authentication/provide-credentials-adc#local-dev) \n### Streaming and non-streaming responsesYou can choose whether the model generates a streaming response or a non-streaming response. Streaming involves receiving responses to prompts as they are generated. That is, as soon as the model generates output tokens, the output tokens are sent. A non-streaming response to prompts is sent only after all of the output tokens are generated.For a streaming response, use the [generateContentStream](/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.preview.GenerativeModel#com_google_cloud_vertexai_generativeai_preview_GenerativeModel_generateContentStream_com_google_cloud_vertexai_api_Content_) method.```\n public ResponseStream generateContentStream(Content content)\n \n```For a non-streaming response, use the [generateContent](/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.preview.GenerativeModel#com_google_cloud_vertexai_generativeai_preview_GenerativeModel_generateContent_com_google_cloud_vertexai_api_Content_) method.```\n public GenerateContentResponse generateContent(Content content)\n \n```\n### Sample code [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/vertexai/snippets/src/main/java/vertexai/gemini/ChatDiscussion.java) \n```\nimport com.google.cloud.vertexai.VertexAI;import com.google.cloud.vertexai.api.GenerateContentResponse;import com.google.cloud.vertexai.generativeai.ChatSession;import com.google.cloud.vertexai.generativeai.GenerativeModel;import com.google.cloud.vertexai.generativeai.ResponseHandler;import java.io.IOException;public class ChatDiscussion {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String projectId = \"your-google-cloud-project-id\";\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 String modelName = \"gemini-1.0-pro\";\u00a0 \u00a0 chatDiscussion(projectId, location, modelName);\u00a0 }\u00a0 // Ask interrelated questions in a row using a ChatSession object.\u00a0 public static void chatDiscussion(String projectId, String location, String modelName)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs\u00a0 \u00a0 // to be created once, and can be reused for multiple requests.\u00a0 \u00a0 try (VertexAI vertexAI = new VertexAI(projectId, location)) {\u00a0 \u00a0 \u00a0 GenerateContentResponse response;\u00a0 \u00a0 \u00a0 GenerativeModel model = new GenerativeModel(modelName, vertexAI);\u00a0 \u00a0 \u00a0 // Create a chat session to be used for interactive conversation.\u00a0 \u00a0 \u00a0 ChatSession chatSession = new ChatSession(model);\u00a0 \u00a0 \u00a0 response = chatSession.sendMessage(\"Hello.\");\u00a0 \u00a0 \u00a0 System.out.println(ResponseHandler.getText(response));\u00a0 \u00a0 \u00a0 response = chatSession.sendMessage(\"What are all the colors in a rainbow?\");\u00a0 \u00a0 \u00a0 System.out.println(ResponseHandler.getText(response));\u00a0 \u00a0 \u00a0 response = chatSession.sendMessage(\"Why does it appear when it rains?\");\u00a0 \u00a0 \u00a0 System.out.println(ResponseHandler.getText(response));\u00a0 \u00a0 \u00a0 System.out.println(\"Chat Ended.\");\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Go setup instructions in the [Vertex AI quickstart.](/vertex-ai/docs/start/client-libraries) For more information, see the [Vertex AI Go SDK for Gemini reference documentation](/go/docs/reference/cloud.google.com/go/vertexai/latest) .To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n### Streaming and non-streaming responsesYou can choose whether the model generates a streamed response or a non-streamed response. Streaming involves receiving responses to prompts as they are generated. That is, as soon as the model generates output tokens, the output tokens are sent. A non-streaming response to prompts is sent only after all of the output tokens are generated.For a streaming response, use the [GenerateContentStream](https://pkg.go.dev/cloud.google.com/go/vertexai/genai#GenerativeModel.GenerateContentStream) method.```\n iter := model.GenerateContentStream(ctx, genai.Text(\"Tell me a story about a lumberjack and his giant ox. Keep it very short.\"))\n \n```For a non-streaming response, use the [GenerateContent](https://pkg.go.dev/cloud.google.com/go/vertexai/genai#GenerativeModel.GenerateContent) method.```\n resp, err := model.GenerateContent(ctx, genai.Text(\"What is the average size of a swallow?\"))\n \n```\n### Sample code [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/vertexai/chat/chat.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"encoding/json\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/vertexai/genai\")var projectId = \"PROJECT_ID\"var region = \"us-central1\"var modelName = \"gemini-1.0-pro-vision\"func makeChatRequests(projectId string, region string, modelName string) error {\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 client, err := genai.NewClient(ctx, projectId, region)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"error creating client: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 gemini := client.GenerativeModel(modelName)\u00a0 \u00a0 \u00a0 \u00a0 chat := gemini.StartChat()\u00a0 \u00a0 \u00a0 \u00a0 r, err := chat.SendMessage(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ctx,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 genai.Text(\"Hello\"))\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 rb, _ := json.MarshalIndent(r, \"\", \" \u00a0\")\u00a0 \u00a0 \u00a0 \u00a0 fmt.Println(string(rb))\u00a0 \u00a0 \u00a0 \u00a0 r, err = chat.SendMessage(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ctx,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 genai.Text(\"What are all the colors in a rainbow?\"))\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 rb, _ = json.MarshalIndent(r, \"\", \" \u00a0\")\u00a0 \u00a0 \u00a0 \u00a0 fmt.Println(string(rb))\u00a0 \u00a0 \u00a0 \u00a0 r, err = chat.SendMessage(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ctx,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 genai.Text(\"Why does it appear when it rains?\"))\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 rb, _ = json.MarshalIndent(r, \"\", \" \u00a0\")\u00a0 \u00a0 \u00a0 \u00a0 fmt.Println(string(rb))\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```You can use REST to send a chat prompt by using the Vertex AI API to send a POST request to the publisher model endpoint.\nBefore using any of the request data, make the following replacements:- : The type of response that you want the model to generate. Choose a method that generates how you want the model's response to be returned:- `streamGenerateContent`: The response is streamed as it's being generated to reduce the perception of latency to a human audience.\n- `generateContent`: The response is returned after it's fully generated.\n- : The region to process the request. Available options include the following:\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The model ID of the multimodal model  that you want to use. The options are:- `gemini-1.0-pro`\n- : The role in a conversation associated with the content. Specifying a role is required even in singleturn use cases. Acceptable values include the following:- `USER`: Specifies content that's sent by you.\n- `MODEL`: Specifies the model's response.- : The text instructions to include in the prompt.\n- : The safety category to configure a threshold for. Acceptable values include the following:\n- : The threshold for blocking responses that could belong to the specified safety category based on probability. Acceptable values include the following:`BLOCK_LOW_AND_ABOVE`blocks the most while`BLOCK_ONLY_HIGH`blocks the least.\n- : The temperature is used for sampling during response generation, which occurs when`topP`and`topK`are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of`0`means that the highest probability tokens are always selected. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.If the model returns a response that's too generic, too short, or the model gives a fallback response, try increasing the temperature.\n- : Top-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K) to least probable until the sum of their probabilities equals the top-P value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-P value is`0.5`, then the model will select either A or B as the next token by using temperature and excludes C as a candidate.Specify a lower value for less random responses and a higher value for more random responses.\n- : Top-K changes how the model selects tokens for output. A top-K of`1`means the next selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-K of`3`means that the next token is selected from among the three most probable tokens by using temperature.For each token selection step, the top-K tokens with the highest probabilities are sampled. Then tokens are further filtered based on top-P with the final token selected using temperature sampling.Specify a lower value for less random responses and a higher value for more random responses.\n- : Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.Specify a lower value for shorter responses and a higher value for potentially longer responses.\n- : Specifies a list of strings that tells the model to stop generating text if one of the strings is encountered in the response. If a string appears multiple times in the response, then the response truncates where it's first encountered. The strings are case-sensitive.For example, if the following is the returned response when`stopSequences`isn't specified:`public static string reverse(string myString)`Then the returned response with`stopSequences`set to`[\"Str\", \"reverse\"]`is:`public static string`\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:GENERATE_RESPONSE_METHOD\n```\nRequest JSON body:\n```\n{\n \"contents\": {\n \"role\": \"ROLE\",\n \"parts\": { \"text\": \"TEXT\" }\n },\n \"safety_settings\": {\n \"category\": \"SAFETY_CATEGORY\",\n \"threshold\": \"THRESHOLD\"\n },\n \"generation_config\": {\n \"temperature\": TEMPERATURE,\n \"topP\": TOP_P,\n \"topK\": TOP_K,\n \"candidateCount\": 1,\n \"maxOutputTokens\": MAX_OUTPUT_TOKENS,\n \"stopSequences\": STOP_SEQUENCES,\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:GENERATE_RESPONSE_METHOD\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:GENERATE_RESPONSE_METHOD\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following.```\nLOCATION=\"us-central1\"MODEL_ID=\"gemini-1.0-pro\"PROJECT_ID=\"test-project\"GENERATE_RESPONSE_METHOD=\"generateContent\"curl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:${GENERATE_RESPONSE_METHOD} -d \\$'{\u00a0 \"contents\": [\u00a0 \u00a0 {\u00a0 \u00a0 \"role\": \"user\",\u00a0 \u00a0 \"parts\": { \"text\": \"Hello!\" }\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \"role\": \"model\",\u00a0 \u00a0 \"parts\": { \"text\": \"Argh! What brings ye to my ship?\" }\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \"role\": \"user\",\u00a0 \u00a0 \"parts\": { \"text\": \"Wow! You are a real-life pirate!\" }\u00a0 \u00a0 }\u00a0 ],\u00a0 \"safety_settings\": {\u00a0 \u00a0 \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\u00a0 \u00a0 \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\u00a0 },\u00a0 \"generation_config\": {\u00a0 \u00a0 \"temperature\": 0.9,\u00a0 \u00a0 \"topP\": 1,\u00a0 \u00a0 \"candidateCount\": 1,\u00a0 \u00a0 \"maxOutputTokens\": 2048\u00a0 }}'\n```To use the Vertex AI Studio to send a chat prompt in the Google Cloud console, do the following:- In the Vertex AI section of the Google Cloud console, go to  the **Language** section of the **Vertex AI Studio** . [Go to Vertex AI Studio](https://console.cloud.google.com/vertex-ai/generative/language) \n- Click **Text chat** .\n- Configure the model and parameters:- **Region** : Select the region that you want to use.\n- **Model** : Select **Gemini Pro** .\n- **Temperature** : Use the slider or textbox to enter a value for  temperature.The temperature is used for sampling during response generation, which occurs when`topP`and`topK`are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of`0`means that the highest probability tokens are always selected. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.If the model returns a response that's too generic, too short, or the model gives a fallback response, try increasing the temperature.\n- **Token limit** : Use the slider or textbox to enter a value for the  max output limit.Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.Specify a lower value for shorter responses and a higher value for potentially longer responses.\n- **Add stop sequence** : Enter a stop sequence, which is a series of   characters (including spaces) that stops response generation if the   model encounters it. The sequence is not included as part of the   response. You can add up to five stop sequences.- Optional: To configure advanced parameters, click **Advanced** and  configure as follows:\n- The Google Cloud console only supports streaming, which involves receiving responses to prompts as they are generated. You are ready to enter a message in the message box to start a conversation with the model.The model uses the previous messages as context for new responses.\n- Optional: To save your prompt to **My prompts** , clicksave_alt **Save** .\n- Optional: To get the Python code or a curl command for your prompt, clickcode **Get code** .\n- Optional: To clear all previous messages, clickdelete **Clear conversation**\n## What's next\n- Learn how to [send multimodal prompt requests](/vertex-ai/generative-ai/docs/multimodal/send-multimodal-prompts) .\n- Learn about [responsible AI best practices and Vertex AI's safety filters](/vertex-ai/generative-ai/docs/learn/responsible-ai) .", "guide": "Generative AI on Vertex AI"}