{"title": "Generative AI on Vertex AI - Stream responses from Generative AI models", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn/streaming", "abstract": "# Generative AI on Vertex AI - Stream responses from Generative AI models\nYou can make streaming requests to the Vertex AI Large Language Model (LLM) using the following:\n- The Vertex AI [REST API with server-sent events (SSE)](#rest-sse) \n- The Vertex AI [REST API](#rest) \n- The [Vertex AI SDK for Python](#sdk) \n- A [client library](/vertex-ai/docs/start/client-libraries) \nThe streaming and non-streaming APIs use the same parameters, and there is no difference in pricing and quotas.\n", "content": "## Vertex AI Studio\nYou can use [Vertex AI Studio](/vertex-ai/generative-ai/docs/learn/overview) to design and run prompts and receive streamed responses. From the prompt design page, click the **Streaming Response** button to enable streaming.\n## Examples\nYou can call the Streaming API by using one of the following:\n- [REST API with server-sent events (SSE)](#rest-sse) \n- [REST API](#rest) \n- [Vertex AI SDK for Python](#sdk) \n### REST API with server-sent events (SSE)\nThe parameters are different across the model types used in the following examples:\nThe current supported models are `text-bison` and `text-unicorn` . See [available versions](/vertex-ai/generative-ai/docs/learn/model-versioning) .\n **Request** \n```\n\u00a0 PROJECT_ID=YOUR_PROJECT_ID\u00a0 PROMPT=\"PROMPT\"\u00a0 MODEL_ID=text-bison\u00a0 curl \\\u00a0 -X POST \\\u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:serverStreamingPredict?alt=sse -d \\\u00a0 '{\u00a0 \u00a0 \"inputs\": [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"prompt\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"string_val\": [ \"'\"${PROMPT}\"'\" ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 ],\u00a0 \u00a0 \"parameters\": {\u00a0 \u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \u00a0 \"temperature\": { \"float_val\": 0.8 },\u00a0 \u00a0 \u00a0 \u00a0 \"maxOutputTokens\": { \"int_val\": 1024 },\u00a0 \u00a0 \u00a0 \u00a0 \"topK\": { \"int_val\": 40 },\u00a0 \u00a0 \u00a0 \u00a0 \"topP\": { \"float_val\": 0.95 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }'\n```\n **Response** \nResponses are server-sent event messages.\n```\n\u00a0 data: {\"outputs\": [{\"structVal\": {\"content\": {\"stringVal\": [RESPONSE]},\"safetyAttributes\": {\"structVal\": {\"blocked\": {\"boolVal\": [BOOLEAN]},\"categories\": {\"listVal\": [{\"stringVal\": [Safety category name]}]},\"scores\": {\"listVal\": [{\"doubleVal\": [Safety category score]}]}}},\"citationMetadata\": {\"structVal\": {\"citations\": {}}}}}]}\n```\nThe current supported model is `chat-bison` . See [available versions](/vertex-ai/generative-ai/docs/learn/model-versioning) .\n **Request** \n```\nPROJECT_ID=YOUR_PROJECT_IDPROMPT=\"PROMPT\"AUTHOR=\"USER\"MODEL_ID=chat-bisoncurl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:serverStreamingPredict?alt=sse -d \\$'{\u00a0 \"inputs\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \u00a0 \"messages\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"list_val\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"string_val\": [ \"'\"${PROMPT}\"'\" ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"author\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"string_val\": [ \"'\"${AUTHOR}\"'\"]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ],\u00a0 \"parameters\": {\u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \"temperature\": { \"float_val\": 0.5 },\u00a0 \u00a0 \u00a0 \"maxOutputTokens\": { \"int_val\": 1024 },\u00a0 \u00a0 \u00a0 \"topK\": { \"int_val\": 40 },\u00a0 \u00a0 \u00a0 \"topP\": { \"float_val\": 0.95 }\u00a0 \u00a0 }\u00a0 }}'\n```\n **Response** \nResponses are server-sent event messages.\n```\ndata: {\"outputs\": [{\"structVal\": {\"candidates\": {\"listVal\": [{\"structVal\": {\"author\": {\"stringVal\": [AUTHOR]},\"content\": {\"stringVal\": [RESPONSE]}}}]},\"citationMetadata\": {\"listVal\": [{\"structVal\": {\"citations\": {}}}]},\"safetyAttributes\": {\"structVal\": {\"blocked\": {\"boolVal\": [BOOLEAN]},\"categories\": {\"listVal\": [{\"stringVal\": [Safety category name]}]},\"scores\": {\"listVal\": [{\"doubleVal\": [Safety category score]}]}}}}}]}\n```\nThe current supported model is `code-bison` . See [available versions](/vertex-ai/generative-ai/docs/learn/model-versioning) .\n **Request** \n```\nPROJECT_ID=YOUR_PROJECT_IDPROMPT=\"PROMPT\"MODEL_ID=code-bisoncurl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:serverStreamingPredict?alt=sse -d \\$'{\u00a0 \"inputs\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \u00a0 \"prefix\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"string_val\": [ \"'\"${PROMPT}\"'\" ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ],\u00a0 \"parameters\": {\u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \"temperature\": { \"float_val\": 0.8 },\u00a0 \u00a0 \u00a0 \"maxOutputTokens\": { \"int_val\": 1024 },\u00a0 \u00a0 \u00a0 \"topK\": { \"int_val\": 40 },\u00a0 \u00a0 \u00a0 \"topP\": { \"float_val\": 0.95 }\u00a0 \u00a0 }\u00a0 }}'\n```\n **Response** \nResponses are server-sent event messages.\n```\ndata: {\"outputs\": [{\"structVal\": {\"citationMetadata\": {\"structVal\": {\"citations\": {}}},\"safetyAttributes\": {\"structVal\": {\"blocked\": {\"boolVal\": [BOOLEAN]},\"categories\": {\"listVal\": [{\"stringVal\": [Safety category name]}]},\"scores\": {\"listVal\": [{\"doubleVal\": [Safety category score]}]}}},\"content\": {\"stringVal\": [RESPONSE]}}}]}\n```\nThe current supported model is `codechat-bison` . See [available versions](/vertex-ai/generative-ai/docs/learn/model-versioning) .\n **Request** \n```\nPROJECT_ID=YOUR_PROJECT_IDPROMPT=\"PROMPT\"AUTHOR=\"USER\"MODEL_ID=codechat-bisoncurl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:serverStreamingPredict?alt=sse -d \\$'{\u00a0 \"inputs\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \u00a0 \"messages\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"list_val\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"string_val\": [ \"'\"${PROMPT}\"'\" ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"author\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"string_val\": [ \"'\"${AUTHOR}\"'\"]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ],\u00a0 \"parameters\": {\u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \"temperature\": { \"float_val\": 0.5 },\u00a0 \u00a0 \u00a0 \"maxOutputTokens\": { \"int_val\": 1024 },\u00a0 \u00a0 \u00a0 \"topK\": { \"int_val\": 40 },\u00a0 \u00a0 \u00a0 \"topP\": { \"float_val\": 0.95 }\u00a0 \u00a0 }\u00a0 }}'\n```\n **Response** \nResponses are server-sent event messages.\n```\ndata: {\"outputs\": [{\"structVal\": {\"safetyAttributes\": {\"structVal\": {\"blocked\": {\"boolVal\": [BOOLEAN]},\"categories\": {\"listVal\": [{\"stringVal\": [Safety category name]}]},\"scores\": {\"listVal\": [{\"doubleVal\": [Safety category score]}]}}},\"citationMetadata\": {\"listVal\": [{\"structVal\": {\"citations\": {}}}]},\"candidates\": {\"listVal\": [{\"structVal\": {\"content\": {\"stringVal\": [RESPONSE]},\"author\": {\"stringVal\": [AUTHOR]}}}]}}}]}\n```\n### REST API\nThe parameters are different across the model types used in the following examples:\nThe current supported models are `text-bison` and `text-unicorn` . See [available versions](/vertex-ai/generative-ai/docs/learn/model-versioning) .\n **Request** \n```\n\u00a0 PROJECT_ID=YOUR_PROJECT_ID\u00a0 PROMPT=\"PROMPT\"\u00a0 MODEL_ID=text-bison\u00a0 curl \\\u00a0 -X POST \\\u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:serverStreamingPredict -d \\\u00a0 '{\u00a0 \u00a0 \"inputs\": [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"prompt\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"string_val\": [ \"'\"${PROMPT}\"'\" ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 ],\u00a0 \u00a0 \"parameters\": {\u00a0 \u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \u00a0 \"temperature\": { \"float_val\": 0.8 },\u00a0 \u00a0 \u00a0 \u00a0 \"maxOutputTokens\": { \"int_val\": 1024 },\u00a0 \u00a0 \u00a0 \u00a0 \"topK\": { \"int_val\": 40 },\u00a0 \u00a0 \u00a0 \u00a0 \"topP\": { \"float_val\": 0.95 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }'\n```\n **Response** \n```\n{\u00a0 \"outputs\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"structVal\": {\u00a0 \u00a0 \u00a0 \u00a0 \"citationMetadata\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"structVal\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"citations\": {}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \"safetyAttributes\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"structVal\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"categories\": {},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"scores\": {},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"blocked\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"boolVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 false\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \"content\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"stringVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 RESPONSE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ]}\n```\nThe current supported model is `chat-bison` . See [available versions](/vertex-ai/generative-ai/docs/learn/model-versioning) .\n **Request** \n```\nPROJECT_ID=YOUR_PROJECT_IDPROMPT=\"PROMPT\"AUTHOR=\"USER\"MODEL_ID=chat-bisoncurl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:serverStreamingPredict -d \\$'{\u00a0 \"inputs\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \u00a0 \"messages\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"list_val\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"string_val\": [ \"'\"${PROMPT}\"'\" ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"author\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"string_val\": [ \"'\"${AUTHOR}\"'\"]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ],\u00a0 \"parameters\": {\u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \"temperature\": { \"float_val\": 0.5 },\u00a0 \u00a0 \u00a0 \"maxOutputTokens\": { \"int_val\": 1024 },\u00a0 \u00a0 \u00a0 \"topK\": { \"int_val\": 40 },\u00a0 \u00a0 \u00a0 \"topP\": { \"float_val\": 0.95 }\u00a0 \u00a0 }\u00a0 }}'\n```\n **Response** \n```\n{\u00a0 \"outputs\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"structVal\": {\u00a0 \u00a0 \u00a0 \u00a0 \"candidates\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"listVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"structVal\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"stringVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 RESPONSE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"author\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"stringVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AUTHOR\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \"citationMetadata\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"listVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"structVal\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"citations\": {}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \"safetyAttributes\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"listVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"structVal\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"categories\": {},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"blocked\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"boolVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 false\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"scores\": {}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ]}\n```\nThe current supported model is `code-bison` . See [available versions](/vertex-ai/generative-ai/docs/learn/model-versioning) .\n **Request** \n```\nPROJECT_ID=YOUR_PROJECT_IDPROMPT=\"PROMPT\"MODEL_ID=code-bisoncurl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:serverStreamingPredict -d \\$'{\u00a0 \"inputs\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \u00a0 \"prefix\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"string_val\": [ \"'\"${PROMPT}\"'\" ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ],\u00a0 \"parameters\": {\u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \"temperature\": { \"float_val\": 0.8 },\u00a0 \u00a0 \u00a0 \"maxOutputTokens\": { \"int_val\": 1024 },\u00a0 \u00a0 \u00a0 \"topK\": { \"int_val\": 40 },\u00a0 \u00a0 \u00a0 \"topP\": { \"float_val\": 0.95 }\u00a0 \u00a0 }\u00a0 }}'\n```\n **Response** \n```\n{\u00a0 \"outputs\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"structVal\": {\u00a0 \u00a0 \u00a0 \u00a0 \"safetyAttributes\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"structVal\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"categories\": {},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"scores\": {},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"blocked\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"boolVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 false\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \"citationMetadata\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"structVal\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"citations\": {}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \"content\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"stringVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 RESPONSE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ]}\n```\nThe current supported model is `codechat-bison` . See [available versions](/vertex-ai/generative-ai/docs/learn/model-versioning) .\n **Request** \n```\nPROJECT_ID=YOUR_PROJECT_IDPROMPT=\"PROMPT\"AUTHOR=\"USER\"MODEL_ID=codechat-bisoncurl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/publishers/google/models/${MODEL_ID}:serverStreamingPredict -d \\$'{\u00a0 \"inputs\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \u00a0 \"messages\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"list_val\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"string_val\": [ \"'\"${PROMPT}\"'\" ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"author\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"string_val\": [ \"'\"${AUTHOR}\"'\"]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ],\u00a0 \"parameters\": {\u00a0 \u00a0 \"struct_val\": {\u00a0 \u00a0 \u00a0 \"temperature\": { \"float_val\": 0.5 },\u00a0 \u00a0 \u00a0 \"maxOutputTokens\": { \"int_val\": 1024 },\u00a0 \u00a0 \u00a0 \"topK\": { \"int_val\": 40 },\u00a0 \u00a0 \u00a0 \"topP\": { \"float_val\": 0.95 }\u00a0 \u00a0 }\u00a0 }}'\n```\n **Response** \n```\n{\u00a0 \"outputs\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"structVal\": {\u00a0 \u00a0 \u00a0 \u00a0 \"candidates\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"listVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"structVal\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"stringVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 RESPONSE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"author\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"stringVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AUTHOR\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \"citationMetadata\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"listVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"structVal\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"citations\": {}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \"safetyAttributes\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"listVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"structVal\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"categories\": {},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"blocked\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"boolVal\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 false\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"scores\": {}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ]}\n```\n### Vertex AI SDK for Python\nFor information about installing the Vertex AI SDK for Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/install-sdk) .\n```\n\u00a0 import vertexai\u00a0 from vertexai.language_models import TextGenerationModel\u00a0 def streaming_prediction(\u00a0 \u00a0 \u00a0 project_id: str,\u00a0 \u00a0 \u00a0 location: str,\u00a0 ) -> str:\u00a0 \u00a0 \u00a0 \"\"\"Streaming Text Example with a Large Language Model\"\"\"\u00a0 vertexai.init(project=project_id, location=location)\u00a0 text_generation_model = TextGenerationModel.from_pretrained(\"text-bison\")\u00a0 parameters = {\u00a0 \u00a0 \u00a0 \"temperature\": temperature, \u00a0# Temperature controls the degree of randomness in token selection.\u00a0 \u00a0 \u00a0 \"max_output_tokens\": 256, \u00a0# Token limit determines the maximum amount of text output.\u00a0 \u00a0 \u00a0 \"top_p\": 0.8, \u00a0# Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\u00a0 \u00a0 \u00a0 \"top_k\": 40, \u00a0# A top_k of 1 means the selected token is the most probable among all tokens.\u00a0 }\u00a0 responses = text_generation_model.predict_streaming(prompt=\"Give me ten interview questions for the role of program manager.\", **parameters)\u00a0 for response in responses:\u00a0 \u00a0 \u00a0 `print(response)`\n```\n```\nimport vertexaifrom vertexai.language_models import ChatModel, InputOutputTextPairdef streaming_prediction(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,) -> str:\u00a0 \u00a0 \"\"\"Streaming Chat Example with a Large Language Model\"\"\"\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 chat_model = ChatModel.from_pretrained(\"chat-bison\")\u00a0 \u00a0 parameters = {\u00a0 \u00a0 \u00a0 \u00a0 \"temperature\": 0.8, \u00a0# Temperature controls the degree of randomness in token selection.\u00a0 \u00a0 \u00a0 \u00a0 \"max_output_tokens\": 256, \u00a0# Token limit determines the maximum amount of text output.\u00a0 \u00a0 \u00a0 \u00a0 \"top_p\": 0.95, \u00a0# Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\u00a0 \u00a0 \u00a0 \u00a0 \"top_k\": 40, \u00a0# A top_k of 1 means the selected token is the most probable among all tokens.\u00a0 \u00a0 }\u00a0 \u00a0 chat = chat_model.start_chat(\u00a0 \u00a0 \u00a0 \u00a0 context=\"My name is Miles. You are an astronomer, knowledgeable about the solar system.\",\u00a0 \u00a0 \u00a0 \u00a0 examples=[\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputOutputTextPair(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 input_text=\"How many moons does Mars have?\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output_text=\"The planet Mars has two moons, Phobos and Deimos.\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 )\u00a0 \u00a0 responses = chat.send_message_streaming(\u00a0 \u00a0 \u00a0 \u00a0 message=\"How many planets are there in the solar system?\", **parameters)\u00a0 \u00a0 for response in responses:\u00a0 \u00a0 \u00a0 \u00a0 `print(response)`\n```\n```\nimport vertexaifrom vertexai.language_models import CodeGenerationModeldef streaming_prediction(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,) -> str:\u00a0 \u00a0 \"\"\"Streaming Chat Example with a Large Language Model\"\"\"\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 code_model = CodeGenerationModel.from_pretrained(\"code-bison\")\u00a0 \u00a0 parameters = {\u00a0 \u00a0 \u00a0 \u00a0 \"temperature\": 0.8, \u00a0# Temperature controls the degree of randomness in token selection.\u00a0 \u00a0 \u00a0 \u00a0 \"max_output_tokens\": 256, \u00a0# Token limit determines the maximum amount of text output.\u00a0 \u00a0 }\u00a0 \u00a0 responses = code.predict_streaming(\u00a0 \u00a0 \u00a0 \u00a0 prefix=\"Write a function that checks if a year is a leap year.\", **parameters)\u00a0 \u00a0 for response in responses:\u00a0 \u00a0 \u00a0 \u00a0 `print(response)`\n```\n```\nimport vertexaifrom vertexai.language_models import CodeChatModeldef streaming_prediction(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,) -> str:\u00a0 \u00a0 \"\"\"Streaming Chat Example with a Large Language Model\"\"\"\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 codechat_model = CodeChatModel.from_pretrained(\"codechat-bison\")\u00a0 \u00a0 parameters = {\u00a0 \u00a0 \u00a0 \u00a0 \"temperature\": 0.8, \u00a0# Temperature controls the degree of randomness in token selection.\u00a0 \u00a0 \u00a0 \u00a0 \"max_output_tokens\": 1024, \u00a0# Token limit determines the maximum amount of text output.\u00a0 \u00a0 }\u00a0 \u00a0 codechat = codechat_model.start_chat()\u00a0 \u00a0 responses = codechat.send_message_streaming(\u00a0 \u00a0 \u00a0 \u00a0 message=\"Please help write a function to calculate the min of two numbers\", **parameters)\u00a0 \u00a0 for response in responses:\u00a0 \u00a0 \u00a0 \u00a0 `print(response)`\n```\n## Available client libraries\nYou can use one of the following [client libraries](/vertex-ai/docs/start/client-libraries) to stream responses:\n- Python\n- Node.js\n- Java\nTo view sample code requests and responses using the REST API, see [Examples using the REST API](#rest) .\nTo view sample code requests and responses using the Vertex AI SDK for Python, see [Examples using Vertex AI SDK for Python](#sdk) .\n## Responsible AI\n[Responsible Artificial Intelligence (RAI) filters](/vertex-ai/generative-ai/docs/learn/responsible-ai#safety_filters_and_attributes) scan the streaming output as the model generates it. If a violation is detected, the filters block the offending output tokens, and return an output with a blocked flag under `safetyAttributes` , which terminates the stream.\n## What's next\n- Learn about [designing text prompts](/vertex-ai/generative-ai/docs/text/text-prompts) . and [text chat prompts](/vertex-ai/generative-ai/docs/chat/chat-prompts) .\n- Learn how to test prompts in [Vertex AI Studio](/vertex-ai/generative-ai/docs/start/quickstarts/quickstart) .\n- Learn about [text embeddings](/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) .\n- Try to [tune a language foundation model](/vertex-ai/generative-ai/docs/start/quickstarts/quickstart-tuning) .\n- Learn about [responsible AI best practices](/vertex-ai/generative-ai/docs/learn/responsible-ai#recommended_practices) and [Vertex AI's safety filters](/vertex-ai/generative-ai/docs/learn/responsible-ai#safety_filters_and_attributes) .", "guide": "Generative AI on Vertex AI"}