{"title": "Generative AI on Vertex AI - Learn about LLMs, Gemini models, PaLM models, and Vertex AI", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/learn-resources", "abstract": "# Generative AI on Vertex AI - Learn about LLMs, Gemini models, PaLM models, and Vertex AI\nLarge language models (LLMs) are deep learning models trained on massive amounts of text data. LLMs can translate language, summarize text, recognize objects and text in images, and complement search engines and recommendation systems. Google provides the following models:\n- [Gemini](/vertex-ai/generative-ai/docs/multimodal/overview) is a family of generative AI models designed for use cases; capable of processing information from multiple modalities, including images, videos, and text.\n- [PaLM 2](https://ai.google/discover/palm2/) builds on Google's legacy of research in machine learning and [responsible AI](/vertex-ai/generative-ai/docs/learn/responsible-ai) .PaLM 2 models excel at advanced reasoning tasks, classification and question answering, translation, and natural language generation. Its large size enables it to learn complex patterns and relationships in language and generate high-quality text for various applications.\nTo see more learning resources, browse the [Generative AI GitHub repo](https://github.com/GoogleCloudPlatform/generative-ai) . Google data scientists, developers, and developer advocates manage this content.\n", "content": "## Get started\nHere are some notebooks, tutorials, and other examples to help you get started. Vertex AI offers Google Cloud console tutorials and Jupyter notebook tutorials that use the Vertex AI SDK for Python. You can open a notebook tutorial in Colab or download the notebook to your preferred environment.\n### Get started with Gemini\n| 0 | 1                                                                                           |\n|----:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| nan | The Gemini model is a groundbreaking multimodal language model developed by Google AI, capable of extracting meaningful insights from a diverse array of data formats, including images, and video. This notebook explores various use cases with multimodal prompts. Jupyter notebook: You can run this tutorial as a Jupyter notebook. Run in Colab | View on GitHub |### Get started with the Vertex AI PaLM API & Vertex AI SDK for Python\n| 0 | 1                                                                                       |\n|----:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| nan | Learn how to use the PaLM API with the Vertex AI SDK for Python. By the end of the notebook, you should understand various nuances of generative model parameters like temperature, top_k, top_p, and how each parameter affects your output results. Jupyter notebook: You can run this tutorial as a Jupyter notebook. Run in Colab | View on GitHub |### Get started with Vertex AI Vertex AI Studio\n| 0 | 1                                 |\n|----:|:-------------------------------------------------------------------------------------------------------------------------------------|\n| nan | Use Vertex AI Studio through the Google Cloud console without the need for the API or the Vertex AI SDK for Python. View on GitHub |### Best practices for prompt design\n| 0 | 1                                                                     |\n|----:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| nan | Learn how to design prompts to improve the quality of your responses from the model. This tutorial covers the essentials of prompt engineering, including some best practices. Jupyter notebook: You can run this tutorial as a Jupyter notebook. Run in Colab | View on GitHub |\n## LangChain \ud83e\udd9c\ufe0f\ud83d\udd17\n[LangChain](https://python.langchain.com/docs/get_started) is a framework for developing applications powered by LLMs like the [PaLM models](/vertex-ai/generative-ai/docs/learn/overview#palm-api) . Use LangChain to bring external data, such as your files, other applications, and API data, to your LLMs.\nTo learn more about LangChain and how it works with Vertex AI, see the [official LangChain and Vertex AI documentation](https://python.langchain.com/docs/integrations/llms/google_vertex_ai_palm) .\n### LangChain and Vertex AI PaLM API\n| 0 | 1                                                                                                                 |\n|----:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| nan | This tutorial provides an introduction to understanding LangChain components and some common use cases for working with LangChain and the Vertex AI PaLM API. Some examples and demos in this tutorial include: How LangChain and the Vertex AI PaLM API work How to summarize large texts How to build a retrieval-based question/answering model from PDFs Jupyter notebook: You can run this tutorial as a Jupyter notebook. Run in Colab | View on GitHub |### Get text summarization from large documents using LangChain\n| 0 | 1                                                                                                                            |\n|----:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| nan | Text summarization is a natural language processing (NLP) task that creates a concise and informative summary of a longer text. You can use LLMs to create summaries of news articles, research papers, technical documents, and other types of text. In this notebook, you use LangChain to apply summarization strategies. The notebook covers several examples of how to summarize large documents. Jupyter notebook: You can run this tutorial as a Jupyter notebook. Run in Colab | View on GitHub |### Answer questions from large documents with LangChain\n| 0 | 1                                                             |\n|----:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| nan | This notebook uses LangChain with Vertex AI PaLM API to build a question-answering (Q&A) system that extracts information from large documents. Jupyter notebook: You can run this tutorial as a Jupyter notebook. Run in Colab | View on GitHub |### Answer questions from documents with LangChain and Vector Search\n| 0 | 1                                                                                                      |\n|----:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| nan | This notebook shows how to implement a question & answering (QA) system that improves an LLM response. You learn how to augment its knowledge with external data sources such as documents and websites. This notebook uses Vector Search, LangChain, and Vertex AI PaLM API for text and embedding creation, . Jupyter notebook: You can run this tutorial as a Jupyter notebook. Run in Colab | View on GitHub |\n## What's next\n- Explore more resources in the [Generative AI GitHub repo](https://github.com/GoogleCloudPlatform/generative-ai) .\n- See other Vertex AI notebook tutorials in the [Tutorials overview](/vertex-ai/generative-ai/docs/tutorials) .", "guide": "Generative AI on Vertex AI"}