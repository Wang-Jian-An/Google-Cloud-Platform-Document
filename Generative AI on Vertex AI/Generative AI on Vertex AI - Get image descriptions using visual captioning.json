{"title": "Generative AI on Vertex AI - Get image descriptions using visual captioning", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/image/image-captioning", "abstract": "# Generative AI on Vertex AI - Get image descriptions using visual captioning\nTo see an example of visual captioning using Imagen,  run the \"Visual captioning with Imagen on Vertex AI\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/visual_captioning.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fvision%2Fgetting-started%2Fvisual_captioning.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/visual_captioning.ipynb)\nVisual captioning lets you generate a relevant description for an image. You can use this information for a variety of uses:\n- Get more detailed metadata about images for storing and searching.\n- Generate automated captioning to support accessibility use cases.\n- Receive quick descriptions of products and visual assets.**Image source** : [Santhosh Kumar](https://unsplash.com/photos/RqYTuWkTdEs) on [Unsplash](https://unsplash.com/) (cropped)\n**Caption (short-form)** :\n", "content": "## Supported languages\nVisual captioning is available in the following languages:\n- English (`en`)\n- French (`fr`)\n- German (`de`)\n- Italian (`it`)\n- Spanish (`es`)## Performance and limitations\nThe following limits apply when you use this model:\n| Limits                | Value  |\n|:-------------------------------------------------------------------|:----------|\n| Maximum number of API requests (short-form) per minute per project | 500  |\n| Maximum number of tokens returned in response (short-form)   | 64 tokens |\n| Maximum number of tokens accepted in request (VQA short-form only) | 80 tokens |\nThe following service latency estimates apply when you use this model. These values are meant to be illustrative and are not a promise of service:\n| Latency     | Value  |\n|:--------------------------|:------------|\n| API requests (short-form) | 1.5 seconds |\n## Locations\nA location is a [region](/about/locations) you can specify in a request to control where data is stored at rest. For a list of available regions, see [Generative AI on Vertex AI locations](/vertex-ai/generative-ai/docs/learn/locations-genai) .\n## Get short-form image captions\nUse the following samples to generate short-form captions for an image.\n- In the Google Cloud console, open the **Vertex AI Studio > Vision** tab in the Vertex AI dashboard. [Go to the Vertex AI Studio tab](https://console.cloud.google.com/vertex-ai/generative/vision) \n- In the lower menu, click **Caption** .\n- Click **Upload image** to select your local image to caption.\n- In the **Parameters** panel, choose your **Number of captions** and **Language** .\n- Click play_arrow **Generate caption** .\nFor more information about `imagetext` model requests, see the [imagetext model API reference](/vertex-ai/generative-ai/docs/model-reference/image-captioning) .\nBefore using any of the request data, make the following replacements:- : Your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : Your project's region. For example,`us-central1`,`europe-west2`, or`asia-northeast3`. For a list of available regions, see [Generative AI on Vertex AI locations](/vertex-ai/generative-ai/docs/learn/locations-genai) .\n- : The image to get captions for. The image must be  specified as a [base64-encoded](/vertex-ai/generative-ai/docs/image/base64-encode) byte string. Size limit:  10 MB.\n- : The number of image captions you want to generate. Accepted integer values: 1-3.\n- : One of the supported language codes. Languages supported:- English (`en`)\n- French (`fr`)\n- German (`de`)\n- Italian (`it`)\n- Spanish (`es`)\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/imagetext:predict\n```\nRequest JSON body:\n```\n{\n \"instances\": [ {\n  \"image\": {\n   \"bytesBase64Encoded\": \"B64_IMAGE\"\n  }\n }\n ],\n \"parameters\": {\n \"sampleCount\": RESPONSE_COUNT,\n \"language\": \"LANGUAGE_CODE\"\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/imagetext:predict\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/imagetext:predict\" | Select-Object -Expand Content\n```The following sample responses are for a request with\n`\"sampleCount\": 2`\n. The response returns two prediction strings.\n **English (en):** \n```\n{\n \"predictions\": [ \"a yellow mug with a sheep on it sits next to a slice of cake\",\n \"a cup of coffee with a heart shaped latte art next to a slice of cake\"\n ],\n \"deployedModelId\": \"DEPLOYED_MODEL_ID\",\n \"model\": \"projects/PROJECT_ID/locations/LOCATION/models/MODEL_ID\",\n \"modelDisplayName\": \"MODEL_DISPLAYNAME\",\n \"modelVersionId\": \"1\"\n}\n```\n **Spanish (es):** \n```\n{\n \"predictions\": [ \"una taza de caf\u00e9 junto a un plato de pastel de chocolate\",\n \"una taza de caf\u00e9 con una forma de coraz\u00f3n en la espuma\"\n ],\n \"deployedModelId\": \"DEPLOYED_MODEL_ID\",\n \"model\": \"projects/PROJECT_ID/locations/LOCATION/models/MODEL_ID\",\n \"modelDisplayName\": \"MODEL_DISPLAYNAME\",\n \"modelVersionId\": \"1\"\n}\n```\nBefore trying this sample, follow the Python setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Python API reference documentation](/python/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nIn this sample you use the `load_from_file` method to reference a local file as the base [Image](/python/docs/reference/aiplatform/latest/vertexai.vision_models.Image) to get a caption for. After you specify the base image, you use the [get_captions](/python/docs/reference/aiplatform/latest/vertexai.vision_models.ImageTextModel#vertexai_vision_models_ImageTextModel_get_captions) method on the [ImageTextModel](/python/docs/reference/aiplatform/latest/vertexai.vision_models.ImageTextModel) and print the output.\n [  generative_ai/imagen/get_short_form_image_captions.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/imagen/get_short_form_image_captions.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/imagen/get_short_form_image_captions.py) \n```\nimport argparseimport vertexaifrom vertexai.preview.vision_models import Image, ImageTextModeldef get_short_form_image_captions(\u00a0 \u00a0 project_id: str, location: str, input_file: str) -> list:\u00a0 \u00a0 \"\"\"Get short-form captions for a local image.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 project_id: Google Cloud project ID, used to initialize Vertex AI.\u00a0 \u00a0 \u00a0 location: Google Cloud region, used to initialize Vertex AI.\u00a0 \u00a0 \u00a0 input_file: Local path to the input image file.\"\"\"\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 model = ImageTextModel.from_pretrained(\"imagetext@001\")\u00a0 \u00a0 source_img = Image.load_from_file(location=input_file)\u00a0 \u00a0 captions = model.get_captions(\u00a0 \u00a0 \u00a0 \u00a0 image=source_img,\u00a0 \u00a0 \u00a0 \u00a0 # Optional parameters\u00a0 \u00a0 \u00a0 \u00a0 language=\"en\",\u00a0 \u00a0 \u00a0 \u00a0 number_of_results=1,\u00a0 \u00a0 )\u00a0 \u00a0 print(captions)\u00a0 \u00a0 return captions\n```\n## Use parameters for image captioning\nWhen you get image captions there are several parameters you can set depending on your use case.\n### Number of results\nUse the number of results parameter to limit the amount of captions returned for each request you send. For more information, see the [imagetext (image captioning) model API reference](/vertex-ai/generative-ai/docs/model-reference/image-captioning#request_body) .\n### Seed number\nA number you add to a request to make generated descriptions deterministic. Adding a seed number with your request is a way to assure you get the same prediction (descriptions) each time. However, the image captions aren't necessarily returned in the same order. For more information, see the [imagetext (image captioning) model API reference](/vertex-ai/generative-ai/docs/model-reference/image-captioning#request_body) .\n## What's next\n- View videos describing Vertex AI foundation models including Imagen, the text-to-image foundation model that lets you generate and edit images:- [Introduction to foundation models on Google Cloud](https://youtu.be/YCZ6nwGnL4o) \n- [Imagen on Vertex AI: Create and edit images from text](https://youtu.be/pN-RTBq6i3I) \n- Read blog posts describing Imagen on Vertex AI and Generative AI on Vertex AI:- [Imagen\u00a02 on Vertex AI is now generally available](/blog/products/ai-machine-learning/imagen-2-on-vertex-ai-is-now-generally-available) \n- [Gemini, Google's most capable model, is now available on Vertex AI](/blog/products/ai-machine-learning/gemini-support-on-vertex-ai) \n- [Announcing General Availability of Duet AI for Developers and Duet AI in Security Operations](https://cloud.google.com/blog/products/ai-machine-learning/duet-ai-for-developers-and-in-security-operations-now-ga)", "guide": "Generative AI on Vertex AI"}