{"title": "Generative AI on Vertex AI - Overview of multimodal models", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview", "abstract": "# Generative AI on Vertex AI - Overview of multimodal models\n- [Try the Gemini API](/vertex-ai/generative-ai/docs/start/quickstarts/quickstart-multimodal) After you're set up on Google Cloud, try some multimodal prompts in the  Vertex AI Studio, or in a notebook tutorial by using the Python SDK or REST  API.\n- [Vertex AI Gemini API reference](/vertex-ai/generative-ai/docs/model-reference/gemini) Learn about the endpoints, parameters, and return values of the Vertex AI  Gemini API.\n- [Multimodal prompt design](/vertex-ai/generative-ai/docs/multimodal/design-multimodal-prompts) Learn best practices for designing multimodal prompts and see example  prompts.\n", "content": "## Multimodal models\n| 0         | 1                                                        |\n|:------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Prompt        | Response                                                      |\n| Give me a recipe for these cookies. | **INGREDIENTS** - 1 c. (2 sticks) unsalted butter, softened - 3/4 c. granulated sugar - 3/4 c. brown sugar, packed - 1 tsp . vanilla extract - 2 large eggs - 2 1/4 c. all-purpose flour - 1 tsp. baking soda - 1 tsp. salt ... |A is a model that is capable of processing  information from multiple modalities, including images, videos, and text.  For example, you can send the model a photo of a plate of cookies and ask  it to give you a recipe for those cookies.\n## Vertex AI Gemini API\n is a family of generative AI models developed by Google  DeepMind that is designed for multimodal use cases. The  Gemini API gives you access to the  Gemini 1.0 Pro Vision and Gemini 1.0 Pro  models. For specifications of the Vertex AI Gemini API models, see [Model information](/vertex-ai/generative-ai/docs/learn/models#gemini-models) .\n### Supported use cases\nThe Vertex AI Gemini API supports the following use cases by model:\n- **Gemini 1.0 Pro:** Designed to handle natural language tasks, multiturn text and code chat, and code generation.\n- **Gemini 1.0 Pro Vision:** Supports multimodal prompts. You can include text, images, and video in your prompt requests and get text or code responses.Gemini 1.0 Pro Vision excels at a wide variety of multimodal use cases, including but not limited to the use cases described in the following table:\n| Use Case      | Description                           |\n|:------------------------------|:--------------------------------------------------------------------------------------------------------------------|\n| Info seeking     | Combine world knowledge with information extracted from the images and videos.          |\n| Object recognition   | Answer questions related to fine-grained identification of the objects in images and videos.      |\n| Digital content understanding | Answer questions by extracting information from content like infographics, charts, figures, tables, and web pages. |\n| Structured content generation | Generate responses in formats like HTML and JSON based on provided prompt instructions.       |\n| Captioning / description  | Generate descriptions of images and videos with varying levels of detail.           |\n| Extrapolation     | Make guesses about what's not shown in an image or what happens before or after a video.       |\nSee also: [Model strengths and limitations](/vertex-ai/generative-ai/docs/multimodal/strengths-limits)\n### Programming language SDKs\nThe Vertex AI Gemini API supports the following SDKs:\n```\nfrom vertexai import generative_modelsfrom vertexai.generative_models import GenerativeModelmodel = GenerativeModel(model_name=\"gemini-1.0-pro-vision\")response = model.generate_content([\"What is this?\", img])\n```\n```\n// Initialize Vertex AI with your Cloud project and locationconst vertexAI = new VertexAI({project: projectId, location: location});const generativeVisionModel = vertexAI.getGenerativeModel({ model: \"gemini-1.0-pro-vision\"});const result = await model.generateContent([\u2003\u2003\"What is this?\",\u2003\u2003{inlineData: {data: imgDataInBase64, mimeType: 'image/png'}}]);\n```\n```\npublic static void main(String[] args) throws Exception {\u00a0 try (VertexAI vertexAi = new VertexAI(PROJECT_ID, LOCATION); ) {\u00a0 \u00a0 GenerativeModel model = new GenerativeModel(\"gemini-1.0-pro-vision\", vertexAI);\u00a0 List<Content> contents = new ArrayList<>();\u00a0 contents.add(ContentMaker\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .fromMultiModalData(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"What is this?\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PartMaker.fromMimeTypeAndData(\"image/jpeg\", IMAGE_URI)));\u00a0 GenerateContentResponse response = model.generateContent(contents);\u00a0 \u00a0 }\u00a0 }}\n```\n```\nmodel := client.GenerativeModel(\"gemini-1.0-pro-vision\", \"us-central1\")img := genai.ImageData(\"jpeg\", image_bytes)prompt := genai.Text(\"What is this?\")resp, err := model.GenerateContent(ctx, img, prompt)\n```\n### What's the difference from Google AI Gemini API\nThe Vertex AI Gemini API and Google AI Gemini API both let you incorporate the capabilities of Gemini models into your applications. The platform that's right for you depends on your goals.\nThe Vertex AI Gemini API is designed for developers and enterprises for use in scaled deployments. It offers features such as enterprise security, data residency, performance, and technical support. If you're an existing Google Cloud customer or deploy medium to large scale applications, you're in the right place.\nIf you're a hobbyist, student, or developer who is new to Google Cloud, try the [Google AI Gemini API](https://ai.google.dev/docs) , which is suitable for experimentation, prototyping, and small deployments. If you're looking for a way to use Gemini directly from your mobile and web apps, see the Google AI SDKs for Android, Swift, and web.\n## Vertex AI Gemini API documentation\nSelect one of the following topics to learn more about the Vertex AI Gemini API.\n### Get started with the Vertex AI Gemini API- [Get set up on Google Cloud](/vertex-ai/docs/start/cloud-environment) If you're new to Google Cloud, follow the setup steps in this page to get  started quickly.\n- [Python SDK classes for Gemini API](/vertex-ai/generative-ai/docs/multimodal/sdk-for-gemini/gemini-sdk-overview) Learn about the classes provided by the Python SDK for the Vertex AI  Gemini API, including attributes, methods, and usage examples.\n- [Python SDK reference](/python/docs/reference/aiplatform/latest/vertexai.language_models) See the full generative AI reference for the Vertex AI SDK for Python.### Migrate to the Vertex AI Gemini API- [Migrate from Google AI to Vertex AI](/vertex-ai/generative-ai/docs/migrate/migrate-google-ai) Learn how to migrate your python code from Google AI Gemini API to Vertex  AI Gemini API.\n- [Migrate from PaLM API to Gemini API](/vertex-ai/generative-ai/docs/migrate/migrate-palm-to-gemini) Learn how to migrate your Python code from the Vertex AI PaLM API to the  Vertex AI Gemini API.### Learn how to use core features- [Send multimodal prompt requests](/vertex-ai/generative-ai/docs/multimodal/send-multimodal-prompts) Learn how to send multimodal prompt requests by using the Cloud Console,  Python SDK, or the REST API.\n- [Send chat prompt requests](/vertex-ai/generative-ai/docs/multimodal/send-chat-prompts-gemini) Learn how to send single-turn and multi-turn chat prompts by using the  Cloud Console, Python SDK, or the REST API.\n- [Call functions](/vertex-ai/generative-ai/docs/multimodal/function-calling) Learn how to get the model to output JSON for calling external functions.", "guide": "Generative AI on Vertex AI"}