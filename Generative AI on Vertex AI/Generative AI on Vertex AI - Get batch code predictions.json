{"title": "Generative AI on Vertex AI - Get batch code predictions", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/code/batch-prediction-genai-code", "abstract": "# Generative AI on Vertex AI - Get batch code predictions\nGetting responses in a batch is a way to efficiently send large numbers of code requests where the latency of the response is not important. Different from getting online responses, where you are limited to one input request at a time, batch predictions send a large number of code generation model requests in a single batch request. Like batch predictions for [tabular data inVertex AI](/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions) , you determine your output location and add your input, then the responses asynchronously populate to your output location.\nAfter you submit a batch request and review its results, you can tune the code generation foundation model to improve how well the results work for your particular task. After tuning, you can submit the tuned model for batch generations. To learn more about tuning models, see [Tune language foundation models](/vertex-ai/generative-ai/docs/models/tune-code-models) .\n", "content": "## Code models that support batch predictions\n- `code-bison`## Prepare your inputs\nThe input for batch requests is a list of prompts that can be stored in a BigQuery table or as a [JSON Lines (JSONL)](https://jsonlines.org/) file in Cloud Storage. Each request can include up to 30,000 prompts.\n### JSONL examples\nThis section shows examples of how to format input and output JSONL files.\n```\n{\"prefix\":\"Write a Python function that determines if a year is a leap year:\"}{\"prefix\":\"Write a unit test for Python code that reverses a string:\"}\n```\n```\n{\"instance\":{\"prefix\":\"Write...\"},\"predictions\": [{\"content\":\"def is_leap_year(year):...\",\"safetyAttributes\":{...}}],\"status\":\"\"}{\"instance\":{\"prefix\":\"Write...\"},\"predictions\": [{\"content\":\"import unittest...\", \"safetyAttributes\":{...}}],\"status\":\"\"}\n```\n### BigQuery example\nThis section shows examples of how to format BigQuery input and output.\nThis example shows a single column BigQuery table.\n| prefix                |\n|:--------------------------------------------------------------------|\n| \"Write a Python function that determines if a year is a leap year:\" |\n| \"Write a unit test for Python code that reverses a string:\"   |\n| prefix                | predictions                                                                                                                              | status |\n|:--------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------:|\n| \"Write a Python function that determines if a year is a leap year:\" | { \"predictions\": [ { \"safetyAttributes\": { \"scores\": [], \"blocked\": false, \"categories\": [] }, \"content\": \"```python\\ndef is_leap_year(year):\\n \\\"\\\"\\\"\\n Determine if a year is a leap year.\\n\\n Args:\\n year: The year to check.\\n\\n Returns:\\n True if the year is a leap year, False otherwise.\\n \\\"\\\"\\\"\\n\\n if year % 4 != 0:\\n return False\\n\\n if year % 100 == 0 and year % 400 != 0:\\n return False\\n\\n return True\\n```\", \"citationMetadata\": { \"citations\": [] }, \"score\": -1.5572503805160522 } ], }  |  nan |\n| \"Write a unit test for Python code that reverses a string:\"   | { \"predictions\": [ { \"safetyAttributes\": { \"scores\": [], \"blocked\": false, \"categories\": [] }, \"score\": -1.7523338794708252, \"citationMetadata\": { \"citations\": [] }, \"content\": \"```python\\nimport unittest\\n\\nclass TestReverseString(unittest.TestCase):\\n\\n def test_reverse_string(self):\\n input_string = \\\"Hello World\\\"\\n expected_output = \\\"dlroW olleH\\\"\\n output = reverse_string(input_string)\\n self.assertEqual(output, expected_output)\\n\\nif __name__ == '__main__':\\n unittest.main()\\n```\" } ], } |  nan |\n## Request a batch response\nYou can create a code generation batch response by using the Google Cloud console or the Vertex AI SDK for Python. The more input items you submit, the longer the batch generation process takes to complete.\nTo test a code prompt by using the Vertex AI API, send a POST request to the publisher model endpoint.\nBefore using any of the request data, make the following replacements:- : The name of your Google Cloud project.\n- : The job name.\n- : A list of key-value pairs that specify model parameters and  their values. For example, you can specify the model's`maxOutputTokens`and`temperature`. For more  information, see [Code generation parameters](/vertex-ai/generative-ai/docs/model-reference/code-generation#request_body) .\n- : The input source URI. The input source is a BigQuery table or a JSONL  file in a Cloud Storage bucket.\n- : Output target URI.\nHTTP method and URL:\n```\nPOST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/batchPredictionJobs\n```\nRequest JSON body:\n```\n{\n \"name\": \"BP_JOB_NAME\",\n \"displayName\": \"BP_JOB_NAME\",\n \"model\": \"publishers/google/models/text-bison\",\n \"model_parameters\": \"MODEL_PARAM\"\n \"inputConfig\": {\n  \"instancesFormat\":\"bigquery\",\n  \"bigquerySource\":{\n  \"inputUri\" : \"INPUT_URI\"\n  }\n },\n \"outputConfig\": {\n  \"predictionsFormat\":\"bigquery\",\n  \"bigqueryDestination\":{\n  \"outputUri\": \"OUTPUT_URI\"\n }\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/batchPredictionJobs\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/batchPredictionJobs\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/{PROJECT_ID}/locations/us-central1/batchPredictionJobs/{BATCH_JOB_ID}\",\n \"displayName\": \"BP_sample_publisher_BQ_20230712_134650\",\n \"model\": \"projects/{PROJECT_ID}/locations/us-central1/models/text-bison\",\n \"inputConfig\": {\n \"instancesFormat\": \"bigquery\",\n \"bigquerySource\": {\n  \"inputUri\": \"bq://sample.text_input\"\n }\n },\n \"modelParameters\": {},\n \"outputConfig\": {\n \"predictionsFormat\": \"bigquery\",\n \"bigqueryDestination\": {\n  \"outputUri\": \"bq://sample.llm_dataset.embedding_out_BP_sample_publisher_BQ_20230712_134650\"\n }\n },\n \"state\": \"JOB_STATE_PENDING\",\n \"createTime\": \"2023-07-12T20:46:52.148717Z\",\n \"updateTime\": \"2023-07-12T20:46:52.148717Z\",\n \"labels\": {\n \"owner\": \"sample_owner\",\n \"product\": \"llm\"\n },\n \"modelVersionId\": \"1\",\n \"modelMonitoringStatus\": {}\n}\n```\nThe response includes a unique identifier for the batch job. You can poll for the status of the batch job using the until the job `state` is `JOB_STATE_SUCCEEDED` . For example:\n```\ncurl \\\u00a0 -X GET \\\u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 -H \"Content-Type: application/json\" \\https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/batchPredictionJobs/BATCH_JOB_ID\n```\n **Note:** You can run only one batch response job at a time. Custom Service account, live progress, CMEK, and VPCSC reports are not supported at this time.\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n```\nfrom vertexai.preview.language_models import CodeGenerationModelcode_model = CodeGenerationModel.from_pretrained(\"code-bison\")batch_prediction_job = code_model.batch_predict(\u00a0 dataset=[\"gs://BUCKET_NAME/test_table.jsonl\"],\u00a0 destination_uri_prefix=\"gs://BUCKET_NAME/tmp/2023-05-25-vertex-LLM-Batch-Prediction/result3\",\u00a0 # Optional:\u00a0 model_parameters={\u00a0 \u00a0 \u00a0 \"maxOutputTokens\": \"200\",\u00a0 \u00a0 \u00a0 \"temperature\": \"0.2\",\u00a0 },)print(batch_prediction_job.display_name)print(batch_prediction_job.resource_name)print(batch_prediction_job.state)\n```\n## Retrieve batch output\nAfter a batch prediction task is complete, the output is stored in the Cloud Storage bucket or BigQuery table that you specified in your request.\n## What's next\n- Learn how to [test code prompts](/vertex-ai/generative-ai/docs/code/test-code-generation-prompts) .\n- See the [Code generation](/vertex-ai/generative-ai/docs/model-reference/code-generation) reference page.", "guide": "Generative AI on Vertex AI"}