{"title": "Generative AI on Vertex AI - Perform automatic side-by-side evaluation", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/side-by-side-eval", "abstract": "# Generative AI on Vertex AI - Perform automatic side-by-side evaluation\n**Preview** This feature is a Preview offering, subject to the Pre-GA Offerings Terms  of the [GCP Service Specific Terms](https://cloud.google.com/terms/service-terms) . Pre-GA products and features may have limited support, and changes to pre-GA  products and features may not be compatible with other pre-GA versions. For more information,  see the [launch stage descriptions](https://cloud.google.com/products#product-launch-stages) .  Further, by using the PaLM API on Vertex AI, you agree to the Generative AI Preview [ terms and conditions ](https://cloud.google.com/trustedtester/aitos?hl=en) (Preview Terms).For PaLM APIs on Vertex AI that are not GA, you can process personal data as outlined in the  Cloud Data Processing Addendum, subject to applicable restrictions and obligations in the  Agreement (as defined in the Preview Terms).\nTo see an example of using AutoSxS for evaluation,  run the \"AutoSxS: Evaluate a LLM in Vertex AI Model Registry against a third-party model\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_evaluation/model_based_llm_evaluation/autosxs_llm_evaluation_for_summarization_task.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fmodel_evaluation%2Fmodel_based_llm_evaluation%2Fautosxs_llm_evaluation_for_summarization_task.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_evaluation/model_based_llm_evaluation/autosxs_llm_evaluation_for_summarization_task.ipynb)\nThis page introduces AutoSxS, a tool for evaluating models relative to each other, and explains how you can use it through the Vertex AI API or Vertex AI SDK for Python.\n", "content": "## AutoSxS\nAutomatic side-by-side (AutoSxS) is a model-assisted evaluation tool that compares two large language models (LLMs) side by side. AutoSxS can be used to evaluate the performance of either generative AI models in [Vertex AIModel Registry](https://console.cloud.google.com/vertex-ai/) or pre-generated predictions, which allows it to support Vertex AI foundation models, tuned generative AI models, and third-party language models. AutoSxS uses an autorater to decide which model gives the better response to a prompt. It's available on demand and evaluates language models with comparable performance to human raters.\n### The autorater\nAt a high level, the diagram shows how AutoSxS compares the predictions of models A and B with a third model, the autorater.\nModels A and B receive input prompts, and each model generates responses that are sent to the autorater. Similar to a human rater, an autorater is a language model that judges the quality of model responses given an original inference prompt. With AutoSxS, the autorater compares the quality of two model responses given their inference instruction by using a [set ofcriteria](#tasks-and-criteria) . The criteria are used to determine which model performed the best by comparing Model A's results with Model B's results. The autorater outputs response preferences as [aggregatemetrics](#aggregate-metrics) and outputs preference explanations and confidence scores for each example. For more information, see the [judgmenttable](#judgments) .\nUsing AutoSxS, you can achieve the following benefits:\n- Evaluate natural language models without human preference data.\n- Achieve better scale, increase availability, and reduce cost compared to evaluating language models with human raters.\n- Achieve rating transparency by capturing preference explanations and confidence scores.## Supported models\nAutoSxS supports evaluation of any model when pre-generated predictions are provided. AutoSxS also supports automatically generating responses for any model in [Vertex AI ModelRegistry](https://console.cloud.google.com/vertex-ai/) that supports batch prediction on Vertex AI.\nIf your [Text model](/vertex-ai/generative-ai/docs/model-reference/text) isn't supported by [Vertex AI ModelRegistry](https://console.cloud.google.com/vertex-ai/) , AutoSxS also accepts pre-generated predictions stored as JSONL in Cloud Storage or a BigQuery table. For pricing, see [Textgeneration](/vertex-ai/generative-ai/pricing#text_generation) .\n## Supported tasks and criteria\nAutoSxS supports evaluating models for summarization and question-answer tasks. The evaluation criteria are predefined for each task, which make language evaluation more objective and improve response quality.\nThe criteria are listed by task.\nThe `summarization@001` task has a 4,096 input [tokenlimit](/vertex-ai/generative-ai/docs/compute-token) .\nThe list of evaluation criteria for `summarization@001` is as follows:\n| Criteria    | Unnamed: 1                        |\n|:------------------------|:----------------------------------------------------------------------------------------------------------|\n| 1. Follows instructions | To what extent does the model's response demonstrate an understanding of the instruction from the prompt? |\n| 2. Grounded    | Does the response include only information from the inference context and inference instruction?   |\n| 3. Comprehensive  | To what extent does the model capture key details in the summarization?         |\n| 4. Brief    | Is the summarization verbose? Does it include flowery language? Is it overly terse?      |\nThe `question_answer@001` task has a 4,096 input [tokenlimit](/vertex-ai/generative-ai/docs/compute-token) .\nThe list of evaluation criteria for `question_answer@001` is as follows:\n| Criteria      | Unnamed: 1                       |\n|:------------------------------|:----------------------------------------------------------------------------------------------------|\n| 1. Fully answers the question | The answer responds to the question, completely.             |\n| 2. Grounded     | Does the response include only information from the instruction context and inference instruction? |\n| 3. Relevance     | Does the content of the answer relate to the question?            |\n| 4. Comprehensive    | To what extent does the model capture key details in the question?         |\n## Prepare evaluation dataset\nThis section details the data you should provide in your evaluation dataset and best practices for dataset construction. The examples should mirror real-world inputs that your models might encounter in production and best contrast how your live models behave.\n### Dataset format\nAutoSxS accepts a single evaluation dataset with a flexible schema. The dataset can be a BigQuery table or stored as [JSON Lines](https://jsonlines.org/) in Cloud Storage.\nEach row of the evaluation dataset represents a single example, and the columns are one of the following:\n- **ID columns** : Used to identify each unique example.\n- **Data columns** : Used to fill out prompt templates. See [Prompt parameters](#prompt-params) \n- **Pre-generated predictions** : Predictions made by the same model using the same prompt. Using pre-generated predictions saves time and resources.\n- **Ground-truth human preferences** : Used to benchmark AutoSxS against your ground-truth preference data when pre-generated predictions are provided for both models.\nHere is an example evaluation dataset where `context` and `question` are data columns, and `model_b_response` contains pre-generated predictions.\n| context                          | question      | model_b_response              |\n|:---------------------------------------------------------------------------------------------------------------|:------------------------------|:----------------------------------------------------------------------|\n| Some might think that steel is the hardest material or titanium, but diamond is actually the hardest material. | What is the hardest material? | Diamond is the hardest material. It is harder than steel or titanium. |\nFor more information on how to call AutoSxS, see [Perform modelevaluation](#perform-eval) . For details about token length, see [Supported tasksand criteria](#tasks-and-criteria) . To upload your data to Cloud Storage, see [Upload evaluation dataset toCloud Storage](/vertex-ai/generative-ai/docs/models/evaluate-models#upload-to-storage) .\n### Prompt parameters\nMany language models take prompt parameters as inputs instead of a single prompt string. For example, [chat-bison](/vertex-ai/generative-ai/docs/model-reference/text-chat#request_body) takes several prompt parameters ( ), which make up pieces of the prompt. However, [text-bison](/vertex-ai/generative-ai/docs/model-reference/text) has only one prompt parameter, named **prompt** , which contains the entire prompt.\nWe outline how you can flexibly specify model prompt parameters at inference and evaluation time. AutoSxS gives you the flexibility to call language models with varying expected inputs through templated prompt parameters.\nIf any of the models don't have pre-generated predictions, AutoSxS uses Vertex AI batch prediction to generate responses. Each model's prompt parameters must be specified.\nIn AutoSxS, you can provide a single column in the evaluation dataset as a prompt parameter.\n```\n{'some_parameter': {'column': 'my_column'}}\n```\nAlternatively, you can define templates, using columns from the evaluation dataset as variables, to specify prompt parameters:\n```\n{'some_parameter': {'template': 'Summarize the following: {{ my_column }}.'}}\n```\nWhen providing model prompt parameters for inference, users can use the protected `default_instruction` keyword as a template argument, which is replaced with the default inference instruction for the given task:\n```\nmodel_prompt_parameters = {\u00a0 \u00a0 \u00a0 \u00a0 'prompt': {'template': '{{ default_instruction }}: {{ context }}'},}\n```\nIf generating predictions, provide model prompt parameters and an output column. For example, [text-bison uses \"prompt\" for input and \"content\" foroutput](/vertex-ai/generative-ai/docs/model-reference/text#sample_response) . Follow these steps:\n- Identify the inputs and outputs needed by the models being evaluated.\n- Define the inputs as model prompt parameters.\n- Pass output to the response column.\n```\nmodel_a_prompt_parameters={\u00a0 \u00a0 'prompt': {\u00a0 \u00a0 \u00a0 \u00a0 'template': {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'Answer the following question from the point of view of a college professor: {{ context }}\\n{{ question }}'\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },},response_column_a='content', \u00a0# Column in Model A response.response_column_b='model_b_response', \u00a0# Column in eval dataset.\n```\nJust as you must provide prompt parameters for inference, you must also provide prompt parameters for evaluation. The [autorater](#autorater) requires the following prompt parameters:\n| Autorater prompt parameter | Configurable by user? | Description                        | Example                                        |\n|:-----------------------------|:------------------------|:--------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Autorater instruction  | No      | A calibrated instruction describing the criteria the autorater should use to judge the given responses. | Pick the response that answers the question and best follows instructions.                       |\n| Inference instruction  | Yes      | A description of the task each candidate model should perform.           | Answer the question accurately: Which is the hardest material?                          |\n| Inference context   | Yes      | Additional context for the task being performed.              | While titanium and diamond are both harder than copper, diamond has a hardness rating of 98 while titanium has a rating of 36. A higher rating means higher hardness. |\n| Responses     | No1      | A pair of responses to evaluate, one from each candidate model.           | Diamond                                        |\nYou can only configure the prompt parameter through pre-generated responses.\nSample code using the parameters:\n```\nautorater_prompt_parameters={\u00a0 \u00a0 'inference_instruction': {\u00a0 \u00a0 \u00a0 \u00a0 'template': 'Answer the following question from the point of view of a college professor: {{ question }}.'\u00a0 \u00a0 },\u00a0 \u00a0 'inference_context': {\u00a0 \u00a0 \u00a0 \u00a0 'column': 'context'\u00a0 \u00a0 }}\n```\nModels A and B can have inference instructions and context that are formatted differently, whether or not the same information is provided. This means that the autorater takes a separate but single inference instruction and context.\n### Example of evaluation dataset\nThis section provides an example of a question-answer task evaluation dataset, including pre-generated predictions for model B. In this example, AutoSxS performs inference only for model A. We provide an `id` column to differentiate between examples with the same question and context.\n```\n{\u00a0\"id\": 1,\u00a0\"question\": \"What is the hardest material?\",\u00a0 \"context\": \"Some might think that steel is the hardest material, or even titanium. However, diamond is actually the hardest material.\",\u00a0 \"model_b_response\": \"Diamond is the hardest material. It is harder than steel or titanium.\"}{\u00a0 \"id\": 2,\u00a0 \"question\": \"What is the highest mountain in the world?\",\u00a0 \"context\": \"K2 and Everest are the two tallest mountains, with K2 being just over 28k feet and Everest being 29k feet tall.\",\u00a0 \"model_b_response\": \"Mount Everest is the tallest mountain, with a height of 29k feet.\"}{\u00a0 \"id\": 3,\u00a0 \"question\": \"Who directed The Godfather?\",\u00a0 \"context\": \"Mario Puzo and Francis Ford Coppola co-wrote the screenplay for The Godfather, and the latter directed it as well.\",\u00a0 \"model_b_response\": \"Francis Ford Coppola directed The Godfather.\"}{\u00a0 \"id\": 4,\u00a0 \"question\": \"Who directed The Godfather?\",\u00a0 \"context\": \"Mario Puzo and Francis Ford Coppola co-wrote the screenplay for The Godfather, and the latter directed it as well.\",\u00a0 \"model_b_response\": \"John Smith.\"}\n```\n### Best practices\nFollow these best practices when defining your evaluation dataset:\n- Provide examples that represent the types of inputs, which your models process in production.\n- Your dataset must include a minimum of one evaluation example. We recommend at least 400 examples to ensure high-quality aggregate metrics. The rate of aggregate-metric quality improvements tends to decrease when more than 600 examples are provided.\n- For a guide to writing prompts, see [Design textprompts](/vertex-ai/generative-ai/docs/text/text-prompts) .\n- If you're using pre-generated predictions for either model, include the pre-generated predictions in a column of your evaluation dataset. Providing pre-generated predictions is useful, because it lets you compare the output of models that aren't in [Vertex ModelRegistry](https://console.cloud.google.com/vertex-ai/) and lets you reuse responses.## Perform model evaluation\nYou can evaluate models by using the REST API or the Vertex AI SDK for Python.\nUse this syntax to specify the path to your model:\n- **Publisher model** :`publishers/` `` `/models/` ``Example:`publishers/google/models/text-bison`\n- **Tuned model** : `projects/` `` `/locations/` `` `/models/` `` `@` `` Example: `projects/123456789012/locations/us-central1/models/1234567890123456789`\nTo create a model evaluation job, send a `POST` request by using the [pipelineJobs](/vertex-ai/docs/reference/rest/v1/projects.locations.pipelineJobs/create) method.\nBefore using any of the request data, make the following replacements:- : Display name for the`pipelineJob`.\n- : Google Cloud project that runs the pipeline components.\n- : Region to run the pipeline components.`us-central1`is supported.\n- : Cloud Storage URI to store evaluation output.\n- : BigQuery table or a comma-separated list of Cloud Storage paths to a JSONL dataset containing evaluation examples.\n- : Evaluation task in the form`{task}@{version}`.`task`can be one of`[summarization, question_answer]`.`version`is an integer with three digits or. Ex:`summarization@001`or`question_answer@latest`.\n- : Columns that distinguish unique evaluation examples.\n- : Autorater prompt parameters mapped to columns or templates. The expected parameters are:`inference_instruction`(details on how to perform a task) and`inference_context`(content to reference to perform the task). As an example,`{'inference_context': {'column': 'my_prompt'}}`uses the evaluation dataset's `my_prompt` column for the autorater's context.\n- : Either the name of a column in the evaluation dataset containing predefined predictions, or the name of the column in the Model A output containing predictions. If no value is provided, the correct model output column name will attempt to be inferred.\n- : Either the name of a column in the evaluation dataset containing predefined predictions, or the name of the column in the Model B output containing predictions. If no value is provided, the correct model output column name will attempt to be inferred.\n- (Optional): A fully-qualified model resource name (`projects/{project}/locations/{location}/models/{model}@{version}`) or publisher model resource name (`publishers/{publisher}/models/{model}`). If Model A responses are specified, this parameter shouldn't be provided.\n- (Optional): A fully-qualified model resource name (`projects/{project}/locations/{location}/models/{model}@{version}`) or publisher model resource name (`publishers/{publisher}/models/{model}`). If Model B responses are specified, this parameter shouldn't be provided.\n- (Optional): Model A's prompt template parameters mapped to columns or templates. If Model A responses are predefined, this parameter shouldn't be provided. Example:`{'prompt': {'column': 'my_prompt'}}`uses the evaluation dataset's`my_prompt`column for the prompt parameter named`prompt`.\n- (Optional): Model B's prompt template parameters mapped to columns or templates. If Model B responses are predefined, this parameter shouldn't be provided. Example:`{'prompt': {'column': 'my_prompt'}}`uses the evaluation dataset's`my_prompt`column for the prompt parameter named`prompt`.\nRequest JSON body\n```\n\u00a0 {\u00a0 \u00a0 \"displayName\": \"PIPELINEJOB_DISPLAYNAME\",\u00a0 \u00a0 \"runtimeConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \"gcsOutputDirectory\": \"gs://OUTPUT_DIR\",\u00a0 \u00a0 \u00a0 \u00a0 \"parameterValues\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"evaluation_dataset\": \"EVALUATION_DATASET\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"id_columns\": [\"ID_COLUMNS\"],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"task\": \"TASK\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"autorater_prompt_parameters\": AUTORATER_PROMPT_PARAMETERS,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"response_column_a\": \"RESPONSE_COLUMN_A\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"response_column_b\": \"RESPONSE_COLUMN_B\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"model_a\": \"MODEL_A\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"model_a_prompt_parameters\": MODEL_A_PROMPT_PARAMETERS,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"model_b\": \"MODEL_B\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"model_b_prompt_parameters\": MODEL_B_PROMPT_PARAMETERS,\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 \u00a0 \"templateUri\": \"https://us-kfp.pkg.dev/ml-pipeline/google-cloud-registry/autosxs-template/default\"\u00a0 }\n```\nUse `curl` to send your request.\n```\ncurl -X POST \\\u00a0 \u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 \u00a0 -H \"Content-Type: application/json; charset=utf-8\" \\\u00a0 \u00a0 -d @request.json \\\u00a0 \u00a0 \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/pipelineJobs\"\n```\nResponse\n```\n\u00a0 \"state\": \"PIPELINE_STATE_PENDING\",\u00a0 \"labels\": {\u00a0 \u00a0 \"vertex-ai-pipelines-run-billing-id\": \"1234567890123456789\"\u00a0 },\u00a0 \"runtimeConfig\": {\u00a0 \u00a0 \"gcsOutputDirectory\": \"gs://my-evaluation-bucket/output\",\u00a0 \u00a0 \"parameterValues\": {\u00a0 \u00a0 \u00a0 \"evaluation_dataset\": \"gs://my-evaluation-bucket/output/data.json\",\u00a0 \u00a0 \u00a0 \"id_columns\": [\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"task\": \"question_answer@001\",\u00a0 \u00a0 \u00a0 \"autorater_prompt_parameters\": {\u00a0 \u00a0 \u00a0 \u00a0 \"inference_instruction\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"template\": \"Answer the following question: {{ question }} }.\"\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \"inference_context\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"column\": \"context\"\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \"response_column_a\": \"\",\u00a0 \u00a0 \u00a0 \"response_column_b\": \"response_b\",\u00a0 \u00a0 \u00a0 \"model_a\": \"publishers/google/models/text-bison@001\",\u00a0 \u00a0 \u00a0 \"model_a_prompt_parameters\": {\u00a0 \u00a0 \u00a0 \u00a0 \"prompt\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"template\": \"Answer the following question from the point of view of a college professor: {{ question }}\\n{{ context }} }\"\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \"model_b\": \"\",\u00a0 \u00a0 \u00a0 \"model_b_prompt_parameters\": {}\u00a0 \u00a0 }\u00a0 },\u00a0 \"serviceAccount\": \"123456789012-compute@developer.gserviceaccount.com\",\u00a0 \"templateUri\": \"https://us-kfp.pkg.dev/ml-pipeline/google-cloud-registry/autosxs-template/default\",\u00a0 \"templateMetadata\": {\u00a0 \u00a0 \"version\": \"sha256:7366b784205551ed28f2c076e841c0dbeec4111b6df16743fc5605daa2da8f8a\"\u00a0 }}\n```\nTo learn how to install or update the Vertex AI SDK for Python, see [Install the Vertex AI SDK forPython](/vertex-ai/docs/python-sdk/use-vertex-ai-python-sdk) . For more information on the Python API, see the [Vertex AI SDK for PythonAPI](/python/docs/reference/aiplatform/latest) .\nFor more information about pipeline parameters, see [Google Cloud Pipeline Components ReferenceDocumentation](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.8.0/_modules/google_cloud_pipeline_components/preview/model_evaluation/model_based_llm_evaluation/autosxs/autosxs_pipeline.html#autosxs_pipeline) .\nBefore using any of the request data, make the following replacements:- : Display name for the`pipelineJob`.\n- : Google Cloud project that runs the pipeline components.\n- : Region to run the pipeline components.`us-central1`is supported.\n- : Cloud Storage URI to store evaluation output.\n- : BigQuery table or a comma-separated list of Cloud Storage paths to a JSONL dataset containing evaluation examples.\n- : Evaluation task in the form`{task}@{version}`.`task`can be one of`[summarization, question_answer]`.`version`is an integer with three digits or. Ex:`summarization@001`or`question_answer@latest`.\n- : Columns that distinguish unique evaluation examples.\n- : Autorater prompt parameters mapped to columns or templates. The expected parameters are:`inference_instruction`(details on how to perform a task) and`inference_context`(content to reference to perform the task). As an example,`{'inference_context': {'column': 'my_prompt'}}`uses the evaluation dataset's `my_prompt` column for the autorater's context.\n- : Either the name of a column in the evaluation dataset containing predefined predictions, or the name of the column in the Model A output containing predictions. If no value is provided, the correct model output column name will attempt to be inferred.\n- : Either the name of a column in the evaluation dataset containing predefined predictions, or the name of the column in the Model B output containing predictions. If no value is provided, the correct model output column name will attempt to be inferred.\n- (Optional): A fully-qualified model resource name (`projects/{project}/locations/{location}/models/{model}@{version}`) or publisher model resource name (`publishers/{publisher}/models/{model}`). If Model A responses are specified, this parameter shouldn't be provided.\n- (Optional): A fully-qualified model resource name (`projects/{project}/locations/{location}/models/{model}@{version}`) or publisher model resource name (`publishers/{publisher}/models/{model}`). If Model B responses are specified, this parameter shouldn't be provided.\n- (Optional): Model A's prompt template parameters mapped to columns or templates. If Model A responses are predefined, this parameter shouldn't be provided. Example:`{'prompt': {'column': 'my_prompt'}}`uses the evaluation dataset's`my_prompt`column for the prompt parameter named`prompt`.\n- (Optional): Model B's prompt template parameters mapped to columns or templates. If Model B responses are predefined, this parameter shouldn't be provided. Example:`{'prompt': {'column': 'my_prompt'}}`uses the evaluation dataset's`my_prompt`column for the prompt parameter named`prompt`.\n```\nimport osfrom google.cloud import aiplatformparameters = {\u00a0 \u00a0 'evaluation_dataset': 'EVALUATION_DATASET',\u00a0 \u00a0 'id_columns': ['ID_COLUMNS'],\u00a0 \u00a0 'task': 'TASK',\u00a0 \u00a0 'autorater_prompt_parameters': AUTORATER_PROMPT_PARAMETERS,\u00a0 \u00a0 'response_column_a': 'RESPONSE_COLUMN_A',\u00a0 \u00a0 'response_column_b': 'RESPONSE_COLUMN_B',\u00a0 \u00a0 'model_a': 'MODEL_A',\u00a0 \u00a0 'model_a_prompt_parameters': MODEL_A_PROMPT_PARAMETERS,\u00a0 \u00a0 'model_b': 'MODEL_B',\u00a0 \u00a0 'model_b_prompt_parameters': MODEL_B_PROMPT_PARAMETERS,}aiplatform.init(project='PROJECT_ID', location='LOCATION', staging_bucket='gs://OUTPUT_DIR')aiplatform.PipelineJob(\u00a0 \u00a0 display_name='PIPELINEJOB_DISPLAYNAME',\u00a0 \u00a0 pipeline_root=os.path.join('gs://OUTPUT_DIR', 'PIPELINEJOB_DISPLAYNAME'),\u00a0 \u00a0 template_path=(\u00a0 \u00a0 \u00a0 'https://us-kfp.pkg.dev/ml-pipeline/google-cloud-registry/autosxs-template/default'),\u00a0 \u00a0 parameter_values=parameters,).run()\n```\n## View evaluation results\nYou can find the evaluation results in the [Vertex AI Pipelines](https://console.cloud.google.com/vertex-ai/pipelines/runs) by inspecting the following artifacts produced by the AutoSxS pipeline:\n- The [judgments](#judgments) table is produced by the AutoSxS arbiter.\n- [Aggregate metrics](#aggregate-metrics) are produced by the AutoSxS metrics component.\n- [Human-preference alignment metrics](#human-metrics) are produced by the AutoSxS metrics component.\n### Judgments\nAutoSxS outputs judgments (example-level metrics) that help users understand model performance at the example level. Judgments include the following information:\n- Inference prompts\n- Model responses\n- Autorater decisions\n- Rating explanations\n- Confidence scores\nJudgments can be written to Cloud Storage in JSONL format or to a BigQuery table with these columns:\n| Column    | Description                                                  |\n|:----------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| id columns   | Columns that distinguish unique evaluation examples.                                        |\n| inference_instruction | Instruction used to generate model responses.                                          |\n| inference_context  | Context used to generate model responses.                                           |\n| response_a   | Model A's response, given inference instruction and context.                                      |\n| response_b   | Model B's response, given inference instruction and context.                                      |\n| choice    | The model with the better response. Possible values are Model A, Model B, or Error. Error means that an error prevented the autorater from determining whether model A's response or model B's response was best. |\n| confidence   | A score between 0 and 1, which signifies how confident the autorater was with its choice.                               |\n| explanation   | The autorater's reason for its choice.                                           |\n### Aggregate metrics\nAutoSxS calculates aggregate (win-rate) metrics using the [judgmentstable](#judgments) . If no human-preference data is provided, then the following aggregate metrics are generated:\n| Metric      | Description                |\n|:---------------------------|:--------------------------------------------------------------------------|\n| AutoRater model A win rate | Percentage of time the autorater decided model A had the better response. |\n| AutoRater model B win rate | Percentage of time the autorater decided model B had the better response. |\nTo better understand the win rate, look at the row-based results and the autorater's explanations to determine if the results and explanations align with your expectations.\n### Human-preference alignment metrics\nTo see an example of benchmarking AutoSxS against human pereference data,  run the \"AutoSxS: Check autorater alignment against a human-preference dataset\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_evaluation/model_based_llm_evaluation/autosxs_check_alignment_against_human_preference_data.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fmodel_evaluation%2Fmodel_based_llm_evaluation%2Fautosxs_check_alignment_against_human_preference_data.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_evaluation/model_based_llm_evaluation/autosxs_check_alignment_against_human_preference_data.ipynb)\nIf human-preference data is provided, AutoSxS outputs the following metrics:\n| Metric       | Description                                                                                          |\n|:----------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| AutoRater model A win rate  | Percentage of time the autorater decided model A had the better response.                                                                           |\n| AutoRater model B win rate  | Percentage of time the autorater decided model B had the better response.                                                                           |\n| Human-preference model A win rate | Percentage of time humans decided model A had the better response.                                                                            |\n| Human-preference model B win rate | Percentage of time humans decided model B had the better response.                                                                            |\n| TP        | Number of examples where both the autorater and human preferences were that Model A had the better response.                                                                  |\n| FP        | Number of examples where the autorater chose Model A as the better response, but the human preference was that Model B had the better response.                                                         |\n| TN        | Number of examples where both the autorater and human preferences were that Model B had the better response.                                                                  |\n| FN        | Number of examples where the autorater chose Model B as the better response, but the human preference was that Model A had the better response.                                                         |\n| Accuracy       | Percentage of time where the autorater agreed with human raters.                                                                             |\n| Precision       | Percentage of time where both the autorater and humans thought Model A had a better response, out of all cases where the autorater thought Model A had a better response.                                                   |\n| Recall       | Percentage of time where both the autorater and humans thought Model A had a better response, out of all cases where humans thought Model A had a better response.                                                    |\n| F1        | Harmonic mean of precision and recall.                                                                                   |\n| Cohen's Kappa      | A measurement of agreement between the autorater and human raters that takes the likelihood of random agreement into account. Cohen suggests the following interpretation: 0.0Agreement equivalent to random chance0.0 - 0.2Slight agreement0.2 - 0.4Fair agreement0.4 - 0.6Moderate agreement0.6 - 0.8Substantial agreement0.8 - 1.0Nearly perfect agreement1.0Perfect agreement |\n| 0.0        | Agreement equivalent to random chance                                                                                    |\n| 0.0 - 0.2       | Slight agreement                                                                                         |\n| 0.2 - 0.4       | Fair agreement                                                                                         |\n| 0.4 - 0.6       | Moderate agreement                                                                                        |\n| 0.6 - 0.8       | Substantial agreement                                                                                        |\n| 0.8 - 1.0       | Nearly perfect agreement                                                                                       |\n| 1.0        | Perfect agreement                                                                                         |\n## AutoSxS use cases\nYou can explore how to use AutoSxS with three use case scenarios.\nEvaluate a tuned first-party (1p) model against a reference 1p model.\nYou can specify that inference runs on both models simultaneously.\n \nThis code sample evaluates a tuned model from [Vertex ModelRegistry](https://console.cloud.google.com/vertex-ai/) against a reference model from the same registry.\n```\n# Evaluation dataset schema:# \u00a0 my_question: str# \u00a0 my_context: strparameters = {\u00a0 \u00a0 'evaluation_dataset': DATASET,\u00a0 \u00a0 'id_columns': ['my_context'],\u00a0 \u00a0 'task': 'question_answer@001',\u00a0 \u00a0 'autorater_prompt_parameters': {\u00a0 \u00a0 \u00a0 'inference_instruction': {'column': 'my_question'},\u00a0 \u00a0 \u00a0 'inference_context': {'column': 'my_context'},\u00a0 },\u00a0 \u00a0 'model_a': 'publishers/google/models/text-bison@001',\u00a0 \u00a0 'model_a_prompt_parameters': {QUESTION: {'template': '{{my_question}}\\nCONTEXT: {{my_context}}'}},\u00a0 'response_column_a': 'content',\u00a0 \u00a0 'model_b': 'projects/abc/locations/abc/models/tuned_bison',\u00a0 \u00a0 'model_b_prompt_parameters': {'prompt': {'template': '{{my_context}}\\n{{my_question}}'}},\u00a0 'response_column_b': 'content',}\n```\nEvaluate a tuned third-party (3p) model against a reference 3p model.\nYou can skip inference by directly supplying model responses.\n \nThis code sample evaluates a tuned 3p model against a reference 3p model.\n```\n# Evaluation dataset schema:# \u00a0 my_question: str# \u00a0 my_context: str# \u00a0 response_b: strparameters = {\u00a0 \u00a0 'evaluation_dataset': DATASET,\u00a0 \u00a0 'id_columns': ['my_context'],\u00a0 \u00a0 'task': 'question_answer@001',\u00a0 \u00a0 'autorater_prompt_parameters':\u00a0 \u00a0 \u00a0 \u00a0 'inference_instruction': {'column': 'my_question'},\u00a0 \u00a0 \u00a0 \u00a0 'inference_context': {'column': 'my_context'},\u00a0 \u00a0 },\u00a0 \u00a0 'response_column_a': 'content',\u00a0 \u00a0 'response_column_b': 'response_b',}\n```\nAll supported tasks have been benchmarked using human-rater data to ensure that the autorater responses are aligned with human preferences. If you want to benchmark AutoSxS for your use cases, provide human-preference data directly to AutoSxS, which outputs alignment-aggregate statistics.\nTo check alignment against a human-preference dataset, you can specify both outputs (prediction results) to the autorater. You can also provide your inference results.\n \nThis code sample verifies that the autorater's results and explanations align with your expectations.\n```\n# Evaluation dataset schema:# \u00a0my_question: str# \u00a0my_context: str# \u00a0 response_a: str# \u00a0 response_b: str# \u00a0 actual: strparameters = {\u00a0 \u00a0 'evaluation_dataset': DATASET,\u00a0 \u00a0 'id_columns': ['my_context'],\u00a0 \u00a0 'task': 'question_answer@001',\u00a0 \u00a0 'autorater_prompt_parameters': {\u00a0 \u00a0 \u00a0 'inference_instruction': {'column': 'my_question'},\u00a0 \u00a0 \u00a0 'inference_context': {'column': 'my_context'},\u00a0 },\u00a0 'response_column_a': 'response_a',\u00a0 'response_column_b': 'response_b',\u00a0 'human_preference_column': 'actual',}\n```\n## What's next\n- Learn more about [metrics-based evaluation](/vertex-ai/generative-ai/docs/models/evaluate-models) .\n- Learn how to [tune language foundation models](/vertex-ai/generative-ai/docs/models/tune-models) .", "guide": "Generative AI on Vertex AI"}