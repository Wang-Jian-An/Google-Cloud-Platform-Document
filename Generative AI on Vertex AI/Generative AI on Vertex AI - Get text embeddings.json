{"title": "Generative AI on Vertex AI - Get text embeddings", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings", "abstract": "# Generative AI on Vertex AI - Get text embeddings\nWith the Vertex AI text-embeddings API, you can easily create a text embedding with [Generative AI](/vertex-ai/generative-ai/docs/learn/overview) . A text embedding is a vector representation of text, and they are used in many ways to find similar items. You interact with them every time you complete a Google search, see recommendations when online shopping, or when your favorite music streaming service suggests a rock band you might like based on your listening history. Some common use cases for text embeddings include:\n- **Semantic search** : Search text ranked by semantic similarity.\n- **Classification** : Return the class of items whose text attributes are similar to the given text.\n- **Clustering** : Cluster items whose text attributes are similar to the given text.\n- **Outlier Detection** : Return items where text attributes are least related to the given text.\n- **Conversational interface** : Clusters groups of sentences which can lead to similar responses, like in a conversation-level embedding space.\nWhen you create text embeddings, you get vector representations of natural text as arrays of floating point numbers. What this means, is that all of your input text is assigned a numerical representation. By comparing the numerical distance between the vector representations of two pieces of text, an application can determine the similarity between the text or the objects represented by the text.\nFor example, let's say you wanted to develop a book recommendation chatbot. The first thing you would do is use a deep neural network (DNN) to convert each book into an embedding vector, where one embedding vector represents one book. We could feed, as input to the DNN, just the book title or just the text content. Or we could use both of these together, along with any other metadata describing the book, such as the genre.\nThe embeddings in this example could be comprised of thousands of book titles with summaries and their genre, and it might have representations for books like by Emily Bront\u00eb and by Jane Austen that are similar to each other (small distance between numerical representation). Whereas the numerical representation for the book by F. Scott Fitzgerald would be further, as the time period, genre, and summary is less similar.\nThe inputs are the main influence to the orientation of the embedding space. For example, if we only had book title inputs, then two books with similar titles, but very different summaries, could be close together. However, if we include the title and summary, then these same books are less similar (further away) in the embedding space.\nWorking with Generative AI, this book-suggestion chatbot could summarize, suggest, and show you books which you might like (or dislike), based on your query.\nTo learn more about embeddings, see [Meet AI's multitool: Vector embeddings](https://cloud.google.com/blog/topics/developers-practitioners/meet-ais-multitool-vector-embeddings) . To take a foundational ML crash course on embeddings, see [Embeddings](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture) .\nAfter converting each book to an embedding representation, it's time to index these embeddings in a vector database, like Vector Search. This enables low-latency retrieval, and is critical as the size of our corpus of books (vectors) increases.\nTo learn more about Vector Search, see [Overview of Vector Search](/vertex-ai/docs/vector-search/overview) .\n", "content": "## Supported models\nTo learn which text embedding model versions are available, see [Available stable model versions](/vertex-ai/generative-ai/docs/learn/model-versioning#stable-versions-available.md) . To learn which text embedding model versions are available, see [Latest models](/vertex-ai/generative-ai/docs/learn/model-versioning#latest-version) .\nIt is strongly recommended to specify a [stable](/vertex-ai/generative-ai/docs/learn/model-versioning#stable-versions-available.md) model version (for example, `textembedding-gecko@003` ). The [latest](/vertex-ai/generative-ai/docs/learn/model-versioning#latest-version) version of a model is in [Preview](https://cloud.google.com/products#product-launch-stages) and is not General Availability (GA). Because the latest version is in [Preview](https://cloud.google.com/products#product-launch-stages) , it isn't guaranteed to be production ready.\nIt is especially important to use a stable model version for example, `textembedding-gecko@003` for applications that require backward compatible embeddings. If backward compatibility isn't a concern and you would like to use the latest model version, you should specify `@latest` explicitly. If no version is specified, `textembedding-gecko` defaults to `textembedding-gecko@003` , and `textembedding-gecko-multilingual` defaults to `textembedding-gecko-multilingual@001` .\n### Prerequisites\nThere are specific prerequisites for successfully creating an embedding. To get started, see quickstart: [Try text embeddings](/vertex-ai/generative-ai/docs/start/quickstarts/quickstart-text-embeddings) .\n| 0                                                    |\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Use this colab to call the newly released text embedding models (textembedding-gecko and textembedding-gecko-multilingual). Jupyter notebook: You can run this tutorial as a Jupyter notebook. Run in Colab |\n## Get text embeddings for a snippet of text\nYou can get text embeddings for a snippet of text by using the Vertex AI API or the Vertex AI SDK for Python. For each request, you're limited to five input texts. Each input text has a token limit of 3,072. Inputs longer than this length are silently truncated. You can also disable silent truncation by setting `autoTruncate` to `false` .\nThese examples use the `textembedding-gecko@003` model.\nTo get text embeddings, send a POST request by specifying the model ID of the publisher model.\nBefore using any of the request data, make the following replacements:- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The text that you want to generate embeddings  for. **Limit:** five texts of up to 3,072 tokens per text.\n- : If set to`false`, text that exceeds the token limit causes the request to fail. The default  value is`true`.\nHTTP method and URL:\n```\nPOST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/textembedding-gecko@003:predict\n```\nRequest JSON body:\n```\n{\n \"instances\": [ { \"content\": \"TEXT\"}\n ],\n \"parameters\": { \n \"autoTruncate\": AUTO_TRUNCATE \n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/textembedding-gecko@003:predict\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/textembedding-gecko@003:predict\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following. Note that `values` has been truncated to save space.```\nMODEL_ID=\"textembedding-gecko@003\"PROJECT_ID=PROJECT_IDcurl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/${MODEL_ID}:predict -d \\$'{\u00a0 \"instances\": [\u00a0 \u00a0 { \"content\": \"What is life?\"}\u00a0 ],}'\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/embedding.py) \n```\nfrom vertexai.language_models import TextEmbeddingModeldef text_embedding() -> list:\u00a0 \u00a0 \"\"\"Text embedding with a Large Language Model.\"\"\"\u00a0 \u00a0 model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\u00a0 \u00a0 embeddings = model.get_embeddings([\"What is life?\"])\u00a0 \u00a0 for embedding in embeddings:\u00a0 \u00a0 \u00a0 \u00a0 vector = embedding.values\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Length of Embedding Vector: {len(vector)}\")\u00a0 \u00a0 return vector\n```Before trying this sample, follow the Go setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Go API reference documentation](/go/docs/reference/cloud.google.com/go/aiplatform/latest/apiv1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/aiplatform/text-embeddings/embeddings.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 aiplatform \"cloud.google.com/go/aiplatform/apiv1beta1\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/aiplatform/apiv1beta1/aiplatformpb\"\u00a0 \u00a0 \u00a0 \u00a0 \"google.golang.org/api/option\"\u00a0 \u00a0 \u00a0 \u00a0 \"google.golang.org/protobuf/types/known/structpb\")// generateEmbeddings creates embeddings from text provided.func generateEmbeddings(w io.Writer, prompt, project, location, publisher, model string) error {\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 apiEndpoint := fmt.Sprintf(\"%s-aiplatform.googleapis.com:443\", location)\u00a0 \u00a0 \u00a0 \u00a0 client, err := aiplatform.NewPredictionClient(ctx, option.WithEndpoint(apiEndpoint))\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"unable to create prediction client: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 // PredictRequest requires an endpoint, instances, and parameters\u00a0 \u00a0 \u00a0 \u00a0 // Endpoint\u00a0 \u00a0 \u00a0 \u00a0 base := fmt.Sprintf(\"projects/%s/locations/%s/publishers/%s/models\", project, location, publisher)\u00a0 \u00a0 \u00a0 \u00a0 url := fmt.Sprintf(\"%s/%s\", base, model)\u00a0 \u00a0 \u00a0 \u00a0 // Instances: the prompt\u00a0 \u00a0 \u00a0 \u00a0 promptValue, err := structpb.NewValue(map[string]interface{}{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": prompt,\u00a0 \u00a0 \u00a0 \u00a0 })\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"unable to convert prompt to Value: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // PredictRequest: create the model prediction request\u00a0 \u00a0 \u00a0 \u00a0 req := &aiplatformpb.PredictRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Endpoint: \u00a0url,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Instances: []*structpb.Value{promptValue},\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // PredictResponse: receive the response from the model\u00a0 \u00a0 \u00a0 \u00a0 resp, err := client.Predict(ctx, req)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"error in prediction: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"embeddings generated: %v\", resp.Predictions[0])\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/PredictTextEmbeddingsSample.java) \n```\nimport com.google.cloud.aiplatform.util.ValueConverter;import com.google.cloud.aiplatform.v1beta1.EndpointName;import com.google.cloud.aiplatform.v1beta1.PredictResponse;import com.google.cloud.aiplatform.v1beta1.PredictionServiceClient;import com.google.cloud.aiplatform.v1beta1.PredictionServiceSettings;import com.google.protobuf.Value;import com.google.protobuf.util.JsonFormat;import java.io.IOException;import java.util.ArrayList;import java.util.List;public class PredictTextEmbeddingsSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 // Details about text embedding request structure and supported models are available in:\u00a0 \u00a0 // https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings\u00a0 \u00a0 String instance = \"{ \\\"content\\\": \\\"What is life?\\\"}\";\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 String publisher = \"google\";\u00a0 \u00a0 String model = \"textembedding-gecko@001\";\u00a0 \u00a0 predictTextEmbeddings(instance, project, location, publisher, model);\u00a0 }\u00a0 // Get text embeddings from a supported embedding model\u00a0 public static void predictTextEmbeddings(\u00a0 \u00a0 \u00a0 String instance, String project, String location, String publisher, String model)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 String endpoint = String.format(\"%s-aiplatform.googleapis.com:443\", location);\u00a0 \u00a0 PredictionServiceSettings predictionServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(endpoint)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests.\u00a0 \u00a0 try (PredictionServiceClient predictionServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceClient.create(predictionServiceSettings)) {\u00a0 \u00a0 \u00a0 EndpointName endpointName =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 EndpointName.ofProjectLocationPublisherModelName(project, location, publisher, model);\u00a0 \u00a0 \u00a0 // Use Value.Builder to convert instance to a dynamically typed value that can be\u00a0 \u00a0 \u00a0 // processed by the service.\u00a0 \u00a0 \u00a0 Value.Builder instanceValue = Value.newBuilder();\u00a0 \u00a0 \u00a0 JsonFormat.parser().merge(instance, instanceValue);\u00a0 \u00a0 \u00a0 List<Value> instances = new ArrayList<>();\u00a0 \u00a0 \u00a0 instances.add(instanceValue.build());\u00a0 \u00a0 \u00a0 PredictResponse predictResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 predictionServiceClient.predict(endpointName, instances, ValueConverter.EMPTY_VALUE);\u00a0 \u00a0 \u00a0 System.out.println(\"Predict Response\");\u00a0 \u00a0 \u00a0 for (Value prediction : predictResponse.getPredictionsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\tPrediction: %s\\n\", prediction);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/predict-text-embeddings.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';const aiplatform = require('@google-cloud/aiplatform');// Imports the Google Cloud Prediction service clientconst {PredictionServiceClient} = aiplatform.v1;// Import the helper module for converting arbitrary protobuf.Value objects.const {helpers} = aiplatform;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};const publisher = 'google';const model = 'textembedding-gecko@001';// Instantiates a clientconst predictionServiceClient = new PredictionServiceClient(clientOptions);async function callPredict() {\u00a0 // Configure the parent resource\u00a0 const endpoint = `projects/${project}/locations/${location}/publishers/${publisher}/models/${model}`;\u00a0 const instance = {\u00a0 \u00a0 content: 'What is life?',\u00a0 };\u00a0 const instanceValue = helpers.toValue(instance);\u00a0 const instances = [instanceValue];\u00a0 const parameter = {\u00a0 \u00a0 temperature: 0,\u00a0 \u00a0 maxOutputTokens: 256,\u00a0 \u00a0 topP: 0,\u00a0 \u00a0 topK: 1,\u00a0 };\u00a0 const parameters = helpers.toValue(parameter);\u00a0 const request = {\u00a0 \u00a0 endpoint,\u00a0 \u00a0 instances,\u00a0 \u00a0 parameters,\u00a0 };\u00a0 // Predict request\u00a0 const [response] = await predictionServiceClient.predict(request);\u00a0 console.log('Get text embeddings response');\u00a0 const predictions = response.predictions;\u00a0 console.log('\\tPredictions :');\u00a0 for (const prediction of predictions) {\u00a0 \u00a0 console.log(`\\t\\tPrediction : ${JSON.stringify(prediction)}`);\u00a0 }}callPredict();\n```\n## API changes to models released on or after August 2023\nWhen using model versions released on or after August 2023, including `textembedding-gecko@003` and `textembedding-gecko-multilingual@001` , there is a new task type parameter and the optional title (only valid with `task_type=RETRIEVAL_DOCUMENT` ).\nThese new parameters apply to these public preview models and all stable models going forward.\n```\n{\u00a0 \"instances\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"task_type\": \"RETRIEVAL_DOCUMENT\",\u00a0 \u00a0 \u00a0 \"title\": \"document title\",\u00a0 \u00a0 \u00a0 \"content\": \"I would like embeddings for this text!\"\u00a0 \u00a0 },\u00a0 ]}\n```\nThe `task_type` parameter is defined as the intended downstream application to help the model produce better quality embeddings. It is a string that can take on one of the following values:\n| task_type   | Description                 |\n|:--------------------|:-----------------------------------------------------------------------------|\n| RETRIEVAL_QUERY  | Specifies the given text is a query in a search or retrieval setting.  |\n| RETRIEVAL_DOCUMENT | Specifies the given text is a document in a search or retrieval setting.  |\n| SEMANTIC_SIMILARITY | Specifies the given text will be used for Semantic Textual Similarity (STS). |\n| CLASSIFICATION  | Specifies that the embeddings will be used for classification.    |\n| CLUSTERING   | Specifies that the embeddings will be used for clustering.     |\n## Language coverage for textembedding-gecko-multilingual models.\nThe `textembedding-gecko-multilingual@001` model has been evaluated on the following languages: `Arabic (ar)` , `Bengali (bn)` , `English (en)` , `Spanish (es)` , `German (de)` , `Persian (fa)` , `Finnish (fi)` , `French (fr)` , `Hindi (hi)` , `Indonesian (id)` , `Japanese (ja)` , `Korean (ko)` , `Russian (ru)` , `Swahili (sw)` , `Telugu (te)` , `Thai (th)` , `Yoruba (yo)` , `Chinese (zh)` .\nThe following is the full list of supported languages: `Afrikaans` , `Albanian` , `Amharic` , `Arabic` , `Armenian` , `Azerbaijani` , `Basque` , `Belarusian` , `Bengali` , `Bulgarian` , `Burmese` , `Catalan` , `Cebuano` , `Chichewa` , `Chinese` , `Corsican` , `Czech` , `Danish` , `Dutch` , `English` , `Esperanto` , `Estonian` , `Filipino` , `Finnish` , `French` , `Galician` , `Georgian` , `German` , `Greek` , `Gujarati` , `Haitian Creole` , `Hausa` , `Hawaiian` , `Hebrew` , `Hindi` , `Hmong` , `Hungarian` , `Icelandic` , `Igbo` , `Indonesian` , `Irish` , `Italian` , `Japanese` , `Javanese` , `Kannada` , `Kazakh` , `Khmer` , `Korean` , `Kurdish` , `Kyrgyz` , `Lao` , `Latin` , `Latvian` , `Lithuanian` , `Luxembourgish` , `Macedonian` , `Malagasy` , `Malay` , `Malayalam` , `Maltese` , `Maori` , `Marathi` , `Mongolian` , `Nepali` , `Norwegian` , `Pashto` , `Persian` , `Polish` , `Portuguese` , `Punjabi` , `Romanian` , `Russian` , `Samoan` , `Scottish Gaelic` , `Serbian` , `Shona` , `Sindhi` , `Sinhala` , `Slovak` , `Slovenian` , `Somali` , `Sotho` , `Spanish` , `Sundanese` , `Swahili` , `Swedish` , `Tajik` , `Tamil` , `Telugu` , `Thai` , `Turkish` , `Ukrainian` , `Urdu` , `Uzbek` , `Vietnamese` , `Welsh` , `West Frisian` , `Xhosa` , `Yiddish` , `Yoruba` , `Zulu` .\n## Use Vector Search\nTo make use of your text-embedding for large search engines or recommendations systems in production, you can take advantage of [Vector Search](/vertex-ai/docs/vector-search/overview) .\n| Feature     | Vertex AI text embedding API                          | Vector Search                            |\n|:------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|\n| Creates text embeddings | Yes                                | No                              |\n| Vector quality   | High                                | Dependent on where the embedding was created                    |\n| Suited for    | Creating high-quality text embeddings (vector representations) which can be used for text classification and question answering | Performing large approximate nearest neighbor (ANN) searches, which can power search engines and recommendation systems. |\n## What's next\n- Learn how to [tune a foundation model](/vertex-ai/generative-ai/docs/models/tune-models) .\n- Learn about [responsible AI best practices and Vertex AI's safety filters](/vertex-ai/generative-ai/docs/learn/responsible-ai) .", "guide": "Generative AI on Vertex AI"}