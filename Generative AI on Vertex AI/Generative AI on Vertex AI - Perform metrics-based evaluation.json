{"title": "Generative AI on Vertex AI - Perform metrics-based evaluation", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluate-models", "abstract": "# Generative AI on Vertex AI - Perform metrics-based evaluation\n**Preview** This feature is a Preview offering, subject to the Pre-GA Offerings Terms  of the [GCP Service Specific Terms](https://cloud.google.com/terms/service-terms) . Pre-GA products and features may have limited support, and changes to pre-GA  products and features may not be compatible with other pre-GA versions. For more information,  see the [launch stage descriptions](https://cloud.google.com/products#product-launch-stages) .  Further, by using the PaLM API on Vertex AI, you agree to the Generative AI Preview [ terms and conditions ](https://cloud.google.com/trustedtester/aitos?hl=en) (Preview Terms).For PaLM APIs on Vertex AI that are not GA, you can process personal data as outlined in the  Cloud Data Processing Addendum, subject to applicable restrictions and obligations in the  Agreement (as defined in the Preview Terms).\nYou can evaluate the performance of foundation models and your tuned generative AI models on Vertex AI. The models are evaluated using a set of metrics against an evaluation dataset that you provide. This page explains how model evaluation works, how to create and format the evaluation dataset, and how to perform evaluation using the Google Cloud console, Vertex AI API, or the Vertex AI SDK for Python.\n", "content": "## How model evaluation works\nTo evaluate the performance of a model, you first create an evaluation dataset that contains prompt and ground truth pairs. For each pair, the prompt is the input that you want to evaluate, and the ground truth is the ideal response for that prompt. During evaluation, the prompt in each pair of the evaluation dataset is passed to the model to produce an output. The output generated by the model and the ground truth from the evaluation dataset are used to compute the evaluation metrics.\nThe type of metrics used for evaluation depends on the task that you are evaluating. The following table shows the supported tasks and the metrics used to evaluate each task:\n| Task    | Metric       |\n|:-------------------|:---------------------------------|\n| Classification  | Micro-F1, Macro-F1, Per class F1 |\n| Summarization  | ROUGE-L       |\n| Question answering | Exact Match      |\n| Text generation | BLEU, ROUGE-L     |\n## Supported models\nModel evaluation is supported for the base and tuned versions of `text-bison` .\n## Prepare evaluation dataset\nThe evaluation dataset that's used for model evaluation includes prompt and ground truth pairs that align with the task that you want to evaluate. Your dataset must include a minimum of one prompt and ground truth pair and at least 10 pairs for meaningful metrics. The more examples you give, the more meaningful the results.\n### Dataset format\nYour evaluation dataset must be in [JSON Lines](https://jsonlines.org/) (JSONL) format where each line contains a single prompt and ground truth pair specified in the `input_text` and `output_text` fields, respectively. The `input_text` field contains the prompt that you want to evaluate, and the `output_text` field contains the ideal response for the prompt.\nThe maximum token length for `input_text` is 8,192, and the maximum token length for `output_text` is 1,024.\n```\n{\"input_text\": \"Multi-choice problem: What is the topic of this text? OPTIONS: -nature - news - politics - sports - health - startups TEXT: #DYK In 2015, theworld produced 322M tonnes of plastic. That equals 900 Empire State Buildings!Act:\\u2026 https://t.co/qGrpumIN20\", \"output_text\": \"nature\"}{\"input_text\": \"Multi-choice problem: What is the topic of this text? OPTIONS: -nature - news - politics - sports - health - startups TEXT: Do you agree withChris Pratt? https://t.co/1q43CvIWAY https://t.co/zcKnTa9hKS\", \"output_text\":\"news\"}{\"input_text\": \"Multi-choice problem: What is the topic of this text? OPTIONS: -nature - news - politics - sports - health - startups TEXT: Cahill and DiegoCosta aren't far away from getting their head on Hazard's corner. Meanwhile,Ibe has replaced Pugh for Bournemouth. #BOUCHE\", \"output_text\": \"sports\"}{\"input_text\": \"Multi-choice problem: What is the topic of this text? OPTIONS: -nature - news - politics - sports - health - startups TEXT: Protecting ouroceans is integral to protecting the global environment #EarthDayhttps://t.co/CN5NHKFbsB\", \"output_text\": \"nature\"}{\"input_text\": \"Multi-choice problem: What is the topic of this text? OPTIONS: -nature - news - politics - sports - health - startups TEXT: Scientists havefound that water is gushing across Antarctica \\u2014 more than they everrealized:\\u2026 https://t.co/ttBGA15w5G\", \"output_text\": \"news\"}\n```\n```\n{\"input_text\": \"What is the capital of Australia?\", \"output_text\": \"Canberra\"}{\"input_text\": \"Who is the author of The Great Gatsby?\", \"output_text\": \"F. ScottFitzgerald\"}{\"input_text\": \"What is the highest mountain in the world?\", \"output_text\":\"Mount Everest\"}{\"input_text\": \"What is the largest country in South America?\", \"output_text\":\"Brazil\"}{\"input_text\": \"Who directed The Godfather?\", \"output_text\": \"Francis FordCoppola\"}\n```\n```\n{\"input_text\":\"Basketball teams in the Midwest.\", \"output_text\":\"There areseveral basketball teams located in the Midwest region of the United States.Here are some of them:\"}{\"input_text\":\"How to bake gluten-free bread?\", \"output_text\":\"Baking gluten-freebread can be a bit challenging because gluten is the protein that gives breadits structure and elasticity.\"}{\"input_text\":\"Want to buy a new phone.\", \"output_text\":\"Great! There are manyfactors to consider when buying a new phone, including your budget, preferredoperating system, desired features, and more. Here are some general steps tofollow to help you make an informed decision:\"}{\"input_text\":\"I told them \\\"see you tomorrow\\\"\", \"output_text\":\"If you toldsomeone \\\"see you tomorrow,\\\" you most likely meant that you will see them thefollowing day. This is a common phrase used when saying goodbye to someonewith the intention of seeing them again soon. If you are unable to meet withthem as planned, it is always polite to let them know as soon as possible sothat they are not left waiting or wondering.\"}{\"input_text\":\"Apples are red\\nOranges are orange\", \"output_text\":\"Yes, applesare often red, although there are other varieties of apples that can be green,yellow, or a combination of these colors. Oranges are typically orange, butthey can also be green or even slightly red depending on their variety andripeness. Colors are an important characteristic of fruits, as they can be anindication of ripeness and nutritional value.\"}\n```\n```\n{\"input_text\": \"Summarize this: The coronavirus pandemic has resulted in a globalhealth crisis, with millions of people infected and hundreds of thousands ofdeaths. Governments and healthcare organizations around the world are workingto contain the spread of the virus and develop effective treatments andvaccines. In addition to the health impacts, the pandemic has also had majoreconomic and social consequences, with many businesses forced to shut down andpeople facing unemployment and financial hardship.\", \"output_text\": \"Thecoronavirus pandemic has caused a global health crisis, with governments andhealthcare organizations working to contain the virus and develop treatmentsand vaccines. The pandemic has also had major economic and socialconsequences, with businesses shutting down and people facing financialhardship.\"}{\"input_text\": \"Summarize this: The United States has a long and complicatedhistory of race relations, with racism and discrimination affecting manyaspects of society. In recent years, there have been numerous protests andmovements calling for racial justice and equality. These efforts have broughtrenewed attention to issues such as police brutality, mass incarceration, andsystemic inequality.\", \"output_text\": \"The United States has a history ofracism and discrimination, with recent protests calling for racial justice andequality. Issues such as police brutality, mass incarceration, and systemicinequality have been brought to the forefront.\"}{\"input_text\": \"Summarize this: Climate change is a pressing global issue, withrising temperatures and sea levels posing significant threats to ecosystemsand human societies. Many governments and organizations are taking steps toreduce greenhouse gas emissions and transition to cleaner sources of energy.However, there is still much work to be done to address the root causes ofclimate change and mitigate its impacts.\", \"output_text\": \"Climate change isa global issue, with rising temperatures and sea levels posing significantthreats. Many governments and organizations are taking steps to reducegreenhouse gas emissions, but more work needs to be done to address the rootcauses and mitigate the impacts.\"}{\"input_text\": \"Summarize this: Artificial intelligence and machine learning arerapidly advancing fields with many potential applications in industry,healthcare, and other sectors. However, there are also concerns about theethical implications of AI and the potential for these technologies toexacerbate existing inequalities. As AI continues to evolve, it will beimportant to ensure that its development and use are guided by principles offairness and accountability.\", \"output_text\": \"Artificial intelligence andmachine learning have many potential applications, but there are also concernsabout the ethical implications and potential for exacerbating existinginequalities. It will be important to ensure that AI development and use areguided by principles of fairness and accountability.\"}\n```\n## Upload evaluation dataset to Cloud Storage\nYou can either [create a new Cloud Storage bucket](/storage/docs/creating-buckets#create_a_new_bucket) or use an existing one to store your dataset file. The bucket must be in the same region as the model.\nAfter your bucket is ready, [upload](/storage/docs/creating-buckets#create_a_new_bucket) your dataset file to the bucket.\n## Perform model evaluation\nYou can evaluate models by using the REST API or the Google Cloud console.\nTo create a model evaluation job, send a `POST` request by using the [pipelineJobs](/vertex-ai/docs/reference/rest/v1/projects.locations.pipelineJobs/create) method.\nBefore using any of the request data, make the following replacements:- : The Google Cloud project that runs the  pipeline components.\n- : A display  name for the pipelineJob.\n- : The region to run the pipeline components.  Currently, only`us-central1`is supported.\n- : The Cloud Storage URI of your  reference dataset. You can specify one or multiple URIs. This parameter supports [wildcards](/storage/docs/gsutil/addlhelp/WildcardNames) . To learn more about  this parameter, see [InputConfig](/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig) .\n- : The Cloud Storage URI to store  evaluation output.\n- : Specify a publisher model or a tuned  model resource as follows:- **Publisher model:** `publishers/google/models/` `` `@` ``Example: `publishers/google/models/text-bison@001`\n- **Tuned model:** `projects/` `` `/locations/` `` `/models/` ``Example: `projects/123456789012/locations/us-central1/models/1234567890123456789`\nThe evaluation job doesn't impact any existing deployments of the model or their resources.\n- : The task that you want to  evaluate the model on. The evaluation job computes a set of metrics relevant to that specific  task. Acceptable values include the following:- `summarization`\n- `question-answering`\n- `text-generation`\n- `classification`\n- : The format of your dataset.  Currently, only`jsonl`is supported. To learn more about this parameter, see [InputConfig](/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig) .\n- : The format of the  evaluation output. Currently, only`jsonl`is supported. To learn more about this  parameter, see [InputConfig](/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#InputConfig) .\n- : (Optional) The machine type for  running the evaluation job. The default value is`e2-highmem-16`. For a list of  supported machine types, see [Machine types](/vertex-ai/docs/training/configure-compute#machine-types) .\n- : (Optional) The service  account to use for running the evaluation job. To learn how to create a custom service account,  see [Configure a service account with granular permissions](/vertex-ai/docs/pipelines/configure-project#service-account) .  If unspecified, the [Vertex\u00a0AI\u00a0Custom\u00a0Code\u00a0Service\u00a0Agent](/vertex-ai/docs/general/access-control#service-agents) is used.\n- : (Optional) The fully qualified name of the  Compute Engine network to peer the evaluatiuon job to. The format of the network name is`projects/` `` `/global/networks/` ``. If you  specify this field, you need to have a [VPC Network Peering for Vertex AI](/vertex-ai/docs/general/vpc-peering) . If left unspecified, the evaluation job is not peered with any network.\n- : (Optional) The name of the customer-managed  encryption key (CMEK). If configured, resources created by the evaluation job is encrypted using  the provided encryption key. The format of the key name is`projects/` `` `/locations/` `` `/keyRings/` `` `/cryptoKeys/` ``.  The key needs to be in the same region as the evaluation job.\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/pipelineJobs\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"PIPELINEJOB_DISPLAYNAME\",\n \"runtimeConfig\": {\n \"gcsOutputDirectory\": \"gs://OUTPUT_DIR\",\n \"parameterValues\": {\n  \"project\": \"PROJECT_ID\",\n  \"location\": \"LOCATION\",\n  \"batch_predict_gcs_source_uris\": [\"gs://DATASET_URI\"],\n  \"batch_predict_gcs_destination_output_uri\": \"gs://OUTPUT_DIR\",\n  \"model_name\": \"MODEL_NAME\",\n  \"evaluation_task\": \"EVALUATION_TASK\",\n  \"batch_predict_instances_format\": \"INSTANCES_FORMAT\",\n  \"batch_predict_predictions_format: \"PREDICTIONS_FORMAT\",\n  \"machine_type\": \"MACHINE_TYPE\",\n  \"service_account\": \"SERVICE_ACCOUNT\",\n  \"network\": \"NETWORK\",\n  \"encryption_spec_key_name\": \"KEY_NAME\"\n }\n },\n \"templateUri\": \"https://us-kfp.pkg.dev/vertex-evaluation/pipeline-templates/evaluation-llm-text-generation-pipeline/1.0.1\"\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/pipelineJobs\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/pipelineJobs\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following. Note that `pipelineSpec` has been truncated to save space.```\nPROJECT_ID=myprojectREGION=us-central1MODEL_NAME=publishers/google/models/text-bison@001TEST_DATASET_URI=gs://my-gcs-bucket-uri/dataset.jsonlOUTPUT_DIR=gs://my-gcs-bucket-uri/outputcurl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json; charset=utf-8\" \\\"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${REGION}/pipelineJobs\" -d \\$'{\u00a0 \"displayName\": \"evaluation-llm-text-generation-pipeline\",\u00a0 \"runtimeConfig\": {\u00a0 \u00a0 \"gcsOutputDirectory\": \"'${OUTPUT_DIR}'\",\u00a0 \u00a0 \"parameterValues\": {\u00a0 \u00a0 \u00a0 \"project\": \"'${PROJECT_ID}'\",\u00a0 \u00a0 \u00a0 \"location\": \"'${REGION}'\",\u00a0 \u00a0 \u00a0 \"batch_predict_gcs_source_uris\": [\"'${TEST_DATASET_URI}'\"],\u00a0 \u00a0 \u00a0 \"batch_predict_gcs_destination_output_uri\": \"'${OUTPUT_DIR}'\",\u00a0 \u00a0 \u00a0 \"model_name\": \"'${MODEL_NAME}'\",\u00a0 \u00a0 }\u00a0 },\u00a0 \"templateUri\": \"https://us-kfp.pkg.dev/vertex-evaluation/pipeline-templates/evaluation-llm-text-generation-pipeline/1.0.1\"}'\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/evaluate_model.py) \n```\nfrom google.auth import defaultimport vertexaifrom vertexai.preview.language_models import (\u00a0 \u00a0 EvaluationTextClassificationSpec,\u00a0 \u00a0 TextGenerationModel,)# Set credentials for the pipeline components used in the evaluation taskcredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])def evaluate_model(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,) -> object:\u00a0 \u00a0 \"\"\"Evaluate the performance of a generative AI model.\"\"\"\u00a0 \u00a0 vertexai.init(project=project_id, location=location, credentials=credentials)\u00a0 \u00a0 # Create a reference to a generative AI model\u00a0 \u00a0 model = TextGenerationModel.from_pretrained(\"text-bison@002\")\u00a0 \u00a0 # Define the evaluation specification for a text classification task\u00a0 \u00a0 task_spec = EvaluationTextClassificationSpec(\u00a0 \u00a0 \u00a0 \u00a0 ground_truth_data=[\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gs://cloud-samples-data/ai-platform/generative_ai/llm_classification_bp_input_prompts_with_ground_truth.jsonl\"\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 class_names=[\"nature\", \"news\", \"sports\", \"health\", \"startups\"],\u00a0 \u00a0 \u00a0 \u00a0 target_column_name=\"ground_truth\",\u00a0 \u00a0 )\u00a0 \u00a0 # Evaluate the model\u00a0 \u00a0 eval_metrics = model.evaluate(task_spec=task_spec)\u00a0 \u00a0 print(eval_metrics)\u00a0 \u00a0 return eval_metrics\n```To create a model evaluation job by using the Google Cloud console, perform the following steps:- In the Vertex AI section of the Google Cloud console, go to  the **Vertex AI Model Registry** page. [Go to Vertex AI Model Registry](https://console.cloud.google.com/vertex-ai/models) \n- Click the name of the model that you want to evaluate.\n- In the **Evaluate** tab, click **Create evaluation** and configure as  follows:\n- **Objective:** Select the task that you want to evaluate.\n- **Target column or field:** (Classification only) Enter the target   column for prediction. Example:`ground_truth`.\n- **Source path:** Enter or select the URI of your evaluation dataset.\n- **Output format:** Enter the format of the evaluation output.   Currently, only`jsonl`is supported.\n- **Cloud Storage path:** Enter or select the URI to store evaluation   output.\n- **Class names:** (Classification only) Enter the list of possible   class names.\n- **Number of compute nodes:** Enter the number of compute nodes to run   the evaluation job.\n- **Machine type:** Select a machine type to use for running the   evaluation job.\n- Click **Start evaluation** \n## View evaluation results\nYou can find the evaluation results in the Cloud Storage output directory that you specified when creating the evaluation job. The file is named `evaluation_metrics.json` .\nFor tuned models, you can also view evaluation results in the Google Cloud console:\n- In the Vertex AI section of the Google Cloud console, go to the **Vertex AI Model Registry** page. [Go to Vertex AI Model Registry](https://console.cloud.google.com/vertex-ai/models) \n- Click the name of the model that you want to view evaluation metrics for.\n- In the **Evaluate** tab, click the name of the evaluation run that you want to view.## What's next\n- Learn about [automatic side-by-side evaluation](/vertex-ai/generative-ai/docs/models/side-by-side-eval) .\n- Learn how to [tune a foundation model](/vertex-ai/generative-ai/docs/models/tune-models) .", "guide": "Generative AI on Vertex AI"}