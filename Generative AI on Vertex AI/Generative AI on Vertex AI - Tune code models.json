{"title": "Generative AI on Vertex AI - Tune code models", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-code-models", "abstract": "# Generative AI on Vertex AI - Tune code models\nCode models can be tuned by using [supervised tuning](/vertex-ai/generative-ai/docs/models/tune-models#supervised-tuning) . Supervised tuning uses labeled examples that demonstrate the type of output you'd like from your code generation or code chat model during inference. Code models don't support tuning by using [Reinforcement learning from human feedback (RLHF) tuning](/vertex-ai/generative-ai/docs/models/tune-models#rlhf-tuning) .\n", "content": "## Scenarios to use code model tuning\nTuning is required when you want a model to learn something niche or specific that deviates from general language and code patterns. The following are examples of what you can teach the `code-bison` and `codechat-bison` models:\n- How to generate code for custom libraries. By training a code model with labeled samples of a custom library, you can generate or chat about code that is specific to that custom library.\n- How to use your code base. By training a model with labeled samples of your code base, you can generate code or chat about code that uses unique qualities in your code base.\n- How to generate code using variants of a programming language. By training a code model with labeled samples of a language variant, you can generate or chat about code that uses that language variant's particular conventions and standards.\nThese scenarios include code requirements that are difficult to capture through prompt instructions alone. The following are some examples:\n- This sample prompt and response helps tune `code-bison` to work with a specific dataset.| 0                                                                                                                                                                                                          |\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Prompt: Python to retrieve the labels of a dataset for a given dataset ID in BigQuery Response: Here is a Python code sample to retrieve the labels of a dataset for a given dataset ID in BigQuery.\\n\\n<code translate=\"no\" dir=\"ltr\">python\\n\\n from google.cloud import bigquery\\n\\n # Construct a BigQuery client object.\\n client = bigquery.Client()\\n\\n # TODO(developer): Set dataset_id to the ID of the dataset to fetch.\\n # dataset_id = \\\"your-project.your_dataset\\\"\\n\\n dataset = client.get_dataset(dataset_id) # Make an API request.\\n\\n # View dataset labels.\\n print(\\\"Dataset ID: {}\\\".format(dataset_id))\\n print(\\\"Labels:\\\")\\n if dataset.labels:\\n for label, value in dataset.labels.items():\\n print(\\\"\\\\t{}: {}\\\".format(label, value))\\n else:\\n print(\\\"\\\\tDataset has no labels defined.\\\")\\n</code> |\n- This sample prompt and response helps train `code-bison` create a product using Python.| 0                                                                                                                                                                                                                                                                                                                                                             |\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Prompt: How to create a product in Python with the Cloud Vision API? Response: Use this Python code sample to learn how to create a product with the Cloud Vision API:\\n\\n<code translate=\"no\" dir=\"ltr\">python\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nfrom google.cloud import vision\\r\\nfrom google.protobuf import field_mask_pb2 as field_mask\\r\\n\\r\\ndef create_product(\\r\\n project_id, location, product_id, product_display_name,\\r\\n product_category):\\r\\n \\\"\\\"\\\"Create one product.\\r\\n Args:\\r\\n project_id: Id of the project.\\r\\n location: A compute region name.\\r\\n product_id: Id of the product.\\r\\n product_display_name: Display name of the product.\\r\\n product_category: Category of the product.\\r\\n \\\"\\\"\\\"\\r\\n client = vision.ProductSearchClient()\\r\\n\\r\\n # A resource that represents Google Cloud Platform location.\\r\\n location_path = f\\\"projects/{project_id}/locations/{location}\\\"\\r\\n\\r\\n # Create a product with the product specification in the region.\\r\\n # Set product display name and product category.\\r\\n product = vision.Product(\\r\\n display_name=product_display_name,\\r\\n product_category=product_category)\\r\\n\\r\\n # The response is the product with the `name` field populated.\\r\\n response = client.create_product(\\r\\n parent=location_path,\\r\\n product=product,\\r\\n product_id=product_id)\\r\\n\\r\\n # Display the product information.\\r\\n print('Product name: {}'.format(response.name))\\r\\n</code> |\n- This sample prompt and response helps tune `codechat-bison` to help a user learn how to work with a specific dataset.| 0                                                                                                                                                                                                                        |\n|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Prompt: Author: User Content: What Python code retrieves the labels of a dataset for a given dataset ID in BigQuery? Response: Author: Assistant Content: Here is a Python code sample to retrieve the labels of a dataset for a given dataset ID in BigQuery.\\n\\n<code translate=\"no\" dir=\"ltr\">python\\n\\n from google.cloud import bigquery\\n\\n # Construct a BigQuery client object.\\n client = bigquery.Client()\\n\\n # TODO(developer): Set dataset_id to the ID of the dataset to fetch.\\n # dataset_id = \\\"your-project.your_dataset\\\"\\n\\n dataset = client.get_dataset(dataset_id) # Make an API request.\\n\\n # View dataset labels.\\n print(\\\"Dataset ID: {}\\\".format(dataset_id))\\n print(\\\"Labels:\\\")\\n if dataset.labels:\\n for label, value in dataset.labels.items():\\n print(\\\"\\\\t{}: {}\\\".format(label, value))\\n else:\\n print(\\\"\\\\tDataset has no labels defined.\\\")\\n</code> |\n- This sample prompt and response helps tune `codechat-bison` to help a user learn how to create a product using Python during a chat session.| 0                                                                                                                                                                                                                                                                                                                                                                          |\n|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Prompt: Author: User Content: How do you create a product in Python with the Cloud Vision API? Response: Author: Assistant Content: Use this Python code sample to learn how to create a product with the Cloud Vision API:\\n\\n<code translate=\"no\" dir=\"ltr\">python\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nfrom google.cloud import vision\\r\\nfrom google.protobuf import field_mask_pb2 as field_mask\\r\\n\\r\\ndef create_product(\\r\\n project_id, location, product_id, product_display_name,\\r\\n product_category):\\r\\n \\\"\\\"\\\"Create one product.\\r\\n Args:\\r\\n project_id: Id of the project.\\r\\n location: A compute region name.\\r\\n product_id: Id of the product.\\r\\n product_display_name: Display name of the product.\\r\\n product_category: Category of the product.\\r\\n \\\"\\\"\\\"\\r\\n client = vision.ProductSearchClient()\\r\\n\\r\\n # A resource that represents Google Cloud Platform location.\\r\\n location_path = f\\\"projects/{project_id}/locations/{location}\\\"\\r\\n\\r\\n # Create a product with the product specification in the region.\\r\\n # Set product display name and product category.\\r\\n product = vision.Product(\\r\\n display_name=product_display_name,\\r\\n product_category=product_category)\\r\\n\\r\\n # The response is the product with the `name` field populated.\\r\\n response = client.create_product(\\r\\n parent=location_path,\\r\\n product=product,\\r\\n product_id=product_id)\\r\\n\\r\\n # Display the product information.\\r\\n print('Product name: {}'.format(response.name))\\r\\n</code> |\n**Important:** Currently, model tuning might improve the model's general knowledge. When tuning a model on a task, if you asked the tuned model a question without including a context, the tuned model might not remember the contexts it was tuned on. We recommend including a context for relevant tasks.\n## Prepare your code model tuning dataset\nThe dataset used to tune a code model includes examples that align with the task that you want the model to perform. Your dataset must include a minimum of 10 examples, but we recommend at least 500 examples for good results. The more examples you give, the better the results.\n### Dataset format for tuning a code model\nThe tuning dataset for a code generation model and a code chat model must be in [JSON Lines](https://jsonlines.org/) (JSONL) format. The following includes details about the dataset format and dataset examples for each model:\nYour code generation model tuning dataset must be in [JSONLines](https://jsonlines.org/) (JSONL) format where each line contains a single tuning example. Each example is composed of an `input_text` field that contains the prompt to the model and an `output_text` field that contains an example response that the tuned model is expected to produce.\nThe maximum number of examples that a dataset for a text generation model can contain is 10,000.This topic shows you two examples of code generation model tuning datasets. The following is the first example of a code generation tuning dataset:\n```\n{\"input_text\": \"How to create a product in Python with the Cloud Vision API?\", \"output_text\": \"Use this\nPython code sample to learn how to create a\nproduct with the Cloud Vision API:\\n\\n```python\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nfrom google.cloud import\nvision\\r\\nfrom google.protobuf import field_mask_pb2 as field_mask\\r\\n\\r\\ndef\ncreate_product(\\r\\n project_id, location, product_id, product_display_name,\\r\\n\nproduct_category):\\r\\n \\\"\\\"\\\"Create one product.\\r\\n Args:\\r\\n project_id: Id of\nthe project.\\r\\n location: A compute region name.\\r\\n product_id: Id of the\nproduct.\\r\\n product_display_name: Display name of the product.\\r\\n\nproduct_category: Category of the product.\\r\\n \\\"\\\"\\\"\\r\\n client =\nvision.ProductSearchClient()\\r\\n\\r\\n # A resource that represents Google Cloud\nPlatform location.\\r\\n location_path =\nf\\\"projects/{project_id}/locations/{location}\\\"\\r\\n\\r\\n # Create a product with\nthe product specification in the region.\\r\\n # Set product display name and\nproduct category.\\r\\n product = vision.Product(\\r\\n\ndisplay_name=product_display_name,\\r\\n\nproduct_category=product_category)\\r\\n\\r\\n # The response is the product with\nthe `name` field populated.\\r\\n response = client.create_product(\\r\\n\nparent=location_path,\\r\\n product=product,\\r\\n product_id=product_id)\\r\\n\\r\\n #\nDisplay the product information.\\r\\n print('Product name:\n{}'.format(response.name))\\r\\n```\"}\n```\nThe following is the second example of a code generation tuning dataset:\n```\n{\"input_text\": \"python to retrieve the labels of a dataset for a given dataset\nID in BigQuery\", \"output_text\": \"Here is a Python code sample to retrieve the labels of a\ndataset for a given dataset ID in BigQuery.\\n\\n```python\\n\\n from google.cloud import\nbigquery\\n\\n # Construct a BigQuery client object.\\n client =\nbigquery.Client()\\n\\n # TODO(developer): Set dataset_id to the ID of the dataset\nto fetch.\\n # dataset_id = \\\"your-project.your_dataset\\\"\\n\\n dataset =\nclient.get_dataset(dataset_id) # Make an API request.\\n\\n # View dataset\nlabels.\\n print(\\\"Dataset ID: {}\\\".format(dataset_id))\\n print(\\\"Labels:\\\")\\n if\ndataset.labels:\\n for label, value in dataset.labels.items():\\n print(\\\"\\\\t{}:\n{}\\\".format(label, value))\\n else:\\n print(\\\"\\\\tDataset has no labels\ndefined.\\\")\\n```\"}\n```\nYour code chat model tuning dataset must be in [JSONLines](https://jsonlines.org/) (JSONL) format where each line contains a single tuning example. Each example is composed of one `messages` field that contains example chat prompts and responses that the tuned model is expected to produce.\nThe maximum number of `author` fields that the examples in the dataset for a code chat model can contain is 10,000. This maximum is for the sum of all `author` fields in all `messages` in all the examples.This topic shows you two examples of code chat model tuning datasets. The following is the first example of a code chat tuning dataset:\n```\n{\"messages\": [{\"author\":\"user\", \"content\": \"How to create a product in\nPython with Cloud Vision?\"}, {\"author\": \"assistant\", \"content\": \"Use this Python code sample to\nlearn how to create a product with Cloud Vision:\\n\\n```python\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nfrom\ngoogle.cloud import vision\\r\\nfrom google.protobuf import field_mask_pb2 as\nfield_mask\\r\\n\\r\\ndef create_product(\\r\\n project_id, location, product_id,\nproduct_display_name,\\r\\n product_category):\\r\\n \\\"\\\"\\\"Create one product.\\r\\n\nArgs:\\r\\n project_id: Id of the project.\\r\\n location: A compute region\nname.\\r\\n product_id: Id of the product.\\r\\n product_display_name: Display name\nof the product.\\r\\n product_category: Category of the product.\\r\\n \\\"\\\"\\\"\\r\\n\nclient = vision.ProductSearchClient()\\r\\n\\r\\n # A resource that represents\nGoogle Cloud Platform location.\\r\\n location_path =\nf\\\"projects/{project_id}/locations/{location}\\\"\\r\\n\\r\\n # Create a product with\nthe product specification in the region.\\r\\n # Set product display name and\nproduct category.\\r\\n product = vision.Product(\\r\\n\ndisplay_name=product_display_name,\\r\\n\nproduct_category=product_category)\\r\\n\\r\\n # The response is the product with\nthe `name` field populated.\\r\\n response = client.create_product(\\r\\n\nparent=location_path,\\r\\n product=product,\\r\\n product_id=product_id)\\r\\n\\r\\n #\nDisplay the product information.\\r\\n print('Product name:\n{}'.format(response.name))\\r\\n```\"}]}\n```\nThe following is the second example of a code chat tuning dataset:\n```\n{\"messages\": [{\"author\":\"user\", \"content\": \"Python to retrieve the labels of a\ndataset for a given dataset ID in BigQuery\"}, {\"author\": \"assistant\", \"content\": \"Here is a\nPython code sample to retrieve the labels of a dataset for a given dataset\nID in BigQuery.\\n\\n```python\\n\\n from google.cloud import bigquery\\n\\n # Construct a\nBigQuery client object.\\n client = bigquery.Client()\\n\\n # TODO(developer): Set\ndataset_id to the ID of the dataset to fetch.\\n # dataset_id =\n\\\"your-project.your_dataset\\\"\\n\\n dataset = client.get_dataset(dataset_id) #\nMake an API request.\\n\\n # View dataset labels.\\n print(\\\"Dataset ID:\n{}\\\".format(dataset_id))\\n print(\\\"Labels:\\\")\\n if dataset.labels:\\n for label,\nvalue in dataset.labels.items():\\n print(\\\"\\\\t{}: {}\\\".format(label, value))\\n\nelse:\\n print(\\\"\\\\tDataset has no labels defined.\\\")\\n```\"}]}\n```\n### Maintain consistency with production data\nThe examples in your datasets should match your expected production traffic. If your dataset contains specific formatting, keywords, instructions, or information, the production data should be formatted in the same way and contain the same instructions.\nFor example, if the examples in your dataset include a `\"question:\"` and a `\"context:\"` , production traffic should also be formatted to include a `\"question:\"` and a `\"context:\"` in the same order as it appears in the dataset examples. If you exclude the context, the model will not recognize the pattern, even if the exact question was in an example in the dataset.\n### Include instructions in examples\nFor tasks such as code generation, you can create a dataset of examples that don't contain instructions. However, excluding instructions from the examples in the dataset leads to worse performance after tuning than including instructions, especially for smaller datasets.\n**Excludes instructions** :\n```\n{\u00a0 \"input_text\": \"Calculate the sum of a list of integers.\",\u00a0 \"output_text\": \"```python\\nnums = [1, 2, 3]\\ntotal_sum = sum(nums)\\n```\"}\n```\n**Includes instructions** :\n```\n{\u00a0 \"input_text\": \"Write the code in Python: calculate the sum of a list of integers\",\u00a0 \"output_text\": \"```python\\nnums = [1, 2, 3]\\ntotal_sum = sum(nums)\\n```\"}\n```\n### Upload tuning datasets to Cloud Storage\nTo run a tuning job, you need to upload one or more datasets to a Cloud Storage bucket. You can either [create a new Cloud Storage bucket](/storage/docs/creating-buckets#create_a_new_bucket) or use an existing one to store dataset files. The region of the bucket doesn't matter, but we recommend that you use a bucket that's in the same Google Cloud project where you plan to tune your model.\nAfter your bucket is ready, [upload](/storage/docs/creating-buckets#create_a_new_bucket) your dataset file to the bucket.\n## Supervised tuning region settings\nYou can specify three Google Cloud region settings when you configure a supervised tuning job. One region is where the pipeline that tunes your model runs. The other region is where the model tuning job runs and the tuned model is uploaded.\n### Pipeline job region\nThe pipeline job region is the region where the pipeline job runs. If the optional [model upload region](#upload-region) isn't specified, then the model is uploaded and deployed to the pipeline job region. Intermediate data, such as the transformed dataset, is stored in the pipeline job region. To learn which regions you can use for the pipeline job region, see [Supported pipeline job and model upload regions](#supported-supervised-tuning-regions) . You must specify the pipeline job region using one of the following methods:\n- If you use the Vertex AI SDK, you can specify the region where the pipeline job runs using the `tuning_job_location` parameter on the `tune_model` method of the object that represents the model you're tuning (for example, the [TextGenerationModel.tune_model](/python/docs/reference/aiplatform/latest/vertexai.language_models.TextGenerationModel#vertexai_language_models_TextGenerationModel_tune_model) method).\n- If you create a supervised tuning job by sending a POST request using the [pipelineJobs.create](/vertex-ai/docs/reference/rest/v1/projects.locations.pipelineJobs/create) method, then you use the URL to specify the region where the pipeline job runs. In the following URL, replacing both instances of with the region where the pipeline runs:```\n\u00a0https://PIPELINE_JOB_REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/PIPELINE_JOB_REGION/pipelineJobs\n```\n- If you use the Google Cloud console to create a supervised model tuning job, then you specify the pipeline job region in the **Region** control when you create your tuning job. In the Google Cloud console, the **Region** control specifies both the pipeline job region and the model upload region. When you use the Google Cloud console to create a supervised model tuning job, both regions are always the same.\n### Model upload region\nYou use the optional `tuned_model_location` parameter to specify where your tuned model is uploaded. If the model upload region isn't specified, then the tuned model is uploaded to the [pipeline job region](#pipeline-region) .You can use one of the [Supported pipeline job and model upload regions](#supported-supervised-tuning-regions) for your model upload region. You can specify the model upload region using one of the following methods:\n- If you use the Vertex AI SDK, the `tuned_model_location` parameter is specified on the `tune_model` method of the object that represents the model you're tuning (for example, the [TextGenerationModel.tune_model](/python/docs/reference/aiplatform/latest/vertexai.language_models.TextGenerationModel#vertexai_language_models_TextGenerationModel_tune_model) method).\n- If you create a supervised model tuning job by sending a POST request using the [pipelineJobs](/vertex-ai/docs/reference/rest/v1/projects.locations.pipelineJobs/create) method, then you can use the `location` parameter to specify the model upload region.\n- If you use the Google Cloud console to create a supervised model tuning job, then you specify the model upload region in the **Region** control when you create your tuning job. In the Google Cloud console, the **Region** control specifies both the model upload region and the pipeline job region. When you use the Google Cloud console to create a supervised model tuning job, both regions are always the same.\n### Model tuning region\nThe model tuning region is where the model tuning computation occurs. This region is determined by the accelerator type you choose. If you specify `TPU` for your accelerator type, then your model tuning computation happens in `europe-west4` . If you specify `GPU` for your accelerator type, then model tuning happens in `us-central1` .\n### Supported pipeline job and model upload regions\nYou can use one of the following regions to specify the model upload region and to specify the pipeline job region:\n- `us-central1`\n- `europe-west4`\n- `asia-southeast1`\n- `us-west1`\n- `europe-west3`\n- `europe-west2`\n- `asia-northeast1`\n- `us-east4`\n- `us-west4`\n- `northamerica-northeast1`\n- `europe-west9`\n- `europe-west1`\n- `asia-northeast3`## Create a code model tuning job\nYou can create a supervised tuning job by using the Google Cloud console, API, or the Vertex AI SDK for Python. For guidance on model tuning configurations, see the [Recommended configurations](#recommended-configurations) .\n### Create a code generation model tuning job\nThe following shows you how to create a code generation model tuning job using the Google Cloud console or REST API commands.\nTo create a code generation model tuning job, send a POST request by using the [pipelineJobs](/vertex-ai/docs/reference/rest/v1/projects.locations.pipelineJobs/create) method.\nBefore using any of the request data, make the following replacements:- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : A display  name for the pipelineJob.\n- : The URI of the bucket to output  pipeline artifacts to.\n- : A display name for the  model uploaded (created) by the pipelineJob.\n- : URI of your dataset file.\n- : (optional) The URI of the JSONL file that contains the evaluation dataset for batch prediction and evaluation. Evaluation isn't supported for`codechat-bison`. For more information, see [Dataset format fortuning a code model](/vertex-ai/generative-ai/docs/models/tune-code-models#dataset-format) . The evaluation dataset requires between ten and 250 examples.\n- : (optional, default`20`) The number of tuning steps between each evaluation. An evaluation interval isn't supported for chat models. Because the evaluation runs on the entire evaluation dataset, a smaller evaluation interval results in a longer tuning time. For example, if`steps`is 200 and`EVAL_INTERVAL`is 100, then you will get only two data points for the evaluation metrics. This parameter requires that the`evaluation_data_uri`is set.\n- : The region where the pipeline tuning job runs. This is also the default region for where the tuned model is uploaded. If you want to upload your model to a different region, then use the`location`parameter to specify the tuned model upload region. For more information, see [Pipeline job region](/vertex-ai/generative-ai/docs/models/tune-code-models#pipeline-region) .\n- : (optional) The region where the tuned model is uploaded. If you don't specify a model upload region, then the tuned model uploads to the same region where the pipeline job runs. For more information, see [Model upload region](/vertex-ai/generative-ai/docs/models/tune-code-models#upload-region) .\n- : (optional, default`GPU`) The type of accelerator to use for model tuning. The valid options are:- `GPU`: Uses eight A100 80 GB GPUs for tuning. Make sure you have enough [quota](/vertex-ai/generative-ai/docs/models/tune-text-models#quota) . If you choose`GPU`, then [VPC\u2011SC](/vertex-ai/docs/general/vpc-service-controls) is supported. [CMEK](/vertex-ai/docs/general/cmek) is supported if the tuning  location and model upload location are`us-centra1`. For more information, see [Supervised tuning region  settings](/vertex-ai/generative-ai/docs/models/tune-code-models#supervised-tuning-regions) . If you choose`GPU`, then your model tuning computations happen in  the`us-central1`region.\n- `TPU`: Uses 64 cores of the TPU v3 pod for tuning. Make sure you have enough [quota](/vertex-ai/generative-ai/docs/models/tune-text-models#quota) . [CMEK](/vertex-ai/docs/general/cmek) isn't supported, but [VPC\u2011SC](/vertex-ai/docs/general/vpc-service-controls) is supported. If you  choose`TPU`, then your model tuning computations happen in the`europe-west4`region.\n- : (optional, default`true`) A`boolean`that, if set to`true`, stops tuning before completing all the tuning steps if model performance, as measured by the accuracy of predicted tokens, does not improve enough between evaluations runs. If`false`, tuning continues until all the tuning steps are complete. This parameter requires that the`evaluation_data_uri`is set. Enable early stopping isn't supported for chat models.\n- : A string value that can be`true`,`false`, or`default`. When set to `true`, Vertex AI selects and returns the checkpoint with the best model evaluation performance from all checkpoints created during the tuning job. When set to `false`, the final checkpoint created during the tuning job is returned. Each checkpoint refers to a snapshot of the model during a tuning job.\n- : (optional) The ID of a [Vertex AI TensorBoard instance](/vertex-ai/docs/experiments/tensorboard-setup#tensorboard_id) . The Vertex AI TensorBoard instance is used to create an [experiment](/vertex-ai/docs/experiments/create-experiment) after the tuning job completes. The Vertex AI TensorBoard instance needs to be in the same region as the tuning pipeline.\n- : (optional) The fully qualified name of a customer-managed encryption key (CMEK) that you want to use for data encryption. A CMEK is available only in`us-central1`. If you use`us-central1`and don't specify a CMEK, then a Google-managed encryption key is used. A Google-managed encryption key is used by default in all other [available regions](/vertex-ai/generative-ai/docs/models/tune-code-models#supported-supervised-tuning-regionss) . For more information, see [CMEK overview](/kms/docs/cmek) .\n- : The number of steps to run for model tuning. The default value is 300. The batch size varies by tuning location and model size. For 8k models, such as`text-bison@002`,`chat-bison@002`,`code-bison@002`, and`codechat-bison@002`:- `us-central1`has a batch size of 8.\n- `europe-west4`has a batch size of 24.\nFor 32k models, such as`text-bison-32k`,`chat-bison-32k`,`code-bison-32k`, and`codechat-bison-32k`:- `us-central1`has a batch size of 8.\n- `europe-west4`has a batch size of 8.\nFor example, if you're training `text-bison@002` in `europe-west4` , there are 240 examples in a training dataset, and you set `steps` to 20, then the number of training examples is the product of 20 steps and the batch size of 24, or 480 training steps. In this case, there are two epochs in the training process because it goes through the examples two times. In `us-central1` , if there are 240 examples in a training dataset and you set `steps` to 15, then the number of training examples is the product of 15 steps and the batch size of 8, or 120 training steps. In this case, there are 0.5 epochs because there are half as many training steps as there are examples.\n- : The step size at each iteration. The default value is 1.\nHTTP method and URL:\n```\nPOST https://PIPELINE_JOB_REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/PIPELINE_JOB_REGION/pipelineJobs\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"PIPELINEJOB_DISPLAYNAME\",\n \"runtimeConfig\": {\n \"gcsOutputDirectory\": \"gs://OUTPUT_DIR\",\n \"parameterValues\": {\n  \"project\": \"PROJECT_ID\",\n  \"model_display_name\": \"MODEL_DISPLAYNAME\",\n  \"dataset_uri\": \"gs://DATASET_URI\",\n  \"evaluation_data_uri\": \"EVAL_DATASET_URI\",\n  \"evaluation_interval\": \"EVAL_INTERVAL\",\n  \"enable_early_stopping\": \"ENABLE_EARLY_STOPPING\",\n  \"enable_checkpoint_selection\": \"ENABLE_CHECKPOINT_SELECTION\",\n  \"tensorboard_resource_id\": \"TENSORBOARD_RESOURCE_ID\",\n  \"location\": \"MODEL_UPLOAD_REGION\",\n  \"accelerator_type\": \"ACCELERATOR_TYPE\",\n  \"large_model_reference\": \"code-bison@002\",\n  \"train_steps\": STEPS,\n  \"learning_rate_multiplier\": LEARNING_RATE_MULTIPLIER\n }\n }\n \"templateUri\": \"https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v3.0.0\"\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://PIPELINE_JOB_REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/PIPELINE_JOB_REGION/pipelineJobs\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://PIPELINE_JOB_REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/PIPELINE_JOB_REGION/pipelineJobs\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following. Note that `pipelineSpec` has been truncated to save space.To tune a code generation or code chat model with supervised tuning by using the Google Cloud console, perform the following steps:- In the Vertex AI section of the Google Cloud console, go to  the **Vertex AI Studio** page. [Go to Vertex AI Studio](https://console.cloud.google.com/vertex-ai/generative/language) \n- Click the **Tune and distill** tab.\n- Clickadd **Create tuned model** .\n- Click **Supervised tuning** .\n- Configure model details:- **Tuned model name** : Enter a name for your tuned model.\n- **Base model** : Select the model that you want to tune.\n- **Region** : Select the region where the pipeline tuning job runs and where the tuned model is deployed.\n- **Output directory** : Enter the Cloud Storage location where artifacts are stored when your model is tuned.\n- Expand **Advanced Options** to configure advanced settings.- **Train steps** : Enter the number of steps to run for model tuning. The default value is 300. The batch size varies by tuning location and model size. For 8k models, such as`text-bison@002`,`chat-bison@002`,`code-bison@002`, and`codechat-bison@002`:- `us-central1`has a batch size of 8.\n- `europe-west4`has a batch size of 24.\nFor 32k models, such as`text-bison-32k`,`chat-bison-32k`,`code-bison-32k`, and`codechat-bison-32k`:- `us-central1`has a batch size of 8.\n- `europe-west4`has a batch size of 8.\nFor example, if you're training `text-bison@002` in `europe-west4` , there are 240 examples in a training dataset, and you set `steps` to 20, then the number of training examples is the product of 20 steps and the batch size of 24, or 480 training steps. In this case, there are two epochs in the training process because it goes through the examples two times. In `us-central1` , if there are 240 examples in a training dataset and you set `steps` to 15, then the number of training examples is the product of 15 steps and the batch size of 8, or 120 training steps. In this case, there are 0.5 epochs because there are half as many training steps as there are examples.\n- **Learning rate multiplier** : Enter the step size at each iteration. The default value is 1.\n- **Accelerator type** : (optional) Enter the type of accelerator to use for model tuning. The valid options are:- `GPU`: Uses eight A100 80 GB GPUs for tuning. Make sure you have enough [quota](/vertex-ai/generative-ai/docs/models/tune-models#quota) . If you choose`GPU`, then [VPC\u2011SC](/vertex-ai/docs/general/vpc-service-controls) is supported. [CMEK](/vertex-ai/docs/general/cmek) is supported if the tuning  location and model upload location are`us-centra1`. For more information, see [  Supervised tuning region settings](/vertex-ai/generative-ai/docs/models/tune-text-models-supervised#supervised-tuning-regions) . If you choose`GPU`, then your model tuning  computations happen in the`us-central1`region.\n- `TPU`: Uses 64 cores of the TPU v3 pod for tuning. Make sure you have enough [quota](/vertex-ai/generative-ai/docs/models/tune-models#quota) . [CMEK](/vertex-ai/docs/general/cmek) isn't supported, but [VPC\u2011SC](/vertex-ai/docs/general/vpc-service-controls) is supported. If you  choose`TPU`, then your model tuning computations happen in the`europe-west4`region.\n- **Add a TensorBoard instance** : (optional) The ID of a [Vertex AI TensorBoard instance](/vertex-ai/docs/experiments/tensorboard-setup#tensorboard_id) . The Vertex AI TensorBoard instance is used to create an [experiment](/vertex-ai/docs/experiments/create-experiment) after the tuning job completes. The Vertex AI TensorBoard instance needs to be in the same region as the tuning pipeline.\n- **Encryption** (optional) Choose to use a Google-managed encryption key or a customer-managed encryption key (CMEK). A CMEK is available for encryption only in the`us-central1`region. In all other [available regions](/vertex-ai/generative-ai/docs/models/tune-text-models-supervised#supported-supervised-tuning-regions) , a Google-managed encryption key is used. For more information, see [CMEK overview](/kms/docs/cmek) .\n- **Service account** (optional) Choose a a user-managed service account. A service account determines which Google Cloud resources your service code can access. If you don't choose a service account, then a Google-managed service account is used that includes permissions appropriate for most models.\n- Click **Continue** \n- If you want to upload your dataset file, selectradio_button_checked **Upload JSONL file to Cloud Storage** . If your  dataset file is already in a Cloud Storage bucket, selectradio_button_checked **Existing JSONL file on Cloud Storage** .- In **Select JSONL file** , click **Browse** and    select your dataset file.\n- In **Dataset location** , click **Browse** and select the Cloud Storage bucket where you want to store your    dataset file.\nIn **Cloud Storage file path** , click **Browse** and select the Cloud Storage bucket where your dataset file is    located.\n- (Optional) To evaluate your tuned model, select **Enable model\n  evaluation** and configure your model evaluation:- **Evaluation dataset** : (optional) The URI of the JSONL file that contains the evaluation dataset for batch prediction and evaluation. Evaluation isn't supported for`codechat-bison`. For more information, see [Dataset format fortuning a code model](/vertex-ai/generative-ai/docs/models/tune-code-models#dataset-format) . The evaluation dataset requires between ten and 250 examples.\n- **Evaluation interval** : (optional, default`20`) The number of tuning steps between each evaluation. An evaluation interval isn't supported for chat models. Because the evaluation runs on the entire evaluation dataset, a smaller evaluation interval results in a longer tuning time. For example, if`steps`is 200 and`EVAL_INTERVAL`is 100, then you will get only two data points for the evaluation metrics. This parameter requires that the`evaluation_data_uri`is set.\n- **Enable early stopping** : (optional, default`true`) A`boolean`that, if set to`true`, stops tuning before completing all the tuning steps if model performance, as measured by the accuracy of predicted tokens, does not improve enough between evaluations runs. If`false`, tuning continues until all the tuning steps are complete. This parameter requires that the`evaluation_data_uri`is set. Enable early stopping isn't supported for chat models.\n- **Enable checkpoint selection** : When enabled, Vertex AI selects and returns the checkpoint with the best model evaluation performance from all checkpoints created during the tuning job. When disabled, the final checkpoint created during the tuning job is returned. Each checkpoint refers to a snapshot of the model during a tuning job.\n- **TensorBoard Id** : (optional) The ID of a [Vertex AI TensorBoard instance](/vertex-ai/docs/experiments/tensorboard-setup#tensorboard_id) . The Vertex AI TensorBoard instance is used to create an [experiment](/vertex-ai/docs/experiments/create-experiment) after the tuning job completes. The Vertex AI TensorBoard instance needs to be in the same region as the tuning pipeline.\n- Click **Start tuning** .\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/tune_code_generation_model.py) \n```\nfrom __future__ import annotationsfrom typing import Optionalfrom google.auth import defaultfrom google.cloud import aiplatformimport pandas as pdimport vertexaifrom vertexai.preview.language_models import CodeGenerationModel, TuningEvaluationSpeccredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])def tune_code_generation_model(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 training_data: pd.DataFrame | str,\u00a0 \u00a0 train_steps: int = 300,\u00a0 \u00a0 evaluation_dataset: Optional[str] = None,\u00a0 \u00a0 tensorboard_instance_name: Optional[str] = None,) -> None:\u00a0 \u00a0 \"\"\"Tune a new model, based on a prompt-response data.\u00a0 \u00a0 \"training_data\" can be either the GCS URI of a file formatted in JSONL format\u00a0 \u00a0 (for example: training_data=f'gs://{bucket}/{filename}.jsonl'), or a pandas\u00a0 \u00a0 DataFrame. Each training example should be JSONL record with two keys, for\u00a0 \u00a0 example:\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \"input_text\": <input prompt>,\u00a0 \u00a0 \u00a0 \u00a0 \"output_text\": <associated output>\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 or the pandas DataFame should contain two columns:\u00a0 \u00a0 \u00a0 ['input_text', 'output_text']\u00a0 \u00a0 with rows for each training example.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 project_id: GCP Project ID, used to initialize vertexai\u00a0 \u00a0 \u00a0 location: GCP Region, used to initialize vertexai\u00a0 \u00a0 \u00a0 training_data: GCS URI of jsonl file or pandas dataframe of training data\u00a0 \u00a0 \u00a0 train_steps: Number of training steps to use when tuning the model.\u00a0 \u00a0 \u00a0 evaluation_dataset: GCS URI of jsonl file of evaluation data.\u00a0 \u00a0 \u00a0 tensorboard_instance_name: The full name of the existing Vertex AI TensorBoard instance:\u00a0 \u00a0 \u00a0 \u00a0 projects/PROJECT_ID/locations/LOCATION_ID/tensorboards/TENSORBOARD_INSTANCE_ID\u00a0 \u00a0 \u00a0 \u00a0 Note that this instance must be in the same region as your tuning job.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 vertexai.init(project=project_id, location=location, credentials=credentials)\u00a0 \u00a0 eval_spec = TuningEvaluationSpec(evaluation_data=evaluation_dataset)\u00a0 \u00a0 eval_spec.tensorboard = aiplatform.Tensorboard(\u00a0 \u00a0 \u00a0 \u00a0 tensorboard_name=tensorboard_instance_name\u00a0 \u00a0 )\u00a0 \u00a0 model = CodeGenerationModel.from_pretrained(\"code-bison@001\")\u00a0 \u00a0 model.tune_model(\u00a0 \u00a0 \u00a0 \u00a0 training_data=training_data,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 train_steps=train_steps,\u00a0 \u00a0 \u00a0 \u00a0 tuning_job_location=\"europe-west4\",\u00a0 \u00a0 \u00a0 \u00a0 tuned_model_location=location,\u00a0 \u00a0 \u00a0 \u00a0 tuning_evaluation_spec=eval_spec,\u00a0 \u00a0 )\u00a0 \u00a0 print(model._job.status)\u00a0 \u00a0 return model\n```\n**Note:** All intermediate pipeline data, such as the processed dataset, is stored in the [pipeline job region](#pipeline-region) . The model tuning computation happens in the [model tuning region](#tuning-region) . The model is deployed to the [model upload region](#upload-region) .\n```\nPROJECT_ID=myprojectDATASET_URI=gs://my-gcs-bucket-uri/datasetEVAL_DATASET_URI=gs://cloud-samples-data/vertex-ai/model-evaluation/eval_sample.jsonlOUTPUT_DIR=gs://my-gcs-bucket-uri/outputACCELERATOR_TYPE=GPULOCATION=us-central1curl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json; charset=utf-8\" \\\"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${REGION}/pipelineJobs?pipelineJobId=tune-large-model-$(date +%Y%m%d%H%M%S)\" -d \\$'{\u00a0 \"displayName\": \"'${PIPELINE_NAME}'\",\u00a0 \"runtimeConfig\": {\u00a0 \u00a0 \"gcsOutputDirectory\": \"'${OUTPUT_DIR}'\",\u00a0 \u00a0 \"parameterValues\": {\u00a0 \u00a0 \u00a0 \"project\": \"'${PROJECT_ID}'\",\u00a0 \u00a0 \u00a0 \"model_display_name\": \"The display name for your model in the UI\",\u00a0 \u00a0 \u00a0 \"dataset_uri\": \"'${DATASET_URI}'\",\u00a0 \u00a0 \u00a0 \"evaluation_data_uri:\": \"'${EVAL_DATASET_URI}'\",\u00a0 \u00a0 \u00a0 \"location\": \"'${LOCATION}'\",\u00a0 \u00a0 \u00a0 \"accelerator_type\": \"'${ACCELERATOR_TYPE}'\",\u00a0 \u00a0 \u00a0 \"large_model_reference\": \"code-bison@002\",\u00a0 \u00a0 \u00a0 \"learning_rate_multiplier\": 1,\u00a0 \u00a0 \u00a0 \"train_steps\": 300\u00a0 \u00a0 }\u00a0 },\u00a0 \"templateUri\": \"https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v3.0.0\"}'\n```\n### Create a code chat model tuning job\nThe following shows you how to create a code chat model tuning job using the Google Cloud console or REST API commands.\nTo create a code chat model tuning job, send a POST request by using the [pipelineJobs](/vertex-ai/docs/reference/rest/v1/projects.locations.pipelineJobs/create) method.\nBefore using any of the request data, make the following replacements:- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : A display  name for the pipelineJob.\n- : The URI of the bucket to output  pipeline artifacs to.\n- : The region where the pipeline tuning job runs. This is also the default region for where the tuned model is uploaded. If you want to upload your model to a different region, then use the`location`parameter to specify the tuned model upload region. For more information, see [Pipeline job region](/vertex-ai/generative-ai/docs/models/tune-code-models#pipeline-region) .\n- : (optional) The region where the tuned model is uploaded. If you don't specify a model upload region, then the tuned model uploads to the same region where the pipeline job runs. For more information, see [Model upload region](/vertex-ai/generative-ai/docs/models/tune-code-models#upload-region) .\n- : (optional, default`GPU`) The type of accelerator to use for model tuning. The valid options are:- `GPU`: Uses eight A100 80 GB GPUs for tuning. Make sure you have enough [quota](/vertex-ai/generative-ai/docs/models/tune-text-models#quota) . If you choose`GPU`, then [VPC\u2011SC](/vertex-ai/docs/general/vpc-service-controls) is supported. [CMEK](/vertex-ai/docs/general/cmek) is supported if the tuning  location and model upload location are`us-centra1`. For more information, see [Supervised tuning region  settings](/vertex-ai/generative-ai/docs/models/tune-code-models#supervised-tuning-regions) . If you choose`GPU`, then your model tuning computations happen in  the`us-central1`region.\n- `TPU`: Uses 64 cores of the TPU v3 pod for tuning. Make sure you have enough [quota](/vertex-ai/generative-ai/docs/models/tune-text-models#quota) . [CMEK](/vertex-ai/docs/general/cmek) isn't supported, but [VPC\u2011SC](/vertex-ai/docs/general/vpc-service-controls) is supported. If you  choose`TPU`, then your model tuning computations happen in the`europe-west4`region.\n- : A display name for the  model uploaded (created) by the pipelineJob.\n- : URI of your dataset file.\n- : (optional) The ID of a [Vertex AI TensorBoard instance](/vertex-ai/docs/experiments/tensorboard-setup#tensorboard_id) . The Vertex AI TensorBoard instance is used to create an [experiment](/vertex-ai/docs/experiments/create-experiment) after the tuning job completes. The Vertex AI TensorBoard instance needs to be in the same region as the tuning pipeline.\n- : (optional) The fully qualified name of a customer-managed encryption key (CMEK) that you want to use for data encryption. A CMEK is available only in`us-central1`. If you use`us-central1`and don't specify a CMEK, then a Google-managed encryption key is used. A Google-managed encryption key is used by default in all other [available regions](/vertex-ai/generative-ai/docs/models/tune-code-models#supported-supervised-tuning-regionss) . For more information, see [CMEK overview](/kms/docs/cmek) .\n- : The  context that applies to all tuning examples in the tuning dataset. Setting the`context`field  in an example overrides the default context.\n- : The number of steps to run for model tuning. The default value is 300. The batch size varies by tuning location and model size. For 8k models, such as`text-bison@002`,`chat-bison@002`,`code-bison@002`, and`codechat-bison@002`:- `us-central1`has a batch size of 8.\n- `europe-west4`has a batch size of 24.\nFor 32k models, such as`text-bison-32k`,`chat-bison-32k`,`code-bison-32k`, and`codechat-bison-32k`:- `us-central1`has a batch size of 8.\n- `europe-west4`has a batch size of 8.\nFor example, if you're training `text-bison@002` in `europe-west4` , there are 240 examples in a training dataset, and you set `steps` to 20, then the number of training examples is the product of 20 steps and the batch size of 24, or 480 training steps. In this case, there are two epochs in the training process because it goes through the examples two times. In `us-central1` , if there are 240 examples in a training dataset and you set `steps` to 15, then the number of training examples is the product of 15 steps and the batch size of 8, or 120 training steps. In this case, there are 0.5 epochs because there are half as many training steps as there are examples.\n- :  The step size at each iteration. The default value is 1.\nHTTP method and URL:\n```\nPOST https://PIPELINE_JOB_REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/PIPELINE_JOB_REGION/pipelineJobs\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"PIPELINEJOB_DISPLAYNAME\",\n \"runtimeConfig\": {\n \"gcsOutputDirectory\": \"gs://OUTPUT_DIR\",\n \"parameterValues\": {\n  \"project\": \"PROJECT_ID\",\n  \"model_display_name\": \"MODEL_DISPLAYNAME\",\n  \"dataset_uri\": \"gs://DATASET_URI\",\n  \"tensorboard_resource_id\": \"TENSORBOARD_RESOURCE_ID\",\n  \"location\": \"MODEL_UPLOAD_REGION\",\n  \"accelerator_type\": \"ACCELERATOR_TYPE\",\n  \"large_model_reference\": \"codechat-bison@002\",\n  \"default_context\": \"DEFAULT_CONTEXT\",\n  \"train_steps\": STEPS,\n  \"learning_rate_multiplier\": LEARNING_RATE_MULTIPLIER\n }\n },\n \"templateUri\": \"https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-chat-model/v3.0.0\"\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://PIPELINE_JOB_REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/PIPELINE_JOB_REGION/pipelineJobs\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://PIPELINE_JOB_REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/PIPELINE_JOB_REGION/pipelineJobs\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following. Note that `pipelineSpec` has been truncated to save space.To tune a code generation or code chat model with supervised tuning by using the Google Cloud console, perform the following steps:- In the Vertex AI section of the Google Cloud console, go to  the **Vertex AI Studio** page. [Go to Vertex AI Studio](https://console.cloud.google.com/vertex-ai/generative/language) \n- Click the **Tune and distill** tab.\n- Clickadd **Create tuned model** .\n- Click **Supervised tuning** .\n- Configure model details:- **Tuned model name** : Enter a name for your tuned model.\n- **Base model** : Select the model that you want to tune.\n- **Region** : Select the region where the pipeline tuning job runs and where the tuned model is deployed.\n- **Output directory** : Enter the Cloud Storage location where artifacts are stored when your model is tuned.\n- Expand **Advanced Options** to configure advanced settings.- **Train steps** : Enter the number of steps to run for model tuning. The default value is 300. The batch size varies by tuning location and model size. For 8k models, such as`text-bison@002`,`chat-bison@002`,`code-bison@002`, and`codechat-bison@002`:- `us-central1`has a batch size of 8.\n- `europe-west4`has a batch size of 24.\nFor 32k models, such as`text-bison-32k`,`chat-bison-32k`,`code-bison-32k`, and`codechat-bison-32k`:- `us-central1`has a batch size of 8.\n- `europe-west4`has a batch size of 8.\nFor example, if you're training `text-bison@002` in `europe-west4` , there are 240 examples in a training dataset, and you set `steps` to 20, then the number of training examples is the product of 20 steps and the batch size of 24, or 480 training steps. In this case, there are two epochs in the training process because it goes through the examples two times. In `us-central1` , if there are 240 examples in a training dataset and you set `steps` to 15, then the number of training examples is the product of 15 steps and the batch size of 8, or 120 training steps. In this case, there are 0.5 epochs because there are half as many training steps as there are examples.\n- **Learning rate multiplier** : Enter the step size at each iteration. The default value is 1.\n- **Accelerator type** : (optional) Enter the type of accelerator to use for model tuning. The valid options are:- `GPU`: Uses eight A100 80 GB GPUs for tuning. Make sure you have enough [quota](/vertex-ai/generative-ai/docs/models/tune-models#quota) . If you choose`GPU`, then [VPC\u2011SC](/vertex-ai/docs/general/vpc-service-controls) is supported. [CMEK](/vertex-ai/docs/general/cmek) is supported if the tuning  location and model upload location are`us-centra1`. For more information, see [  Supervised tuning region settings](/vertex-ai/generative-ai/docs/models/tune-text-models-supervised#supervised-tuning-regions) . If you choose`GPU`, then your model tuning  computations happen in the`us-central1`region.\n- `TPU`: Uses 64 cores of the TPU v3 pod for tuning. Make sure you have enough [quota](/vertex-ai/generative-ai/docs/models/tune-models#quota) . [CMEK](/vertex-ai/docs/general/cmek) isn't supported, but [VPC\u2011SC](/vertex-ai/docs/general/vpc-service-controls) is supported. If you  choose`TPU`, then your model tuning computations happen in the`europe-west4`region.\n- **Add a TensorBoard instance** : (optional) The ID of a [Vertex AI TensorBoard instance](/vertex-ai/docs/experiments/tensorboard-setup#tensorboard_id) . The Vertex AI TensorBoard instance is used to create an [experiment](/vertex-ai/docs/experiments/create-experiment) after the tuning job completes. The Vertex AI TensorBoard instance needs to be in the same region as the tuning pipeline.\n- **Encryption** (optional) Choose to use a Google-managed encryption key or a customer-managed encryption key (CMEK). A CMEK is available for encryption only in the`us-central1`region. In all other [available regions](/vertex-ai/generative-ai/docs/models/tune-text-models-supervised#supported-supervised-tuning-regions) , a Google-managed encryption key is used. For more information, see [CMEK overview](/kms/docs/cmek) .\n- **Service account** (optional) Choose a a user-managed service account. A service account determines which Google Cloud resources your service code can access. If you don't choose a service account, then a Google-managed service account is used that includes permissions appropriate for most models.\n- Click **Continue** \n- If you want to upload your dataset file, selectradio_button_checked **Upload JSONL file to Cloud Storage** . If your  dataset file is already in a Cloud Storage bucket, selectradio_button_checked **Existing JSONL file on Cloud Storage** .- In **Select JSONL file** , click **Browse** and    select your dataset file.\n- In **Dataset location** , click **Browse** and select the Cloud Storage bucket where you want to store your    dataset file.\nIn **Cloud Storage file path** , click **Browse** and select the Cloud Storage bucket where your dataset file is    located.\n- (Optional) To evaluate your tuned model, select **Enable model\n  evaluation** and configure your model evaluation:- **Evaluation dataset** : (optional) The URI of the JSONL file that contains the evaluation dataset for batch prediction and evaluation. Evaluation isn't supported for`codechat-bison`. For more information, see [Dataset format fortuning a code model](/vertex-ai/generative-ai/docs/models/tune-code-models#dataset-format) . The evaluation dataset requires between ten and 250 examples.\n- **Evaluation interval** : (optional, default`20`) The number of tuning steps between each evaluation. An evaluation interval isn't supported for chat models. Because the evaluation runs on the entire evaluation dataset, a smaller evaluation interval results in a longer tuning time. For example, if`steps`is 200 and`EVAL_INTERVAL`is 100, then you will get only two data points for the evaluation metrics. This parameter requires that the`evaluation_data_uri`is set.\n- **Enable early stopping** : (optional, default`true`) A`boolean`that, if set to`true`, stops tuning before completing all the tuning steps if model performance, as measured by the accuracy of predicted tokens, does not improve enough between evaluations runs. If`false`, tuning continues until all the tuning steps are complete. This parameter requires that the`evaluation_data_uri`is set. Enable early stopping isn't supported for chat models.\n- **Enable checkpoint selection** : When enabled, Vertex AI selects and returns the checkpoint with the best model evaluation performance from all checkpoints created during the tuning job. When disabled, the final checkpoint created during the tuning job is returned. Each checkpoint refers to a snapshot of the model during a tuning job.\n- **TensorBoard Id** : (optional) The ID of a [Vertex AI TensorBoard instance](/vertex-ai/docs/experiments/tensorboard-setup#tensorboard_id) . The Vertex AI TensorBoard instance is used to create an [experiment](/vertex-ai/docs/experiments/create-experiment) after the tuning job completes. The Vertex AI TensorBoard instance needs to be in the same region as the tuning pipeline.\n- Click **Start tuning** .**Note:** All intermediate pipeline data, such as the processed dataset, is stored in the [pipeline job region](/vertex-ai/generative-ai/docs/models/tune-code-models#pipeline-region) . The model tuning computation happens in the [model tuning region](/vertex-ai/generative-ai/docs/models/tune-code-models#tuning-region) . The model is deployed to the [model upload region](/vertex-ai/generative-ai/docs/models/tune-code-models#upload-region) .\n```\nPROJECT_ID=myprojectDATASET_URI=gs://my-gcs-bucket-uri/datasetOUTPUT_DIR=gs://my-gcs-bucket-uri/outputACCELERATOR_TYPE=GPULOCATION=us-central1curl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json; charset=utf-8\" \\\"https://${REGION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${REGION}/pipelineJobs?pipelineJobId=tune-large-chat-model-$(date +%Y%m%d%H%M%S)\" -d \\$'{\u00a0 \"displayName\": \"'${PIPELINE_NAME}'\",\u00a0 \"runtimeConfig\": {\u00a0 \u00a0 \"gcsOutputDirectory\": \"'${OUTPUT_DIR}'\",\u00a0 \u00a0 \"parameterValues\": {\u00a0 \u00a0 \u00a0 \"project\": \"'${PROJECT_ID}'\",\u00a0 \u00a0 \u00a0 \"model_display_name\": \"your-model-display-name\",\u00a0 \u00a0 \u00a0 \"dataset_uri\": \"'${DATASET_URI}'\",\u00a0 \u00a0 \u00a0 \"location\": \"'${LOCATION}'\",\u00a0 \u00a0 \u00a0 \"large_model_reference\": \"codechat-bison@002\",\u00a0 \u00a0 \u00a0 \"train_steps\": 300,\u00a0 \u00a0 \u00a0 \"learning_rate_multiplier\": 1,\u00a0 \u00a0 \u00a0 \"encryption_spec_key_name\": \"projects/myproject/locations/us-central1/keyRings/sample-key/cryptoKeys/sample-key\"\u00a0 \u00a0 }\u00a0 },\u00a0 \"encryptionSpec\": {\u00a0 \u00a0 \"kmsKeyName\": \"projects/myproject/locations/us-central1/keyRings/sample-key/cryptoKeys/sample-key\"\u00a0 \"templateUri\": \"https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-chat-model/v3.0.0\"}'\n```\n## Recommended code model tuning configurations\nThe following table shows the recommended configurations for tuning a code model by task:\n| Task   | No. of examples in dataset | Train steps |\n|:----------------|:-----------------------------|:--------------|\n| Code generation | 500+       | 200-1000  |\n| Code chat  | 500+       | 200-1000  |\nFor train steps, you can try more than one value to get the best performance on a particular dataset, for example, 100, 200, 500.\n## View a list of tuned models\nYou can use the Google Cloud console or the Vertex AI SDK for Python to view a list of your tuned code models in your current project.\n### View a list of tuned code models (console)\nTo view your tuned code chat and code generation models in the Google Cloud console, go to the **Vertex AI Model Registry** page.\n[Go to Vertex AI Model Registry](https://console.cloud.google.com/vertex-ai/models)\n### View a list of tuned code generation models (SDK)\nThe following sample code uses the Vertex AI SDK for Python to list the tuned code generation models in your current project:\n```\nimport vertexaifrom vertexai.preview.language_models import CodeGenerationModelmodel = CodeGenerationModel.from_pretrained(\"code-bison@002\").list_tuned_model_names()\n```\n### View a list of tuned code chat models (SDK)\nThe following sample code uses the Vertex AI SDK for Python to list the tuned code chat models in your current project:\n```\nimport vertexaifrom vertexai.preview.language_models import CodeChatModelmodel = CodeChatModel.from_pretrained(\"codechat-bison@002\").list_tuned_model_names()\n```\n## Load a tuned model\nYou can use the Vertex AI SDK for Python to load a tuned code model.\n### Load a tuned code generation model\nThe following sample code uses the Vertex AI SDK for Python to load a tuned code generation model. In the sample code, replace `` with the qualified resource name of your tuned model. This name is in the format `projects/` `` `/locations/` `` `/models/` `` . You can find the model ID of your tuned model in [Vertex AI Model Registry](https://console.cloud.google.com/vertex-ai/models) .\n```\nimport vertexaifrom vertexai.preview.language_models import CodeGenerationModelmodel = CodeGenerationModel.get_tuned_model(TUNED_MODEL_NAME)\n```\n### Load a tuned code chat model\nThe following sample code uses the Vertex AI SDK for Python to load a tuned code chat model:\n```\nimport vertexaifrom vertexai.preview.language_models import CodeChatModelmodel = CodeChatModel.get_tuned_model(TUNED_MODEL_NAME)\n```\n## Tuning and evaluation metrics\nYou can configure a model tuning job to collect and report model tuning and model evaluation metrics, which can then be visualized by using [Vertex AI TensorBoard](/vertex-ai/docs/experiments/tensorboard-introduction) . To connect your tuning job to Vertex AI TensorBoard, specify a [Vertex AI TensorBoard instance ID](/vertex-ai/docs/experiments/tensorboard-setup#tensorboard_id) and an evaluation dataset.\n### Model tuning metrics\nYou can configure a model tuning job to collect the following tuning metrics for `chat-bison` , `code-bison` , `codechat-bison` , and `text-bison` :\n- `/train_total_loss`: Loss for the tuning dataset at a training step.\n- `/train_fraction_of_correct_next_step_preds`: The token accuracy at a training step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the tuning dataset.\n- `/train_num_predictions:`Number of predicted tokens at a training step.\n### Model evaluation metrics:\nYou can configure a model tuning job to collect the following evaluation metrics for `code-bison` and `text-bison` :\n- `/eval_total_loss`: Loss for the evaluation dataset at an evaluation step.\n- `/eval_fraction_of_correct_next_step_preds`: The token accuracy at an evaluation step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the evaluation dataset.\n- `/eval_num_predictions`: Number of predicted tokens at an evaluation step.\nThe metrics visualizations are available after the model tuning job completes. If you specify only a Vertex AI TensorBoard instance ID and not an evaluation dataset when you create the tuning job, only the visualizations for the tuning metrics are available.\n## Quota\nTuning jobs in `us-central1` use eight A100 80 GB GPUs.\nTuning jobs in `europe-west4` use 64 cores of the `TPU v3 pod` custom model training resource.\nIf you don't have enough quota or want to run multiple concurrent tuning jobs in your Google Cloud project, you must [request additional quota](/docs/quotas/view-manage#requesting_higher_quota) :\n- For `us-central1` , submit a request for `Restricted image training Nvidia A100 80 GB GPUs per region` in the `us-central1` region in multiples of eight.\n- For `europe-west4` , submit a request for `Restricted image training TPU V3 pod cores per region` in the `europe-west4` region in multiples of 64.\n## What's next\n- For more models, advanced features, and the ability to transcribe files up to eight hours, see [Speech-to-Text](/speech-to-text/docs/transcribe-console) .", "guide": "Generative AI on Vertex AI"}