{"title": "Generative AI on Vertex AI - Ground responses for PaLM 2 models", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/grounding/ground-language-models", "abstract": "# Generative AI on Vertex AI - Ground responses for PaLM 2 models\n**Preview** This feature is a Preview offering, subject to the Pre-GA Offerings Terms  of the [GCP Service Specific Terms](https://cloud.google.com/terms/service-terms) . Pre-GA products and features may have limited support, and changes to pre-GA  products and features may not be compatible with other pre-GA versions. For more information,  see the [launch stage descriptions](https://cloud.google.com/products#product-launch-stages) .  Further, by using the PaLM API on Vertex AI, you agree to the Generative AI Preview [ terms and conditions ](https://cloud.google.com/trustedtester/aitos?hl=en) (Preview Terms).For PaLM APIs on Vertex AI that are not GA, you can process personal data as outlined in the  Cloud Data Processing Addendum, subject to applicable restrictions and obligations in the  Agreement (as defined in the Preview Terms).\nThis page shows you how to ground model responses to a Vertex AI Search data store by using the Google Cloud console and the Vertex AI API.\n", "content": "## Before you begin\nTo get started with model grounding in Generative AI on Vertex AI, you need to complete some prerequisites. These include creating a Vertex AI Search data source, enabling Enterprise edition for your data store, and linking your data store to your app in Vertex AI Search. The data source serves as the foundation for grounding `text-bison` and `chat-bison` in Vertex AI.\nVertex AI Search helps you get started with high-quality search or recommendations based on data that you provide. To learn more about Vertex AI Search, see the [Introduction to Vertex AI Search](/generative-ai-app-builder/docs/enterprise-search-introduction) .\n### Enable Vertex AI Search\n- In the Google Cloud console, go to the **Search & Conversation** page. [Search & Conversation](https://console.cloud.google.com/gen-app-builder/engines) \n- Read and agree to the Terms of Service, then click **Continue and activatethe API** . **Important:** You must accept the Discovery Solutions data use terms for every project that you want to use Vertex AI Search with.\n### Create a data store in Vertex AI Search\nTo ground your models to your source data, you need to have prepared and saved your data to Vertex AI Search. To do this, you need to create a [data store in Vertex AI Search](/generative-ai-app-builder/docs/create-datastore-ingest#datastores-engines) .\nIf you are starting from scratch, you need to prepare your data for ingestion into Vertex AI Search. See [Prepare data for ingesting](/generative-ai-app-builder/docs/prepare-data) to get started. Depending on the size of your data, ingestion can take several minutes to several hours. Only unstructured data stores are supported for grounding. After you've prepared your data for ingestion, you can [Create a search data store](/generative-ai-app-builder/docs/create-data-store-es) . After you've successfully created a data store, [Create a search app](/generative-ai-app-builder/docs/create-engine-es) to link to it and [Turn Enterprise edition on](/generative-ai-app-builder/docs/enterprise-edition) .\n## Ground the text-bison model\nGrounding is available for the `text-bison` and `chat-bison` models. These following examples use the `text-bison` foundation model.\nIf using the API, you ground the `text-bison` when calling predict. To do this, you add the optional `groundingConfig` and reference your data store location, and your data store ID.\nIf you don't know your data store ID, follow these steps:\n- In the Google Cloud console, go to the **Vertex AI Search** page and in the navigation menu, click **Data stores** . [Go to the Data stores page](https://console.cloud.google.com/gen-app-builder/data-stores) \n- Click the name of your data store.\n- On the **Data** page for your data store, get the data store ID.\nTo test a text prompt by using the Vertex AI API, send a POST request to the publisher model endpoint.\nBefore using any of the request data, make the following replacements:- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : A prompt is a natural language request submitted to a language model to receive a response back. Prompts can contain questions, instructions, contextual information, examples, and text for the model to complete or continue. (Don't add quotes around the prompt here.)\n- : The temperature is used for sampling during response generation, which occurs when`topP`and`topK`are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of`0`means that the highest probability tokens are always selected. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.If the model returns a response that's too generic, too short, or the model gives a fallback response, try increasing the temperature.\n- : Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.Specify a lower value for shorter responses and a higher value for potentially longer responses.\n- : Top-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K) to least probable until the sum of their probabilities equals the top-P value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-P value is`0.5`, then the model will select either A or B as the next token by using temperature and excludes C as a candidate.Specify a lower value for less random responses and a higher value for more random responses.\n- : Top-K changes how the model selects tokens for output. A top-K of`1`means the next selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-K of`3`means that the next token is selected from among the three most probable tokens by using temperature.For each token selection step, the top-K tokens with the highest probabilities are sampled. Then tokens are further filtered based on top-P with the final token selected using temperature sampling.Specify a lower value for less random responses and a higher value for more random responses.\n- The data source type that the model grounds to. Only Vertex AI Search is supported.\n- : The Vertex AI Search data store ID path.The must use the following format. Use the provided values for locations and collections: `projects/{project_id}/locations/global/collections/default_collection/dataStores/{data_store_id}`Note: The project ID in this data store ID path is your Vertex AI Search project ID.\nHTTP method and URL:\n```\nPOST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/text-bison:predict\n```\nRequest JSON body:\n```\n{\n \"instances\": [ { \"prompt\": \"PROMPT\"}\n ],\n \"parameters\": {\n \"temperature\": TEMPERATURE,\n \"maxOutputTokens\": MAX_OUTPUT_TOKENS,\n \"topP\": TOP_P,\n \"topK\": TOP_K,\n \"groundingConfig\": {\n  \"sources\": [   {\n    \"type\": \"VERTEX_AI_SEARCH\",\n    \"vertexAiSearchDatastore\": \"VERTEX_AI_SEARCH_DATA_STORE\"\n   }\n  ]\n }\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/text-bison:predict\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/text-bison:predict\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following.To ground a model from Vertex AI Studio, follow these instructions.\n- Select the **PaLM 2 for Text Bison** or **PaLM 2 for Chat Bison** model card in the Model Garden. [Go to the Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/text-bison) \n- From the model card, click **Open prompt design** . The Vertex AI Studio opens.\n- From the parameters panel, select **Advanced** .\n- Toggle the **Enable Grounding** option and select **Customize** .\n- From the grounding source dropdown, select **Vertex AI Search** .\n- Enter the Vertex AI Search data store path to your content. Path should follow this format:`projects/{project_id}/locations/global/collections/default_collection/dataStores/{data_store_id}`.\n- Enter your prompt and click **Submit** .To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/grounding.py) \n```\nfrom typing import Optionalimport vertexaifrom vertexai.language_models import (\u00a0 \u00a0 GroundingSource,\u00a0 \u00a0 TextGenerationModel,\u00a0 \u00a0 TextGenerationResponse,)def grounding(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 data_store_location: Optional[str],\u00a0 \u00a0 data_store_id: Optional[str],) -> TextGenerationResponse:\u00a0 \u00a0 \"\"\"Grounding example with a Large Language Model\"\"\"\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 # TODO developer - override these parameters as needed:\u00a0 \u00a0 parameters = {\u00a0 \u00a0 \u00a0 \u00a0 \"temperature\": 0.7, \u00a0# Temperature controls the degree of randomness in token selection.\u00a0 \u00a0 \u00a0 \u00a0 \"max_output_tokens\": 256, \u00a0# Token limit determines the maximum amount of text output.\u00a0 \u00a0 \u00a0 \u00a0 \"top_p\": 0.8, \u00a0# Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\u00a0 \u00a0 \u00a0 \u00a0 \"top_k\": 40, \u00a0# A top_k of 1 means the selected token is the most probable among all tokens.\u00a0 \u00a0 }\u00a0 \u00a0 model = TextGenerationModel.from_pretrained(\"text-bison@002\")\u00a0 \u00a0 if data_store_id and data_store_location:\u00a0 \u00a0 \u00a0 \u00a0 # Use Vertex AI Search data store\u00a0 \u00a0 \u00a0 \u00a0 grounding_source = GroundingSource.VertexAISearch(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 data_store_id=data_store_id, location=data_store_location\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 \u00a0 # Use Google Search for grounding (Private Preview)\u00a0 \u00a0 \u00a0 \u00a0 grounding_source = GroundingSource.WebSearch()\u00a0 \u00a0 response = model.predict(\u00a0 \u00a0 \u00a0 \u00a0 \"What are the price, available colors, and storage size options of a Pixel Tablet?\",\u00a0 \u00a0 \u00a0 \u00a0 grounding_source=grounding_source,\u00a0 \u00a0 \u00a0 \u00a0 **parameters,\u00a0 \u00a0 )\u00a0 \u00a0 print(f\"Response from Model: {response.text}\")\u00a0 \u00a0 print(f\"Grounding Metadata: {response.grounding_metadata}\")\n```\n## Pricing\nYou are charged for using PaLM 2 API as well as Vertex AI Search separately.\n## What's next\n- Learn about [responsible AI best practices and Vertex AI's safety filters](/vertex-ai/generative-ai/docs/learn/responsible-ai) .", "guide": "Generative AI on Vertex AI"}