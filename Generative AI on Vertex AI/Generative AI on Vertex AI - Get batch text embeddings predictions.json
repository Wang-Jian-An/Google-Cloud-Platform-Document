{"title": "Generative AI on Vertex AI - Get batch text embeddings predictions", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/batch-prediction-genai-embeddings", "abstract": "# Generative AI on Vertex AI - Get batch text embeddings predictions\nGetting responses in a batch is a way to efficiently send large numbers of non-latency sensitive embeddings requests. Different from getting online responses, where you are limited to one input request at a time, you can send a large number of LLM requests in a single batch request. Similar to how batch prediction is done for [tabular data in Vertex AI](/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions) , you determine your output location, add your input, and your responses asynchronously populate into your output location.\nAfter you submit a batch request and review its results, you can tweak the model through model tuning. After tuning, you can submit your updated model for batch generations as usual. To learn more about tuning models, see [Tune language foundation models](/vertex-ai/generative-ai/docs/models/tune-models) .\n", "content": "## Text embeddings models that support batch predictions\n- `textembedding-gecko@`## Prepare your inputs\nThe input for batch requests are a list of prompts that can either be stored in a BigQuery table or as a [JSON Lines (JSONL)](https://jsonlines.org/) file in Cloud Storage. Each request can include up to 30,000 prompts.\n### JSONL example\nThis section shows examples of how to format JSONL input and output.\n```\n{\"content\":\"Give a short description of a machine learning model:\"}{\"content\":\"Best recipe for banana bread:\"}\n```\n```\n{\"instance\":{\"content\":\"Give...\"},\"predictions\": [{\"embeddings\":{\"statistics\":{\"token_count\":8,\"truncated\":false},\"values\":[0.2,....]}}],\"status\":\"\"}{\"instance\":{\"content\":\"Best...\"},\"predictions\": [{\"embeddings\":{\"statistics\":{\"token_count\":3,\"truncated\":false},\"values\":[0.1,....]}}],\"status\":\"\"}\n```\n### BigQuery example\nThis section shows examples of how to format BigQuery input and output.\nThis example shows a single column BigQuery table.\n| content             |\n|:--------------------------------------------------------|\n| \"Give a short description of a machine learning model:\" |\n| \"Best recipe for banana bread:\"       |\n| content             | predictions                       | status |\n|:--------------------------------------------------------|:----------------------------------------------------------------------------------------------------|---------:|\n| \"Give a short description of a machine learning model:\" | '[{\"embeddings\": { \"statistics\":{\"token_count\":8,\"truncated\":false}, \"Values\":[0.1,....] } } ]' |  nan |\n| \"Best recipe for banana bread:\"       | '[{\"embeddings\": { \"statistics\":{\"token_count\":3,\"truncated\":false}, \"Values\":[0.2,....] } } ]' |  nan |\n## Request a batch response\nDepending on the number of input items that you've submitted, a batch generation task can take some time to complete.\nTo test a text prompt by using the Vertex AI API, send a POST request to the publisher model endpoint.\nBefore using any of the request data, make the following replacements:- : The ID of your Google Cloud project.\n- : The job name.\n- : The input source URI. This is either a BigQuery table URI or a JSONL  file URI in Cloud Storage.\n- : Output target URI.\nHTTP method and URL:\n```\nPOST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/batchPredictionJobs\n```\nRequest JSON body:\n```\n{\n \"name\": \"BP_JOB_NAME\",\n \"displayName\": \"BP_JOB_NAME\",\n \"model\": \"publishers/google/models/textembedding-gecko\",\n \"inputConfig\": {\n  \"instancesFormat\":\"bigquery\",\n  \"bigquerySource\":{\n  \"inputUri\" : \"INPUT_URI\"\n  }\n },\n \"outputConfig\": {\n  \"predictionsFormat\":\"bigquery\",\n  \"bigqueryDestination\":{\n  \"outputUri\": \"OUTPUT_URI\"\n }\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/batchPredictionJobs\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/batchPredictionJobs\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/123456789012/locations/us-central1/batchPredictionJobs/1234567890123456789\",\n \"displayName\": \"BP_sample_publisher_BQ_20230712_134650\",\n \"model\": \"projects/{PROJECT_ID}/locations/us-central1/models/textembedding-gecko\",\n \"inputConfig\": {\n \"instancesFormat\": \"bigquery\",\n \"bigquerySource\": {\n  \"inputUri\": \"bq://project_name.dataset_name.text_input\"\n }\n },\n \"modelParameters\": {},\n \"outputConfig\": {\n \"predictionsFormat\": \"bigquery\",\n \"bigqueryDestination\": {\n  \"outputUri\": \"bq://project_name.llm_dataset.embedding_out_BP_sample_publisher_BQ_20230712_134650\"\n }\n },\n \"state\": \"JOB_STATE_PENDING\",\n \"createTime\": \"2023-07-12T20:46:52.148717Z\",\n \"updateTime\": \"2023-07-12T20:46:52.148717Z\",\n \"labels\": {\n \"owner\": \"sample_owner\",\n \"product\": \"llm\"\n },\n \"modelVersionId\": \"1\",\n \"modelMonitoringStatus\": {}\n}\n```\nThe response includes a unique identifier for the batch job. You can poll for the status of the batch job using the until the job `state` is `JOB_STATE_SUCCEEDED` . For example:\n```\ncurl \\\u00a0 -X GET \\\u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 -H \"Content-Type: application/json\" \\https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/batchPredictionJobs/BATCH_JOB_ID\n```\n **Note:** You can run only one batch response job at a time. Custom Service account, live progress, CMEK, and VPC-SC reports are not supported at this time.\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n```\nfrom vertexai.preview.language_models import TextEmbeddingModeltextembedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko\")batch_prediction_job = textembedding_model.batch_predict(\u00a0 dataset=[\"gs://BUCKET_NAME/test_table.jsonl\"],\u00a0 destination_uri_prefix=\"gs://BUCKET_NAME/tmp/2023-05-25-vertex-LLM-Batch-Prediction/result3\",)print(batch_prediction_job.display_name)print(batch_prediction_job.resource_name)print(batch_prediction_job.state)\n```\n## Retrieve batch output\nWhen a batch prediction task is complete, the output is stored in the Cloud Storage bucket or BigQuery table that you specified in your request.\n## What's next\n- Learn how to [get text embeddings](/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) .", "guide": "Generative AI on Vertex AI"}