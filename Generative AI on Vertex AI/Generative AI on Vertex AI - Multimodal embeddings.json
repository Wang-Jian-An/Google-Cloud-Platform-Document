{"title": "Generative AI on Vertex AI - Multimodal embeddings", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-embeddings", "abstract": "# Generative AI on Vertex AI - Multimodal embeddings\nThe Embeddings for Multimodal ( `multimodalembedding` ) model generates dimension vectors (128, 256, 512, or 1408 dimensions) based on the input you provide. This input which can include any combination text, image, or video. The embedding vectors can then be used for other subsequent tasks like image classification or content moderation.\nThe text, image, and video embedding vectors are in the same semantic space with the same dimensionality. Therefore, these vectors can be used interchangeably for use cases like searching images by text, or searching video by image.\n", "content": "## Use cases\nSome common use cases for multimodal embeddings are:\n- **Image or video classification** : Takes an image or video as input and predicts one or more classes (labels).\n- **Image search** : Search for relevant or similar images.\n- **Video content search** - **Using semantic search** : Take a text as an input, and return a set of ranked frames matching the query.\n- **Using similarity search** :- Take a video as an input, and return a set of videos matching the query.\n- Take an image as an input, and return a set of videos matching the query.\n- **Recommendations** : Generate product or advertisement recommendations based on images or videos (similarity search).\nTo explore this model in the console, see the **Embeddings for Multimodal** model card in the Model Garden.\n[Go to the Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/multimodalembedding)\n## HTTP request\n```\nPOST https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT}/locations/us-central1/publishers/google/models/multimodalembedding:predict\n```\n## Request body\n```\n{\u00a0 \"instances\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"text\": string,\u00a0 \u00a0 \u00a0 \"image\": {\u00a0 \u00a0 \u00a0 \u00a0 // Union field can be only one of the following:\u00a0 \u00a0 \u00a0 \u00a0 \"bytesBase64Encoded\": string,\u00a0 \u00a0 \u00a0 \u00a0 \"gcsUri\": string,\u00a0 \u00a0 \u00a0 \u00a0 // End of list of possible types for union field.\u00a0 \u00a0 \u00a0 \u00a0 \"mimeType\": string\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \"video\": {\u00a0 \u00a0 \u00a0 \u00a0 // Union field can be only one of the following:\u00a0 \u00a0 \u00a0 \u00a0 \"bytesBase64Encoded\": string,\u00a0 \u00a0 \u00a0 \u00a0 \"gcsUri\": string,\u00a0 \u00a0 \u00a0 \u00a0 // End of list of possible types for union field.\u00a0 \u00a0 \u00a0 \u00a0 \"videoSegmentConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"startOffsetSec\": integer,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"endOffsetSec\": integer,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"intervalSec\": integer\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \"parameters\": {\u00a0 \u00a0 \u00a0 \u00a0 \"dimension\": integer\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ]}\n```\nUse the following parameters for the multimodal generation model `multimodal embeddings` . For more information, see [Get multimodal embeddings](/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings) .\n| Parameter       | Description                                                                                                                                                             | Acceptable values                      |\n|:----------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------|\n| instances       | An array that contains the object with data (text, image, and video) to get information about.                                                                                                                                         | array (1 object allowed)                    |\n| text        | The input text you want to create an embedding for.                                                                                                                                                   | String (32 tokens max)                    |\n| image.bytesBase64Encoded   | The image to get embeddings for. If you specify image.bytesBase64Encoded you can't set image.gcsUri.                                                                                                                                       | Base64-encoded image string (BMP, GIF, JPG, or PNG file, 20\u00a0MB max)         |\n| image.gcsUri      | The Cloud Storage URI of the image to get embeddings for. If you specify image.gcsUri you can't set image.bytesBase64Encoded.                                                                                                                                 | string URI of the image file in Cloud Storage (BMP, GIF, JPG, or PNG file, 20\u00a0MB max)     |\n| image.mimeType     | Optional. The MIME type of the image you specify.                                                                                                                                                    | string (image/bmp, image/gif, image/jpeg, or image/png)            |\n| video.bytesBase64Encoded   | The video to get embeddings for. If you specify video.bytesBase64Encoded you can't set video.gcsUri.                                                                                                                                       | Base64-encoded video string (AVI, FLV, MKV, MOV, MP4, MPEG, MPG, WEBM, or WMV file)     |\n| video.gcsUri      | The Cloud Storage URI of the video to get embeddings for. If you specify video.gcsUri you can't set video.bytesBase64Encoded.                                                                                                                                 | string URI of the video file in Cloud Storage (AVI, FLV, MKV, MOV, MP4, MPEG, MPG, WEBM, or WMV file) |\n| videoSegmentConfig.startOffsetSec | Optional. The time (in seconds) where the model begins embedding detection. Default: 0                                                                                                                                           | integer                        |\n| videoSegmentConfig.endOffsetSec | Optional. The time (in seconds) where the model ends embedding detection. Default: 120                                                                                                                                           | integer                        |\n| videoSegmentConfig.intervalSec | Optional. The time (in seconds) of video data segments that embeddings are generated for. This value corresponds to the video embedding mode (Essential, Standard, or Plus), which affects feature pricing. Essential mode (intervalSec >= 15): Fewest segments of video that embeddings are generated for. The lowest cost option. Standard tier (8 <= intervalSec < 15): More segments of video that embeddings are generated for than Essential mode, but fewer than Plus mode. Intermediate cost option. Plus mode (4 <= intervalSec < 8): Most segments of video that embeddings are generated for. The highest cost option. Default: 16 (Essential mode) | integer (minimum value: 4)                   |\n| parameters.dimension    | Optional. The vector dimension to generate embeddings for (text or image only). If not set, the default value of 1408 is used.                                                                                                                                 | integer (128, 256, 512 or 1408 [default])                |\n## Sample request\nThe following example uses image, text, and video data. You can use any combination of these data types in your request body.\nAdditionally, this sample uses a video located in Cloud Storage. You can also use the `video.bytesBase64Encoded` field to provide a [base64-encoded](/vertex-ai/generative-ai/docs/image/base64-encode) string representation of the video.\nBefore using any of the request data, make the following replacements:- : Your project's region. For example,`us-central1`,`europe-west2`, or`asia-northeast3`. For a list of available regions, see [Generative AI on Vertex AI locations](/vertex-ai/generative-ai/docs/learn/locations-genai) .\n- : Your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The target text to get embeddings for. For example,`a cat`.\n- : The Cloud Storage URI of the target video to get embeddings for. For example,`gs://my-bucket/embeddings/supermarket-img.png`.\n- : The Cloud Storage URI of the target video to get embeddings for. For example,`gs://my-bucket/embeddings/supermarket-video.mp4`.\n- `videoSegmentConfig`(,,). Optional. The specific video segments (in seconds) the embeddings are generated for.The value you set for`videoSegmentConfig.intervalSec`affects  the pricing tier you are charged at. For more information, see  the [video embedding modes](/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings#video-modes) section and [pricing](/vertex-ai/generative-ai/pricing) page.For example:```\n[...]\n\"videoSegmentConfig\": {\n \"startOffsetSec\": 10,\n \"endOffsetSec\": 60,\n \"intervalSec\": 10\n}\n[...]\n```Using this config specifies video data from 10 seconds to 60 seconds and generates embeddings  for the following 10 second video intervals: [10, 20), [20, 30), [30, 40), [40, 50), [50, 60).  This video interval ( `\"intervalSec\": 10` ) falls in the [Standard video embedding mode](/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings#video-modes) , and the user  is charged at the [Standard mode pricing rate](/vertex-ai/generative-ai/pricing) .If you omit `videoSegmentConfig` , the service uses the following default values: `\"videoSegmentConfig\": { \"startOffsetSec\": 0, \"endOffsetSec\": 120, \"intervalSec\": 16 }` .  This video interval ( `\"intervalSec\": 16` ) falls in the [Essential video embedding mode](/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings#video-modes) , and the user  is charged at the [Essential mode pricing rate](/vertex-ai/generative-ai/pricing) .\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\n```\nRequest JSON body:\n```\n{\n \"instances\": [ {\n  \"text\": \"TEXT\",\n  \"image\": {\n  \"gcsUri\": \"IMAGE_URI\"\n  },\n  \"video\": {\n  \"gcsUri\": \"VIDEO_URI\",\n  \"videoSegmentConfig\": {\n   \"startOffsetSec\": START_SECOND,\n   \"endOffsetSec\": END_SECOND,\n   \"intervalSec\": INTERVAL_SECONDS\n  }\n  }\n }\n ]\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/multimodalembedding@001:predict\" | Select-Object -Expand Content\n```The embedding the model returns is a 1408 float vector. The following sample response is shortened for space.\n```\n{\n \"predictions\": [ {\n  \"textEmbedding\": [  0.0105433334,\n  -0.00302835181,\n  0.00656806398,\n  0.00603460241,\n  [...]\n  0.00445805816,\n  0.0139605571,\n  -0.00170318608,\n  -0.00490092579\n  ],\n  \"videoEmbeddings\": [  {\n   \"startOffsetSec\": 0,\n   \"endOffsetSec\": 7,\n   \"embedding\": [   -0.00673126569,\n   0.0248149596,\n   0.0128901172,\n   0.0107588246,\n   [...]\n   -0.00180952181,\n   -0.0054573305,\n   0.0117037306,\n   0.0169312079\n   ]\n  }\n  ],\n  \"imageEmbedding\": [  -0.00728622358,\n  0.031021487,\n  -0.00206603738,\n  0.0273937676,\n  [...]\n  -0.00204976718,\n  0.00321615417,\n  0.0121978866,\n  0.0193375275\n  ]\n }\n ],\n \"deployedModelId\": \"DEPLOYED_MODEL_ID\"\n}\n```\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/multimodal_embedding_image_video_text.py) \n```\nfrom typing import Optionalimport vertexaifrom vertexai.vision_models import (\u00a0 \u00a0 Image,\u00a0 \u00a0 MultiModalEmbeddingModel,\u00a0 \u00a0 MultiModalEmbeddingResponse,\u00a0 \u00a0 Video,\u00a0 \u00a0 VideoSegmentConfig,)def get_image_video_text_embeddings(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 image_path: str,\u00a0 \u00a0 video_path: str,\u00a0 \u00a0 contextual_text: Optional[str] = None,\u00a0 \u00a0 dimension: Optional[int] = 1408,\u00a0 \u00a0 video_segment_config: Optional[VideoSegmentConfig] = None,) -> MultiModalEmbeddingResponse:\u00a0 \u00a0 \"\"\"Example of how to generate multimodal embeddings from image, video, and text.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 project_id: Google Cloud Project ID, used to initialize vertexai\u00a0 \u00a0 \u00a0 \u00a0 location: Google Cloud Region, used to initialize vertexai\u00a0 \u00a0 \u00a0 \u00a0 image_path: Path to image (local or Google Cloud Storage) to generate embeddings for.\u00a0 \u00a0 \u00a0 \u00a0 video_path: Path to video (local or Google Cloud Storage) to generate embeddings for.\u00a0 \u00a0 \u00a0 \u00a0 contextual_text: Text to generate embeddings for.\u00a0 \u00a0 \u00a0 \u00a0 dimension: Dimension for the returned embeddings.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings#low-dimension\u00a0 \u00a0 \u00a0 \u00a0 video_segment_config: Define specific segments to generate embeddings for.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-multimodal-embeddings#video-best-practices\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding\")\u00a0 \u00a0 image = Image.load_from_file(image_path)\u00a0 \u00a0 video = Video.load_from_file(video_path)\u00a0 \u00a0 embeddings = model.get_embeddings(\u00a0 \u00a0 \u00a0 \u00a0 image=image,\u00a0 \u00a0 \u00a0 \u00a0 video=video,\u00a0 \u00a0 \u00a0 \u00a0 video_segment_config=video_segment_config,\u00a0 \u00a0 \u00a0 \u00a0 contextual_text=contextual_text,\u00a0 \u00a0 \u00a0 \u00a0 dimension=dimension,\u00a0 \u00a0 )\u00a0 \u00a0 print(f\"Image Embedding: {embeddings.image_embedding}\")\u00a0 \u00a0 # Video Embeddings are segmented based on the video_segment_config.\u00a0 \u00a0 print(\"Video Embeddings:\")\u00a0 \u00a0 for video_embedding in embeddings.video_embeddings:\u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f\"Video Segment: {video_embedding.start_offset_sec} - {video_embedding.end_offset_sec}\"\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Embedding: {video_embedding.embedding}\")\u00a0 \u00a0 print(f\"Text Embedding: {embeddings.text_embedding}\")\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/predict-image-from-image-and-text.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// const bastImagePath = \"YOUR_BASED_IMAGE_PATH\"// const textPrompt = 'YOUR_TEXT_PROMPT';const aiplatform = require('@google-cloud/aiplatform');// Imports the Google Cloud Prediction service clientconst {PredictionServiceClient} = aiplatform.v1;// Import the helper module for converting arbitrary protobuf.Value objects.const {helpers} = aiplatform;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};const publisher = 'google';const model = 'multimodalembedding@001';// Instantiates a clientconst predictionServiceClient = new PredictionServiceClient(clientOptions);async function predictImageFromImageAndText() {\u00a0 // Configure the parent resource\u00a0 const endpoint = `projects/${project}/locations/${location}/publishers/${publisher}/models/${model}`;\u00a0 const fs = require('fs');\u00a0 const imageFile = fs.readFileSync(baseImagePath);\u00a0 // Convert the image data to a Buffer and base64 encode it.\u00a0 const encodedImage = Buffer.from(imageFile).toString('base64');\u00a0 const prompt = {\u00a0 \u00a0 text: textPrompt,\u00a0 \u00a0 image: {\u00a0 \u00a0 \u00a0 bytesBase64Encoded: encodedImage,\u00a0 \u00a0 },\u00a0 };\u00a0 const instanceValue = helpers.toValue(prompt);\u00a0 const instances = [instanceValue];\u00a0 const parameter = {\u00a0 \u00a0 sampleCount: 1,\u00a0 };\u00a0 const parameters = helpers.toValue(parameter);\u00a0 const request = {\u00a0 \u00a0 endpoint,\u00a0 \u00a0 instances,\u00a0 \u00a0 parameters,\u00a0 };\u00a0 // Predict request\u00a0 const [response] = await predictionServiceClient.predict(request);\u00a0 console.log('Get image embedding response');\u00a0 const predictions = response.predictions;\u00a0 console.log('\\tPredictions :');\u00a0 for (const prediction of predictions) {\u00a0 \u00a0 console.log(`\\t\\tPrediction : ${JSON.stringify(prediction)}`);\u00a0 }}await predictImageFromImageAndText();\n```Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/PredictImageFromImageAndTextSample.java) \n```\nimport com.google.cloud.aiplatform.v1beta1.EndpointName;import com.google.cloud.aiplatform.v1beta1.PredictResponse;import com.google.cloud.aiplatform.v1beta1.PredictionServiceClient;import com.google.cloud.aiplatform.v1beta1.PredictionServiceSettings;import com.google.gson.Gson;import com.google.gson.JsonObject;import com.google.protobuf.InvalidProtocolBufferException;import com.google.protobuf.Value;import com.google.protobuf.util.JsonFormat;import java.io.IOException;import java.nio.charset.StandardCharsets;import java.nio.file.Files;import java.nio.file.Paths;import java.util.ArrayList;import java.util.Base64;import java.util.HashMap;import java.util.List;import java.util.Map;public class PredictImageFromImageAndTextSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace this variable before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String textPrompt = \"YOUR_TEXT_PROMPT\";\u00a0 \u00a0 String baseImagePath = \"YOUR_BASE_IMAGE_PATH\";\u00a0 \u00a0 // Learn how to use text prompts to update an image:\u00a0 \u00a0 // https://cloud.google.com/vertex-ai/docs/generative-ai/image/edit-images\u00a0 \u00a0 Map<String, Object> parameters = new HashMap<String, Object>();\u00a0 \u00a0 parameters.put(\"sampleCount\", 1);\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 String publisher = \"google\";\u00a0 \u00a0 String model = \"multimodalembedding@001\";\u00a0 \u00a0 predictImageFromImageAndText(\u00a0 \u00a0 \u00a0 \u00a0 project, location, publisher, model, textPrompt, baseImagePath, parameters);\u00a0 }\u00a0 // Update images using text prompts\u00a0 public static void predictImageFromImageAndText(\u00a0 \u00a0 \u00a0 String project,\u00a0 \u00a0 \u00a0 String location,\u00a0 \u00a0 \u00a0 String publisher,\u00a0 \u00a0 \u00a0 String model,\u00a0 \u00a0 \u00a0 String textPrompt,\u00a0 \u00a0 \u00a0 String baseImagePath,\u00a0 \u00a0 \u00a0 Map<String, Object> parameters)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 final String endpoint = String.format(\"%s-aiplatform.googleapis.com:443\", location);\u00a0 \u00a0 final PredictionServiceSettings predictionServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceSettings.newBuilder().setEndpoint(endpoint).build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests.\u00a0 \u00a0 try (PredictionServiceClient predictionServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceClient.create(predictionServiceSettings)) {\u00a0 \u00a0 \u00a0 final EndpointName endpointName =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 EndpointName.ofProjectLocationPublisherModelName(project, location, publisher, model);\u00a0 \u00a0 \u00a0 // Convert the image to Base64\u00a0 \u00a0 \u00a0 byte[] imageData = Base64.getEncoder().encode(Files.readAllBytes(Paths.get(baseImagePath)));\u00a0 \u00a0 \u00a0 String encodedImage = new String(imageData, StandardCharsets.UTF_8);\u00a0 \u00a0 \u00a0 JsonObject jsonInstance = new JsonObject();\u00a0 \u00a0 \u00a0 jsonInstance.addProperty(\"text\", textPrompt);\u00a0 \u00a0 \u00a0 JsonObject jsonImage = new JsonObject();\u00a0 \u00a0 \u00a0 jsonImage.addProperty(\"bytesBase64Encoded\", encodedImage);\u00a0 \u00a0 \u00a0 jsonInstance.add(\"image\", jsonImage);\u00a0 \u00a0 \u00a0 Value instanceValue = stringToValue(jsonInstance.toString());\u00a0 \u00a0 \u00a0 List<Value> instances = new ArrayList<>();\u00a0 \u00a0 \u00a0 instances.add(instanceValue);\u00a0 \u00a0 \u00a0 Gson gson = new Gson();\u00a0 \u00a0 \u00a0 String gsonString = gson.toJson(parameters);\u00a0 \u00a0 \u00a0 Value parameterValue = stringToValue(gsonString);\u00a0 \u00a0 \u00a0 PredictResponse predictResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 predictionServiceClient.predict(endpointName, instances, parameterValue);\u00a0 \u00a0 \u00a0 System.out.println(\"Predict Response\");\u00a0 \u00a0 \u00a0 System.out.println(predictResponse);\u00a0 \u00a0 \u00a0 for (Value prediction : predictResponse.getPredictionsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\tPrediction: %s\\n\", prediction);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\u00a0 // Convert a Json string to a protobuf.Value\u00a0 static Value stringToValue(String value) throws InvalidProtocolBufferException {\u00a0 \u00a0 Value.Builder builder = Value.newBuilder();\u00a0 \u00a0 JsonFormat.parser().merge(value, builder);\u00a0 \u00a0 return builder.build();\u00a0 }}\n```\n## Response body\n```\n{\u00a0 \"predictions\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"textEmbedding\": [\u00a0 \u00a0 \u00a0 \u00a0 float,\u00a0 \u00a0 \u00a0 \u00a0 // array of 128, 256, 512, or 1408 float values\u00a0 \u00a0 \u00a0 \u00a0 float\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"imageEmbedding\": [\u00a0 \u00a0 \u00a0 \u00a0 float,\u00a0 \u00a0 \u00a0 \u00a0 // array of 128, 256, 512, or 1408 float values\u00a0 \u00a0 \u00a0 \u00a0 float\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"videoEmbeddings\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"startOffsetSec\": integer,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"endOffsetSec\": integer,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"embedding\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 float,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // array of 1408 float values\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 float\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 }\u00a0 ],\u00a0 \"deployedModelId\": string}\n```\n| Response element | Description                              |\n|:-------------------|:-----------------------------------------------------------------------------------------------------------------------------------|\n| imageEmbedding  | 128, 256, 512, or 1408 dimension list of floats.                     |\n| textEmbedding  | 128, 256, 512, or 1408 dimension list of floats.                     |\n| videoEmbeddings | 1408 dimension list of floats with the start and end time (in seconds) of the video segment that the embeddings are generated for. |", "guide": "Generative AI on Vertex AI"}