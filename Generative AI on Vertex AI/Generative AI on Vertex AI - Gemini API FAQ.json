{"title": "Generative AI on Vertex AI - Gemini API FAQ", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/multimodal-faqs", "abstract": "# Generative AI on Vertex AI - Gemini API FAQ\nThis document provides answers to frequently asked questions (FAQs) about Gemini API, organized into the following categories:\n- [Model comparisons](#model-comparisons) \n- [Safety and data usage](#data-usage) \n- [Migration](#migration) \n- [Availability and pricing](#availability-pricing) \n- [Quotas](#quotas) ", "content": "## Model comparisons\n### What is the difference between PaLM and Gemini?\nGemini models are designed for multimodal applications. Gemini models accept prompts that include, for example, text and images, and then return a text response. Gemini also supports [function calling](/vertex-ai/generative-ai/docs/multimodal/function-calling) , which lets developers pass a description of a function and then the model returns a function and parameters that best matches the description. Developers can then call that function in external APIs and services.\nPaLM 2 models are Generally Available (GA). PaLM 2 models are designed for language applications and perform well on use cases such as text summarization and text generation. PaLM 2 also offers full support for MLOps services on Vertex AI, such as auto side-by-side comparison and model monitoring, which aren't available with Gemini.\nWith Vertex AI Studio, you can customize both Gemini and PaLM 2 models with full data controls and take advantage of Google Cloud's security, safety, privacy, and data governance and compliance support. Prompts and tuning data for both Gemini and PaLM 2 are never used to train or enhance our foundation models.\n### Why would you choose PaLM over Gemini?\nFor use cases that exclusively require text input-output (like text summarization, text generation, and Q&A), PaLM 2 models can provide sufficiently high quality responses.\nGemini models are a good fit for use cases that include multimodal input, require function calling, or require complex prompting techniques (like chain-of-thought and complex instruction following).\n### Is PaLM 2 being deprecated?\nThere are no plans to deprecate PaLM 2.\n### What is the difference between Imagen on Vertex AI and Gemini API for vision use cases?\nImagen is a vision model for image generation, editing, captioning, and Q&A use cases. As part of your prompts, Gemini can take multiple images or a video and provide answers about your inputs, where as Imagen can take only a one input image. Gemini doesn't support image generation or image editing.\n### What is the difference between Vertex AI Codey APIs and Gemini API for coding use cases?\nCodey APIs is purpose-built for code generation, code completion, and code chat. The Codey APIs are powered by Gemini and other models developed by Google. You can use the APIs across the software development lifecycle by integrating it into IDEs, CI/CD workflows, dashboards, and other applications. You can also customize the models with your codebase. We don't recommend Gemini 1.0 Pro Vision for code generation.\n### How do I send a prompt to the Gemini 1.0 Pro or Gemini 1.0 Pro Vision model\nThere are a number of different methods you can use to send requests to the Gemini API. You can, for example, use Google Cloud console, a programming language SDK, or the REST API to send requests to `gemini-1.0-pro` (Gemini 1.0 Pro) or `gemini-1.0-pro-vision` (Gemini 1.0 Pro Vision).\nTo get started, see [Try the Gemini API](/vertex-ai/generative-ai/docs/start/quickstarts/quickstart-multimodal) .\n### Is fine tuning available for Gemini?\nFine tuning for Gemini is coming soon.\n## Safety and data usage\n### Why are my responses blocked?\nGenerative AI on Vertex AI uses safety filters to prevent potentially harmful responses. You can adjust this safety filter threshold. For more information, see [Responsible AI](/vertex-ai/generative-ai/docs/learn/responsible-ai) .\n### How is my input data used?\nGoogle ensures that its teams are following our AI/ML privacy commitment through robust data governance practices, which include reviews of the data that Google Cloud uses in the development of its products. For details, see [Generative AI and Data Governance](/vertex-ai/generative-ai/docs/data-governance)\n## Migration\n### How do I migrate Gemini on Google AI Studio to Vertex AI Studio?\nMigrating to Google Cloud's Vertex AI platform offers a suite of MLOps tools that streamline the usage, deployment, and monitoring of AI models for efficiency and reliability. To migrate your work to Vertex AI, import and upload your existing data to Vertex AI Studio and use the Vertex AI Gemini API. For more information, see [Migratefrom Gemini on Google AI to Vertex AI](/vertex-ai/generative-ai/docs/migrate/migrate-google-ai) .\n### How do I switch from PaLM 2 to Vertex AI Gemini API as the underlying model?\nYou don't need to make any major architectural changes to your applications when switching from PaLM models to Gemini models. From an API perspective, switching from one model to another requires changing a single line of code or updating the SDK. For more information, see [Migrate from PaLM API toVertex AI Gemini API](/vertex-ai/generative-ai/docs/migrate/migrate-palm-to-gemini) .\nBecause responses can vary between models, we recommend you do prompt testing to compare the responses of PaLM and Gemini models to check that responses meet your expectations.\n## Availability and pricing\n### In what locations is Gemini available?\nGemini 1.0 Pro and Gemini 1.0 Pro Vision are available in the Asia, US, and Europe regions. For more information, see [Generative AI on Vertex AI locations](/vertex-ai/generative-ai/docs/learn/locations-genai) .\n### Is there a free evaluation tier for the Vertex AI Gemini API?\n[Contact](/contact) your Google Cloud representative for more information.\n### What is the pricing for Vertex AI Gemini API?\nPricing information for Gemini models is available in the **Multimodal** section of the [Pricing for Generative AI on Vertex AI](/vertex-ai/generative-ai/pricing#generative_ai_models) .\n### How do I get access to Gemini Ultra?\nContact your Google account representative to request access.\n## Quotas\n### How do I resolve a quota (429) error when making API requests?\nThere is either excessive demand or the request exceeded your per-project quota. Check that your request rate is less than the quota for your project. To view you project quotas, go to the **Quotas** page in the Google Cloud console. For more information, see [Generative AI on Vertex AI onVertex AI quota and limits](/vertex-ai/generative-ai/docs/quotas-genai) .\n### How do I increase my project quotas for Gemini?\nYou can request an increase from the Google Cloud console. For more information, see [Generative AI on Vertex AI on Vertex AI quota andlimits](/vertex-ai/generative-ai/docs/quotas-genai) .", "guide": "Generative AI on Vertex AI"}