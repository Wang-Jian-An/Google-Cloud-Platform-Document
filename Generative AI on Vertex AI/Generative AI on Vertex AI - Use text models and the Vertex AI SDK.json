{"title": "Generative AI on Vertex AI - Use text models and the Vertex AI SDK", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/sdk-for-llm/sdk-use-text-models", "abstract": "# Generative AI on Vertex AI - Use text models and the Vertex AI SDK\nThere are three types of generative AI text foundation models in Vertex AI. There's a text generation model, a text chat model, and a text embedding model. Text chat and text generation models generate text. The text embedding model generates a vector representation of text that you use to find similar items.\n- The text generation model names are `text-bison` and `text-unicorn` and their class in the Vertex AI SDK is [TextGenerationModel](/python/docs/reference/aiplatform/latest/vertexai.language_models.TextGenerationModel) .\n- The text chat model name is `chat-bison` and its class in the Vertex AI SDK is [ChatModel](/python/docs/reference/aiplatform/latest/vertexai.language_models.ChatModel) .\n- The text embedding model name is `textembedding-gecko` and its class in the Vertex AI SDK is [TextEmbeddingModel](/python/docs/reference/aiplatform/latest/vertexai.language_models.TextEmbeddingModel) .\nThe following topics show you how to use these classes and the Vertex AI SDK to perform some common generative AI tasks.\n", "content": "## Generate text\nYou can use the Vertex AI SDK `TextGenerationModel` class to generate text. The following sample code loads a [stable version](/vertex-ai/generative-ai/docs/learn/model-versioning#stable-version)\nversion of the `text-bison` model, then uses the `predict` method to generate a recipe. This code sample doesn't include optional parameters. The predict method returns a [TextGenerationResponse](/python/docs/reference/aiplatform/latest/vertexai.language_models.TextGenerationResponse) object that has `text` , `safety_attributes` and `is_blocked` attributes. To learn more about generating text with the text generation foundation model, see [Design text prompts](/vertex-ai/generative-ai/docs/text/text-prompts) and [Test text prompts](/vertex-ai/generative-ai/docs/text/test-text-prompts) .\n```\nfrom vertexai.language_models import TextGenerationModelmodel = TextGenerationModel.from_pretrained(\"text-bison@002\")print(model.predict(\u00a0 \u00a0 \"What is the best recipe for banana bread? Recipe:\",\u00a0 \u00a0 # The following are optional parameters:\u00a0 \u00a0 #max_output_tokens=128,\u00a0 \u00a0 #temperature=0,\u00a0 \u00a0 #top_p=1,\u00a0 \u00a0 #top_k=5,))\n```\nThe beginning of the output might be similar to the following:\n```\nIngredients:\n* 3 very ripe bananas, mashed\n* 1/2 cup (1 stick) unsalted butter, at room temperature\n* 3/4 cup granulated sugar\n* 3/4 cup packed light brown sugar\n* 2 large eggs\n* 2 teaspoons vanilla extract\n* 1 1/2 cups all-purpose flour\n* 1 teaspoon baking soda\n* 1/2 teaspoon salt\n* 1/2 cup chopped walnuts or pecans (optional)\nInstructions:\n1. Preheat oven to 350 degrees F\n ...\n```\n## Generate text chat\nThe following sample code shows you how to load a [stable version](/vertex-ai/generative-ai/docs/learn/model-versioning#stable-version) version of the text chat foundation model. Next, it uses the `start_chat` method to begin a chat, and the `send_message` method to send chat messages. To learn more about using the text chat foundation model, see [Design chat prompts](/vertex-ai/generative-ai/docs/chat/chat-prompts) and [Test chat prompts](/vertex-ai/generative-ai/docs/chat/test-chat-prompts) .\n```\nfrom vertexai.language_models import ChatModel, InputOutputTextPairchat_model = ChatModel.from_pretrained(\"chat-bison@002\")chat = chat_model.start_chat(\u00a0 \u00a0 # Optional parameters, such ase top_p, top_k, temperature, max_output_tokens,\u00a0 \u00a0 # aren't specified in this example\u00a0 \u00a0 context=\"My name is Ned. You are my personal assistant. My favorite movies are Lord of the Rings and Hobbit.\",\u00a0 \u00a0 examples=[\u00a0 \u00a0 \u00a0 \u00a0 InputOutputTextPair(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 input_text=\"Who do you work for?\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output_text=\"I work for Ned.\",\u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 InputOutputTextPair(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 input_text=\"What do I like?\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output_text=\"Ned likes watching movies.\",\u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 ],)print(chat.send_message(\"Are my favorite movies based on a book series?\"))\n```\nThe `send_message` output might be similar to the following:\n```\nYes, your favorite movies are based on a book series.\n```\nThe following `send_message` sends a second message using the same text chat session.\n```\nprint(chat.send_message(\"When where these books published?\"))\n```\nThe output for this second `send_message` might be similar to the following:\n```\nThe books were published in 1954 and 1955.\n```\n## Stream text model responses\nYou might want to receive responses from the text generation and text chat models as they're generated. Receiving responses from a foundation model as the responses are generated is known as streaming. When text generation and text chat model responses are streamed, the output tokens are sent when they're generated. To stream text generation, use the `TextGenerationModel.predict_streaming` method. To stream text chat, use the `ChatModel.predict_streaming` method. To learn more about streaming from foundation models, see [Stream responses from Generative AI models](/vertex-ai/generative-ai/docs/learn/streaming) .\n### Stream text generation\nThe following sample code streams text that counts to 100 as the text is generated. It also outputs the time before and the time after `from_pretrained` is called to demonstrate how long it takes to stream the output.\n```\nimport datetimefrom vertexai.language_models import TextGenerationModeltext_generation_model = TextGenerationModel.from_pretrained(\"text-bison@002\")print(\"Start: \", datetime.datetime.now())for response in text_generation_model.predict_streaming(\u00a0 \u00a0 prompt=\"Count to 100\",\u00a0 \u00a0 max_output_tokens=1000,\u00a0 \u00a0 # The following parameters are optional\u00a0 \u00a0 #temperature=0,\u00a0 \u00a0 #top_p=1,\u00a0 \u00a0 #top_k=5,):\u00a0 \u00a0 print(datetime.datetime.now(), \"|\", response)print(\"End: \", datetime.datetime.now())\n```\nThe response might be similar to the following:\n```\nStart: \u00a0YYYY-MM-DD 06:31:07.825599YYYY-MM-DD 06:31:08.933534 | 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 1YYYY-MM-DD 06:31:09.335374 | 9. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 3YYYY-MM-DD 06:31:09.738079 | 5. 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 5YYYY-MM-DD 06:31:10.142726 | 1. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 6YYYY-MM-DD 06:31:10.535045 | 7. 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 8YYYY-MM-DD 06:31:10.937847 | 3. 84. 85. 86. 87. 88. 89. 90. 91. 92. 93. 94. 95. 96. 97. 98. 9YYYY-MM-DD 06:31:10.996782 | 9. 100.End: \u00a0YYYY-MM-DD 06:31:10.998498\n```\n### Stream text chat\nThe following sample code streams text chat that is in response to a chatbot request to count to 99. The code sample also outputs the time before and the time after `from_pretrained` is called to demonstrate how long it takes to stream the output.\n```\nimport datetimefrom vertexai.language_models import ChatModelchat_model = ChatModel.from_pretrained(\"chat-bison@002\")chat = chat_model.start_chat()print(\"Start: \", datetime.datetime.now())for response in chat.send_message_streaming(\u00a0 \u00a0 message=\"Hello. How are you today? Please count to 99\",\u00a0 \u00a0 max_output_tokens=1024,):\u00a0 \u00a0 print(datetime.datetime.now(), \"|\", response)print(\"End: \", datetime.datetime.now())\n```\nThe response might be similar to the following:\n```\nStart: YYYY-MM-DD 06:31:19.808735\nYYYY-MM-DD 06:31:20.957465 | Hello, I am doing well today. Thank you for asking. 1, 2, 3, 4,\nYYYY-MM-DD 06:31:21.312577 | 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 2\nYYYY-MM-DD 06:31:DD.709306 | 2, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 3\nYYYY-MM-DD 06:31:22.132016 | 8, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 5\nYYYY-MM-DD 06:31:22.517211 | 4, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 7\nYYYY-MM-DD 06:31:22.911003 | 0, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 8\nYYYY-MM-DD 06:31:23.257773 | 6, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99.\nEnd: YYYY-MM-DD 06:31:23.265454\n```\n## Generate text embeddings\nYou can use the `TextEmbeddingModel` class in the Vertex AI SDK to calculate text embeddings. The following Python code sample uses the `TextEmbeddingModel.get_embeddings` method to generate text embeddings using a prompt. In this example, `get_embeddings` returns one `embeddings` object that contains one `embedding` object. The example prints the length and statistics of the returned vector. To learn more about text embeddings and the text embedding foundation model, see [Get text embeddings](/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) .\n```\nfrom vertexai.language_models import TextEmbeddingModelmodel = TextEmbeddingModel.from_pretrained(\"textembedding_gecko_current\")embeddings = model.get_embeddings([\"What is life?\"])for embedding in embeddings:\u00a0 \u00a0 vector = embedding.values\u00a0 \u00a0 print(len(vector))\u00a0 \u00a0 print(embedding.statistics)\n```\nThe output is similar to the following:\n```\n768\nTextEmbeddingStatistics(token_count=4.0, truncated=False)\n```\n## What's next\n- Learn about [use code model classes and the Vertex AI SDK](/vertex-ai/generative-ai/docs/sdk-for-llm/sdk-use-code-models) .\n- Learn how to [use the Vertex AI SDK to tune foundation models](/vertex-ai/generative-ai/docs/sdk-for-llm/sdk-tune-models) .\n- Learn about [Vertex AI SDK classes not related to generative AI](/vertex-ai/docs/python-sdk/python-sdk-class-overview) .", "guide": "Generative AI on Vertex AI"}