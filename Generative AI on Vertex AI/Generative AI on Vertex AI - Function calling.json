{"title": "Generative AI on Vertex AI - Function calling", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling", "abstract": "# Generative AI on Vertex AI - Function calling\nFunction calling is a feature of `Gemini 1.0 Pro` models. It allows developers to define custom functions and provide these to a generative AI model. While processing a query, the model can choose to delegate certain data processing tasks to these functions. It does not call the functions. Instead, it provides structured data output that includes the name of a selected function and the arguments that the model proposes the function to be called with. Developers can use this output to invoke external APIs. They can then provide the API output back to the model, allowing it to complete its answer to the query. When it is used this way, function calling enables LLMs to access real-time information and interact with various services, such as SQL databases, customer relationship management systems, and document repositories.\nTo learn about the use cases of function calling, see [Use cases of function calling](#use-cases) . To learn how function calling works, see [How function calling works](#how-works) .\nFor a technical introduction to function calling, refer to the following resources:\n- High-level codelab tutorial: [How to Interact with APIs Using Function Calling in Gemini](https://codelabs.developers.google.com/codelabs/gemini-function-calling) .\n- Detailed, end-to-end Jupyter notebook: [Function Calling with the Vertex AI Gemini API & Python SDK](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb) .\n- Samples for Vertex AI SDK for Python, Node.js, Java, Go, and REST: [Function calling samples](#function-calling-samples) .", "content": "## Use cases of function calling\nFunction calling can be applied to the following tasks:\n- Extract information from text and use that information to create calendar events.\n- Replace if-then statements in a codebase.\n- Translate voice commands into vehicle system tasks.\n- Interact with SQL databases using natural language.\n- Build context-aware document Q&A systems by integrating with vector databases such as Vertex AI Search and Vertex AI Vector Search.\n- Enhance chatbots with the ability to access and process information from external sources in real-time.\n- Build automated workflows based on environmental triggers to automate processes with little to no user input.\n- Automate the assignment of support tickets based on their content, logs, and context-aware rules.## How function calling works\nTo enable a user to interface with the model and use function calling, you must create code that performs the following tasks:\n- Define a set of available functions and describe them using [function declarations](#function-declarations) .\n- Submit a user's query and the function declarations to the model.\n- Receive the structured data output from the model.\n- Use the structured data output to invoke an external API.\n- Provide the output of the external API back to the model.\nYou can create an application that manages all of these tasks. This application can be a text chatbot, a voice agent, an automated workflow, or any other program.\n### Function declarations\nEach function declaration must include the following:\n- Function name\n- Function parameters in a schema that's compatible with the OpenAPI schema format.- When using curl, the schema is specified using JSON.\n- When using the Vertex AI SDK, the schema is specified using a Python dictionary.\n- Function description (optional). For the best results, we recommend that you include a description.\nThe following is an example of a function declaration:\n```\nget_current_weather_func = FunctionDeclaration(\u00a0 \u00a0 name=\"get_current_weather\",\u00a0 \u00a0 description=\"Get the current weather in a given location\",\u00a0 \u00a0 parameters={\u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"object\",\u00a0 \u00a0 \u00a0 \u00a0 \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"Location\"}},\u00a0 \u00a0 },)\n```\n### Function calling modalities\nDepending on your use-case, you can use function calling with the [text modality](/vertex-ai/generative-ai/docs/text/text-prompts) or the [chat modality](/vertex-ai/generative-ai/docs/chat/chat-prompts) of `Gemini 1.0 Pro` .\n- The text modality is stateless. As a developer, you must provide the model with the full context of the interaction when you call it. The text modality is most suitable when an ad hoc response is sufficient. For example, you could use the text modality to design prompts for specific business tasks, including code generation.\n- The chat modality is stateful. As a developer, you don't need to provide the model with the full context of the interaction. The chat modality is most suitable in a freeform, conversational scenario, in which the user is likely to ask follow-up questions.\nFor both text modalities and chat modalities, the history of the interaction is stored on the client side.\n### Sequence of interactions\nThe following diagram illustrates a sequence of interactions between the user, the application, the model, and the external API. It represents a complete [text modality](#modalities) set of interactions or a single conversation turn of a [chat modality](#modalities) .\nThis sequence is composed of the following steps:\n- The user provides a prompt to the application. The following is an example of a function calling prompt: \"What is the weather like in Boston?\".\n- The application passes the prompt and the function declarations to the model.\n- The model selects the most suitable function and proposes parameter values that the function should be called with. The function name and the parameter values are passed back to the application. The model doesn't actually call the function.The following is an example of a model response. The model proposes calling the `get_current_weather` function with the parameter `Boston, MA` .```\ncandidates {\n content {\n  role: \"model\"\n  parts {\n  function_call {\n   name: \"get_current_weather\"\n   args {\n   fields {\n    key: \"location\"\n    value {\n    string_value: \"Boston, MA\"\n    }\n   }\n   }\n  }\n  }\n }\n ...\n}\n```\n- The application connects with the API and calls the function.\n- The API provides a response to the application. The following is an example of an API response:```\napi_response = \"\"\"{ \"location\": \"Boston, MA\", \"temperature\": 38, \"description\": \"Partly Cloudy\",\u00a0 \"icon\": \"partly-cloudy\", \"humidity\": 65, \"wind\": { \"speed\": 10, \"direction\": \"NW\" } }\"\"\"\n```\n- The application passes the API response to the model.\n- The model performs one of two actions:- It decides that the API response is sufficient for answering the user's query. It creates a natural language response and returns it to the application. The following is an example of a query response:`It is currently 38 degrees Fahrenheit in Boston, MA with partly cloudy skies. The humidity is 65% and the wind is blowing at 10 mph from the northwest.`\n- It decides that the output of another function is necessary for answering the query. As before, it passes the function name and the corresponding parameter values to the application.\n- If the model has answered the query, the application passes the response back to the user.\nAt this point the user can choose to terminate the interaction or ask another question.\n## Function calling samples\nThis document provides the following samples:\n- [Text modality examples](#text-samples) \n- [Chat modality examples](#chat-samples) \nTo learn more about text and chat modalities, see [Function calling modalities](#modalities) .\n### Text modality examples\nThis example demonstrates a text modality scenario with one function and one prompt. You must provide the model with the full context of the interaction when you call it. To learn more about text modalities for function calling, see [Function calling modalities](#modalities) .\nThis example uses the `GenerativeModel` class and its methods. For more information about using the Vertex AI SDK with Gemini multimodal models, see [Introduction to multimodal classes in the Vertex AI SDK](/vertex-ai/generative-ai/docs/multimodal/sdk-for-gemini/gemini-sdk-overview) .\nThe example steps are as follows:- Initialize the Gemini model.\n- Declare the function (`get_current_weather`). The function declaration must use a Python dictionary and must be in a format that's that's compatible with the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schema) .\n- Include the function declaration in a tool (`weather_tool`).\n- Provide the model with the prompt (\"What is the weather like in Boston?\"). The following is an example of a model response. The model proposes calling the`get_current_weather`function with the parameter`Boston, MA`.```\ncandidates {\n content {\n role: \"model\"\n parts {\n  function_call {\n  name: \"get_current_weather\"\n  args {\n   fields {\n   key: \"location\"\n   value {\n    string_value: \"Boston, MA\"\n   }\n   }\n  }\n  }\n }\n }\n ...\n}\n```\n- Provide the model with the response of the suggested function with the suggested parameter.- This example uses synthetic data instead of calling the external API.\n- The following is an example of the model's natural language response to the query:`It is currently 38 degrees Fahrenheit in Boston, MA with partly cloudy skies. The humidity is 65% and the wind is blowing at 10 mph from the northwest.`\n### PythonTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/function_calling.py) \n```\nimport vertexaifrom vertexai.generative_models import (\u00a0 \u00a0 Content,\u00a0 \u00a0 FunctionDeclaration,\u00a0 \u00a0 GenerativeModel,\u00a0 \u00a0 Part,\u00a0 \u00a0 Tool,)def generate_function_call(prompt: str, project_id: str, location: str) -> tuple:\u00a0 \u00a0 # Initialize Vertex AI\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 # Initialize Gemini model\u00a0 \u00a0 model = GenerativeModel(\"gemini-1.0-pro\")\u00a0 \u00a0 # Specify a function declaration and parameters for an API request\u00a0 \u00a0 get_current_weather_func = FunctionDeclaration(\u00a0 \u00a0 \u00a0 \u00a0 name=\"get_current_weather\",\u00a0 \u00a0 \u00a0 \u00a0 description=\"Get the current weather in a given location\",\u00a0 \u00a0 \u00a0 \u00a0 # Function parameters are specified in OpenAPI JSON schema format\u00a0 \u00a0 \u00a0 \u00a0 parameters={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"object\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"Location\"}},\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 )\u00a0 \u00a0 # Define a tool that includes the above get_current_weather_func\u00a0 \u00a0 weather_tool = Tool(\u00a0 \u00a0 \u00a0 \u00a0 function_declarations=[get_current_weather_func],\u00a0 \u00a0 )\u00a0 \u00a0 # Define the user's prompt in a Content object that we can reuse in model calls\u00a0 \u00a0 user_prompt_content = Content(\u00a0 \u00a0 \u00a0 \u00a0 role=\"user\",\u00a0 \u00a0 \u00a0 \u00a0 parts=[\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Part.from_text(prompt),\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 )\u00a0 \u00a0 # Send the prompt and instruct the model to generate content using the Tool that you just created\u00a0 \u00a0 response = model.generate_content(\u00a0 \u00a0 \u00a0 \u00a0 user_prompt_content,\u00a0 \u00a0 \u00a0 \u00a0 generation_config={\"temperature\": 0},\u00a0 \u00a0 \u00a0 \u00a0 tools=[weather_tool],\u00a0 \u00a0 )\u00a0 \u00a0 response_function_call_content = response.candidates[0].content\u00a0 \u00a0 # Check the function name that the model responded with, and make an API call to an external system\u00a0 \u00a0 if (\u00a0 \u00a0 \u00a0 \u00a0 response.candidates[0].content.parts[0].function_call.name\u00a0 \u00a0 \u00a0 \u00a0 == \"get_current_weather\"\u00a0 \u00a0 ):\u00a0 \u00a0 \u00a0 \u00a0 # Extract the arguments to use in your API call\u00a0 \u00a0 \u00a0 \u00a0 location = (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 response.candidates[0].content.parts[0].function_call.args[\"location\"]\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 # Here you can use your preferred method to make an API request to fetch the current weather, for example:\u00a0 \u00a0 \u00a0 \u00a0 # api_response = requests.post(weather_api_url, data={\"location\": location})\u00a0 \u00a0 \u00a0 \u00a0 # In this example, we'll use synthetic data to simulate a response payload from an external API\u00a0 \u00a0 \u00a0 \u00a0 api_response = \"\"\"{ \"location\": \"Boston, MA\", \"temperature\": 38, \"description\": \"Partly Cloudy\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"icon\": \"partly-cloudy\", \"humidity\": 65, \"wind\": { \"speed\": 10, \"direction\": \"NW\" } }\"\"\"\u00a0 \u00a0 # Return the API response to Gemini so it can generate a model response or request another function call\u00a0 \u00a0 response = model.generate_content(\u00a0 \u00a0 \u00a0 \u00a0 [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 user_prompt_content, \u00a0# User prompt\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 response_function_call_content, \u00a0# Function call response\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Content(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parts=[\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Part.from_function_response(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name=\"get_current_weather\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 response={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": api_response, \u00a0# Return the API response to Gemini\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 tools=[weather_tool],\u00a0 \u00a0 )\u00a0 \u00a0 # Get the model summary response\u00a0 \u00a0 summary = response.candidates[0].content.parts[0].text\u00a0 \u00a0 return summary, response\n```\nThis example demonstrates a text modality scenario with one function and one prompt. You must provide the model with the full context of the interaction when you call it. To learn more about text modalities for function calling, see [Function calling modalities](#modalities) .\nIn this example, you call the generative AI model twice.- In the first call, you provide the model with the prompt and the function declarations.\n- In the second call, you provide the model with the API response.\n### Node.jsBefore trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/generative-ai/snippets/functionCallingStreamContent.js) \n```\nconst {\u00a0 VertexAI,\u00a0 FunctionDeclarationSchemaType,} = require('@google-cloud/vertexai');const functionDeclarations = [\u00a0 {\u00a0 \u00a0 function_declarations: [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 name: 'get_current_weather',\u00a0 \u00a0 \u00a0 \u00a0 description: 'get weather in a given location',\u00a0 \u00a0 \u00a0 \u00a0 parameters: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: FunctionDeclarationSchemaType.OBJECT,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 properties: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 location: {type: FunctionDeclarationSchemaType.STRING},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 unit: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: FunctionDeclarationSchemaType.STRING,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enum: ['celsius', 'fahrenheit'],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 required: ['location'],\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ],\u00a0 },];const functionResponseParts = [\u00a0 {\u00a0 \u00a0 functionResponse: {\u00a0 \u00a0 \u00a0 name: 'get_current_weather',\u00a0 \u00a0 \u00a0 response: {name: 'get_current_weather', content: {weather: 'super nice'}},\u00a0 \u00a0 },\u00a0 },];/**\u00a0* TODO(developer): Update these variables before running the sample.\u00a0*/async function functionCallingStreamChat(\u00a0 projectId = 'PROJECT_ID',\u00a0 location = 'us-central1',\u00a0 model = 'gemini-1.0-pro') {\u00a0 // Initialize Vertex with your Cloud project and location\u00a0 const vertexAI = new VertexAI({project: projectId, location: location});\u00a0 // Instantiate the model\u00a0 const generativeModel = vertexAI.preview.getGenerativeModel({\u00a0 \u00a0 model: model,\u00a0 });\u00a0 const request = {\u00a0 \u00a0 contents: [\u00a0 \u00a0 \u00a0 {role: 'user', parts: [{text: 'What is the weather in Boston?'}]},\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 role: 'model',\u00a0 \u00a0 \u00a0 \u00a0 parts: [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 functionCall: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: 'get_current_weather',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 args: {location: 'Boston'},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 {role: 'function', parts: functionResponseParts},\u00a0 \u00a0 ],\u00a0 \u00a0 tools: functionDeclarations,\u00a0 };\u00a0 const streamingResp = await generativeModel.generateContentStream(request);\u00a0 for await (const item of streamingResp.stream) {\u00a0 \u00a0 console.log(item.candidates[0].content.parts[0].text);\u00a0 }}\n```This example demonstrates a text modality scenario with three functions and one prompt. You must provide the model with the full context of the interaction when you call it. To learn more about text modalities for function calling, see [Function calling modalities](#modalities) .\nIn this example, you call the generative AI model twice.- In the [first call](#text-rest-first-request) , you provide the model with the prompt and the function declarations.\n- In the [second call](#text-rest-second-request) , you provide the model with the API response.\nThe request must define a query in the `text` parameter. This example defines the following query: \"Which theaters in Mountain View show the Barbie movie?\".\nThe request must also define a tool ( `tools` ) with a set of function declarations ( `functionDeclarations` ). These function declarations must be specified in a format that's compatible with the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schema) . This example defines the following functions:- `find_movies`finds movie titles playing in theaters.\n- `find_theatres`finds theaters based on location.\n- `get_showtimes`finds the start times for movies playing in a specific theater.\nTo learn more about the parameters of the model request, see [Gemini API](/vertex-ai/generative-ai/docs/model-reference/gemini) .\nReplace with the name of your Google Cloud project.For the prompt \"Which theaters in Mountain View show the Barbie movie?\", the model might return the function `find_theatres` with parameters `Barbie` and `Mountain View, CA` .This example uses synthetic data instead of calling the external API. There are two results, each with two parameters ( `name` and `address` ):- `name`:`AMC Mountain View 16`,`address`:`2000 W El Camino Real, Mountain View, CA 94040`\n- `name`:`Regal Edwards 14`,`address`:`245 Castro St, Mountain View, CA 94040`\nReplace with the name of your Google Cloud project.The model's response might be similar to the following:\n### Chat modality examples\nThis example demonstrates a chat scenario with two functions and two sequential prompts. You don't need to provide the full context during each conversation turn when using the chat modality. To learn more about chat modalities for function calling, see [Function calling modalities](#modalities) .\nThis example uses the `GenerativeModel` class and its methods. For more information about using the Vertex AI SDK with Gemini multimodal models, see [Introduction to multimodal classes in the Vertex AI SDK](/vertex-ai/generative-ai/docs/multimodal/sdk-for-gemini/gemini-sdk-overview) .\nThe example steps are as follows:- Declare the functions.- The function declarations must use a Python dictionary and must be in a format that's that's compatible with the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schema) .\n- This example declares the following functions:- `get_store_location_func`gets the SKU for a product.\n- `get_store_location`gets the location of the closest store.\n- Include the function declarations in a tool (`retail_tool`).\n- Initialize the Gemini model and provide the tool.\n- Begin the chat session with the model using the`start_chat`method.\n- Provide the model with the first prompt (\"Do you have the Pixel 8 Pro in stock?\").\n- Provide the model with the response of the suggested function with the suggested parameter. This example uses synthetic data instead of calling the external API.\n- Provide the model with the second prompt (\"Is there a store in Mountain View, CA that I can visit to try it out?\").\n- Provide the model with the response of the suggested function with the suggested parameter. This example uses synthetic data instead of calling the external API.### PythonTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/function_calling_chat.py) \n```\nimport vertexaifrom vertexai.generative_models import (\u00a0 \u00a0 FunctionDeclaration,\u00a0 \u00a0 GenerativeModel,\u00a0 \u00a0 Part,\u00a0 \u00a0 Tool,)def generate_function_call_chat(project_id: str, location: str) -> tuple:\u00a0 \u00a0 prompts = []\u00a0 \u00a0 summaries = []\u00a0 \u00a0 # Initialize Vertex AI\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 # Specify a function declaration and parameters for an API request\u00a0 \u00a0 get_product_info_func = FunctionDeclaration(\u00a0 \u00a0 \u00a0 \u00a0 name=\"get_product_sku\",\u00a0 \u00a0 \u00a0 \u00a0 description=\"Get the SKU for a product\",\u00a0 \u00a0 \u00a0 \u00a0 # Function parameters are specified in OpenAPI JSON schema format\u00a0 \u00a0 \u00a0 \u00a0 parameters={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"object\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"properties\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"product_name\": {\"type\": \"string\", \"description\": \"Product name\"}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 )\u00a0 \u00a0 # Specify another function declaration and parameters for an API request\u00a0 \u00a0 get_store_location_func = FunctionDeclaration(\u00a0 \u00a0 \u00a0 \u00a0 name=\"get_store_location\",\u00a0 \u00a0 \u00a0 \u00a0 description=\"Get the location of the closest store\",\u00a0 \u00a0 \u00a0 \u00a0 # Function parameters are specified in OpenAPI JSON schema format\u00a0 \u00a0 \u00a0 \u00a0 parameters={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"object\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"Location\"}},\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 )\u00a0 \u00a0 # Define a tool that includes the above functions\u00a0 \u00a0 retail_tool = Tool(\u00a0 \u00a0 \u00a0 \u00a0 function_declarations=[\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 get_product_info_func,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 get_store_location_func,\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 )\u00a0 \u00a0 # Initialize Gemini model\u00a0 \u00a0 model = GenerativeModel(\u00a0 \u00a0 \u00a0 \u00a0 \"gemini-1.0-pro\", generation_config={\"temperature\": 0}, tools=[retail_tool]\u00a0 \u00a0 )\u00a0 \u00a0 # Start a chat session\u00a0 \u00a0 chat = model.start_chat()\u00a0 \u00a0 # Send a prompt for the first conversation turn that should invoke the get_product_sku function\u00a0 \u00a0 prompt = \"Do you have the Pixel 8 Pro in stock?\"\u00a0 \u00a0 response = chat.send_message(prompt)\u00a0 \u00a0 prompts.append(prompt)\u00a0 \u00a0 # Check the function name that the model responded with, and make an API call to an external system\u00a0 \u00a0 if response.candidates[0].content.parts[0].function_call.name == \"get_product_sku\":\u00a0 \u00a0 \u00a0 \u00a0 # Extract the arguments to use in your API call\u00a0 \u00a0 \u00a0 \u00a0 product_name = (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 response.candidates[0].content.parts[0].function_call.args[\"product_name\"]\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 product_name\u00a0 \u00a0 \u00a0 \u00a0 # Here you can use your preferred method to make an API request to retrieve the product SKU, as in:\u00a0 \u00a0 \u00a0 \u00a0 # api_response = requests.post(product_api_url, data={\"product_name\": product_name})\u00a0 \u00a0 \u00a0 \u00a0 # In this example, we'll use synthetic data to simulate a response payload from an external API\u00a0 \u00a0 \u00a0 \u00a0 api_response = {\"sku\": \"GA04834-US\", \"in_stock\": \"yes\"}\u00a0 \u00a0 # Return the API response to Gemini so it can generate a model response or request another function call\u00a0 \u00a0 response = chat.send_message(\u00a0 \u00a0 \u00a0 \u00a0 Part.from_function_response(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name=\"get_product_sku\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 response={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": api_response,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 )\u00a0 \u00a0 # Extract the text from the summary response\u00a0 \u00a0 summary = response.candidates[0].content.parts[0].text\u00a0 \u00a0 summaries.append(summary)\u00a0 \u00a0 # Send a prompt for the second conversation turn that should invoke the get_store_location function\u00a0 \u00a0 prompt = \"Is there a store in Mountain View, CA that I can visit to try it out?\"\u00a0 \u00a0 response = chat.send_message(prompt)\u00a0 \u00a0 prompts.append(prompt)\u00a0 \u00a0 # Check the function name that the model responded with, and make an API call to an external system\u00a0 \u00a0 if (\u00a0 \u00a0 \u00a0 \u00a0 response.candidates[0].content.parts[0].function_call.name\u00a0 \u00a0 \u00a0 \u00a0 == \"get_store_location\"\u00a0 \u00a0 ):\u00a0 \u00a0 \u00a0 \u00a0 # Extract the arguments to use in your API call\u00a0 \u00a0 \u00a0 \u00a0 location = (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 response.candidates[0].content.parts[0].function_call.args[\"location\"]\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 location\u00a0 \u00a0 \u00a0 \u00a0 # Here you can use your preferred method to make an API request to retrieve store location closest to the user, as in:\u00a0 \u00a0 \u00a0 \u00a0 # api_response = requests.post(store_api_url, data={\"location\": location})\u00a0 \u00a0 \u00a0 \u00a0 # In this example, we'll use synthetic data to simulate a response payload from an external API\u00a0 \u00a0 \u00a0 \u00a0 api_response = {\"store\": \"2000 N Shoreline Blvd, Mountain View, CA 94043, US\"}\u00a0 \u00a0 # Return the API response to Gemini so it can generate a model response or request another function call\u00a0 \u00a0 response = chat.send_message(\u00a0 \u00a0 \u00a0 \u00a0 Part.from_function_response(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name=\"get_store_location\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 response={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": api_response,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 )\u00a0 \u00a0 # Extract the text from the summary response\u00a0 \u00a0 summary = response.candidates[0].content.parts[0].text\u00a0 \u00a0 summaries.append(summary)\u00a0 \u00a0 return prompts, summaries\n```\nThis example demonstrates a chat scenario. You don't need to provide the full context during each conversation turn when using the chat modality. Use the `startChat` method to begin the chat with the generative AI model. To learn more about chat modalities for function calling, see [Function calling modalities](#modalities) .\nIn this example, you call the generative AI model twice.- In the first call, you provide the model with a natural language text prompt and the function declarations. Pass in the function declarations when you start the chat session. Use the`sendMessageStream`method to provide the model with the prompt.\n- In the second call, you provide the model with the API response. Use the`sendMessageStream`method again.\n### Node.jsBefore trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/generative-ai/snippets/functionCallingStreamChat.js) \n```\nconst {\u00a0 VertexAI,\u00a0 FunctionDeclarationSchemaType,} = require('@google-cloud/vertexai');const functionDeclarations = [\u00a0 {\u00a0 \u00a0 function_declarations: [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 name: 'get_current_weather',\u00a0 \u00a0 \u00a0 \u00a0 description: 'get weather in a given location',\u00a0 \u00a0 \u00a0 \u00a0 parameters: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: FunctionDeclarationSchemaType.OBJECT,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 properties: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 location: {type: FunctionDeclarationSchemaType.STRING},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 unit: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: FunctionDeclarationSchemaType.STRING,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enum: ['celsius', 'fahrenheit'],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 required: ['location'],\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ],\u00a0 },];const functionResponseParts = [\u00a0 {\u00a0 \u00a0 functionResponse: {\u00a0 \u00a0 \u00a0 name: 'get_current_weather',\u00a0 \u00a0 \u00a0 response: {name: 'get_current_weather', content: {weather: 'super nice'}},\u00a0 \u00a0 },\u00a0 },];/**\u00a0* TODO(developer): Update these variables before running the sample.\u00a0*/async function functionCallingStreamChat(\u00a0 projectId = 'PROJECT_ID',\u00a0 location = 'us-central1',\u00a0 model = 'gemini-1.0-pro') {\u00a0 // Initialize Vertex with your Cloud project and location\u00a0 const vertexAI = new VertexAI({project: projectId, location: location});\u00a0 // Instantiate the model\u00a0 const generativeModel = vertexAI.preview.getGenerativeModel({\u00a0 \u00a0 model: model,\u00a0 });\u00a0 // Create a chat session and pass your function declarations\u00a0 const chat = generativeModel.startChat({\u00a0 \u00a0 tools: functionDeclarations,\u00a0 });\u00a0 const chatInput1 = 'What is the weather in Boston?';\u00a0 // This should include a functionCall response from the model\u00a0 const result1 = await chat.sendMessageStream(chatInput1);\u00a0 for await (const item of result1.stream) {\u00a0 \u00a0 console.log(item.candidates[0]);\u00a0 }\u00a0 await result1.response;\u00a0 // Send a follow up message with a FunctionResponse\u00a0 const result2 = await chat.sendMessageStream(functionResponseParts);\u00a0 for await (const item of result2.stream) {\u00a0 \u00a0 console.log(item.candidates[0]);\u00a0 }\u00a0 // This should include a text response from the model using the response content\u00a0 // provided above\u00a0 const response2 = await result2.response;\u00a0 console.log(response2.candidates[0].content.parts[0].text);}\n```This example demonstrates a chat scenario. You don't need to provide the full context during each conversation turn when using the chat modality. To learn more about chat modalities for function calling, see [Function calling modalities](#modalities) .\nIn this example, you call the generative AI model twice.- In the first call, you provide the model with a natural language text prompt and the function declarations.\n- In the second call, you provide the model with the API response.### JavaBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/vertexai/snippets/src/main/java/vertexai/gemini/FunctionCalling.java) \n```\nimport com.google.cloud.vertexai.VertexAI;import com.google.cloud.vertexai.api.Content;import com.google.cloud.vertexai.api.FunctionDeclaration;import com.google.cloud.vertexai.api.GenerateContentResponse;import com.google.cloud.vertexai.api.Schema;import com.google.cloud.vertexai.api.Tool;import com.google.cloud.vertexai.api.Type;import com.google.cloud.vertexai.generativeai.ChatSession;import com.google.cloud.vertexai.generativeai.ContentMaker;import com.google.cloud.vertexai.generativeai.GenerativeModel;import com.google.cloud.vertexai.generativeai.PartMaker;import com.google.cloud.vertexai.generativeai.ResponseHandler;import java.io.IOException;import java.util.Arrays;import java.util.Collections;public class FunctionCalling {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String projectId = \"your-google-cloud-project-id\";\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 String modelName = \"gemini-1.0-pro\";\u00a0 \u00a0 String promptText = \"What's the weather like in Paris?\";\u00a0 \u00a0 whatsTheWeatherLike(projectId, location, modelName, promptText);\u00a0 }\u00a0 public static String whatsTheWeatherLike(String projectId, String location,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0String modelName, String promptText)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 try (VertexAI vertexAI = new VertexAI(projectId, location)) {\u00a0 \u00a0 \u00a0 FunctionDeclaration functionDeclaration = FunctionDeclaration.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setName(\"getCurrentWeather\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDescription(\"Get the current weather in a given location\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setParameters(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Schema.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setType(Type.OBJECT)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .putProperties(\"location\", Schema.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setType(Type.STRING)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDescription(\"location\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addRequired(\"location\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 System.out.println(\"Function declaration:\");\u00a0 \u00a0 \u00a0 System.out.println(functionDeclaration);\u00a0 \u00a0 \u00a0 // Add the function to a \"tool\"\u00a0 \u00a0 \u00a0 Tool tool = Tool.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addFunctionDeclarations(functionDeclaration)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // Start a chat session from a model, with the use of the declared function.\u00a0 \u00a0 \u00a0 GenerativeModel model =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 GenerativeModel.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModelName(modelName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setVertexAi(vertexAI)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTools(Arrays.asList(tool))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 ChatSession chat = model.startChat();\u00a0 \u00a0 \u00a0 System.out.println(String.format(\"Ask the question: %s\", promptText));\u00a0 \u00a0 \u00a0 GenerateContentResponse response = chat.sendMessage(promptText);\u00a0 \u00a0 \u00a0 // The model will most likely return a function call to the declared\u00a0 \u00a0 \u00a0 // function `getCurrentWeather` with \"Paris\" as the value for the\u00a0 \u00a0 \u00a0 // argument `location`.\u00a0 \u00a0 \u00a0 System.out.println(\"\\nPrint response: \");\u00a0 \u00a0 \u00a0 System.out.println(ResponseHandler.getContent(response));\u00a0 \u00a0 \u00a0 // Provide an answer to the model so that it knows what the result\u00a0 \u00a0 \u00a0 // of a \"function call\" is.\u00a0 \u00a0 \u00a0 Content content =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ContentMaker.fromMultiModalData(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PartMaker.fromFunctionResponse(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"getCurrentWeather\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Collections.singletonMap(\"currentWeather\", \"sunny\")));\u00a0 \u00a0 \u00a0 System.out.println(\"Provide the function response: \");\u00a0 \u00a0 \u00a0 System.out.println(content);\u00a0 \u00a0 \u00a0 response = chat.sendMessage(content);\u00a0 \u00a0 \u00a0 // See what the model replies now\u00a0 \u00a0 \u00a0 System.out.println(\"Print response: \");\u00a0 \u00a0 \u00a0 String finalAnswer = ResponseHandler.getText(response);\u00a0 \u00a0 \u00a0 System.out.println(finalAnswer);\u00a0 \u00a0 \u00a0 return finalAnswer;\u00a0 \u00a0 }\u00a0 }}\n```\nThis example demonstrates a chat scenario. You don't need to provide the full context during each conversation turn when using the chat modality. To learn more about chat modalities for function calling, see [Function calling modalities](#modalities) .\nIn this example, you call the generative AI model twice.- In the first call, you provide the model with a natural language text prompt and the function declarations.\n- In the second call, you provide the model with the API response.\n### GoBefore trying this sample, follow the Go setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Go API reference documentation](/go/docs/reference/cloud.google.com/go/aiplatform/latest/apiv1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/vertexai/function-calling/functioncalling.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"encoding/json\"\u00a0 \u00a0 \u00a0 \u00a0 \"errors\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/vertexai/genai\")// functionCalls opens a chat session and sends 2 messages to the model:// - first, to convert a text into a structured function call request// - second, to convert a structured function call response into natural languagefunc functionCalls(w io.Writer, prompt, projectID, location, modelName string) error {\u00a0 \u00a0 \u00a0 \u00a0 // prompt := \"What's the weather like in Paris?\"\u00a0 \u00a0 \u00a0 \u00a0 // location := \"us-central1\"\u00a0 \u00a0 \u00a0 \u00a0 // modelName := \"gemini-1.0-pro\"\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 client, err := genai.NewClient(ctx, projectID, location)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"unable to create client: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 model := client.GenerativeModel(modelName)\u00a0 \u00a0 \u00a0 \u00a0 // Build an OpenAPI schema, in memory\u00a0 \u00a0 \u00a0 \u00a0 params := &genai.Schema{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Type: genai.TypeObject,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Properties: map[string]*genai.Schema{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"location\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Type: \u00a0 \u00a0 \u00a0 \u00a0genai.TypeString,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Description: \"location\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fundecl := &genai.FunctionDeclaration{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Name: \u00a0 \u00a0 \u00a0 \u00a0\"getCurrentWeather\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Description: \"Get the current weather in a given location\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Parameters: \u00a0params,\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 model.Tools = []*genai.Tool{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {FunctionDeclarations: []*genai.FunctionDeclaration{fundecl}},\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 chat := model.StartChat()\u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Question: %s\\n\", prompt)\u00a0 \u00a0 \u00a0 \u00a0 resp, err := chat.SendMessage(ctx, genai.Text(prompt))\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 if len(resp.Candidates) == 0 ||\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 len(resp.Candidates[0].Content.Parts) == 0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return errors.New(\"empty response from model\")\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // The model has returned a function call to the declared function `getCurrentWeather`\u00a0 \u00a0 \u00a0 \u00a0 // with a value for the argument `location`.\u00a0 \u00a0 \u00a0 \u00a0 jsondata, err := json.MarshalIndent(resp.Candidates[0].Content.Parts[0], \"\", \" \u00a0\")\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"json.Marshal: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"function call generated by the model:\\n%s\\n\\n\", string(jsondata))\u00a0 \u00a0 \u00a0 \u00a0 // Create a function call response, to simulate the result of a call to a\u00a0 \u00a0 \u00a0 \u00a0 // real service\u00a0 \u00a0 \u00a0 \u00a0 funresp := &genai.FunctionResponse{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Name: \"getCurrentWeather\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Response: map[string]any{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"currentWeather\": \"sunny\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 jsondata, err = json.MarshalIndent(funresp, \"\", \" \u00a0\")\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"json.Marshal: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"function call response sent to the model:\\n%s\\n\\n\", string(jsondata))\u00a0 \u00a0 \u00a0 \u00a0 // And provide the function call response to the model\u00a0 \u00a0 \u00a0 \u00a0 resp, err = chat.SendMessage(ctx, funresp)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 if len(resp.Candidates) == 0 ||\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 len(resp.Candidates[0].Content.Parts) == 0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return errors.New(\"empty response from model\")\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // The model has taken the function call response as input, and has\u00a0 \u00a0 \u00a0 \u00a0 // reformulated the response to the user.\u00a0 \u00a0 \u00a0 \u00a0 jsondata, err = json.MarshalIndent(resp.Candidates[0].Content.Parts[0], \"\", \" \u00a0\")\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"json.Marshal: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Answer generated by the model:\\n%s\\n\", string(jsondata))\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```", "guide": "Generative AI on Vertex AI"}