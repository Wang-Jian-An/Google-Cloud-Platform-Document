{"title": "Generative AI on Vertex AI - Visual question and answering (VQA)", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/visual-question-answering", "abstract": "# Generative AI on Vertex AI - Visual question and answering (VQA)\nImagen for Captioning & VQA ( `imagetext` ) is the name of the model that supports image question and answering. Imagen for Captioning & VQA answers a question provided for a given image, even if it hasn't been seen before by the model.\nTo explore this model in the console, see the Imagen for Captioning & VQA model card in the Model Garden.\n[Go to the Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/imagetext)\n", "content": "## Use cases\nSome common use cases for image question and answering include:\n- Empower users to engage with visual content with Q&A.\n- Enable customers to engage with product images shown on retail apps and websites.\n- Provide accessibility options for visually impaired users.## HTTP request\n```\nPOST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/publishers/google/models/imagetext:predict\n```\n## Request body\n```\n{\u00a0 \"instances\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"prompt\": string,\u00a0 \u00a0 \u00a0 \"image\": {\u00a0 \u00a0 \u00a0 \u00a0 // Union field can be only one of the following:\u00a0 \u00a0 \u00a0 \u00a0 \"bytesBase64Encoded\": string,\u00a0 \u00a0 \u00a0 \u00a0 \"gcsUri\": string,\u00a0 \u00a0 \u00a0 \u00a0 // End of list of possible types for union field.\u00a0 \u00a0 \u00a0 \u00a0 \"mimeType\": string\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ],\u00a0 \"parameters\": {\u00a0 \u00a0 \"sampleCount\": integer,\u00a0 \u00a0 \"seed\": integer\u00a0 }}\n```\nUse the following parameters for the visual Q&A generation model `imagetext` . For more information, see [Use Visual Question Answering (VQA)](/vertex-ai/generative-ai/docs/image/visual-question-answering) .\n| Parameter   | Description                                   | Acceptable values              |\n|:-------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|\n| instances   | An array that contains the object with prompt and image details to get information about.               | array (1 image object allowed)           |\n| prompt    | The question you want to get answered about your image.                        | string (80 tokens max)             |\n| bytesBase64Encoded | The image to get information about.                             | Base64-encoded image string (PNG or JPEG, 20\u00a0MB max)     |\n| gcsUri    | The Cloud Storage URI of the image to get information about.                       | string URI of the image file in Cloud Storage (PNG or JPEG, 20\u00a0MB max) |\n| mimeType   | Optional. The MIME type of the image you specify.                         | string (image/jpeg or image/png)          |\n| sampleCount  | Number of generated text strings.                             | Int value: 1-3               |\n| seed    | Optional. The seed for random number generator (RNG). If RNG seed is the same for requests with the inputs, the prediction results will be the same. | integer                |\n## Sample request\nBefore using any of the request data, make the following replacements:\n- : Your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : Your project's region. For example,`us-central1`,`europe-west2`, or`asia-northeast3`. For a list of available regions, see [Generative AI on Vertex AI locations](/vertex-ai/generative-ai/docs/learn/locations-genai) .\n- : The question you want to get answered about your image.\n- : The image to get captions for. The image must be  specified as a [base64-encoded](/vertex-ai/generative-ai/docs/image/base64-encode) byte string. Size  limit: 10 MB.\n- : The number of answers you want to generate. Accepted integer values: 1-3.\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/imagetext:predict\n```\nRequest JSON body:\n```\n{\n \"instances\": [ {\n  \"prompt\": \"VQA_PROMPT\",\n  \"image\": {\n   \"bytesBase64Encoded\": \"B64_IMAGE\"\n  }\n }\n ],\n \"parameters\": {\n \"sampleCount\": RESPONSE_COUNT\n }\n}\n```\nTo send your request, choose one of these options:\n**Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/imagetext:predict\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/imagetext:predict\" | Select-Object -Expand Content\n```\n`\"sampleCount\": 2`\n`\"prompt\": \"What is this?\"`\n```\n{\n \"predictions\": [ \"cappuccino\",\n \"coffee\"\n ],\n \"deployedModelId\": \"DEPLOYED_MODEL_ID\",\n \"model\": \"projects/PROJECT_ID/locations/LOCATION/models/MODEL_ID\",\n \"modelDisplayName\": \"MODEL_DISPLAYNAME\",\n \"modelVersionId\": \"1\"\n}\n```\n## Response body\n```\n{\u00a0 \"predictions\": [\u00a0 \u00a0 string\u00a0 ]}\n```\n| Response element | Description               |\n|:-------------------|:--------------------------------------------------------------------|\n| predictions  | List of text strings representing VQA answer, sorted by confidence. |\n## Sample response\nThe following sample responses is for a request with `\"sampleCount\": 2` and `\"prompt\": \"What is this?\"` . The response returns two prediction string answers.\n```\n{\u00a0 \"predictions\": [\u00a0 \u00a0 \"cappuccino\",\u00a0 \u00a0 \"coffee\"\u00a0 ],\u00a0 \"deployedModelId\": \"DEPLOYED_MODEL_ID\",\u00a0 \"model\": \"projects/PROJECT_ID/locations/us-central1/models/MODEL_ID\",\u00a0 \"modelDisplayName\": \"MODEL_DISPLAYNAME\",\u00a0 \"modelVersionId\": \"1\"}\n```", "guide": "Generative AI on Vertex AI"}