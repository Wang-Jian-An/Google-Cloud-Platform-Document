{"title": "Generative AI on Vertex AI - Model distillation", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/tuning/model-distillation", "abstract": "# Generative AI on Vertex AI - Model distillation\n**Preview** Distillation is a Preview offering, subject to the Pre-GA Offerings Terms of the [GCP Service Specific Terms](https://cloud.google.com/terms/service-terms) . Pre-GA products and features may have limited support, and changes to pre-GA products and features may not be compatible with other pre-GA versions. For more information, see the [launch stage descriptions](https://cloud.google.com/products#product-launch-stages) . Further, by using these features, you agree to the Generative AI Preview [ terms and conditions ](https://cloud.google.com/trustedtester/aitos) (Preview Terms).For these features, you can process personal data as outlined in the Cloud Data Processing Addendum, subject to applicable restrictions and obligations in the Agreement (as defined in the Preview Terms).\nWhen you distill a foundation model, you use a large model (the model), a smaller model (the model), and supervised tuning to create a tuned model. The resulting distilled model is about the same size as the student model, includes the capabilities you care about in the teacher model, and costs less to use and has lower latency than the teacher model.\n", "content": "## Models that support distilling\nThe following text models support distilling:\n- The text generation foundation model,`text-bison@002`. For more information, see [Text generation model](/vertex-ai/generative-ai/docs/model-reference/text) .## Workflow for tuning and distilling a model\nThe distilling workflow on Vertex AI includes the following steps:\n- Prepare your model tuning dataset.\n- Specify the teacher model.\n- Specify the student model.\n- Upload the model tuning dataset to a Cloud Storage bucket.\n- Create a model distilling job.\nAfter model distillation completes, the distilled model is deployed to a Vertex AI endpoint. The name of the endpoint is the same as the name of the distilled model. Distilled models are available to select in Vertex AI Studio when you want to create a new prompt. To learn about distilling a text model, see [Create distilled text models](/vertex-ai/generative-ai/docs/models/distill-text-models) .\n## Distilling region settings\nDistilling supports the following two regions:\n- `us-central1`- If you choose this region, then 8 Nvidia A100 80GB GPUs are used.\n- `europe-west4`- If you choose this region, then 64 cores of the TPU v3 pod are used.\nThe region you choose is where Vertex AI distills the model and then uploads the distilled model.\n## What's next\n- Learn about [supervised tuning](/vertex-ai/generative-ai/docs/tuning/supervised-tuning) .\n- Learn about [RLHF tuning](/vertex-ai/generative-ai/docs/tuning/rlhf-tuning) .", "guide": "Generative AI on Vertex AI"}