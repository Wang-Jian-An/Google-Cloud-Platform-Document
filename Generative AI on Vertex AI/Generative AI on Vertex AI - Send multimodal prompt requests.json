{"title": "Generative AI on Vertex AI - Send multimodal prompt requests", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-multimodal-prompts", "abstract": "# Generative AI on Vertex AI - Send multimodal prompt requests\nThis page shows you how to send multimodal prompts to the Gemini 1.0 Pro Vision ( `gemini-1.0-pro-vision` ) model by using the Google Cloud console, REST API, and supported SDKs. The Gemini 1.0 Pro Vision model supports prompts that include text, code, images, and video, and can output text and code.\nFor a list of languages supported by Gemini 1.0 Pro Vision, see model information [Language support](/vertex-ai/generative-ai/docs/learn/models#language-support) .\nTo explore this model in the console, select the `gemini-1.0-pro-vision` model card in the Model Garden.\n[Go to the Model Garden](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemini-1.0-pro-vision)\nTo learn more about how to design multimodal prompts, see [Design multimodal prompts](/vertex-ai/generative-ai/docs/multimodal/design-multimodal-prompts) .\nIf you're looking for a way to use Gemini directly from your mobile and web apps, check out the [Google AI SDKs](https://ai.google.dev/docs) for Android, Swift, and web.\n", "content": "## Send multimodal prompt requests with images\nFor testing and iterating on multimodal prompts with images, we recommend using the Google Cloud console. To send a multimodal prompt with images programmatically to the model, you can use the REST API, Vertex AI SDK for Python, or one of the other supported libraries and SDKs shown in the following tabs:\nTo learn how to install or update the Vertex AI SDK for Python, see [ Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk#install-vertex-ai-python-sdk) . For more information, see the [Vertex AI SDK for Python API reference documentation.](/python/docs/reference/aiplatform/latest) \n### Streaming and non-streaming responsesYou can choose whether the model generates a streaming response or a non-streaming response. Streaming involves receiving responses to prompts as they are generated. That is, as soon as the model generates output tokens, the output tokens are sent. A non-streaming response to prompts is sent only after all of the output tokens are generated.For a streaming response, use the `stream` parameter in [generate_content](/python/docs/reference/aiplatform/latest/vertexai.preview.generative_models#content) .```\n response = model.generate_content(contents=[...], stream = True)\n \n```For a non-streaming response, remove the parameter, or set the parameter to `False` .\n### Sample code [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/gemini_multi_image_example.py) \n```\nimport http.clientimport typingimport urllib.requestfrom vertexai.generative_models import GenerativeModel, Image# create helper functiondef load_image_from_url(image_url: str) -> Image:\u00a0 \u00a0 with urllib.request.urlopen(image_url) as response:\u00a0 \u00a0 \u00a0 \u00a0 response = typing.cast(http.client.HTTPResponse, response)\u00a0 \u00a0 \u00a0 \u00a0 image_bytes = response.read()\u00a0 \u00a0 return Image.from_bytes(image_bytes)# Load images from Cloud Storage URIlandmark1 = load_image_from_url(\u00a0 \u00a0 \"https://storage.googleapis.com/cloud-samples-data/vertex-ai/llm/prompts/landmark1.png\")landmark2 = load_image_from_url(\u00a0 \u00a0 \"https://storage.googleapis.com/cloud-samples-data/vertex-ai/llm/prompts/landmark2.png\")landmark3 = load_image_from_url(\u00a0 \u00a0 \"https://storage.googleapis.com/cloud-samples-data/vertex-ai/llm/prompts/landmark3.png\")# Pass multimodal promptmodel = GenerativeModel(\"gemini-1.0-pro-vision\")response = model.generate_content(\u00a0 \u00a0 [\u00a0 \u00a0 \u00a0 \u00a0 landmark1,\u00a0 \u00a0 \u00a0 \u00a0 \"city: Rome, Landmark: the Colosseum\",\u00a0 \u00a0 \u00a0 \u00a0 landmark2,\u00a0 \u00a0 \u00a0 \u00a0 \"city: Beijing, Landmark: Forbidden City\",\u00a0 \u00a0 \u00a0 \u00a0 landmark3,\u00a0 \u00a0 ])print(response)\n```Before trying this sample, follow the Node.js setup instructions in the [Generative AI quickstart using the Node.js SDK](/nodejs/docs/reference/vertexai/latest#before-you-begin) . For more information, see the [Node.js SDK for Gemini reference documentation](/nodejs/docs/reference/vertexai/latest) .To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n### Streaming and non-streaming responsesYou can choose whether the model generates a streaming response or a non-streaming response. Streaming involves receiving responses to prompts as they are generated. That is, as soon as the model generates output tokens, the output tokens are sent. A non-streaming response to prompts is sent only after all of the output tokens are generated.For a streaming response, use the [generateContentStream](/nodejs/docs/reference/vertexai/latest#streaming-content-generation) method.```\n const streamingResp = await generativeModel.generateContentStream(request);\n \n```For a non-streaming response, use the [generateContent](/nodejs/docs/reference/vertexai/latest#content-generation-non-streaming) method.```\n const streamingResp = await generativeModel.generateContent(request);\n \n```\n### Sample code [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/generative-ai/snippets/sendMultiModalPromptWithImage.js) \n```\nconst {VertexAI} = require('@google-cloud/vertexai');const axios = require('axios');async function getBase64(url) {\u00a0 const image = await axios.get(url, {responseType: 'arraybuffer'});\u00a0 return Buffer.from(image.data).toString('base64');}/**\u00a0* TODO(developer): Update these variables before running the sample.\u00a0*/async function sendMultiModalPromptWithImage(\u00a0 projectId = 'PROJECT_ID',\u00a0 location = 'us-central1',\u00a0 model = 'gemini-1.0-pro-vision') {\u00a0 // For images, the SDK supports base64 strings\u00a0 const landmarkImage1 = await getBase64(\u00a0 \u00a0 'https://storage.googleapis.com/cloud-samples-data/vertex-ai/llm/prompts/landmark1.png'\u00a0 );\u00a0 const landmarkImage2 = await getBase64(\u00a0 \u00a0 'https://storage.googleapis.com/cloud-samples-data/vertex-ai/llm/prompts/landmark2.png'\u00a0 );\u00a0 const landmarkImage3 = await getBase64(\u00a0 \u00a0 'https://storage.googleapis.com/cloud-samples-data/vertex-ai/llm/prompts/landmark3.png'\u00a0 );\u00a0 // Initialize Vertex with your Cloud project and location\u00a0 const vertexAI = new VertexAI({project: projectId, location: location});\u00a0 const generativeVisionModel = vertexAI.getGenerativeModel({\u00a0 \u00a0 model: model,\u00a0 });\u00a0 // Pass multimodal prompt\u00a0 const request = {\u00a0 \u00a0 contents: [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 role: 'user',\u00a0 \u00a0 \u00a0 \u00a0 parts: [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 inlineData: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 data: landmarkImage1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mimeType: 'image/png',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 text: 'city: Rome, Landmark: the Colosseum',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 inlineData: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 data: landmarkImage2,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mimeType: 'image/png',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 text: 'city: Beijing, Landmark: Forbidden City',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 inlineData: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 data: landmarkImage3,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mimeType: 'image/png',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ],\u00a0 };\u00a0 // Create the response\u00a0 const response = await generativeVisionModel.generateContent(request);\u00a0 // Wait for the response to complete\u00a0 const aggregatedResponse = await response.response;\u00a0 // Select the text from the response\u00a0 const fullTextResponse =\u00a0 \u00a0 aggregatedResponse.candidates[0].content.parts[0].text;\u00a0 console.log(fullTextResponse);}\n```Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart](/vertex-ai/docs/start/client-libraries) . For more information, see the [Vertex AI Java SDK for Gemini reference documentation](/java/docs/reference/google-cloud-vertexai/latest/overview) .To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n### Streaming and non-streaming responsesYou can choose whether the model generates a streaming response or a non-streaming response. Streaming involves receiving responses to prompts as they are generated. That is, as soon as the model generates output tokens, the output tokens are sent. A non-streaming response to prompts is sent only after all of the output tokens are generated.For a streaming response, use the [generateContentStream](/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.preview.GenerativeModel#com_google_cloud_vertexai_generativeai_preview_GenerativeModel_generateContentStream_com_google_cloud_vertexai_api_Content_) method.```\n public ResponseStream generateContentStream(Content content)\n \n```For a non-streaming response, use the [generateContent](/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.preview.GenerativeModel#com_google_cloud_vertexai_generativeai_preview_GenerativeModel_generateContent_com_google_cloud_vertexai_api_Content_) method.```\n public GenerateContentResponse generateContent(Content content)\n \n```\n### Sample code [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/vertexai/snippets/src/main/java/vertexai/gemini/MultimodalMultiImage.java) \n```\nimport com.google.cloud.vertexai.VertexAI;import com.google.cloud.vertexai.api.Content;import com.google.cloud.vertexai.api.GenerateContentResponse;import com.google.cloud.vertexai.generativeai.ContentMaker;import com.google.cloud.vertexai.generativeai.GenerativeModel;import com.google.cloud.vertexai.generativeai.PartMaker;import com.google.cloud.vertexai.generativeai.ResponseHandler;import java.io.ByteArrayOutputStream;import java.io.IOException;import java.io.InputStream;import java.net.HttpURLConnection;import java.net.URL;public class MultimodalMultiImage {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String projectId = \"your-google-cloud-project-id\";\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 String modelName = \"gemini-1.0-pro-vision\";\u00a0 \u00a0 multimodalMultiImage(projectId, location, modelName);\u00a0 }\u00a0 // Generates content from multiple input images.\u00a0 public static void multimodalMultiImage(String projectId, String location, String modelName)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs\u00a0 \u00a0 // to be created once, and can be reused for multiple requests.\u00a0 \u00a0 try (VertexAI vertexAI = new VertexAI(projectId, location)) {\u00a0 \u00a0 \u00a0 GenerativeModel model = new GenerativeModel(modelName, vertexAI);\u00a0 \u00a0 \u00a0 Content content = ContentMaker.fromMultiModalData(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PartMaker.fromMimeTypeAndData(\"image/png\", readImageFile(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"https://storage.googleapis.com/cloud-samples-data/vertex-ai/llm/prompts/landmark1.png\")),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"city: Rome, Landmark: the Colosseum\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PartMaker.fromMimeTypeAndData(\"image/png\", readImageFile(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"https://storage.googleapis.com/cloud-samples-data/vertex-ai/llm/prompts/landmark2.png\")),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"city: Beijing, Landmark: Forbidden City\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PartMaker.fromMimeTypeAndData(\"image/png\", readImageFile(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"https://storage.googleapis.com/cloud-samples-data/vertex-ai/llm/prompts/landmark3.png\"))\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 GenerateContentResponse response = model.generateContent(content);\u00a0 \u00a0 \u00a0 String output = ResponseHandler.getText(response);\u00a0 \u00a0 \u00a0 System.out.println(output);\u00a0 \u00a0 }\u00a0 }\u00a0 // Reads the image data from the given URL.\u00a0 public static byte[] readImageFile(String url) throws IOException {\u00a0 \u00a0 URL urlObj = new URL(url);\u00a0 \u00a0 HttpURLConnection connection = (HttpURLConnection) urlObj.openConnection();\u00a0 \u00a0 connection.setRequestMethod(\"GET\");\u00a0 \u00a0 int responseCode = connection.getResponseCode();\u00a0 \u00a0 if (responseCode == HttpURLConnection.HTTP_OK) {\u00a0 \u00a0 \u00a0 InputStream inputStream = connection.getInputStream();\u00a0 \u00a0 \u00a0 ByteArrayOutputStream outputStream = new ByteArrayOutputStream();\u00a0 \u00a0 \u00a0 byte[] buffer = new byte[1024];\u00a0 \u00a0 \u00a0 int bytesRead;\u00a0 \u00a0 \u00a0 while ((bytesRead = inputStream.read(buffer)) != -1) {\u00a0 \u00a0 \u00a0 \u00a0 outputStream.write(buffer, 0, bytesRead);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 return outputStream.toByteArray();\u00a0 \u00a0 } else {\u00a0 \u00a0 \u00a0 throw new RuntimeException(\"Error fetching file: \" + responseCode);\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Go setup instructions in the [Vertex AI quickstart.](/vertex-ai/docs/start/client-libraries) For more information, see the [Vertex AI Go SDK for Gemini reference documentation](/go/docs/reference/cloud.google.com/go/vertexai/latest) .To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment.](/docs/authentication/provide-credentials-adc#local-dev) \n### Streaming and non-streaming responsesYou can choose whether the model generates a streaming response or a non-streaming response. Streaming involves receiving responses to prompts as they are generated. That is, as soon as the model generates output tokens, the output tokens are sent. A non-streaming response to prompts is sent only after all of the output tokens are generated.For a streaming response, use the [GenerateContentStream](https://pkg.go.dev/cloud.google.com/go/vertexai/genai#GenerativeModel.GenerateContentStream) method.```\n iter := model.GenerateContentStream(ctx, genai.Text(\"Tell me a story about a lumberjack and his giant ox. Keep it very short.\"))\n \n```For a non-streaming response, use the [GenerateContent](https://pkg.go.dev/cloud.google.com/go/vertexai/genai#GenerativeModel.GenerateContent) method.```\n resp, err := model.GenerateContent(ctx, genai.Text(\"What is the average size of a swallow?\"))\n \n```\n### Sample code [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/vertexai/multimodal-multiple/multiple-multimodal.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 \"log\"\u00a0 \u00a0 \u00a0 \u00a0 \"net/http\"\u00a0 \u00a0 \u00a0 \u00a0 \"net/url\"\u00a0 \u00a0 \u00a0 \u00a0 \"os\"\u00a0 \u00a0 \u00a0 \u00a0 \"strings\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/vertexai/genai\")func main() {\u00a0 \u00a0 \u00a0 \u00a0 projectID := os.Getenv(\"GOOGLE_CLOUD_PROJECT\")\u00a0 \u00a0 \u00a0 \u00a0 location := \"us-central1\"\u00a0 \u00a0 \u00a0 \u00a0 modelName := \"gemini-1.0-pro-vision\"\u00a0 \u00a0 \u00a0 \u00a0 temperature := 0.4\u00a0 \u00a0 \u00a0 \u00a0 if projectID == \"\" {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatal(\"require environment variable GOOGLE_CLOUD_PROJECT\")\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // construct this multimodal prompt:\u00a0 \u00a0 \u00a0 \u00a0 // [image of colosseum] city: Rome, Landmark: the Colosseum\u00a0 \u00a0 \u00a0 \u00a0 // [image of forbidden city] \u00a0city: Beijing, Landmark: the Forbidden City\u00a0 \u00a0 \u00a0 \u00a0 // [new image]\u00a0 \u00a0 \u00a0 \u00a0 // create prompt image parts\u00a0 \u00a0 \u00a0 \u00a0 // colosseum\u00a0 \u00a0 \u00a0 \u00a0 colosseum, err := partFromImageURL(\"https://storage.googleapis.com/cloud-samples-data/vertex-ai/llm/prompts/landmark1.png\")\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"unable to read image: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // forbidden city\u00a0 \u00a0 \u00a0 \u00a0 forbiddenCity, err := partFromImageURL(\"https://storage.googleapis.com/cloud-samples-data/vertex-ai/llm/prompts/landmark2.png\")\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"unable to read image: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // new image\u00a0 \u00a0 \u00a0 \u00a0 newImage, err := partFromImageURL(\"https://storage.googleapis.com/cloud-samples-data/vertex-ai/llm/prompts/landmark3.png\")\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"unable to read image: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // create a multimodal (multipart) prompt\u00a0 \u00a0 \u00a0 \u00a0 prompt := []genai.Part{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 colosseum,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 genai.Text(\"city: Rome, Landmark: the Colosseum \"),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 forbiddenCity,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 genai.Text(\"city: Beijing, Landmark: the Forbidden City \"),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 newImage,\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // generate the response\u00a0 \u00a0 \u00a0 \u00a0 err = generateMultimodalContent(os.Stdout, prompt, projectID, location, modelName, float32(temperature))\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"unable to generate: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }}// generateMultimodalContent provide a generated response using multimodal inputfunc generateMultimodalContent(w io.Writer, parts []genai.Part, projectID, location, modelName string, temperature float32) error {\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 client, err := genai.NewClient(ctx, projectID, location)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatal(err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 model := client.GenerativeModel(modelName)\u00a0 \u00a0 \u00a0 \u00a0 model.SetTemperature(temperature)\u00a0 \u00a0 \u00a0 \u00a0 res, err := model.GenerateContent(ctx, parts...)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"unable to generate contents: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"generated response: %s\\n\", res.Candidates[0].Content.Parts[0])\u00a0 \u00a0 \u00a0 \u00a0 return nil}// partFromImageURL create a multimodal prompt part from an image URLfunc partFromImageURL(image string) (genai.Part, error) {\u00a0 \u00a0 \u00a0 \u00a0 var img genai.Blob\u00a0 \u00a0 \u00a0 \u00a0 imageURL, err := url.Parse(image)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return img, err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 res, err := http.Get(image)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil || res.StatusCode != 200 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return img, err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer res.Body.Close()\u00a0 \u00a0 \u00a0 \u00a0 data, err := io.ReadAll(res.Body)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return img, fmt.Errorf(\"unable to read from http: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 position := strings.LastIndex(imageURL.Path, \".\")\u00a0 \u00a0 \u00a0 \u00a0 if position == -1 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return img, fmt.Errorf(\"couldn't find a period to indicate a file extension\")\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 ext := imageURL.Path[position+1:]\u00a0 \u00a0 \u00a0 \u00a0 img = genai.ImageData(ext, data)\u00a0 \u00a0 \u00a0 \u00a0 return img, nil}\n```You can use REST to test a text prompt by using the Vertex AI API to send a POST request to the publisher model endpoint.\nBefore using any of the request data, make the following replacements:- : The type of response that you want the model to generate. Choose a method that generates how you want the model's response to be returned:- `streamGenerateContent`: The response is streamed as it's being generated to reduce the perception of latency to a human audience.\n- `generateContent`: The response is returned after it's fully generated.\n- : The region to process the request. Available options include the following:\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The model ID of the multimodal model  that you want to use. The options are:- `gemini-1.0-pro-vision`\n- : The role in a conversation associated with the content. Specifying a role is required even in singleturn use cases. Acceptable values include the following:- `USER`: Specifies content that's sent by you.- : The text instructions to include in the prompt.\n- : The [base64 encoding](/vertex-ai/generative-ai/docs/image/base64-encode) of the image or video to include inline in the prompt. When including media inline, you must also specify.\n- : The Cloud Storage URI of the image or video to include in the prompt. The bucket that stores the file must be in the same Google Cloud project that's sending the request. You must also specify.\n- : The media type of the image or video specified in the`data`or`fileUri`fields. Acceptable values include the following:\n- : The safety category to configure a threshold for. Acceptable values include the following:\n- : The threshold for blocking responses that could belong to the specified safety category based on probability. Acceptable values include the following:`BLOCK_LOW_AND_ABOVE`blocks the most while`BLOCK_ONLY_HIGH`blocks the least.\n- : The temperature is used for sampling during response generation, which occurs when`topP`and`topK`are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of`0`means that the highest probability tokens are always selected. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.If the model returns a response that's too generic, too short, or the model gives a fallback response, try increasing the temperature.\n- : Top-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K) to least probable until the sum of their probabilities equals the top-P value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-P value is`0.5`, then the model will select either A or B as the next token by using temperature and excludes C as a candidate.Specify a lower value for less random responses and a higher value for more random responses.\n- : Top-K changes how the model selects tokens for output. A top-K of`1`means the next selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-K of`3`means that the next token is selected from among the three most probable tokens by using temperature.For each token selection step, the top-K tokens with the highest probabilities are sampled. Then tokens are further filtered based on top-P with the final token selected using temperature sampling.Specify a lower value for less random responses and a higher value for more random responses.\n- : Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.Specify a lower value for shorter responses and a higher value for potentially longer responses.\n- : Specifies a list of strings that tells the model to stop generating text if one of the strings is encountered in the response. If a string appears multiple times in the response, then the response truncates where it's first encountered. The strings are case-sensitive.For example, if the following is the returned response when`stopSequences`isn't specified:`public static string reverse(string myString)`Then the returned response with`stopSequences`set to`[\"Str\", \"reverse\"]`is:`public static string`\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:GENERATE_RESPONSE_METHOD\n```\nRequest JSON body:\n```\n{\n \"contents\": {\n \"role\": \"ROLE\",\n \"parts\": [  {\n  \"inlineDATA\": {\n   \"mimeType\": \"MIME_TYPE\",\n   \"data\": \"B64_BASE_IMAGE\"\n  }\n  },\n  {\n  \"fileData\": {\n   \"mimeType\": \"MIME_TYPE\",\n   \"fileUri\": \"FILE_URI\"\n  }\n  },\n  {\n  \"text\": \"TEXT\"\n  }\n ]\n },\n \"safety_settings\": {\n \"category\": \"SAFETY_CATEGORY\",\n \"threshold\": \"THRESHOLD\"\n },\n \"generation_config\": {\n \"temperature\": TEMPERATURE,\n \"topP\": TOP_P,\n \"topK\": TOP_K,\n \"candidateCount\": 1,\n \"maxOutputTokens\": MAX_OUTPUT_TOKENS,\n \"stopSequences\": STOP_SEQUENCES,\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:GENERATE_RESPONSE_METHOD\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:GENERATE_RESPONSE_METHOD\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following.```\nLOCATION=\"us-central1\"MODEL_ID=\"gemini-1.0-pro-vision\"PROJECT_ID=\"test-project\"curl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth application-default print-access-token)\" \\-H \"Content-Type: application/json\"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:${GENERATE_RESPONSE_METHOD} -d \\$'{\u00a0 \"contents\": {\u00a0 \u00a0 \"role\": \"user\",\u00a0 \u00a0 \"parts\": [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \"fileData\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"mimeType\": \"image/png\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fileUri\": \"gs://my-bucket/images/cat.png\"\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \"text\": \"Describe this picture.\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ]\u00a0 },\u00a0 \"safety_settings\": {\u00a0 \u00a0 \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\u00a0 \u00a0 \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\u00a0 },\u00a0 \"generation_config\": {\u00a0 \u00a0 \"temperature\": 0.4,\u00a0 \u00a0 \"topP\": 1,\u00a0 \u00a0 \"topK\": 32,\u00a0 \u00a0 \"maxOutputTokens\": 2048,\u00a0 }}'\n```To send a multimodal prompt by using the Google Cloud console, do the following:- In the Vertex AI section of the Google Cloud console, go to  the **Vertex AI Studio** page. [Go to Vertex AI Studio](https://console.cloud.google.com/vertex-ai/generative/multimodal) \n- Under **Prompt design (single turn)** , click **Open** .\n- Configure the model and parameters:- **Region** : Select the region that you want to use.\n- **Model** : Select **Gemini Pro Vision** .\n- **Temperature** : Use the slider or textbox to enter a value for  temperature.The temperature is used for sampling during response generation, which occurs when`topP`and`topK`are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of`0`means that the highest probability tokens are always selected. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.If the model returns a response that's too generic, too short, or the model gives a fallback response, try increasing the temperature.\n- **Token limit** : Use the slider or textbox to enter a value for the  max output limit.Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.Specify a lower value for shorter responses and a higher value for potentially longer responses.\n- **Add stop sequence** : Enter a stop sequence, which is a series of   characters (including spaces) that stops response generation if the   model encounters it. The sequence is not included as part of the   response. You can add up to five stop sequences.- Optional: To configure advanced parameters, click **Advanced** and  configure as follows:\n- The Google Cloud console only supports streaming, which involves receiving responses to prompts as they are generated. You are ready to enter a message in the message box to start a conversation with the model.The model uses the previous messages as context for new  responses. To include an image or video in the prompt, click the add_photo_alternate icon.To learn about multimodal prompts, see [Design multimodal prompts](/vertex-ai/generative-ai/docs/multimodal/design-multimodal-prompts) .\n- Optional: To save your prompt to **My prompts** , clicksave_alt **Save** .\n- Optional: To get the Python code or a curl command for your prompt, clickcode **Get code** .\n- Optional: To clear all previous messages, clickdelete **Clear conversation**\n## Send multimodal prompt requests that include video\nFor testing and iterating on multimodal prompts, we recommend using the Google Cloud console. To send a multimodal prompt that includes video programmatically to the model, you can use the REST API, Vertex AI SDK for Python, or one of the other supported libraries and SDKs shown in the following tabs:\nTo learn how to install or update the Vertex AI SDK for Python, see [ Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk#install-vertex-ai-python-sdk) . For more information, see the [Vertex AI SDK for Python API reference documentation.](/python/docs/reference/aiplatform/latest) \n### Streaming and non-streaming responsesYou can choose whether the model generates a streaming response or a non-streaming response. Streaming involves receiving responses to prompts as they are generated. That is, as soon as the model generates output tokens, the output tokens are sent. A non-streaming response to prompts is sent only after all of the output tokens are generated.For a streaming response, use the `stream` parameter in [generate_content](/python/docs/reference/aiplatform/latest/vertexai.preview.generative_models#content) .```\n response = model.generate_content(contents=[...], stream = True)\n \n```For a non-streaming response, remove the parameter, or set the parameter to `False` .\n### Sample code [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/gemini_single_turn_video_example.py) \n```\nimport vertexaifrom vertexai.generative_models import GenerativeModel, Partdef generate_text(project_id: str, location: str) -> str:\u00a0 \u00a0 # Initialize Vertex AI\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 # Load the model\u00a0 \u00a0 vision_model = GenerativeModel(\"gemini-1.0-pro-vision\")\u00a0 \u00a0 # Generate text\u00a0 \u00a0 response = vision_model.generate_content(\u00a0 \u00a0 \u00a0 \u00a0 [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Part.from_uri(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gs://cloud-samples-data/video/animals.mp4\", mime_type=\"video/mp4\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"What is in the video?\",\u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 )\u00a0 \u00a0 print(response)\u00a0 \u00a0 return response.text\n```Before trying this sample, follow the Node.js setup instructions in the [Generative AI quickstart using the Node.js SDK](/nodejs/docs/reference/vertexai/latest#before-you-begin) . For more information, see the [Node.js SDK for Gemini reference documentation](/nodejs/docs/reference/vertexai/latest) .To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n### Streaming and non-streaming responsesYou can choose whether the model generates a streaming response or a non-streaming response. Streaming involves receiving responses to prompts as they are generated. That is, as soon as the model generates output tokens, the output tokens are sent. A non-streaming response to prompts is sent only after all of the output tokens are generated.For a streaming response, use the [generateContentStream](/nodejs/docs/reference/vertexai/latest#streaming-content-generation) method.```\n const streamingResp = await generativeModel.generateContentStream(request);\n \n```For a non-streaming response, use the [generateContent](/nodejs/docs/reference/vertexai/latest#content-generation-non-streaming) method.```\n const streamingResp = await generativeModel.generateContent(request);\n \n```\n### Sample code [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/generative-ai/snippets/sendMultiModalPromptWithVideo.js) \n```\nconst {VertexAI} = require('@google-cloud/vertexai');/**\u00a0* TODO(developer): Update these variables before running the sample.\u00a0*/async function sendMultiModalPromptWithVideo(\u00a0 projectId = 'PROJECT_ID',\u00a0 location = 'us-central1',\u00a0 model = 'gemini-1.0-pro-vision') {\u00a0 // Initialize Vertex with your Cloud project and location\u00a0 const vertexAI = new VertexAI({project: projectId, location: location});\u00a0 const generativeVisionModel = vertexAI.getGenerativeModel({\u00a0 \u00a0 model: model,\u00a0 });\u00a0 // Pass multimodal prompt\u00a0 const request = {\u00a0 \u00a0 contents: [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 role: 'user',\u00a0 \u00a0 \u00a0 \u00a0 parts: [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fileData: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fileUri: 'gs://cloud-samples-data/video/animals.mp4',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mimeType: 'video/mp4',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 text: 'What is in the video?',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ],\u00a0 };\u00a0 // Create the response\u00a0 const response = await generativeVisionModel.generateContent(request);\u00a0 // Wait for the response to complete\u00a0 const aggregatedResponse = await response.response;\u00a0 // Select the text from the response\u00a0 const fullTextResponse =\u00a0 \u00a0 aggregatedResponse.candidates[0].content.parts[0].text;\u00a0 console.log(fullTextResponse);}\n```Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart](/vertex-ai/docs/start/client-libraries) . For more information, see the [Vertex AI Java SDK for Gemini reference documentation](/java/docs/reference/google-cloud-vertexai/latest/overview) .To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n### Streaming and non-streaming responsesYou can choose whether the model generates a streaming response or a non-streaming response. Streaming involves receiving responses to prompts as they are generated. That is, as soon as the model generates output tokens, the output tokens are sent. A non-streaming response to prompts is sent only after all of the output tokens are generated.For a streaming response, use the [generateContentStream](/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.preview.GenerativeModel#com_google_cloud_vertexai_generativeai_preview_GenerativeModel_generateContentStream_com_google_cloud_vertexai_api_Content_) method.```\n public ResponseStream generateContentStream(Content content)\n \n```For a non-streaming response, use the [generateContent](/java/docs/reference/google-cloud-vertexai/latest/com.google.cloud.vertexai.generativeai.preview.GenerativeModel#com_google_cloud_vertexai_generativeai_preview_GenerativeModel_generateContent_com_google_cloud_vertexai_api_Content_) method.```\n public GenerateContentResponse generateContent(Content content)\n \n```\n### Sample code [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/vertexai/snippets/src/main/java/vertexai/gemini/MultimodalVideoInput.java) \n```\nimport com.google.cloud.vertexai.VertexAI;import com.google.cloud.vertexai.api.GenerateContentResponse;import com.google.cloud.vertexai.generativeai.ContentMaker;import com.google.cloud.vertexai.generativeai.GenerativeModel;import com.google.cloud.vertexai.generativeai.PartMaker;import com.google.cloud.vertexai.generativeai.ResponseHandler;import java.io.IOException;public class MultimodalVideoInput {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String projectId = \"your-google-cloud-project-id\";\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 String modelName = \"gemini-1.0-pro-vision\";\u00a0 \u00a0 multimodalVideoInput(projectId, location, modelName);\u00a0 }\u00a0 // Analyzes the given video input.\u00a0 public static void multimodalVideoInput(String projectId, String location, String modelName)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs\u00a0 \u00a0 // to be created once, and can be reused for multiple requests.\u00a0 \u00a0 try (VertexAI vertexAI = new VertexAI(projectId, location)) {\u00a0 \u00a0 \u00a0 String videoUri = \"gs://cloud-samples-data/video/animals.mp4\";\u00a0 \u00a0 \u00a0 GenerativeModel model = new GenerativeModel(modelName, vertexAI);\u00a0 \u00a0 \u00a0 GenerateContentResponse response = model.generateContent(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ContentMaker.fromMultiModalData(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"What is in the video?\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PartMaker.fromMimeTypeAndData(\"video/mp4\", videoUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ));\u00a0 \u00a0 \u00a0 String output = ResponseHandler.getText(response);\u00a0 \u00a0 \u00a0 System.out.println(output);\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Go setup instructions in the [Vertex AI quickstart.](/vertex-ai/docs/start/client-libraries) For more information, see the [Vertex AI Go SDK for Gemini reference documentation](/go/docs/reference/cloud.google.com/go/vertexai/latest) .To authenticate to Vertex AI, set up Application Default Credentials. For more information, see [Set up authentication for a local development environment.](/docs/authentication/provide-credentials-adc#local-dev) \n### Streaming and non-streaming responsesYou can choose whether the model generates a streaming response or a non-streaming response. Streaming involves receiving responses to prompts as they are generated. That is, as soon as the model generates output tokens, the output tokens are sent. A non-streaming response to prompts is sent only after all of the output tokens are generated.For a streaming response, use the [GenerateContentStream](https://pkg.go.dev/cloud.google.com/go/vertexai/genai#GenerativeModel.GenerateContentStream) method.```\n iter := model.GenerateContentStream(ctx, genai.Text(\"Tell me a story about a lumberjack and his giant ox. Keep it very short.\"))\n \n```For a non-streaming response, use the [GenerateContent](https://pkg.go.dev/cloud.google.com/go/vertexai/genai#GenerativeModel.GenerateContent) method.```\n resp, err := model.GenerateContent(ctx, genai.Text(\"What is the average size of a swallow?\"))\n \n```\n### Sample code [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/vertexai/multimodal-video/multimodalvideo.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"errors\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 \"mime\"\u00a0 \u00a0 \u00a0 \u00a0 \"path/filepath\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/vertexai/genai\")// generateMultimodalContent generates a response into w, based upon the prompt// and video provided.// video is a Google Cloud Storage path starting with \"gs://\"func generateMultimodalContent(w io.Writer, prompt, video, projectID, location, modelName string) error {\u00a0 \u00a0 \u00a0 \u00a0 // prompt := \"What is in this video?\"\u00a0 \u00a0 \u00a0 \u00a0 // video := \"gs://cloud-samples-data/video/animals.mp4\"\u00a0 \u00a0 \u00a0 \u00a0 // location := \"us-central1\"\u00a0 \u00a0 \u00a0 \u00a0 // modelName := \"gemini-1.0-pro-vision\"\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 client, err := genai.NewClient(ctx, projectID, location)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"unable to create client: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 model := client.GenerativeModel(modelName)\u00a0 \u00a0 \u00a0 \u00a0 model.SetTemperature(0.4)\u00a0 \u00a0 \u00a0 \u00a0 // Given a video file URL, prepare video file as genai.Part\u00a0 \u00a0 \u00a0 \u00a0 img := genai.FileData{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MIMEType: mime.TypeByExtension(filepath.Ext(video)),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 FileURI: \u00a0video,\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 res, err := model.GenerateContent(ctx, img, genai.Text(prompt))\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"unable to generate contents: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 if len(res.Candidates) == 0 ||\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 len(res.Candidates[0].Content.Parts) == 0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return errors.New(\"empty response from model\")\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"generated response: %s\\n\", res.Candidates[0].Content.Parts[0])\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```You can use REST to test a text prompt by using the Vertex AI API to send a POST request to the publisher model endpoint.\nBefore using any of the request data, make the following replacements:- : The type of response that you want the model to generate. Choose a method that generates how you want the model's response to be returned:- `streamGenerateContent`: The response is streamed as it's being generated to reduce the perception of latency to a human audience.\n- `generateContent`: The response is returned after it's fully generated.\n- : The region to process the request. Available options include the following:\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The model ID of the multimodal model  that you want to use. The options are:- `gemini-1.0-pro-vision`\n- : The role in a conversation associated with the content. Specifying a role is required even in singleturn use cases. Acceptable values include the following:- `USER`: Specifies content that's sent by you.- : The text instructions to include in the prompt.\n- : The [base64 encoding](/vertex-ai/generative-ai/docs/image/base64-encode) of the image or video to include inline in the prompt. When including media inline, you must also specify.\n- : The Cloud Storage URI of the image or video to include in the prompt. The bucket that stores the file must be in the same Google Cloud project that's sending the request. You must also specify.\n- : The media type of the image or video specified in the`data`or`fileUri`fields. Acceptable values include the following:\n- : The safety category to configure a threshold for. Acceptable values include the following:\n- : The threshold for blocking responses that could belong to the specified safety category based on probability. Acceptable values include the following:`BLOCK_LOW_AND_ABOVE`blocks the most while`BLOCK_ONLY_HIGH`blocks the least.\n- : The temperature is used for sampling during response generation, which occurs when`topP`and`topK`are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of`0`means that the highest probability tokens are always selected. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.If the model returns a response that's too generic, too short, or the model gives a fallback response, try increasing the temperature.\n- : Top-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K) to least probable until the sum of their probabilities equals the top-P value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-P value is`0.5`, then the model will select either A or B as the next token by using temperature and excludes C as a candidate.Specify a lower value for less random responses and a higher value for more random responses.\n- : Top-K changes how the model selects tokens for output. A top-K of`1`means the next selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-K of`3`means that the next token is selected from among the three most probable tokens by using temperature.For each token selection step, the top-K tokens with the highest probabilities are sampled. Then tokens are further filtered based on top-P with the final token selected using temperature sampling.Specify a lower value for less random responses and a higher value for more random responses.\n- : Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.Specify a lower value for shorter responses and a higher value for potentially longer responses.\n- : Specifies a list of strings that tells the model to stop generating text if one of the strings is encountered in the response. If a string appears multiple times in the response, then the response truncates where it's first encountered. The strings are case-sensitive.For example, if the following is the returned response when`stopSequences`isn't specified:`public static string reverse(string myString)`Then the returned response with`stopSequences`set to`[\"Str\", \"reverse\"]`is:`public static string`\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:GENERATE_RESPONSE_METHOD\n```\nRequest JSON body:\n```\n{\n \"contents\": {\n \"role\": \"ROLE\",\n \"parts\": [  {\n  \"inlineDATA\": {\n   \"mimeType\": \"MIME_TYPE\",\n   \"data\": \"B64_BASE_IMAGE\"\n  }\n  },\n  {\n  \"fileData\": {\n   \"mimeType\": \"MIME_TYPE\",\n   \"fileUri\": \"FILE_URI\"\n  }\n  },\n  {\n  \"text\": \"TEXT\"\n  }\n ]\n },\n \"safety_settings\": {\n \"category\": \"SAFETY_CATEGORY\",\n \"threshold\": \"THRESHOLD\"\n },\n \"generation_config\": {\n \"temperature\": TEMPERATURE,\n \"topP\": TOP_P,\n \"topK\": TOP_K,\n \"candidateCount\": 1,\n \"maxOutputTokens\": MAX_OUTPUT_TOKENS,\n \"stopSequences\": STOP_SEQUENCES,\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:GENERATE_RESPONSE_METHOD\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/publishers/google/models/MODEL_ID:GENERATE_RESPONSE_METHOD\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following.```\nLOCATION=\"us-central1\"MODEL_ID=\"gemini-1.0-pro-vision\"PROJECT_ID=\"test-project\"curl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth application-default print-access-token)\" \\-H \"Content-Type: application/json\"https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:${GENERATE_RESPONSE_METHOD} -d \\$'{\u00a0 \"contents\": {\u00a0 \u00a0 \"role\": \"user\",\u00a0 \u00a0 \"parts\": [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \"fileData\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"mimeType\": \"image/png\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fileUri\": \"gs://my-bucket/images/cat.png\"\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \"text\": \"Describe this picture.\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ]\u00a0 },\u00a0 \"safety_settings\": {\u00a0 \u00a0 \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\u00a0 \u00a0 \"threshold\": \"BLOCK_LOW_AND_ABOVE\"\u00a0 },\u00a0 \"generation_config\": {\u00a0 \u00a0 \"temperature\": 0.4,\u00a0 \u00a0 \"topP\": 1,\u00a0 \u00a0 \"topK\": 32,\u00a0 \u00a0 \"maxOutputTokens\": 2048,\u00a0 }}'\n```To send a multimodal prompt by using the Google Cloud console, do the following:- In the Vertex AI section of the Google Cloud console, go to  the **Vertex AI Studio** page. [Go to Vertex AI Studio](https://console.cloud.google.com/vertex-ai/generative/multimodal) \n- Under **Prompt design (single turn)** , click **Open** .\n- Configure the model and parameters:- **Region** : Select the region that you want to use.\n- **Model** : Select **Gemini Pro Vision** .\n- **Temperature** : Use the slider or textbox to enter a value for  temperature.The temperature is used for sampling during response generation, which occurs when`topP`and`topK`are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of`0`means that the highest probability tokens are always selected. In this case, responses for a given prompt are mostly deterministic, but a small amount of variation is still possible.If the model returns a response that's too generic, too short, or the model gives a fallback response, try increasing the temperature.\n- **Token limit** : Use the slider or textbox to enter a value for the  max output limit.Maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.Specify a lower value for shorter responses and a higher value for potentially longer responses.\n- **Add stop sequence** : Enter a stop sequence, which is a series of   characters (including spaces) that stops response generation if the   model encounters it. The sequence is not included as part of the   response. You can add up to five stop sequences.- Optional: To configure advanced parameters, click **Advanced** and  configure as follows:\n- The Google Cloud console only supports streaming, which involves receiving responses to prompts as they are generated. You are ready to enter a message in the message box to start a conversation with the model.The model uses the previous messages as context for new  responses. To include an image or video in the prompt, click the add_photo_alternate icon.To learn about multimodal prompts, see [Design multimodal prompts](/vertex-ai/generative-ai/docs/multimodal/design-multimodal-prompts) .\n- Optional: To save your prompt to **My prompts** , clicksave_alt **Save** .\n- Optional: To get the Python code or a curl command for your prompt, clickcode **Get code** .\n- Optional: To clear all previous messages, clickdelete **Clear conversation**\n## Image requirements for prompts\nPrompts that use image data are subject to the following limitations and requirements:\n- Images must be in one of the following image data MIME types:- PNG - image/png\n- JPEG - image/jpeg\n- Maximum of 16 individual images.\n- Maximum of 4MB for the entire prompt, including images and text.\n- No specific limits to the number of pixels in an image. However, larger images are scaled down and padded to fit a maximum resolution of 3072 x 3072 while preserving their original aspect ratio.\n- Each image accounts for 258 tokens.\n### Image best practices\nWhen using images in your prompt, follow these recommendations for best results:\n- Prompts with a single image produce better results than prompts with multiple images when you want to detect text in an image.\n- If your prompt contains a single image, placing the image before the text prompt might produce better results.\n- If there are multiple images in the prompt, and you want to refer to them later in your prompt or have the model refer to them in the model response, it can help to give each image an index before the image. Use `a` `b` `c` , or `image 1` `image 2` `image 3` for your index. The following is an example of using indexed images in a prompt:```\nimage 1 <piano_recital.jpeg>\nimage 2 <family_dinner.jpeg>\nimage 3 <coffee_shop.jpeg>\nWrite a blogpost about my day using image 1 and image 2. Then, give me ideas\nfor tomorrow based on image 3.\n```\n- Images with higher resolution yield better results.\n- Include a few examples in the prompt.\n- Rotate images to their proper orientation before adding them to the prompt.\nFor more multimodal prompting tips, see [Design multimodal prompts](/vertex-ai/generative-ai/docs/multimodal/design-multimodal-prompts) .\n## Video limits and requirements for prompts\nPrompts that use video data are subject to the following limitations and requirements:\n- Videos must be in one of the following image data MIME types:- MOV - video/mov\n- MPEG - video/mpeg\n- MP4 - video/mp4\n- MPG - video/mpg\n- AVI - video/avi\n- WMV - video/wmv\n- MPEGPS - video/mpegps\n- FLS - video/flv\n- We recommend no more than one video per prompt.\n- The model processes videos as non-contiguous image frames from the video. Audio isn't included. If you notice the model missing some content from the video, try making the video shorter so that the model captures a greater portion of the video content.\n- Only information in the first 2 minutes is processed.\n- Each video accounts for 1,032 tokens.\n- No audio information or timestamp metadata is analyzed. Because of this, the model might not perform well in use cases that require audio input, such as captioning audio, or time-related information, such as speed or rhythm.\nFor more multimodal prompting tips, see [Design multimodal prompts](/vertex-ai/generative-ai/docs/multimodal/design-multimodal-prompts) .\n## What's next\n- Learn how to [send chat prompt requests](/vertex-ai/generative-ai/docs/multimodal/send-chat-prompts-gemini) .\n- Learn about [responsible AI best practices and Vertex AI's safety filters](/vertex-ai/generative-ai/docs/learn/responsible-ai) .", "guide": "Generative AI on Vertex AI"}