{"title": "Generative AI on Vertex AI - Tune text models by using supervised tuning", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-models-supervised", "abstract": "# Generative AI on Vertex AI - Tune text models by using supervised tuning\nSupervised tuning uses labeled examples to tune a model. Each example demonstrates output you want from your text model during inference. Supervised tuning is a good option when the output of your model isn't very complex and is easy to define. If the output from your model is difficult to define, consider tuning your text model by using [Reinforcement learning from human feedback (RLHF) tuning](/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf) .\nTo see an example of using the Vertex AI SDK for Python to tune the text generation foundation model,  run the \"Vertex AI tuning a PEFT model\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/tune_peft.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fgenerative_ai%2Ftune_peft.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/tune_peft.ipynb)\n", "content": "## Text model supervised tuning step-by-step guidance\nThe following guided tutorial helps you learn how to use supervised tuning to tune a text foundation model in the Google Cloud console.\nTo follow step-by-step guidance for this task directly in the Google Cloud console, click **Guide me** :\n[Guide me](https://console.cloud.google.com/?walkthrough_id=prompt_tuning)\n## Supported models\nThe following text models support supervised tuning:\n- `text-bison@002`\n- `chat-bison@002`\n- `text-bison-32k`\n- `chat-bison-32k`## Use cases for using supervised tuning on text models\nFoundation text models work well when the expected output or task can be clearly and concisely defined in a prompt and the prompt consistently produces the expected output. If you want a model to learn something niche or specific that deviates from general language patterns, then you might want to consider tuning that model. For example, you can use model tuning to teach the model the following:\n- Specific structures or formats for generating output.\n- Specific behaviors such as when to provide a terse or verbose output.\n- Specific customized outputs for specific types of inputs.\nThe following examples are use cases that are difficult to capture with only prompt instructions:\n- **Classification** : The expected response is a specific word or phrase.| 0                                        |\n|:----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Prompt: Classify the following text into one of the following classes: [business, entertainment]. Text: Diversify your investment portfolio Response: business |Tuning the model can help prevent the model from generating verbose responses.\n- **Summarization** : The summary follows a specific format. For example, you might need to remove personally identifiable information (PII) in a chat summary.| 0                                            |\n|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Prompt: Summarize: Jessica: That sounds great! See you in Times Square! Alexander: See you at 10! Response: #Person1 and #Person2 agree to meet at Times Square at 10:00 AM. |This formatting of replacing the names of the speakers with `#Person1` and `#Person2` is difficult to describe and the foundation model might not naturally produce such a response.\n- **Extractive question answering** : The question is about a context and the answer is a substring of the context.| 0                                                                    |\n|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Prompt: Context: There is evidence that there have been significant changes in Amazon rainforest vegetation over the last 21,000 years through the Last Glacial Maximum (LGM) and subsequent deglaciation. Question: What does LGM stand for? Response: Last Glacial Maximum |The response \"Last Glacial Maximum\" is a specific phrase from the context.\n- **Chat** : You need to customize model response to follow a persona, role, or character.| 0                                         |\n|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Prompt: User: What's the weather like today? Response: Assistant: As the virtual shopkeeper of Lola Lollipops, I can only help you with the purchases and shipping. |\nYou can also tune a model in the following situations:\n- Prompts are not producing the expected results consistently enough.\n- The task is too complicated to define in a prompt. For example, you want the model to do behavior cloning for a behavior that's hard to articulate in a prompt.\n- You have complex intuitions about a task that are easy to elicit but difficult to formalize in a prompt.\n- You want to reduce the context length by removing the few-shot examples.## Prepare a supervised tuning dataset\nThe dataset used to tune a foundation model needs to include examples that align with the task that you want the model to perform. Structure your training dataset in a text-to-text format. Each record, or row, in the dataset contains the input text (also referred to as the prompt) which is paired with its expected output from the model. Supervised tuning uses the dataset to teach the model to mimic a behavior, or task, you need by giving it hundreds of examples that illustrate that behavior.\nYour dataset must include a minimum of 10 examples, but we recommend at least 100 to 500 examples for good results. The more examples you provide in your dataset, the better the results.\nFor sample datasets, see [Sample datasets](#sample-datasets) on this page.\n### Dataset format\nYour model tuning dataset must be in [JSON Lines](https://jsonlines.org/) (JSONL) format, where each line contains a single tuning example. The dataset format used to tune a text generation model is different from the dataset format for tuning a text chat model. Before tuning your model, you must [upload your dataset to a Cloud Storage bucket](#upload-datasets) .\nEach example is composed of an `input_text` field that contains the prompt to the model and an `output_text` field that contains an example response that the tuned model is expected to produce. Additional fields from structured prompts, such as `context` , are ignored.\nThe maximum token length for `input_text` is 8,192 and the maximum token length for `output_text` is 1,024. If either field exceeds the maximum token length, the excess tokens are truncated.\nThe maximum number of examples that a dataset for a text generation model can contain is 10,000.\n### Dataset example```\n{\"input_text\": \"question: How many people live in Beijing? context:\nWith over 21 million residents, Beijing is the world's most populous national\ncapital city and is China's second largest city after Shanghai. It is\nlocated in Northern China, and is governed as a municipality under the direct\nadministration of the State Council with 16 urban, suburban, and rural\ndistricts.[14] Beijing is mostly surrounded by Hebei Province with the exception\nof neighboring Tianjin to the southeast; together, the three divisions form the\nJingjinji megalopolis and the national capital region of China.\",\n\"output_text\": \"over 21 million people\"}\n{\"input_text\": \"question: How many parishes are there in Louisiana? context:\nThe U.S. state of Louisiana is divided into 64 parishes (French: paroisses) in\nthe same manner that 48 other states of the United States are divided into\ncounties, and Alaska is divided into boroughs.\", \"output_text\": \"64\"}\n```\n### Include instructions in examplesFor tasks such as classification, it is possible to create a dataset of examples that don't contain instructions. However, excluding instructions from the examples in the dataset leads to worse performance after tuning than including instructions, especially for smaller datasets.\n **Excludes instructions** :\n```\n{\"input_text\": \"5 stocks to buy now\",\"output_text\": \"business\"}\n```\n **Includes instructions** :\n```\n{\"input_text\": \"Classify the following text into one of the following classes:[business, entertainment] Text: 5 stocks to buy now\",\"output_text\": \"business\"}\n```\nEach conversation example in a chat tuning dataset is composed of a `messages` field (required) and a `context` field (optional).\nThe `messages` field consists of an array of author-content pairs. The `author` field refers to the author of the message and is set to either `user` or `assistant` in an alternating manner. The `content` field is the content of the message. Each conversation example should have two to three user-assistant message pairs, which represent a message from the user and a response from the model.\nThe [context](/vertex-ai/generative-ai/docs/chat/chat-prompts#context) field lets you specify a context for the chat. If you specify a context for an example, it will override the value provided in `default_context` .\nFor each conversation example, the maximum token length for `context` and `messages` combined is 8,192 tokens. Additionally, each `content` field for `assistant` shouldn't exceed 1,024 tokens.\nThe maximum number of `author` fields that the examples in the dataset for a text chat model can contain is 10,000. This maximum is for the sum of all `author` fields in all `messages` in all the examples.```\n{\u00a0 \"context\": \"You are a pirate dog named Captain Barktholomew.\",\u00a0 \"messages\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"author\": \"user\",\u00a0 \u00a0 \u00a0 \"content\": \"Hi\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"author\": \"assistant\",\u00a0 \u00a0 \u00a0 \"content\": \"Argh! What brings ye to my ship?\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"author\": \"user\",\u00a0 \u00a0 \u00a0 \"content\": \"What's your name?\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"author\": \"assistant\",\u00a0 \u00a0 \u00a0 \"content\": \"I be Captain Barktholomew, the most feared pirate dog of the seven seas.\"\u00a0 \u00a0 }\u00a0 ]}\n```\n### Sample datasets\nYou can use a sample dataset to get started with tuning the `text-bison@002` model. The following is a classification task dataset that contains sample medical transcriptions for various medical specialties. The data is from [mtsamples.com](https://mtsamples.com) as made available on [Kaggle](https://www.kaggle.com/datasets/tboyle10/medicaltranscriptions) .\n- Sample tuning dataset URI:`gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`\n- Sample eval dataset URI:`gs://cloud-samples-data/vertex-ai/model-evaluation/peft_eval_sample.jsonl`\nTo use these datasets, specify the URIs in the applicable parameters when [creating a text model supervised tuning job](#create_a_text_model_supervised_tuning_job) .\nFor example:\n```\n...\"dataset_uri\": \"gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl\",...\"evaluation_data_uri\": \"gs://cloud-samples-data/vertex-ai/model-evaluation/peft_eval_sample.jsonl\",...\n```\n### Maintain consistency with production data\nThe examples in your datasets should match your expected production traffic. If your dataset contains specific formatting, keywords, instructions, or information, the production data should be formatted in the same way and contain the same instructions.\nFor example, if the examples in your dataset include a `\"question:\"` and a `\"context:\"` , production traffic should also be formatted to include a `\"question:\"` and a `\"context:\"` in the same order as it appears in the dataset examples. If you exclude the context, the model will not recognize the pattern, even if the exact question was in an example in the dataset.\n### Upload tuning datasets to Cloud Storage\nTo run a tuning job, you need to upload one or more datasets to a Cloud Storage bucket. You can either [create a new Cloud Storage bucket](/storage/docs/creating-buckets#create_a_new_bucket) or use an existing one to store dataset files. The region of the bucket doesn't matter, but we recommend that you use a bucket that's in the same Google Cloud project where you plan to tune your model.\nAfter your bucket is ready, [upload](/storage/docs/creating-buckets#create_a_new_bucket) your dataset file to the bucket.\n## Supervised tuning region settings\nYou can specify three Google Cloud region settings when you configure a supervised tuning job. One region is where the pipeline that tunes your model runs. The other region is where the model tuning job runs and the tuned model is uploaded.\n### Pipeline job region\nThe pipeline job region is the region where the pipeline job runs. If the optional [model upload region](#upload-region) isn't specified, then the model is uploaded and deployed to the pipeline job region. Intermediate data, such as the transformed dataset, is stored in the pipeline job region. To learn which regions you can use for the pipeline job region, see [Supported pipeline job and model upload regions](#supported-supervised-tuning-regions) . You must specify the pipeline job region using one of the following methods:\n- If you use the Vertex AI SDK, you can specify the region where the pipeline job runs using the `tuning_job_location` parameter on the `tune_model` method of the object that represents the model you're tuning (for example, the [TextGenerationModel.tune_model](/python/docs/reference/aiplatform/latest/vertexai.language_models.TextGenerationModel#vertexai_language_models_TextGenerationModel_tune_model) method).\n- If you create a supervised tuning job by sending a POST request using the [pipelineJobs.create](/vertex-ai/docs/reference/rest/v1/projects.locations.pipelineJobs/create) method, then you use the URL to specify the region where the pipeline job runs. In the following URL, replacing both instances of with the region where the pipeline runs:```\n\u00a0https://PIPELINE_JOB_REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/PIPELINE_JOB_REGION/pipelineJobs\n```\n- If you use the Google Cloud console to create a supervised model tuning job, then you specify the pipeline job region in the **Region** control when you create your tuning job. In the Google Cloud console, the **Region** control specifies both the pipeline job region and the model upload region. When you use the Google Cloud console to create a supervised model tuning job, both regions are always the same.\n### Model upload region\nYou use the optional `tuned_model_location` parameter to specify where your tuned model is uploaded. If the model upload region isn't specified, then the tuned model is uploaded to the [pipeline job region](#pipeline-region) .You can use one of the [Supported pipeline job and model upload regions](#supported-supervised-tuning-regions) for your model upload region. You can specify the model upload region using one of the following methods:\n- If you use the Vertex AI SDK, the `tuned_model_location` parameter is specified on the `tune_model` method of the object that represents the model you're tuning (for example, the [TextGenerationModel.tune_model](/python/docs/reference/aiplatform/latest/vertexai.language_models.TextGenerationModel#vertexai_language_models_TextGenerationModel_tune_model) method).\n- If you create a supervised model tuning job by sending a POST request using the [pipelineJobs](/vertex-ai/docs/reference/rest/v1/projects.locations.pipelineJobs/create) method, then you can use the `location` parameter to specify the model upload region.\n- If you use the Google Cloud console to create a supervised model tuning job, then you specify the model upload region in the **Region** control when you create your tuning job. In the Google Cloud console, the **Region** control specifies both the model upload region and the pipeline job region. When you use the Google Cloud console to create a supervised model tuning job, both regions are always the same.\n### Model tuning region\nThe model tuning region is where the model tuning computation occurs. This region is determined by the accelerator type you choose. If you specify `TPU` for your accelerator type, then your model tuning computation happens in `europe-west4` . If you specify `GPU` for your accelerator type, then model tuning happens in `us-central1` .\n### Supported pipeline job and model upload regions\nYou can use one of the following regions to specify the model upload region and to specify the pipeline job region:\n- `us-central1`\n- `europe-west4`\n- `asia-southeast1`\n- `us-west1`\n- `europe-west3`\n- `europe-west2`\n- `asia-northeast1`\n- `us-east4`\n- `us-west4`\n- `northamerica-northeast1`\n- `europe-west9`\n- `europe-west1`\n- `asia-northeast3`## Create a text model supervised tuning job\nYou can create a supervised text model tuning job by using the Google Cloud console, API, or the Vertex AI SDK for Python. For guidance on model tuning configurations, see [Recommended configurations](#recommended_configurations) .\nTo see an example of using the Vertex AI SDK for Python to tune the text generation foundation model,  run the \"Vertex AI tuning a PEFT model\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/tune_peft.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fgenerative_ai%2Ftune_peft.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/tune_peft.ipynb)\nTo create a model tuning job, send a POST request by using the [pipelineJobs](/vertex-ai/docs/reference/rest/v1/projects.locations.pipelineJobs/create) method. Note that some of the parameters are not supported by all of the models. Ensure that you only include the applicable parameters for the model that you're tuning.\nBefore using any of the request data, make the following replacements:- : A display  name for the pipelineJob.\n- : The URI of the bucket to output  pipeline artifacts to.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : A display name for the  model uploaded (created) by the pipelineJob.\n- : URI of your dataset file.\n- : The region where the pipeline tuning job runs. This is also the default region for where the tuned model is uploaded. If you want to upload your model to a different region, then use the`location`parameter to specify the tuned model upload region. For more information, see [Model upload region](/vertex-ai/generative-ai/docs/models/tune-text-models-supervised#pipeline-region) .\n- : (optional) The region where the tuned model is uploaded. If you don't specify a model upload region, then the tuned model uploads to the same region where the pipeline job runs. For more information, see [Model upload region](/vertex-ai/generative-ai/docs/models/tune-text-models-supervised#upload-region) .\n- : (optional, default`GPU`) The type of accelerator to use for model tuning. The valid options are:- `GPU`: Uses eight A100 80 GB GPUs for tuning. Make sure you have enough [quota](/vertex-ai/generative-ai/docs/models/tune-models#quota) . If you choose`GPU`, then [VPC\u2011SC](/vertex-ai/docs/general/vpc-service-controls) is supported. [CMEK](/vertex-ai/docs/general/cmek) is supported if the tuning  location and model upload location are`us-centra1`. For more information, see [  Supervised tuning region settings](/vertex-ai/generative-ai/docs/models/tune-text-models-supervised#supervised-tuning-regions) . If you choose`GPU`, then your model tuning  computations happen in the`us-central1`region.\n- `TPU`: Uses 64 cores of the TPU v3 pod for tuning. Make sure you have enough [quota](/vertex-ai/generative-ai/docs/models/tune-models#quota) . [CMEK](/vertex-ai/docs/general/cmek) isn't supported, but [VPC\u2011SC](/vertex-ai/docs/general/vpc-service-controls) is supported. If you  choose`TPU`, then your model tuning computations happen in the`europe-west4`region.\n- : Name of the  foundation model to tune. The options are:- `text-bison@002`\n- `chat-bison@002`\n- : The  context that applies to all tuning examples in the tuning dataset. Setting the`context`field in an example overrides the default context.\n- : The number of steps to run for model tuning. The default value is 300. The batch size varies by tuning location and model size. For 8k models, such as`text-bison@002`,`chat-bison@002`,`code-bison@002`, and`codechat-bison@002`:- `us-central1`has a batch size of 8.\n- `europe-west4`has a batch size of 24.\nFor 32k models, such as`text-bison-32k`,`chat-bison-32k`,`code-bison-32k`, and`codechat-bison-32k`:- `us-central1`has a batch size of 8.\n- `europe-west4`has a batch size of 8.\nFor example, if you're training `text-bison@002` in `europe-west4` , there are 240 examples in a training dataset, and you set `steps` to 20, then the number of training examples is the product of 20 steps and the batch size of 24, or 480 training steps. In this case, there are two epochs in the training process because it goes through the examples two times. In `us-central1` , if there are 240 examples in a training dataset and you set `steps` to 15, then the number of training examples is the product of 15 steps and the batch size of 8, or 120 training steps. In this case, there are 0.5 epochs because there are half as many training steps as there are examples.\n- : A  multiplier to apply to the recommended learning rate. To use the recommended learning rate,  use`1.0`.\n- : (optional) The URI of the JSONL file that contains the evaluation dataset for batch prediction and evaluation. Evaluation isn't supported for`chat-bison`. For more information, see [Dataset format fortuning a code model](/vertex-ai/generative-ai/docs/models/tune-text-models#dataset-format) . The evaluation dataset requires between ten and 250 examples.\n- : (optional, default`20`) The number of tuning steps between each evaluation. An evaluation interval isn't supported for chat models. Because the evaluation runs on the entire evaluation dataset, a smaller evaluation interval results in a longer tuning time. For example, if`steps`is 200 and`EVAL_INTERVAL`is 100, then you will get only two data points for the evaluation metrics. This parameter requires that the`evaluation_data_uri`is set.\n- :  (optional, default`true`) A`boolean`that, if set to`true`, stops tuning before completing all the tuning steps if model performance, as measured by the accuracy of predicted tokens, does not improve enough between evaluations runs. If`false`, tuning continues until all the tuning steps are complete. This parameter requires that the`evaluation_data_uri`is set. Enable early stopping isn't supported for chat models.\n- :  (optional) The ID of a [Vertex AI TensorBoard instance](/vertex-ai/docs/experiments/tensorboard-setup#tensorboard_id) . The Vertex AI TensorBoard instance is used to create an [experiment](/vertex-ai/docs/experiments/create-experiment) after the tuning job completes. The Vertex AI TensorBoard instance needs to be in the same region as the tuning pipeline.\n- : (optional) The fully qualified name of a customer-managed encryption key (CMEK) that you want to use for data encryption. A CMEK is available only in`us-central1`. If you use`us-central1`and don't specify a CMEK, then a Google-managed encryption key is used. A Google-managed encryption key is used by default in all other [available regions](/vertex-ai/generative-ai/docs/models/tune-text-models-supervised#supported-supervised-tuning-regions) . For more information, see [CMEK overview](/kms/docs/cmek) .\n- : The tuning template to use depends  on the model that you're tuning:- Text model:`https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v2.0.0`\n- Chat model:`https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-chat-model/v3.0.0`\n- : (optional) The service  account that Vertex AI uses to run your pipeline job. By default, your project's  Compute Engine default service account (`PROJECT_NUMBER\u2011compute@developer.gserviceaccount.com`)  is used. Learn more about [ attaching a custom service account](/vertex-ai/docs/general/custom-service-account) .\nHTTP method and URL:\n```\nPOST https://PIPELINE_JOB_REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/PIPELINE_JOB_REGION/pipelineJobs\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"PIPELINEJOB_DISPLAYNAME\",\n \"runtimeConfig\": {\n \"gcsOutputDirectory\": \"gs://OUTPUT_DIR\",\n \"parameterValues\": {\n  \"project\": \"PROJECT_ID\",\n  \"model_display_name\": \"MODEL_DISPLAYNAME\",\n  \"dataset_uri\": \"gs://DATASET_URI\",\n  \"location\": \"MODEL_UPLOAD_REGION\",\n  \"accelerator_type\": \"ACCELERATOR_TYPE\",\n  \"large_model_reference\": \"LARGE_MODEL_REFERENCE\",\n  \"default_context\": \"DEFAULT_CONTEXT (chat only)\",\n  \"train_steps\": STEPS,\n  \"learning_rate_multiplier\": LEARNING_RATE_MULTIPLIER,\n  \"evaluation_data_uri\": \"gs://EVAL_DATASET_URI (text only)\",\n  \"evaluation_interval\": EVAL_INTERVAL (text only),\n  \"enable_early_stopping\": ENABLE_EARLY_STOPPING (text only),\n  \"enable_checkpoint_selection\": \"ENABLE_CHECKPOINT_SELECTION (text only)\",\n  \"tensorboard_resource_id\": \"TENSORBOARD_ID\",\n  \"encryption_spec_key_name\": \"ENCRYPTION_KEY_NAME\"\n }\n },\n \"encryptionSpec\": {\n \"kmsKeyName\": \"ENCRYPTION_KEY_NAME\"\n },\n \"serviceAccount\": \"SERVICE_ACCOUNT\",\n \"templateUri\": \"TEMPLATE_URI\"\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://PIPELINE_JOB_REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/PIPELINE_JOB_REGION/pipelineJobs\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://PIPELINE_JOB_REGION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/PIPELINE_JOB_REGION/pipelineJobs\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following. Note that `pipelineSpec` has been truncated to save space.To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/tuning.py) \n```\nfrom __future__ import annotationsfrom typing import Optionalfrom google.auth import defaultfrom google.cloud import aiplatformimport pandas as pdimport vertexaifrom vertexai.language_models import TextGenerationModelfrom vertexai.preview.language_models import TuningEvaluationSpeccredentials, _ = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])def tuning(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 model_display_name: str,\u00a0 \u00a0 training_data: pd.DataFrame | str,\u00a0 \u00a0 train_steps: int = 10,\u00a0 \u00a0 evaluation_dataset: Optional[str] = None,\u00a0 \u00a0 tensorboard_instance_name: Optional[str] = None,) -> TextGenerationModel:\u00a0 \u00a0 \"\"\"Tune a new model, based on a prompt-response data.\u00a0 \u00a0 \"training_data\" can be either the GCS URI of a file formatted in JSONL format\u00a0 \u00a0 (for example: training_data=f'gs://{bucket}/{filename}.jsonl'), or a pandas\u00a0 \u00a0 DataFrame. Each training example should be JSONL record with two keys, for\u00a0 \u00a0 example:\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \"input_text\": <input prompt>,\u00a0 \u00a0 \u00a0 \u00a0 \"output_text\": <associated output>\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 or the pandas DataFame should contain two columns:\u00a0 \u00a0 \u00a0 ['input_text', 'output_text']\u00a0 \u00a0 with rows for each training example.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 project_id: GCP Project ID, used to initialize vertexai\u00a0 \u00a0 \u00a0 location: GCP Region, used to initialize vertexai\u00a0 \u00a0 \u00a0 model_display_name: Customized Tuned LLM model name.\u00a0 \u00a0 \u00a0 training_data: GCS URI of jsonl file or pandas dataframe of training data.\u00a0 \u00a0 \u00a0 train_steps: Number of training steps to use when tuning the model.\u00a0 \u00a0 \u00a0 evaluation_dataset: GCS URI of jsonl file of evaluation data.\u00a0 \u00a0 \u00a0 tensorboard_instance_name: The full name of the existing Vertex AI TensorBoard instance:\u00a0 \u00a0 \u00a0 \u00a0 projects/PROJECT_ID/locations/LOCATION_ID/tensorboards/TENSORBOARD_INSTANCE_ID\u00a0 \u00a0 \u00a0 \u00a0 Note that this instance must be in the same region as your tuning job.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 vertexai.init(project=project_id, location=location, credentials=credentials)\u00a0 \u00a0 eval_spec = TuningEvaluationSpec(evaluation_data=evaluation_dataset)\u00a0 \u00a0 eval_spec.tensorboard = aiplatform.Tensorboard(\u00a0 \u00a0 \u00a0 \u00a0 tensorboard_name=tensorboard_instance_name\u00a0 \u00a0 )\u00a0 \u00a0 model = TextGenerationModel.from_pretrained(\"text-bison@002\")\u00a0 \u00a0 model.tune_model(\u00a0 \u00a0 \u00a0 \u00a0 training_data=training_data,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 model_display_name=model_display_name,\u00a0 \u00a0 \u00a0 \u00a0 train_steps=train_steps,\u00a0 \u00a0 \u00a0 \u00a0 tuning_job_location=\"europe-west4\",\u00a0 \u00a0 \u00a0 \u00a0 tuned_model_location=location,\u00a0 \u00a0 \u00a0 \u00a0 tuning_evaluation_spec=eval_spec,\u00a0 \u00a0 )\u00a0 \u00a0 print(model._job.status)\u00a0 \u00a0 return model\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/tuning.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';const aiplatform = require('@google-cloud/aiplatform');const {PipelineServiceClient} = aiplatform.v1;// Import the helper module for converting arbitrary protobuf.Value objects.const {helpers} = aiplatform;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'europe-west4-aiplatform.googleapis.com',};const model = 'text-bison@001';const pipelineClient = new PipelineServiceClient(clientOptions);async function tuneLLM() {\u00a0 // Configure the parent resource\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 const parameters = {\u00a0 \u00a0 train_steps: helpers.toValue(trainSteps),\u00a0 \u00a0 project: helpers.toValue(project),\u00a0 \u00a0 location: helpers.toValue('us-central1'),\u00a0 \u00a0 dataset_uri: helpers.toValue(datasetUri),\u00a0 \u00a0 large_model_reference: helpers.toValue(model),\u00a0 \u00a0 model_display_name: helpers.toValue(modelDisplayName),\u00a0 \u00a0 accelerator_type: helpers.toValue('GPU'), // Optional: GPU or TPU\u00a0 };\u00a0 const runtimeConfig = {\u00a0 \u00a0 gcsOutputDirectory,\u00a0 \u00a0 parameterValues: parameters,\u00a0 };\u00a0 const pipelineJob = {\u00a0 \u00a0 templateUri:\u00a0 \u00a0 \u00a0 'https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v2.0.0',\u00a0 \u00a0 displayName: 'my-tuning-job',\u00a0 \u00a0 runtimeConfig,\u00a0 };\u00a0 const createPipelineRequest = {\u00a0 \u00a0 parent,\u00a0 \u00a0 pipelineJob,\u00a0 \u00a0 pipelineJobId,\u00a0 };\u00a0 await new Promise((resolve, reject) => {\u00a0 \u00a0 pipelineClient.createPipelineJob(createPipelineRequest).then(\u00a0 \u00a0 \u00a0 response => resolve(response),\u00a0 \u00a0 \u00a0 e => reject(e)\u00a0 \u00a0 );\u00a0 }).then(response => {\u00a0 \u00a0 const [result] = response;\u00a0 \u00a0 console.log('Tuning pipeline job:');\u00a0 \u00a0 console.log(`\\tName: ${result.name}`);\u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 `\\tCreate time: ${new Date(1970, 0, 1)\u00a0 \u00a0 \u00a0 \u00a0 .setSeconds(result.createTime.seconds)\u00a0 \u00a0 \u00a0 \u00a0 .toLocaleString()}`\u00a0 \u00a0 );\u00a0 \u00a0 console.log(`\\tStatus: ${result.status}`);\u00a0 });}await tuneLLM();\n```Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreatePipelineJobModelTuningSample.java) \n```\nimport com.google.cloud.aiplatform.v1beta1.CreatePipelineJobRequest;import com.google.cloud.aiplatform.v1beta1.LocationName;import com.google.cloud.aiplatform.v1beta1.PipelineJob;import com.google.cloud.aiplatform.v1beta1.PipelineJob.RuntimeConfig;import com.google.cloud.aiplatform.v1beta1.PipelineServiceClient;import com.google.cloud.aiplatform.v1beta1.PipelineServiceSettings;import com.google.protobuf.Value;import java.io.IOException;import java.util.HashMap;import java.util.Map;public class CreatePipelineJobModelTuningSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"PROJECT\";\u00a0 \u00a0 String location = \"europe-west4\"; // europe-west4 and us-central1 are the supported regions\u00a0 \u00a0 String pipelineJobDisplayName = \"PIPELINE_JOB_DISPLAY_NAME\";\u00a0 \u00a0 String modelDisplayName = \"MODEL_DISPLAY_NAME\";\u00a0 \u00a0 String outputDir = \"OUTPUT_DIR\";\u00a0 \u00a0 String datasetUri = \"DATASET_URI\";\u00a0 \u00a0 int trainingSteps = 300;\u00a0 \u00a0 createPipelineJobModelTuningSample(\u00a0 \u00a0 \u00a0 \u00a0 project,\u00a0 \u00a0 \u00a0 \u00a0 location,\u00a0 \u00a0 \u00a0 \u00a0 pipelineJobDisplayName,\u00a0 \u00a0 \u00a0 \u00a0 modelDisplayName,\u00a0 \u00a0 \u00a0 \u00a0 outputDir,\u00a0 \u00a0 \u00a0 \u00a0 datasetUri,\u00a0 \u00a0 \u00a0 \u00a0 trainingSteps);\u00a0 }\u00a0 // Create a model tuning job\u00a0 public static void createPipelineJobModelTuningSample(\u00a0 \u00a0 \u00a0 String project,\u00a0 \u00a0 \u00a0 String location,\u00a0 \u00a0 \u00a0 String pipelineJobDisplayName,\u00a0 \u00a0 \u00a0 String modelDisplayName,\u00a0 \u00a0 \u00a0 String outputDir,\u00a0 \u00a0 \u00a0 String datasetUri,\u00a0 \u00a0 \u00a0 int trainingSteps)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 final String endpoint = String.format(\"%s-aiplatform.googleapis.com:443\", location);\u00a0 \u00a0 PipelineServiceSettings pipelineServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PipelineServiceSettings.newBuilder().setEndpoint(endpoint).build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests.\u00a0 \u00a0 try (PipelineServiceClient client = PipelineServiceClient.create(pipelineServiceSettings)) {\u00a0 \u00a0 \u00a0 Map<String, Value> parameterValues = new HashMap<>();\u00a0 \u00a0 \u00a0 parameterValues.put(\"project\", stringToValue(project));\u00a0 \u00a0 \u00a0 parameterValues.put(\"model_display_name\", stringToValue(modelDisplayName));\u00a0 \u00a0 \u00a0 parameterValues.put(\"dataset_uri\", stringToValue(datasetUri));\u00a0 \u00a0 \u00a0 parameterValues.put(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"location\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 stringToValue(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"us-central1\")); // Deployment is only supported in us-central1 for Public Preview\u00a0 \u00a0 \u00a0 parameterValues.put(\"large_model_reference\", stringToValue(\"text-bison@001\"));\u00a0 \u00a0 \u00a0 parameterValues.put(\"train_steps\", numberToValue(trainingSteps));\u00a0 \u00a0 \u00a0 parameterValues.put(\"accelerator_type\", stringToValue(\"GPU\")); // Optional: GPU or TPU\u00a0 \u00a0 \u00a0 RuntimeConfig runtimeConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 RuntimeConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setGcsOutputDirectory(outputDir)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .putAllParameterValues(parameterValues)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 PipelineJob pipelineJob =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PipelineJob.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTemplateUri(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v2.0.0\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(pipelineJobDisplayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setRuntimeConfig(runtimeConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 LocationName parent = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 CreatePipelineJobRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CreatePipelineJobRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setParent(parent.toString())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setPipelineJob(pipelineJob)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 PipelineJob response = client.createPipelineJob(request);\u00a0 \u00a0 \u00a0 System.out.format(\"response: %s\\n\", response);\u00a0 \u00a0 \u00a0 System.out.format(\"Name: %s\\n\", response.getName());\u00a0 \u00a0 }\u00a0 }\u00a0 static Value stringToValue(String str) {\u00a0 \u00a0 return Value.newBuilder().setStringValue(str).build();\u00a0 }\u00a0 static Value numberToValue(int n) {\u00a0 \u00a0 return Value.newBuilder().setNumberValue(n).build();\u00a0 }}\n```To tune a text model with supervised tuning by using the Google Cloud console, perform the following steps:- In the Vertex AI section of the Google Cloud console, go to  the **Vertex AI Studio** page. [Go to Vertex AI Studio](https://console.cloud.google.com/vertex-ai/generative/language) \n- Click the **Tune and distill** tab.\n- Clickadd **Create tuned model** .\n- Click **Supervised tuning** .\n- Configure model details:- **Tuned model name** : Enter a name for your tuned model.\n- **Base model** : Select the model that you want to tune.\n- **Region** : Select the region where the pipeline tuning job runs and where the tuned model is deployed.\n- **Output directory** : Enter the Cloud Storage location where artifacts are stored when your model is tuned.\n- Expand **Advanced Options** to configure advanced settings.- **Train steps** : Enter the number of steps to run for model tuning. The default value is 300. The batch size varies by tuning location and model size. For 8k models, such as`text-bison@002`,`chat-bison@002`,`code-bison@002`, and`codechat-bison@002`:- `us-central1`has a batch size of 8.\n- `europe-west4`has a batch size of 24.\nFor 32k models, such as`text-bison-32k`,`chat-bison-32k`,`code-bison-32k`, and`codechat-bison-32k`:- `us-central1`has a batch size of 8.\n- `europe-west4`has a batch size of 8.\nFor example, if you're training `text-bison@002` in `europe-west4` , there are 240 examples in a training dataset, and you set `steps` to 20, then the number of training examples is the product of 20 steps and the batch size of 24, or 480 training steps. In this case, there are two epochs in the training process because it goes through the examples two times. In `us-central1` , if there are 240 examples in a training dataset and you set `steps` to 15, then the number of training examples is the product of 15 steps and the batch size of 8, or 120 training steps. In this case, there are 0.5 epochs because there are half as many training steps as there are examples.\n- **Learning rate multiplier** : Enter the step size at each iteration. The default value is 1.\n- **Accelerator type** : (optional) Enter the type of accelerator to use for model tuning. The valid options are:- `GPU`: Uses eight A100 80 GB GPUs for tuning. Make sure you have enough [quota](/vertex-ai/generative-ai/docs/models/tune-models#quota) . If you choose`GPU`, then [VPC\u2011SC](/vertex-ai/docs/general/vpc-service-controls) is supported. [CMEK](/vertex-ai/docs/general/cmek) is supported if the tuning  location and model upload location are`us-centra1`. For more information, see [  Supervised tuning region settings](/vertex-ai/generative-ai/docs/models/tune-text-models-supervised#supervised-tuning-regions) . If you choose`GPU`, then your model tuning  computations happen in the`us-central1`region.\n- `TPU`: Uses 64 cores of the TPU v3 pod for tuning. Make sure you have enough [quota](/vertex-ai/generative-ai/docs/models/tune-models#quota) . [CMEK](/vertex-ai/docs/general/cmek) isn't supported, but [VPC\u2011SC](/vertex-ai/docs/general/vpc-service-controls) is supported. If you  choose`TPU`, then your model tuning computations happen in the`europe-west4`region.\n- **Add a TensorBoard instance** : (optional) The ID of a [Vertex AI TensorBoard instance](/vertex-ai/docs/experiments/tensorboard-setup#tensorboard_id) . The Vertex AI TensorBoard instance is used to create an [experiment](/vertex-ai/docs/experiments/create-experiment) after the tuning job completes. The Vertex AI TensorBoard instance needs to be in the same region as the tuning pipeline.\n- **Encryption** (optional) Choose to use a Google-managed encryption key or a customer-managed encryption key (CMEK). A CMEK is available for encryption only in the`us-central1`region. In all other [available regions](/vertex-ai/generative-ai/docs/models/tune-text-models-supervised#supported-supervised-tuning-regions) , a Google-managed encryption key is used. For more information, see [CMEK overview](/kms/docs/cmek) .\n- **Service account** (optional) Choose a a user-managed service account. A service account determines which Google Cloud resources your service code can access. If you don't choose a service account, then a Google-managed service account is used that includes permissions appropriate for most models.\n- Click **Continue** \n- If you want to upload your dataset file, selectradio_button_checked **Upload JSONL file to Cloud Storage** . If your  dataset file is already in a Cloud Storage bucket, selectradio_button_checked **Existing JSONL file on Cloud Storage** .- In **Select JSONL file** , click **Browse** and    select your dataset file.\n- In **Dataset location** , click **Browse** and select the Cloud Storage bucket where you want to store your    dataset file.\nIn **Cloud Storage file path** , click **Browse** and select the Cloud Storage bucket where your dataset file is    located.\n- (Optional) To evaluate your tuned model, select **Enable model\n  evaluation** and configure your model evaluation:- **Evaluation dataset** : (optional) The URI of the JSONL file that contains the evaluation dataset for batch prediction and evaluation. Evaluation isn't supported for`chat-bison`. For more information, see [Dataset format fortuning a code model](/vertex-ai/generative-ai/docs/models/tune-text-models#dataset-format) . The evaluation dataset requires between ten and 250 examples.\n- **Evaluation interval** : (optional, default`20`) The number of tuning steps between each evaluation. An evaluation interval isn't supported for chat models. Because the evaluation runs on the entire evaluation dataset, a smaller evaluation interval results in a longer tuning time. For example, if`steps`is 200 and`EVAL_INTERVAL`is 100, then you will get only two data points for the evaluation metrics. This parameter requires that the`evaluation_data_uri`is set.\n- **Enable early stopping** : (optional, default`true`) A`boolean`that, if set to`true`, stops tuning before completing all the tuning steps if model performance, as measured by the accuracy of predicted tokens, does not improve enough between evaluations runs. If`false`, tuning continues until all the tuning steps are complete. This parameter requires that the`evaluation_data_uri`is set. Enable early stopping isn't supported for chat models.\n- **Enable checkpoint selection** : When enabled, Vertex AI selects and returns the checkpoint with the best model evaluation performance from all checkpoints created during the tuning job. When disabled, the final checkpoint created during the tuning job is returned. Each checkpoint refers to a snapshot of the model during a tuning job.\n- **TensorBoard instance** : (optional) The ID of a [Vertex AI TensorBoard instance](/vertex-ai/docs/experiments/tensorboard-setup#tensorboard_id) . The Vertex AI TensorBoard instance is used to create an [experiment](/vertex-ai/docs/experiments/create-experiment) after the tuning job completes. The Vertex AI TensorBoard instance needs to be in the same region as the tuning pipeline.\n- Click **Start tuning** .**Note:** All intermediate pipeline data, such as the processed dataset, is stored in the [pipeline job region](#pipeline-region) . The model tuning computation happens in the [model tuning region](#tuning-region) . The model is deployed to the [model upload region](#upload-region) .\n```\nPROJECT_ID=myprojectDATASET_URI=gs://my-gcs-bucket-uri/datasetOUTPUT_DIR=gs://my-gcs-bucket-uri/outputACCELERATOR_TYPE=GPULOCATION=us-central1curl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json; charset=utf-8\" \\\"https://europe-west4-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/europe-west4/pipelineJobs?pipelineJobId=tune-large-model-$(date +%Y%m%d%H%M%S)\" -d \\$'{\u00a0 \"displayName\": \"tune-llm\",\u00a0 \"runtimeConfig\": {\u00a0 \u00a0 \"gcsOutputDirectory\": \"'${OUTPUT_DIR}'\",\u00a0 \u00a0 \"parameterValues\": {\u00a0 \u00a0 \u00a0 \"project\": \"'${PROJECT_ID}'\",\u00a0 \u00a0 \u00a0 \"model_display_name\": \"The display name for your model in the UI\",\u00a0 \u00a0 \u00a0 \"dataset_uri\": \"'${DATASET_URI}'\",\u00a0 \u00a0 \u00a0 \"location\": \"'${LOCATION}'\",\u00a0 \u00a0 \u00a0 \"accelerator_type:\": \"'${ACCELERATOR_TYPE}'\",\u00a0 \u00a0 \u00a0 \"large_model_reference\": \"text-bison@002\",\u00a0 \u00a0 \u00a0 \"train_steps\": 300,\u00a0 \u00a0 \u00a0 \"learning_rate_multiplier\": 1,\u00a0 \u00a0 \u00a0 \"encryption_spec_key_name\": \"projects/myproject/locations/us-central1/keyRings/sample-key/cryptoKeys/sample-key\"\u00a0 \u00a0 }\u00a0 },\u00a0 \"encryptionSpec\": {\u00a0 \u00a0 \"kmsKeyName\": \"projects/myproject/locations/us-central1/keyRings/sample-key/cryptoKeys/sample-key\"\u00a0 },\u00a0 \"templateUri\": \"https://us-kfp.pkg.dev/ml-pipeline/large-language-model-pipelines/tune-large-model/v2.0.0\"}'\n```\n### Recommended configurations\nThe following table shows the recommended configurations for tuning a foundation model by task:\n| Task   | No. of examples in dataset | Train steps |\n|:---------------|:-----------------------------|:--------------|\n| Classification | 100+       | 100-500  |\n| Summarization | 100-500+      | 200-1000  |\n| Extractive QA | 100+       | 100-500  |\n| Chat   | 200+       | 1000   |\nFor train steps, you can try more than one value to get the best performance on a particular dataset, for example, 100, 200, 500.\n## View a list of tuned models\nYou can view a list of models in your current project, including your tuned models, by using the Google Cloud console or the Vertex AI SDK for Python.\nBefore trying this sample, follow the Python setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Python API reference documentation](/python/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/generative_ai/list_tuned_models.py) \n```\nimport vertexaifrom vertexai.language_models import TextGenerationModeldef list_tuned_models(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,) -> None:\u00a0 \u00a0 \"\"\"List tuned models.\"\"\"\u00a0 \u00a0 vertexai.init(project=project_id, location=location)\u00a0 \u00a0 model = TextGenerationModel.from_pretrained(\"text-bison@002\")\u00a0 \u00a0 tuned_model_names = model.list_tuned_model_names()\u00a0 \u00a0 print(tuned_model_names)\u00a0 \u00a0 return tuned_model_names\n```To view your tuned models in the Google Cloud console, go to the **Vertex AI Model Registry** page.\n [Go to Vertex AI Model Registry](https://console.cloud.google.com/vertex-ai/models)\n## Load a tuned text model\nThe following sample code uses the Vertex AI SDK for Python to load a text generation model that was tuned using supervised tuning:\n```\nimport vertexaifrom vertexai.preview.language_models import TextGenerationModelmodel = TextGenerationModel.get_tuned_model(TUNED_MODEL_NAME)\n```\nReplace `` with the qualified resource name of your tuned model. This name is in the format `projects/` `` `/locations/` `` `/models/` `` . You can find the model ID of your tuned model in [Vertex AI Model Registry](https://console.cloud.google.com/vertex-ai/models) .\n## Tuning and evaluation metrics\nYou can configure a model tuning job to collect and report model tuning and model evaluation metrics, which can then be visualized by using [Vertex AI TensorBoard](/vertex-ai/docs/experiments/tensorboard-introduction) . To connect your tuning job to Vertex AI TensorBoard, specify a [Vertex AI TensorBoard instance ID](/vertex-ai/docs/experiments/tensorboard-setup#tensorboard_id) and an evaluation dataset.\n### Model tuning metrics\nYou can configure a model tuning job to collect the following tuning metrics for `chat-bison` , `code-bison` , `codechat-bison` , and `text-bison` :\n- `/train_total_loss`: Loss for the tuning dataset at a training step.\n- `/train_fraction_of_correct_next_step_preds`: The token accuracy at a training step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the tuning dataset.\n- `/train_num_predictions:`Number of predicted tokens at a training step.\n### Model evaluation metrics:\nYou can configure a model tuning job to collect the following evaluation metrics for `code-bison` and `text-bison` :\n- `/eval_total_loss`: Loss for the evaluation dataset at an evaluation step.\n- `/eval_fraction_of_correct_next_step_preds`: The token accuracy at an evaluation step. A single prediction consists of a sequence of tokens. This metric measures the accuracy of the predicted tokens when compared to the ground truth in the evaluation dataset.\n- `/eval_num_predictions`: Number of predicted tokens at an evaluation step.\nThe metrics visualizations are available after the model tuning job completes. If you specify only a Vertex AI TensorBoard instance ID and not an evaluation dataset when you create the tuning job, only the visualizations for the tuning metrics are available.\n## Troubleshooting\nThe following topics might help you solve issues with tuning a foundation text model using supervised tuning.\nIf you encounter this 500 error when trying to tune a model, try this workaround:\nRun the following cURL command to create an empty Vertex AI dataset. Ensure that you configure your project ID in the command.\n```\ncurl \\-X POST \\-H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\-H \"Content-Type: application/json\" \\https://europe-west4-aiplatform.googleapis.com/ui/projects/$PROJECT_ID/locations/europe-west4/datasets \\-d '{\u00a0 \u00a0 \"display_name\": \"test-name1\",\u00a0 \u00a0 \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/image_1.0.0.yaml\",\u00a0 \u00a0 \"saved_queries\": [{\"display_name\": \"saved_query_name\", \"problem_type\": \"IMAGE_CLASSIFICATION_MULTI_LABEL\"}]}'\n```\nAfter the command completes, wait five minutes and try model tuning again.\nMake sure that the Compute Engine API is enabled and that the default Compute Engine service account ( `` `\u2011compute@developer.gserviceaccount.com` ) is granted the [aiplatform.admin](/vertex-ai/docs/general/access-control#aiplatform.admin) and the [storage.objectAdmin](/storage/docs/access-control/iam-roles#standard-roles) roles.\nTo grant the [aiplatform.admin](/vertex-ai/docs/general/access-control#aiplatform.admin) and the [storage.objectAdmin](/storage/docs/access-control/iam-roles#standard-roles) roles to the Compute Engine service account, do the following:\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) At the bottom of the Google Cloud console, a [Cloud Shell](/shell/docs/how-cloud-shell-works) session starts and displays a command-line prompt. Cloud Shell is a shell environment  with the Google Cloud CLI  already installed and with values already set for  your current project. It can take a few seconds for the session to initialize.If you prefer to use a terminal on your machine, [install and configure the Google Cloud CLI](/sdk/docs/install-sdk) .\n- Attach the [aiplatform.admin](/vertex-ai/docs/general/access-control#aiplatform.admin) role to your Compute Engine service account using the [gcloud projects add-iam-policy-binding command](/sdk/gcloud/reference/projects/add-iam-policy-binding) :Replace the following:- ``with your Google Cloud project ID.\n- ``with your Google Cloud project number.\n```\ngcloud projects add-iam-policy-binding PROJECT_ID --member serviceAccount:PROJECT_NUM-compute@developer.gserviceaccount.com --role roles/aiplatform.admin\n```\n- Attach the [storage.objectAdmin](/storage/docs/access-control/iam-roles#standard-roles) role to your Compute Engine service account using the [gcloud projects add-iam-policy-binding command](/sdk/gcloud/reference/projects/add-iam-policy-binding) :- ``with your Google Cloud project ID.\n- ``with your Google Cloud project number.\n```\ngcloud projects add-iam-policy-binding PROJECT_ID --member serviceAccount:PROJECT_NUM-compute@developer.gserviceaccount.com \u00a0--role roles/storage.objectAdmin\n```This permission error is due to a propagation delay. A subsequent retry should resolve this error.\n## What's next\n- Learn how to [evaluate a tuned model](/vertex-ai/generative-ai/docs/models/evaluate-models) .\n- Learn how to [tune a foundation model using RLHF tuning](/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf) .\n- Learn how to [tune a code model](/vertex-ai/generative-ai/docs/models/tune-code-models) .", "guide": "Generative AI on Vertex AI"}