{"title": "Generative AI on Vertex AI - Tune text embeddings", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-embeddings", "abstract": "# Generative AI on Vertex AI - Tune text embeddings\nThis page shows you how to tune the [text embedding model](/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) , `textembedding-gecko` and `textembedding-gecko-multilingual` . These foundation models have been trained on a large set of public text data. If you have a unique use case which requires your own specific training data you can use model tuning. After you tune a foundation embedding model, the model should be catered for your use case. Tuning is supported for [stable versions](/vertex-ai/generative-ai/docs/learn/model-versioning#stable-version-available) of the text embedding model.\nText embedding models support [supervised tuning](/vertex-ai/generative-ai/docs/models/tune-text-models-supervised) . Supervised tuning uses labeled examples that demonstrate the type of output you'd like from your text embedding model during inference. Text embedding models don't support tuning by using [Reinforcement learning from human feedback (RLHF)](/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf) .\nTo learn more about model tuning, see [How model tuning works](/vertex-ai/generative-ai/docs/models/tune-models#how_model_tuning_works) .\n", "content": "## Use case for tuning an embedding model\nTuning a text embeddings model can enable your model to adapt to the embeddings to a specific domain or task. This can be useful if the pre-trained embeddings model is not well-suited to your specific needs. For example, you might fine-tune an embeddings model on a specific dataset of customer support tickets for your company. This could help a chatbot understand the different types of customer support issues your customers typically have, and be able to answer their questions more effectively. Without tuning, the model doesn't know the specifics of your customer support tickets or the solutions to specific problems for your product.\n## Tuning workflow\nThe model tuning workflow on Vertex AI for `textembedding-gecko` and `textembedding-gecko-multilingual` is as follows:\n- Prepare your model tuning dataset.\n- Upload the model tuning dataset to a Cloud Storage bucket.\n- Create a model tuning job.\n- Deploy the tuned model to a Vertex AI endpoint of the same name. Unlike text or Codey model tuning jobs, a text embedding tuning job doesn't deploy your tuned models to a Vertex AI endpoint.## Prepare your embeddings dataset\nThe dataset used to tune an embeddings model includes data that align with the task that you want the model to perform.\n### Dataset format for tuning an embeddings model\nThe training dataset consists of the following files, which need to be in Cloud Storage. The path of the files are defined by parameters when launching the tuning pipeline. The four required files are the **corpus file** , **query file** , **training labels** , and **test labels** .\n- **Corpus file** : The path is defined by parameter `corpus_path` . It's a JSONL file where each line has the fields `_id` , `title` , and `text` with string values. `_id` and `text` are required, while `title` is optional. For example:```\n{\"_id\": \"doc1\", \"title\": \"Get an introduction to generative AI on Vertex AI\", \"text\": \"Vertex AI's Generative AI Studio offers a Google Cloud console tool for rapidly prototyping and testing generative AI models. Learn how you can use Generative AI Studio to test models using prompt samples, design and save prompts, tune a foundation model, and convert between speech and text.\"}{\"_id\": \"doc2\", \"title\": \"Use gen AI for summarization, classification, and extraction\", \"text\": \"Learn how to create text prompts for handling any number of tasks with Vertex AI's generative AI support. Some of the most common tasks are classification, summarization, and extraction. Vertex AI's PaLM API for text lets you design prompts with flexibility in terms of their structure and format.\"}{\"_id\": \"doc3\", \"title\": \"Custom ML training overview and documentation\", \"text\": \"Get an overview of the custom training workflow in Vertex AI, the benefits of custom training, and the various training options that are available. This page also details every step involved in the ML training workflow from preparing data to predictions.\"}\n```\n- **Query file** : The query file contains your example queries. The path is defined by the parameter `queries_path` . The query file is in JSONL format and has the same fields as the corpus file. For example:```\n{\"_id\": \"query1\", \"title\": \"generative AI on Vertex\", \"text\": \"Does Vertex support generative AI?\"}{\"_id\": \"query2\", \"text\": \"What can I do with Vertex GenAI offerings?\"}{\"_id\": \"query3\", \"text\": \"How do I train my models using Vertex?\"}\n```\n- **Training labels** : The path is defined by the parameter `train_label_path` . The train_label_path is the Cloud Storage URI to the train label data location and is specified when you create your tuning job. The labels need to be a TSV file with a header. A subset of the queries and the corpus need be included in your training labels file. The file must have the columns `query-id` , `corpus-id` and `score` . The `query-id` is a string that matches the `_id` key from the query file, the `corpus-id` is a string that matches the `_id` in the corpus file. `Score` is a non-negative integer value. Any score greater than zero indicates that the document is related to the query. Larger numbers indicate a greater level of relevance. If the score is omitted, the default value is 1. For example:```\nquery-id \u00a0corpus-id \u00a0 scorequery1 \u00a0 \u00a0doc1 \u00a0 \u00a01query2 \u00a0 \u00a0doc2 \u00a0 \u00a01query3 \u00a0 \u00a0doc3 \u00a0 \u00a02\n``` **Note:** Ensure the separators in this file are tab characters ( `\\t` ). Some browsers and text editors may replace tabs with a sequence of spaces.\n- **Test labels** : The test labels file path is specified by the `test_label_path` parameter. `test_label_path` is the Cloud Storage URI to the test label data location and is specified when you create your tuning job. This file has the same format as the training labels.\n### Create a embedding model tuning job\nThe following shows you how to create an embedding model tuning job using REST API commands. You can't tune an embedding model using the Google Cloud console at this time.\nBefore using any of the request data, make the following replacements:- ``: A display name for the pipelineJob.\n- ``: Path for the pipeline output artifacts.\n- ``: your Google Cloud project ID.\n- ``: Optional. Use this to specify what text embedding model to tune, otherwise it defaults to `textembedding-gecko@001`.\n- ``: Optional. Setting this parameter optimizes the tuned model for a specific downstream task.\n- ``: The URI of the Cloud Storage bucket to store queries\n- ``: The Cloud Storage URI for the corpus data. .\n- ``: The Cloud Storage URI to the train label data location.\n- ``: The Cloud Storage URI to the test label data location.\n- ``: The training batch size.\n- ``: The number of steps to perform model tuning.\nHTTP method and URL:\n```\nPOST https://us-central1-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/us-central1/pipelineJobs\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"DISPLAY_NAME\",\n \"runtimeConfig\": {\n \"gcsOutputDirectory\": \"PIPELINE_SCRATCH_PATH\",\n \"parameterValues\": {\n  \"project\": \"PROJECT_ID\",\n  \"base_model_version_id\": \"BASE_MODEL_VERSION_ID\",\n  \"task_type\": \"TASK_TYPE\"\n  \"location\": \"us-central1\",\n  \"queries_path\": \"QUERIES_PATH\",\n  \"corpus_path\": \"CORPUS_PATH\",\n  \"train_label_path\": \"TRAIN_LABEL_PATH\",\n  \"test_label_path\": \"TEST_LABEL_PATH\",\n  \"batch_size\": \"BATCH_SIZE\",\n  \"iterations\": \"ITERATIONS\"\n }\n },\n \"templateUri\": \"https://us-kfp.pkg.dev/ml-pipeline/llm-text-embedding/tune-text-embedding-model/v1.1.1\"\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:After launching the pipeline, follow the progress of your tuning job through the **Google Cloud console** .\n [Go to Google Cloud console](https://console.cloud.google.com/vertex-ai/models) Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n```\npackage aiplatform;// [START aiplatform_sdk_embedding_tuning]import com.google.cloud.aiplatform.v1beta1.CreatePipelineJobRequest;import com.google.cloud.aiplatform.v1beta1.LocationName;import com.google.cloud.aiplatform.v1beta1.PipelineJob;import com.google.cloud.aiplatform.v1beta1.PipelineJob.RuntimeConfig;import com.google.cloud.aiplatform.v1beta1.PipelineServiceClient;import com.google.cloud.aiplatform.v1beta1.PipelineServiceSettings;import com.google.protobuf.Value;import java.io.IOException;import java.util.HashMap;import java.util.Map;public class CreatePipelineJobEmbeddingModelTuningSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"PROJECT\";\u00a0 \u00a0 String baseModelVersionId = \"BASE_MODEL_VERSION_ID\";\u00a0 \u00a0 String taskType = \"TASK_TYPE\";\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 String pipelineJobDisplayName = \"PIPELINE_JOB_DISPLAY_NAME\";\u00a0 \u00a0 String modelDisplayName = \"MODEL_DISPLAY_NAME\";\u00a0 \u00a0 String outputDir = \"OUTPUT_DIR\";\u00a0 \u00a0 String queriesPath = \"DATASET_URI\";\u00a0 \u00a0 String corpusPath = \"DATASET_URI\";\u00a0 \u00a0 String trainLabelPath = \"DATASET_URI\";\u00a0 \u00a0 String testLabelPath = \"DATASET_URI\";\u00a0 \u00a0 int batchSize = 50;\u00a0 \u00a0 int iterations = 300;\u00a0 \u00a0 createPipelineJobEmbeddingModelTuningSample(\u00a0 \u00a0 \u00a0 \u00a0 project,\u00a0 \u00a0 \u00a0 \u00a0 baseModelVersionId,\u00a0 \u00a0 \u00a0 \u00a0 taskType,\u00a0 \u00a0 \u00a0 \u00a0 location,\u00a0 \u00a0 \u00a0 \u00a0 pipelineJobDisplayName,\u00a0 \u00a0 \u00a0 \u00a0 modelDisplayName,\u00a0 \u00a0 \u00a0 \u00a0 outputDir,\u00a0 \u00a0 \u00a0 \u00a0 queriesPath,\u00a0 \u00a0 \u00a0 \u00a0 corpusPath,\u00a0 \u00a0 \u00a0 \u00a0 trainLabelPath,\u00a0 \u00a0 \u00a0 \u00a0 testLabelPath,\u00a0 \u00a0 \u00a0 \u00a0 batchSize,\u00a0 \u00a0 \u00a0 \u00a0 iterations);\u00a0 }\u00a0 // Create a model tuning job\u00a0 public static void createPipelineJobEmbeddingModelTuningSample(\u00a0 \u00a0 \u00a0 String project,\u00a0 \u00a0 \u00a0 String baseModelVersionId,\u00a0 \u00a0 \u00a0 String taskType,\u00a0 \u00a0 \u00a0 String location,\u00a0 \u00a0 \u00a0 String pipelineJobDisplayName,\u00a0 \u00a0 \u00a0 String modelDisplayName,\u00a0 \u00a0 \u00a0 String outputDir,\u00a0 \u00a0 \u00a0 String queriesPath,\u00a0 \u00a0 \u00a0 String corpusPath,\u00a0 \u00a0 \u00a0 String trainLabelPath,\u00a0 \u00a0 \u00a0 String testLabelPath,\u00a0 \u00a0 \u00a0 int batchSize,\u00a0 \u00a0 \u00a0 int iterations)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 final String endpoint = String.format(\"%s-aiplatform.googleapis.com:443\", location);\u00a0 \u00a0 PipelineServiceSettings pipelineServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PipelineServiceSettings.newBuilder().setEndpoint(endpoint).build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests.\u00a0 \u00a0 try (PipelineServiceClient client = PipelineServiceClient.create(pipelineServiceSettings)) {\u00a0 \u00a0 \u00a0 Map<String, Value> parameterValues = new HashMap<>();\u00a0 \u00a0 \u00a0 parameterValues.put(\"project\", stringToValue(project));\u00a0 \u00a0 \u00a0 parameterValues.put(\"base_model_version_id\", stringToValue(baseModelVersionId));\u00a0 \u00a0 \u00a0 parameterValues.put(\"task_type\", stringToValue(taskType));\u00a0 \u00a0 \u00a0 parameterValues.put(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"location\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 stringToValue(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"us-central1\")); // Deployment is only supported in us-central1.\u00a0 \u00a0 \u00a0 parameterValues.put(\"queries_path\", stringToValue(queriesPath));\u00a0 \u00a0 \u00a0 parameterValues.put(\"corpus_path\", stringToValue(corpusPath));\u00a0 \u00a0 \u00a0 parameterValues.put(\"train_label_path\", stringToValue(trainLabelPath));\u00a0 \u00a0 \u00a0 parameterValues.put(\"test_label_path\", stringToValue(testLabelPath));\u00a0 \u00a0 \u00a0 parameterValues.put(\"batch_size\", numberToValue(batchSize));\u00a0 \u00a0 \u00a0 parameterValues.put(\"iterations\", numberToValue(iterations));\u00a0 \u00a0 \u00a0 RuntimeConfig runtimeConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 RuntimeConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setGcsOutputDirectory(outputDir)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .putAllParameterValues(parameterValues)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 PipelineJob pipelineJob =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PipelineJob.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTemplateUri(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"https://us-kfp.pkg.dev/ml-pipeline/llm-text-embedding/tune-text-embedding-model/v1.1.2\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(pipelineJobDisplayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setRuntimeConfig(runtimeConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 LocationName parent = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 CreatePipelineJobRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CreatePipelineJobRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setParent(parent.toString())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setPipelineJob(pipelineJob)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 PipelineJob response = client.createPipelineJob(request);\u00a0 \u00a0 \u00a0 System.out.format(\"response: %s\\n\", response);\u00a0 \u00a0 \u00a0 System.out.format(\"Name: %s\\n\", response.getName());\u00a0 \u00a0 }\u00a0 }\u00a0 static Value stringToValue(String str) {\u00a0 \u00a0 return Value.newBuilder().setStringValue(str).build();\u00a0 }\u00a0 static Value numberToValue(int n) {\u00a0 \u00a0 return Value.newBuilder().setNumberValue(n).build();\u00a0 }}// [END aiplatform_sdk_embedding_tuning]\n```To tune a text embedding model by using the Google Cloud console, you can launch a customization pipeline using the following steps:- In the Vertex AI section of the Google Cloud console, go to the **Vertex AI Pipelines** page. [Go to Vertex AI Pipelines](https://console.cloud.google.com/vertex-ai//vertex-ai/pipelines/runs) \n- Click ** Create run** to open the **Create pipeline run** pane.\n- Click **Select from existing pipelines** and enter the following details:- Select \"ml-pipeline\" from the **select a resource** drop-down.\n- Select \"llm-embedding-text\" from the **Repository** drop-down.\n- Select \"tune-text-embedding-model\" from the **Pipeline or component** drop-down.\n- Select the version labeled \"v1.1.2\" from the **Version** drop-down.- Specify a **Run name** to uniquely identify the pipeline run.\n- In the **Region** drop-down list, select the region to create the pipeline run. Currently, only us-central1 is supported.\n- Click **Continue** . The **Runtime configuration** pane appears.\n- Under **Cloud storage location** , click **Browse** to select the Cloud Storage bucket for storing the pipeline output artifacts, and then click **Select** .\n- under **Pipeline parameters** , specify your parameters for the tuning pipeline. Please refer to the REST documentation for the meaning of the parameters.\n- Click **Submit** to create your pipeline run.\n```\nPROJECT_ID=PROJECT_IDBASE_MODEL_VERSION_ID=BASE_MODEL_VERSION_IDTASK_TYPE=TASK_TYPEPIPELINE_SCRATCH_PATH=PIPELINE_SCRATCH_PATHQUERIES_PATH=QUERIES_PATHCORPUS_PATH=CORPUS_PATHTRAIN_LABEL_PATH=TRAIN_LABEL_PATHTEST_LABEL_PATH=TEST_LABEL_PATHBATCH_SIZE=BATCH_SIZEITERATIONS=ITERATIONScurl -X POST \u00a0\\\u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 -H \"Content-Type: application/json; charset=utf-8\" \\\"https://us-central1-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/us-central1/pipelineJobs?pipelineJobId=tune-text-embedding-$(date +%Y%m%d%H%M%S)\" \\-d '{\u00a0 \"displayName\": \"tune-text-embedding-model\",\u00a0 \"runtimeConfig\": {\u00a0 \u00a0 \"gcsOutputDirectory\": \"'${PIPELINE_SCRATCH_PATH}'\",\u00a0 \u00a0 \"parameterValues\": {\u00a0 \u00a0 \u00a0 \"project\": \u00a0\"'${PROJECT_ID}'\",\u00a0 \u00a0 \u00a0 \"base_model_version_id\": \u00a0\"'${BASE_MODEL_VERSION_ID}'\",\u00a0 \u00a0 \u00a0 \"task_type\": \"'${TASK_TYPE}'\",\u00a0 \u00a0 \u00a0 \"location\": \"us-central1\",\u00a0 \u00a0 \u00a0 \"queries_path\": \u00a0\"'${QUERIES_PATH}'\",\u00a0 \u00a0 \u00a0 \"corpus_path\": \u00a0\"'${CORPUS_PATH}'\",\u00a0 \u00a0 \u00a0 \"train_label_path\": \u00a0\"'${TRAIN_LABEL_PATH}'\",\u00a0 \u00a0 \u00a0 \"test_label_path\": \u00a0\"'${TEST_LABEL_PATH}'\",\u00a0 \u00a0 \u00a0 \"batch_size\": \u00a0\"'${BATCH_SIZE}'\",\u00a0 \u00a0 \u00a0 \"iterations\": \u00a0\"'${ITERATIONS}'\"\u00a0 \u00a0 }\u00a0 },\u00a0 \"templateUri\": \"https://us-kfp.pkg.dev/ml-pipeline/llm-text-embedding/tune-text-embedding-model/v1.1.1\"}'\n```\n## View tuned models in Model Registry\nYou can view a list of models in your current project, including your tuned models, by using the Google Cloud console.\nTo view your tuned models in the Google Cloud console, go to the **Vertex AI Model Registry** page.\n[Go to Vertex AI Model Registry](https://console.cloud.google.com/vertex-ai/models)\n## Deploy your model\nWhen your tuning job completes, the tuned model isn't deployed to an endpoint. After you've tuned the embeddings model, you need to deploy your model. To deploy your tuned embeddings model, see [Deploy a model to an endpoint](/vertex-ai/docs/general/deployment) .\nUnlike foundation models, tuned text embedding models are managed by the user. This includes managing serving resources, like machine type and accelerators. To prevent out-of-memory errors during prediction, it's recommended that you deploy using the `NVIDIA_TESLA_A100` GPU type, which can support batch sizes up to 5 for any input length.\nSimilar to the `textembedding-gecko` foundation model, your tuned model supports up to 3072 tokens and can truncate longer inputs.\n## Get predictions on a deployed model\n### Example curl commands for tuned textembedding-gecko@001 models\nTo get predictions from a tuned version of `textembedding-gecko@001` , use the example curl command below.\n```\nENDPOINT_URI=https://us-central1-aiplatform.googleapis.comPROJECT_ID=<your-project>LOCATION=us-central1MODEL_ENDPOINT=<created-vertex-endpoint-id>curl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 \u00a0 -H \"Content-Type: application/json\" \u00a0\\\u00a0 \u00a0 ${ENDPOINT_URI}/v1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/${MODEL_ENDPOINT}:predict \\\u00a0 \u00a0 -d '{\u00a0 \"instances\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"content\": \"Dining in New York City\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"content\": \"Best resorts on the east coast\"\u00a0 \u00a0 }\u00a0 ]}'\n```\n### Example curl commands for non textembedding-gecko@001 models\nTuned versions of `textembedding-gecko@002` and `textembedding-gecko-multilingual@001` require 2 additional inputs: `task_type` and `title` . More documentation for these parameters can be found at [curl command](/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings#api_changes_to_models_released_in_or_after_august_2023)\n**Note:** `task_type` and `title` are required for every request. For default behavior, set `task_type=\"DEFAULT\"` and `title=\"\"` .\n```\nENDPOINT_URI=https://us-central1-aiplatform.googleapis.comPROJECT_ID=<your-project>LOCATION=us-central1MODEL_ENDPOINT=<created-vertex-endpoint-id>curl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 \u00a0 -H \"Content-Type: application/json\" \u00a0\\\u00a0 \u00a0 ${ENDPOINT_URI}/v1/projects/${PROJECT_ID}/locations/${LOCATION}/endpoints/${MODEL_ENDPOINT}:predict \\\u00a0 \u00a0 -d '{\u00a0 \"instances\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"content\": \"Dining in New York City\",\u00a0 \u00a0 \u00a0 \"task_type\": \"DEFAULT\",\u00a0 \u00a0 \u00a0 \"title\": \"\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"content\": \"There are many resorts to choose from on the East coast...\",\u00a0 \u00a0 \u00a0 \"task_type\": \"RETRIEVAL_DOCUMENT\",\u00a0 \u00a0 \u00a0 \"title\": \"East Coast Resorts\"\u00a0 \u00a0 }\u00a0 ]}'\n```\n### Example output\nThis output applies to both `textembedding-gecko` and `textembedding-gecko-multilingual` models, regardless of version.\n**Note:** The output of a prediction request to the deployed tuned `textembedding-gecko` model is not the same as the text embedding API output.\n```\n{\u00a0\"predictions\": [\u00a0 \u00a0[ ... ],\u00a0 \u00a0[ ... ],\u00a0 \u00a0...\u00a0],\u00a0\"deployedModelId\": \"...\",\u00a0\"model\": \"projects/.../locations/us-central1/models/...\",\u00a0\"modelDisplayName\": \"tuned-text-embedding-model\",\u00a0\"modelVersionId\": \"1\"}\n```\n## What's next\n- Learn how to [tune a foundation model](/vertex-ai/generative-ai/docs/models/tune-models) .\n- Learn about [responsible AI best practices and Vertex AI's safety filters](/vertex-ai/generative-ai/docs/learn/responsible-ai) .", "guide": "Generative AI on Vertex AI"}