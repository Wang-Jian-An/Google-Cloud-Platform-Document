{"title": "Generative AI on Vertex AI - Tune text models by using RLHF tuning", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf", "abstract": "# Generative AI on Vertex AI - Tune text models by using RLHF tuning\n**Preview** Reinforcement Learning from Human Feedback (RLHF) is a Preview offering, subject to the Pre-GA Offerings Terms of the [GCP Service Specific Terms](https://cloud.google.com/terms/service-terms) . Pre-GA products and features may have limited support, and changes to pre-GA products and features may not be compatible with other pre-GA versions. For more information, see the [launch stage descriptions](https://cloud.google.com/products#product-launch-stages) . Further, by using these features, you agree to the Generative AI Preview [ terms and conditions ](https://cloud.google.com/trustedtester/aitos) (Preview Terms).For these features, you can process personal data as outlined in the Cloud Data Processing Addendum, subject to applicable restrictions and obligations in the Agreement (as defined in the Preview Terms).\nReinforcement learning from human feedback (RLHF) uses feedback gathered from humans to tune a model. RLHF is recommended when the output of your model is complex and difficult to describe. The human feedback is in the form of choices between different output options. These choices provide better data than labeled prompts, used by supervised tuning, to tune a model that produces output that's difficult to describe. If the output from your model isn't difficult to define, consider tuning your text model by using [Supervised tuning](/vertex-ai/generative-ai/docs/models/tune-text-models-supervised) .\nThis page provides detailed information about tuning a text model using RLHF tuning. You learn about which text models support RLHF tuning, how to create a dataset, and how to tune a text model using RLHF tuning. You also learn how to view and load tuned models tuned using RLHF tuning. For more details about RLHF tuning in Vertex AI, see [RLHF modeltuning](/vertex-ai/generative-ai/docs/models/tune-models#rlhf-tuning) .\n", "content": "## Supported models\nThe following text models support RLHF tuning:\n- The text generation foundation model,`text-bison@001`. For more information, see [Text generation model](/vertex-ai/generative-ai/docs/model-reference/text) .\n- The chat generation foundation model,`chat-bison@001`. For more information, see [Chat generation model](/vertex-ai/generative-ai/docs/model-reference/text-chat) .\n- The`t5-small`,`t5-large`,`t5-xl`, and`t5-xxl`Flan text-to-text transfer transformer (Flan-T5) models. Flan-T5 models can be fine-tuned to perform tasks such as text classification, language translation, and question answering. For more information, see [Flan-T5 checkpoints](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) .## Prepare RLHF tuning datasets\nRLHF tuning requires that you prepare two datasets and one optional dataset. All datasets are in [JSON Lines](https://jsonlines.org/) (JSONL) format and need to be uploaded to a Cloud Storage bucket. The dataset format used to tune a text generation model is different from the dataset format for tuning a text chat model.\n### Prompt dataset\nA dataset that contains unlabeled prompts. Prompts can be the same prompts from the preference dataset, or they can be different. Each line in the prompt dataset contains the following fields:\nThe text generation dataset includes one field:- `input_text`- a required field that contains the prompt.\n```\n{\u00a0 \"input_text\": \"Create a description for Plantation Palms.\"}\n```\nThe chat generation dataset includes two fields:- `messages` - an array of author-content pairs. The `author` field refers to the author of the message and alternates between `user` and `assistant` . The `content` field is the content of the message. The `content` can't be empty, and the first and last `author` must be set to `user` .\n- `context` - (optional) additional [context](/vertex-ai/generative-ai/docs/chat/chat-prompts#context) for the model to use when it responds to a prompt.\n```\n{\u00a0 \"context\": \"You are a pirate dog named Captain Barktholomew.\",\u00a0 \"messages\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"author\": \"user\",\u00a0 \u00a0 \u00a0 \"content\": \"Hi\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"author\": \"assistant\",\u00a0 \u00a0 \u00a0 \"content\": \"Argh! What brings ye to my ship?\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"author\": \"user\",\u00a0 \u00a0 \u00a0 \"content\": \"What's your name?\"\u00a0 \u00a0 },\u00a0 ]}\n```\nTo learn more, you can download and view this sample [prompt dataset](https://storage.googleapis.com/vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/train/shard-00000-of-00002.jsonl) .\n### Human preference dataset\nThe human preference dataset contains preferences from humans. Each line in the human preference dataset records the preference between two options that were presented to a human. We recommend that the human preference dataset includes 5,000 to 10,000 examples. Each line in the human preference dataset contains one example preference that includes the [prompt dataset fields](#prompt-dataset) for the model being tuned plus the following fields:\n- `candidate_0`and`candidate_1`- each of these fields contains two responses. The human helps tune the model by choosing which of the two responses they prefer.\n- `choice`- contains an integer,`0`or`1`, that indicates which candidate the human preferred. A`0`indicates the human chose`candidate_0`, and a`1`indicates the human chose`candidate_1`.\nAn example of a row in the human preference dataset is the following:\n`{\"input_text\": \"Create a description for Plantation Palms.\", \"candidate_0\": \"Enjoy some fun in the sun at Gulf Shores.\", \"candidate_1\": \"A Tranquil Oasis of Natural Beauty.\", \"choice\": 0}`\nTo learn more, you can download and view this sample [human preference dataset](https://storage.googleapis.com/vertex-ai/generative-ai/rlhf/text_small/summarize_from_feedback_tfds/comparisons/train/shard-00000-of-00002.jsonl) .\n### Evaluation dataset (optional)\nA dataset that includes unlabeled prompts for prediction after the model is tuned. If the evaluation dataset is provided, then inference is performed on it after the tuning job completes. The format of the evaluation dataset is the same as the format of the prompt dataset. However, the prompts in an evaluation dataset need to be different from the prompts in the prompt dataset.\nTo learn more, you can download and view this sample [evaluation dataset](https://storage.googleapis.com/vertex-ai/generative-ai/rlhf/text_small/reddit_tfds/val/shard-00000-of-00001.jsonl) .\n### Reward model\nThe [human preference dataset](#human-preference-dataset) is used to train a reward model. Vertex AI creates and then uses the reward model during RLHF tuning. Reward models are created in a private Cloud Storage bucket in a customer tenant project. A customer tenant project is an internal project that's unique to a customer. You can't access a reward model, and it's deleted after the tuning job completes. For more information, see [Tenant project](/service-infrastructure/docs/glossary#tenant) .\n### Maintain consistency with production data\nThe examples in your datasets should match your expected production traffic. If your dataset contains specific formatting, keywords, instructions, or information, the production data should be formatted in the same way and contain the same instructions.\nFor example, if the examples in your dataset include a `\"question:\"` and a `\"context:\"` , production traffic should also be formatted to include a `\"question:\"` and a `\"context:\"` in the same order as it appears in the dataset examples. If you exclude the context, the model will not recognize the pattern, even if the exact question was in an example in the dataset.\n### Upload tuning datasets to Cloud Storage\nTo run a tuning job, you need to upload one or more datasets to a Cloud Storage bucket. You can either [create a new Cloud Storage bucket](/storage/docs/creating-buckets#create_a_new_bucket) or use an existing one to store dataset files. The region of the bucket doesn't matter, but we recommend that you use a bucket that's in the same Google Cloud project where you plan to tune your model.\nAfter your bucket is ready, [upload](/storage/docs/creating-buckets#create_a_new_bucket) your dataset file to the bucket.\n## Create an RLHF tuning job\nYou can perform RLHF tuning by using the Google Cloud console or the Vertex AI SDK for Python.\n**Note:** RLHF tuning sets the accelerator type and count based on the selected region. Jobs in `us-central1` use eight Nvidia A100 80GB. Jobs in `europe-west4` use 32 TPU v3s.\nTo learn how to use the Vertex AI SDK for Python to tune your models with RLHF, open and run the following notebook with Colab, GitHub, or Vertex AI Workbench:- [Open with Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/rlhf_tune_llm.ipynb) \n- [Open with GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/rlhf_tune_llm.ipynb) \n- [Open with Vertex AI Workbench](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/generative_ai/rlhf_tune_llm.ipynb) To tune a text model in the Google Cloud console by using RLHF tuning, perform the following steps:- In the Vertex AI section of the Google Cloud console, go to  the **Vertex AI Studio** page. [Go to  Vertex AI Studio](https://console.cloud.google.com/vertex-ai/generative/language) \n- Click the **Tune and distill** tab.\n- Clickadd **Create tuned model** .\n- Select **Reinforcement learning from human feedback (RLHF)** .\n- Configure model details:- **Tune model name** : Enter a name for your tuned model.\n- **Base model** : Select the foundation model that you want to tune.\n- **Region** : Enter the region where model tuning takes place. Supported regions are:- `us-central1`: Uses 8 Nvidia A100 80GB GPUs.\n- `europe-west4`: Uses 64 cores of the TPU v3 pod.\n- **Output directory** : Enter the Cloud Storage location where artifacts are stored when your model is tuned.\n- Expand **Advanced Options** to configure advanced settings.- **Reward train steps** : Enter the number of steps to use when training the [reward model](/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf#reward-model) . The reward model is used to tune your model. The default value is 1000.\n- **Reward learning rate multiplier** : Enter a float value that affects the learning rate when training the [reward model](/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf#reward-model) . To increase the default learning rate, enter a higher value. To decrease the default learning rate, enter a lower value. The default value is 1.0.\n- **Reinforcement train steps** : Enter the number of steps to perform when tuning the base model using reinforcement learning. The default value is 1000.\n- **Reinforcement learning rate multiplier** : Enter a float value that affects the learning rate when training a reinforcement model. To increase the default learning rate, enter a higher value. To decrease the default learning rate, enter a lower value. The default value is 1.0.\n- Click **Continue** \n- In **Human preference dataset** , upload  or choose a human preference dataset used to create a [reward model](/vertex-ai/generative-ai/docs/models/tune-text-models-rlhf#reward-model) .  If you want to upload your dataset file, selectradio_button_checked **Upload JSONL file to\n  Cloud Storage** . If your dataset file is already in a  Cloud Storage bucket, selectradio_button_checked **Existing\n  JSONL file on Cloud Storage** .- In **Select JSONL file** , click **Browse** and    select your dataset file.\n- In **Dataset location** , click **Browse** and select the Cloud Storage bucket where you want to store your    dataset file.\nIn **Cloud Storage file path** , click **Browse** and select the Cloud Storage bucket where your dataset file is    located.\n- In **Prompt dataset** , if you want to upload your dataset file, selectradio_button_checked **Upload JSONL file to Cloud Storage** . Otherwise, if your prompt dataset file is already in a Cloud Storage bucket, selectradio_button_checked **Existing JSONL file on Cloud Storage** .- In **Select JSONL file** , click **Browse** and    select your dataset file.\n- In **Dataset location** , click **Browse** and select the Cloud Storage bucket where you want to store your    dataset file.\nIn **Cloud Storage file path** , click **Browse** and select the Cloud Storage bucket where your dataset file is    located.\n- (Optional) To evaluate your tuned model, do the following:- Click **Enable model evaluation** .\n- In **Evaluation dataset** , click **Browse** .\n- Navigate to the Cloud Storage bucket that contains your evaluation dataset and select your evaluation dataset.\nFor more information, see [Evaluation dataset](#evaluation-dataset) .\n- Click **Start tuning** .## Check tuning operation status\nTo check the status of your model tuning job, in the Google Cloud console, go to the **Vertex AI Pipelines** page. This page shows the status of text and code model tuning jobs.\n[Go to Pipelines](https://console.cloud.google.com/vertex-ai/pipelines)\nAlternatively, you can [configure email notifications](/vertex-ai/docs/pipelines/email-notifications) for Vertex AI Pipelines so you are notified by email when the model tuning job finishes or fails.\n## What's next\n- Learn about [responsible AI best practices and Vertex AI's safety filters](/vertex-ai/generative-ai/docs/learn/responsible-ai) .\n- Learn how to [enable Data Access audit logs](/vertex-ai/generative-ai/docs/enable-audit-logs) for your endpoints.\n- Learn how to [evaluate your tuned model](/vertex-ai/generative-ai/docs/models/evaluate-models) .", "guide": "Generative AI on Vertex AI"}