{"title": "Generative AI on Vertex AI - Use Gemma open models", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma", "abstract": "# Generative AI on Vertex AI - Use Gemma open models\nGemma is a set of lightweight, generative artificial intelligence (AI) open models. Gemma models are available to run in your applications and on your hardware, mobile devices, or hosted services. You can also customize these models using tuning techniques so that they excel at performing tasks that matter to you and your users. Gemma models are based on [Gemini](/vertex-ai/generative-ai/docs/multimodal/overview) models and intended for the AI development community to extend and take further.\nYou can use Gemma models for text generation. You can also tune them to improve their performance in specific tasks. A tuned Gemma model can help make your generative AI solutions more targeted and efficient.\nTo test and learn more about the Gemma model, see the [Gemma Model Garden model card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/335) .\nThe following are some options for where you can use Gemma:\n", "content": "## Use Gemma with Vertex AI\nVertex AI offers a managed platform for rapidly building and scaling machine learning projects without needing in-house MLOps expertise. You can use Vertex AI as the downstream application that serves the Gemma model. For example, you might port weights from the Keras implementation of Gemma. Next, you can use Vertex AI to serve that version of Gemma to get predictions. We recommend using Vertex AI if you want end-to-end MLOps capabilities, value-added ML features, and a serverless experience for streamlined development.\nTo get started with Gemma, see the following notebooks:\n- [Serve Gemma in Vertex AI](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_deployment_on_vertex.ipynb) \n- [Fine-tune Gemma using PEFT and then deploy to Vertex AI from Vertex](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_finetuning_on_vertex.ipynb) \n- [Fine-tune Gemma using PEFT and then deploy to Vertex AI from Huggingface](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_gemma_peft_finetuning_hf.ipynb) \n- [Fine-tune Gemma using KerasNLP and then deploy to Vertex AI](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma_kerasnlp_to_vertexai.ipynb) ## Use Gemma in other Google Cloud products\nYou can use Gemma with other Google Cloud products, such as Google Kubernetes Engine and Dataflow.\n### Use Gemma with GKE\nGoogle Kubernetes Engine (GKE) is the Google Cloud solution for managed Kubernetes that provides scalability, security, resilience, and cost effectiveness. We recommend this option if you have existing Kubernetes investments, your organization has in-house MLOps expertise, or if you need granular control over complex AI/ML workloads with unique security, data pipeline, and resource management requirements. To learn more, see the following tutorials in the GKE documentation:\n- [Serve Gemma with Saxml](/kubernetes-engine/docs/tutorials/serve-gemma-tpu-saxml) \n- [Serve Gemma with vLLM](/kubernetes-engine/docs/tutorials/serve-gemma-gpu-vllm) \n- [Serve Gemma with TGI](/kubernetes-engine/docs/tutorials/serve-gemma-gpu-tgi) \n- [Serve Gemma with Triton and TensorRT-LLM](/kubernetes-engine/docs/tutorials/serve-gemma-gpu-tensortllm) \n### Use Gemma with Dataflow\nYou can use Gemma models with Dataflow for [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) . Use Dataflow to run inference pipelines that use the Gemma models. To learn more, see [Run inference pipelines with Gemma open models](/dataflow/docs/machine-learning/gemma) .\n### Use Gemma with Colab\nYou can use Gemma with Colaboratory to create your Gemma solution. In Colab, you can use Gemma with framework options such as PyTorch and JAX. To learn more, see:\n- [Get started with Gemma using Keras](https://ai.google.dev/gemma/docs/get_started) .\n- [Get started with Gemma using PyTorch](https://ai.google.dev/gemma/docs/pytorch_gemma) .\n- [Basic tuning with Gemma using Keras](https://ai.google.dev/gemma/docs/lora_tuning) .\n- [Distributed tuning with Gemma using Keras](https://ai.google.dev/gemma/docs/distributed_tuning) .## Gemma model sizes and capabilities\nGemma models are available in several sizes so you can build generative AI solutions based on your available computing resources, the capabilities you need, and where you want to run them. Each model is available in a tuned and an untuned version:\n- **Pretrained** - This version of the model wasn't trained on any specific tasks or instructions beyond the Gemma core data training set. We don't recommend using this model without performing some tuning.\n- **Instruction tuned** - This version of the model was trained with human language interactions so that it can participate in a conversation, similar to a simple chat bot.\nIf you're not sure which one to try, consider the Gemma 2B. Its lower parameter sizes mean it has lower resource requirements and more deployment flexibility than Gemma 7B.\n| Model name | Parameters size | Input | Output | Tuned versions    | Intended platforms     |\n|:-------------|:------------------|:--------|:---------|:-----------------------------|:------------------------------------|\n| Gemma 2B  | 2.2 billion  | Text | Text  | Pretrained Instruction tuned | Mobile devices and laptops   |\n| Gemma 7B  | 7 billion   | Text | Text  | Pretrained Instruction tuned | Desktop computers and small servers |\nGemma has been tested using Google's purpose built v5e TPU hardware and NVIDIA's L4(G2 standard), A100(A2 standard), H100(A3 standard) GPU hardware.", "guide": "Generative AI on Vertex AI"}