{"title": "Generative AI on Vertex AI - Migrate from PaLM API to Gemini API on Vertex AI", "url": "https://cloud.google.com/vertex-ai/generative-ai/docs/migrate/migrate-palm-to-gemini", "abstract": "# Generative AI on Vertex AI - Migrate from PaLM API to Gemini API on Vertex AI\nThis guide shows how to migrate your Vertex AI SDK for Python code from using the PaLM API to the Gemini API. You can generate both text and multi-turn conversations (chat) with Gemini, but make sure to check your responses since they may be different from PaLM outputs.\n", "content": "## Migrate your code\n- Replace`TextGenerationModel`and`ChatModel`class usage with the`GenerativeModel`class by changing the import to call`GenerativeModel`from`vertexai.preview.generative_models`.\n- Instead of using`from_pretrained`classmethod, use the constructor to instantiate`GenerativeModel`. Replace`TextGenerationModel.from_pretrained('text-bison')`with`GenerativeModel('gemini-1.0-pro')`.\n- Replace`TextGenerationModel.predict`with`GenerativeModel.generate_content`.\n- For the methods, call the same methods from`GenerativeModel`. For example, replace`ChatModel.start_chat`with`GenerativeModel.start_chat`.\n- Update the syntax as shown in the examples on this page.- Output response structure has changed. Refer to the [Gemini API model reference response body](/vertex-ai/generative-ai/docs/model-reference/gemini#response_body) for details.\n- Safety setting categories have changed. Refer to the [Gemini API model reference request body](/vertex-ai/generative-ai/docs/model-reference/gemini#request_body) for details.\n## Common setup instructions\nFor both PaLM API and Gemini API in Vertex AI, the setup process is the same:\n```\npip install google-cloud-aiplatformimport vertexaivertexai.init(project=\"PROJECT_ID\", location=\"LOCATION\")\n```\nWhere:\n- is your Google Cloud project ID.\n- is the location of your Google Cloud project. For example, `us-central1` .## Text Generation: Basic\nThe following code samples show the differences between the PaLM API and Gemini API for creating a text generation model.\n| PaLM                                                   | Gemini                                                        |\n|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| from vertexai.language_models import TextGenerationModel model = TextGenerationModel.from_pretrained(\"text-bison@002\") response = model.predict(prompt=\"The opposite of hot is\") print(response.text) # 'cold.' | from vertexai.preview.generative_models import GenerativeModel model = GenerativeModel(\"gemini-1.0-pro\") responses = model.generate_content(\"The opposite of hot is\", stream=True) for response in responses: print(response.text) |\n## Text Generation: Optional parameters\nThe following code samples show the differences between the PaLM API and Gemini API for creating a text generation model, with optional [parameters](/vertex-ai/generative-ai/docs/start/quickstarts/api-quickstart#parameter_definitions) .\n| PaLM                                                                                                                                              | Gemini                                                                                                                                                               |\n|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| from vertexai.language_models import TextGenerationModel model = TextGenerationModel.from_pretrained(\"text-bison@002\") prompt = \"\"\" You are an expert at solving word problems. Solve the following problem: I have three houses, each with three cats. each cat owns 4 mittens, and a hat. Each mitten was knit from 7m of yarn, each hat from 4m. How much yarn was needed to make all the items? Think about it step by step, and show your work. \"\"\" response = model.predict( prompt=prompt, temperature=0.1, max_output_tokens=800, top_p=1.0, top_k=40 ) print(response.text) | from vertexai.preview.generative_models import GenerativeModel model = GenerativeModel(\"gemini-1.0-pro\") prompt = \"\"\" You are an expert at solving word problems. Solve the following problem: I have three houses, each with three cats. each cat owns 4 mittens, and a hat. Each mitten was knit from 7m of yarn, each hat from 4m. How much yarn was needed to make all the items? Think about it step by step, and show your work. \"\"\" responses = model.generate_content( prompt, generation_config={ \"temperature\": 0.1, \"max_output_tokens\": 800, \"top_p\": 1.0, \"top_k\": 40, }, stream=True ) for response in responses: print(response.text) |\n## Chat: Basic\nThe following code samples show the differences between the PaLM API and Gemini API for creating a chat model.\n| PaLM                                                                                           | Gemini                                                                                                                             |\n|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| from vertexai.language_models import ChatModel model = ChatModel.from_pretrained(\"chat-bison@002\") chat = model.start_chat() print( chat.send_message( \"\"\" Hello! Can you write a 300 word abstract for a research paper I need to write about the impact of AI on society? \"\"\" ) ) print( chat.send_message( \"\"\" Could you give me a catchy title for the paper? \"\"\" ) ) | from vertexai.preview.generative_models import GenerativeModel model = GenerativeModel(\"gemini-1.0-pro\") chat = model.start_chat() responses = chat.send_message( \"\"\" Hello! Can you write a 300 word abstract for a research paper I need to write about the impact of AI on society? \"\"\", stream=True ) for response in responses: print(response.text) responses = chat.send_message( \"\"\" Could you give me a catchy title for the paper? \"\"\", stream=True ) for response in responses: print(response.text) |\n## Next steps\n- See the [Vertex AI Gemini API overview](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview) for more details on the latest models and features.", "guide": "Generative AI on Vertex AI"}