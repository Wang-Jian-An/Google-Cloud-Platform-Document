{"title": "Google Kubernetes Engine (GKE) - Node upgrade strategies", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview", "abstract": "# Google Kubernetes Engine (GKE) - Node upgrade strategies\nThis page discusses the node upgrade strategies you can use with your Google Kubernetes Engine (GKE) clusters.\nIn GKE Standard clusters, you can configure one of the following node upgrade strategies for each node pool:\n- **Surge upgrades:** Nodes are upgraded in a rolling window. You can control how many nodes can be upgraded at once and how disruptive upgrades are to the workloads.\n- **Blue-green upgrades:** Existing nodes are kept available for rolling back while the workloads are validated on the new node configuration.\nIn Autopilot clusters, GKE uses surge upgrades. To learn more, see the Autopilot cluster upgrades page's [Surge upgrades](/kubernetes-engine/docs/concepts/cluster-upgrades-autopilot#surge) section.\nBy choosing an upgrade strategy for your Standard cluster node pool, you can pick the process with the right balance of speed, workload disruption, risk mitigation, and cost optimization. To learn more about which node upgrade strategy is right for your environment, see [Choose surgeupgrades](#choose-surge-upgrades) and [Choose blue-greenupgrades](#choose-blue-green-upgrades) .\nWith both strategies, you can configure upgrade settings to optimize the process based on your environment's needs. To learn more, see [Configure your chosenupgrade strategy](/kubernetes-engine/docs/how-to/node-pool-upgrade-strategies) . Ensure that for the strategy that you pick, you have enough quota, resource availability, or reservation capacity to upgrade your nodes using that strategy. For more information, see [Ensure resources for nodeupgrades](/kubernetes-engine/docs/how-to/node-upgrades-quota) .\n**Note:** Node upgrade strategies are used for any type of node configuration change that requires the nodes to be recreated. To learn more, see [When surge upgradesare used](#when-surge-upgrades-are-used) and [When blue-greenupgrades are used](#when-blue-green-upgrades-are-used) .\n", "content": "## Surge upgrades\nSurge upgrades are the default upgrade strategy, and best for applications that can handle incremental changes. Surge upgrades use a rolling method to upgrade nodes, in an undefined order. Find the optimal balance of speed and disruption for your environment by choosing how many new, surge nodes can be created, with `maxSurge` , and how many existing nodes can be disrupted at once, with `maxUnavailable` .\nSurge upgrades also work with the cluster autoscaler to prevent changes to nodes that are being upgraded.\n### Choose surge upgrades for your environment\nIf cost optimization is important for you and your workload can tolerate being shut down in less than 60 minutes, we recommend choosing surge upgrades for your node pools.\n[Surge upgrades](#surge) are optimal for the following scenarios:\n- if you want to optimize for the speed of upgrades.\n- if workloads are more tolerant of disruptions, where graceful termination up to 60 minutes is acceptable.\n- if you want to control costs by minimizing the creation of new nodes.\n### When GKE uses surge upgrades\nIf enabled, GKE uses surge upgrades when the following types of changes occur:\n- [Version changes (upgrades)](/kubernetes-engine/docs/concepts/cluster-upgrades#node_pool_upgrades) \n- [Vertically scaling the nodes by changing the node machine attributes](/kubernetes-engine/docs/how-to/node-pools#change-machine-attributes) , including machine type, disk type, and disk size\n- [Image type changes](/sdk/gcloud/reference/container/clusters/upgrade#--image-type) \n- [IP rotation](/kubernetes-engine/docs/how-to/ip-rotation) \n- [Credential rotation](/kubernetes-engine/docs/how-to/credential-rotation) \n- [Network policy creation](/kubernetes-engine/docs/how-to/network-policy) \n- [Enabling image streaming](/kubernetes-engine/docs/how-to/image-streaming) \n- [Network performance configuration updates](/sdk/gcloud/reference/container/node-pools/update#--network-performance-configs) \n- [Enabling gVNIC](/sdk/gcloud/reference/container/node-pools/update#--enable-gvnic) \n- [System config changes](/sdk/gcloud/reference/container/node-pools/update#--system-config-from-file) \n- [Confidential nodes](/kubernetes-engine/docs/how-to/confidential-gke-nodes#on_an_existing_node_pool) \nOther changes, including [applying updates to node labels and taints of existingnode pools](/kubernetes-engine/docs/how-to/update-existing-nodepools) , don't use surge upgrades as they don't require recreating the nodes.\n### Understand surge upgrade settings\nUse surge upgrade settings to select the appropriate balance between speed and disruption for your node pool during cluster maintenance using the surge settings. You can change how many nodes GKE attempts to upgrade at once by [changing](/kubernetes-engine/docs/how-to/node-pool-upgrade-strategies#surge) the surge upgrade parameters on a Standard node pool.\nSurge upgrade behavior is determined by the `maxSurge` and `maxUnavailable` settings, which determine how many nodes are upgraded at the same time in a rolling window with the described steps.\nSet `maxSurge` to choose the maximum number of additional, surge nodes that can be added to the node pool during an upgrade, per zone, increasing the likelihood that workloads running on the existing node can migrate to a new node immediately. The default is one. To upgrade one node, GKE does the following steps:\n- Provision a new node.\n- Wait for the new node to be ready.\n- [Cordon](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#cordon) the existing node.\n- [Drain](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#drain) the existing node, respecting [PodDisruptionBudget](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) and [GracefulTerminationPeriod](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination) settings for up to one hour.\n- Delete the existing node.\nFor GKE to create surge nodes, your project must have the resources to temporarily create additional nodes. If you don't have additional capacity, GKE won't start upgrading a node until the resources are available. To learn more, see [Resources for surgeupgrades](/kubernetes-engine/docs/how-to/node-upgrades-quota#quota-surge-upgrades) .\nSet `maxUnavailable` to choose the maximum number of nodes that can be simultaneously unavailable during an upgrade, per zone. The default is zero. Workloads running on the existing node might need to wait for the existing node to upgrade, if no other nodes have capacity. To upgrade one node, GKE does the following steps:\n- [Cordon](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#cordon) the existing node.\n- [Drain](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#drain) the existing node, respecting [PodDisruptionBudget](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) and [GracefulTerminationPeriod](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination) settings for up to one hour.\n- Recreate the existing node with the new configuration.\n- Wait for the existing node to be ready.\n- Uncordon the existing, upgraded node.\nWhen GKE recreates the existing node, GKE temporarily releases the capacity of the node if the capacity isn't from a reservation. This means that if there is limited capacity, you risk losing the existing capacity. So, if your environment is resource-constrained, use this setting only if you're using reserved nodes. To learn more, see [Upgrade in aresource-constrainedenvironment](/kubernetes-engine/docs/how-to/node-upgrades-quota#upgrade-resource-constrained) .\nFor example, a GKE cluster has a single-zone node pool with 5 nodes and the following surge upgrade configuration: `maxSurge=2;maxUnavailable=1` .\nDuring a surge upgrade with this node pool, in a rolling window, GKE creates two upgraded nodes, and disrupts at most one existing node at a time. GKE brings down at most three existing nodes after the upgraded nodes are ready. During the upgrade process, the node pool will include between four and seven nodes.\nConsider the following information before configuring surge upgrade settings:\n- Nodes created by surge upgrade are subject to your Google Cloud [resource quotas](/compute/quotas) , [resourceavailability](/compute/resource-usage#quotas_and_resource_availability) , and [reservationcapacity](/kubernetes-engine/docs/how-to/consuming-reservations) , for node pools with [specific reservationaffinity](/kubernetes-engine/docs/how-to/consuming-reservations#specific) . If your environment is resource-constrained, see [Upgrade in aresource-constrainedenvironment](/kubernetes-engine/docs/how-to/node-upgrades-quota#upgrade-resource-constrained) .\n- The number of nodes that GKE upgrades simultaneously is the sum of`maxSurge`and`maxUnavailable`. The maximum number of nodes upgraded simultaneously is limited to 20. Surge upgrades also work with the [clusterautoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) to prevent changes to nodes that are being upgraded.\n- GKE upgrades [multi-zone nodepools](/kubernetes-engine/docs/concepts/node-pools#multiple-zones) one zone at a time. Surge upgrade parameters are applicable only up to the number of nodes in the zone. The maximum number of nodes that can be upgraded in parallel will be no higher than the sum of`maxSurge`plus`maxUnavailable`, and no higher than the number of nodes in the zone.\n- If your node pool uses Spot VMs, GKE creates surge nodes with Spot VMs, but doesn't wait for Spot VMs to be ready before cordoning and draining existing nodes. To learn more, see [Upgrade Standard node pools usingSpot VMs](/kubernetes-engine/docs/concepts/spot-vms#upgrade) .\n### Tune surge upgrade settings to balance speed and disruption\nThe following table describes four different upgrade profiles as examples to help you understand different configurations:\n| Description          | Configuration    | Typical use case          |\n|:------------------------------------------------|:-----------------------------|:------------------------------------------------------|\n| Balanced (Default), slower but least disruptive | maxSurge=1 maxUnavailable=0 | Most workloads          |\n| Fast, no surge resources, most disruptive  | maxSurge=0 maxUnavailable=20 | Large node pools after jobs have to run to completion |\n| Fast, most surge resources and less disruptive | maxSurge=20 maxUnavailable=0 | Large node pools          |\n| Slowest, disruptive, no surge resources   | maxSurge=0 maxUnavailable=1 | Resource-constrained node pool with reservation  |\nThe simplest way to take advantage of surge upgrades is to use the default configuration, `maxSurge=1;maxUnavailable=0.` With this configuration, upgrades progress slowly, with only one surge node added at a time, meaning only one node is upgraded at a time. Pods can restart immediately on the new, surge node. This configuration only requires the resources to temporarily create one new node.\nIf you have a large node pool and your workload isn't sensitive to disruption (for example, a batch job that has run to completion), use the following configuration to maximize speed without using any additional resources: `maxSurge=0;maxUnavailable=20` . This configuration does not bring up additional surge nodes and allows 20 nodes to be upgraded at the same time.\nIf your workload is sensitive to disruption and you have already set up [PodDisruptionBudgets](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) (PDB) and you are not using `externalTrafficPolicy: Local` , which does not work with parallel node drains, you can increase the speed of the upgrade by using `maxSurge=20;maxUnavailable=0` . This configuration upgrades 20 nodes in parallel while the PDB limits the number of Pods that can be drained at a given time. Although the configurations of PDBs may vary, if you create a PDB with `maxUnavailable=1` for one or more workloads running on the node pool, then only one Pod of those workloads can be evicted at a time, limiting the parallelism of the entire upgrade. This configuration requires the resources to temporarily create 20 new nodes.\nIf you can't use any additional resources, you can use `maxSurge=0;maxUnavailable=1` to recreate one node at a time.\n### Control an in-progress surge upgrade\nWith surge upgrades, while an upgrade is in progress you can use commands to exercise some control over it. For more control over the upgrade process, we recommend using [blue-green upgrades](#blue-green-upgrade-strategy) .\nYou can cancel an in-progress surge upgrade at any time during the upgrade process. Cancelling pauses the upgrade, stopping GKE from upgrading new nodes, but doesn't automatically roll back the upgrade of the already-upgraded nodes. After you cancel an upgrade, you can either [resume](#resume-a-surge-upgrade) or [roll back](#rollback-a-surge-upgrade) .\nWhen you cancel an upgrade, GKE does the following with each of the nodes:\n- Nodes that have started the upgrade complete it.\n- Nodes that have not started the upgrade don't upgrade.\n- Nodes that have already successfully completed the upgrade are unaffected and are not rolled back.\nThis means that the node pool might end up in a state where nodes are running two different versions. If [automatic upgrades](/kubernetes-engine/docs/how-to/node-auto-upgrades) are enabled for the node pool, the node pool can be scheduled for auto-upgrade again, which would upgrade the remaining nodes in the node pool running the older version.\nLearn how to [cancel a node pool upgrade](/kubernetes-engine/docs/how-to/upgrading-a-cluster#cancel) .\nIf a node pool upgrade was canceled and left partially upgraded, you can resume the upgrade to complete the upgrade process for the node pool. This will upgrade any remaining nodes that had not been upgraded in the original operation. Learn how to [resume a node pool upgrade](/kubernetes-engine/docs/how-to/upgrading-a-cluster#resume) .\nIf a node pool is left partially upgraded, you can roll back the node pool to revert it to its previous state. You cannot roll back node pools after they have been successfully upgraded. Nodes that have not started an upgrade are unaffected. Learn how to [roll back a node pool upgrade](/kubernetes-engine/docs/how-to/upgrading-a-cluster#rollback) .\nIf you want to downgrade a node pool back to its previous version after the upgrade is already complete, see [Downgrading node pools](/kubernetes-engine/docs/how-to/upgrading-a-cluster#downgrading_nodes) .\n## Blue-green upgrades\nBlue-green upgrades are an alternative upgrade strategy to the default surge upgrade strategy. With blue-green upgrades, GKE first creates a new set of node resources (\"green\" nodes) with the new node configuration before evicting any workloads on the original resources (\"blue\" nodes). GKE keeps the \"blue\" resources, if needed, for rolling back workloads until their soaking time has been met. You can adjust the pace of upgrades and soaking time based on your environment's needs.\nWith this strategy, you have more control over the upgrade process. You can roll back an in-progress upgrade, if necessary, as the original environment is maintained during the upgrade. This upgrade strategy, however, is also more resource intensive. As the original environment is replicated, the node pool uses double the number of resources during the upgrade.\n### Choose blue-green upgrades for your environment\nIf you have highly-available production workloads that you need to be able to roll back quickly in case the workload does not tolerate the upgrade, and a temporary cost increase is acceptable, we recommend choosing blue-green upgrades for your node pools.\n[Blue-green upgrades](#blue-green-upgrade-strategy) are optimal for the following scenarios:\n- if you want a gradual rollout where risk mitigation is most important, where graceful termination greater than 60 minutes is needed.\n- if your workloads are less tolerant of disruptions.\n- if a temporary cost increase due to higher resource usage is acceptable.\n### When GKE uses blue-green upgrades\nFor GKE nodes, there are different types of configuration changes that require the nodes to be recreated. If enabled, GKE uses blue-green upgrades when the following types of changes occur:\n- [Version changes (upgrades)](/kubernetes-engine/docs/concepts/cluster-upgrades#node_pool_upgrades) \n- [Vertically scaling the nodes by changing the node machine attributes](/kubernetes-engine/docs/how-to/node-pools#change-machine-attributes) , including machine type, disk type, and disk size\n- [Image type changes](/sdk/gcloud/reference/container/clusters/upgrade#--image-type) \nSurge upgrades will be used for any other features requiring the nodes to be recreated. To learn more, see [When surge upgrades are used](#when-surge-upgrades-are-used) .\n### Phases of blue-green upgrades\nWith blue-green upgrades, you can customize and control the process by:\n- using the upgrade configuration parameters.\n- using commands to [cancel (pause)](#cancel-a-blue-green-upgrade) , [resume](#resume-a-blue-green-upgrade) , [roll back](#rollback-a-blue-green-upgrade) , or [complete](#complete-a-blue-green-upgrade) the steps.\nThis section explains the phases of the upgrade process. You can use upgrade settings to tune how the phases work, and commands to [control the upgrade process](#control-in-progress-blue-green-upgrade) .\nIn this phase, a new set of managed instance groups (MIGs)\u2014known as the \"green\" pool\u2014are created for each zone under the target pool with the new node configuration (new version or image type).\n[Quota](/kubernetes-engine/docs/how-to/node-upgrades-quota) will be checked before starting provisioning new green resources.\nIn this phase, the original MIGs\u2014known as the blue pool\u2014cluster autoscaler will stop scaling up or down. The green pool can only scale up in this phase.\nIn this phase, you can [cancel](#cancel-a-blue-green-upgrade) the upgrade if necessary. When you cancel a blue-green upgrade, the upgrade is paused in its current phase. After you've canceled it, you can either [resume](#resume-a-blue-green-upgrade) it or [rollback](#rollback-a-blue-green-upgrade) . At this phase, rolling back will delete the green pool.\nIn this phase, all the original nodes in the blue pool (existing MIGs) will be cordoned (marked as unschedulable). Existing workloads will keep running, but new workloads won't be scheduled on the existing nodes.\nIn this phase, you can [cancel](#cancel-a-blue-green-upgrade) the upgrade if necessary. When you cancel a blue-green upgrade, the upgrade is paused in its current phase. After you've canceled it, you can either [resume](#resume-a-blue-green-upgrade) it or [rollback](#rollback-a-blue-green-upgrade) . At this phase, rolling back will un-cordon the blue pool and delete the green pool.\nIn this phase, the original nodes in the blue pool (existing MIGs) will be drained in batches. When Kubernetes drains a nodes, eviction requests are sent to all the Pods running on the node. The Pods will be rescheduled. For Pods that have [PodDisruptionBudget](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#how-disruption-budgets-work) violations or long [terminationGracePeriodSeconds](https://v1-20.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#podtemplatespec-v1-core) during the draining, they will be deleted in the [Delete blue pool](#bg-phase-delete-blue-pool) phase when the node is deleted. You can use `BATCH_SOAK_DURATION` and `NODE_POOL_SOAK_DURATION` , which are described here and in the next section, to extend the period before Pods are deleted.\n**Note:** To ensure that Pods evicted from nodes in the blue pool only get rescheduled to nodes in the green pool, add a nodeSelector for the label `cloud.google.com/gke-nodepool: ${MY_NODE_POOL}` to your workload. If you omit this label and have other node pools in your cluster, your evicted Pods might be scheduled to those nodes.\nYou can control the size of the batches with either of the following settings:\n- `BATCH_NODE_COUNT`: the absolute number of nodes to drain in a batch.\n- `BATCH_PERCENT`: the percentage of nodes to drain in a batch, expressed as a decimal between 0 and 1, inclusive.\nIf either of these settings are set to zero, GKE skips this phase and proceeds to the [Soak node pool](#bg-phase-soak-node-pool) phase.\nAdditionally, you can control how long each batch drain soaks with `BATCH_SOAK_DURATION` . This duration is defined in seconds, with the default being zero seconds.\nIn this phase, you can still [cancel](#cancel-a-blue-green-upgrade) the upgrade if necessary. When you cancel a blue-green upgrade, the upgrade is paused in its current phase. After you've canceled it, you can either [resume](#resume-a-blue-green-upgrade) it or [rollback](#rollback-a-blue-green-upgrade) . At this phase, rolling back will stop the draining of the blue pool, and un-cordon the blue pool. Workloads can then be rescheduled on the blue pool (not guaranteed), and the green pool will be deleted.\nThis phase is used for you to verify the workload's health after the blue pool nodes have been drained.\nThe soak time is set with `NODE_POOL_SOAK_DURATION` , in seconds. By default, it is set to one hour (3600 seconds). If the total soak duration reaches 7 days (604,800 seconds), the [Delete blue pool phase](#bg-phase-delete-blue-pool) begins immediately.\nThe total soak duration is the sum of `NODE_POOL_SOAK_DURATION` , plus `BATCH_SOAK_DURATION` multiplied by the number of batches, which is determined by either `BATCH_NODE_COUNT` or `BATCH_PERCENT` .\nIn this phase, you can finish the upgrade and skip any remaining soak time by [completing the upgrade](#complete-a-blue-green-upgrade) . This will immediately begin the process of removing the blue pool nodes.\nYou can still [cancel](#cancel-a-blue-green-upgrade) the upgrade if necessary. When you cancel a blue-green upgrade, the upgrade is paused in its current phase. After you've canceled it, you can either [resume](#resume-a-blue-green-upgrade) it or [rollback](#rollback-a-blue-green-upgrade) .\nIn this phase, cluster autoscaler can now scale up or down the green pool as normal.\nAfter the expiration of the soaking time, the blue pool nodes will be removed from the target pool. This phase cannot be paused. Also, this phase does not use eviction and instead attempts to delete the Pods. Unlike eviction, deletion doesn't respect PDBs and forcibly deletes the Pods. The deletion caps a Pod's `terminationGracePeriodSeconds` to no more than 60 minutes. After this final attempt is made to delete the remaining Pods, the blue pool nodes are deleted from the node pool.\nAt the completion of this phase, your node pool will have only new nodes with the updated configuration (version or image type).\n### How cluster autoscaler works with blue-green upgrades\n**Note:** [Cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) and [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) are supported with blue-green upgrades for GKE version 1.23 and later. For usage in earlier versions, we recommend [disabling cluster autoscaler](/kubernetes-engine/docs/how-to/cluster-autoscaler#disable_autoscaling) on the node pool to avoid potential unanticipated scale-down events.\nDuring the phases of a blue-green upgrade, the original \"blue\" pool does not scale up or down. When the new \"green\" pool is created, it can only be scaled up until the [Soak node pool phase](#bg-phase-soak-node-pool) , where it can scale up or down. If an upgrade is [rolledback](#rollback-a-blue-green-upgrade) , the original \"blue\" pool might scale up during this process if additional capacity is needed.\n### Control an in-progress blue-green upgrade\nWith blue-green upgrades, while an upgrade is in progress you can use commands to exercise control over it. This gives you a high level of control over the process in case you determine, for instance, that your workloads need to be rolled back to the old node configuration.\nWhen you cancel a blue-green upgrade, you pause the upgrade in its current phase. This command can be used at all phases except the [Deleteblue pool phase](#bg-phase-delete-blue-pool) . When cancelled, the node pool will be paused at an intermediate status based on the phase where the request was issued.\nLearn how to [cancel a node poolupgrade](/kubernetes-engine/docs/how-to/upgrading-a-cluster#cancel) .\nAfter an upgrade is canceled, you can choose one of two paths forward: [resume](#resume-a-blue-green-upgrade) or [rollback](#rollback-a-blue-green-upgrade) .\nIf you have determined the upgrade is okay to move forward, you can resume it.\nIf you resume, the upgrade process will continue at the intermediate phase it was paused. To learn how to resume a node pool upgrade, see [Resume a node pool upgrade](/kubernetes-engine/docs/how-to/upgrading-a-cluster#resume) .\nIf you have determined that the upgrade shouldn't move forward and you want to bring the node pool back to its original state, you can roll back. To learn how to roll back a node pool upgrade, see [roll back a node pool upgrade](/kubernetes-engine/docs/how-to/upgrading-a-cluster#rollback) .\nWith the roll back workflow, the process reverses itself to bring the node pool back to its original state. The blue pool will be un-cordoned so that workloads may be rescheduled on it. During this process, cluster autoscaler may scale up the blue pool as needed. The green pool will be drained and deleted.\nIf you want to downgrade a node pool back to its previous version after the upgrade is already complete, see [Downgrading node pools](/kubernetes-engine/docs/how-to/upgrading-a-cluster#downgrading_nodes) .\nDuring the [Soak phase](#bg-phase-soak-node-pool) , you can complete an upgrade if you have determined that the workload does not need further validation on the new node configuration and the old nodes can be removed. Completing an upgrade skips the rest of the [Soak phase](#bg-phase-soak-node-pool) and proceeds to the [Delete blue pool phase](#bg-phase-delete-blue-pool) .\nTo learn more about how to use the `complete` command, see [Complete a blue-green node pool upgrade](/kubernetes-engine/docs/how-to/upgrading-a-cluster#complete) .\n## What's next\n- [Configuring node upgrade strategies](/kubernetes-engine/docs/how-to/node-pool-upgrade-strategies)", "guide": "Google Kubernetes Engine (GKE)"}