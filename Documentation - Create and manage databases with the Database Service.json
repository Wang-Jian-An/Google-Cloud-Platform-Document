{"title": "Documentation - Create and manage databases with the Database Service", "url": "https://cloud.google.com/distributed-cloud/hosted/docs/latest/gdch/overview", "abstract": "# Documentation - Create and manage databases with the Database Service\n**Important:** To run Oracle databases on Google Distributed Cloud Hosted, you must bring your own license (BYOL).\n", "content": "## Authorization and roles\nUsers must be authorized to access the Database Service. Authorization is required to access Database Service with both the GDCH console and the Distributed Cloud Hosted gdcloud CLI.\n**Note:** Authorization to access the Database Service is distinct from authorization to access the databases managed by Database Service. Refer to your database management system (DBMS) documentation for guidance on controlling access to databases.\nThe following roles grant permissions to users:\n## Available database engines\nThe following database engines are available to use in a GDCH environment:\n| Database     | Versions |\n|:----------------------------|:-----------|\n| AlloyDB Omni (preview only) | 0   |\n| Oracle      | 19   |\n| PostgreSQL     | 13, 14, 15 |\n**Preview:** The AlloyDB Omni database engine is a Preview feature that is available as-is and is not recommended for production environments. Google provides no Service-Level agreements (SLA) or technical support commitments for Preview features. For more information, see GDCH's [feature stages](/distributed-cloud/hosted/docs/latest/gdch/resources/feature-stages) .\n**Note:** If you use the AlloyDB Omni Preview and then decide to disable it, ensure that all AlloyDB Omni database clusters are deleted before you disable the AlloyDB Omni feature. After the feature is disabled, you cannot make any changes to AlloyDB Omni clusters. For assistance, reach out to your IO.\n## Choose a database engine type and create a database cluster\nIf you want to enable backups for the database cluster, first create a [Distributed Cloud Hosted storage bucket](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/create-storage-buckets) or any bucket that is accessible with a S3-compatible endpoint, then [create a backup repository](/distributed-cloud/hosted/docs/latest/gdch/platform-application/pa-ao-operations/backup-restore#add_a_backup_repository) named \"dbs-backup-repository\". If using storage bucket outside of Distributed Cloud Hosted, it's your responsibility to ensure the bucket is properly encrypted.\nA user with the Project DB Admin role must perform the following steps. Use either the GDCH console or the Distributed Cloud Hosted gdcloud CLI to create database clusters:\n- From the main menu, choose **Database Service** .\n- Click **Create Database Cluster** .\n- In the **Choose a database engine** dialog, choose a [database engine](#available-db-engines) . **Note:** For instructions on uploading an Oracle Database image, see [Distribute Oracle images on Google Distributed Cloud Hosted](/distributed-cloud/hosted/docs/latest/gdch/platform-application/pa-ao-operations/oracle-image-distribution) .\n- In the **Configure your cluster** dialog, specify the cluster ID, password, and database version. You can enable backups and configure the backup retention period.\n- If you chose the PostgreSQL database engine, you can enable high availability for your database cluster. If enabled, the Database Service provisions a standby instance in the same zone as your primary instance to protect against failure. See [High availability](#high-availability) for more information.\n- In the **Configure your primary instance** dialog, specify the CPU, memory, and storage capacity of the primary instance of the database cluster. We recommend you choose enough memory to hold your largest table.\n- Click **Create** . Creating the database cluster can take a few minutes. Check the status of the cluster from the **Cluster overview** page. The status changes from to when the cluster is ready.\n- Before using Distributed Cloud Hosted gdcloud CLI, [install and initialize](/distributed-cloud/hosted/docs/latest/gdch/resources/gdcloud-install) it. Then, [authenticate](/distributed-cloud/hosted/docs/latest/gdch/resources/gdcloud-auth) with your organization.\n- Run the following command to create a database cluster:```\ngdcloud database clusters create CLUSTER_NAME \\\u00a0 \u00a0 --database-version DB_VERSION \\\u00a0 \u00a0 --admin-password ADMIN_PASSWORD\n```Replace the following variables:- with the name for the new cluster.\n- with the version string for the new cluster. For example,`POSTGRESQL_13`,`ORACLE_19_ENTERPRISE`or`ALLOYDBOMNI_0.2`(AlloyDB is preview-only).\n- with the admin password for the new cluster.\n- For more information on configuring the CPU, memory, and storage resources for the database cluster, configuring backup, enabling high availability, and for other available options, run:```\ngdcloud database clusters create --help\n```## List database clusters\nWork through the following steps to list database clusters with the GDCH console or the Distributed Cloud Hosted gdcloud CLI:\nFrom the main menu, choose **Database Service** . The console takes you to a filterable list of your database clusters.- Before using Distributed Cloud Hosted gdcloud CLI, [install and initialize](/distributed-cloud/hosted/docs/latest/gdch/resources/gdcloud-install) it. Then, [authenticate](/distributed-cloud/hosted/docs/latest/gdch/resources/gdcloud-auth) with your organization.\n- Run the following command to list database clusters:```\ngdcloud database clusters list\n```The output lists the namespace, name, database version, primary DNS endpoint, and status of each database cluster.## Start and stop database clusters\nStopping a database cluster pauses the cluster to save resources. You can stop and start database clusters with the GDCH console or with the gdcloud CLI tool.\n- From the main menu of the GDCH console, choose **Database Service** .\n- Select the database cluster to start or stop.\n- Click **STOP** or **START** .\nUse the following command to stop a database cluster:\n```\ngdcloud database clusters stop CLUSTER_NAME\n```\nUse the following command to start a database cluster:\n```\ngdcloud database clusters start CLUSTER_NAME\n```\nReplace the following:- with the name of the database cluster to start or stop.## Update database cluster attributes\nYou can change the following database cluster attributes with the GDCH console or the gdcloud CLI:\n- Database password for the admin user\n- External connections (enabled/disabled)\n- Availability level (PostgreSQL only)\n- Backup enabled and backup retention days\n- Database flags\n- CPU, memory, or storage allocated to the database cluster\nFor information on how to modify an attribute, see the workflow corresponding to the attribute type you want to update:\n**High Availability** \nFor PostgreSQL database clusters, enable or disable same zone high availability. See the [Configure high availability](#high-availability) section for more information.\n **Data Protection** \nFor PostgreSQL and Oracle database clusters, you can enable or disable Data protection:- Clickedit **Edit** to the right of the **Data protection** header to access the form to control the data protection settings. If automated backups are enabled, you can also configure how long the backups are to be retained.\n- After making changes, click **Save** to have the updates applied to your database cluster.\n **Connectivity** \nFor all database cluster types, you can choose whether the database cluster can be accessed only from inside the GDCH project, or if it is accessible from outside the project as well:- Clickedit **Edit** to the right of the **Connectivity** header to access the form where this setting can be modified.\n- After making the change, click **Save** .\n- After the change has been applied, note that the Endpoint and other connection string fields will be updated. If you have been using the old connection string, you will have to update to use the new one.\n **Instances** \nFor all database cluster types, you can edit the primary instance properties:- Clickedit **Edit** at the bottom of the **Primary Instance** card.\n- On the resulting form, select from the **High Performance** , **Standard** , **Minimum** , or **Custom** configurations. Note that if any configuration change results in a smaller storage size, that option will be disabled. If you select the **Custom** option, you have complete control over the number of CPUs, the amount of memory, and the amount of storage for your primary instance. Similarly, you cannot decrease the amount of storage, but you can increase or decrease the CPUs or memory.\n- Changing the CPU, memory or storage requires the database to be restarted. After making a change, the **Save** button changes to **Save and Restart** , and you will see a message just above that button indicating that the database cluster will have to be restarted for this change to take effect.\nFor PostgreSQL and Oracle database clusters, this form also allows you to add, modify, or remove database flags. The set of flags available is predetermined by GDCH. See the [Configure database flags](#configure-database-flags) section for more information.\nUse the following command to update a database cluster: `sh gdcloud database clusters update` `` `[options]`\nReplace the following:- with the name of the database cluster to update.\nFor the full list of options, see the [command reference](/distributed-cloud/hosted/docs/latest/gdch/resources/gdcloud-reference/gdcloud-database-clusters-update) or run `sh gdcloud database clusters update --help`\n### Configure database flags\nDatabase images that ship with GDCH come with default settings. However, you can customize the default database engine to satisfy requirements for your applications. Database clusters provide predefined flags that you can set using the GDCH console or gdcloud CLI:\n- In the navigation menu, select **Database Service** .\n- From the database cluster list, click the database cluster you want to configure database flags for.\n- In the **Instances in your database cluster** section, click **Edit Primary** .\n- In the **Flags** section, click **Add a Database Flag** .\n- Choose your flag and enter the value. If the value you input is not valid, the GDCH console gives you instructions on how to satisfy the required range or value type.\n- Click **Done** .\n- To set the configuration, click **Save** . For some flags to take effect, you must restart the database cluster. For this case, click **Save and Restart** .\n- To confirm your new flag is set, return to the **Instances in yourdatabase cluster** section of your database cluster and verify the new flag and value are visible.\nTo edit your database flags, return to the **Flags** section and modify the existing flags. Hover over a flag component and click the delete **Delete** icon to remove a database flag.\nYou can configure new database flags for your cluster or reset all existing flags back to their default values:- To configure a database flag for your database cluster, run:```\ngdcloud database clusters update CLUSTER_NAME \\\u00a0 \u00a0 --database-flags DB_FLAGS\n```Replace the following:- ``: the name of the database cluster.\n- ``: the comma-separated list of database flags to set on the database running in the database cluster. Each database flag and value are set as a key-value pair. Flags without a value can be defined without a string following the`=`character.\n **Important:** When updating an existing database cluster with new flags, you must include all the previously set flags in addition to the new flags you want to configure. Failure to include your existing flags to the database flag update command sets them back to the default values.For example, the following command sets several database flags for the `test-db-cluster` :```\ngdcloud database clusters update test-db-cluster \\\u00a0 \u00a0 --database-flags max_allowed_packet=55555,skip_grant_tables=,log_output=1\n```For a list of available database flags, see [Available database flags](#available-db-flags) .\n- To reset all your database flags to their default settings, run:```\ngdcloud database clusters update CLUSTER_NAME --clear-database-flags\n```Replace `` with the name of the database cluster.\n### Available database flags\nThe available database flags to configure for your database cluster are provided next based on the database engine you configured.\n| Flag        | Range                                | Type   | Need restart |\n|:------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------|:----------------|:---------------|\n| max_connections      | [1, 262143]                               | Integer   | True   |\n| temp_file_limit      | [1048576, 2147483647]                            | Integer   | False   |\n| log_min_duration_statement   | [-1, 2147483647]                              | Integer   | False   |\n| log_connections      | on, off                                | Boolean   | False   |\n| log_lock_waits      | on, off                                | Boolean   | False   |\n| log_disconnections     | on, off                                | Boolean   | False   |\n| log_checkpoints      | on, off                                | Boolean   | False   |\n| log_temp_files      | [-1, 2147483647]                              | Integer   | False   |\n| log_statement      | none, ddl, mod, all                             | Enum   | False   |\n| pgaudit.log       | read, write, function, role, ddl, misc, misc_set, all, none , -read, -write, -function, -role, -ddl, -misc, -misc_set, -all, -none | Repeated string | False   |\n| work_mem       | [64, 2147483647]                              | Integer   | False   |\n| autovacuum       | on, off                                | Boolean   | False   |\n| maintenance_work_mem    | [1024, 2147483647]                             | Integer   | False   |\n| random_page_cost     | [0, 2147483647]                              | Float   | False   |\n| log_min_messages     | debug5, debug4, debug3, debug2, debug1, info notice, warning, error, log, fatal, panic            | Enum   | False   |\n| idle_in_transaction_session_timeout | [0, 2147483647]                              | Integer   | False   |\n| max_wal_size      | [2, 2147483647],.                             | Integer   | False   |\n| autovacuum_vacuum_scale_factor  | [0, 100]                                | Float   | False   |\n| log_autovacuum_min_duration   | [-1, 2147483647]                              | Integer   | False   |\n| autovacuum_vacuum_cost_limit  | [-1, 10000]                               | Integer   | False   |\n| autovacuum_max_workers    | [1, 262143]                               | Integer   | True   |\n| autovacuum_analyze_scale_factor  | [0, 100]                                | Float   | False   |\n| track_activity_query_size   | [100, 1048576]                              | Integer   | True   |\n| log_duration      | on, off                                | Boolean   | False   |\n| autovacuum_vacuum_cost_delay  | [-1, 100]                               | Integer   | False   |\n| checkpoint_completion_target  | [0, 1]                                | Float   | False   |\n| log_statement_stats     | on, off                                | Boolean   | False   |\n| max_worker_processes    | [8, 4096]                               | Integer   | True   |\n| log_min_error_statement    | debug5, debug4, debug3, debug2, debug1, info, notice, warning, error, log, fatal, panic            | Enum   | False   |\n| default_statistics_target   | [1, 10000]                               | Integer   | False   |\n| checkpoint_timeout     | [30, 86400]                               | Integer   | False   |\n| wal_buffers       | [-1, 262143 ]                              | Integer   | True   |\n| effective_cache_size    | [1, 2147483647]                              | Integer   | False   |\n| autovacuum_work_mem     | [1, 2147483647]                              | Integer   | False   |\n| log_hostname      | on, off                                | Boolean   | False   |\n| autovacuum_vacuum_threshold   | [0, 2147483647]                              | Integer   | False   |\n| autovacuum_naptime     | [1, 2147483]                               | Integer   | False   |\n| autovacuum_analyze_threshold  | [0, 2147483647]                              | Integer   | False   |\n| pgaudit.log_client     | on, off                                | Boolean   | False   |\n| pgaudit.log_parameter    | on, off                                | Boolean   | False   |\n| pgaudit.log_level     | debug5, debug4, debug3, debug2, debug1, info, notice, warning, error, log               | Enum   | False   |\n| pgaudit.log_relation    | on, off                                | Boolean   | False   |\n| pgaudit.log_catalog     | on, off                                | Boolean   | False   |\n| pgaudit.role      | nan                                 | String   | False   |\n| autovacuum_freeze_max_age   | [100000, 2000000000]                             | Integer   | True   |\n| autovacuum_multixact_freeze_max_age | [10000, 2000000000]                             | Integer   | True   |\n| pgaudit.log_statement_once   | on, off                                | Boolean   | False   |\nThe following table specifies default values for flags different from the vendor default:\n| Flag     | Value/Formula   | Value/Formula.1  |\n|:----------------------|:-----------------------|:-----------------------|\n| shared_buffers  | 1/3 * totalMemory(MiB) | 1/3 * totalMemory(MiB) |\n| max_wal_size   | 1504MB     | 1504MB     |\n| max_connections  | Total memory >= ?  | Value     |\n| max_connections  | 120GB     | 1000     |\n| max_connections  | 60GB     | 800     |\n| max_connections  | 15GB     | 500     |\n| max_connections  | 7.5GB     | 400     |\n| max_connections  | 6GB     | 200     |\n| max_connections  | 3.75GB     | 100     |\n| max_connections  | 1.7GB     | 50      |\n| max_connections  | 0      | 25      |\n| effective_cache_size | 2/5 * totalMemory  | 2/5 * totalMemory  |\n| temp_file_limit  | 1/10 * storageSize  | 1/10 * storageSize  |\n| log_connections  | on      | on      |\n| log_disconnections | on      | on      |\n| pgaudit.log   | all, -misc    | all, -misc    |\n| log_hostname   | on      | on      |\n| pgaudit.log_parameter | on      | on      |\n| Flag        | Range        | Type | Need restart |\n|:---------------------------------|:------------------------------------|:--------|:---------------|\n| aq_tm_processes     | [0, 40]        | Integer | False   |\n| background_core_dump    | FULL, PARTIAL      | Enum | False   |\n| bitmap_merge_area_size   | [0, 2147483647]      | Integer | True   |\n| control_management_pack_access | NONE, DIAGNOSTIC, DIAGNOSTIC+TUNING | Enum | False   |\n| cursor_sharing     | FORCE, EXACT, SIMILAR    | Enum | False   |\n| db_cache_size     | [0, 10995116277760]     | Integer | False   |\n| db_files       | [200, 20000]      | Integer | True   |\n| db_flashback_retention_target | [30, 2147483647]     | Integer | False   |\n| db_keep_cache_size    | [0, 10995116277760]     | Integer | False   |\n| db_recovery_file_dest_size  | [0, 10995116277760]     | Integer | False   |\n| event       | nan         | String | True   |\n| large_pool_size     | [0, 10995116277760]     | Integer | False   |\n| log_buffer      | [2097152, 10995116277760]   | Integer | True   |\n| open_cursors      | [5, 65535]       | Integer | False   |\n| pga_aggregate_limit    | [0, 10995116277760]     | Integer | False   |\n| pga_aggregate_target    | [10485760, 10995116277760]   | Integer | False   |\n| processes      | [100, 20000]      | Integer | True   |\n| recyclebin      | ON, OFF        | Enum | True   |\n| resource_limit     | TRUE, FALSE       | Boolean | False   |\n| sec_max_failed_login_attempts | [1, 2147483647]      | Integer | True   |\n| sga_max_size      | [377487360, 10995116277760]   | Integer | True   |\n| sga_target      | [377487360, 10995116277760]   | Integer | False   |\n| shared_pool_size     | [0, 10995116277760]     | Integer | False   |\n| undo_retention     | [0, 2147483647]      | Integer | False   |\n| global_names      | TRUE, FALSE       | Boolean | False   |\n| DBFIPS_140      | TRUE, FALSE       | Boolean | True   |\n| parallel_max_servers    | [0, 32767]       | Integer | False   |\n| _fix_control      | nan         | String | False   |\n| _sql_plan_directive_mgmt_control | [0, 65535]       | Integer | False   |\n| _optimizer_dsdir_usage_control | [0, 126]       | Integer | False   |\n| skip_unusable_indexes   | TRUE, FALSE       | Boolean | False   |\nThe following table specifies default values for flags different from the vendor default:\n| Flag     | Value/Formula   |\n|:---------------------|:------------------------|\n| sga_target   | 4/5* (totalMemory-1GiB) |\n| pga_aggregate_target | 1/5* (totalMemory-1GiB) |\n| open_cursors   | 300      |\n| processes   | 300      |\n| DBFIPS_140   | TRUE     |\n| global_names   | TRUE     |\n| Flag          | Range    | Type | Need restart |\n|:-----------------------------------------|:------------------|:--------|:---------------|\n| google_columnar_engine.enabled   | on, off   | Boolean | True   |\n| google_columnar_engine.memory_size_in_mb | [128, 2147483647] | Integer | True   |\n## Configure high availability\n**Note:** High availability is only available for the PostgreSQL database engine at this time.\nThe purpose of a high availability configuration is to reduce downtime when a database cluster instance becomes unavailable. This might happen when an instance runs out of memory. With high availability, your data continues to be available to client applications.\nWithin a site, the configuration is made up of a primary instance and a standby replica. All writes made to the primary instance are replicated to the standby replica before a transaction is reported as committed. In the event of an instance failure, you can request that the standby replica become the new primary instance. Application traffic is then rerouted to the new primary instance. This process is called a .\nYou can [manually trigger a failover](#trigger-failover) at any time. The failover involves the following process, in order:\n- GDCH takes the primary instance offline.\n- GDCH turns the standby replica into the new active database cluster.\n- GDCH deletes the previous active database cluster.\n- GDCH creates a new standby replica.\nFor PostgreSQL database clusters, you can enable or disable same zone high availability. For information on enabling high availability when creating a database cluster, see [Choose a database engine type and create a database cluster](#create) .\n### Update an existing cluster\nYou can update your high availability settings for an existing database cluster:\n- In the navigation menu, select **Database Service** .\n- From the database cluster list, click the database cluster to update.\n- Select edit **Edit** in the **High availability** section.\n- Select **Enable same zone standby** to either toggle on or off the availability of a standby instance in the same zone as your primary database cluster.\n- Click **Save** .\n- Verify your database cluster reflects your high availability update by viewing its status in the **High availability** column of the database cluster list.\n- Update your database cluster's high availability configuration:```\ngdcloud database clusters update CLUSTER_NAME \\\u00a0 \u00a0 --availability-type HA_TYPE\n```Replace the following:- ``: the name of the database cluster.\n- ``: the high availability level for the database cluster. You can set`zonal`or`zonal_ha`. The`zonal`value is set by default.\n- Verify your database cluster reflects your high availability update:```\ngdcloud database clusters list\n```\n### Trigger a failover\nIf you have configured high availability for your database cluster, you can trigger a failover. To trigger a failover, complete the following steps:\n- In the navigation menu, select **Database Service** .\n- From the database cluster list, click the database cluster to trigger a failover for. Your database cluster must have [high availability enabled](#update-existing-cluster-ha) to be eligible for a failover.\n- Click **Failover** .\n- Type the cluster's ID for the confirmation phrase and click **Failover** to trigger the failover process.\n- Trigger the failover for the database cluster:```\ngdcloud database clusters failover CLUSTER_NAME\n```Replace `` with the name of the database cluster.## Delete database clusters\nYou can delete database clusters with the GDCH console or the Distributed Cloud Hosted gdcloud CLI.\n**Note:** When backup is disabled, deleting a database cluster is an irreversible action. When backup is enabled, you have an option in the console to delete a cluster while keeping its backups. This makes the cluster recoverable within your set retention period.\n- From the main menu of the GDCH console, choose **Database Service** .\n- Select the database cluster to delete.\n- Clickdelete **DELETE** .\n- Optional: If backup was previously enabled, select **Delete all backups** . This results in a permanent deletion. Otherwise, backups will be automatically deleted once the project's retention period expires.\n- Confirm deletion by typing the cluster's ID.\n- Clickdelete **DELETE** to finish.\nUse the following command to delete a database cluster:\n```\ngdcloud database clusters delete CLUSTER_NAME\n```\nReplace the following:- with the name of the database cluster to delete.## Connect to a database cluster\nBy default, a database cluster only allows connection from within the [user cluster](/distributed-cloud/hosted/docs/latest/gdch/overview#cluster) and the same project.\nTo enable connections to all database clusters in your project from another project, see [Enable cross-project connections](#cross_project_connections) .\nTo enable connections to a database cluster from IP addresses outside your GDC Hosted organization, see [Enable external connections](#external_connections) .\nSign in to the GDCH console with an account bound to the `project-db-admin` role to find the following information for connecting to your database cluster. This information is in the **Connectivity** section of the **Database Service** page.\n- The password of the administrator account (the username is`dbsadmin`)\n- IP address and port number of the cluster's load balancer\n- Endpoint DNS name of the cluster's load balancer\n- Whether the cluster allows external connections or not\n- A`psql`command for connecting to the cluster\n- A string for connecting to the cluster with Java Database Connectivity (JDBC)\n- A link to download the certification authority (CA) certificate of the database\nFollow these steps to connect to the database with the `psql` command:\n- Download the CA certificate from the Distributed Cloud Hosted console.\n- Set the `PGSSLROOTCERT` environment variable to the path of the certificate file:```\nexport PGSSLROOTCERT=path/to/accounts_cert.pem\n```Replace with the file path to the `accounts_cert.pem` certificate.\n- Run the `psql` command:```\npsql -h IP_ADDRESS_OR_HOSTNAME -p PORT -U USERNAME postgres\n```Replace the following variables:- with the value from the console.\n- with the port number from the console.\n- with the username from the console.\n- Enter the password from the **Connectivity** section of the **DatabaseService** page at the prompt. **Tip:** You can copy the password from the console.## Create a user\nTo create a user, [connect to the database cluster](#connect) and complete the following steps:\n- At the`psql`prompt, create the user:```\n\u00a0 CREATE USER USER_NAME\u00a0 \u00a0 \u00a0 WITH PASSWORD PASSWORD\u00a0 \u00a0 \u00a0 ATTRIBUTE1\u00a0 \u00a0 \u00a0 ATTRIBUTE2...;\u00a0 \n```Enter the password when prompted.For more information about role attributes, see the [ PostgreSQL documentation](https://www.postgresql.org/docs/current/static/role-attributes.html) .\n- You can confirm the user creation by displaying the user table:```\n\u00a0 SELECT * FROM pg_roles;\u00a0 \n```## Clone a database cluster\nYou can clone a database cluster to create a new database cluster that contains the same data as the original cluster. Cloning is a good way to make database clusters for testing purposes.\nYou can specify any point in time to base the clone on. You aren't limited to cloning the present state of a database cluster. The database service clones a new database cluster at the exact point in time you specify.\nYou can clone a database cluster with the GDCH console or with the gdcloud CLI tool:\n- From the main menu of the GDCH console, choose **Database Service** .\n- Select the database cluster to clone.\n- Clickadd_box **CLONE** .\n- In thedialog, specify the point in time to clone from and specify an ID for the new database cluster.\n- Click **CLONE** . This takes you to the Database cluster overview page for the new cluster where you can monitor its status.\nUse the following command to clone a database cluster:\n```\ngdcloud database clusters clone SOURCE \\\u00a0 \u00a0DESTINATION --point-in-time POINT_IN_TIME\n```\nReplace the following:- with the name of the database cluster to create a clone from.\n- with the name of the new database cluster to create.\n- with the timestamp of the point in time to use as the basis of the clone. Use the [RFC 3339](https://www.ietf.org/rfc/rfc3339.txt) timestamp format (`yyyy-MM-dd'T'HH:mm:ss'Z'`).## Plan maintenance windows\nGDCH offers you the ability to configure maintenance windows to schedule times for automatic updates. Maintenance windows are designed to target times where a brief downtime causes the lowest impact to your database clusters. You can schedule maintenance windows based on day of the week and hour, and length in which the maintenance window is open. For example, you could set a maintenance window to start at 3:00 AM on Tuesdays that spans eight hours.\nYou can also plan maintenance exclusions, which prevents disruptions to your workloads during date ranges where, due to unique circumstances, you don't want to allow the set maintenance window.\n### Create a maintenance window\nTo create a maintenance window for your database cluster, complete the following steps:\n- From the navigation menu of the GDCH console, choose **DatabaseService** .\n- Select the database cluster that you want to plan maintenance settings for.\n- In the **Maintenance** section, click edit **Edit** .\n- Select the **Start time** and **Length** for the maintenance window. Also select the days of the week to apply the maintenance window for. **Important:** Due to other possible processes running when the maintenance window opens for your database cluster, automatic updates for your database cluster could have a delayed start. The maintenance window length defines when the maintenance processes can start. After maintenance on the database cluster starts, the process runs until completion, possibly running past the defined maintenance window.\n- To apply a maintenance exclusion, click **Add Maintenance Exclusion** .\n- Give the exclusion a name and define the time window to exclude for your maintenance window.\n- Click **Save** .\n- To create a maintenance window for your database cluster, run:```\ngdcloud maintenance policies create POLICY_NAME \\\u00a0 \u00a0 --project PROJECT_ID \\\u00a0 \u00a0 --weekly-cycle-days DAYS_OF_WEEK \\\u00a0 \u00a0 --weekly-cycle-start-time START_TIME \\\u00a0 \u00a0 --weekly-cycle-duration DURATION\n```Replace the following:- ``: The name of the maintenance policy.\n- ``: The ID of the project in which to create the maintenance policy.\n- ``: Comma-separated list of days of the week when maintenance can begin. Accepted values are RFC-822 formatted days of the week, such as`Mon`.\n- ``: The time of the day, in UTC timezone, when maintenance can begin. The value must follow the`hh:mm`format, such as`20:36`.\n- ``: The maximum duration that the maintenance can last for, such as`2h3m`.\n- To apply a maintenance exclusion to your maintenance window, run:```\ngdcloud maintenance policies update POLICY_NAME \\\u00a0 \u00a0 --project PROJECT_ID \\\u00a0 \u00a0 --add-exclusion-name EXCLUSION_NAME \\\u00a0 \u00a0 --add-exclusion-start START_TIME \\\u00a0 \u00a0 --add-exclusion-end END_TIME\n```Replace the following:- ``: The name of the maintenance policy.\n- ``: The ID of the project in which to create the maintenance policy.\n- ``: The name of the new maintenance exclusion to add.\n- ``: The start date and time for the new maintenance exclusion. Accepted values must follow the RFC-3339 formatted timestamp, such as`2006-01-02T15:04:05Z`.\n- ``: The end date and time for the new maintenance exclusion. Accepted values must follow the RFC-3339 formatted timestamp, such as`2006-01-02T15:04:05Z`.\n- Verify the maintenance policy exists and reflects your intended configuration:```\ngdcloud maintenance policies list --project PROJECT_ID\n```\n- To attach the maintenance policy to your database cluster, run:```\ngdcloud maintenance policy-bindings create POLICY_BINDING_NAME \\\u00a0 \u00a0 --project PROJECT_ID \\\u00a0 \u00a0 --policy-name POLICY_NAME \\\u00a0 \u00a0 --resource-name RESOURCE_NAME\n```Replace the following:- ``: The name of the policy binding resource.\n- ``: The ID of the project in which to create the maintenance policy.\n- ``: The name of the maintenance policy created in the previous step.\n- `` : The resource name to bind to. For example, for a database cluster name `my-cluster` , the resource name for each of the available database engines would be the following:- AlloyDB Omni:`DBCluster.alloydbomni.dbadmin.gdc.goog/my-cluster`\n- Oracle:`DBCluster.oracle.dbadmin.gdc.goog/my-cluster`\n- PostGreSQL:`DBClusters.postgresql.dbadmin.gdc.goog/my-cluster`\n- Verify the policy binding exists, and its `Valid` condition is `True` :```\ngdcloud maintenance policy-bindings list --project PROJECT_ID\n```Replace `` with the ID of the project in which to create the maintenance policy.\n### Remove a maintenance window\nTo remove a maintenance window for your database cluster, complete the following steps:\n- From the navigation menu of the GDCH console, choose **DatabaseService** .\n- Select the database cluster that you want to plan maintenance settings for.\n- In the **Maintenance** section, click edit **Edit** .\n- In the **Length** field, select **24h** .\n- Select all the days of the week.\n- Click **Save** .\nThis removes the defined maintenance window and allows maintenance updates to start at any time.- To remove a maintenance window from your database cluster, run:```\ngdcloud maintenance policies delete POLICY_NAME \\\u00a0 \u00a0 --project PROJECT_ID\n```Replace the following:- ``: The name of the maintenance policy.\n- ``: The ID of the project in which the maintenance policy exists.\n- Remove the maintenance policy binding from the database cluster:```\ngdcloud maintenance policy-bindings delete POLICY_BINDING_NAME \\\u00a0 \u00a0 --project PROJECT_ID\n```Replace the following:- ``: The name of the maintenance policy binding.\n- ``: The ID of the project in which the maintenance policy exists.\n### Manage maintenance exclusions\nIf you created a maintenance exclusion for your maintenance window, you can edit or remove the exclusion without affecting the maintenance window. To edit or remove a maintenance exclusion, complete the following steps:\n- From the navigation menu of the GDCH console, choose **DatabaseService** .\n- Select the database cluster that you want to manage a maintenance exclusion for.\n- In the **Maintenance** section, click edit **Edit** .\n- Edit the date ranges and start times of the exclusion. If you want to delete the exclusion, click delete **Delete** .\n- Click **Save** to save your maintenance exclusion modifications to the maintenance window.\nThe gdcloud CLI only supports adding and deleting maintenance exclusions. Therefore, you must delete the existing exclusion and add a new one if you want to modify your exclusion date range or times.- To delete a maintenance exclusion, run:```\ngdcloud maintenance policies update POLICY_NAME \\\u00a0 \u00a0 --project PROJECT_ID \\\u00a0 \u00a0 --remove-exclusion-name EXCLUSION_NAME\n```Replace the following:- ``: The name of the maintenance policy.\n- ``: The ID of the project in which to delete the maintenance policy's exclusion.\n- ``: The name of the maintenance exclusion to delete.\n- To add a new maintenance exclusion to apply any previous exclusion edits, run:```\ngdcloud maintenance policies update POLICY_NAME \\\u00a0 \u00a0 --project PROJECT_ID \\\u00a0 \u00a0 --add-exclusion-name EXCLUSION_NAME \\\u00a0 \u00a0 --add-exclusion-start START_TIME \\\u00a0 \u00a0 --add-exclusion-end END_TIME\n```Replace the following:- ``: The name of the maintenance policy.\n- ``: The ID of the project in which the maintenance policy exists.\n- ``: The name of the new maintenance exclusion to add.\n- ``: The start date and time for the new maintenance exclusion. Accepted values must follow the RFC-3339 formatted timestamp, such as`2006-01-02T15:04:05Z`.\n- ``: The end date and time for the new maintenance exclusion. Accepted values must follow the RFC-3339 formatted timestamp, such as`2006-01-02T15:04:05Z`.\n## Enable cross-project connections\nBy default, a database cluster only allows connections from within the [user cluster](/distributed-cloud/hosted/docs/latest/gdch/overview#cluster) and the same project. To allow connections from workloads in another project to all database clusters in your project:\n- Sign in to the GDCH console with an account bound to the`project-networkpolicy-admin`role to create firewall rules.\n- From the main menu of the GDCH console, choose **Firewall** .\n- In the **User created rules** section, click **Create** .\n- In **Firewall rule details** , create a name for your firewall rule.\n- In the **Direction of traffic** dialog, choose **INGRESS** .\n- In the **Target** dialog, choose **Service** and then select **dbs** .\n- In the **From** dialog, choose **Another project** and select the project ID from which you would like to allow connectivity.\n- Click **Create** .\n- Wait for the **Status** column of the new rule to show **Ready** .## Enable external connections\nBy default, a database cluster only allows connections from within the [user cluster](/distributed-cloud/hosted/docs/latest/gdch/overview#cluster) and the same project. To allow external connections from IP addresses outside of your Google Distributed Cloud Hosted organization:\n- Sign in to the GDCH console with an account bound to the`project-networkpolicy-admin`role to create firewall rules.\n- From the main menu of the GDCH console, choose **Firewall** .\n- In the **User created rules** section, click **Create** .\n- In **Firewall rule details** , create a name for your firewall rule.\n- In the **Direction of traffic** dialog, choose **INGRESS** .\n- In the **Target** dialog, choose **Service** and then select **dbs** .\n- In the **From** dialog, choose **Outside the organization** and input the CIDR range from which you would like to allow external connectivity.\n- Click **Create** .\n- Wait for the **Status** column of the new rule to show **Ready** .\n- Sign in to the GDCH console with an account bound to the`project-db-admin`\n- From the main menu of the GDCH console, choose **Database Service** .\n- Select the database cluster that you want to enable external connections to.\n- Check the **Allow external connections** line of the **Connectivity** section of the Database cluster overview to see whether external connections are already allowed.\n- In the **Connectivity** section of the Database cluster overview, clickedit **Edit** .\n- Select the **Allow external connections** checkbox.\n- Click **SAVE** .## Export a database cluster\nYou can export a database cluster to a data dump file using either the GDCH console or the Distributed Cloud Hosted gdcloud CLI:\n- From the main menu, choose **Database Service** .\n- Select the database cluster you want to export. This takes you to the **Database cluster overview** page for that cluster.\n- Clickfile_upload **EXPORT** . The **Export data** panel opens.\n- In the **Export data** panel, specify the storage location to export to.\n- Click **EXPORT** . On screen messages indicate the status of the export process.\n- Before using Distributed Cloud Hosted gdcloud CLI, [install and initialize](/distributed-cloud/hosted/docs/latest/gdch/resources/gdcloud-install) it. Then, [authenticate](/distributed-cloud/hosted/docs/latest/gdch/resources/gdcloud-auth) with your organization.\n- Run the following command to export a database file to a dump file:```\ngdcloud database export sql DATABASE_CLUSTER \\\u00a0 \u00a0 \u00a0s3://BUCKET_NAME/SAMPLE.dmp --project=PROJECT_NAME\n```Replace the following:- with the name of the database cluster to export.\n- with the destination for the exported dump file.\n- with the name of the project that the database cluster is in.\n## Import from a dump file\nBefore importing data, you must:\n- [Create a database cluster](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/db-service#create) to import the data to.\n- Upload the dump file to a storage bucket. See [Upload objects to storage buckets](/distributed-cloud/hosted/docs/latest/gdch/platform/pa-user/object-storage#upload_objects_to_storage_buckets) for instructions.The Database Service import service account must have access to the dump file. The service account is named `postgresql-import-` `` or `oracle-import-` `` , depending on the type of database you are importing.Replace with the name of the database cluster where you are importing data.\nYou can import a dump file into a database cluster using either the GDCH console or the Distributed Cloud Hosted gdcloud CLI:\n- Open the **Database cluster overview** page in the GDCH console to see the cluster that contains the database you are importing.\n- Click **Import** . The **Import data to accounts** panel opens.\n- In the **Source** section of the **Import data to accounts** panel, specify the location of the SQL data dump file you uploaded previously.\n- In the **Destination** field, specify an existing destination database for the import. **Note:** You can leave this field empty if the dump file already specifies a destination. If the dump file specifies a destination and you fill the **Destination** field, the **Destination** field overrides the destination specified in the dump file.\n- Click **Import** . A banner on the GDCH console shows the status of the import.\n- Before using Distributed Cloud Hosted gdcloud CLI, [install and initialize](/distributed-cloud/hosted/docs/latest/gdch/resources/gdcloud-install) it. Then, [authenticate](/distributed-cloud/hosted/docs/latest/gdch/resources/gdcloud-auth) with your organization.\n- Run the following command to import a dump file into a database:```\ngdcloud database import sql DATABASE_CLUSTER s3://BUCKET_NAME/sample.dmp \\\u00a0 \u00a0 --project=PROJECT_NAME\n```Replace the following:- with the name of the database cluster to import data into.\n- with the location of the dump file.\n- with the name of the project that the database cluster is in.\n## Preserve database clusters before an upgrade\nIn GDCH 1.11.0, existing database clusters will not be migrated forward from previous GDCH versions. These database clusters are automatically deleted after the GDCH instance is upgraded to version 1.11.0.\nBefore applying the release, you must confirm that there is no critical data that must be preserved. For data that must be preserved, follow these steps to migrate them forward:\n- [List database clusters](#list_database_clusters) and identify the ones that must be preserved.\n- For each database cluster, [export the data](#export) and store it in a GDCH bucket.\n- Optional: To avoid additional changes, completely [delete the database cluster](#delete_database_clusters) before applying the new GDCH version.\n- After the upgrade is complete, recreate the database cluster and [import the dump file](#import) into it.## Manage advanced migration\nAdvanced migration is a solution for migrating data for large-sized databases with less downtime. This feature is only available for PostgreSQL.\nA user with the Project DB Admin role must perform the following steps. Use either the GDCH console or the Distributed Cloud Hosted gdcloud CLI to manage migrations:\n- From the main menu, choose **Database Service** .\n- Click **Create Migration** .\n- In the **Get started** dialog, review requirements for the PostgreSQL source and connectivity.\n- In the **Specify your source database** dialog, specify the source database hostname or IP address, username, password, encryption type, and certificate.\n- In the **Configure your cluster** dialog, specify the Cluster ID, password, database version, CPU, memory, and storage capacity of the target database cluster. Ensure you choose enough memory to hold your largest table.\n- Click **Create** . Creating the migration and target database cluster can take a few minutes. The status changes from`Reconciling`to`Ready`when the cluster is ready. The migration status changes to`Unsynced`when migration is set up successfully. Use the following options to manage your migration:- **Start** : This starts the migration and changes the migration status to`Running`.\n- **Stop** : This stops the migration and changes the migration status to`Stopped`.\n- **Promote** : This promotes the target database cluster to a stand alone database.\n- **Delete** : This deletes the migration and target database cluster created for this migration.\nPeriodically rotate the source database replication user password with the following steps:- Clickedit **Edit** to the right of **Source database** to access the form where this setting can be modified.\n- After making the required changes to rotate the replication user password, click **Save** .\nAfter the change is applied, the migration backend uses the new password.- Before using Distributed Cloud Hosted gdcloud CLI, [install and initialize](/distributed-cloud/hosted/docs/latest/gdch/resources/gdcloud-install) it. Then, [authenticate](/distributed-cloud/hosted/docs/latest/gdch/resources/gdcloud-auth) with your organization.\n- Create a migration:```\ngdcloud database connection-profiles create postgresql SOURCE_CONNECTION_PROFILE \\\u00a0 \u00a0 --username REPLICATION_USERNAME \\\u00a0 \u00a0 --password REPLICATION_PASSWORD \\\u00a0 \u00a0 --ca-certificate CA_CERT_FILE_PATHgdcloud database migrations create MIGRATION_NAME \\\u00a0 \u00a0 --source SOURCE_CONNECTION_PROFILE \\\u00a0 \u00a0 --destination DESTINATION_DBCLUSTERgdcloud database clusters create DESTINATION_DBCLUSTER \\\u00a0 \u00a0 --database-version DB_VERSION \\\u00a0 \u00a0 --admin-password ADMIN_PASSWORD\n```Replace the following variables:- is the name for the new connection profile.\n- is the name for the replication user of the source database.\n- is the password for the replication user of the source database.\n- is the file path for the source database CA certificate.\n- is the name for the new migration.\n- is the name for the target database cluster.\n- is the version string for the new cluster. For example,`POSTGRESQL_13`.\n- is the admin password for the new cluster.\n- Start a migration:```\ngdcloud database migrations start MIGRATION_NAME\n```\n- Stop a migration:```\ngdcloud database migrations stop MIGRATION_NAME\n```\n- Promote a migration:```\ngdcloud database migrations promote MIGRATION_NAME\n```\n- List existing connection-profiles:```\ngdcloud database connection-profiles list postgresql\n```\n- List the existing migration:```\ngdcloud database migrations list --destination DESTINATION_DBCLUSTER\n```## Observe metrics\nYou can observe Database Service metrics with the monitoring instance. See [Monitoring and visualizingmetrics](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/observability-monitoring-and-visualizing) for general information about the monitoring and visualizing processes for Application Operators (AO) in GDCH.\nYou must create a database cluster before you can observe its metrics.\n### View diagnostic logs\n- Navigate to the monitoring instance UI to access the database diagnostic logs for a database cluster.\n- Click the explore **Explore** button from the menu to open the **Explore** page.\n- Enter a query to search for Database Service logs using LogQL.- Using the **Label filters** drop-down menu, create a filter for `service_name=ods` .\n- Click the add **Operations** button and select **Line contains** . Enter the database cluster's name in the text box.\n- Click the add **Operations** button again and select **Line contains** . Enter in the text box.\n- Click the **Run query** button.\n### View metrics\n- Navigate to the monitoring instance UI to access the metrics for a database cluster.\n- From the drop-down menu, select **prometheus** as the data source to retrieve metrics.\n- Application operators can access Database Service metrics that have the `ods_` prefix. Enter `ods_` in the **Select a metric** text box in the **Metrics browser** panel to view all Database Service metric types.\n- Application operators can also access metrics with the `pg_` prefix.", "guide": "Documentation"}