{"title": "Google Kubernetes Engine (GKE) - Troubleshoot GKE authentication issues", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview", "abstract": "# Google Kubernetes Engine (GKE) - Troubleshoot GKE authentication issues\nThis page shows you how to resolve issues related to security configurations in your Google Kubernetes Engine (GKE) Autopilot and Standard clusters.\n[Cloud Customer Care](/kubernetes-engine/docs/getting-support)\n", "content": "## RBAC and IAM\n### Authenticated IAM accounts fail to perform in-cluster actions\nThe following issue occurs when you try to perform an action in the cluster but GKE can't find an RBAC policy that authorizes the action. GKE attempts to find an IAM policy that grants the same permission. If that fails, you see an error message similar to the following:\n```\nError from server (Forbidden): roles.rbac.authorization.k8s.io is forbidden:\nUser \"example-account@example-project.iam.gserviceaccount.com\" cannot list resource \"roles\" in\nAPI group \"rbac.authorization.k8s.io\" in the namespace \"kube-system\": requires\none of [\"container.roles.list\"] permission(s).\n```\nTo resolve this issue, use an RBAC policy to grant the permissions for the attempted action. For example, to resolve the issue in the previous sample, grant a Role that has the `list` permission on `roles` objects in the `kube-system` namespace. For instructions, see [Authorize actions in clusters using role-based access control](/kubernetes-engine/docs/how-to/role-based-access-control) .\n## Workload identity federation for GKE\n### Pod can't authenticate to Google Cloud\nIf your application can't authenticate to Google Cloud, make sure that the following settings are configured properly:\n- Check that you have enabled the IAM Service Account Credentials API in the project containing the GKE cluster. [EnableIAM Credentials API](https://console.cloud.google.com/apis/api/iamcredentials.googleapis.com/overview) \n- Confirm that workload identity federation for GKE is enabled on the cluster by verifying that it has a workload identity pool set:```\ngcloud container clusters describe CLUSTER_NAME \\\u00a0 \u00a0 --format=\"value(workloadIdentityConfig.workloadPool)\"\n```Replace `` with the name of your GKE cluster.If you haven't already specified [a default zone or region for gcloud](/kubernetes-engine/docs/how-to/managing-clusters#before_you_begin) , you might also need to specify a `--region` or `--zone` flag when running this command.\n- Make sure that the GKE metadata server is configured on the node pool where your application is running:```\ngcloud container node-pools describe NODEPOOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --format=\"value(config.workloadMetadataConfig.mode)\"\n```Replace the following:- ``with the name of your nodepool.\n- ``with the name of your GKE cluster.\n- Verify that the Kubernetes service account is annotated correctly:```\nkubectl describe serviceaccount \\\u00a0 \u00a0 --namespace NAMESPACE KSA_NAME\n```Replace the following:- ``with your GKE cluster's namespace.\n- ``with the name of your Kubernetes service account.\nThe expected output contains an annotation similar to the following:```\niam.gke.io/gcp-service-account: GSA_NAME@PROJECT_ID.iam.gserviceaccount.com\n```\n- Check that the IAM service account is configured correctly:```\ngcloud iam service-accounts get-iam-policy \\\u00a0 \u00a0 GSA_NAME@GSA_PROJECT.iam.gserviceaccount.com\n```The expected output contains a binding similar to the following:```\n- members:\u00a0 - serviceAccount:PROJECT_ID.svc.id.goog[NAMESPACE/KSA_NAME]\u00a0 role: roles/iam.workloadIdentityUser\n```\n- If you have a [cluster network policy](/kubernetes-engine/docs/how-to/network-policy) , you must allow egress to `127.0.0.1/32` on port `988` for clusters running GKE versions prior to 1.21.0-gke.1000, or to `169.254.169.252/32` on port `988` for clusters running GKE version 1.21.0-gke.1000 and later. For clusters running GKE Dataplane V2, you must allow egress to `169.254.169.254/32` on port `80` .```\nkubectl describe networkpolicy NETWORK_POLICY_NAME\n```Replace `` with the name of your GKE network policy.\n### DNS Resolution issues\nSome Google Cloud client libraries are configured to connect to the GKE and Compute Engine metadata servers by resolving the DNS name `metadata.google.internal` ; for these libraries, healthy in-cluster DNS resolution is a critical dependency for your workloads to authenticate to Google Cloud services.\nHow you detecting this problem depends on details of your deployed application, including its logging configuration. Look for error messages that:\n- tell you to configure GOOGLE_APPLICATION_CREDENTIALS, or\n- tell you that your requests to a Google Cloud service were rejected because the request had no credentials.\nIf you encounter problems with DNS resolution of `metadata.google.internal` , some Google Cloud client libraries can be instructed to skip DNS resolution by setting the environment variable `GCE_METADATA_HOST` to `169.254.169.254` :\n```\napiVersion: v1kind: Podmetadata:\u00a0 name: example-pod\u00a0 namespace: defaultspec:\u00a0 containers:\u00a0 - image: debian\u00a0 \u00a0 name: main\u00a0 \u00a0 command: [\"sleep\", \"infinity\"]\u00a0 \u00a0 env:\u00a0 \u00a0 - name: GCE_METADATA_HOST\u00a0 \u00a0 \u00a0 value: \"169.254.169.254\"\n```\nThis is the hardcoded IP address at which the metadata service is always available on Google Cloud compute platforms.\nSupported Google Cloud libraries:\n- [Python](https://googleapis.dev/python/google-auth/latest/reference/google.auth.environment_vars.html#:%7E:text=SDK%E2%80%99s%20config%20files.-,GCE_METADATA_HOST,-%3D%20%27GCE_METADATA_HOST%27) \n- [Java](https://github.com/googleapis/google-auth-library-java/blob/ab872812d0f6e9ad7598ba4c4c503d5bff6c2a2b/oauth2_http/java/com/google/auth/oauth2/DefaultCredentialsProvider.java#L71) \n- [Node.js](/nodejs/docs/reference/gcp-metadata/latest#:%7E:text=Environment%20variables-,GCE_METADATA_HOST,-%3A%20provide%20an%20alternate) \n- [Golang](https://pkg.go.dev/cloud.google.com/go/compute/metadata#section-readme:%7E:text=If%20the-,GCE_METADATA_HOST,-environment%20variable%20is) (Note, however, that the Golang client library already prefers to connect by IP, rather than DNS name).\n### Timeout errors at Pod start up\nThe GKE metadata server needs a few seconds before it can start accepting requests on a new Pod. Attempts to authenticate using workload identity federation for GKE within the first few seconds of a Pod's life might fail for applications and Google Cloud client libraries configured with a short timeout.\nIf you encounter timeout errors try the following:\n- Update the Google Cloud client libraries that your workloads use.\n- Change the application code to wait a few seconds and retry.\n- Deploy an [initContainer](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) that waits until the GKE metadata server is ready before running the Pod's main container.For example, the following manifest is for a Pod with an `initContainer` :```\napiVersion: v1kind: Podmetadata:\u00a0 name: pod-with-initcontainerspec:\u00a0 serviceAccountName: KSA_NAME\u00a0 initContainers:\u00a0 - image: \u00a0gcr.io/google.com/cloudsdktool/cloud-sdk:alpine\u00a0 \u00a0 name: workload-identity-initcontainer\u00a0 \u00a0 command:\u00a0 \u00a0 - '/bin/bash'\u00a0 \u00a0 - '-c'\u00a0 \u00a0 - |\u00a0 \u00a0 \u00a0 curl -sS -H 'Metadata-Flavor: Google' 'http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token' --retry 30 --retry-connrefused --retry-max-time 60 --connect-timeout 3 --fail --retry-all-errors > /dev/null && exit 0 || echo 'Retry limit exceeded. Failed to wait for metadata server to be available. Check if the gke-metadata-server Pod in the kube-system namespace is healthy.' >&2; exit 1\u00a0 containers:\u00a0 - image: gcr.io/your-project/your-image\u00a0 \u00a0 name: your-main-application-container\n```\n### Workload identity federation for GKE fails due to control plane unavailability\nThe metadata server can't return the workload identity federation for GKE when the cluster control plane is unavailable. Calls to the metadata server return status code 500.\nA log entry might appear similar to the following in the Logs Explorer:\n```\ndial tcp 35.232.136.58:443: connect: connection refused\n```\nThis behavior leads to unavailability of workload identity federation for GKE.\nThe control plane might be unavailable on zonal clusters during cluster maintenance like rotating IPs, upgrading control plane VMs, or resizing clusters or node pools. See [Choosing a regional or zonal control plane](/kubernetes-engine/docs/concepts/planning-scalability#choosing_a_regional_or_zonal_control_plane) to learn about control plane availability. Switching to a regional cluster eliminates this issue.\n### Workload identity federation for GKE authentication fails in clusters using Istio\nIf the GKE metadata server is blocked for any reason, workload identity federation for GKE authentication fails.\nIf you are using Istio or Anthos Service Mesh, add the following Pod-level annotation to all workloads that use workload identity federation for GKE to exclude the IP from redirection:\n```\ntraffic.sidecar.istio.io/excludeOutboundIPRanges: 169.254.169.254/32\n```\nYou can change the `global.proxy.excludeIPRanges` [Istio ConfigMap](https://istio.io/latest/docs/tasks/traffic-management/egress/egress-control/#direct-access-to-external-services) key to do the same thing.\nAlternatively, you could also add the following Pod-level annotation to all workloads that use workload identity federation for GKE, to delay application container start until the sidecar is ready:\n```\nproxy.istio.io/config: '{ \"holdApplicationUntilProxyStarts\": true }'\n```\nYou can change the `global.proxy.holdApplicationUntilProxyStarts` [Istio ConfigMap](https://istio.io/latest/docs/reference/config/istio.mesh.v1alpha1/#ProxyConfig) key to do the same thing.\n**Note:** The `global.proxy.*` configuration option is not available if you are using managed Anthos Service Mesh.\n### gke-metadata-server Pod is crashing\nThe `gke-metadata-server` system DaemonSet Pod facilitates workload identity federation for GKE on your nodes. The Pod uses memory resources proportional to the number of Kubernetes service accounts in your cluster.\nThe following issue occurs when the resource usage of the `gke-metadata-server` Pod exceeds its limits. The kubelet evicts the Pod with an out of memory error. You might have this issue if your cluster has more than 3,000 Kubernetes service accounts.\nTo identify the issue, do the following:\n- Find crashing `gke-metadata-server` Pods in the `kube-system` namespace:```\nkubectl get pods -n=kube-system | grep CrashLoopBackOff\n```The output is similar to the following:```\nNAMESPACE  NAME      READY  STATUS    RESTARTS AGE\nkube-system gke-metadata-server-8sm2l 0/1  CrashLoopBackOff 194  16h\nkube-system gke-metadata-server-hfs6l 0/1  CrashLoopBackOff 1369  111d\nkube-system gke-metadata-server-hvtzn 0/1  CrashLoopBackOff 669  111d\nkube-system gke-metadata-server-swhbb 0/1  CrashLoopBackOff 30   136m\nkube-system gke-metadata-server-x4bl4 0/1  CrashLoopBackOff 7   15m\n```\n- Describe the crashing Pod to confirm that the cause was an out-of-memory eviction:```\nkubectl describe pod POD_NAME --namespace=kube-system | grep OOMKilled\n```Replace `` with the name of the Pod to check.\nTo restore functionality to the GKE metadata server, reduce the number of service accounts in your cluster to less than 3,000.\n### Workload identity federation for GKE fails to enable with DeployPatch failed error message\nGKE uses the Google Cloud-managed [Kubernetes Engine Service Agent](/kubernetes-engine/docs/how-to/service-accounts#gke-service-agents) to facilitate workload identity federation for GKE in your clusters. Google Cloud automatically grants this service agent the Kubernetes Engine Service Agent role ( `roles/container.serviceAgent` ) on your project when you enable the Google Kubernetes Engine API.\nIf you try to enable workload identity federation for GKE on clusters in a project where the service agent doesn't have the Kubernetes Engine Service Agent role, the operation fails with an error message similar to the following:\n```\nError waiting for updating GKE cluster workload identity config: DeployPatch failed\n```\nTo resolve this issue, try the following steps:\n- Check whether the service agent exists in your project and is configured correctly:```\ngcloud projects get-iam-policy PROJECT_ID \\\u00a0 \u00a0 --flatten=bindings \\\u00a0 \u00a0 --filter=bindings.role=roles/container.serviceAgent \\\u00a0 \u00a0 --format=\"value[delimiter='\\\\n'](bindings.members)\"\n```Replace `` with your Google Cloud project ID.If the service agent is configured correctly, the output shows the full identity of the service agent:```\nserviceAccount:service-PROJECT_NUMBER@container-engine-robot.iam.gserviceaccount.com\n```If the output doesn't display the service agent, you must grant it the Kubernetes Engine Service Agent role. To grant this role, complete the following steps.\n- Get your Google Cloud project number:```\ngcloud projects describe PROJECT_ID \\\u00a0 \u00a0 --format=\"value(projectNumber)\"\n```The output is similar to the following:```\n123456789012\n```\n- Grant the service agent the role:```\ngcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member=serviceAccount:service-PROJECT_NUMBER@container-engine-robot.iam.gserviceaccount.com \\\u00a0 \u00a0 --role=roles/container.serviceAgent \\\u00a0 \u00a0 --condition=None\n```Replace `` with your Google Cloud project number.\n- Try to enable workload identity federation for GKE again.## What's next\n[Cloud Customer Care](/kubernetes-engine/docs/getting-support)", "guide": "Google Kubernetes Engine (GKE)"}