{"title": "Docs - Automatically apply sensitivity tags in Data Catalog to files, databases, and BigQuery tables using Sensitive Data Protection and Dataflow", "url": "https://cloud.google.com/architecture/automatically-apply-sensitivity-tags-in-data-catalog", "abstract": "# Docs - Automatically apply sensitivity tags in Data Catalog to files, databases, and BigQuery tables using Sensitive Data Protection and Dataflow\nLast reviewed 2022-01-11 UTC\nThis document shows you how to use\n [Data Catalog](/data-catalog) \nwith an automated Dataflow pipeline to identify and apply\n [data sensitivity](/dlp/docs/sensitivity-risk-calculation) \ntags to your data in Cloud Storage files, relational databases (like MySQL, PostgreSQL, and others), and\n [BigQuery](/bigquery) \n.\nThis Dataflow pipeline uses [Sensitive Data Protection](/dlp) to detect sensitive data, like personally identifiable information (PII), and then it tags the findings in Data Catalog.\nThe solution described in this document builds on the architecture of the file-based tokenizing solution described in its companion document: [Automatically tokenize sensitive file-based data with Sensitive Data Protection, Cloud Key Management Service, and Dataflow](/community/tutorials/auto-data-tokenize) . The primary difference between the two documents is that this document describes a solution that also creates a Data Catalog entry with a schema of the source and data sensitivity tags for Sensitive Data Protection findings. It can also inspect relational databases using [Java database connectivity (JDBC)](https://wikipedia.org/wiki/Java_Database_Connectivity) connections.\nThis document is intended for a technical audience whose responsibilities include data security, data governance, data processing, or data analytics. This document assumes that you're familiar with data processing and data privacy, without being an expert. It also assumes that you have some familiarity with shell scripts and a basic knowledge of Google Cloud.", "content": "## ArchitectureThis architecture defines a pipeline that performs the following actions:- Extracts the data from a relational database using JDBC.\n- Samples the records using the database's`LIMIT`clause.\n- Processes records through the Cloud Data Loss Prevention API (part of Sensitive Data Protection) to identify sensitivity categories.\n- Saves the findings to a BigQuery table and Data Catalog.\nThe following diagram illustrates the actions that the pipeline performs:The solution uses JDBC connections to access relational databases. When using BigQuery tables as a data source, the solution uses the [BigQuery Storage API](/bigquery/docs/reference/storage) to improve load times.\nThe pipeline outputs the following files to Cloud Storage:- Avro schema (equivalent) of the source's schema\n- Detected [infoTypes data](/dlp/docs/infotypes-reference) for each of the input columns (`PERSON_NAME`,`PHONE_NUMBER`, and`STREET_ADDRESS`)\nThis solution uses [record flattening](/community/tutorials/auto-data-tokenize#concepts) to handle nested and repeated fields in records.\n## Objectives\n- Create Data Catalog tags and entity group\n- Deploy the sampling-and-identify pipeline\n- Create custom Data Catalog entity\n- Apply sensitivity tags to custom Data Catalog entity\n- Verify that sensitivity tags data is also in BigQuery\n## CostsIn this document, you use the following billable components of Google Cloud:- [Sensitive Data Protection](/dlp/pricing) \n- [Cloud SQL](/sql/pricing) \n- [Cloud Storage](/storage/pricing) \n- [Dataflow](/dataflow/pricing) \n- [Data Catalog](/dataplex/pricing#data-catalog-pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . ## Before you begin- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) At the bottom of the Google Cloud console, a [Cloud Shell](/shell/docs/how-cloud-shell-works) session starts and displays a command-line prompt. Cloud Shell is a shell environment  with the Google Cloud CLI  already installed and with values already set for  your current project. It can take a few seconds for the session to initialize.\n## Set up your environment\n- In Cloud Shell, clone the source repository and go to the directory for the cloned files:```\ngit clone https://github.com/GoogleCloudPlatform/auto-data-tokenize.gitcd auto-data-tokenize/\n```\n- Use a text editor to modify the `set_variables.sh` script to set the required environment variables. Ignore the other variables in the script. They aren't relevant in this document.```\n# The Google Cloud project to use:\nexport PROJECT_ID=\"PROJECT_ID\"\n# The Compute Engine region to use for running dataflow jobs and create a\n# temporary storage bucket:\nexport REGION_ID= \"REGION_ID\"\n# The Cloud Storage bucket to use as a temporary bucket for Dataflow:\nexport TEMP_GCS_BUCKET=\"CLOUD_STORAGE_BUCKET_NAME\"\n# Name of the service account to use (not the email address)\n# (For example, tokenizing-runner):\nexport DLP_RUNNER_SERVICE_ACCOUNT_NAME=\"DLP_RUNNER_SERVICE_ACCOUNT_NAME\"\n# Entry Group ID to use for creating/searching for Entries\n# in Data Catalog for non-BigQuery entries.\n# The ID must begin with a letter or underscore, contain only English\n# letters, numbers and underscores, and have 64 characters or fewer.\nexport\nDATA_CATALOG_ENTRY_GROUP_ID=\"DATA_CATALOG_ENTRY_GROUP_ID\"\n# The Data Catalog Tag Template ID to use\n# for creating sensitivity tags in Data Catalog.\n# The ID must contain only lowercase letters (a-z), numbers (0-9), or\n# underscores (_), and must start with a letter or underscore.\n# The maximum size is 64 bytes when encoded in UTF-8\nexport INSPECTION_TAG_TEMPLATE_ID=\"INSPECTION_TAG_TEMPLATE_ID\"\n```Replace the following:- : Your project ID.\n- : The region containing your storage bucket or buckets. Select a location that's in a [Data Catalog region](/data-catalog/docs/concepts/regions) .\n- : The name of your storage bucket.\n- : The name of your service account.\n- : The name of your non BigQuery data catalog entry group.\n- : The name you gave to your tag template for Data Catalog\n- Run the script to set the environment variables:```\nsource set_variables.sh\n```\n## Create resourcesThe architecture that's described in this document uses the following resources:- A service account to run Dataflow pipelines, enabling fine-grained access control\n- A Cloud Storage bucket to store temporary data and test data\n- A Data Catalog tag template to attach sensitivity tags to entries\n- A MySQL on Cloud SQL instance as the JDBC source\n### Create service accountsWe recommend that you run pipelines with fine-grained access control to improve access partitioning. If your project doesn't have a user-created service account, create one.- In Cloud Shell, create a service account to use as the user-managed controller service account for Dataflow:```\n\u00a0 gcloud iam service-accounts create ${DLP_RUNNER_SERVICE_ACCOUNT_NAME} \\\u00a0 --project=\"${PROJECT_ID}\" \\\u00a0 --description=\"Service Account for Sampling and Cataloging pipelines.\" \\\u00a0 --display-name=\"Sampling and Cataloging pipelines\"\n```\n- Create a custom role with required permissions for accessing Sensitive Data Protection, Dataflow, Cloud SQL, and Data Catalog:```\n\u00a0 export SAMPLING_CATALOGING_ROLE_NAME=\"sampling_cataloging_runner\"\u00a0 gcloud iam roles create ${SAMPLING_CATALOGING_ROLE_NAME} \\\u00a0 --project=\"${PROJECT_ID}\" \\\u00a0 --file=tokenizing_runner_permissions.yaml\n```\n- Apply the custom role and the Dataflow Worker role to the service account to let it to run as a Dataflow worker:```\n\u00a0 gcloud projects add-iam-policy-binding ${PROJECT_ID} \\\u00a0 --member=\"serviceAccount:${DLP_RUNNER_SERVICE_ACCOUNT_EMAIL}\" \\\u00a0 --role=projects/${PROJECT_ID}/roles/${SAMPLING_CATALOGING_ROLE_NAME}\u00a0 gcloud projects add-iam-policy-binding ${PROJECT_ID} \\\u00a0 --member=\"serviceAccount:${DLP_RUNNER_SERVICE_ACCOUNT_EMAIL}\" \\\u00a0 --role=roles/dataflow.worker\n```\n### Create the Cloud Storage bucket\n- In Cloud Shell, create a Cloud Storage bucket for storing test data and as a Dataflow staging location:```\ngsutil mb -p ${PROJECT_ID} -l ${REGION_ID} \"gs://${TEMP_GCS_BUCKET}\"\n```\n### Create a Data Catalog entry groupThe Data Catalog maintains a list of entries that represent Google Cloud resources or other resources. The entries are organized in entry groups. An implicit entry group exists for BigQuery ( `@bigquery` ). You must create entry groups for other types of resources. To learn more about Data Catalog entries, see [Surface files from Cloud Storage with fileset entries](/data-catalog/docs/how-to/filesets) .\nIn Data Catalog, an entry group is like a folder that contains entries. An entry represents a data asset.- In Cloud Shell, create a new entry group where the pipeline can add an entry for your MySQL table:```\ngcloud data-catalog entry-groups create \\\"${DATA_CATALOG_ENTRY_GROUP_ID}\" \\--project=\"${PROJECT_ID}\" \\--location=\"${REGION_ID}\"\n```\n### Create the inspection tag template\n- In Cloud Shell, create a Data Catalog tag template to enable tagging entries with sensitivity information with Sensitive Data Protection:```\ngcloud data-catalog tag-templates create ${INSPECTION_TAG_TEMPLATE_ID} \\--project=\"${PROJECT_ID}\" \u00a0\\--location=\"${REGION_ID}\" \\--display-name=\"Auto DLP sensitive categories\" \\--field=id=infoTypes,type=string,display-name=\"DLP infoTypes\",required=TRUE \\--field=id=inspectTimestamp,type=timestamp,display-name=\"Inspection run timestamp\",required=TRUE\n```\n### Create an inspection results table in BigQuery\n- In Cloud Shell, create a BigQuery table to store aggregated findings from Sensitive Data Protection:```\nbq mk --dataset \\--location=\"${REGION_ID}\" \\--project_id=\"${PROJECT_ID}\" \u00a0\\inspection_resultsbq mk --table \\--project_id=\"${PROJECT_ID}\" \u00a0\\inspection_results.SensitivityInspectionResults \\inspection_results_bigquery_schema.json\n```\n### Set up a MySQL on Cloud SQL instanceFor the data source, you use a Cloud SQL instance.- In Cloud Shell, instantiate a MySQL on Cloud SQL instance and load it with sample data:```\nexport SQL_INSTANCE=\"mysql-autodlp-instance\"export SQL_ROOT_PASSWORD=\"root1234\"gcloud sql instances create \"${SQL_INSTANCE}\" \\--project=\"${PROJECT_ID}\" \u00a0\\--region=\"${REGION_ID}\" \\--database-version=MYSQL_5_7 \\--root-password=\"${SQL_ROOT_PASSWORD}\"\n``` **Note:** It can take some time to start a new instance.\n- Save the database password in Secret Manager.The database password and other secret information shouldn't be stored or logged. [Secret Manager](/secret-manager) lets you store and retrieve such secrets securely.Store the MySQL database root password as a cloud secret:```\nexport SQL_PASSWORD_SECRET_NAME=\"mysql-password\"printf $SQL_ROOT_PASSWORD |gcloud secrets create \"${SQL_PASSWORD_SECRET_NAME}\" \\--data-file=- \\--locations=\"${REGION_ID}\" \\--replication-policy=\"user-managed\" \\--project=\"${PROJECT_ID}\"\n```\n### Copy test data to the Cloud SQL instanceThe test data is a demonstration dataset that contains 5,000 randomly generated first and last names and US-style phone numbers. The `demonstration-dataset` table contains four columns: `row_id` , `person_name` , `contact_type` , `contact_number` . You can also use your own dataset. If you use your own dataset, remember to adjust the suggested values in [Verify in BigQuery](#verify-in-bigquery) in this document. To copy the included demonstration dataset ( `contacts5k.sql.gz` ) to your Cloud SQL instance, do the following:- In Cloud Shell, copy the sample dataset to Cloud Storage for staging into Cloud SQL:```\ngsutil cp contacts5k.sql.gz gs://${TEMP_GCS_BUCKET}\n```\n- Create a new database in the Cloud SQL instance:```\nexport DATABASE_ID=\"auto_dlp_test\"gcloud sql databases create \"${DATABASE_ID}\" \\--project=\"${PROJECT_ID}\" \u00a0\\--instance=\"${SQL_INSTANCE}\"\n```\n- Grant the Storage Object Admin role to your Cloud SQL service account so it can access storage:```\nexport SQL_SERVICE_ACCOUNT=$(gcloud sql instances describe\"${SQL_INSTANCE}\" --project=\"${PROJECT_ID}\" | grepserviceAccountEmailAddress: | sed \"s/serviceAccountEmailAddress: //g\")gsutil iam ch \"serviceAccount:${SQL_SERVICE_ACCOUNT}:objectAdmin\" \\gs://${TEMP_GCS_BUCKET}\n```\n- Load the data into a new table:```\ngcloud sql import sql \"${SQL_INSTANCE}\" \\\"gs://${TEMP_GCS_BUCKET}/contacts5k.sql.gz\" \\--project=\"${PROJECT_ID}\" \u00a0\\--database=\"${DATABASE_ID}\"\n```To learn more about importing data into Cloud SQL, see [Best practices for importing and exporting data](/sql/docs/mysql/import-export#mysqldump) .\n## Compile modules\n- In Cloud Shell, compile the modules to build the executables for deploying the sampling-and-identify pipeline and the tokenize pipeline:```\n\u00a0./gradlew clean buildNeeded shadowJar -x test\n```Optionally, to run the unit and integration test, remove the `-x test` flag. If you don't already have `libncurses5` installed, install it in Cloud Shell with: `sudo apt-get install libncurses5` .\n## Run the sampling-and-identify pipelineThe sampling and Sensitive Data Protection identification pipeline performs the following tasks in the following order:- Extracts records from the provided source. For example, the Sensitive Data Protection identify method supports only flat tables, so the pipeline flattens the Avro, Parquet, or BigQuery records, since those records can contain nested and repeated fields.\n- Samples the individual columns for required samples, excluding`null`or empty values.\n- Identifies sensitive`infoTypes`data using Sensitive Data Protection, by batching the samples into batch sizes that are acceptable for Sensitive Data Protection (<500 Kb and <50,000 values).\n- Writes reports to Cloud Storage and to BigQuery for future reference.\n- Creates Data Catalog entities, when you provide tag template and entry group information. When you provide this information, the pipeline creates sensitivity tags for entries in Data Catalog against the appropriate columns.\n### Create a Dataflow Flex Template [Dataflow Flex Templates](/dataflow/docs/concepts/dataflow-templates) let you use the Google Cloud console, the Google Cloud CLI, or REST API calls to set up and run your pipelines on Google Cloud. This document provides instructions for Google Cloud console. Classic templates are staged as execution graphs on Cloud Storage, while Flex Templates bundle the pipeline as a container image in your project's [Container Registry](/container-registry) . Flex Templates let you decouple building and running pipelines, and integrate with orchestration systems for scheduled pipeline runs. For more information about Dataflow Flex Templates, see [Evaluating which template type to use](/dataflow/docs/concepts/dataflow-templates#comparing-templated-jobs) .\nDataflow Flex Templates separate the building and staging steps from the running steps. They do so by making it possible to launch a Dataflow pipeline from an API call, and from Cloud Composer, using the [DataflowStartFlexTemplateOperator](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataflow/index.html#airflow.providers.google.cloud.operators.dataflow.DataflowStartFlexTemplateOperator) module.- In Cloud Shell, define the location to store the template specification file that contains the information necessary to run the Dataflow job:```\nexportFLEX_TEMPLATE_PATH=\"gs://${TEMP_GCS_BUCKET}/dataflow/templates/sample-inspect-tag-pipeline.json\"exportFLEX_TEMPLATE_IMAGE=\"us.gcr.io/${PROJECT_ID}/dataflow/sample-inspect-tag-pipeline:latest\"\n```\n- Build the Dataflow Flex Template:```\ngcloud dataflow flex-template build \"${FLEX_TEMPLATE_PATH}\" \\--image-gcr-path=\"${FLEX_TEMPLATE_IMAGE}\" \\--service-account-email=\"${DLP_RUNNER_SERVICE_ACCOUNT_EMAIL}\" \\--sdk-language=\"JAVA\" \\--flex-template-base-image=JAVA11 \\--metadata-file=\"sample_identify_tag_pipeline_metadata.json\" \\--jar=\"build/libs/auto-data-tokenize-all.jar\" \\--env=\"FLEX_TEMPLATE_JAVA_MAIN_CLASS=\\\"com.google.cloud.solutions.autotokenize.pipeline.DlpInspectionPipeline\\\"\"\n```\n### Run the pipelineThe sampling and identification pipeline extracts the number of records that are specified by the `sampleSize` value. It then flattens each record and identifies the `infoTypes` fields using Sensitive Data Protection (to identify sensitive information types). The `infoTypes` values are counted and then aggregated by column name and by `infoType` field to build a sensitivity report.- In Cloud Shell, launch the sampling-and-identify pipeline to identify sensitive columns in the data source:```\nexportCLOUD_SQL_JDBC_CONNECTION_URL=\"jdbc:mysql:///${DATABASE_ID}?cloudSqlInstance=${PROJECT_ID}%3A${REGION_ID}%3A${SQL_INSTANCE}&socketFactory=com.google.cloud.sql.mysql.SocketFactory\"gcloud dataflow flex-template run \"sample-inspect-tag-`date +%Y%m%d-%H%M%S`\" \\\u00a0 --template-file-gcs-location \"${FLEX_TEMPLATE_PATH}\" \\\u00a0 --region \"${REGION_ID}\" \\\u00a0 --service-account-email \"${DLP_RUNNER_SERVICE_ACCOUNT_EMAIL}\" \\\u00a0 --staging-location \"gs://${TEMP_GCS_BUCKET}/staging\" \\\u00a0 --worker-machine-type \"n1-standard-1\" \\\u00a0 --parameters sampleSize=2000 \\\u00a0 --parameters sourceType=\"JDBC_TABLE\" \\\u00a0 --parameters inputPattern=\"Contacts\" \\\u00a0 --parameters reportLocation=\"gs://${TEMP_GCS_BUCKET}/auto_dlp_report/\" \\\u00a0 --parameters reportBigQueryTable=\"${PROJECT_ID}:inspection_results.SensitivityInspectionResults\" \\\u00a0 --parameters jdbcConnectionUrl=\"${CLOUD_SQL_JDBC_CONNECTION_URL}\" \\\u00a0 --parameters jdbcDriverClass=\"com.mysql.cj.jdbc.Driver\" \\\u00a0 --parameters jdbcUserName=\"root\" \\\u00a0 --parameters jdbcPasswordSecretsKey=\"projects/${PROJECT_ID}/secrets/${SQL_PASSWORD_SECRET_NAME}/versions/1\" \\\u00a0 --parameters ^:^jdbcFilterClause=\"ROUND(RAND() * 10) IN (1,3)\" \\\u00a0 --parameters dataCatalogEntryGroupId=\"projects/${PROJECT_ID}/locations/${REGION_ID}/entryGroups/${DATA_CATALOG_ENTRY_GROUP_ID}\" \\\u00a0 --parameters dataCatalogInspectionTagTemplateId=\"projects/${PROJECT_ID}/locations/${REGION_ID}/tagTemplates/${INSPECTION_TAG_TEMPLATE_ID}\"\n```\nThe `jdbcConnectionUrl` parameter specifies a JDBC database connection URL with username and password details. The details of building the exact connection URL depend on your database vendor and hosting partner. To understand details for connecting to Cloud SQL based relational databases, see [Connecting using Cloud SQL connectors](/sql/docs/mysql/connect-connectors) .\nThe pipeline constructs a query like `SELECT * FROM [TableName]` to read the table records for inspection.\nThis query can result in loading on the database and also on the pipeline, especially for a large table. Optionally, you can optimize your sample of the records you want to inspect on the database side. To do so, insert `jdbcFilterClause` as the `WHERE` clause of the query that appears in the code sample offered in the [Verify in BigQuery section](#verify-in-bigquery) later in this document.\nTo run a report, you can choose one or more of the following reporting sinks:- `reportLocation`to store the report in a Cloud Storage bucket\n- `report``BIGQUERY_TABLE`to store the report in a`BigQueryTable`\n- `dataCatalogEntryGroupId`to create and tag the entry in Data Catalog (omit this parameter if the`sourceType`is`BIGQUERY_TABLE`)\nThe pipeline supports the following source types. To determine the correct combination of `sourceType` and `inputPattern` arguments, use the options listed in the following table.\nIn this case, you only use the `JDBC_TABLE` table.\n| sourceType  | Data source                                                   | inputPattern    |\n|:---------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------|\n| JDBC_TABLE  | Relational databases (using JDBC)                                              | TABLE_NAME     |\n| AVRO   | Avro file in Cloud Storage. To select multiple files matching a pattern, you can use a single wildcard. The following pattern selects all files starting with prefix (data-): gs://my-bucket/path/to/folder/data-* | gs://LOCATION_OF_FILES  |\n| PARQUET  | Parquet file in Cloud Storage. To select multiple files matching a pattern, you can use a single wildcard. The following pattern selects all files starting with prefix (data-): gs://my-bucket/path/to/folder/data-* | gs://LOCATION_OF_FILES  |\n| BIGQUERY_TABLE | BigQuery table. Reads all the rows and then randomly samples using the pipeline.                                  | PROJECT_ID:=DATASET.=TABLE |\nThe pipeline detects all the [standard infoTypes](/dlp/docs/infotypes-reference) supported by Sensitive Data Protection. You can provide additional custom `infoTypes` by using the `--observableinfoTypes` parameter.The following diagram shows the Dataflow execution DAG. The DAG has two branches. Both branches start at `ReadJdbcTable` and conclude at `ExtractReport` . From there, reports are generated or data is stored.\n### Retrieve the reportThe sampling and identification pipeline outputs the following files:- Avro schema file (or a schema converted to Avro) of the source\n- One file for each of the sensitive columns with`infoType`information and counts\nTo retrieve the report, do the following:- In Cloud Shell, retrieve the report:```\nmkdir -p auto_dlp_report/gsutil -m cp\"gs://${TEMP_GCS_BUCKET}/auto_dlp_report/*.json\"auto_dlp_report/\n```\n- List all identified column names:```\ncat auto_dlp_report/col-*.json | jq .columnName\n```The output is as follows:`\"$.topLevelRecord.contact_number\" \"$.topLevelRecord.person_name\"`\n- View the details of an identified column with the `cat` command for the file:```\ncat auto_dlp_report/col-topLevelRecord-contact_number-00000-of-00001.json\n```The following is a snippet of the `cc` column:`{ \"columnName\": \"$.topLevelRecord.contact_number\", \"infoTypes\": [{ \"infoType\": \"PHONE_NUMBER\", \"count\": \"990\" }] }`- The`columnName`value is unusual because of the implicit conversion of a database row to an Avro record.\n- The`count`value varies based on the randomly selected samples during execution.\n### Verify sensitivity tags in Data CatalogThe sampling and identification pipeline creates a new entry and applies the sensitivity tags to the appropriate columns.- In Cloud Shell, retrieve the created entry for the Contacts table:```\ngcloud data-catalog entries describe Contacts \\\u00a0 --entry-group=${DATA_CATALOG_ENTRY_GROUP_ID} \\\u00a0 --project=\"${PROJECT_ID}\" \u00a0\\\u00a0 --location=\"${REGION_ID}\"\n```This command shows the details of the table, including its schema.\n- Show all the sensitivity tags that are attached to this entry:```\ngcloud data-catalog tags list --entry=Contacts\u00a0 --entry-group=${DATA_CATALOG_ENTRY_GROUP_ID} \\\u00a0 --project=\"${PROJECT_ID}\" \u00a0\\\u00a0 --location=\"${REGION_ID}\"\n```\n- Verify that the sensitivity tags are present on the following columns: `contact_number` , `person_name` .The `infoTypes` data identified by Sensitive Data Protection can contain some false types. For example, it can identify the `person_name` type as a `DATE` type, because some random `person_names` strings can be April, May, June, or others.The sensitivity tag details that are output are as follows:```\ncolumn: contact_number\nfields:\n infoTypes:\n displayName: DLP infoTypes\n stringValue: '[PHONE_NUMBER]'\n inspectTimestamp:\n displayName: Inspection run timestamp\n timestampValue: '2021-05-20T16:34:29.596Z'\nname: projects/auto-dlp/locations/asia-southeast1/entryGroups/sql_databases/entries/Contacts/tags/CbS0CtGSpZyJ\ntemplate: projects/auto-dlp/locations/asia-southeast1/tagTemplates/auto_dlp_inspection\ntemplateDisplayName: Auto DLP sensitive categories\n--column: person_name\nfields:\n infoTypes:\n displayName: DLP infoTypes\n stringValue: '[DATE, PERSON_NAME]'\n inspectTimestamp:\n displayName: Inspection run timestamp\n timestampValue: '2021-05-20T16:34:29.594Z'\nname: projects/auto-dlp/locations/asia-southeast1/entryGroups/sql_databases/entries/Contacts/tags/Cds1aiO8R0pT\ntemplate: projects/auto-dlp/locations/asia-southeast1/tagTemplates/auto_dlp_inspection\ntemplateDisplayName: Auto DLP sensitive categories\n```\n## Verify in BigQueryThe Dataflow pipeline appends the aggregated findings to the provided BigQuery table. The query prints the inspection results retrieved from the BigQuery table.- In Cloud Shell, check the results:```\nbq query \\\u00a0 --location=\"${REGION_ID}\" \u00a0\\\u00a0 --project=\"${PROJECT_ID}\" \u00a0\\\u00a0 --use_legacy_sql=false \\'SELECT\u00a0 \u00a0input_pattern AS table_name,\u00a0 \u00a0ColumnReport.column_name AS column_name,\u00a0 \u00a0ColumnReport.info_types AS info_types\u00a0FROM\u00a0 \u00a0`inspection_results.SensitivityInspectionResults`,\u00a0 \u00a0UNNEST(column_report) ColumnReport;\u00a0WHERE column_name=\"$.topLevelRecord.contact_number\"'\n```The output is as follows:```\n+------------+---------------------------------+----------------------------------------------+| table_name | \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 column_name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 | \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0info_types \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0|+------------+---------------------------------+----------------------------------------------+| Contacts \u00a0 | $.topLevelRecord.contact_number | [{\"info_type\":\"PHONE_NUMBER\",\"count\":\"990\"}] |+------------+---------------------------------+----------------------------------------------+\n```## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Read the companion document about a similar solution that uses files as input: [Automatically tokenize sensitive file-based data with Sensitive Data Protection, Cloud Key Management Service, and Dataflow](/community/tutorials/auto-data-tokenize) .\n- Learn about [inspecting storage and databases for sensitive data](/dlp/docs/inspecting-storage) .\n- Learn about handling [de-identification and re-identification of PII in large-scale datasets using Sensitive Data Protection](/solutions/de-identification-re-identification-pii-using-cloud-dlp) .\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Docs"}