{"title": "Docs - Optimize networking and API performance", "url": "https://cloud.google.com/architecture/framework/performance-optimization/networking", "abstract": "# Docs - Optimize networking and API performance\nLast reviewed 2024-01-04 UTC\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides recommendations to help you optimize the performance of your networking resources and APIs in Google Cloud.\n", "content": "## Network Service Tiers\nNetwork Service Tiers lets you optimize the network cost and performance of your workloads. You can choose from the following tiers:\n- **Premium Tier** uses Google's highly reliable global backbone to help you achieve minimal packet loss and latency. Traffic enters and leaves the Google network at a global edge point of presence (PoP) that's close to your end user. We recommend using Premium Tier as the default tier for optimal performance. Premium Tier supports both regional and global external IP addresses for VMs and load balancers.\n- **Standard Tier** is available only for resources that use regional external IP addresses. Traffic enters and leaves the Google network at an edge PoP that's closest to the Google Cloud location where your workload runs. The pricing for Standard Tier is lower than Premium Tier. Standard Tier is suitable for traffic that isn't sensitive to packet loss and that doesn't have low latency requirements.\nYou can view the network latency for Standard Tier and Premium Tier for each cloud region in the [Network Intelligence Center Performance Dashboard](/network-intelligence-center/docs/performance-dashboard/concepts/overview) .\n## Jumbo frames\nVirtual Private Cloud (VPC) networks have a default [maximum transmission unit(MTU)](https://www.wikipedia.org/wiki/Maximum_transmission_unit) of 1460 bytes. However, you can configure your VPC networks to to support an MTU of up to `8896` (jumbo frames).\nWith a higher MTU, the network needs fewer packets to send the same amount of data, thus reducing the bandwidth used up by TCP/IP headers. This leads to a higher effective bandwidth for the network.\nFor more information about intra-VPC MTU and the maximum MTU of other connections, see the [Maximum transmission unit](/vpc/docs/mtu) page in the VPC documentation.\n## VM performance\nCompute Engine VMs have a maximum egress bandwidth that in part depends upon the machine type. One aspect of choosing an appropriate machine type is to consider how much traffic you expect the VM to generate.\nThe [Network bandwidth](/compute/docs/network-bandwidth) page contains a discussion and table of network bandwidths for Compute Engine machine types.\nIf your inter-VM bandwidth requirements are very high, consider VMs that support [Tier_1 networking](/compute/docs/networking/configure-vm-with-high-bandwidth-configuration) .\n## Cloud Load Balancing\nThis section provides best practices to help you optimize the performance of your [Cloud Load Balancing](/load-balancing/docs/load-balancing-overview) instances.\n### Deploy applications close to your users\nProvision your application backends close to the location where you expect user traffic to arrive at the load balancer. The closer your users or client applications are to your workload servers, the lower the network latency between the users and the workload. To minimize latency to clients in different parts of the world, you might have to deploy the backends in multiple regions. For more information, see [Best practices for Compute Engine regions selection](/solutions/best-practices-compute-engine-region-selection) .\n### Choose an appropriate load balancer type\nThe [type of load balancer](/load-balancing/docs/choosing-load-balancer#lb-summary) that you choose for your application can determine the latency that your users experience. For information about measuring and optimizing application latency for different load balancer types, see [Optimizing application latency with load balancing](/load-balancing/docs/tutorials/optimize-app-latency) .\n### Enable caching\nTo accelerate content serving, enable caching and [Cloud CDN](/cdn/docs/overview) as part of your default external HTTP load balancer configuration. Make sure that the backend servers are [configured](/cdn/docs/troubleshooting-steps#responses-not-cached) to send the response headers that are necessary for static responses to be cached.\n### Use HTTP when HTTPS isn't necessary\nGoogle automatically [encrypts traffic between proxy load balancers and backends](/load-balancing/docs/ssl-certificates/encryption-to-the-backends#encryption-to-backends) at the packet level. Packet-level encryption makes Layer 7 encryption using HTTPS between the load balancer and the backends redundant for most purposes. Consider using HTTP rather than HTTPS or HTTP/2 for traffic between the load balancer and your backends. By using HTTP, you can also reduce the CPU usage of your backend VMs. However, when the backend is an internet network endpoint group (NEG), use HTTPS or HTTP/2 for traffic between the load balancer and the backend. This helps ensure that your traffic is secure on the public internet. For optimal performance, we recommend benchmarking your application's traffic patterns.\n## Network Intelligence Center\nGoogle Cloud [Network Intelligence Center](/network-intelligence-center) provides a comprehensive view of the performance of the Google Cloud network across all regions. Network Intelligence Center helps you determine whether latency issues are caused by [problems in your project or in the network](/network-intelligence-center/docs/performance-dashboard/concepts/use-cases-google-cloud#gpd-current-diagnostics) . You can also use this information to [select the regions and zones](/network-intelligence-center/docs/performance-dashboard/concepts/use-cases-google-cloud#workload_optimization_planning_for_performance) where you should deploy your workloads to optimize network performance.\nUse the following tools provided by Network Intelligence Center to monitor and analyze network performance for your workloads in Google Cloud:\n- [Performance Dashboard](/network-intelligence-center/docs/performance-dashboard/concepts/overview) shows latency between Google Cloud regions and between individual regions and locations on the internet. Performance Dashboard can help you determine where to place workloads for best latency and help determine when an application issue might be due to underlying network issues.\n- [Network Topology](/network-intelligence-center/docs/network-topology/concepts/overview) shows a visual view of your Virtual Private Cloud (VPC) networks, hybrid connectivity with your on-premises networks, and connectivity to Google-managed services. Network Topology provides real-time operational metrics that you can use to analyze and understand network performance and identify unusual traffic patterns.\n- [Network Analyzer](/network-intelligence-center/docs/network-analyzer/overview) is an automatic configuration monitoring and diagnostics tool. It verifies VPC network configurations for firewall rules, routes, configuration dependencies, and connectivity for services and applications. It helps you identify network failures, and provides root cause analysis and recommendations. Network Analyzer provides prioritized insights to help you analyze problems with network configuration, such as high utilization of IP addresses in a subnet.## API Gateway and Apigee\nThis section provides recommendations to help you optimize the performance of the APIs that you deploy in Google Cloud by using [API Gateway](/api-gateway) and [Apigee](/apigee) .\nAPI Gateway lets you create and manage APIs for Google Cloud serverless backends, including Cloud Functions, Cloud Run, and App Engine. These services are managed services, and they scale automatically. But as the applications that are deployed on these services scale, you might need to increase the [quotas and rate limits](/api-gateway/docs/quotas) for API Gateway.\n**Note:** Exposing your applications or infrastructure to more traffic can cause performance bottlenecks at the next layer in the application stack.\nApigee provides the following analytics dashboards to help you monitor the performance of your managed APIs:\n- [API Proxy Performance Dashboard](/apigee/docs/api-platform/analytics/api-proxy-performance-dashboard) : Monitor API proxy traffic patterns and processing times.\n- [Target Performance Dashboard](/apigee/docs/api-platform/analytics/endpoint-performance-dashboard) : Visualize traffic patterns and performance metrics for API proxy backend targets.\n- [Cache Performance Dashboard](/apigee/docs/api-platform/analytics/cache-performance-dashboard) : Monitor performance metrics for Apigee cache, such as average cache-hit rate and average time in cache.\nIf you use [Apigee Integration](/apigee/docs/api-platform/integration/what-is-apigee-integration) , consider the [system-configuration limits](/apigee/docs/api-platform/integration/system-limits) when you build and manage your integrations.\n## What's next\nReview the best practices for optimizing the performance of your compute, storage, database, and analytics resources:\n- [Optimize compute performance](/architecture/framework/performance-optimization/compute) .\n- [Optimize storage performance](/architecture/framework/performance-optimization/storage) .\n- [Optimize database performance](/architecture/framework/performance-optimization/databases) .\n- [Optimize analytics performance](/architecture/framework/performance-optimization/analytics) .", "guide": "Docs"}