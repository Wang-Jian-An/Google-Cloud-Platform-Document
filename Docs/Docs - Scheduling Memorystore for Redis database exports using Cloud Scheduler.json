{"title": "Docs - Scheduling Memorystore for Redis database exports using Cloud Scheduler", "url": "https://cloud.google.com/architecture/scheduling-memorystore-for-redis-database-exports-using-cloud-scheduler", "abstract": "# Docs - Scheduling Memorystore for Redis database exports using Cloud Scheduler\nLast reviewed 2023-03-18 UTC\nThis tutorial shows how to use [Cloud Scheduler](/scheduler) and [Cloud Functions](/functions) to automatically export a [Memorystore for Redis](/memorystore/docs/redis) database to [Cloud Storage](/storage) . Having database exports on Cloud Storage lets you create a robust, diverse disaster recovery plan. For example, you can export to a different region, and import to other Memorystore for Redis instances.", "content": "## ArchitectureThis tutorial includes the following Google Cloud components:- [Cloud Scheduler jobs](/scheduler/docs) : Jobs to make calls on a set schedule to start the database export.\n- [Cloud Functions](/functions/docs/concepts/overview) : Functions to export the data from Memorystore to Cloud Storage.\n- [Pub/Sub messages](/pubsub/docs/overview) : Messages sent and received for each data export event.\n- [Cloud Storage buckets](/storage/docs/buckets) : Bucket to store the exported data.\n- [Memorystore for Redis](/memorystore/docs/redis) : Source database to export the data from.\nA Cloud Scheduler job posts a message on a Pub/Sub topic with information about the Memorystore instance ID, the project ID, the region where it's located, and the Cloud Storage location at which to store the backup. This event triggers a Cloud Function that gets this payload and starts a database export on Memorystore for Redis through its API. The database generates the export and saves it to Cloud Storage. The following diagram shows this workflow.## Objectives\n- Create a [Cloud Storage bucket and a Memorystore instance](#creating_a_cloud_storage_bucket_and_a_memorystore_instance) .\n- Create a [Pub/Sub](/pubsub/pricing) topic, a Cloud Function, and a Cloud Scheduler job.\n- Trigger the export of database data by running the Cloud Scheduler job manually.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Cloud Scheduler](/scheduler/pricing) \n- [Cloud Functions](/functions/pricing) \n- [Pub/Sub](/pubsub/pricing) \n- [Cloud Storage](/storage/pricing) \n- [Memorystore for Redis](/memorystore/docs/redis/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- In the Google Cloud console, on the project selector page,   select or [create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . **Note** : If you don't plan to keep the  resources that you create in this procedure, create a project instead of  selecting an existing project. After you finish these steps, you can  delete the project, removing all resources associated with the project. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Enable the Memorystore for Redis, Cloud Functions, Cloud Scheduler, and Cloud Build APIs. [Enable the APIs](https://console.cloud.google.com/flows/enableapi?apiid=compute.googleapis.com,redis.googleapis.com,cloudfunctions.googleapis.com,cloudscheduler.googleapis.com,cloudbuild.googleapis.com) Throughout this tutorial, you run all commands from Cloud Shell.## Set up your environmentTo get started, you first configure your environment and then create custom roles that have the permissions needed for this tutorial.- In Cloud Shell, configure the following environment variables:```\nexport PROJECT_ID=`gcloud config get-value project`export DEMO=\"mem-exporter\"export BUCKET_NAME=${USER}-mem-$(date +%s)export MEM_INSTANCE=\"${DEMO}-instance\"export FUNCTION_NAME=\"${DEMO}-gcf\"export PUBSUB_TOPIC=\"${DEMO}-topic\"export SCHEDULER_JOB=\"${DEMO}-job\"export MEM_EXPORT_ROLE=\"memExporter\"export STORAGE_ROLE=\"simpleStorageRole\"export REGION=\"us-central1\"\n```\n- Create two custom roles that have only the permissions needed for this tutorial:```\ngcloud iam roles create ${STORAGE_ROLE} --project=${PROJECT_ID} \\\u00a0 \u00a0 --title=\"Simple Storage Role\" \\\u00a0 \u00a0 --description=\"Grant permissions to view and create objects in Cloud Storage\" \\\u00a0 \u00a0 --permissions=\"storage.objects.create,storage.buckets.get\"gcloud iam roles create ${MEM_EXPORT_ROLE} --project=${PROJECT_ID} \\\u00a0 \u00a0 --title=\"Memorystore Exporter Role\" \\\u00a0 \u00a0 --description=\"Grant permissions to export data from a Memorystore instance to a Cloud Storage bucket\" \\\u00a0 \u00a0 --permissions=\"redis.instances.export\"\n```These roles reduce the scope of access of Cloud Functions and Memorystore service accounts, following the [principle of least privilege](/blog/products/identity-security/dont-get-pwned-practicing-the-principle-of-least-privilege) .\n## Create a Cloud Storage bucket and a Memorystore instanceIn this section, you first create a Cloud Storage bucket and a Memorystore for Redis instance. Then you populate the Memorystore with sample data.\n### Create a Cloud Storage bucketYou use the `gsutil` command-line tool to create a Cloud Storage bucket.- Create a Cloud Storage bucket where you want to save the data exports:```\ngsutil mb -l ${REGION} gs://${BUCKET_NAME}\n```\n### Create a Memorystore instance and grant permissions to its service accountNext, you create a Memorystore instance and grant its service account the permissions to export data to Cloud Storage.- Create a Memorystore for Redis 4 instance:```\ngcloud redis instances create ${MEM_INSTANCE} --size=1 --region=${REGION}\n```This operation takes a few minutes to complete.\n- Verify that the Memorystore instance is `READY` :```\ngcloud redis instances list --region=${REGION}\n```The output looks similar to the following:```\nINSTANCE_NAME VERSION REGION  TIER SIZE_GB HOST   PORT NETWORK RESERVED_IP  STATUS CREATE_TIME\nredis-instance REDIS_4_0 us-central1 BASIC 1  10.61.20.131 6379 default 10.61.20.128/29 READY 2020-04-23T18:38:54\n```\n- Grant your Memorystore service account the permissions to export data to Cloud Storage with the custom `Simple Storage` role that you created earlier:```\nexport MEM_SA=$(gcloud redis instances describe ${MEM_INSTANCE} --region ${REGION} \\\u00a0 \u00a0 --project ${PROJECT_ID} \\\u00a0 \u00a0 --format \"value(persistenceIamIdentity)\")gsutil iam ch ${MEM_SA}:projects/${PROJECT_ID}/roles/${STORAGE_ROLE} gs://${BUCKET_NAME}\n```\n## Create the scheduled data export taskIn this section, you create a custom service account and bind it to the custom Redis role that you create. You then create a Pub/Sub topic that's used to trigger the execution of a Cloud Function. You also create a Cloud Scheduler job to periodically execute the data export function.\n### Create a service account for the Cloud FunctionThe first step is to create a service account and bind it to the roles.- Create an IAM service account for the Cloud Function to use and save it to the variable:```\ngcloud iam service-accounts create ${FUNCTION_NAME} \\\u00a0 \u00a0 --display-name=\"Service Account for GCF and Memorystore\"export GCF_SA=$(gcloud iam service-accounts list --filter=\"${FUNCTION_NAME}\" --format=\"value(email)\")\n```\n- Grant the service account access to the custom `Memorystore Exporter` role allowing it to request Memorystore exports:```\ngcloud projects add-iam-policy-binding ${PROJECT_ID} \\\u00a0 \u00a0 --member=\"serviceAccount:${GCF_SA}\" \\\u00a0 \u00a0 --role=\"projects/${PROJECT_ID}/roles/${MEM_EXPORT_ROLE}\"\n```\n- Grant the service-account access to the custom `Simple Storage` role```\ngsutil iam ch \\\u00a0 \u00a0 serviceAccount:${GCF_SA}:projects/${PROJECT_ID}/roles/${STORAGE_ROLE} \\\u00a0 \u00a0 gs://${BUCKET_NAME}\n```\n### Create a Pub/Sub topicThe next step is to create a Pub/Sub topic that's used to trigger the Cloud Function that interacts with the Memorystore database.- Create the Pub/Sub topic:```\ngcloud pubsub topics create ${PUBSUB_TOPIC}\n```\n### Create a Cloud FunctionNext, you create the Cloud Function.- Create a folder for the Cloud Function code:```\nmkdir scheduler_gcf_code && cd scheduler_gcf_code\n```\n- Create a `main.py` file by pasting the following into Cloud Shell:```\ncat <<EOF \u00a0> main.pyimport base64import loggingimport jsonfrom datetime import datetimefrom httplib2 import Httpfrom googleapiclient import discoveryfrom googleapiclient.errors import HttpErrorfrom oauth2client.client import GoogleCredentialsdef main(event, context):\u00a0 \u00a0 pubsub_message = json.loads(base64.b64decode(event['data']).decode('utf-8'))\u00a0 \u00a0 credentials = GoogleCredentials.get_application_default()\u00a0 \u00a0 service = discovery.build('redis', 'v1beta1', http=credentials.authorize(Http()), cache_discovery=False)\u00a0 \u00a0 datestamp = datetime.now().strftime(\"%Y%m%d%H%M\") # format timestamp: YearMonthDayHourMinute\u00a0 \u00a0 instance_name=pubsub_message['name'].split(\"/\")[-1]\u00a0 \u00a0 uri = f\"{pubsub_message['gs']}/backup-{instance_name}-{datestamp}.rdb\"\u00a0 \u00a0 request_body = {\u00a0 \u00a0 \u00a0 \u00a0 \"outputConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gcsDestination\" : {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"uri\": uri\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 \u00a0 try:\u00a0 \u00a0 \u00a0 \u00a0 request = service.projects().locations().instances().export(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name=pubsub_message['name'],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 body=request_body\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 response = request.execute()\u00a0 \u00a0 except HttpError as err:\u00a0 \u00a0 \u00a0 \u00a0 logging.error(f\"Could NOT run backup. Reason: {err}\")\u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 \u00a0 logging.info(f\"Backup task status: {response}\")EOF\n```\n- Create a `requirements.txt` file by pasting the following into Cloud Shell:```\ncat <<EOF > requirements.txtgoogle-api-python-clientOauth2clientEOF\n```\n- Deploy the code.```\ngcloud functions deploy ${FUNCTION_NAME} \\\u00a0 \u00a0 --trigger-topic=${PUBSUB_TOPIC} \\\u00a0 \u00a0 --runtime=python37 \\\u00a0 \u00a0 --entry-point=main \\\u00a0 \u00a0 --service-account=${FUNCTION_NAME}@${PROJECT_ID}.iam.gserviceaccount.com \\\u00a0 \u00a0 --ingress-settings=internal-and-gclb\n```\n### Create a Cloud Scheduler jobFinally, you create a Cloud Scheduler job to periodically execute the data export function.- Save the Memorystore complete name into a variable:```\nexport MEM_NAME=$(gcloud redis instances describe ${MEM_INSTANCE} --region ${REGION} --format \"value(name)\")\n```\n- Create a Cloud Scheduler job to periodically execute the data export function:```\ngcloud scheduler jobs create pubsub ${SCHEDULER_JOB} \\\u00a0 \u00a0 --schedule='0 23 * * *' --topic=${PUBSUB_TOPIC} \\\u00a0 \u00a0 --message-body='{\"name\":'\\\"${MEM_NAME}\\\"',\"gs\":'\\\"gs://${BUCKET_NAME}\\\"'}' \\\u00a0 \u00a0 --time-zone='America/Los_Angeles' --location=${REGION}\n```This job is scheduled to run at 11 PM Pacific time every day.The message body contains the name of the Memorystore instance to be exported, and the destination Cloud Storage bucket.\n## Test your solutionThe final step is to test your solution. You start by running the Cloud Scheduler job.- Run the Cloud Scheduler job manually to trigger a Memorystore export of your database.```\ngcloud scheduler jobs run ${SCHEDULER_JOB} --location=${REGION}\n```\n- List the operations performed on the Memorystore instance, and verify that there's an operation of type `EXPORT` :```\ngcloud redis operations list --region=${REGION} --filter=\"${MEM_INSTANCE}\"\n```The following output example shows an export job with a `DONE` status of `True` to indicate that it has completed. If the `DONE` status shows `False` , this indicates that the job is still processing; wait a minute and then re-run the previous command.```\nOPERATION_NAME           REGION  TYPE TARGET     DONE CREATE_TIME   DURATION\noperation-1592329364987-5a837122a600c-b22c2703-5077c6b7 us-central1 export mem-exporter-instance True 2020-06-16T17:42:45 16S\n```\n- Check the Cloud Storage bucket to see if the export file file was created:```\ngsutil ls -l gs://${BUCKET_NAME}/*.rdb\n```You see a file named `backup-` `` `-` `` `.rdb` .\n## Clean upYou can avoid incurring charges to your Google Cloud account for the resources used in this tutorial by following these steps. The easiest way to eliminate billing is to delete the project you created for the tutorial.- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Learn how to [schedule compute instances with Cloud Scheduler](/scheduler/docs/start-and-stop-compute-engine-instances-on-a-schedule) .\n- Learn about [scheduling Cloud SQL database exports using Cloud Scheduler](https://cloud.google.com/blog/topics/developers-practitioners/scheduling-cloud-sql-exports-using-cloud-functions-and-cloud-scheduler) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Docs"}