{"title": "Docs - Run a hybrid render farm proof of concept", "url": "https://cloud.google.com/architecture/running-hybrid-render-farm-poc", "abstract": "# Docs - Run a hybrid render farm proof of concept\nThis document shows how to run a proof of concept (PoC) to build a hybrid render farm on Google Cloud. This document is a companion to [Build a hybrid render farm](/solutions/building-a-hybrid-render-farm) and is designed to facilitate testing and benchmarking rendering for animation, film, commercials, or video games on Google Cloud.\nYou can run a PoC for your hybrid render farm on Google Cloud if you narrow the scope of your tests to only the essential components. In contrast to architecting an entire end-to-end solution, consider the following purposes of a PoC:- Determine how to reproduce your on-premises rendering environment on the cloud.\n- Measure differences in rendering and networking performance between on-premises render workers and cloud instances.\n- Determine cost differences between on-premises and cloud workloads.\nOf lesser importance are the following tasks that you can postpone or even eliminate from a PoC:- Determine how assets are synchronized (if at all) between your facility and the cloud.\n- Determine how to deploy jobs to cloud render workers by using queue management software.\n- Determine the best way to connect to Google Cloud.\n- Measure latency between your facility and Google data centers.\n", "content": "## ConnectivityFor a rendering PoC, you don't need [enterprise-grade](/hybrid-connectivity) connectivity to Google. A connection over the public internet is sufficient. Connection speed, latency, and bandwidth are of secondary importance to rendering performance.\nYou can treat connectivity as a separate PoC because arranging [Dedicated Interconnect](/network-connectivity/docs/interconnect/concepts/dedicated-overview) or [Partner Interconnect](/network-connectivity/docs/interconnect/concepts/partner-overview) for a PoC can take time, and can be performed concurrently with rendering testing.## Objectives\n- Create a Compute Engine instance and customize it to serve as a render worker.\n- Create a custom image.\n- Deploy a render worker.\n- Copy assets to the render worker.\n- Perform render benchmarks.\n- Copy test renders from the render worker to your local workstation for evaluation.\n## Costs\nWhen you estimate your projected usage, estimate the difference in cost between an on-premises and cloud-based render workers.\nIn this document, you use the following billable components of Google Cloud:- [Compute Engine](/compute/pricing) \n- [Cloud Storage](/storage/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) At the bottom of the Google Cloud console, a [Cloud Shell](/shell/docs/how-cloud-shell-works) session starts and displays a command-line prompt. Cloud Shell is a shell environment  with the Google Cloud CLI  already installed and with values already set for  your current project. It can take a few seconds for the session to initialize.\n- In this document, you mostly use Cloud Shell to perform the steps, but copying data from an on-premises machine to Cloud Storage requires that you have the Google Cloud CLI running on that machine.## Setting up your environment\n- In Cloud Shell, set the Compute Engine zone:```\ngcloud config set compute/zone [ZONE]\n```Where `[ZONE]` is the zone where all of your resources are created.\n## Deploying an instanceFor your PoC, you might want to recreate your on-premises render worker hardware. While Google Cloud offers a number of CPU platforms that might match your own hardware, the architecture of a cloud-based virtual machine is different from a bare-metal render blade in an on-premises render farm.\nOn Google Cloud, resources are virtualized and independent of other resources. Virtual machines (instances) are composed of the following major components:- Virtual CPUs (vCPUs)\n- Memory (RAM)\n- Disks- Boot disk and guest OS\n- Additional storage disks\n- NVIDIA Tesla GPUs (optional)\nYou can also control other aspects of resources, such as networking, firewall rules, and user access. But for the purposes of your PoC, you need only pay attention to the four components mentioned previously.\n### Create an instance\n- In Cloud Shell, create your prototype render worker instance:```\ngcloud compute instances create [INSTANCE_NAME] \\\u00a0 \u00a0 --machine-type [MACHINE_TYPE] \\\u00a0 \u00a0 --image-project [IMAGE_PROJECT] \\\u00a0 \u00a0 --image-family [IMAGE_FAMILY] \\\u00a0 \u00a0 --boot-disk-size [SIZE]\n```Where:- `[INSTANCE_NAME]`is a name of your instance.\n- `[MACHINE_TYPE]`is either a [predefined machine type](/compute/docs/machine-types#predefined_machine_types) or a [custom machine type](/compute/docs/instances/creating-instance-with-custom-machine-type) using the format`custom-[NUMBER_OF_CPUS]-[NUMBER_OF_MB]`where you define the number of vCPUs and amount of memory for the machine type.\n- `[IMAGE_PROJECT]`is the [image project](/compute/docs/images#os-compute-support) of that image family.\n- `[IMAGE_FAMILY]`is an optional flag that specifies which [image family](/compute/docs/images#image_families) this image belongs to.\n- `[SIZE]`is the size of the boot disk in GB.\nFor example:```\ngcloud compute instances create render-worker-proto \\\u00a0 \u00a0 --machine-type custom-24-32768 \\\u00a0 \u00a0 --image-project centos-cloud \\\u00a0 \u00a0 --image-family centos-7 \\\u00a0 \u00a0 --boot-disk-size 100\n```The preceding command creates a CentOS 7 instance with 24 vCPUs, 32 GB RAM, and a standard 100 GB boot disk. The instance is created in the zone you set earlier as your default compute zone.\nYou can choose to create a VM of any size, up to 96 vCPUs (if you need more, [try the ultramem types](/blog/products/gcp/introducing-ultramem-google-compute-engine-machine-types) ), over 624 GB RAM, or multiple NVIDIA Tesla GPUs. The possibilities are endless, but be careful not to overprovision; you want to architect a cost-effective, scalable, cloud-based render farm, suitable for jobs of any size.## Logging on to an instance\n- In Cloud Shell, connect to your instance by using SSH:```\ngcloud compute ssh [INSTANCE_NAME]\n```\n- Install and license software on your instance as you would with an on-premises render worker.\n## Building your default imageUnless you have custom software to test that requires things like a custom Linux kernel or older OS versions, we recommend you start with one of our [public disk images](/compute/docs/instances/create-start-instance#publicimage) and add the software you're going to use.\nIf you choose to import your own image, you need to [configure](/compute/docs/images/configuring-imported-images) this image by installing additional libraries to enable your guest OS to communicate with Google Cloud.\n### Set up your render worker\n- In Cloud Shell, on the instance you created earlier, set up your render worker as you would your on-premises worker by installing your software and libraries.\n- Stop the instance:```\ngcloud compute instances stop [INSTANCE_NAME]\n```\n### Create a custom image\n- In Cloud Shell, determine the name of your VM's boot disk:```\ngcloud compute instances describe [INSTANCE_NAME]\n```The output contains the name of your instance's boot disk:```\nmode: READ_WRITE\nsource:https://www.googleapis.com/compute/v1/projects/[PROJECT]/zones/[ZONE]/disks/[DISK_NAME]\n```Where:- `[PROJECT]`is the name of your Google Cloud project.\n- `[ZONE]`is the zone where the disk is located.\n- `[DISK_NAME]`is the name of the boot disk attached to your instance. The disk name is typically the same (or similar) to your instance name.\n- Create an image from your instance:```\ngcloud compute images create [IMAGE_NAME] \\\u00a0 \u00a0 --source-disk [DISK_NAME] \\\u00a0 \u00a0 --source-disk-zone [ZONE]\n```Where:- `[IMAGE_NAME]`is a name for the new image.\n- `[DISK_NAME]`is the disk from which you want to create the new image.\n- `[ZONE]`is the zone where the disk is located.## Deploying a render workerNow that you have a custom image ready with the OS, software, and libraries you need, you can deploy a render worker instance using your custom image, rather than using a public image.- In Cloud Shell, create a render worker instance. Add the scope `devstorage.read_write` so that you can write to Cloud Storage from this instance.```\ngcloud compute instances create [WORKER_NAME] \\\u00a0 \u00a0 --machine-type [MACHINE_TYPE] \\\u00a0 \u00a0 --image [IMAGE_NAME] \\\u00a0 \u00a0 --scopes https://www.googleapis.com/auth/devstorage.read_write \\\u00a0 \u00a0 --boot-disk-size [SIZE]\n```Where `[WORKER_NAME]` is a name for the render worker.\n## Licensing softwareYou can use your on-premises license server to provide licenses during a PoC, because you don't need to reissue licenses for new cloud-based license servers. To securely connect to your on-premises license server from your cloud instance, create a [firewall rule](/vpc/docs/firewalls) that only allows traffic over the necessary ports. This firewall rule also allows traffic from the IP address of your on-premises internet gateway or of the license server itself.\nYou might need to configure your facility's internet gateway to allow traffic from your Google Cloud instance to reach your on-premises license server.\n### Use your on-premises license serverYou can allow traffic into your Virtual Private Cloud (VPC) network by creating a firewall rule.- In Cloud Shell, create the firewall rule:```\ngcloud compute firewall-rules create [RULE_NAME] \\\u00a0 \u00a0--direction=INGRESS \\\u00a0 \u00a0--priority=1000 \\\u00a0 \u00a0--network=default \\\u00a0 \u00a0--action=ALLOW \\\u00a0 \u00a0--rules=[PROTOCOL]:[PORT] \\\u00a0 \u00a0--source-ranges=[IP_ADDRESS]\n```\nWhere:- `[RULE_NAME]`is a name for the firewall rule.\n- `[PROTOCOL]`is the protocol for the traffic.\n- `[PORT]`is the port over which the traffic travels.\n- `[IP_ADDRESS]`is the IP address of your on-premises license server.\n### Use a cloud-based license serverA cloud-based license server doesn't require connectivity to your on-premises network, and runs on the same VPC network as your render worker. Because license serving is a relatively lightweight task, a small instance (2-4 vCPUs, 6-8 GB RAM) can handle the workload of serving licenses to a handful of render workers.\nDepending on the type of software you need to license, you might need to re-key your licenses to a unique hardware ID number, such as the MAC address of the license server. Other license managers can validate software licenses from any internet-connected host. There are many license managers, so consult your product licensing documentation for instructions.\n### Allow communication between instancesRender workers and license server instances need to communicate with each other. The firewall rule `default-allow-internal` allows all instances in your project to communicate with each other. This firewall rule is created when you create a new project. If you're using a new project, you can skip this section. If you're using an existing project, you need to test if the firewall rule is still in your Google Cloud project.- In Cloud Shell, check to see if the firewall rule is in your project:```\ngcloud compute firewall-rules list \\\u00a0 \u00a0 --filter=\"name=default-allow-internal\"\n```If the firewall rule is in your project, you see the following output:```\nNAME     NETWORK DIRECTION PRIORITY ALLOW DENY     DISABLED\ndefault-allow-internal default INGRESS 65534m tcp:0-65535,udp:0-65535,icmp False\n```If the firewall rule isn't in your project, the output doesn't display anything.\n- If you need to create the firewall rule, use the following command:```\ngcloud compute firewall-rules create default-allow-internal \\\u00a0 \u00a0 --direction=INGRESS \\\u00a0 \u00a0 --priority=65534 \\\u00a0 \u00a0 --network=default \\\u00a0 \u00a0 --action=ALLOW \\\u00a0 \u00a0 --rules=tcp:0-65535,udp:0-65535,icmp \\\u00a0 \u00a0 --source-ranges=0.0.0.0/0\n```\n## Storing assetsRender pipelines can differ vastly, even within a single company. To implement your PoC quickly and with minimal configuration, you can use the boot disk of your render worker instance to store assets. Your PoC shouldn't yet evaluate data synchronization or more advanced storage solutions. You can evaluate those options in a separate PoC.\nThere are a number of [storage options](/solutions/filers-on-compute-engine) available on Google Cloud, but we recommend testing a scalable shared storage solution in a separate PoC.\nIf you're testing multiple render worker configurations and need a shared file system, you can create a [Filestore](/filestore) volume and mount it by using NFS to your render workers. Filestore is a managed file storage service that can be mounted to read/write across many instances, acting as a file server.## Getting data to Google CloudTo run a render PoC, you need to get your scene files, caches, and assets to your render workers. For larger (>10 GB) datasets, you can use `gsutil` to copy your data to Cloud Storage and then onto your render workers. For smaller (<10 GB) datasets, you can use the gcloud CLI to copy data directly to a path on your render workers (Linux only).\n### Create a destination directory on your render worker\n- In Cloud Shell, connect to your render worker by using SSH:```\ngcloud compute ssh [WORKER_NAME]\n```Where `[WORKER_NAME]` is the name of your render worker.\n- Create a destination directory for your data:```\nmkdir [ASSET_DIR]\n```Where `[ASSET_DIR]` is a local directory anywhere on your render worker.\n### Use gsutil to copy large amounts of dataIf you're transferring large datasets to your render worker, use [gsutil](/storage/docs/gsutil_install) with Cloud Storage as an intermediate step. If you're transferring smaller datasets, you can skip to the next section and [use the gcloud CLI to transfer smaller amounts of data](/solutions/running-hybrid-render-farm-poc#use_the_gcloud_tool_to_copy_small_amounts_of_data) .- On your local workstation, create a Cloud Storage bucket:```\ngsutil mb gs://[BUCKET_NAME_ASSETS]\n```Where `[BUCKET_NAME_ASSETS]` represents the name of the Cloud Storage bucket for your files or directories that you want to copy.\n- Copy data from your local directory to the bucket:```\ngsutil -m cp -r [ASSETS] gs://[BUCKET_NAME_ASSETS]\n```Where `[ASSETS]` is a list of files or directories to copy to your bucket.\n- Connect to your render worker by using SSH:```\ngcloud compute ssh [WORKER_NAME]\n```\n- Copy the contents of your bucket to your render worker:```\ngsutil -m cp -r gs://[BUCKET_NAME_ASSETS]/* [ASSET_DIR]\n```\n### Use the gcloud CLI to copy small amounts of dataIf you're transferring smaller datasets, you can copy directly from your local workstation to a running Linux render worker by using the gcloud CLI.- On your local workstation, copy data between your local directory and your render worker:```\ngcloud compute scp --recurse [ASSETS] [INSTANCE_NAME]:[ASSET_DIR]\n```Where:- `[ASSETS]`is a list of files or directories to copy to your bucket.\n- `[INSTANCE_NAME]`is the name of your render worker.\n- `[ASSET_DIR]`is any local path on your render worker.## Running test rendersAfter you've installed and licensed your render software and copied scene data, you're ready to run render tests. This process depends entirely on how your render pipeline runs render commands.\n### Benchmark toolsIf you want to benchmark cloud resources against your on-premises hardware, you can use [Perfkit Benchmarker](https://github.com/GoogleCloudPlatform/PerfKitBenchmarker) to measure statistics for things such as network bandwidth and disk performance.\nSome rendering software has its own benchmarking tools, such as [V-Ray](https://www.chaosgroup.com/vray/benchmark) , [Octane](https://render.otoy.com/octanebench/) , or [Maxon](https://www.maxon.net/en/products/cinebench/) , which you might want to run both on-premises and on the cloud to compare common render configurations.## Getting data from Google CloudAfter you've performed your render tests and want to see the results, you need to copy the resulting renders to your local workstation. Depending on the size of the dataset to transfer, you can either use `gsutil` or the  gcloud CLI.\n### Create a destination directory on your local workstation\n- On your local workstation, create a directory for your renders:```\nmkdir [RENDER_DIR]\n```Where `[RENDER_DIR]` is a local path on your render worker.\n### Use gsutil to copy large amounts of dataIf you're transferring large datasets, use `gsutil` . Otherwise, skip to the next section to [use the gcloud CLI](/solutions/running-hybrid-render-farm-poc#use_the_gcloud_tool_to_copy_small_amounts_of_data_2) . To copy data from your render worker to a Cloud Storage bucket, create a separate Cloud Storage bucket to keep your renders separate from your asset data.- On your local workstation, create a new Cloud Storage bucket:```\ngsutil mb gs://[BUCKET_NAME_RENDERS]\n```Where `[BUCKET_NAME_RENDERS]` represents the name of your Cloud Storage bucket for your rendered data.\n- Connect to your render worker by using SSH:```\ngcloud compute ssh [WORKER_NAME]\n```\n- Copy your rendered data to your bucket:```\ngsutil -m cp -r [RENDERS] gs://[BUCKET_NAME_RENDERS]\n```Where:- `[RENDERS]`is a list of files or directories to copy to your bucket.\n- On your local worksation, copy the files from the Cloud Storage bucket to a local directory:```\ngsutil -m cp -r gs://[BUCKET_NAME_RENDERS]/* [RENDER_DIR]\n```\n### Use the gcloud CLI to copy small amounts of dataIf you're copying smaller datasets, you can copy directly from your render worker to your local workstation.- On your local workstation, copy renders into your destination directory:```\ngcloud compute scp --recurse [WORKER_NAME]:[RENDERS] [RENDER_DIR]\n```Where `[RENDERS]` is a list of files or directories to copy to your local  workstation.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Delete individual resources\n- Delete the instance:```\ngcloud compute instances delete INSTANCE_NAME\n```\n- Delete the bucket:```\ngcloud storage buckets delete BUCKET_NAME\n``` **Important:** Your bucket must  be empty before you can delete it.\n## What's next\n- Read more about [building a hybrid render farm](/solutions/building-a-hybrid-render-farm) on Google Cloud.\n- Read more about [resource mapping from on-premises](/solutions/resource-mappings-from-on-premises-hardware-to-gcp) to Google Cloud.\n- Read more about [configuring your render worker instances](/solutions/building-a-hybrid-render-farm#choosing_a_disk_image) .\n- Read more about [gcloud compute scp](/sdk/gcloud/reference/compute/scp) .\n- Read more about [gsutil cp](/storage/docs/gsutil/commands/cp) .\n- Read more about [creating Cloud Storage buckets](/storage/docs/creating-buckets) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Docs"}