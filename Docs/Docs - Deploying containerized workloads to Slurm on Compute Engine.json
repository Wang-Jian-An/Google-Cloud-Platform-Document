{"title": "Docs - Deploying containerized workloads to Slurm on Compute Engine", "url": "https://cloud.google.com/architecture/deploying-containerized-workloads-slurm-cluster-compute-engine", "abstract": "# Docs - Deploying containerized workloads to Slurm on Compute Engine\nThis tutorial shows how to run a containerized workload in a\n [Slurm](https://slurm.schedmd.com/overview.html) \ncluster on\n [Compute Engine](/compute) \n. This tutorial is intended for developers who run their workloads in a Slurm cluster, and assumes a basic knowledge of the following:\n- Slurm\n- Linux command-line usage\n- [Cloud Build](/build) \n- Containers\nThe [Singularity container platform](https://sylabs.io/singularity/) is a standard mechanism for packaging and executing high performance computing (HPC) workloads in a container format. When you package a workload in a Singularity container, your administrator doesn't have to install any additional software in your cluster. You can put your entire workflow\u2014including software, libraries, and data\u2014into a container and run it in a Slurm job.\nThe following diagram shows how a Slurm cluster is augmented with a Cloud Storage-based container repository to support the execution of workloads that are packaged as Singularity containers.In the preceding diagram, a Slurm cluster is deployed in a Google Cloud project as follows:- The Slurm cluster contains these standard components:- A login node\n- A controller node\n- Shared NFS storage\n- Multiple compute nodes\n- A Cloud Storage bucket that contains Singularity container images is associated with the cluster.\n- Jobs that execute on the compute nodes can use the Singularity runtime to pull workloads that are packaged as containers to the node for execution.\n", "content": "## Objectives\n- Install the Singularity container platform in a Slurm cluster on Compute Engine.\n- Use Cloud Build to create a Singularity container.\n- Run the container on a Slurm compute node.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Compute Engine](/compute/all-pricing) \n- [Cloud Build](/build/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin- [ Deploy a Slurm cluster](/hpc-toolkit/docs/quickstarts/slurm-cluster) if necessary. Make sure that the cluster you deploy, or the one you use with this tutorial, has the [cloud-platform OAuth scope](/storage/docs/authentication#oauth-scopes) enabled for its compute nodes.\n## Preparing your environmentIn this section, you set envirornment variables that you use throughout the tutorial.- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- In Cloud Shell, set environment variables:```\nexport PROJECT_ID=\"$(gcloud config get-value core/project)\"export SINGULARITY_REPO=\"${PROJECT_ID}-singularity\"export SINGULARITY_VERSION=RELEASE_NUMBERexport JOBOUTPUT_BUCKET=\"${PROJECT_ID}-singularity-job-out\"\n```\nReplace `` with the semantic version number of the Singularity release that you want to use. We recommend using the most [recent Singularity release](https://github.com/hpcng/singularity/releases) in most cases.## Installling Singularity container platformThe following steps show how to install Singularity in the `/apps` directory of your Slurm cluster.- In Cloud Shell, log in to the login node of your Slurm cluster:```\nexport CLUSTER_LOGIN_NODE=$(gcloud compute instances list \\\\\u00a0 --zones CLUSTER_ZONE \\\\\u00a0 --filter=\"name ~ .*login.\" \\\\\u00a0 --format=\"value(name)\" | head -n1)\\gcloud compute ssh ${CLUSTER_LOGIN_NODE} \\\\\u00a0 --zone CLUSTER_ZONE\n```\n- Update installed packages and install the necessary development tools:```\nsudo yum update -y && \\\u00a0 \u00a0 \u00a0sudo yum groupinstall -y 'Development Tools' && \\\u00a0 \u00a0 \u00a0sudo yum install -y \\\u00a0 \u00a0 \u00a0openssl-devel \\\u00a0 \u00a0 \u00a0libuuid-devel \\\u00a0 \u00a0 \u00a0libseccomp-devel \\\u00a0 \u00a0 \u00a0wget \\\u00a0 \u00a0 \u00a0squashfs-tools \\\u00a0 \u00a0 \u00a0cryptsetup\n```\n- Install the [Go programming language](https://golang.org/dl/) , replacing with the version number of the Go release that you want to use. We recommend using the most recent Go release in most cases.```\nexport GOLANG_VERSION=GOLANG_VERSIONexport OS=linux ARCH=amd64wget https://dl.google.com/go/go$GOLANG_VERSION.$OS-$ARCH.tar.gzsudo tar -C /usr/local -xzvf go$GOLANG_VERSION.$OS-$ARCH.tar.gzrm go$GOLANG_VERSION.$OS-$ARCH.tar.gzecho 'export GOPATH=${HOME}/go' >> ~/.bashrcecho 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrcsource ~/.bashrc\n```\n- Download a Singularity release:```\nexport SINGULARITY_VERSION=RELEASE_NUMBERwget https://github.com/sylabs/singularity/releases/download/v${SINGULARITY_VERSION}/singularity-${SINGULARITY_VERSION}.tar.gz && \\tar -xzf singularity-${SINGULARITY_VERSION}.tar.gz && \\cd singularity\n```\n- Build and install Singularity in the `/apps` directory:```\n./mconfig --prefix=/apps/singularity/${SINGULARITY_VERSION} && \\\u00a0 \u00a0 make -C ./builddir && \\\u00a0 \u00a0 sudo make -C ./builddir install\n```By default, Singularity assumes that its configuration files are in the `/etc` directory. The `--prefix` flag in the preceding command alters the build so that Singularity looks for those files in the `/apps/singularity/` `` directory. The `/apps` directory is available on all of the Slurm compute nodes.\n- Create a Singularity modulefile:```\nsudo mkdir /apps/modulefiles/singularitysudo bash -c \"cat > /apps/modulefiles/singularity/${SINGULARITY_VERSION}\" <<SINGULARITY_MODULEFILE#%Module1.0\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n### modules singularity/${SINGULARITY_VERSION}.\n##\n## modulefiles/singularity/${SINGULARITY_VERSION}.\n##proc ModulesHelp { } {\u00a0 \u00a0 \u00a0 \u00a0 global version modroot\u00a0 \u00a0 \u00a0 \u00a0 puts stderr \"singularity/${SINGULARITY_VERSION} - sets the environment for Singularity ${SINGULARITY_VERSION}\"}module-whatis \u00a0 \"Sets the environment for using Singularity ${VERSION}\"# for Tcl script use onlyset \u00a0 \u00a0 topdir \u00a0 \u00a0 \u00a0 \u00a0 \u00a0/apps/singularity/${SINGULARITY_VERSION}set \u00a0 \u00a0 version \u00a0 \u00a0 \u00a0 \u00a0 ${SINGULARITY_VERSION}set \u00a0 \u00a0 sys \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 linux86prepend-path \u00a0 \u00a0PATH \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\\$topdir/binSINGULARITY_MODULEFILE\n```\n- Verify the Singularity installation:```\nmodule load singularity/${SINGULARITY_VERSION}singularity\n```The output is similar to the following:```\nUsage:\n singularity [global options...] <command>\nAvailable Commands:\n build  Build a Singularity image\n cache  Manage the local cache\n capability Manage Linux capabilities for users and groups\n config  Manage various singularity configuration (root user only)\n delete  Deletes requested image from the library\n exec  Run a command within a container\n inspect  Show metadata for an image\n instance Manage containers running as services\n key   Manage OpenPGP keys\n oci   Manage OCI containers\n plugin  Manage Singularity plugins\n pull  Pull an image from a URI\n push  Upload image to the provided URI\n remote  Manage singularity remote endpoints\n run   Run the user-defined default command within a container\n run-help Show the user-defined help for an image\n search  Search a Container Library for images\n shell  Run a shell within a container\n sif   siftool is a program for Singularity Image Format (SIF) file manipulation\n sign  Attach a cryptographic signature to an image\n test  Run the user-defined tests within a container\n verify  Verify cryptographic signatures attached to an image\n version  Show the version for Singularity\nRun 'singularity --help' for more detailed usage information.\n```\n- Exit the Slurm cluster login node by pressing .\n## Building a container using Cloud BuildThe following diagram illustrates the Singularity container build process:In the preceding diagram, a Cloud Build custom Singularity build step is added to the Container Registry for a Google Cloud project. After the Singularity build step is in place, Cloud Build packages the workload as a Singularity container image stored in Cloud Storage.\nThe following steps set up this process.- In Cloud Shell, create a Dockerfile that specifies a container with the Singularity runtime installed:```\ncat <<SINGULARITYDOCKERFILE > DockerfileFROM gcr.io/cloud-builders/go:debianARG singularity_version=${SINGULARITY_VERSION}RUN GOPATH=/go && \\\\\u00a0 \u00a0 apt-get update && \\\\\u00a0 \u00a0 apt-get install -y build-essential \\\\\u00a0 \u00a0 \u00a0 \u00a0 libssl-dev \\\\\u00a0 \u00a0 \u00a0 \u00a0 uuid-dev \\\\\u00a0 \u00a0 \u00a0 \u00a0 libgpgme11-dev \\\\\u00a0 \u00a0 \u00a0 \u00a0 squashfs-tools \\\\\u00a0 \u00a0 \u00a0 \u00a0 libseccomp-dev \\\\\u00a0 \u00a0 \u00a0 \u00a0 pkg-config wget && \\\\\u00a0 \u00a0 go get github.com/golang/dep/cmd/dep && \\\\\u00a0 \u00a0 mkdir -p \\${GOPATH}/src/github.com/sylabs && \\\\\u00a0 \u00a0 cd \\${GOPATH}/src/github.com/sylabs && \\\\\u00a0 \u00a0 wget https://github.com/sylabs/singularity/releases/download/v\\${singularity_version}/singularity-\\${singularity_version}.tar.gz && \\\\\u00a0 \u00a0 tar -xzvf singularity-\\${singularity_version}.tar.gz && \\\\\u00a0 \u00a0 cd singularity && \\\\\u00a0 \u00a0 ./mconfig -p /usr/local && \\\\\u00a0 \u00a0 make -C builddir && \\\\\u00a0 \u00a0 make -C builddir installENTRYPOINT [\"/usr/local/bin/singularity\"]SINGULARITYDOCKERFILE\n```Cloud Build supports creation of [custom build steps](/build/docs/create-custom-build-steps) . When you run the preceding command, a custom build step is packaged as a Docker container.\n- Define a custom build step by creating a build configuration file:```\ncat <<SINGULARITYBUILDER > singularitybuilder.yamlsteps:- name: 'gcr.io/cloud-builders/docker'\u00a0 args: ['build', '--build-arg', 'singularity_version=\\${_SINGULARITY_VERSION}', '-t', 'gcr.io/${PROJECT_ID}/singularity-\\${_SINGULARITY_VERSION}', '.']images: ['gcr.io/${PROJECT_ID}/singularity-\\${_SINGULARITY_VERSION}']SINGULARITYBUILDER\n```The preceding command creates a [build configuration](/build/docs/configuring-builds/create-basic-configuration) file that Cloud Build uses to build the Singularity build step container image and push it to your project's `gcr.io` container registry.\n- Use Cloud Build to create the Singularity build step:```\ngcloud builds submit \\\u00a0 --config=singularitybuilder.yaml \\\u00a0 --substitutions=_SINGULARITY_VERSION=${SINGULARITY_VERSION}\n```\n- Define a Singularity container:```\ncat << CONTAINERDEF > oceananigans.defBootstrap: dockerFrom: julia:1.4%environment\u00a0 \u00a0 export JULIA_DEPOT_PATH=:/opt/julia\u00a0 \u00a0 export GKSwstype=100%runscript\u00a0 \u00a0 julia /usr/local/src/two_dimensional_turbulence.jl%files\u00a0 \u00a0 Oceananigans.jl/examples/two_dimensional_turbulence.jl /usr/local/src%post\u00a0 \u00a0 export JULIA_DEPOT_PATH=/opt/julia\u00a0 \u00a0 export PATH=/usr/local/julia/bin:$PATH\u00a0 \u00a0 julia -e 'using Pkg; Pkg.add(\"Oceananigans\"); Pkg.add(\"Plots\"); Pkg.add(\"Statistics\"); using Oceananigans, Plots, Statistics;'\u00a0 \u00a0 chmod -R 645 /opt/juliaCONTAINERDEF\n```A [Singularity definition file](https://sylabs.io/guides/3.8/user-guide/definition_files.html) is a recipe for building a Singularity container image. The `workload.def` file this command creates starts with the [Juila](https://julialang.org/) 1.4 Docker container image and converts it to the Singularity image format. The Singularity build mechanism then copies the `two_dimensional_turbulence.jl` example code to the image, and finally loads additional Julia packages. When the container is executed, it uses the Julia runtime to run the example code.\n- Create a Cloud Storage bucket for the container image:```\ngsutil mb gs://${SINGULARITY_REPO}\n```\n- Create a container build specification:```\ncat <<CONTAINERBUILDER > containerbuilder.yamlsteps:- name: gcr.io/cloud-builders/git\u00a0 args: ['clone', 'https://github.com/CliMA/Oceananigans.jl']- name: gcr.io/$PROJECT_ID/singularity-\\${_SINGULARITY_VERSION}\u00a0 args: ['build', 'oceananigans.sif', 'oceananigans.def']artifacts:\u00a0 objects:\u00a0 \u00a0 location: 'gs://${SINGULARITY_REPO}'\u00a0 \u00a0 paths: ['oceananigans.sif']CONTAINERBUILDER\n```The preceding command creates a Cloud Build build configuration file that specifies the following steps:- The first step clones the [Oceananigans.jl](https://github.com/CliMA/Oceananigans.jl) repository that contains the example code that the Singularity container will run.\n- The second step uses the Singularity custom build step that you created to build the container. The container image is saved to the`gs://${SINGULARITY_REPO}`Cloud Storage bucket.\n- Build the container:```\ngcloud builds submit --config=containerbuilder.yaml --substitutions=_SINGULARITY_VERSION=${SINGULARITY_VERSION} --timeout 45m\n```\n- Verify the container build:```\ngsutil ls gs://${SINGULARITY_REPO}\n```The output is similar to the following:```\ngs://SINGULARITY_REPO/oceanagins.sif\n```\n- Allow read access to the container so that the Slurm job can pull the container image:```\ngsutil acl ch -g All:R gs://${SINGULARITY_REPO}/oceananigans.sif\n```\n## Using the container in a Slurm jobThis section describes how to run a Slurm job that uses the Singularity container that you created in the preceding section.- In Cloud Shell, create a script that runs the containerized workload:```\ncat <<EXAMPLESCRIPT > workload.sh#!/usr/bin/env bashset -xcb=\\${1} \u00a0 # container bucketob=\\${2} \u00a0 # output bucket# create a temporary directory so that multiple jobs on the same# don't interfere with one anothertmpdir=\\$(mktemp -d)cd \\${tmpdir}# generates a file named: 2d_turbulence_vorticity.mp4singularity run \\${cb}/oceananigans.sif# append a random string to make the file name uniquers=\\$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 16 | head -n 1)mp4=\"\\${ob}/2d_turbulence_vorticity_\\${rs}.mp4\"\u00a0 # write the output file to the output bucket with the unique name\u00a0 # and make it readablegsutil cp ./2d_turbulence_vorticity.mp4 \\${mp4}gsutil acl ch -g All:R \\${mp4}# clean up and exitrm 2d_turbulence_vorticity.mp4cdrm -rf \\${tmpdir}exit 0EXAMPLESCRIPT\n```The `workload.sh` script that you create with the preceding command uses the Singularity runtime to execute the Oceananigans container. The result of the execution is a 6-second movie that shows the viscous, turbulent decay of a random velocity field in two dimensions. After the `.mp4` file that contains the movie is generated, the workload script gives it a unique name and copies it to the `${JOBOUTPUT_BUCKET}` Cloud Storage bucket.\n- Create a script that runs the workload as a parallel job:```\ncat <<JOBSCRIPT > job.sh#!/usr/bin/env bashmodule load singularity/${SINGULARITY_VERSION}srun ./workload.sh \\${1} \\${2}module unload singularity/${SINGULARITY_VERSION}exit 0JOBSCRIPT\n```The preceding command creates the `job.sh` script, which uses the Slurm [srun](https://slurm.schedmd.com/srun.html) command to execute the workload script as a parallel job when scheduled on a Slurm cluster.\n- Move the scripts to the cluster login node:```\nexport CLUSTER_LOGINNODE=\"$(gcloud compute instances list --filter=\"name ~ .*login.\" --format=\"value(name)\")\"gcloud compute scp job.sh workload.sh ${CLUSTER_LOGINNODE}:~gcloud compute ssh ${CLUSTER_LOGINNODE} --command \"chmod a+x ~/workload.sh\"\n```\n- Create a public Cloud Storage bucket for the output of the container jobs:```\ngsutil mb gs://${JOBOUTPUT_BUCKET}gsutil acl ch -g All:W gs://${JOBOUTPUT_BUCKET}\n```\n- Schedule the job:```\ngcloud compute ssh ${CLUSTER_LOGINNODE} \\--command \"sbatch -N1 --mem-per-cpu=2G -t00:10:00 \\./job.sh https://storage.googleapis.com/${SINGULARITY_REPO} \\gs://${JOBOUTPUT_BUCKET}\"\n```The output is similar to the following, where is the Slurm identification number for the scheduled job:```\nSubmitted batch job JOB_ID\n```\n- Watch the progress of the job,:```\n```shwatch -n60 gcloud compute ssh ${CLUSTER_LOGINNODE} --command \"squeue\"```\n```Wait for to disappear from the queue and then exit the `watch` command by pressing .\n- Examine the results of the job:```\ngsutil ls -l gs://${JOBOUTPUT_BUCKET}\n```The output is similar to the following:```\n 642907 2020-05-01T18:33:46Z gs://JOBOUTPUT_BUCKET/2d_turbulence_vorticity_47FqamVUPHzVcaRf.mp4\n 640056 2020-05-01T16:42:47Z gs://JOBOUTPUT_BUCKET/2d_turbulence_vorticity_4cdm6V65MBPJVhGw.mp4\n```To view the animation the workload created, direct your browser to the URL returned by:```\ngsutil ls gs://${JOBOUTPUT_BUCKET} | head -n 1 | sed 's|gs\\:/|https\\://storage.googleapis.com|'\n```\n## Clean up\n### Deleting the project\nThe easiest way to eliminate billing is to delete the project that you created for the tutorial.\nTo delete the project:\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Learn about [Best practices for running tightly coupled HPC applications on Compute Engine](/solutions/best-practices-for-using-mpi-on-compute-engine) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Docs"}