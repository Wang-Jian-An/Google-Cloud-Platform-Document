{"title": "Docs - Enterprise foundations blueprint", "url": "https://cloud.google.com/architecture/security-foundations/printable", "abstract": "# Docs - Enterprise foundations blueprint\nLast reviewed 2023-12-20 UTC\nYou can print this page or save the page in PDF format using your browser's print function and choose the **Save as PDF** option. For the standard version of this page, return to the [ Enterprise foundations blueprint](/architecture/security-foundations) .This document describes the best practices that let you deploy a foundational set of resources in Google Cloud. A cloud foundation is the baseline of resources, configurations, and capabilities that enable companies to adopt Google Cloud for their business needs. A well-designed foundation enables consistent governance, security controls, scale, visibility, and access to shared services across all workloads in your Google Cloud environment. After you deploy the controls and governance that are described in this document, you can deploy workloads to Google Cloud.\nThe (formerly known as the ) is intended for architects, security practitioners, and platform engineering teams who are responsible for designing an enterprise-ready environment on Google Cloud. This blueprint consists of the following:\n- A [terraform-example-foundation GitHub repository](https://github.com/terraform-google-modules/terraform-example-foundation) that contains the deployable Terraform assets.\n- A guide that describes the architecture, design, and controls that you implement with the blueprint (this document).\nYou can use this guide in one of two ways:\n- To create a complete foundation based on Google's best practices. You can deploy all the recommendations from this guide as a starting point, and then customize the environment to address your business' specific requirements.\n- To review an existing environment on Google Cloud. You can compare specific components of your design against Google-recommended best practices.", "content": "## Supported use cases\nThe enterprise foundation blueprint provides a baseline layer of resources and configurations that help enable all types of workloads on Google Cloud. Whether you're migrating existing compute workloads to Google Cloud, building containerized web applications, or creating big data and machine learning workloads, the enterprise foundation blueprint helps you build your environment to support enterprise workloads at scale.\nAfter you deploy the enterprise foundation blueprint, you can deploy workloads directly or deploy additional blueprints to support complex workloads that require additional capabilities.\n## A defense-in-depth security model\nGoogle Cloud services benefit from the underlying [Google infrastructure security design](/docs/security/infrastructure/design) . It is your responsibility to design security into the systems that you build on top of Google Cloud. The enterprise foundation blueprint helps you to implement a defense-in-depth security model for your Google Cloud services and workloads.\nThe following diagram shows a defense-in-depth security model for your Google Cloud organization that combines architecture controls, policy controls, and detective controls.\nThe diagram describes the following controls:\n- **Policy controls** are programmatic constraints that enforce acceptable resource configurations and prevent risky configurations. The blueprint uses a combination of policy controls including infrastructure-as-code (IaC) validation in your pipeline and organization policy constraints.\n- **Architecture controls** are the configuration of Google Cloud resources like networks and resource hierarchy. The blueprint architecture is based on security best practices.\n- **Detective controls** let you detect anomalous or malicious behavior within the organization. The blueprint uses platform features such as Security Command Center, integrates with your existing detective controls and workflows such as a security operations center (SOC), and provides capabilities to enforce custom detective controls.## Key decisions\nThis section summarizes the high-level architectural decisions of the blueprint.\nThe diagram describes how Google Cloud services contribute to key architectural decisions:\n- **Cloud Build:** Infrastructure resources are managed using a GitOps model. Declarative IaC is written in Terraform and managed in a version control system for review and approval, and resources are deployed using Cloud Build as the continuous integration and continuous deployment (CI/CD) automation tool. The pipeline also enforces policy-as-code checks to validate that resources meet expected configurations before deployment.\n- **Cloud Identity:** Users and group membership are synchronized from your existing identity provider. Controls for user account lifecycle management and single sign-on (SSO) rely on the existing controls and processes of your identity provider.\n- **Identity and Access Management (IAM):** Allow policies (formerly known as IAM policies) allow access to resources and are applied to groups based on job function. Users are added to the appropriate groups to receive view-only access to foundation resources. All changes to foundation resources are deployed through the CI/CD pipeline which uses privileged service account identities.\n- **Resource Manager:** All resources are managed under a single organization, with a resource hierarchy of folders that organizes projects by environments. Projects are labeled with metadata for governance including cost attribution.\n- **Networking** : Network topologies use Shared VPC to provide network resources for workloads across multiple regions and zones, separated by environment, and managed centrally. All network paths between on-premises hosts, Google Cloud resources in the VPC networks, and Google Cloud services are private. No outbound traffic to or inbound traffic from the public internet is permitted by default.\n- **Cloud Logging** : Aggregated log sinks are configured to collect logs relevant for security and auditing into a centralized project for long-term retention, analysis, and export to external systems.\n- **Cloud Monitoring** : Monitoring scoping projects are configured to view application performance metrics across multiple projects in one place.\n- **Organization Policy Service:** Organization policy constraints are configured to prevent various high-risk configurations.\n- **Secret Manager:** Centralized projects are created for a team responsible for managing and auditing the use of sensitive application secrets to help meet compliance requirements.\n- **Cloud Key Management Service (Cloud KMS):** Centralized projects are created for a team responsible for managing and auditing encryption keys to help meet compliance requirements.\n- **Security Command Center:** Threat detection and monitoring capabilities are provided using a combination of built-in security controls from Security Command Center and custom solutions that let you detect and respond to security events.\nFor alternatives to these key decisions, see [alternatives](/architecture/security-foundations/summary#alternatives) .\n## What's next\n- Read about [authentication and authorization](/architecture/security-foundations/authentication-authorization) (next document in this series).\n# Authentication and authorization\nThis section introduces how to use [Cloud Identity](/identity/docs/overview) to manage the [identities that your employees use](/architecture/identity/overview-google-authentication#google_identities) to access Google Cloud services.\n## External identity provider as the source of truth\nWe recommend federating your Cloud Identity account with your existing identity provider. Federation helps you ensure that your existing account management processes apply to Google Cloud and other Google services.\nIf you don't have an existing identity provider, you can create user accounts directly in Cloud Identity.\n**Note:** If you're already using Google Workspace, Cloud Identity uses the same console, administrative controls, and user accounts as your Google Workspace account.\nThe following diagram shows a high-level view of identity federation and single sign-on (SSO). It uses Microsoft Active Directory, located in the on-premises environment, as the example identity provider.\nThis diagram describes the following best practices:\n- User identities are managed in an Active Directory domain that is located in the on-premises environment and federated to Cloud Identity. Active Directory uses Google Cloud Directory Sync to provision identities to Cloud Identity.\n- Users attempting to sign in to Google services are redirected to the external identity provider for [single sign-on with SAML](/architecture/identity/single-sign-on) , using their existing credentials to authenticate. No passwords are synchronized with Cloud Identity.\nThe following table provides links to setup guidance for identity providers.\n| Identity provider            | Guidance                                            |\n|:--------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Active Directory            | Active Directory user account provisioning Active Directory single sign-on                            |\n| Microsoft Entra ID (formerly Azure AD)      | Federating Google Cloud with Microsoft Entra ID                                  |\n| Other external identity providers (for example, Ping or Okta) | Integrating Ping Identity Solutions with Google Identity Services Using Okta with Google Cloud Providers Best practices for federating Google Cloud with an external identity provider |\nWe strongly recommend that you enforce multi-factor authentication at your identity provider with a phishing-resistant mechanism such as a [Titan Security Key](/titan-security-key) .\nThe recommended settings for Cloud Identity aren't automated through the Terraform code in this blueprint. See [administrative controls for Cloud Identity](#administrative-controls-for-cloud-identity) for the recommended security settings that you must configure in addition to deploying the Terraform code.\n## Groups for access control\nA is an identity that can be granted access to a resource. Principals include [Google Accounts for users](/docs/authentication#user-accounts) , Google groups, Google Workspace accounts, Cloud Identity domains, and service accounts. Some services also let you grant access to all users who authenticate with a Google Account, or to all users on the internet. For a principal to interact with Google Cloud services, you must grant them roles in [Identity and Access Management (IAM)](/iam/docs/overview) .\nTo manage IAM roles at scale, we recommend that you assign users to groups based on their job functions and access requirements, then grant IAM roles to those groups. You should add users to groups using the processes in your existing identity provider for group creation and membership.\nWe don't recommend granting IAM roles to individual users because individual assignments can increase the complexity of managing and auditing roles.\nThe blueprint configures groups and roles for view-only access to foundation resources. We recommend that you deploy all resources in the blueprint through the foundation pipeline, and that you don't grant roles to users to groups to modify foundation resources outside of the pipeline.\nThe following table shows the groups that are configured by the blueprint for viewing foundation resources.\n| Name         | Description                                       | Roles      | Scope    |\n|:--------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------|:-------------------|\n| grp-gcp-org-admin@example.com   | Highly privileged administrators who can grant IAM roles at the organization level. They can access any other role. This privilege is not recommended for daily use. | Organization Administrator | organization  |\n| grp-gcp-billing-admin@example.com  | Highly privileged administrators who can modify the Cloud Billing account. This privilege is not recommended for daily use.           | Billing Account Admin  | organization  |\n| grp-gcp-billing-viewer@example.com | The team who is responsible for viewing and analyzing the spending across all projects.                    | Billing Account Viewer  | organization  |\n| grp-gcp-billing-viewer@example.com | The team who is responsible for viewing and analyzing the spending across all projects.                    | BigQuery User    | billing project |\n| grp-gcp-audit-viewer@example.com  | The team who is responsible for auditing security-related logs.                          | Logs Viewer BigQuery User | logging project |\n| grp-gcp-monitoring-users@example.com | The team who is responsible for monitoring application performance metrics.                       | Monitoring Viewer   | monitoring project |\n| grp-gcp-security-reviewer@example.com | The team who is responsible for reviewing cloud security.                           | Security Reviewer   | organization  |\n| grp-gcp-network-viewer@example.com | The team who is responsible for viewing and maintaining network configurations.                      | Compute Network Viewer  | organization  |\n| grp-gcp-scc-admin@example.com   | The team who is responsible for configuring Security Command Center.                         | Security Center Admin Editor | organization  |\n| grp-gcp-secrets-admin@example.com  | The team who is responsible for managing, storing, and auditing credentials and other secrets that are used by applications.           | Secret Manager Admin   | secrets projects |\n| grp-gcp-kms-admin@example.com   | The team who is responsible for enforcing encryption key management to meet compliance requirements.                 | Cloud KMS Viewer    | kms projects  |\nAs you build your own workloads on top of the foundation, you create additional groups and grant IAM roles that are based on the access requirements for each workload.\nWe strongly recommend that you avoid [basic roles](/iam/docs/understanding-roles#basic) (such as Owner, Editor, or Viewer) and use [predefined roles](/iam/docs/understanding-roles#predefined_roles) instead. Basic roles are overly permissive and a potential security risk. Owner and Editor roles can lead to privilege escalation and lateral movement, and the Viewer role includes access to read all data. For best practices on IAM roles, see [Use IAM securely](/iam/docs/using-iam-securely) .\n## Super admin accounts\nCloud Identity users with the [super admin account](/resource-manager/docs/super-admin-best-practices) bypass the organization's SSO settings and authenticate directly to Cloud Identity. This exception is by design, so that the super admin can still access the Cloud Identity console in the event of an SSO misconfiguration or outage. However, it means you must consider additional protection for super admin accounts.\nTo protect your super admin accounts, we recommend that you always enforce 2-step verification with security keys in Cloud Identity. For more information, see [Security best practices for administrator accounts](https://support.google.com/a/answer/9011373) .\n## Issues with consumer user accounts\nIf you didn't use Cloud Identity or Google Workspace before you onboarded to Google Cloud, it's possible that your organization's employees are already using [consumer accounts](/architecture/identity/overview-google-authentication#consumer_account) that are associated with their corporate email identities to access other Google services such as Google Marketing Platform or YouTube. Consumer accounts are accounts that are fully owned and managed by the individuals who created them. Because those accounts aren't under your organization's control and might include both personal and corporate data, you must decide how to consolidate these accounts with other corporate accounts.\nWe recommend that you [consolidate existing consumer user accounts](/architecture/landing-zones/decide-how-to-onboard-identities#account-consolidation) as part of onboarding to Google Cloud. If you aren't using Google Workspace for all your user accounts already, we recommend [blocking the creation of new consumer accounts](https://knowledge.workspace.google.com/kb/how-to-prevent-the-consumer-unmanaged-google-accounts-creation-in-the-domain-000006505) .\n## Administrative controls for Cloud Identity\nCloud Identity has various administrative controls that are not automated by Terraform code in the blueprint. We recommend that you enforce each of these best practice security controls early in the process of building your foundation.\n| Control             | Description                                                                                                                                                                |\n|:----------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Deploy 2-step verification        | User accounts might be compromised through phishing, social engineering, password spraying, or various other threats. 2-step verification helps mitigate these threats. We recommend that you enforce 2-step verification for all user accounts in your organization with a phishing-resistant mechanism such as Titan Security Keys or other keys that are based on the phishing-resistant FIDO U2F (CTAP1) standards.                                                             |\n| Set session length for Google Cloud services    | Persistent OAuth tokens on developer workstations can be a security risk if exposed. We recommend that you set a reauthentication policy to require authentication every 16 hours using a security key.                                                                                                                 |\n| Set session length for Google Services     | (Google Workspace customers only) Persistent web sessions across other Google services can be a security risk if exposed. We recommend that you enforce a maximum web session length and align this with session length controls in your SSO provider.                                                                                                     |\n| Share data from Cloud Identity with Google Cloud services | Admin Activity audit logs from Google Workspace or Cloud Identity are ordinarily managed and viewed in the Admin Console, separately from your logs in your Google Cloud environment. These logs contain information that is relevant for your Google Cloud environment, such as user login events. We recommend that you share Cloud Identity audit logs to your Google Cloud environment to centrally manage logs from all sources.                                                          |\n| Set up post SSO verification        | The blueprint assumes that you set up SSO with your external identity provider. We recommend that you enable an additional layer of control based on Google's sign-in risk analysis. After you apply this setting, users might see additional risk-based login challenges at sign-in if Google deems that a user sign-in is suspicious.                                                                                 |\n| Remediate issues with consumer user accounts    | Users with a valid email address at your domain but no Google Account can sign up for unmanaged consumer accounts. These accounts might contain corporate data, but are not controlled by your account lifecycle management processes. We recommend that you take steps to ensure that all user accounts are managed accounts.                                                                                   |\n| Disable account recovery for super admin accounts   | Super admin account self-recovery is off by default for all new customers (existing customers might have this setting on). Turning this setting off helps to mitigate the risk that a compromised phone, compromised email, or social engineering attack could let an attacker gain super admin privileges over your environment. Plan an internal process for a super admin to contact another super admin in your organization if they have lost access to their account, and ensure that all super admins are familiar with the process for support-assisted recovery.                         |\n| Enforce and monitor password requirements for users  | In most cases, user passwords are managed through your external identity provider, but super admin accounts bypass SSO and must use a password to sign in to Cloud Identity. Disable password reuse and monitor password strength for any users who use a password to log in to Cloud Identity, particularly super admin accounts.                                                                                  |\n| Set organization-wide policies for using groups   | By default, external user accounts can be added to groups in Cloud Identity. We recommend that you configure sharing settings so that group owners can't add external members. Note that this restriction doesn't apply to the super admin account or other delegated administrators with Groups admin permissions. Because federation from your identity provider runs with administrator privileges, the group sharing settings don't apply to this group synchronization. We recommend that you review controls in the identity provider and synchronization mechanism to ensure that non-domain members aren't added to groups, or that you apply group restrictions. |\n## What's next\n- Read about [organization structure](/architecture/security-foundations/organization-structure) (next document in this series).\n# Organization structure\nThe root node for managing resources in Google Cloud is the [organization](/resource-manager/docs/cloud-platform-resource-hierarchy#organizations) . The Google Cloud organization provides a [resource hierarchy](/resource-manager/docs/cloud-platform-resource-hierarchy) that provides an ownership structure for resources and attachment points for [organization policies](/resource-manager/docs/organization-policy/overview#organization_policy) and access controls. The resource hierarchy consists of folders, projects, and resources, and it defines the structure and use of Google Cloud services within an organization.\nResources lower in the hierarchy inherit policies such as IAM allow policies and organization policies. All access permissions are denied by default, until you apply allow policies directly to a resource or the resource inherits the allow policies from a higher level in the resource hierarchy.\nThe following diagram shows the folders and projects that are deployed by the blueprint.\nThe following sections describe the folders and projects in the diagram.\n## Folders\nThe blueprint uses [folders](/resource-manager/docs/cloud-platform-resource-hierarchy#folders) to group projects based on their environment. This logical grouping is used to apply configurations like allow policies and organization policies at the folder level and then all resources within the folder inherit the policies. The following table describes the folders that are part of the blueprint.\n| Folder  | Description                         |\n|:--------------|:---------------------------------------------------------------------------------------------------------------|\n| bootstrap  | Contains the projects that are used to deploy foundation components.           |\n| common  | Contains projects with resources that are shared by all environments.           |\n| production | Contains projects with production resources.                 |\n| nonproduction | Contains a copy of the production environment to let you test workloads before you promote them to production. |\n| development | Contains the cloud resources that are used for development.             |\n| networking | Contains the networking resources that are shared by all environments.           |\n## Projects\nThe blueprint uses [projects](/resource-manager/docs/cloud-platform-resource-hierarchy#projects) to group individual resources based on their functionality and intended boundaries for access control. This following table describes the projects that are included in the blueprint.\n| Folder               | Project       | Description                                           |\n|:------------------------------------------------------------------|:--------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| bootstrap               | prj-b-cicd      | Contains the deployment pipeline that's used to build out the foundation components of the organization. For more information, see deployment methodology.       |\n| bootstrap               | prj-b-seed      | Contains the Terraform state of your infrastructure and the Terraform service account that is required to run the pipeline. For more information, see deployment methodology.   |\n| common               | prj-c-secrets     | Contains organization-level secrets. For more information, see store application credentials with Secret Manager.                  |\n| common               | prj-c-logging     | Contains the aggregated log sources for audit logs. For more information, see centralized logging for security and audit.                |\n| common               | prj-c-scc      | Contains resources to help configure Security Command Center alerting and other custom security monitoring. For more information, see threat monitoring with Security Command Center. |\n| common               | prj-c-billing-logs    | Contains a BigQuery dataset with the organization's billing exports. For more information, see allocate costs between internal cost centers.           |\n| common               | prj-c-infra-pipeline   | Contains an infrastructure pipeline for deploying resources like VMs and databases to be used by workloads. For more information, see pipeline layers.        |\n| common               | prj-c-kms      | Contains organization-level encryption keys. For more information, see manage encryption keys.                      |\n| networking              | prj-net-{env}-shared-base  | Contains the host project for a Shared VPC network for workloads that don't require VPC Service Controls. For more information, see network topology.         |\n| networking              | prj-net-{env}-shared-restricted | Contains the host project for a Shared VPC network for workloads that do require VPC Service Controls. For more information, see network topology.         |\n| networking              | prj-net-interconnect   | Contains the Cloud Interconnect connections that provide connectivity between your on-premises environment and Google Cloud. For more information, see hybrid connectivity.   |\n| networking              | prj-net-dns-hub     | Contains resources for a central point of communication between your on-premises DNS system and Cloud DNS. For more information, see centralized DNS setup.       |\n| environment folders (production, non-production, and development) | prj-{env}-monitoring   | Contains a scoping project to aggregate metrics from projects in that environment. For more information, see alerting on log-based metrics and performance metrics     |\n| environment folders (production, non-production, and development) | prj-{env}-secrets    | Contains folder-level secrets. For more information, see store and audit application credentials with Secret Manager.                 |\n| environment folders (production, non-production, and development) | prj-{env}-kms     | Contains folder-level encryption keys. For more information, see manage encryption keys.                        |\n| environment folders (production, non-production, and development) | application projects   | Contains various projects in which you create resources for applications. For more information, see project deployment patterns and pipeline layers.         |\n## Governance for resource ownership\nWe recommend that you apply labels consistently to your projects to assist with governance and cost allocation. The following table describes the project labels that are added to each project for governance in the blueprint.\n| Label   | Description                                                   |\n|:-----------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| application  | The human-readable name of the application or workload that is associated with the project.                               |\n| businesscode  | A short code that describes which business unit owns the project. The code shared is used for common projects that are not explicitly tied to a business unit.               |\n| billingcode  | A code that's used to provide chargeback information.                                         |\n| primarycontact | The username of the primary contact that is responsible for the project. Because project labels can't include special characters such as the ampersand (@), it is set to the username without the @example.com suffix. |\n| secondarycontact | The username of the secondary secondary contact that is responsible for the project. Because project labels can't include special characters such as @, set only the username without the @example.com suffix.   |\n| environment  | A value that identifies the type of environment, such as bootstrap, common, production, non-production,development, or network.                      |\n| envcode   | A value that identifies the type of environment, shortened to b, c, p, n, d, or net.                                 |\n| vpc    | The ID of the VPC network that this project is expected to use.                                      |\nGoogle might occasionally send important notifications such as account suspensions or updates to product terms. The blueprint uses [Essential Contacts](/resource-manager/docs/managing-notification-contacts) to send those notifications to the groups that you configure during deployment. Essential Contacts is configured at the organization node and inherited by all projects in the organization. We recommend that you review these groups and ensure that emails are monitored reliably.\nEssential Contacts is used for a different purpose than the `primarycontact` and `secondarycontact` fields that are configured in project labels. The contacts in project labels are intended for internal governance. For example, if you identify non-compliant resources in a workload project and need to contact the owners, you could use the `primarycontact` field to find the person or team responsible for that workload.\n## What's next\n- Read about [networking](/architecture/security-foundations/networking) (next document in this series).\n# Networking\nNetworking is required for resources to communicate within your Google Cloud organization and between your cloud environment and on-premises environment. This section describes the structure in the blueprint for VPC networks, IP address space, DNS, firewall policies, and connectivity to the on-premises environment.\n## Network topology\nThe blueprint repository provides the following options for your network topology:\n- Use separate Shared VPC networks for each environment, with no network traffic directly allowed between environments.\n- Use a hub-and-spoke model that adds a hub network to connect each environment in Google Cloud, with the network traffic between environments gated by a network virtual appliance (NVA).\nChoose the [dual Shared VPC network topology](#dual-shared-vpc-network-topology) when you don't want direct network connectivity between environments. Choose the [hub-and-spoke network topology](#hub-spoke-network-topology) when you want to allow network connectivity between environments that is filtered by an NVA such as when you rely on existing tools that require a direct network path to every server in your environment.\nBoth topologies use Shared VPC as a principal networking construct because Shared VPC allows a clear separation of responsibilities. Network administrators manage network resources in a centralized host project, and workload teams deploy their own application resources and consume the network resources in service projects that are attached to the host project.\nBoth topologies include a base and restricted version of each VPC network. The base VPC network is used for resources that contain non-sensitive data, and the restricted VPC network is used for resources with sensitive data that require VPC Service Controls. For more information on implementing VPC Service Controls, see [Protect your resources with VPC Service Controls](/architecture/security-foundations/operation-best-practices#protect-resources) .\n### Dual Shared VPC network topology\nIf you require network isolation between your development, non-production, and production networks on Google Cloud, we recommend the dual Shared VPC network topology. This topology uses separate Shared VPC networks for each environment, with each environment additionally split between a base Shared VPC network and a restricted Shared VPC network.\nThe following diagram shows the dual Shared VPC network topology.\nThe diagram describes these key concepts of the dual Shared VPC topology:\n- Each environment (production, non-production, and development) has one Shared VPC network for the base network and one Shared VPC network for the restricted network. This diagram shows only the production environment, but the same pattern is repeated for each environment.\n- Each Shared VPC network has two subnets, with each subnet in a different region.\n- Connectivity with on-premises resources is enabled through four VLAN attachments to the Dedicated Interconnect instance for each Shared VPC network, using four Cloud Router services (two in each region for redundancy). For more information, see [Hybrid connectivity between on-premises environment and Google Cloud](#hybrid-connectivity) .\nBy design, this topology doesn't allow network traffic to flow directly between environments. If you do require network traffic to flow directly between environments, you must take additional steps to allow this network path. For example, you might [configure Private Service Connect endpoints](/vpc/docs/about-accessing-vpc-hosted-services-endpoints) to expose a service from one VPC network to another VPC network. Alternatively, you might configure your on-premises network to let traffic flow from one Google Cloud environment to the on-premises environment and then to another Google Cloud environment.\n### Hub-and-spoke network topology\nIf you deploy resources in Google Cloud that require a direct network path to resources in multiple environments, we recommend the hub-and-spoke network topology.\nThe hub-and-spoke topology uses several of the concepts that are part of the dual Shared VPC topology, but modifies the topology to add a hub network. The following diagram shows the hub-and-spoke topology.\nThe diagram describes these key concepts of hub-and-spoke network topology:\n- This model adds a hub network, and each of the development, non-production, and production networks (spokes) are connected to the hub network through VPC Network Peering. Alternatively, if you anticipate exceeding [the quota limit](/vpc/docs/quota#per_network) , you can use an [HA VPN gateway](/network-connectivity/docs/vpn/concepts/topologies) instead.\n- Connectivity to on-premises networks is allowed only through the hub network. All spoke networks can communicate with shared resources in the hub network and use this path to connect to on-premises networks.\n- The hub networks include an NVA for each region, deployed redundantly behind internal Network Load Balancer instances. This NVA serves as the gateway to allow or deny traffic to communicate between spoke networks.\n- The hub network also hosts tooling that requires connectivity to all other networks. For example, you might deploy tools on VM instances for configuration management to the common environment.\n- The hub-and-spoke model is duplicated for a base version and restricted version of each network.\nTo enable spoke-to-spoke traffic, the blueprint deploys NVAs on the hub Shared VPC network that act as gateways between networks. Routes are exchanged from hub-to-spoke VPC networks through [custom routes exchange](/vpc/docs/vpc-peering#importing-exporting-routes) . In this scenario, connectivity between spokes must be routed through the NVA because VPC Network Peering is non-transitive, and therefore, spoke VPC networks can't exchange data with each other directly. You must configure the virtual appliances to selectively allow traffic between spokes.\nFor more information on using NVAs to control traffic between spokes, see [centralized network appliances on Google Cloud](/architecture/architecture-centralized-network-appliances-on-google-cloud#example_architecture_using_vpc_network_peering_and_internal_tcp_udp_load_balancer_as_next_hop) .\n## Project deployment patterns\nWhen creating new projects for workloads, you must decide how resources in this project connect to your existing network. The following table describes the patterns for deploying projects that are used in the blueprint.\n| Pattern     | Description                                                                                                                                                                                                                     | Example usage       |\n|:---------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------|\n| Shared base projects  | These projects are configured as service projects to a base Shared VPC host project. Use this pattern when resources in your project have the following criteria: Require network connectivity to the on-premises environment or resources in the same Shared VPC topology. Require a network path to the Google services that are contained on the private virtual IP address. Don't require VPC Service Controls.                                                                                                                   | example_base_shared_vpc_project.tf  |\n| Shared restricted projects | These projects are configured as service projects to a restricted Shared VPC host project. Use this pattern when resources in your project have the following criteria: Require network connectivity to the on-premises environment or resources in the same Shared VPC topology. Require a network path to the Google services contained on the restricted virtual IP address. Require VPC Service Controls.                                                                                                                     | example_restricted_shared_vpc_project.tf |\n| Floating projects   | Floating projects are not connected to other VPC networks in your topology. Use this pattern when resources in your project have the following criteria: Don't require full mesh connectivity to an on-premises environment or resources in the Shared VPC topology. Don't require a VPC network, or you want to manage the VPC network for this project independently of your main VPC network topology (such as when you want to use an IP address range that clashes with the ranges already in use). You might have a scenario where you want to keep the VPC network of a floating project separate from the main VPC network topology but also want to expose a limited number of endpoints between networks. In this case, publish services by using Private Service Connect to share network access to an individual endpoint across VPC networks without exposing the entire network. | example_floating_project.tf    |\n| Peering projects   | Peering projects create their own VPC networks and peer to other VPC networks in your topology. Use this pattern when resources in your project have the following criteria: Require network connectivity in the directly peered VPC network, but don't require transitive connectivity to an on-premises environment or other VPC networks. Must manage the VPC network for this project independently of your main network topology. If you create peering projects, it's your responsibility to allocate non-conflicting IP address ranges and plan for peering group quota.                                                                            | example_peering_project.tf    |\n## IP address allocation\nThis section introduces how the blueprint architecture allocates IP address ranges. You might need to change the specific IP address ranges used based on the IP address availability in your existing hybrid environment.\nThe following table provides a breakdown of the IP address space that's allocated for the blueprint. The hub environment only applies in the hub-and-spoke topology.\n| Purpose       | VPC type | Region  | Hub environment | Development environment | Non-production environment | Production environment |\n|:----------------------------------|:-----------|:------------|:-------------------|:--------------------------|:-----------------------------|:-------------------------|\n| Primary subnet ranges    | Base  | Region 1 | 10.0.0.0/18  | 10.0.64.0/18    | 10.0.128.0/18    | 10.0.192.0/18   |\n| Primary subnet ranges    | Base  | Region 2 | 10.1.0.0/18  | 10.1.64.0/18    | 10.1.128.0/18    | 10.1.192.0/18   |\n| Primary subnet ranges    | Base  | Unallocated | 10.{2-7}.0.0/18 | 10.{2-7}.64.0/18   | 10.{2-7}.128.0/18   | 10.{2-7}.192.0/18  |\n| Primary subnet ranges    | Restricted | Region 1 | 10.8.0.0/18  | 10.8.64.0/18    | 10.8.128.0/18    | 10.8.192.0/18   |\n| Primary subnet ranges    | Restricted | Region 2 | 10.9.0.0/18  | 10.9.64.0/18    | 10.9.128.0/18    | 10.9.192.0/18   |\n| Primary subnet ranges    | Restricted | Unallocated | 10.{10-15}.0.0/18 | 10.{10-15}.64.0/18  | 10.{10-15}.128.0/18   | 10.{10-15}.192.0/18  |\n| Private services access   | Base  | Global  | 10.16.0.0/21  | 10.16.8.0/21    | 10.16.16.0/21    | 10.16.24.0/21   |\n| Private services access   | Restricted | Global  | 10.16.32.0/21  | 10.16.40.0/21    | 10.16.48.0/21    | 10.16.56.0/21   |\n| Private Service Connect endpoints | Base  | Global  | 10.17.0.1/32  | 10.17.0.2/32    | 10.17.0.3/32     | 10.17.0.4/32    |\n| Private Service Connect endpoints | Restricted | Global  | 10.17.0.5/32  | 10.17.0.6/32    | 10.17.0.7/32     | 10.17.0.8/32    |\n| Proxy-only subnets    | Base  | Region 1 | 10.18.0.0/23  | 10.18.2.0/23    | 10.18.4.0/23     | 10.18.6.0/23    |\n| Proxy-only subnets    | Base  | Region 2 | 10.19.0.0/23  | 10.19.2.0/23    | 10.19.4.0/23     | 10.19.6.0/23    |\n| Proxy-only subnets    | Base  | Unallocated | 10.{20-25}.0.0/23 | 10.{20-25}.2.0/23   | 10.{20-25}.4.0/23   | 10.{20-25}.6.0/23  |\n| Proxy-only subnets    | Restricted | Region 1 | 10.26.0.0/23  | 10.26.2.0/23    | 10.26.4.0/23     | 10.26.6.0/23    |\n| Proxy-only subnets    | Restricted | Region 2 | 10.27.0.0/23  | 10.27.2.0/23    | 10.27.4.0/23     | 10.27.6.0/23    |\n| Proxy-only subnets    | Restricted | Unallocated | 10.{28-33}.0.0/23 | 10.{28-33}.2.0/23   | 10.{28-33}.4.0/23   | 10.{28-33}.6.0/23  |\n| Secondary subnet ranges   | Base  | Region 1 | 100.64.0.0/18  | 100.64.64.0/18   | 100.64.128.0/18    | 100.64.192.0/18   |\n| Secondary subnet ranges   | Base  | Region 2 | 100.65.0.0/18  | 100.65.64.0/18   | 100.65.128.0/18    | 100.65.192.0/18   |\n| Secondary subnet ranges   | Base  | Unallocated | 100.{66-71}.0.0/18 | 100.{66-71}.64.0/18  | 100.{66-71}.128.0/18   | 100.{66-71}.192.0/18  |\n| Secondary subnet ranges   | Restricted | Region 1 | 100.72.0.0/18  | 100.72.64.0/18   | 100.72.128.0/18    | 100.72.192.0/18   |\n| Secondary subnet ranges   | Restricted | Region 2 | 100.73.0.0/18  | 100.73.64.0/18   | 100.73.128.0/18    | 100.73.192.0/18   |\n| Secondary subnet ranges   | Restricted | Unallocated | 100.{74-79}.0.0/18 | 100.{74-79}.64.0/18  | 100.{74-79}.128.0/18   | 100.{74-79}.192.0/18  |\nThe preceding table demonstrates these concepts for allocating IP address ranges:\n- IP address allocation is subdivided into ranges for each combination of base Shared VPC, restricted Shared VPC, region, and environment.\n- Some resources are global and don't require subdivisions for each region.\n- By default, for regional resources, the blueprint deploys in two regions. In addition, there are unused IP address ranges so that you can can expand into six additional regions.\n- The hub network is only used in the hub-and-spoke network topology, while the development, non-production, and production environments are used in both network topologies.\nThe following table introduces how each type of IP address range is used.\n| ('Purpose', 'Purpose')  | ('Description', 'Description')                                                                                              |\n|:---------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| nan      | nan                                                                                                    |\n| Primary subnet ranges  | Resources that you deploy to your VPC network, such as virtual machine instances, use internal IP addresses from these ranges.                                                                      |\n| Primary subnet ranges  | Resources that you deploy to your VPC network, such as virtual machine instances, use internal IP addresses from these ranges.                                                                      |\n| Primary subnet ranges  | Resources that you deploy to your VPC network, such as virtual machine instances, use internal IP addresses from these ranges.                                                                      |\n| Private services access | Some Google Cloud services such as Cloud SQL require you to preallocate a subnet range for private services access. The blueprint reserves a /21 range globally for each of the Shared VPC networks to allocate IP addresses for services that require private services access. When you create a service that depends on private services access, you allocate a regional /24 subnet from the reserved /21 range. |\n| Private Service Connect | The blueprint provisions each VPC network with a Private Service Connect endpoint to communicate with Google Cloud APIs. This endpoint lets your resources in the VPC network reach Google Cloud APIs without relying on outbound traffic to the internet or publicly advertised internet ranges.                             |\n| Proxy-based load balancers | Some types of Application Load Balancers require you to preallocate proxy-only subnets. Although the blueprint doesn't deploy Application Load Balancers that require this range, allocating ranges in advance helps reduce friction for workloads when they need to request a new subnet range to enable certain load balancer resources.                   |\n| Secondary subnet ranges | Some use cases, such as container-based workloads, require secondary ranges. The blueprint allocates ranges from the RFC 6598 IP address space for secondary ranges.                                                            |\n## Centralized DNS setup\nFor DNS resolution between Google Cloud and on-premises environments, we recommend that you use a hybrid approach with two authoritative DNS systems. In this approach, Cloud DNS handles authoritative DNS resolution for your Google Cloud environment and your existing on-premises DNS servers handle authoritative DNS resolution for on-premises resources. Your on-premises environment and Google Cloud environment perform DNS lookups between environments through forwarding requests.\nThe following diagram demonstrates the DNS topology across the multiple VPC networks that are used in the blueprint.\nThe diagram describes the following components of the DNS design that is deployed by the blueprint:\n- The DNS hub project in the common folder is the central point of DNS exchange between the on-premises environment and the Google Cloud environment. DNS forwarding uses the same Dedicated Interconnect instances and Cloud Routers that are already configured in your network topology.- In the dual Shared VPC topology, the DNS hub uses the base production Shared VPC network.\n- In the hub-and-spoke topology, the DNS hub uses the base hub Shared VPC network.\n- Servers in each Shared VPC network can resolve DNS records from other Shared VPC networks through [DNS forwarding](/dns/docs/overview#dns-forwarding-methods) , which is configured between Cloud DNS in each Shared VPC host project and the DNS hub.\n- On-premises servers can resolve DNS records in Google Cloud environments using [DNS server policies](/dns/docs/best-practices#use_dns_server_policies_to_allow_queries_from_on-premises) that allow queries from on-premises servers. The blueprint configures an inbound server policy in the DNS hub to allocate IP addresses, and the on-premises DNS servers forward requests to these addresses. All DNS requests to Google Cloud reach the DNS hub first, which then resolves records from DNS peers.\n- Servers in Google Cloud can resolve DNS records in the on-premises environment using [forwarding zones](/dns/docs/best-practices#use_forwarding_zones_to_query_on-premises_servers) that query on-premises servers. All DNS requests to the on-premises environment originate from the DNS hub. The DNS request source is 35.199.192.0/19.\n### Firewall policies\nGoogle Cloud has multiple [firewall policy](/firewall/docs/firewall-policies-overview) types. Hierarchical firewall policies are enforced at the organization or folder level to inherit firewall policy rules consistently across all resources in the hierarchy. In addition, you can configure network firewall policies for each VPC network. The blueprint combines these firewall policies to enforce common configurations across all environments using Hierarchical firewall policies and to enforce more specific configurations at each individual VPC network using network firewall policies.\nThe blueprint doesn't use [legacy VPC firewall rules](/firewall/docs/firewalls) . We recommend using only firewall policies and avoid mixing use with legacy VPC firewall rules.\n### Hierarchical firewall policies\nThe blueprint defines a single [hierarchical firewall policy](/firewall/docs/firewall-policies) and attaches the policy to each of the production, non-production, development, bootstrap, and common folders. This hierarchical firewall policy contains the rules that should be enforced broadly across all environments, and delegates the evaluation of more granular rules to the network firewall policy for each individual environment.\nThe following table describes the hierarchical firewall policy rules deployed by the blueprint.\n| Rule description                   | Direction of traffic | Filter (IPv4 range)            | Protocols and ports | Action  |\n|:-------------------------------------------------------------------------------------------|:-----------------------|:----------------------------------------------------------------|:----------------------|:-----------|\n| Delegate the evaluation of inbound traffic from RFC 1918 to lower levels in the hierarchy. | Ingress    | 192.168.0.0/16, 10.0.0.0/8, 172.16.0.0/12      | all     | Go to next |\n| Delegate the evaluation of outbound traffic to RFC 1918 to lower levels in the hierarchy. | Egress     | 192.168.0.0/16, 10.0.0.0/8, 172.16.0.0/12      | all     | Go to next |\n| IAP for TCP forwarding                  | Ingress    | 35.235.240.0/20             | tcp:22,3390   | Allow  |\n| Windows server activation                 | Egress     | 35.190.247.13/32            | tcp:1688    | Allow  |\n| Health checks for Cloud Load Balancing              | Ingress    | 130.211.0.0/22, 35.191.0.0/16, 209.85.152.0/22, 209.85.204.0/22 | tcp:80,443   | Allow  |\n### Network firewall policies\nThe blueprint configures a [network firewall policy](/vpc/docs/network-firewall-policies) for each network. Each network firewall policy starts with a minimum set of rules that allow access to Google Cloud services and deny egress to all other IP addresses.\nIn the hub-and-spoke model, the network firewall policies contain additional rules to allow communication between spokes. The network firewall policy allows outbound traffic from one to the hub or another spoke, and allows inbound traffic from the NVA in the hub network.\nThe following table describes the rules in the global network firewall policy deployed for each VPC network in the blueprint.\n| Rule description                     | Direction of traffic | Filter                                    | Protocols and ports |\n|:-------------------------------------------------------------------------------------------------|:-----------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------|\n| Allow outbound traffic to Google Cloud APIs.              | Egress     | The Private Service Connect endpoint that is configured for each individual network. See Private access to Google APIs.        | tcp:443    |\n| Deny outbound traffic not matched by other rules.            | Egress     | all                                     | all     |\n| Allow outbound traffic from one spoke to another spoke (for hub-and-spoke model only).   | Egress     | The aggregate of all IP addresses used in the hub-and-spoke topology. Traffic that leaves a spoke VPC is routed to the NVA in the hub network first. | all     |\n| Allow inbound traffic to a spoke from the NVA in the hub network (for hub-and-spoke model only). | Ingress    | Traffic originating from the NVAs in the hub network.                        | all     |\nWhen you first deploy the blueprint, a VM instance in a VPC network can communicate with Google Cloud services, but not to other infrastructure resources in the same VPC network. To allow VM instances to communicate, you must add additional rules to your network firewall policy and [tags](/resource-manager/docs/tags/tags-overview) that explicitly allow the VM instances to communicate. Tags are added to VM instances, and traffic is evaluated against those tags. Tags additionally have IAM controls so that you can define them centrally and delegate their use to other teams.\n**Note:** All references to tags in this document refer to [tags with IAM controls](/resource-manager/docs/tags/tags-overview) . We don't recommend VPC firewall rules with [legacy network tags](/vpc/docs/add-remove-network-tags) .\nThe following diagram shows an example of how you can add custom tags and network firewall policy rules to let workloads communicate inside a VPC network.\nThe diagram demonstrates the following concepts of this example:\n- The network firewall policy contains Rule 1 that denies outbound traffic from all sources at priority 65530.\n- The network firewall policy contains Rule 2 that allows inbound traffic from instances with the`service=frontend`tag to instances with the`service=backend`tag at priority 999.\n- The instance-2 VM can receive traffic from instance-1 because the traffic matches the tags allowed by Rule 2. Rule 2 is matched before Rule 1 is evaluated, based on the priority value.\n- The instance-3 VM doesn't receive traffic. The only firewall policy rule that matches this traffic is Rule 1, so outbound traffic from instance-1 is denied.## Private access to Google Cloud APIs\nTo let resources in your VPC networks or on-premises environment reach Google Cloud services, we recommend private connectivity instead of outbound internet traffic to public API endpoints. The blueprint configures [Private Google Access](/vpc/docs/private-google-access) on every subnet and creates internal endpoints with [Private Service Connect](/vpc/docs/about-accessing-google-apis-endpoints) to communicate with Google Cloud services. Used together, these controls allow a private path to Google Cloud services, without relying on internet outbound traffic or publicly advertised internet ranges.\nThe blueprint configures Private Service Connect endpoints with [API bundles](/vpc/docs/about-accessing-google-apis-endpoints#supported-apis) to differentiate which services can be accessed in which network. The base network uses the `all-apis` bundle and can reach any Google service, and the restricted network uses the `vpcsc` bundle which allows access to a limited set of services that [support VPC Service Controls](/vpc-service-controls/docs/supported-products) .\nFor access from hosts that are located in an on-premises environment, we recommend that you use a convention of custom FQDN for each endpoint, as described in the following table. The blueprint uses a unique Private Service Connect endpoint for each VPC network, configured for access to a different set of API bundles. Therefore, you must consider how to route service traffic from the on-premises environment to the VPC network with the correct API endpoint, and if you're using VPC Service Controls, ensure that traffic to Google Cloud services reaches the endpoint inside the intended perimeter. Configure your on-premise controls for DNS, firewalls, and routers to allow access to these endpoints, and configure on-premise hosts to use the appropriate endpoint. For more information, see [access Google APIs through endpoints](/vpc/docs/configure-private-service-connect-apis#on-premises) .\nThe following table describes the Private Service Connect endpoints created for each network.\n| VPC  | Environment | API bundle | Private Service Connect endpoint IP address | Private Service Connect endpoint IP address.1 | Private Service Connect endpoint IP address.2 | Custom FQDN     | Custom FQDN.1    | Custom FQDN.2    |\n|:-----------|:---------------|:-------------|:----------------------------------------------|:------------------------------------------------|:------------------------------------------------|:----------------------------|:----------------------------|:----------------------------|\n| Base  | Common   | all-apis  | 10.17.0.1/32         | 10.17.0.1/32         | 10.17.0.1/32         | c.private.googleapis.com | c.private.googleapis.com | c.private.googleapis.com |\n| Base  | Development | all-apis  | 10.17.0.2/32         | 10.17.0.2/32         | 10.17.0.2/32         | d.private.googleapis.com | d.private.googleapis.com | d.private.googleapis.com |\n| Base  | Non-production | all-apis  | 10.17.0.3/32         | 10.17.0.3/32         | 10.17.0.3/32         | n.private.googleapis.com | n.private.googleapis.com | n.private.googleapis.com |\n| Base  | Production  | all-apis  | 10.17.0.4/32         | 10.17.0.4/32         | 10.17.0.4/32         | p.private.googleapis.com | p.private.googleapis.com | p.private.googleapis.com |\n| Restricted | Common   | vpcsc  | 10.17.0.5/32         | 10.17.0.5/32         | 10.17.0.5/32         | c.restricted.googleapis.com | c.restricted.googleapis.com | c.restricted.googleapis.com |\n| Restricted | Development | vpcsc  | 10.17.0.6/32         | 10.17.0.6/32         | 10.17.0.6/32         | d.restricted.googleapis.com | d.restricted.googleapis.com | d.restricted.googleapis.com |\n| Restricted | Non-production | vpcsc  | 10.17.0.7/32         | 10.17.0.7/32         | 10.17.0.7/32         | n.restricted.googleapis.com | n.restricted.googleapis.com | n.restricted.googleapis.com |\n| Restricted | Production  | vpcsc  | 10.17.0.8/32         | 10.17.0.8/32         | 10.17.0.8/32         | p.restricted.googleapis.com | p.restricted.googleapis.com | p.restricted.googleapis.com |\nTo ensure that traffic for Google Cloud services has a DNS lookup to the correct endpoint, the blueprint configures private DNS zones for each VPC network. The following table describes these private DNS zones.\n| Private zone name | DNS name                       | Record type | Data                        |\n|:--------------------|:--------------------------------------------------------------------------------------------------|:--------------|:----------------------------------------------------------------------------------------------------|\n| googleapis.com.  | *.googleapis.com.                     | CNAME   | private.googleapis.com. (for base networks) or restricted.googleapis.com. (for restricted networks) |\n| googleapis.com.  | private.googleapis.com (for base networks) or restricted.googleapis.com (for restricted networks) | A    | The Private Service Connect endpoint IP address for that VPC network.        |\n| gcr.io.    | *.gcr.io                       | CNAME   | gcr.io.                        |\n| gcr.io.    | gcr.io                       | A    | The Private Service Connect endpoint IP address for that VPC network.        |\n| pkg.dev.   | *.pkg.dev.                      | CNAME   | pkg.dev.                       |\n| pkg.dev.   | pkg.dev.                       | A    | The Private Service Connect endpoint IP address for that VPC network.        |\nThe blueprint has additional configurations to enforce that these Private Service Connect endpoints are used consistently. Each Shared VPC network also enforces the following:\n- A network firewall policy rule that allows outbound traffic from all sources to the IP address of the Private Service Connect endpoint on TCP:443.\n- A network firewall policy rule that denies outbound traffic to 0.0.0.0/0, which includes the [default domains](/vpc/docs/configure-private-google-access#domain-options) that are used for access to Google Cloud services.## Internet connectivity\nThe blueprint doesn't allow inbound or outbound traffic between its VPC networks and the internet. For workloads that require internet connectivity, you must take additional steps to design the access paths required.\nFor workloads that require outbound traffic to the internet, we recommend that you manage outbound traffic through [Cloud NAT](/nat/docs/overview) to allow outbound traffic without unsolicited inbound connections, or through [Secure Web Proxy](/secure-web-proxy/docs/overview) for more granular control to allow outbound traffic to trusted web services only.\nFor workloads that require inbound traffic from the internet, we recommend that you design your workload with [Cloud Load Balancing](/load-balancing/docs/https) and [Google Cloud Armor](/armor/docs/cloud-armor-overview) to benefit from DDoS and WAF protections.\nWe don't recommend that you design workloads that allow direct connectivity between the internet and a VM using an external IP address on the VM.\n## Hybrid connectivity between an on-premises environment and Google Cloud\nTo establish connectivity between the on-premises environment and Google Cloud, we recommend that you use [Dedicated Interconnect](/interconnect/docs/concepts/dedicated-overview) to maximize security and reliability. A Dedicated Interconnect connection is a direct link between your on-premises network and Google Cloud.\nThe following diagram introduces hybrid connectivity between the on-premises environment and a Google Virtual Private Cloud network.\nThe diagram describes the following components of the pattern for [99.99% availability for Dedicated Interconnect](/interconnect/docs/tutorials/dedicated-creating-9999-availability) :\n- Four Dedicated Interconnect connections, with two connections in one metropolitan area (metro) and two connections in another metro.\n- The connections are divided into two pairs, with each pair connected to a separate on-premises data center.\n- [VLAN attachments](/interconnect/docs/how-to/dedicated/creating-vlan-attachments) are used to connect each Dedicated Interconnect instance to [Cloud Routers](/network-connectivity/docs/router/concepts/overview) that are attached to the Shared VPC topology. These VLAN attachments are hosted in the`prj-c-interconnect`project.\n- Each Shared VPC network has four Cloud Routers, two in each region, with the dynamic routing mode set to`global`so that every Cloud Router can announce all subnets, independent of region.\nWith global dynamic routing, Cloud Router advertises routes to all subnets in the VPC network. Cloud Router advertises routes to remote subnets (subnets outside of the Cloud Router's region) with a lower priority compared to local subnets (subnets that are in the Cloud Router's region). Optionally, you can change [advertised prefixes and priorities](/network-connectivity/docs/router/concepts/overview#advertised-prefixes-and-priorities) when you configure the BGP session for a Cloud Router.\nTraffic from Google Cloud to an on-premises environment uses the Cloud Router closest to the cloud resources. Within a single region, multiple routes to on-premises networks have the same multi-exit discriminator (MED) value, and Google Cloud uses [equal cost multi-path (ECMP)](https://en.wikipedia.org/wiki/Equal-cost_multi-path_routing) routing to distribute outbound traffic between all possible routes.\n## On-premises configuration changes\nTo configure connectivity between the on-premises environment and Google Cloud, you must configure additional changes in your on-premises environment. The Terraform code in the blueprint automatically configures Google Cloud resources but doesn't modify any of your on-premises network resources.\nSome of the components for hybrid connectivity from your on-premises environment to Google Cloud are automatically enabled by the blueprint, including the following:\n- Cloud DNS is configured with DNS forwarding between all Shared VPC networks to a single hub, as described in [DNS setup](#dns-setup) . A Cloud DNS server policy is configured with [inbound forwarder IP addresses](/dns/docs/policies#list-in-entrypoints) .\n- Cloud Router is configured to export routes for all subnets and custom routes for the IP addresses used by the Private Service Connect endpoints.\nTo enable hybrid connectivity, you must take the following additional steps:\n- [Order a Dedicated Interconnect connection](/network-connectivity/docs/interconnect/how-to/dedicated/ordering-dedicated-interconnect) .\n- Configure on-premises routers and firewalls to allow outbound traffic to the internal IP address space defined in [IP address space allocation](#ip-address-allocation) .\n- Configure your on-premises DNS servers to forward DNS lookups bound for Google Cloud to the [inbound forwarder IP addresses](/dns/docs/policies#list-in-entrypoints) that is already configured by the blueprint.\n- Configure your on-premises DNS servers, firewalls, and routers to accept DNS queries from the Cloud DNS [forwarding zone](/dns/docs/zones/forwarding-zones) (35.199.192.0/19).\n- Configure on-premise DNS servers to respond to queries from on-premises hosts to Google Cloud services with the IP addresses defined in [private access to Cloud APIs](#private-access-to-google-cloud-apis) .\n- For encryption in transit over the Dedicated Interconnect connection, configure [MACsec for Cloud Interconnect](/network-connectivity/docs/interconnect/concepts/macsec-overview) or configure [HA VPN over Cloud Interconnect](/network-connectivity/docs/interconnect/concepts/ha-vpn-interconnect) for IPsec encryption.\nFor more information, see [Private Google Access for on-premises hosts](/vpc/docs/configure-private-google-access-hybrid) .\n## What's next\n- Read about [detective controls](/architecture/security-foundations/detective-controls) (next document in this series).\n# Detective controls\nThreat detection and monitoring capabilities are provided using a combination of built-in security controls from Security Command Center and custom solutions that let you detect and respond to security events.\n## Centralized logging for security and audit\nThe blueprint configures logging capabilities to track and analyze changes to your Google Cloud resources with logs that are aggregated to a single project.\nThe following diagram shows how the blueprint aggregates logs from multiple sources in multiple projects into a centralized log sink.\nThe diagram describes the following:\n- Log sinks are configured at the organization node to aggregate logs from all projects in the resource hierarchy.\n- Multiple log sinks are configured to send logs that match a filter to different destinations for storage and analytics.\n- The`prj-c-logging`project contains all the resources for log storage and analytics.\n- Optionally, you can configure additional tooling to export logs to a SIEM.\nThe blueprint uses different log sources and includes these logs in the log sink filter so that the logs can be exported to a centralized destination. The following table describes the log sources.\n| Log source      | Description                                                                                      |\n|:-------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Admin Activity audit logs  | You cannot configure, disable, or exclude Admin Activity audit logs.                                                                        |\n| System Event audit logs  | You cannot configure, disable, or exclude System Event audit logs.                                                                        |\n| Policy Denied audit logs  | You cannot configure or disable Policy Denied audit logs, but you can optionally exclude them with exclusion filters.                                                            |\n| Data Access audit logs   | By default, the blueprint doesn't enable data access logs because the volume and cost of these logs can be high.To determine whether you should enable data access logs, evaluate where your workloads handle sensitive data and consider whether you have a requirement to enable data access logs for each service and environment working with sensitive data. |\n| VPC Flow Logs     | The blueprint enables VPC Flow Logs for every subnet. The blueprint configures log sampling to sample 50% of logs to reduce cost.If you create additional subnets, you must ensure that VPC Flow Logs are enabled for each subnet.                                |\n| Firewall Rules Logging   | The blueprint enables Firewall Rules Logging for every firewall policy rule.If you create additional firewall policy rules for workloads, you must ensure that Firewall Rules Logging is enabled for each new rule.                                    |\n| Cloud DNS logging    | The blueprint enables Cloud DNS logs for managed zones.If you create additional managed zones, you must enable those DNS logs.                                                         |\n| Google Workspace audit logging | Requires a one-time enablement step that is not automated by the blueprint. For more information, see Share data with Google Cloud services.                                                      |\n| Access Transparency logs  | Requires a one-time enablement step that is not automated by the blueprint. For more information, see Enable Access Transparency.                                                         |\nThe following table describes the log sinks and how they are used with [supported destinations](/logging/docs/routing/overview#destinations) in the blueprint.\n| Sink    | Destination                     | Purpose                                                                                                             |\n|:-----------------|:----------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| sk-c-logging-la | Logs routed to Cloud Logging buckets with Log Analytics and a linked BigQuery dataset enabled | Actively analyze logs. Run ad hoc investigations by using Logs Explorer in the console, or write SQL queries, reports, and views using the linked BigQuery dataset.                                                                      |\n| sk-c-logging-bkt | Logs routed to Cloud Storage                 | Store logs long-term for compliance, audit, and incident-tracking purposes.Optionally, if you have compliance requirements for mandatory data retention, we recommend that you additionally configure Bucket Lock.                                                          |\n| sk-c-logging-pub | Logs routed to Pub/Sub                  | Export logs to an external platform such as your existing SIEM.This requires additional work to integrate with your SIEM, such as the following mechanisms: For many tools, third-party integration with Pub/Sub is the preferred method to ingest logs. For Google Chronicle, you can ingest Google Cloud data to Chronicle without provisioning additional infrastructure. For Splunk, you can stream logs from Google Cloud to Splunk using Dataflow. |\nFor guidance on enabling additional log types and writing log sink filters, see the [log scoping tool](/architecture/security-log-analytics#log_scoping_tool) .\n## Threat monitoring with Security Command Center\nWe recommend that you activate [Security Command Center Premium](/security-command-center/docs/concepts-security-command-center-overview) for your organization to automatically detect threats, vulnerabilities, and misconfigurations in your Google Cloud resources. Security Command Center creates security findings from multiple sources including the following:\n- **Security Health Analytics:** detects common vulnerabilities and misconfigurations across Google Cloud resources.\n- **Attack path exposure:** shows a simulated path of how an attacker could exploit your high-value resources, based on the vulnerabilities and misconfigurations that are detected by other Security Command Center sources.\n- **Event Threat Detection:** applies detection logic and proprietary threat intelligence against your logs to identify threats in near-real time.\n- **Container Threat Detection:** detects common container runtime attacks.\n- **Virtual Machine Threat Detection:** detects potentially malicious applications that are running on virtual machines.\n- **Web Security Scanner:** scans for OWASP Top Ten vulnerabilities in your web-facing applications on Compute Engine, App Engine, or Google Kubernetes Engine.\nFor more information on the vulnerabilities and threats addressed by Security Command Center, see [Security Command Center sources](/security-command-center/docs/concepts-security-sources) .\nYou must activate Security Command Center after you deploy the blueprint. For instructions, see [Activate Security Command Center for an organization](/security-command-center/docs/activate-scc-for-an-organization) .\nAfter you activate Security Command Center, we recommend that you export the findings that are produced by Security Command Center to your existing tools or processes for triaging and responding to threats. The blueprint creates the `prj-c-scc` project with a Pub/Sub topic to be used for this integration. Depending on your existing tools, use one of the following methods to export findings:\n- If you use the console to manage security findings directly in Security Command Center, configure [folder-level and project-level roles](/security-command-center/docs/access-control-org#folder-level_and_project-level_roles) for Security Command Center to let teams view and manage security findings just for the projects for which they are responsible.\n- If you use Chronicle as your SIEM, [ingest Google Cloud data to Chronicle](/chronicle/docs/ingestion/cloud/ingest-gcp-logs) .\n- If you use a SIEM or SOAR tool with integrations to Security Command Center, share data with [Cortex XSOAR](/security-command-center/docs/how-to-configure-scc-cortex-xsoar) , [Elastic Stack](/security-command-center/docs/how-to-configure-scc-elastic-stack-docker) , [ServiceNow](/security-command-center/docs/how-to-configure-scc-servicenow) , [Splunk](/security-command-center/docs/how-to-configure-scc-splunk) , or [QRadar](/security-command-center/docs/how-to-configure-scc-qradar) .\n- If you use an external tool that can ingest findings from Pub/Sub, configure [continuous exports](/security-command-center/docs/how-to-export-data#continuous_exports) to Pub/Sub and configure your existing tools to ingest findings from the Pub/Sub topic.## Alerting on log-based metrics and performance metrics\nWhen you begin to deploy workloads on top of your foundation, we recommend that you use Cloud Monitoring to measure the performance metrics.\nThe blueprint creates a monitoring project such as `prj-p-monitoring` for each environment. This project is configured as a [scoping project](/monitoring/settings#example) to gather aggregated performance metrics across multiple projects. The blueprint deploys an example with [log-based metrics](/logging/docs/logs-based-metrics) and an [alerting policy](/monitoring/alerts) to generate email notifications if there are any changes to the IAM policy that is applied to Cloud Storage buckets. This helps monitor for suspicious activities on sensitive resources such as the bucket in the `prj-b-seed` project that contains the Terraform state.\nMore generally, you can also use Cloud Monitoring to measure the performance metrics and health of your workload applications. Depending on the operational responsibility for supporting and monitoring applications in your organization, you might make more granular monitoring projects for different teams. Use these monitoring projects to view performance metrics, create dashboards of application health, and trigger alerts when your expected SLO is not met.\nThe following diagram shows a high-level view of how Cloud Monitoring aggregates performance metrics.\nFor guidance on how to monitor workloads effectively for reliability and availability, see the [Site Reliability Engineeringbook](https://sre.google/books/) by Google, particularly the chapter on [monitoring distributedsystems](https://sre.google/sre-book/monitoring-distributed-systems/) .\n## Custom solution for automated log analysis\nYou might have requirements to create alerts for security events that are based on custom queries against logs. Custom queries can help supplement the capabilities of your SIEM by analyzing logs on Google Cloud and exporting only the events that merit investigation, especially if you don't have the capacity to export all cloud logs to your SIEM.\nThe blueprint helps enable this log analysis by setting up a centralized source of logs that you can query using a linked BigQuery dataset. To automate this capability, you must implement the code sample at [bq-log-alerting](https://github.com/terraform-google-modules/terraform-google-log-export/tree/master/modules/bq-log-alerting) and extend the foundation capabilities. The sample code lets you regularly query a log source and send a custom finding to Security Command Center.\nThe following diagram introduces the high-level flow of the automated log analysis.\nThe diagram shows the following concepts of automated log analysis:\n- Logs from various sources are aggregated into a centralized logs bucket with log analytics and a linked BigQuery dataset.\n- BigQuery views are configured to query logs for the security event that you want to monitor.\n- Cloud Scheduler pushes an event to a Pub/Sub topic every 15 minutes and triggers Cloud Functions.\n- Cloud Functions queries the views for new events. If it finds events, it pushes them to Security Command Center as custom findings.\n- Security Command Center publishes notifications about new findings to another Pub/Sub topic.\n- An external tool such as a SIEM subscribes to the Pub/Sub topic to ingest new findings.\nThe sample has several [usecases](https://github.com/terraform-google-modules/terraform-google-log-export/blob/master/modules/bq-log-alerting/use-cases/README.md) to query for potentially suspicious behavior. Examples include a login from a list of super admins or other highly privileged accounts that you specify, changes to logging settings, or changes to network routes. You can extend the use cases by writing new query views for your requirements. Write your own queries or reference [security log analytics](/architecture/security-log-analytics#analyze_logs) for a library of SQL queries to help you analyze Google Cloud logs.\n## Custom solution to respond to asset changes\nTo respond to events in real time, we recommend that you use Cloud Asset Inventory to [monitor asset changes](/asset-inventory/docs/monitoring-asset-changes) . In this custom solution, an asset feed is configured to trigger notifications to Pub/Sub about changes to resources in real time, and then Cloud Functions runs custom code to enforce your own business logic based on whether the change should be allowed.\nThe blueprint has an example of this custom governance solution that monitors for IAM changes that add highly sensitive roles including Organization Admin, Owner, and Editor. The following diagram describes this solution.\nThe previous diagram shows these concepts:\n- Changes are made to an allow policy.\n- The Cloud Asset Inventory feed sends a real-time notification about the allow policy change to Pub/Sub.\n- Pub/Sub triggers a function.\n- Cloud Functions runs custom code to enforce your policy. The example function has logic to assess if the change has added the Organization Admin, Owner, or Editor roles to an allow policy. If so, the function [creates a custom security finding](/security-command-center/docs/how-to-api-create-manage-findings) and sends it to Security Command Center.\n- Optionally, you can use this model to automate remediation efforts. Write additional business logic in Cloud Functions to automatically take action on the finding, such as reverting the allow policy to its previous state.\nIn addition, you can extend the infrastructure and logic used by this sample solution to add custom responses to other events that are important to your business.\n## What's next\n- Read about [preventative controls](/architecture/security-foundations/preventative-controls) (next document in this series).\n# Preventative controls for acceptable resource configurations\nWe recommend that you define policy constraints that enforce acceptable resource configurations and prevent risky configurations. The blueprint uses a combination of organization policy constraints and infrastructure-as-code (IaC) validation in your pipeline. These controls prevent the creation of resources that don't meet your policy guidelines. Enforcing these controls early in the design and build of your workloads helps you to avoid remediation work later.\n## Organization policy constraints\nThe [Organization Policy](/resource-manager/docs/organization-policy/overview) service enforces constraints to ensure that certain resource configurations can't be created in your Google Cloud organization, even by someone with a sufficiently privileged IAM role.\nThe blueprint enforces policies at the organization node so that these controls are inherited by all folders and projects within the organization. This bundle of policies is designed to prevent certain high-risk configurations, such as exposing a VM to the public internet or granting public access to storage buckets, unless you deliberately allow an exception to the policy.\nThe following table introduces the [organization policy constraints](/resource-manager/docs/organization-policy/org-policy-constraints) that are implemented in the blueprint:\n| Organization policy constraint      | Description                                                                                                    |\n|:---------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| compute.disableNestedVirtualization    | Nested virtualization on Compute Engine VMs can evade monitoring and other security tools for your VMs if poorly configured. This constraint prevents the creation of nested virtualization.                                                       |\n| compute.disableSerialPortAccess     | IAM roles like compute.instanceAdmin allow privileged access to an instance's serial port using SSH keys. If the SSH key is exposed, an attacker could access the serial port and bypass network and firewall controls. This constraint prevents serial port access.                                    |\n| compute.disableVpcExternalIpv6      | External IPv6 subnets can be exposed to unauthorized internet access if they are poorly configured. This constraint prevents the creation of external IPv6 subnets.                                                             |\n| compute.requireOsLogin        | The default behavior of setting SSH keys in metadata can allow unauthorized remote access to VMs if keys are exposed. This constraint enforces the use of OS Login instead of metadata-based SSH keys.                                                    |\n| compute.restrictProtocolForwardingCreationForTypes | VM protocol forwarding for external IP addresses can lead to unauthorized internet egress if forwarding is poorly configured. This constraint allows VM protocol forwarding for internal addresses only.                                                    |\n| compute.restrictXpnProjectLienRemoval    | Deleting a Shared VPC host project can be disruptive to all the service projects that use networking resources. This constraint prevents accidental or malicious deletion of the Shared VPC host projects by preventing the removal of the project lien on these projects.                                   |\n| compute.setNewProjectDefaultToZonalDNSOnly   | A legacy setting for global (project-wide) internal DNS is not recommended because it reduces service availability. This constraint prevents the use of the legacy setting.                                                           |\n| compute.skipDefaultNetworkCreation     | A default VPC network and overly permissive default VPC firewall rules are created in every new project that enables the Compute Engine API. This constraint skips the creation of the default network and default VPC firewall rules.                                            |\n| compute.vmExternalIpAccess       | By default, a VM is created with an external IPv4 address that can lead to unauthorized internet access. This constraint configures an empty allowlist of external IP addresses that the VM can use and denies all others.                                               |\n| essentialcontacts.allowedContactDomains   | By default, Essential Contacts can be configured to send notifications about your domain to any other domain. This constraint enforces that only email addresses in approved domains can be set as recipients for Essential Contacts.                                             |\n| iam.allowedPolicyMemberDomains      | By default, allow policies can be granted to any Google Account, including unmanaged accounts, and accounts belonging to external organizations. This constraint ensures that allow policies in your organization can only be granted to managed accounts from your own domain. Optionally, you can allow additional domains.                      |\n| iam.automaticIamGrantsForDefaultServiceAccounts | By default, default service accounts are automatically granted overly permissive roles. This constraint prevents the automatic IAM role grants to default service accounts.                                                           |\n| iam.disableServiceAccountKeyCreation    | Service account keys are a high-risk persistent credential, and in most cases a more secure alternative to service account keys can be used. This constraint prevents the creation of service account keys.                                                   |\n| iam.disableServiceAccountKeyUpload     | Uploading service account key material can increase risk if key material is exposed. This constraint prevents the uploading of service account keys.                                                                 |\n| sql.restrictAuthorizedNetworks      | Cloud SQL instances can be exposed to unauthenticated internet access if the instances are configured to use authorized networks without a Cloud SQL Auth Proxy. This policy prevents the configuration of authorized networks for database access and forces the use of the Cloud SQL Auth Proxy instead.                           |\n| sql.restrictPublicIp        | Cloud SQL instances can be exposed to unauthenticated internet access if the instances are created with public IP addresses. This constraint prevents public IP addresses on Cloud SQL instances.                                                      |\n| storage.uniformBucketLevelAccess     | By default, objects in Cloud Storage can be accessed through legacy Access Control Lists (ACLs) instead of IAM, which can lead to inconsistent access controls and accidental exposure if misconfigured. Legacy ACL access is not affected by the iam.allowedPolicyMemberDomains constraint. This constraint enforces that access can only be configured through IAM uniform bucket-level access, not legacy ACLs. |\n| storage.publicAccessPrevention      | Cloud Storage buckets can be exposed to unauthenticated internet access if misconfigured. This constraint prevents ACLs and IAM permissions that grant access to allUsers and allAuthenticatedUsers.                                                     |\nThese policies are a starting point that we recommend for most customers and most scenarios, but you might need to modify organization policy constraints to accommodate certain workload types. For example, a workload that uses a Cloud Storage bucket as the backend for Cloud CDN to host public resources is blocked by `storage.publicAccessPrevention` , or a public-facing Cloud Run app that doesn't require authentication is blocked by `iam.allowedPolicyMemberDomains` . In these cases, modify the organization policy at the folder or project level to allow a narrow exception. You can also [conditionally add constraints to organization policy](/resource-manager/docs/organization-policy/tags-organization-policy#conditionally_add_constraints_to_organization_policy) by defining a tag that grants an exception or enforcement for policy, then applying the tag to projects and folders.\nFor additional constraints, see [available constraints](/resource-manager/docs/organization-policy/org-policy-constraints) and [custom constraints](/resource-manager/docs/organization-policy/creating-managing-custom-constraints) .\n## Pre-deployment validation of infrastructure-as-code\nThe blueprint uses a GitOps approach to manage infrastructure, meaning that all infrastructure changes are implemented through version-controlled infrastructure-as-code (IaC) and can be validated before deploying.\nThe [policies enforced in the blueprint](https://github.com/terraform-google-modules/terraform-example-foundation/tree/master/policy-library/policies/constraints) define acceptable resource configurations that can be deployed by your pipeline. If code that is submitted to your GitHub repository does not pass the policy checks, no resources are deployed.\nFor information on how pipelines are used and how controls are enforced through CI/CD automation, see [deployment methodology](/architecture/security-foundations/deployment-methodology) .\n## What's next\n- Read about [deployment methodology](/architecture/security-foundations/deployment-methodology) (next document in this series)\n# Deployment methodology\nWe recommend that you use declarative infrastructure to deploy your foundation in a consistent and controllable manner. This approach helps enable consistent governance by enforcing policy controls about acceptable resource configurations into your pipelines. The blueprint is deployed using a GitOps flow, with Terraform used to define infrastructure as code (IaC), a Git repository for version control and approval of code, and Cloud Build for CI/CD automation in the deployment pipeline. For an introduction to this concept, see [managing infrastructure as code with Terraform, Cloud Build, and GitOps](/docs/terraform/resource-management/managing-infrastructure-as-code) .\nThe following sections describe how the deployment pipeline is used to manage resources in your organization.\n## Pipeline layers\nTo separate the teams and technology stack that are responsible for managing different layers of your environment, we recommend a model that uses different pipelines and different personas that are responsible for each layer of the stack.\nThe following diagram introduces our recommended model for separating a foundation pipeline, infrastructure pipeline, and application pipeline.\nThe diagram introduces the pipeline layers in this model:\n- Thedeploys the foundation resources that are used across the platform. We recommend that a single central team is responsible for managing the foundation resources that are consumed by multiple business units and workloads.\n- Thedeploys projects and infrastructure that are used by workloads, such as VM instances or databases. The blueprint sets up a separate infrastructure pipeline for each business unit, or you might prefer a single infrastructure pipeline used by multiple teams.\n- Thedeploys the artifacts for each workload, such as containers or images. You might have many different application teams with individual application pipelines.\nThe following sections introduce the usage of each pipeline layer.\n### The foundation pipeline\nThe foundation pipeline deploys the foundation resources. It also sets up the infrastructure pipeline that is used to deploy infrastructure used by workloads.\nTo create the foundation pipeline, you first clone or fork the terraform-example-foundation to your own Git repository. Follow the steps in the [0-bootstrap README file](https://github.com/terraform-google-modules/terraform-example-foundation/tree/master/0-bootstrap) to configure your bootstrap folder and resources.\n| Stage  | Description                                                                                                                                                      |\n|:------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 0-bootstrap | Bootstraps a Google Cloud organization. This step also configures a CI/CD pipeline for the blueprint code in subsequent stages. The CICD project contains the Cloud Build foundation pipeline for deploying resources. The seed project includes the Cloud Storage buckets that contain the Terraform state of the foundation infrastructure and includes highly privileged service accounts that are used by the foundation pipeline to create resources. The Terraform state is protected through storage Object Versioning. When the CI/CD pipeline runs, it acts as the service accounts that are managed in the seed project. |\nAfter you create the foundation pipeline in the `0-bootstrap` stage, the following stages deploy resources on the foundation pipeline. Review the README directions for each stage and implement each stage sequentially.\n| Stage           | Description                                    |\n|:-----------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| 1-org           | Sets up top-level shared folders, projects for shared services, organization-level logging, and baseline security settings through organization policies. |\n| 2-environments         | Sets up development, non-production, and production environments within the Google Cloud organization that you've created.        |\n| 3-networks-dual-svpcor3-networks-hub-and-spoke | Sets up shared VPCs in your chosen topology and the associated network resources.                   |\n### The infrastructure pipeline\nThe infrastructure pipeline deploys the projects and infrastructure (for example, the VM instances and databases) that are used by workloads. The foundation pipeline deploys multiple infrastructure pipelines. This separation between the foundation pipeline and infrastructure pipeline allows for a separation between platform-wide resources and workload-specific resources.\nThe following diagram describes how the blueprint configures multiple infrastructure pipelines that are intended for use by separate teams.\nThe diagram describes the following key concepts:\n- Each infrastructure pipeline is used to manage infrastructure resources independently of the foundation resources.\n- Each business unit has its own infrastructure pipeline, managed in a dedicated project in the`common`folder.\n- Each of the infrastructure pipelines has a service account with permission to deploy resources only to the projects that are associated with that business unit. This strategy creates a separation of duties between the privileged service accounts used for the foundation pipeline and those used by each infrastructure pipeline\nThis approach with multiple infrastructure pipelines is recommended when you have multiple entities inside your organization that have the skills and appetite to manage their infrastructure separately, particularly if they have different requirements such as the types of pipeline validation policy they want to enforce. Alternatively, you might prefer to have a single infrastructure pipeline managed by a single team with consistent validation policies.\nIn the [terraform-example-foundation](https://github.com/terraform-google-modules/terraform-example-foundation/tree/master) , stage 4 configures an infrastructure pipeline, and stage 5 demonstrates an example of using that pipeline to deploy infrastructure resources.\n| Stage     | Description                        |\n|:-----------------------|:----------------------------------------------------------------------------------------------------------|\n| 4-projects    | Sets up a folder structure, projects, and an infrastructure pipeline.          |\n| 5-app-infra (optional) | Deploys workload projects with a Compute Engine instance using the infrastructure pipeline as an example. |\n### The application pipeline\nThe application pipeline is responsible for deploying application artifacts for each individual workload, such as images or Kubernetes containers that run the business logic of your application. These artifacts are deployed to infrastructure resources that were deployed by your infrastructure pipeline.\nThe enterprise foundation blueprint sets up your foundation pipeline and infrastructure pipeline, but doesn't deploy an application pipeline. For an example application pipeline, see the [enterprise application blueprint](/architecture/enterprise-application-blueprint/architecture) .\n## Automating your pipeline with Cloud Build\nThe blueprint uses Cloud Build to automate CI/CD processes. The following table describes the controls are built into the foundation pipeline and infrastructure pipeline that are deployed by the [terraform-example-foundation](https://github.com/terraform-google-modules/terraform-example-foundation/tree/master) repository. If you are developing your own pipelines using other CI/CD automation tools, we recommend that you apply similar controls.\n| Control               | Description                                                                                                                                           |\n|:----------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Separate build configurations to validate code before deploying | The blueprint uses two Cloud Build build configuration files for the entire pipeline, and each repository that is associated with a stage has two Cloud Build triggers that are associated with those build configuration files. When code is pushed to a repository branch, the build configuration files are triggered to first run cloudbuild-tf-plan.yaml which validates your code with policy checks and Terraform plan against that branch, then cloudbuild-tf-apply.yaml runs terraform apply on the outcome of that plan.             |\n| Terraform policy checks           | The blueprint includes a set of Open Policy Agent constraints that are enforced by the policy validation in Google Cloud CLI. These constraints define the acceptable resource configurations that can be deployed by your pipeline. If a build doesn't meet policy in the first build configuration, then the second build configuration doesn't deploy any resources.The policies enforced in the blueprint are forked from GoogleCloudPlatform/policy-library on GitHub. You can write additional policies for the library to enforce custom policies to meet your requirements. |\n| Principle of least privilege         | The foundation pipeline has a different service account for each stage with an allow policy that grants only the minimum IAM roles for that stage. Each Cloud Build trigger runs as the specific service account for that stage. Using different accounts helps mitigate the risk that modifying one repository could impact the resources that are managed by another repository. To understand the particular IAM roles applied to each service account, see the sa.tf Terraform code in the bootstrap stage.                  |\n| Cloud Build private pools          | The blueprint uses Cloud Build private pools. Private pools let you optionally enforce additional controls such as restricting access to public repositories or running Cloud Build inside a VPC Service Controls perimeter.                                                                                      |\n| Cloud Build custom builders          | The blueprint creates its own custom builder to run Terraform. For more information, see 0-bootstrap/Dockerfile. This control enforces that the pipeline consistently runs with a known set of libraries at pinned versions.                                                                                      |\n| Deployment approval            | Optionally, you can add a manual approval stage to Cloud Build. This approval adds an additional checkpoint after the build is triggered but before it runs so that a privileged user can manually approve the build.                                                                                        |\n## Branching strategy\nWe recommend a [persistent branch](https://git-scm.com/book/en/v2/Git-Branching-Branching-Workflows) strategy for submitting code to your Git system and deploying resources through the foundation pipeline. The following diagram describes the persistent branch strategy.\nThe diagram describes three persistent branches in Git (development, non-production, and production) that reflect the corresponding Google Cloud environments. There are also multiple ephemeral feature branches that don't correspond to resources that are deployed in your Google Cloud environments.\nWe recommend that you enforce a [pull request (PR)](https://git-scm.com/docs/git-request-pull) process into your Git system so that any code that is merged to a persistent branch has an approved PR.\nTo develop code with this persistent branch strategy, follow these high-level steps:\n- When you're developing new capabilities or working on a bug fix, create a new branch based off of the development branch. Use a naming convention for your branch that includes the type of change, a ticket number or other identifier, and a human-readable description, like`feature/123456-org-policies`.\n- When you complete the work in the feature branch, open a PR that targets the development branch.\n- When you submit the PR, the PR triggers the foundation pipeline to perform`terraform plan`and`terraform validate`to stage and verify the changes.\n- After you validate the changes to the code, merge the feature or bug fix into the development branch.\n- The merge process triggers the foundation pipeline to run`terraform apply`to deploy the latest changes in the development branch to the development environment.\n- Review the changes in the development environment using any manual reviews, functional tests, or end-to-end tests that are relevant to your use case. Then promote changes to the non-production environment by opening a PR that targets the non-production branch and merge your changes.\n- To deploy resources to the production environment, repeat the same process as step 6: review and validate the deployed resources, open a PR to the production branch, and merge.## What's next\n- Read about [operations best practices](/architecture/security-foundations/operation-best-practices) (next document in this series).\n# Operations best practices\nThis section introduces operations that you must consider as you deploy and operate additional workloads into your Google Cloud environment. This section isn't intended to be exhaustive of all operations in your cloud environment, but introduces decisions related to the architectural recommendations and resources deployed by the blueprint.\n## Update foundation resources\nAlthough the blueprint provides an opinionated starting point for your foundation environment, your foundation requirements might grow over time. After your initial deployment, you might adjust configuration settings or build new shared services to be consumed by all workloads.\nTo modify foundation resources, we recommend that you make all changes through the foundation pipeline. Review the [branching strategy](/architecture/security-foundations/deployment-methodology#branching-strategy) for an introduction to the flow of writing code, merging it, and triggering the deployment pipelines.\n## Decide attributes for new workload projects\nWhen creating new projects through the project factory module of the automation pipeline, you must configure various attributes. Your process to design and create projects for new workloads should include decisions for the following:\n- Which Google Cloud APIs to enable\n- Which Shared VPC to use, or whether to create a new VPC network\n- Which IAM roles to create for the initial`project-service-account`that is created by the pipeline\n- Which project labels to apply\n- The folder that the project is deployed to\n- Which billing account to use\n- Whether to add the project to a VPC Service Controls perimeter\n- Whether to configure a budget and billing alert threshold for the project\nFor a complete reference of the configurable attributes for each project, see the [input variables for the project factory](https://github.com/terraform-google-modules/terraform-example-foundation/tree/master/4-projects/modules/single_project#inputs) in the automation pipeline.\n## Manage permissions at scale\nWhen you deploy workload projects on top of your foundation, you must consider how you will grant access to the intended developers and consumers of those projects. We recommend that you add users into a group that is managed by your existing identity provider, synchronize the groups with Cloud Identity, and then apply IAM roles to the groups. Always keep in mind the [principle of least privilege](/iam/docs/using-iam-securely#least_privilege) .\nWe also recommend that you use [IAM recommender](/policy-intelligence/docs/role-recommendations-overview) to identify allow policies that grant over-privileged roles. Design a process to periodically review recommendations or automatically apply recommendations into your deployment pipelines.\n## Coordinate changes between the networking team and the application team\nThe network topologies that are deployed by the blueprint assume that you have a team responsible for managing network resources, and separate teams responsible for deploying workload infrastructure resources. As the workload teams deploy infrastructure, they must create firewall rules to allow the intended access paths between components of their workload, but they don't have permission to modify the network firewall policies themselves.\nPlan how teams will work together to coordinate the changes to the centralized networking resources that are needed to deploy applications. For example, you might design a process where a workload team requests [tags](/firewall/docs/tags-firewalls-overview) for their applications. The networking team then creates the tags and adds rules to the network firewall policy that allows traffic to flow between resources with the tags, and delegates the [IAM roles to use the tags](/firewall/docs/tags-firewalls-overview#iam) to the workload team.\n## Optimize your environment with the Active Assist portfolio\nIn addition to IAM recommender, Google Cloud provides the [Active Assist](/solutions/active-assist) portfolio of services to make recommendations about how to optimize your environment. For example, [firewall insights](/network-intelligence-center/docs/firewall-insights/how-to/using-firewall-insights) or the [unattended project recommender](/recommender/docs/unattended-project-recommender) provide actionable recommendations that can help tighten your security posture.\nDesign a process to periodically review recommendations or automatically apply recommendations into your deployment pipelines. Decide which recommendations should be managed by a central team and which should be the responsibility of workload owners, and apply IAM roles to access the recommendations accordingly.\n## Grant exceptions to organization policies\nThe blueprint enforces a set of organization policy constraints that are recommended to most customers in most scenarios, but you might have legitimate use cases that require limited exceptions to the organization policies you enforce broadly.\nFor example, the blueprint enforces the [iam.disableServiceAccountKeyCreation](/resource-manager/docs/organization-policy/restricting-service-accounts#disable_service_account_key_creation) constraint. This constraint is an important security control because a leaked service account key can have a significant negative impact, and most scenarios should use [more secure alternatives to service account keys](/docs/authentication#auth-decision-tree) to authenticate. However, there might be use cases that can only authenticate with a service account key, such as an on-premises server that requires access to Google Cloud services and cannot use workload identity federation. In this scenario, you might decide to allow an exception to the policy, so long as additional compensating controls like [best practices for managing service account keys](/iam/docs/best-practices-for-managing-service-account-keys) are enforced.\nTherefore, you should design a process for workloads to request an exception to policies, and ensure that the decision makers who are responsible for granting exceptions have the technical knowledge to validate the use case and consult on whether additional controls must be in place to compensate. When you grant an exception to a workload, modify the organization policy constraint as narrowly as possible. You can also [conditionally add constraints to an organization policy](/resource-manager/docs/organization-policy/tags-organization-policy#conditionally_add_constraints_to_organization_policy) by defining a tag that grants an exception or enforcement for policy, then applying the tag to projects and folders.\n## Protect your resources with VPC Service Controls\nThe blueprint helps prepare your environment for VPC Service Controls by separating the base and restricted networks. However, by default, the Terraform code doesn't enable VPC Service Controls because this enablement can be a disruptive process.\nA perimeter denies access to restricted Google Cloud services from traffic that originates outside the perimeter, which includes the console, developer workstations, and the foundation pipeline used to deploy resources. If you use VPC Service Controls, you must design exceptions to the perimeter that allow the access paths that you intend.\nA VPC Service Controls perimeter is intended for exfiltration controls between your Google Cloud organization and external sources. The perimeter isn't intended to replace or duplicate allow policies for granular access control to individual projects or resources. When you [design and architect a perimeter](/vpc-service-controls/docs/architect-perimeters) , we recommend using a common unified perimeter for lower management overhead.\nIf you must design multiple perimeters to granularly control service traffic within your Google Cloud organization, we recommend that you clearly define the threats that are addressed by a more complex perimeter structure and the access paths between perimeters that are needed for intended operations.\nTo adopt VPC Service Controls, evaluate the following:\n- Which of your use cases require VPC Service Controls.\n- Whether the required Google Cloud services [support VPC Service Controls](/vpc-service-controls/docs/supported-products) .\n- How to configure breakglass access to modify the perimeter in case it disrupts your automation pipelines.\n- How to use [best practices for enablingb VPC Service Controls](/vpc-service-controls/docs/enable) to design and implement your perimeter.\nAfter the perimeter is enabled, we recommend that you design a process to consistently add new projects to the correct perimeter, and a process to design exceptions when developers have a new use case that is denied by your current perimeter configuration.\n## Test organization-wide changes in a separate organization\nWe recommend that you never deploy changes to production without testing. For workload resources, this approach is facilitated by separate environments for development, non-production, and production. However, some resources at the organization don't have separate environments to facilitate testing.\nFor changes at the organization-level, or other changes that can affect production environments like the configuration between your identity provider and Cloud Identity, consider creating a separate organization for test purposes.\n## Control remote access to virtual machines\nBecause we recommend that you deploy immutable infrastructure through the foundation pipeline, infrastructure pipeline, and application pipeline, we also recommend that you only grant developers direct access to a virtual machine through SSH or RDP for limited or exceptional use cases.\nFor scenarios that require remote access, we recommend that you manage user access using [OS Login](/compute/docs/oslogin) where possible. This approach uses managed Google Cloud services to enforce access control, account lifecycle management, two-step verification, and audit logging. Alternatively, if you must allow access through [SSH keys in metadata](/compute/docs/instances/access-overview#ssh-access) or [RDP credentials](/compute/docs/instances/windows/generating-credentials) , it is your responsibility to manage the credential lifecycle and store credentials securely outside of Google Cloud.\nIn any scenario, a user with SSH or RDP access to a VM can be a privilege escalation risk, so you should design your access model with this in mind. The user can run code on that VM with the privileges of the associated service account or [query the metadata server](/compute/docs/metadata/overview#metadata_security_considerations) to view the access token that is used to authenticate API requests. This access can then be a privilege escalation if you didn't deliberately intend for the user to operate with the privileges of the service account.\n## Mitigate overspending by planning budget alerts\nThe blueprint implements best practices introduced in the [Google Cloud Architecture Framework: Cost Optimization](/architecture/framework/cost-optimization) for managing cost, including the following:\n- Use a single billing account across all projects in the enterprise foundation.\n- Assign each project a `billingcode` metadata label that is used to allocate cost between cost centers.\n- Set budgets and alert thresholds.\nIt's your responsibility to plan budgets and configure billing alerts. The blueprint [creates budget alerts](/billing/docs/how-to/budgets) for workload projects when the forecasted spending is on track to reach 120% of the budget. This approach lets a central team identify and mitigate incidents of significant overspending. Significant unexpected increases in spending without a clear cause can be an indicator of a security incident and should be investigated from the perspectives of both cost control and security.\n**Note:** Budget alerts cover a different type of notification than the [Billing category of Essential Contacts](/resource-manager/docs/managing-notification-contacts) . Budget alerts are related to the consumption of budgets that you define for each project. Billing notifications from Essential Contacts are related to pricing updates, errors, and credits.\nDepending on your use case, you might set a budget that is based on the cost of an entire environment folder, or all projects related to a certain cost center, instead of setting granular budgets for each project. We also recommend that you delegate budget and alert setting to workload owners who might set more granular alerting threshold for their day-to-day monitoring.\nFor guidance on building FinOps capabilities, including forecasting budgets for workloads, see [Getting started with FinOps on Google Cloud](https://services.google.com/fh/files/misc/cloud_finops_getting_started.pdf) .\n## Allocate costs between internal cost centers\nThe console lets you [view your billing reports](/billing/docs/how-to/reports) to view and forecast cost in multiple dimensions. In addition to the prebuilt reports, we recommend that you export billing data to a BigQuery dataset in the `prj-c-billing-logs` project. The exported billing records allow you to allocate cost on custom dimensions, such as your internal cost centers, based on project label metadata like `billingcode` .\nThe following SQL query is a sample query to understand costs for all projects that are grouped by the `billingcode` project label.\n```\n#standardSQLSELECT\u00a0 \u00a0(SELECT value from UNNEST(labels) where key = 'billingcode') AS costcenter,\u00a0 \u00a0service.description AS description,\u00a0 \u00a0SUM(cost) AS charges,\u00a0 \u00a0SUM((SELECT SUM(amount) FROM UNNEST(credits))) AS creditsFROM PROJECT_ID.DATASET_ID.TABLE_NAMEGROUP BY costcenter, descriptionORDER BY costcenter ASC, description ASC\n```\nTo set up this export, see [export Cloud Billing data to BigQuery](/billing/docs/how-to/export-data-bigquery) .\nIf you require internal accounting or chargeback between cost centers, it's your responsibility to incorporate the data that is obtained from this query into your internal processes.\n## Ingest findings from detective controls into your existing SIEM\nAlthough the foundation resources help you configure aggregated destinations for audit logs and security findings, it is your responsibility to decide how to consume and use these signals.\nIf you have a requirement to aggregate logs across all cloud and on-premise environments into an existing SIEM, decide how to ingest logs from the `prj-c-logging` project and findings from Security Command Center into your existing tools and processes. You might create a single export for all logs and findings if a single team is responsible for monitoring security across your entire environment, or you might create multiple exports filtered to the set of logs and findings needed for multiple teams with different responsibilities.\nAlternatively, if log volume and cost are prohibitive, you might avoid duplication by retaining Google Cloud logs and findings only in Google Cloud. In this scenario, ensure that your existing teams have the right access and training to work with logs and findings directly in Google Cloud.\n- For audit logs, design [log views](/logging/docs/logs-views#create_view) to grant access to a subset of logs in your centralized logs bucket to individual teams, instead of duplicating logs to multiple buckets which increases log storage cost.\n- For security findings, grant [folder-level and project-level roles](/security-command-center/docs/access-control-org#folder-level_and_project-level_roles) for Security Command Center to let teams view and manage security findings just for the projects for which they are responsible, directly in the console.## Continuously develop your controls library\nThe blueprint starts with a baseline of controls to detect and prevent threats. We recommend that you review these controls and add additional controls based on your requirements. The following table summarizes the mechanisms to enforce governance policies and how to extend these for your additional requirements:\n| Policy controls enforced by the blueprint                                | Guidance to extend these controls                                     |\n|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Security Command Center detects vulnerabilities and threats from multiple security sources.                   | Define custom modules for Security Health Analytics and custom modules for Event Threat Detection.                     |\n| The Organization Policy service enforces a recommended set of organization policy constraints on Google Cloud services.            | Enforce additional constraints from the premade list of available constraints or create custom constraints.                   |\n| Open Policy Agent (OPA) policy validates code in the foundation pipeline for acceptable configurations before deployment.            | Develop additional constraints based on the guidance at GoogleCloudPlatform/policy-library.                       |\n| Alerting on log-based metrics and performance metrics configures log-based metrics to alert on changes to IAM policies and configurations of some sensitive resources. | Design additional log-based metrics and alerting policies for log events that you expect shouldn't occur in your environment.              |\n| A custom solution for automated log analysis regularly queries logs for suspicious activity and creates Security Command Center findings.        | Write additional queries to create findings for security events that you want to monitor, using security log analytics as a reference.            |\n| A custom solution to respond to asset changes creates Security Command Center findings and can automate remediation actions.           | Create additional Cloud Asset Inventory feeds to monitor changes for particular asset types and write additional Cloud Functions with custom logic to respond to policy violations. |\nThese controls might evolve as your requirements and maturity on Google Cloud change.\n## Manage encryption keys with Cloud Key Management Service\nGoogle Cloud provides [default encryption atrest](/docs/security/encryption/default-encryption) for all customer content, but also provides [Cloud Key Management Service (Cloud KMS)](/kms/docs/key-management-service) to provide you additional control over your encryption keys for data at rest. We recommend that you evaluate whether the default encryption is sufficient, or whether you have a compliance requirement that you must use Cloud KMS to manage keys yourself. For more information, see [decide how to meet compliance requirements for encryption at rest](/architecture/landing-zones/decide-security#encrypt-rest) .\nThe blueprint provides a `prj-c-kms` project in the common folder and a `prj-{env}-kms` project in each environment folder for managing encryption keys centrally. This approach lets a central team audit and manage encryption keys that are used by resources in workload projects, in order to meet regulatory and compliance requirements.\nDepending on your operational model, you might prefer a single centralized project instance of Cloud KMS under the control of a single team, you might prefer to manage encryption keys separately in each environment, or you might prefer multiple distributed instances so that accountability for encryption keys can be delegated to the appropriate teams. Modify the Terraform code sample as needed to fit your operational model.\nOptionally, you can enforce [customer-managed encryption keys (CMEK) organization policies](/kms/docs/cmek-org-policy) to enforce that certain resource types always require a CMEK key and that only CMEK keys from an allowlist of trusted projects can be used.\n## Store and audit application credentials with Secret Manager\nWe recommend that you never commit sensitive secrets (such as API keys, passwords, and private certificates) to source code repositories. Instead, commit the secret to [Secret Manager](/secret-manager/docs/overview) and grant the [Secret Manager Secret Accessor](/secret-manager/docs/access-control#secretmanager.secretAccessor) IAM role to the user or service account that needs to access the secret. We recommend that you grant the IAM role to an individual secret, not to all secrets in the project.\nWhen possible, you should generate production secrets automatically within the CI/CD pipelines and keep them inaccessible to human users except in breakglass situations. In this scenario, ensure that you don't grant IAM roles to view these secrets to any users or groups.\nThe blueprint provides a single `prj-c-secrets` project in the common folder and a `prj-{env}-secrets` project in each environment folder for managing secrets centrally. This approach lets a central team audit and manage secrets used by applications in order to meet regulatory and compliance requirements.\nDepending on your operational model, you might prefer a single centralized instance of Secret Manager under the control of a single team, or you might prefer to manage secrets separately in each environment, or you might prefer multiple distributed instances of Secret Manager so that each workload team can manage their own secrets. Modify the Terraform code sample as needed to fit your operational model.\n## Plan breakglass access to highly privileged accounts\nAlthough we recommend that changes to foundation resources are managed through version-controlled IaC that is deployed by the foundation pipeline, you might have exceptional or emergency scenarios that require privileged access to modify your environment directly. We recommend that you plan for breakglass accounts (sometimes called firecall or emergency accounts) that have highly privileged access to your environment in case of an emergency or when the automation processes break down.\nThe following table describes some example purposes of breakglass accounts.\n| Breakglass purpose    | Description                                         |\n|:----------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Super admin      | Emergency access to the Super admin role used with Cloud Identity, to, for example, fix issues that are related to identity federation or multi-factor authentication (MFA). |\n| Organization administrator  | Emergency access to the Organization Administrator role, which can then grant access to any other IAM role in the organization.            |\n| Foundation pipeline administrator | Emergency access to modify the resources in your CICD project on Google Cloud and external Git repository in case the automation of the foundation pipeline breaks down.  |\n| Operations or SRE     | An operations or SRE team needs privileged access to respond to outages or incidents. This can include tasks like restarting VMs or restoring data.       |\nYour mechanism to permit breakglass access depends on the existing tools and procedures you have in place, but a few example mechanisms include the following:\n- Use your existing tools for privileged access management to temporarily add a user to a group that is predefined with highly-privileged IAM roles or use the credentials of a highly-privileged account.\n- Pre-provision accounts intended only for administrator usage. For example, developer Dana might have an identity dana@example.com for daily use and admin-dana@example.com for breakglass access.\n- Use an application like [just-in-time privileged access](/architecture/manage-just-in-time-privileged-access-to-project) that allows a developer to self-escalate to more privileged roles.\nRegardless of the mechanism you use, consider how you operationally address the following questions:\n- How do you design the scope and granularity of breakglass access? For example, you might design a different breakglass mechanism for different business units to ensure that they cannot disrupt each other.\n- How does your mechanism prevent abuse? Do you require approvals? For example, you might have split operations where one person holds credentials and one person holds the MFA token.\n- How do you audit and alert on breakglass access? For example, you might configure a [custom Event Threat Detection module](/security-command-center/docs/custom-modules-etd-overview) to create a security finding when a predefined breakglass account is used.\n- How do you remove the breakglass access and resume normal operations after the incident is over?\nFor common privilege escalation tasks and rolling back changes, we recommend designing automated workflows where a user can perform the operation without requiring privilege escalation for their user identity. This approach can help reduce human error and improve security.\nFor systems that require regular intervention, automating the fix might be the best solution. Google encourages customers to adopt a zero-touch production approach to make all production changes using automation, [safe proxies](https://google.github.io/building-secure-and-reliable-systems/raw/ch03.html) , or audited breakglass. Google provides the [SRE books](https://sre.google/books/) for customers who are looking to adopt Google's SRE approach.\n## What's next\n- Read [Deploy the blueprint](/architecture/security-foundations/summary) (next document in this series).\n# Deploy the blueprint\nThis section describes the process that you can use to deploy the blueprint, its naming conventions, and alternatives to blueprint recommendations.\n## Bringing it all together\nTo deploy your own enterprise foundation in alignment with the best practices and recommendations from this blueprint, follow the high-level tasks summarized in this section. Deployment requires a combination of prerequisite setup steps, automated deployment through the [terraform-example-foundation](https://github.com/terraform-google-modules/terraform-example-foundation) on GitHub, and additional steps that must be configured manually after the initial foundation deployment is complete.\n| Process               | Steps                                                                                                                                                                                                                                                                                           |\n|:-----------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Prerequisites before deploying the foundation pipeline resources | Complete the following steps before you deploy the foundation pipeline: Create a Cloud Identity account and verify domain ownership. Apply for an invoiced billing account with your Google Cloud sales team or create a self-service billing account. Enforce security best practices for administrator accounts. Verify and reconcile issues with consumer user accounts. Configure your external identity provider as source of truth for synchronizing user accounts and SSO. Provision the groups for access control that are required to run the blueprint. Determine the network topology that you will use. Decide your source code management tool. The instructions for terraform-example-foundation are written to a Git repository that is hosted in Cloud Source Repositories. Decide your CI/CD automation tools. The terraform-example-foundation provides different sets of directions for different automation tools. To connect to an an existing on-premises environment, prepare the following: Plan your IP address allocation based on the number and size of ranges that are required by the blueprint. Order your Dedicated Interconnect connections. |\n| Steps to deploy the terraform-example-foundation from GitHub  | Follow the README directions for each stage to deploy the terraform-example-foundation from GitHub: Stage 0-bootstrap to create a foundation pipeline. If using a self-service billing account, you must request additional project quota before proceeding to the next stage. Stage 1-org to configure organization-level resources. Stage 2-environments to create environments. Stage either 3-networks-dual-svpc or 3-networks-hub-and-spoke to create networking resources in your preferred topology. Stage 4-projects to create an infrastructure pipeline. Optionally, stage 5-app-infra for sample usage of the infrastructure pipeline.                                                                                                                                |\n| Additional steps after IaC deployment       | After you deploy the Terraform code, complete the following: Complete the on-premises configuration changes. Activate Security Command Center Premium. Export Cloud Billing data to BigQuery. Sign up for a Cloud Customer Care plan. Enable Access Transparency logs. Share data from Cloud Identity with Google Cloud. Apply the administrative controls for Cloud Identity which aren't automated by the IaC deployment. Assess which of the additional administrative controls for customers with sensitive workloads are appropriate for your use case. Review operation best practices and plan how to connect your existing operations and capabilities to the foundation resources.                                                                                                                     |\n## Additional administrative controls for customers with sensitive workloads\nGoogle Cloud provides additional administrative controls that can help your security and compliance requirements. However, some controls involve additional cost or operational trade-offs that might not be appropriate for every customer. These controls also require customized inputs for your specific requirements that can't be fully automated in the blueprint with a default value for all customers.\nThis section introduces security controls that you apply centrally to your foundation. This section isn't intended to be exhaustive of all the security controls that you can apply to specific workloads. For more information on Google's security products and solutions, see [Google Cloud security best practices center](/security/best-practices) .\nEvaluate whether the following controls are appropriate for your foundation based on your compliance requirements, risk appetite, and sensitivity of data.\n| Control           | Description                                                                                                                                                                           |\n|:-------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Protect your resources with VPC Service Controls | VPC Service Controls lets you define security policies that prevent access to Google-managed services outside of a trusted perimeter, block access to data from untrusted locations, and mitigate data exfiltration risks. However, VPC Service Controls can cause existing services to break until you define exceptions to allow intended access patterns.Evaluate whether the value of mitigating exfiltration risks justifies the increased complexity and operational overhead of adopting VPC Service Controls. The blueprint prepares restricted networks and optional variables to configure VPC Service Controls, but the perimeter isn't enabled until you take additional steps to design and enable it. |\n| Restrict resource locations      | You might have regulatory requirements that cloud resources must only be deployed in approved geographical locations. This organization policy constraint enforces that resources can only be deployed in the list of locations you define.                                                                                                                   |\n| Enable Assured Workloads       | Assured Workloads provides additional compliance controls that help you meet specific regulatory regimes. The blueprint provides optional variables in the deployment pipeline for enablement.                                                                                                                              |\n| Enable data access logs       | You might have a requirement to log all access to certain sensitive data or resources.Evaluate where your workloads handle sensitive data that requires data access logs, and enable the logs for each service and environment working with sensitive data.                                                                                                               |\n| Enable Access Approval       | Access Approval ensures that Cloud Customer Care and engineering require your explicit approval whenever they need to access your customer content.Evaluate the operational process required to review Access Approval requests to mitigate possible delays in resolving support incidents.                                                                                                       |\n| Enable Key Access Justifications     | Key Access Justifications lets you programmatically control whether Google can access your encryption keys, including for automated operations and for Customer Care to access your customer content.Evaluate the cost and operational overhead associated with Key Access Justifications as well as its dependency on Cloud External Key Manager (Cloud EKM).                                                                                      |\n| Disable Cloud Shell        | Cloud Shell is an online development environment. This shell is hosted on a Google-managed server outside of your environment, and thus it isn't subject to the controls that you might have implemented on your own developer workstations.If you want to strictly control which workstations a developer can use to access cloud resources, disable Cloud Shell. You might also evaluate Cloud Workstations for a configurable workstation option in your own environment.                                                          |\n| Restrict access to the Google Cloud console  | Google Cloud lets you restrict access to the Google Cloud console based on access level attributes like group membership, trusted IP address ranges, and device verification. Some attributes require an additional subscription to BeyondCorp Enterprise.Evaluate the access patterns that you trust for user access to web-based applications such as the console as part of a larger zero trust deployment.                                                                          |\n## Naming conventions\nWe recommend that you have a standardized naming convention for your Google Cloud resources. The following table describes recommended conventions for resource names in the blueprint.\n| Resource       | Naming convention                                                                                                                                                                                                                                                       |\n|:-----------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Folder        | fldr-environmentenvironment is a description of the folder-level resources within the Google Cloud organization. For example, bootstrap, common, production, nonproduction, development, or network.For example: fldr-production                                                                                                                                                                                                   |\n| Project ID       | prj-environmentcode-description-randomid environmentcode is a short form of the environment field (one of b, c, p, n, d, or net). Shared VPC host projects use the environmentcode of the associated environment. Projects for networking resources that are shared across environments, like the interconnect project, use the net environment code. description is additional information about the project. You can use short, human-readable abbreviations. randomid is a randomized suffix to prevent collisions for resource names that must be globally unique and to mitigate against attackers guessing resource names. The blueprint automatically adds a random four-character alphanumeric identifier. For example: prj-c-logging-a1b2                                                                      |\n| VPC network      | vpc-environmentcode-vpctype-vpcconfig environmentcode is a short form of the environment field (one of b, c, p, n, d, or net). vpctype is one of shared, float, or peer. vpcconfig is either base or restricted to indicate whether the network is intended to be used with VPC Service Controls or not. For example: vpc-p-shared-base                                                                                                                                                                         |\n| Subnet        | sn-environmentcode-vpctype-vpcconfig-region{-description} environmentcode is a short form of the environment field (one of b, c, p, n, d, or net). vpctype is one of shared, float, or peer. vpcconfig is either base or restricted to indicate whether the network is intended to be used with VPC Service Controls or not. region is any valid Google Cloud region that the resource is located in. We recommend removing hyphens and using an abbreviated form of some regions and directions to avoid hitting character limits. For example, au (Australia), na (North America), sa (South America), eu (Europe), se (southeast), or ne (northeast). description is additional information about the subnet. You can use short, human-readable abbreviations. For example: sn-p-shared-restricted-uswest1                                                        |\n| Firewall policies     | fw-firewalltype-scope-environmentcode{-description} firewalltype is hierarchical or network. scope is global or the Google Cloud region that the resource is located in. We recommend removing hyphens and using an abbreviated form of some regions and directions to avoid reaching character limits. For example, au (Australia), na (North America), sa (South America), eu (Europe), se (southeast), or ne (northeast). environmentcode is a short form of the environment field (one of b, c, p, n, d, or net) that owns the policy resource. description is additional information about the hierarchical firewall policy. You can use short, human-readable abbreviations. For example:fw-hierarchical-global-c-01fw-network-uswest1-p-shared-base                                                                    |\n| Cloud Router      | cr-environmentcode-vpctype-vpcconfig-region{-description} environmentcode is a short form of the environment field (one of b, c, p, n, d, or net). vpctype is one of shared, float, or peer. vpcconfig is either base or restricted to indicate whether the network is intended to be used with VPC Service Controls or not. region is any valid Google Cloud region that the resource is located in. We recommend removing hyphens and using an abbreviated form of some regions and directions to avoid reaching character limits. For example, au (Australia), na (North America), sa (South America), eu (Europe), se (southeast), or ne (northeast). description is additional information about the Cloud Router. You can use short, human-readable abbreviations. For example: cr-p-shared-base-useast1-cr1                                                      |\n| Cloud Interconnect connection  | ic-dc-colo dc is the name of your data center to which a Cloud Interconnect is connected. colo is the colocation facility name that the Cloud Interconnect from the on-premises data center is peered with. For example: ic-mydatacenter-lgazone1                                                                                                                                                                                               |\n| Cloud Interconnect VLAN attachment | vl-dc-colo-environmentcode-vpctype-vpcconfig-region{-description} dc is the name of your data center to which a Cloud Interconnect is connected. colo is the colocation facility name that the Cloud Interconnect from the on-premises data center is peered with. environmentcode is a short form of the environment field (one of b, c, p, n, d, or net). vpctype is one of shared, float, or peer. vpcconfig is either base or restricted to indicate whether the network is intended to be used with VPC Service Controls or not. region is any valid Google Cloud region that the resource is located in. We recommend removing hyphens and using an abbreviated form of some regions and directions to avoid reaching character limits. For example, au (Australia), na (North America), sa (South America), eu (Europe), se (southeast), or ne (northeast). description is additional information about the VLAN. You can use short, human-readable abbreviations. For example: vl-mydatacenter-lgazone1-p-shared-base-useast1-cr1 |\n| Group        | grp-gcp-description@example.com Where description is additional information about the group. You can use short, human-readable abbreviations.For example: grp-gcp-billingadmin@example.com                                                                                                                                                                                                            |\n| Custom role      | rl-descriptionWhere description is additional information about the role. You can use short, human-readable abbreviations.For example: rl-customcomputeadmin                                                                                                                                                                                                                    |\n| Service account     | sa-description@projectid.iam.gserviceaccount.comWhere: description is additional information about the service account. You can use short, human-readable abbreviations. projectid is the globally unique project identifier. For example: sa-terraform-net@prj-b-seed-a1b2.iam.gserviceaccount.com                                                                                                                                                                                  |\n| Storage bucket      | bkt-projectid-descriptionWhere: projectid is the globally unique project identifier. description is additional information about the storage bucket. You can use short, human-readable abbreviations. For example: bkt-prj-c-infra-pipeline-a1b2-app-artifacts                                                                                                                                                                                           |\n## Alternatives to default recommendations\nThe best practices that are recommended in the blueprint might not work for every customer. You can customize any of the recommendations to meet your specific requirements. The following table introduces some of the common variations that you might require based on your existing technology stack and ways of working.\n| Decision area                                                              | Possible alternatives                                                                                                                                                                                           |\n|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Organization: The blueprint uses a single organization as the root node for all resources.                                           | Decide a resource hierarchy for your Google Cloud landing zone introduces scenarios in which you might prefer multiple organizations, such as the following: Your organization includes sub-companies that are likely to be sold in the future or that run as completely separate entities. You want to experiment in a sandbox environment with no connectivity to your existing organization.                                                                                                 |\n| Folder structure: The blueprint has a simple folder structure, with workloads divided into production, non-production and development folders at the top layer.                         | Decide a resource hierarchy for your Google Cloud landing zone introduces other approaches for structuring folders based on how you want to manage resources and inherit policies, such as: Folders based on application environments Folders based on regional entities or subsidiaries Folders based on accountability framework                                                                                                                |\n| Organization policies: The blueprint enforces all organization policy constraints at the organization node.                                      | You might have different security policies or ways of working for different parts of the business. In this scenario, enforce organization policy constraints at a lower node in the resource hierarchy. Review the complete list of organization policy constraints that help meet your requirements.                                                                                                                       |\n| Deployment pipeline tooling: The blueprint uses Cloud Build to run the automation pipeline.                                          | You might prefer other products for your deployment pipeline, such as Terraform Enterprise, GitLab Runners, GitHub Actions, or Jenkins. The blueprint includes alternative directions for each product.                                                                                                                                               |\n| Code repository for deployment: The blueprint uses Cloud Source Repositories as the managed private Git repository.                                    | Use your preferred version control system for managing code repositories, such as GitLab, GitHub, or Bitbucket.If you use a private repository that is hosted in your on-premises environment, configure a private network path from your repository to your Google Cloud environment.                                                                                                                           |\n| Identity provider: The blueprint assumes an on-premises Active Directory and federates identities to Cloud Identity using Google Cloud Directory Sync.                            | If you already use Google Workspace, you can use the Google identities that are already managed in Google Workspace.If you don't have an existing identity provider, you might create and manage user identities directly in Cloud Identity.If you have an existing identity provider, such as Okta, Ping, or Azure Entra ID, you might manage user accounts in your existing identity provider and synchronize to Cloud Identity.If you have data sovereignty or compliance requirements that prevent you from using Cloud Identity, and if you don't require managed Google user identities for other Google services such as Google Ads or Google Marketing Platform, then you might prefer workforce identity federation. In this scenario, be aware of limitations with supported services. |\n| Multiple regions: The blueprint deploys regional resources into two different Google Cloud regions to help enable workload design with high availability and disaster recovery requirements in mind.                | If you have end users in more geographical locations, you might configure more Google Cloud regions to create resources closer to the end user with less latency.If you have data sovereignty constraints or your availability needs can be met in a single region, you might configure only one Google Cloud region.                                                                                                                   |\n| IP address allocation: The blueprint provides a set of IP address ranges.                                               | You might need to change the specific IP address ranges that are used based on the IP address availability in your existing hybrid environment. If you modify the IP address ranges, use the blueprint as guidance for the number and size of ranges required, and review the valid IP address ranges for Google Cloud.                                                                                                                   |\n| Hybrid networking: The blueprint uses Dedicated Interconnect across multiple physical sites and Google Cloud regions for maximum bandwidth and availability.                          | Depending on your requirements for cost, bandwidth, and reliability requirements, you might configure Partner Interconnect or Cloud VPN instead.If you need to start deploying resources with private connectivity before a Dedicated Interconnect can be completed, you might start with Cloud VPN and change to using Dedicated Interconnect later.If you don't have an existing on-premises environment, you might not need hybrid networking at all.                                                                                   |\n| VPC Service Controls perimeter: The blueprint recommends a single perimeter which includes all the service projects that are associated with a restricted VPC network. Projects that are associated with a base VPC network are not included inside the perimeter. | You might have a use case that requires multiple perimeters for an organization or you might decide not to use VPC Service Controls at all.For information, see decide how to mitigate data exfiltration through Google APIs.                                                                                                                                         |\n| Secret Manager: The blueprint deploys a project for using Secret Manager in the common folder for organization-wide secrets, and a project in each environment folder for environment-specific secrets.               | If you have a single team who is responsible for managing and auditing sensitive secrets across the organization, you might prefer to use only a single project for managing access to secrets.If you let workload teams manage their own secrets, you might not use a centralized project for managing access to secrets, and instead let teams use their own instances of Secret Manager in workload projects.                                                                                             |\n| Cloud KMS: The blueprint deploys a project for using Cloud KMS in the common folder for organization-wide keys, and a project for each environment folder for keys in each environment.                   | If you have a single team who is responsible for managing and auditing encryption keys across the organization, you might prefer to use only a single project for managing access to keys. A centralized approach can help meet compliance requirements like PCI key custodians.If you let workload teams manage their own keys, you might not use a centralized project for managing access to keys, and instead let teams use their own instances of Cloud KMS in workload projects.                                                                           |\n| Aggregated log sinks: The blueprint configures a set of log sinks at the organization node so that a central security team can review audit logs from across the entire organization.                    | You might have different teams who are responsible for auditing different parts of the business, and these teams might require different logs to do their jobs. In this scenario, design multiple aggregated sinks at the appropriate folders and projects and create filters so that each team receives only the necessary logs, or design log views for granular access control to a common log bucket.                                                                                              |\n| Monitoring scoping projects: The blueprint configures a single monitoring scoping project for each environment.                                     | You might configure more granular scoping projects that are managed by different teams, scoped to the set of projects that contain the applications that each team manages.                                                                                                                                                      |\n| Granularity of infrastructure pipelines: The blueprint uses a model where each business unit has a separate infrastructure pipeline to manage their workload projects.                        | You might prefer a single infrastructure pipeline that is managed by a central team if you have a central team who is responsible for deploying all projects and infrastructure. This central team can accept pull requests from workload teams to review and approve before project creation, or the team can create the pull request themselves in response to a ticketed system.You might prefer more granular pipelines if individual workload teams have the ability to customize their own pipelines and you want to design more granular privileged service accounts for the pipelines.                                                 |\n| SIEM exports:The blueprint manages all security findings in Security Command Center.                                            | Decide whether you will export security findings from Security Command Center to tools such as Chronicle or your existing SIEM, or whether teams will use the console to view and manage security findings. You might configure multiple exports with unique filters for different teams with different scopes and responsibilities.                                                                                                                |\n| DNS lookups for Google Cloud services from on-premises: The blueprint configures a unique Private Service Connect endpoint for each Shared VPC, which can help enable designs with multiple VPC Service Controls perimeters.          | You might not require routing from an on-premises environment to Private Service Connect endpoints at this level of granularity if you don't require multiple VPC Service Control perimeters.Instead of mapping on-premises hosts to Private Service Connect endpoints by environment, you might simplify this design to use a single Private Service Connect endpoint with the appropriate API bundle, or use the generic endpoints for private.googlepais.com and restricted.googleapis.com.                                                                         |\n## What's next\n- Implement the blueprint using the [Terraform example foundation](https://github.com/terraform-google-modules/terraform-example-foundation/tree/master) on GitHub.\n- Learn more about best practice design principles with the [Google Cloud Architecture Framework](/architecture/framework) .\n- Review the [library of blueprints](/docs/terraform/blueprints/terraform-blueprints) to help you accelerate the design and build of common enterprise workloads, including the following:- [Import data from Google Cloud into a secured BigQuery data warehouse](/architecture/confidential-data-warehouse-blueprint) \n- [Import data from an external network into a secured BigQuery data warehouse](/architecture/secured-data-warehouse-blueprint-onprem) \n- [Deploy a secured serverless architecture using Cloud Functions](/architecture/serverless-functions-blueprint) \n- [Deploy a secured serverless architecture using Cloud Run](/architecture/serverless-blueprint) \n- See [related solutions](/solutions) to deploy on top of your foundation environment.\n- For access to a demonstration environment, contact us at [security-foundations-blueprint-support@google.com](mailto:security-foundations-blueprint-support@google.com) .", "guide": "Docs"}