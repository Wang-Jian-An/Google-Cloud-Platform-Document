{"title": "Docs - Data transformation between MongoDB Atlas and Google Cloud", "url": "https://cloud.google.com/architecture/data-pipeline-mongodb-gcp", "abstract": "# Docs - Data transformation between MongoDB Atlas and Google Cloud\nLast reviewed 2023-12-13 UTC\nMany companies use MongoDB as an operational datastore and want to enrich the value of that data by performing complex analytics on it. To do this, the MongoDB data needs to be aggregated and moved into a data warehouse where analytics can be performed. This reference architecture describes how you can configure this integration pipeline in Google Cloud.\nIn this architecture, you use Dataflow templates to integrate data from [MongoDB Atlas](https://www.mongodb.com/atlas/database) into [BigQuery](/bigquery) . These Dataflow templates transform the document format that is used by MongoDB into the columnar format that is used by BigQuery. These templates rely on [Apache Beam](https://beam.apache.org/) libraries to perform this transformation. Therefore, this document assumes that you're familiar with MongoDB, and have some familiarity with Dataflow and Apache Beam.\n", "content": "## Architecture\nThe following diagram shows the reference architecture that you use when you deploy this solution. This diagram demonstrates how various Dataflow templates move and transform data from MongoDB into a BigQuery data warehouse.\nAs the diagram shows, this architecture is based on the following three templates:\n- [MongoDB to BigQuery template](/dataflow/docs/guides/templates/provided/mongodb-to-bigquery) . This Dataflow template is a batch pipeline that reads documents from MongoDB and writes them to BigQuery where that data can be analyzed. If you wanted, you could extend this template by writing a user-defined function (UDF) in JavaScript. For a sample UDF, see [Operational efficiency](#operational-efficiency) .\n- [BigQuery to MongoDB template](/dataflow/docs/guides/templates/provided/bigquery-to-mongodb) . This Dataflow template is a batch template that can be used to read the analyzed data from BigQuery and write them to MongoDB.\n- [MongoDB to BigQuery (CDC) template](/dataflow/docs/guides/templates/provided/mongodb-change-stream-to-bigquery) . This Dataflow template is a streaming pipeline that works with MongoDB change streams. You create [a publisher application](/pubsub/docs/publisher) that pushes changes from the MongoDB change stream to Pub/Sub. The pipeline then reads the JSON records from Pub/Sub and writes them to BigQuery. Like the MongoDB to BigQuery template, you could extend this template by writing a UDF.By using the MongoDB to BigQuery (CDC) template, you can make sure that any changes that occur in the MongoDB collection are published to Pub/Sub. To set up a MongoDB change stream, follow the instructions in [Change streams](https://www.mongodb.com/docs/manual/changeStreams/) in the MongoDB documentation.## Use cases\nUsing BigQuery to analyze MongoDB Atlas data can be useful in a range of industries, which includes financial services, retail, manufacturing and logistics, and gaming applications.\n### Financial services\nGoogle Cloud and MongoDB Atlas offer solutions to handle the complex and ever changing data needs of today's financial institutions. By using BigQuery to analyze your financial data from MongoDB Atlas, you can develop solutions for the following tasks:\n- **Real-time fraud detection** . Financial institutions want to detect and prevent fraudulent transactions in real time. By using machine learning (ML) and analyzing customer behavior data in BigQuery, you can identify patterns that are indicative of fraud.\n- **Personalized customer experiences** . Financial institutions are also interested in delivering personalized customer experiences. By storing and analyzing customer data in BigQuery, you can create solutions that generate personalized recommendations, offer tailored products and services, and provide better customer support.\n- **Risk management** . Financial institutions are always wanting processes that help identify and mitigate risks. By analyzing data from a variety of sources in BigQuery, you can help identify patterns and trends that indicate potential risks.\n### Retail\nSmart use of customer data with the ability to combine them with product data and execution of real-time personalized engagements define future e-commerce. To meet customer needs, retailers need to make data-driven decisions by collecting and analyzing data. BigQuery and MongoDB Atlas lets you use customer data to drive innovation in personalization, such as in the following areas:\n- **Omnichannel commerce** . Use MongoDB to store and manage data from a variety of sources, including online and offline stores, mobile apps, and social media. This storage and management of data coupled with BigQuery analytics makes it ideal for omnichannel retailers who need to provide a seamless experience for their customers across all channels.\n- **Real-time insights** . By using BigQuery, you can gain real-time insights into your customers, inventory, and sales performance. These insights help you make better decisions about pricing, promotion and product placement.\n- **Personalized recommendations** . Personalized recommendation engines help retailers increase sales and customer satisfaction. By storing and analyzing customer data, you can identify patterns and trends that can be used to recommend products that are likely to be of interest to each individual customer.\n### Manufacturing and Logistics\nAnalyzing MongoDB data in BigQuery also offers the following benefits to the manufacturing and logistics industry:\n- **Real-time visibility** . You can gain real-time visibility into your operations. This helps you make better decisions about your production, inventory, and shipping.\n- **Supply chain optimization** . Managing supply chain uncertainty and analyzing data from different sources helps you reduce costs and improve efficiency.\n### Gaming\nAnalysis in BigQuery also empowers game developers and publishers to create cutting-edge games and deliver unparalleled gaming experiences, including the following:\n- **Real-time gameplay** . You can use your analysis to create real-time gameplay experiences to generate leaderboards, matchmaker systems, and multiplayer features.\n- **Personalized player experiences** . You can use artificial intelligence (AI) and ML to deliver targeted recommendations and personalize game experience for players.\n- **Game analytics** . You can analyze game data to identify trends and patterns that help you improve game design, gameplay, and your business decisions.## Design alternatives\nYou have two alternatives to using Dataflow templates as an integration pipeline from MongoDB to BigQuery: Pub/Sub with a BigQuery subscription, or Confluent Cloud.\n### Pub/Sub with a BigQuery subscription\nAs an alternative to using Dataflow templates, you can use Pub/Sub to set up an integration pipeline between your MongoDB cluster and BigQuery. To use Pub/Sub instead of Dataflow, do the following steps:\n- Configure a Pub/Sub schema and topic to ingest the messages from your MongoDB change stream.\n- Create a [BigQuery subscription in Pub/Sub](/pubsub/docs/create-bigquery-subscription) that writes messages to an existing BigQuery table as they are received. If you don't use a BigQuery subscription, you will need a pull or push subscription and a subscriber (such as Dataflow) that reads messages and writes them to BigQuery. **Note:** Pub/Sub BigQuery subscriptions might generate duplicate records in the destination table.\n- Set up a change stream that listens for new documents inserted in your MongoDB and matches the schema used for Pub/Sub.\nFor details about this alternative, see [Create a Data Pipeline for MongoDB Change Stream Using Pub/Sub BigQuery Subscription](https://www.mongodb.com/developer/products/mongodb/stream-data-mongodb-bigquery-subscription/) .\n### Confluent Cloud\nIf you don't want to create your own publisher application to monitor the MongoDB change stream, you could use Confluent Cloud instead. In this approach, you use Confluent to configure a MongoDB Atlas source connector to read the MongoDB data stream. You then configure a BigQuery sink connector to sink the data from the Confluent cluster to BigQuery.\nFor details about this alternative, see [Streaming Data from MongoDB to BigQuery Using Confluent Connectors](https://www.mongodb.com/developer/products/atlas/mongodb-bigquery-pipeline-using-confluent/) .\n## Design considerations\nWhen creating a MongoDB Atlas to BigQuery solution, you should take into consideration the following areas.\n### Security, privacy, and compliance\nWhen you run your integration pipeline, Dataflow uses the following two service accounts to manage security and permissions:\n- [The Dataflow service account.](/dataflow/docs/concepts/security-and-permissions#df-service-account) The Dataflow service uses the Dataflow service account as part of the job creation request, such as to check project quota and to create worker instances on your behalf. The Dataflow service also uses this account to manage the job during job execution. This account is also known as the Dataflow service agent.\n- [The worker service account.](/dataflow/docs/concepts/security-and-permissions#worker-service-account) Worker instances use the worker service account to access input and output resources after you submit your job. By default, workers use your project's [Compute Engine default service account](/compute/docs/access/service-accounts#default_service_account) as the worker service account. The worker service account must have`roles/dataflow.worker`.\nIn addition, your Dataflow pipelines need to be able to access Google Cloud resources. To allow this access, you need to grant the required roles to the worker service account for your Dataflow project so that the project can access the resources while running the Dataflow job. For example, if your job writes to BigQuery, your service account must also have at least the `roles/bigquery.dataEditor` role on the table or other resource that is to be updated.\n### Cost optimization\nThe cost of running the Dataflow templates depends on the worker nodes that are scheduled and the type of pipeline. To understand costs, see [Dataflow pricing](/dataflow/pricing) .\nEach Dataflow template can take care of moving data between one MongoDB collection to one BigQuery table. Therefore, as the number of collections increases, the cost of utilizing Dataflow templates might increase as well.\n### Operational efficiency\nTo efficiently use and analyze your MongoDB data, you might need to perform a custom transformation of that data. For example, you might need to reformat your MongoDB data to match a target schema or to redact sensitive data or to filter some elements from the output. If you need to perform such a transformation, you can use a UDF to extend the functionality of the MongoDB to BigQuery template without having to modify the template code.\nA UDF is a JavaScript function. The UDF should expect to receive and return a JSON string. The following code shows an example transformation:\n```\n/*** A simple transform function.* @param {string} inJson* @return {string} outJson*/function transform(inJson) {\u00a0 \u00a0var outJson = JSON.parse(inJson);\u00a0 \u00a0outJson.key = \"value\";\u00a0 \u00a0return JSON.stringify(outJson);}\n```\nFor more information about how to create a UDF, see [Create user-defined functions for Dataflow templates](/dataflow/docs/guides/templates/create-template-udf) .\nAfter you've created your UDF, you then need to do the following steps to extend the MongoDB to BigQuery template to use this UDF:\n- First, you need to load the JavaScript file that contains the UDF into Google Cloud Storage.\n- Then, when you create the Dataflow job from the template, you need to set the following template parameters:- Set the`javascriptDocumentTransformGcsPath`parameter to the Cloud Storage location of the JavaScript file.\n- Set the`javascriptDocumentTransformFunctionName`parameter to the name of the UDF.For more information about extending the template with a UDF, see [MongoDB to BigQuery template](/dataflow/docs/guides/templates/provided/mongodb-to-bigquery) .\n### Performance optimization\nThe performance of the MongoDB to BigQuery transformation depends on the following factors:\n- The size of the MongoDB document.\n- The number of MongoDB collections.\n- Whether the transformation relies on a fixed schema or a varying schema.\n- The implementation team's knowledge of schema transformations that use Javascript-based UDFs.## Deployment\nTo deploy this reference architecture, see [Deploy a data transformation between MongoDB and Google Cloud](/architecture/data-pipeline-mongodb-gcp/deployment) .\n## What's next\n- To customize Google Dataflow Templates, see the templates on [GitHub](https://github.com/GoogleCloudPlatform/DataflowTemplates) .\n- Learn more about MongoDB Atlas and Google Cloud solutions on [Cloud skill boost](https://www.cloudskillsboost.google/quests/306?utm_source=qwiklabs&utm_medium=lp&utm_campaign=gpe-hol) .\n- Learn more about the Google Cloud products used in this reference architecture:- [BigQuery](/bigquery) \n- [Pub/Sub](/pubsub) \n- [Dataflow](/dataflow) \n- [Compute Engine](/compute) \n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .## Contributors\nAuthors:\n- [Saurabh Kumar](https://www.linkedin.com/in/saurabh-kumar-48556a19) | ISV Partner Engineer\n- [Venkatesh Shanbhag](https://www.linkedin.com/in/venkatesh-shanbhag) | Senior Solutions Architect (MongoDB)\nOther contributors:\n- [Jun Liu](https://www.linkedin.com/in/jun-liu-1759093a) | Supportability Tech Lead\n- [Maridi Raju Makaraju](https://www.linkedin.com/in/mmraju) | Supportability Tech Lead\n- [Sergei Lilichenko](https://www.linkedin.com/in/sergei-lilichenko) | Solutions Architect\n- Shan Kulandaivel | Group Product Manager", "guide": "Docs"}