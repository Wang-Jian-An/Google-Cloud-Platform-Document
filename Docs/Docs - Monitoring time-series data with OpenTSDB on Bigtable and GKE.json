{"title": "Docs - Monitoring time-series data with OpenTSDB on Bigtable and GKE", "url": "https://cloud.google.com/architecture/monitoring-time-series-data-opentsdb", "abstract": "# Docs - Monitoring time-series data with OpenTSDB on Bigtable and GKE\nLast reviewed 2022-01-12 UTC\nThis guide describes how to collect, record, and monitor\n [time-series data](https://wikipedia.org/wiki/Time_series) \non Google Cloud by using\n [OpenTSDB](http://opentsdb.net/) \nrunning on\n [Google Kubernetes Engine (GKE)](/kubernetes-engine) \nand\n [Bigtable](/bigtable) \n.\nTime-series data is a highly valuable asset that you can use for various applications, including trending, monitoring, and machine learning. You can generate time-series data from server infrastructure, application code, and other sources. OpenTSDB can collect and retain large amounts of time-series data with a high degree of granularity.\nThis guide shows software engineers and architects how to create a scalable collection layer for time-series data by using GKE. It also shows how to work with the collected data by using Bigtable. This guide assumes that you are familiar with Kubernetes and Bigtable.\nThe following diagram shows the high-level architecture of this guide:The preceding diagram shows multiple sources of time-series data, such as IoT events and system metrics, that are stored in Bigtable by using OpenTSDB deployed on GKE.", "content": "## Objectives\n- Build container images used in this guide using Cloud Build.\n- Manage these container images using Artifact Registry.\n- Create a Bigtable instance.\n- Create a GKE cluster.\n- Deploy OpenTSDB to your GKE cluster.\n- Send time-series metrics to OpenTSDB.\n- Visualize metrics using OpenTSDB and [Grafana](https://grafana.com/) .\n## CostsIn this document, you use the following billable components of Google Cloud:- [Compute Engine](/compute/vm-instance-pricing) \n- [GKE](/kubernetes-engine/pricing) \n- [Bigtable](/bigtable/pricing) \n- [Cloud Storage](/storage/pricing) \n- [Cloud Build](/cloud-build/pricing) \n- [Artifact Registry](/artifact-registry/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- In the Google Cloud console, on the project selector page,   select or [create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . **Note** : If you don't plan to keep the  resources that you create in this procedure, create a project instead of  selecting an existing project. After you finish these steps, you can  delete the project, removing all resources associated with the project. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- Enable the Bigtable, Bigtable Admin, GKE, Compute Engine, Cloud Build, and Artifact Registry APIs. [Enable the APIs](https://console.cloud.google.com/flows/enableapi?apiid=bigtable,bigtableadmin.googleapis.com,compute.googleapis.com,container.googleapis.com,cloudbuild.googleapis.com,artifactregistry.googleapis.com) \n- In the Google Cloud console, go to the **Welcome** page. [Go to the Welcome page](https://console.cloud.google.com/welcome) Make a note of the project ID because it's used in a later step.\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n## Create a Bigtable instanceThis guide uses Bigtable to store the time-series data that you collect, so you must create a Bigtable instance.\nBigtable is a key/wide-column store that [works well for time-series data](/bigtable/docs/schema-design-time-series) . Bigtable supports the HBase API, so you can use software designed to work with [Apache HBase](https://hbase.apache.org/) , such as OpenTSDB. For more information about the HBase schema used by OpenTSDB, see [HBase Schema](http://opentsdb.net/docs/build/html/user_guide/backends/hbase.html) .\nA key component of OpenTSDB is the [AsyncHBase](https://github.com/OpenTSDB/asynchbase) client, which enables you to bulk-write to HBase in a fully asynchronous, non-blocking, thread-safe manner. When you use OpenTSDB with Bigtable, AsyncHBase is implemented as the [AsyncBigtable](https://github.com/OpenTSDB/asyncbigtable) client.\nThis guide uses a Bigtable instance with a single-node cluster. When moving to a production environment, consider using Bigtable instances with larger clusters. For more information about picking a cluster size, see [Understanding Bigtable performance](/bigtable/docs/performance) .- In Cloud Shell, set the environment variables for your Google Cloud zone where you will create your Bigtable cluster and GKE cluster and the instance identifier for your Bigtable cluster:```\nexport BIGTABLE_INSTANCE_ID=BIGTABLE_INSTANCE_IDexport ZONE=ZONE\n```Replace the following:- ``: The identifier for your Bigtable instance.\n- ``: The zone where your Bigtable cluster and GKE cluster will be created.\nThe command should look similar to the following example:```\nexport BIGTABLE_INSTANCE_ID=bt-opentsdb\nexport ZONE=us-central1-f\n```\n- Create the Bigtable instance:```\ngcloud bigtable instances create ${BIGTABLE_INSTANCE_ID} \\\u00a0 \u00a0 --cluster-config=id=${BIGTABLE_INSTANCE_ID}-${ZONE},zone=${ZONE},nodes=1 \\\u00a0 \u00a0 --display-name=OpenTSDB\n```\n## Create the images used to deploy and test OpenTSDBTo deploy and demonstrate OpenTSDB with a Bigtable storage backend, this guide uses a series of Docker container images that are deployed to GKE. You build several of these images using code from an accompanying GitHub repository with Cloud Build. When deploying infrastructure to GKE, a container repository is used. In this guide, you use Artifact Registry to manage these container images.- In Cloud Shell, set the environment variables for your Google Cloud zone where you will create your Artifact Registry repository:```\nexport PROJECT_ID=PROJECT_IDexport REGION=REGIONexport AR_REPO=AR_REPO\n```Replace the following:- ``: Your project ID\n- ``: The region where your Artifact Registry repository will be created\n- ``: The name of your Artifact Registry repository\nThe command should look similar to the following example:```\nexport PROJECT_ID=bt-opentsdb-project-id\nexport REGION=us-central1\nexport AR_REPO=opentsdb-bt-repo\n```\n- Create an Artifact Registry repository:```\ngcloud artifacts repositories create ${AR_REPO} \\\u00a0 \u00a0 --repository-format=docker \u00a0\\\u00a0 \u00a0 --location=${REGION} \\\u00a0 \u00a0 --description=\"OpenTSDB on bigtable container images\"\n```\n## Create and manage the images used to deploy and demonstrate OpenTSDBTwo Docker container images are used in this guide. The first image is used for two purposes: to perform the one-time Bigtable database setup for OpenTSDB, and to deploy the read and write service containers for the OpenTSDB deployment. The second image is used to generate sample metric data to demonstrate your OpenTSDB deployment.\nWhen you submit the container image build job to Cloud Build, you tag the images so that they are stored in the Artifact Registry after they are built.- In Cloud Shell, clone the GitHub repository that contains the accompanying code:```\ngit clone https://github.com/GoogleCloudPlatform/opentsdb-bigtable.git\n```\n- Go to the sample code directory:```\ncd opentsdb-bigtable\n```\n- Set the environment variables for the OpenTSDB server image that uses Bigtable as the storage backend:```\nexport SERVER_IMAGE_NAME=opentsdb-server-bigtableexport SERVER_IMAGE_TAG=2.4.1\n```\n- Build the image using Cloud Build:```\ngcloud builds submit \\\u00a0 \u00a0 --tag ${REGION}-docker.pkg.dev/${PROJECT_ID}/${AR_REPO}/${SERVER_IMAGE_NAME}:${SERVER_IMAGE_TAG} \\\u00a0 \u00a0 build\n```Because you tagged the image appropriately, when the build is complete, the image will be managed by your Artifact Registry repository.\n- Set the environment variables for the demonstration time series data generation image:```\nexport GEN_IMAGE_NAME=opentsdb-timeseries-generateexport GEN_IMAGE_TAG=0.1\n```\n- Build the image using Cloud Build:```\ncd generate-ts./build-cloud.shcd ..\n```\n## Create a GKE clusterGKE provides a managed [Kubernetes](https://kubernetes.io/) environment. After you create a GKE cluster, you can deploy Kubernetes [Pods](https://kubernetes.io/docs/concepts/workloads/pods/pod/) to it. This guide uses GKE and Kubernetes Pods to run OpenTSDB.\nOpenTSDB separates its storage from its application layer, which enables it to be simultaneously deployed across multiple instances. By running in parallel, OpenTSDB can handle a large amount of time-series data.- In Cloud Shell, set the environment variables for the Google Cloud zone where you will create your Bigtable cluster and GKE cluster and the name, node type, and version for your GKE cluster:```\nexport GKE_CLUSTER_NAME=GKE_CLUSTER_NAMEexport GKE_VERSION=1.20export GKE_NODE_TYPE=n1-standard-4\n```Replace `` with the name of your GKE cluster.The command should look similar to the following example:```\nexport GKE_CLUSTER_NAME=gke-opentsdb\nexport GKE_VERSION=1.20\nexport GKE_NODE_TYPE=n1-standard-4\n```\n- Create a GKE cluster:```\ngcloud container clusters create ${GKE_CLUSTER_NAME} \\\u00a0 \u00a0 --zone=${ZONE} \\\u00a0 \u00a0 --cluster-version=${GKE_VERSION} \\\u00a0 \u00a0 --machine-type ${GKE_NODE_TYPE} \\\u00a0 \u00a0 --scopes \"https://www.googleapis.com/auth/cloud-platform\"\n```This operation can take a few minutes to complete. Adding the [scopes](https://developers.google.com/identity/protocols/googlescopes) to your GKE cluster allows your OpenTSDB container to interact with Bigtable and [Container Registry](/container-registry) .The rest of this guide uses the containers you have just built that are managed by [Artifact Registry](/artifact-registry) . The [Dockerfile](https://github.com/GoogleCloudPlatform/opentsdb-bigtable/blob/master/build/Dockerfile) and [entrypoint](https://github.com/GoogleCloudPlatform/opentsdb-bigtable/blob/master/build/docker-entrypoint.sh) script used to build the container are located in the `build` folder of the guide repository.\n- Get the credentials so that you can connect to your GKE cluster:```\ngcloud container clusters get-credentials ${GKE_CLUSTER_NAME} --zone ${ZONE}\n```\n## Create a ConfigMap with configuration detailsKubernetes uses the [ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/) to decouple configuration details from the container image in order to make applications more portable. The configuration for OpenTSDB is specified in the `opentsdb.conf` file. A ConfigMap containing the `opentsdb.conf` file is included with the sample code.\nIn this and following steps, you use the GNU `envsubst` utility to replace environment variable placeholders in the YAML template files will the respective values for your deployment.- Create a ConfigMap from the updated `opentsdb-config.yaml` file:```\nenvsubst < configmaps/opentsdb-config.yaml.tpl | kubectl create -f ``` **Note:** OpenTSDB offers various configuration [options](http://opentsdb.net/docs/build/html/user_guide/configuration.html) . To apply your configuration, modify the `opentsdb.conf` ConfigMap and [push the changes](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#kubectl-apply) to the cluster. Some changes might require you to restart processes.\n## Create OpenTSDB tables in BigtableBefore you can read or write data using OpenTSDB, you need to [create tables](http://opentsdb.net/docs/build/html/installation.html#create-tables) in Bigtable to store that data. To create the tables, you will [create a Kubernetes job](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/) .- In Cloud Shell, launch the job:```\nenvsubst < jobs/opentsdb-init.yaml.tpl | kubectl create -f ```The job can take up to a minute or more to complete. Verify that the job has completed successfully:```\nkubectl describe jobs\n```The output shows that one job has succeeded when `Pods Statuses` shows `1 Succeeded`\n- Examine the table creation job logs:```\nOPENTSDB_INIT_POD=$(kubectl get pods --selector=job-name=opentsdb-init \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --output=jsonpath={.items..metadata.name})kubectl logs $OPENTSDB_INIT_POD\n```The output is similar to the following:```\ncreate 'tsdb-uid',\n {NAME => 'id', COMPRESSION => 'NONE', BLOOMFILTER => 'ROW', DATA_BLOCK_ENCODING => 'DIFF'},\n {NAME => 'name', COMPRESSION => 'NONE', BLOOMFILTER => 'ROW', DATA_BLOCK_ENCODING => 'DIFF'}\n0 row(s) in 3.2730 seconds\ncreate 'tsdb',\n {NAME => 't', VERSIONS => 1, COMPRESSION => 'NONE', BLOOMFILTER => 'ROW', DATA_BLOCK_ENCODING => 'DIFF'}\n0 row(s) in 1.8440 seconds\ncreate 'tsdb-tree',\n {NAME => 't', VERSIONS => 1, COMPRESSION => 'NONE', BLOOMFILTER => 'ROW', DATA_BLOCK_ENCODING => 'DIFF'}\n0 row(s) in 1.5420 seconds\ncreate 'tsdb-meta',\n {NAME => 'name', COMPRESSION => 'NONE', BLOOMFILTER => 'ROW', DATA_BLOCK_ENCODING => 'DIFF'}\n0 row(s) in 1.9910 seconds\n```The output lists each table that was created. This job runs several table creation commands, each using the format of `create` `` . The tables are successfully created when you have output in the form of `0 row(s) in` `` `seconds` .- ``: the name of the table that the job creates\n- ``: the amount of time it took to create the table\n **Note:** Bigtable automatically performs [data compression](/bigtable/docs/overview#data_compression) , so it disables user-configurable compression at an HBase level.\n### Data modelThe tables that you created store data points from OpenTSDB. In a later step, you write time-series data into these tables. Time-series data points are organized and stored as follows:\n| Field  | Required      | Description          | Example      |\n|:----------|:-----------------------------|:------------------------------------------------|:----------------------------|\n| metric | Required      | Item that is being measured\u2014the default key  | sys.cpu.user    |\n| timestamp | Required      | Unix epoch time of the measurement    | 1497561091     |\n| tags  | At least one tag is required | Qualifies the measurement for querying purposes | hostname=www cpu=0 env=prod |\n| value  | Required      | Measurement value        | 89.3      |\nThe metric, timestamp, and tags (tag key and tag value) form the row key. The timestamp is normalized to one hour, to ensure that a row does not contain too many data points. For more information, see [HBase Schema](http://opentsdb.net/docs/build/html/user_guide/backends/hbase.html) .## Deploy OpenTSDBThe following diagram shows the deployment architecture for OpenTSTB with its services running on GKE and with Bigtable as the storage backend:This guide uses two OpenTSDB Kubernetes [deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) : one deployment sends metrics to Bigtable and the other deployment reads from it. Using two deployments prevents long-running reads and writes from blocking each other. The Pods in each deployment use the same container image. OpenTSDB provides a daemon called [tsd](http://opentsdb.net/docs/build/html/user_guide/cli/tsd.html) that runs in each container. A single `tsd` process can handle a high throughput of events per second. To distribute load, each deployment in this guide creates three replicas of the read and write Pods.- In Cloud Shell, create a deployment for writing metrics:```\nenvsubst < deployments/opentsdb-write.yaml.tpl | kubectl create -f \u00a0```The configuration information for the write deployment is in the `opentsdb-write.yaml.tpl` file in the `deployments` folder of the guide repository.\n- Create a deployment for reading metrics:```\nenvsubst < deployments/opentsdb-read.yaml.tpl | kubectl create -f \u00a0```The configuration information for the reader deployment is in the `opentsdb-read.yaml.tpl` file in the `deployments` folder of the guide repository.\nIn a production deployment, you can increase the number of `tsd` Pods that are running, either manually or by using [autoscaling](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/) in Kubernetes. Similarly, you can increase the number of instances in your GKE cluster manually or by using [cluster autoscaler](/container-engine/docs/cluster-autoscaler) .## Create the OpenTSDB servicesIn order to provide consistent network connectivity to the deployments, you create two Kubernetes [services](https://kubernetes.io/docs/concepts/services-networking/service/) : one service writes metrics into OpenTSDB and the other reads.- In Cloud Shell, create the service for writing metrics:```\nkubectl create -f services/opentsdb-write.yaml\n```The configuration information for the metrics writing service is contained in the `opentsdb-write.yaml` file in the `services` folder of the guide repository. This service is created inside your Kubernetes cluster and is reachable by other services running in your cluster. **Note:** In a production environment, you can expose the service to the rest of your network by using an [internal load balancer](/solutions/prep-container-engine-for-prod#connecting_to_a_container_engine_cluster_from_inside_gcp) or [you can expose it](/solutions/prep-container-engine-for-prod#connecting_to_a_container_engine_cluster_from_inside_gcp) to the internet by adding a [LoadBalancer](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/) in the service definition.\n- Create the service for reading metrics:```\nkubectl create -f services/opentsdb-read.yaml\n```The configuration information for the metrics reading service is contained in the `opentsdb-read.yaml` file in the `services` folder of the guide repository.\n## Write time-series data to OpenTSDBThere are several mechanisms to [write data](http://opentsdb.net/docs/build/html/user_guide/writing/index.html) into OpenTSDB. After you define service endpoints, you can direct processes to begin writing data to them. This guide deploys a Python service that emits demonstrative time-series data for two metrics: **Cluster Memory Utilization** ( `memory_usage_gauge` ) and **Cluster CPU Utilization** ( `cpu_node_utilization_gauge` ).- In Cloud Shell, deploy the time series metric generator to your cluster:```\nenvsubst < deployments/generate.yaml.tpl | kubectl create -f ```\n## Examine the example time-series data with OpenTSDBYou can query time-series metrics by using the `opentsdb-read` service endpoint that you deployed earlier in the guide. You can use the data in various ways. One common option is to visualize it. OpenTSDB includes a basic interface to visualize metrics that it collects. This guide uses [Grafana](https://grafana.com/) , a popular alternative for visualizing metrics that provides additional functionality.\nRunning Grafana in your cluster requires a similar process that you used to set up OpenTSDB. In addition to creating a ConfigMap and a deployment, you need to configure [port forwarding](https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/) so that you can access Grafana while it is running in your Kubernetes cluster.- In Cloud Shell, create the Grafana ConfigMap using the configuration information in the `grafana.yaml` file in the `configmaps` folder of the guide repository:```\nkubectl create -f configmaps/grafana.yaml\n```\n- Create the Grafana deployment using the configuration information in the `grafana.yaml` file in the `deployments` folder of the guide repository:```\nkubectl create -f deployments/grafana.yaml\n```\n- Get the name of the Grafana Pod in the cluster and use it to set up port forwarding:```\nGRAFANA_PODS=$(kubectl get pods --selector=app=grafana \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0--output=jsonpath={.items..metadata.name})kubectl port-forward $GRAFANA_PODS 8080:3000\n```Verify that forwarding was successful. The output is similar to the following:```\nForwarding from 127.0.0.1:8080 -> 3000\n```\n- To connect to the Grafana web interface, in Cloud Shell, click **Web Preview** and then select **Preview on port 8080** .For more information, see [Using web preview](/shell/docs/using-web-preview) .A new browser tab opens and connects to the Grafana web interface. After a few moments, the browser displays graphs like the following: This deployment of Grafana has been customized for this guide. The files `configmaps/grafana.yaml` and `deployments/grafana.yaml` configure Grafana to connect to the `opentsdb-read` service, allow anonymous authentication, and display some basic cluster metrics. For a deployment of Grafana in a production environment, we recommend that you implement the proper [authentication mechanisms](https://grafana.com/docs/grafana/latest/auth/) and use richer time-series graphs.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the individual resources\n- Delete the Kubernetes cluster to delete all the artifacts that you created:```\ngcloud container clusters delete GKE_CLUSTER_NAME\n```To delete the Kubernetes cluster, confirm by typing `Y` .\n- To delete the Bigtable instance, do the following:- In the Google Cloud console, go to **Bigtable** . [Go to Bigtable](https://console.cloud.google.com/bigtable/instances) \n- Select the instance that you previously created, and then click **Delete instance** .\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- To learn how to improve the performance of your uses of OpenTSDB, see [Bigtable Schema Design for Time Series Data](/bigtable/docs/schema-design-time-series) .\n- To learn how to migrate from HBase to Bigtable, see [Migrating data from HBase to Bigtable](/solutions/migration/hadoop/hadoop-gcp-migration-data-hbase-to-bigtable) .\n- The video [Bigtable in Action](https://www.youtube.com/watch?v=KaRbKdMInuc#t) , from Google Cloud Next 17, describes field promotion\u2014an important performance improvement.\n- To learn more about default scopes for GKE clusters, see [cluster scopes](/sdk/gcloud/reference/container/clusters/create#--scopes) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Docs"}