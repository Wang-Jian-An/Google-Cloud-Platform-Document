{"title": "Docs - Deployment methodology", "url": "https://cloud.google.com/architecture/enterprise-application-blueprint/deployment-methodology", "abstract": "# Docs - Deployment methodology\nLast reviewed 2023-12-20 UTC\n**    Preview     ** This product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA products and features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nThe enterprise application blueprint is deployed through a series of automated systems and pipelines. Each pipeline deploys a particular aspect of the blueprint. Pipelines provide a controllable, auditable, and repeatable mechanism for building out the blueprint. The following diagram shows the interaction of the various pipelines, repositories, and personas.\nThe blueprint uses the following pipelines:\n- The(part of the enterprise foundations blueprint) deploys the application factory, the multi-tenant infrastructure pipeline, and the fleep-scope pipeline.\n- Thedeploys the GKE clusters, and the other managed services that the enterprise application blueprint relies on.\n- Theconfigures fleet scopes, namespaces, and RBAC roles and bindings.\n- Theprovides a mechanism to deploy new application pipelines through a templated process.\n- Theprovides a CI/CD pipeline to deploy services into GKE clusters.\n- deploys and maintains additional Kubernetes configurations, including Policy Controller constraints.", "content": "## Repositories, repository contributors, and repository change approvers\nThe blueprint pipelines are triggered through changes to Git repositories. The following table describes the repositories that are used throughout the blueprint, who contributes to the repository, who approves changes to the repository, which pipeline uses the repository, and the description of what the repository contains.\n| Repository  | Repository contributor  | Repository change approver  | Pipeline          | Description                               |\n|:------------------|:-----------------------------|:---------------------------------|:-----------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------|\n| infra    | Developer platform developer | Developer platform administrator | Foundation infrastructure pipeline    | Repository that contains the code to deploy the multi-tenant infrastructure pipeline, the application, and the fleet-scope pipeline |\n| eab-infra   | Developer platform developer | Developer platform administrator | Multi-tenant infrastructure     | The Terraform modules that are used by developer platform teams when they create the infrastructure         |\n| fleet-scope  | Developer platform developer | Developer platform administrator | Fleet-scope         | The repository that defines the fleet team scopes and namespaces in the fleet              |\n| app-factory  | Developer platform developer | Developer platform administrator | Application factory       | The code that defines the application repository and references the modules in the terraform-modules repository      |\n| app-template  | Application developer  | Application operator    | Application factory       | The base code that is placed in the app-code repository when the repository is first created          |\n| terraform-modules | Developer platform developer | Developer platform administrator | Application factoryMulti-tenant infrastructure | The Terraform modules that define the application and the infrastructure               |\n| app-code   | Application developer  | Application operator    | Application CI/CD        | The application code that is deployed into the GKE clusters                   |\n| config-policy  | Developer platform developer | Developer platform administrator | Config Sync         | The policies that are used by the GKE clusters to maintain their configurations              |\nAutomated pipelines help build security, auditability, traceability, repeatability, controllability, and compliance into the deployment process. By using different systems that have different permissions and putting different people into different operating groups, you create separation of responsibilities and follow the principle of least privilege.\n## Foundation infrastructure pipeline\nThe [foundation infrastructure](https://github.com/terraform-google-modules/terraform-example-foundation/tree/master/5-app-infra) pipeline is described in the enterprise foundations blueprint and is used as a generic entrypoint for further resource deployments. The following table describes the components that the pipeline creates.\n| Component       | Description                    |\n|:-------------------------------------|:-----------------------------------------------------------------------------------------|\n| Multi-tenant infrastructure pipeline | Creates the shared infrastructure that is used by all tenants of the developer platform. |\n| Fleet-scope pipeline     | Creates namespaces and RBAC role bindings.            |\n| Application factory     | Creates the application CI/CD pipelines that are used to deploy the services.   |\n## Multi-tenant infrastructure pipeline\nThe multi-tenant infrastructure infrastructure pipeline deploys fleets, GKE clusters, and related shared resources. The following diagram shows the components of the multi-tenant infrastructure pipeline.\nThe following table describes the components that the multi-tenant infrastructure pipeline builds.\n| Component         | Description                                 |\n|:---------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------|\n| GKE clusters         | Provides hosting for the services of the containerized application.                   |\n| Policy Controller       | Provides policies that help ensure the proper configuration of the GKE clusters and services.            |\n| Config Sync         | Applies the Policy Controller policies to clusters and maintains consistant appplication of the policies.         |\n| Cloud Key Management Service (Cloud KMS) key | Creates the encryption key that is based on the customer-managed encryption key (CMEK) for GKE, AlloyDB for PostgreSQL, and Secret Manager. |\n| Secret Manager secret      | Provides a secret store for the RSA key pair that's used for user authentication with JSON Web Tokens (JWT).        |\n| Google Cloud Armor security policy   | Provides the policy that's used by the Google Cloud Armor web-application firewall.               |\n## Fleet-scope pipeline\nThe fleet-scope pipeline is responsible for configuring the namespaces and RBAC bindings in the fleet project, which is then reflected in the associated GKE clusters. The following table describes the components that the fleet-scope pipeline builds.\n| Component     | Description                        |\n|:--------------------------|:---------------------------------------------------------------------------------------------------------|\n| Namespace     | Defines the logical clusters within the physical cluster.            |\n| RBAC (roles and bindings) | Defines the authorization that a Kubernetes service account has at the cluster level or namespace level. |\n## Application factory\nThe application factory is deployed by the foundation infrastructure pipeline and is used to create infrastructure for each new application. This infrastructure includes a Google Cloud project which holds the application CI/CD pipeline.\nAs engineering organizations scale, the application team can onboard new applications using the application factory. Scaling enables growth by adding discrete application CI/CD pipelines and supporting the infrastructure for deploying new applications within the multi-tenant architecture. The following diagram shows the application factory.\nThe application factory has the following components:\n- **Application factory repository:** A Git repository that stores the declarative application definition.\n- **Pipelines to create applications:** Pipelines that require Cloud Build to complete the following:- Create a declarative application definition and store it in the application catalog.\n- Apply the declarative application definition to create the application resources.\n- **Starter application template repository:** Templates for creating a simple application (for example, a Python, Golang, or Java microservice).\n- **Shared modules:** Terraform modules that are created with standard practices and that are used for multiple purposes, including application onboarding and deployment.\nThe following table lists the components that the application factory creates for each application.\n| Component       | Description                                   |\n|:-----------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------|\n| Application source code repository | Contains source code and related configuration used for building and deploying a specific application.            |\n| Application CI/CD pipeline   | A Cloud Build based pipeline that is used to connect to the source code repository and provides a CI/CD pipeline for deploying application services. |\n## Application CI/CD pipeline\nThe application CI/CD pipeline enables automated build and deployment of container-based applications. The pipeline consists of [continuous integration (CI)](https://en.wikipedia.org/wiki/Continuous_integration) and [continuous deployment (CD)](https://en.wikipedia.org/wiki/Continuous_deployment) steps. The pipeline architecture is based on the [Secure CI/CD blueprint](http://github.com/googlecloudplatform/terraform-google-secure-cicd) .\nThe application CI/CD pipeline uses [immutable](/solutions/best-practices-for-operating-containers#immutability) container images across your environments. Immutable container images help ensure that the same image is deployed across all environments and isn't modified while the container is running. If you must update the application code or apply a patch, you build a new image and redeploy it. The use of immutable container images requires you to [externalize your container configuration](https://kubernetes.io/docs/tutorials/configuration/configure-java-microservice/configure-java-microservice/) so that configuration information is read during run time.\nTo reach GKE clusters over a private network path and manage `kubeconfig` authentication, the application CI/CD pipeline interacts with the GKE clusters through the Connect gateway. The pipeline also uses [private pools](/build/docs/private-pools/private-pools-overview) for the CI/CD environment.\nEach application source code repository includes Kubernetes configurations. These configurations enable applications to successfully run as Kubernetes services on GKE. The following table describes the types of Kubernetes configurations that the application CI/CD pipeline applies.\n| Component     | Description                                                |\n|:---------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Deployment     | Defines a scaled set of pods (containers).                                        |\n| Service     | Makes a deployment reachable over the cluster network.                                     |\n| Virtual service   | Makes a service part of the service mesh.                                        |\n| Destination rule   | Defines how peers on the service mesh should reach a virtual service. Used in the blueprint to configure locality load balancing for east-west traffic.             |\n| Authorization policy  | Sets access control between workloads in the service mesh.                                    |\n| Kubernetes service account | Defines the identity that's used by a Kubernetes service. Workload Identity defines the Google Cloud service account that's used to access Google Cloud resources.          |\n| Gateway     | Allows external ingress traffic to reach a service. The gateway is only required by deployments that receive external traffic.                   |\n| GCPBackendPolicy   | Configure SSL, Google Cloud Armor, session affinity, and connection draining for deployments that receive external traffic. GCPBackendPolicy is used only by deployments that receive external traffic. |\n| PodMonitoring    | Configures collection of Prometheus metrics exported by an application.                                 |\n### Continuous integration\nThe following diagram shows the continuous integration process.\nThe process is the following:\n- A developer commits application code to the application source repository. This operation [triggers](/build/docs/automating-builds/create-manage-triggers) Cloud Build to begin the integration pipeline.\n- Cloud Build creates a [container image](/build/docs/building/build-containers) , [pushes](/artifact-registry/docs/docker/pushing-and-pulling) the container image to Artifact Registry, and creates an [image digest](/solutions/using-container-images) .\n- Cloud Build performs automated tests for the application. Depending on the application language, different testing packages may be performed.\n- Cloud Build performs the following scans on the container image:- Cloud Build analyzes the container using the [Container Structure Tests](https://github.com/GoogleContainerTools/container-structure-test) framework. This framework performs command tests, file existence tests, file content tests, and metadata tests.\n- Cloud Build uses [vulnerability scanning](/container-analysis/docs/vulnerability-scanning) to identify any vulnerabilities in the container image against a vulnerability database that's maintained by Google Cloud.\n- Cloud Build approves the image to continue in the pipeline after successful scan results.\n- [Binary Authorization](/binary-authorization/docs) signs the image. Binary Authorization is a service on Google Cloud that provides software supply-chain security for container-based applications by using [policies](/binary-authorization/docs/key-concepts#policies) , [rules](/binary-authorization/docs/key-concepts#rules) , [notes](/container-registry/docs/container-analysis#note) , [attestations](/binary-authorization/docs/key-concepts#attestations) , [attestors](/binary-authorization/docs/key-concepts#attestors) , and [signers](/binary-authorization/docs/key-concepts#signers) . At deployment time, the Binary Authorization policy enforcer helps ensure the provenance of the container before allowing the container to deploy.\n- Cloud Build creates a release in Cloud Deploy to begin the deployment process.\nTo see the security information for a build, go to the [Security insights panel](/build/docs/view-build-security-insights#view_the_security_insights_side_panel) . These insights include [vulnerabilities](/build/docs/view-build-security-insights#vulnerabilities) that were detected using Artifact Analysis, and the build's [level of security assurance](/build/docs/view-build-security-insights#slsa_level) denoted by SLSA guidelines.\nWhen you build your application, you should follow [best practices for building containers](/architecture/best-practices-for-building-containers) .\n### Continuous deployment\nThe following diagram shows the continuous deployment process.\nThe process is the following:\n- At the end of the build process, the application CI/CD pipeline creates a new [Cloud Deploy](/deploy/docs/overview) release to launch the newly built container images progressively to each environment.\n- Cloud Deploy initiates a rollout to the first environment of the deployment pipeline, which is usually development. Each deployment stage is configured to require manual approval.\n- The Cloud Deploy pipelines uses sequential deployment to deploy images to each cluster in an environment in order.\n- At the end of each deployment stage, Cloud Deploy [verifies](/deploy/docs/verify-deployment) the functionality of the deployed containers. These steps are configurable within the [Skaffold](https://skaffold.dev/) configuration for the applications.## Deploying a new application\nThe following diagram shows how the application factory and application CI/CD pipeline work together to create and deploy a new application.\nThe process for defining a new application is the following:\n- An application operator defines a new application within their tenant by executing a Cloud Build trigger to generate the application definition.\n- The trigger adds a new entry for the application in Terraform and commits the change to the application factory repository.\n- The committed change triggers the creation of application-specific repositories and projects.\n- Cloud Build completes the following:- Creates two new Git repositories to host the application's source code and IaC.\n- Pushes the Kubernetes manifests for network policies, and Workload Identity to the Configuration management repository.\n- Creates the application's CI/CD project and the Cloud Build IaC trigger.\n- The Cloud Build IaC trigger for the application creates the application CI/CD pipeline and the Artifact Registry repository in the application's CI/CD project.\n- Config Sync deploys the network policies and Workload Identity configurations to the multi-tenant GKE clusters.\n- The fleet scope creation pipeline creates the fleet scope and namespace for the application on multi-tenant GKE clusters.\n- The application's CI/CD pipeline performs the initial deployment of the application to the GKE clusters.\n- Optionally, the application team uses the Cloud Build IaC trigger to deploy projects and additional resources (for example, databases and other managed services) to dedicated single-tenant projects, one for each environment.## GKE Enterprise configuration and policy management\nIn the blueprint, developer platform administrators use [Config Sync](/anthos-config-management/docs/config-sync-overview) to create cluster-level configurations in each environment. Config Sync connects to a Git repository which serves as the source of truth for the chosen state of the cluster configuration. Config Sync continuously monitors the actual state of the configuration in the clusters and reconciles any discrepancies by applying updates to ensure adherence to the chosen state, despite manual changes. [Configs](/anthos-config-management/docs/how-to/configs) are applied to the environments (development, non-production, and production) by using a [branching strategy](/solutions/safe-rollouts-with-anthos-config-management#use_git_branches) on the repository.\nIn this blueprint, Config Sync applies [Policy Controller](/anthos-config-management/docs/concepts/policy-controller) constraints. These configurations define security and compliance controls as defined by developer platform administrators for the organization. This blueprint relies on other pipelines to apply other configurations: the application CI/CD pipelines apply application-specific configuration, and the fleet-scope pipeline creates namespaces and associated role bindings.\n## What's next\n- Read about the [Cymbal Bank application architecture](/architecture/enterprise-application-blueprint/cymbal-bank) (next document in this series).", "guide": "Docs"}