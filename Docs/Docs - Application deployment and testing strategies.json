{"title": "Docs - Application deployment and testing strategies", "url": "https://cloud.google.com/architecture/application-deployment-and-testing-strategies", "abstract": "# Docs - Application deployment and testing strategies\nLast reviewed 2023-07-20 UTC\nThis document provides an overview of commonly used application deployment and testing patterns. It looks at how the patterns work, the benefits they offer, and things to consider when you implement them.\nSuppose you want to upgrade a running application to a new version. To ensure a seamless rollout, you would typically consider the following:\n- How to minimize application downtime, if any.\n- How to manage and resolve incidents with minimal impact on users.\n- How to address failed deployments in a reliable, effective way.\n- How to minimize people and process errors to achieve predictable, repeatable deployments.\nThe deployment pattern you choose largely depends on your business goals. For example, you might need to roll out changes without any downtime, or roll out changes to an environment or a subset of users before you make a feature generally available. Each methodology discussed in this document accounts for particular goals that you need to meet before a deployment is deemed successful.\nThis document is intended for system administrators and DevOps engineers who work on defining and implementing release and deployment strategies for various applications, systems, and frameworks.\n", "content": "## Deployment strategies\nWhen you a service, it's not always exposed immediately to users. Sometimes, it's only after the service is that users see changes in the application. However, when a service is , deployment and release occur simultaneously. In this case, when you deploy the new version, it starts accepting production traffic. Alternatively, there are deployment strategies for provisioning multiple service versions in parallel. These deployment patterns let you control and manage which version receives an incoming request. Read [Kubernetes and the challenges of continuous software delivery](/solutions/addressing-continuous-delivery-challenges-in-a-kubernetes-world) for more information on , , and related concepts.\nThe deployment patterns discussed in this section offer you flexibility in automating the release of new software. What approach is best for you depends upon your goals.\n### Recreate deployment pattern\nWith a recreate deployment, you fully scale down the existing application version before you scale up the new application version.\nThe following diagram shows how a recreate deployment works for an application.\nVersion 1 represents the current application version, and Version 2 represents the new application version. When you update the current application version, you first scale down the existing replicas of Version 1 to zero, and then you concurrently deploy replicas with the new version.\nThe advantage of the recreate approach is its simplicity. You don't have to manage more than one application version in parallel, and therefore you avoid backward compatibility challenges for your data and applications.\nThe recreate method involves downtime during the update process. Downtime is not an issue for applications that can handle maintenance windows or outages. However, if you have mission-critical applications with high service level agreements (SLAs) and availability requirements, you might choose a different deployment strategy.\n### Rolling update deployment pattern\nIn a rolling update deployment, you update a subset of running application instances instead of simultaneously updating every application instance, as the following diagram shows.\nIn this deployment approach, the number of instances that you update simultaneously is called the . In the preceding diagram, the rolling update has a window size of 1. One application instance is updated at a time. If you have a large cluster, you might increase the window size.\nWith rolling updates, you have flexibility in how you update your application:\n- You can scale up the application instances with the new version before you scale down the old version (a process known as a).\n- You can specify the maximum number of application instances that remain unavailable while you scale up new instances in parallel.- **No downtime.** Based on the window size, you incrementally update deployment targets, for example, one by one or two by two. You direct traffic to the updated deployment targets only after the new version of the application is ready to accept traffic.\n- **Reduced deployment risk.** When you roll out an update incrementally, any instability in the new version affects only a portion of the users.- **Slow rollback.** If the new rollout is unstable, you can terminate the new replicas and redeploy the old version. However, like a rollout, a rollback is a gradual, incremental process.\n- **Backward compatibility.** Because new code and old code live side by side, users might be routed to either one of the versions arbitrarily. Therefore, ensure that the new deployment is backward compatible; that is, the new application version can read and handle data that the old version stores. This data can include data stored on disk, in a database, or as part of a user's browser session.\n- **Sticky sessions.** If the application requires [session persistence](https://wikipedia.org/wiki/Load_balancing_(computing)#Persistence) , we recommend that the load balancer supports [stickiness](https://wikipedia.org/wiki/Load_balancing_(computing)#Persistence) and [connection draining](/load-balancing/docs/enabling-connection-draining) . Also, we recommend that you invoke session-sharing when possible (through session replication or session management using a datastore) so that the sessions can be decoupled from underlying resources.\n### Blue/green deployment pattern\nIn a blue/green deployment (also known as a red/black deployment), you perform two identical deployments of your application, as the following diagram shows.\nIn the diagram, blue represents the current application version and green represents the new application version. Only one version is live at a time. Traffic is routed to the blue deployment while the green deployment is created and tested. After you're finished testing, you route traffic to the new version.\nAfter the deployment succeeds, you can either keep the blue deployment for a possible rollback or decommission it. Alternatively, you can deploy a newer version of the application on these instances. In that case, the current (blue) environment serves as the staging area for the next release.- **Zero downtime.** Blue/green deployment allows cutover to happen quickly with no downtime.\n- **Instant rollback.** You can roll back at any time during the deployment process by adjusting the load balancer to direct traffic back to the blue environment. The impact of downtime is limited to the time it takes to switch traffic to the blue environment after you detect an issue.\n- **Environment separation.** Blue/green deployment ensures that spinning up a parallel green environment doesn't affect resources that support the blue environment. This separation reduces your deployment risk.- **Cost and operational overhead.** Adopting the blue/green deployment pattern can increase operational overhead and cost because you must maintain duplicate environments with identical infrastructure.\n- **Backward compatibility.** Blue and green deployments can share data points and datastores. We recommend that you verify that both versions of the application can use the schema of the datastore and the format of the records. This backward compatibility is necessary if you want to switch seamlessly between the two versions if you need to roll back.\n- **Cutover.** If you plan to decommission the current version, we recommend that you allow for appropriate connection draining on existing transactions and sessions. This step allows requests processed by the current deployment to be completed or terminated gracefully.## Testing strategies\nThe testing patterns discussed in this section are typically used to validate a service's reliability and stability over a reasonable period under a realistic level of concurrency and load.\n### Canary test pattern\nIn canary testing, you partially roll out a change and then evaluate its performance against a baseline deployment, as the following diagram shows.\nIn this test pattern, you deploy a new version of your application alongside the production version. You then split and route a percentage of traffic from the production version to the canary version and evaluate the canary's performance.\nYou select the key metrics for the evaluation when you configure the canary. We recommend that you compare the canary against an equivalent baseline and not the live production environment.\nTo reduce factors that might affect your analysis (such as caching, long-lived connections, and hash objects), we recommend that you take the following steps for the baseline version of your application:\n- Ensure that the baseline and production versions of your application are identical.\n- Deploy the baseline version at the same time that you deploy the canary.\n- Ensure that the baseline deployment (such as the number of application instances and autoscaling policies) matches the canary deployment.\n- Use the baseline version to serve the same traffic as the canary.\nIn canary tests, partial rollout can follow various partitioning strategies. For example, if the application has geographically distributed users, you can roll out the new version to a region or a specific location first. For more information, see [Automating canary analysis on GKE with Spinnaker](/solutions/automated-canary-analysis-kubernetes-engine-spinnaker) .- **Ability to test live production traffic.** Instead of testing an application by using simulated traffic in a staging environment, you can run canary tests on live production traffic. With canary rollouts, you need to decide in what increments you release the new application and when you trigger the next step in a release. The canary needs enough traffic so that monitoring can clearly detect any problems.\n- **Fast rollback.** You can roll back quickly by redirecting the user traffic to the older version of the application.\n- **Zero downtime.** Canary releases let you route the live production traffic to different versions of the application without any downtime.- **Slow rollout.** Each incremental release requires monitoring for a reasonable period and, as a result, might delay the overall release. Canary tests can often take several hours.\n- **Observability.** A prerequisite to implementing canary tests is the ability to effectively observe and monitor your infrastructure and application stack. Implementing robust monitoring can require a substantial effort.\n- **Backward compatibility and sticky sessions.** As with rolling updates, canary testing can pose risks with backward incompatibility and session persistence because multiple application versions run in the environment while the canary is deployed.\n### A/B test pattern\nWith A/B testing, you test a hypothesis by using [variant implementations](https://wikipedia.org/wiki/Multivariate_testing) . A/B testing is used to make business decisions (not only predictions) based on the results derived from data.\nWhen you perform an A/B test, you route a subset of users to new functionality based on routing rules, as the following diagram shows.\nRouting rules often include factors such as browser version, user agent, geolocation, and operating system. After you measure and compare the versions, you update the production environment with the version that yielded better results.\nA/B testing is best used to measure the effectiveness of functionality in an application. Use cases for the deployment patterns discussed earlier focus on releasing new software safely and rolling back predictably. In A/B testing, you control your target audience for the new features and monitor any [statistically significant](https://wikipedia.org/wiki/Statistical_significance) differences in user behavior.- **Complex setup.** A/B tests need a [representative sample](https://wikipedia.org/wiki/Simple_random_sample) that can be used to provide evidence that one version is better than the other. You need to pre-calculate the sample size (for example, by using an [A/B test sample size calculator](https://www.surveymonkey.com/mp/ab-testing-significance-calculator/) ) and run the tests for a reasonable period to reach statistical significance of at least 95%.\n- **Validity of results.** Several factors can skew the test results, including false positives, [biased sampling](https://wikipedia.org/wiki/Sampling_bias) , or external factors (such as seasonality or marketing promotions).\n- **Observability.** When you run multiple A/B tests on overlapping traffic, the processes of monitoring and troubleshooting can be difficult. For example, if you test product page A versus product page B, or checkout page C versus checkout page D, distributed tracing becomes important to determine metrics such as the traffic split between versions.\n### Shadow test pattern\nSequential experiment techniques like canary testing can potentially expose customers to an inferior application version during the early stages of the test. You can manage this risk by using o\ufb04ine techniques like simulation. However, o\ufb04ine techniques do not validate the application's improvements because there is no user interaction with the new versions.\nWith shadow testing, you deploy and run a new version alongside the current version, but in such a way that the new version is hidden from the users, as the following diagram shows.\nAn incoming request is mirrored and replayed in a test environment. This process can happen either in real time or asynchronously after a copy of the previously captured production traffic is replayed against the newly deployed service.\nYou need to ensure that the shadow tests do not trigger side effects that can alter the existing production environment or the user state.- **Zero production impact.** Because traffic is duplicated, any bugs in services that are processing shadow data have no impact on production.\n- **Testing new backend features by using the production load.** When used with tools such as [Diffy](https://github.com/twitter/diffy) , traffic shadowing lets you measure the behavior of your service against live production traffic. This ability lets you test for errors, exceptions, performance, and result parity between application versions.\n- **Reduced deployment risk.** Traffic shadowing is typically combined with other approaches like canary testing. After testing a new feature by using traffic shadowing, you then test the user experience by gradually releasing the feature to an increasing number of users over time. No full rollout occurs until the application meets stability and performance requirements.- **Side effects.** With traffic shadowing, you need to be cautious in how you handle services that mutate state or interact with third-party services. For example, if you want to shadow test the payment service for a shopping cart platform, the customers could pay twice for their order. To avoid shadow tests that might result in unwanted mutations or other risk-prone interactions, we recommend that you use either stubs or virtualization tools such as [Hoverfly](https://hoverfly.io/) instead of third-party systems or datastores.\n- **Cost and operational overhead.** Shadow testing is fairly complex to set up. Also, like blue/green deployments, shadow deployments carry cost and operational implications because the setup requires running and managing two environments in parallel.## Choosing the right strategy\nYou can deploy and release your application in several ways. Each approach has advantages and disadvantages. The best choice comes down to the needs and constraints of your business. Consider the following:\n- What are your most critical considerations? For example, is downtime acceptable? Do costs constrain you? Does your team have the right skills to undertake complex rollout and rollback setups?\n- Do you have tight testing controls in place, or do you want to test the new releases against production traffic to ensure the stability of the release and limit any negative impact?\n- Do you want to test features among a pool of users to cross-verify certain business hypotheses? Can you control whether targeted users accept the update? For example, updates on mobile devices require explicit user action and might require extra permissions.\n- Are microservices in your environment fully autonomous? Or, do you have a hybrid of microservice-style applications working alongside traditional, difficult-to-change applications? For more information, see [deployment patterns on hybrid and multi-cloud environments](/solutions/hybrid-and-multi-cloud-architecture-patterns#distributed_deployment_patterns) .\n- Does the new release involve any schema changes? If yes, are the schema changes too complex to decouple from the code changes?\nThe following table summarizes the salient characteristics of the deployment and testing patterns discussed earlier in this document. When you weigh the advantages and disadvantages of various deployment and testing approaches, consider your business needs and technological resources, and then select the option that benefits you the most.\n| Deployment or testing pattern                     | Zero downtime | Real production traffic testing | Releasing to users based on conditions | Rollback duration      | Impact on hardware and cloud costs             |\n|:---------------------------------------------------------------------------------------------------------------|:----------------|:----------------------------------|:-----------------------------------------|:----------------------------------------|:------------------------------------------------------------------------------------|\n| Recreate Version 1 is terminated, and Version 2 is rolled out.             | x    | x         | x          | Fast but disruptive because of downtime | No extra setup required                |\n| Rolling update Version 2 is gradually rolled out and replaces Version 1.          | \u2713    | x         | x          | Slow         | Can require extra setup for surge upgrades           |\n| Blue/green Version 2 is released alongside Version 1; the traffic is switched to Version 2 after it is tested. | \u2713    | x         | x          | Instant         | Need to maintain blue and green environments simultaneously       |\n| Canary Version 2 is released to a subset of users, followed by a full rollout.         | \u2713    | \u2713         | x          | Fast         | No extra setup required                |\n| A/B Version 2 is released, under specific conditions, to a subset of users.         | \u2713    | \u2713         | \u2713          | Fast         | No extra setup required                |\n| Shadow Version 2 receives real-world traffic without impacting user requests.         | \u2713    | \u2713         | x          | Does not apply       | Need to maintain parallel environments in order to capture and replay user requests |\n## Best practices\nIn order to keep deployment and testing risks to a minimum, application teams can follow several best practices:\n- **Backward compatibility.** When you run multiple application versions at the same time, ensure that the database is compatible with all active versions. For example, a new release requires a schema change to the database (such as a new column). In such a scenario, you need to change the database schema so that it's backward compatible with the older version. After you complete a full rollout, you can remove support for the old schema, leaving support only for the newest version. One way to achieve backward compatibility is to decouple schema changes from the code changes. For more information, see [parallel change](https://martinfowler.com/bliki/ParallelChange.html) and [database refactoring](https://databaserefactoring.com/) patterns.\n- **Continuous integration/continuous deployment (CI/CD).** CI ensures that code checked into the feature branch merges with its main branch only after it successfully passes dependency checks, unit and integration tests, and the build process. Therefore, every change to an application is tested before it can be deployed. With CD, the CI-built code artifact is packaged and ready to be deployed in one or more environments. For more information, see [building a CI/CD pipeline with Google Cloud](/docs/ci-cd) .\n- **Automation.** If you continuously deliver application updates to the users, we recommend that you build an automated process that reliably builds, tests, and deploys the software. We also recommend that your code changes automatically flow through a CI/CD pipeline that includes artifact creation, unit testing, functional testing, and production rollout. By using automation tools such as [Cloud Build](/build#section-5) , [Cloud Deploy](/deploy#section-3) , [Spinnaker](https://www.spinnaker.io/) , and [Jenkins](https://jenkins.io/) , you can automate the deployment processes to be more efficient, reliable, and predictable.\n- **IaC and GitOps.** If you need to manage complicated deployment and testing strategies, consider using Infrastructure as Code (IaC) and GitOps tools. Using IaC with [Terraform](https://cloud.google.com/docs/terraform) and [Config Connector](/config-connector) can help you use declarative language to define your infrastructure and strategies. Using GitOps with [Config Sync](/anthos-config-management/docs/config-sync-overview) , and [Argo CD](https://argoproj.github.io/cd) . can help you manage your code with git.\n- **Rollback strategy.** Sometimes, things go wrong. We recommend creating a rollback   strategy to follow when the unexpected occurs. Having a reliable rollback strategy can help administrators and DevOps engineers manage risk. You can create a rollback strategy by using a platform that supports rollbacks as built-in feature, such as [App Engine](/appengine) , and [Cloud Run](/run) . To support your rollback needs, you can also use release automation tools like [Cloud Deploy](/deploy) , [Spinnaker](https://spinnaker.io/) , and [Argo RollOuts](https://argoproj.github.io/rollouts) .\n- **Post-deployment monitoring.** To monitor critical metrics and to alert the responsible team when a deployment or test fails, build a monitoring system using [Google Cloud Observability](/products/operations) . You can also enable automated rollbacks for deployments that fail health checks. Using [Error Reporting](/error-reporting) , [Cloud Trace](/trace) and [Cloud Profiler](/profiler/docs) can help you find the cause of simple and complex post-deployment issues.## What's next\n- Read about the concepts of this document in [Kubernetes and the challenges of continuous software delivery](/solutions/addressing-continuous-delivery-challenges-in-a-kubernetes-world) .\n- Learn more about other [DevOps solutions on Google Cloud](/solutions/devops) .\n- Learn more about [best practices for running containers](/solutions/best-practices-for-building-containers) .\n- Learn more about [continuous integration and delivery on GKE](/kubernetes-engine/continuous-deployment) .\n- Learn more about [CRE life lessons for reliable releases and rollbacks](/blog/products/gcp/reliable-releases-and-rollbacks-cre-life-lessons) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Docs"}