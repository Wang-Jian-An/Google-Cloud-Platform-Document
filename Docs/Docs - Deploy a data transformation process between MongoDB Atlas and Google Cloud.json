{"title": "Docs - Deploy a data transformation process between MongoDB Atlas and Google Cloud", "url": "https://cloud.google.com/architecture/data-pipeline-mongodb-gcp/deployment", "abstract": "# Docs - Deploy a data transformation process between MongoDB Atlas and Google Cloud\nLast reviewed 2023-12-13 UTC\nThis document describes how you deploy [Data transformation between MongoDB Atlas and Google Cloud](/architecture/data-pipeline-mongodb-gcp) . In this document, you deploy an extract, transform, and load (ETL) process between data from MongoDB Atlas to BigQuery.\nThese instructions are intended for data administrators who want to use BigQuery to perform complex analyses on the operational data stored in MongoDB Atlas. You should be familiar with MongoDB Atlas, BigQuery, and Dataflow.\n", "content": "## Architecture\nThe following diagram shows the reference architecture that you use when you deploy this solution.\nAs shown in the diagram, there are three Dataflow templates that handle the integration process. The first template, [MongoDB to BigQuery](/dataflow/docs/guides/templates/provided/mongodb-to-bigquery) , is a batch pipeline that reads documents from MongoDB and writes them to BigQuery. The second template, [BigQuery to MongoDB](/dataflow/docs/guides/templates/provided/bigquery-to-mongodb) , is a batch template that can be used to read the analyzed data from BigQuery and write them to MongoDB. The third template, [MongoDB to BigQuery (CDC)](/dataflow/docs/guides/templates/provided/mongodb-change-stream-to-bigquery) , is a streaming pipeline that works with MongoDB change streams to handle changes in the operational data. For details, see [Data transformation between MongoDB Atlas and Google Cloud](/architecture/data-pipeline-mongodb-gcp) .\n## Objectives\nThe following deployment steps demonstrate how to use the MongoDB to BigQuery template to perform the ETL process between data from MongoDB Atlas to BigQuery. To deploy this ETL process, you perform the following tasks:\n- Provision a MongoDB Atlas cluster in Google Cloud.\n- Load data into your MongoDB cluster.\n- Configure cluster access.\n- Set up a BigQuery table on Google Cloud.\n- Create and monitor the Dataflow job that transfers the MongoDB data into BigQuery.\n- Validate the output tables on BigQuery.## Costs\nIn this document, you use the following billable components of Google Cloud:\n- [BigQuery](https://cloud.google.com/bigquery/pricing) \n- [Dataflow](https://cloud.google.com/dataflow/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) .\nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .\n## Before you begin\nComplete the following steps to set up an environment for your MongoDB to BigQuery architecture.\n## Install MongoDB Atlas\nIn this section, you use [Cloud Marketplace](/marketplace) to install a MongoDB Atlas instance. These instructions assume that you don't have an existing MongoDB account. For complete details on setting up a subscription and linking your Google billing account to your MongoDB account, see [Google Cloud Self-Serve Marketplace](https://www.mongodb.com/docs/atlas/billing/gcp-self-serve-marketplace/) in the MongoDB documentation.\n- In the Google Cloud console, expand the navigation menu, and then select **Marketplace** .\n- In the Marketplace search box, enter **MongoDB\nAtlas** .\n- In the search results, select **MongoDB Atlas (Pay as You Go)** .\n- On the **MongoDB Atlas (Pay as You Go)** page, review the overview for terms and conditions, and then click **Sign up with MongoDB** .\n- On the MongoDB subscription page, select your billing account, accept the terms, and click **Subscribe** .\n- Click the **Register with MongoDB** button and create a MongoDB account.\n- On the page that asks you to select an organization, select the MongoDB organization to which to link your Google Cloud billing account.\n- Wait for Google Cloud to finish syncing your organization.\nWhen the accounts are synced, the **MongoDB Atlas (Pay as You Go)** page in the Google Cloud console will update to display a **Manage onprovider** button.\n## Create a MongoDB Atlas Cluster\nIn this section, you create a MongoDB cluster. During the creation process, you select the following information:\n- Your cluster type. Select the [Cluster Tier](https://www.mongodb.com/pricing?utm_source=google) based on your infrastructure requirements.\n- The preferred region for your cluster. We recommend that you select the region that's closest to your physical location.\nFor details about how to create and deploy a free MongoDB cluster, see [Deploy a Free Cluster](https://www.mongodb.com/docs/atlas/tutorial/deploy-free-tier-cluster/) in the MongoDB documentation.\nTo create and set up your cluster, follow these steps:\n- In the Google Cloud console, on the **MongoDB Atlas (Pay as You\nGo)** page, click **Manage on Provider** .\n- On the MongoDB login page, click **Google** , and then click the Google Account that you used to install MongoDB Atlas.As a new user, the MongoDB UI automatically opens to the **DatabaseDeployments** page.\n- In the Atlas UI, on the **Database Deployments** page, click **Create** .\n- On the **Create a Cluster** page, click **Shared** .The **Shared** option provides a free cluster that you can use to test out this reference architecture.\n- On the **Create a Shared Cluster** page, in the **Cloud Provider &Region** section, do the following:- Select **Google Cloud** .\n- Select the region that is closest to you geographically and has the characteristics that you want.\n- In the **Cluster Tier** section, select the **M0** option.`M0` clusters are free and suitable for small proof-of-concept applications.\n- In **Cluster Name** , enter a name for your cluster.\n- Click **Create Cluster** to deploy the cluster.## Set up your MongoDB cluster\nIn this section, you complete the following procedures:\n- Loading the sample data into your cluster.\n- Configuring access to your cluster.\n- Connecting to your cluster.\n### Load sample data into your MongoDB cluster\nNow that you have created a MongoDB cluster, you need to load data into that cluster. MongoDB loads a [variety of sample datasets](https://www.mongodb.com/docs/atlas/sample-data/#available-sample-datasets) . You can use any of these datasets to test this deployment. However, you might want to use a dataset that is similar to the actual data that you'll use in your production deployment.\nFor details about how to load the sample data, see [Load the Sample Data](https://www.mongodb.com/docs/charts/tutorial/order-data/prerequisites-setup/) in the MongoDB documentation.\nTo load the sample data, do the following steps:\n- In the Atlas UI, on the **Database Deployments** page, locate the cluster that you just deployed.\n- Click the **Ellipses** (...) button, and then click **Load Sample Dataset** .Loading the sample data takes approximately 5 minutes.\n- Review the sample datasets and make a note of which collection you want to use when testing this deployment.\n### Configure cluster access\nTo connect your cluster, you need to create both a database user and set the IP address for the cluster:\n- The database user is separate from the MongoDB user. You need the database user to connect to MongoDB from Google Cloud.\n- For this reference architecture, you use the CIDR block of`0.0.0.0/0`as your IP address. This CIDR block allows access from anywhere and is only suitable for a proof-of-concept deployment such as this one. However, when you deploy a production version of this architecture, make sure to enter a suitable IP address range that is appropriate for your application.\nFor details about how to set up a database user and the IP address for your cluster, see [Configure cluster access with the QuickStart Wizard](https://www.mongodb.com/docs/atlas/security/quick-start/) in the MongoDB documentation.\nTo configure cluster access, do the following steps:\n- In the **Security** section of the left navigation pane, click **Quickstart** .\n- On the **Username and Password** page, do the following to create the database user:- For **Username** , enter the name for the database user.\n- For **Password** , enter the password for the database user.\n- Click **Create User** .\n- On the **Username and Password** page, do the following to add an IP address for your cluster:- In **IP Address** , enter **0.0.0.0/0** .For your production environment, select the IP address that is appropriate for that environment.\n- (Optional) For **Description** , enter a description of your cluster.\n- Click **Add Entry** .\n- Click **Finish and Close** .\n### Connect to your cluster\nWith access to your cluster configured, you now need to connect to your cluster. For details about how to connect to your cluster, see [Connect to Your Cluster](https://www.mongodb.com/docs/atlas/tutorial/connect-to-your-cluster/) in the MongoDB documentation.\nFollow these steps to connect to your cluster:\n- In the Atlas UI, on the **Database Deployments** page, locate the cluster that you just deployed.\n- Select **Connect.** \n- On the **Connect** page, click the **Compass** option.\n- Locate the **Copy the connection string** field, and then copy and save MongoDB connection string. You use this connection string while running the Dataflow templates.The connection string has the following syntax:```\nmongodb+srv://<UserName>:<Password>@<HostName>\n```The connection string automatically has the username of the database user that you created in the previous step. However, you'll be prompted for the password of the database user when you use this string to connect.\n- Click **Close** .## Create a dataset in BigQuery\nWhen you create a dataset in BigQuery, you only have to enter a dataset name and select a geographic location for the dataset. However, there are optional fields that you can set on your dataset. For more information about those optional fields, see [Create datasets](/bigquery/docs/datasets#create-dataset) .\n- In the Google Cloud console, go to the [BigQuery](https://console.cloud.google.com/bigquery) page. [Go to BigQuery](https://console.cloud.google.com/bigquery) \n- In the **Explorer** panel, select the project where you want to create the dataset.\n- Expand the more_vert option and click **Create dataset** .\n- On the **Create dataset** page, do the following:- For **Dataset ID** , enter a unique dataset [name](/bigquery/docs/datasets#dataset-naming) .\n- For **Location type** , choose a geographic [location](/bigquery/docs/locations) for the dataset. After a dataset is created, the location can't be changed.If you choose **EU** or an EU-based region for the dataset location, your core BigQuery Customer Data resides in the EU. For a definition of core BigQuery Customer Data, see [Service Specific Terms](/terms/service-terms#13-google-bigquery-service) .\n- Click **Create dataset** .\n## Create, monitor, and validate a Dataflow batch job\nIn Dataflow, use the following instructions to create a one-time batch job that loads the sample data from MongoDB to BigQuery. After you create the batch job, you monitor the job's progress in the Dataflow monitoring interface. For complete details on using the monitoring interface, see [Use the Dataflow monitoring interface.](/dataflow/docs/guides/monitoring-overview)\n- In the Google Cloud console, go to the [Dataflow](https://console.cloud.google.com/dataflow) page. [Go to Dataflow](https://console.cloud.google.com/dataflow) \n- Click **Create job from template** .\n- On the **Create job from template** page, do the following steps:- For **Job name** , enter a unique job name, such as **mongodb-to-bigquery-batch** . Make sure that no other Dataflow job with that name is currently running in that project.\n- For **Regional endpoint** , select the same location as that of the BigQuery dataset that you just created.\n- For **Dataflow template** , in the **Process Data in Bulk (batch)\nlist** , select **MongoDB to BigQuery** .\n- In the **Required Parameters** section, enter the following parameters:- For **MongoDB Connection URI** , enter your Atlas MongoDB connection string.\n- For **Mongo database** , enter the name of the database that you created earlier.\n- For the **Mongo collection** , enter the name of the sample collection that you noted earlier.\n- For the BigQuery **destination table** , click **Browse** , and then select the BigQuery table that you created in the previous step.\n- For the **User option** , enter either **NONE** or **FLATTEN.** **NONE** will load the entire document in JSON string format into BigQuery. **FLATTEN** flattens the document to one level. If you don't supply a UDF, the **FLATTEN** option only works with documents that have a fixed schema.\n- To start the job, click **Run Job** .\n- Use the following steps to open the Dataflow monitoring interface where you can check on the progress of the batch job and validate that the job completes without errors:- In the Google Cloud console, in the project for this deployment, open the navigation menu.\n- In **Analytics** , click **Dataflow** .\n- After the pipeline runs successfully, do the following to validate the table output:- In BigQuery, open the **Explorer** pane.\n- Expand your project, click the dataset, and then double-click on the table.You should now be able to view the MongoDB data in the table.\n## Clean up\nTo avoid incurring charges to your MongoDB and Google Cloud accounts, you should pause or terminate your MongoDB Atlas cluster and delete the Google Cloud project that you created for this reference architecture.\n### Pause or terminate your MongoDB Atlas cluster\nThe following procedure provides the basics for pausing your cluster. For complete details, see [Pause, Resume, or Terminate a Cluster](https://www.mongodb.com/docs/atlas/pause-terminate-cluster/) in the MongoDB documentation.\n- In the Atlas UI, go to the **Database Deployments** page for your Atlas project.\n- For the cluster that you want to pause, clickmore_horiz.\n- Click **Pause Cluster** .\n- Click **Pause Cluster** to confirm your choice.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Find Google Dataflow Templates on [GitHub](https://github.com/GoogleCloudPlatform/DataflowTemplates) if you want to customize the templates.\n- Learn more about MongoDB Atlas and Google Cloud solutions on [Cloud skill boost](https://www.cloudskillsboost.google/quests/306?utm_source=qwiklabs&utm_medium=lp&utm_campaign=gpe-hol) .\n- Learn more about the Google Cloud products used in this reference architecture:- [BigQuery](/bigquery) \n- [Pub/Sub](/pubsub) \n- [Dataflow](/dataflow) \n- [Compute Engine](/compute) \n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .## Contributors\nAuthors:\n- [Saurabh Kumar](https://www.linkedin.com/in/saurabh-kumar-48556a19) | ISV Partner Engineer\n- [Venkatesh Shanbhag](https://www.linkedin.com/in/venkatesh-shanbhag) | Senior Solutions Architect (MongoDB)\nOther contributors:\n- [Jun Liu](https://www.linkedin.com/in/jun-liu-1759093a) | Supportability Tech Lead\n- [Maridu Raju Makaraju](https://www.linkedin.com/in/mmraju) | Supportability Tech Lead\n- [Sergei Lilichenko](https://www.linkedin.com/in/sergei-lilichenko) | Solutions Architect\n- Shan Kulandaivel | Group Product Manager", "guide": "Docs"}