{"title": "Docs - Set up a regulatory reporting architecture with BigQuery", "url": "https://cloud.google.com/architecture/set-up-regulatory-reporting-architecture-bigquery", "abstract": "# Docs - Set up a regulatory reporting architecture with BigQuery\nLast reviewed 2022-05-24 UTC\nThis document shows you how to get started with a regulatory reporting solution for cloud and run a basic pipeline. It's intended for data engineers in financial institutions who want to familiarize themselves with an architecture and best practices for producing stable, reliable regulatory reports.\nIn this tutorial, you establish a working example of a regulatory data processing platform on Google Cloud resources. The example platform demonstrates how you can implement a data processing pipeline that maintains quality of data, auditability, and ease of change and deployment and also meets the following requirements of regulatory reporting:- Ingestion of data from source\n- Processing of large volumes of granular data\n- Aggregation of data into reports.\nThis document assumes that you're familiar with Terraform version 1.1.7, data build tool (dbt) version 1.0.4, Cloud Storage, and BigQuery.", "content": "## Objectives\n- Create infrastructure from a cloned repository.\n- Load manufactured data into BigQuery.\n- Extract regulatory metrics from granular data.\n- Containerize the extraction pipeline.\n## CostsIn this document, you use the following billable components of Google Cloud:- [BigQuery](/bigquery/pricing) \n- [Cloud Storage](/storage/pricing) \n- [Optionally, Cloud Composer](/composer/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- In the Google Cloud console, on the project selector page,   select or [create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . **Note** : If you don't plan to keep the  resources that you create in this procedure, create a project instead of  selecting an existing project. After you finish these steps, you can  delete the project, removing all resources associated with the project. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n## Prepare your environment\n- In Cloud Shell, specify the project that you want to use for this tutorial:```\ngcloud config set project PROJECT_ID\n```Replace `` with the ID of the project that you selected or created for this tutorial.If a dialog displays, click **Authorize** .\n- Specify a default region to use for infrastructure creation:```\n\u00a0gcloud config set compute/region REGION\n```\n- Create and activate a Python virtual environment:```\npython -m venv reg-rpt-envsource reg-rpt-env/bin/activate\n```You see that your command-line prompt is prefixed with the name of the virtual environment.\n- Clone the repository:```\ngit clone \\\"https://github.com/GoogleCloudPlatform/reg-reporting-blueprint\"\n```\n- Install Terraform. To learn how to do this installation, see the [HashiCorp documentation](https://learn.hashicorp.com/tutorials/terraform/install-cli#install-terraform) .\n- [Verify](https://learn.hashicorp.com/tutorials/terraform/install-cli#verify-the-installation) the installation.\n- Install dbt:```\npip3 install dbt-bigquery --upgrade\n```\n- Verify the dbt installation:```\n\u00a0dbt --version\n```You see the installation details displayed.\n- Initialize the environment variables:```\ncd reg-reporting-blueprint && source environment-variables.sh\n```\n- Run the setup script:```\ncd common_components && ./setup_script.sh\n```\n- Run terraform to create the required infrastructure:```\ncd orchestration/infrastructure/terraform init -upgradeterraform planterraform apply\n```Type 'yes' when you see the confirmation prompt.\n- To verify that an ingest bucket has been created, in the Google Cloud console, go to the **Cloud Storage** page and check for a bucket with a name that's similar to the value of `PROJECT ID` .\n- Go to the **BigQuery** page and verify that the following datasets have been created:```\nhomeloan_devhomeloan_datahomeloan_expectedresults\n```\n## Upload the sample dataIn this section, you explore the contents of the repository's `data` and `data_load` folders, and load sample data to BigQuery.- In the Cloud Shell Editor instance, navigate to the `data` folder in the repository:```\ncd ../../../use_cases/examples/home_loan_delinquency/data/\n```This folder contains two subfolders which are named `input` and `expected` .Inspect the contents of the `input` folder. This folder contains CSV files with sample input data. This sample data is provided only for test purposes.Inspect the contents of the `expected` folder. This folder contains the CSV files specifying the expected results once the transformations are applied.\n- Open, and inspect, the `data_load/schema` folder, which contains files specifying the schema of the staging data:```\ncd ../data_load\n```The scripts in this folder allow the data to be loaded into Cloud Storage first, and then into BigQuery. The data conforms to the expected schema for the example regulatory reporting pipeline use case in this tutorial.\n- Load the data into Cloud Storage:```\n./load_to_gcs.sh ../data/input./load_to_gcs.sh ../data/expected\n```The data is now available in your Cloud Storage ingest bucket.\n- Load the data from the Cloud Storage ingest bucket to BigQuery:```\n./load_to_bq.sh\n```\n- To verify that the data has been loaded in BigQuery, in the Google Cloud console, go to the BigQuery page and select a table in both the `homeloan_data` and `homeloan_expectedresults` datasets.Select the **Preview** tab for each table, and confirm that each table has data.\n### Run the regulatory reporting pipeline\n- In your development environment, initialize the dependencies of dbt:```\ncd ../dbt/dbt deps\n```This will install any needed dbt dependencies in your dbt project.\n- Test the connection between your local dbt installation and your BigQuery datasets:```\ndbt debug\n```At the end of the connectivity, configuration, and dependency information returned by the command, you should see the following message: `All checks passed!`In the `models` folder, open a SQL file and inspect the logic of the sample reporting transformations implemented in dbt.\n- Run the reporting transformations to create the regulatory reporting metrics:```\ndbt run\n```\n- Run the transformations for a date of your choice:```\ndbt run --vars '{\"reporting_day\": \"2021-09-03\"}'\n```Notice the variables that control the execution of the transformations. The variable `reporting_day` indicates the date value that the portfolio should have. When you run the `dbt run` command, it's a best practice to provide this value.\n- In the Google Cloud console, go to the **BigQuery** page and inspect the `homeloan_dev` dataset. Notice how the data has been populated, and how the `reporting_day` variable that you passed is used in the **control.reporting_day** field of the `wh_denormalised` view.\n- Inspect the `models/schema.yml` file:```\nmodels:\u00a0- <<: *src_current_accounts_attributes\u00a0 \u00a0name: src_current_accounts_attributes\u00a0 \u00a0columns:\u00a0 \u00a0 \u00a0- name: ACCOUNT_KEY\u00a0 \u00a0 \u00a0 \u00a0tests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0- unique\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - not_null\n```Notice how the file defines the definitions of the columns and the associated data quality tests. For example, the **ACCOUNT_KEY** field in the `src_current_accounts_attributes` table must be unique and not null.\n- Run the data quality tests that are specified in the config files:```\ndbt test -s test_type:generic\n```\n- Inspect the code in the `use_cases/examples/home_loan_delinquency/dbt/tests` folder, which contains `singular` tests. Notice that the tests in this folder implement a table comparison between the actual results that are output by the `dbt run` command, and the expected results that are saved in the `homeloan_expectedresults` dataset.\n- Run the singular tests:```\ndbt test -s test_type:singular\n```\n- Generate the documentation for the project:```\ndbt docs generate && dbt docs serve\n```\n- In the output that you see, search for, and then click, the following URL text: `http://127.0.0.1:8080`Your browser opens a new tab that shows the dbt documentation web interface.\n- Inspect the lineage of the models and their documentation. You see that the documentation includes all of the code and the documentation for the models (as specified in the `models/schema.yml` files).\n- In Cloud Shell, enter the following:```\nCtrl + c\n```Cloud Shell stops hosting the dbt web interface.\n## Optional: Containerize the transformations\n- In Cloud Shell, create a container for the BigQuery data load step, and push the container to Google Container Repository:```\ncd ../../../../ \u00a0# the gcloud command should be executed from the root gcloud builds submit --config use_cases/examples/home_loan_delinquency/data_load/cloudbuild.yaml\n```The Dockerfile in the `data_load` directory enables containerization, which simplifies orchestration of the workflow.\n- Containerize the code for the data transformation step, and push the container to Container Registry:```\ngcloud builds submit --config use_cases/examples/home_loan_delinquency/dbt/cloudbuild.yaml\n```Containerization helps you to create a package that you can version and deploy.\n- Retrieve the path of the Airflow page and the **Cloud Storage** bucket for dags, and store them in environment variables:```\ncd common_components/orchestration/infrastructure/ AIRFLOW_DAG_GCS=$(terraform output --raw airflow_dag_gcs_prefix)AIRFLOW_UI=$(terraform output --raw airflow_uri)\n```\n- Upload the home loan delinquency dag:```\ncd ../../../use_cases/examples/home_loan_delinquency/deploy/gsutil cp run_homeloan_dag.py $AIRFLOW_DAG_GCS\n```\n- Go to the Airflow page by executing the following command to retrieve the UI, and clicking on the link:```\necho $AIRFLOW_UI\n```\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Delete the individual resourcesTo avoid incurring further charges, delete the individual resources that you use in this tutorial:\n```\ncd ../../../../common_components/orchestration/infrastructure/terraform destroy\n```## What's next\n- Explore more [Google Cloud for financial service solutions](/solutions/financial-services#section-1) .\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Docs"}