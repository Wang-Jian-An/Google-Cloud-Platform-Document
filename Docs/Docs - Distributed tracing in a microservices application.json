{"title": "Docs - Distributed tracing in a microservices application", "url": "https://cloud.google.com/architecture/microservices-architecture-distributed-tracing", "abstract": "# Docs - Distributed tracing in a microservices application\nThis document is the fourth in a four-part series about designing, building, and deploying microservices. This series describes the various elements of a microservices architecture. The series includes information about the benefits and drawbacks of the microservices architecture pattern, and how to apply it.- [Introduction to microservices](/architecture/microservices-architecture-introduction) \n- [Refactoring a monolith into microservices](/architecture/microservices-architecture-refactoring-monoliths) \n- [Interservice communication in a microservices setup](/architecture/microservices-architecture-interservice-communication) \n- Distributed tracing in a microservices application (this document)\nThis series is intended for application developers and architects who design and implement the migration to refactor a monolith application to a microservices application.\nIn a distributed system, it's important to know how a request flows from one service to another and how long it takes to perform a task in each service. Consider the microservices-based Online Boutique application that you deployed in the previous document, [Refactoring a monolith into microservices](/architecture/microservices-architecture-refactoring-monoliths) . The application is composed of multiple services. For example, the following screenshot shows the product details page, which fetches information from the frontend, recommendation, and ads services.To render the product details page, the frontend service communicates with the recommendation service and the ads service, as show in the following diagram:\n \n **Figure 1.** Services written in different languages.\nIn figure 1, the frontend service is written in Go. The recommendation service, which is written in Python, uses gRPC to communicate with the frontend service. The ads service, which is written in Java, also uses gRPC to communicate with the frontend service. Besides gRPC the inter-service communication method can also be in REST HTTP.\nWhen you build such a distributed system, you want your observability tools to provide the following insights:- The services that a request went through.\n- Where delays occurred if a request was slow.\n- Where an error occurred if the request failed.\n- How the execution of the request was different from the normal behavior of the system.\n- Whether differences in the execution of the request were related to performance (whether some service calls took a longer or shorter time than usual).\n", "content": "## Objectives\n- Use kustomize manifest files to set up the infrastructure.\n- Deploy the Online Boutique example application to Google Kubernetes Engine (GKE).\n- Use Cloud Trace to review a user's journey in the example application.\n## CostsIn this document, you use the following billable components of Google Cloud:- [GKE](/kubernetes-engine/pricing) \n- [Cloud SQL](/sql/pricing) \n- [Container Registry](/container-registry/pricing) \n- [Cloud Trace](/stackdriver/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish this document, you can avoid continued billing by deleting the resources you created. For more information, see [Cleaning up](#clean-up) .## Before you beginIf you already set up a project by completing the previous document in this series, [Interservice communication in a microservices setup](/architecture/microservices-architecture-interservice-communication) , you can reuse the project. Complete the following steps to enable additional APIs and set environment variables.- In the Google Cloud console, on the project selector page,   select or [create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Enable the APIs for Compute Engine, GKE, Cloud SQL, Artifact Analysis, Trace, and Container Registry:```\n\u00a0gcloud services enable \\\u00a0 \u00a0compute.googleapis.com \\\u00a0 \u00a0sql-component.googleapis.com \\\u00a0 \u00a0servicenetworking.googleapis.com\\\u00a0 \u00a0container.googleapis.com \\\u00a0 \u00a0containeranalysis.googleapis.com \\\u00a0 \u00a0containerregistry.googleapis.com \\\u00a0 \u00a0sqladmin.googleapis.com\n```\n## Distributed tracingDistributed tracing attaches contextual metadata to each request and ensures that metadata is shared between requests. You use trace points to instrument distributed tracing. For example, you can instrument your services (frontend, recommendation, and ads) with two trace points to handle a client request to view a product's details: one trace point to send the request and another trace point to receive the response. The following diagram shows how this trace point instrumentation works: **Figure 2.** Each interservice call has two trace points that consist of a request-response pair.\nFor trace points to understand which request to execute when the service is invoked, the originating service passes a along the execution flow. The process that passes the trace ID is called or . Context propagation transfers metadata over network calls when services of a distributed application communicate with each other during the execution of a given request. The following diagram shows the metadata propagation: **Figure 3.** Trace metadata is passed between services. The metadata includes information like which service calls which and their timestamps.\nIn the Online Boutique example, a trace begins when a user sends an initial request to fetch product details. A new trace ID is generated, and each successive request is decorated with headers that contain contextual metadata back to the original request.\nEach individual operation that is invoked as part of fulfilling the end user's request is called a . The originating service tags each span with its own unique ID and the trace ID of the parent span. The following diagram shows a Gantt chart visualization of a trace: **Figure 4.** A parent span includes the response time of child spans.\nFigure 4 shows a trace tree in which the frontend service calls the recommendation service and the ads service. The frontend service is the parent span, which describes the response time as observed by the end user. The child spans describe how the recommendation service and the ads service were called and responded, including response time information.\nA service mesh like [Istio](https://istio.io/) enables distributed tracing of service-to-service traffic without the need for any dedicated instrumentation. However, there might be situations in which you want to have more control over the traces, or you might need to trace code that isn't running within a service mesh.\nThis document uses [OpenTelemetry](https://opentelemetry.io/) to enable instrumentation of distributed microservice applications to collect traces and metrics. OpenTelemetry lets you collect metrics and traces and then export them to [backends](https://opentelemetry.io/docs/instrumentation/js/exporters/) , such as Prometheus, Cloud Monitoring, Datadog, Graphite, Zipkin, and Jaeger.## Instrumentation using OpenTelemetryThe following sections show how to use context propagation to allow spans from multiple requests to be appended to a single parent trace.\nThis example uses the OpenTelemetry [Javascript](https://opentelemetry.io/docs/instrumentation/js/) , [Python](https://opentelemetry.io/docs/instrumentation/python/getting-started/) , and [Go](https://opentelemetry.io/docs/instrumentation/go/getting-started/) libraries to instrument trace implementation for the payment, recommendation, and frontend services. Depending on the verbosity of the instrumentation, tracing data can affect on the project's cost (Cloud Trace billing). To mitigate cost concerns, most tracing systems employ various forms of sampling to capture only a certain percentage of the observed traces. In your production environments, your organization might have reasons for both what they want to sample and why. You might want to customize your sampling strategy based on managing costs, focusing on interesting traces, or filtering out noise. To learn more about sampling see [OpenTelemetry Sampling](https://opentelemetry.io/docs/concepts/sampling/) .\nThis document uses [Trace](/trace) to visualize distributed traces. You use an OpenTelemetry exporter to send traces to Trace.\n### Register trace exportersThis section shows how to register the trace exporter in each service by adding lines to the microservice code.\nFor the frontend service (written in Go), the following [code sample](https://github.com/GoogleCloudPlatform/microservices-demo/blob/e3722eb82897361f3be80bd2dc238cb2afa9b399/src/frontend/main.go#L165C1-L179C16) registers the exporter:\n```\n[...]\nexporter, err := otlptracegrpc.New(\n  ctx,\n  otlptracegrpc.WithGRPCConn(svc.collectorConn))\n if err != nil {\n  log.Warnf(\"warn: Failed to create trace exporter: %v\", err)\n }\ntp := sdktrace.NewTracerProvider(\n  sdktrace.WithBatcher(exporter),\n  sdktrace.WithSampler(sdktrace.AlwaysSample()))\n otel.SetTracerProvider(tp)\n```\nFor the recommendation service (written in Python), the following [code sample](https://github.com/GoogleCloudPlatform/microservices-demo/blob/e3722eb82897361f3be80bd2dc238cb2afa9b399/src/recommendationservice/recommendation_server.py#L111-L121) registers the exporter:\n```\nif os.environ[\"ENABLE_TRACING\"] == \"1\":\n trace.set_tracer_provider(TracerProvider())\n otel_endpoint = os.getenv(\"COLLECTOR_SERVICE_ADDR\", \"localhost:4317\")\n trace.get_tracer_provider().add_span_processor(\n  BatchSpanProcessor(\n   OTLPSpanExporter(\n   endpoint = otel_endpoint,\n   insecure = True\n   )\n  )\n )\n```\nFor the payment service (written in Javascript), the following [code sample](https://github.com/GoogleCloudPlatform/microservices-demo/blob/e3722eb82897361f3be80bd2dc238cb2afa9b399/src/paymentservice/index.js#L42-L51) registers the exporter:\n```\nprovider.addSpanProcessor(new SimpleSpanProcessor(new OTLPTraceExporter({url: collectorUrl})));\nprovider.register();\n```\n### Set up context propagationThe tracing system needs to follow a [trace context specification](https://www.w3.org/TR/trace-context/) that defines the format to propagate tracing context between services. Propagation format examples include [Zipkin's B3 format](https://github.com/openzipkin/b3-propagation) and [X-Google-Cloud-Trace](/trace/docs/setup) .\nOpenTelemetry propagates context using the global `TextMapPropagator` . This example uses the Trace Context propagator, which uses the [W3C traceparent format](https://www.w3.org/TR/trace-context/#trace-context-http-headers-format) . Instrumentation libraries, such as OpenTelemetry's HTTP and gRPC libraries, use the global propagator to add trace context as metadata to HTTP or gRPC requests. For context propagation to succeed, the client and server must use the same propagation format.\n### Context propagation over HTTPThe frontend service injects a trace context into the HTTP request headers. The backend services extracts the trace context. The following [code sample](https://github.com/GoogleCloudPlatform/microservices-demo/blob/e3722eb82897361f3be80bd2dc238cb2afa9b399/src/frontend/main.go#L99-L110) shows how the frontend service is instrumented to configure the trace context:\n```\notel.SetTextMapPropagator(\n propagation.NewCompositeTextMapPropagator(\n  propagation.TraceContext{}, propagation.Baggage{}))\nif os.Getenv(\"ENABLE_TRACING\") == \"1\" {\n log.Info(\"Tracing enabled.\")\n initTracing(log, ctx, svc)\n} else {\n log.Info(\"Tracing disabled.\")\n}\n...\nvar handler http.Handler = r\nhandler = &logHandler{log: log, next: handler}  // add logging\nhandler = ensureSessionID(handler)     // add session ID\nhandler = otelhttp.NewHandler(handler, \"frontend\") // add OpenTelemetry tracing\n```\n### Context propagation over gRPCConsider the flow in which the checkout service places the order based on the product that a user selects. These services communicate over gRPC.\nThe following [code sample](https://github.com/GoogleCloudPlatform/microservices-demo/blob/e3722eb82897361f3be80bd2dc238cb2afa9b399/src/checkoutservice/main.go#L128-L138) uses a gRPC call interceptor that intercepts the outgoing calls and injects the trace context:\n```\nvar srv *grpc.Server\n// Propagate trace context always\notel.SetTextMapPropagator(\n propagation.NewCompositeTextMapPropagator(\n  propagation.TraceContext{}, propagation.Baggage{}))\nsrv = grpc.NewServer(\n grpc.UnaryInterceptor(otelgrpc.UnaryServerInterceptor()),\n grpc.StreamInterceptor(otelgrpc.StreamServerInterceptor()),\n)\n```\nAfter receiving the request, the payment or product catalogue service ( `ListProducts` ) extracts the context from the request headers and uses the parent trace metadata to spawn a child span.\nThe following sections provide details of how to set up and review distributed tracing for the example Online Boutique application.## Deploying the applicationIf you already have a running application from completing the previous document in this series, [Interservice communication in a microservices setup](/architecture/microservices-architecture-interservice-communication) , you can skip to the next section, [Reviewing traces](#reviewing-traces-using-cloud-trace) . Otherwise, complete the following steps to deploy the example Online Boutique example:- To set up infrastructure, in Cloud Shell clone the GitHub repository:```\ngit clone https://github.com/GoogleCloudPlatform/microservices-demo.git\n```\n- For the new deployment, reset the environment variables:```\nPROJECT_ID=PROJECT_IDREGION=us-central1GSA_NAME=microservices-saGSA_EMAIL=$GSA_NAME@$PROJECT_ID.iam.gserviceaccount.com\n```Replace the following:- : The identifier for your project ID.\n- Optional: Create a new cluster or reuse an existing cluster if it exists:```\ngcloud container clusters create-auto online-boutique --project=${PROJECT_ID}\u00a0 --region=${REGION}\n```\n- Create a Google service account:```\ngcloud iam service-accounts create $GSA_NAME \\\u00a0 --project=$PROJECT_ID\n```\n- Enable the APIs:```\ngcloud services enable \\monitoring.googleapis.com \\cloudtrace.googleapis.com \\cloudprofiler.googleapis.com \\\u00a0 --project ${PROJECT_ID}\n```\n- Grant the roles required for Cloud Tracing to the GSA:```\ngcloud projects add-iam-policy-binding ${PROJECT_ID} \\--member \"serviceAccount:${GSA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\" \\--role roles/cloudtrace.agentgcloud projects add-iam-policy-binding ${PROJECT_ID} \\--member \"serviceAccount:${GSA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\" \\--role roles/monitoring.metricWritergcloud projects add-iam-policy-binding ${PROJECT_ID} \\--member \"serviceAccount:${GSA_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\" \\--role roles/cloudprofiler.agentgcloud iam service-accounts add-iam-policy-binding ${GSA_EMAIL} \\--role roles/iam.workloadIdentityUser \\--member \"serviceAccount:${PROJECT_ID}.svc.id.goog[default/default]\"\n```\n- Annotate your Kubernetes service account ( `default` / `default` for the default namespace) to use the Google IAM service account:```\nkubectl annotate serviceaccount default \\\u00a0 \u00a0 iam.gke.io/gcp-service-account=${GSA_EMAIL}\n```\n- Enable Cloud Operations for GKE configuration; that enables tracing:```\ncd ~/microservices-demo/kustomize && \\kustomize edit add component components/google-cloud-operations\n```\n- This will update the `kustomize` / `kustomization.yaml` file which could be similar to:```\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n- base\ncomponents:\n- components/google-cloud-operations\n[...]\n```\n- Deploy the microservices:```\nkubectl apply -k .\n```\n- Check the status of the deployment:```\nkubectl rollout status deployment/frontendkubectl rollout status deployment/paymentservicekubectl rollout status deployment/recommendationservicekubectl rollout status deployment/adservice\n```The output for each command looks like the following:```\nWaiting for deployment \"\" rollout to finish: 0 of 1 updated replicas are available...\ndeployment \"\" successfully rolled out\n```\n- Get the IP Address of the deployed application:```\nkubectl get service frontend-external | awk '{print $4}'\n```Wait for the load balancer's IP address to be published. To exit the command, press `Ctrl+C` . Note the load balancer IP address and then access the application at the URL `http://` `` . It might take some time for the load balancer to become healthy and start passing traffic.\n## Reviewing traces using Cloud TraceA user's purchase journey in the Online Boutique application has the following flow:- The user sees a product catalog on the landing page.\n- To make a purchase, the user clicks **Buy** .\n- The user is redirected to a product details page where they add the item to their cart.\n- The user is redirected to a checkout page where they can make a payment to complete the order.\nConsider a scenario in which you need to troubleshoot high response times when loading the product details page. As described earlier, the product details page is comprised of multiple microservices. To determine where and why the high latency is occurring, you can view distributed tracing graphs to review the performance of the entire request across the different services.\nTo review the distributed tracing graphs, do the following:- Access the application and then click any product. The product details page is displayed.\n- In the Google Cloud console, go to the [Trace list](https://console.cloud.google.com/traces/list) page and review the timeline.\n- To see the distributed trace results, click **Frontend** in the URI column.\n- The **Trace Waterfall View** displays the spans associated with the URI:In the preceding screenshot, the trace for a product contains the following spans:- The **Frontend** span captures the end-to-end latency (150.349 ms) that the client observes in loading the product details page.\n- The **Recommendation Service** span captures the latency of the backend calls in fetching recommendations (4.246 ms) that are related to the product.\n- The **Ad Service** span captures the latency of the backend calls in fetching ads (4.511 ms) that are relevant to the product page.\nTo troubleshoot high response times, you can review insights that include latency distribution graphs of any outlier requests when the service's dependencies aren't meeting their [service level objectives](https://wikipedia.org/wiki/Service-level_objective) (SLOs). You can also [use Cloud Trace to get performance insights and create analysis reports](/trace/docs/analysis-reports) from the sampled data.## TroubleshootingIf the traces in Application Performance Management don't appear, check Logs Explorer for a permission denied error. The permission denied occurs when the service account doesn't have access to export the traces. Review the steps on [granting the roles](https://cloud.google.com/architecture/microservices-architecture-distributed-tracing#deploying_the_application) required for Cloud Trace and make sure to annotate the service account with the correct namespace. After that, restart the `opentelemetrycollector` :\n```\n```kubectl rollout restart deployment opentelemetrycollector```\n```## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Delete the resourcesIf you want to keep the Google Cloud project that you used in this document, delete the individual resources:- In Cloud Shell, delete the resources:```\ngcloud container clusters delete online-boutique --project=${PROJECT_ID} --region=${REGION}\n```\n## What's next\n- Read the first document in this series to learn about [microservices, their benefits, challenges, and use cases](/architecture/microservices-architecture-introduction) .\n- Read the second document in this series to learn about [application refactoring strategies to decompose microservices](/architecture/microservices-architecture-refactoring-monoliths) .\n- Read the third document in this series to learn about [interservice communication in a microservices setup](/architecture/microservices-architecture-interservice-communication) .\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Docs"}