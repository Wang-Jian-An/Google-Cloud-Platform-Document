{"title": "Docs - Optimize cost: Compute, containers, and serverless", "url": "https://cloud.google.com/architecture/framework/cost-optimization/compute", "abstract": "# Docs - Optimize cost: Compute, containers, and serverless\nLast reviewed 2023-07-12 UTC\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides recommendations to help you optimize the cost of your virtual machines (VMs), containers, and serverless resources in Google Cloud.\nThe guidance in this section is intended for architects, developers, and administrators who are responsible for provisioning and managing compute resources for workloads in the cloud.\nCompute resources are the most important part of your cloud infrastructure. When you migrate your workloads to Google Cloud, a typical first choice is Compute Engine, which lets you provision and manage VMs efficiently in the cloud. Compute Engine offers a wide range of machine types, and is available globally in all the [Google Cloud regions](/about/locations#products-available-by-location) . Compute Engine's predefined and custom machine types let you provision VMs that offer similar compute capacity as your on-premises infrastructure, enabling you to accelerate the migration process. Compute Engine gives you the pricing advantage of paying only for the infrastructure that you use and provides significant savings as you use more compute resources with sustained-use discounts.\nIn addition to Compute Engine, Google Cloud offers containers and serverless compute services. The serverless approach can be more cost-efficient for new services that aren't always running (for example, APIs, data processing, and event processing).\nAlong with general recommendations, this document provides guidance to help you optimize the cost of your compute resources when using the following products:\n- Compute Engine\n- Google Kubernetes Engine (GKE)\n- Cloud Run\n- Cloud Functions\n- App Engine", "content": "## General recommendations\nThe following recommendations are applicable to all the compute, containers, and serverless services in Google Cloud that are discussed in this section.\n### Track usage and cost\nUse the following tools and techniques to monitor resource usage and cost:\n- View and respond to cost-optimization recommendations in the [Recommendation Hub](/recommender/docs/recommendation-hub/identify-configuration-problems) .\n- Get email notifications for potential increases in resource usage and cost by [configuring budget alerts](/billing/docs/how-to/budgets) .\n- [Manage and respond to alerts programmatically](/billing/docs/how-to/budgets-programmatic-notifications) by using the Pub/Sub and Cloud Functions services.\n### Control resource provisioning\nUse the following recommendations to control the quantity of resources provisioned in the cloud and the location where the resources are created:\n- To help ensure that resource consumption and cost don't exceed the forecast, use resource [quotas](https://console.cloud.google.com/iam-admin/quotas) .\n- Provision resources in the lowest-cost region that meets the latency requirements of your workload. To control where resources are provisioned, you can use the organization policy constraint [gcp.resourceLocations](/resource-manager/docs/organization-policy/org-policy-constraints#constraints-supported-by-multiple-google-cloud-services) .\n### Get discounts for committed use\n[Committed use discounts (CUDs)](/docs/cuds) are ideal for workloads with predictable resource needs. After migrating your workload to Google Cloud, find the baseline for the resources required, and get deeper discounts for committed usage. For example, purchase a one or three-year commitment, and get a substantial discount on Compute Engine VM pricing.\n### Automate cost-tracking using labels\nDefine and assign [labels](/resource-manager/docs/creating-managing-labels) consistently. The following are examples of how you can use labels to automate cost-tracking:\n- For VMs that only developers use during business hours, assign the label `env: development` . You can use Cloud Scheduler to set up a serverless Cloud Function to shut down these VMs after business hours, and restart them when necessary.\n- For an application that has several Cloud Run services and Cloud Functions instances, assign a consistent label to all the [Cloud Run](/run/docs/configuring/labels) and [Cloud Functions](/sdk/gcloud/reference/functions/deploy#--update-labels) resources. Identify the high-cost areas, and take action to reduce cost.\n### Customize billing reports\nConfigure your [Cloud Billing reports](https://console.cloud.google.com/billing/reports) by setting up the required filters and grouping the data as necessary (for example, by projects, services, or labels).\n### Promote a cost-saving culture\nTrain your developers and operators on your cloud infrastructure. Create and promote learning programs using traditional or online classes, discussion groups, peer reviews, pair programming, and cost-saving games. As shown in [Google's DORA research](/devops) , organizational culture is a key driver for improving performance, reducing rework and burnout, and optimizing cost. By giving employees visibility into the cost of their resources, you help them align their priorities and activities with business objectives and constraints.\n## Compute Engine\nThis section provides guidance to help you optimize the cost of your Compute Engine resources. In addition to this guidance, we recommend that you follow the [general recommendations](#general_recommendations) discussed earlier.\n### Understand the billing model\nTo learn about the billing options for Compute Engine, see [Pricing](/compute/all-pricing) .\n### Analyze resource consumption\nTo help you to understand resource consumption in Compute Engine, export usage data to BigQuery. Query the BigQuery datastore to analyze your project's virtual CPU (vCPU) usage trends, and determine the number of vCPUs that you can reclaim. If you've defined thresholds for the number of cores per project, analyze usage trends to spot anomalies and take corrective actions.\n### Reclaim idle resources\nUse the following recommendations to identify and reclaim unused VMs and disks, such as VMs for proof-of-concept projects that have since been deprioritized:\n- Use the [idle VM recommender](/compute/docs/instances/viewing-and-applying-idle-vm-recommendations) to identify inactive VMs and persistent disks based on usage metrics.\n- Before deleting resources, assess the potential impact of the action and plan to recreate the resources if that becomes necessary.\n- Before deleting a VM, consider [taking a snapshot](/compute/docs/disks/create-snapshots) . When you delete a VM, the attached disks are deleted, unless you've selected the **Keep disk** option.\n- When feasible, consider stopping VMs instead of deleting them. When you stop a VM, the instance is terminated, but disks and IP addresses are retained until you detach or delete them.\n### Adjust capacity to match demand\nSchedule your VMs to start and stop automatically. For example, if a VM is used only eight hours a day for five days a week (that's 40 hours in the week), you can potentially reduce costs by 75 percent by stopping the VM during the 128 hours in the week when the VM is not used.\nAutoscale compute capacity based on demand by using [managed instance groups](/compute/docs/autoscaler) . You can autoscale capacity based on the parameters that matter to your business (for example, CPU usage or load-balancing capacity).\n### Choose appropriate machine types\nSize your VMs to match your workload's compute requirements by using the [VM machine type recommender](/compute/docs/instances/apply-machine-type-recommendations-for-instances) .\nFor workloads with predictable resource requirements, tailor the machine type to your needs and save money by using [custom VMs](/compute/docs/instances/creating-instance-with-custom-machine-type#specifications) .\nFor batch-processing workloads that are fault-tolerant, consider using [Spot VMs](/compute/docs/instances/spot) . High-performance computing (HPC), big data, media transcoding, continuous integration and continuous delivery (CI/CD) pipelines, and stateless web applications are examples of workloads that can be deployed on Spot VMs. For an example of how Descartes Labs reduced their analysis costs by using preemptible VMs (the older version of Spot VMs) to process satellite imagery, see the [Descartes Labs case study](/customers/descartes-labs) .\n### Evaluate licensing options\nWhen you migrate third-party workloads to Google Cloud, you might be able to reduce cost by bringing your own licenses (BYOL). For example, to deploy Microsoft Windows Server VMs, instead of using a [premium image](/compute/disks-image-pricing#premiumimages) that incurs additional cost for the third-party license, you can create and use a [custom Windows BYOL image](/compute/docs/images/creating-custom-windows-byol-images) . You then pay only for the VM infrastructure that you use on Google Cloud. This strategy helps you continue to realize value from your existing investments in third-party licenses.\nIf you decide to use a BYOL approach, we recommend that you do the following:\n- Provision the required number of compute CPU cores independently of memory by using [custom machine types](/compute/docs/instances/creating-instance-with-custom-machine-type#extendedmemory) , and limit the third-party licensing cost to the number of CPU cores that you need.\n- Reduce the number of vCPUs per core from 2 to 1 by disabling [simultaneous multithreading](/compute/docs/instances/configuring-simultaneous-multithreading) (SMT), and reduce your licensing costs by 50 percent.\nIf your third-party workloads need dedicated hardware to meet security or compliance requirements, you can bring your own licenses to [sole-tenant nodes](/compute/docs/nodes/bringing-your-own-licenses) .\n## Google Kubernetes Engine\nThis section provides guidance to help you optimize the cost of your GKE resources.\nIn addition to the following recommendations, see the [general recommendations](#general_recommendations) discussed earlier:\n- Use [GKE Autopilot](/kubernetes-engine/docs/concepts/autopilot-overview) to let GKE maximize the efficiency of your cluster's infrastructure. You don't need to monitor the health of your nodes, handle bin-packing, or calculate the capacity that your workloads need.\n- Fine-tune GKE autoscaling by using Horizontal Pod Autoscaler ( [HPA](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#horizontal_pod_autoscaler) ), Vertical Pod Autoscaler ( [VPA](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#vertical_pod_autoscaler) ), Cluster Autoscaler ( [CA](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#cluster_autoscaler) ), or [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) based on your workload's requirements.\n- For batch workloads that aren't sensitive to startup latency, use the [optimization-utilization](/kubernetes-engine/docs/concepts/cluster-autoscaler#autoscaling_profiles) autoscaling profile to help improve the utilization of the cluster.\n- Use [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) to extend the GKE cluster autoscaler, and efficiently create and delete node pools based on the [specifications](https://kubernetes.io/docs/concepts/workloads/pods) of pending pods without over-provisioning.\n- Use separate node pools: a static node pool for static load, and dynamic node pools with cluster autoscaling groups for dynamic loads.\n- Use [Spot VMs](/kubernetes-engine/docs/how-to/spot-vms) for Kubernetes node pools when your pods are fault-tolerant and can terminate gracefully in less than 25 seconds. Combined with the GKE cluster autoscaler, this strategy helps you ensure that the node pool with lower-cost VMs (in this case, the node pool with Spot VMs) scales first.\n- Choose [cost-efficient machine types](/compute#section-6) (for example: [E2](/compute/docs/general-purpose-machines#e2_machine_types) , [N2D](/blog/products/compute/announcing-the-n2d-vm-family-based-on-amd) , [T2D](/blog/products/compute/google-cloud-introduces-tau-vms) ), which provide 20\u201340% higher performance-to-price.\n- Use [GKE usage metering](/kubernetes-engine/docs/how-to/cluster-usage-metering) to analyze your clusters' usage profiles by namespaces and labels. Identify the team or application that's spending the most, the environment or component that caused spikes in usage or cost, and the team that's wasting resources.\n- Use [resource quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/) in multi-tenant clusters to prevent any tenant from using more than its assigned share of cluster resources.\n- [Schedule automatic downscaling](/architecture/reducing-costs-by-scaling-down-gke-off-hours) of development and test environments after business hours.\n- Follow the [best practices](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke) for running cost-optimized Kubernetes applications on GKE.## Cloud Run\nThis section provides guidance to help you optimize the cost of your Cloud Run resources.\nIn addition to the following recommendations, see the [general recommendations](#general_recommendations) discussed earlier:\n- Adjust the [concurrency setting](/run/docs/about-concurrency) (default: 80) to reduce cost. Cloud Run determines the number of requests to be sent to an instance based on CPU and memory usage. By increasing the request concurrency, you can reduce the number of instances required.\n- [Set a limit](/run/docs/configuring/max-instances) for the number of instances that can be deployed.\n- Estimate the number of instances required by using the [Billable Instance Time](/monitoring/api/metrics_gcp#gcp-run) metric. For example, if the metric shows`100s/s`, around 100 instances were scheduled. Add a 30% buffer to preserve performance; that is, 130 instances for 100s/s of traffic.\n- To reduce the impact of cold starts, configure a [minimum number of instances](/run/docs/configuring/min-instances) . When these instances are idle, they are billed at a tenth of the price.\n- Track [CPU usage](/monitoring/api/metrics_gcp#gcp-run) , and adjust the CPU limits accordingly.\n- Use [ traffic management](/run/docs/rollouts-rollbacks-traffic-migration) to determine a cost-optimal configuration.\n- Consider using [Cloud CDN](/cdn/docs/setting-up-cdn-with-serverless) or Firebase Hosting for serving static assets.\n- For Cloud Run apps that handle requests globally, consider deploying the app to [multiple regions](/run/docs/multiple-regions) , because cross continent data transfer can be expensive. This design is recommended if you use a load balancer and CDN.\n- [Reduce the startup times](/run/docs/tips/general#starting_services_quickly) for your instances, because the startup time is also billable.\n- Purchase [Committed Use Discounts](/run/cud) , and save up to 17% off the on-demand pricing for a one-year commitment.## Cloud Functions\nThis section provides guidance to help you optimize the cost of your Cloud Functions resources.\nIn addition to the following recommendations, see the [general recommendations](#general_recommendations) discussed earlier:\n- Observe [the execution time](/monitoring/api/metrics_gcp#gcp-cloudfunctions) of your functions. Experiment and benchmark to design the smallest function that still meets your required performance threshold.\n- If your Cloud Functions workloads run constantly, consider using GKE or Compute Engine to handle the workloads. Containers or VMs might be lower-cost options for always-running workloads.\n- Limit the [number of function instances](/functions/docs/configuring/max-instances) that can co-exist.\n- Benchmark the runtime performance of the Cloud Functions [programming languages](/functions/docs/writing) against the workload of your function. Programs in compiled languages have longer cold starts, but run faster. Programs in interpreted languages run slower, but have a lower cold-start overhead. Short, simple functions that run frequently might cost less in an interpreted language.\n- [Delete temporary files](/functions/docs/bestpractices/tips#always_delete_temporary_files) written to the local disk, which is an in-memory file system. Temporary files consume memory that's allocated to your function, and sometimes persist between invocations. If you don't delete these files, an out-of-memory error might occur and trigger a cold start, which increases the execution time and cost.## App Engine\nThis section provides guidance to help you optimize the cost of your App Engine resources.\nIn addition to the following recommendations, see the [general recommendations](#general_recommendations) discussed earlier:\n- Set maximum instances based on your traffic and request latency. App Engine usually [scales capacity](/appengine/docs/standard/go/how-instances-are-managed#scaling_types) based on the traffic that the applications receive. You can control cost by limiting the number of instances that App Engine can create.\n- To limit the memory or CPU available for your application, set an [instance class](/appengine/docs/standard#instance_classes) . For CPU-intensive applications, allocate more CPU. Test a few configurations to determine the optimal size.\n- Benchmark your App Engine workload in multiple [programming languages](/appengine/docs) . For example, a workload implemented in one language may need fewer instances and lower cost to complete tasks on time than the same workload programmed in another language.\n- Optimize for fewer cold starts. When possible, reduce CPU-intensive or long-running tasks that occur in the global scope. Try to break down the task into smaller operations that can be \"lazy loaded\" into the context of a request.\n- If you expect bursty traffic, configure a [minimum number of idle instances](/appengine/docs/standard/go/how-instances-are-managed#scaling_types) that are pre-warmed. If you are not expecting traffic, you can configure the minimum idle instances to zero.\n- To balance performance and cost, run an A/B test by [splitting traffic](/appengine/docs/standard/go/splitting-traffic) between two versions, each with a different configuration. Monitor the performance and cost of each version, tune as necessary, and decide the configuration to which traffic should be sent.\n- Configure [request concurrency](/appengine/docs/standard/go/how-requests-are-handled#handling_requests) , and set the maximum concurrent requests higher than the default. The more requests each instance can handle concurrently, the more efficiently you can use existing instances to serve traffic.## What's next\n- Learn about optimizing the cost of GKE resources:- [Run cost-optimized Kubernetes applications on GKE: best practices](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke) \n- [Best practices for reducing GKE over-provisioning](/blog/products/containers-kubernetes/gke-best-practices-to-lessen-over-provisioning) \n- [Optimize resource usage in a multi-tenant GKE cluster using node auto-provisioning](/architecture/optimizing-resources-in-multi-tenant-gke-clusters-with-auto-provisioning) \n- [Reduce costs by scaling down GKE clusters during off-peak hours](/architecture/reducing-costs-by-scaling-down-gke-off-hours) \n- [Run web applications on GKE using cost-optimized Spot VMs](/kubernetes-engine/docs/archive/run-web-applications-on-gke-using-cost-optimized-spot-vms-and-traffic-director) \n- [Estimate your GKE costs early in the development cycle using GitLab](/architecture/estimate-gke-costs-early-using-gitlab) \n- [Video series: GKE Cost Optimization](https://www.youtube.com/playlist?list=PLIivdWyY5sqIUx9ZVsn4BzaIVTRAWYPxi) \n- [Training: Optimize Costs for GKE](https://www.cloudskillsboost.google/quests/157) \n- Learn about optimizing the cost of Cloud Run and Cloud Functions resources:- [Manage cost and reliability in fully managed applications](/blog/products/serverless/managing-cost-and-reliability-serverless-applications) \n- [Optimize for long-term cost management in fully managed applications](/blog/products/serverless/cost-optimization-for-serverless-workloads) \n- [Maximize your Cloud Run investments with new committed use discounts](/blog/products/serverless/introducing-committed-use-discounts-for-cloud-run) \n- Optimize cost for storage, databases, networking, and operations:- [Optimize cost: Storage](/architecture/framework/cost-optimization/storage) \n- [Optimize cost: Databases and smart analytics](/architecture/framework/cost-optimization/databases) \n- [Optimize cost: Networking](/architecture/framework/cost-optimization/networking) \n- [Optimize cost: Cloud operations](/architecture/framework/cost-optimization/cloudops) \n- Explore the other categories of the [Google Cloud Architecture Framework](/architecture/framework)", "guide": "Docs"}