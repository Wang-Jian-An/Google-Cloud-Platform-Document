{"title": "Docs - Analyze your data", "url": "https://cloud.google.com/architecture/framework/system-design/data-analytics", "abstract": "# Docs - Analyze your data\nLast reviewed 2023-08-08 UTC\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) explains some of the core principles and best practices for data analytics in Google Cloud. You learn about some of the key data-analytics services, and how they can help at the various stages of the data lifecycle. These best practices help you to meet your data analytics needs and create your system design.\n", "content": "## Core principles\nBusinesses want to analyze data and generate actionable insights from that data. Google Cloud provides you with various services that help you through the entire data lifecycle, from data ingestion through reports and visualization. Most of these services are fully managed, and some are serverless. You can also build and manage a data-analytics environment on [Compute Engine VMs](/compute) , such as to self-host Apache [Hadoop](https://hadoop.apache.org/) or [Beam](https://beam.apache.org/) .\nYour particular focus, team expertise, and strategic outlook help you to determine which Google Cloud services you adopt to support your data analytics needs. For example, [Dataflow](/dataflow) lets you write complex transformations in a serverless approach, but you must rely on an opinionated version of configurations for compute and processing needs. Alternatively, [Dataproc](/dataproc) lets you run the same transformations, but you manage the clusters and fine-tune the jobs yourself.\nIn your system design, think about which processing strategy your teams use, such as [extract, transform, load (ETL)](/learn/what-is-etl) or [extract, load, transform (ELT)](https://wikipedia.org/wiki/Extract,_load,_transform) . Your system design should also consider whether you need to process [batch analytics or streaming analytics](/learn/what-is-streaming-analytics) . Google Cloud provides a unified data platform, and it lets you build a [data lake](/learn/what-is-a-data-lake) or a [data warehouse](/learn/what-is-a-data-warehouse) to meet your business needs.\n## Key services\nThe following table provides a high-level overview of Google Cloud analytics services:\n| Google Cloud service | Description                                                            |\n|:-----------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Pub/Sub    | Simple, reliable, and scalable foundation for stream analytics and event-driven computing systems.                                      |\n| Dataflow    | A fully managed service to transform and enrich data in stream (real time) and batch (historical) modes.                                     |\n| Dataprep by Trifacta | Intelligent data service to visually explore, clean, and prepare structured and unstructured data for analysis.                                   |\n| Dataproc    | Fast, easy-to-use, and fully managed cloud service to run Apache Spark and Apache Hadoop clusters.                                      |\n| Cloud Data Fusion  | Fully managed, data integration service that's built for the cloud and lets you build and manage ETL/ELT data pipelines. Cloud DataFusion provides a graphical interface and a broad open source library of preconfigured connectors and transformations. |\n| BigQuery    | Fully managed, low-cost, serverless data warehouse that scales with your storage and compute power needs. BigQuery is a columnar and ANSI SQL database that can analyze terabytes to petabytes of data.             |\n| Cloud Composer   | Fully managed workflow orchestration service that lets you author, schedule, and monitor pipelines that span clouds and on-premises data centers.                           |\n| Data Catalog   | Fully managed and scalable metadata management service that helps you discover, manage, and understand all your data.                                  |\n| Looker Studio   | Fully managed visual analytics service that can help you unlock insights from data through interactive dashboards.                                  |\n| Looker     | Enterprise platform that connects, analyzes, and visualizes data across multi-cloud environments.                                       |\n| Dataform    | Fully managed product to help you collaborate, create, and deploy data pipelines, and ensure data quality.                                    |\n| Dataplex    | Managed data lake service that centrally manages, monitors, and governs data across data lakes, data warehouses, and data marts using consistent controls.                        |\n| AnalyticsHub   | Platform that efficiently and securely exchanges data analytics assets across your organization to address challenges of data reliability and cost.                          |\n## Data lifecycle\nWhen you create your system design, you can group the Google Cloud data analytics services around the general data movement in any system, or around the data lifecycle.\nThe data lifecycle includes the following stages and example services:\n- includes services such as [Pub/Sub](/pubsub) , [Storage Transfer Service](/storage-transfer-service) , [Transfer Appliance](/transfer-appliance) , and [BigQuery](/bigquery) .\n- includes services such as [Cloud Storage](/storage) , [Bigtable](/bigtable) , [Memorystore](/memorystore) , and [BigQuery](/bigquery) .\n- includes services such as [Dataflow](/dataflow) , [Dataproc](/dataproc) , [Dataprep](/dataprep) , [Sensitive Data Protection](/dlp) , and [BigQuery](/bigquery) .\n- includes services such as [BigQuery](/bigquery) .\n- includes services such as [Looker Studio](https://lookerstudio.google.com/overview) and [Looker](https://looker.com/google-cloud) .\nThe following stages and services run across the entire data lifecycle:\n- includes services such as [Data Fusion](/data-fusion) .\n- includes services such as [Data Catalog](/data-catalog) .\n- includes services such as [Cloud Composer](/composer) .## Data ingestion\nApply the following data ingestion best practices to your own environment.\n### Determine the data source for ingestion\nData typically comes from another cloud provider or service, or from an on-premises location:\n- To ingest data from other cloud providers, you typically use [Cloud Data Fusion](/data-fusion) , [Storage Transfer Service](/storage-transfer-service) , or [BigQuery Transfer Service](/bigquery-transfer/docs/introduction) .\n- For on-premises data ingestion, consider the volume of data to ingest and your team's skill set. If your team prefers a low-code, graphical user interface (GUI) approach, use [Cloud Data Fusion](/data-fusion) with a suitable connector, such as [Java Database Connectivity (JDBC)](https://docs.oracle.com/javase/tutorial/jdbc/overview/index.html) . For large volumes of data, you can use [Transfer Appliance](/transfer-appliance) or [Storage Transfer Service](/storage-transfer-service) .\nConsider how you want to process your data after you ingest it. For example, Storage Transfer Service only writes data to a Cloud Storage bucket, and [BigQuery Data Transfer Service](/bigquery-transfer/docs) only writes data to a BigQuery dataset. Cloud Data Fusion supports multiple destinations.\n### Identify streaming or batch data sources\nConsider how you need to use your data and identify where you have streaming or batch use cases. For example, if you run a global streaming service that has low latency requirements, you can use [Pub/Sub](/pubsub) . If you need your data for analytics and reporting uses, you can [stream data into BigQuery](/bigquery/streaming-data-into-bigquery) .\nIf you need to stream data from a system like [Apache Kafka](https://kafka.apache.org/) in an on-premises or other cloud environment, use the [Kafka to BigQuery Dataflow template](https://github.com/GoogleCloudPlatform/DataflowTemplates/tree/master/v2/kafka-to-bigquery) . For batch workloads, the first step is usually to ingest data into Cloud Storage. Use the [gsutil](/storage/docs/gsutil) tool or [Storage Transfer Service](/storage-transfer/docs/overview) to ingest data.\n### Ingest data with automated tools\nManually moving data from other systems into the cloud can be a challenge. If possible, use tools that let you automate the data ingestion processes. For example, [Cloud Data Fusion](/data-fusion) provides connectors and plugins to bring data from external sources with a drag-and-drop GUI. If your teams want to write some code, [Data Flow](/dataflow) or [BigQuery](/bigquery) can help to automate data ingestion. [Pub/Sub](/pubsub) can help in both a low-code or code-first approach. To ingest data into storage buckets, use [gsutil](/storage/docs/gsutil) for [data sizes of up to 1 TB](/architecture/migration-to-google-cloud-transferring-your-large-datasets#options_available_from_google) . To ingest amounts of data larger than 1 TB, use [Storage Transfer Service](/storage-transfer/docs/overview) .\n### Use migration tools to ingest from another data warehouse\nIf you need to migrate from another data warehouse system, such as Teradata, Netezza, or Redshift, you can use the BigQuery Data Transfer Service [migration assistance](/bigquery-transfer/docs/migrations) . The BigQuery Data Transfer Service also provides [third-party transfers](/bigquery-transfer/docs/third-party-transfer) that help you ingest data on a schedule from external sources. For more information, see the [detailed migration approaches](/architecture/dw2bq/dw-bq-migration-overview) for each data warehouse.\n### Estimate your data ingestion needs\nThe volume of data that you need to ingest helps you to determine which service to use in your system design. For streaming ingestion of data, [Pub/Sub](/pubsub) scales to tens of gigabytes per second. Capacity, storage, and regional requirements for your data help you to determine whether Pub/Sub Lite is a better option for your system design. For more information, see [Choosing Pub/Sub or Pub/Sub Lite](/pubsub/docs/choosing-pubsub-or-lite) .\nFor batch ingestion of data, estimate how much data you want to transfer in total, and how quickly you want to do it. Review the [available migration options](/architecture/migration-to-google-cloud-transferring-your-large-datasets#options_available_from_google) , including an [estimate on time](/architecture/migration-to-google-cloud-transferring-your-large-datasets#time) and [comparison of online versus offline transfers](/architecture/migration-to-google-cloud-transferring-your-large-datasets#online_versus_offline_transfer) .\n### Use appropriate tools to regularly ingest data on a schedule\n[Storage Transfer Service](/storage-transfer/docs/overview) and [BigQuery Data Transfer Service](/bigquery-transfer/docs/introduction) both let you schedule ingestion jobs. For fine-grain control of the timing of ingestion or the source and destination system, use a workflow-management system like [Cloud Composer](/composer) . If you want a more manual approach, you can [use Cloud Scheduler and Pub/Sub to trigger a Cloud Function](/scheduler/docs/tut-gcf-pub-sub) . If you want to manage the Compute infrastructure, you can use the [gsutil](/storage/docs/gsutil) command with cron for data transfer of up to 1 TB. If you use this manual approach instead of Cloud Composer, follow the [best practices to script production transfers](/storage/docs/gsutil/addlhelp/ScriptingProductionTransfers) .\n### Review FTP/SFTP server data ingest needs\nIf you need a code-free environment to ingest data from an FTP/SFTP server, you can use the [FTP copy plugins](https://github.com/data-integrations/ftp-copy-action) . If you want to modernize and create a long-term workflow solution, Cloud Composer is a fully managed service that lets you read and write from various sources and sinks.\n### Use Apache Kafka connectors to ingest data\nIf you use Pub/Sub, Dataflow, or BigQuery, you can ingest data using one of the [Apache Kafka connectors](https://cloud.google.com/blog/products/data-analytics/apache-kafka-for-gcp-users-connectors-for-pubsub-dataflow-and-bigquery) . For example, the [open source Pub/Sub Kafka connector](https://github.com/GoogleCloudPlatform/pubsub/tree/master/kafka-connector) lets you ingest data from Pub/Sub or Pub/Sub Lite.\n### Additional resources\n- [Cloud Storage Transfer Service agent best practices](/storage-transfer/docs/on-prem-agent-best-practices) \n- [How to ingest data into BigQuery so you can analyze it](https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-data-ingestion) \n- [Ingesting clinical and operational data with Cloud Data Fusion](/architecture/ingesting-clinical-and-operational-data-with-cloud-data-fusion) \n- [Optimizing large-scale ingestion of analytics events and logs](/architecture/optimized-large-scale-analytics-ingestion) ## Data storage\nApply the following data storage best practices to your own environment.\n### Choose the appropriate data store for your needs\nTo help you choose what type of storage solution to use, review and understand the downstream usage of your data. The following common use cases for your data give recommendations for which Google Cloud product to use:\n| Data use case       | Product recommendation                         |\n|:---------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|\n| File-based        | Filestore                            |\n| Object-based       | Cloud Storage                           |\n| Low latency       | Bigtable                            |\n| Time series       | Bigtable                            |\n| Online cache       | Memorystore                           |\n| Transaction processing     | Cloud SQL                            |\n| Business intelligence (BI) & analytics | BigQuery                            |\n| Batch processing      | Cloud Storage Bigtable if incoming data is time series and you need low latency access to it. BigQuery if you use SQL. |\n### Review your data structure needs\nFor most data, such as documents and text files, audio and video files, or logs, an object-based store is the most suitable choice. You can then load and process the data from object storage when you need it.\nFor data, such as XML or JSON, your use cases and data access patterns help guide your choice. You can load such datasets into BigQuery for [automatic schema detection](/bigquery/docs/schema-detect) . If you have low latency requirements, you can load your JSON data into Bigtable. If you have legacy requirements or your applications work with relational databases, you can also load datasets into a relation store.\nFor , such as CSV, Parquet, Avro, or ORC, you can use BigQuery if you have BI and analytics requirements that use SQL. For more information, see [how to batch load data](/bigquery/docs/batch-loading-data) . If you want to create a data lake on open standards and technologies, you can use Cloud Storage.\n### Migrate data and reduce costs for HDFS\nLook for ways to move Hadoop Distributed File System (HDFS) data from on-premises or from another cloud provider to a cheaper object-storage system. Cloud Storage is the most common choice that enterprises make as an alternative data store. For information about the advantages and disadvantages of this choice, see [HDFS vs. Cloud Storage](https://cloud.google.com/blog/products/storage-data-transfer/hdfs-vs-cloud-storage-pros-cons-and-migration-tips) .\nYou can move data with a push or pull method. Both methods use the `hadoop distcp` command. For more information, see [Migrating HDFS Data from On-Premises to Google Cloud](/architecture/hadoop/hadoop-gcp-migration-data) .\nYou can also use the open source [Cloud Storage connector](/dataproc/docs/concepts/connectors/cloud-storage) to let Hadoop and Spark jobs access data in Cloud Storage. The connector is installed by default on Dataproc clusters, and can be [manually installed on other clusters](/dataproc/docs/concepts/connectors/cloud-storage#non-dataproc_clusters) .\n### Use object storage to build a cohesive data lake\nA [data lake](/learn/what-is-a-data-lake) is a centralized repository designed to store, process, and secure large amounts of structured, semistructured, and unstructured data. You can [use Cloud Composer and Cloud Data Fusion to build a data lake](https://cloud.google.com/blog/topics/developers-practitioners/architect-your-data-lake-google-cloud-data-fusion-and-composer) .\nTo build a modern data platform, you can use [BigQuery](/bigquery) as your central data source instead of Cloud Storage. BigQuery is a modern data warehouse with [separation of storage and compute](https://cloud.google.com/blog/products/bigquery/separation-of-storage-and-compute-in-bigquery) . A data lake built on BigQuery lets you perform traditional analytics from BigQuery in the Cloud console. It also lets you [access the data stored](/dataproc/docs/tutorials/bigquery-connector-spark-example) from other frameworks such as Apache Spark.\n### Additional resources\n- [Best practices for Cloud Storage](/storage/docs/best-practices) \n- [Best practices for Cloud Storage cost optimization](https://cloud.google.com/blog/products/storage-data-transfer/best-practices-for-cloud-storage-cost-optimization) \n- [Best practices for ensuring privacy and security of your data in Cloud Storage](https://cloud.google.com/blog/products/storage-data-transfer/google-cloud-storage-best-practices-to-help-ensure-data-privacy-and-security) \n- [Best practices for Memorystore](/memorystore/docs/redis/general-best-practices) \n- [Optimizing storage in BigQuery](/bigquery/docs/best-practices-storage) \n- [Bigtable schema design](/bigtable/docs/schema-design) ## Process and transform data\nApply the following data analytics best practices to your own environment when you process and transform data.\n### Explore the open source software you can use in Google Cloud\nMany Google Cloud services use open source software to help make your transition seamless. Google Cloud offers managed and serverless solutions that have Open APIs and are compatible with open source frameworks to reduce vendor lock-in.\n[Dataproc](/dataproc) is a Hadoop-compatible managed service that lets you host open source software, with little operational burden. Dataproc [includes support](/dataproc/docs/concepts/versioning/dataproc-versions) for Spark, Hive, Pig, Presto, and Zookeeper. It also provides [Hive Metastore as a managed service](/dataproc-metastore/docs) to remove itself as a single point of failure in the Hadoop ecosystem.\nYou can migrate to [Dataflow](/dataflow) if you currently use Apache Beam as a batch and streaming processing engine. Dataflow is a fully managed and serverless service that uses Apache Beam. Use Dataflow to write jobs in Beam, but let Google Cloud manage the execution environment.\nIf you use [CDAP](https://github.com/cdapio/cdap) as your data integration platform, you can migrate to [Cloud Data Fusion](/data-fusion) for a fully managed experience.\n### Determine your ETL or ELT data-processing needs\nYour team's experience and preferences help determine your system design for how to process data. Google Cloud lets you use either [traditional ETL](/learn/what-is-etl) or [more modern ELT](https://en.wikipedia.org/wiki/Extract,_load,_transform) data-processing systems.\n- For ETL pipelines, you can use [Data Fusion](/data-fusion) , [Dataproc](/dataproc) , or [Dataflow](/dataflow) .- For new environments, we recommend Dataflow for a [unified way to create batch and streaming applications](/dataflow/docs/samples/reference-patterns) .\n- For a fully managed approach, Data Fusion provides a drag-and-drop GUI to help you create pipelines.\n- For ELT pipelines, use BigQuery, which supports both [ batch and streaming data load](/bigquery/docs/loading-data) . After your data is in BigQuery, use SQL to perform all transformations to derive new datasets for your business use cases.\n- If you want to modernize and [move from ETL to ELT](https://dataform.co/academy/data-warehousing-elt) , you can use Dataform. To support BI with BigQuery, you can also use a third-party service like [Fivetran](/architecture/partners/using-fivetran-and-elt-with-bigquery) .\n### Use the appropriate framework for your data use case\nYour data use cases determine which tools and frameworks to use. Some Google Cloud products are built to handle all of the following data use cases, while others best support only one particular use case.\n- For adata processing system, you can process and transform data in BigQuery with a familiar SQL interface. If you have an existing pipeline that runs on Apache Hadoop or Spark on-premises or in another public cloud, you can use Dataproc.- You can also use Dataflow if you want a unified programing interface for both batch and streaming use cases. We recommend that you modernize and use Dataflow for ETL and BigQuery for ELT.\n- For data pipelines, you use a managed and serverless service like Dataflow that provides windowing, autoscaling, and templates. For more information, see [Building production-ready data pipelines using Dataflow](/architecture/building-production-ready-data-pipelines-using-dataflow-overview) .- If you have analytics and SQL-focused teams and capabilities, you can also [stream data into BigQuery](/bigquery/streaming-data-into-bigquery) .\n- For use cases, such as time series analysis or streaming video analytics, use Dataflow.\n### Retain future control over your execution engine\nTo minimize vendor lock-in and to be able to use a different platform in the future, use the [Apache Beam programming model](https://beam.apache.org/documentation/programming-guide/) and [Dataflow](/dataflow) as a managed serverless solution. The Beam programming model lets you [change the underlying execution engine](https://beam.apache.org/documentation/runners/capability-matrix/) , such as changing from Dataflow to [Apache Flink](https://flink.apache.org/) or [Apache Spark](https://spark.apache.org/) .\n### Use Dataflow to ingest data from multiple sources\nTo ingest data from multiple sources, such as Pub/Sub, Cloud Storage, HDFS, S3, or Kafka, use [Dataflow](/dataflow) . Dataflow is a managed serverless service that supports [Dataflow templates](/dataflow/docs/concepts/dataflow-templates) , which lets your teams run templates from different tools.\n[Dataflow Prime](https://cloud.google.com/blog/products/data-analytics/simplify-and-automate-data-processing-with-dataflow-prime) provides horizontal and vertical autoscaling of machines that are used in the execution process of a pipeline. It also provides smart diagnostics and recommendations that identify problems and suggest how to fix them.\n### Discover, identify, and protect sensitive data\nUse [Sensitive Data Protection](/dlp) to inspect and transform structured and unstructured data. Sensitive Data Protection works for data located anywhere in Google Cloud, such as in Cloud Storage or databases. You can classify, mask, and tokenize your sensitive data to continue to use it safely for downstream processing. Use Sensitive Data Protection to perform actions such as to [scan BigQuery data](/bigquery/docs/scan-with-dlp) or [de-identify and re-identify PII in large-scale datasets](/architecture/de-identification-re-identification-pii-using-cloud-dlp) .\n### Modernize your data transformation processes\nUse [Dataform](https://cloud.google.com/blog/products/data-analytics/welcoming-dataform-to-bigquery) to write data transformations as code and to start to use version control by default. You can also adopt software development best practices such as CI/CD, unit tests, and version control to SQL code. Dataform supports all major cloud data warehouse products and databases, such as PostgreSQL.\n### Additional Resources\n- Dataproc- [Best practices guide](https://cloud.google.com/blog/topics/developers-practitioners/dataproc-best-practices-guide) \n- [Tips for long-running clusters](https://cloud.google.com/blog/products/data-analytics/10-tips-for-building-long-running-clusters-using-cloud-dataproc) \n- [Using Apache Hive on Dataproc](/architecture/using-apache-hive-on-cloud-dataproc) \n- [Best practices for running in production](https://cloud.google.com/blog/products/data-analytics/7-best-practices-for-running-cloud-dataproc-in-production) \n- [Best practices to use Apache Ranger](https://cloud.google.com/blog/products/data-analytics/running-cloud-managed-spark-and-hadoop-using-ranger) \n- [Autoscaling configuration recommendations](/dataproc/docs/concepts/configuring-clusters/autoscaling#autoscaling_configuration_recommendations) \n- [Help for slow Hadoop/Spark jobs](https://cloud.google.com/blog/products/data-analytics/help-for-slow-hadoopspark-jobs-on-google-cloud-10-questions-to-ask-about-your-hadoop-and-spark-cluster-performance) \n- Dataflow- [Move pipelines to production](https://cloud.google.com/blog/products/data-analytics/tips-and-tricks-to-get-your-cloud-dataflow-pipelines-into-production) \n- [Common use cases](https://cloud.google.com/blog/products/data-analytics/guide-to-common-cloud-dataflow-use-case-patterns-part-1) \n- Data Fusion- [Pipeline Performance](/data-fusion/docs/concepts/pipeline-performance) \n- BigQuery- [Optimize query performance](/bigquery/docs/best-practices-performance-overview) \n- Dataform- [Best practices for managing projects](https://docs.dataform.co/best-practices/best-practices) \n- Sensitive Data Protection- [Keep costs under control ](/dlp/docs/best-practices-costs) \n- [Inspect Storage and Databases for sensitive data](/dlp/docs/inspecting-storage) \n- [Inspect text for sensitive data](/dlp/docs/inspecting-text) \n## Data analytics and warehouses\nApply the following data analytics and warehouse best practices to your own environment.\n### Review your data storage needs\nData lakes and data warehouses aren't mutually exclusive. Data lakes are useful for unstructured and semi-structured data storage and processing. Data warehouses are best for analytics and BI.\nReview your data needs to help determine where to store your data and [which Google Cloud product is the most appropriate](#data-store) to process and analyze your data. Products like BigQuery can process PBs of data and grow with your demands.\n### Identify opportunities to migrate from a traditional data warehouse to BigQuery\nReview the traditional data warehouses that are currently in use in your environment. To reduce complexity and potentially reduce costs, identify opportunities to migrate your traditional data warehouses to a Google Cloud service like BigQuery. For more information and example scenarios, see [Migrating data warehouses to BigQuery](/architecture/dw2bq/dw-bq-migration-overview) .\n### Plan for federated access to data\nReview your data requirements and how you might need to interact with other products and services. Identify your data federation needs, and create an appropriate system design.\nFor example, BigQuery lets you define [external tables](/bigquery/external-data-sources) that can read data from other sources, such as Bigtable, Cloud SQL, Cloud Storage, or Google Drive. You can join these external sources with tables that you store in BigQuery.\n### Use BigQuery flex slots to provide on-demand burst capacity\nSometimes you need extra capacity to do experimental or exploratory analysis that needs a lot of compute resources. BigQuery lets you get additional compute capacity in the form of [flex slots](https://cloud.google.com/blog/products/data-analytics/introducing-bigquery-flex-slots) . These flex slots help you when there's a period of high demand or when you want to complete an important analysis.\n### Understand schema differences if you migrate to BigQuery\nBigQuery supports both and schemas, but by default it uses [nested and repeated fields](/bigquery/docs/nested-repeated) . Nested and repeated fields can be easier to read and correlate compared to other schemas. If your data is represented in a star or snowflake schema, and if you want to migrate to BigQuery, review your system design for any necessary changes to processes or analytics.\n### Additional resources\n- [Best practices for multi-tenant workloads on BigQuery](/bigquery/docs/best-practices-for-multi-tenant-workloads-on-bigquery) \n- [Best practices for row-level security in BigQuery](/bigquery/docs/best-practices-row-level-security) \n- [Best practices for materialized views in BigQuery](/bigquery/docs/materialized-views-best-practices) ## Reports and visualization\nApply the following reporting and visualization best practices to your own environment.\n### Use BigQuery BI Engine to visualize your data\n[BigQuery BI Engine](/bigquery/docs/bi-engine-intro) is a fast, in-memory analysis service. You can use BI Engine to analyze data stored in BigQuery with subsecond query response time and with high concurrency. BI Engine is integrated into the BigQuery API. Use [reserved BI Engine capacity](/bigquery/docs/bi-engine-reserve-capacity) to manage the on-demand or flat-rate pricing for your needs. BI Engine can also work with other BI or custom dashboard applications that require subsecond response times.\n### Modernize your BI processes with Looker\nLooker is a [modern, enterprise platform](https://looker.com/blog/what-is-modern-bi) for BI, data applications, and embedded analytics. You can create consistent data models on top of your data with speed and accuracy, and you can access data inside transactional and analytical datastores. Looker can also [analyze your data on multiple databases and clouds](https://cloud.google.com/blog/products/data-analytics/analyze-data-on-multiple-databases-clouds) . If you have existing BI processes and tools, we recommend that you modernize and use a central platform such as Looker.\n### Additional resources\n- [Migrating data warehouses to BigQuery: Reporting and analysis](/architecture/dw2bq/dw-bq-reporting-and-analysis) \n- [Architecture for connecting visualization software to Hadoop on Google Cloud](/architecture/hadoop/architecture-for-connecting-visualization-software-to-hadoop-on-google-cloud) \n- [Speeding up small queries in BigQuery with BI Engine](https://cloud.google.com/blog/topics/developers-practitioners/speeding-small-queries-bigquery-bi-engine) ## Use workflow management tools\nData analytics involves many processes and services. Data moves across different tools and processing pipelines during the data analytics lifecycle. To manage and maintain end-to-end data pipelines, use appropriate workflow management tools. [Cloud Composer](/composer) is a fully managed workflow management tool based on the open source [Apache Airflow](https://airflow.apache.org/) project.\nYou can use Cloud Composer to [launch Dataflow pipelines](/composer/docs/how-to/using/using-dataflow-template-operator) and to [use Dataproc Workflow Templates](/dataproc/docs/tutorials/workflow-composer) . Cloud Composer can also help you [create a CI/CD pipeline to test, synchronize, and deploy DAGs](/composer/docs/dag-cicd-integration-guide) or [use a CI/CD pipeline for data-processing workflows](/architecture/cicd-pipeline-for-data-processing) . For more information, watch [Cloud Composer: Development best practices](https://www.youtube.com/watch?v=RrKXZcKOz4A) .\n## Migration resources\nIf you already run a data analytics platform and if you want to migrate some or all of the workloads to Google Cloud, review the following migration resources for best practices and guidance:\n- General migration guidance- [Migration to Google Cloud: Choosing your migration path](/architecture/migration-to-gcp-choosing-your-path) .\n- [Example of modernizing Twitter's ad engagement analytics platform](https://cloud.google.com/blog/products/data-analytics/modernizing-twitters-ad-engagement-analytics-platform) .\n- Cloud Storage migration- [Migrating HDFS Data from on-premises to Google Cloud](/architecture/hadoop/hadoop-gcp-migration-data) .\n- [Transferring data from Amazon S3 to Cloud Storage using VPC Service Controls and Storage Transfer Service](/architecture/transferring-data-from-amazon-s3-to-cloud-storage-using-vpc-service-controls-and-storage-transfer-service) .\n- Pub/Sub migration- [Migration from Kafka to Pub/Sub](/architecture/migrating-from-kafka-to-pubsub) .\n- Bigtable migration- [Migrating data from HBase to Bigtable](/architecture/hadoop/hadoop-gcp-migration-data-hbase-to-bigtable) .\n- [Migrating from Aerospike to Bigtable](/architecture/migrating-aerospike-cloud-bigtable) .\n- Dataproc migration- [Migrating on-premises Hadoop infrastructure to Google Cloud](/architecture/hadoop) .\n- [Hadoop migration costs and tips](https://cloud.google.com/blog/products/data-analytics/on-prem-hadoop-migration-costs-and-tips) .\n- [Hadoop to Dataproc](https://cloud.google.com/blog/topics/developers-practitioners/migrating-apache-hadoop-dataproc-decision-tree) .\n- [On-premises Hadoop Jobs to Dataproc](/architecture/hadoop/hadoop-gcp-migration-jobs) .\n- [Apache Spark to Dataproc](/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc) .\n- [External Hive Metastore to Dataproc Metastore](/dataproc-metastore/docs/migrate-mysql-metastore) .\n- BigQuery migration- [Legacy data warehouse migration challenges](https://cloud.google.com/blog/products/data-analytics/data-warehouse-migration-challenges-and-how-to-meet-them) .\n- [Migrating data warehouses to BigQuery](/architecture/dw2bq/dw-bq-migration-overview) .\n- [BigQuery for Data Warehouse professionals](/architecture/bigquery-data-warehouse) .\n- Composer migration- [Migrate environments to Airflow 2](/composer/docs/migrate-environments-airflow-2) .\n## What's next\nLearn about system design best practices for Google Cloud AI and machine learning, including the following:\n- Learn about [Google Cloud AI and machinelearning services](/architecture/framework/system-design/ai-ml#key) that support system design.\n- Learn [ML data processing best practices](/architecture/framework/system-design/ai-ml#data-process) .\n- Learn [best practices for model development and training](/architecture/framework/system-design/ai-ml#dev-train) .\nExplore other categories in the [Architecture Framework](/architecture/framework) such as reliability, operational excellence, and security, privacy, and compliance.", "guide": "Docs"}