{"title": "Docs - Build a collaborative incident management process", "url": "https://cloud.google.com/architecture/framework/reliability/build-incident-management-process", "abstract": "# Docs - Build a collaborative incident management process\nLast reviewed 2023-08-08 UTC\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices to manage services and define processes to respond to incidents. Incidents occur in all services, so you need a well-documented process to efficiently respond to these issues and mitigate them.\n", "content": "## Incident management overview\nIt's inevitable that your well-designed system eventually fails to meet its SLOs. In the absence of an SLO, your customers loosely define what the acceptable service level is themselves from their past experience. Customers escalate to your technical support or similar group, regardless of what's in your SLA.\nTo properly serve your customers, establish and regularly test an incident management plan. The plan can be as short as a single-page checklist with ten items. This process helps your team to reduce time to detect (TTD) and time to mitigate (TTM).\nTTM is preferred as opposed to TTR, where the R for or is often used to mean a full fix versus mitigation. TTM emphasizes fast mitigation to quickly end the customer impact of an outage, and then start the often much longer process to fully fix the problem.\nA well-designed system where operations are excellent increases the time between failures (TBF). In other words, operational principles for reliability, including good incident management, aim to make failures less frequent.\nTo run reliable services, apply the following best practices in your incident management process.\n## Assign clear service ownership\nAll services and their critical dependencies must have clear owners responsible for adherence to their SLOs. If there are reorganizations or team attrition, engineering leads must ensure that ownership is explicitly handed off to a new team, along with the documentation and training as required. The owners of a service must be easily discoverable by other teams.\n## Reduce time to detect (TTD) with well tuned alerts\nBefore you can reduce TTD, review and implement the recommendations in the [build observability into your infrastructure and applications](/architecture/framework/reliability/observability-infrastructure-applications) and [define your reliability goals](/architecture/framework/reliability/define-goals) sections of the Architecture Framework reliability category. For example, disambiguate between application issues and underlying cloud issues.\nA well-tuned set of SLIs alerts your team at the right time without alert overload. For more information, see the [build efficient alerts](/architecture/framework/reliability/build-efficient-alerts) section of the Architecture Framework reliability category or [Tune up your SLI metrics: CRE life lessons](/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons) .\n## Reduce time to mitigate (TTM) with incident management plans and training\nTo reduce TTM, define a documented and well-exercised incident management plan. Have readily available data on what's changed in the environment. Make sure that teams know [generic mitigations](https://www.oreilly.com/content/generic-mitigations/) they can quickly apply to minimize TTM. These mitigation techniques include draining, rolling back changes, upsizing resources, and degrading quality of service.\nAs discussed in another Architecture Framework reliability category document, [create reliable operational processes and tools](/architecture/framework/reliability/create-operational-processes-tools) to support the safe and rapid rollback of changes.\n## Design dashboard layouts and content to minimize TTM\nOrganize your service dashboard layout and navigation so that an operator can determine in a minute or two if the service and all of its critical dependencies are running. To quickly pinpoint potential causes of problems, operators must be able to scan all charts on the dashboard to rapidly look for graphs that change significantly at the time of the alert.\nThe following list of example graphs might be on your dashboard to help troubleshoot issues. Incident responders should be able to glance at them in a single view:\n- Service level indicators, such as successful requests divided by total valid requests\n- Configuration and binary rollouts\n- Requests per second to the system\n- Error responses per second from the system\n- Requests per second from the system to its dependencies\n- Error responses per second to the system from its dependencies\nOther common graphs to help troubleshoot include latency, saturation, request size, response size, query cost, thread pool utilization, and Java virtual machine (JVM) metrics (where applicable). refers to fullness by some limit such as quota or system memory size. looks for regressions due to pool exhaustion.\nTest the placement of these graphs against a few outage scenarios to ensure that the most important graphs are near the top, and that the order of the graphs matches your standard diagnostic workflow. You can also apply machine learning and statistical anomaly detection to surface the right subset of these graphs.\n## Document diagnostic procedures and mitigation for known outage scenarios\nWrite playbooks and link to them from alert notifications. If these documents are accessible from the alert notifications, operators can quickly get the information they need to troubleshoot and mitigate problems.\n## Use blameless postmortems to learn from outages and prevent recurrences\nEstablish a [blameless postmortem culture and an incident review process](https://sre.google/sre-book/postmortem-culture/) . means that your team evaluates and documents what went wrong in an objective manner, without the need to assign blame.\nMistakes are opportunities to learn, not a cause for criticism. Always aim to make the system more resilient so that it can recover quickly from human error, or even better, detect and prevent human error. Extract as much learning as possible from each postmortem and follow up diligently on each postmortem action item in order to make outages less frequent, thereby increasing TBF.\n## Incident management plan example\nProduction issues have been detected, such as through an alert or page, or escalated to me:\n- Should I delegate to someone else?- Yes, if you and your team can't resolve the issue.\n- Is this issue a privacy or security breach?- If yes, delegate to the privacy or security team.\n- Is this issue an emergency or are SLOs at risk?- If in doubt, treat it as an emergency.\n- Should I involve more people?- Yes, if it impacts more than X% of customers or if it takes more than Y minutes to resolve. If in doubt, always involve more people, especially within business hours.\n- Define a primary communications channel, such as IRC, Hangouts Chat, or Slack.\n- Delegate previously defined roles, such as the following:- who is responsible for overall coordination.\n- who is responsible for internal and external communications.\n- who is responsible to mitigate the issue.\n- Define when the incident is over. This decision might require an acknowledgment from a support representative or other similar teams.\n- Collaborate on the blameless postmortem.\n- Attend a postmortem incident review meeting to discuss and staff action items.## Recommendations\nTo apply the guidance in the Architecture Framework to your own environment, follow these recommendations::\n- Establish an incident management plan, and train your teams to use it.\n- To reduce TTD, implement the recommendations to [build observability into your infrastructure and applications](/architecture/framework/reliability/observability-infrastructure-applications) .\n- Build a \"What's changed?\" dashboard that you can glance at when there's an incident.\n- Document query snippets or build a [Looker Studio](https://lookerstudio.google.com/overview) dashboard with frequent log queries.\n- Evaluate [Firebase Remote Config](https://firebase.google.com/products/remote-config) to mitigate rollout issues for mobile applications.\n- [Test failure recovery](/architecture/framework/reliability/create-operational-processes-tools#test_failure_recovery) , including restoring data from backups, to decrease TTM for a subset of your incidents.\n- Design for and test configuration and binary rollbacks.\n- [Replicate data across regions for disaster recovery](/architecture/framework/reliability/design-scale-high-availability#replicate_data_across_regions_for_disaster_recovery) and use [disaster recovery tests](/architecture/framework/reliability/create-operational-processes-tools#conduct_disaster_recovery_tests) to decrease TTM after regional outages.\n- [Design a multi-region architecture for resilience to regional outages](/architecture/framework/reliability/design-scale-high-availability#design_a_multi-region_architecture_for_resilience_to_regional_outages) if the business need for high availability justifies the cost, to increase TBF.## What's next\nLearn more about how to build a collaborative incident management process with the following resources:\n- [Google Cloud status dashboard](https://status.cloud.google.com/) \n- [Incident management at Google](/blog/products/gcp/incident-management-at-google-adventures-in-sre-land) \n- [Data incident response process](/security/incident-response) \n- [SRE book chapter on managing incidents](https://sre.google/sre-book/managing-incidents/) \nExplore other categories in the [Architecture Framework](/architecture/framework) such as system design, operational excellence, and security, privacy, and compliance.", "guide": "Docs"}