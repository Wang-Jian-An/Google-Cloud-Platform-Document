{"title": "Docs - Optimizing resource usage in a multi-tenant GKE cluster using node auto-provisioning", "url": "https://cloud.google.com/architecture/optimizing-resources-in-multi-tenant-gke-clusters-with-auto-provisioning", "abstract": "# Docs - Optimizing resource usage in a multi-tenant GKE cluster using node auto-provisioning\nThis tutorial shows how to use node auto-provisioning to scale a multi-tenant [Google Kubernetes Engine (GKE)](/kubernetes-engine) cluster, and how to use [Workload Identity](/kubernetes-engine/docs/how-to/workload-identity) to control tenant access to resources like Cloud Storage buckets. This guide is for developers and architects; it assumes basic knowledge of Kubernetes and GKE. If you need an introduction, see [GKE overview](/kubernetes-engine/docs/concepts/kubernetes-engine-overview) .\n [Cluster multi-tenancy](/kubernetes-engine/docs/concepts/multitenancy-overview#what_is_multi-tenancy) is often implemented to reduce costs or to standardize operations across tenants. To fully realize cost savings, you should size your cluster so that cluster resources are used efficiently. You should also minimize resource waste when your cluster is autoscaled by making sure that cluster nodes that are added are of an appropriate size.\nIn this tutorial, you use node auto-provisioning to scale the cluster. Node auto-provisioning can help optimize your cluster resource usage, and therefore control your costs, by adding cluster nodes that best fit your pending workloads.", "content": "## Objectives\n- Create a GKE cluster that has node auto-provisioning and Workload Identity enabled.\n- Set up the cluster for multi-tenancy.\n- Submit jobs to the cluster to demonstrate how node auto-provisioning creates and destroys nodes of optimized sizes.\n- Use taints and labels to instruct node auto-provisioning to create dedicated node pools for each tenant.\n- Use Workload Identity to control access to tenant-specific resources like Cloud Storage buckets.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Google Kubernetes Engine](/kubernetes-engine/pricing) \n- [Cloud Storage](/storage/pricing) \n- [Cloud Build](/build/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . ## Before you begin- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) At the bottom of the Google Cloud console, a [Cloud Shell](/shell/docs/how-cloud-shell-works) session starts and displays a command-line prompt. Cloud Shell is a shell environment  with the Google Cloud CLI  already installed and with values already set for  your current project. It can take a few seconds for the session to initialize.\n- In Cloud Shell, enable the APIs for GKE and Cloud Build API:```\ngcloud services enable container.googleapis.com \\\n cloudbuild.googleapis.com\n```This operation can take a few minutes to complete.\n## Preparing your environmentIn this section, you get the code you need for this tutorial and you set up your environment with values that you use throughout the tutorial.- In Cloud Shell, define the environment variables that you use for this tutorial:```\nexport PROJECT_ID=$(gcloud config get-value project)\n```\n- Clone the GitHub repository that contains the code for this tutorial:```\ngit clone https://github.com/GoogleCloudPlatform/solutions-gke-autoprovisioning\n```\n- Change to the repository directory:```\ncd solutions-gke-autoprovisioning\n```\n- Update the Kubernetes YAML job configuration file with your Google project ID:```\nsed -i \"s/MY_PROJECT/$PROJECT_ID/\" manifests/bases/job/base-job.yaml\n```\n- Submit a Cloud Build job to build a container image:```\ngcloud builds submit pi/ --tag gcr.io/$PROJECT_ID/generate-pi\n```The image is a Go program that generates an approximation of pi. You use this container image later.Cloud Build exports the image to your project's Container Registry.\n## Creating a GKE clusterIn this section, you create a GKE cluster that has node auto-provisioning and workload identity enabled. Note the following details of the cluster creation process:- You specify CPU and memory limits for the cluster. Node auto-provisioning respects these limits when it adds or removes nodes from the cluster. For more information, see [Enabling node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning#enable) in the GKE documentation.\n- You specify the default service account and scopes that are used by the nodes within the auto-provisioned node pools. Using these settings, you can control the provisioned node's access permissions. For more information, see [Setting identity defaults for auto-provisioned nodes](/kubernetes-engine/docs/how-to/node-auto-provisioning#identity) in the GKE documentation.\n- You set an [autoscaling profile](/kubernetes-engine/docs/concepts/cluster-autoscaler#autoscaling_profiles) that prioritizes utilization. This profile tells the cluster autoscaler to quickly scale down the cluster to minimize unused resources. This can help with resource efficiency for batch or job-centric workloads. The setting applies to all node pools in the cluster.\n- You enable Workload Identity by specifying the workload pool.\nTo create the cluster:- Create a service account:```\ngcloud iam service-accounts create nap-sa\n```This service account is used by the auto-provisioned nodes.\n- Grant the new service account permissions to pull images from the Cloud Storage bucket that's used by Container Registry:```\ngsutil iam ch \\\u00a0 \u00a0 serviceAccount:nap-sa@$PROJECT_ID.iam.gserviceaccount.com:objectViewer \\\u00a0 \u00a0 gs://artifacts.$PROJECT_ID.appspot.com\n```\n- Create a GKE cluster that has node auto-provisioning and workload identity enabled:```\ngcloud container clusters create multitenant \\\u00a0 \u00a0 --release-channel=regular \\\u00a0 \u00a0 --zone=us-central1-c \\\u00a0 \u00a0 --num-nodes=2 \\\u00a0 \u00a0 --machine-type=n1-standard-2 \\\u00a0 \u00a0 --workload-pool=${PROJECT_ID}.svc.id.goog \\\u00a0 \u00a0 --autoscaling-profile=optimize-utilization \\\u00a0 \u00a0 --enable-autoprovisioning \\\u00a0 \u00a0 --autoprovisioning-service-account=nap-sa@${PROJECT_ID}.iam.gserviceaccount.com \\\u00a0 \u00a0 --autoprovisioning-scopes=\\https://www.googleapis.com/auth/devstorage.read_write,\\https://www.googleapis.com/auth/cloud-platform \\\u00a0 \u00a0 --min-cpu 1 \\\u00a0 \u00a0 --min-memory 1 \\\u00a0 \u00a0 --max-cpu 50 \\\u00a0 \u00a0 --max-memory 256 \\\u00a0 \u00a0 --enable-network-policy \\\u00a0 \u00a0 --enable-ip-alias\n```\n- Set the default cluster name and compute zone:```\ngcloud config set container/cluster multitenantgcloud config set compute/zone us-central1-c\n```\n## Setting up the cluster for multi-tenancyWhen you operate a multi-tenant software-as-a-service (SaaS) app, you typically should separate your tenants. Separating tenants can help minimize any damage from a compromised tenant. It can also help you allocate cluster resources evenly across tenants, and track how many resources each tenant is consuming. Kubernetes cannot guarantee perfectly secure isolation between tenants, but it does offer features that might be sufficient for specific use cases. For more information about GKE multi-tenancy features, see the [overview](/kubernetes-engine/docs/concepts/multitenancy-overview) and [best practices](/kubernetes-engine/docs/best-practices/enterprise-multitenancy) guides in the GKE documentation.\nIn the example app, you create two tenants, `tenant1` and `tenant2` . You separate each tenant and its Kubernetes resources into its own [namespace](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) . You create a simple [network policy](https://kubernetes.io/docs/concepts/services-networking/network-policies/) that enforces tenant isolation by preventing communication from other namespaces. Later, you use [node taints](/kubernetes-engine/docs/how-to/node-taints) and [nodeSelector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector) fields to prevent Pods from different tenants from being scheduled on the same node. You can provide an additional degree of separation by running tenant workloads on dedicated nodes.\nYou use [Kustomize](https://kustomize.io/) to manage the Kubernetes manifests that you submit to the cluster. Kustomize lets you combine and customize YAML files for multiple purposes.- Create a namespace, a service account, and a network policy resource for `tenant1` :```\nkubectl apply -k manifests/setup/tenant1\n```The output looks like the following:```\nnamespace/tenant1-ns created\nserviceaccount/tenant1-ksa created\nnetworkpolicy.networking.k8s.io/tenant1-deny-from-other-namespaces created\n```\n- Create the cluster resources for `tenant2` :```\nkubectl apply -k manifests/setup/tenant2\n```\n## Verifying the behavior of node auto-provisioningA GKE cluster consists of one of more [node pools](/kubernetes-engine/docs/concepts/node-pools) . All nodes within a node pool have the same machine type, which means that they have the same amount of CPU and memory. If your workload resource demands are variable, you might benefit from having multiple node pools that have different machine types within your cluster. In this way, the cluster autoscaler can add nodes of the most suitable type, which can improve your resource efficiency and therefore lower costs. However, maintaining many node pools adds management overhead. It also might not be practical in a multi-tenant cluster if you want to execute tenant workloads in dedicated node pools.\nInstead, you can use [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) to extend the cluster autoscaler. When node auto-provisioning is enabled, the cluster autoscaler can create new node pools automatically based on the [specifications](https://kubernetes.io/docs/concepts/workloads/pods#pod-templates) of pending Pods. As a result, the cluster autoscaler can create nodes of the most suitable type, but you don't have to create or manage the node pools yourself. Using node auto-provisioning, your cluster can efficiently autoscale without over-provisioning, which can help lower your costs.\nFurthermore, if pending Pods have [workload separation constraints](/kubernetes-engine/docs/how-to/node-auto-provisioning#workload_separation) , node auto-provisioning can create nodes that satisfy the constraints. In this way, you can use node auto-provisioning to automatically create node pools that will be used by only a single tenant.\nIn this section, you submit various jobs to the cluster to verify the behavior of node auto-provisioning. The jobs use the `generate-pi` image that you created earlier.\n### Submit a simple jobFirst, you submit a simple job to the cluster. The job does not specify any tenant-specific constraints. There is enough spare capacity in the cluster to handle the job's CPU and memory requests. Therefore, you expect the job to be scheduled into one of the existing nodes in the default node pool. No additional nodes are provisioned.- List the node pools in the cluster:```\ngcloud container node-pools list\n```You see a single default pool.\n- Print the job's configuration to the console:```\nkubectl kustomize manifests/jobs/simple-job/\n```The output looks like the following:```\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: pi-job\nspec:\n...\n```The configuration does not specify any node taints or selectors.\n- Submit the job:```\nkubectl apply -k manifests/jobs/simple-job/\n```\n- Watch the node pools in the cluster:```\nwatch -n 5 gcloud container node-pools list\n```You still see a single default pool. No new node pools are created.\n- After about 30 seconds, press `Control+C` to stop watching the node pools.\n- Watch the nodes in the cluster:```\nkubectl get nodes -w\n```You do not see any new nodes being created.\n- After watching for 1 minute, press `Control+C` to stop watching.\n- List the jobs in the cluster:```\nkubectl get jobs --all-namespaces\n```The output looks like the following:```\nNAMESPACE NAME  COMPLETIONS DURATION AGE\ndefault  pi-job 1/1   14s  21m\n```The `1/1` value in the `Completions` column indicates that 1 job out of a total of 1 jobs has completed.\n### Submit a job that has tenant-specific constraintsIn this section, you submit another job to confirm that node auto-provisioning obeys workload separation constraints. The job configuration includes a tenant-specific [node selector](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector) and a tenant-specific [toleration](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) . The job can be scheduled only onto a node that has labels that match the selector's key-value pairs. A toleration works in conjunction with [node taints](/kubernetes-engine/docs/how-to/node-taints) , which also limit which jobs can be scheduled onto a node. A best practice with node auto-provisioning is to include both a node selector and a toleration for workload separation.\nThis job cannot be scheduled into the default node pool, because that pool does not have any nodes that satisfy the selector constraint. Therefore, node auto-provisioning creates a new node pool with node labels that satisfy the selector requirement. Node auto-provisioning also adds a tenant-specific taint to the nodes that matches the toleration in the job configuration. Only Pods that have a matching toleration can be scheduled onto the nodes in the pool, which lets you further separate tenant workloads.- List the node pools in the cluster:```\ngcloud container node-pools list\n```You see a single default pool.\n- Print the job's configuration to the console:```\nkubectl kustomize manifests/jobs/one-tenant/\n```The configuration includes a tenant-specific node selector requirement and a toleration. The output looks like the following:```\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: tenant1-pi-job\nspec:\n...\n```\n- Submit the job:```\nkubectl apply -k manifests/jobs/one-tenant/\n```\n- Watch the node pools in the cluster:```\nwatch -n 5 gcloud container node-pools list\n```After some time, you see a new node pool. The output looks like the following:```\nNAME       MACHINE_TYPE  DISK_SIZE_GB\ndefault-pool     n1-standard-2  100\nnap-n1-standard-1-15jwludl  n1-standard-1  100\n```The node pool name is prefixed with `nap-` , which indicates that it was created by node auto-provisioning. The node pool name also includes the machine type of the nodes in the pool, for example, `n1-standard-1` .\n- Watch the nodes in the cluster:```\nkubectl get nodes -w\n```After about a minute, you see a new node appear in the list. The node name includes the name of the `nap-` node pool. The new node initially has a `Not Ready` status. After some time, the status of the new node changes to `Ready` , which means the node can now accept pending work.\n- To stop watching the nodes, press `Control+C` .\n- List the node taints:```\nkubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints\n```You see that the new node has a `NoSchedule` taint for the key-value pair `tenant:\u00a0tenant1` . Therefore, only Pods that have a corresponding toleration for `tenant:\u00a0tenant1` can be scheduled onto the node.\n- Watch the jobs in the cluster:```\nkubectl get jobs -w --all-namespaces\n```After some time, you see that `tenant1-pi-job` has `1/1` completion, which indicates that it finished successfully.\n- To stop watching the jobs, press `Control+C` .\n- Watch the node pools in the cluster:```\nwatch -n 5 gcloud container node-pools list\n```After some time, you see that the `nap-` pool is deleted, and the cluster once again has only the single default node pool. Node auto-provisioning has deleted the `nap-` node pool, because there is no more pending work that matches the pool's constraints.\n- To stop watching the node pools, press `Control+C` .\n### Submit two larger jobs that have tenant constraintsIn this section, you submit two jobs that have tenant-specific constraints, and you also increase the resource requests for each job. Once again, these jobs cannot be scheduled into the default node pool due to the node selector constraints. Because each job has its own selector constraint, node auto-provisioning creates two new node pools. In this way, you can use node auto-provisioning to keep the tenant jobs separated. Because the jobs have a higher number of resource requests compared to the previous job, node auto-provisioning creates node pools that have larger machine types than last time.- List the node pools in the cluster:```\ngcloud container node-pools list\n```You see a single default pool.\n- Print the combined configuration:```\nkubectl kustomize manifests/jobs/two-tenants/\n```The configuration includes two separate jobs, each with a tenant-specific node selector and toleration, and with increased resource requests.The output looks like the following:```\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: tenant1-larger-pi-job\nspec:\n...\n```\n- Submit the jobs:```\nkubectl apply -k manifests/jobs/two-tenants/\n```\n- Watch the node pools in the cluster:```\nwatch -n 5 gcloud container node-pools list\n```After some time, you see two additional node pools. The output looks like the following:```\nNAME       MACHINE_TYPE  DISK_SIZE_GB\ndefault-pool     n1-standard-2  100\nnap-n1-standard-2-6jxjqobt  n1-standard-2  100\nnap-n1-standard-2-z3s06luj  n1-standard-2  100\n```The node pool names are prefixed with `nap-` , which indicates that they were created by node auto-provisioning. The node pool names also include the machine type of the nodes in the pool, for example, `n1-standard-2` .\n- To stop watching the nodes, press `Control+C` .\n- Watch the nodes in the cluster:```\nkubectl get nodes -w\n```After about a minute, you see two new nodes appear in the list. The node names include the name of their associated `nap-` node pool. The new nodes initially have a `Not Ready` status. After some time, the status of the new nodes changes to `Ready` , which means that the nodes can now accept pending work.\n- To stop watching the nodes, press `Control+C` .\n- List the node taints:```\nkubectl get nodes -o custom-columns=NAME:.metadata.name,TAINTS:.spec.taints\n```You see that the new nodes have `NoSchedule` taints, one with the key-value pair `tenant:\u00a0tenant1` , and the other with `tenant:\u00a0tenant2` . Only Pods that have corresponding tenant tolerations can be scheduled onto the nodes.\n- Watch the jobs in the cluster:```\nkubectl get jobs -w --all-namespaces\n```After some time, you see that `tenant1-larger-pi-job` and `tenant2-larger-pi-job` change to have `1/1` completion each, which indicates that the jobs finished successfully.\n- To stop watching the jobs, press `Control+C` .\n- Watch the node pools in the cluster:```\nwatch -n 5 gcloud container node-pools list\n```After some time, you see that both `nap-` pools are deleted, and the cluster once again has only a single default node pool. Node auto-provisioning has deleted the `nap-` node pools, because there is no more pending work that matches the pools constraints.\n- To stop watching the node pools, press `Control+C` .\n## Controlling access to Google Cloud resourcesIn addition to maintaining separation of tenants within the cluster, you typically want to control tenant access to Google Cloud resources such as Cloud Storage buckets or Pub/Sub topics. For example, each tenant might require a Cloud Storage bucket that shouldn't be accessible by other tenants.\nUsing [Workload Identity](/kubernetes-engine/docs/how-to/workload-identity) , you can create a mapping between Kubernetes service accounts and Google Cloud [service accounts](/iam/docs/service-accounts) . You can then assign appropriate Identity and Access Management (IAM) roles to the Google Cloud service account. In this way, you can enforce the principle of least privilege so that tenant jobs can access their assigned resources, but they're prevented from accessing the resources that are owned by other tenants.\n### Set up GKE workload identityConfigure the mapping between your Kubernetes service account and a Google Cloud service account that you create.- Create a Google Cloud service account for `tenant1` :```\ngcloud iam service-accounts create tenant1-gsa\n```\n- Grant the Kubernetes service account for `tenant1` IAM permissions to use the corresponding Google Cloud service account for `tenant1` :```\ngcloud iam service-accounts add-iam-policy-binding \\\u00a0 \u00a0 tenant1-gsa@${PROJECT_ID}.iam.gserviceaccount.com \\\u00a0 \u00a0 --role roles/iam.workloadIdentityUser \\\u00a0 \u00a0 --member \"serviceAccount:${PROJECT_ID}.svc.id.goog[tenant1-ns/tenant1-ksa]\"\n```\n- Complete the mapping between the service accounts by annotating the Kubernetes service account with the Google Cloud service account:```\nkubectl annotate serviceaccount tenant1-ksa -n tenant1-ns \\\u00a0 \u00a0 iam.gke.io/gcp-service-account=tenant1-gsa@${PROJECT_ID}.iam.gserviceaccount.com\n```\n### Submit a job that writes to a Cloud Storage bucketIn this section, you confirm that a job that's executing as a particular Kubernetes service account can use the IAM permissions of its mapped Google Cloud service account.- Create a new Cloud Storage bucket for `tenant1` :```\nexport BUCKET=tenant1-$PROJECT_IDgsutil mb -b on -l us-central1 gs://$BUCKET\n```You use your project ID as a suffix on the bucket name to make the name unique.\n- Update the job's configuration file to use the Cloud Storage bucket:```\nsed -i \"s/MY_BUCKET/$BUCKET/\" \\\u00a0 \u00a0 manifests/jobs/write-gcs/bucket-write.yaml\n```\n- Grant the `tenant1` service account permissions to read and write objects in the bucket:```\ngsutil iam ch \\\u00a0 \u00a0 serviceAccount:tenant1-gsa@$PROJECT_ID.iam.gserviceaccount.com:objectAdmin \\\u00a0 \u00a0 gs://$BUCKET\n```\n- Print the job configuration:```\nkubectl kustomize manifests/jobs/write-gcs/\n```The output looks like the following:```\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: tenant1-pi-job-gcs\nspec:\n...\n```The new bucket name is passed as an argument to the `generate-pi` container, and the job specifies the appropriate `tenant1-ksa` Kubernetes service account.\n- Submit the job:```\nkubectl apply -k manifests/jobs/write-gcs/\n```As in the previous section, node auto-provisioning creates a new node pool and a new node to execute the job.\n- Watch the job's Pod:```\nkubectl get pods -n tenant1-ns -w\n```In this case, you watch the Pod rather than watching the node pool. You see the Pod transition through different statuses. After a couple of minutes, the status changes to `Completed` . This status indicates that the job has successfully finished.\n- To stop watching, press `Control+C` .\n- Confirm that a file has been written to the Cloud Storage bucket:```\ngsutil ls -l gs://$BUCKET\n```You see a single file.\n- To clean up, delete the job:```\nkubectl delete job tenant1-pi-job-gcs -n tenant1-ns\n```You will resubmit this job in the next section.\n### Revoke IAM permissionsFinally, you confirm that revoking IAM permissions from the Google Cloud service account prevents the mapped Kubernetes service account from accessing the Cloud Storage bucket.- Revoke the Google Cloud service account's permissions to write to the Cloud Storage bucket:```\ngsutil iam ch -d \\\u00a0 \u00a0 serviceAccount:tenant1-gsa@$PROJECT_ID.iam.gserviceaccount.com:objectAdmin \\\u00a0 \u00a0 gs://$BUCKET\n```\n- Submit the same job as previously:```\nkubectl apply -k manifests/jobs/write-gcs/\n```\n- Once again watch the job's Pod status:```\nkubectl get pods -n tenant1-ns -w\n```After a couple of minutes, the status changes to `Error` , which indicates that the job failed. This error is expected, because the job is executing as a Kubernetes service account that maps to a Google Cloud service account that in turn no longer has write permissions to the Cloud Storage bucket.\n- To stop watching the Pod, press `Control+C` .\n- List the files in the bucket:```\ngsutil ls -l gs://$BUCKET\n```You see a single file in the bucket; a new file hasn't been written.\n## Clean upThe easiest way to eliminate billing is to delete the Google Cloud project you created for the tutorial.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Delete the GKE clusterIf you don't want to delete the project, delete the GKE cluster:\n```\ngcloud container clusters delete multitenant\n```## What's next\n- Learn more about GKE [multi-tenancy](/kubernetes-engine/docs/concepts/multitenancy-overview) .\n- Explore the [cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Docs"}