{"title": "Docs - Describe and organize data products and resources in a data mesh", "url": "https://cloud.google.com/architecture/describe-organize-data-products-resources-data-mesh", "abstract": "# Docs - Describe and organize data products and resources in a data mesh\nLast reviewed 2022-12-07 UTC\nIn a data mesh, data consumers discover and trust data products based on how those data products are organized and described. The more consistent the organization and descriptions are, the easier it is for data consumers to discover the right data product for their needs. This consistency is also important for ensuring the interoperability of data products, and that they are used appropriately. The approaches that we recommend an organization take to govern their data products and the underlying data resources are described in this document.\nThis document is part of a series which describes how to implement a data mesh on Google Cloud. It assumes that you have read and are familiar with the concepts described in [Architecture and functions in a data mesh](/architecture/data-mesh) and [Build a modern, distributed Data Mesh with Google Cloud](https://services.google.com/fh/files/misc/build-a-modern-distributed-datamesh-with-google-cloud-whitepaper.pdf) .\nThe series has the following parts:\n- [Architecture and functions in a data mesh](/architecture/data-mesh) \n- [Design a self-service data platform for a data mesh](/architecture/design-self-service-data-platform-data-mesh) \n- Describe and organize data products and resources in a data mesh (this document)\n- [Build data products in a data mesh](/architecture/build-data-products-data-mesh) \n- [Discover and consume data products in a data mesh](/architecture/discover-consume-data-products-data-mesh) \nIn large, distributed organizations, data is typically organized according to the functions of individual teams. Financial data, for example, is organized in ways that make sense to individuals working in finance. Logistics data is organized in ways that make sense to people working in areas such as supply chains. The problem is that this approach results in data which is inconsistent across the entire organization.\nTo help overcome these challenges, the data governance team of a data mesh-driven organization must do the following:\n- Define a common vocabulary and approach for creating tags on all the data products of an organization.\n- Create a common library of tags to enable the search, discovery, and understanding of data products from diverse businesses.\nas discussed in this document refers to a collection of attributes that describe properties of the data. Metadata is represented through tag templates and tags in Google Cloud Data Catalog. A tag template is a collection of related fields that represent your vocabulary for classifying data entities. A tag is created from the tag template when it is associated with a data entity, and its fields are filled with values that describe attributes of that entity. Tag templates and tags are the building blocks that organizations can use to capture consistent metadata across the business. To learn more about tag templates and tags, see [Understanding the fundamentals of tagging in Data Catalog](/blog/products/data-analytics/cloud-metadata-management-tagging-tips) .\nThis common vocabulary and library serves as a foundational layer for a data mesh, helping to scale understanding of data across an organization. It also enables the central teams to automate privacy requirements and apply policy enforcement consistently across the organization.\n", "content": "## How technical metadata is created\nThe data products from diverse data domains are typically deployed into multiple Google Cloud projects. Their resources (for example, dataset, tables, or views) are represented in Data Catalog as [entries](/data-catalog/docs/entries-and-entry-groups) . Entries contain the technical metadata of a resource, such as its unique name, its storage location, or creation time.\nData Catalog automatically creates the entries and the technical metadata for BigQuery datasets, tables, views, and Pub/Sub topics. However, automatic entry creation is not available for resource types such as Cloud Storage files, Looker Block objects, and machine learning (ML) models. To create entries for those resource types, you must use the Data Catalog API or console.\nIn addition to technical metadata, entries can be annotated with tags by using tag templates.\n## Tag templates\nWe recommend that you use the following types of tag templates to annotate data products with their data governance attributes:\n- The [data product template](#the_data_product_template) \n- The [data resource template](#the_data_resource_template) \n- The [data attribute templates](#the_data_attribute_templates_data_sensitivity_and_data_standardization) :- The data sensitivity template\n- The data standardization templateYou can find the YAML file for each of these templates in the [GitHub repository](https://github.com/GoogleCloudPlatform/datacatalog-templates) that is associated with this document. This repository also contains a [Python script](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/create_template.py) for creating the templates in Data Catalog. Before using these templates, we recommend that the data governance team in your organization customizes the templates to include the specific terms that your organization uses to describe metadata. This approach enables data producers to create the tags on their own, without assistance from a data governance specialist.\nThe templates are organized by the resource hierarchy of a , a , and a . These concepts are defined in [Architecture and functions in a data mesh](/architecture/architecture-functions-data-mesh) . The templates are described in more detail in the following sections.\n### The data product template\nThe data product template, created by the [Python script](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/create_template.py) from the [data_product.yaml](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/data_product.yaml) file, operates at the dataset or folder level. This template is intended to record the intent of a data product. It captures the data domains of the product, the name and description of the product, and its ownership information. The template also captures the business criticality, the expected data latency, and the data retention period of the data product. The data producer should provide most of the field values for each tag in this template, except for the `number_of_data_resources` and `storage_location` fields. The information for these two fields can be inferred from the contents of the data product.\nThe following image shows what the data product tag template defined by the `data_product.yaml` file looks like in the Data Catalog console:\nIdeally, we recommend that you only tag a data product once from the data product template. The following image is an example of such a tag created from the template:\nIf the data product is stored in BigQuery, you must attach the tag to the BigQuery dataset that contains the resources. If the data product is stored in Cloud Storage, you must attach the tag to the top-level folder (or bucket) that holds the data resources for the product. In certain cases, some storage systems, such as Pub/Sub, don't provide a construct for grouping related data resources. When you're using the data product template and there's no logical container available, you must tag each individual data resource, for example, every Pub/Sub topic. It's better to have redundant data product tags on a subset of resource types than to be missing those tags.\n### The data resource template\nThe data resource template, created by the [Python script](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/create_template.py) from the [data_resource.yaml](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/data_resource.yaml) file, contains aggregate statistics about the data in the resource. You use this template to tag the data resources in the data product (for example, the tables, views, or files). Analysts can then use this information to determine whether the data within the resource matches their use case.\nThe data resource template contains information about the data sensitivity of the resource, the number of fields, the number of records, and the volume of the data. It also contains a listing of the global identifiers that are present in the resource. The identifiers are useful for understanding whether one data resource can be joined with another, potentially from a different domain.\nThe following image shows what the data resource tag template that's defined by the `data_resource.yaml` file looks like in the Data Catalog console:\nIn the preceding image, the fields for the data resource tag are a mix of user-defined and auto-generated values. The `data_sensitivity` field is inferred from the data attribute tags, which are discussed in the data attribute section. The aggregate statistics fields ( `num_fields` , `num_records` , and `size` ) are computed by using basic data profiling techniques. When the data resource is the source-of-truth for the data, the global identifier field values are user-defined. When the data resource is derived, the global identifier field values can be auto-generated from data flows. The goal is to auto-generate as many values as possible without compromising on the accuracy of the metadata.\nThe following image is an example of a tag that is generated by this template:\nBefore adopting this template, you must customize the specific global identifiers in the template to match the identifiers that your organization uses. The fields in this template are not intended to be relevant to every resource type. For example, the `num_records` field is not applicable to a Pub/Sub topic or ML model resource type. In those cases, you would omit the non-applicable field from the tag.\n### The data attribute templates: Data sensitivity and data standardization\nThe data attribute templates consist of the data sensitivity template and the data standardization template.\nThe data sensitivity template, created by the [Python script](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/create_template.py) from the [data_sensitivity.yaml](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/data_sensitivity.yaml) file, classifies the sensitivity of individual elements in a data resource. The `sensitive_type` field is a refinement of the `data_sensitivity` field from the data resource template and applies to a single data attribute. The following image shows what the data sensitivity tag template defined by the `data_sensitivity.yaml` file looks like in Data Catalog:\nThe data standardization template, created by the [Python script](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/create_template.py) from the [data_standardization.yaml](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/data_standardization.yaml) file, indicates the degree to which the data in a field is standardized in relation to a standard set of terms. This data quality metric states by how much the data element conforms to a standard vocabulary. The following image shows what the data standardization tag template defined by the `data_standardization.yaml` file looks like in the Data Catalog console:\nYou attach data attribute tags to the columns of a data resource. However, you don't need to tag every column. The data sensitivity tags should be created on all sensitive columns and the data standardization tags should be created only on the columns that contain standard terms (for example, a country or an industry).\nThe following image is an example of the tags that the data sensitivity template applies:\nThe following image is an example of the tags that the data standardization template applies:\nYou can auto-generate both types of data attribute tags from these templates. You can use Sensitive Data Protection to automate the sensitivity tag. With this approach, the output from a [Sensitive Data Protection inspection job](/dlp/docs/inspecting-storage) is mapped to a classification for data sensitivity types (for example, personal identifiable information). Similarly, you can automate the standardization tags by joining the data element with a reference column that contains only the terms in use.\n## Policies\nIn addition to improving data discovery, the data governance tags that are discussed in this document are also machine readable. Machine-readable tags can help you to automate privacy requirements, and achieve other privacy policy goals. The following are examples of how you can use machine-readable tags to help protect data:\n- Deletion policies can use the data retention tag field and technical metadata to decide when to archive or purge a data resource.\n- Column-level security policies can use the sensitivity tags to control the access to the sensitive data in those fields.## Registration workflow\nWhen a data producer wants to launch a data product, they follow a process to register their product and all of its resources in the central catalog. This process is the registration workflow. To reduce the manual work involved in this registration workflow, we recommend that the workflow include a data enrichment pipeline that auto-generates the data attribute and data resource tags. You can find [an example](https://github.com/GoogleCloudPlatform/datacatalog-tag-engine/tree/main/examples/product_registration_pipeline) of this enrichment pipeline in the GitHub repository for [datacatalog-tag-engine](https://github.com/GoogleCloudPlatform/datacatalog-tag-engine) .\nThe following diagram illustrates the registration workflow from the initial creation of the tags through tag reviews to the actual sharing of the tags with data consumers:\nAs the preceding diagram shows, the steps in the process are as follows:\n- The data producer checks the catalog entries and creates any entries that are missing. They then do the following:- If the data resources for this data product are stored either in BigQuery or Pub/Sub, the data producer verifies that a catalog entry exists for each data resource belonging to the product. Real-time sync in Data Catalog processes automatically create those entries.\n- Depending on where the data resources are created and managed, the following occurs:- If the data resources for this product are stored in Cloud Storage, the data producer checks which, if any, catalog entries have been created.\n- If the Cloud Storage files are managed by Dataplex, the Dataplex discovery process creates an entry for each folder in Cloud Storage. Dataplex assumes that all the files in a folder share the same schema and are part of the same logical file. If the files represent distinct assets, however, then you must create separate catalog entries for each file.\n- If the Cloud Storage files are not managed by Dataplex, then the data producer must create catalog entries for both the folder and the files.\n- The data producer creates the data product tag and attaches it to the dataset or top-level folder of the data product. The data producer defines the values for the fields in the tag, and the producer sets the `data_product_status` field to `DRAFT` .\n- When the data producer is ready to share the data product tag, they update the `data_product_status` field to `PENDING` . This status change triggers the data enrichment pipeline.\n- The data enrichment pipeline auto-generates the data resource and the data attribute tags.\n- Upon completion, the enrichment pipeline updates the `data_product_status` field to `REVIEW` .\n- The data producer reviews the data resource and the data attribute tags for accuracy.\n- If a global identifier is missing from the data resource tag, the data producer updates the tag. If an identifier does not belong in the tag, they remove it.\n- Once they have completed their review, the data producer updates the `data_product_status` field to `DEPLOY` .\n- The `DEPLOY` status triggers an access control list (ACL) pipeline that shares access to all the tags for the data product with the data consumers group. The pipeline grants metadata viewer permissions to the BigQuery dataset or Cloud Storage folder that contains the resources for the data product. If the data resources are in BigQuery, the pipeline uses the predefined role of `roles/bigquery.metadataViewer` . If the resources are in Cloud Storage, the pipeline uses a custom role as no predefined role exists for this purpose.\n- After the appropriate ACLs have been updated, the pipeline updates the `data_product_status` field to `LAUNCHED` .## Tooling\nTo help scale the tagging activities, we recommend that your organization establishes the following:\n- A data enrichment pipeline similar to the one described earlier in this document. You can find [an example](https://github.com/GoogleCloudPlatform/datacatalog-tag-engine/tree/main/examples/product_registration_pipeline) of this pipeline in the [datacatalog-tag-engine](https://github.com/GoogleCloudPlatform/datacatalog-tag-engine) GitHub repository.\n- A central tagging service that enables independent teams across the organization to create data product, data resource and data attribute tags only on their authorized datasets. A team in a marketing department, for example, should not be allowed to create tags on the data resources owned by a team in a finance department.\n- A tag template lifecycle management tool that enables the data governance team to edit a published tag template. This tool must allow the team to add new fields and enum values when required so that they don't need to repeatedly recreate the tag template and all of its tags. You can find [an example](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/evolve_template.py) of this tool in the [GitHub repository](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/README.md) for [datacatalog-templates](https://github.com/GoogleCloudPlatform/datacatalog-templates) .## What's next\n- Check out the [suggested tag templates](https://github.com/GoogleCloudPlatform/datacatalog-templates) and other sample tag templates.\n- Learn how to [create bulk tags in Data Catalog](/architecture/tag-engine-and-data-catalog) using Tag Engine, an open source extension to Data Catalog.\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Docs"}