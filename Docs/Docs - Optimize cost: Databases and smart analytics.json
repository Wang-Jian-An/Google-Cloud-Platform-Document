{"title": "Docs - Optimize cost: Databases and smart analytics", "url": "https://cloud.google.com/architecture/framework/cost-optimization/databases", "abstract": "# Docs - Optimize cost: Databases and smart analytics\nLast reviewed 2023-06-26 UTC\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides recommendations to help you optimize the cost of your databases and analytics workloads in Google Cloud.\nThe guidance in this section is intended for architects, developers, and administrators responsible for provisioning and managing databases and analytics workloads in the cloud.\nThis section includes cost-optimization recommendations for the following products:\n- [Cloud SQL](#sql) \n- [Spanner](#spanner) \n- [Bigtable](#bigtable) \n- [BigQuery](#bigquery) \n- [Dataflow](#dataflow) \n- [Dataproc](#dataproc) ", "content": "## Cloud SQL\n[Cloud SQL](/sql) is a fully managed relational database for MySQL, PostgreSQL, and SQL Server.\n### Monitor usage\nReview the metrics on the [monitoring dashboard](https://console.cloud.google.com/monitoring/dashboards/resourceList/cloudsql_database) , and validate that your deployment meets the requirements of your workload.\n### Optimize resources\nThe following are recommendations to help you optimize your Cloud SQL resources:\n- Design a high availability and disaster recovery strategy that aligns with your recovery time objective (RTO) and recovery point objective (RPO). Depending on your workload, we recommend the following:- For workloads that need a short RTO and RPO, use the [high-availability configuration](/sql/docs/mysql/configure-ha) and [replicas for regional failover](/sql/docs/mysql/replication/cross-region-replicas) .\n- For workloads that can withstand a longer RTO and RPO, use automated and on-demand [backups](/sql/docs/mysql/backup-recovery/backups) , which can take a little longer to restore after a failure.\n- Provision the database with the minimum required storage capacity.\n- To scale storage capacity automatically as your data grows, enable the [automatic storage increase](/sql/docs/postgres/instance-settings#automatic-storage-increase-2ndgen) feature.\n- Choose a storage type, [solid-state drives (SSD) or hard disk drives (HDD)](/sql/docs/mysql/choosing-ssd-hdd) , that's appropriate for your use case. SSD is the most efficient and cost-effective choice for most use cases. HDD might be appropriate for large datasets (>10 TB) that aren't latency-sensitive or are accessed infrequently.\n### Optimize rates\nConsider purchasing [committed use discounts](/sql/cud#pricing) for workloads with predictable resource needs. You can save 25% of on-demand pricing for a 1-year commitment and 52% for a 3-year commitment.\n## Spanner\n[Spanner](/spanner) is a cloud-native, unlimited-scale, strong-consistency database that offers up to 99.999% availability.\n### Monitor usage\nThe following are recommendations to help you track the usage of your Spanner resources:\n- Monitor your deployment, and configure the node count based on [CPU recommendations](/spanner/docs/cpu-utilization#recommended-max) .\n- Set alerts on your deployments to optimize storage resources. To determine the appropriate configuration, refer to the recommended [limits per node](/spanner/quotas#database_limits) .\n### Optimize resources\nThe following are recommendations to help you optimize your Spanner resources:\n- Run smaller workloads on Spanner at much lower cost by provisioning resources with Processing Units (PUs) versus nodes; [one Spanner node is equal to 1,000 PUs](/spanner/docs/compute-capacity#compute_capacity) .\n- Improve query execution performance by using the [query optimizer](/spanner/docs/query-optimizer/manage-query-optimizer) .\n- Construct SQL statements using [best practices](/spanner/docs/sql-best-practices) for building efficient execution plans.\n- Manage the usage and performance of Spanner deployments by using the [Autoscaler](/architecture/autoscaling-cloud-spanner) tool. The tool monitors instances, adds or removes nodes automatically, and helps you ensure that the instances remain within the recommended CPU and storage limits.\n- Protect against accidental deletion or writes by using [point-in-time recovery (PITR)](/spanner/docs/pitr) . Databases with longer version retention periods (particularly databases that overwrite data frequently) use more system resources and need more nodes.\n- Review your backup strategy, and choose between the following options:- Backup and restore\n- Export and import\n### Optimize rates\nWhen deciding the location of your Spanner nodes, consider the cost differences between Google Cloud regions. For example, a node that's deployed in the `us-central1` region costs considerably less per hour than a node in the `southamerica-east1` region.\n## Bigtable\n[Bigtable](/bigtable) is a cloud-native, wide-column NoSQL store for large scale, low-latency workloads.\n### Monitor usage\nThe following are recommendations to help you track the usage of your Bigtable resources:\n- Analyze [usage metrics](/bigtable/docs/monitoring-instance#console-overview) to identify opportunities for resource optimization.\n- Identify hotspots and hotkeys in your Bigtable cluster by using the [Key Visualizer](/bigtable/docs/keyvis-overview) diagnostic tool.\n### Optimize resources\nThe following are recommendations to help you optimize your Bigtable resources:\n- To help you ensure [CPU and disk usage](/bigtable/docs/monitoring-instance#cpu-and-disk) that provides a balance between latency and storage capacity, evaluate and adjust the node count and size of your Bigtable cluster.\n- Maintain performance at the lowest cost possible by [programmatically scaling](/bigtable/docs/scaling) your Bigtable cluster to adjust the node count automatically.\n- Evaluate the most cost-effective [storage type](/bigtable/docs/choosing-ssd-hdd) (HDD or SSD) for your use case, based on the following considerations:- HDD storage costs less than SSD, but has lower performance.\n- SSD storage costs more than HDD, but provides faster and predictable performance.\nThe cost savings from HDD are minimal, relative to the cost of the nodes in your Bigtable cluster, unless you store large amounts of data. HDD storage is sometimes appropriate for large datasets (>10 TB) that are not latency-sensitive or are accessed infrequently.\n- Remove expired and obsolete data using [garbage collection](/bigtable/docs/garbage-collection) .\n- To avoid hotspots, apply best practices for [row key design](/bigtable/docs/schema-design#row-keys-avoid) .\n- Design a cost-effective [backup plan](/bigtable/docs/backups) that aligns with your RPO.\n- To lower the cluster usage and reduce the node count, consider adding a [capacity cache](/community/tutorials/bigtable-memcached) for cacheable queries by using [Memorystore](/memorystore/docs) .\n### Additional reading\n- [Blog: A primer on Bigtable cost optimization](/blog/products/databases/how-to-save-money-when-using-cloud-databases) \n- [Blog: Best practices for Bigtable performance and cost optimization](/blog/products/databases/check-out-how-to-optimize-database-service-cloud-bigtable) ## BigQuery\n[BigQuery](/bigquery) is a serverless, highly scalable, and cost-effective multicloud data warehouse designed for business agility.\n### Monitor usage\nThe following are recommendations to help you track the usage of your BigQuery resources:\n- [Visualize](/bigquery/docs/visualize-looker-studio) your BigQuery costs segmented by projects and users. Identify the most expensive queries and optimize them.\n- Analyze [slot utilization](/blog/topics/developers-practitioners/monitoring-bigquery-reservations-and-slot-utilization-information_schema) across projects, jobs, and reservations by using`INFORMATION_SCHEMA`metadata tables.\n### Optimize resources\nThe following are recommendations to help you optimize your BigQuery resources:\n- Set up dataset-level, table-level, or partition-level [expirations](/bigquery/docs/updating-datasets#table-expiration) for data, based on your compliance strategy.\n- [Limit query costs](/bigquery/docs/best-practices-costs#limit_query_costs_by_restricting_the_number_of_bytes_billed) by restricting the number of bytes billed per query. To prevent accidental human errors, enable cost control at the user level and project level.\n- [Query only the data](/bigquery/docs/best-practices-costs#avoid_select_) that you need. Avoid full query scans. To explore and understand data semantics, use the no-charge [data preview](/bigquery/docs/best-practices-costs#preview-data) options.\n- To reduce the processing cost and improve performance, [partition](/bigquery/docs/partitioned-tables) and [cluster](/bigquery/docs/clustered-tables) your tables when possible.\n- Filter your [query](/bigquery/query-plan-explanation#background) as early and as often as you can.\n- When processing data from multiple sources (like Bigtable, Cloud Storage, Google Drive, and Cloud SQL), avoid duplicating data, by using a [federated access data model](/bigquery/external-data-sources) and querying data directly from the sources.\n- Take advantage of [BigQuery's backup](/bigquery/docs/availability) instead of duplicating data. See [Disaster recovery scenarios for data](/architecture/dr-scenarios-for-data#managed-database-services-on-gcp) .\n### Optimize rates\nThe following are recommendations to help you reduce the billing rates for your BigQuery resources:\n- Evaluate how you edit data, and take advantage of lower [long-term](/bigquery/docs/best-practices-storage#take_advantage_of_long-term_storage) storage prices.\n- Review the differences between flat-rate and on-demand [pricing](/bigquery/pricing) , and [choose an option](/bigquery/docs/reservations-workload-management) that suits your requirements.\n- Assess whether you can use batch-loading instead of [streaming inserts](/bigquery/streaming-data-into-bigquery) for your data workflows. Use streaming inserts if the data loaded to BigQuery is consumed immediately.\n- To increase performance and reduce the cost of retrieving data, use [cached query results](/bigquery/docs/cached-results) .\n### Additional reading\n- [Controlling BigQuery costs](/bigquery/docs/controlling-costs) \n- [Cost optimization best practices for BigQuery](/blog/products/data-analytics/cost-optimization-best-practices-for-bigquery) \n- [Understanding the principles of cost optimization (PDF)](https://services.google.com/fh/files/misc/understanding_the_principles_of_cost_optimization_2020_whitepaper_google_cloud.pdf) ## Dataflow\n[Dataflow](/dataflow) is a fast and cost-effective serverless service for unified stream and batch data processing.\n### Monitor usage\nThe following are recommendations to help you track the usage of your Dataflow resources:\n- [Predict the cost of Dataflow jobs](/blog/products/data-analytics/predicting-cost-dataflow-job) by running small load experiments, finding your job's optimal performance, and extrapolating the throughput factor.\n- Gain increased visibility into throughput and CPU usage by using [observability dashboards](/blog/products/data-analytics/better-data-pipeline-observability-for-batch-and-stream-processing) .\n- Observe performance, execution, and health-related pipeline metrics using the [Dataflow monitoring interface](/dataflow/docs/guides/using-monitoring-intf) .\n### Optimize resources\nThe following are recommendations to help you optimize your Dataflow resources:\n- Consider [Dataflow Prime](/blog/products/data-analytics/simplify-and-automate-data-processing-with-dataflow-prime) for processing big data efficiently.\n- Reduce batch-processing costs by using [Flexible Resource Scheduling (FlexRS)](/dataflow/docs/guides/flexrs) for autoscaled batched pipelines. FlexRS uses advanced scheduling, Dataflow shuffle, and a combination of preemptible and regular VMs to reduce the cost for batch pipelines.\n- Improve performance by using the in-memory [shuffle service](/dataflow/pricing#dataflow_services) instead of Persistent Disk and worker nodes.\n- For more responsive autoscaling, and to reduce resource consumption, use [Streaming Engine](/dataflow/docs/guides/deploying-a-pipeline#streaming-engine) , which moves pipeline execution out of the worker VMs and into the Dataflow service backend.\n- If the pipeline doesn't need access to and from the internet and other Google Cloud networks, disable public IP addresses. Disabling internet access helps you reduce network costs and improve pipeline security.\n- Follow the best practices for [efficient pipelining with Dataflow](/blog/products/data-analytics/how-to-efficiently-process-both-real-time-and-aggregate-data-with-dataflow) .## Dataproc\n[Dataproc](/dataproc) is a managed Apache Spark and Apache Hadoop service for batch processing, querying, streaming, and machine learning.\nThe following are recommendations to help you optimize the cost of your Dataproc resources:\n- [Choose machine types](/blog/products/data-analytics/optimize-dataproc-costs-using-vm-machine-type) that are appropriate for your workload.\n- Scale automatically to match demand by using [autoscaling clusters](/dataproc/docs/concepts/configuring-clusters/autoscaling) , and pay for only the resources that you need.\n- If a cluster can be deleted when the job is completed, consider provisioning an ephemeral cluster using a [managed-cluster workflow template](/dataproc/docs/concepts/workflows/overview) .\n- To avoid charges for an inactive cluster, use [scheduled deletion](/dataproc/docs/concepts/configuring-clusters/scheduled-deletion) , which lets you delete a cluster after a specified idle period, at a specified time, or after a specified period.\n- Follow the best practices for [building long-running clusters on Dataproc](/blog/products/data-analytics/10-tips-for-building-long-running-clusters-using-cloud-dataproc) .\n- Purchase [committed use discounts](/compute/docs/instances/signing-up-committed-use-discounts) for always-on workloads.## What's next\n- Optimize cost for compute services, storage, networking, and operations:- [Optimize cost: Compute, containers, and serverless](/architecture/framework/cost-optimization/compute) \n- [Optimize cost: Storage](/architecture/framework/cost-optimization/storage) \n- [Optimize cost: Networking](/architecture/framework/cost-optimization/networking) \n- [Optimize cost: Cloud operations](/architecture/framework/cost-optimization/cloudops) \n- Explore the other categories of the [Google Cloud Architecture Framework](/architecture/framework)", "guide": "Docs"}