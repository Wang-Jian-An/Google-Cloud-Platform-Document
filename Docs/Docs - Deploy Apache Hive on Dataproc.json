{"title": "Docs - Deploy Apache Hive on Dataproc", "url": "https://cloud.google.com/architecture/using-apache-hive-on-cloud-dataproc/deployment", "abstract": "# Docs - Deploy Apache Hive on Dataproc\nLast reviewed 2023-05-08 UTC\n**Important:** We recommend that you use [Dataproc Metastore](/dataproc-metastore/docs) . to manage Hive metadata on Google Cloud, rather than the legacy workflow described in this deployment.\nThis document describes how you deploy the architecture in [Use Apache Hive on Dataproc](/architecture/using-apache-hive-on-cloud-dataproc) .\nThis document is intended for cloud architects and data engineers who are interested in deploying Apache Hive on Dataproc and the Hive Metastore in Cloud SQL.\n", "content": "## Architecture\nIn this deployment guide you deploy all compute and storage services in the same Google Cloud [region](/compute/docs/regions-zones) to minimize network latency and network transport costs.\nThe following diagram shows the lifecycle of a Hive query.\nIn the diagram, the Hive client submits a query, which is processed, fetched, and returned. Processing takes place in the Hive server. The data is requested and returmed from a Hive warehouse stored in a regional bucket in Cloud Storage.\n## Objectives\n- Create a MySQL instance on Cloud SQL for the Hive metastore.\n- Deploy Hive servers on Dataproc.\n- Install the [Cloud SQL Proxy](/sql/docs/mysql/sql-proxy) on the Dataproc cluster instances.\n- Upload Hive data to Cloud Storage.\n- Run Hive queries on multiple Dataproc clusters.## Costs\nThis deployment uses the following billable components of Google Cloud:\n- Dataproc\n- Cloud Storage\n- Cloud SQL\nYou can use the [pricing calculator](/products/calculator) to generate a cost estimate based on your projected usage.\nNew Google Cloud users might be eligible for a [free trial](/free-trial) .\nWhen you finish this deployment, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .\n## Before you begin\n- In the Google Cloud console, on the project selector page, [select or create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . **Note:** If you don't plan to keep the resources that you create in this deployment, create a project instead of selecting an existing project. After you finish this deployment, you can delete the project to remove all resources that are associated with the project. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- Make sure that billing is enabled for your Google Cloud project. Learn how to [check if billing is enabled on a project](/billing/docs/how-to/verify-billing-enabled) .\n### Initialize the environment\n- Start a Cloud Shell instance: [Go to Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- In Cloud Shell, set the default Compute Engine zone to the zone where you are going to create your Dataproc clusters.```\nexport PROJECT=$(gcloud info --format='value(config.project)')\nexport REGION=REGION\nexport ZONE=ZONE\ngcloud config set compute/zone ${ZONE}\n```Replace the following:- ``: The region where you want to create the cluster, such as`us-central1`.\n- ``: The zone where you want to create the cluster, such as`us-central1-a`.\n- Enable the Dataproc and Cloud SQL Admin APIs by running this command in Cloud Shell:```\ngcloud services enable dataproc.googleapis.com sqladmin.googleapis.com\n```\n## (Optional) Creating the warehouse bucket\nIf you don't have a Cloud Storage bucket to store Hive data, create a warehouse bucket (you can run the following commands in Cloud Shell) replacing `` with a unique bucket name:\n```\nexport WAREHOUSE_BUCKET=BUCKET_NAME\ngsutil mb -l ${REGION} gs://${WAREHOUSE_BUCKET}\n```\n## Creating the Cloud SQL instance\nIn this section, you create a new Cloud SQL instance that will later be used to host the Hive metastore.\nIn Cloud Shell, create a new Cloud SQL instance:\n```\ngcloud sql instances create hive-metastore \\\n --database-version=\"MYSQL_5_7\" \\\n --activation-policy=ALWAYS \\\n --zone ${ZONE}\n```\nThis command might take a few minutes to complete.\n## Creating a Dataproc cluster\nCreate the first Dataproc cluster, replacing `` with a name such as `hive-cluster` :\n```\ngcloud dataproc clusters create CLUSTER_NAME \\\n --scopes sql-admin \\\n --region ${REGION} \\\n --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/cloud-sql-proxy/cloud-sql-proxy.sh \\\n --properties \"hive:hive.metastore.warehouse.dir=gs://${WAREHOUSE_BUCKET}/datasets\" \\\n --metadata \"hive-metastore-instance=${PROJECT}:${REGION}:hive-metastore\" \\\n --metadata \"enable-cloud-sql-proxy-on-workers=false\"\n```\n**Notes:**\n- You provide the`sql-admin` [access scope](/sdk/gcloud/reference/dataproc/clusters/create#--scopes) to allow cluster instances to access the Cloud SQL Admin API.\n- You put your initialization action in a script that you store in a Cloud Storage bucket, and you reference that bucket with the`--initialization-actions`flag. See [Initialization actions - Important considerations and guidelines](/dataproc/docs/concepts/configuring-clusters/init-actions#important_considerations_and_guidelines) for more information.\n- You provide the URI to the Hive warehouse bucket in the`hive:hive.metastore.warehouse.dir`property. This configures the Hive servers to read from and write to the correct location. This propertycontain at least one directory (for example,`gs://my-bucket/my-directory`); Hive will not work properly if this property is set to a bucket name without a directory (for example,`gs://my-bucket`).\n- You specify`enable-cloud-sql-proxy-on-workers=false`to ensure that the Cloud SQL Proxy only runs on master nodes, which is sufficient for the Hive metastore service to function and avoids unnecessary load on Cloud SQL.\n- You provide the Cloud SQL Proxy [initialization action](/dataproc/docs/concepts/configuring-clusters/init-actions) that Dataproc automatically runs on all cluster instances. The action does the following:- Installs the Cloud SQL Proxy.\n- Establishes a secure connection to the Cloud SQL instance specified in the`hive-metastore-instance`metadata parameter.\n- Creates the`hive`user and the Hive metastore's database.\nYou can see the [full code](https://github.com/GoogleCloudDataproc/initialization-actions/tree/master/cloud-sql-proxy) for the Cloud SQL Proxy initialization action on GitHub.\n- This deployment uses a Cloud SQL instance with public IP address. If instead you use an instance with [only a private IP address](https://cloud.google.com/sql/docs/mysql/private-ip) , then you can force the proxy to use the private IP address by passing the `--metadata \"use-cloud-sql-private-ip=true\"` parameter.## Creating a Hive table\nIn this section, you upload a sample dataset to your warehouse bucket, create a new Hive table, and run some HiveQL queries on that dataset.\n- Copy the sample dataset to your warehouse bucket:```\ngsutil cp gs://hive-solution/part-00000.parquet \\\ngs://${WAREHOUSE_BUCKET}/datasets/transactions/part-00000.parquet\n```The sample dataset is compressed in the [Parquet](https://parquet.apache.org/) format and contains thousands of fictitious bank transaction records with three columns: date, amount, and transaction type.\n- Create an external Hive table for the dataset:```\ngcloud dataproc jobs submit hive \\\n --cluster CLUSTER_NAME \\\n --region ${REGION} \\\n --execute \"\n  CREATE EXTERNAL TABLE transactions\n  (SubmissionDate DATE, TransactionAmount DOUBLE, TransactionType STRING)\n  STORED AS PARQUET\n  LOCATION 'gs://${WAREHOUSE_BUCKET}/datasets/transactions';\"\n```## Running Hive queries\nYou can use different tools inside Dataproc to run Hive queries. In this section, you learn how to perform queries using the following tools:\n- Dataproc's [Hive jobs API](/sdk/gcloud/reference/dataproc/jobs/submit/hive) .\n- [Beeline](https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline%E2%80%93CommandLineShell) , a popular command line client that is based on [SQLLine](http://sqlline.sourceforge.net/) .\n- [SparkSQL](https://spark.apache.org/sql/) , Apache Spark's API for querying structured data.\nIn each section, you run a sample query.\n### Querying Hive with the Dataproc Jobs API\nRun the following simple HiveQL query to verify that the parquet file is correctly linked to the Hive table:\n```\ngcloud dataproc jobs submit hive \\\n --cluster CLUSTER_NAME \\\n --region ${REGION} \\\n --execute \"\n  SELECT *\n  FROM transactions\n  LIMIT 10;\"\n```\nThe output includes the following:\n```\n+-----------------+--------------------+------------------+\n| submissiondate | transactionamount | transactiontype |\n+-----------------+--------------------+------------------+\n| 2017-12-03  | 1167.39   | debit   |\n| 2017-09-23  | 2567.87   | debit   |\n| 2017-12-22  | 1074.73   | credit   |\n| 2018-01-21  | 5718.58   | debit   |\n| 2017-10-21  | 333.26    | debit   |\n| 2017-09-12  | 2439.62   | debit   |\n| 2017-08-06  | 5885.08   | debit   |\n| 2017-12-05  | 7353.92   | authorization |\n| 2017-09-12  | 4710.29   | authorization |\n| 2018-01-05  | 9115.27   | debit   |\n+-----------------+--------------------+------------------+\n```\n### Querying Hive with Beeline\n- Open an SSH session with the Dataproc's master instance( `` -m):```\ngcloud compute ssh CLUSTER_NAME-m\n```\n- In the master instance's command prompt, open a Beeline session:```\nbeeline -u \"jdbc:hive2://localhost:10000\"\n```Notes:- You can also reference the master instance's name as the host instead of `localhost` :```\nbeeline -u \"jdbc:hive2://CLUSTER_NAME-m:10000\"\n```\n- If you were using the high-availability mode with 3 masters, you would have to use the following command instead:```\nbeeline -u \"jdbc:hive2://CLUSTER_NAME-m-0:2181,CLUSTER_NAME-m-1:2181,CLUSTER_NAME-m-2:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2\"\n```\n- When the Beeline prompt appears, run the following HiveQL query:```\nSELECT TransactionType, AVG(TransactionAmount) AS AverageAmount\nFROM transactions\nWHERE SubmissionDate = '2017-12-22'\nGROUP BY TransactionType;\n```The output includes the following:```\n+------------------+--------------------+\n| transactiontype | averageamount |\n+------------------+--------------------+\n| authorization | 4890.092525252529 |\n| credit   | 4863.769269565219 |\n| debit   | 4982.781458176331 |\n+------------------+--------------------+\n```\n- Close the Beeline session:```\n!quit\n```\n- Close the SSH connection:```\nexit\n```\n### Querying Hive with SparkSQL\n- Open an SSH session with the Dataproc's master instance:```\ngcloud compute ssh CLUSTER_NAME-m\n```\n- In the master instance's command prompt, open a new [PySpark](https://pypi.org/project/pyspark/) shell session:```\npyspark\n```\n- When the PySpark shell prompt appears, type the following Python code:```\nfrom pyspark.sql import HiveContext\nhc = HiveContext(sc)\nhc.sql(\"\"\"\nSELECT SubmissionDate, AVG(TransactionAmount) as AvgDebit\nFROM transactions\nWHERE TransactionType = 'debit'\nGROUP BY SubmissionDate\nHAVING SubmissionDate >= '2017-10-01' AND SubmissionDate < '2017-10-06'\nORDER BY SubmissionDate\n\"\"\").show()\n```The output includes the following:```\n+-----------------+--------------------+\n| submissiondate |  avgdebit  |\n+-----------------+--------------------+\n| 2017-10-01  | 4963.114920399849 |\n| 2017-10-02  | 5021.493300510582 |\n| 2017-10-03  | 4982.382279569891 |\n| 2017-10-04  | 4873.302702503676 |\n| 2017-10-05  | 4967.696333583777 |\n+-----------------+--------------------+\n```\n- Close the PySpark session:```\nexit()\n```\n- Close the SSH connection:```\nexit\n```## Inspecting the Hive metastore\nYou now verify that the Hive metastore in Cloud SQL contains information about the `transactions` table.\n- In Cloud Shell, start a new MySQL session on the Cloud SQL instance:```\ngcloud sql connect hive-metastore --user=root\n```When you're prompted for the `root` user password, do not type anything and just press the `RETURN` key. For the sake of simplicity in this deployment, you did not set any password for the `root` user. For information about setting a password to further protect the metastore database, refer to the Cloud SQL [documentation](/sql/docs/mysql/create-manage-users#changing_a_user_password) . The Cloud SQL Proxy initialization action also provides a mechanism for protecting passwords through encryption\u2014for more information, see the action's [code repository](https://github.com/GoogleCloudDataproc/initialization-actions/tree/master/cloud-sql-proxy#protecting-passwords-with-kms) .\n- In the MySQL command prompt, make `hive_metastore` the default database for the rest of the session:```\nUSE hive_metastore;\n```\n- Verify that the warehouse bucket's location is recorded in the metastore:```\nSELECT DB_LOCATION_URI FROM DBS;\n```The output looks like this:```\n+-------------------------------------+\n| DB_LOCATION_URI      |\n+-------------------------------------+\n| gs://[WAREHOUSE_BUCKET]/datasets |\n+-------------------------------------+\n```\n- Verify that the table is correctly referenced in the metastore:```\nSELECT TBL_NAME, TBL_TYPE FROM TBLS;\n```The output looks like this:```\n+--------------+----------------+\n| TBL_NAME  | TBL_TYPE  |\n+--------------+----------------+\n| transactions | EXTERNAL_TABLE |\n+--------------+----------------+\n```\n- Verify that the table's columns are also correctly referenced:```\nSELECT COLUMN_NAME, TYPE_NAME\nFROM COLUMNS_V2 c, TBLS t\nWHERE c.CD_ID = t.SD_ID AND t.TBL_NAME = 'transactions';\n```The output looks like this:```\n+-------------------+-----------+\n| COLUMN_NAME  | TYPE_NAME |\n+-------------------+-----------+\n| submissiondate | date  |\n| transactionamount | double |\n| transactiontype | string |\n+-------------------+-----------+\n```\n- Verify that the input format and location are also correctly referenced:```\nSELECT INPUT_FORMAT, LOCATION\nFROM SDS s, TBLS t\nWHERE s.SD_ID = t.SD_ID AND t.TBL_NAME = 'transactions';\n```The output looks like this:```\n+---------------------------------------------------------------+------------------------------------------------+\n| INPUT_FORMAT             | LOCATION          |\n+---------------------------------------------------------------+------------------------------------------------+\n| org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat | gs://[WAREHOUSE_BUCKET]/datasets/transactions |\n+---------------------------------------------------------------+------------------------------------------------+\n```\n- Close the MySQL session:```\nexit\n```## Creating another Dataproc cluster\nIn this section, you create another Dataproc cluster to verify that the Hive data and Hive metastore can be shared across multiple clusters.\n- Create a new Dataproc cluster:```\ngcloud dataproc clusters create other-CLUSTER_NAME \\\n --scopes cloud-platform \\\n --image-version 2.0 \\\n --region ${REGION} \\\n --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/cloud-sql-proxy/cloud-sql-proxy.sh \\\n --properties \"hive:hive.metastore.warehouse.dir=gs://${WAREHOUSE_BUCKET}/datasets\" \\\n --metadata \"hive-metastore-instance=${PROJECT}:${REGION}:hive-metastore\"\\\n --metadata \"enable-cloud-sql-proxy-on-workers=false\"\n```\n- Verify that the new cluster can access the data:```\ngcloud dataproc jobs submit hive \\\n --cluster other-CLUSTER_NAME \\\n --region ${REGION} \\\n --execute \"\n  SELECT TransactionType, COUNT(TransactionType) as Count\n  FROM transactions\n  WHERE SubmissionDate = '2017-08-22'\n  GROUP BY TransactionType;\"\n```The output includes the following:```\n+------------------+--------+\n| transactiontype | count |\n+------------------+--------+\n| authorization | 696 |\n| credit   | 1722 |\n| debit   | 2599 |\n+------------------+--------+\n```\nCongratulations, you've completed the steps in the deployment.\n## Clean up\nThe following sections explain how you can avoid future charges for your Google Cloud project and the Apache Hive and Dataproc resources that you used in this deployment.\n### Delete the Google Cloud project\nTo avoid incurring charges to your Google Cloud account for the resources used in this deployment, you can delete the Google Cloud project.\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Deleting individual resources\nRun the following commands in Cloud Shell to delete individual resources instead of deleting the whole project:\n```\ngcloud dataproc clusters delete CLUSTER_NAME --region ${REGION} --quiet\ngcloud dataproc clusters delete other-CLUSTER_NAME --region ${REGION} --quiet\ngcloud sql instances delete hive-metastore --quiet\ngsutil rm -r gs://${WAREHOUSE_BUCKET}/datasets\n```\n## What's next\n- Try [BigQuery](/bigquery) , Google's serverless, highly scalable, low-cost enterprise data warehouse.\n- Check out this [guide](/solutions/migration/hadoop/hadoop-gcp-migration-overview) on migrating Hadoop workloads to Google Cloud.\n- Check out this [initialization action](https://github.com/GoogleCloudDataproc/initialization-actions/tree/master/hive-hcatalog) for more details on how to use [Hive HCatalog](https://cwiki.apache.org/confluence/display/Hive/HCatalog) on Dataproc.\n- Learn how to configure Cloud SQL for [high availability](/sql/docs/mysql/configure-ha) to increase service reliability.\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Docs"}