{"title": "Docs - Migrating containers to Google Cloud: Migrating from OpenShift to GKE Enterprise", "url": "https://cloud.google.com/architecture/migrating-containers-openshift-anthos", "abstract": "# Docs - Migrating containers to Google Cloud: Migrating from OpenShift to GKE Enterprise\nThis document helps you plan, design, and implement your migration from [OpenShift](https://www.openshift.com/) to [GKE Enterprise](/anthos) . If done incorrectly, moving your workloads from one environment to another can be a challenging task, so plan and execute your migration carefully.\nThis document is part of a multi-part series about migrating to Google Cloud. If you're interested in an overview of the series, see [Migration to Google Cloud: Choosing your migration path](/solutions/migration-to-gcp-choosing-your-path) .\nThis document is part of a series that discusses migrating [containers](https://wikipedia.org/wiki/OS-level_virtualization) to Google Cloud:\n- [Migrating containers to Google Cloud: Migrating Kubernetes to Google Kubernetes Engine (GKE)](/solutions/migrating-containers-kubernetes-gke) \n- [Migrating containers to Google Cloud: Migrating to a new GKE environment](/architecture/migrating-containers-multi-cluster-gke) \n- Migrating containers to Google Cloud: Migrating from OpenShift to GKE Enterprise (this document)\n- [Migrating containers to Google Cloud: Migrate OpenShift projects to GKE Enterprise](/architecture/migrating-containers-openshift-anthos-projects) \n- [Migrating from OpenShift to GKE Enterprise: Migrate OpenShift SCCs to Policy Controller Constraints](/architecture/migrating-containers-openshift-anthos-scc) \nThis document is useful if you're planning to migrate from OpenShift running in an on-premises or private hosting environment, or in another cloud provider, to [GKE Enterprise](/anthos) . This document is also useful if you're evaluating the opportunity to migrate and want to explore what it might look like. The target environment can be one of the following:\n- A hosted environment entirely on Google Cloud.\n- A hybrid environment where you maintain part of your workload on-premises or in a private hosting environment and migrate the rest to Google Cloud.\nTo decide which environment suits your needs, consider your requirements. For example, you can focus on increasing the value of your business instead of worrying about the infrastructure, by [migrating to a public cloud environment](/solutions/migration-to-gcp-getting-started) and outsourcing some responsibilities to Google. You benefit from an [elastic consumption model](/pricing) to optimize your spending and resource usage. If you have any requirements, such that you have to keep some of your workloads outside Google Cloud, you might consider a hybrid environment, for example, if you're required to keep part of your workloads in your current environment to comply with data location policies and regulations. Or, you can implement an [improve and move](/solutions/migration-to-gcp-getting-started#improve_and_move) migration strategy, where you first modernize your workloads in place, and then migrate to Google Cloud.\nRegardless of your target environment type, the goal of this migration is to manage your workloads running in that environment using GKE Enterprise. By adopting GKE Enterprise, you have access to a range of services, including the following:\n- [Multi-cluster management](/anthos/multicluster-management) to help you and your organization manage clusters, infrastructure, and workloads across cloud and on-premises environments from a single place.\n- [Config Sync](/anthos-config-management/docs/config-sync-overview) and [Policy Controller](/anthos-config-management/docs/concepts/policy-controller) to create a common configuration and policies across all your infrastructure, and to apply them both on-premises and in the cloud.\n- [Anthos Service Mesh](/service-mesh/docs) to adopt a fully managed service mesh that simplifies operating services, traffic management, telemetry, and securing communications between services.\n- [Binary Authorization](/binary-authorization) to help ensure that the containers you deploy in your environments are trusted.\n- [Cloud Run for Anthos](/anthos/run) to support your serverless workloads in your GKE Enterprise environment.\nWe recommend that you evaluate these services early in your migration process while you're still designing your migration. It's easier to adopt these services now, instead of modifying your processes and infrastructure later. You can start using these services immediately, or when you're ready to modernize your workloads.\nIn this migration, you follow the migration framework defined in [Migration to Google Cloud: Getting started](/solutions/migration-to-gcp-getting-started) . The framework has four phases:\n- Assessing and discovering your workloads.\n- Planning and building a foundation.\n- Deploying your workloads.\n- Optimizing your environment.\nThe following diagram illustrates the path of your migration journey.\nThis document relies on concepts covered in [Migrating containers to Google Cloud: Migrating Kubernetes to GKE](/solutions/migrating-containers-kubernetes-gke) , so there are links to that document, where appropriate.\n", "content": "## Assessing and discovering your workloads\nIn the assessment phase, you determine the requirements and dependencies to migrate to your workloads from OpenShift to GKE Enterprise:\n- Build a comprehensive inventory of your processes and apps.\n- Catalog your processes and apps according to their properties and dependencies.\n- Train and educate your teams on Google Cloud.\n- Build experiments and proofs-of-concept on Google Cloud.\n- Calculate the total cost of ownership (TCO) of the target environment.\n- Choose the workloads that you want to migrate first.\nThe following sections rely on [Migration to Google Cloud: Assessing and discovering your workloads](/solutions/migration-to-gcp-assessing-and-discovering-your-workloads) , but they provide information that is specific to assessing workloads that you want to migrate from OpenShift to GKE Enterprise.\n### Build your inventories\nTo build the inventory of the components of your environment, consider the following:\n- Service delivery and platform management model\n- OpenShift projects\n- Build and deployment process\n- Workloads, requirements, and dependencies\n- OpenShift clusters configurationTo migrate workloads from OpenShift to GKE Enterprise, you assess the current service delivery and platform management model of your OpenShift environment. This model probably reflects your current organizational structure and needs. If you realize that the current model doesn't satisfy the organization needs, you can use this migration as an opportunity to improve the model.\nFirst, you gather information about the teams responsible for the following aspects:\n- Application development and deployment, including all OpenShift users, typically development, or workload release teams.\n- OpenShift platform management, including creating [OpenShift projects](https://docs.openshift.com/container-platform/4.3/applications/projects/working-with-projects.html) , assigning roles to users, configuring security contexts, and configuring [CI/CD](https://wikipedia.org/wiki/CI/CD) pipelines.\n- OpenShift installation and cluster management, including OpenShift installation, upgrade, cluster scaling, and capacity management.\n- Infrastructure management. These teams manage physical servers, storage, networking, the virtualization platform, and operating systems.\nA service delivery and platform management model can consist of the following teams:\n- **The development team** . This team develops workloads and deploys them on OpenShift. When dealing with complex production environments, the team that deploys workloads might be different from the development team. For simplicity in this document, we consider this team to be part of the development team. In self-service environments, the development team also has the responsibility of creating OpenShift projects.\n- **The** **platform team** . This team is responsible for OpenShift platform management, typically referred to as OpenShift cluster administrators. This team configures [OpenShift project templates](https://docs.openshift.com/container-platform/4.3/applications/projects/configuring-project-creation.html) for different development teams and, in more managed environments, creates OpenShift projects. This team also assigns roles and permissions, configures security contexts and [role-based access control (RBAC)](https://docs.openshift.com/container-platform/4.3/authentication/using-rbac.html) , defines quotas for compute resources and objects, and defines build and deployment strategies. They are sometimes referred to as theor as the, if they manage middleware and application server configurations for developers. The platform team and the infrastructure team might also be involved in low-level OpenShift cluster management activities, such as software installation and upgrade, cluster scaling, and capacity management.\n- **The infrastructure team** . This team manages the underlying infrastructure that supports the OpenShift environment. For example, they're in charge of servers, storage, networking, the virtualization platform, and the base operating system. This team is sometimes referred to as theor. If OpenShift is deployed in a public cloud environment, this team is responsible for the infrastructure as a service (IaaS) services that a public cloud provider offers.\nIt's also important to assess if you have dedicated OpenShift clusters for different environments. For example, you might have different environments for development, quality assurance, and production, or to segregate different network and security zones, such as internal zones and [de-militarized zones](https://wikipedia.org/wiki/DMZ_(computing)) .\nAn [OpenShift project](https://docs.openshift.com/container-platform/4.3/applications/projects/working-with-projects.html) is a Kubernetes [namespace](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) with additional annotations that lets developers manage resources in isolation from other teams to logically separate resources. To build the inventory of your OpenShift projects, consider the following for each project:\n- **Cluster roles \nand\nlocal roles.** OpenShift supports both roles that are local to an OpenShift project, or cluster-wide roles. You assess if you created any cluster and local roles to design an effective access control mechanism in the target environment.\n- **Role bindings, both for\ncluster roles \nand\nlocal roles.** Users and groups are granted permissions to perform operations on OpenShift projects by assigning them role bindings. Roles can be at the cluster level or the local level. Often, local role bindings are bound to predefined cluster roles. For example, the default OpenShift project admin role binding might be bound to the default cluster admin role.\n- **ResourceQuotas** . To constrain aggregate resource consumption, OpenShift lets you define both [OpenShift project-level quotas](https://docs.openshift.com/container-platform/4.3/applications/quotas/quotas-setting-per-project.html) , and [quotas across multiple OpenShift projects](https://docs.openshift.com/container-platform/4.3/applications/quotas/quotas-setting-across-multiple-projects.html) . You assess how they map to [Kubernetes ResourceQuotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/) , and populate a list of all ResourceQuotas that you provisioned and configured in your OpenShift environment.\n[Assessing your environment](/solutions/migrating-containers-kubernetes-gke#assessing_your_environment) describes how to assess Kubernetes resources, such as ServiceAccounts, and PersistentVolumes.\nAfter gathering information about the service delivery and platform management model, and about OpenShift projects, you assess how you're building your workloads and deploying them in your environment.\nIn your existing OpenShift environment, you might have the same building and deployment process for all your workloads, or there might be different processes to assess. Artifacts of the building process for a containerized workload are container images. In an OpenShift environment, you might be building container images, storing them, and deploying them in different ways:\n- The container image building process runs completely outside OpenShift. The build process can be based on manual steps or can be based on an automated continuous integration and continuous deployment (CI/CD) pipeline that has the container image and Kubernetes manifests as the final product.\n- The container image building process runs inside OpenShift. [OpenShift supports different options](https://docs.openshift.com/container-platform/4.3/builds/understanding-image-builds.html) , such as providing a [Dockerfile and all the required artifacts to build a container image](https://docs.openshift.com/container-platform/4.3/builds/understanding-image-builds.html#builds-strategy-docker-build_understanding-image-builds) , [configuring a source-to-image build](https://docs.openshift.com/container-platform/4.3/builds/understanding-image-builds.html#build-strategy-s2i_understanding-image-builds) , [configuring a pipeline build](https://docs.openshift.com/container-platform/4.3/builds/build-strategies.html#builds-strategy-pipeline-build_build-strategies) , or [configuring a custom build](https://docs.openshift.com/container-platform/4.3/builds/understanding-image-builds.html#build-strategy-custom-build_understanding-image-builds) . These build strategies create a [BuildConfig](https://docs.openshift.com/container-platform/4.3/builds/understanding-buildconfigs.html#builds-buildconfig_understanding-builds) resource that defines the building choice, the source artifacts location, the target container images, and the events that can trigger the container image building process.\nAfter building each container image, you store it in a container registry that you can later deploy. Your container registry can be hosted either [on OpenShift](https://docs.openshift.com/container-platform/4.3/registry/registry-options.html#registry-integrated-openshift-registry_registry-options) , or [outside your OpenShift environment](https://docs.openshift.com/container-platform/4.3/registry/registry-options.html#registry-third-party-registries_registry-options) . Assess this aspect because you might need a similar system in your target environment.\nEach [OpenShift application](https://docs.openshift.com/container-platform/4.3/applications/application_life_cycle_management/odc-creating-applications-using-developer-perspective.html) contains the following components:\n- An [OpenShift DeploymentConfig](https://docs.openshift.com/container-platform/4.3/applications/deployments/what-deployments-are.html#deployments-and-deploymentconfigs_what-deployments-are) or a [Kubernetes Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) object. For more information about the differences between these objects, see [Comparing Deployments and DeploymentConfigs](https://docs.openshift.com/container-platform/4.3/applications/deployments/what-deployments-are.html#deployments-comparing-deploymentconfigs_what-deployments-are) .\n- A [Kubernetes Service](https://kubernetes.io/docs/concepts/services-networking/service/) to make your application reachable by clients, and an [OpenShift Route](https://docs.openshift.com/container-platform/4.3/networking/routes/route-configuration.html) to connect to that Kubernetes Service from outside the cluster.\n- An [OpenShift ImageStream](https://docs.openshift.com/container-platform/4.3/openshift_images/image-streams-manage.html) to provide an abstraction to reference container images. An OpenShift ImageStream includes one or more container images, each identified by tags, and presents a single abstract view of related images, similar to a container image repository.\n- An [OpenShift BuildConfig](https://docs.openshift.com/container-platform/4.3/builds/understanding-buildconfigs.html#builds-buildconfig_understanding-builds) to build the container images of that OpenShift application in OpenShift.\nDepending on the purpose of the application, you can use different objects to define the app instead of using the Deployment or DeploymentConfig objects:\n- Define batch applications using [Job](https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/) or [cron job](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/) .\n- Define stateful applications using [StatefulSets](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) .\n- If you have operations-related workloads that need to run on every node, or be bound to specific nodes, you can define them by using [DaemonSets](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) .\nThe following table lists the most important [specs](https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#object-spec-and-status) and parameters that you gather from OpenShift resources in order to migrate the applications to the target GKE Enterprise environment.\n| Source OpenShift resource manifest      | Most important specs and parameters                                            |\n|:---------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Deployment, DeploymentConfig, StatefulSet, Job, cron job | Container image and repository, container port, number of Pod replicas, ConfigMaps, Secrets, PersistentVolumeClaims, resource requests and limits, update strategy, StatefulSet Service Name, cron job schedule |\n| ImageStream            | Container image, image pull policy, container image repository                                     |\n| Horizontal Pod Autoscaler        | Autoscale criteria                                                |\n| Service             | Hostname used to connect to the application from inside the cluster, IP address and port on which the Service is exposed, endpoints created for external resources            |\n| Route             | Hostname and resource path that is used to connect to the application from outside the cluster, routing rules, encryption, certificate chain information              |\n[Assessing your environment](/solutions/migrating-containers-kubernetes-gke#assessing_your_environment) describes how to assess Kubernetes resources such as the following:\n- [Other Kubernetes controllers](https://kubernetes.io/docs/concepts/architecture/controller/) \n- [Horizontal Pod Autoscalers](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) \n- [Pod security contexts](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/) \n- Stateless and stateful workloads\n- Storage\n- Configuration and secret injection\n- Ingresses\n- Logging and monitoring\nOpenShift 4 introduced the [Operator Framework](https://docs.openshift.com/container-platform/4.3/operators/olm-what-operators-are.html#olm-operator-framework_olm-what-operators-are) . If you are using this OpenShift version, you might have deployed some applications using installed [Operators](https://docs.openshift.com/container-platform/4.3/operators/olm-what-operators-are.html) . In this case, you get the list of the installed Operators and you gather information for each of them about the deployed Operator instances. These instances are Operator-defined [Custom Resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) that deploy some of the previously listed Kubernetes [Resources](#table) .\nIn addition to assessing these resources, you assess the following:\n- **Application's network connectivity requirements.** For example, do your Services or Pods need to be exposed to a specific network? Do they need to reach specific backend systems?\n- **Constraints to run workloads in a specific location.** For example, do any workloads or datasets need to remain on-premises to comply with requirements such as latency in communicating with other workloads, policies related to data location, and proximity to users?Next, you assess your OpenShift clusters. To complete this task, you gather the following information:\n- **OpenShift version.** OpenShift major versions in scope of this document are OpenShift 3 and OpenShift 4. Different OpenShift versions might have different capabilities. You assess which version of OpenShift you're running to know whether you're using any OpenShift version-specific features.\n- **Identity provider used for authentication.** For authentication, you might be using the [built-in OAuth server](https://docs.openshift.com/container-platform/4.3/authentication/understanding-identity-provider.html) , and one or more [identity providers](https://docs.openshift.com/container-platform/4.3/authentication/understanding-identity-provider.html#supported-identity-providers) .\n- **Security Context Constraints** . Assess the OpenShift [Security Context Constraints](https://docs.openshift.com/container-platform/4.3/authentication/managing-security-context-constraints.html) that you defined in your clusters, their configuration, and to which users, groups, and service accounts they are assigned.\n- **Network policy and isolation.** Assess NetworkPolicies, how you configured Pod network isolation, and which [OpenShift SDN mode](https://docs.openshift.com/container-platform/4.3/networking/openshift_sdn/about-openshift-sdn.html) that you configured in your clusters.\n- **Monitoring.** Assess your current monitoring requirements, and how you provisioned and configured your current monitoring system to decide how to design and implement a monitoring solution in the target environment. This assessment can help you determine whether to use a new monitoring solution or if you can continue to use the existing solution. Many OpenShift versions include a monitoring stack based on [Prometheus](https://prometheus.io/) to monitor system components, which can also be used for application monitoring. When designing your target solution, consider the following:- The monitoring solution that you're currently using in your OpenShift environment, for example, an OpenShift-hosted Prometheus, an independent [Prometheus](https://prometheus.io/) - [Grafana](https://grafana.com/) stack, [Zabbix](https://www.zabbix.com/) , [InfluxData](https://www.influxdata.com/) , or [Nagios](https://www.nagios.org/) .\n- How metrics are produced and gathered, for example, a pull or a push mechanism.\n- Dependencies on any components deployed in your OpenShift clusters.\n- The location of your monitoring system, for example, deployed in a cloud environment or on-premises.\n- The metrics that you're currently gathering for your workloads.\n- Any alerts on metrics that you configured in your current monitoring system.\n- **Logging.** Assess your current logging requirements, and how you provisioned and configured your current logging system to decide how to design and implement a logging solution in the target environment. This assessment can help you determine whether to use a new logging solution or if you can continue to use the existing solution. Many OpenShift versions ship with a logging solution based on an [Elasticsearch](https://www.elastic.co/elasticsearch/) , [Fluentd](https://www.fluentd.org/) , and [Kibana](https://www.elastic.co/kibana) (EFK) stack that is used to gather logs from system components. This solution can also be used for application logging. When designing your target solution, consider the following:- The logging system that you're currently using in your OpenShift environment, for example, an OpenShift-hosted EFK stack, an independent EFK stack, or Splunk.\n- Dependencies on any components deployed in your OpenShift clusters.\n- The architecture and the capacity of the log storage components.\n- The location of your logging system, for example, deployed in a cloud environment or on-premises.\n- The log retention policies and configuration.[Assessing your environment](/solutions/migrating-containers-kubernetes-gke#assessing_your_environment) describes how to assess the following:\n- Number of clusters\n- Number and type of nodes per cluster\n- Additional considerations about logging, monitoring, and tracing\n- Custom Kubernetes resources\n### Complete the assessment\nAfter building the inventories related to your OpenShift processes and workloads, you complete the rest of the activities of the assessment phase in [Migration to Google Cloud: Assessing and discovering your workloads](/solutions/migration-to-gcp-assessing-and-discovering-your-workloads) .\n## Planning and building your foundation\nIn the planning and building phase, you provision and configure the infrastructure and services that support your workloads:\n- Build a resource hierarchy.\n- Configure identity and access management.\n- Set up billing.\n- Set up network connectivity.\n- Harden your security.\n- Set up monitoring and alerting.\nThis section provides information that is specific to building your foundation on GKE Enterprise, building on the information in [Migration to Google Cloud: Building your foundation](/solutions/migration-to-google-cloud-building-your-foundation) .\nBefore building a foundation in Google Cloud, read the [GKE Enterprise technical overview](/anthos/docs/concepts/overview) to understand how GKE Enterprise works, and which GKE Enterprise components you might need. Depending on the workload and data locality requirements that you gathered in the assessment phase, you deploy your workloads on [GKE on VMware](/anthos/gke/docs/on-prem) , on [GKE clusters on Google Cloud](/kubernetes-engine/docs) , or on [GKE on AWS](/anthos/gke/docs/aws) . Your clusters might be distributed among different environments. For more information about building a foundation for GKE on Google Cloud, see [Planning and building your foundation](/solutions/migrating-containers-kubernetes-gke#planning_and_building_your_foundation) .\nTo build a foundation for GKE on VMware, read about its [core concepts](/anthos/gke/docs/on-prem/concepts) , and then consider the following when [installing GKE on VMware](/anthos/gke/docs/on-prem/how-to/install-overview-basic) :\n- **Ensure that your on-premises environment meets the\nrequirements for Anthos GKE on-prem.** You need to provide [enough capacity](/anthos/gke/docs/on-prem/how-to/cpu-ram-storage) in your VMware vSphere environment to accommodate [admin cluster](/anthos/gke/docs/on-prem/how-to/cpu-ram-storage#admin_cluster) and [user clusters](/anthos/gke/docs/on-prem/how-to/cpu-ram-storage#user_cluster) requirements. These requirements depend on the amount of your workloads resource requests, and the number of clusters that you need. You assessed both aspects in the assessment phase.\n- **Set up your network.** You need to configure your on-premises network to satisfy the applications' network connectivity requirements gathered in the assessment, in addition to the GKE on VMware installation requirements. Consider the following networking needs:- [Networking](/anthos/gke/docs/on-prem/how-to/network-basic) \n- [Load balancing](/anthos/gke/docs/on-prem/how-to/setup-load-balance) \n- [Firewall rules](/anthos/gke/docs/on-prem/how-to/firewall-rules) \n- [Google Cloud projects](/anthos/gke/docs/on-prem/how-to/gcp-project) \n- [Service accounts](/anthos/gke/docs/on-prem/how-to/service-accounts) To build a foundation for [GKE on AWS](/anthos/gke/docs/aws) , read about its [core concepts](/anthos/gke/docs/aws/concepts) , such as [GKE on AWS architecture](/anthos/gke/docs/aws/concepts) and [GKE on AWS storage](/anthos/gke/docs/aws/concepts/storage) . Consider the following when installing GKE on AWS:\n- **Ensure that your Amazon Web Services (AWS) and Google Cloud\nenvironments meet the\nrequirements for GKE on AWS.** GKE on AWS requires an (AWS) account with command-line access and an [AWS Key Management Service (KMS)](https://aws.amazon.com/kms/) key to encrypt application-layer secrets in clusters. You need [Terraform](https://www.terraform.io/) and [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) .\n- **Configure the AWS environment.** You need to [configure your AWS environment](/anthos/gke/docs/aws/how-to/prerequisites#configuring_aws) , install tools, such as the [AWS Command Line Interface (CLI)](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html) , [configure AWS IAM credentials](https://www.google.com/url?q=https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-role.html) , and provision resources in your AWS environment, such as an [AWS KMS key](/anthos/gke/docs/aws/how-to/prerequisites#creating_a_kms_key) .\n- **Configure the Google Cloud environment.** You need to configure your [Google Cloud environment](/anthos/gke/docs/aws/how-to/prerequisites#gcp_requirements) , create the necessary [Google Cloud projects and service accounts, and configure IAM](/anthos/gke/docs/aws/how-to/prerequisites#gcp_requirements) .## Deploying your workloads\nIn the deployment phase, you deploy your workloads on GKE Enterprise:\n- Provision and configure your runtime platform and environments.\n- Migrate data from your old environment to your new environment.\n- Deploy your workloads.\nThe following sections provide information that is specific to deploying workloads to GKE Enterprise, building on the information in [Migration to Google Cloud: Transferring large datasets](/solutions/migration-to-google-cloud-transferring-your-large-datasets) , [Migration to Google Cloud: Deploying your workloads](/solutions/migration-to-gcp-deploying-your-workloads) , and [Migration to Google Cloud: Migrating from manual deployments to automated, containerized deployments](/solutions/migration-to-google-cloud-automated-containerized-deployments) .\n### Provision and configure your runtime platform and environments\nBefore you can deploy any workload, you provision and configure the necessary GKE Enterprise clusters.\nYou can provision GKE clusters on Google Cloud, GKE on VMware clusters, or GKE on AWS clusters. For example, if you have a workload that you must deploy on-premises, then you provision one or more GKE on VMware clusters. If your workloads don't have any locality requirement, you provision GKE clusters on Google Cloud. In both cases, you manage and monitor your clusters with GKE Enterprise. If you have any multi-cloud requirements, then you provision GKE on AWS clusters, along with other GKE Enterprise clusters.\nFirst, you define the number and type of GKE Enterprise clusters that you need. These requirements largely depend on the information that you gathered in the assessment phase, such as the service model that you want to implement and how you want to isolate different environments. If multiple development teams are currently sharing your OpenShift clusters, you must implement a multi-tenancy model on GKE Enterprise:\n- **Use different Kubernetes namespaces.** The platform team creates a Kubernetes namespace for each OpenShift project, and implements a [cluster multi-tenancy model](/kubernetes-engine/docs/concepts/multitenancy-overview) . This model closely resembles the one you likely adopted in your OpenShift environment, so it might require a number of GKE Enterprise clusters that's similar to the number of your OpenShift clusters. If needed, you can still have dedicated clusters for different environments.\n- **Use different GKE Enterprise clusters.** The infrastructure team provides an GKE Enterprise cluster for each development team, and the platform team manages each of these clusters. This model might require a number of GKE Enterprise clusters more than the number of your OpenShift clusters because it provides greater flexibility and isolation for your development.\n- **Use different Google Cloud projects.** The infrastructure team creates a Google Cloud project for each development team, and provisions GKE Enterprise clusters inside that Google Cloud project. The platform team then manages these clusters. This model might require some GKE Enterprise clusters more than the number of your OpenShift clusters because it provides the maximum flexibility and isolation for your development teams.\nAfter deciding the number of clusters that you need and in which environment to provision them, you define cluster size, configuration, and node types. Then you provision your clusters and node pools, according to the workload requirements that you gathered during the assessment phase. For example, your workloads might require certain performance and scalability guarantees, along with any other requirements, such as the need for [GPUs](/kubernetes-engine/docs/how-to/gpus) and [TPUs](/tpu/docs/kubernetes-engine-setup) .\nFor more information about provisioning and configuring clusters, see the following:\n- [Provision and configure your runtime platform and environments for GKE clusters on Google Cloud](/solutions/migrating-containers-kubernetes-gke#provision_and_configure_your_runtime_platform_and_environments) .\n- [Creating admin and user clusters for GKE on VMware clusters](/anthos/gke/docs/on-prem/how-to/admin-user-cluster-basic) .\n- [Installing the management cluster](/anthos/gke/docs/aws/how-to/installing-management) and [creating a user cluster](/anthos/gke/docs/aws/how-to/creating-user-cluster) for GKE on AWS clusters.\nAfter you create your clusters and before deploying any workload, you configure the following components to meet the requirements that you gathered in the [OpenShift projects](#openshift_projects) and [clusters](#openshift_clusters_configuration) assessment phase:\n- **Identity and access management.** You can configure identity and access management as described in [Configure identity and access management](/solutions/migrating-containers-kubernetes-gke#configure_identity_and_access_management) . You can migrate to [Cloud Identity](/iam/docs) as your main identity provider, or use [Google Cloud Directory Sync](https://support.google.com/cloudidentity/answer/106368) to synchronize Cloud Identity with an existing [LDAP](https://wikipedia.org/wiki/Lightweight_Directory_Access_Protocol) or Active Directory server. GKE on VMware supports [OpenID Connect (OIDC)](https://openid.net/connect/) for authenticating against user clusters using the command line. Follow [Authenticating with OIDC and Google](/anthos/gke/docs/on-prem/how-to/oidc-google) to integrate command-line authentication with Cloud Identity.\n- **Monitoring.** You can adapt your current monitoring solution to the target GKE Enterprise environment according to your constraints and requirements. If your current solution is hosted on OpenShift, you can implement [Cloud Monitoring](/monitoring/docs) as described in [Building your foundation](/solutions/migration-to-google-cloud-building-your-foundation#monitoring_and_alerting) or you can implement [Prometheus and Grafana with GKE on VMware.](/anthos/gke/docs/on-prem/concepts/logging-and-monitoring#prometheus_and_grafana) \n- **Logging** . You can adapt your current logging solution to the target GKE Enterprise environment according to your constraints and requirements. If your current solution is hosted on OpenShift, you can implement [Cloud Logging](/logging/docs) as described in [Building your foundation - Monitoring and Alerting.](/anthos/gke/docs/on-prem/concepts/logging-and-monitoring#prometheus_and_grafana) \nUsing [Config Sync](/anthos-config-management/docs/config-sync-overview) , you can centrally define the configuration of the following resources in a common Git-compliant repository, and apply that configuration to all clusters, both on-premises and in the cloud:\n- **Role-based access control (RBAC)** . After you configure authentication, you can implement your authorization policies using a mix of [Identity and Access Management](/kubernetes-engine/docs/how-to/iam) and [Kubernetes RBAC](/kubernetes-engine/docs/how-to/role-based-access-control) . These policies meet the requirements that you gathered in the [OpenShift Project assessment](#openshift_projects) and the [multi-tenancy model that you chose](#provision_and_configure_your_runtime_platform_and_environments) .\n- **Resource quotas** . You can apply [Resource quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/) to namespaces to assign quotas to developer teams as needed.\n- **Security context for your workloads** . You can use [Policy Controller](/anthos-config-management/docs/concepts/policy-controller) to create [constraints](https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/pod-security-policy) to [enforce Pod security](/anthos-config-management/docs/how-to/using-constraints-to-enforce-pod-security) according to your requirements and OpenShift Security Context Constraints configuration gathered in the assessment phase.\n- **Network policy and isolation** . You can implement the required network isolation between namespaces or workloads using Kubernetes [Network Policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) .\nTo install Config Sync, see [Install Config Sync](/anthos-config-management/docs/how-to/installing-config-sync) . To install Policy Controller, see [Install Policy Controller](/anthos-config-management/docs/how-to/installing-policy-controller) .\n### Migrate data from your old environment\nNow you can migrate data from your source environment to the target environment.\nIf your OpenShift stateful applications host data on Kubernetes [persistent volumes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) , there are different strategies to migrate data to the target environment. Choosing the right strategy depends on various factors, such as your source and target backend storage providers and deployment locations:\n- Rely on your storage provider's volume cloning, exporting, and importing capabilities. If you are using [VMware vSphere volumes](https://docs.openshift.com/container-platform/4.3/storage/persistent_storage/persistent-storage-vsphere.html) in your on-premises environment and you are migrating to GKE on VMware, you clone the PersistentVolumes underlying [VMDK](https://wikipedia.org/wiki/VMDK) virtual disks, and mount them as volumes in your target environment. If you are migrating to GKE, you [import](/compute/docs/import/importing-virtual-disks) your virtual disks as [Compute Engine persistent disks](/persistent-disk) and [use them](/kubernetes-engine/docs/how-to/persistent-volumes/preexisting-pd) as persistent volumes.\n- Back up your data from your source environment by using operating system tools or database tools. Host that data in a temporary location that is accessible from both environments, and then restore the data in your target environment.\n- Use a remote copy tool, such as [rsync](https://wikipedia.org/wiki/Rsync) , to copy data from the source environment to the target environment.\n- Use a storage-independent backup solution, such as [Velero](https://velero.io/docs/v1.3.2/index.html) with [restic integration](https://velero.io/docs/v1.3.2/restic/) .\nFor more information, see [Migration to Google Cloud: Transferring large datasets](/solutions/migration-to-google-cloud-transferring-your-large-datasets) .\nFor more information about migrating data and strategies to manage storage in GKE, see [Migrate data from your old environment to your new environment](/solutions/migrating-containers-kubernetes-gke#migrate_data_from_your_old_environment_to_your_new_environment) and the GKE documents about [storage configuration](/kubernetes-engine/docs/how-to#configuring-cluster-storage) .\nWith GKE on VMware, you can choose between [different options for integrating with external storage systems](/anthos/gke/docs/on-prem/concepts/storage) , such as through VMware vSphere storage, [Kubernetes in-tree volume plugins](https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes) , and [Container Storage Interface](https://github.com/container-storage-interface/spec/blob/master/spec.md) (CSI) drivers. Your choice depends on which external storage system that you need to integrate with, the supported access modes, and if you need dynamic volume provisioning.\nGKE on AWS automatically deploys the [CSI driver for Amazon Elastic Block Store (EBS)](/anthos/gke/docs/aws/how-to/storage-class) and a [default StorageClass that backs PersistentVolumeClaims with EBS volumes](/anthos/gke/docs/aws/how-to/storage-class#using_the_default_storageclass) and [StorageClasses for other EBS volume types](/anthos/gke/docs/aws/how-to/storage-class#using_another_preinstalled_storageclass) . You can also install [additional CSI drivers](/anthos/gke/docs/aws/how-to/storage-drivers) and [custom StorageClasses](/anthos/gke/docs/aws/how-to/storage-class#custom) . If you have an EBS volume that you want to import in GKE on AWS, you can [create a PersistentVolume from it](/anthos/gke/docs/aws/how-to/preexisting-volume) .\n### Deploy your workloads\nAfter provisioning the GKE Enterprise cluster and migrating data, you now build and deploy your workloads. You have different options, ranging from manual deployments to [fully automated ones](/solutions/migration-to-gcp-deploying-your-workloads#deploy_automatically) .\nIf you need to use Operators to deploy workloads that use this deployment method in your OpenShift environment, you need to install the Operator before deploying your workload. You can verify the availability of the Operators that you need in the following sources:\n- [Google Cloud Marketplace](https://console.cloud.google.com/marketplace/browse?filter=solution-type) \n- [operatorhub.io](https://operatorhub.io) \n- Specific software vendor website or GitHub repositoryIf you are manually deploying your workloads in your OpenShift environment, you can adapt this manual deployment process to your new GKE Enterprise environment. For example, you can manually translate the OpenShift resource manifests that you assessed in [workloads, requirements, and dependencies](#bookmark=id.o34g8tkxyj5o) to the corresponding GKE Enterprise resource manifests.\nThe following table extends [the table](#table) in the [workloads, requirements, and dependencies](#workloads_requirements_and_dependencies) section of this document and adds information about the target GKE Enterprise resources that you can use them in.\n| Source OpenShift resource manifest      | Most important specs and parameters                                            | Target GKE Enterprise resource manifest |\n|:---------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------|\n| Deployment, DeploymentConfig, StatefulSet, Job, cron job | Container image and repository, container port, number of Pod replicas, ConfigMaps, Secrets, PersistentVolumeClaims, resource requests and limits, update strategy, StatefulSet Service Name, cron job schedule | Deployment, StatefulSet, Job, cron job |\n| ImageStream            | Container image, image pull policy, container image repository                                     | Deployment        |\n| Horizontal Pod Autoscaler        | Autoscale criteria                                                | Horizontal Pod Autoscaler     |\n| Service             | Hostname used to connect to the application from inside the cluster, IP address and port on which the Service is exposed, endpoints created for external resources            | Service         |\n| Route             | Hostname and resource path used to connect to the application from outside the cluster, routing rules                            | Ingress         |\nTo automatically build and deploy your workloads, you design and implement build and deployment processes, or adapt the existing ones to support your new environment. If you need to deploy your workloads in a hybrid environment, your deployment processes must support both GKE on Google Cloud and GKE on VMware.\nTo implement your build and deployment processes, you can use [Cloud Build](/build) . If you want to [automate your build and deployment processes](/build/docs/deploying-builds/deploy-gke#automating_deployments) , you can configure [build triggers](/build/docs/automating-builds/create-manage-triggers) or [GitHub App triggers](/build/docs/automating-builds/create-github-app-triggers) , or [set up automated deployments from Google Cloud console](/kubernetes-engine/docs/how-to/automated-deployment) . If you are using any policy controller constraints, you can [check your Kubernetes and GKE Enterprise descriptors against policies in your Cloud Build jobs](/anthos-config-management/docs/how-to/policy-agent-ci-pipeline) in order to provide feedback to developers.\nIf you need to run build jobs or store source code on-premises, you might use [GitLab](https://about.gitlab.com/stages-devops-lifecycle/source-code-management/) . GitLab offers source code repositories and a collaboration platform, CI/CD capabilities, and a container image registry. You might deploy GitLab on your GKE Enterprise clusters directly from the [Cloud Marketplace](https://console.cloud.google.com/marketplace/details/gitlab-public/gitlab) , or use one of the other available [installation options](https://about.gitlab.com/install/) .\nIf you are currently using one of the OpenShift facilities to build or automatically deploy your workloads you can adopt one of the following strategies, based on your current process:\n- **Jenkins pipelines.** If you're using Jenkins pipelines to automate your build and deployment process, you can port your pipeline to Cloud Build, use your existing Jenkins environment, or deploy [Jenkins in Google Cloud](/architecture/jenkins-on-kubernetes-engine-tutorial) .\n- **Builds and deployments from a Dockerfile and the required artifacts.** You can use Cloud Build to [build container images](/build/docs/building/build-containers) with a [Dockerfile](/build/docs/building/build-containers#build_using_dockerfile) or a [build configuration file](/build/docs/building/build-containers#build_using_a_build_config_file) . If you want to execute your builds on an on-premises cluster, you can use GitLab.\n- **Source-to-image builds.** In Cloud Build, you must implement a preliminary step to build the artifacts that the resulting container image requires. If your source-to-image job builds a Python app and produces a container image, you need to [configure a custom build step](/build/docs/create-custom-build-steps) to build the Python app, and then [build the container image](/build/docs/building/build-containers) . This approach also requires that you provide a Dockerfile, or if you don't want to provide one, you can use [Google Cloud's buildpacks](/docs/buildpacks/overview) or [Jib](https://github.com/GoogleContainerTools/jib) for Java applications.\n- **Custom builds.** You can [create custom Cloud Build builders](/build/docs/cloud-builders#writing_your_own_custom_builder) like you're doing now in OpenShift. If your custom builders are not using any OpenShift-specific features, you might be able to use them as they are in Cloud Build.\nWhatever approach you choose to build your container images, you need to store them in a container image repository. You have the following different options:\n- **Keep your existing container image repository.** If you're using an external container image repository that's not running on OpenShift, and you're not yet ready to migrate,you can continue using that repository to store your container images.\n- **Container Registry.** If you prefer a fully managed service, you can use [Container Registry](/container-registry) to store your container images. If you need additional security layers, you can [manage the Container Registry encryption keys by yourself](/container-registry/docs/using-encryption-keys) , configure a [secure perimeter to access Container Registry](/container-registry/docs/securing-with-vpc-sc) , [enhance the security of your software supply chain](/container-registry/docs/container-best-practices) , and [scan your container images for known vulnerabilities](/container-registry/docs/get-image-vulnerabilities) with [Artifact Analysis](/container-registry/docs/container-analysis) . Container Registry also supports [managed base images](/artifact-registry/docs/docker/manage-images) that are maintained by Google, as a base for your container images.\n- **On-premises repository.** If you need to migrate away from your current repository because it's hosted on OpenShift, and you need to store your container images on-premises, you can choose the [registry provided with GitLab](https://docs.gitlab.com/ee/user/packages/container_registry/) .\n- **Hybrid approach.** You can combine the previous options to benefit from the strengths of each one. For example, you can use Container Registry as your main repository, and mirror that to your on-premises repository. In this case, you use Container Registry features, and still benefit from having an on-premises repository.\nRegardless of your choice to store container images, you need to provision and configure credentials for your clusters to access the container image repository.\nIf you need to send notifications about the status of your builds and your container images to users or third-party services, you can use [Cloud Functions](/functions) to respond to events produced by [Cloud Build](/build/docs/configure-third-party-notifications) and [Container Registry](/container-registry/docs/configuring-notifications) .\n### Summary of OpenShift to GKE Enterprise capability mapping\nThe following table is a summary of how to map GKE Enterprise capabilities to the ones that you used on OpenShift.\n| OpenShift        | GKE Enterprise                                                |\n|:---------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| OpenShift projects      | Kubernetes namespaces that are centrally managed by Config Sync Kubernetes RBAC authorization that are centrally managed by Config Sync Kubernetes resource quotas that are centrally managed by Config Sync |\n| OpenShift SDN and network isolation | Calico network security policies that are centrally managed by Config Sync                                 |\n| OpenShift Security Context Constraints | Policy Controller constraints                                            |\n| OpenShift Pipelines     | Cloud Build Jenkins integration using the Google Cloud Jenkins plugin GitLab                                 |\n| OpenShift Source-to-Image    | Cloud Native Buildpacks Jib                                             |\n| Identity integration     | Cloud Identity Google Cloud Directory Sync GKE on VMware OIDC integration                                 |\n## Optimizing your environment\nOptimization is the last phase of your migration. In this phase, you make  your environment more efficient than it was before. In this phase, you  execute multiple iterations of a repeatable loop until your environment  meets your optimization requirements. The steps of this repeatable loop are  as follows:\n- Assessing your current environment, teams, and optimization loop.\n- Establishing your optimization requirements and goals.\n- Optimizing your environment and your teams.\n- Tuning the optimization loop.\nThe following sections rely on [Migration to Google Cloud: Optimizing your environment](/solutions/migration-to-google-cloud-optimizing-your-environment) .\n### Assess your current environment, teams, and optimization loop\nWhile the [first assessment](#assessing_and_discovering_your_workloads) focuses on the migration from your current environment to GKE Enterprise, this assessment is tailored for the optimization phase.\n### Establish your optimization requirements\nFor GKE on Google Cloud optimization requirements, review the optimization requirements established in [Optimizing your environment](/solutions/migrating-containers-kubernetes-gke#optimizing_your_environment) .\nReview the following [optimization requirements](/solutions/migration-to-google-cloud-optimizing-your-environment#establishing_your_optimization_requirements_and_goals) for your GKE Enterprise environment:\n- **Start deploying workloads in a serverless environment.** If you need to reduce the strain on your operations teams, you can start using fully managed serverless platforms such as [Cloud Run](/run) and [Cloud Run for Anthos](/anthos/run) .\n- **Modernize your deployment processes** . [Migration to Google Cloud: Deploying your workloads](/solutions/migration-to-gcp-deploying-your-workloads#deploy_automatically) describes typical end-to-end deployment processes and how to modernize your existing processes. If you want to modernize your existing deployment processes, or want to design new ones, refer to [Migration to Google Cloud: Migrating from manual deployments to automated, containerized deployments](/solutions/migration-to-google-cloud-automated-containerized-deployments) for guidance.\n- **Deploy with Spinnaker.** If you need to implement deployment logic, such as [canary deployments](https://martinfowler.com/bliki/CanaryRelease.html) and [blue/green deployments](https://martinfowler.com/bliki/BlueGreenDeployment.html) to increase the reliability of your environment and reduce the impact for your users, you can use [Spinnaker](https://www.spinnaker.io/) . To use Spinnaker on Google Cloud, you need to [install it](/docs/ci-cd/spinnaker/spinnaker-for-gcp#install_and_use_spinnaker_google_cloud_platform) . After that, you implement your deployment processes with Spinnaker. For example, you can [register your existing GKE clusters in Spinnaker](https://www.spinnaker.io/setup/install/providers/kubernetes-v2/) , [enable Kustomize support for Spinnaker](https://www.spinnaker.io/guides/user/kubernetes-v2/kustomize-manifests/) , or implement continuous delivery pipelines with Spinnaker and GKE.\n- **Implement a secure software supply-chain.** For security-critical workloads, you can [implement a secure software supply chain](/software-supply-chain-security/docs/sds/overview) in your GKE Enterprise clusters by using [Binary Authorization](/binary-authorization/docs) .\n- **Switch to Anthos Service Mesh.** If you're already using [OpenShift Service Mesh](https://docs.openshift.com/container-platform/4.6/service_mesh/v1x/servicemesh-release-notes.html) or you are looking for the traffic management, observability, and security capabilities that a service mesh provides, you can adopt [Anthos Service Mesh](/service-mesh/docs/overview) . Anthos Service Mesh provides an GKE Enterprise tested and supported distribution of Istio, together with Google-managed backend capabilities for [observability](/service-mesh/docs/overview#observability_features) , [mTLS certificate management](/service-mesh/docs/overview#security_features) , and integration with [Identity Aware Proxy (IAP)](/service-mesh/docs/iap-integration) .\n### Complete the optimization\nAfter populating the list of your optimization requirements, you complete the rest of the activities of the optimization phase in [Migration to Google Cloud: Optimizing your environment](/solutions/migration-to-google-cloud-optimizing-your-environment) .\n## Finding help\nGoogle Cloud offers various options and resources for you to find the necessary help and support to best use Google Cloud services:\n- [Self-service resources](/solutions/migration-to-gcp-getting-started#self-service_resources) . If you don't need dedicated support, you have various options that you can use at your own pace.\n- [Technology partners](https://cloud.google.com/find-a-partner/?search=anthos) . Google Cloud has partnered with multiple companies to help you use our products and services.\n- [Google Cloud professional services](/consulting) . Our professional services can help you get the most out of your investment in Google Cloud.\nThere are more resources to help migrate workloads to Google Cloud in the [Google Cloud Migration Center](/migration-center) .\nFor more information about these resources, see the [finding help](/solutions/migration-to-gcp-getting-started#finding_help) section of [Migration to Google Cloud: Getting started](/solutions/migration-to-gcp-getting-started) .\n## What's next\n- [Migration to Google Cloud: Getting started](/solutions/migration-to-gcp-getting-started) .\n- Learn more about [GKE Enterprise](/anthos) and [Migrate to Containers](/velostrata/docs/anthos-migrate) .\n- Read how you can [support your migration with Istio mesh expansion](/solutions/supporting-your-migration-with-istio-mesh-expansion-concept) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Docs"}