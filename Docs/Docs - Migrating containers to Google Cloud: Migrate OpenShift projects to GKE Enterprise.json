{"title": "Docs - Migrating containers to Google Cloud: Migrate OpenShift projects to GKE Enterprise", "url": "https://cloud.google.com/architecture/migrating-containers-openshift-anthos-projects", "abstract": "# Docs - Migrating containers to Google Cloud: Migrate OpenShift projects to GKE Enterprise\nLast reviewed 2022-01-24 UTC\nThis document helps you to plan, design, and implement migration of your projects from [OpenShift](https://www.openshift.com/) to [GKE Enterprise](/anthos) . If done incorrectly, moving your workloads from one environment to another can be a challenging task, so plan and execute your migration carefully.\nThis document is part of a multi-part series about migrating to Google Cloud. If you're interested in an overview of the series, see [Migration to Google Cloud: Choosing your migration path](/architecture/migration-to-gcp-choosing-your-path) .\nThis document is part of a series that discusses migrating [containers](https://wikipedia.org/wiki/OS-level_virtualization) to Google Cloud:\n- [Migrating containers to Google Cloud: Migrating Kubernetes to Google Kubernetes Engine (GKE)](/architecture/migrating-containers-kubernetes-gke) \n- [Migrating containers to Google Cloud: Migrating from OpenShift to GKE Enterprise](/architecture/migrating-containers-openshift-anthos) \n- Migrating containers to Google Cloud: Migrate OpenShift projects to GKE Enterprise (this document)\n- [Migrating from OpenShift to GKE Enterprise: Migrate OpenShift SCCs to Policy Controller Constraints](/architecture/migrating-containers-openshift-anthos-scc) \nThis document is useful if you're planning to migrate [OpenShift projects](https://docs.openshift.com/container-platform/latest/applications/projects/working-with-projects.html) to [GKE Enterprise](/anthos) . This document is also useful if you're evaluating the opportunity to migrate and want to explore what it might look like.\nThis document relies on concepts that are discussed in [Migration to Google Cloud: Getting started](/architecture/migration-to-gcp-getting-started) , in [Migrating containers to Google Cloud: Migrating Kubernetes to GKE](/architecture/migrating-containers-kubernetes-gke) , in [Migrating containers to Google Cloud: Migrating from OpenShift to GKE Enterprise](/architecture/migrating-containers-openshift-anthos) , and in [Best practices for GKE networking](/kubernetes-engine/docs/best-practices/networking) . This document links to the preceding documents where appropriate.\nThe guidance in this document assumes that you want to execute a [lift and shift migration](/architecture/migration-to-gcp-getting-started#lift_and_shift) of your workloads. In a lift and shift migration, you apply only the minimum changes that you need for your workloads to operate in the target GKE Enterprise environment.\nTo migrate OpenShift resources to GKE Enterprise, you map and convert them to their Kubernetes equivalents. This document describes the migration of the following OpenShift project configuration resources necessary to deploy and operate your workloads to GKE Enterprise:\n- [OpenShift projects](https://docs.openshift.com/container-platform/latest/applications/projects/working-with-projects.html) \n- [Resource quotas for each OpenShift project](https://docs.openshift.com/container-platform/latest/applications/quotas/quotas-setting-per-project.html) and [resource quotas across multiple OpenShift projects](https://docs.openshift.com/container-platform/latest/applications/quotas/quotas-setting-across-multiple-projects.html) \n- [Roles](https://docs.openshift.com/container-platform/latest/authentication/using-rbac.html#viewing-local-roles_using-rbac) and [ClusterRoles](https://docs.openshift.com/container-platform/latest/authentication/using-rbac.html#viewing-cluster-roles_using-rbac) \n- [RoleBindings](https://docs.openshift.com/container-platform/latest/authentication/using-rbac.html#viewing-local-roles_using-rbac) and [ClusterRoleBindings](https://docs.openshift.com/container-platform/latest/authentication/using-rbac.html#viewing-cluster-roles_using-rbac) \n- [OpenShift network namespaces configuration](https://docs.openshift.com/container-platform/latest/networking/openshift_sdn/multitenant-isolation.html) and [NetworkPolicies](https://docs.openshift.com/container-platform/latest/networking/network_policy/about-network-policy.html) \nTo migrate OpenShift project configuration and related resources to GKE Enterprise, we recommend that you do the following:\n- Export OpenShift project configuration resource descriptors.\n- Map the OpenShift project configuration resources to Kubernetes resources.\n- Create Kubernetes resources that map to OpenShift project configuration resources.\n- Manage the Kubernetes resources using [Config Sync](/anthos-config-management/docs/config-sync-overview) .\nThis document provides examples of how you can complete the migration steps.\n", "content": "## Export OpenShift project configuration resource descriptors\nTo export the OpenShift project configuration resources, we recommend that you do the following:\n- Export OpenShift project descriptors.\n- Export cluster-scoped resource descriptors.\n- Export project-scoped resource descriptors.\nThe descriptors that you export from an OpenShift cluster include [fields that describe the configuration and the status of resources](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status) , such as the `spec` and `status` fields. The descriptors also include [fields that hold resource status information](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata) , such as the `metadata.managedFields` field. Kubernetes and OpenShift manage the fields that hold resource status information and their values for you. To simplify the assessment of OpenShift resource descriptors, we recommend that you do the following for each resource descriptor:\n- Record the fields that hold dynamically generated resource status information along with their values, such as the following:- Any field nested under`metadata.annotations`that starts with the`openshift.io`prefix\n- `metadata.creationTimestamp`\n- `metadata.generation`\n- `metadata.managedFields`\n- `metadata.resourceVersion`\n- `metadata.selfLink`\n- `metadata.uid`\n- `status`\n- Remove the fields that hold dynamically generated resource status information from the resource descriptor.\nTo export OpenShift project configuration resource descriptors, you use the [OpenShift command-line interface (oc CLI)](https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/getting-started-cli.html) . To export resource descriptors in `oc` CLI, you need to authenticate with the [cluster-admin role](https://docs.openshift.com/container-platform/latest/authentication/using-rbac.html#default-roles_using-rbac) . For a list of all the OpenShift resources that the `oc` CLI supports, run the [oc api-resources](https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/developer-cli-commands.html#api-resources) command.\n### Export OpenShift project descriptors\nThis section describes how to export project descriptors. We recommend that you exclude OpenShift projects that run system components, such as the `istio-system` component, and exclude OpenShift projects that have names starting with `openshift-` , `kube-` , or `knative-` . [OpenShift manages](https://docs.openshift.com/container-platform/latest/applications/projects/working-with-projects.html#creating-a-project-using-the-CLI_projects) these OpenShift projects for you, and they're out of the scope of this migration because you don't use them to deploy your workloads. To export OpenShift project descriptors, do the following for each OpenShift cluster:\n- In a terminal that has access to the OpenShift cluster, get the list of [OpenShift projects](https://docs.openshift.com/container-platform/latest/rest_api/project_apis/project-project-openshift-io-v1.html) by using the [oc get](https://docs.openshift.com/container-platform/latest/cli_reference/openshift_cli/developer-cli-commands.html#get) command:```\noc get projects\n```The output is similar to the following:```\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 DISPLAY NAME \u00a0 \u00a0 \u00a0STATUSexample-project \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Active...\n```The output displays a list of the OpenShift projects that are currently set up in your OpenShift cluster.\n- For each OpenShift project in the list, export its descriptor in [YAML](https://yaml.org/) file format, display the output, and save it to a file using the `tee` command, for later processing. For example, export the descriptor of an `example-project` OpenShift project:```\noc get project example-project -o yaml | tee project-example-project.yaml\n```The output is similar to the following:```\napiVersion: project.openshift.io/v1kind: Projectmetadata:\u00a0 annotations:\u00a0 name: example-projectspec:\u00a0 finalizers:\u00a0 - kubernetes\n```The output displays the descriptor of the `example-project` OpenShift project in YAML file format. The output is saved to the `project-example-project.yaml` file.\n### Export cluster-scoped resource descriptors\nThis section describes how to export the descriptors for resources that have a cluster scope, not including security context constraints. For information about migrating security policies, see [Migrating from OpenShift to GKE Enterprise: Migrate OpenShift SCCs to Policy Controller Constraints](/architecture/migrating-containers-openshift-anthos-scc) . To export other resource descriptors, do the following for each OpenShift cluster:\n- In your terminal, get the list of [ClusterResourceQuotas](https://docs.openshift.com/container-platform/latest/rest_api/schedule_and_quota_apis/clusterresourcequota-quota-openshift-io-v1.html) :```\noc get clusterresourcequotas\n```The output is similar to the following:```\nNAME \u00a0 \u00a0 \u00a0 AGEfor-name \u00a0 6m15sfor-user \u00a0 4s...\n```The output displays a list of ClusterResourceQuotas that are currently set up in your OpenShift cluster.\n- For each ClusterResourceQuota in the list, export its descriptor in YAML file format, display the output, and save it to a file for later processing. For example, export the descriptor of the `for-name` ClusterResourceQuota:```\noc get clusterresourcequota for-name -o yaml | tee clusterresourcequota-for-name.yaml\n```The output is similar to the following:```\napiVersion: quota.openshift.io/v1kind: ClusterResourceQuotametadata:\u00a0 name: for-namespec:\u00a0 quota:\u00a0 \u00a0 hard:\u00a0 \u00a0 \u00a0 pods: \"10\"\u00a0 \u00a0 \u00a0 secrets: \"20\"\u00a0 selector:\u00a0 \u00a0 annotations: null\u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 \u00a0 name: frontend\n```The output displays the descriptor of the `for-name` ClusterResourceQuota in YAML format. The output is saved to the `clusterresourcequota-for-name.yaml` file.\n- Get the list of [ClusterRoles](https://docs.openshift.com/container-platform/latest/rest_api/role_apis/clusterrole-authorization-openshift-io-v1.html) :```\noc get clusterroles\n```The output is similar to the following:```\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CREATED ATadmin \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a02021-02-02T06:17:02Zaggregate-olm-edit \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 2021-02-02T06:17:59Zaggregate-olm-view \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 2021-02-02T06:18:01Zalertmanager-main \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a02021-02-02T06:48:26Zbasic-user \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 2021-02-02T06:26:42Z...\n```The output displays a list of ClusterRoles that are currently set up in your OpenShift cluster. The list of ClusterRoles includes [OpenShift default ClusterRoles](https://docs.openshift.com/container-platform/latest/authentication/using-rbac.html#default-roles_using-rbac) , and ClusterRoles that refer to OpenShift system components. We recommend that you assess all the ClusterRoles in the list to evaluate which roles you need to migrate, and which roles aren't applicable in the target GKE Enterprise environment.\n- For each ClusterRole in the list, export its descriptor in YAML file format, display the output, and save it to a file for later processing. For example, export the descriptor of the `admin` ClusterRole:```\noc get clusterrole admin -o yaml | tee clusterrole-admin.yaml\n```The output is similar to the following:```\naggregationRule:\u00a0 clusterRoleSelectors:\u00a0 - matchLabels:\u00a0 \u00a0 \u00a0 rbac.authorization.k8s.io/aggregate-to-admin: \"true\"apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata:\u00a0 annotations:\u00a0 \u00a0 rbac.authorization.kubernetes.io/autoupdate: \"true\"\u00a0 labels:\u00a0 \u00a0 kubernetes.io/bootstrapping: rbac-defaults\u00a0 name: adminrules:- apiGroups:\u00a0 - operators.coreos.com\u00a0 resources:\u00a0 - subscriptions\u00a0 verbs:\u00a0 - create\u00a0 - update\u00a0 - patch\u00a0 - delete...\n```The output displays the descriptor of the `admin` ClusterRole in YAML format. The output is saved to the `clusterrole-admin.yaml` file.\n- Get the list of [ClusterRoleBindings](https://docs.openshift.com/container-platform/latest/rest_api/role_apis/clusterrolebinding-authorization-openshift-io-v1.html) :```\noc get clusterrolebindings\n```The output is similar to the following:```\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0ROLE \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AGEalertmanager-main \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterRole/alertmanager-main \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a021dbasic-users \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterRole/basic-user \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 21dcloud-credential-operator-rolebinding \u00a0 \u00a0 \u00a0 ClusterRole/cloud-credential-operator-role \u00a0 \u00a0 \u00a0 21dcluster-admin \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterRole/cluster-admin \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a021dcluster-admins \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0ClusterRole/cluster-admin \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a021dcluster-autoscaler \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0ClusterRole/cluster-autoscaler \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 21dcluster-autoscaler-operator \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterRole/cluster-autoscaler-operator \u00a0 \u00a0 \u00a0 \u00a0 \u00a021dcluster-monitoring-operator \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterRole/cluster-monitoring-operator \u00a0 \u00a0 \u00a0 \u00a0 \u00a021d...\n```The output displays a list of ClusterRoleBindings that are currently set up in your OpenShift cluster. The list of ClusterRoleBindings includes ClusterRoleBindings that refer to OpenShift system components. We recommend that you assess all the ClusterRoleBindings in the list to evaluate which bindings you need to migrate, and which bindings aren't applicable in the target GKE Enterprise environment.\n- For each ClusterRoleBinding in the list, export its descriptor in YAML file format, display the output, and save it to a file for later processing. For example, export the descriptor of the `cluster-admin` ClusterRoleBinding:```\noc get clusterrolebinding cluster-admin -o yaml | tee clusterrolebinding-cluster-admin.yaml\n```The output is similar to the following:```\napiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata:\u00a0 annotations:\u00a0 \u00a0 rbac.authorization.kubernetes.io/autoupdate: \"true\"\u00a0 labels:\u00a0 \u00a0 kubernetes.io/bootstrapping: rbac-defaults\u00a0 name: cluster-adminroleRef:\u00a0 apiGroup: rbac.authorization.k8s.io\u00a0 kind: ClusterRole\u00a0 name: cluster-adminsubjects:- apiGroup: rbac.authorization.k8s.io\u00a0 kind: Group\u00a0 name: system:masters\n```The output displays the descriptor of the `cluster-admin` ClusterRoleBinding in YAML format. The output is saved to the `clusterrolebinding-cluster-admin.yaml` file.This section describes how to assess the configuration of [multi-tenant isolation](https://docs.openshift.com/container-platform/latest/networking/openshift_sdn/multitenant-isolation.html) . This section applies if you created customized [NetNamespaces](https://docs.openshift.com/container-platform/latest/rest_api/network_apis/netnamespace-network-openshift-io-v1.html) for any OpenShift project in your cluster to isolate or join network namespaces. If you didn't create customized NetNamespaces, skip to [Export project-scoped resource descriptors](#export-project-scoped-resource-descriptors) .\nOpenShift automatically creates and manages NetNamespaces for managed OpenShift projects. NetNamespaces for OpenShift-managed projects are out of the scope of this migration.\nTo export customized NetNamespaces, do the following:\n- Get the list of NetNamespaces:```\noc get netnamespaces\n```The output is similar to the following.```\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0NETID \u00a0 \u00a0 \u00a0EGRESS IPSdefault \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 0kube-node-lease \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 13240579kube-public \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 15463168kube-system \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 16247265openshift \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 9631477openshift-apiserver \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 12186643openshift-apiserver-operator \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a06097417openshift-authentication \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a02862939openshift-authentication-operator \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 723750openshift-cloud-credential-operator \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 11544971openshift-cluster-csi-drivers \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 7650297openshift-cluster-machine-approver \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a07836750openshift-cluster-node-tuning-operator \u00a0 \u00a0 \u00a0 \u00a07531826...\n```The output displays a list of NetNamespaces that are currently set up in your OpenShift cluster.\n- For each NetNamespace in the list, export its descriptor in YAML file format, display the output, and save it to a file for later processing. For example, export the descriptor of the `default` `NetNamespace` :```\noc get netnamespace example-project -o yaml | tee netnamespace-example-project.yaml\n```For NetNamespaces that don't have the same `netid` value, the output is similar to the following:```\napiVersion: network.openshift.io/v1kind: NetNamespacemetadata:\u00a0 name: example-projectnetid: 1234netname: example-project\n```The output displays the descriptor of the `example-project` NetNamespace in YAML file format. The output is saved to the `netnamespace-example-project.yaml` file.For NetNamespaces that have the same `netid` value, the output is similar to the following:```\napiVersion: network.openshift.io/v1kind: NetNamespacemetadata:\u00a0 name: example-projectnetid: 1234netname: example-projectapiVersion: network.openshift.io/v1kind: NetNamespacemetadata:\u00a0 name: example-project-2netid: 1234netname: example-project-2\n```\n### Export project-scoped resource descriptors\nTo export the descriptors for resources that have a project scope, do the following for each OpenShift project.\n- In your terminal, select the OpenShift project that you want to assess. For example, select the `example-project` OpenShift project:```\noc project example-project\n```\n- Get the list of [ResourceQuotas](https://docs.openshift.com/container-platform/latest/rest_api/schedule_and_quota_apis/resourcequota-v1.html) :```\noc get resourcequotas\n```The output is similar to the following:```\nNAME \u00a0 \u00a0 \u00a0 \u00a0AGE \u00a0 REQUEST \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0LIMITgpu-quota \u00a0 6s \u00a0 \u00a0requests.nvidia.com/gpu: 1/1...\n```The output displays a list of `ResourceQuotas` that are currently set up in your OpenShift cluster for the selected OpenShift project.\n- For each ResourceQuota in the list, export its descriptor in YAML format, display the output, and save it to a file for later processing. For example, export the descriptor of the `gpu-quota` ResourceQuota:```\noc get resourcequota gpu-quota -o yaml | tee resourcequota-gpu-quota.yaml\n```The output is similar to the following:```\napiVersion: v1kind: ResourceQuotametadata:\u00a0 name: gpu-quota\u00a0 namespace: example-projectspec:\u00a0 hard:\u00a0 \u00a0 requests.nvidia.com/gpu: \"1\"\n```The output displays the descriptor of the `gpu-quota` ResourceQuota in YAML file format. The output is saved to the `resourcequota-gpu-quota.yaml` file.\n- Get the list of [Roles](https://docs.openshift.com/container-platform/latest/rest_api/role_apis/role-authorization-openshift-io-v1.html) :```\noc get roles\n```The output is similar to the following:```\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CREATED ATexample \u00a0 \u00a0 \u00a0 \u00a0 \u00a02021-02-02T06:48:27Z...\n```The output displays a list of Roles that are currently set up in your OpenShift cluster for the selected OpenShift project. The list of Roles includes Roles that refer to OpenShift system components. We recommend that you assess all the Roles in the list to evaluate which roles you need to migrate, and which roles aren't applicable in the target GKE Enterprise environment.\n- For each Role in the list, export its descriptor in YAML file format, display the output, and save it to a file for later processing. For example, export the descriptor of the `example` Role:```\noc get role example -o yaml | tee role-example.yaml\n```The output is similar to the following:```\napiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata:\u00a0 name: example\u00a0 namespace: example-projectrules:- apiGroups:\u00a0 - \"\"\u00a0 resources:\u00a0 - services\u00a0 - endpoints\u00a0 - pods\u00a0 verbs:\u00a0 - get\u00a0 - list\u00a0 - watch\n```The output displays the descriptor of the `example` Role in YAML file format. The output is saved to the `role-example.yaml` file.\n- Get the list of [RoleBindings](https://docs.openshift.com/container-platform/latest/rest_api/role_apis/rolebinding-authorization-openshift-io-v1.html) :```\noc get rolebindings\n```The output is similar to the following:```\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ROLE \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AGEmachine-config-controller-events \u00a0 ClusterRole/machine-config-controller-events \u00a0 21dmachine-config-daemon-events \u00a0 \u00a0 \u00a0 ClusterRole/machine-config-daemon-events \u00a0 \u00a0 \u00a0 21dexample \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Role/example \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 21dsystem:deployers \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterRole/system:deployer \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a021dsystem:image-builders \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0ClusterRole/system:image-builder \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 21dsystem:image-pullers \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterRole/system:image-puller \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a021d...\n```The output displays a list of RoleBindings that are set up in your OpenShift cluster for the selected OpenShift project. The list of RoleBindings includes RoleBindings that refer to OpenShift system components. We recommend that you assess all the RoleBindings in the list to evaluate which bindings you need to migrate, and which bindings aren't applicable in the target GKE Enterprise environment.\n- For each RoleBinding in the list, export its descriptor in YAML file format, display the output, and save it to a file for later processing. For example, export the descriptor of the `example` RoleBinding:```\noc get rolebinding example -o yaml | tee rolebinding-example.yaml\n```The output is similar to the following:```\napiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata:\u00a0 name: example\u00a0 namespace: example-projectroleRef:\u00a0 apiGroup: rbac.authorization.k8s.io\u00a0 kind: Role\u00a0 name: examplesubjects:- kind: ServiceAccount\u00a0 name: example\u00a0 namespace: example-ns\n```The output displays the descriptor of the `example` RoleBinding in YAML file format. The output is saved to the `rolebinding-example.yaml` file.\n- Get the list of [EgressNetworkPolicies](https://docs.openshift.com/container-platform/latest/rest_api/network_apis/egressnetworkpolicy-network-openshift-io-v1.html) :```\noc get egressnetworkpolicies\n```The output is similar to the following:```\nNAME \u00a0 \u00a0 \u00a0AGEdefault \u00a0 2m2s...\n```The output displays a list of EgressNetworkPolicies that are currently set up in your OpenShift cluster for the selected OpenShift project.\n- For each EgressNetworkPolicy in the list, export its descriptor in YAML file format, display the output, and save it to a file for later processing. For example, export the descriptor of the `default` EgressNetworkPolicy:```\noc get egressnetworkpolicy default -o yaml | tee egressnetworkpolicy-default.yaml\n```The output is similar to the following:```\napiVersion: network.openshift.io/v1kind: EgressNetworkPolicymetadata:\u00a0 name: default\u00a0 namespace: example-projectspec:\u00a0 egress:\u00a0 - to:\u00a0 \u00a0 \u00a0 cidrSelector: 1.2.3.0/24\u00a0 \u00a0 type: Allow\u00a0 - to:\u00a0 \u00a0 \u00a0 dnsName: www.example.com\u00a0 \u00a0 type: Allow\u00a0 - to:\u00a0 \u00a0 \u00a0 cidrSelector: 0.0.0.0/0\u00a0 \u00a0 type: Deny\n```The output displays the descriptor of the `default` EgressNetworkPolicy in YAML format. The output is also saved to the `egressnetworkpolicy-default.yaml` file.\n- Get the list of [NetworkPolicies](https://docs.openshift.com/container-platform/latest/rest_api/network_apis/networkpolicy-networking-k8s-io-v1.html) :```\noc get networkpolicies\n```The output is similar to the following:```\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0POD-SELECTOR \u00a0 AGEtest-policy \u00a0 app=mongodb \u00a0 \u00a03s...\n```The output displays a list of NetworkPolicies that are currently set up in your OpenShift cluster for the selected OpenShift project.\n- For each NetworkPolicy in the list, export its descriptor in YAML file format, display the output, and save it to a file for later processing. For example, export the descriptor of the `test-policy` NetworkPolicy:```\noc get networkpolicy test-policy -o yaml | tee networkpolicy-test-policy.yaml\n```The output is similar to the following:```\napiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata:\u00a0 name: test-policy\u00a0 namespace: example-projectspec:\u00a0 ingress:\u00a0 - from:\u00a0 \u00a0 - podSelector:\u00a0 \u00a0 \u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 app: app\u00a0 \u00a0 ports:\u00a0 \u00a0 - port: 27017\u00a0 \u00a0 \u00a0 protocol: TCP\u00a0 podSelector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: mongodb\u00a0 policyTypes:\u00a0 - Ingress\n```The output displays the descriptor of the `test-policy` NetworkPolicy in YAML file format. The output is saved to the `networkpolicy-test-policy.yaml` file.## Map OpenShift project configuration resources to Kubernetes resources\nAfter you complete the inventory of the OpenShift project configuration resources, you assess those resources as follows:\n- Evaluate which resources in the inventory are Kubernetes resources and which are OpenShift resources.\n- Map OpenShift resources to their Kubernetes, GKE, and GKE Enterprise equivalents.\nThe following list helps you to evaluate which resources that you provisioned in your OpenShift clusters are Kubernetes resources and which resources are available in OpenShift only:\n- OpenShift projects are [Kubernetes Namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) with additional annotations.\n- Roles, ClusterRoles, RoleBindings, and ClusterRoleBindings are [Kubernetes resources](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) .\n- ResourceQuotas are [Kubernetes resources](https://kubernetes.io/docs/concepts/policy/resource-quotas/) .\n- NetworkPolicies are [Kubernetes resources](https://kubernetes.io/docs/concepts/services-networking/network-policies/) .\n- ClusterResourceQuotas aren't Kubernetes resources; they are available in OpenShift only.\n- NetNamespaces and EgressNetworkPolicies aren't Kubernetes resources; they are available in OpenShift only.\nThe following table provides a summary of how to map OpenShift project configuration resources to the resources that you used in GKE Enterprise.\n| OpenShift             | GKE Enterprise             |\n|:-----------------------------------------------------------|:---------------------------------------------------------------|\n| Projects             | Convert to Kubernetes Namespaces with additional annotations |\n| Roles, ClusterRoles, RoleBindings, and ClusterRoleBindings | Kubernetes RBAC resources          |\n| ResourceQuotas            | Kubernetes RBAC resources          |\n| ClusterResourceQuotas          | Convert to ResourceQuotas or use hierarchical resource quotas |\n| NetworkPolicies           | Kubernetes Network resources         |\n| NetNamespaces, EgressNetworkPolicies      | Convert to NetworkPolicies          |\nAfter you evaluate the resources from your OpenShift environment, map the OpenShift resources to resources that you can provision and configure in GKE and GKE Enterprise. You don't need to map the Kubernetes resources that you're using in your OpenShift clusters because GKE Enterprise supports them directly. As described in [Summary of OpenShift to GKE Enterprise capability mapping](/architecture/migrating-containers-openshift-anthos#summary_of_openshift_to_anthos_capability_mapping) , we recommend that you map the following:\n- OpenShift projects to Kubernetes Namespaces.\n- ClusterResourceQuotas to ResourceQuotas.\n- NetNamespaces and EgressNetworkPolicies to NetworkPolicies.## Create Kubernetes resources that map to OpenShift project configuration resources\nAfter you complete the [mapping](#map-openshift-project-configuration-resources-to-kubernetes-resources) , you create the Kubernetes resources that you mapped to your OpenShift resources. We recommend that you create the following:\n- One Kubernetes Namespace for each OpenShift project.\n- One ResourceQuota for each Kubernetes Namespace that your ClusterResourceQuotas are limiting.\n- NetworkPolicies to match your NetNamespaces and EgressNetworkPolicies.\n### Create Kubernetes Namespaces\nOpenShift projects are [Kubernetes Namespaces with additional annotations](/architecture/migrating-containers-openshift-anthos#openshift_projects) . The [OpenShift project API](https://docs.openshift.com/container-platform/latest/rest_api/project_apis/project-project-openshift-io-v1.html) closely matches the [Kubernetes Namespace API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#namespace-v1-core) . To migrate your OpenShift projects, we recommend that you create a Kubernetes Namespace for each OpenShift project. The APIs are compatible, so you can create a Kubernetes Namespace from an OpenShift project.\nTo create a Kubernetes Namespace from an OpenShift project, we recommend that you change values in the OpenShift project descriptor to the corresponding Kubernetes Namespace API version for each OpenShift project. To do so, you change the `apiVersion` field value in the OpenShift project descriptor from the OpenShift project object API version and the `kind` field value to the corresponding [Kubernetes Namespace object API version](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#namespace-v1-core) . For example, the OpenShift project that you assessed in the previous section is similar to the following:\n```\napiVersion: project.openshift.io/v1kind: Projectmetadata:\u00a0 annotations:\u00a0 name: defaultspec:\u00a0 finalizers:\u00a0 - kubernetes\n```\nTo migrate the project, change the `apiVersion` field value from `project.openshift.io/v1` to `v1` and change the `kind` field value from `Project` to `Namespace` :\n```\napiVersion: v1kind: NamespaceMetadata:\u00a0 annotations:\u00a0 name: defaultspec:\u00a0 finalizers:\u00a0 - kubernetes\n```\n### Create Kubernetes ResourceQuotas\nOpenShift ClusterResourceQuotas let you [share quotas across multiple OpenShift projects](https://docs.openshift.com/container-platform/latest/applications/quotas/quotas-setting-across-multiple-projects.html) . When you create a ClusterResourceQuota, you define the quota and you define the selector to match OpenShift projects for which you want to enforce the quota. In this section, you migrate your OpenShift ClusterResourceQuotas to Kubernetes ResourceQuotas in the Namespaces that you created earlier.\nTo migrate your ClusterResourceQuotas, we recommend that you do the following, for each ClusterResourceQuota:\n- Map the ClusterResourceQuota to OpenShift projects by assessing the `spec.quota` field and the `spec.selector` field of the ClusterResourceQuota. For example, the `for-name` ClusterResourceQuota that you exported in the previous section looked like the following:```\napiVersion: quota.openshift.io/v1kind: ClusterResourceQuotametadata:\u00a0 name: for-namespec:\u00a0 quota:\u00a0 \u00a0 hard:\u00a0 \u00a0 \u00a0 pods: \"10\"\u00a0 \u00a0 \u00a0 secrets: \"20\"\u00a0 selector:\u00a0 \u00a0 annotations: null\u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 \u00a0 name: frontend\n```The `for-name` ClusterResourceQuota enforces Pod and Secret quota limits. The `spec.selector` field enforces those limits on the `frontend` OpenShift project.\n- In [Create Kubernetes Namespace](#create-kubernetes-namespaces) earlier, you created Kubernetes Namespaces that correspond to your OpenShift projects. Use that information to map the ClusterResourceQuota to the new Kubernetes Namespaces. For example, the `frontend` OpenShift project contains the `for-name` ClusterResourceQuota. The project corresponds to the `frontend` Kubernetes Namespace, so you map the `for-name` ClusterResourceQuota to the `frontend` Kubernetes Namespace.\n- For each quota definition in the `quota` field of the ClusterResourceQuota, divide the quota amount among the Kubernetes Namespaces that you mapped the ClusterResourceQuota to, according to the criteria of interest. For example, you can divide the quota amounts set by the `for-name` ClusterResourceQuota equally among the `frontend` Kubernetes Namespaces.\n- For each Kubernetes Namespace that you mapped to the ClusterResourceQuota, you create a Kubernetes [ResourceQuota](https://kubernetes.io/docs/concepts/policy/resource-quotas/) that enforces the quota on that Namespace. You set the quota amounts according to the information that you gathered in the previous step. For example, you create a ResourceQuota for the `frontend` Kubernetes Namespace:```\napiVersion: v1kind: ResourceQuotametadata:\u00a0 name: pods-secrets\u00a0 namespace: frontendspec:\u00a0 hard:\u00a0 \u00a0 pods: \"10\"\u00a0 \u00a0 secrets: \"20\"\n```As another example, you map the `for-name` ClusterResourceQuota to two distinct Kubernetes Namespaces, `example-1` and `example-2` . To complete the mapping, divide the resources equally between them by creating a ResourceQuota for the Kubernetes Namespaces. Allocate the first half of the ResourceQuota:```\napiVersion: v1kind: ResourceQuotametadata:\u00a0 name: pods-secrets\u00a0 namespace: example-1spec:\u00a0 hard:\u00a0 \u00a0 pods: \"5\"\u00a0 \u00a0 secrets: \"10\"\n```After you allocate the first half of the ResourceQuota, you then allocate the second half of the ResourceQuota:```\napiVersion: v1kind: ResourceQuotametadata:\u00a0 name: pods-secrets\u00a0 namespace: example-2spec:\u00a0 hard:\u00a0 \u00a0 pods: \"5\"\u00a0 \u00a0 secrets: \"10\"\n```This approach lets you enforce limits on each Kubernetes Namespace that you create ResourceQuotas in, rather than setting a single limit for multiple Kubernetes Namespaces.\nUsing ResourceQuotas to enforce quotas on your Kubernetes Namespaces isn't the same as using a ClusterResourceQuota to enforce one quota on all Kubernetes Namespaces. Dividing a cluster-scoped quota among different Kubernetes Namespaces might be suboptimal: the division might over-provision quotas for some Kubernetes Namespaces, and under-provision quotas for other Kubernetes Namespaces.\nWe recommend that you optimize the quota allocation by dynamically tuning the configuration of the ResourceQuotas in your Kubernetes Namespaces, respecting the total amount of quota that the corresponding ClusterResourceQuota established. For example, you can dynamically increase or decrease the quota amounts that are enforced by the `pods-secrets` ResourceQuotas to avoid over-provisioning or under-provisioning quota amounts for the `example-1` and `example-2` Kubernetes Namespaces. The total quota amounts of the `pods-secrets` ResourceQuotas shouldn't exceed the quota amounts in the corresponding ClusterResourceQuota.\nWhen you configure your ResourceQuotas, consider [GKE quotas and limits](/kubernetes-engine/quotas) and [GKE on VMware quotas and limits](/anthos/clusters/docs/on-prem/1.6/quotas) . These quotas and limits might impose lower limits than your ResourceQuotas. For example, GKE limits the [maximum number of Pods per node](/kubernetes-engine/quotas#limits_per_cluster) , regardless of how you configured your ResourceQuotas.\n### Create NetworkPolicies\nOpenShift NetNamespaces let you [configure network isolation between OpenShift projects](https://docs.openshift.com/container-platform/latest/networking/openshift_sdn/multitenant-isolation.html) . OpenShift EgressNetworkPolicies [let you regulate outbound traffic leaving your OpenShift clusters](https://docs.openshift.com/container-platform/latest/networking/openshift_sdn/configuring-egress-firewall.html) . This section relies on traffic restricting concepts.\nTo migrate your NetNamespaces and EgressNetworkPolicies, do the following:\n- Assess your NetNamespaces and EgressNetworkPolicies to understand how they are regulating network traffic between OpenShift projects and outbound traffic that's leaving your OpenShift clusters. For example, assess the NetNamespace and the EgressNetworkPolicy that you exported in the previous section:```\napiVersion: network.openshift.io/v1kind: NetNamespacemetadata:\u00a0 name: example-projectnetid: 1234netname: example-projectapiVersion: network.openshift.io/v1kind: NetNamespacemetadata:\u00a0 name: example-project-2netid: 1234netname: example-project-2apiVersion: network.openshift.io/v1kind: EgressNetworkPolicymetadata:\u00a0 name: default\u00a0 namespace: example-projectspec:\u00a0 egress:\u00a0 - to:\u00a0 \u00a0 \u00a0 cidrSelector: 1.2.3.0/24\u00a0 \u00a0 type: Allow\u00a0 - to:\u00a0 \u00a0 \u00a0 cidrSelector: 0.0.0.0/0\u00a0 \u00a0 type: Deny\n```The `example-project` and the `example-project-2` NetNamespaces define an overlay network with the same `netid` value of `1234` . Therefore, Pods in the `example-project` OpenShift project can communicate with Pods in `example-project-2` OpenShift project and the other way around.The `default` EgressNetworkPolicy defines the following outbound network traffic rules:- Allow outbound traffic to the`1.2.3.0/24`subnet.\n- Deny outbound traffic that doesn't match with other rules.\n- Create NetworkPolicies and OPA policies to match your network traffic restriction requirements. For example, the `default-np` and the `default-np-2` implement the following policies:- The policies that are enforced by the`default`EgressNetworkPolicy.\n- The policies that are enforced by the`example-project`and`example-project-2`NetNamespaces on Namespaces that have the`netid`label set to`example-project`and`example-project-2`:\nThe policies are similar to the following:```\n---kind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata:\u00a0 name: default-np\u00a0 namespace: example-projectspec:\u00a0 policyTypes:\u00a0 - Ingress\u00a0 - Egress\u00a0 podSelector: {}\u00a0 egress:\u00a0 \u00a0 - to:\u00a0 \u00a0 \u00a0 - namespaceSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 netid: example-project-2\u00a0 \u00a0 \u00a0 - podSelector: {}\u00a0 \u00a0 \u00a0 - ipBlock:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cidr: 1.2.3.0/24\u00a0 ingress:\u00a0 \u00a0 - from:\u00a0 \u00a0 \u00a0 - namespaceSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 netname: example-project-2---kind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata:\u00a0 name: default-np-2\u00a0 namespace: example-project-2spec:\u00a0 policyTypes:\u00a0 - Ingress\u00a0 - Egress\u00a0 podSelector: {}\u00a0 egress:\u00a0 \u00a0 - to:\u00a0 \u00a0 \u00a0 - namespaceSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 netid: example-project-2\u00a0 \u00a0 \u00a0 - podSelector: {}\u00a0 \u00a0 \u00a0 - ipBlock:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cidr: 1.2.3.0/24\u00a0 ingress:\u00a0 \u00a0 - from:\u00a0 \u00a0 \u00a0 - namespaceSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 netname: example-project\n```## Manage Kubernetes resources using Config Sync\nTo manage the Kubernetes resources and the configuration of your GKE clusters, we recommend that you use [Config Sync](/anthos-config-management/docs/config-sync-overview) .\nTo learn how to enable Config Sync on your GKE clusters, see [Install Config Sync](/anthos-config-management/docs/how-to/installing-config-sync) .\nAfter you provision and configure Config Sync in your GKE Enterprise environment, you use it to [create and automatically apply configuration to your GKE clusters](/kubernetes-engine/docs/add-on/config-sync/how-to/configs) .\n## What's next\n- Read about how to [get started with your migration to Google Cloud](/architecture/migration-to-gcp-getting-started) .\n- Read about best practices for [building](/architecture/best-practices-for-building-containers) and [operating](/architecture/best-practices-for-operating-containers) containers.\n- Learn [best practices for GKE networking](/kubernetes-engine/docs/best-practices/networking) .\n- Understand how to [harden your cluster's security](/kubernetes-engine/docs/how-to/hardening-your-cluster) .\n- Read the [GKE security overview](/kubernetes-engine/docs/concepts/security-overview) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Docs"}