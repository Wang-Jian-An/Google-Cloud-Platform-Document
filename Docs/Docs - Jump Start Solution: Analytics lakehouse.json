{"title": "Docs - Jump Start Solution: Analytics lakehouse", "url": "https://cloud.google.com/architecture/big-data-analytics/analytics-lakehouse", "abstract": "# Docs - Jump Start Solution: Analytics lakehouse\nLast reviewed 2023-11-20 UTC\nThis guide helps you understand, deploy, and use the [Analytics lakehouse](https://console.cloud.google.com/products/solutions/details/analytics-lakehouse) Jump Start Solution. This solution demonstrates how you can unify data lakes and data warehouses by creating an analytics lakehouse to store, process, analyze, and activate data using a unified data stack.\nCommon use cases for building an analytics lakehouse include the following:\n- Large scale analysis of telemetry data combined with reporting data.\n- Unifying structured and unstructured data analysis.\n- Providing real-time analytics capabilities for a data warehouse.\nThis document is intended for developers who have some background with data analysis and have used a database or data lake to perform an analysis. It assumes that you're familiar with basic cloud concepts, though not necessarily Google Cloud. Experience with Terraform is helpful.\n**Note:** This solution helps you explore the capabilities of Google Cloud. The solution is not intended to be used as is for production environments. For information about designing and setting up production-grade environments in Google Cloud, see [Landing zone design in Google Cloud](/architecture/landing-zones) and [Google Cloud setup checklist](/docs/enterprise/setup-checklist) .\n", "content": "## Objectives\n- Learn how to set up an analytics lakehouse.\n- Secure an analytics lakehouse using a common governance layer.\n- Build dashboards from the data to perform data analysis.\n- Create a machine learning model to predict data values over time.## Products used\nThe solution uses the following Google Cloud products:\n- [BigQuery](/bigquery) : A fully managed, highly scalable data warehouse with built-in machine learning capabilities.\n- [Dataproc](/dataproc) : A fully managed service for data lake modernization, ETL, and secure data science, at scale.\n- [Looker Studio](/looker-studio) : Self-service business intelligence platform that helps you create and share data insights.\n- [Dataplex](/dataplex) : Centrally discover, manage, monitor, and govern data at scale.\n- [Cloud Storage](/storage) : An enterprise-ready service that provides low-cost, no-limit object storage for diverse data types. Data is accessible from within and outside of Google Cloud and is replicated geo-redundantly.\n- [BigLake](/biglake) : BigLake is a storage engine that unifies data warehouses and lakes by enabling BigQuery and open source frameworks like Spark to access data with fine-grained access control.\nThe following Google Cloud products are used to stage data in the solution for first use:\n- [Workflows](/workflows) : A fully managed orchestration platform that executes services in a specified order as a workflow. Workflows can combine services, including custom services hosted on Cloud Run or Cloud Functions, Google Cloud services such as BigQuery, and any HTTP-based API.\n### Architecture\nThe example lakehouse architecture that this solution deploys analyzes an ecommerce dataset to understand a retailer's performance over time. The following diagram shows the architecture of the Google Cloud resources that the solution deploys.### Solution flow\nThe architecture represents a common data flow to populate and transform data in an analytics lakehouse architecture:\n- Data lands in [Cloud Storage](/storage) buckets.\n- A data lake is created in [Dataplex](/dataplex) . Data in the buckets are organized into entities, or tables, in the data lake.\n- Tables in the data lake are immediately available in [BigQuery](/bigquery) as [BigLake](/biglake) : tables.\n- Data transformations using Spark or BigQuery, and using open file formats including Apache Iceberg.\n- Data can be secured using policy tags and row access policies.\n- Machine learning can be applied on the tables.\n- Dashboards are created from the data to perform more analytics by using [Looker Studio](/looker-studio) .## Cost\nFor an estimate of the cost of the Google Cloud resources that the analytics lakehouse solution uses, see the precalculated estimate in the [Google Cloud Pricing Calculator](https://cloud.google.com/products/calculator#id=938f4fd5-234a-474b-af0a-ea05996bdcc4) .\nUse the estimate as a starting point to calculate the cost of your deployment. You can modify the estimate to reflect any configuration changes that you plan to make for the resources that are used in the solution.\nThe precalculated estimate is based on assumptions for certain factors, including the following:\n- The Google Cloud locations where the resources are deployed.\n- The amount of time that the resources are used.## Deploy the solution\nThis section guides you through the process of deploying the solution.\n**Note:** To ensure the solution deploys successfully, make sure that the organizational policy constraint `constraints/compute.requireOsLogin` is not enforced in the project you want to deploy to. Go to the [Policy details](https://console.cloud.google.com/iam-admin/orgpolicies/compute-requireOsLogin) page for your project, and confirm that the **Status** is .\n### Create or choose a Google Cloud project\nWhen you deploy the solution, you choose the [Google Cloud project](/resource-manager/docs/creating-managing-projects) where the resources are deployed. When you're deciding whether to use an existing project or to create a new project, consider the following factors:\n- If you create a project for the solution, then when you no longer need the deployment, you can delete the project and avoid continued billing. If you use an existing project, you must delete the deployment when you no longer need it.\n- Using a new project can help avoid conflicts with previously provisioned resources, such as resources that are used for production workloads.\nIf you want to deploy the solution in a new project, create the project you begin the deployment.\nTo create a project, complete the following steps:\n- In the Google Cloud console, go to the project selector page. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- To begin creating a   Google Cloud project, click **Create project** .\n- Name your project. Make a note of your generated project ID.\n- Edit the other fields as needed.\n- To create the project, click **Create** .### Get the required IAM permissions\nTo start the deployment process, you need the Identity and Access Management (IAM) permissions that are listed in the following table. If you have the `roles/owner` [basic role](/iam/docs/understanding-roles#basic) for the project in which you plan to deploy the solution, then you already have all the necessary permissions. If you don't have the `roles/owner` role, then ask your administrator to grant these permissions (or the roles that include these permissions) to you.\n| IAM permission required       | Predefined role that includes the required permissions  |\n|:--------------------------------------------------|:------------------------------------------------------------|\n| serviceusage.services.enable      | Service Usage Admin (roles/serviceusage.serviceUsageAdmin) |\n| iam.serviceAccounts.create      | Service Account Admin (roles/iam.serviceAccountAdmin)  |\n| resourcemanager.projects.setIamPolicy    | Project IAM Admin (roles/resourcemanager.projectIamAdmin) |\n| config.deployments.create config.deployments.list | Cloud Infrastructure Manager Admin (roles/config.admin) |\nWhen you initiate the deployment process, a [service account](/iam/docs/service-accounts) is created to deploy the solution on your behalf (and to delete the deployment later if you choose). This service account is assigned certain IAM permissions ; that is, the permissions are revoked automatically after the solution deployment and deletion operations are completed. Google recommends that after you delete the deployment, you delete the service account, as described later in this guide.### Choose a deployment method\nTo help you deploy this solution with minimal effort, a Terraform configuration is provided in GitHub. The Terraform configuration defines all the Google Cloud resources that are required for the solution.\nYou can deploy the solution by using one of the following methods:\n- **Through the console** : Use this method if you want to try the solution with the default configuration and see how it works. Cloud Build deploys all the resources that are required for the solution. When you no longer need the deployed solution, you can delete it through the console. Any resources that you create after you deploy the solution might need to be deleted separately.To use this deployment method, follow the instructions in [Deploy through the console](#deploy-from-console) .\n- **Using the Terraform CLI** : Use this method if you want to customize the solution or if you want to automate the provisioning and management of the resources by using the infrastructure as code (IaC) approach. Download the Terraform configuration from GitHub, optionally customize the code as necessary, and then deploy the solution by using the Terraform CLI. After you deploy the solution, you can continue to use Terraform to manage the solution.To use this deployment method, follow the instructions in [Deploy using the Terraform CLI](#deploy-using-terraform) .\n### Deploy through the console\nComplete the following steps to deploy the preconfigured solution.\n**Note:** If you want to customize the solution or automate the provisioning and management of the solution by using the infrastructure as code (IaC) approach, then see [Deploy using the Terraform CLI](#deploy-using-terraform) .\n- In the Google Cloud Jump Start Solutions catalog, go to the **Analytics lakehouse** solution. [Go to the Analytics lakehouse solution](https://console.cloud.google.com/products/solutions/details/analytics-lakehouse) \n- Review the information that's provided on the page, such as the estimated cost of the solution and the estimated deployment time.\n- When you're ready to start deploying the solution, click **Deploy** .A step-by-step interactive guide is displayed.\n- Complete the steps in the interactive guide.Note the name that you enter for the deployment. This name is required later when you delete the deployment.When you click **Deploy** , the **Solution deployments** page is displayed. The **Status** field on this page shows **Deploying** .\n- Wait for the solution to be deployed.If the deployment fails, the **Status** field shows **Failed** . You can use the Cloud Build log to diagnose the errors. For more information, see [Errors when deploying through the console](#console-deploy-errors) .After the deployment is completed, the **Status** field changes to **Deployed** .\n- To view and use the solution, return to the **Solution deployments** page in the console.- Click themore_vert **Actions** menu.\n- Select **View Looker Studio Dashboard** to open a dashboard that's built on top of the sample data that's transformed by using the solution.\n- Select **Open BigQuery Editor** to run queries and build machine learning (ML) models using the sample data in the solution.\n- Select **View Colab** to run queries in a notebook environment.When you no longer need the solution, you can delete the deployment to avoid continued billing for the Google Cloud resources. For more information, see [Delete the deployment](#delete-deployment) .\n### Deploy using the Terraform CLI\nThis section describes how you can customize the solution or automate the provisioning and management of the solution by using the Terraform CLI. Solutions that you deploy by using the Terraform CLI are not displayed in the **Solution deployments** page in the Google Cloud console.\n**Note:** If you want to deploy the solution with the default configuration to see how it works, then follow the instructions in [Deploy through the console](#deploy-from-console) .\nYou can run Terraform either in Cloud Shell or on your local host. This guide describes how to run Terraform in Cloud Shell, which has Terraform preinstalled and configured to authenticate with Google Cloud.\nThe Terraform code for this solution is available in a GitHub repository.\n- Clone the GitHub repository to Cloud Shell. [](https://ssh.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https://github.com/GoogleCloudPlatform/terraform-google-analytics-lakehouse) A prompt is displayed to confirm downloading the GitHub repository to Cloud Shell.\n- Click **Confirm** .Cloud Shell is launched in a separate browser tab, and the Terraform code is downloaded to the `$HOME/cloudshell_open` directory of your Cloud Shell environment.\n- In Cloud Shell, check whether the current working directory is `$HOME/cloudshell_open/terraform-google-analytics-lakehouse/` . This is the directory that contains the Terraform configuration files for the solution. If you need to change to that directory, run the following command:```\ncd $HOME/cloudshell_open/terraform-google-analytics-lakehouse/\n```\n- Initialize Terraform by running the following command:```\nterraform init\n```Wait until you see the following message:```\nTerraform has been successfully initialized!\n```The Terraform code that you downloaded includes variables that you can use to customize the deployment based on your requirements. For example, you can specify the Google Cloud project and the region where you want the solution to be deployed.\n- Make sure that the current working directory is `$HOME/cloudshell_open/terraform-google-analytics-lakehouse/` . If it isn't, go to that directory.\n- In the same directory, create a text file named `terraform.tfvars` .\n- In the `terraform.tfvars` file, copy the following code snippet, and set values for the required variables.- Follow the instructions that are provided as comments in the code snippet.\n- This code snippet includes only the variables for which youset values. The Terraform configuration includes other variables that have default values. To review all the variables and the default values, see the`variables.tf`file that's available in the`$HOME/cloudshell_open/terraform-google-analytics-lakehouse/`directory.\n- Make sure that each value that you set in the`terraform.tfvars`file matches the variable [type](https://www.terraform.io/docs/language/values/variables.html#type-constraints) as declared in the`variables.tf`file. For example, if the type that\u2019s defined for a variable in the`variables.tf`file is`bool`, then you must specify`true`or`false`as the value of that variable in the`terraform.tfvars`file.\n```\n# This is an example of the terraform.tfvars file.# The values in this file must match the variable types declared in variables.tf.# The values in this file override any defaults in variables.tf.# ID of the project in which you want to deploy the solutionproject_id = \"PROJECT_ID\"# Google Cloud region where you want to deploy the solution# Example: us-central1region = \"REGION\"# Whether or not to enable underlying apis in this solution.# Example: trueenable_apis = true# Whether or not to protect Cloud Storage and BigQuery resources from deletion when solution is modified or changed.# Example: falseforce_destroy = false\n```- Make sure that the current working directory is `$HOME/cloudshell_open/terraform-google-analytics-lakehouse/` . If it isn't, go to that directory.\n- Verify that the Terraform configuration has no errors:```\nterraform validate\n```If the command returns any errors, make the required corrections in the configuration and then run the `terraform validate` command again. Repeat this step until the command returns the following message:```\nSuccess! The configuration is valid.\n```\n- Review the resources that are defined in the configuration:```\nterraform plan\n```\n- If you didn't create the `terraform.tfvars` file as described earlier, Terraform prompts you to enter values for the variables that don't have default values. Enter the required values.The output of the `terraform plan` command is a list of the resources that Terraform provisions when you apply the configuration.If you want to make any changes, edit the configuration and then run the `terraform validate` and `terraform plan` commands again.When no further changes are necessary in the Terraform configuration, deploy the resources.\n- Make sure that the current working directory is `$HOME/cloudshell_open/terraform-google-analytics-lakehouse/` . If it isn't, go to that directory.\n- Apply the Terraform configuration:```\nterraform apply\n```\n- If you didn't create the `terraform.tfvars` file as described earlier, Terraform prompts you to enter values for the variables that don't have default values. Enter the required values.Terraform displays a list of the resources that will be created.\n- When you're prompted to perform the actions, enter `yes` .Terraform displays messages showing the progress of the deployment.If the deployment can't be completed, Terraform displays the errors that caused the failure. Review the error messages and update the configuration to fix the errors. Then run the `terraform apply` command again. For help with troubleshooting Terraform errors, see [Errors when deploying the solution using the Terraform CLI](#tf-deploy-errors) .After all the resources are created, Terraform displays the following message:```\nApply complete!\n```The Terraform output also lists the following additional information that you'll need:- The Looker Studio URL of the dashboard that was deployed.\n- The link to open the BigQuery editor for some sample queries.\n- The link to open the Colab tutorial.\nThe following example shows what the output looks like:```\nlookerstudio_report_url = \"https://lookerstudio.google.com/reporting/create?c.reportId=79675b4f-9ed8-4ee4-bb35-709b8fd5306a&ds.ds0.datasourceName=vw_ecommerce&ds.ds0.projectId=${var.project_id}&ds.ds0.type=TABLE&ds.ds0.datasetId=gcp_lakehouse_ds&ds.ds0.tableId=view_ecommerce\"bigquery_editor_url = \"https://console.cloud.google.com/bigquery?project=my-cloud-project&ws=!1m5!1m4!6m3!1smy-cloud-project!2sds_edw!3ssp_sample_queries\"lakehouse_colab_url = \"https://colab.research.google.com/github/GoogleCloudPlatform/terraform-google-analytics-lakehouse/blob/main/assets/ipynb/exploratory-analysis.ipynb\"\n```\n- To view and use the dashboard and to run queries in BigQuery, copy the output URLs from the previous step and open the URLs in new browser tabs.The dashboard, notebook, and BigQuery editors appear in the new tabs.\nWhen you no longer need the solution, you can delete the deployment to avoid continued billing for the Google Cloud resources. For more information, see [Delete the deployment](#delete-deployment) .\n## Customize the solution\nThis section provides information that Terraform developers can use to modify the analytics lakehouse solution in order to meet their own technical and business requirements. The guidance in this section is relevant only if you deploy the solution by using the Terraform CLI.\n**Note:** Changing the Terraform code for this solution requires familiarity with the [Terraform configuration language](https://developer.hashicorp.com/terraform/language) . If you modify the Google-provided Terraform configuration, and then experience errors, create issues in [GitHub](https://github.com/GoogleCloudPlatform/terraform-google-analytics-lakehouse) . GitHub issues are reviewed on a best-effort basis and are not intended for general usage questions.\nAfter you've seen how the solution works with the sample data, you might want to work with your own data. To use your own data, you put it into the Cloud Storage bucket named `edw-raw-` `` . The hash is a random set of 8 characters that's generated during the deployment. You can change the Terraform code in the following ways:\n- **Dataset ID** . Change the Terraform code so that when the code creates the BigQuery dataset, it uses the dataset ID that you want to use for your data.\n- **Schema** . Change the Terraform code so that it creates the BigQuery table ID that you want to use to store your data. This includes the external table schema so that BigQuery can read the data from Cloud Storage.\n- **Zone** . Create the lake zones that match your business need (usually a two or three tier zoning based on data quality and usage).\n- **Looker dashboards** . Change the Terraform code that creates a Looker dashboard so that the dashboard reflects the data that you're using.\n- **PySpark jobs** . Change the Terraform code to execute PySpark jobs using Dataproc.\nThe following are common analytics lakehouse objects, showing the Terraform example code in `main.tf` .\n- BigQuery [dataset](/bigquery/docs/datasets-intro) :   The schema where database objects are grouped and stored.```\nresource \"google_bigquery_dataset\" \"ds_edw\" {\n  project = module.project-services.project_id\n  dataset_id = \"DATASET_PHYSICAL_ID\"\n  friendly_name = \"DATASET_LOGICAL_NAME\"\n  description = \"DATASET_DESCRIPTION\"\n  location = \"REGION\"\n  labels = var.labels\n  delete_contents_on_destroy = var.force_destroy\n }\n```\n- BigQuery [table](/bigquery/docs/tables-intro) :   A database object that represents data that's stored in BigQuery or that   represents a data schema that's stored in Cloud Storage.```\nresource \"google_bigquery_table\" \"tbl_edw_taxi\" {\n  dataset_id = google_bigquery_dataset.ds_edw.dataset_id\n  table_id = \"TABLE_NAME\"\n  project = module.project-services.project_id\n  deletion_protection = var.deletion_protection\n  ...\n }\n```\n- BigQuery [stored procedure](/bigquery/docs/procedures) :  A database object that represents one or more SQL statements to be executed  when called. This could be to transform data from one table to another or  to load data from an external table into a standard table.```\nresource \"google_bigquery_routine\" \"sp_sample_translation_queries\" {\n  project = module.project-services.project_id\n  dataset_id = google_bigquery_dataset.ds_edw.dataset_id\n  routine_id = \"sp_sample_translation_queries\"\n  routine_type = \"PROCEDURE\"\n  language = \"SQL\"\n  definition_body = templatefile(\"${path.module}/assets/sql/sp_sample_translation_queries.sql\", { project_id = module.project-services.project_id })\n }\n```\n- Cloud Workflows [workflow](/workflows/docs/reference/syntax) :  A Workflows workflow represents a combination of steps to be executed in a specific order.  This could be used to set up data or perform data transformations along with  other execution steps.```\nresource \"google_workflows_workflow\" \"copy_data\" {\n name   = \"copy_data\"\n project   = module.project-services.project_id\n region   = var.region\n description  = \"Copies data and performs project setup\"\n service_account = google_service_account.workflows_sa.email\n source_contents = templatefile(\"${path.module}/src/yaml/copy-data.yaml\", {\n  public_data_bucket = var.public_data_bucket,\n  textocr_images_bucket = google_storage_bucket.textocr_images_bucket.name,\n  ga4_images_bucket  = google_storage_bucket.ga4_images_bucket.name,\n  tables_bucket   = google_storage_bucket.tables_bucket.name,\n  dataplex_bucket  = google_storage_bucket.dataplex_bucket.name,\n  images_zone_name  = google_dataplex_zone.gcp_primary_raw.name,\n  tables_zone_name  = google_dataplex_zone.gcp_primary_staging.name,\n  lake_name    = google_dataplex_lake.gcp_primary.name\n })\n }\n \n```\nTo customize the solution, complete the following steps in Cloud Shell:\n- Verify that the current working directory is `$HOME/cloudshell_open/terraform-google-analytics-lakehouse` . If it isn't, go to that directory:```\ncd $HOME/cloudshell_open/terraform-google-analytics-lakehouse\n```\n- Open `main.tf` and make the changes you want to make.For more information about the effects of such customization on reliability, security, performance, cost, and operations, see [Design recommendations](#design-recommendations) .\n- [Validate and review the Terraform configuration](#validate-config) .\n- [Provision the resources](#provision-resources) .## Design recommendations\nThis section provides recommendations for using the analytics lakehouse solution to develop an architecture that meets your requirements for security, reliability, cost, and performance.\nAs you begin to scale your lakehouse solution, you have available a number of ways to help improve your query performance and to reduce your total spend. These methods include changing how your data is physically stored, modifying your SQL queries, and changing how your queries are executed using different technologies. To learn more about methods for optimizing your Spark workloads, see [Dataproc best practices for production](/dataproc/docs/guides/dataproc-best-practices) .\nNote the following:\n- Before you make any design changes, assess the cost impact and consider potential trade-offs with other features. You can assess the cost impact of design changes by using the [Google Cloud Pricing Calculator](/products/calculator#id=938f4fd5-234a-474b-af0a-ea05996bdcc4) .\n- To implement design changes in the solution, you need expertise in Terraform coding and advanced knowledge of the Google Cloud services that are used in the solution.\n- If you modify the Google-provided Terraform configuration and if you then experience errors, create issues in [GitHub](https://github.com/GoogleCloudPlatform/terraform-google-analytics-lakehouse) . GitHub issues are reviewed on a best-effort basis and are not intended for general usage questions.\n- For more information about designing and setting up production-grade environments in Google Cloud, see [Landing zone design in Google Cloud](/architecture/landing-zones) and [Google Cloud setup checklist](/docs/enterprise/setup-checklist) .## Delete the solution deployment\nWhen you no longer need the solution deployment, to avoid continued billing for the resources that you created, delete the deployment.\n### Delete the deployment through the console\nUse this procedure if you deployed the solution through the console.\n- In the Google Cloud console, go to the **Solution deployments** page. [Go to Solution deployments](https://console.cloud.google.com/products/solutions/deployments) \n- Select the project that contains the deployment that you want to delete.\n- Locate the deployment that you want to delete.\n- Click more_vert **Actions** and then select **Delete** .\n- Enter the name of the deployment and then click **Confirm** .The **Status** field shows **Deleting** .If the deletion fails, see the troubleshooting guidance in [Error when deleting a deployment](#error-deleting-deployment) .\nWhen you no longer need the Google Cloud project that you used for the solution, you can delete the project. For more information, see [Optional: Delete the project](#delete-project) .\n### Delete the deployment using the Terraform CLI\nUse this procedure if you deployed the solution by using the Terraform CLI.\n- In Cloud Shell, make sure that the current working directory is `$HOME/cloudshell_open/terraform-google-analytics-lakehouse/` . If it isn't, go to that directory.\n- Remove the resources that were provisioned by Terraform:```\nterraform destroy\n```Terraform displays a list of the resources that will be destroyed.\n- When you're prompted to perform the actions, enter `yes` .Terraform displays messages showing the progress. After all the resources are deleted, Terraform displays the following message:```\nDestroy complete!\n```If the deletion fails, see the troubleshooting guidance in [Error when deleting a deployment](#error-deleting-deployment) .\nWhen you no longer need the Google Cloud project that you used for the solution, you can delete the project. For more information, see [Optional: Delete the project](#delete-project) .\n### Optional: Delete the project\nIf you deployed the solution in a new Google Cloud project, and if you no longer need the project, then delete it by completing the following steps:\n**Caution:** If you delete a project, all the resources in the project are permanently deleted.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you want to delete, and then  click **Delete** .\n- At the prompt, type the project ID, and then click **Shut down** .\nIf you decide to retain the project, then delete the service account that was created for this solution, as described in the next section.\n### Optional: Delete the service account\nIf you deleted the project that you used for the solution, then skip this section.\nAs mentioned earlier in this guide, when you deployed the solution, a service account was created on your behalf. The service account was assigned certain IAM permissions ; that is, the permissions were revoked automatically after the solution deployment and deletion operations were completed, but the service account isn't deleted. Google recommends that you delete this service account.\n- If you deployed the solution through the Google Cloud console, go to the [Solution deployments](https://console.cloud.google.com/products/solutions/deployments) page. (If you're already on that page, refresh the browser.) A process is triggered in the background to delete the service account. No further action is necessary.\n- If you deployed the solution by using the Terraform CLI, complete the following steps:- In the Google Cloud console, go to the **Service accounts** page. [Go to Service accounts](https://console.cloud.google.com/iam-admin/serviceaccounts) \n- Select the project that you used for the solution.\n- Select the service account that you want to delete.The email ID of the service account that was created for the solution is in the following format:```\ngoog-sc-DEPLOYMENT_NAME-NNN@PROJECT_ID.iam.gserviceaccount.com\n```The email ID contains the following values:- : the name of the deployment.\n- : a random 3-digit number.\n- : the ID of the project in which you deployed the solution.\n- Click **Delete** .\n## Troubleshoot errors\nThe actions that you can take to diagnose and resolve errors depend on the deployment method and the complexity of the error.\n### Errors when deploying the solution through the console\nIf the deployment fails when you use the console, do the following:\n- Go to the **Solution deployments** page.If the deployment failed, the **Status** field shows **Failed** .\n- View the details of the errors that caused the failure:- Click more_vert **Actions** .\n- Select **View Cloud Build logs** .\n- Review the Cloud Build log and take appropriate action to resolve the issue that caused the failure.\n### Errors when deploying the solution using the Terraform CLI\nIf the deployment fails when you use Terraform, the output of the `terraform apply` command includes error messages that you can review to diagnose the problem.\nThe examples in the following sections show deployment errors that you might encounter when you use Terraform.\nIf you create a project and then immediately attempt to deploy the solution in the new project, the deployment might fail with an error like the following:\n```\nError: Error creating Network: googleapi: Error 403: Compute Engine API has not\nbeen used in project PROJECT_ID before or it is disabled. Enable it by visiting\nhttps://console.developers.google.com/apis/api/compute.googleapis.com/overview?project=PROJECT_ID\nthen retry. If you enabled this API recently, wait a few minutes for the action\nto propagate to our systems and retry.\n```\nIf this error occurs, wait a few minutes and then run the `terraform apply` command again.\nWhen you run the `terraform apply` command, a `cannot assign requested address` error might occur, with a message like the following:\n```\nError: Error creating service account:\n Post \"https://iam.googleapis.com/v1/projects/PROJECT_ID/serviceAccounts:\n dial tcp [2001:db8:ffff:ffff::5f]:443:\n connect: cannot assign requested address\n```\nIf this error occurs, run the `terraform apply` command again.\n### Errors accessing data in BigQuery or Looker Studio\nThere is a provisioning step that runs after the Terraform provisioning steps that loads data to the environment. If you get an error when the data is being loaded into the Looker Studio dashboard, or if there are no objects when you start exploring BigQuery, wait a few minutes and try again.\n### Error when deleting a deployment\nIn certain cases, attempts to delete a deployment might fail:\n- After deploying a solution through the console, if you change any resource that was provisioned by the solution, and if you then try to delete the deployment, the deletion might fail. The **Status** field on the **Solution deployments** page shows **Failed** , and the Cloud Build log shows the cause of the error.\n- After deploying a solution by using the Terraform CLI, if you change any resource by using a non-Terraform interface (for example, the console), and if you then try to delete the deployment, the deletion might fail. The messages in the output of the`terraform destroy`command show the cause of the error.\nReview the error logs and messages, identify and delete the resources that caused the error, and then try deleting the deployment again.\nIf a console-based deployment doesn't get deleted and if you can't diagnose the error by using the Cloud Build log, then you can delete the deployment by using the Terraform CLI, as described in the next section.\n### Delete a console-based deployment by using the Terraform CLI\nThis section describes how to delete a console-based deployment if errors occur when you try to delete it through the console. In this approach, you download the Terraform configuration for the deployment that you want to delete and then use the Terraform CLI to delete the deployment.\n- Identify the region where the deployment's Terraform code, logs, and other data are stored. This region might be different from the region that you selected while deploying the solution.- In the Google Cloud console, go to the **Solution deployments** page. [Go to Solution deployments](https://console.cloud.google.com/products/solutions/deployments) \n- Select the project that contains the deployment that you want to delete.\n- In the list of deployments, identify the row for the deployment that you want to delete.\n- Click expand_more **View all rowcontent** .\n- In the **Location** column, note the location, as highlighted in the following example: \n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) At the bottom of the Google Cloud console, a [Cloud Shell](/shell/docs/how-cloud-shell-works) session starts and displays a command-line prompt. Cloud Shell is a shell environment  with the Google Cloud CLI  already installed and with values already set for  your current project. It can take a few seconds for the session to initialize.\n- Create environment variables for the project ID, region, and name of the deployment that you want to delete:```\nexport REGION=\"REGION\"export PROJECT_ID=\"PROJECT_ID\"export DEPLOYMENT_NAME=\"DEPLOYMENT_NAME\"\n```In these commands, replace the following:- : the location that you noted earlier in this procedure.\n- : the ID of the project where you deployed the solution.\n- : the name of the deployment that you want to delete.\n- Get the ID of the latest revision of the deployment that you want to delete:```\nexport REVISION_ID=$(curl \\\u00a0 \u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 \u00a0 -H \"Content-Type: application/json\" \\\u00a0 \u00a0 \"https://config.googleapis.com/v1alpha2/projects/${PROJECT_ID}/locations/${REGION}/deployments/${DEPLOYMENT_NAME}\" \\\u00a0 \u00a0 | jq .latestRevision -r)\u00a0 \u00a0 echo $REVISION_ID\n```The output is similar to the following:```\nprojects/PROJECT_ID/locations/REGION/deployments/DEPLOYMENT_NAME/revisions/r-0\n```\n- Get the Cloud Storage location of the Terraform configuration for the deployment:```\nexport CONTENT_PATH=$(curl \\\u00a0 \u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 \u00a0 -H \"Content-Type: application/json\" \\\u00a0 \u00a0 \"https://config.googleapis.com/v1alpha2/${REVISION_ID}\" \\\u00a0 \u00a0 | jq .applyResults.content -r)\u00a0 \u00a0 echo $CONTENT_PATH\n```The following is an example of the output of this command:```\ngs://PROJECT_ID-REGION-blueprint-config/DEPLOYMENT_NAME/r-0/apply_results/content\n```\n- Download the Terraform configuration from Cloud Storage to Cloud Shell:```\ngsutil cp -r $CONTENT_PATH $HOMEcd $HOME/content/\n```Wait until the `Operation completed` message is displayed, as shown in the following example:```\nOperation completed over 45 objects/268.5 KiB\n```\n- Initialize Terraform:```\nterraform init\n```Wait until you see the following message:```\nTerraform has been successfully initialized!\n```\n- Remove the deployed resources:```\nterraform destroy\n```Terraform displays a list of the resources that will be destroyed.If any warnings about undeclared variables are displayed, ignore the warnings.\n- When you're prompted to perform the actions, enter `yes` .Terraform displays messages showing the progress. After all the resources are deleted, Terraform displays the following message:```\nDestroy complete!\n```\n- Delete the deployment artifact:```\ncurl -X DELETE \\\u00a0 \u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 \u00a0 -H \"Content-Type: application/json\" \\\u00a0 \u00a0 \"https://config.googleapis.com/v1alpha2/projects/${PROJECT_ID}/locations/${REGION}/deployments/${DEPLOYMENT_NAME}?force=true&delete_policy=abandon\"\n```\n- Wait a few seconds and then verify that the deployment artifact was deleted:```\ncurl -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 \u00a0 -H \"Content-Type: application/json\" \\\u00a0 \u00a0 \"https://config.googleapis.com/v1alpha2/projects/${PROJECT_ID}/locations/${REGION}/deployments/${DEPLOYMENT_NAME}\" \\\u00a0 \u00a0 | jq .error.message\n```If the output shows `null` , wait a few seconds and then run the command again.After the deployment artifact is deleted, a message as shown in the following example is displayed:```\nResource 'projects/PROJECT_ID/locations/REGION/deployments/DEPLOYMENT_NAME' was not found\n```\n### Submit feedback\nJump Start Solutions are for informational purposes only and are not officially supported products. Google may change or remove solutions without notice.\nTo troubleshoot errors, review the Cloud Build logs and the Terraform output.\nTo submit feedback, do the following:\n- For documentation, in-console tutorials, or the solution, use the **Send Feedback** button on the page.\n- For unmodified Terraform code, create issues in the [GitHub repository](https://github.com/GoogleCloudPlatform/terraform-google-analytics-lakehouse) . GitHub issues are reviewed on a best-effort basis and are not intended for general usage questions.\n- For issues with the products that are used in the solution, contact [Cloud Customer Care](/support-hub) .## What's next\n- [Create a data lake using Dataplex](/dataplex/docs/create-lake) \n- [Create Apache Iceberg tables using BigLake](/bigquery/docs/iceberg-tables) \n- [Use Apache Spark on Google Cloud](/solutions/spark) \n- [Learn about BigQuery](/bigquery/docs)", "guide": "Docs"}