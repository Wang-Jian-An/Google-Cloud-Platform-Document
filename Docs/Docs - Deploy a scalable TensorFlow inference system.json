{"title": "Docs - Deploy a scalable TensorFlow inference system", "url": "https://cloud.google.com/architecture/scalable-tensorflow-inference-system/deployment", "abstract": "# Docs - Deploy a scalable TensorFlow inference system\nLast reviewed 2023-11-02 UTC\n**Note:** This document or section includes references to one or more terms that Google considers disrespectful or offensive. The terms are used because they are keywords in the software that's described in the document.\nThis document describes how you deploy the reference architecture described in [Scalable TensorFlow inference system](/architecture/scalable-tensorflow-inference-system) .\nThis series is intended for developers who are familiar with Google Kubernetes Engine and machine learning (ML) frameworks, including TensorFlow and [NVIDIA TensorRT](https://developer.nvidia.com/tensorrt) .\nAfter you complete this deployment, see [Measure and tune performance of a TensorFlow inference system](/architecture/scalable-tensorflow-inference-system/measure-deployment) .\n", "content": "## Architecture\nThe following diagram shows the architecture of the inference system.\nThe Cloud Load Balancing sends the request traffic to the closest GKE cluster. The cluster contains a Pod for each node. In each Pod, a Triton Inference Server provides an inference service (to serve ResNet-50 models), and an NVIDIA T4 GPU improves performance. Monitoring servers on the cluster collect metrics data on GPU utilization and memory usage.\nFor details, see [Scalable TensorFlow inference system](/architecture/scalable-tensorflow-inference-system) .\n## Objectives\n- Download a pretrained [ResNet-50](https://www.mathworks.com/help/deeplearning/ref/resnet50.html) model, and use TensorFlow integration with TensorRT (TF-TRT) to apply optimizations\n- Serve a ResNet-50 model from an NVIDIA Triton Inference Server\n- Build a monitoring system for Triton by using [Prometheus](https://prometheus.io/) and [Grafana](https://grafana.com/) \n- Build a load testing tool by using [Locust](https://locust.io/) ## Costs\nIn addition to NVIDIA T4 GPU, in this deployment, you use the following billable components of Google Cloud:\n- [Google Kubernetes Engine](/kubernetes-engine/pricing) \n- [Compute Engine](/compute/pricing) \n- [Cloud Storage](/storage/pricing) \nTo generate a cost estimate based on your projected usage, use the [pricing calculator](/products/calculator) .\nWhen you finish this deployment, don't delete the resources you created. You need these resources when you [measure and tune the deployment](/architecture/scalable-tensorflow-inference-system/measure-deployment) .\n## Before you begin\n## Build optimized models with TF-TRT\nIn this section, you create a working environment and optimize the pretrained model.\nThe pretrained model uses the fake dataset at `gs://cloud-tpu-test-datasets/fake_imagenet/` . There is also a copy of the pretrained model in the Cloud Storage location at `gs://solutions-public-assets/tftrt-tutorial/resnet/export/1584366419/` .\n### Create a working environment\nFor your working environment, you create a Compute Engine instance by using Deep Learning VM Images. You optimize and quantize the ResNet-50 model with TensorRT on this instance.\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Deploy an instance named `working-vm` :```\ngcloud config set project PROJECT_IDgcloud config set compute/zone us-west1-bgcloud compute instances create working-vm \\\u00a0 \u00a0 --scopes cloud-platform \\\u00a0 \u00a0 --image-family common-cu113 \\\u00a0 \u00a0 --image-project deeplearning-platform-release \\\u00a0 \u00a0 --machine-type n1-standard-8 \\\u00a0 \u00a0 --min-cpu-platform=\"Intel Skylake\" \\\u00a0 \u00a0 --accelerator=type=nvidia-tesla-t4,count=1 \\\u00a0 \u00a0 --boot-disk-size=200GB \\\u00a0 \u00a0 --maintenance-policy=TERMINATE \\\u00a0 \u00a0 --metadata=\"install-nvidia-driver=True\"\n```Replace `` with the ID of the Google Cloud project that you created earlier.This command launches a Compute Engine instance using NVIDIA T4. Upon the first boot, it automatically installs the NVIDIA GPU driver that is compatible with TensorRT 5.1.5.\n### Create model files with different optimizations\nIn this section, you apply the following optimizations to the original ResNet-50 model by using TF-TRT:\n- Graph optimization\n- Conversion to FP16 with the graph optimization\n- Quantization with INT8 with the graph optimization\nFor details about these optimizations, see [Performance optimization](/architecture/scalable-tensorflow-inference-system#performance-optimization) .\n- In the Google Cloud console, go to **Compute Engine > VM instances** . [Go to VM Instances](https://console.cloud.google.com/compute/instances) You see the `working-vm` instance that you created earlier.\n- To open the terminal console of the instance, click **SSH** .You use this terminal to run the rest of the commands in this document.\n- In the terminal, clone the required repository and change the current directory:```\ncd $HOMEgit clone https://github.com/GoogleCloudPlatform/gke-tensorflow-inference-system-tutorialcd gke-tensorflow-inference-system-tutorial/server\n```\n- Download the pretrained ResNet-50 model to a local directory:```\nmkdir -p models/resnet/original/00001gsutil cp -R gs://solutions-public-assets/tftrt-tutorial/resnet/export/1584366419/* models/resnet/original/00001\n```\n- Build a container image that contains optimization tools for TF-TRT:```\ndocker build ./ -t trt-optimizerdocker image list\n```The last command shows a table of repositories.\n- In the table, in the row for the `tft-optimizer` repository, copy the image ID.\n- Apply the optimizations (graph optimization, conversion to FP16, and quantization with INT8) to the original model:```\nexport IMAGE_ID=IMAGE_IDnvidia-docker run --rm \\\u00a0 \u00a0 -v `pwd`/models/:/workspace/models ${IMAGE_ID} \\\u00a0 \u00a0 --input-model-dir='models/resnet/original/00001' \\\u00a0 \u00a0 --output-dir='models/resnet' \\\u00a0 \u00a0 --precision-mode='FP32' \\\u00a0 \u00a0 --batch-size=64nvidia-docker run --rm \\\u00a0 \u00a0 -v `pwd`/models/:/workspace/models ${IMAGE_ID} \\\u00a0 \u00a0 --input-model-dir='models/resnet/original/00001' \\\u00a0 \u00a0 --output-dir='models/resnet' \\\u00a0 \u00a0 --precision-mode='FP16' \\\u00a0 \u00a0 --batch-size=64nvidia-docker run --rm \\\u00a0 \u00a0 -v `pwd`/models/:/workspace/models ${IMAGE_ID} \\\u00a0 \u00a0 --input-model-dir='models/resnet/original/00001' \\\u00a0 \u00a0 --output-dir='models/resnet' \\\u00a0 \u00a0 --precision-mode='INT8' \\\u00a0 \u00a0 --batch-size=64 \\\u00a0 \u00a0 --calib-image-dir='gs://cloud-tpu-test-datasets/fake_imagenet/' \\\u00a0 \u00a0 --calibration-epochs=10\n```Replace `` with the image ID for `tft-optimizer` that you copied in the previous step.The `--calib-image-dir` option specifies the location of the training data that is used for the pretrained model. The same training data is used for a calibration for INT8 quantization. The calibration process can take about 5 minutes.When the commands finish running, the last output line is similar to the following, in which the optimized models are saved in `./models/resnet` :```\nINFO:tensorflow:SavedModel written to: models/resnet/INT8/00001/saved_model.pb\n```The directory structure is similar to the following:```\nmodels\u2514\u2500\u2500 resnet\u00a0 \u00a0 \u251c\u2500\u2500 FP16\u00a0 \u00a0 \u2502 \u00a0 \u2514\u2500\u2500 00001\u00a0 \u00a0 \u2502 \u00a0 \u00a0 \u00a0 \u251c\u2500\u2500 saved_model.pb\u00a0 \u00a0 \u2502 \u00a0 \u00a0 \u00a0 \u2514\u2500\u2500 variables\u00a0 \u00a0 \u251c\u2500\u2500 FP32\u00a0 \u00a0 \u2502 \u00a0 \u2514\u2500\u2500 00001\u00a0 \u00a0 \u2502 \u00a0 \u00a0 \u00a0 \u251c\u2500\u2500 saved_model.pb\u00a0 \u00a0 \u2502 \u00a0 \u00a0 \u00a0 \u2514\u2500\u2500 variables\u00a0 \u00a0 \u251c\u2500\u2500 INT8\u00a0 \u00a0 \u2502 \u00a0 \u2514\u2500\u2500 00001\u00a0 \u00a0 \u2502 \u00a0 \u00a0 \u00a0 \u251c\u2500\u2500 saved_model.pb\u00a0 \u00a0 \u2502 \u00a0 \u00a0 \u00a0 \u2514\u2500\u2500 variables\u00a0 \u00a0 \u2514\u2500\u2500 original\u00a0 \u00a0 \u00a0 \u00a0 \u2514\u2500\u2500 00001\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u251c\u2500\u2500 saved_model.pb\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u2514\u2500\u2500 variables\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u251c\u2500\u2500 variables.data-00000-of-00001\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u2514\u2500\u2500 variables.index\n```\nThe following table summarizes the relationship between directories and optimizations.\n| Directory | Optimization             |\n|:------------|:-------------------------------------------------------------|\n| FP16  | Conversion to FP16 in addition to the graph optimization  |\n| FP32  | Graph optimization           |\n| INT8  | Quantization with INT8 in addition to the graph optimization |\n| original | Original model (no optimization with TF-TRT)     |\n## Deploy an inference server\nIn this section, you deploy Triton servers with five models. First, you upload the model binary that you created in the previous section to Cloud Storage. Then, you create a GKE cluster and deploy Triton servers on the cluster.\n### Upload the model binary\n- In the SSH terminal, upload the model binaries and `config.pbtxt` configuration files to a storage bucket:```\nexport PROJECT_ID=PROJECT_IDexport BUCKET_NAME=${PROJECT_ID}-modelsmkdir -p original/1/model/cp -r models/resnet/original/00001/* original/1/model/cp original/config.pbtxt original/1/model/cp original/imagenet1k_labels.txt original/1/model/mkdir -p tftrt_fp32/1/model/cp -r models/resnet/FP32/00001/* tftrt_fp32/1/model/cp tftrt_fp32/config.pbtxt tftrt_fp32/1/model/cp tftrt_fp32/imagenet1k_labels.txt tftrt_fp32/1/model/mkdir -p tftrt_fp16/1/model/cp -r models/resnet/FP16/00001/* tftrt_fp16/1/model/cp tftrt_fp16/config.pbtxt tftrt_fp16/1/model/cp tftrt_fp16/imagenet1k_labels.txt tftrt_fp16/1/model/mkdir -p tftrt_int8/1/model/cp -r models/resnet/INT8/00001/* tftrt_int8/1/model/cp tftrt_int8/config.pbtxt tftrt_int8/1/model/cp tftrt_int8/imagenet1k_labels.txt tftrt_int8/1/model/mkdir -p tftrt_int8_bs16_count4/1/model/cp -r models/resnet/INT8/00001/* tftrt_int8_bs16_count4/1/model/cp tftrt_int8_bs16_count4/config.pbtxt tftrt_int8_bs16_count4/1/model/cp tftrt_int8_bs16_count4/imagenet1k_labels.txt tftrt_int8_bs16_count4/1/model/gsutil mb gs://${BUCKET_NAME}gsutil -m cp -R original tftrt_fp32 tftrt_fp16 tftrt_int8 tftrt_int8_bs16_count4 \\\u00a0 \u00a0 gs://${BUCKET_NAME}/resnet/\n```Replace `` with the ID of the Google Cloud project that you created earlier.The following tuning parameters are specified in the `config.pbtxt` files:- Model name\n- Input tensor name and output tensor name\n- GPU allocation to each model\n- Batch size and number of instance groups\nAs an example, the `original/1/model/config.pbtxt` file contains the following content:```\nname: \"original\"platform: \"tensorflow_savedmodel\"max_batch_size: 64input {\u00a0 \u00a0 name: \"input\"\u00a0 \u00a0 data_type: TYPE_FP32\u00a0 \u00a0 format: FORMAT_NHWC\u00a0 \u00a0 dims: [ 224, 224, 3 ]}output {\u00a0 \u00a0 name: \"probabilities\"\u00a0 \u00a0 data_type: TYPE_FP32\u00a0 \u00a0 dims: 1000\u00a0 \u00a0 label_filename: \"imagenet1k_labels.txt\"}default_model_filename: \"model\"instance_group [\u00a0 {\u00a0 \u00a0 count: 1\u00a0 \u00a0 kind: KIND_GPU\u00a0 }]dynamic_batching {\u00a0 preferred_batch_size: [ 64 ]\u00a0 max_queue_delay_microseconds: 20000}\n```\nFor details on batch size and number of instance groups, see [Performance optimization](/architecture/scalable-tensorflow-inference-system#performance-optimization) .\nThe following table summarizes the five models that you deployed in this section.\n| Model name    | Optimization                      |\n|:-----------------------|:-------------------------------------------------------------------------------------------------|\n| original    | Original model (no optimization with TF-TRT)              |\n| tftrt_fp32    | Graph optimization (batch size=64, instance groups=1)           |\n| tftrt_fp16    | Conversion to FP16 in addition to the graph optimization (batch size=64, instance groups=1)  |\n| tftrt_int8    | Quantization with INT8 in addition to the graph optimization (batch size=64, instance groups=1) |\n| tftrt_int8_bs16_count4 | Quantization with INT8 in addition to the graph optimization (batch size=16, instance groups=4) |\n### Deploy inference servers by using Triton\n- In the SSH terminal, install and configure the authentication package, which manages GKE clusters:```\nexport USE_GKE_GCLOUD_AUTH_PLUGIN=Truesudo apt-get install google-cloud-sdk-gke-gcloud-auth-plugin\n```\n- Create a GKE cluster and a GPU node pool with compute nodes that use an NVIDIA T4 GPU:```\ngcloud auth logingcloud config set compute/zone us-west1-bgcloud container clusters create tensorrt-cluster \\\u00a0 \u00a0 --num-nodes=20gcloud container node-pools create t4-gpu-pool \\\u00a0 \u00a0 --num-nodes=1 \\\u00a0 \u00a0 --machine-type=n1-standard-8 \\\u00a0 \u00a0 --cluster=tensorrt-cluster \\\u00a0 \u00a0 --accelerator type=nvidia-tesla-t4,count=1\n```The `--num-nodes` flag specifies 20 instances for the GKE cluster and one instance for the GPU node pool `t4-gpu-pool` .The GPU node pool consists of a single `n1-standard-8` instance with an NVIDIA T4 GPU. The number of GPU instances should be equal to or larger than the number of inference server pods, because the NVIDIA T4 GPU cannot be shared by multiple pods on the same instance.\n- Show the cluster information:```\ngcloud container clusters list\n```The output is similar to the following:```\nNAME    LOCATION MASTER_VERSION MASTER_IP  MACHINE_TYPE NODE_VERSION NUM_NODES STATUS\ntensorrt-cluster us-west1-b 1.14.10-gke.17 XX.XX.XX.XX n1-standard-1 1.14.10-gke.17 21   RUNNING\n```\n- Show the node pool information:```\ngcloud container node-pools list --cluster tensorrt-cluster\n```The output is similar to the following:```\nNAME   MACHINE_TYPE DISK_SIZE_GB NODE_VERSION\ndefault-pool n1-standard-1 100   1.14.10-gke.17\nt4-pool  n1-standard-8 100   1.14.10-gke.17\n```\n- Enable the `daemonSet` workload:```\ngcloud container clusters get-credentials tensorrt-clusterkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml\n```This command loads the NVIDIA GPU driver on the nodes in the GPU node pool. It also automatically loads the driver when you add a new node to the GPU node pool.\n- Deploy inference servers on the cluster:```\nsed -i.bak \"s/YOUR-BUCKET-NAME/${PROJECT_ID}-models/\" trtis_deploy.yamlkubectl create -f trtis_service.yamlkubectl create -f trtis_deploy.yaml\n``` **Note:** Don't change or replace `YOUR-BUCKET-NAME` with your real bucket name.\n- Wait a few minutes until services become available.\n- Get the `clusterIP` address of Triton and store it in an environment variable:```\nexport TRITON_IP=$(kubectl get svc inference-server \\\u00a0 -o \"jsonpath={.spec['clusterIP']}\")echo ${TRITON_IP}\n```\nAt this point, the inference server is serving four ResNet-50 models that you created in the section [Create model files with different optimizations](#create-model-files-with-different-optimizations) . Clients can specify the model to use when sending inference requests.\n### Deploy monitoring servers with Prometheus and Grafana\n- In the SSH terminal, deploy Prometheus servers on the cluster:```\nsed -i.bak \"s/CLUSTER-IP/${TRITON_IP}/\" prometheus-configmap.ymlkubectl create namespace monitoringkubectl apply -f prometheus-service.yml -n monitoringkubectl create -f clusterRole.ymlkubectl create -f prometheus-configmap.yml -n monitoringkubectl create -f prometheus-deployment.yml -n monitoring\n```\n- Get the endpoint URL of the Prometheus service.```\nip_port=$(kubectl get svc prometheus-service \\\u00a0 -o \"jsonpath={.spec['clusterIP']}:{.spec['ports'][0]['port']}\" -n monitoring)echo \"http://${ip_port}\"\n```Make a note of the Prometheus endpoint URL, because you use it to configure Grafana later.\n- Deploy Grafana servers on the cluster:```\nkubectl create -f grafana-service.yml -n monitoringkubectl create -f grafana-deployment.yml -n monitoring\n```\n- Wait a few minutes until all services become available.\n- Get the endpoint URL of the Grafana service.```\nip_port=$(kubectl get svc grafana-service \\\u00a0 -o \"jsonpath={.status['loadBalancer']['ingress'][0]['ip']}:{.spec['ports'][0]['port']}\" -n monitoring)echo \"http://${ip_port}\"\n```Make a note of the Grafana endpoint URL to use in the next step.\n- In a web browser, go to the Grafana URL that you noted in the preceding step.\n- Sign in with the default user ID and password ( `admin` and `admin` ). When prompted, change the default password.\n- Click **Add your first data source** , and in the **Timeseries databases** list, select **Prometheus** .\n- In the **Settings** tab, in the **URL** field, enter the Prometheus endpoint URL that you noted earlier.\n- Click **Save and Test** , and then return to the home screen.\n- Add a monitoring metric for `nv_gpu_utilization` :- Click **Create your first dashboard** , and then click **Add visualization** .\n- In the **Data source** list, select **Prometheus** .\n- In the **Query** tab, in the **Metric** field, enter `nv_gpu_utilization` .\n- In the **Panel options** section, in the **Title** field, enter `GPU Utilization` , and then click **Apply** .The page displays a panel for GPU utilization.\n- Add a monitoring metric for `nv_gpu_memory_used_bytes` :- Click **Add** , and select **Visualization** .\n- In the **Query** tab, in the **Metric** field, enter `nv_gpu_memory_used_bytes` .\n- In the **Panel options** section, in the **Title** field, enter `GPU Memory Used` , and then click **Save** .\n- To add the dashboard, in the **Save dashboard** panel, click **Save** .You see the graphs for GPU Utilization and GPU Memory Used.## Deploy a load testing tool\nIn this section, you deploy the Locust load testing tool on GKE, and generate workload to measure performance of the inference servers.\n- In the SSH terminal, build a Docker image that contains Triton client libraries, and upload it to Container Registry:```\ncd ../clientgit clone https://github.com/triton-inference-server/servercd servergit checkout r19.05sed -i.bak \"s/bootstrap.pypa.io\\/get-pip.py/bootstrap.pypa.io\\/pip\\/2.7\\/get-pip.py/\" Dockerfile.clientdocker build -t tritonserver_client -f Dockerfile.client .gcloud auth configure-dockerdocker tag tritonserver_client \\\u00a0 \u00a0 gcr.io/${PROJECT_ID}/tritonserver_clientdocker push gcr.io/${PROJECT_ID}/tritonserver_client\n```The build process can take about 5 minutes. When the process is complete, a command prompt appears in the SSH terminal.\n- When the build process is finished, build a Docker image to generate testing workload, and upload it to Container Registry:```\ncd ..sed -i.bak \"s/YOUR-PROJECT-ID/${PROJECT_ID}/\" Dockerfiledocker build -t locust_tester -f Dockerfile .docker tag locust_tester gcr.io/${PROJECT_ID}/locust_testerdocker push gcr.io/${PROJECT_ID}/locust_tester\n```Don't change or replace `YOUR-PROJECT-ID` in the commands.This image is built from the image that you created in the previous step.\n- Deploy the Locust files `service_master.yaml` and `deployment_master.yaml` :```\nsed -i.bak \"s/YOUR-PROJECT-ID/${PROJECT_ID}/\" deployment_master.yamlsed -i.bak \"s/CLUSTER-IP-TRTIS/${TRITON_IP}/\" deployment_master.yamlkubectl create namespace locustkubectl create configmap locust-config --from-literal model=original --from-literal saddr=${TRITON_IP} --from-literal rps=10 -n locustkubectl apply -f service_master.yaml -n locustkubectl apply -f deployment_master.yaml -n locust\n```The `configmap` resource is used to specify the machine learning model to which clients send requests for inference.\n- Wait a few minutes until services become available.\n- Get the `clusterIP` address of the `locust-master` client, and store that address in an environment variable:```\nexport LOCUST_MASTER_IP=$(kubectl get svc locust-master -n locust \\\u00a0 \u00a0 -o \"jsonpath={.spec['clusterIP']}\")echo ${LOCUST_MASTER_IP}\n```\n- Deploy the Locust client:```\nsed -i.bak \"s/YOUR-PROJECT-ID/${PROJECT_ID}/\" deployment_slave.yamlsed -i.bak \"s/CLUSTER-IP-LOCUST-MASTER/${LOCUST_MASTER_IP}/\" deployment_slave.yamlkubectl apply -f deployment_slave.yaml -n locust\n```These commands deploy 10 Locust client Pods that you can use to generate testing workloads. If you can't generate enough requests with the current number of clients, you can change the number of Pods by using the following command:```\nkubectl scale deployment/locust-slave --replicas=20 -n locust\n```When there is not enough capacity for a default cluster to increase the number of replicas, we recommend that you increase the number of nodes in the GKE cluster.\n- Copy the URL of the Locust console, and then open this URL in a web browser:```\nexport LOCUST_IP=$(kubectl get svc locust-master -n locust \\\u00a0 \u00a0 \u00a0-o \"jsonpath={.status.loadBalancer.ingress[0].ip}\")echo \"http://${LOCUST_IP}:8089\"\n```The Locust console opens, and you can generate testing workloads from it.## Check the running Pods\nTo ensure that the components are deployed successfully, check that the Pods are running.\n- In the SSH terminal, check the inference server Pod:```\nkubectl get pods\n```The output is similar to the following:```\nNAME        READY STATUS RESTARTS AGE\ninference-server-67786cddb4-qrw6r 1/1  Running 0   83m\n```If you don't get the expected output, ensure that you've completed the steps in [Deploy inference servers by using Triton](#deploy_inference_servers_by_using_triton) .\n- Check the Locust Pods:```\nkubectl get pods -n locust\n```The output is similar to the following:```\nNAME        READY STATUS RESTARTS AGE\nlocust-master-75f6f6d4bc-ttllr  1/1  Running 0   10m\nlocust-slave-76ddb664d9-8275p  1/1  Running 0   2m36s\nlocust-slave-76ddb664d9-f45ww  1/1  Running 0   2m36s\nlocust-slave-76ddb664d9-q95z9  1/1  Running 0   2m36s\n```If you don't get the expected output, ensure that you've completed the steps in [Deploy a load testing tool](#deploy-load-testing-tool) .\n- Check the monitoring Pods:```\nkubectl get pods -n monitoring\n```The output is similar to the following:```\nNAME          READY STATUS RESTARTS AGE\ngrafana-deployment-644bbcb84-k6t7v  1/1  Running 0   79m\nprometheus-deployment-544b9b9f98-hl7q8 1/1  Running 0   81m\n```If you don't get the expected output, ensure that you've completed the steps in [Deploy monitoring servers with Prometheus and Grafana](#deploy-monitoring-servers) .\nIn the next part of this series, you use this inference server system to learn how various optimizations improve performance and how to interpret those optimizations. For next steps, see [Measure and tune performance of a TensorFlow inference system](/architecture/scalable-tensorflow-inference-system/measure-deployment) .\n## What's next\n- Learn more about [Google Kubernetes Engine (GKE)](/kubernetes-engine) .\n- Learn more about [Cloud Load Balancing](/load-balancing) .\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Docs"}