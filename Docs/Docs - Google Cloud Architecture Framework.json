{"title": "Docs - Google Cloud Architecture Framework", "url": "https://cloud.google.com/architecture/framework/printable", "abstract": "# Docs - Google Cloud Architecture Framework\nLast reviewed 2023-11-09 UTC\nYou can print this page or save the page in PDF format using your browser's print function and choose the **Save as PDF** option. For the standard version of this page, return to the [ Architecture Framework](/architecture/framework) .\nThe Google Cloud Architecture Framework provides recommendations and describes best practices to help architects, developers, administrators, and other cloud practitioners design and operate a cloud topology that's secure, efficient, resilient, high-performing, and cost-effective. The Google Cloud Architecture Framework is our version of a well-architected framework.\nA cross-functional team of experts at Google validates the design recommendations and best practices that make up the Architecture Framework. The team curates the Architecture Framework to reflect the expanding capabilities of Google Cloud, industry best practices, community knowledge, and feedback from you. For a summary of the significant changes, see [What's new](/architecture/framework/whats-new) .\nThe design guidance in the Architecture Framework applies to applications built for the cloud and for workloads migrated from on-premises to Google Cloud, hybrid cloud deployments, and multi-cloud environments.\nThe Google Cloud Architecture Framework is organized into six categories (also known as ), as shown in the following diagram:\nTo view summaries of Google Cloud products and how they relate to one another, see [Google Cloud products, features, and services in four words or less](https://googlecloudcheatsheet.withgoogle.com/) .# Google Cloud Architecture Framework: System design\nSystem design is the foundational category of the [Google Cloud Architecture Framework](/architecture/framework) . This category provides design recommendations and describes best practices and principles to help you define the architecture, components, modules, interfaces, and data on a cloud platform to satisfy your system requirements. You also learn about Google Cloud products and features that support system design.\nThe documents in the system design category assume that you understand basic system design principles. These documents don't assume that you are familiar with cloud concepts and Google Cloud products.\nFor complex cloud migration and deployment scenarios, we recommend that you use [Google Cloud consulting services](/consulting) . Our consultants provide expertise on best practices and guiding principles to help you succeed in your cloud journey. Google Cloud also has a strong ecosystem of [partners](/partners) , from large global systems integrators to partners with a deep specialization in a particular area like machine learning. We recommend that you engage Google Cloud partners to accelerate your digital transformation and improve business outcomes.\n**Note:** Before you change your production environment, we recommend that you experiment with new features or design in a sandbox environment.\nIn the system design category of the Architecture Framework, you learn to do the following:\n- [Apply core principles of system design](/architecture/framework/system-design/principles) .\n- [Select geographic regions to support your business applications](/architecture/framework/system-design/geographic-zones-regions) .\n- [Manage cloud resources](/architecture/framework/system-design/resource-management) .\n- [Choose and manage compute](/architecture/framework/system-design/compute) .\n- [Design your network infrastructure](/architecture/framework/system-design/networking) .\n- [Select and implement a storage strategy](/architecture/framework/system-design/storage) .\n- [Optimize your database](/architecture/framework/system-design/databases) .\n- [Analyze your data](/architecture/framework/system-design/data-analytics) .\n- [Implement machine learning](/architecture/framework/system-design/ai-ml) .\n- [Design your cloud workloads for sustainability](/architecture/framework/system-design/sustainability) .# Core principles of system design\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) describes the core principles of system design. A robust system design is secure, reliable, scalable, and independent. It lets you make iterative and reversible changes without disrupting the system, minimize potential risks, and improve operational efficiency. To achieve a robust system design, we recommend that you follow four core principles.\n", "content": "## Document everything\nWhen you start to move your workloads to the cloud or build your applications, a major blocker to success is lack of documentation of the system. Documentation is especially important for correctly visualizing the architecture of your current deployments.\nA properly documented cloud architecture establishes a common language and standards, which enable cross-functional teams to communicate and collaborate effectively. It also provides the information that's necessary to identify and guide future design decisions. Documentation should be written with your use cases in mind, to provide context for the design decisions.\nOver time, your design decisions will evolve and change. The change history provides the context that your teams require to align initiatives, avoid duplication, and measure performance changes effectively over time. Change logs are particularly valuable when you onboard a new cloud architect who is not yet familiar with your current system design, strategy, or history.\n## Simplify your design and use fully managed services\nSimplicity is crucial for system design. If your architecture is too complex to understand, it will be difficult to implement the design and manage it over time. Where feasible, use fully managed services to minimize the risks, time, and effort associated with managing and maintaining baseline systems.\nIf you're already running your workloads in production, test with managed services to see how they might help to reduce operational complexities. If you're developing new workloads, then start simple, establish a minimal viable product (MVP), and resist the urge to over-engineer. You can identify exceptional use cases, iterate, and improve your systems incrementally over time.\n## Decouple your architecture\nDecoupling is a technique that's used to separate your applications and service components into smaller components that can operate independently. For example, you might break up a monolithic application stack into separate service components. In a decoupled architecture, an application can run its functions independently, regardless of the various dependencies.\nA decoupled architecture gives you increased flexibility to do the following:\n- Apply independent upgrades.\n- Enforce specific security controls.\n- Establish reliability goals for each subsystem.\n- Monitor health.\n- Granularly control performance and cost parameters.\nYou can start decoupling early in your design phase or incorporate it as part of your system upgrades as you scale.\n## Use a stateless architecture\nA stateless architecture can increase both the reliability and scalability of your applications.\nStateful applications rely on various dependencies to perform tasks, such as locally cached data. Stateful applications often require additional mechanisms to capture progress and restart gracefully. Stateless applications can perform tasks without significant local dependencies by using shared storage or cached services. A stateless architecture enables your applications to scale up quickly with minimum boot dependencies. The applications can withstand hard restarts, have lower downtime, and provide better performance for end users.\nThe system design category describes recommendations to make your applications stateless or to utilize cloud-native features to improve capturing machine state for your stateful applications.\n## What's next\n- [Select geographic regions to support your business applications](/architecture/framework/system-design/geographic-zones-regions) .\n- [Manage cloud resources](/architecture/framework/system-design/resource-management) .\n- [Choose and manage compute](/architecture/framework/system-design/compute) .\n- [Design your network infrastructure](/architecture/framework/system-design/networking) .\n- [Select and implement a storage strategy](/architecture/framework/system-design/storage) .\n- [Optimize your database](/architecture/framework/system-design/databases) .\n- [Analyze your data](/architecture/framework/system-design/data-analytics) .\n- [Implement machine learning](/architecture/framework/system-design/ai-ml) .\n- [Design your cloud workloads for sustainability](/architecture/framework/system-design/sustainability) .# Choose Google Cloud deployment archetypes\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) describes six \u2014zonal, regional, multi-regional, global, hybrid, and multicloud\u2014that you can use to build architectures for your cloud workloads based your requirements for availability, cost, performance, and operational efficiency.\n## What is a deployment archetype?\nA deployment archetype is an abstract, provider-independent model that you use as the foundation to build application-specific that meet your business and technical requirements. Each deployment archetype specifies a combination of failure domains where an application can run. These failure domains can be one or more [Google Cloud zones or regions](/architecture/infra-reliability-guide/building-blocks#regions_and_zones) , and they can extend to include your on-premises data centers or failure domains in other cloud providers.\nThe following diagram shows six applications deployed in Google Cloud. Each application uses a deployment archetype that meets its specific requirements.\nAs the preceding diagram shows, in an architecture that uses the hybrid or multicloud deployment archetype, the cloud topology is based on one of the archetypes: zonal, regional, multi-regional, or global. In this sense, the hybrid and multicloud deployment archetypes can be considered as deployment archetypes that include one of the basic archetypes.\n**Note:** Deployment archetypes are different from [location scopes](/architecture/infra-reliability-guide/building-blocks#location_scopes) . The location scope of a Google Cloud resource defines its availability boundary. For example, the location scope of a Compute Engine VM is . This means that if the Google Cloud zone in which a VM is provisioned has an outage, the availability of the VM is affected. However, by distributing VMs across multiple zones, you can build a highly available architecture that's based on the deployment archetype.\nChoosing a deployment archetype helps to simplify subsequent decisions regarding the Google Cloud products and features that you should use. For example, for a highly available containerized application, if you choose the regional deployment archetype, then regional Google Kubernetes Engine (GKE) clusters are more appropriate than zonal GKE clusters.\nWhen you choose a deployment archetype for an application, you need to consider tradeoffs between factors like availability, cost, and operational complexity. For example, if an application serves users in multiple countries and needs high availability, you might choose the multi-regional deployment archetype. But for an internal application that's used by employees in a single geographical region, you might prioritize cost over availability and, therefore, choose the regional deployment archetype.\n## Overview of the deployment archetypes\nThe following tabs provide definitions for the deployment archetypes and a summary of the use cases and design considerations for each.\nYour application runs within a single Google Cloud zone, as shown in the following diagram:| 0      | 1                                                              |\n|:----------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Use cases    | Development and test environments. Applications that don't need high availability. Low-latency networking between application components. Migrating commodity workloads. Applications that use license-restricted software.       |\n| Design considerations | Downtime during zone outages. For business continuity, you can provision a passive replica of the application in another zone in the same region. If a zone outage occurs, you can restore the application to production by using the passive replica. |\n| More information  | See the following sections: Zonal deployment archetype Comparative analysis of all the deployment archetypes                                   |Your application runs independently in two or more zones within a single Google Cloud region, as shown in the following diagram:| 0      | 1                                                                         |\n|:----------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Use cases    | Highly available applications that serves users within a geographic area. Compliance with data residency and sovereignty requirements.                                        |\n| Design considerations | Downtime during region outages. For business continuity, you can back up the application and data to another region. If a region outage occurs, you can use the backups in the other region to restore the application to production. Cost and effort to provision and manage redundant resources. |\n| More information  | See the following sections: Regional deployment archetype Comparative analysis of all the deployment archetypes                                             |Your application runs independently in multiple zones across two or more Google Cloud regions. You can use [DNS routing policies](/dns/docs/policies-overview#routing_policies) to route incoming traffic to the regional load balancers. The regional load balancers then distribute the traffic to the zonal replicas of the application, as shown in the following diagram:| 0      | 1                                                         |\n|:----------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Use cases    | Highly available application with geographically dispersed users. Applications that require low end-user latency experience. Compliance with data residency and sovereignty requirements by using a geofenced DNS routing policy. |\n| Design considerations | Cost for cross-region data transfer and data replication. Operational complexity.                                     |\n| More information  | See the following sections: Multiregional deployment archetype Comparative analysis of all the deployment archetypes                            |Your application runs across Google Cloud regions worldwide, either as a globally distributed (location-unaware) stack or as regionally isolated stacks. A global [anycast](https://wikipedia.org/wiki/Anycast) load balancer distributes traffic to the region that's nearest to the user. Other components of the application stack can also be global, such as the database, cache, and object store.\nThe following diagram shows the globally distributed variant of the global deployment archetype. A global anycast load balancer forwards requests to an application stack that's distributed across multiple regions and that uses a globally replicated database.The following diagram shows a variant of the global deployment archetype with regionally isolated application stacks. A global anycast load balancer forwards requests to an application stack in one of the regions. All the application stacks use a single, globally replicated database.| 0      | 1                                                  |\n|:----------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Use cases    | Highly available applications that serve globally dispersed users. Opportunity to optimize cost and simplify operations by using global resources instead of multiple instances of regional resources. |\n| Design considerations | Costs for cross-region data transfer and data replication.                                    |\n| More information  | See the following sections: Global deployment archetype Comparative analysis of all the deployment archetypes                       |Certain parts of your application are deployed in Google Cloud, while other parts run on-premises, as shown in the following diagram. The topology in Google Cloud can use the zonal, regional, multi-regional, or global deployment archetype.| 0      | 1                                                        |\n|:----------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Use cases    | Disaster recovery (DR) site for on-premises workloads. On-premises development for cloud applications. Progressive migration to the cloud for legacy applications. Enhancing on-premises applications with cloud capabilities. |\n| Design considerations | Setup effort and operational complexity. Cost of redundant resources.                                       |\n| More information  | See the following sections: Hybrid deployment archetype Comparative analysis of all the deployment archetypes                             |Some parts of your application are deployed in Google Cloud, and other parts are deployed in other cloud platforms, as shown in the following diagram. The topology in each cloud platform can use the zonal, regional, multi-regional, or global deployment archetype.| 0      | 1                                |\n|:----------------------|:---------------------------------------------------------------------------------------------------------------------------------|\n| Use cases    | Google Cloud as the primary site and another cloud as a DR site. Enhancing applications with advanced Google Cloud capabilities. |\n| Design considerations | Setup effort and operational complexity. Cost of redundant resources and cross-cloud network traffic.       |\n| More information  | See the following sections: Multicloud deployment archetype Comparative analysis of all the deployment archetypes    |# Select geographic zones and regions\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices to deploy your system based on geographic requirements. You learn how to select optimal geographic zones and regions based on availability and proximity, to support compliance, optimize costs, and implement load balancing.\nWhen you select a region or multiple regions for your business applications, you consider criteria including service availability, end-user latency, application latency, cost, and regulatory or sustainability requirements. To support your business priorities and policies, balance these requirements and identify the best tradeoffs. For example, the most compliant region might not be the most cost-efficient region or it might not have the lowest carbon footprint.\n## Deploy over multiple regions\nare independent geographic areas that consist of multiple zones. A is a deployment area for Google Cloud resources within a region; each zone represents a single failure domain within a region.\nTo help protect against expected downtime (including maintenance) and help protect against unexpected downtime like incidents, we recommend that you deploy fault-tolerant applications that have high availability and deploy your applications across multiple zones in one or more regions. For more information, see [Geography and regions](/docs/geography-and-regions) , [Application deployment considerations](/docs/geography-and-regions#application_deployment_considerations) , and [Best practices for Compute Engine regions selection](/solutions/best-practices-compute-engine-region-selection) .\nMulti-zonal deployments can provide resiliency if multi-region deployments are limited due to cost or other considerations. This approach is especially helpful in preventing zonal or regional outages and in addressing disaster recovery and business continuity concerns. For more information, see [Design for scale and high availability](/architecture/framework/reliability/design-scale-high-availability) .\n## Select regions based on geographic proximity\nLatency impacts the user experience and affects costs associated with serving external users. To minimize latency when serving traffic to external users, select a region or set of regions that are geographically close to your users and where your services run in a compliant way. For more information, see [Cloud locations](/about/locations) and the [Compliance resource center](/security/compliance) .\n## Select regions based on available services\nSelect a region based on the available services that your business requires. Most services are available across all regions. Some enterprise-specific services might be available in a subset of regions with their initial release. To verify region selection, see [Cloud locations](/about/locations) .\n## Choose regions to support compliance\nSelect a specific region or set of regions to meet geographic regulatory or compliance regulations that require the use of certain geographies, for example [General Data Protection Regulation (GDPR)](https://gdpr-info.eu/) or data residency. To learn more about designing secure systems, see [Compliance offerings](/security/compliance/offerings#/regions=Global) and [Data residency, operational transparency, and privacy for European customers on Google Cloud](https://services.google.com/fh/files/misc/googlecloud_european_commitments_whitepaper.pdf) .\n## Compare pricing of major resources\nRegions have different cost rates for the same services. To identify a cost-efficient region, compare pricing of the major resources that you plan to use. Cost considerations differ depending on backup requirements and resources like compute, networking, and data storage. To learn more, see the [Costoptimization category](/architecture/framework/cost-optimization) .\n## Use Cloud Load Balancing to serve global users\nTo improve the user experience when you serve global users, use [Cloud Load Balancing](/load-balancing) to help provide a single IP address that is routed to your application. To learn more about designing reliable systems, see [Google Cloud Architecture Framework: Reliability](/architecture/framework/reliability) .\n## Use the Cloud Region Picker to support sustainability\nGoogle has been carbon neutral since 2007 and is committed to being carbon-free by 2030. To select a region by its carbon footprint, use the [Google Cloud Region Picker](https://googlecloudplatform.github.io/region-picker/) . To learn more about designing for sustainability, see [Cloud sustainability](/sustainability) .\n## What's next\nLearn how to [manage your cloud resources](/architecture/framework/system-design/resource-management) using [Resource Manager](/resource-manager/docs) , the Google Cloud [resource hierarchy](/resource-manager/docs/cloud-platform-resource-hierarchy) , and the [Organization Policy Service](/resource-manager/docs/organization-policy/overview) .\nExplore other categories in the [Architecture Framework](/architecture/framework) such as reliability, operational excellence, and security, privacy, and compliance.# Manage cloud resources\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices to organize and manage your resources in Google Cloud.\n## Resource hierarchy\nGoogle Cloud resources are arranged [hierarchically](/resource-manager/docs/cloud-platform-resource-hierarchy) in organizations, folders, and projects. This hierarchy lets you manage common aspects of your resources like access control, configuration settings, and policies. For best practices to design the hierarchy of your cloud resources, see [Decide a resource hierarchy for your Google Cloud landing zone](/architecture/landing-zones/decide-resource-hierarchy) .\n## Resource labels and tags\nThis section provides best practices for using labels and tags to organize your Google Cloud resources.\n### Use a simple folder structure\nFolders let you group any combination of projects and subfolders. Create a simple folder structure to organize your Google Cloud resources. You can add more levels as needed to define your resource hierarchy so that it supports your business needs. The folder structure is flexible and extensible.To learn more, see [Creating and managing folders](/resource-manager/docs/creating-managing-folders) .\n### Use folders and projects to reflect data governance policies\nUse folders, subfolders, and projects to separate resources from each other to reflect data governance policies within your organization. For example, you can use a combination of folders and projects to separate financial, human resources, and engineering.\nUse projects to group resources that share the same trust boundary. For example, resources for the same product or microservice can belong to the same project. For more information, see [Decide a resource hierarchy for your Google Cloud landing zone](/architecture/landing-zones/decide-resource-hierarchy) .\n### Use tags and labels at the outset of your project\nUse [labels and tags](/resource-manager/docs/tags/tags-overview) when you start to use Google Cloud products, even if you don't need them immediately. Adding labels and tags later on can require manual effort that can be error prone and difficult to complete.\nA provides a way to conditionally allow or deny policies based on whether a resource has a specific tag. A is a key-value pair that helps you organize your Google Cloud instances. For more information on labels, see [requirements for labels](/resource-manager/docs/creating-managing-labels#requirements) , a [list of services that support labels](/resource-manager/docs/creating-managing-labels#label_support) , and [label formats](/compute/docs/labeling-resources) .\n[Resource Manager](/resource-manager/docs) provides [labels and tags](/resource-manager/docs/tags/tags-overview) to help you manage resources, allocate and report on cost, and assign policies to different resources for granular access controls. For example, you can use labels and tags to apply granular access and management principles to different tenant resources and services. For information about VM labels and network tags, see [Relationship between VM labels and network tags](/compute/docs/labeling-resources#labels_tags) .\nYou can use labels for multiple purposes, including the following:\n- Managing resource billing: Labels are available in the billing system, which lets you separate cost by labels. For example, you can label different cost centers or budgets.\n- Grouping resources by similar characteristics or by relation: You can use labels to separate different application lifecycle stages or environments. For example, you can label production, development, and testing environments.\n### Assign labels to support cost and billing reporting\nTo support granular cost and billing reporting based on attributes outside of your integrated reporting structures (like per-project or per-product type), assign labels to resources. Labels can help you allocate consumption to cost centers, departments, specific projects, or internal recharge mechanisms. For more information, see the [Cost optimization category](/architecture/framework/cost-optimization) .\n### Avoid creating large numbers of labels\nAvoid creating large numbers of labels. We recommend that you create labels primarily at the project level, and that you avoid creating labels at the sub-team level. If you create overly granular labels, it can add noise to your analytics. To learn about common use cases for labels, see [Common uses of labels](/resource-manager/docs/creating-managing-labels#common-uses) .\n### Avoid adding sensitive information to labels\nLabels aren't designed to handle sensitive information. Don't include sensitive information in labels, including information that might be personally identifiable, like an individual's name or title.\n### Anonymize information in project names\nFollow a project naming pattern like `` `-` `` `-` `` , where the placeholders are unique and don't reveal company or application names. Don't include attributes that can change in the future, for example, a team name or technology.\n### Apply tags to model business dimensions\nYou can apply tags to model additional business dimensions like organization structure, regions, workload types, or cost centers. To learn more about tags, see [Tags overview](/resource-manager/docs/tags/tags-overview) , [Tag inheritance](/resource-manager/docs/tags/tags-overview#inheritance) , and [Creating and managing tags](/resource-manager/docs/tags/tags-creating-and-managing) . To learn how to use tags with policies, see [Policies and tags](/resource-manager/docs/tags/tags-overview#policies) . To learn how to use tags to manage access control, see [Tags and access control](/iam/docs/tags-access-control) .\n## Organizational policies\nThis section provides best practices for configuring governance rules on Google Cloud resources across the cloud resource hierarchy.\n### Establish project naming conventions\nEstablish a standardized project naming convention, for example, `` `-` `` ( `dev` , `test` , `uat` , `stage` , `prod` ).\nProject names have a 30-character limit.\nAlthough you can apply a prefix like `` `-` `` , project names can become out of date when companies go through reorganizations. Consider moving identifiable names from project names to project labels.\n### Automate project creation\nTo create projects for production and large-scale businesses, use an automated process like the [Deployment Manager](/deployment-manager/docs) or the [Google Cloud project factory Terraform module](https://github.com/terraform-google-modules/terraform-google-project-factory) . These tools do the following:\n- Automatically create development, test, and production environments or projects that have the appropriate permissions.\n- Configure logging and monitoring.\nThe [Google Cloud project factory Terraform module](https://github.com/terraform-google-modules/terraform-google-project-factory) helps you to automate the creation of Google Cloud projects. In large enterprises, we recommend that you review and approve projects before you create them in Google Cloud. This process helps to ensure the following:\n- Costs can be attributed. For more information, see the [Costoptimization category](/architecture/framework/cost-optimization) .\n- Approvals are in place for data uploads.\n- Regulatory or compliance requirements are met.\nWhen you automate the creation and management of Google Cloud projects and resources, you get the benefit of consistency, reproducibility, and testability. Treating your configuration as code lets you version and manage the lifecycle of your configuration together with your software artifacts. Automation lets you support best practices like consistent naming conventions and labeling of resources. As your requirements evolve, automation simplifies project refactoring.\n### Audit your systems regularly\nTo ensure that requests for new projects can be audited and approved, integrate with your enterprise's ticketing system or a standalone system that provides auditing.\n### Configure projects consistently\nConfigure projects to consistently meet your organization's needs. Include the following when you set up projects:\n- Project ID and naming conventions\n- Billing account linking\n- Networking configuration\n- Enabled APIs and services\n- Compute Engine access configuration\n- Logs export and usage reports\n- [Project removal lien](/resource-manager/docs/project-liens) \n### Decouple and isolate workloads or environments\nQuotas and limits are enforced at the project level. To manage quotas and limits, decouple and isolate workloads or environments at the project level. For more information, see [Working with quotas](/docs/quota) .\nDecoupling environments is different from data classification requirements. Separating data from infrastructure can be expensive and complex to implement, so we recommend that you implement data classification based on data sensitivity and compliance requirements.\n### Enforce billing isolation\nEnforce billing isolation to support different billing accounts and cost visibility per workload and environment. For more information, see [Create, modify, or close your self-serve Cloud Billing account ](/billing/docs/how-to/manage-billing-account) and [Enable, disable, or change billing for a project](/billing/docs/how-to/modify-project) .\nTo minimize administrative complexity, use granular access management controls for critical environments at the project level, or for workloads that spread across multiple projects. When you curate access control for critical production applications, you ensure that workloads are secured and managed effectively.\n### Use the Organization Policy Service to control resources\nThe [Organization Policy Service](/resource-manager/docs/organization-policy/overview) gives policy administrators centralized and programmatic control over your organization's cloud resources so that they can configure constraints across the [resource hierarchy](/resource-manager/docs/cloud-platform-resource-hierarchy) . For more information, see [Add an organization policy administrator](/resource-manager/docs/organization-policy/using-constraints#add-org-policy-admin) .\n### Use the Organization Policy Service to comply with regulatory policies\nTo meet compliance requirements, use the [Organization Policy Service](/resource-manager/docs/organization-policy/overview) to enforce compliance requirements for resource sharing and access. For example, you can limit sharing with external parties or determine where to deploy cloud resources geographically. Other compliance scenarios include the following:\n- Centralizing control to configure restrictions that define how your organization's resources can be used.\n- Defining and establishing policies to help your development teams remain within compliance boundaries.\n- Helping project owners and their teams make system changes while maintaining regulatory compliance and minimizing concerns about breaking compliance rules.\n### Limit resource sharing based on domain\nA restricted sharing organization policy helps you to prevent Google Cloud resources from being shared with identities outside your organization. For more information, see [Restricting identities by domain](/resource-manager/docs/organization-policy/restricting-domains) and [Organization policy constraints](/resource-manager/docs/organization-policy/org-policy-constraints) .\n### Disable service account and key creation\nTo help improve security, limit the use of Identity and Access Management (IAM) service accounts and corresponding keys. For more information, see [Restricting service account usage](/resource-manager/docs/organization-policy/restricting-service-accounts) .\n### Restrict the physical location of new resources\nRestrict the physical location of newly created resources by [restricting resource locations](/resource-manager/docs/organization-policy/defining-locations) . To see a list of constraints that give you control of your organization's resources, see [Organization Policy Service constraints](/resource-manager/docs/organization-policy/org-policy-constraints) .\n## What's next\nLearn how to [choose and manage compute](/architecture/framework/system-design/compute#platform) , including the following:\n- [Compute migration approach](/architecture/framework/system-design/compute#migration) .\n- [Design workloads](/architecture/framework/system-design/compute#design) and [scale your workloads](/architecture/framework/system-design/compute#scale) .\n- [Manage operations](/architecture/framework/system-design/compute#manage-ops) .\n- [Manage VM migrations](/architecture/framework/system-design/compute#vm-migrate) .\nExplore other categories in the [Architecture Framework](/architecture/framework) such as reliability, operational excellence, and security, privacy, and compliance.# Choose and manage compute\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices to deploy your system based on compute requirements. You learn how to choose a compute platform and a migration approach, design and scale workloads, and manage operations and VM migrations.\nComputation is at the core of many workloads, whether it refers to the execution of custom business logic or the application of complex computational algorithms against datasets. Most solutions use compute resources in some form, and it's critical that you select the right compute resources for your application needs.\nGoogle Cloud provides several options for using time on a CPU. Options are based on CPU types, performance, and how your code is scheduled to run, including usage billing.\nGoogle Cloud compute options include the following:\n- Virtual machines (VM) with cloud-specific benefits like live migration.\n- Bin-packing of containers on cluster-machines that can share CPUs.\n- Functions and serverless approaches, where your use of CPU time can be metered to the work performed during a single HTTP request.## Choosing compute\nThis section provides best practices for choosing and migrating to a compute platform.\n### Choose a compute platform\nWhen you choose a compute platform for your workload, consider the technical requirements of the workload, lifecycle automation processes, regionalization, and security.\nEvaluate the nature of CPU usage by your app and the entire supporting system, including how your code is packaged and deployed, distributed, and invoked. While some scenarios might be compatible with multiple platform options, a portable workload should be capable and performant on a range of compute options.\nThe following table provides an overview of the recommended Google Cloud compute services for various use cases:\n| Compute platform  | Use cases                                        | Recommended products                                                                                                                                  |\n|:-----------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Serverless    | Deploy your first app. Focus on data and processing logic and on app development, rather than maintaining infrastructure operations.         | Cloud Run: Put your business logic in containers by using this fully managed serverless option. Cloud Run is designed for workloads that are compute intensive, but not always on. Scale cost effectively from 0 (no traffic) and define the CPU and RAM of your tasks and services. Deploy with a single command and Google automatically provisions the right amount of resources. Cloud Functions: Separate your code into flexible pieces of business logic without the infrastructure concerns of load balancing, updates, authentication, or scaling. |\n| Kubernetes    | Build complex microservice architectures that need additional services like Istio to manage service mesh control.              | Google Kubernetes Engine: An open source container-orchestration engine that automates deploying, scaling, and managing containerized apps.                                                                                                     |\n| Virtual machines (VMs) | Create and run VMs from predefined and customizable VM families that support your application and workload requirements, as well as third-party software and services. | Compute Engine: Add graphics processing units (GPUs) to your VM instances. You can use these GPUs to accelerate specific workloads on your instances like machine learning and data processing. To select appropriate machine types based on your requirements, see Recommendations for machine families.                                                             |\nFor more information, see [Choosing compute options](/docs/choosing-a-compute-option) .\n### Choose a compute migration approach\nIf you're migrating your existing applications from another cloud or from on-premises, use one of the following Google Cloud products to help you optimize for performance, scale, cost, and security.\n| Migration goal  | Use case                     | Recommended product   |\n|:----------------------|:-----------------------------------------------------------------------------------------|:----------------------------|\n| Lift and shift  | Migrate or extend your VMware workloads to Google Cloud in minutes.      | Google Cloud VMware Engine |\n| Lift and shift  | Move your VM-based applications to Compute Engine.          | Migrate to Virtual Machines |\n| Upgrade to containers | Modernize traditional applications into built-in containers on Google Kubernetes Engine. | Migrate to Containers  |\nTo learn how to migrate your workloads while aligning internal teams, see [VM Migration lifecycle](/migrate/compute-engine/docs/5.0/concepts/lifecycle) and [Building a Large Scale Migration Program with Google Cloud](https://inthecloud.withgoogle.com/building-a-large-scale-migration-20/dl-cd.html) .\n## Designing workloads\nThis section provides best practices for designing workloads to support your system.\n### Evaluate serverless options for simple logic\nis a type of compute that doesn't require specialized hardware or machine types like CPU-optimized machines. Before you invest in [Google Kubernetes Engine (GKE)](/kubernetes-engine/docs) or [Compute Engine](/compute/docs) implementations to abstract operational overhead and optimize for cost and performance, evaluate [serverless options](#platform) for lightweight logic.\n### Decouple your applications to be stateless\nWhere possible, decouple your applications to be stateless to maximize use of [serverless computing](/serverless) options. This approach lets you use managed compute offerings, scale applications based on demand, and optimize for cost and performance. For more information about decoupling your application to design for scale and high availability, see [Design for scale and high availability](/architecture/framework/reliability/design-scale-high-availability) .\n### Use caching logic when you decouple architectures\nIf your application is designed to be stateful, use caching logic to decouple and make your workload scalable. For more information, see [Database best practices](/architecture/framework/system-design/databases#caching) .\n### Use live migrations to facilitate upgrades\nTo facilitate Google maintenance upgrades, use live migration by setting instance availability policies. For more information, see [Set VM host maintenance policy](/compute/docs/instances/host-maintenance-options) .\n## Scaling workloads\nThis section provides best practices for scaling workloads to support your system.\n### Use startup and shutdown scripts\nFor stateful applications, use [startup](/compute/docs/instances/startup-scripts) and [shutdown](/compute/docs/shutdownscript) scripts where possible to start and stop your application state gracefully. A is when a computer is turned on by a software function and the operating system is allowed to perform its tasks of safely starting processes and opening connections.\nGraceful startups and shutdowns are important because stateful applications depend on immediate availability to the data that sits close to the compute, usually on local or persistent disks, or in RAM. To avoid running application data from the beginning for each startup, use a startup script to reload the last saved data and run the process from where it previously stopped on shutdown. To save the application memory state to avoid losing progress on shutdown, use a shutdown script. For example, use a shutdown script when a VM is scheduled to be shut down due to downscaling or Google maintenance events.\n### Use MIGs to support VM management\nWhen you use [Compute Engine VMs](/compute/docs/quickstarts) , [managed instance groups](/compute/docs/instance-groups/creating-groups-of-managed-instances) (MIGs) support features like autohealing, load balancing, autoscaling, auto updating, and [stateful workloads](/compute/docs/instance-groups/stateful-migs) . You can create zonal or [regional MIGs](/compute/docs/instance-groups/regional-migs) based on your availability goals. You can use MIGs for stateless serving or batch workloads and for stateful applications that need to preserve each VM's unique state.\n### Use pod autoscalers to scale your GKE workloads\nUse [horizontal](/kubernetes-engine/docs/concepts/horizontalpodautoscaler) and [vertical Pod autoscalers](/kubernetes-engine/docs/concepts/verticalpodautoscaler) to scale your workloads, and use [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) to scale underlying compute resources.\n### Distribute application traffic\nTo scale your applications globally, use [Cloud Load Balancing](/load-balancing) to distribute your application instances across more than one region or zone. Load balancers optimize packet routing from Google Cloud [edge networks](/vpc/docs/edge-locations) to the nearest zone, which increases serving traffic efficiency and minimizes serving costs. To optimize for end-user latency, use [Cloud CDN](/cdn/docs) to cache static content where possible.\n### Automate compute creation and management\nMinimize human-induced errors in your production environment by automating compute creation and management.\n## Managing operations\nThis section provides best practices for managing operations to support your system.\n### Use Google-supplied public images\nUse public images supplied by Google Cloud. The Google Cloud public images are regularly updated. For more information, see [List of public images available on Compute Engine](/compute/docs/images#list_of_public_images_available_on) .\nYou can also [create your own images](/compute/docs/images/create-delete-deprecate-private-images) with specific configurations and settings. Where possible, automate and centralize image creation in a separate project that you can share with authorized users within your organization. Creating and curating a custom image in a separate project lets you update, patch, and create a VM using your own configurations. You can then share the curated VM image with relevant projects.\n### Use snapshots for instance backups\nSnapshots let you create backups for your instances. Snapshots are especially useful for stateful applications, which aren't flexible enough to maintain state or save progress when they experience abrupt shutdowns. If you frequently use snapshots to create new instances, you can optimize your backup process by creating a base image from that snapshot.\n### Use a machine image to enable VM instance creation\nAlthough a snapshot only captures an image of the data inside a machine, a machine image captures machine configurations and settings, in addition to the data. Use a [machine image](/compute/docs/machine-images/create-machine-images) to store all of the configurations, metadata, permissions, and data from one or more disks that are needed to create a VM instance.\nWhen you create a machine from a snapshot, you must configure instance settings on the new VM instances, which requires a lot of work. Using machine images lets you copy those known settings to new machines, reducing overhead. For more information, see [When to use a machine image](/compute/docs/machine-images#when-to-use) .\n## Capacity, reservations, and isolation\nThis section provides best practices for managing capacity, reservations, and isolation to support your system.\n### Use committed-use discounts to reduce costs\nYou can reduce your operational expenditure (OPEX) cost for workloads that are always on by using [committed use discounts](/compute/vm-instance-pricing#committed_use) . For more information, see the [Costoptimization category](/architecture/framework/cost-optimization) .\n### Choose machine types to support cost and performance\nGoogle Cloud offers machine types that let you choose compute based on cost and performance parameters. You can choose a low-performance offering to optimize for cost or choose a high-performance compute option at higher cost. For more information, see the [Costoptimization category](/architecture/framework/cost-optimization) .\n### Use sole-tenant nodes to support compliance needs\nare physical Compute Engine servers that are dedicated to hosting only your project's VMs. Sole-tenant nodes can help you to meet compliance requirements for physical isolation, including the following:\n- Keep your VMs physically separated from VMs in other projects.\n- Group your VMs together on the same host hardware.\n- Isolate payments processing workloads.\nFor more information, see [Sole-tenant nodes](/compute/docs/nodes/sole-tenant-nodes) .\n### Use reservations to ensure resource availability\nGoogle Cloud lets you define [reservations](/compute/docs/instances/reserving-zonal-resources) for your workloads to ensure those resources are always available. There is no additional charge to create reservations, but you pay for the reserved resources even if you don't use them. For more information, see [Consuming and managing reservations](/compute/docs/instances/reserving-zonal-resources) .\n## VM migration\nThis section provides best practices for migrating VMs to support your system.\n### Evaluate built-in migration tools\nEvaluate built-in migration tools to move your workloads from another cloud or from on-premises. For more information, see [Migration to Google Cloud](/architecture/migration-to-gcp-getting-started) . Google Cloud offers tools and services to help you migrate your workloads and optimize for cost and performance. To receive a free migration cost assessment based on your current IT landscape, see [Google Cloud Rapid Assessment & Migration Program](/solutions/cloud-migration-program) .\n### Use virtual disk import for customized operating systems\nTo import customized [supported operating systems](/compute/docs/images/os-details#import) , see [Importing virtual disks](/compute/docs/import/importing-virtual-disks) . Sole-tenant nodes can help you meet your hardware bring-your-own-license requirements for per-core or per-processor licenses. For more information, see [Bringing your own licenses](/compute/docs/nodes/bringing-your-own-licenses) .\n## Recommendations\nTo apply the guidance in the Architecture Framework to your own environment, we recommend that you do following:\n- Review [Google Cloud Marketplace](/marketplace) offerings to evaluate whether your application is listed under a supported vendor. Google Cloud supports running various open source systems and various [third-party software](/compute/docs/infrastructure-software) .\n- Consider [Migrate to Containers and GKE](/migrate/anthos/docs/getting-started) to extract and package your VM-based application as a containerized application running on GKE.\n- Use [Compute Engine](/compute/docs) to run your applications on Google Cloud. If you have legacy dependencies running in a VM-based application, verify whether they meet your vendor requirements.\n- Evaluate using a Google Cloud internal passthrough Network Load Balancer to scale your decoupled architecture. For more information, see [Internal passthrough Network Load Balancer overview](/load-balancing/docs/internal) .\n- Evaluate your options for switching from traditional on-premises use cases like HA-Proxy usage. For more information, see [best practice for floating IP address](/solutions/best-practices-floating-ip-addresses) .\n- Use [VM Manager](/compute/docs/vm-manager) to manage operating systems for your large VM fleets running windows or Linux on Compute Engine, and apply consistent configuration policies.\n- Consider using [GKE Autopilot ](/kubernetes-engine/docs/concepts/autopilot-overview) and let Google SRE fully manage your clusters.\n- Use [Policy Controller](/anthos-config-management/docs/concepts/policy-controller) and [Config Sync](/anthos-config-management/docs/config-sync-overview) for policy and configuration management across your GKE clusters.\n- Ensure availability and scalability of machines in specific regions and zones. Google Cloud can scale to support your compute needs. However, if you need a lot of specific machine types in a specific region or zone, work with your account teams to ensure availability. For more information, see [Reservations for Compute Engine](https://cloud.google.com/compute/docs/instances/reservations-overview) .## What's next\nLearn [networking design principles](/architecture/framework/system-design/networking#core) , including the following:\n- Design [workload VPC architectures](/architecture/framework/system-design/networking#workload) .\n- Design [inter-VPC connectivity](/architecture/framework/system-design/networking#connectivity) .\nExplore other categories in the [Architecture Framework](/architecture/framework) such as reliability, operational excellence, and security, privacy, and compliance.# Design your network infrastructure\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices to deploy your system based on networking design. You learn how to choose and implement [Virtual Private Cloud (VPC)](/vpc/docs) , and how to test and manage network security.\n## Core principles\nNetworking design is critical to successful system design because it helps you optimize for performance and secure application communications with internal and external services. When you choose networking services, it's important to evaluate your application needs and evaluate how the applications will communicate with each other. For example, while some components require global services, other components might need to be geo-located in a specific region.\nGoogle's private network connects regional locations to more than 100 global network points of presence. Google Cloud uses [software-defined networking](https://www.usenix.org/system/files/conference/nsdi18/nsdi18-dalton.pdf) and distributed systems technologies to host and deliver your services around the world. Google's core element for networking within Google Cloud is the global [VPC](/vpc/docs) . VPC uses Google's global high-speed network to link your applications across regions while supporting privacy and reliability. Google ensures that your content is delivered with high throughput by using technologies like [Bottleneck Bandwidth and Round-trip propagation time (BBR)](https://cloud.google.com/blog/products/networking/tcp-bbr-congestion-control-comes-to-gcp-your-internet-just-got-faster) congestion-control intelligence.\nDeveloping your cloud networking design includes the following steps:\n- Design the workload VPC architecture. Start by identifying how many Google Cloud projects and VPC networks you require.\n- Add inter-VPC connectivity. Design how your workloads connect to other workloads in different VPC networks.\n- Design hybrid network connectivity. Design how your workload VPCs connect to on-premises and other cloud environments.\nWhen you design your Google Cloud network, consider the following:\n- A [VPC](/vpc) provides a private networking environment in the cloud for interconnecting services that are built on [Compute Engine](/compute) , [Google Kubernetes Engine (GKE)](/kubernetes-engine) , and [Serverless Computing Solutions](/serverless) . You can also use a VPC to privately access Google-managed services such as [Cloud Storage](/storage) , [BigQuery](/bigquery) , and [Cloud SQL](/sql) .\n- VPC networks, including their associated routes and firewall rules, are global resources; they aren't associated with any particular region or zone.\n- Subnets are regional resources. [Compute Engine](/compute/docs) VM instances that are deployed in different zones in the same cloud region can use IP addresses from the same subnet.\n- Traffic to and from instances can be controlled by using VPC firewall rules.\n- Network administration can be secured by using [Identity and Access Management (IAM) roles](/iam/docs/understanding-roles) .\n- VPC networks can be securely connected in hybrid environments by using [Cloud VPN](/network-connectivity/docs/vpn/concepts/overview) or [Cloud Interconnect](/network-connectivity/docs/interconnect) .\nTo see a complete list of VPC specifications, see [Specifications](/vpc/docs/vpc#specifications) .\n## Workload VPC architecture\nThis section provides best practices for designing workload VPC architectures to support your system.\n### Consider VPC network design early\nMake [VPC](/vpc) network design an early part of designing your organizational setup in Google Cloud. Organizational-level design choices can't be easily reversed later in the process. For more information, see [Best practices and reference architectures for VPC design](/architecture/best-practices-vpc-design) and [Decide the network design for your Google Cloud landing zone](/architecture/landing-zones/decide-network-design) .\n### Start with a single VPC network\nFor many use cases that include resources with common requirements, a single VPC network provides the features that you need. Single VPC networks are simple to create, maintain, and understand. For more information, see [VPC Network Specifications](/vpc/docs/vpc#specifications) .\n### Keep VPC network topology simple\nTo ensure a manageable, reliable, and well-understood architecture, keep the design of your [VPC](/vpc) network topology as simple as possible.\n### Use VPC networks in custom mode\nTo ensure that Google Cloud networking integrates seamlessly with your existing networking systems, we recommend that you use [custom mode](/vpc/docs/vpc#subnet-ranges) when you create VPC networks. Using custom mode helps you integrate Google Cloud networking into existing IP address management schemes and it lets you control which cloud regions are included in the VPC. For more information, see [VPC](/vpc) .\n## Inter-VPC connectivity\nThis section provides best practices for designing inter-VPC connectivity to support your system.\n### Choose a VPC connection method\nIf you decide to implement multiple VPC networks, you need to connect those networks. VPC networks are isolated tenant spaces within Google's [Andromeda software-defined network (SDN)](https://cloud.google.com/blog/products/networking/google-cloud-networking-in-depth-how-andromeda-2-2-enables-high-throughput-vms) . There are several ways that VPC networks can communicate with each other. Choose how you connect your network based on your bandwidth, latency, and service level agreement (SLA) requirements. To learn more about the connection options, see [Choose the VPC connection method that meets your cost, performance, and security needs](/architecture/best-practices-vpc-design#choose-method) .\n### Use Shared VPC to administer multiple working groups\nFor organizations with multiple teams, Shared VPC provides an effective tool to extend the architectural simplicity of a single VPC network across multiple working groups.\n### Use simple naming conventions\nChoose simple, intuitive, and consistent naming conventions. Doing so helps administrators and users to understand the purpose of each resource, where it's located, and how it's differentiated from other resources.\n### Use connectivity tests to verify network security\nIn the context of network security, you can use connectivity tests to verify that traffic you intend to prevent between two endpoints is blocked. To verify that traffic is blocked and why it's blocked, define a test between two endpoints and evaluate the results. For example, you might test a VPC feature that lets you define rules that support blocking traffic. For more information, see [Connectivity Tests overview](/network-intelligence-center/docs/connectivity-tests/concepts/overview) .\n### Use Private Service Connect to create private endpoints\nTo create private endpoints that let you access Google services with your own IP address scheme, use [Private Service Connect](https://codelabs.developers.google.com/codelabs/cloudnet-psc#0) . You can access the private endpoints from within your VPC and through hybrid connectivity that terminates in your VPC.\n### Secure and limit external connectivity\nLimit internet access only to those resources that need it. Resources with only a private, internal IP address can still access many Google APIs and services through [Private Google Access](/vpc/docs/private-google-access) .\n### Use Network Intelligence Center to monitor your cloud networks\n[Network Intelligence Center](/network-intelligence-center) provides a comprehensive view of your Google Cloud networks across all regions. It helps you to identify traffic and access patterns that can cause operational or security risks.\n## What's next\nLearn [best practices for storage management](/architecture/framework/system-design/storage#management) , including the following:\n- [Select a storage type](/architecture/framework/system-design/storage#storage-type) .\n- Choose [storage access patterns and workload types](/architecture/framework/system-design/storage#workload-type) .\nExplore other categories in the [Architecture Framework](/architecture/framework) such as reliability, operational excellence, and security, privacy, and compliance.# Select and implement a storage strategy\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices to deploy your system based on storage. You learn how to select a storage strategy and how to manage storage, access patterns, and workloads.\nTo facilitate data exchange and securely back up and store data, organizations need to choose a storage plan based on workload, [input/output operations per second (IOPS)](https://wikipedia.org/wiki/IOPS) , latency, retrieval frequency, location, capacity, and format (block, file, and object).\n[Cloud Storage](/storage/docs/introduction) provides reliable, secure object storage services, including the following:\n- Built-in redundancy options to protect your data against equipment failure and to ensure data availability during data center maintenance.\n- Data transfer options, including the following:- [Storage Transfer Service](/storage/transfer) \n- [Transfer Appliance](/transfer-appliance) \n- [BigQuery Data Transfer Service](/bigquery/docs/dts-introduction) \n- [Migration to Google Cloud: Transferring your large datasets](/architecture/migration-to-google-cloud-transferring-your-large-datasets) \n- [Storage classes](/storage/docs/storage-classes) to support your workloads.\n- Calculated checksums for all Cloud Storage operations that enable Google to verify reads and writes.\nIn Google Cloud, IOPS scales according to your provisioned storage space. Storage types like Persistent Disk require manual replication and backup because they are zonal or regional. By contrast, [object storage](/learn/what-is-object-storage) is highly available and it automatically replicates data across a single region or across multiple regions.\n## Storage type\nThis section provides best practices for choosing a storage type to support your system.\n### Evaluate options for high-performance storage needs\nEvaluate persistent disks or local solid-state drives (SSD) for compute applications that require high-performance storage. [Cloud Storage](/storage/docs) is an immutable object store with versioning. Using Cloud Storage with [Cloud CDN](/cdn/docs) helps optimize for cost, especially for frequently accessed static objects.\n[Filestore](/filestore/docs) supports multi-write applications that need high-performance shared space. Filestore also supports legacy and modern applications that require [POSIX](https://wikipedia.org/wiki/POSIX) -like file operations through [Network File System](https://wikipedia.org/wiki/Network_File_System) (NFS) mounts.\nCloud Storage supports use cases such as creating data lakes and addressing archival requirements. Make tradeoff decisions based on how you choose [Cloud Storage class](/storage/docs/storage-classes) due to access and retrieval costs, especially when you configure retention policies. For more information, see [Design an optimal storage strategy for your cloud workload](/architecture/storage-advisor) .\nAll storage options are by default encrypted at rest and in-transit using Google-managed keys. For storage types such as Persistent Disk and Cloud Storage, you can either supply your own key or manage them through [Cloud Key Management Service (Cloud KMS)](/kms/docs) . Establish a strategy for handling such keys before you employ them on production data.\n### Choose Google Cloud services to support storage design\nTo learn about the Google Cloud services that support storage design, use the following table:\n| Google Cloud service  | Description                                                                                                                                |\n|:---------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Cloud Storage    | Provides global storage and retrieval of any amount of data at any time. You can use Cloud Storage for multiple scenarios including serving website content, storing data for archival and disaster recovery, or distributing large data objects to users through direct download. For more information, see the following: Cloud Storage best practices Bucket locations Storage classes Cloud Storage FUSE                               |\n| Persistent Disk   | A high-performance block storage for Google Cloud. Persistent Disk provides SSD and hard disk drive (HDD) storage that you can attach to instances running in Compute Engine or Google Kubernetes Engine (GKE). Regional disks provide durable storage and replication of data between two zones in the same region. If you need higher IOPS and low latency, Google Cloud offers Filestore. Local SSDs are physically attached to the server that hosts your virtual machine instance. You can use local SSDs as temporary disk space. |\n| Filestore     | A managed file storage service for applications that require a file system interface and a shared file system for data. Filestore gives users a seamless experience for standing up managed Network Attached Storage (NAS) with their Compute Engine and GKE instances.                                                                 |\n| Cloud Storage for Firebase | Built for app developers who need to store and serve user-generated content, such as photos or videos. All your files are stored in Cloud Storage buckets, so they are accessible from both Firebase and Google Cloud.                                                                             |\n### Choose a storage strategy\nTo select a storage strategy that meets your application requirements, use  the following table:\n| Use case                                   | Recommendations             |\n|:--------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------|\n| You want to store data at scale at the lowest cost, and access performance is not an issue.              | Cloud Storage             |\n| You are running compute applications that need immediate storage. For more information, see Optimizing Persistent Disk and Local SSD performance. | Persistent Disk or Local SSD         |\n| You are running high-performance workloads that need read and write access to shared space.              | Filestore              |\n| You have high-performance computing (HPC) or high-throughput computing (HTC) use cases.               | Using clusters for large-scale technical computing in the cloud |\n### Choose active or archival storage based on storage access needs\nA [storage class](/storage/docs/storage-classes) is a piece of [metadata](/storage/docs/metadata) that is used by every object. For data that is served at a high rate with high availability, use the Standard Storage class. For data that is infrequently accessed and can tolerate slightly lower availability, use the Nearline Storage, Coldline Storage, or Archive Storage class. For more information about cost considerations for choosing a storage class, see [Cloud Storage pricing](/storage/pricing) .\n### Evaluate storage location and data protection needs for Cloud Storage\nFor a Cloud Storage bucket located in a region, data contained within it is automatically replicated across zones within the region. Data replication across zones protects the data if there is a zonal failure within a region.\nCloud Storage also offers locations that are redundant across regions, which means data is replicated across multiple, geographically separate data centers. For more information, see [Bucket locations](/storage/docs/locations) .\n### Use Cloud CDN to improve static object delivery\nTo optimize the cost to retrieve objects and minimize access latency, use [Cloud CDN](/cdn/docs) . Cloud CDN uses the Cloud Load Balancing [external Application Load Balancer](/load-balancing/docs/https) to provide routing, health checking, and anycast IP address support. For more information, see [Setting up Cloud CDN with cloud buckets](/cdn/docs/setting-up-cdn-with-bucket#send-traffic) .\n## Storage access pattern and workload type\nThis section provides best practices for choosing storage access patterns and workload types to support your system.\n### Use Persistent Disk to support high-performance storage access\nData access patterns depend on how you design system performance. Cloud Storage provides scalable storage, but it isn't an ideal choice when you run heavy compute workloads that need high throughput access to large amounts of data. For high-performance storage access, use [Persistent Disk](/persistent-disk) .\n### Use exponential backoff when implementing retry logic\nUse exponential backoff when implementing retry logic to handle 5XX, 408, and 429 errors. Each Cloud Storage bucket is provisioned with initial I/O capacity. For more information, see [Request rate and access distribution guidelines](/storage/docs/request-rate) . Plan a gradual ramp-up for retry requests.\n## Storage management\nThis section provides best practices for storage management to support your system.\n### Assign unique names to every bucket\nMake every bucket name unique across the Cloud Storage namespace. Don't include sensitive information in a bucket name. Choose bucket and object names that are difficult to guess. For more information, see the [bucket naming guidelines](/storage/docs/buckets#naming) and [Object naming guidelines](/storage/docs/objects#naming) .\n### Keep Cloud Storage buckets private\nUnless there is a business-related reason, ensure that your Cloud Storage bucket isn't anonymously or publicly accessible. For more information, see [Overview of access control](/storage/docs/access-control#public_access_prevention) .\n### Assign random object names to distribute load evenly\nAssign random object names to facilitate performance and avoid [hotspotting](/spanner/docs/schema-design#primary-key-prevent-hotspots) . Use a randomized prefix for objects where possible. For more information, see [Use a naming convention that distributes load evenly across key ranges](/storage/docs/request-rate#naming-convention) .\n### Use public access prevention\nTo prevent access at the organization, folder, project, or bucket level, use public access prevention. For more information, see [Using public access prevention.](/storage/docs/using-public-access-prevention)\n## What's next\nLearn about [Google Cloud database services](/architecture/framework/system-design/databases#key-services) and best practices, including the following:\n- [Select](/architecture/framework/system-design/databases#select-database) and [migrate your database](/architecture/framework/system-design/databases#data-migration) .\n- Manage [database encryption](/architecture/framework/system-design/databases#encryption) .\n- Manage database [networking and access](/architecture/framework/system-design/databases#access) .\nExplore other categories in the [Architecture Framework](/architecture/framework) such as reliability, operational excellence, and security, privacy, and compliance.# Optimize your database\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices to deploy your system based on database design. You learn how to design, migrate, and scale databases, encrypt database information, manage licensing, and monitor your database for events.\n## Key services\nThis document in the Architecture Framework system design category provides best practices that include various Google Cloud database services. The following table provides a high-level overview of these services:\n| Google Cloud service  | Description                                                                                             |\n|:---------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Cloud SQL     | A fully managed database service that lets you set up, maintain, manage, and administer your relational databases that use Cloud SQL for PostgreSQL, Cloud SQL for MySQL, and Cloud SQL for SQL Server. Cloud SQL offers high performance and scalability. Hosted on Google Cloud, Cloud SQL provides a database infrastructure for applications running anywhere.       |\n| Bigtable     | A table that can scale to billions of rows and thousands of columns, letting you store up to petabytes of data. A single value in each row is indexed; this value is known as the row key. Use Bigtable to store very large amounts of single-keyed data with very low latency. It supports high read and write throughput at low latency, and it is a data source for MapReduce operations. |\n| Spanner     | A scalable, globally distributed, enterprise database service built for the cloud that includes relational database structure and non-relational horizontal scale. This combination delivers high-performance transactions and consistency across rows, regions, and continents. Spanner provides a 99.999% availability SLA, no planned downtime, and enterprise-grade security.   |\n| Memorystore    | A fully managed Redis service for Google Cloud. Applications that run on Google Cloud can increase performance by using the highly available, scalable, secure Redis service without managing complex Redis deployments.                                          |\n| Firestore     | A NoSQL document database built for automatic scaling, high performance, and application development. Although the Firestore interface has many of the same features as traditional databases, it is a NoSQL database and it describes relationships between data objects differently.                          |\n| Firebase Realtime Database | A cloud-hosted database. Firebase stores data as JSON and it synchronizes in real time to every connected client. When you build cross-platform apps with Google, iOS, Android, and JavaScript SDKs, all of your clients share one real-time database instance and automatically receive updates with the newest data.                  |\n| Open source databases  | Google partners offer different open source databases, including MongoDB, MariaDB, and Redis.                                                                        |\n| AlloyDB for PostgreSQL  | A fully managed PostgreSQL-compatible database service for demanding enterprise workloads. Provides up to 4x faster performance for transactional workloads and up to 100x faster analytical queries when compared to standard PostgreSQL. AlloyDB for PostgreSQL simplifies management with machine learning-enabled autopilot systems.              |\n## Database selection\nThis section provides best practices for choosing a database to support your system.\n### Consider using a managed database service\nEvaluate Google Cloud [managed database services](/products/databases) before you install your own database or database cluster. Installing your own database involves maintenance overhead including installing patches and updates, and managing daily operational activities like monitoring and performing backups.\nUse functional and non-functional application requirements to drive database selection. Consider low latency access, time series data processing, disaster recovery, and mobile client synchronization.\nTo migrate databases, use one of the products described in the following table:\n| Database migration product | Description                               |\n|:-----------------------------|:------------------------------------------------------------------------------------------------------------------------------------|\n| Cloud SQL     | A regional service that supports read replicas in remote regions, low-latency reads, and disaster recovery.       |\n| Spanner      | A multi-regional offering providing external consistency, global replication, and a five nines service level agreement (SLA).  |\n| Bigtable      | A fully managed, scalable NoSQL database service for large analytical and operational workloads with up to 99.999% availability. |\n| Memorystore     | A fully managed database service that provides a managed version of two popular open source caching solutions: Redis and Memcached. |\n| Firebase Realtime Database | The Firebase Realtime Database is a cloud-hosted NoSQL database that lets you store and sync data between your users in real time. |\n| Firestore     | A NoSQL document database built for automatic scaling, high performance, and ease of application development.      |\n| Open source     | Alternative database options including MongoDB and MariaDB.                   |\n## Database migration\nTo ensure that users experience zero application downtime when you migrate existing workloads to Google Cloud, it's important to choose database technologies that support your requirements. For information about database migration options and best practices, see [Database migration solutions](/solutions/database-migration) and [Best practices for homogeneous database migrations](https://cloud.google.com/blog/products/databases/tips-for-migrating-across-compatible-database-engines) .\nPlanning for a database migration includes the following:\n- Assessment and discovery of the current database.\n- Definitions of migration success criteria.\n- Environment setup for migration and the target database.\n- Creation of the schema in the target database.\n- Migration of the data into the target database.\n- Validation of the migration to verify that all the data is migrated correctly and is present in the database.\n- Creation of rollback strategy.\n### Choose a migration strategy\nSelecting the appropriate target database is one of the keys to a successful migration. The following table provides migration options for some use cases:\n| Use case                    | Recommendation                                |\n|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------|\n| New development in Google Cloud.              | Select one of the managed databases that's built for the cloud\u2014Cloud SQL, Spanner, Bigtable, or Firestore\u2014to meet your use-case requirements. |\n| Lift-and-shift migration.                | Choose a compatible, managed-database service like Cloud SQL, MYSQL, PostgreSQL, or SQLServer.            |\n| Your application requires granular access to a database that CloudSQL doesn't support. | Run your database on Compute Engine VMs.                          |\n### Use Memorystore to support your caching database layer\n[Memorystore](/memorystore/docs) is a fully managed Redis and Memcached database that supports submilliseconds latency. Memorystore is fully compatible with open source [Redis](https://redis.io/) and [Memcached](https://memcached.org/) . If you use these caching databases in your applications, you can use Memorystore without making application-level changes in your code.\n### Use Bare Metal servers to run an Oracle database\nIf your workloads require an Oracle database, use [Bare Metal servers](/bare-metal/docs) provided by Google Cloud. This approach fits into a [lift-and-shift](/architecture/hybrid-multicloud-patterns/adopt#migration_and_modernization) migration strategy.\nIf you want to move your workload to Google Cloud and modernize after your baseline workload is working, consider using managed database options like [Spanner](/spanner/docs) , [Bigtable](/bigtable/docs) , and [Firestore](https://firebase.google.com/docs/firestore) .\nDatabases built for the cloud are modern managed databases which are built from the bottom up on the cloud infrastructure. These databases provide unique default capabilities like scalability and high availability, which are difficult to achieve if you run your own database.\n### Modernize your database\nPlan your database strategy early in the system design process, whether you're designing a new application in the cloud or you're migrating an existing database to the cloud. Google Cloud provides managed database options for open source databases such as [Cloud SQL for MySQL](/sql/docs/mysql/connect-instance-cloud-shell) and [Cloud SQL for PostgreSQL](/sql/docs/postgres) . We recommend that you use the migration as an opportunity to modernize your database and prepare it to support future business needs.\n### Use fixed databases with off-the-shelf applications\n[Commercial off-the-shelf (COTS)](https://wikipedia.org/wiki/Commercial_off-the-shelf) applications require a fixed type of database and fixed configuration. Lift and shift is usually the most appropriate migration approach for COTS applications.\n### Verify your team's database migration skill set\nChoose a cloud database-migration approach based on your team's database migration capabilities and skill sets. Use [Google Cloud Partner Advantage](/partners) to find a partner to support you throughout your migration journey.\n### Design your database to meet HA and DR requirements\nWhen you design your databases to meet high availability (HA) and disaster recovery (DR) requirements, evaluate the tradeoffs between reliability and cost. Database services that are built for the cloud create multiple copies of your data within a region or in multiple regions, depending upon the database and configuration.\nSome Google Cloud services have multi-regional variants, such as [BigQuery](/bigquery/docs) and [Spanner](/spanner/docs) . To be resilient against regional failures, use these multi-regional services in your design where possible.\nIf you design your database on Compute Engine VMs instead of using managed databases on Google Cloud, ensure that you run multiple copies of your databases. For more information, see [Design for scale and high availability](/architecture/framework/reliability/design-scale-high-availability#design_a_multi-region_architecture_for_disaster_recovery) in the Reliability category.\n### Specify cloud regions to support data residency\nData residency describes where your data physically resides at rest. Consider choosing specific cloud regions to deploy your databases based on your data residency requirements.\nIf you deploy your databases in multiple regions, there might be data replication between them depending on how you configure them. Select the configuration that keeps your data within the desired regions at rest. Some databases, like Spanner, offer default multi-regional replication. You can also enforce data residency by setting an [organization policy](/resource-manager/docs/organization-policy/overview) that includes the resource locations [constraints](/resource-manager/docs/organization-policy/understanding-constraints) . For more information, see [Restricting Resource Locations](/resource-manager/docs/organization-policy/defining-locations) .\n### Include disaster recovery in data residency design\nInclude Recovery Time Objective (RTO) and Recovery Point Objective (RPO) in your data residency plans, and consider the trade-off between RTO/RPO and costs of the disaster recovery solution. Smaller RTO/RPO numbers result in higher costs. If you want your system to recover faster from disruptions, your system will cost more to run. Also, factor customer happiness into your disaster recovery approach to make sure that your reliability investments are appropriate. For more information, see [100% reliability is the wrong target](/architecture/framework/reliability/principles#100_reliability_is_the_wrong_target) and [Disaster recovery planning guide](/architecture/dr-scenarios-planning-guide) .\n### Make your database Google Cloud-compliant\nWhen you choose a database for your workload, ensure that the selected service meets compliance for the geographic region that you are operating in and where your data is physically stored. For more information about Google's certifications and compliance standards, see [Compliance offerings](/security/compliance/offerings) .\n## Encryption\nThis section provides best practices for identifying encryption requirements and choosing an encryption key strategy to support your system.\n### Determine encryption requirements\nYour encryption requirements depend on several factors, including company security policies and compliance requirements. All data that is stored in Google Cloud is encrypted at rest by default, without any action required by you, using AES256. For more information, see [Encryption at rest in Google Cloud](/security/encryption/default-encryption) .\n### Choose an encryption key strategy\nDecide if you want to manage encryption keys yourself or if you want to use a managed service. Google Cloud supports both scenarios. If you want a fully managed service to manage your encryption keys on Google Cloud, choose [Cloud Key Management Service (Cloud KMS)](/kms/docs) . If you want to manage your encryption keys to maintain more control over a key's lifecycle, use [Customer-managed encryption keys (CMEK)](/kms/docs/cmek) .\nTo create and manage your encryption keys outside of Google Cloud, choose one of the following options:\n- If you use a partner solution to manage your keys, use [Cloud External Key Manager](/kms/docs/ekm) .\n- If you manage your keys on-premises and if you want to use those keys to encrypt the data on Google Cloud, [import](/kms/docs/key-import) those keys into [Cloud KMS](/kms/docs) either as KMS keys or Hardware Key Module (HSM) keys. Use those keys to encrypt your data on Google Cloud.## Database design and scaling\nThis section provides best practices for designing and scaling a database to support your system.\n### Use monitoring metrics to assess scaling needs\nUse metrics from existing monitoring tools and environments to establish a baseline understanding of database size and scaling requirements\u2014for example, right-sizing and designing scaling strategies for your database instance.\nFor new database designs, determine scaling numbers based on expected load and traffic patterns from the serving application. For more information, see [Monitoring Cloud SQL instances](/sql/docs/mysql/monitor-instance) , [Monitoring with Cloud Monitoring](/spanner/docs/monitoring-cloud) , and [Monitoring an instance](/bigtable/docs/monitoring-instance) .\n## Networking and access\nThis section provides best practices for managing networking and access to support your system.\n### Run databases inside a private network\nRun your databases inside your private network and grant restricted access only from the clients who need to interact with the database. You can [create Cloud SQL instances inside a VPC](/sql/docs/mysql/private-ip) . Google Cloud also provides [VPC Service Controls for Cloud SQL](/sql/docs/mysql/admin-api/configure-service-controls) , [Spanner, and Bigtable databases](/vpc-service-controls/docs/supported-products) to ensure that access to these resources is restricted only to clients within authorized VPC networks.\n### Grant minimum privileges to users\n[Identity and Access Management (IAM)](/iam/docs) controls access to Google Cloud services, including database services. To minimize the risk of unauthorized access, grant the least number of privileges to your users. For application-level access to your databases, use service accounts with the least number of privileges.\n## Automation and right-sizing\nThis section provides best practices for defining automation and right-sizing to support your system.\n### Define database instances as code\nOne of the benefits of migrating to Google Cloud is the ability to automate your infrastructure and other aspects of your workload like compute and database layers. [Google Deployment Manager](/deployment-manager/docs) and third-party tools like [Terraform Cloud](https://www.terraform.io/cloud) let you define your database instances as code, which lets you apply a consistent and repeatable approach to creating and updating your databases.\n### Use Liquibase to version control your database\nGoogle database services like Cloud SQL and [Spanner](/spanner/docs/use-liquibase) support [Liquibase](https://www.liquibase.org/) , an open source version control tool for databases. Liquibase helps you to track your database schema changes, roll back schema changes, and perform repeatable migrations.\n### Test and tune your database to support scaling\nPerform load tests on your database instance and tune it based on the test results to meet your application's requirements. Determine the initial scale of your database by load testing key performance indicators (KPI) or by using monitoring KPIs derived from your current database.\nWhen you create database instances, start with a size that is based on the testing results or historical monitoring metrics. Test your database instances with the expected load in the cloud. Then fine-tune the instances until you get the desired results for the expected load on your database instances.\n### Choose the right database for your scaling requirements\nScaling databases is different from scaling compute layer components. Databases have state; when one instance of your database isn't able to handle the load, consider the appropriate strategy to scale your database instances. Scaling strategies vary depending on the database type.\nUse the following table to learn about Google products that address scaling use cases.\n| Use case                                                                     | Recommended product | Description                              |\n|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------|:----------------------------------------------------------------------------------------------------------------------------------|\n| Horizontally scale your database instance by adding nodes to your database when you need to scale up the serving capacity and storage.                                     | Spanner    | A relational database that's built for the cloud.                     |\n| Add nodes to scale your database.                                                               | Bigtable    | Fully managed NoSQL big data database service.                     |\n| Automatically handle database scaling.                                                             | Firestore    | Flexible, scalable database for mobile, web, and server development.                |\n| To serve more queries, vertically scale up Cloud SQL database instances to give them more compute and memory capacity. In Cloud SQL, the storage layer is decoupled from the database instance. You can choose to scale your storage layer automatically whenever it approaches capacity. | Cloud SQL    | Fully managed database service that helps you set up, maintain, manage, and administer your relational databases on Google Cloud. |\n## Operations\nThis section provides best practices for operations to support your system.\n### Use Cloud Monitoring to monitor and set up alerts for your database\nUse [Cloud Monitoring](/monitoring/docs) to monitor your database instances and set up alerts to notify appropriate teams of events. For information about efficient alerting best practices, see [Build efficient alerts](/architecture/framework/reliability/build-efficient-alerts) .\nAll databases that are built for the cloud provide logging and monitoring metrics. Each service provides a dashboard to visualize logging and monitoring metrics. The monitoring metrics for all services integrate with [Google Cloud Observability](/stackdriver/docs) . [Spanner](/spanner/docs) provides query introspection tools like the [Key Visualizer](/spanner/docs/key-visualizer) for debugging and root cause analysis. The Key Visualizer provides the following capabilities:\n- Helps you analyze Spanner usage patterns by generating visual reports for your databases. The reports display usage patterns by ranges of rows over time.\n- Provides insights into usage patterns at scale.\n[Bigtable](/bigtable/docs) also provides a [Key Visualizer](/bigtable/docs/keyvis-overview) diagnostic tool that helps you to analyze Bigtable instance usage patterns.\n## Licensing\nThis section provides best practices for licensing to support your system.\n### Choose between on-demand licenses and existing licenses\nIf you use [Cloud SQL for SQL Server](/sql/docs/sqlserver/introduction) , [bringing your own licenses](/compute/docs/nodes/bringing-your-own-licenses) isn't supported; your licensing costs are based on per-core hour usage.\nIf you want to use existing [Cloud SQL for SQL Server](/sql/docs/sqlserver) licenses, consider running Cloud SQL for SQL Server on Compute VMs. For more information, see [Microsoft licenses](/compute/docs/instances/windows/ms-licensing) and [Choosing between on-demand licenses and bringing existing licenses](/compute/docs/instances/windows/ms-licensing#flowchart) .\nIf you use Oracle and if you're migrating to the [Bare Metal Solution for Oracle](/bare-metal) , you can bring your own licenses. For more information, see [Plan for Bare Metal Solution](/bare-metal/docs/bms-planning) .\n## Migration timelines, methodology, and toolsets\nThis section provides best practices for planning and supporting your database migration to support your system.\n### Determine database modernization readiness\nAssess whether your organization is ready to modernize your databases and use databases that are built for the cloud.\nConsider database modernization when you plan workload migration timelines, because modernization is likely to impact your application side.\n### Involve relevant stakeholders in migration planning\nTo migrate a database, you complete the following tasks:\n- Set up the target databases.\n- Convert the schema.\n- Set up data replication between the source and target database.\n- Debug issues as they arise during the migration.\n- Establish network connectivity between the application layer and the database.\n- Implement target database security.\n- Ensure that the applications connect to the target databases.\nThese tasks often require different skill sets and multiple teams collaborate across your organization to complete the migration. When you plan the migration, include stakeholders from all teams, such as app developers, database administrators, and infrastructure and security teams.\nIf your team lacks skills to support this type of migration, Google's partners can help you perform your migrations. For more information, see [Google Cloud Partner Advantage](/partners) .\n### Identify tool sets for homogeneous and heterogeneous migrations\nA is a database migration between the source and target databases of the same database technology. A is a migration whose target database is different from the source database.\nHeterogeneous migrations usually involve additional steps of schema conversion from the source database to the target database engine type. Your database teams need to assess the challenges involved in the schema conversion, because they depend on the complexity of the source database schema.\n### Test and validate each step in data migration\nData migrations involve multiple steps. To minimize migration errors, test and validate each step in the migration before moving to the next step. The following factors drive the migration process:\n- Whether the migration is homogeneous or heterogeneous.\n- What type of tools and skill sets you have to perform the migration.\n- For heterogeneous migrations, your experience with the target database engine.\n### Determine continuous data replication requirements\nCreate a plan to migrate the data initially and then continuously replicate the data from the source to the target database. Continue replication until the target is stabilized and the application is completely migrated to the new database. This plan helps you to identify potential downtime during the database switch and plan accordingly.\nIf you plan to migrate database engines from [Cloud SQL](/sql/docs) , [Cloud SQL for MySQL](/sql/docs/mysql) , or [Cloud SQL for PostgreSQL](/sql/docs/postgres) , use [Database Migration Service](/database-migration) to automate this process in a fully managed way. For information about third-party tools that support other types of migrations, see [Cloud Marketplace](/marketplace) .\n## Recommendations\nTo apply the guidance in the Architecture Framework to your own environment, we recommend that you do the following:\n- Multi-tenancy for databases involves storing data from multiple customers on a shared piece of infrastructure, in this case a database. If you offer a software-as-a-service (SaaS) based offering to your customers, make sure that you understand how you can logically isolate datasets that belong to different customers, and support their access requirements. Also, evaluate your requirements based on levels of separation.For relational databases such as [Spanner](/spanner/docs) and [Cloud SQL](/sql/docs) , there are multiple approaches, such as isolating tenants' data at the database-instance level, database level, schema level, or the database-table level. Like other design decisions, there is a tradeoff between the degree of isolation and other factors such as cost and performance. [IAM](/iam/docs) policies control access to your database instances.\n- Choose the right database for your data model requirements.\n- Choose key values to avoid key hotspotting. A is a location within a table that receives many more access requests than other locations. For more information about hotspots, see [Schema design best practices](/spanner/docs/schema-design) .\n- Shard your database instance whenever possible.\n- Use connection-management best practices, such as connection pooling and exponential backoff.\n- Avoid very large transactions.\n- Design and test your application's response to maintenance updates on databases.\n- Secure and isolate connections to your database.\n- Size your database and growth expectations to ensure that the database supports your requirements.\n- Test your HA and DR failover strategies.\n- Perform backups and restore as well as exports and imports so that you're familiar with the process.\n### Cloud SQL recommendations\n- Use private IP address networking (VPC). For additional security, consider the following:- Use [Cloud SQL Auth proxy](/sql/docs/mysql/sql-proxy) to support private networking.\n- Restrict public IP address access [constraints/sql.restrictPublicIp](/sql/docs/mysql/connection-org-policy#connection-constraints) .\n- If you need public IP address networking, consider the following:- Use the built-in firewall with a limited or narrow IP address list and ensure that Cloud SQL instances require that incoming connections use SSL. For more information, see [Configuring SSL/TLS certificates](/sql/docs/mysql/configure-ssl-instance) .\n- For additional security, consider the following:- Don't grant general access; instead, use Cloud SQL Auth proxy.\n- Restrict authorized networks [constraints/sql.restrictAuthorizedNetworks](/sql/docs/mysql/connection-org-policy#connection-constraints) .\n- Use limited privileges for database users.## What's next\nLearn [data analytics best practices](/architecture/framework/system-design/data-analytics) , including the following:\n- Learn [core data analytics principles](/architecture/framework/system-design/data-analytics#core) and [key Google Cloud services](/architecture/framework/system-design/data-analytics#key) .\n- Learn about the [data lifecycle](/architecture/framework/system-design/data-analytics#lifecycle) .\n- Learn how to [ingest data](/architecture/framework/system-design/data-analytics#ingest) .\n- Choose and manage [data storage](/architecture/framework/system-design/data-analytics#data-storage) .\n- [Process and transform data](/architecture/framework/system-design/data-analytics#transform) .\nExplore other categories in the [Architecture Framework](/architecture/framework) such as reliability, operational excellence, and security, privacy, and compliance.# Analyze your data\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) explains some of the core principles and best practices for data analytics in Google Cloud. You learn about some of the key data-analytics services, and how they can help at the various stages of the data lifecycle. These best practices help you to meet your data analytics needs and create your system design.\n## Core principles\nBusinesses want to analyze data and generate actionable insights from that data. Google Cloud provides you with various services that help you through the entire data lifecycle, from data ingestion through reports and visualization. Most of these services are fully managed, and some are serverless. You can also build and manage a data-analytics environment on [Compute Engine VMs](/compute) , such as to self-host Apache [Hadoop](https://hadoop.apache.org/) or [Beam](https://beam.apache.org/) .\nYour particular focus, team expertise, and strategic outlook help you to determine which Google Cloud services you adopt to support your data analytics needs. For example, [Dataflow](/dataflow) lets you write complex transformations in a serverless approach, but you must rely on an opinionated version of configurations for compute and processing needs. Alternatively, [Dataproc](/dataproc) lets you run the same transformations, but you manage the clusters and fine-tune the jobs yourself.\nIn your system design, think about which processing strategy your teams use, such as [extract, transform, load (ETL)](/learn/what-is-etl) or [extract, load, transform (ELT)](https://wikipedia.org/wiki/Extract,_load,_transform) . Your system design should also consider whether you need to process [batch analytics or streaming analytics](/learn/what-is-streaming-analytics) . Google Cloud provides a unified data platform, and it lets you build a [data lake](/learn/what-is-a-data-lake) or a [data warehouse](/learn/what-is-a-data-warehouse) to meet your business needs.\n## Key services\nThe following table provides a high-level overview of Google Cloud analytics services:\n| Google Cloud service | Description                                                            |\n|:-----------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Pub/Sub    | Simple, reliable, and scalable foundation for stream analytics and event-driven computing systems.                                      |\n| Dataflow    | A fully managed service to transform and enrich data in stream (real time) and batch (historical) modes.                                     |\n| Dataprep by Trifacta | Intelligent data service to visually explore, clean, and prepare structured and unstructured data for analysis.                                   |\n| Dataproc    | Fast, easy-to-use, and fully managed cloud service to run Apache Spark and Apache Hadoop clusters.                                      |\n| Cloud Data Fusion  | Fully managed, data integration service that's built for the cloud and lets you build and manage ETL/ELT data pipelines. Cloud DataFusion provides a graphical interface and a broad open source library of preconfigured connectors and transformations. |\n| BigQuery    | Fully managed, low-cost, serverless data warehouse that scales with your storage and compute power needs. BigQuery is a columnar and ANSI SQL database that can analyze terabytes to petabytes of data.             |\n| Cloud Composer   | Fully managed workflow orchestration service that lets you author, schedule, and monitor pipelines that span clouds and on-premises data centers.                           |\n| Data Catalog   | Fully managed and scalable metadata management service that helps you discover, manage, and understand all your data.                                  |\n| Looker Studio   | Fully managed visual analytics service that can help you unlock insights from data through interactive dashboards.                                  |\n| Looker     | Enterprise platform that connects, analyzes, and visualizes data across multi-cloud environments.                                       |\n| Dataform    | Fully managed product to help you collaborate, create, and deploy data pipelines, and ensure data quality.                                    |\n| Dataplex    | Managed data lake service that centrally manages, monitors, and governs data across data lakes, data warehouses, and data marts using consistent controls.                        |\n| AnalyticsHub   | Platform that efficiently and securely exchanges data analytics assets across your organization to address challenges of data reliability and cost.                          |\n## Data lifecycle\nWhen you create your system design, you can group the Google Cloud data analytics services around the general data movement in any system, or around the data lifecycle.\nThe data lifecycle includes the following stages and example services:\n- includes services such as [Pub/Sub](/pubsub) , [Storage Transfer Service](/storage-transfer-service) , [Transfer Appliance](/transfer-appliance) , and [BigQuery](/bigquery) .\n- includes services such as [Cloud Storage](/storage) , [Bigtable](/bigtable) , [Memorystore](/memorystore) , and [BigQuery](/bigquery) .\n- includes services such as [Dataflow](/dataflow) , [Dataproc](/dataproc) , [Dataprep](/dataprep) , [Sensitive Data Protection](/dlp) , and [BigQuery](/bigquery) .\n- includes services such as [BigQuery](/bigquery) .\n- includes services such as [Looker Studio](https://lookerstudio.google.com/overview) and [Looker](https://looker.com/google-cloud) .\nThe following stages and services run across the entire data lifecycle:\n- includes services such as [Data Fusion](/data-fusion) .\n- includes services such as [Data Catalog](/data-catalog) .\n- includes services such as [Cloud Composer](/composer) .## Data ingestion\nApply the following data ingestion best practices to your own environment.\n### Determine the data source for ingestion\nData typically comes from another cloud provider or service, or from an on-premises location:\n- To ingest data from other cloud providers, you typically use [Cloud Data Fusion](/data-fusion) , [Storage Transfer Service](/storage-transfer-service) , or [BigQuery Transfer Service](/bigquery-transfer/docs/introduction) .\n- For on-premises data ingestion, consider the volume of data to ingest and your team's skill set. If your team prefers a low-code, graphical user interface (GUI) approach, use [Cloud Data Fusion](/data-fusion) with a suitable connector, such as [Java Database Connectivity (JDBC)](https://docs.oracle.com/javase/tutorial/jdbc/overview/index.html) . For large volumes of data, you can use [Transfer Appliance](/transfer-appliance) or [Storage Transfer Service](/storage-transfer-service) .\nConsider how you want to process your data after you ingest it. For example, Storage Transfer Service only writes data to a Cloud Storage bucket, and [BigQuery Data Transfer Service](/bigquery-transfer/docs) only writes data to a BigQuery dataset. Cloud Data Fusion supports multiple destinations.\n### Identify streaming or batch data sources\nConsider how you need to use your data and identify where you have streaming or batch use cases. For example, if you run a global streaming service that has low latency requirements, you can use [Pub/Sub](/pubsub) . If you need your data for analytics and reporting uses, you can [stream data into BigQuery](/bigquery/streaming-data-into-bigquery) .\nIf you need to stream data from a system like [Apache Kafka](https://kafka.apache.org/) in an on-premises or other cloud environment, use the [Kafka to BigQuery Dataflow template](https://github.com/GoogleCloudPlatform/DataflowTemplates/tree/master/v2/kafka-to-bigquery) . For batch workloads, the first step is usually to ingest data into Cloud Storage. Use the [gsutil](/storage/docs/gsutil) tool or [Storage Transfer Service](/storage-transfer/docs/overview) to ingest data.\n### Ingest data with automated tools\nManually moving data from other systems into the cloud can be a challenge. If possible, use tools that let you automate the data ingestion processes. For example, [Cloud Data Fusion](/data-fusion) provides connectors and plugins to bring data from external sources with a drag-and-drop GUI. If your teams want to write some code, [Data Flow](/dataflow) or [BigQuery](/bigquery) can help to automate data ingestion. [Pub/Sub](/pubsub) can help in both a low-code or code-first approach. To ingest data into storage buckets, use [gsutil](/storage/docs/gsutil) for [data sizes of up to 1 TB](/architecture/migration-to-google-cloud-transferring-your-large-datasets#options_available_from_google) . To ingest amounts of data larger than 1 TB, use [Storage Transfer Service](/storage-transfer/docs/overview) .\n### Use migration tools to ingest from another data warehouse\nIf you need to migrate from another data warehouse system, such as Teradata, Netezza, or Redshift, you can use the BigQuery Data Transfer Service [migration assistance](/bigquery-transfer/docs/migrations) . The BigQuery Data Transfer Service also provides [third-party transfers](/bigquery-transfer/docs/third-party-transfer) that help you ingest data on a schedule from external sources. For more information, see the [detailed migration approaches](/architecture/dw2bq/dw-bq-migration-overview) for each data warehouse.\n### Estimate your data ingestion needs\nThe volume of data that you need to ingest helps you to determine which service to use in your system design. For streaming ingestion of data, [Pub/Sub](/pubsub) scales to tens of gigabytes per second. Capacity, storage, and regional requirements for your data help you to determine whether Pub/Sub Lite is a better option for your system design. For more information, see [Choosing Pub/Sub or Pub/Sub Lite](/pubsub/docs/choosing-pubsub-or-lite) .\nFor batch ingestion of data, estimate how much data you want to transfer in total, and how quickly you want to do it. Review the [available migration options](/architecture/migration-to-google-cloud-transferring-your-large-datasets#options_available_from_google) , including an [estimate on time](/architecture/migration-to-google-cloud-transferring-your-large-datasets#time) and [comparison of online versus offline transfers](/architecture/migration-to-google-cloud-transferring-your-large-datasets#online_versus_offline_transfer) .\n### Use appropriate tools to regularly ingest data on a schedule\n[Storage Transfer Service](/storage-transfer/docs/overview) and [BigQuery Data Transfer Service](/bigquery-transfer/docs/introduction) both let you schedule ingestion jobs. For fine-grain control of the timing of ingestion or the source and destination system, use a workflow-management system like [Cloud Composer](/composer) . If you want a more manual approach, you can [use Cloud Scheduler and Pub/Sub to trigger a Cloud Function](/scheduler/docs/tut-gcf-pub-sub) . If you want to manage the Compute infrastructure, you can use the [gsutil](/storage/docs/gsutil) command with cron for data transfer of up to 1 TB. If you use this manual approach instead of Cloud Composer, follow the [best practices to script production transfers](/storage/docs/gsutil/addlhelp/ScriptingProductionTransfers) .\n### Review FTP/SFTP server data ingest needs\nIf you need a code-free environment to ingest data from an FTP/SFTP server, you can use the [FTP copy plugins](https://github.com/data-integrations/ftp-copy-action) . If you want to modernize and create a long-term workflow solution, Cloud Composer is a fully managed service that lets you read and write from various sources and sinks.\n### Use Apache Kafka connectors to ingest data\nIf you use Pub/Sub, Dataflow, or BigQuery, you can ingest data using one of the [Apache Kafka connectors](https://cloud.google.com/blog/products/data-analytics/apache-kafka-for-gcp-users-connectors-for-pubsub-dataflow-and-bigquery) . For example, the [open source Pub/Sub Kafka connector](https://github.com/GoogleCloudPlatform/pubsub/tree/master/kafka-connector) lets you ingest data from Pub/Sub or Pub/Sub Lite.\n### Additional resources\n- [Cloud Storage Transfer Service agent best practices](/storage-transfer/docs/on-prem-agent-best-practices) \n- [How to ingest data into BigQuery so you can analyze it](https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-data-ingestion) \n- [Ingesting clinical and operational data with Cloud Data Fusion](/architecture/ingesting-clinical-and-operational-data-with-cloud-data-fusion) \n- [Optimizing large-scale ingestion of analytics events and logs](/architecture/optimized-large-scale-analytics-ingestion) ## Data storage\nApply the following data storage best practices to your own environment.\n### Choose the appropriate data store for your needs\nTo help you choose what type of storage solution to use, review and understand the downstream usage of your data. The following common use cases for your data give recommendations for which Google Cloud product to use:\n| Data use case       | Product recommendation                         |\n|:---------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|\n| File-based        | Filestore                            |\n| Object-based       | Cloud Storage                           |\n| Low latency       | Bigtable                            |\n| Time series       | Bigtable                            |\n| Online cache       | Memorystore                           |\n| Transaction processing     | Cloud SQL                            |\n| Business intelligence (BI) & analytics | BigQuery                            |\n| Batch processing      | Cloud Storage Bigtable if incoming data is time series and you need low latency access to it. BigQuery if you use SQL. |\n### Review your data structure needs\nFor most data, such as documents and text files, audio and video files, or logs, an object-based store is the most suitable choice. You can then load and process the data from object storage when you need it.\nFor data, such as XML or JSON, your use cases and data access patterns help guide your choice. You can load such datasets into BigQuery for [automatic schema detection](/bigquery/docs/schema-detect) . If you have low latency requirements, you can load your JSON data into Bigtable. If you have legacy requirements or your applications work with relational databases, you can also load datasets into a relation store.\nFor , such as CSV, Parquet, Avro, or ORC, you can use BigQuery if you have BI and analytics requirements that use SQL. For more information, see [how to batch load data](/bigquery/docs/batch-loading-data) . If you want to create a data lake on open standards and technologies, you can use Cloud Storage.\n### Migrate data and reduce costs for HDFS\nLook for ways to move Hadoop Distributed File System (HDFS) data from on-premises or from another cloud provider to a cheaper object-storage system. Cloud Storage is the most common choice that enterprises make as an alternative data store. For information about the advantages and disadvantages of this choice, see [HDFS vs. Cloud Storage](https://cloud.google.com/blog/products/storage-data-transfer/hdfs-vs-cloud-storage-pros-cons-and-migration-tips) .\nYou can move data with a push or pull method. Both methods use the `hadoop distcp` command. For more information, see [Migrating HDFS Data from On-Premises to Google Cloud](/architecture/hadoop/hadoop-gcp-migration-data) .\nYou can also use the open source [Cloud Storage connector](/dataproc/docs/concepts/connectors/cloud-storage) to let Hadoop and Spark jobs access data in Cloud Storage. The connector is installed by default on Dataproc clusters, and can be [manually installed on other clusters](/dataproc/docs/concepts/connectors/cloud-storage#non-dataproc_clusters) .\n### Use object storage to build a cohesive data lake\nA [data lake](/learn/what-is-a-data-lake) is a centralized repository designed to store, process, and secure large amounts of structured, semistructured, and unstructured data. You can [use Cloud Composer and Cloud Data Fusion to build a data lake](https://cloud.google.com/blog/topics/developers-practitioners/architect-your-data-lake-google-cloud-data-fusion-and-composer) .\nTo build a modern data platform, you can use [BigQuery](/bigquery) as your central data source instead of Cloud Storage. BigQuery is a modern data warehouse with [separation of storage and compute](https://cloud.google.com/blog/products/bigquery/separation-of-storage-and-compute-in-bigquery) . A data lake built on BigQuery lets you perform traditional analytics from BigQuery in the Cloud console. It also lets you [access the data stored](/dataproc/docs/tutorials/bigquery-connector-spark-example) from other frameworks such as Apache Spark.\n### Additional resources\n- [Best practices for Cloud Storage](/storage/docs/best-practices) \n- [Best practices for Cloud Storage cost optimization](https://cloud.google.com/blog/products/storage-data-transfer/best-practices-for-cloud-storage-cost-optimization) \n- [Best practices for ensuring privacy and security of your data in Cloud Storage](https://cloud.google.com/blog/products/storage-data-transfer/google-cloud-storage-best-practices-to-help-ensure-data-privacy-and-security) \n- [Best practices for Memorystore](/memorystore/docs/redis/general-best-practices) \n- [Optimizing storage in BigQuery](/bigquery/docs/best-practices-storage) \n- [Bigtable schema design](/bigtable/docs/schema-design) ## Process and transform data\nApply the following data analytics best practices to your own environment when you process and transform data.\n### Explore the open source software you can use in Google Cloud\nMany Google Cloud services use open source software to help make your transition seamless. Google Cloud offers managed and serverless solutions that have Open APIs and are compatible with open source frameworks to reduce vendor lock-in.\n[Dataproc](/dataproc) is a Hadoop-compatible managed service that lets you host open source software, with little operational burden. Dataproc [includes support](/dataproc/docs/concepts/versioning/dataproc-versions) for Spark, Hive, Pig, Presto, and Zookeeper. It also provides [Hive Metastore as a managed service](/dataproc-metastore/docs) to remove itself as a single point of failure in the Hadoop ecosystem.\nYou can migrate to [Dataflow](/dataflow) if you currently use Apache Beam as a batch and streaming processing engine. Dataflow is a fully managed and serverless service that uses Apache Beam. Use Dataflow to write jobs in Beam, but let Google Cloud manage the execution environment.\nIf you use [CDAP](https://github.com/cdapio/cdap) as your data integration platform, you can migrate to [Cloud Data Fusion](/data-fusion) for a fully managed experience.\n### Determine your ETL or ELT data-processing needs\nYour team's experience and preferences help determine your system design for how to process data. Google Cloud lets you use either [traditional ETL](/learn/what-is-etl) or [more modern ELT](https://en.wikipedia.org/wiki/Extract,_load,_transform) data-processing systems.\n- For ETL pipelines, you can use [Data Fusion](/data-fusion) , [Dataproc](/dataproc) , or [Dataflow](/dataflow) .- For new environments, we recommend Dataflow for a [unified way to create batch and streaming applications](/dataflow/docs/samples/reference-patterns) .\n- For a fully managed approach, Data Fusion provides a drag-and-drop GUI to help you create pipelines.\n- For ELT pipelines, use BigQuery, which supports both [ batch and streaming data load](/bigquery/docs/loading-data) . After your data is in BigQuery, use SQL to perform all transformations to derive new datasets for your business use cases.\n- If you want to modernize and [move from ETL to ELT](https://dataform.co/academy/data-warehousing-elt) , you can use Dataform. To support BI with BigQuery, you can also use a third-party service like [Fivetran](/architecture/partners/using-fivetran-and-elt-with-bigquery) .\n### Use the appropriate framework for your data use case\nYour data use cases determine which tools and frameworks to use. Some Google Cloud products are built to handle all of the following data use cases, while others best support only one particular use case.\n- For adata processing system, you can process and transform data in BigQuery with a familiar SQL interface. If you have an existing pipeline that runs on Apache Hadoop or Spark on-premises or in another public cloud, you can use Dataproc.- You can also use Dataflow if you want a unified programing interface for both batch and streaming use cases. We recommend that you modernize and use Dataflow for ETL and BigQuery for ELT.\n- For data pipelines, you use a managed and serverless service like Dataflow that provides windowing, autoscaling, and templates. For more information, see [Building production-ready data pipelines using Dataflow](/architecture/building-production-ready-data-pipelines-using-dataflow-overview) .- If you have analytics and SQL-focused teams and capabilities, you can also [stream data into BigQuery](/bigquery/streaming-data-into-bigquery) .\n- For use cases, such as time series analysis or streaming video analytics, use Dataflow.\n### Retain future control over your execution engine\nTo minimize vendor lock-in and to be able to use a different platform in the future, use the [Apache Beam programming model](https://beam.apache.org/documentation/programming-guide/) and [Dataflow](/dataflow) as a managed serverless solution. The Beam programming model lets you [change the underlying execution engine](https://beam.apache.org/documentation/runners/capability-matrix/) , such as changing from Dataflow to [Apache Flink](https://flink.apache.org/) or [Apache Spark](https://spark.apache.org/) .\n### Use Dataflow to ingest data from multiple sources\nTo ingest data from multiple sources, such as Pub/Sub, Cloud Storage, HDFS, S3, or Kafka, use [Dataflow](/dataflow) . Dataflow is a managed serverless service that supports [Dataflow templates](/dataflow/docs/concepts/dataflow-templates) , which lets your teams run templates from different tools.\n[Dataflow Prime](https://cloud.google.com/blog/products/data-analytics/simplify-and-automate-data-processing-with-dataflow-prime) provides horizontal and vertical autoscaling of machines that are used in the execution process of a pipeline. It also provides smart diagnostics and recommendations that identify problems and suggest how to fix them.\n### Discover, identify, and protect sensitive data\nUse [Sensitive Data Protection](/dlp) to inspect and transform structured and unstructured data. Sensitive Data Protection works for data located anywhere in Google Cloud, such as in Cloud Storage or databases. You can classify, mask, and tokenize your sensitive data to continue to use it safely for downstream processing. Use Sensitive Data Protection to perform actions such as to [scan BigQuery data](/bigquery/docs/scan-with-dlp) or [de-identify and re-identify PII in large-scale datasets](/architecture/de-identification-re-identification-pii-using-cloud-dlp) .\n### Modernize your data transformation processes\nUse [Dataform](https://cloud.google.com/blog/products/data-analytics/welcoming-dataform-to-bigquery) to write data transformations as code and to start to use version control by default. You can also adopt software development best practices such as CI/CD, unit tests, and version control to SQL code. Dataform supports all major cloud data warehouse products and databases, such as PostgreSQL.\n### Additional Resources\n- Dataproc- [Best practices guide](https://cloud.google.com/blog/topics/developers-practitioners/dataproc-best-practices-guide) \n- [Tips for long-running clusters](https://cloud.google.com/blog/products/data-analytics/10-tips-for-building-long-running-clusters-using-cloud-dataproc) \n- [Using Apache Hive on Dataproc](/architecture/using-apache-hive-on-cloud-dataproc) \n- [Best practices for running in production](https://cloud.google.com/blog/products/data-analytics/7-best-practices-for-running-cloud-dataproc-in-production) \n- [Best practices to use Apache Ranger](https://cloud.google.com/blog/products/data-analytics/running-cloud-managed-spark-and-hadoop-using-ranger) \n- [Autoscaling configuration recommendations](/dataproc/docs/concepts/configuring-clusters/autoscaling#autoscaling_configuration_recommendations) \n- [Help for slow Hadoop/Spark jobs](https://cloud.google.com/blog/products/data-analytics/help-for-slow-hadoopspark-jobs-on-google-cloud-10-questions-to-ask-about-your-hadoop-and-spark-cluster-performance) \n- Dataflow- [Move pipelines to production](https://cloud.google.com/blog/products/data-analytics/tips-and-tricks-to-get-your-cloud-dataflow-pipelines-into-production) \n- [Common use cases](https://cloud.google.com/blog/products/data-analytics/guide-to-common-cloud-dataflow-use-case-patterns-part-1) \n- Data Fusion- [Pipeline Performance](/data-fusion/docs/concepts/pipeline-performance) \n- BigQuery- [Optimize query performance](/bigquery/docs/best-practices-performance-overview) \n- Dataform- [Best practices for managing projects](https://docs.dataform.co/best-practices/best-practices) \n- Sensitive Data Protection- [Keep costs under control ](/dlp/docs/best-practices-costs) \n- [Inspect Storage and Databases for sensitive data](/dlp/docs/inspecting-storage) \n- [Inspect text for sensitive data](/dlp/docs/inspecting-text) \n## Data analytics and warehouses\nApply the following data analytics and warehouse best practices to your own environment.\n### Review your data storage needs\nData lakes and data warehouses aren't mutually exclusive. Data lakes are useful for unstructured and semi-structured data storage and processing. Data warehouses are best for analytics and BI.\nReview your data needs to help determine where to store your data and [which Google Cloud product is the most appropriate](#data-store) to process and analyze your data. Products like BigQuery can process PBs of data and grow with your demands.\n### Identify opportunities to migrate from a traditional data warehouse to BigQuery\nReview the traditional data warehouses that are currently in use in your environment. To reduce complexity and potentially reduce costs, identify opportunities to migrate your traditional data warehouses to a Google Cloud service like BigQuery. For more information and example scenarios, see [Migrating data warehouses to BigQuery](/architecture/dw2bq/dw-bq-migration-overview) .\n### Plan for federated access to data\nReview your data requirements and how you might need to interact with other products and services. Identify your data federation needs, and create an appropriate system design.\nFor example, BigQuery lets you define [external tables](/bigquery/external-data-sources) that can read data from other sources, such as Bigtable, Cloud SQL, Cloud Storage, or Google Drive. You can join these external sources with tables that you store in BigQuery.\n### Use BigQuery flex slots to provide on-demand burst capacity\nSometimes you need extra capacity to do experimental or exploratory analysis that needs a lot of compute resources. BigQuery lets you get additional compute capacity in the form of [flex slots](https://cloud.google.com/blog/products/data-analytics/introducing-bigquery-flex-slots) . These flex slots help you when there's a period of high demand or when you want to complete an important analysis.\n### Understand schema differences if you migrate to BigQuery\nBigQuery supports both and schemas, but by default it uses [nested and repeated fields](/bigquery/docs/nested-repeated) . Nested and repeated fields can be easier to read and correlate compared to other schemas. If your data is represented in a star or snowflake schema, and if you want to migrate to BigQuery, review your system design for any necessary changes to processes or analytics.\n### Additional resources\n- [Best practices for multi-tenant workloads on BigQuery](/bigquery/docs/best-practices-for-multi-tenant-workloads-on-bigquery) \n- [Best practices for row-level security in BigQuery](/bigquery/docs/best-practices-row-level-security) \n- [Best practices for materialized views in BigQuery](/bigquery/docs/materialized-views-best-practices) ## Reports and visualization\nApply the following reporting and visualization best practices to your own environment.\n### Use BigQuery BI Engine to visualize your data\n[BigQuery BI Engine](/bigquery/docs/bi-engine-intro) is a fast, in-memory analysis service. You can use BI Engine to analyze data stored in BigQuery with subsecond query response time and with high concurrency. BI Engine is integrated into the BigQuery API. Use [reserved BI Engine capacity](/bigquery/docs/bi-engine-reserve-capacity) to manage the on-demand or flat-rate pricing for your needs. BI Engine can also work with other BI or custom dashboard applications that require subsecond response times.\n### Modernize your BI processes with Looker\nLooker is a [modern, enterprise platform](https://looker.com/blog/what-is-modern-bi) for BI, data applications, and embedded analytics. You can create consistent data models on top of your data with speed and accuracy, and you can access data inside transactional and analytical datastores. Looker can also [analyze your data on multiple databases and clouds](https://cloud.google.com/blog/products/data-analytics/analyze-data-on-multiple-databases-clouds) . If you have existing BI processes and tools, we recommend that you modernize and use a central platform such as Looker.\n### Additional resources\n- [Migrating data warehouses to BigQuery: Reporting and analysis](/architecture/dw2bq/dw-bq-reporting-and-analysis) \n- [Architecture for connecting visualization software to Hadoop on Google Cloud](/architecture/hadoop/architecture-for-connecting-visualization-software-to-hadoop-on-google-cloud) \n- [Speeding up small queries in BigQuery with BI Engine](https://cloud.google.com/blog/topics/developers-practitioners/speeding-small-queries-bigquery-bi-engine) ## Use workflow management tools\nData analytics involves many processes and services. Data moves across different tools and processing pipelines during the data analytics lifecycle. To manage and maintain end-to-end data pipelines, use appropriate workflow management tools. [Cloud Composer](/composer) is a fully managed workflow management tool based on the open source [Apache Airflow](https://airflow.apache.org/) project.\nYou can use Cloud Composer to [launch Dataflow pipelines](/composer/docs/how-to/using/using-dataflow-template-operator) and to [use Dataproc Workflow Templates](/dataproc/docs/tutorials/workflow-composer) . Cloud Composer can also help you [create a CI/CD pipeline to test, synchronize, and deploy DAGs](/composer/docs/dag-cicd-integration-guide) or [use a CI/CD pipeline for data-processing workflows](/architecture/cicd-pipeline-for-data-processing) . For more information, watch [Cloud Composer: Development best practices](https://www.youtube.com/watch?v=RrKXZcKOz4A) .\n## Migration resources\nIf you already run a data analytics platform and if you want to migrate some or all of the workloads to Google Cloud, review the following migration resources for best practices and guidance:\n- General migration guidance- [Migration to Google Cloud: Choosing your migration path](/architecture/migration-to-gcp-choosing-your-path) .\n- [Example of modernizing Twitter's ad engagement analytics platform](https://cloud.google.com/blog/products/data-analytics/modernizing-twitters-ad-engagement-analytics-platform) .\n- Cloud Storage migration- [Migrating HDFS Data from on-premises to Google Cloud](/architecture/hadoop/hadoop-gcp-migration-data) .\n- [Transferring data from Amazon S3 to Cloud Storage using VPC Service Controls and Storage Transfer Service](/architecture/transferring-data-from-amazon-s3-to-cloud-storage-using-vpc-service-controls-and-storage-transfer-service) .\n- Pub/Sub migration- [Migration from Kafka to Pub/Sub](/architecture/migrating-from-kafka-to-pubsub) .\n- Bigtable migration- [Migrating data from HBase to Bigtable](/architecture/hadoop/hadoop-gcp-migration-data-hbase-to-bigtable) .\n- [Migrating from Aerospike to Bigtable](/architecture/migrating-aerospike-cloud-bigtable) .\n- Dataproc migration- [Migrating on-premises Hadoop infrastructure to Google Cloud](/architecture/hadoop) .\n- [Hadoop migration costs and tips](https://cloud.google.com/blog/products/data-analytics/on-prem-hadoop-migration-costs-and-tips) .\n- [Hadoop to Dataproc](https://cloud.google.com/blog/topics/developers-practitioners/migrating-apache-hadoop-dataproc-decision-tree) .\n- [On-premises Hadoop Jobs to Dataproc](/architecture/hadoop/hadoop-gcp-migration-jobs) .\n- [Apache Spark to Dataproc](/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc) .\n- [External Hive Metastore to Dataproc Metastore](/dataproc-metastore/docs/migrate-mysql-metastore) .\n- BigQuery migration- [Legacy data warehouse migration challenges](https://cloud.google.com/blog/products/data-analytics/data-warehouse-migration-challenges-and-how-to-meet-them) .\n- [Migrating data warehouses to BigQuery](/architecture/dw2bq/dw-bq-migration-overview) .\n- [BigQuery for Data Warehouse professionals](/architecture/bigquery-data-warehouse) .\n- Composer migration- [Migrate environments to Airflow 2](/composer/docs/migrate-environments-airflow-2) .\n## What's next\nLearn about system design best practices for Google Cloud AI and machine learning, including the following:\n- Learn about [Google Cloud AI and machinelearning services](/architecture/framework/system-design/ai-ml#key) that support system design.\n- Learn [ML data processing best practices](/architecture/framework/system-design/ai-ml#data-process) .\n- Learn [best practices for model development and training](/architecture/framework/system-design/ai-ml#dev-train) .\nExplore other categories in the [Architecture Framework](/architecture/framework) such as reliability, operational excellence, and security, privacy, and compliance.# Implement machine learning\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) explains some of the core principles and best practices for data analytics in Google Cloud. You learn about some of the key AI and machine learning (ML) services, and how they can help during the various stages of the AI and ML lifecycle. These best practices help you to meet your AI and ML needs and create your system design. This document assumes that you're familiar with basic AI and ML concepts.\nTo simplify the development process and minimize overhead when you build ML models on Google Cloud, consider the highest level of abstraction that makes sense for your use case. is defined as the amount of complexity by which a system is viewed or programmed. The higher the level of abstraction, the less detail is available to the viewer.\nTo select Google AI and ML services based on your business needs, use the following table:\n| Persona                   | Google services                                                                                                   |\n|:--------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Business users                 | Standard solutions such as Contact Center AI Insights, Document AI, Discovery AI, and Cloud Healthcare API.                                                                            |\n| Developers with minimum ML experience           | Pretrained APIs address common perceptual tasks such as vision, video, and natural language. These APIs are supported by pretrained models and provide default detectors. They are ready to use without any ML expertise or model development effort. Pretrained APIs include: Vision API, Video API, Natural Language API, Speech-to-Text API, Text-to-Speech API, and Cloud Translation API.       |\n| Generative AI for Developers             | Vertex AI Search and Conversation lets developers use its out-of-the-box capabilities to build and deploy chatbots in minutes and search engines in hours. Developers who want to combine multiple capabilities into enterprise workflows can use the Gen App Builder API for direct integration.                              |\n| Developers and data scientists             | AutoML enables custom model development with your own image, video, text, or tabular data. AutoML accelerates model development with automatic search through the Google model zoo for the most performant model architecture, so you don't need to build the model. AutoML handles common tasks for you, such as choosing a model architecture, hyperparameter tuning, provisioning machines for training and serving. |\n| Data scientists and ML engineers            | Vertex AI custom model toolings let you train and serve custom models, and they operationalize the ML workflow. You can also run your ML workload on self-managed compute such as Compute Engine VMs.                                                     |\n| Data scientists & machine learning engineers         | Generative AI support on Vertex AI (also known as genai) provides access to Google's large generative AI models so you can test, tune, and deploy the models in your AI-powered applications.                                                       |\n| Data engineers, data scientists, and data analysts familiar with SQL interfaces | BigQuery ML lets you develop SQL-based models on top of data that's stored in BigQuery.                                                                                 |\n## Key services\nThe following table provides a high-level overview of AI and ML services:\n| Google service          | Description                                                                                                                                                                                                                                                          |\n|:---------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Cloud Storage and BigQuery       | Provide flexible storage options for machine learning data and artifacts.                                                                                                                                                                                                                                          |\n| BigQuery ML          | Lets you build machine learning models directly from data housed inside BigQuery.                                                                                                                                                                                                                                        |\n| Pub/Sub, Dataflow, Cloud Data Fusion, and Dataproc | Support batch and real-time data ingestion and processing. For more information, see Data Analytics.                                                                                                                                                                                                                                   |\n| Vertex AI           | Offers data scientists and machine learning engineers a single platform to create, train, test, monitor, tune, and deploy ML models for everything from generative AI to MLOps. Tooling includes the following: Vertex data labeling for getting accurate data labels. Deep Learning Containers for building and deploying models in a portable and consistent environment. Vertex AI Workbench for a managed or user-managed JupyterLab environment. Vertex training for a fully managed training service. Vertex AI TensorBoard for experiment tracking. Vertex AI Vizier for hyperparameter tuning. Vertex predictions for a unified deployment framework that supports custom-trained, BigQuery ML, and AutoML models. Vertex Explainable AI for surfacing explanations for model predictions. Vertex Experiments for accelerating the deployment of models into production. Now with faster model selection. MLOps tools such as Vertex AI Pipelines for orchestration and Vertex AI Feature Store for a fully managed feature repository. |\n| Vertex AI Search and Conversation     | Lets you build chatbots and search engines for websites and for use across enterprise data. Conversational AI on Vertex AI Search and Conversation can help reinvent customer and employee interactions with generative-AI-powered chatbots and digital assistants. For example, with these tools, you can provide more than just information by enabling transactions from within the chat experience. Enterprise Search on Vertex AI Search and Conversation, lets enterprises build search experiences for customers and employees on their public or private websites. In addition to providing high-quality multimodal search results, Enterprise Search can also summarize results and provide corresponding citations with generative AI.                                                                        |\n| Generative AI on Vertex AI       | Gives you access to Google's large generative AI models so you can test, tune, and deploy them for use in your AI-powered applications. Generative AI on Vertex AI is also known as genai. Generative AI models, which are also known as Foundation models, are categorized by the type of content they're designed to generate. This content includes text and chat, image, code, and text embeddings. Vertex AI Studio lets you rapidly prototype and test generative AI models in Google Cloud console. You can test sample prompts, design your own prompts, and customize foundation models to handle tasks that meet your application's needs. Model Tuning - lets you customize foundation models for specific use cases by tuning them using a dataset of input-output examples. Model Garden provides enterprise-ready foundation models, task-specific models, and APIs.                                        |\n| Pretrained APIs         | Pretrained APIs include Vision API, Video API, Natural Language API, Speech-to-Text API, Text-to-Speech API, and Cloud Translation API.                                                                                                                                                                                                                           |\n| AutoML            | Provides custom model tooling to build, deploy, and scale ML models. Developers can upload their own data and use the applicable AutoML service to build a custom model. AutoML Image: Performs image classification and object detection on image data. AutoML Video: Performs object detection, classification, and action recognition on video data. AutoML Text: Performs language classification, entity extraction, and sentiment analysis on text data. AutoML Translation: Detects and translates between language pairs. AutoML Tabular: Lets you build a regression, classification, or forecasting model. Intended for structured data.                                                                                                |\n| AI infrastructure         | Lets you use AI accelerators to process large-scale ML workloads. These accelerators let you train and get inference from deep learning models and from machine learning models in a cost-effective way. GPUs can help with cost-effective inference and scale-up or scale-out training for deep learning models. Tensor Processing Units (TPUs) are custom-built ASICs to train and execute deep neural networks.                                                                                                                                                        |\n| Dialogflow           | Delivers virtual agents that provide a conversational experience.                                                                                                                                                                                                                                            |\n| Contact Center AI         | Delivers an automated, insights-rich contact-center experience with Agent Assist functionality for human agents.                                                                                                                                                                                                                                |\n| Document AI          | Provides document understanding at scale for documents in general, and for specific document types like lending-related and procurement-related documents.                                                                                                                                                                                                                      |\n| Lending DocAI          | Automates mortgage document processing. Reduces processing time and streamlines data capture while supporting regulatory and compliance requirements.                                                                                                                                                                                                                       |\n| Procurement DocAI         | Automates procurement data capture at scale by turning unstructured documents (like invoices and receipts) into structured data to increase operational efficiency, improve customer experience, and inform decision-making.                                                                                                                                                                                                     |\n| Recommendations         | Delivers personalized product recommendations.                                                                                                                                                                                                                                                 |\n| Healthcare Natural Language AI      | Lets you review and analyze medical documents.                                                                                                                                                                                                                                                 |\n| Media Translation API        | Enables real-time speech translation from audio data.                                                                                                                                                                                                                                               |\n## Data processing\nApply the following data processing best practices to your own environment.\n### Ensure that your data meets ML requirements\nThe data that you use for ML should meet certain basic requirements, regardless of data type. These requirements include the data's ability to predict the target, consistency in granularity between the data used for training and the data used for prediction, and accurately labeled data for training. Your data should also be sufficient in volume. For more information, see [Data processing](/architecture/ml-on-gcp-best-practices#data-processing) .\n### Store tabular data in BigQuery\nIf you use tabular data, consider storing all data in BigQuery and using the [BigQuery Storage API](/bigquery/docs/reference/storage) to read data from it. To simplify interaction with the API, use one of the following additional tooling options, depending on where you want to read the data:\n- If you use Dataflow, use the [BigQuery I/O Connector](https://beam.apache.org/documentation/io/built-in/google-bigquery/) .\n- If you use TensorFlow or Keras, use the [tf.data.dataset reader for BigQuery](https://towardsdatascience.com/how-to-read-bigquery-data-from-tensorflow-2-0-efficiently-9234b69165c8) .\n- If you use unstructured data such as images or videos, consider storing all data in [Cloud Storage](/storage/docs) .\nThe input data type also determines the available model development tooling. Pre-trained APIs, AutoML, and BigQuery ML can provide more cost-effective and time-efficient development environments for certain image, video, text, and structured data use cases.\n### Ensure you have enough data to develop an ML model\nTo develop a useful ML model, you need to have enough data. To predict a category, the recommended number of examples for each category is 10 times the number of features. The more categories you want to predict, the more data you need. Imbalanced datasets require even more data. If you don't have enough labeled data available, consider semi-supervised learning.\nDataset size also has training and serving implications: if you have a small dataset, you can train it directly within a [Notebooks instance](/ai-platform-notebooks) ; if you have larger datasets that require distributed training, use [Vertex AI custom training service](/vertex-ai/docs/training/create-model-custom-training) . If you want Google to train the model for your data, use [AutoML](/automl/docs) .\n### Prepare data for consumption\nWell-prepared data can accelerate model development. When you configure your data pipeline, make sure that it can process both batch and stream data so that you get consistent results from both types of data.\n## Model development and training\nApply the following model development and training best practices to your own environment.\n### Choose managed or custom-trained model development\nWhen you build your model, consider the highest level of abstraction possible. Use AutoML when possible so that the development and training tasks are handled for you. For custom-trained models, choose managed options for scalability and flexibility, instead of self-managed options. To learn more about model development options, see [Use recommended tools and products](/architecture/ml-on-gcp-best-practices#use-recommended-tools-and-products) .\nConsider the [Vertex AI training service](/vertex-ai/docs/training/custom-training) instead of self-managed training on [Compute Engine](/compute/docs) VMs or [Deep Learning VM containers](/vertex-ai/docs/general/deep-learning) . For a JupyterLab environment, consider [Vertex AI Workbench](/vertex-ai/docs/workbench/introduction) , which provides both managed and user-managed JupyterLab environments. For more information, see [Machine learning development](/architecture/ml-on-gcp-best-practices#machine-learning-development) and [Operationalized training](/architecture/ml-on-gcp-best-practices#operationalized-training) .\n### Use pre-built or custom containers for custom-trained models\nFor custom-trained models on [Vertex AI](/vertex-ai/docs) , you can use pre-built or custom containers depending on your machine learning framework and framework version. [Pre-built containers](/vertex-ai/docs/training/pre-built-containers#available_container_images) are available for Python training applications that are created for specific TensorFlow, scikit-learn, PyTorch, and XGBoost versions.\nOtherwise, you can choose to [build a custom container for your training job](/vertex-ai/docs/training/create-custom-container) . For example, use a custom container if you want to train your model using a Python ML framework that isn't available in a pre-built container, or if you want to train using a programming language other than Python. In your custom container, pre-install your training application and all its dependencies onto an image that runs your training job.\n### Consider distributed training requirements\nConsider your [distributed training](/vertex-ai/docs/training/code-requirements#distributed) requirements. Some ML frameworks, like [TensorFlow](https://www.tensorflow.org/guide/distributed_training#multiworkermirroredstrategy) and [PyTorch](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) , let you run identical training code on multiple machines. These frameworks automatically coordinate division of work based on environment variables that are set on each machine. Other frameworks might require additional customization.\n## What's next\nFor more information about AI and machine learning, see the following:\n- [Best practices for implementing machine learning on Google Cloud](/architecture/ml-on-gcp-best-practices) .\n- [Practitioners guide to MLOps: A framework for continuous delivery and automation of machine learning](/resources/mlops-whitepaper) .\nExplore other categories in the [Architecture Framework](/architecture/framework) such as reliability, operational excellence, and security, privacy, and compliance.# Design for environmental sustainability\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) summarizes how you can approach environmental sustainability for your workloads in Google Cloud. It includes information about how to minimize your carbon footprint on Google Cloud.\n## Understand your carbon footprint\nTo understand the carbon footprint from your Google Cloud usage, use the [Carbon Footprint](/carbon-footprint) dashboard. The Carbon Footprint dashboard attributes emissions to the Google Cloud projects that you own and the cloud services that you use.\nFor more information, see [Understand your carbon footprint](/architecture/reduce-carbon-footprint#understand_your_carbon_footprint) in \"Reduce your Google Cloud carbon footprint.\"\n## Choose the most suitable cloud regions\nOne simple and effective way to reduce carbon emissions is to choose cloud regions with lower carbon emissions. To help you make this choice, Google publishes carbon data for all Google Cloud regions.\nWhen you choose a region, you might need to balance lowering emissions with other requirements, such as pricing and network latency. To help select a region, use the [Google Cloud Region Picker](https://cloud.withgoogle.com/region-picker/) .\nFor more information, see [Choose the most suitable cloud regions](/architecture/reduce-carbon-footprint#choose_the_most_suitable_cloud_regions) in \"Reduce your Google Cloud carbon footprint.\"\n## Choose the most suitable cloud services\nTo help reduce your existing carbon footprint, consider migrating your on-premises VM workloads to [Compute Engine](/compute) .\nAlso consider that many workloads don't require VMs. Often you can utilize a serverless offering instead. These managed services can optimize cloud resource usage, often automatically, which simultaneously reduces cloud costs and carbon footprint.\nFor more information, see [Choose the most suitable cloud services](/architecture/reduce-carbon-footprint#choose_the_most_suitable_cloud_services) in \"Reduce your Google Cloud carbon footprint.\"\n## Minimize idle cloud resources\nIdle resources incur unnecessary costs and emissions. Some common causes of idle resources include the following:\n- Unused active cloud resources, such as [idle VM instances](/compute/docs/instances/viewing-and-applying-idle-vm-recommendations) .\n- Over-provisioned resources, [such as larger VM instances](/compute/docs/instances/apply-machine-type-recommendations-for-instances) machine types than necessary for a workload.\n- Non-optimal architectures, such as [lift-and-shift](/architecture/migration-to-gcp-getting-started#lift_and_shift) migrations that aren't always optimized for efficiency. Consider making incremental improvements to these architectures.\nThe following are some general strategies to help minimize wasted cloud resources:\n- Identify idle or overprovisioned resources and either delete them or rightsize them.\n- Refactor your architecture to incorporate a more optimal design.\n- Migrate workloads to managed services.\nFor more information, see [Minimize idle cloud resources](/architecture/reduce-carbon-footprint#minimize_idle_cloud_resources) in \"Reduce your Google Cloud carbon footprint.\"\n## Reduce emissions for batch workloads\nRun batch workloads in regions with lower carbon emissions. For further reductions, run workloads at times that coincide with lower grid carbon intensity when possible.\nFor more information, see [Reduce emissions for batch workloads](/architecture/reduce-carbon-footprint#reduce_emissions_for_batch_workloads) in \"Reduce your Google Cloud carbon footprint.\"\n## What's next\n- Learn how to use [Carbon Footprint data](/carbon-footprint) to measure, report, and reduce your cloud carbon emissions.\n- Learn how to [reduce your Google Cloud carbon footprint](/architecture/reduce-carbon-footprint) .# Google Cloud Architecture Framework: Operational excellence\nThis category in the [Google Cloud Architecture Framework](/architecture/framework) shows you how to operate services efficiently on Google Cloud. It discusses how to run, manage, and monitor systems that deliver business value. It also discusses Google Cloud products and features that support operational excellence. Using the principles of operational excellence helps you build a foundation for reliability. It does so by setting up foundational elements like observability, automation, and scalability.\nThis Architecture Framework describes best practices, provides implementation recommendations, and explains some available products and services that help you achieve operational excellence. The framework aims to help you design your Google Cloud deployment so that it best matches your business needs.\nIn the operational excellence category of the Architecture Framework, you learn to do the following:\n- [Automate your deployments](/architecture/framework/operational-excellence/automate-your-deployments) \n- [Set up monitoring, alerting, and logging](/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging) \n- [Establish cloud support and escalation processes](/architecture/framework/operational-excellence/establish-cloud-support-and-escalation-processes) \n- [Manage capacity and quota](/architecture/framework/operational-excellence/manage-capacity-and-quota) \n- [Plan for peak traffic and launch events](/architecture/framework/operational-excellence/plan-for-peak-traffic-and-launch-events) \n- [Create a culture of automation](/architecture/framework/operational-excellence/create-a-culture-of-automation) # Automate your deployments\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices for automating your builds, tests, and deployments.\nAutomation helps you standardize your builds, tests, and deployments by eliminating human-induced errors for repeated processes like code updates. This section describes how to use various checks and guards as you automate. A standardized machine-controlled process helps ensure that your deployments are applied safely. It also provides a mechanism to restore previous deployments as needed without significantly affecting your user's experience.\n## Store your code in central code repositories\nStore your code in central code repositories that include a version control system with tagging and the ability to roll back code changes. Version control lets you organize files and control access, updates, and deletion across teams and organizations.\nFor different stages of development, version and label the repositories as needed. For example, labels could be `test` , `dev` , and `prod` .\nIn Google Cloud, you can store your code in [Cloud Source Repositories](/source-repositories/docs) , and version and integrate them with other Google Cloud products. If you are building containerized applications, use [Artifact Registry](/artifact-registry/docs) , a managed registry for containers.\nFor more details about version control, see [Version control](https://dora.dev/devops-capabilities/technical/version-control/) . For details about implementing trunk-based development with your repositories, see [Trunk-based development](https://dora.dev/devops-capabilities/technical/trunk-based-development/) .\n## Use continuous integration and continuous deployment (CI/CD)\nAutomate your deployments using a continuous integration and continuous deployment (CI/CD) approach. A CI/CD approach is a combination of pipelines that you configure and processes that your development team follows.\nA CI/CD approach increases deployment velocity by making your software development team more productive. This approach lets developers make smaller and more frequent changes that are thoroughly tested while reducing the time needed to deploy those changes.\nAs part of your CI/CD approach, automate all the steps that are part of building, testing, and deploying your code. For example:\n- Whenever new code is committed to the repository, have the commit automatically invoke the build and test pipeline.\n- Automate integration testing.\n- Automate your deployment so that changes deploy after your build meets specific testing criteria.\nIn Google Cloud, you can use [Cloud Build](/build/docs) and [Cloud Deploy](/deploy/docs) for your CI/CD pipelines.\nUse Cloud Build to help define dependencies and versions that you can use for packaging and building an application package. Version your build configuration to make sure all your builds are consistent, and to make sure you can roll back to a previous configuration if necessary.\nUse Cloud Deploy to deploy your applications to specific environments on Google Cloud, and to manage your deployment pipelines.\nFor more details about implementing CI/CD, read [Continuous integration](https://dora.dev/devops-capabilities/technical/continuous-integration/) and [Deployment automation](https://dora.dev/devops-capabilities/technical/deployment-automation/) .\n## Provision and manage your infrastructure using infrastructure as code\nInfrastructure as code is the use of a descriptive model to manage infrastructure, such as VMs, and configurations, such as firewall rules. Infrastructure as code lets you do the following:\n- Create your cloud resources automatically, including the deployment or test environments for your CI/CD pipeline.\n- Treat infrastructure changes like you treat application changes. For example, ensure changes to the configuration are reviewed, tested, and can be audited.\n- Have a single version of the truth for your cloud infrastructure.\n- Replicate your cloud environment as needed.\n- Roll back to a previous configuration if necessary.\nThis concept of infrastructure as code also applies to projects in Google Cloud. You can use this approach to define resources such as Shared VPC connectivity or Identity and Access Management (IAM) access in your projects. For an example of this approach, see the [Google Cloud Project Factory Terraform Module](https://registry.terraform.io/modules/terraform-google-modules/project-factory/google/latest) .\nThird-party tools, like [Terraform](/docs/terraform) , help you to automatically create your infrastructure on Google Cloud. For more information, see [Managing infrastructure as code with Terraform, Cloud Build, and GitOps](/architecture/managing-infrastructure-as-code) .\nConsider using Google Cloud features, such as [project liens](/resource-manager/docs/project-liens) , [Cloud Storage retention policies](/storage/docs/bucket-lock) , and Cloud Storage [bucket locks](/storage/docs/using-bucket-lock#lock-bucket) , to protect critical resources from being accidentally or maliciously deleted.\n## Incorporate testing throughout the software delivery lifecycle\nTesting is critical to successfully launching your software. Continuous testing helps teams create high-quality software faster and enhance software stability.\nTesting types:\n- **Unit tests.** Unit tests are fast and help you perform rapid deployments. Treat unit tests as part of the codebase and include them as part of the build process.\n- **Integration tests.** Integration tests are important, especially for workloads that are designed for scale and dependent on more than one service. These tests can become complex when you test for integration with interconnected services.\n- **System tests.** System tests are time consuming and complex, but they help you identify edge cases and fix issues before deployment.\n- **Other tests.** There are other tests you should run, including static testing, load testing, security testing, policy validation testing, and others. Run these tests before deploying your application in production.\nTo incorporate testing:\n- Perform all types of testing continuously throughout the software delivery lifecycle.\n- Automate these tests and include them in the CI/CD pipeline. Make your pipeline fail if any of the tests fail.\n- Update and add new tests continuously to improve and maintain the operational health of your deployment.\nFor your testing environments:\n- Use separate Google Cloud projects for each test environment you have. For each application, use a separate project environment. This separation provides a clear demarcation between production environment resources and the resources of your lower environments. This separation helps ensure that any changes in one environment don't accidentally affect other environments.\n- Automate the creation of test environments. One way to do this automation is using [infrastructure as code](/architecture/framework/operational-excellence/automate-your-deployments#provision_and_manage_your_infrastructure_using_infrastructure_as_code) .\n- Use a synthetic production environment to test changes. This environment provides a production-like environment to test your application and perform various types of tests on your workloads, including end-to-end testing and performance testing.\nFor more information about implementing continuous testing, see [Test automation](https://dora.dev/devops-capabilities/technical/test-automation/) .\n## Launch deployments gradually\nChoose your deployment strategy based on important parameters, like minimum disruption to end users, rolling updates, rollback strategies, and A/B testing strategies. For each workload, evaluate these requirements and pick a deployment strategy from proven techniques, such as rolling updates, blue/green deployments, and canary deployments.\nOnly let CI/CD processes make and push changes in your production environment.\nConsider using an immutable infrastructure. An immutable infrastructure is an infrastructure that isn't changed or updated. When you need to deploy new code or change any other configuration in your environment, you replace the entire environment (a collection of VMs, or Pods for example) with the new environment. Blue/green deployments are an example of immutable infrastructure.\nWe recommend that you do canary testing and observe your system for any errors as you deploy changes. This type of observation is easier if you have a robust monitoring and alerting system. To do A/B testing or canary testing, you can use Google Cloud's [managed instance groups.](/compute/docs/instance-groups/rolling-out-updates-to-managed-instance-groups) . Then you can perform a slow rollout, or a restoration if necessary.\nConsider using [Cloud Deploy](/deploy/docs) to automate deployments and manage your deployment pipeline. You can also use many third-party tools, like [Spinnaker](https://spinnaker.io/) and [Tekton](/tekton) , on Google Cloud for both automated deployments and for creating deployment pipelines.\n## Restore previous releases seamlessly\nDefine your restoration strategy as part of your deployment strategy. Ensure that you can roll back a deployment, or an infrastructure configuration, to a previous version of the source code. Restoring a previous stable deployment is an important step in incident management for both reliability and security incidents.\nAlso ensure that you can restore the environment to the state it was in before the deployment process started. This can include:\n- The ability to revert any code changes in your application.\n- The ability to revert any configuration changes made to the environment.\n- Using immutable infrastructure and ensuring that deployments don't change the environment. These practices make reverting configuration changes easier.## Monitor your CI/CD pipelines\nTo keep your automated build, test, and deploy process running smoothly, monitor your CI/CD pipelines. Set alerts that indicate when anything in any pipeline fails. Each step of your pipeline should write suitable log statements so that your team can perform root cause analysis if a pipeline fails.\nIn Google Cloud, all the CI/CD services are integrated with Google Cloud Observability. For example:\n- Cloud Source Repositories are [integrated with the Pub/Sub service](/source-repositories/docs/code-change-notification) .\n- Cloud Build is [integrated with Pub/Sub](/build/docs/subscribe-build-notifications) and also stores [audit logs](/build/docs/securing-builds/audit-logs) and [build logs](/build/docs/securing-builds/store-manage-build-logs) . You can set alerts for certain keywords in your build logs and for many other metrics using the [Cloud Monitoring](/monitoring) service.\n- Cloud Deploy [stores audit logs](/deploy/docs/audit-logs) .\nFor details about monitoring and logging, see [Set up monitoring, alerting, and logging](/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging) .\n## Deploy applications securely\nReview the [Deploy applications securely](/architecture/framework/security/app-security) section from the security, compliance and privacy category of the Architecture Framework.\n## Establish management guidelines for version releases\nTo help your engineers avoid making mistakes, and to enable high-velocity software delivery, ensure that your management guidelines for releasing new software versions are clearly documented.\nRelease engineers oversee how software is built and delivered. The system of release engineering is guided by four practices:\n- **Self-service mode.** Establish guidelines to help software engineers avoid common mistakes. These guidelines are generally codified in automated processes.\n- **Frequent releases.** High velocity helps troubleshooting and makes fixing issues easier. Frequent releases rely on automated unit tests.\n- **Hermetic builds.** Ensure consistency with your build tools. Version the build compilers you use to build versions now versus one month ago.\n- **Policy enforcement.** All changes need code review, ideally including a set of guidelines and policy to enforce security. Policy enforcement improves code review, troubleshooting, and testing a new release.## What's next\n- [Set up monitoring, alerting, and logging](/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging) (next document in this series)\n- [Deploy applications securely](/architecture/framework/security/app-security) from the security, compliance and privacy category of the Architecture Framework\n- Review [best practices for building containers ](/architecture/best-practices-for-building-containers) \n- Learn about [Technical capabilities](https://dora.dev/devops-capabilities/) \n- Explore other categories in the [Architecture Framework](/architecture/framework) such as system design, security, privacy, and compliance, reliability, cost optimization, and performance optimization.# Set up monitoring, alerting, and logging\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) shows you how to set up monitoring, alerting, and logging so that you can act based on the behavior of your system. This includes identifying meaningful metrics to track and building dashboards to make it easier to view information about your systems.\nThe [DevOps Resource and Assessment (DORA)](https://dora.dev/) research program [defines monitoring](https://dora.dev/devops-capabilities/technical/monitoring-and-observability/) as:\n\"The process of collecting, analyzing, and using information to track applications and infrastructure in order to guide business decisions. Monitoring is a key capability because it gives you insight into your systems and your work.\"\nMonitoring enables service owners to:\n- Make informed decisions when changes to the service affect performance\n- Apply a scientific approach to incident response\n- Measure your service's alignment with business goals\nWith monitoring, logging, and alerting in place, you can do the following:\n- Analyze long-term trends\n- Compare your experiments over time\n- Define alerting on critical metrics\n- Build relevant real-time dashboards\n- Perform retrospective analysis\n- Monitor both business-driven metrics and system-health metric- Business-driven metrics help you understand how well your systems support your business. For example, use metrics to monitor the following:- The cost to an application to serve a user\n- The volume change in site traffic following a redesign\n- How long it takes a customer to purchase a product on your site\n- System health metrics help you understand whether your systems are operating correctly and within acceptable performance levels.Use the following [four golden signals](https://sre.google/sre-book/monitoring-distributed-systems/#xref_monitoring_golden-signals) to monitor your system:\n- **Latency** . The time it takes to service a request.\n- **Traffic** . How much demand is being placed on your system.\n- **Errors** . The rate of requests that fail. Failure can be explicit (for example, HTTP 500s), implicit (for example, an HTTP 200 success response coupled with the wrong content), or by policy (for example, if you commit to one-second response times, any request over one second is an error).\n- **Saturation** . How full your service is. Saturation is a measure of your system fraction, emphasizing the resources that are most constrained (that is, in a memory-constrained system, show memory; in an I/O-constrained system, show I/O).## Create a monitoring plan\nCreate a monitoring plan that aligns with your organization's mission and its operations strategy. Include monitoring and observability planning during application development. Including a monitoring plan early in application development can drive an organization toward operational excellence.\nInclude the following details in your monitoring plan:\n- Include all your systems, including on-premises resources and cloud resources.\n- Include monitoring of your cloud costs to help make sure that scaling events doesn't cause usage to cross your budget thresholds.\n- Build different monitoring strategies for measuring infrastructure performance, user experience, and business key performance indicators (KPIs). For example, static thresholds might work well to measure infrastructure performance but don't truly reflect the user's experience.\nUpdate the plan as your monitoring strategies mature. Iterate on the plan to improve the health of your systems.\n## Define metrics that measure all aspects of your organization\nDefine the metrics that are required to measure how your deployment behaves. To do so:\n- Define your business objectives.\n- Identify the metrics and KPIs that can provide you with quantifiable information to measure performance. Make sure your metric definitions translate to all aspects of your organization, from business needs\u2014including cloud costs\u2014to technical components.\n- Use these metrics to create [service level indicators](/architecture/framework/reliability/principles) (SLIs) for your applications. For more information, see [Choose appropriate SLIs](/architecture/framework/reliability/define-goals#choose_appropriate_slis) .\n### Common metrics for various components\nMetrics are generated at all levels of your service, from infrastructure and networking to business logic. For example:\n- Infrastructure metrics:- Virtual machine statistics, including instances, CPU, memory, utilization, and counts\n- Container-based statistics, including cluster utilization, cluster capacity, pod level utilization, and counts\n- Networking statistics, including ingress/egress, bandwidth between components, latency, and throughput\n- Requests per second, as measured by the load balancer\n- Total disk blocks read, per disk\n- Packets sent over a given network interface\n- Memory heap size for a given process\n- Distribution of response latencies\n- Number of invalid queries rejected by a database instance\n- Application metrics:- Application-specific behavior, including queries per second, writes per second, and messages sent per second\n- Managed services statistics metrics:- QPS, throughput, latency, utilization for Google-managed services (APIs or products such as BigQuery, App Engine, and Bigtable)\n- Network connectivity statistics metrics:- VPN/interconnect-related statistics about connecting to on-premises systems or systems that are external to Google Cloud.\n- SLIs- Metrics associated with the overall health of the system.\n## Set up monitoring\nSet up monitoring to monitor both on-premises resources and cloud resources.\nChoose a monitoring solution that:\n- Is platform independent\n- Provides uniform capabilities for monitoring of on-premises, hybrid, and multi-cloud environments\nUsing a single platform to consolidate the monitoring data that comes in from different sources lets you build uniform metrics and visualization dashboards.\nAs you set up monitoring, automate monitoring tasks where possible.\n### Monitoring with Google Cloud\nUsing a monitoring service, such as [Cloud Monitoring](/monitoring) , is easier than building a monitoring service yourself. Monitoring a complex application is a substantial engineering endeavor by itself. Even with existing infrastructure for instrumentation, data collection and display, and alerting in place, it is a full-time job for someone to build and maintain.\nConsider using Cloud Monitoring to obtain visibility into the performance, availability, and health of your applications and infrastructure for both on-premises and cloud resources.\nCloud Monitoring is a managed service that is part of the [Google Cloud Observability](/stackdriver/docs) . You can use Cloud Monitoring to monitor Google Cloud services and custom metrics. Cloud Monitoring provides an API for integration with third-party monitoring tools.\nCloud Monitoring aggregates metrics, logs, and events from your system's cloud-based infrastructure. That data gives developers and operators a rich set of observable signals that can speed root-cause analysis and reduce mean time to resolution. You can use Cloud Monitoring to define alerts and custom metrics that meet your business objectives and help you aggregate, visualize, and monitor system health.\nCloud Monitoring provides default dashboards for cloud and open source application services. Using the [metrics model](/monitoring/api/metrics) , you can define custom dashboards with powerful visualization tools and configure charts in [Metrics Explorer](/monitoring/charts/metrics-explorer) .\n## Set up alerting\nA good alerting system improves your ability to release features. It helps compare performance over time to determine the velocity of feature releases or the need to roll back a feature release. For information about rollbacks, see [Restore previous releases seamlessly](/architecture/framework/operational-excellence/automate-your-deployments#restore_previous_releases_seamlessly) .\nAs you set up alerting, map alerts directly to critical metrics. These critical metrics include:\n- The:- Latency\n- Traffic\n- Errors\n- Saturation\n- System health\n- Service usage\n- Security events\n- User experience\nMake alerts actionable to minimize the time to resolution. To do so, for each alert:\n- Include a clear description, including stating what is monitored and its business impact.\n- Provide all the information necessary to act immediately. If it takes a few clicks and navigation to understand alerts, it is challenging for the on-call person to act.\n- Define priority levels for various alerts.\n- Clearly identify the person or team responsible for responding to the alert.\nFor critical applications and services, build self-healing actions into the alerts triggered due to common fault conditions such as service health failure, configuration change, or throughput spikes.\nAs you set up alerts, try to eliminate toil. For example, eliminate toil by eliminating frequent errors, or automating fixes for these errors which possibly avoids an alert being triggered. Eliminating toil lets those on call focus on making your application's operational components reliable. For more information, see [Create a culture of automation](/architecture/framework/operational-excellence/create-a-culture-of-automation) .\n## Build monitoring and alerting dashboards\nOnce monitoring is in place, build relevant, uncomplicated dashboards that include information from your monitoring and alerting systems.\nChoosing an appropriate way to visualize your dashboard can be difficult to tie into your reliability goals. Create dashboards to visualize both:\n- Short-term and real-time analysis\n- Long-term analysis\nFor more information about implementing visual management, see the capability article [Visual management](https://dora.dev/devops-capabilities/process/visual-management/) .\n## Enable logging for critical applications\nLogging services are critical to monitoring your systems. While metrics form the basis of specific items to monitor, logs contain valuable information that you need for debugging, security-related analysis, and compliance requirements.\nLogging the data your systems generate helps you ensure an effective security posture. For more information about logging and security, see [Implement logging and detective controls](/architecture/framework/security/logging-detection) in the security category of the Architecture Framework.\n[Cloud Logging](/logging/docs) is an integrated logging service you can use to store, search, analyze, monitor, and alert on log data and events. Logging automatically collects logs from the services of Google Cloud and other cloud providers. You can use these logs to build metrics for monitoring and to create logging exports to external services such as [Cloud Storage](/storage/docs) , [BigQuery](/bigquery/docs) , and [Pub/Sub](/pubsub/docs) .\n## Set up an audit trail\nTo help answer questions like \"who did what, where, and when\" in your Google Cloud projects, use [Cloud Audit Logs](/logging/docs/audit) .\nCloud Audit Logs captures several types of activity, such as the following:\n- logs contain log entries for API calls or other administrative actions that modify the configuration or metadata of resources. Admin Activity logs are always enabled.\n- audit logs record API calls that create, modify, or read user-provided data. Data Access audit logs are disabled by default because they can be quite large. You can configure which Google Cloud services produce data access logs.\nFor a list of Google Cloud services that write audit logs, see [Google services with audit logs](/logging/docs/audit/services) . Use Identity and Access Management (IAM) controls to limit who has access to view audit logs.\n## What's next\n- [Establish cloud support and escalation processes](/architecture/framework/operational-excellence/establish-cloud-support-and-escalation-processes) (next document in this series).\n- Explore [Google Cloud Observability](/stackdriver/docs) .\n- Deploy [Cloud Monitoring](/monitoring) to gain visibility into the performance, availability, and health of your applications and infrastructure.\n- Learn more about [Monitoring and observability](https://dora.dev/devops-capabilities/technical/monitoring-and-observability/) .\n- Explore other categories in the [Architecture Framework](/architecture/framework) such as system design, security, privacy, and compliance, reliability, cost optimization, and performance optimization.# Establish cloud support and escalation processes\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) shows you how to define an effective escalation process. Establishing support from your cloud provider or other third-party service providers is a key part of effective escalation management.\nGoogle Cloud provides you with various support channels, including [live support](/support) or through published guidance such as [developer communities](/support/docs/community) or [product documentation](/docs) . An offering from [Cloud Customer Care](/support) ensures you can work with Google Cloud to run your workloads efficiently.\n## Establish support from your providers\nPurchase a support contract from your cloud provider or other third-party service providers. Support is critical to ensure the prompt response and resolution of various operational issues.\nTo work with Google Cloud Customer Care, consider purchasing a [Customer Care offering](/support) that includes Standard, Enhanced, or Premium Support. Consider using Enhanced or Premium Support for your major production environments.\n## Define your escalation process\nA well-defined escalation process is key to reducing the effort and time that it takes to identify and address any issues in your systems. This includes issues that require support for Google Cloud products or for other cloud producers or third-party services.\nTo create your escalation path:\n- Define when and how to escalate issues internally.\n- Define when and how to create support cases with your cloud provider or other third-party service provider.\n- Learn how to work with the teams that provide you support. For Google Cloud, you and your operations teams should review the [Best practices for working with Customer Care](/support/docs/best-practices) . Incorporate these practices into your escalation path.\n- Find or create documents that describe your architecture. Ensure these documents include information that is helpful for support engineers.\n- Define how your teams communicate during an outage.\n- Ensure that people who need support have [appropriate levels of support permissions](/support/docs/manage-cases#before_you_begin) to access the Google Cloud Support Center, or to communicate with other support providers. To learn about using the Google Cloud Support Center, visit [Support procedures](/support/docs/procedures) .\n- [Set up monitoring, alerting, and logging](/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging) so that you have the information needed to act on when issues arise.\n- Create templates for incident reporting. For information to include in your incident reports, see [Best practices for working with Customer Care](/support/docs/best-practices) .\n- Document your organization's escalation process. Ensure that you have clear, well-defined actions to address escalations.\n- Include a plan to teach new team members how to interact with support.\nRegularly test your escalation process internally. Test your escalation process before major events, such as migrations, new product launches, and peak traffic events. If you have Google Cloud Customer Care [Premium Support](/support/docs/premium) , your [Technical Account Manager](/tam) can help review your escalation process and coordinate your tests with Google Cloud Customer Care.\n## Ensure you receive communication from support\nEnsure that your administrators are receiving communication from your cloud providers and third-party services. This information allows admins to make informed decisions and fix issues before they cause larger problems. Ensure that the following are true:\n- Security, network, and system administrators are set up to receive critical emails from Google Cloud and your other providers or services.\n- Security, network, and system administrators are set up to receive system alerts generated by monitoring tools, like Cloud Monitoring.\n- Project owners have email-routable usernames so that they can receive critical emails.\nFor information about managing notifications from Google Cloud, see [Managing contacts for notifications](/resource-manager/docs/managing-notification-contacts) .\n## Establish review processes\nEstablish a review or postmortem processes. Follow these processes after you raise a new support ticket or escalate an existing support ticket. As part of the postmortem, document the lessons learned and track mitigations. As you do this review, foster a blameless postmortem culture.\nFor more information about postmortems, see [Postmortem Culture: Learning from Failure](https://sre.google/sre-book/postmortem-culture/) .\n## Build centers of excellence\nIt can be valuable to capture your organization's information, experience, and patterns in an internal knowledge base, such as a wiki, Google site, or intranet site. As new products and features are continually being rolled out in Google Cloud, this knowledge can help track why you chose a particular design for your applications and services. For more information, see [Architecture decision records](/architecture/architecture-decision-records) .\nIt's also a good practice to nominate Google Cloud experts and champions in your organization. A range of [training and certification](/training) options are available to help nominated champions grow in their area of expertise. Teams can stay up to date on the latest news, announcements, and customer stories by subscribing to the [Google Cloud blog](/blog) .\n## What's next\n- [Manage capacity and quota](/architecture/framework/operational-excellence/manage-capacity-and-quota) (next document in this series)\n- Explore other categories in the [Architecture Framework](/architecture/framework) such as system design, security, privacy, and compliance, reliability, cost optimization, and performance optimization.# Manage capacity and quota\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) shows you how to evaluate and plan your capacity and quota on the cloud.\nIn conventional data centers, you typically spend cycles each quarter reviewing current resource requirements and forecasting future ones. You must consider physical, logistical, and human-resource-related concerns. Concerns like rack space, cooling, electricity, bandwidth, cabling, procurement times, shipping times, and how many engineers are available to rack and stack new equipment need to be considered. You also have to actively manage capacity and workload distributions so that resource-intensive jobs, such as Hadoop pipelines, don't interfere with services, such as web servers, that must be highly available.\nIn contrast, when you use Google Cloud you cede most capacity planning to Google. Using the cloud means you don't have to provision and maintain idle resources when they aren't needed. For example, you can create, scale up, and scale down VM instances as needed. Because you pay for what you use, you can optimize your spending, including excess capacity that you only need at peak traffic times. To help you save, Compute Engine provides [machine type recommendations](/compute/docs/instances/apply-machine-type-recommendations-for-instances) if it detects that you have underutilized VM instances that can be resized or deleted.\n## Evaluate your cloud capacity requirements\nTo manage your capacity effectively, you need to know your organization's capacity requirements.\nTo evaluate your capacity requirements, start by identifying your top cloud workloads. Evaluate the average and peak utilizations of these workloads, and their current and future capacity needs.\nIdentify the teams who use these top workloads. Work with them to establish an internal demand-planning process. Use this process to understand their current and forecasted cloud resource needs.\nAnalyze load pattern and call distribution. Use factors like last 30 days peak, hourly peak, and peak per minute in your analysis.\nConsider using [Cloud Monitoring](/monitoring) to get visibility into the performance, uptime, and overall health of your applications and infrastructure.\n## View your infrastructure utilization metrics\nTo make capacity planning easier, gather and store historical data about your organization's use of cloud resources.\nEnsure you have visibility into infrastructure utilization metrics. For example, for top workloads, evaluate the following:\n- Average and peak utilization\n- Spikes in usage patterns\n- Seasonal spikes based on business requirements, such as holiday periods for retailers\n- How much over-provisioning is needed to prepare for peak events and rapidly handle potential traffic spikes\nEnsure your organization has set up alerts to automatically notify of when you get close to quota and capacity limitations.\nUse Google's monitoring tools to get insights on application usage and capacity. For example, you can define custom metrics with Monitoring. Use these custom metrics to define alerting trends. Monitoring also provides flexible dashboards and rich visualization tools to help identify emergent issues.\n## Create a process for capacity planning\nEstablish a process for capacity planning and document this plan.\nAs you create this plan do the following:\n- Run load tests to determine how much load the system can handle while meeting its latency targets, given a fixed amount of resources. Load tests should use a mix of request types that matches production traffic profiles from live users. Don't use a uniform or random mix of operations. Include spikes in usage in your traffic profile.\n- Create a capacity model. A capacity model is a set of formulas for calculating incremental resources needed per unit increase in service load, as determined from load testing.\n- Forecast future traffic and account for growth. See the article [Measure Future Load](https://cacm.acm.org/magazines/2019/4/235621-metrics-that-matter/fulltext#body-4) for a summary of how Google builds traffic forecasts.\n- Apply the capacity model to the forecast to determine future resource needs.\n- Estimate the cost of resources your organization needs. Then, get budget approval from your Finance organization. This step is essential because the business can choose to make cost versus risk tradeoffs across a range of products. Those tradeoffs can mean acquiring capacity that's lower or higher than the predicted need for a given product based on business priorities.\n- Work with your cloud provider to get the correct amount of resources at the correct time with quotas and reservations. Involve infrastructure teams for capacity planning and have operations create capacity plans with confidence intervals.\n- Repeat the previous steps every quarter or two.\nFor more detailed guidance on the process of planning capacity while also optimizing resource usage, see [Capacity Planning](https://www.usenix.org/publications/login/feb15/capacity-planning) .\n## Ensure your quotas match your capacity requirements\nGoogle Cloud uses [quotas](/docs/quota) to restrict how much of a particular shared Google Cloud resource that you can use. Each quota represents a specific countable resource, such as API calls to a particular service, the number of load balancers used concurrently by your project, or the number of projects that you can create. For example, quotas ensure that a few customers or projects can't monopolize CPU cores in a particular region or zone.\nAs you review your quota, consider these details:\n- Plan the capacity requirements of your projects in advance to prevent unexpected limiting of your resource consumption.\n- Set up your quota and capacity to handle full region failure.\n- Use quotas to cap the consumption of a particular resource. For example, you can set a maximum [query usage per day](/bigquery/quotas) quota over the BigQuery API to ensure that a project doesn't overspend on BigQuery.\n- Plan for spikes in usage and include these spikes as part of your quota planning. Spikes in usage can be expected fluctuations throughout the day, unexpected peak traffic events, or known peak traffic and launch events. For details about how to plan for peak traffic and launch events, read the next section in Operational Excellence: [Plan for peak traffic and launch events](/architecture/framework/operational-excellence/plan-for-peak-traffic-and-launch-events) .\nIf your current quotas aren't sufficient, you can [manage your quota using the Google Cloud console](/docs/quota#managing_your_quota_console) . If you require a large capacity, contact your Google Cloud sales team. However, you should know that many services also have limits that are unrelated to the quota system, see [Working with quotas](/docs/quota) for more information.\nRegularly review your quotas. Submit quota requests before they're needed. Read [Working with quotas](/docs/quota) for details about how quota requests are evaluated and how requests are approved or denied.\nThere are several ways to view and manage your Google Cloud quota:\n- [Using the Google Cloud console](/docs/quota#viewing_your_quota_console) \n- [Using the Google Cloud CLI](/docs/quota#managing_your_quota_gcloud) \n- [Using the Service Usage API](/docs/quota_detail/view_manage#managing_your_quota_api) \n- [Using Monitoring to view quote metrics](/docs/quota_detail/monitor) \n- To manage quotas over many Google Cloud projects within an organization or folder, consider implementing the [Quota Monitoring Dashboard Solution](https://github.com/GoogleCloudPlatform/professional-services/tree/main/tools/quota-monitoring-alerting/java) .## What's next\n- [Plan for peak traffic and launch events](/architecture/framework/operational-excellence/plan-for-peak-traffic-and-launch-events) (next document in this series)\n- Explore other categories in the [Architecture Framework](/architecture/framework) such as system design, security, privacy, and compliance, reliability, cost optimization, and performance optimization.# Plan for peak traffic and launch events\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) shows you how to plan for peak traffic and launch events to avoid disrupting your business.\nPeak events are major business-related events that cause a sharp increase in traffic beyond the application's standard baseline. These peak events require planned scaling.\nFor example, retail businesses with an online presence can expect peak events during holidays. Black Friday, which occurs the day after Thanksgiving in the United States, is one of the busiest shopping days of the year. For the healthcare industry in the United States, the months of October and November can have peak events due to spikes in online traffic for benefits enrollment.\nLaunch events are any substantial roll outs or migrations of new capability in production. For example, a migration from on-premises to the cloud, or a launch of a new product service or feature.\nIf you are launching a new product, you should expect an increased load on your systems during the announcement and potentially after. These events can often cause load increases of 5\u201320 times (or greater) on frontend systems. That increased load increases the load on backend systems as well. Often, these frontend and backend loads are characterized by rapid scaling over a short time as the event opens for web traffic. Launch events involve a trailing decrease in traffic to normal levels. This decrease is usually slower than the scale to peak.\nPeak and launch events includes three stages:\n- Planning and preparation for the launch or peak traffic event\n- Launching the event\n- Reviewing event performance and post event analysis\nThe practices described in this document can help each of these stages run smoothly.\n## Create a general playbook for launch and peak events\nBuild a general playbook with a long-term view of current and future peak events. Keep adding lessons learned to the document, so it can be a reference for future peak events.\n## Plan for your launch and for peak events\nPlan ahead. Create business projections for upcoming launches and for expected (and unexpected) peak events. Preparing your system for scale spikes depends on understanding your business projections. The more you know about prior forecasts, the more accurate you can make your new business forecasts. Those new forecasts are critical inputs into projecting expected demand on your system.\nEstablishing program management and coordinated planning\u2014across your organization and with third-party vendors\u2014is also a key to success. Get these teams set up early so that your program management team can set timelines, secure budgets, and gather resources for additional infrastructure, testing support, and training.\nIt's important to set up clear communication channels. Communication is critical for all stages of a launch or a peak event. Discuss risks and areas of concern early and swarm issues before they become blockers. Create event planning documentation. Condense the most critical information about the peak event and distribute it. Doing so helps people absorb planning information and resolves basic questions. The document helps bring new people up to speed on peak-event planning.\nDocument your plan for each event. As you document your plan, ensure that you do the following:\n- Identify any assumptions, risks, and unknown factors.\n- Review past events to determine relevant information for the upcoming launch or peak event. Determine what data is available and what value that data has provided in the past.\n- Detail the rollback plan for launch and migration events.\n- Perform an architecture review:- Document key resources and architectural components.\n- Use the [Architecture Framework](/architecture/framework) to review all aspects of the environment for risks and scale concerns.\n- Create a diagram that shows how the major components of the architecture are connected. A review of the diagram might help you isolate issues and expedite their resolution.\n- If appropriate, configure the service to use alert actions to auto-restart if there's a failure. When using Compute Engine, consider using [autoscaling](/compute/docs/autoscaler) for handling throughput spikes.\n- To make sure that Compute Engine resources are available when you need them, use [Reservations](/compute/docs/instances/reservations-overview) . Reservations provide a very high level of assurance in obtaining capacity for Compute Engine zonal resources. You can use reservations to help ensure that your project has resources available.\n- Identify metrics and alerts to track:- Identify business and system metrics to monitor for the event. If any metrics or service level indicators (SLIs) aren't being collected, modify the system to collect this data.\n- Ensure you have sufficient monitoring and alerting capabilities and have reviewed normal and previous peak traffic patterns. Ensure alerts are set appropriately. Use [Google Cloud Monitoring](//monitoring/docs) tools to view application use, capacity, and the overall health of your applications and infrastructure.\n- Ensure system metrics are being captured with monitoring and alert levels of interest.\n- Review your increased capacity requirements with your Google Cloud account team and plan for the required quota management. For more details, review [Ensure your quotas match your capacity requirements](/architecture/framework/operational-excellence/manage-capacity-and-quota#ensure_your_quotas_match_your_capacity_requirements) .\n- Ensure you have appropriate cloud support levels, your team understands how to open support cases, and you have an escalation path established. For more details, review [Establish cloud support and escalation processes](/architecture/framework/operational-excellence/establish-cloud-support-and-escalation-processes) .\n- Define a communication plan, timeline, and responsibilities:- Engage cross-functional stakeholders to coordinate communication and program planning. These stakeholders can include appropriate people from technical, operational, and leadership teams, and third-party vendors.\n- Establish an unambiguous timeline containing critical tasks and the teams that own them.\n- Establish a responsibility assignment matrix (RACI) to communicate ownership for teams, team leads, stakeholders, and responsible parties.\n- You can use Premium Support's [Event Management Service](/support/docs/premium#event_management) for planned peak events. With this service, Customer Care partners with your team to create a plan and provide guidance throughout the event.\n## Establish review processes\nWhen the peak traffic event or launch event is over, review the event to document the lessons you learned. Then, update your playbook with those lessons. Finally, apply what you learned to the next major event. Learning from prior events is important, especially when they highlight constraints to the system while under stress.\nRetrospective reviews, also called postmortems, for peak traffic events or launch events are a useful technique for capturing data and understanding the incidents that occurred. Do this review for peak traffic and launch events that went as expected, and for any incidents that caused problems. As you do this review foster a blameless culture.\nFor more information about postmortems, see [Postmortem Culture: Learning from Failure](https://sre.google/sre-book/postmortem-culture/) .\n## What's next\n- [Create a culture of automation](/architecture/framework/operational-excellence/create-a-culture-of-automation) (next document in this series)\n- Explore other categories in the [Architecture Framework](/architecture/framework) such as system design, security, privacy, and compliance, reliability, cost optimization, and performance optimization.# Create a culture of automation\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) shows you how to assess toil and mitigate its impacts on your systems and your teams.\nToil is manual and repetitive work with no enduring value, and it increases as a service grows. Continually aim to reduce or eliminate toil. Otherwise, operational work can eventually overwhelm operators, and any growth in product use or complexity can require additional staffing.\nAutomation is a key way to minimize toil. Automation also improves release velocity and helps minimize human-induced errors.\nFor more information, see [Eliminating Toil](https://landing.google.com/sre/workbook/chapters/eliminating-toil/) .\n## Create an inventory and assess the cost of toil\nStart by creating an inventory and assessing the cost of toil on the teams managing your systems. Make this a continuous process, followed by investing in customized automation to extend what's already provided by Google Cloud services and partners. You can often modify Google Cloud's own automation\u2014for example, [Compute Engine's autoscaler](/compute/docs/autoscaler) .\n## Prioritize eliminating toil\nAutomation is useful but isn't a solution to all operational problems. As a first step in addressing known toil, we recommend reviewing your inventory of existing toil and prioritize eliminating as much toil as you can. Then, you can focus on automation.\n## Automate necessary toil\nSome toil in your systems cannot be eliminated. As a second step in addressing known toil, automate this toil using the solutions that Google Cloud provides through configurable automation.\nThe following are some areas where configurable automation or customized automation can assist your organization in eliminating toil:\n- Identity management\u2014for example, [Cloud Identity and Identity and Access Management](/iam) .\n- Google Cloud hosted solutions, as opposed to self-designed solutions\u2014for example, cluster management ( [Google Kubernetes Engine](/kubernetes-engine) (GKE)), relational database management ( [Cloud SQL](/sql) ), data warehouse management ( [BigQuery](/bigquery) ), and API management ( [Apigee](/apigee/api-management) ).\n- Google Cloud services and tenant provisioning\u2014for example [Terraform](https://github.com/GoogleCloudPlatform/terraform-google-examples) and [Cloud Foundation Toolkit](/foundation-toolkit) .\n- Automated workflow orchestration for multi-step operations\u2014for example, [Cloud Composer](/composer) .\n- Additional capacity provisioning\u2014for example, several Google Cloud products, like [Compute Engine](/compute/docs/autoscaler) and [GKE](/kubernetes-engine/docs/concepts/cluster-autoscaler) , offer configurable autoscaling. Evaluate the Google Cloud services you are using to determine if they include configurable autoscaling.\n- CI/CD pipelines with automated deployment\u2014for example, [Cloud Build](/build/docs) .\n- Canary analysis to validate deployments.\n- Automated model training (for machine learning)\u2014for example, [AutoML](/vertex-ai/docs/beginner/beginners-guide) .\nIf a Google Cloud product or service only partially satisfies your technical needs when automating or eliminating manual workflows, consider filing a feature request through your Google Cloud account representative. Your issue might be a priority for other customers or already a part of our roadmap. If so, knowing the feature's priority and timeline helps you to better assess the trade-offs of building your own solution versus waiting to use a Google Cloud feature.\n## Build or buy solutions for high-cost toil\nThe third step, which can be completed in parallel with the first and second steps, entails evaluating building or buying other solutions if your toil cost stays high\u2014for example, if toil takes a significant amount of time for any team managing your production systems.\nWhen building or buying solutions, consider integration, security, privacy, and compliance costs. Designing and implementing your own automation comes with maintenance costs and risks to reliability beyond its initial development and setup costs, so consider this option as a last resort.\n## What's next\nExplore other categories in the [Architecture Framework](/architecture/framework) such as system design, security, privacy, and compliance, reliability, cost optimization, and performance optimization.# Google Cloud Architecture Framework: Security, privacy, and compliance\nThis category in the [Google Cloud Architecture Framework](/architecture/framework) shows you how to architect and operate secure services on Google Cloud. You also learn about Google Cloud products and features that support security and compliance.\nThe Architecture Framework describes best practices, provides implementation recommendations, and explains some of the available products and services. The framework helps you design your Google Cloud deployment so that it matches your business needs.\nMoving your workloads into Google Cloud requires an evaluation of your business requirements, risks, compliance obligations, and security controls. This document helps you consider key best practices related to designing a secure solution in Google Cloud.\nGoogle core principles include defense in depth, at scale, and by default. In Google Cloud, data and systems are protected through multiple layered defenses using policies and controls that are configured across IAM, encryption, networking, detection, logging, and monitoring.\nGoogle Cloud comes with many security controls that you can build on, such as the following:\n- Secure options for data in transit, and default encryption for data at rest.\n- Built-in security features for Google Cloud products and services.\n- A global infrastructure that's designed for geo-redundancy, with security controls throughout the [information processing lifecycle](/security/infrastructure/design) .\n- Automation capabilities that use infrastructure as code (IaC) and configuration guardrails.\nFor more information about the security posture of Google Cloud, see the [Google security paper](/security/overview/whitepaper) and the [Google Infrastructure Security Design Overview](/security/infrastructure/design) . For an example secure-by-default environment, see the [Google Cloud enterprise foundations blueprint](/architecture/security-foundations) .\nIn the security category of the Architecture Framework, you learn to do the following:\n- [Review shared responsibility and shared fate on Google Cloud](/architecture/framework/security/shared-responsibility-shared-fate) \n- [Understand security principles](/architecture/framework/security/security-principles) \n- [Manage risks with controls](/architecture/framework/security/risk-management) \n- [Manage your assets](/architecture/framework/security/asset-management) \n- [Manage identity and access](/architecture/framework/security/identity-access) \n- [Implement compute and container security](/architecture/framework/security/compute-container-security) \n- [Secure your network](/architecture/framework/security/network-security) \n- [Implement data security](/architecture/framework/security/data-security) \n- [Deploy applications security](/architecture/framework/security/app-security) \n- [Manage compliance obligations](/architecture/framework/security/compliance) \n- [Implement data residency and sovereignty requirements](/architecture/framework/security/data-residency-sovereignty) \n- [Implement privacy requirements](/architecture/framework/security/privacy) \n- [Implement logging and detective controls](/architecture/framework/security/logging-detection) # Shared responsibilities and shared fate on Google Cloud\nThis document describes the differences between the shared responsibility model and shared fate in Google Cloud. It discusses the challenges and nuances of the shared responsibility model. This document describes what shared fate is and how we partner with our customers to address cloud security challenges.\nUnderstanding the shared responsibility model is important when determining how to best protect your data and workloads on Google Cloud. The shared responsibility model describes the tasks that you have when it comes to security in the cloud and how these tasks are different for cloud providers.\nUnderstanding shared responsibility, however, can be challenging. The model requires an in-depth understanding of each service you utilize, the configuration options that each service provides, and what Google Cloud does to secure the service. Every service has a different configuration profile, and it can be difficult to determine the best security configuration. Google believes that the shared responsibility model stops short of helping cloud customers achieve better security outcomes. Instead of shared responsibility, we believe in .\nShared fate includes us building and operating a trusted cloud platform for your workloads. We provide best practice guidance and secured, attested infrastructure code that you can use to deploy your workloads in a secure way. We release solutions that combine various Google Cloud services to solve complex security problems and we offer innovative insurance options to help you measure and mitigate the risks that you must accept. Shared fate involves us more closely interacting with you as you secure your resources on Google Cloud.\n## Shared responsibility\nYou're the expert in knowing the security and regulatory requirements for your business, and knowing the requirements for protecting your confidential data and resources. When you run your workloads on Google Cloud, you must identify the security controls that you need to configure in Google Cloud to help protect your confidential data and each workload. To decide which security controls to implement, you must consider the following factors:\n- Your regulatory compliance obligations\n- Your organization's security standards and risk management plan\n- Security requirements of your customers and your vendors\n### Defined by workloads\nTraditionally, responsibilities are defined based on the type of workload that you're running and the cloud services that you require. Cloud services include the following categories:\n| Cloud service        | Description                                                                                                                                                   |\n|:-------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Infrastructure as a service (IaaS)   | IaaS services include Compute Engine, Cloud Storage, and networking services such as Cloud VPN, Cloud Load Balancing, and Cloud DNS. IaaS provides compute, storage, and network services on demand with pay-as-you-go pricing. You can use IaaS if you plan on migrating an existing on-premises workload to the cloud using lift-and-shift, or if you want to run your application on particular VMs, using specific databases or network configurations. In IaaS, the bulk of the security responsibilities are yours, and our responsibilities are focused on the underlying infrastructure and physical security. |\n| Platform as a service (PaaS)    | PaaS services include App Engine, Google Kubernetes Engine (GKE), and BigQuery. PaaS provides the runtime environment that you can develop and run your applications in. You can use PaaS if you're building an application (such as a website), and want to focus on development not on the underlying infrastructure. In PaaS, we're responsible for more controls than in IaaS, including network controls. You share responsibility with us for application-level controls and IAM management. You remain responsible for your data security and client protection.            |\n| Software as a service (SaaS)    | SaaS applications include Google Workspace, Chronicle, and third-party SaaS applications that are available in Google Cloud Marketplace. SaaS provides online applications that you can subscribe to or pay for in some way. You can use SaaS applications when your enterprise doesn't have the internal expertise or business requirement to build the application themselves, but does require the ability to process workloads. In SaaS, we own the bulk of the security responsibilities. You remain responsible for your access controls and the data that you choose to store in the application.    |\n| Function as a service (FaaS) or serverless | FaaS provides the platform for developers to run small, single-purpose code (called functions) that run in response to particular events. You would use FaaS when you want particular things to occur based on a particular event. For example, you might create a function that runs whenever data is uploaded to Cloud Storage so that it can be classified. FaaS has a similar shared responsibility list as SaaS. Cloud Functions is a FaaS application.                                       |\nThe following diagram shows the cloud services and defines how responsibilities are shared between the cloud provider and customer.\nAs the diagram shows, the cloud provider always remains responsible for the underlying network and infrastructure, and customers always remain responsible for their access policies and data.\n### Defined by industry and regulatory framework\nVarious industries have regulatory frameworks that define the security controls that must be in place. When you move your workloads to the cloud, you must understand the following:\n- Which security controls are your responsibility\n- Which security controls are available as part of the cloud offering\n- Which default security controls are inherited\nInherited security controls (such as our [default encryption](/docs/security/encryption/default-encryption) and [infrastructure controls](/docs/security/infrastructure/design) ) are controls that you can provide as part of your evidence of your security posture to auditors and regulators. For example, the Payment Card Industry Data Security Standard (PCI DSS) defines regulations for payment processors. When you move your business to the cloud, these regulations are shared between you and your CSP. To understand how PCI DSS responsibilities are shared between you and Google Cloud, see [Google Cloud: PCI DSS Shared Responsibility Matrix](https://services.google.com/fh/files/misc/gcp_pci_dss_v4_responsibility_matrix.pdf) .\nAs another example, in the United States, the Health Insurance Portability and Accountability Act (HIPAA) has set standards for handling electronic personal health information (PHI). These responsibilities are also shared between the CSP and you. For more information on how Google Cloud meets our responsibilities under HIPAA, see [HIPAA - Compliance](/security/compliance/hipaa-compliance) .\nOther industries (for example, finance or manufacturing) also have regulations that define how data can be gathered, processed, and stored. For more information about shared responsibility related to these, and how Google Cloud meets our responsibilities, see [Compliance resource center](/security/compliance) .\n### Defined by location\nDepending on your business scenario, you might need to consider your responsibilities based on the location of your business offices, your customers, and your data. Different countries and regions have created regulations that inform how you can process and store your customer's data. For example, if your business has customers who reside in the European Union, your business might need to abide by the requirements that are described in the [General Data Protection Regulation](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32016R0679) (GDPR), and you might be obligated to keep your customer data in the EU itself. In this circumstance, you are responsible for ensuring that the data that you collect remains in the [Google Cloud regions in the EU](/about/locations#europe) . For more information about how we meet our GDPR obligations, see [GDPR and Google Cloud](/privacy/gdpr) .\nFor information about the requirements related to your region, see [Compliance offerings](/security/compliance/offerings) . If your scenario is particularly complicated, we recommend speaking with our [sales team](/contact) or one of our [partners](/find-a-partner) to help you evaluate your security responsibilities.\n### Challenges for shared responsibility\nThough shared responsibility helps define the security roles that you or the cloud provider has, relying on shared responsibility can still create challenges. Consider the following scenarios:\n- Most cloud security breaches are the direct result of misconfiguration (listed as [number 3 in the Cloud Security Alliance's Pandemic 11 Report](https://cloudsecurityalliance.org/press-releases/2022/06/07/cloud-security-alliance-s-top-threats-to-cloud-computing-pandemic-11-report-finds-traditional-cloud-security-issues-becoming-less-concerning/) ) and this trend is expected to increase. Cloud products are constantly changing, and new ones are constantly being launched. Keeping up with constant change can seem overwhelming. Customers need cloud providers to provide them with opinionated best practices to help keep up with the change, starting with best practices by default and having a baseline secure configuration.\n- Though dividing items by cloud services is helpful, many enterprises have workloads that require multiple cloud services types. In this circumstance, you must consider how various security controls for these services interact, including whether they overlap between and across services. For example, you might have an on-premises application that you're migrating to Compute Engine, use Google Workspace for corporate email, and also run BigQuery to analyze data to improve your products.\n- Your business and markets are constantly changing; as regulations change, as you enter new markets, or as you acquire other companies. Your new markets might have different requirements, and your new acquisition might host their workloads on another cloud. To manage the constant changes, you must constantly re-assess your risk profile and be able to implement new controls quickly.\n- How and where to manage your data encryption keys is an important decision that ties with your responsibilities to protect your data. The option that you choose depends on your regulatory requirements, whether you're running a hybrid cloud environment or still have an on-premises environment, and the sensitivity of the data that you're processing and storing.\n- Incident management is an important, and often overlooked, area where your responsibilities and the cloud provider responsibilities aren't easily defined. Many incidents require close collaboration and support from the cloud provider to help investigate and mitigate them. Other incidents can result from poorly configured cloud resources or stolen credentials, and ensuring that you meet the best practices for securing your resources and accounts can be quite challenging.\n- Advanced persistent threats (APTs) and new vulnerabilities can impact your workloads in ways that you might not consider when you start your cloud transformation. Ensuring that you remain up-to-date on the changing landscape, and who is responsible for threat mitigation is difficult, particularly if your business doesn't have a large security team.## Shared fate\nWe developed shared fate in Google Cloud to start addressing the challenges that the shared responsibility model doesn't address. Shared fate focuses on how all parties can better interact to continuously improve security. Shared fate builds on the shared responsibility model because it views the relationship between cloud provider and customer as an ongoing partnership to improve security.\nShared fate is about us taking responsibility for making Google Cloud more secure. Shared fate includes helping you get started with a secured [landing zone](/architecture/landing-zones#what-is-a-google-cloud-landing-zone) and being clear, opinionated, and transparent about recommended security controls, settings, and associated best practices. It includes helping you better quantify and manage your risk with cyber-insurance, using our Risk Protection Program. Using shared fate, we want to evolve from the standard shared responsibility framework to a better model that helps you secure your business and build trust in Google Cloud.\nThe following sections describe various components of shared fate.\n### Help getting started\nA key component of shared fate is the resources that we provide to help you get started, in a secure configuration in Google Cloud. Starting with a secure configuration helps reduce the issue of misconfigurations which is the root cause of most security breaches.\nOur resources include the following:\n- [Enterprise foundations blueprint](/architecture/security-foundations) that discuss top security concerns and our top recommendations.\n- [Secure blueprints](/security/best-practices#section-2) that let you deploy and maintain secure solutions using (IaC). Blueprints have our security recommendations enabled by default. Many blueprints are created by Google security teams and managed as products. This support means that they're updated regularly, go through a rigorous testing process, and receive attestations from third-party testing groups. Blueprints include the [enterprise foundations blueprint](/architecture/security-foundations) , the [secured data warehouse blueprint](/architecture/confidential-data-warehouse-blueprint) , and the [ Vertex AI Workbench notebooks blueprint](/architecture/protecting-confidential-data-in-ai-platform-notebooks) .\n- Architecture Framework best practices that address the top recommendations for building security into your designs. The Architecture Framework includes a [security section](/architecture/framework/security) and a [community zone](https://www.googlecloudcommunity.com/gc/Architecture-Framework/ct-p/cloud-architecture-framework) that you can use to connect with experts and peers.\n- [Landing zone navigation guides](/architecture/landing-zones) that step you through the top decisions that you need to make to build a secure foundation for your workloads, including resource hierarchy, identity onboarding, security and key management, and network structure.\n### Risk Protection Program\nShared fate also includes the [Risk Protection Program](/risk-protection-program) (currently in preview), which helps you use the power of Google Cloud as a platform to manage risk, rather than just seeing cloud workloads as another source of risk that you need to manage. The Risk Protection Program is a collaboration between Google Cloud and two leading cyber insurance companies, Munich Re and Allianz Global & Corporate Speciality.\nThe Risk Protection Program includes [Risk Manager](/risk-manager/docs/overview) , which provides data-driven insights that you can use to better understand your cloud security posture. If you're looking for cyber insurance coverage, you can share these insights from Risk Manager directly with our insurance partners to obtain a quote. For more information, see [Google Cloud Risk Protection Program now in Preview](/blog/products/identity-security/google-cloud-risk-protection-program-now-in-preview) .\n### Help with deployment and governance\nShared fate also helps with your continued governance of your environment. For example, we focus efforts on products such as the following:\n- [Assured Workloads](/assured-workloads/docs/concept-overview) , which helps you meet your compliance obligations.\n- [Security Command Center](/security-command-center/docs/concepts-security-command-center-overview) Premium, which uses threat intelligence, threat detection, web scanning, and other advanced methods to monitor and detect threats. It also provides a way to resolve many of these threats quickly and automatically.\n- [Organization policies](/resource-manager/docs/organization-policy/overview) and [resource settings](/resource-manager/docs/cloud-platform-resource-hierarchy) that let you configure policies throughout your hierarchy of folders and projects.\n- [Policy Intelligence tools](/policy-intelligence/docs/overview) that provide you with insights on access to accounts and resources.\n- [Confidential Computing](/confidential-computing/confidential-vm/docs/about-cvm) , which allows you to encrypt data in use.\n- [Sovereign Cloud](/t-systems-sovereign-cloud) , which is available in Germany and implements data residency requirements.## Putting shared responsibility and shared fate into practice\nAs part of your planning process, consider the following actions to help you understand and implement appropriate security controls:\n- Create a list of the type of workloads that you will host in Google Cloud, and whether they require IaaS, PaaS, and SaaS services. You can use the [shared responsibility diagram](#shared-diagram) as a checklist to ensure that you know the security controls that you need to consider.\n- Create a list of regulatory requirements that you must comply with, and access resources in the [Compliance resource center](/security/compliance) that relate to those requirements.\n- Review the list of available blueprints and architectures in the [Architecture Center](/architecture) for the security controls that you require for your particular workloads. The blueprints provide a list of recommended controls and the IaC code that you require to deploy that architecture.\n- Use the [landing zone documentation](/architecture/landing-zones) and the recommendations in the [enterprise foundations guide](/architecture/security-foundations) to design a resource hierarchy and network architecture that meets your requirements. You can use the opinionated workload blueprints, like the secured data warehouse, to accelerate your development process.\n- After you deploy your workloads, verify that you're meeting your security responsibilities using services such as the Risk Manager, Assured Workloads, Policy Intelligence tools, and Security Command Center Premium.\nFor more information, see the [CISO's Guide to Cloud Transformation paper](https://services.google.com/fh/files/misc/ciso-guide-to-security-transformation.pdf) .\n## What's next\n- Review [security principles](/architecture/framework/security/security-principles) (next document in this series).\n- Keep up to date with [shared fate resources](/security/shared-fate) .\n- Familiarize yourself with available [blueprints](/security/best-practices#section-2) , including the security foundations blueprint and workload examples like the secured data warehouse.\n- Read more about [shared fate](/blog/products/identity-security/delivering-the-industrys-most-trusted-cloud) .\n- Read about our underlying secure infrastructure in the [Google infrastructure security design overview](/docs/security/infrastructure/design) .\n- Read how to implement [NIST Cybersecurity Framework best practices in Google Cloud (PDF)](https://services.google.com/fh/files/misc/gcp_nist_cybersecurity_framework.pdf) .# Security principles\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) explains core principles for running secure and compliant services on Google Cloud. Many of the security principles that you're familiar with in your on-premises environment apply to cloud environments.\n## Build a layered security approach\nImplement security at each level in your application and infrastructure by applying a defense-in-depth approach. Use the features in each product to limit access and configure encryption where appropriate.\n## Design for secured decoupled systems\nSimplify system design to accommodate flexibility where possible, and document security requirements for each component. Incorporate a robust secured mechanism to account for resiliency and recovery.\n## Automate deployment of sensitive tasks\nTake humans out of the workstream by [automating deployment](/architecture/migration-to-google-cloud-automated-containerized-deployments) and other admin tasks.\n## Automate security monitoring\nUse automated tools to monitor your application and infrastructure. To scan your infrastructure for vulnerabilities and detect security incidents, use automated scanning in your continuous integration and continuous deployment (CI/CD) pipelines.\n## Meet the compliance requirements for your regions\nBe mindful that you might need to obfuscate or redact personally identifiable information (PII) to meet your regulatory requirements. Where possible, automate your compliance efforts. For example, use Sensitive Data Protection and Dataflow to [automate the PII redaction job](/architecture/de-identification-re-identification-pii-using-cloud-dlp) before new data is stored in the system.\n## Comply with data residency and sovereignty requirements\nYou might have internal (or external) requirements that require you to control the locations of data storage and processing. These requirements vary based on systems design objectives, industry regulatory concerns, national law, tax implications, and culture. Data residency describes where your data is stored. To help comply with data residency requirements, Google Cloud lets you control where data is stored, how data is accessed, and how it's processed.\n## Shift security left\nDevOps and deployment automation let your organization increase the velocity of delivering products. To help ensure that your products remain secure, [incorporate security processes](https://dora.dev/devops-capabilities/technical/shifting-left-on-security/) from the start of the development process. For example, you can do the following:\n- Test for security issues in code early in the deployment pipeline.\n- [Scan container images](/blog/products/identity-security/scan-for-vulnerabilities-early-to-shift-security-left-in-cicd) and the cloud infrastructure on an ongoing basis.\n- Automate detection of misconfiguration and security anti-patterns. For example, use automation to look for secrets that are hard-coded in applications or in configuration.## What's next\nLearn more about core security principles with the following resources:\n- [Manage risks with controls](/architecture/framework/security/risk-management) (next document in this series)\n- [Google Cloud enterprise foundations blueprint](/architecture/security-foundations) \n- [Google security paper](/security/overview/whitepaper) \n- [Google Infrastructure Security Design Overview](/security/infrastructure/design) \n- [Build a collaborative incident management process](/architecture/framework/reliability/build-incident-management-process) \n- [Deployable blueprints for industries](/security/best-practices#:%7E:text=Deployable%20blueprints%20for%20industries) # Manage risk with controls\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) describes best practices for managing risks in a cloud deployment. Performing a careful analysis of the risks that apply to your organization allows you to determine the security controls that you require. You should complete risk analysis before you deploy workloads on Google Cloud, and regularly afterwards as your business needs, regulatory requirements, and the threats relevant to your organization change.\n## Identify risks to your organization\nBefore you create and deploy resources on Google Cloud, complete a risk assessment to determine what security features you need in order to meet your internal security requirements and external regulatory requirements. Your risk assessment provides you with a catalog of risks that are relevant to you, and tells you how capable your organization is in detecting and counteracting security threats.\nYour risks in a cloud environment differ from your risks in an on-premises environment due to the shared responsibility arrangement that you enter with your cloud provider. For example, in an on-premises environment you need to mitigate vulnerabilities to the hardware stack. In contrast, in a cloud environment these risks are borne by the cloud provider.\nIn addition, your risks differ depending on how you plan on using Google Cloud. Are you transferring some of your workloads to Google Cloud, or all of them? Are you using Google Cloud only for disaster recovery purposes? Are you setting up a hybrid cloud environment?\nWe recommend that you use an industry-standard risk assessment framework that applies to cloud environments and to your regulatory requirements. For example, the Cloud Security Alliance (CSA) provides the [Cloud Controls Matrix (CCM)](https://cloudsecurityalliance.org/research/cloud-controls-matrix/) . In addition, there are threat models such as [OWASP application threat modeling](https://owasp.org/www-community/Threat_Modeling_Process) that provide you with a list of potential gaps, and that suggest actions to remediate any gaps that are found. You can check our [partner directory](https://cloud.google.com/find-a-partner) for a list of experts in conducting risk assessments for Google Cloud.\nTo help catalog your risks, consider [Risk Manager](/risk-manager/docs/overview) , which is part of the [Risk Protection Program](/risk-protection-program) . (This program is currently in preview.) Risk Manager scans your workloads to help you understand your business risks. Its detailed reports provide you with a security baseline. In addition, you can use Risk Manager reports to compare your risks against the risks outlined in the [Center for Internet Security (CIS) Benchmark](https://www.cisecurity.org/cis-benchmarks/) .\nAfter you catalog your risks, you must determine how to address them\u2014that is, whether you want to accept, avoid, transfer, or mitigate them. The following section describes mitigation controls.\n## Mitigate your risks\nYou can mitigate risks using technical controls, contractual protections, and third-party verifications or attestations. The following table lists how you can use these mitigations when you adopt new public cloud services.\n| Mitigation        | Description                                                                                                                                                                                                          |\n|:------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Technical controls      | Technical controls refer to the features and technologies that you use to protect your environment. These include built-in cloud security controls, such as firewalls and logging. Technical controls can also include using third-party tools to reinforce or support your security strategy. There are two categories of technical controls: Google Cloud includes various security controls to let you mitigate the risks that apply to you. For example, if you have an on-premises environment, you can use Cloud VPN and Cloud Interconnect to secure the connection between your on-premises and your cloud resources. Google has robust internal controls and auditing to protect against insider access to customer data. Our audit logs provide our customers with near real-time logs of Google administrator access on Google Cloud. |\n| Contractual protections     | Contractual protections refer to the legal commitments made by us regarding Google Cloud services. Google is committed to maintaining and expanding our compliance portfolio. The Cloud Data Processing Addendum (CDPA) document defines our commitment to maintaining our ISO 27001, 27017, and 27018 certifications and to updating our SOC 2 and SOC 3 reports every 12 months. The DPST document also outlines the access controls that are in place to limit access by Google support engineers to customers' environments, and it describes our rigorous logging and approval process. We recommend that you review Google Cloud's contractual controls with your legal and regulatory experts and verify that they meet your requirements. If you need more information, contact your technical account representative.     |\n| Third-party verifications or attestations | Third-party verifications or attestations refers to having a third-party vendor audit the cloud provider to ensure that the provider meets compliance requirements. For example, Google was audited by a third party for ISO 27017 compliance. You can see the current Google Cloud certifications and letters of attestation at the Compliance Resource Center.                                                                                                                     |\n## What's next\nLearn more about risk management with the following resources:\n- [Manage your assets](/architecture/framework/security/asset-management) (next document in this series)\n- [Risk Protection Program](/risk-protection-program) \n- [Compliance resource center](/security/compliance) \n- [Risk governance of digital transformation in the cloud (PDF)](https://services.google.com/fh/files/misc/risk-governance-of-digital-transformation.pdf) # Manage your assets\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices for managing assets.\nAsset management is an important part of your business requirements analysis. You must know what assets you have, and you must have a good understanding of all your assets, their value, and any critical paths or processes related to them. You must have an accurate asset inventory before you can design any sort of security controls to protect your assets.\nTo manage security incidents and meet your organization's regulatory requirements, you need an accurate and up-to-date asset inventory that includes a way to analyze historical data. You must be able to track your assets, including how their risk exposure might change over time.\nMoving to Google Cloud means that you need to modify your asset management processes to adapt to a cloud environment. For example, one of the benefits of moving to the cloud is that you increase your organization's ability to scale quickly. However, the ability to scale quickly can cause shadow IT issues, in which your employees create cloud resources that aren't properly managed and secured. Therefore, your asset management processes must provide sufficient flexibility for employees to get their work done while also providing for appropriate security controls.\n## Use cloud asset management tools\nGoogle Cloud asset management tools are tailored specifically to our environment and to top customer use cases.\nOne of these tools is [Cloud Asset Inventory](/asset-inventory/docs/overview) , which provides you with both real-time information on the current state of your resources and with a five-week history. By using this service, you can get an organization-wide snapshot of your inventory for a wide variety of Google Cloud resources and policies. Automation tools can then use the snapshot for monitoring or for policy enforcement, or the tools can archive the snapshot for compliance auditing. If you want to analyze changes to the assets, asset inventory also lets you export metadata history.\nFor more information about Cloud Asset Inventory, see [Custom solution to respond to asset changes](/architecture/security-foundations/detective-controls#custom-solution-asset) and [Detective controls](/architecture/security-foundations/detective-controls) .\n## Automate asset management\nAutomation lets you quickly create and manage assets based on the security requirements that you specify. You can automate aspects of the asset lifecycle in the following ways:\n- Deploy your cloud infrastructure using automation tools such as Terraform. Google Cloud provides the [enterprise foundations blueprint](/architecture/security-foundations) , which helps you set up infrastructure resources that meet security best practices. In addition, it configures asset changes and policy compliance notifications in Cloud Asset Inventory.\n- Deploy your applications using automation tools such as [Cloud Run](/run/docs) and the [Artifact Registry](/artifact-registry/docs/overview) .## Monitor for deviations from your compliance policies\nDeviations from policies can occur during all phases of the asset lifecycle. For example, assets might be created without the proper security controls, or their privileges might be escalated. Similarly, assets might be abandoned without the appropriate end-of-life procedures being followed.\nTo help avoid these scenarios, we recommend that you monitor assets for deviation from compliance. Which set of assets that you monitor depends on the results of your risk assessment and business requirements. For more information about monitoring assets, see [Monitoring asset changes](/asset-inventory/docs/monitoring-asset-changes) .\n## Integrate with your existing asset management monitoring systems\nIf you already use a [SIEM](https://wikipedia.org/wiki/Security_information_and_event_management) system or other monitoring system, integrate your Google Cloud assets with that system. Integration ensures that your organization has a single, comprehensive view into all resources, regardless of environment. For more information, see [Export Google Cloud security data to your SIEM system](/community/tutorials/exporting-security-data-to-your-siem) and [Scenarios for exporting Cloud Logging data: Splunk](/architecture/exporting-stackdriver-logging-for-splunk) .\n## Use data analysis to enrich your monitoring\nYou can export your inventory to a [BigQuery table](/asset-inventory/docs/analyzing-iam-policy-longrunning-bigquery) or [Cloud Storage bucket](/asset-inventory/docs/analyzing-iam-policy-longrunning-cloud-storage) for additional analysis.\n## What's next\nLearn more about managing your assets with the following resources:\n- [Manage identity and access](/architecture/framework/security/identity-access) (next document in this series)\n- [Resource management](/architecture/framework/system-design/resource-management) \n- [System design](/architecture/framework/system-design) # Manage identity and access\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices for managing identity and access.\nThe practice of identity and access management (generally referred to as ) helps you ensure that the right people can access the right resources. IAM addresses the following aspects of authentication and authorization:\n- Account management, including provisioning\n- Identity governance\n- Authentication\n- Access control (authorization)\n- Identity federation\nManaging IAM can be challenging when you have different environments or you use multiple identity providers. However, it's critical that you set up a system that can meet your business requirements while mitigating risks.\nThe recommendations in this document help you review your current IAM policies and procedures and determine which of those you might need to modify for your workloads in Google Cloud. For example, you must review the following:\n- Whether you can use existing groups to manage access or whether you need to create new ones.\n- Your authentication requirements (such as [multi-factor authentication (MFA)](https://wikipedia.org/wiki/Multi-factor_authentication) using a token).\n- The impact of service accounts on your current policies.\n- If you're using Google Cloud for disaster recovery, maintaining appropriate separation of duties.\nWithin Google Cloud, you use [Cloud Identity](/identity/docs/overview) to authenticate your users and resources and Google's [Identity and Access Management (IAM)](/iam/docs) product to dictate resource access. Administrators can restrict access at the organization, folder, project, and resource level. Google IAM policies dictate who can do what on which resources. Correctly configured IAM policies help secure your environment by preventing unauthorized access to resources.\nFor more information, see [Overview of identity and access management](/architecture/identity) .\n## Use a single identity provider\nMany of our customers have user accounts that are managed and provisioned by identity providers outside of Google Cloud. Google Cloud supports [federation](https://wikipedia.org/wiki/Federated_identity) with most identity providers and with on-premises directories such as [Active Directory](/architecture/identity/federating-gcp-with-active-directory-introduction) .\nMost identity providers let you enable single sign-on (SSO) for your users and groups. For applications that you deploy on Google Cloud and that use your external identity provider, you can extend your identity provider to Google Cloud. For more information, see [Reference architectures](/architecture/identity/reference-architectures) and [Patterns for authentication corporate users in a hybrid environment](/architecture/patterns-for-authenticating-corporate-users-in-a-hybrid-environment) .\nIf you don't have an existing identity provider, you can use either [Cloud Identity Premium](/identity) or [Google Workspace](https://workspace.google.com/) to manage identities for your employees.\n## Protect the super admin account\nThe super admin account (managed by Google Workspace or Cloud Identity) lets you create your [Google Cloud organization](/resource-manager/docs/creating-managing-organization) . This admin account is therefore highly privileged. Best practices for this account include the following:\n- Create a new account for this purpose; don't use an existing user account.\n- Create and protect backup accounts.\n- Enable MFA.\nFor more information, see [Super administrator account best practices](/resource-manager/docs/super-admin-best-practices) .\n## Plan your use of service accounts\nA [service account](/iam/docs/understanding-service-accounts) is a Google account that applications can use to [call the Google API of a service](https://developers.google.com/identity/protocols/OAuth2ServiceAccount#authorizingrequests) .\nUnlike your user accounts, service accounts are created and managed within Google Cloud. Service accounts also authenticate differently than user accounts:\n- To let an application running on Google Cloud authenticate using a service account, you can [attach a service account](/iam/docs/attach-service-accounts#attaching-to-resources) to the compute resource the application runs on.\n- To let an application running on GKE authenticate using a service account, you can use [Workload Identity](/kubernetes-engine/docs/how-to/workload-identity) .\n- To let applications running outside of Google Cloud authenticate using a service account, you can use [Workload identity federation](/iam/docs/workload-identity-federation) \nWhen you use service accounts, you must consider an appropriate segregation of duties during your design process. Note the API calls that you must make, and determine the service accounts and associated roles that the API calls require. For example, if you're setting up a BigQuery data warehouse, you probably need identities for at least the following processes and services:\n- Cloud Storage or Pub/Sub, depending on whether you're providing a batch file or creating a streaming service.\n- [Dataflow](/dataflow/docs/concepts/security-and-permissions#security_and_permissions_for_pipelines_on) and Sensitive Data Protection to de-identify sensitive data.\nFor more information, see [Best practices for working with service accounts](/iam/docs/best-practices-service-accounts) .\n## Update your identity processes for the cloud\nIdentity governance lets you track access, risks, and policy violations so that you can support your regulatory requirements. This governance requires that you have processes and policies in place so that you can grant and audit access control roles and permissions to users. Your processes and policies must reflect the requirements of your environments\u2014for example, test, development, and production.\nBefore you deploy workloads on Google Cloud, review your current identity processes and update them if appropriate. Ensure that you appropriately [plan](/architecture/identity/best-practices-for-planning) for the types of accounts that your organization needs and that you have a good understanding of their role and access requirements.\nTo help you audit Google IAM activities, Google Cloud creates [audit logs](/iam/docs/audit-logging) , which include the following:\n- Administrator activity. This logging can't be disabled.\n- Data access activity. You must [enable](/logging/docs/audit/configure-data-access#config-console-enable) this logging.\nIf necessary for compliance purposes, or if you want to set up log analysis (for example, with your SIEM system), you can [export the logs](/logging/docs/export) . Because logs can increase your storage requirements, they might affect your costs. Ensure that you log only the actions that you require, and set appropriate retention schedules.\n## Set up SSO and MFA\nYour identity provider manages user account authentication. Federated identities can authenticate to Google Cloud using SSO. For privileged accounts, such as super admins, you should configure MFA. [Titan Security Keys](/titan-security-key) are physical tokens that you can use for two-factor authentication (2FA) to help prevent phishing attacks.\n[Cloud Identity](/identity) supports MFA using various methods. For more information, see [Enforce uniform MFA to company-owned resources](/identity/solutions/enforce-mfa) .\nGoogle Cloud supports authentication for workload identities using the OAuth 2.0 protocol or [signed JSON Web Tokens (JWT)](https://developers.google.com/identity/protocols/oauth2/service-account#jwt-auth) . For more information about workload authentication, see [Authentication overview](/docs/authentication) .\n## Implement least privilege and separation of duties\nYou must ensure that the right individuals get access only to the resources and services that they need in order to perform their jobs. That is, you should follow the [principle of least privilege](https://wikipedia.org/wiki/Principle_of_least_privilege) . In addition, you must ensure there is an appropriate [separation of duties](/binary-authorization/docs/reference/organizational-and-iam-roles) .\nOverprovisioning user access can increase the risk of insider threat, misconfigured resources, and non-compliance with audits. Underprovisioning permissions can prevent users from being able to access the resources they need in order to complete their tasks.\nOne way to avoid overprovisioning is to implement \u2014 that is, to [provide privileged access only as needed, and to only grant ittemporarily](/architecture/manage-just-in-time-privileged-access-to-project) .\nBe aware that when a Google Cloud organization is created, all users in your domain are granted the Billing Account Creator and Project Creator roles by default. Identify the users who will perform these duties, and revoke these roles from other users. For more information, see [Creating and managing organizations](/resource-manager/docs/creating-managing-organization) .\nFor more information about how roles and permissions work in Google Cloud, see [Overview](/iam/docs/overview) and [Understanding roles](/iam/docs/understanding-roles) in the IAM documentation. For more information about enforcing least privilege, see [Enforce least privilege with role recommendations](/iam/docs/recommender-overview) .\n## Audit access\nTo monitor the activities of privileged accounts for deviations from approved conditions, use [Cloud Audit Logs](/audit-logs) . Cloud Audit Logs records the actions that members in your Google Cloud organization have taken in your Google Cloud resources. You can work with various [audit log types](/logging/docs/audit/understanding-audit-logs) across [Google services](/logging/docs/audit/services) . For more information, see [Using Cloud Audit Logs to Help Manage Insider Risk (video)](https://www.youtube.com/watch?v=dqoZEfJ7UbM) .\nUse [IAM recommender](/recommender/docs) to track usage and to adjust permissions where appropriate. The roles that are recommended by IAM recommender can help you determine which roles to grant to a user based on the user's past behavior and on other criteria. For more information, see [Best practices for role recommendations](/iam/docs/recommender-best-practices) .\nTo audit and control access to your resources by Google support and engineering personnel, you can use [Access Transparency](/access-transparency) . Access Transparency records the actions taken by Google personnel. Use [Access Approval](/assured-workloads/access-approval/docs/overview) , which is part of Access Transparency, to grant explicit approval every time customer content is accessed. For more information, see [Control cloud administrators' access to your data](/architecture/framework/security/data-security#control_cloud_administrators_access_to_your_data) .\n## Automate your policy controls\nSet access permissions programmatically whenever possible. For best practices, see [Organization policy constraints](/architecture/security-foundations/preventative-controls#organization-policy) . The Terraform scripts for the enterprise foundations blueprint are in the [example foundation repository](https://github.com/terraform-google-modules/terraform-example-foundation/tree/master/1-org) .\nGoogle Cloud includes [Policy Intelligence](/policy-intelligence) , which lets you automatically review and update your access permissions. Policy Intelligence includes the [Recommender](/iam/docs/recommender-overview) , [Policy Troubleshooter](/iam/docs/troubleshooting-access) , and [Policy Analyzer](/asset-inventory/docs/analyzing-iam-policy) tools, which do the following:\n- Provide recommendations for IAM role assignment.\n- Monitor and help prevent overly permissive IAM policies.\n- Assist with troubleshooting access-control-related issues.## Set restrictions on resources\nGoogle [IAM](/iam) focuses on , and it lets you [authorize](/resource-manager/docs/access-control-org) who can act on specific resources based on permissions. The [Organization Policy Service](/resource-manager/docs/organization-policy/overview) focuses on , and it lets you set restrictions on resources to specify how they can be configured. For example, you can use an organization policy to do the following:\n- [Limit resource sharing](/resource-manager/docs/organization-policy/restricting-domains) based on domain.\n- [Limit the use of service accounts](/resource-manager/docs/organization-policy/restricting-service-accounts) .\n- [Restrict the physical location](/resource-manager/docs/organization-policy/defining-locations) of newly created resources.\nIn addition to using organizational policies for these tasks, you can restrict access to resources using one of the following methods:\n- [Use tags](/iam/docs/tags-access-control) to manage access to your resources without defining the access permissions on each resource. Instead, you add the tag and then set the access definition for the tag itself.\n- [Use IAM Conditions](/iam/docs/conditions-overview) for conditional, attribute-based control of access to resources.\n- Implement defense-in-depth using [VPC Service Controls](/vpc-service-controls/docs) to further restrict access to resources.\nFor more about resource management, see [Resource management](/architecture/framework/system-design/resource-management) .\n## What's next\nLearn more about IAM with the following resources:\n- [Implement compute and container security](/architecture/framework/security/compute-container-security) (next document in this series)\n- [Using IAM securely](/iam/docs/using-iam-securely) \n- [Reference architectures](/architecture/identity/reference-architectures#using_google_as_an_idp) \n- [IAM Recommender: least privilege with less effort](/blog/products/identity-security/achieve-least-privilege-with-less-effort-using-iam-recommender) \n- [Cloud Data Privacy](/security/privacy) \n- [Overview of identity and access management](/architecture/identity) # Implement compute and container security\nGoogle Cloud includes controls to protect your compute resources and Google Kubernetes Engine (GKE) container resources. This document in the [Google Cloud Architecture Framework](/architecture/framework) describes key controls and best practices for using them.\n## Use hardened and curated VM images\nGoogle Cloud includes [Shielded VM](/shielded-vm) , which allows you to harden your VM instances. Shielded VM is designed to prevent malicious code from being loaded during the boot cycle. It provides boot security, monitors integrity, and uses the [Virtual Trusted Platform Module (vTPM)](/security/shielded-cloud/shielded-vm#vtpm) . Use Shielded VM for sensitive workloads.\nIn addition to using Shielded VM, you can use Google Cloud partner solutions to further protect your VMs. Many partner solutions offered on Google Cloud integrate with [Security Command Center](/security-command-center/docs) , which provides event threat detection and health monitoring. You can use partners for advanced threat analysis or extra runtime security.\n## Use Confidential Computing for processing sensitive data\nBy default, Google Cloud encrypts data at rest and in transit across the network, but data isn't encrypted while it's in use in memory. If your organization handles confidential data, you need to mitigate against threats that undermine the confidentiality and integrity of either the application or the data in system memory. Confidential data includes personally identifiable information (PII), financial data, and health information.\n[Confidential Computing](/confidential-computing) builds on Shielded VM. It protects data in use by performing computation in a hardware-based trusted execution environment. This type of secure and isolated environment helps prevent unauthorized access or modification of applications and data while that data is in use. A trusted execution environment also increases the security assurances for organizations that manage sensitive and regulated data.\nIn Google Cloud, you can enable Confidential Computing by running [Confidential VMs](/confidential-computing/confidential-vm/docs/about-cvm) or [Confidential GKE nodes](/kubernetes-engine/docs/how-to/confidential-gke-nodes) . Turn on Confidential Computing when you're processing confidential workloads, or when you have confidential data (for example, secrets) that must be exposed while they are processed. For more information, see the [Confidential Computing Consortium.](https://confidentialcomputing.io/)\n## Protect VMs and containers\n[OS Login](/compute/docs/oslogin) lets your employees connect to your VMs using [Identity and Access Management (IAM)](/iam/docs) permissions as the source of truth instead of relying on SSH keys. You therefore don't have to manage SSH keys throughout your organization. OS Login ties an administrator's access to their employee lifecycle, which means that if employees move to another role or leave your organization, their access is revoked with their account. [OS Login also supports two-factor authentication](/compute/docs/oslogin/setup-two-factor-authentication) , which adds an extra layer of security from account takeover attacks.\nIn GKE, [App Engine](/appengine/docs) runs application instances within Docker containers. To enable a defined risk profile and to restrict employees from making changes to containers, ensure that your containers are [stateless and immutable](/architecture/best-practices-for-operating-containers#ensure_that_your_containers_are_stateless_and_immutable) . The [principle of immutability](/architecture/best-practices-for-operating-containers#immutability) means that your employees do not modify the container or access it interactively. If it must be changed, you build a new image and redeploy. Enable SSH access to the underlying containers only in specific debugging scenarios.\n## Disable external IP addresses unless they're necessary\nTo [disable external IP address allocation (video)](https://www.youtube.com/watch?v=1BQFk1Bi9YQ) for your production VMs and to prevent the use of external load balancers, you can use organization policies. If you require your VMs to reach the internet or your on-premises data center, you can enable a [Cloud NAT gateway](/nat/docs/overview) .\nYou can deploy [private clusters](/kubernetes-engine/docs/how-to/private-clusters) in GKE. In a private cluster, nodes have only internal IP addresses, which means that nodes and Pods are isolated from the internet by default. You can also define a network policy to manage Pod-to-Pod communication in the cluster. For more information, see [Private access options for services](/vpc/docs/private-access-options) .\n## Monitor your compute instance and GKE usage\n[Cloud Audit Logs](/logging/docs/audit) are automatically enabled for [Compute Engine](/compute/docs/logging/audit-logging) and [GKE](/kubernetes-engine/docs/how-to/audit-logging) . Audit logs let you automatically capture all activities with your cluster and monitor for any suspicious activity.\nYou can integrate GKE with partner products for runtime security. You can integrate these solutions with the [Security Command Center](/security-command-center/docs) to provide you with a single interface for monitoring your applications.\n## Keep your images and clusters up to date\nGoogle Cloud provides curated OS images that are patched regularly. You can bring custom images and run them on Compute Engine, but if you do, you have to patch them yourself. Google Cloud regularly updates OS images to mitigate new vulnerabilities as described in [security bulletins](/compute/docs/security-bulletins) and provides remediation to fix vulnerabilities for existing deployments.\nIf you're using GKE, we recommend that you [enable node auto-upgrade](/kubernetes-engine/docs/how-to/node-auto-upgrades) to have Google update your cluster nodes with the latest patches. Google manages GKE control planes, which are automatically updated and patched. In addition, use Google-curated container-optimized images for your deployment. Google regularly patches and updates these images.\n## Control access to your images and clusters\nIt's important to know who can create and launch instances. You can [control this access](/compute/docs/access) using IAM. For information about how to determine what access workloads need, see [Plan your workload identities](/architecture/framework/security/identity-access#plan-workload-ids) .\nIn addition, you can use [VPC Service Controls](/vpc-service-controls/docs) to define custom quotas on projects so that you can limit who can launch images. For more information, see the [Secure your network](/architecture/framework/security/network-security) section.\nTo provide infrastructure security for your cluster, GKE lets you use [IAM with role-based access control (RBAC)](/kubernetes-engine/docs/how-to/role-based-access-control) to manage access to your cluster and namespaces.\n## Isolate containers in a sandbox\nUse [GKE Sandbox](/kubernetes-engine/docs/concepts/sandbox-pods) to deploy multi-tenant applications that need an extra layer of security and isolation from their host kernel. For example, use GKE Sandbox when you are executing unknown or untrusted code. GKE Sandbox is a container isolation solution that provides a second layer of defense between containerized workloads on GKE.\nGKE Sandbox was built for applications that have low I/O requirements but that are highly scaled. These containerized workloads need to maintain their speed and performance, but might also involve untrusted code that demands added security. Use [gVisor](https://gvisor.dev/docs/) , a container runtime sandbox, to provide additional security isolation between applications and the host kernel. gVisor provides additional integrity checks and limits the scope of access for a service. It's not a container hardening service to protect against external threats. For more inforamtion about gVisor, see [gVisor: Protecting GKE and serverless users in the real world](/blog/products/containers-kubernetes/how-gvisor-protects-google-cloud-services-from-cve-2020-14386) .\n## What's next\nLearn more about compute and container security with the following resources:\n- [Secure your network](/architecture/framework/security/network-security) (next document in this series)\n- [Why container security matters (PDF)](https://services.google.com/fh/files/misc/why_container_security_matters_to_your_business.pdf) \n- [Launch checklist for Google Cloud](/compute/docs/launch-checklist) \n- [Verifying the identity of instances](/compute/docs/instances/verifying-instance-identity) \n- [Workload Identity](/kubernetes-engine/docs/how-to/workload-identity) \n- [Shielded VM](/security/shielded-cloud/shielded-vm) \n- [Best practices for persistent disk snapshots](/compute/docs/disks/snapshot-best-practices) \n- [Image management best practices](/solutions/image-management-best-practices) # Secure your network\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices for securing your network.\nExtending your existing network to include cloud environments has many implications for security. Your on-premises approach to multi-layered defenses likely involves a distinct perimeter between the internet and your internal network. You probably protect the perimeter by using physical firewalls, routers, intrusion detection systems, and so on. Because the boundary is clearly defined, you can easily monitor for intrusions and respond accordingly.\nWhen you move to the cloud (either completely or in a hybrid approach), you move beyond your on-premises perimeter. This document describes ways that you can continue to secure your organization's data and workloads on Google Cloud. As mentioned in [Manage risks with controls](/architecture/framework/security/risk-management#manageriskscontrols) , how you set up and secure your Google Cloud network depends on your business requirements and risk appetite.\nThis section assumes that you have read the [Networking](/architecture/framework/system-design/networking) section in the [System design](/architecture/framework/system-design) category, and that you've already created a basic architecture diagram of your Google Cloud network components. For an example diagram, see [Hub-and-spoke](/architecture/security-foundations/networking#hub-spoke-network-topology) .\n## Deploy zero trust networks\nMoving to the cloud means that your network trust model must change. Because your users and your workloads are no longer behind your on-premises perimeter, you can't use perimeter protections in the same way to create a trusted, inner network. The means that no one is trusted by default, whether they are inside or outside of your organization's network. When verifying access requests, the zero trust security model requires you to check both the user's identity and context. Unlike a VPN, you shift access controls from the network perimeter to the users and devices.\nIn Google Cloud, you can use [BeyondCorp Enterprise](/beyondcorp) as your zero trust solution. BeyondCorp Enterprise provides [threat and data protection](https://support.google.com/a/answer/10104463) and additional [access controls](/beyondcorp-enterprise/docs/access-protection) . For more information about how to set it up, see [Getting started with BeyondCorp Enterprise](/beyondcorp-enterprise/docs/apply-resources) .\nIn addition to BeyondCorp Enterprise, Google Cloud includes [Identity-Aware Proxy (IAP)](/iap) . IAP lets you extend zero trust security to your applications both within Google Cloud and on-premises. IAP uses access control policies to provide authentication and authorization for users who access your applications and resources.\n## Secure connections to your on-premises or multi-cloud environments\nMany organizations have workloads both in cloud environments and on-premises. In addition, for resiliency, some organizations use multi-cloud solutions. In these scenarios, it's critical to secure your connectivity between all of your environments.\nGoogle Cloud includes [private access methods for VMs](/vpc/docs/private-access-options) that are supported by [Cloud VPN](/network-connectivity/docs/vpn/concepts/overview) or [Cloud Interconnect](/network-connectivity/docs/interconnect/concepts/overview) , including the following:\n- Use [Cross-Cloud Interconnect](/network-connectivity/docs/interconnect/concepts/cci-overview) , as a managed service, to link your VPC networks to other [supported cloudproviders](/network-connectivity/docs/interconnect/concepts/cci-overview#supported-cloud-service-providers) over high-speed direct connections. With Cross-Cloud Interconnect, you don't have to supply your own router or work with a third-party vendor.\n- Use [Dedicated Interconnect and Partner Interconnect](/network-connectivity/docs/interconnect/concepts/overview) to link your VPC networks to your on-premises data center or to other cloud providers over high-speed direct connections.\n- Use IPsec VPNs to link your [Virtual Private Cloud (VPC) networks](/vpc/docs) to your on-premises data center or to other cloud providers.\n- Use [Private Service Connect endpoints to access published services](/vpc/docs/about-accessing-vpc-hosted-services-endpoints) that are provided by your organization or by another provider.\n- Use [Private Service Connect endpoints to let your VMs access GoogleAPIs](/vpc/docs/about-accessing-google-apis-endpoints) by using internal IP addresses. With Private Service Connect, your VMs don't have to have external IP addresses in order to access Google services.\n- If you use [GKE Enterprise](/anthos) , consider [Anthos Service Mesh egress gateways](/service-mesh/docs/security/egress-gateway-gke-tutorial) . If you're not using [GKE Enterprise](/anthos) , use a third-party option.\nFor a comparison between the products, see [Choosing a Network Connectivity product](/network-connectivity/docs/how-to/choose-product#compare-interconnect-solutions) .\n## Disable default networks\nWhen you create a new Google Cloud project, a default Google Cloud [VPC](/vpc/docs) network with [auto mode](/vpc/docs/subnets#ip-ranges) IP addresses and [pre-populated firewall rules](/vpc/docs/firewalls#more_rules_default_vpc) is automatically provisioned. For production deployments, we recommend that you [delete the default networks](/vpc/docs/create-modify-vpc-networks#deleting_a_network) in existing projects, and [disable the creation of default networks](/vpc/docs/vpc#default-network) in new projects.\nVirtual Private Cloud networks let you use any internal IP address. To avoid IP address conflicts, we recommend that you first plan your network and IP address allocation across your connected deployments and across your projects. A project allows multiple VPC networks, but it's usually a best practice to limit these networks to one per project in order to enforce access control effectively.\n## Secure your perimeter\nIn Google Cloud, you can use various methods to segment and secure your cloud perimeter, including firewalls and [VPC Service Controls](/vpc-service-controls/docs) .\nUse [Shared VPC](/vpc/docs/shared-vpc) to build a production deployment that gives you a single shared network and that isolates workloads into individual projects that can be managed by different teams. Shared VPC provides centralized deployment, management, and control of the network and network security resources across multiple projects. Shared VPC consists of host and service projects that perform the following functions:\n- A host project contains the networking and network security-related resources, such as VPC networks, subnets, firewall rules, and hybrid connectivity.\n- A service project attaches to a host project. It lets you isolate workloads and users at the project level by using Identity and Access Management (IAM), while it shares the networking resources from the centrally managed host project.\nDefine [firewall policies and rules](/firewall/docs/about-firewalls) at the organization, folder, and VPC network level. You can configure firewall rules to permit or deny traffic to or from VM instances. For examples, see [Global and regional network firewall policy examples](/firewall/docs/network-firewall-policy-examples) and [Hierarchical firewall policy examples](/firewall/docs/firewall-policies-examples) . In addition to defining rules based on IP addresses, protocols, and ports, you can manage traffic and apply firewall rules based on the [service account](/vpc/docs/firewalls#service-accounts-vs-tags) that's used by a VM instance or by using [secure tags](/firewall/docs/tags-firewalls-overview) .\nTo control the movement of data in Google services and to set up context-based perimeter security, consider VPC Service Controls. VPC Service Controls provides an extra layer of security for Google Cloud services that's independent of IAM and firewall rules and policies. For example, VPC Service Controls lets you set up perimeters between confidential and non-confidential data so that you can apply controls that help prevent data exfiltration.\nUse [Google Cloud Armor security policies](/armor/docs/security-policy-overview) to allow, deny, or redirect requests to your external Application Load Balancer at the Google Cloud edge, as close as possible to the source of incoming traffic. These policies prevent unwelcome traffic from consuming resources or entering your network.\nUse [Secure Web Proxy](/secure-web-proxy/docs/overview) to apply granular access policies to your egress web traffic and to monitor access to untrusted web services.\n## Inspect your network traffic\nYou can use Cloud IDS and Packet Mirroring to help you ensure the security and compliance of workloads running in [Compute Engine](/compute) and [Google Kubernetes Engine (GKE)](/kubernetes-engine) .\nUse [Cloud Intrusion Detection System](/ids) (currently in preview) to get visibility in to the traffic moving into and out of your VPC networks. Cloud IDS creates a Google-managed peered network that has mirrored VMs. [Palo Alto Networks threat protection technologies](https://www.paloaltonetworks.com/products/secure-the-network/subscriptions/threat-prevention) mirror and inspect the traffic. For more information, see [Cloud IDS overview](/intrusion-detection-system/docs/overview) .\n[Packet Mirroring](/vpc/docs/packet-mirroring) clones traffic of specified VM instances in your VPC network and forwards it for collection, retention, and examination. After you configure Packet Mirroring, you can use Cloud IDS or third-party tools to collect and inspect network traffic at scale. Inspecting network traffic in this way helps provide intrusion detection and application performance monitoring.\n## Use a web application firewall\nFor external web applications and services, you can enable [Google Cloud Armor](/armor/docs/cloud-armor-overview) to provide distributed denial-of-service (DDoS) protection and web application firewall (WAF) capabilities. Google Cloud Armor supports Google Cloud workloads that are exposed using external HTTP(S) load balancing, TCP Proxy load balancing, or SSL Proxy load balancing.\nGoogle Cloud Armor is offered in two service tiers, Standard and [Managed Protection Plus](/armor/docs/managed-protection-overview) . To take full advantage of advanced Google Cloud Armor capabilities, you should invest in Managed Protection Plus for your key workloads.\n## Automate infrastructure provisioning\nAutomation lets you create immutable infrastructure, which means that it can't be changed after provisioning. This measure gives your operations team a known good state, fast rollback, and troubleshooting capabilities. For automation, you can use tools such as Terraform, Jenkins, and [Cloud Build](/build/docs/overview) .\nTo help you build an environment that uses automation, Google Cloud provides a series of [security blueprints](/security/best-practices) that are in turn built on the [enterprise foundations blueprint](/architecture/security-foundations) . The security foundations blueprint provides Google's opinionated design for a secure application environment and describes step by step how to configure and deploy your Google Cloud estate. Using the instructions and the scripts that are part of the security foundations blueprint, you can configure an environment that meets our security best practices and guidelines. You can build on that blueprint with additional blueprints or design your own automation.\nFor more information about automation, see [Use a CI/CD pipeline for data-processing workflows](/architecture/cicd-pipeline-for-data-processing) .\n## Monitor your network\nMonitor your network and your traffic using telemetry.\n[VPC Flow Logs](/vpc/docs/flow-logs) and [Firewall Rules Logging](/vpc/docs/firewall-rules-logging) provide near real-time visibility into the traffic and firewall usage in your Google Cloud environment. For example, [Firewall Rules Logging](/vpc/docs/firewall-rules-logging) logs traffic to and from Compute Engine VM instances. When you combine these tools with [Cloud Logging](/logging/docs) and [Cloud Monitoring](/monitoring/docs) , you can track, alert, and visualize traffic and access patterns to improve the operational security of your deployment.\n[Firewall Insights](/network-intelligence-center/docs/firewall-insights/concepts/overview) lets you review which firewall rules matched incoming and outgoing connections and whether the connections were allowed or denied. The [shadowed rules feature](/network-intelligence-center/docs/firewall-insights/concepts/overview#shadowed-firewall-rules) helps you tune your firewall configuration by showing you which rules are never triggered because another rule is always triggered first.\nUse [Network Intelligence Center](/network-intelligence-center/docs) to see how your network topology and architecture are performing. You can get detailed insights into network performance and you can then optimize your deployment to eliminate any bottlenecks in your service. [Connectivity Tests](/network-intelligence-center/docs/connectivity-tests/concepts/overview) provide you with insights into the firewall rules and policies that are applied to the network path.\nFor more information about monitoring, see [Implement logging and detective controls](/architecture/framework/security/logging-detection) .\n## What's next\nLearn more about network security with the following resources:\n- [Implement data security](/architecture/framework/security/data-security) (next document in this series)\n- [Best practices and reference architectures for VPC design](/solutions/best-practices-vpc-design) \n- [IAM roles for administering VPC Service Controls](/vpc-service-controls/docs/access-control) \n- [Onboarding as a Security Command Center partner](/security-command-center/docs/how-to-partner-onboard) \n- [Viewing vulnerabilities and threats in Security Command Center](/security-command-center/docs/how-to-view-vulnerabilities-threats) \n- [Packet Mirroring: Visualize and protect your cloud network](/blog/products/networking/packet-mirroring-visualize-and-protect-your-cloud-network) \n- [Using Packet Mirroring for intrusion detection](/vpc/docs/packet-mirroring#enterprise_security) \n- [Using Packet Mirroring with a partner IDS solution](https://live.paloaltonetworks.com/t5/community-blogs/vm-series-now-integrates-with-gcp-packet-mirroring/ba-p/302784) # Implement data security\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices for implementing data security.\nAs part of your deployment architecture, you must consider what data you plan to process and store in Google Cloud, and the sensitivity of the data. Design your controls to help secure the data during its lifecycle, to identify data ownership and classification, and to help protect data from unauthorized use.\nFor a security blueprint that deploys a BigQuery data warehouse with the security best practices described in this document, see [Secure a BigQuery data warehouse that stores confidential data](/architecture/confidential-data-warehouse-blueprint) .\n## Automatically classify your data\nPerform data classification as early in the data management lifecycle as possible, ideally when the data is created. Usually, data classification efforts require only a few categories, such as the following:\n- Public: Data that has been approved for public access.\n- Internal: Non-sensitive data that isn't released to the public.\n- Confidential: Sensitive data that's available for general internal distribution.\n- Restricted: Highly sensitive or regulated data that requires restricted distribution.\nUse [Sensitive Data Protection](/dlp) to discover and classify data across your Google Cloud environment. Sensitive Data Protection has built-in support for scanning and classifying sensitive data in [Cloud Storage](/dlp/docs/dlp-gcs) , [BigQuery](/dlp/docs/dlp-bigquery) , and [Datastore](/dlp/docs/inspecting-storage) . It also has a [streaming API](/dlp/docs/libraries) to support additional data sources and custom workloads.\nSensitive Data Protection can identify sensitive data using [built-in infotypes](/dlp/docs/concepts-infotypes) . It can automatically classify, mask, tokenize, and transform sensitive elements (such as PII data) to let you manage the risk of collecting, storing, and using data. In other words, it can integrate with your data lifecycle processes to ensure that data in every stage is protected.\nFor more information, see [De-identification and re-identification of PII in large-scale datasets using Sensitive Data Protection](/architecture/de-identification-re-identification-pii-using-cloud-dlp) .\n## Manage data governance using metadata\n[Data governance](/learn/what-is-data-governance) is a combination of processes that ensure that data is secure, private, accurate, available, and usable. Although you are responsible for defining a data governance strategy for your organization, Google Cloud provides tools and technologies to help you put your strategy into practice. Google Cloud also provides a [framework for data governance (PDF)](https://services.google.com/fh/files/misc/principles_best_practices_for_data-governance.pdf) in the cloud.\nUse [Data Catalog](/data-catalog/docs/concepts/overview) to [find](/data-catalog/docs/how-to/search) , curate, and use [metadata](/data-catalog/docs/tags-and-tag-templates) to describe your data assets in the cloud. You can use Data Catalog to search for data assets, then tag the assets with metadata. To help accelerate your data classification efforts, integrate Data Catalog with Sensitive Data Protection to automatically identify confidential data. After data is tagged, you can use Google Identity and Access Management (IAM) to restrict which data users can query or use through Data Catalog views.\nUse [Dataproc Metastore](/dataproc-metastore/docs) or [Hive metastore](/architecture/using-apache-hive-on-cloud-dataproc) to manage metadata for workloads. Data Catalog has a [hive connector](https://github.com/GoogleCloudPlatform/datacatalog-connectors-hive) that allows the service to discover metadata that's inside a hive metastore.\nUse [Dataprep by Trifacta](/dataprep) to define and enforce data quality rules through a console. You can use Dataprep from within [Cloud Data Fusion](/data-fusion) or use Dataprep as a standalone service.\n## Protect data according to its lifecycle phase and classification\nAfter you define data within the context of its lifecycle and classify it based on its sensitivity and risk, you can assign the right security controls to protect it. You must ensure that your controls deliver adequate protections, meet compliance requirements, and reduce risk. As you move to the cloud, review your current strategy and where you might need to change your current processes.\nThe following table describes three characteristics of a data security strategy in the cloud.\n| Characteristic  | Description                                                                                                                            |\n|:--------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Identification  | Understand the identity of users, resources, and applications as they create, modify, store, use, share, and delete data. Use Cloud Identity and IAM to control access to data. If your identities require certificates, consider Certificate Authority Service. For more information, see Manage identity and access.                                                 |\n| Boundary and access | Set up controls for how data is accessed, by whom, and under what circumstances. Access boundaries to data can be managed at these levels: Network layer access using controls such as Cloud Firewall. For more insight into your network layer, use Network Intelligence Center. Use VPC Service Controls to create perimeters that protect the resources and data of services you specify. IAM access controls using IAM. For information about managing IAM roles, see Using resource hierarchies for access control. |\n| Visibility   | You can audit usage and create reports that demonstrate how data is controlled and accessed. Google Cloud Logging and Access Transparency provide insights into the activities of your own cloud administrators and Google personnel. For more information, see Monitor your data.                                                          |\n## Encrypt your data\nBy default, Google Cloud encrypts customer data stored at rest, with  no action required from you. In addition to default encryption,  Google Cloud provides options for envelope encryption and encryption  key management. For example, Compute Engine persistent disks are  automatically encrypted, but you can supply or manage your own keys.\nYou must identify the solutions that best fit your requirements for key  generation, storage, and rotation, whether you're choosing the keys for  your storage, for compute, or for big data workloads.\nGoogle Cloud includes the following options for encryption and key  management:\n- **Customer-managed encryption keys (CMEK)** . You can generate and manage your encryption keys using [Cloud Key Management Service (Cloud KMS)](/kms/docs) . Use this option if you have certain key management requirements, such as the need to rotate encryption keys regularly.\n- **Customer-supplied encryption keys (CSEK)** . You can create and manage your own encryption keys, and then provide them to Google Cloud when necessary. Use this option if you generate your own keys using your on-premises key management system to bring your own key (BYOK). If you provide your own keys using CSEK, Google replicates them and makes them available to your workloads. However, the security and availability of CSEK is your responsibility because customer-supplied keys aren't stored in instance templates or in Google infrastructure. If you lose access to the keys, Google can't help you recover the encrypted data. Think carefully about which keys you want to create and manage yourself. You might use CSEK for only the most sensitive information. Another option is to perform client-side encryption on your data and then store the encrypted data in Google Cloud, where the data is encrypted again by Google.\n- **Third-party key management system with\nCloud External Key Manager (Cloud EKM)** . Cloud EKM protects your data at rest by using encryption keys that are stored and managed in a third-party key management system that you control outside of the Google infrastructure. When you use this method, you have high assurance that your data can't be accessed by anyone outside of your organization. Cloud EKM lets you achieve a secure hold-your-own-key (HYOK) model for key management. For compatibility information, see the [Cloud EKM enabled services list](/kms/docs/ekm#supported_services) .\n[Cloud KMS](/kms) also lets you encrypt your data with either software-backed encryption keys or FIPS 140-2 Level 3 validated hardware security modules (HSMs). If you're using Cloud KMS, your cryptographic keys are stored in the region where you deploy the resource. [Cloud HSM](/kms/docs/hsm) distributes your key management needs across regions, providing redundancy and global availability of keys.\nFor information on how envelope encryption works, see [Encryption at rest in Google Cloud](/security/encryption/default-encryption) .\n## Control cloud administrators' access to your data\nYou can control access by Google support and engineering personnel to your environment on Google Cloud. [Access Approval](/access-approval/docs) lets you explicitly approve before Google employees access your data or resources on Google Cloud. This product complements the visibility provided by [Access Transparency](/access-transparency) , which generates logs when Google personnel interact with your data. These logs include the office location and the reason for the access.\nUsing these products together, you can deny Google the ability to decrypt your data for any reason.\n## Configure where your data is stored and where users can access it from\nYou can control the network locations from which users can access data by using [VPC Service Controls](/vpc-service-controls/docs/overview) . This product lets you limit access to users in a specific region. You can enforce this constraint even if the user is authorized according to your [Google IAM](/iam) policy. Using VPC Service Controls, you create a [service perimeter](/vpc-service-controls/docs/create-service-perimeters) which defines the virtual boundaries from which a service can be accessed, which prevents data from being moved outside those boundaries.\nFor more information, see the following:\n- [Automating the classification of data uploaded to Cloud Storage](/dlp/docs/automating-classification-of-data-uploaded-to-cloud-storage) \n- [Data governance in the cloud](/blog/products/data-analytics/principles-and-best-practices-for-data-governance-in-the-cloud) \n- [Data warehouse to BigQuery data governance](/architecture/dw2bq/dw-bq-data-governance) \n- [Cloud Hives metastore now available](/blog/products/data-analytics/cloud-hive-metastore-now-available) ## Manage secrets using Secret Manager\n[Secret Manager](/secret-manager) lets you store all of your secrets in a centralized place. are configuration information such as database passwords, API keys, or TLS certificates. You can [automatically rotate secrets](/secret-manager/docs/secret-rotation) , and you can [configure applications to automatically use the latest version of a secret](/secret-manager/docs/rotation-recommendations#binding) . Every interaction with Secret Manager generates an audit log, so you view every access to every secret.\n[Sensitive Data Protection](/dlp) also has a [category of detectors](/dlp/docs/infotypes-reference#credentials_and_secrets) to help you identify credentials and secrets in data that could be protected with Secret Manager.\n## Monitor your data\nTo view administrator activity and key use logs, use [Cloud Audit Logs](/logging/docs/audit) . To help secure your data, monitor logs using [Cloud Monitoring](/monitoring/docs) to ensure proper use of your keys.\n[Cloud Logging](/logging) captures Google Cloud events and lets you add additional sources if necessary. You can [segment your logs by region](/logging/docs/regionalized-logs) , [store them in buckets](/logging/docs/buckets) , and integrate custom code for processing logs. For an example, see [Custom solution for automated log analysis](/architecture/security-foundations/detective-controls#custom-solution) .\nYou can also [export logs to BigQuery](/logging/docs/export/bigquery) to perform security and access analytics to help identify unauthorized changes and inappropriate access to your organization's data.\n[Security Command Center](/security-command-center/docs/concepts-security-command-center-overview) can help you identify and resolve insecure-access problems to sensitive organizational data that's stored in the cloud. Through a single management interface, you can scan for a wide variety of security vulnerabilities and risks to your cloud infrastructure. For example, you can monitor for data exfiltration, scan storage systems for confidential data, and detect which Cloud Storage buckets are open to the internet.\n## What's next\nLearn more about data security with the following resources:\n- [Deploy applications securely](/architecture/framework/security/app-security) (next document in this series)\n- [Secure a BigQuery data warehouse that stores confidential data](/architecture/confidential-data-warehouse-blueprint) \n- [Designing and deploying a data security strategy (PDF)](https://services.google.com/fh/files/misc/designing_and_deploying_data_security_strategy.pdf) \n- [Trusting your data with Google Cloud (PDF)](https://services.google.com/fh/files/misc/072022_google_cloud_trust_whitepaper.pdf) \n- [Retention policies using Bucket Lock](/storage/docs/bucket-lock) \n- [Best practices for Cloud Storage](/storage/docs/best-practices) \n- [Best practices for SQL Server instances](/compute/docs/instances/sql-server/best-practices) \n- [Datastore best practices](/datastore/docs/best-practices) \n- [Data Catalog](/data-catalog) # Deploy applications securely\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices for deploying applications securely.\nTo deploy secure applications, you must have a well-defined software development lifecycle, with appropriate security checks during the design, development, testing, and deployment stages. When you design an application, we recommend a layered system architecture that uses standardized frameworks for identity, authorization, and access control.\n## Automate secure releases\nWithout automated tools, it can be hard to deploy, update, and patch complex application environments to meet consistent security requirements. Therefore, we recommend that you build a CI/CD pipeline for these tasks, which can solve many of these issues. Automated pipelines remove manual errors, provide standardized development feedback loops, and enable fast product iterations. For example, Cloud Build [private pools](/build/docs/private-pools/private-pools-overview) let you deploy a highly secure, managed CI/CD pipeline for highly regulated industries, including finance and healthcare.\nYou can use automation to scan for security vulnerabilities when artifacts are created. You can also define policies for different environments (development, test, production, and so on) so that only verified artifacts are deployed.\n## Ensure that application deployments follow approved processes\nIf an attacker compromises your CI/CD pipeline, your entire stack can be affected. To help secure the pipeline, you should enforce an established approval process before you deploy the code into production.\nIf you plan to use Google Kubernetes Engine (GKE) or GKE Enterprise, you can establish these checks and balances by using [Binary Authorization](/binary-authorization/docs/overview) . Binary Authorization attaches configurable signatures to container images. These signatures (also called ) help to validate the image. At deployment, Binary Authorization uses these attestations to determine that a process was completed earlier. For example, you can use Binary Authorization to do the following:\n- Verify that a specific build system or continuous integration (CI) pipeline created a container image.\n- Validate that a container image is compliant with a vulnerability signing policy.\n- Verify that a container image passes criteria for promotion to the next deployment environment, such as from development to QA.## Scan for known vulnerabilities before deployment\nWe recommend that you use automated tools that can continuously perform vulnerability scans on container images before the containers are deployed to production.\nUse [Artifact Analysis](/container-analysis/docs) to automatically scan for vulnerabilities for containers that are stored in [Artifact Registry](/artifact-registry) and [Container Registry](/container-registry) . This process includes two tasks: scanning and continuous analysis.\nTo start, Artifact Analysis scans new images when they're uploaded to Artifact Registry or Container Registry. The scan extracts information about the system packages in the container.\nArtifact Analysis then looks for vulnerabilities when you upload the image. After the initial scan, Artifact Analysis continuously monitors the metadata of scanned images in Artifact Registry and Container Registry for new vulnerabilities. When Artifact Analysis receives new and updated vulnerability information from [vulnerability sources](/container-analysis/docs/vulnerability-scanning#sources) , it does the following:\n- Updates the metadata of the scanned images to keep them up to date.\n- Creates new vulnerability occurrences for new notes.\n- Deletes vulnerability occurrences that are no longer valid.## Monitor your application code for known vulnerabilities\nIt's a best practice to use automated tools that can constantly monitor your application code for known vulnerabilities such as the [OWASP Top 10](https://owasp.org/www-project-top-ten/) . For a description of Google Cloud products and features that support OWASP Top 10 mitigation techniques, see [OWASP Top 10 mitigation options on Google Cloud](/architecture/owasp-top-ten-mitigation) .\nUse [Web Security Scanner](/security-command-center/docs/concepts-web-security-scanner-overview) to help identify security vulnerabilities in your App Engine, Compute Engine, and Google Kubernetes Engine web applications. The scanner crawls your application, following all links within the scope of your starting URLs, and attempts to exercise as many user inputs and event handlers as possible. It can automatically scan for and detect common vulnerabilities, including cross-site scripting (XSS), Flash injection, mixed content (HTTP in HTTPS), and outdated or insecure libraries. Web Security Scanner gives you early identification of these types of vulnerabilities with low false positive rates.\n## Control movement of data across perimeters\nTo control the movement of data across a perimeter, you can configure security perimeters around the resources of your Google-managed services. Use [VPC Service Controls](/vpc-service-controls/docs/overview) to place all components and services in your CI/CD pipeline (for example, Container Registry, Artifact Registry, Artifact Analysis, and Binary Authorization) inside a security perimeter.\nVPC Service Controls improves your ability to mitigate the risk of unauthorized copying or transfer of data (data exfiltration) from Google-managed services. With VPC Service Controls, you configure security perimeters around the resources of your Google-managed services to control the movement of data across the perimeter boundary. When a service perimeter is enforced, requests that violate the perimeter policy are denied, such as requests that are made to protected services from outside a perimeter. When a service is protected by an enforced perimeter, VPC Service Controls ensures the following:\n- A service can't transmit data out of the perimeter. Protected services function as normal inside the perimeter, but can't send resources and data out of the perimeter. This restriction helps prevent malicious insiders who might have access to projects in the perimeter from exfiltrating data.\n- Requests that come from outside the perimeter to the protected service are honored only if the requests meet the criteria of [access levels](/vpc-service-controls/docs/use-access-levels) that are assigned to the perimeter.\n- A service can be made accessible to projects inperimeters using [perimeter bridges](/vpc-service-controls/docs/share-across-perimeters) .## Encrypt your container images\nIn Google Cloud, you can encrypt your container images using [customer-managed encryption keys (CMEK)](/kms/docs/cmek) . CMEK keys are managed in Cloud Key Management Service (Cloud KMS). When you use CMEK, you can temporarily or permanently disable access to an encrypted container image by disabling or destroying the key.\n## What's next\nLearn more about securing your supply chain and application security with the following resources:\n- [Manage compliance obligations](/architecture/framework/security/compliance) (next document in this series)\n- [Operational excellence](/architecture/framework/operational-excellence) \n- [Binary Authorization](/binary-authorization/docs) \n- [Artifact Analysis](/container-analysis/docs) \n- [Artifact Registry](/artifact-registry) # Manage compliance obligations\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices for managing compliance obligations.\nYour cloud regulatory requirements depend on a combination of factors, including the following:\n- The laws and regulations that apply your organization's physical locations.\n- The laws and regulations that apply to your customers' physical locations.\n- Your industry's regulatory requirements.\nThese requirements shape many of the decisions that you need to make about which security controls to enable for your workloads in Google Cloud.\nA typical compliance journey goes through three stages: assessment, gap remediation, and continual monitoring. This section addresses the best practices that you can use during each stage.\n## Assess your compliance needs\nCompliance assessment starts with a thorough review of all of your regulatory obligations and how your business is implementing them. To help you with your assessment of Google Cloud services, use the [Compliance resource center](/security/compliance) . This site provides you with details on the following:\n- Service support for various regulations\n- Google Cloud certifications and attestations\nYou can [ask for an engagement](/support-hub) with a Google compliance specialist to better understand the compliance lifecycle at Google and how your requirements can be met.\nFor more information, see [Assuring compliance in the cloud (PDF)](https://services.google.com/fh/files/misc/assuringcompliance_in_the_cloud.pdf) .\n## Deploy Assured Workloads\n[Assured Workloads](/assured-workloads) is the Google Cloud tool that builds on the controls within Google Cloud to help you meet your compliance obligations. Assured Workloads lets you do the following:\n- Select your compliance regime. The tool then automatically sets the baseline personnel access controls.\n- Set the location for your data using organization policies so that your data at rest and your resources remain only in that region.\n- Select the key management option (such as the key rotation period) that best fits your security and compliance requirements.\n- For certain regulatory requirements such as FedRAMP Moderate, select the criteria for access by Google support personnel (for example, whether they have completed appropriate background checks).\n- Ensure that Google-managed encryption keys are FIPS-140-2 compliant and support FedRAMP Moderate compliance. For an added layer of control and separation of duties, you can use customer-managed encryption keys (CMEK). For more information about keys, see [Encrypt your data](/architecture/framework/security/data-security#encrypt-data) .## Review blueprints for templates and best practices that apply to your compliance regime\nGoogle has published blueprints and solutions guides that describe best practices and that provide Terraform modules to let you roll out an environment that helps you achieve compliance. The following table lists a selection of blueprints that address security and alignment with compliance requirements.\n| Standard | Description                                      |\n|:-----------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| PCI  | Security blueprint: PCI on GKE PCI Data Security Standard compliance Limiting scope of compliance for PCI environments in Google Cloud PCI DSS compliance on GKE |\n| FedRAMP | Google Cloud FedRAMP implementation guide (PDF) Setting up a FedRAMP Aligned Three-Tier Workload on Google Cloud             |\n| HIPAA  | Protecting healthcare data on Google Cloud (PDF) Setting up a HIPAA aligned workload using Data Protection Toolkit (PDF)           |\n## Monitor your compliance\nMost regulations require you to monitor particular activities, including  access controls. To help with your monitoring, you can use the following:\n- [Access Transparency](/assured-workloads/access-transparency/docs/overview) , which provides near real-time logs when Google Cloud admins access your content.\n- [Firewall Rules Logging](/vpc/docs/firewall-rules-logging) to record TCP and UDP connections inside a VPC network for any rules that you create yourself. These logs can be useful for auditing network access or for providing early warning that the network is being used in an unapproved manner.\n- [VPC Flow Logs](/vpc/docs/using-flow-logs) to record network traffic flows that are sent or received by VM instances.\n- [Security Command Center Premium](/security-command-center/docs/concepts-vulnerabilities-findings) to monitor for compliance with various standards.\n- [OSSEC](https://ossec.github.io/index.html) (or another open source tool) to log the activity of individuals who have admin access to your environment.\n- [Key Access Justifications](/assured-workloads/key-access-justifications/docs/overview) to view the reasons for a key access request.## Automate your compliance\nTo help you remain in compliance with changing regulations, determine if there are ways that you can automate your security policies by incorporating them into your infrastructure as code deployments. For example, consider the following:\n- Use security blueprints to build your security policies into your infrastructure deployments.\n- Configure Security Command Center to alert when non-compliance issues occur. For example, monitor for issues such as users disabling two-step verification or over-privileged service accounts. For more information, see [Setting up finding notifications](/security-command-center/docs/how-to-notifications) .\n- Set up automatic remediation to particular notifications. For more information, see [Cloud Functions code](https://github.com/GoogleCloudPlatform/security-response-automation) .\nFore more information about compliance automation, see the [Risk and Compliance as Code (RCaC) solution](/solutions/risk-and-compliance-as-code) .\n## What's next\nLearn more about compliance with the following resources:\n- [Implement data residency and sovereignty requirements](/architecture/framework/security/data-residency-sovereignty) (next document in this series)\n- [Compliance Resource Center](/security/compliance) \n- [Google security whitepaper (PDF)](https://services.google.com/fh/files/misc/google_security_wp.pdf) \n- [Assured Workloads](/assured-workloads/docs) # Implement data residency and sovereignty requirements\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices for implementing data residency and sovereignty requirements.\nData residency and sovereignty requirements are based on your regional and industry-specific regulations, and different organizations might have different data sovereignty requirements. For example, you might have the following requirements:\n- Control over all access to your data by Google Cloud, including what type of personnel can access the data and from which region they can access it.\n- Inspectability of changes to cloud infrastructure and services, which can have an impact on access to your data or the security of your data. Insight into these types of changes helps ensure that Google Cloud is unable to circumvent controls or move your data out of the region.\n- Survivability of your workloads for an extended time when you are unable to receive software updates from Google Cloud.## Manage your data sovereignty\nData sovereignty provides you with a mechanism to prevent Google from accessing your data. You approve access only for provider behaviors that you agree are necessary.\nFor example, you can manage your data sovereignty in the following ways:\n- Store and manage [encryption keys](/kms/docs/ekm) outside the cloud.\n- Only [grant access](/blog/products/identity-security/control-access-to-gcp-data-with-key-access-justifications) to these keys based on detailed access justifications.\n- [Protect data in use](/blog/products/identity-security/introducing-google-cloud-confidential-computing-with-confidential-vms) .## Manage your operational sovereignty\nOperational sovereignty provides you with assurances that Google personnel can't compromise your workloads.\nFor example, you can manage operational sovereignty in the following ways:\n- [Restrict the deployment](/blog/products/identity-security/assured-workloads-for-government-compliance-without-compromise) of new resources to specific provider regions.\n- [Limit Google personnel access](/blog/products/identity-security/assured-workloads-for-government-compliance-without-compromise) based on predefined attributes such as their citizenship or geographic location.## Manage software sovereignty\nSoftware sovereignty provides you with assurances that you can control the availability of your workloads and run them wherever you want, without depending on (or being locked in to) a single cloud provider. Software sovereignty includes the ability to survive events that require you to quickly change where your workloads are deployed and what level of outside connection is allowed.\nFor example, Google Cloud supports [hybrid and multicloud deployments](/architecture/hybrid-multicloud-patterns-and-practices) . In addition, [GKE Enterprise](/anthos/docs/concepts/overview) lets you manage and deploy your applications in both cloud environments and on-premises environments.\n## Control data residency\nData residency describes where your data is stored at rest. Data residency requirements vary based on systems design objectives, industry regulatory concerns, national law, tax implications, and even culture.\nControlling data residency starts with the following:\n- Understanding the type of your data and its location.\n- Determining what risks exist to your data, and what laws and regulations apply.\n- Controlling where data is or where it goes.\nTo help comply with data residency requirements, Google Cloud lets you control where your data is stored, how it is accessed, and how it's processed. You can use [resource location policies](/resource-manager/docs/organization-policy/defining-locations) to restrict where resources are created and to limit where data is replicated between regions. You can use the location property of a resource to identify where the service deploys and who maintains it.\nFor supportability information, see [Resource locations supported services](/resource-manager/docs/organization-policy/defining-locations-supported-services) .\n## What's next\nLearn more about data residency and sovereignty with the following resources:\n- [Implement privacy requirements](/architecture/framework/security/privacy) (next document in this series)\n- [Data residency, operational transparency, and privacy for European customers on Google Cloud (PDF)](https://services.google.com/fh/files/misc/googlecloud_european_commitments_whitepaper.pdf) \n- [Designing and deploying data security strategy (PDF)](https://services.google.com/fh/files/misc/designing_and_deploying_data_security_strategy.pdf) \n- [Cloud Key Management Service](/security-key-management) \n- [Trusting your data with Google Cloud (PDF)](https://services.google.com/fh/files/misc/072022_google_cloud_trust_whitepaper.pdf) \n- [Privileged access at Google](/logging/docs/audit/privileged-access) \n- [Google Cloud Access Transparency](/logging/docs/audit/access-transparency-overview) # Implement privacy requirements\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices for implementing privacy requirements.\nPrivacy regulations help define how you can obtain, process, store, and manage your users' data. Many privacy controls (for example, controls for cookies, session management, and obtaining user permission) are your responsibility because you own your data (including the data that you receive from your users).\nGoogle Cloud includes the following controls that promote privacy:\n- Default encryption of all data when it's at rest, when it's in transit, and while it's being processed.\n- Safeguards against insider access.\n- Support for numerous privacy regulations.\nFor more information, see [Google Cloud Privacy Commitments](/security/privacy) .\n## Classify your confidential data\nYou must define what data is confidential and then ensure that the confidential data is properly protected. Confidential data can include credit card numbers, addresses, phone numbers, and other personal identifiable information (PII).\nUsing [Sensitive Data Protection](/dlp) , you can set up appropriate classifications. You can then tag and tokenize your data before you store it in Google Cloud. For more information, see [Automatically classify your data](/architecture/framework/security/data-security#dataautoclass) .\n## Lock down access to sensitive data\nPlace sensitive data in its own service perimeter using VPC Service Controls, and set Google Identity and Access Management (IAM) access controls for that data. Configure multi-factor authentication (MFA) for all users who require access to sensitive data.\nFor more information, see [Control movement of data across perimeters](/architecture/framework/security/app-security#controlmovement) and [Set up SSO and MFA](/architecture/framework/security/identity-access#ssomfa) .\n## Monitor for phishing attacks\nEnsure that your email system is configured to protect against phishing attacks, which are often used for fraud and malware attacks.\nIf your organization uses Gmail, you can use [advanced phishing and malware protection](https://support.google.com/a/answer/9157861) . This collection of settings provides controls to quarantine emails, defends against anomalous attachment types, and helps protect against from inbound spoofing emails. [Security Sandbox](https://support.google.com/a/answer/7676854) detects malware in attachments. Gmail is continually and automatically updated with the latest security improvements and protections to help keep your organization's email safe.\n## Extend zero trust security to your hybrid workforce\nA zero trust security model means that no one is trusted implicitly, whether they are inside or outside of your organization's network. When your IAM systems verify access requests, a zero trust security posture means that the user's identity context (for example, their IP address or location) are considered. Unlike a VPN, zero trust security shifts access controls from the network perimeter to users and their devices. Zero trust security allows users to work more securely from any location. For example, users can access your organization's resources from their laptops or mobile devices while at home.\nOn Google Cloud, you can configure [BeyondCorp Enterprise](/beyondcorp-enterprise) and [Identity-Aware Proxy (IAP)](/iap) to enable zero trust for your Google Cloud resources. If your users use Google Chrome and you enable BeyondCorp Enterprise, you can [integrate zero-trust security into your users browsers](https://support.google.com/a/answer/10104463) .\n## What's next\nLearn more about security and privacy with the following resources:\n- [Implement logging and detective controls](/architecture/framework/security/logging-detection) (next document in this series)\n- [Privacy Center](/privacy) \n- [Google Cloud Privacy Notice](/terms/cloud-privacy-notice) \n- [Privacy best practices when working with Google Cloud Support](https://support.google.com/a/answer/10985601) \n- [How Google protects your organization's security and privacy](https://support.google.com/a/answer/60762) \n- [Privacy FAQ](https://support.google.com/googlecloud/answer/6056650) # Implement logging and detective controls\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices for implementing logging and detective controls.\nDetective controls use telemetry to detect misconfigurations, vulnerabilities, and potentially malicious activity in a cloud environment. Google Cloud lets you create tailored monitoring and detective controls for your environment. This section describes these additional features and recommendations for their use.\n## Monitor network performance\n[Network Intelligence Center](/network-intelligence-center/docs) gives you visibility into how your network topology and architecture are performing. You can get detailed insights into network performance and then use that information to optimize your deployment by eliminating bottlenecks on your services. [Connectivity Tests](/network-intelligence-center/docs/connectivity-tests/concepts/overview) provides you with insights into the firewall rules and policies that are applied to the network path.\n## Monitor and prevent data exfiltration\nData exfiltration is a key concern for organizations. Typically, it occurs when an authorized person extracts data from a secured system and then shares that data with an unauthorized party or moves it to an insecure system.\nGoogle Cloud provides several features and tools that help you detect and prevent data exfiltration. For more information, see [Preventing data exfiltration](/security/data-loss-prevention/preventing-data-exfiltration) .\n## Centralize your monitoring\n[Security Command Center](/security-command-center/docs) provides visibility into the resources that you have in Google Cloud and into their security state. Security Command Center helps you prevent, detect, and respond to threats. It provides a centralized dashboard that you can use to help identify security misconfigurations in virtual machines, in networks, in applications, and in storage buckets. You can address these issues before they result in business damage or loss. The built-in capabilities of Security Command Center can reveal suspicious activity in your Cloud Logging security logs or indicate compromised virtual machines.\nYou can respond to threats by following actionable recommendations or by exporting logs to your [SIEM](https://wikipedia.org/wiki/Security_information_and_event_management) system for further investigation. For information about using a SIEM system with Google Cloud, see [Security log analytics in Google Cloud](/architecture/exporting-stackdriver-logging-for-security-and-access-analytics) .\nSecurity Command Center also provides multiple detectors that help you analyze the security of your infrastructure. These detectors include the following:\n- [Event Threat Detection](/security-command-center/docs/concepts-event-threat-detection-overview) \n- [Security Health Analytics (SHA)](/security-command-center/docs/how-to-use-security-health-analytics) \n- [Sensitive Actions Service](/security-command-center/docs/how-to-use-sensitive-actions) \nOther Google Cloud services, such as [Google Cloud Armor logs](/armor/docs/cscc-findings) , also provide findings for display in Security Command Center.\nEnable the services that you need for your workloads, and then only monitor and analyze important data. For more information about enabling logging on services, see the [enable logs](/architecture/exporting-stackdriver-logging-for-security-and-access-analytics#enable_logs) section in Security log analytics in Google Cloud.\n## Monitor for threats\n[Event Threat Detection](/security-command-center/docs/concepts-event-threat-detection-overview) is an optional managed service of Security Command Center Premium that detects threats in your log stream. By using Event Threat Detection, you can detect high-risk and costly threats such as malware, cryptomining, unauthorized access to Google Cloud resources, DDoS attacks, and brute-force SSH attacks. Using the tool's features to distill volumes of log data, your security teams can quickly identify high-risk incidents and focus on remediation.\nTo help detect potentially compromised user accounts in your organization, use the [Sensitive Actions Cloud Platform logs](/logging/docs/api/platform-logs#sensitive_actions_service) to identify when sensitive actions are taken and to confirm that valid users took those actions for valid purposes. A is an action, such as the addition of a highly privileged role, that could be damaging to your business if a malicious actor took the action. Use Cloud Logging to [view](/logging/docs/view/logs-explorer-summary) , [monitor](/logging/docs/alerting/monitoring-logs) , and [query](/logging/docs/view/logging-query-language) the Sensitive Actions Cloud Platform logs. You can also view the sensitive action log entries with the [Sensitive Actions Service](/security-command-center/docs/concepts-sensitive-actions-overview) , a built-in service of [Security Command Center](/security-command-center/docs) Premium.\n[Chronicle](/chronicle/docs/overview) can store and analyze all of your security data centrally. To help you see the entire span of an attack, Chronicle can map logs into a common model, enrich them, and then link them together into timelines. Furthermore, you can use Chronicle to create detection rules, set up indicators of compromise (IoC) matching, and perform threat-hunting activities. You write your detection rules in the [YARA-L language](/chronicle/docs/detection/yara-l-2-0-overview) . For sample threat detection rules in YARA-L, see the [Community Security Analytics (CSA)](https://github.com/GoogleCloudPlatform/security-analytics) repository. In addition to writing your own rules, you can take advantage of [curated detections](/chronicle/docs/detection/curated-detections) in Chronicle. These curated detections are a set of predefined and managed YARA-L rules that can help you identify threats.\nAnother option to centralizing your logs for security analysis, audit, and investigation is to use [BigQuery](/bigquery/docs) . In BigQuery, you monitor common threats or misconfigurations by using SQL queries (such as those in the CSA repository) to analyze permission changes, provisioning activity, workload usage, data access, and network activity. For more information about security log analytics in BigQuery from setup through analysis, see [Security log analytics in Google Cloud](/architecture/exporting-stackdriver-logging-for-security-and-access-analytics) .\nThe following diagram shows how to centralize your monitoring by using both the built-in threat detection capabilities of Security Command Center and the threat detection that you do in BigQuery, Chronicle, or a third-party SIEM.\nAs shown in the diagram, there are variety of security data sources that you should monitor. These data sources include logs from Cloud Logging, asset changes from Cloud Asset Inventory, Google Workspace logs, or events from hypervisor or a guest kernel. The diagram shows that you can use Security Command Center to monitor these data sources. This monitoring occurs automatically provided that you've enabled the appropriate features and threat detectors in Security Command Center. The diagram shows that you can also monitor for threats by exporting security data and Security Command Center findings to an analytics tool such as BigQuery, Chronicle, or a third-party SIEM. In your analytics tool, the diagram shows that you can perform further analysis and investigation by using and extending queries and rules like those available in CSA.\n## What's next\nLearn more about logging and detection with the following resources:\n- [Cloud Logging](/logging) \n- [Introducing Chronicle Detect from Google Cloud](/blog/products/identity-security/introducing-chronicle-detect-from-google-cloud) \n- [Build a collaborative incident management process](/architecture/framework/reliability/build-incident-management-process) # Google Cloud Architecture Framework: Reliability\nThis category in the [Google Cloud Architecture Framework](/architecture/framework) shows you how to architect and operate reliable services on a cloud platform. You also learn about some of the Google Cloud products and features that support reliability.\nThe Architecture Framework describes best practices, provides implementation recommendations, and explains some of the available products and services. The framework aims to help you design your Google Cloud deployment so that it best matches your business needs.\nTo run a reliable service, your architecture must include the following:\n- Measurable reliability goals, with deviations that you promptly correct.\n- Design patterns for scalability, high availability, disaster recovery, and automated change management.\n- Components that self-heal where possible, and code that includes instrumentation for observability.\n- Operational procedures that run the service with minimal manual work and cognitive load on operators, and that let you rapidly detect and mitigate failures.\nReliability is the responsibility of everyone in engineering, such as the development, product management, operations, and site reliability engineering (SRE) teams. Everyone must be accountable and understand their application's reliability targets, and risk and error budgets. Teams should be able to prioritize work appropriately and escalate priority conflicts between reliability and product feature development.\nIn the reliability category of the Architecture Framework, you learn to do the following:\n- [Understand the core reliability principles](/architecture/framework/reliability/principles) \n- [Define your reliability goals](/architecture/framework/reliability/define-goals) \n- [Define SLOs](/architecture/framework/reliability/defining-SLOs) \n- [Adopt SLOs](/architecture/framework/reliability/adopting-slos) \n- [Build observability into your infrastructure and application](/architecture/framework/reliability/observability-infrastructure-applications) \n- [Design for scale and high availability](/architecture/framework/reliability/design-scale-high-availability) \n- [Create reliable operational processes and tools](/architecture/framework/reliability/create-operational-processes-tools) \n- [Build efficient alerts](/architecture/framework/reliability/build-efficient-alerts) \n- [Build a collaborative incident management process](/architecture/framework/reliability/build-incident-management-process) # Reliability principles\nThis document in the [Architecture Framework](/architecture/framework) explains some of the core principles to run reliable services on a cloud platform. These principles help to create a common understanding as you read additional sections of the Architecture Framework that show you how some of the Google Cloud products and features support reliable services.\n## Key terminology\nThere are several common terms associated with reliability practices. These may be familiar to many readers. However, for a refresher see the detailed descriptions at the [Terminology](/architecture/framework/reliability/terminology) page.\n## Core principles\nGoogle's approach to reliability is based on the following core principles.\n### Reliability is your top feature\nNew product features are sometimes your top priority in the short term. However, reliability is your top product feature in the long term, because if the product is too slow or is unavailable over a long period of time, your users might leave, making other product features irrelevant.\n### Reliability is defined by the user\nFor user-facing workloads, measure the user experience. The user must be happy with how your service performs. For example, measure the success ratio of user requests, not just server metrics like CPU usage.\nFor batch and streaming workloads, you might need to measure key performance indicators (KPIs) for data throughput, such as rows scanned per time window, instead of server metrics such as disk usage. Throughput KPIs can help ensure a daily or quarterly report required by the user finishes on time.\n### 100% reliability is the wrong target\nYour systems should be reliable enough that users are happy, but not excessively reliable such that the investment is unjustified. Define SLOs that set the reliability threshold you want, then use error budgets to manage the appropriate rate of change.\nApply the design and operational principles in this framework to a product only if the SLO for that product or application justifies the cost.\n### Reliability and rapid innovation are complementary\nUse error budgets to achieve a balance between system stability and developer agility. The following guidance helps you determine when to move fast or slow:\n- When an adequate error budget is available, you can innovate rapidly and improve the product or add product features.\n- When the error budget is diminished, slow down and focus on reliability features.## Design and operational principles\nTo maximize system reliability, the following design and operational principles apply. Each of these principles is discussed in detail in the rest of the Architecture Framework reliability category.\n### Define your reliability goals\nThe best practices covered in this section of the Architecture Framework include the following:\n- Choose appropriate SLIs.\n- Set SLOs based on the user experience.\n- Iteratively improve SLOs.\n- Use strict internal SLOs.\n- Use error budgets to manage development velocity.\nFor more information, see [Define your reliability goals](/architecture/framework/reliability/define-goals) in the Architecture Framework reliability category.\n### Build observability into your infrastructure and applications\nThe following design principle is covered in this section of the Architecture Framework:\n- Instrument your code to maximize observability.\nFor more information, see [Build observability into your infrastructure and applications](/architecture/framework/reliability/observability-infrastructure-applications) in the Architecture Framework reliability category.\n### Design for scale and high availability\nThe following design principles are covered in this section of the Architecture Framework:\n- Create redundancy for higher availability.\n- Replicate data across regions for disaster recovery.\n- Design a multi-region architecture for resilience to regional outages.\n- Eliminate scalability bottlenecks.\n- Degrade service levels gracefully when overloaded.\n- Prevent and mitigate traffic spikes.\n- Sanitize and validate inputs.\n- Fail safe in a way that preserves system function.\n- Design API calls and operational commands to be retryable.\n- Identify and manage system dependencies.\n- Minimize critical dependencies.\n- Ensure that every change can be rolled back.\nFor more information, see [Design for scale and high availability](/architecture/framework/reliability/design-scale-high-availability) in the Architecture Framework reliability category.\n### Create reliable operational processes and tools\nThe following operational principles are covered in this section of the Architecture Framework:\n- Choose good names for applications and services.\n- Implement progressive rollouts with canary testing procedures.\n- Spread out traffic for timed promotion and launches.\n- Automate the build, test, and deployment process.\n- Defend against operator error.\n- Test failure recovery procedures.\n- Conduct disaster recovery tests.\n- Practice chaos engineering.\nFor more information, see [Create reliable operational processes and tools](/architecture/framework/reliability/create-operational-processes-tools) in the Architecture Framework reliability category.\n### Build efficient alerts\nThe following operational principles are covered in this section of the Architecture Framework:\n- Optimize alert delays.\n- Alert on symptoms, not causes.\n- Alert on outliers, not averages.\nFor more information, see [Build efficient alerts](/architecture/framework/reliability/build-efficient-alerts) in the Architecture Framework reliability category.\n### Build a collaborative incident management process\nThe following operational principles are covered in this section of the Architecture Framework:\n- Assign clear service ownership.\n- Reduce time to detect (TTD) with well tuned alerts.\n- Reduce time to mitigate (TTM) with incident management plans and training.\n- Design dashboard layouts and content to minimize TTM.\n- Document diagnostic procedures and mitigation for known outage scenarios.\n- Use blameless postmortems to learn from outages and prevent recurrences.\nFor more information, see [Build a collaborative incident management process](/architecture/framework/reliability/build-incident-management-process) in the Architecture Framework reliability category.\n## What's next\n- [Define your reliability goals](/architecture/framework/reliability/define-goals) (next document in this series)\nExplore other categories in the [Architecture Framework](/architecture/framework) such as system design, operational excellence, security, privacy, and compliance.# Define your reliability goals\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices to define appropriate ways to measure the customer experience of your services so you can run reliable services. You learn how to iterate on the service level objectives (SLOs) you define, and use error budgets to know when reliability might suffer if you release additional updates.\n## Choose appropriate SLIs\nIt's important to choose appropriate service level indicators (SLIs) to fully understand how your service performs. For example, if your application has a multi-tenant architecture that is typical of SaaS applications used by multiple independent customers, capture SLIs at a per-tenant level. If your SLIs are measured only at a global aggregate level, you might miss critical problems in your application that affect a single important customer or a minority of customers. Instead, design your application to include a tenant identifier in each user request, then propagate that identifier through each layer of the stack. This identifier lets your monitoring system aggregate statistics at the per-tenant level at every layer or microservice along the request path.\nThe type of service you run also determines what SLIs to monitor, as shown in the following examples.\n### Serving systems\nThe following SLIs are typical in systems that serve data:\n- tells you the fraction of the time that a service is usable. It's often defined in terms of the fraction of well-formed requests that succeed, such as 99%.\n- tells you how quickly a certain percentage of requests can be fulfilled. It's often defined in terms of a percentile other than 50th, such as \"99th percentile at 300 ms\".\n- tells you how good a certain response is. The definition of quality is often service-specific, and indicates the extent to which the content of the response to a request varies from the ideal response content. The response quality could be binary (good or bad) or expressed on a scale from 0% to 100%.\n### Data processing systems\nThe following SLIs are typical in systems that process data:\n- tells you the fraction of data that has been processed, such as 99.9%.\n- tells you the fraction of output data deemed to be correct, such as 99.99%.\n- tells you how fresh the source data or the aggregated output data is. Typically the more recently updated, the better, such as 20 minutes.\n- tells you how much data is being processed, such as 500 MiB/sec or even 1000 requests per second (RPS).\n### Storage systems\nThe following SLIs are typical in systems that store data:\n- tells you how likely the data written to the system can be retrieved in the future, such as 99.9999%. Any permanent data loss incident reduces the durability metric.\n- Throughput and latency are also common SLIs for storage systems.## Choose SLIs and set SLOs based on the user experience\nOne of the core principles in this Architecture Framework section is that reliability is defined by the user. Measure reliability metrics as close to the user as possible, such as the following options:\n- If possible, instrument the mobile or web client.- For example, use [Firebase performance monitoring](https://firebase.google.com/docs/perf-mon) to gain insight into the performance characteristics of your iOS, Android, and web apps.\n- If that's not possible, instrument the load balancer.- For example, use Cloud Monitoring for [external Application Load Balancer logging and monitoring](/load-balancing/docs/https/https-logging-monitoring) .\n- A measure of reliability at the server should be the last option.- For example, [monitor a Compute Engine instance](/monitoring/monitor-compute-engine-virtual-machine) with Cloud Monitoring.Set your SLO just high enough that almost all users are happy with your service, and no higher. Because of network connectivity or other transient client-side issues, your customers might not notice brief reliability issues in your application, allowing you to lower your SLO.\nFor uptime and other vital metrics, aim for a target lower than 100% but close to it. Service owners should objectively assess the minimum level of service performance and availability that would make most users happy, not just set targets based on external contractual levels.\nThe rate at which you change affects your system's reliability. However, the ability to make frequent, small changes helps you deliver features faster and with higher quality. Achievable reliability goals tuned to the customer experience help define the maximum pace and scope of changes (feature velocity) that customers can tolerate.\nIf you can't measure the customer experience and define goals around it, you can run a competitive benchmark analysis. If there's no comparable competition, measure the customer experience, even if you can't define goals yet. For example, measure system availability or the rate of meaningful and successful transactions to the customer. You can correlate this data with business metrics or KPIs such as the volume of orders in retail or the volume of customer support calls and tickets and their severity. Over a period of time, you can use such correlation exercises to get to a reasonable threshold of customer happiness. This threshold is your SLO.\n## Iteratively improve SLOs\nSLOs shouldn't be set in stone. Revisit SLOs quarterly, or at least annually, and confirm that they continue to accurately reflect user happiness and correlate well with service outages. Make sure that they cover current business needs and new [critical user journeys](https://sre.google/workbook/implementing-slos/#modeling-user-journeys) . Revise and augment your SLOs as needed after these periodic reviews.\n## Use strict internal SLOs\nIt's a good practice to have stricter internal SLOs than external SLAs. As SLA violations tend to require issuing a financial credit or customer refunds, you want to address problems before they have financial impact.\nWe recommend that you use these stricter internal SLOs with a blameless postmortem process and incident reviews. For more information, see [Build a collaborative incident management process](/architecture/framework/reliability/build-incident-management-process) in the Architecture Center reliability category.\n## Use error budgets to manage development velocity\nError budgets tell you if your system is more or less reliable than is needed over a certain time window. Error budgets are calculated as over a period of time, such as 30 days.\nWhen you have capacity left in your error budget, you can continue to launch improvements or new features quickly. When the error budget is close to zero, freeze or slow down service changes and invest engineering resources to improve reliability features.\n[Google Cloud Observability](/products/operations) includes SLO monitoring to minimize the effort of setting up SLOs and error budgets. The services include a graphical user interface to help you to configure SLOs manually, an API for programmatic setup of SLOs, and built-in dashboards to track the error budget burn rate. For more information, see how to [create an SLO](/stackdriver/docs/solutions/slo-monitoring/ui/create-slo) .\n## Recommendations\nTo apply the guidance in the Architecture Framework to your own environment, follow these recommendations::\n- Define and measure customer-centric SLIs, such as the availability or latency of the service.\n- Define a customer-centric error budget that's stricter than your external SLA. Include consequences for violations, such as production freezes.\n- Set up latency SLIs to capture outlier values, such as 90th or 99th percentile, to detect the slowest responses.\n- Review SLOs at least annually and confirm that they correlate well with user happiness and service outages.## What's next\nLearn more about how to define your reliability goals with the following resources:\n- [Build observability into your infrastructure and application](/architecture/framework/reliability/observability-infrastructure-applications) (next document in this series)\n- [Coursera - SRE: Measuring and Managing Reliability](https://www.coursera.org/learn/site-reliability-engineering-slos) \n- [SRE book chapter SLOs](https://landing.google.com/sre/sre-book/chapters/service-level-objectives/) and [SRE workbook to implement SLOs](https://landing.google.com/sre/workbook/chapters/implementing-slos/) \n- [Tune up your SLI metrics](/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons) \nExplore other categories in the [Architecture Framework](/architecture/framework) such as system design, operational excellence, and security, privacy, and compliance.# Define SLOs\nThis document is Part 1 of two parts that show how teams that operate online services can begin to build and adopt a culture of Site Reliability Engineering (SRE) by using service level objectives (SLOs). An SLO is a target level of reliability for a service.\nIn software as a service (SaaS), a natural tension exists between the velocity of product development and operational stability. The more you change your system, the more likely it will break. Monitoring and observability tools can help you maintain confidence in your operational stability as you increase development velocity. However, although such tools\u2014known also as (APM) tools\u2014are important, one of the most important applications of these tools is in setting SLOs.\nIf defined correctly, an SLO can help teams make data-driven operational decisions that increase development velocity without sacrificing stability. The SLO can also align development and operations teams around a single agreed-to objective, which can alleviate the natural tension that exists between their objectives\u2014creating and iterating products (development) and maintaining system integrity (operations).\nSLOs are described in detail in [The SRE Book](https://landing.google.com/sre/sre-book/chapters/embracing-risk/) and [The SRE Workbook](https://landing.google.com/sre/workbook/chapters/implementing-slos/) , alongside other SRE practices. This series attempts to simplify the process of understanding and developing SLOs to help you more easily adopt them. Once you have read and understood these articles, you can find more in the books.\nThis series aims to show you a clear path to implementing SLOs in your organization:\n- This document reviews what SLOs are and how to define them for your services.\n- [Adopting SLOs](/solutions/adopting-SLOs) covers different types of SLOs based on workload types, how to measure those SLOs, and how to develop alerts based on them.\nThis series is intended for SREs, operations teams, DevOps, systems administrators, and others who are responsible for the stability and reliability of an online service. It assumes that you understand how internet services communicate with web browsers and mobile devices, and that you have a basic understanding of how web services are monitored, deployed, and troubleshot.\nThe [State of DevOps](/devops) reports identified capabilities that drive software delivery performance. This series will help you with the following capabilities:\n- [Monitoring and observability](/solutions/devops/devops-measurement-monitoring-and-observability) \n- [Monitoring systems to inform business decisions](/solutions/devops/devops-measurement-monitoring-systems) \n- [Proactive failure notification](/solutions/devops/devops-measurement-proactive-failure-notification) ## Why SLOs?\nWhen you build a culture of SRE, why start with SLOs? In short, if you don't define a service level, it's difficult to measure whether your customers are happy with your service. Even if you know that you can improve your service, the lack of a defined service level makes it hard to determine where and how much to invest in improvements.\nIt can be tempting to develop separate SLOs for every service, user-facing or not. For instance, a common mistake is to measure two or more services \u2014for example, a frontend service and a backend datastore\u2014when the user relies on services and isn't aware of the distinction. A better approach is to develop SLOs that are based on the product (the collection of services) and focus on the most important interactions that your users have with it.\nTherefore, to develop an effective SLO, it's ideal that you understand your users' interactions with your service, which are called (CUJs). A CUJ considers the goals of your users, and how your users use your services to accomplish those goals. The CUJ is defined from the perspective of your customer without consideration for service boundaries. If the CUJ is met, the customer is happy, and happy customers are a key measurement of success for a service.\nA key aspect to customer happiness with a service is a service's reliability. It doesn't matter what a service does if it's not reliable. Thus, reliability is the most critical feature of any service. A common metric for reliability is , which conventionally means the amount of time a system has been up. However, we prefer a more helpful and precise metric: . still answers the question of whether a system is up but in a more precise way than by measuring the time since a system was down. In today's distributed systems, services can be partially down, a factor that uptime doesn't capture well.\nAvailability is often described in terms of \u2014such as 99.9% available (three nines), or 99.99% available (four nines). Measuring an availability SLO is one of the best ways to measure your system's reliability.\nIn addition to helping define operational success, an SLO can help you choose where to invest resources. For example, SRE books often note that each that you engineer for can result in [an incremental cost](https://landing.google.com/sre/sre-book/chapters/embracing-risk/) with [marginal utility](https://landing.google.com/sre/workbook/chapters/implementing-slos/) . It is generally recognized that achieving the next in availability costs you ten times as much as the preceding one.\n## Choose an SLI\nTo determine if an SLO is met (that is, successful), you need a measurement. That measurement is called the (SLI). An SLI measures the level of a particular service that you're delivering to your customer. Ideally, the SLI is tied to an accepted CUJ.\n### Select the best metrics\nThe first step in developing an SLI is to choose a metric to measure, such as requests per second, errors per second, queue length, the distribution of response codes during a given time period, or the number of bytes transmitted.\nSuch metrics tend to be of the following types:\n- **Counter.** For example, the number of errors that occurred up to a given point of measurement. This type of metric can increase but not decrease.\n- **Distribution.** For example, the number of events that populate a particular measurement segment for a given time period. You might measure how many requests take 0-10 ms to complete, how many take 11-30 ms, and how many take 31-100 ms. The result is a count for each bucket\u2014for example, [0-10: 50], [11-30: 220], [31-100: 1103].\n- **Gauge.** For example, the actual value of a measurable part of the system (such as queue length). This type of metric can increase or decrease.\nFor more information about these types, see the [Prometheus project documentation](https://prometheus.io/docs/concepts/metric_types/) and [the Cloud Monitoring metric types](/monitoring/api/v3/kinds-and-types) `ValueType` and `MetricKind` .\nAn important distinction about SLIs is that . In fact, the [SRE Workbook](https://landing.google.com/sre/workbook/chapters/implementing-slos/) states the following (emphasis added):\nMany software companies track hundreds or thousands of metrics; only a handful of metrics qualify as SLIs. So apart from being a ratio of good events to total events, what qualifies a metric as a good SLI? A good SLI metric has the following characteristics:\n- **The metric directly relates to user happiness.** Generally, users are unhappy if a service does not behave the way they expect it to, fails, or is slow. Any SLOs based on these metrics can be [validated](https://landing.google.com/sre/workbook/chapters/implementing-slos/#continuous-improvement-of-slo-targets) by comparing your SLI to other signals of user happiness\u2014for example, the number of customer complaint tickets, support call volume, social media sentiment, or escalations. If your metric doesn't correspond to these other metrics of user happiness, it might not be a good metric to use as an SLI.\n- **Metric deterioration correlates with outages.** A metric that looks good during an outage is the wrong metric for an SLI. A metric that looks bad during normal operation is also the wrong metric for an SLI.\n- **The metric provides a good signal-to-noise ratio.** Any metric that results in a large number of false negatives or false positives is not a good SLI.\n- **The metric scales monotonically, and approximately linearly, with\ncustomer happiness.** As the metric improves, customer happiness improves.\nConsider the graphs in the following diagram. Two metrics that might be used as SLIs for a service are graphed over time. The period when a service degrades is highlighted in red, and the period when a service is good is highlighted in blue.\nIn the case of the bad SLI, the user's unhappiness doesn't correspond directly with a negative event (such as service degradation, slowness, or an outage). Also, the SLI fluctuates independently of user happiness. With the good SLI, the SLI and user happiness correlate, the different happiness levels are clear, and there are far fewer irrelevant fluctuations.\n### Select the right number of metrics\nUsually, a single service has multiple SLIs, especially if the service performs different types of work or serves different types of users. For example, separating read requests from write requests is a good idea, as these requests tend to act in different ways. In this case, it is best to select metrics appropriate to each service.\nIn contrast, many services perform similar types of work across the service, which can be directly comparable. For example, if you have an online marketplace, users might view a homepage, view a subcategory or a top-10 list, view a details page, or search for items. Instead of developing and measuring a separate SLI for each of these actions, you might combine them into a single SLI category\u2014for example, .\nIn reality, the expectations of a user don't change much between actions of a similar category. Their happiness is not dependent on the structure of the data they are browsing, whether the data is derived from a static list of promoted items or is the dynamically generated result of a machine learning-assisted search across a massive dataset. Their happiness is quantifiable by an answer to a question: \"Did I see a full page of items quickly?\"\nIdeally, you want to use as few SLIs as possible to accurately represent the tolerances of a given service. Typically, a service should have between two and six SLIs. If you have too few SLIs, you can miss valuable signals. If you have too many SLIs, your on-call team has too much to track with only marginal added utility. Remember, SLIs should simplify your understanding of production health and provide a sense of coverage.\n## Choose an SLO\nAn SLO is composed of the following values:\n- **An SLI.** For example, the ratio of the number of responses with HTTP code`200`to the total number of responses.\n- **A duration.** The time period in which a metric is measured. This period can be calendar-based (for example, from the first day of one month to the first day of the next) or a rolling window (for example, the last 30 days).\n- **A target.** For example, a target percentage of good events to total events (such as 99.9%) that you expect to meet for a given duration.\nAs you develop an SLO, defining the duration and target can be difficult. One way to begin the process is to identify SLIs and chart them over time. If you can't decide what duration and target to use, remember that your SLO doesn't have to be perfect right away. You likely will iterate on your SLO to ensure that it aligns with customer happiness and meets your business needs. You might try starting with two (99.0%) for a month.\nAs you track SLO compliance during events such as deployments, outages, and daily traffic patterns, you can gain insights on what target is good, bad, or even tolerable. For example, in a background process, you might define 75% success as adequate. But for mission-critical, user-facing requests, you might aim for something more aggressive, like 99.95%.\nOf course, there isn't a single SLO that you can apply for every use case. SLOs depend on several factors:\n- customer expectations\n- workload type\n- infrastructure for serving, execution, and monitoring\n- the problem domain\nPart 2 in this series, [Adopt SLOs](/solutions/adopting-SLOs) , focuses on domain-independent SLOs. Domain-independent SLOs (such as service availability) do not replace high-level indicators (such as widgets sold per minute). However, they can help measure whether a service is working regardless of the business use case.\nDomain-independent indicators can often be reduced to a question\u2010for example, \"Is the service available?\" or \"Is the service fast enough?\" The answer is most often found in an SLO that accounts for two factors: availability and latency. You might describe an SLO in the following terms, where X = 99.9% and Y = 800 ms:\n## What's next?\n- Read [Adopt SLOs](/solutions/adopting-SLOs) , which explores these definitions in more detail and introduces other SLIs that might be more appropriate for various workload types.\n- Check out other SRE resources:- [The SRE Book](https://landing.google.com/sre/sre-book/chapters/embracing-risk/) \n- [The SRE Workbook](https://landing.google.com/sre/workbook/chapters/implementing-slos/) \n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .\n- Read our resources about [DevOps](/devops) .\n- Learn more about the DevOps capabilities related to this series:- [Monitoring and observability](/solutions/devops/devops-measurement-monitoring-and-observability) \n- [Monitoring systems to inform business decisions](/solutions/devops/devops-measurement-monitoring-systems) \n- [Proactive failure notification](/solutions/devops/devops-measurement-proactive-failure-notification) \n- Take the [DevOps quick check](https://www.devops-research.com/quickcheck.html) to understand where you stand in comparison with the rest of the industry.# Adopt SLOs\nThis document defines several service level objectives (SLOs) that are useful for different types of common service workloads. This document is Part 2 of two parts. Part 1, [Define SLOs](/solutions/defining-SLOs) , introduces SLOs, shows how SLOs are derived from service level indicators (SLIs), and describes what makes a good SLO.\nThe [State of DevOps](/devops) reports identified capabilities that drive software delivery performance. These two documents will help you with the following capabilities:\n- [Monitoring and observability](/solutions/devops/devops-measurement-monitoring-and-observability) \n- [Monitoring systems to inform business decisions](/solutions/devops/devops-measurement-monitoring-systems) \n- [Proactive failure notification](/solutions/devops/devops-measurement-proactive-failure-notification) ## What to measure\nRegardless of your [domain](https://wikipedia.org/wiki/Domain_(software_engineering)) , many services share common features and can use generic SLOs. The following discussion about generic SLOs is organized by service type and provides detailed explanations of SLIs that apply to each SLO.\n### Request-driven services\nA receives a request from a client (another service or a user), performs some computation, possibly sends network requests to a backend, and then returns a response to the client. Request-driven services are most often measured by availability and latency SLIs.\nThe SLI for availability indicates whether the service is working. The SLI for is defined as follows:You first have to define . Some basic definitions might be \"not zero-length\" or \"adheres to a client-server protocol,\" but it is up to a service owner to define what they mean by . A common method to gauge validity is to use an HTTP (or RPC) response code. For example, we often consider HTTP `500` errors to be server errors that count against an SLO, while `400` errors are client errors that do not.\nAfter you decide what to measure, you need to examine every response code returned by your system to ensure that the application uses those codes properly and consistently. When using error codes for SLOs, it's important to ask whether a code is an accurate indicator of your users' experience of your service. For example, if a user attempts to order an item that is out of stock, does the site break and return an error message, or does the site suggest similar products? For use with SLOs, error codes need to be tied to .\nDevelopers can misuse errors. In the case where a user asks for a product that is temporarily out of stock, a developer might mistakenly program an error to be returned. However, the system is actually functioning correctly and not in error. The code needs to return as a success, even though the user could not purchase the item they wanted. Of course, the owners of this service need to know that a product is out of stock, but the inability to make a sale is not an error from the customer's perspective and should not count against an SLO. However, if the service cannot connect to the database to determine if the item is in stock, that is an error that counts against your error budget.\nYour service might be more complex. For example, perhaps your service handles asynchronous requests or provides a long-running process for customers. In these cases, you might expose availability in another way. However, we recommend that you still represent as the proportion of valid requests that are successful. You might define availability as the number of minutes that a customer's workload is running as requested. (This approach is sometimes referred to as the \"good minutes\" method of measuring availability.) In the case of a virtual machine, you could measure availability in terms of the proportion of minutes after an initial request for a VM that the VM is accessible through SSH.\nThe SLI for latency (sometimes called ) indicates whether the service is fast enough. The SLI for is defined similarly to :You can measure latency by calculating the difference between when a timer starts and when it stops for a given request type. The key is a user's perception of latency. A common pitfall is to be too precise in measuring latency. In reality, users [cannot distinguish](https://developers.google.com/web/fundamentals/performance/rail#ux) between a 100-millisecond (ms) and a 300-ms refresh and might accept any point between 300 ms and 1000 ms.\nInstead, it's a good idea to develop activity-centric metrics that keep the user in focus, for example, in the following processes:\n- **Interactive:** 1000 ms for the time that a user waits for a result after clicking an element.\n- **Write:** 1500 ms for changing an underlying distributed system. While this length of time is considered slow for a system, users tend to accept it. We recommend that you explicitly distinguish between writes and reads in your metrics.\n- **Background:** 5000 ms for an action that is not user-visible, like a periodic refresh of data or other asynchronous requests.\nLatency is commonly measured as a [distribution](https://landing.google.com/sre/sre-book/chapters/service-level-objectives/#fig_sl-star_latency-distribution) (see [Choosing an SLI](/solutions/defining-SLOs#choose_an_sli) in Part 1 of this series). Given a distribution, you can measure various percentiles. For example, you might measure the number of requests that are slower than the historical 99th percentile. In this case, we consider good events to be events that are faster than this threshold, which was set by examining the historical distribution. You can also set this threshold based on product requirements. You can even set multiple latency SLOs, for example typical latency versus tail latency.\nWe recommend that you do not use only the average (or median) latency as your SLI. Discovering that the median latency is too slow means that half your users are already unhappy. In other words, you can have bad latency for days before you discover a real threat to your long-term error budget. Therefore, we recommend that you define your SLO for (95th percentile) and for (50th percentile).\nIn the ACM article [Metrics That Matter](https://cacm.acm.org/magazines/2019/4/235621-metrics-that-matter/fulltext) , Benjamin Treynor Sloss writes the following:Treynor Sloss continues:A good model to follow is to determine your latency thresholds based on historical percentiles, then measure how many requests fall into each bucket. For more details, see the section on latency alerts later in this document.\nQuality is a helpful SLI for complex services that are designed to fail gracefully by degrading when dependencies are slow or unavailable. The SLI for is defined as follows:For example, a web page might load its main content from one datastore and load ancillary, optional assets from 100 other services and datastores. If one optional service is out of service or too slow, the page can still be rendered without the ancillary elements. By measuring the number of requests that are served a degraded response (that is, a response missing at least one backend service's response), you can report the ratio of requests that were bad. You might even track how many responses to the user were missing a response from a single backend, or were missing responses from multiple backends.\n### Data processing services\nSome services are not built to respond to user requests but instead consume data from an input, process that data, and generate an output. How these services perform at intermediate steps is not as important as the final result. With services like these, your strongest SLIs are , , , and , not and .\nThe SLI for is defined as follows:In batch processing systems, for example, freshness can be measured as the time elapsed since a processing run completed successfully for a given output. In more complex or real-time processing systems, you might track the age of the most-recent record processed in a pipeline.\nFor example, consider an online game that generates map tiles in real time. Users might not notice how quickly map tiles are created, but they might notice when map data is missing or is not fresh.\nOr, consider a system that reads records from an in-stock tracking system to generate the message \"X items in stock\" for an ecommerce website. You might define the SLI for as follows:You can also use a metric for serving non-fresh data to inform the [SLI for quality](#sli-quality) .\nThe SLI for is defined as follows:To define coverage, you first determine whether to accept an input as valid or to skip it. For example, if an input record is corrupted or zero-length and cannot be processed, you might consider that record as invalid for measuring your system.\nNext, you count the number of your valid records. You might do this step with a simple `count()` method or another method. This number is your total record count.\nFinally, to generate your SLI for coverage, you count the number of records that processed successfully and compare that number against the total valid record count.\nThe SLI for is defined as follows:In some cases, there are methods of determining the correctness of an output that can be used to validate the processing of the output. For example, a system that rotates or colorizes an image should never produce a zero-byte image, or an image with a length or width of zero. It is important to separate this validation logic from the processing logic itself.\nOne method of measuring a correctness SLI is to use known-good test input data, which is data that has a known correct output. The input data needs to be representative of user data. In other cases, it is possible that a mathematical or logical check might be made against the output, like in the preceding example of rotating an image. Another example might be a billing system that determines if a transaction is valid by checking whether the difference between the balance before the transaction and the balance after the transaction matches the value of the transaction itself.\nThe SLI for is defined as follows:In a data processing system, throughput is often more representative of user happiness than, for example, a single latency measurement for a given piece of work. For example, if the size of each input varies dramatically, it might not make sense to compare how long each element takes to finish if a job progresses at an acceptable rate.\nis a common way to measure the amount of work it takes to process data regardless of the size of a dataset. But any metric that roughly scales linearly with respect to the cost of processing can work.\nIt might be worthwhile to partition your data processing systems based upon expected throughput rates, or implement a [quality of service](https://wikipedia.org/wiki/Quality_of_service) system to ensure that high-priority inputs are handled and low-priority inputs are queued. Either way, measuring throughput as defined in this section can help you determine if your system is working as expected.\n### Scheduled execution services\nFor services that need to perform an action at a regular interval, such as Kubernetes cron jobs, you can measure and . The following is a sample scheduled Kubernetes cron job:\n```\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n name: hello\nspec:\n schedule: \"0 * * * *\"\n```\nAs an SLI, is defined as follows:Skew measures the time difference between when a job is scheduled to start and when it does start. For example, if the preceding Kubernetes cron job, which is set up to start at minute zero of every hour, starts at three minutes past the hour, then the skew is three minutes. When a job runs early, you have a negative skew.\nYou can measure skew as a distribution over time, with corresponding acceptable ranges that define good skew. To determine the SLI, you would compare the number of runs that were within a good range.\nAs an SLI, is defined as follows:is the time a job takes to complete. For a given execution, a common failure mode is for actual duration to exceed scheduled duration.\nOne interesting case is how to apply this SLI to catch a never-ending job. Because these jobs don't finish, you need to record the time spent on a given job instead of waiting for a job to complete. This approach provides an accurate distribution of how long work takes to complete, even in worst-case scenarios.\nAs with skew, you can track execution duration as a distribution and define acceptable upper and lower bounds for good events.\n### Types of metrics for other systems\nMany other workloads have their own metrics that you can use to generate SLIs and SLOs. Consider the following examples:\n- **Storage systems:** durability, throughput, time to first byte, blob availability\n- **Media/video:** client playback continuity, time to start playback, transcode graph execution completeness\n- **Gaming:** time to match active players, time to generate a map## How to measure\nAfter you know you're measuring, you can decide to take the measurement. You can gather your SLIs in several ways.\n### Server-side logging\nProcessing server-side logs of requests or processed data.\nAdvantages:\n- Existing logs can be reprocessed to backfill historical SLI records.\n- Cross-service session identifiers can reconstruct complex user journeys across multiple services.\nDisadvantages:\n- Requests that do not arrive at the server are not recorded.\n- Requests that cause a server to crash might not be recorded.\n- Length of time to process logs can result in stale SLIs, which might be inadequate data for an operational response.\n- Writing code to process logs can be an error-prone, time- consuming task.\nImplementation methods and tools:\n- [BigQuery](/bigquery) \n- [Dataflow](/dataflow) , [Apache Spark](https://spark.apache.org/) \n- [Splunk](https://www.splunk.com/) \n### Application server metrics\nExporting SLI metrics from the code that serves requests from users or processes their data.\nAdvantage:\n- Adding new metrics to code is typically fast and inexpensive.\nDisadvantages:\n- Requests that do not arrive to application servers are not recorded.\n- Multi-service requests might be hard to track.\nImplementation methods and tools:\n- [Cloud Monitoring](/monitoring) \n- [Prometheus](https://prometheus.io/) \n- Third-party APM products\n### Frontend infrastructure metrics\nUtilizing metrics from the load-balancing infrastructure (for example, Google Cloud's global Layer 7 load balancer).\nAdvantages:\n- Metrics and historical data often already exist, thus reducing the engineering effort to get started.\n- Measurements are taken at the point nearest the customer yet still within the serving infrastructure.\nDisadvantages:\n- Not viable for data processing SLIs.\n- Can only approximate multi-request user journeys.\nImplementation methods and tools:\n- [Cloud Monitoring](/monitoring) \n- CDN/LB metrics or reporting APIs ( [Cloudflare](https://www.cloudflare.com/) , [Akamai](https://www.akamai.com/) , [Fastly](https://www.fastly.com/) )\n### Synthetic clients or data\nBuilding a client that sends fabricated requests at regular intervals and validates the responses. For data processing pipelines, creating synthetic known-good input data and validating outputs.\nAdvantages:\n- Measures all steps of a multi-request user journey.\n- Sending requests from outside your infrastructure captures more of the overall request path in the SLI.\nDisadvantages:\n- Approximates user experience with synthetic requests, which might be misleading (both false positives or false negatives).\n- Covering all corner cases is hard and can devolve into integration testing.\n- High reliability targets require frequent probing for accurate measurement.\n- Probe traffic can drown out real traffic.\nImplementation methods and tools:\n- [Cloud Monitoring uptime checks](/monitoring/uptime-checks) \n- Open source solutions:- [Cloudprober](https://cloudprober.org) \n- [blackbox_exporter](https://github.com/prometheus/blackbox_exporter) \n- Vendor solutions:- [Catchpoint Synthetic Monitoring](https://www.catchpoint.com/synthetic-monitoring) \n- [New Relic Synthetics](https://newrelic.com/products/synthetics) \n- [Datadog Synthetic Monitoring](https://www.datadoghq.com/synthetics/) \n### Client instrumentation\nAdding observability features to the client that the user interacts with, and logging events back to your serving infrastructure that tracks SLIs.\nAdvantages:\n- Provides the most accurate measure of user experience.\n- Can quantify reliability of third parties, for example, CDN or payments providers.\nDisadvantages:\n- Client logs ingestion and processing latency make these SLIs unsuitable for triggering an operational response.\n- SLI measurements will contain a number of highly variable factors potentially outside of direct control.\n- Building instrumentation into the client can involve lots of engineering work.\nImplementation methods and tools:\n- [Google Analytics](https://developers.google.com/analytics/) , [Firebase](https://firebase.google.com/) \n- User analytics: [Amplitude](https://amplitude.com/) and similar tools\n- Mobile/frontend monitoring: [New Relic Browser](https://newrelic.com/products/browser-monitoring) , [Raygun Real User Monitoring](https://raygun.com/platform/real-user-monitoring) , [Sentry](https://sentry.io/) \n- Custom client and custom server to record and track## Choose a measurement method\nIdeally, you need to choose a measurement method that most closely aligns with your customer's experience of your service and demands the least effort on your part. To achieve this ideal, you might need to use a combination of the methods in the preceding tables. Here is a suggested approach that you can implement over time, listed in order of increasing effort:\n- **Using application server exports and infrastructure metrics.** Typically, you can access these metrics immediately, and they quickly provide value. Some APM tools include built-in SLO tooling.\n- **Using client instrumentation.** Because legacy systems typically lack built-in, end-user client instrumentation, setting up instrumentation might require a significant investment. However, if you use an APM suite or frontend framework that provides client instrumentation, you can quickly gain insight into your customer's happiness.\n- **Using logs processing.** If you cannot implement server exports or client instrumentation but logs exist, you might find logs processing to be your best value. Another approach is to combine exports and logs processing, using exports as an immediate source for some SLIs (such as immediate availability) and logs processing for long-term signals (such as slow-burn alerts discussed later in the [SLOs and Alert](/architecture/framework/reliability/slo-and-alerts) ) guide.\n- **Implementing synthetic testing.** After you have a basic understanding of how your customers use your service, you test your service level. For example, you can seed test accounts with known-good data and query for it. This testing can help highlight failure modes that aren't easily observed, such as in the case of low-traffic services.## Set your objectives\nOne of the best ways to set objectives is to create a shared document that describes your SLOs and how you developed them. Your team can iterate on the document as it implements and iterates on the SLOs over time.\nWe recommend that business owners, product owners, and executives review this document. Those stakeholders can offer insights about service expectations and your product's reliability tradeoffs.\nFor your company's most important critical user journeys (CUJs), here is a template for developing an SLO:\n- Choose an SLI specification (for example, availability or freshness).\n- Define how to implement the SLI specification.\n- Read through your plan to ensure that your CUJs are covered.\n- Set SLOs based on past performance or business needs.\nCUJs should be constrained to a single service, nor to a single development team or organization. If your users depend on hundreds of microservices that operate at 99.5% yet nobody tracks end-to-end availability, your customer is likely not happy.\nSuppose that you have a query that depends on five services that work in sequence: a load balancer, a frontend, a mixer, a backend, and a database.\nIf each component has a 99.5% availability, the worst-case user-facing availability is as follows:This is the worst-case user-facing availability because the overall system fails if any one of the five services fails. This would only be true if all layers of the stack must always be immediately available to handle each user request, without any resilience factors such as intermediate retries, caches, or queues. A system with such tight coupling between services is a bad design and defies the microservices model.\nSimply measuring performance against the SLO of a distributed system in this piecemeal manner (service by service) doesn't accurately reflect your customer's experience and might result in an overly sensitive interpretation.\nInstead, you should measure performance against the SLO at the frontend to understand what users experience. The user does not care if a component service fails, causing a query to be automatically and successfully retried, if the user's query still succeeds. If you have shared internal services, these services can separately measure performance against their SLOs, with the user-facing services acting as their customers. You should handle these SLOs separately from each other.\nIt is possible to build a highly available service (for example, 99.99%) on top of a less-available service (for example, 99.9%) by using resilience factors such as smart retries, caching, and queueing.\nAs a general rule, anyone with a working knowledge of statistics should be able to read and understand your SLO without understanding your underlying service or [organizational layout](https://wikipedia.org/wiki/Conway%27s_law) .\n### Example SLO worksheet\nWhen you develop your SLO, remember to do the following:\n- Make sure that your SLIs specify an event, a success criterion, and where and how you record success or failure.\n- Define the SLI specification in terms of the proportion of events that are good.\n- Make sure that your SLO specifies both a target level and a measurement window.\n- Describe the advantages and disadvantages of your approach so that interested parties understand the tradeoffs and subtleties involved.\nFor example, consider the following SLO worksheet.\n| CUJ: Home page load                                                                                                                                                                           |\n|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| SLI type: Latency SLI specification: Proportion of home page requests served in less than 100 ms SLI implementations: Proportion of home page requests served in less than 100 ms as measured from the latency column of the server log. (Disadvantage: This measurement misses requests that fail to reach the backend.) Proportion of home page requests served in less than 100 ms as measured by probers that execute JavaScript in a browser running in a virtual machine. (Advantages and disadvantages: This measurement catches errors when requests cannot reach the network but might miss issues affecting only a subset of users.) SLO: 99% of home page requests in the past 28 days served in less than 100 ms |\n**Note:** If you can't agree as a team on a target level, choose some level and agree to revisit it in six months. Picking the wrong number is better than picking no number.\n## What's next?\n- Read [The Art of SLOs](https://landing.google.com/sre/resources/practicesandprocesses/art-of-slos/) , a workshop developed by Google's [Customer Reliability Engineering](/blog/products/gcp/introducing-a-new-era-of-customer-support-google-customer-reliability-engineering) team.\n- Try [Site Reliability Engineering: Measuring and Managing Reliability](https://www.coursera.org/learn/site-reliability-engineering-slos) , an online course about building SLOs.\n- Read [Site Reliability Engineering: Implementing SLOs](https://landing.google.com/sre/workbook/chapters/implementing-slos/) .\n- Read [Concepts in service monitoring](/monitoring/service-monitoring) .\n- Read about [developing SLOs with Cloud Monitoring](https://medium.com/google-cloud/slos-with-stackdriver-service-monitoring-62f193147b3f) .\n- Try out the flexible [SLO Generator](https://github.com/google/slo-generator) from Google's Professional Services Organization (PSO).\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .\n- Read our resources about [DevOps](/devops) .\n- Learn more about the DevOps capabilities related to this series:- [Monitoring and observability](/solutions/devops/devops-measurement-monitoring-and-observability) \n- [Monitoring systems to inform business decisions](/solutions/devops/devops-measurement-monitoring-systems) \n- [Proactive failure notification](/solutions/devops/devops-measurement-proactive-failure-notification) \n- Take the [DevOps quick check](https://dora.dev/quickcheck/) to understand where you stand in comparison with the rest of the industry.# Terminology\nThis document provides common definitions for SLO-related terms used in the [Google CloudArchitecture Framework: Reliability](/architecture/framework) section.\n- **service level:** a measurement of how well a given service performs its expected work for the user. You can describe this measurement in terms of and measure it by various methods, depending on what the service does and what the user expects it to do or is told it can do.Example: \"A user expects our service to be available and fast.\"\n- **critical user journey (CUJ):** a set of interactions a user has with a service to achieve a single goal\u2014for example, a single click or a multi-step pipeline.Example: \"A user clicks the checkout button and waits for the response that the cart is processed and a receipt is returned.\"\n- **service level indicator (SLI):** a gauge of user happiness that can be measured quantitatively for a service level. In other words, to measure a service level, you must measure an indicator that represents user happiness with that service level\u2014for example, a service's availability. An SLI can be thought of as a line on a graph that changes over time, as the service improves or degrades. This tends to be a radio of \"good\" / \"total\" expressed as a unit-less percentage. By consistently using these percentages, teams can understand the SLI without deep knowledge of its implementationExample: \"Measure the number of successful requests in the last 10 minutes divided by the number of all valid requests in the last 10 minutes.\"\n- **service level objective (SLO):** the level that you expect a service to achieve most of the time and against which an SLI is measured.Example: \"Service responses are be faster than 400 milliseconds (ms) for 95% of all valid requests measured over 14 days.\"\n- **service level agreement (SLA):** a description of what must happen if an SLO is not met. Generally, an SLA is a legal agreement between providers and customers and might even include terms of compensation. In technical discussions about SRE, this term is often avoided.Example: \"If the service does not provide 99.95% availability over a calendar month, the service provider compensates the customer for every minute out of compliance.\"\n- **error budget:** how much time or how many negative events you can withstand before you violate your SLO. This measurement tells you how many errors your business can expect or tolerate. The error budget is critical in [helping you make potentially risky decisions](https://landing.google.com/sre/sre-book/chapters/embracing-risk/) .Example: \"If our SLO is 99.9% available, we allow 0.1% of our requests to serve errors, either through incidents, accidents, or experimentation.\"\n# SLOs and alerts\nThis document in the [Google Cloud Architecture Framework: Reliability](/architecture/framework) section provides details about alerting around SLOs.\nA mistaken approach to introducing a new observability system like SLOs is to use the system to completely replace an earlier system. Rather, you should see SLOs as a complementary system. For example, instead of deleting your existing alerts, we recommend that you run them in parallel with the SLO alerts introduced here. This approach lets you discover which legacy alerts are predictive of SLO alerts, which alerts fire in parallel with your SLO alerts, and which alerts never fire.\nA tenet of SRE is to [alert based on symptoms, not on causes](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/) . SLOs are, by their very nature, measurements of symptoms. As you adopt SLO alerts, you might find that the symptom alert fires alongside other alerts. If you discover that your legacy, cause-based alerts fire with no SLO or symptoms, these are good candidates to be turned off entirely, turned into [ticketing alerts](https://landing.google.com/sre/sre-book/chapters/dealing-with-interrupts/) , or logged for later reference.\nFor more information, see [SRE Workbook, Chapter 5](https://landing.google.com/sre/workbook/chapters/alerting-on-slos/) .\n### SLO burn rate\nAn SLO's is a measurement of how quickly an outage exposes users to errors and depletes the error budget. By measuring your burn rate, you can determine the time until a service violates its SLO. Alerting based on the SLO burn rate is a valuable approach. Remember that your SLO is based on a duration, which might be quite long (weeks or even months). However, the goal is to quickly detect a condition that results in an SLO violation before that violation actually occurs.\nThe following table shows the time it takes to exceed an objective if 100% of requests are failing for the given interval, assuming queries per second (QPS) is constant. For example, if you have a 99.9% SLO measured over 30 days, you can withstand 43.2 minutes of full downtime during that 30 days. For example, that downtime can occur all at once, or spaced over several incidents.\n| Objective | 90 days  | 30 days  | 7 days  | 1 day  |\n|:------------|:------------|:-------------|:-------------|:-------------|\n| 90%   | 9 days  | 3 days  | 16.8 hours | 2.4 hours |\n| 99%   | 21.6 hours | 7.2 hours | 1.7 hours | 14.4 minutes |\n| 99.9%  | 2.2 hours | 43.2 minutes | 10.1 minutes | 1.4 minutes |\n| 99.99%  | 13 minutes | 4.3 minutes | 1 minute  | 8.6 seconds |\n| 99.999%  | 1.3 minutes | 25.9 seconds | 6 seconds | 0.9 seconds |\nIn practice, you cannot afford any 100%-outage incidents if you want to achieve high-success percentages. However, many distributed systems can partially fail or degrade gracefully. Even in those cases, you still want to know if a human needs to step in, even in such partial failures, and SLO alerts give you a way to determine that.\n### When to alert\nAn important question is when to act based on your SLO burn rate. As a rule, if you will exhaust your error budget in 24 hours, the time to page someone to fix an issue is .\nMeasuring the rate of failure isn't always straightforward. A series of small errors might look terrifying in the moment but turn out to be short-lived and have an inconsequential impact on your SLO. Similarly, if a system is slightly broken for a long time, these errors can add up to an SLO violation.\nIdeally, your team will react to these signals so that you spend almost all of your error budget (but not exceed it) for a given time period. If you spend too much, you violate your SLO. If you spend too little, you're not taking enough risk or possibly burning out your on-call team.\nYou need a way to determine when a system is broken enough that a human should intervene. The following sections discuss some approaches to that question.\n### Fast burns\nOne type of SLO burn is a because it burns through your error budget quickly and demands that you intervene to avoid an SLO violation.\nSuppose your service operates normally at 1000 queries per second (QPS), and you want to maintain 99% availability as measured over a seven-day week. Your error budget is about 6 million allowable errors (out of about 600 million requests). If you have 24 hours before your error budget is exhausted, for example, that gives you a limit of about 70 errors per second, or 252,000 errors in one hour. These parameters are based on the general rule that pageable incidents should consume at least 1% of the quarterly error budget.\nYou can choose to detect this rate of errors before that one hour has elapsed. For example, after observing 15 minutes of a 70-error-per-second rate, you might decide to page the on-call engineer, as the following diagram shows.\nIdeally, the problem is solved before you expend one hour of your 24-hour budget. Choosing to detect this rate in a shorter window (for example, one minute) is likely to be too error-prone. If your target time to detect is shorter than 15 minutes, this number can be adjusted.\n### Slow burns\nAnother type of burn rate is a . Suppose you introduce a bug that burns your weekly error budget by day five or six, or your monthly budget by week two? What is the best response?\nIn this case, you might introduce a alert that lets you know you're on course to consume your entire error budget before the end of the alerting window. Of course, that alert might return many false positives. For example, there might often be a condition where errors occur briefly but at a rate that would quickly consume your error budget. In these cases, the condition is a false positive because it lasts only a short time and does not threaten your error budget in the long term. Remember, the goal is not to eliminate all sources of error; it is to . You want to avoid alerting a human to intervene for events that are not legitimately threatening your error budget.\nWe recommend that you notify a ticket queue (as opposed to paging or emailing) for slow-burn events. Slow-burn events are not emergencies but do require human attention before the budget expires. These alerts shouldn't be emails to a team list, which quickly become a nuisance to be ignored. Tickets should be trackable, assignable, and transferrable. Teams should develop reports for ticket load, closure rates, actionability, and duplicates. Excessive, unactionable tickets are a great example of [toil](https://landing.google.com/sre/sre-book/chapters/eliminating-toil/) .\nUsing SLO alerts skillfully can take time and depend on your team's culture and expectations. Remember that you can fine-tune your SLO alerts over time. You can also have multiple alert methods, with varying alert windows, depending on your needs.\n### Latency alerts\nIn addition to availability alerts, you can also have latency alerts. With latency SLOs, you're measuring the percent of requests that are not meeting a latency target. By using this model, you can use the same alerting model that you use to detect fast or slow burns of your error budget.\nAs noted earlier about median latency SLOs, fully half your requests can be out of SLO. In other words, your users can suffer bad latency for days before you detect the impact on your long-term error budget. Instead, services should define and objectives. We suggest using the historical 90th percentile to define typical and the 99th percentile for tail. After you set these targets, you can define SLOs based on the number of requests you expect to land in each latency category and how many are too slow. This approach is the same concept as an error budget and should be treated the same. Thus, you might end up with a statement like \"90% of requests will be handled within typical latency and 99.9% within tail latency targets.\" These targets ensure that most users experience your typical latency and still let you track how many requests are slower than your tail latency targets.\nSome services might have highly variant expected runtimes. For example, you might have dramatically different performance expectations for reading from a datastore system versus writing to it. Instead of enumerating every possible expectation, you can introduce runtime performance buckets, as the following tables show. This approach presumes that these types of requests are identifiable and pre-categorized into each bucket. You shouldn't expect to categorize requests on the fly.\n| User-facing website | Unnamed: 1    |\n|:----------------------|:-------------------------|\n| Bucket    | Expected maximum runtime |\n| Read     | 1 second     |\n| Write / update  | 3 seconds    |\n| Data processing systems | Unnamed: 1    |\n|:--------------------------|:-------------------------|\n| Bucket     | Expected maximum runtime |\n| Small      | 10 seconds    |\n| Medium     | 1 minute     |\n| Large      | 5 minutes    |\n| Giant      | 1 hour     |\n| Enormous     | 8 hours     |\nBy measuring the system as it is today, you can understand how long these requests typically take to run. As an example, consider a system for processing video uploads. If the video is very long, the processing time should be expected to take longer. We can use the length of the video in seconds to categorize this work into a bucket, as the following table shows. The table records the number of requests per bucket as well as various percentiles for runtime distribution over the course of a week.\n| Video length | Number of requests measured in one week | 10%    | 90%   | 99.95%  |\n|:---------------|:------------------------------------------|:-----------------|:------------|:-------------|\n| Small   | 0           | -    | -   | -   |\n| Medium   | 1.9 million        | 864 milliseconds | 17 seconds | 86 seconds |\n| Large   | 25 million        | 1.8 seconds  | 52 seconds | 9.6 minutes |\n| Giant   | 4.3 million        | 2 seconds  | 43 seconds | 23.8 minutes |\n| Enormous  | 81000          | 36 seconds  | 1.2 minutes | 41 minutes |\nFrom such analysis, you can derive a few parameters for alerting:\n- : At most, 10% of requests are faster than this time. If too many requests are faster than this time, your targets might be wrong, or something about your system might have changed.\n- : At least 90% of requests are faster than this time. This limit drives your main latency SLO. This parameter indicates whether most of the requests are fast enough.\n- : At least 99.95% of requests are faster than this time. This limit ensures that there aren't too many slow requests.\n- : The point at which a user RPC or background processing times out and fails (a limit typically already hard-coded into the system). These requests won't actually be slow but will have actually failed with an error and instead count against your availability SLO.\nA guideline in defining buckets is to keep a bucket's , , and within an order of magnitude of each other. This guideline ensures that you don't have too broad of a bucket. We recommend that you don't attempt to prevent overlap or gaps between the buckets.\n| Bucket | fast_typical  | slow_typical | slow_tail    | deadline |\n|:---------|:-----------------|:---------------|:------------------------|:------------|\n| Small | 100 milliseconds | 1 second  | 10 seconds    | 30 seconds |\n| Medium | 600 milliseconds | 6 seconds  | 60 seconds (1 minute) | 300 seconds |\n| Large | 3 seconds  | 30 seconds  | 300 seconds (5 minutes) | 10 minutes |\n| Giant | 30 seconds  | 6 minutes  | 60 minutes (1 hour)  | 3 hours  |\n| Enormous | 5 minutes  | 50 minutes  | 500 minutes (8 hours) | 12 hours |\nThis results in a rule like `api.method: SMALL => [` ` **1s, 10s** ` `]` . In this case, the SLO tracking system would see a request, determine its bucket (perhaps by analysing its method name or URI and comparing the name to a lookup table), then update the statistic based on the runtime of that request. If this took 700 milliseconds, it is within the target. If it is 3 seconds, it is within . If it is 22 seconds, it is beyond , but not yet an error.\nIn terms of user happiness, you can think of missing tail latency as equivalent to being unavailable. (That is, the response is so slow that it should be considered a failure.) Due to this, we suggest using the same percentage that you use for availability, for example:What you consider typical latency is up to you. Some teams within Google consider 90% to be a good target. This is related to your analysis and how you chose durations for . For example:### Suggested alerts\nGiven these guidelines, the following table includes a suggested baseline set of SLO alerts.\n| SLOs                  | Measurement window | Burn rate        | Action   |\n|:---------------------------------------------------------------------------|:---------------------|:---------------------------------------|:----------------|\n| Availability, fast burn Typical latency Tail latency      | 1-hour window  | Less than 24 hours to SLO violation | Page someone |\n| Availability, slow burn Typical latency, slow burn Tail latency, slow burn | 7-day window   | Greater than 24 hours to SLO violation | Create a ticket |\nSLO alerting is a skill that can take time to develop. The durations in this section are suggestions; you can adjust these according to your own needs and level of precision. Tying your alerts to the measurement window or error budget expenditure might be helpful, or you might add another layer of alerting between fast burns and slow burns.\n# Build observability into your infrastructure and applications\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices to add observability into your services so that you can better understand your service performance and quickly identify issues. Observability includes monitoring, logging, tracing, profiling, debugging, and similar systems.\nMonitoring is at the base of the [service reliability hierarchy in the Google SRE Handbook](https://sre.google/sre-book/part-III-practices/) . Without proper monitoring, you can't tell whether an application works correctly.\n## Instrument your code to maximize observability\nA well-designed system aims to have the right amount of observability that starts in its development phase. Don't wait until an application is in production before you start to observe it. Instrument your code and consider the following guidance:\n- To debug and troubleshoot efficiently, think about what log and trace entries to write out, and what metrics to monitor and export. Prioritize by the most likely or frequent failure modes of the system.\n- Periodically audit and prune your monitoring. Delete unused or useless dashboards, graphs, alerts, tracing, and logging to eliminate clutter.\n[Google Cloud Observability](/products/operations) provides real-time monitoring, [hybrid multi-cloud monitoring and logging](/architecture/hybrid-and-multi-cloud-monitoring-and-logging-patterns) (such as for AWS and Azure), plus tracing, profiling, and debugging. Google Cloud Observability can also [auto-discover and monitor microservices](/stackdriver/docs/solutions/slo-monitoring/microservices) running on App Engine or in a service mesh like Istio.\nIf you generate lots of application data, you can [optimize large-scale ingestion of analytics events logs with BigQuery](/architecture/optimized-large-scale-analytics-ingestion) . BigQuery is also suitable for persisting and analyzing high-cardinality timeseries data from your monitoring framework. This approach is useful because it lets you run arbitrary queries at a lower cost rather than trying to design your monitoring perfectly from the start, and decouples reporting from monitoring. You can create reports from the data using [Looker Studio](https://lookerstudio.google.com/overview) or [Looker](/looker) .\n## Recommendations\nTo apply the guidance in the Architecture Framework to your own environment, follow these recommendations:\n- Implement monitoring early, such as before you initiate a migration or before you deploy a new application to a production environment.\n- Disambiguate between application issues and underlying cloud issues. Use the [Monitoring API](/apis/docs/monitoring) , or other [Cloud Monitoring](/monitoring/api/metrics_gcp) products and the [Google Cloud Status Dashboard](https://status.cloud.google.com/) .\n- Define an observability strategy beyond monitoring that includes tracing, profiling, and debugging.\n- Regularly clean up observability artifacts that you don't use or that don't provide value, such as unactionable alerts.\n- If you generate large amounts of observability data, send application events to a data warehouse system such as BigQuery.## What's next\n- [Design for scale and high availability](/architecture/framework/reliability/design-scale-high-availability) (next document in this series)\nExplore other categories in the [Architecture Framework](/architecture/framework) such as system design, operational excellence, security, privacy, and compliance.# Design for scale and high availability\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides design principles to architect your services so that they can tolerate failures and scale in response to customer demand. A reliable service continues to respond to customer requests when there's a high demand on the service or when there's a maintenance event. The following reliability design principles and best practices should be part of your system architecture and deployment plan.\n## Create redundancy for higher availability\nSystems with high reliability needs must have no single points of failure, and their resources must be replicated across multiple failure domains. A failure domain is a pool of resources that can fail independently, such as a VM instance, zone, or region. When you replicate across failure domains, you get a higher aggregate level of availability than individual instances could achieve. For more information, see [Regions and zones](/compute/docs/regions-zones) .\nAs a specific example of redundancy that might be part of your system architecture, to isolate failures in DNS registration to individual zones, use [zonal DNS names](/compute/docs/networking/zonal-dns) for instances on the same network to access each other.\n## Design a multi-zone architecture with failover for high availability\nMake your application resilient to zonal failures by architecting it to use pools of resources distributed across multiple zones, with data replication, load balancing and automated failover between zones. Run zonal replicas of every layer of the application stack, and eliminate all cross-zone dependencies in the architecture.\n## Replicate data across regions for disaster recovery\nReplicate or archive data to a remote region to enable disaster recovery in the event of a regional outage or data loss. When replication is used, recovery is quicker because storage systems in the remote region already have data that is almost up to date, aside from the possible loss of a small amount of data due to replication delay. When you use periodic archiving instead of continuous replication, disaster recovery involves restoring data from backups or archives in a new region. This procedure usually results in longer service downtime than activating a continuously updated database replica and could involve more data loss due to the time gap between consecutive backup operations. Whichever approach is used, the entire application stack must be redeployed and started up in the new region, and the service will be unavailable while this is happening.\nFor a detailed discussion of disaster recovery concepts and techniques, see [Architecting disaster recovery for cloud infrastructure outages](https://cloud.google.com/architecture/disaster-recovery) .\n## Design a multi-region architecture for resilience to regional outages\nIf your service needs to run continuously even in the rare case when an entire region fails, design it to use pools of compute resources distributed across different regions. Run regional replicas of every layer of the application stack.\nUse data replication across regions and automatic failover when a region goes down. Some Google Cloud services have multi-regional variants, such as [Spanner](/spanner) . To be resilient against regional failures, use these multi-regional services in your design where possible. For more information on regions and service availability, see [Google Cloud locations](/about/locations) .\nMake sure that there are no cross-region dependencies so that the breadth of impact of a region-level failure is limited to that region.\nEliminate regional single points of failure, such as a single-region primary database that might cause a global outage when it is unreachable. Note that multi-region architectures often cost more, so consider the business need versus the cost before you adopt this approach.\nFor further guidance on implementing redundancy across failure domains, see the survey paper [Deployment Archetypes for Cloud Applications (PDF)](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/cd6b7106c4decf552edc20c125dcb587c4cdcba9.pdf) .\n## Eliminate scalability bottlenecks\nIdentify system components that can't grow beyond the resource limits of a single VM or a single zone. Some applications scale vertically, where you add more CPU cores, memory, or network bandwidth on a single VM instance to handle the increase in load. These applications have hard limits on their scalability, and you must often manually configure them to handle growth.\nIf possible, redesign these components to scale horizontally such as with sharding, or partitioning, across VMs or zones. To handle growth in traffic or usage, you add more shards. Use standard VM types that can be added automatically to handle increases in per-shard load. For more information, see [Patterns for scalable and resilient apps](/architecture/scalable-and-resilient-apps) .\nIf you can't redesign the application, you can replace components managed by you with fully managed cloud services that are designed to scale horizontally with no user action.\n## Degrade service levels gracefully when overloaded\nDesign your services to tolerate overload. Services should detect overload and return lower quality responses to the user or partially drop traffic, not fail completely under overload.\nFor example, a service can respond to user requests with static web pages and temporarily disable dynamic behavior that's more expensive to process. This behavior is detailed in the [warm failover pattern from Compute Engine to Cloud Storage](/architecture/warm-recoverable-static-site-failover-load-balancer) . Or, the service can allow read-only operations and temporarily disable data updates.\nOperators should be notified to correct the error condition when a service degrades.\n## Prevent and mitigate traffic spikes\nDon't synchronize requests across clients. Too many clients that send traffic at the same instant causes traffic spikes that might cause cascading failures.\nImplement spike mitigation strategies on the server side such as throttling, [queueing](https://sre.google/sre-book/addressing-cascading-failures/#xref_cascading-failure_queue-management) , [load shedding](https://sre.google/sre-book/addressing-cascading-failures/#xref_cascading-failure_load-shed-graceful-degredation) or [circuit breaking](https://martinfowler.com/bliki/CircuitBreaker.html) , [graceful degradation](https://sre.google/sre-book/addressing-cascading-failures/#xref_cascading-failure_load-shed-graceful-degredation) , and [prioritizing critical requests](https://sre.google/sre-book/handling-overload/#criticality-00sDCK) .\nMitigation strategies on the client include [client-side throttling](https://sre.google/sre-book/handling-overload/#client-side-throttling-a7sYUg) and [exponential backoff with jitter](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/) .\n## Sanitize and validate inputs\nTo prevent erroneous, random, or malicious inputs that cause service outages or security breaches, sanitize and validate input parameters for APIs and operational tools. For example, [Apigee and Google Cloud Armor can help protect against injection attacks](/architecture/owasp-top-ten-mitigation#a1_injection) .\nRegularly use where a test harness intentionally calls APIs with random, empty, or too-large inputs. Conduct these tests in an isolated test environment.\nOperational tools should automatically validate configuration changes before the changes roll out, and should reject changes if validation fails.\n## Fail safe in a way that preserves function\nIf there's a failure due to a problem, the system components should fail in a way that allows the overall system to continue to function. These problems might be a software bug, bad input or configuration, an unplanned instance outage, or human error. What your services process helps to determine whether you should be overly permissive or overly simplistic, rather than overly restrictive.\nConsider the following example scenarios and how to respond to failure:\n- It's usually better for a firewall component with a bad or empty configuration to fail open and allow unauthorized network traffic to pass through for a short period of time while the operator fixes the error. This behavior keeps the service available, rather than to fail closed and block 100% of traffic. The service must rely on authentication and authorization checks deeper in the application stack to protect sensitive areas while all traffic passes through.\n- However, it's better for a permissions server component that controls access to user data to fail closed and block all access. This behavior causes a service outage when it has the configuration is corrupt, but avoids the risk of a leak of confidential user data if it fails open.\nIn both cases, the failure should raise a high priority alert so that an operator can fix the error condition. Service components should err on the side of failing open unless it poses extreme risks to the business.\n## Design API calls and operational commands to be retryable\nAPIs and operational tools must make invocations retry-safe as far as possible. A natural approach to many error conditions is to retry the previous action, but you might not know whether the first try was successful.\nYour system architecture should make actions - if you perform the identical action on an object two or more times in succession, it should produce the same results as a single invocation. Non-idempotent actions require more complex code to avoid a corruption of the system state.\n## Identify and manage service dependencies\nService designers and owners must maintain a complete list of dependencies on other system components. The service design must also include recovery from dependency failures, or graceful degradation if full recovery is not feasible. Take account of dependencies on cloud services used by your system and external dependencies, such as third party service APIs, recognizing that every system dependency has a non-zero failure rate.\nWhen you set reliability targets, recognize that the SLO for a service is mathematically constrained by the SLOs of all its critical dependencies. You can't be more reliable than the lowest SLO of one of the dependencies. For more information, see [the calculus of service availability](https://research.google/pubs/pub46285/) .\n### Startup dependencies\nServices behave differently when they start up compared to their steady-state behavior. Startup dependencies can differ significantly from steady-state runtime dependencies.\nFor example, at startup, a service may need to load user or account information from a user metadata service that it rarely invokes again. When many service replicas restart after a crash or routine maintenance, the replicas can sharply increase load on startup dependencies, especially when caches are empty and need to be repopulated.\nTest service startup under load, and provision startup dependencies accordingly. Consider a design to gracefully degrade by saving a copy of the data it retrieves from critical startup dependencies. This behavior allows your service to restart with potentially stale data rather than being unable to start when a critical dependency has an outage. Your service can later load fresh data, when feasible, to revert to normal operation.\nStartup dependencies are also important when you bootstrap a service in a new environment. Design your application stack with a layered architecture, with no cyclic dependencies between layers. Cyclic dependencies may seem tolerable because they don't block incremental changes to a single application. However, cyclic dependencies can make it difficult or impossible to restart after a disaster takes down the entire service stack.\n## Minimize critical dependencies\nMinimize the number of critical dependencies for your service, that is, other components whose failure will inevitably cause outages for your service. To make your service more resilient to failures or slowness in other components it depends on, consider the following example design techniques and principles to convert critical dependencies into non-critical dependencies:\n- Increase the level of redundancy in critical dependencies. Adding more replicas makes it less likely that an entire component will be unavailable.\n- Use asynchronous requests to other services instead of blocking on a response or use publish/subscribe messaging to decouple requests from responses.\n- Cache responses from other services to recover from short-term unavailability of dependencies.\nTo render failures or slowness in your service less harmful to other components that depend on it, consider the following example design techniques and principles:\n- Use prioritized request queues and give higher priority to requests where a user is waiting for a response.\n- Serve responses out of a cache to reduce latency and load.\n- Fail safe in a way that preserves function.\n- Degrade gracefully when there's a traffic overload.## Ensure that every change can be rolled back\nIf there's no well-defined way to undo certain types of changes to a service, change the design of the service to support rollback. Test the rollback processes periodically. APIs for every component or microservice must be versioned, with backward compatibility such that the previous generations of clients continue to work correctly as the API evolves. This design principle is essential to permit progressive rollout of API changes, with rapid rollback when necessary.\nRollback can be costly to implement for mobile applications. [Firebase Remote Config](https://firebase.google.com/products/remote-config) is a Google Cloud service to make feature rollback easier.\nYou can't readily roll back database schema changes, so execute them in multiple phases. Design each phase to allow safe schema read and update requests by the latest version of your application, and the prior version. This design approach lets you safely roll back if there's a problem with the latest version.\n## Recommendations\nTo apply the guidance in the Architecture Framework to your own environment, follow these recommendations:\n- Implement exponential backoff with randomization in the error retry logic of client applications.\n- Implement a multi-region architecture with automatic failover for high availability.\n- Use load balancing to distribute user requests across shards and regions.\n- Design the application to degrade gracefully under overload. Serve partial responses or provide limited functionality rather than failing completely.\n- Establish a data-driven process for capacity planning, and use load tests and traffic forecasts to determine when to provision resources.\n- Establish disaster recovery procedures and test them periodically.## What's next\n- [Create reliable operational processes and tools](/architecture/framework/reliability/create-operational-processes-tools) (next document in this series)\nExplore other categories in the [Architecture Framework](/architecture/framework) such as system design, operational excellence, and security, privacy, and compliance.# Create reliable operational processes and tools\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides operational principles to run your service in a reliable manner, such as how to deploy updates, run services in production environments, and test for failures. Architecting for reliability should cover the whole lifecycle of your service, not just software design.\n## Choose good names for applications and services\nAvoid using internal code names in production configuration files, because they can be confusing, particularly to newer employees, potentially increasing time to mitigate (TTM) for outages. As much as possible, choose good names for all of your applications, services, and critical system resources such as VMs, clusters, and database instances, subject to their respective limits on name length. A good name describes the entity's purpose; is accurate, specific, and distinctive; and is meaningful to anybody who sees it. A good name avoids acronyms, code names, abbreviations, and potentially offensive terminology, and would not create a negative public response even if published externally.\n## Implement progressive rollouts with canary testing\nInstantaneous global changes to service binaries or configuration are inherently risky. Roll out new versions of executables and configuration changes incrementally. Start with a small scope, such as a few VM instances in a zone, and gradually expand the scope. Roll back rapidly if the change doesn't perform as you expect, or negatively impacts users at any stage of the rollout. Your goal is to identify and address bugs when they only affect a small portion of user traffic, before you roll out the change globally.\nSet up a system that's aware of service changes and does A/B comparison of the metrics of the changed servers with the remaining servers. The system should flag unexpected or anomalous behavior. If the change doesn't perform as you expect, the canary testing system should automatically halt rollouts. Problems can be clear, such as user errors, or subtle, like CPU usage increase or memory bloat.\nIt's better to stop and roll back at the first hint of trouble and diagnose issues without the time pressure of an outage. After the change passes canary testing, propagate it to larger scopes gradually, such as to a full zone, then to a second zone. Allow time for the changed system to handle progressively larger volumes of user traffic to expose any latent bugs.\nFor more information, see [Application deployment and testing strategies](/architecture/application-deployment-and-testing-strategies) .\n## Spread out traffic for timed promotions and launches\nYou might have promotional events, such as sales that start at a precise time and encourage many users to connect to the service simultaneously. If so, design client code to spread the traffic over a few seconds. Use random delays before they initiate requests.\nYou can also the system. When you pre-warm the system, you send the user traffic you anticipate to it ahead of time to ensure it performs as you expect. This approach prevents instantaneous traffic spikes that could crash your servers at the scheduled start time.\n## Automate build, test, and deployment\nEliminate manual effort from your release process with the use of continuous integration and continuous delivery (CI/CD) pipelines. Perform automated integration testing and deployment. For example, [create a modern CI/CD process with GKE](/kubernetes-engine/docs/tutorials/modern-cicd-gke-user-guide) .\nFor more information, see [continuous integration](https://dora.dev/devops-capabilities/technical/continuous-integration/) , [continuous delivery](https://dora.dev/devops-capabilities/technical/continuous-delivery/) , [test automation](https://dora.dev/devops-capabilities/technical/test-automation/) , and [deployment automation](https://dora.dev/devops-capabilities/technical/deployment-automation/) .\n## Defend against operator error\nDesign your operational tools to reject potentially invalid configurations. Detect and alert when a configuration version is empty, partial or truncated, corrupt, logically incorrect or unexpected, or not received within the expected time. Tools should also reject configuration versions that differ too much from the previous version.\nDisallow changes or commands with too broad a scope that are potentially destructive. These broad commands might be to \"Revoke permissions for all users\", \"Restart all VMs in this region\", or \"Reformat all disks in this zone\". Such changes should only be applied if the operator adds emergency override command-line flags or option settings when they deploy the configuration.\nTools must display the breadth of impact of risky commands, such as number of VMs the change impacts, and require explicit operator acknowledgment before the tool proceeds. You can also use features to lock critical resources and prevent their accidental or unauthorized deletion, such as [Cloud Storage retention policy locks](/storage/docs/bucket-lock) .\n## Test failure recovery\nRegularly test your operational procedures to recover from failures in your service. Without regular tests, your procedures might not work when you need them if there's a real failure. Items to test periodically include regional failover, how to roll back a release, and how to restore data from backups.\n## Conduct disaster recovery tests\nLike with failure recovery tests, don't wait for a disaster to strike. Periodically test and verify your disaster recovery procedures and processes.\nYou might create a system architecture to provide high availability (HA). This architecture doesn't entirely overlap with disaster recovery (DR), but it's often necessary to take HA into account when you think about recovery time objective (RTO) and recovery point objective (RPO) values.\nHA helps you to meet or exceed an agreed level of operational performance, such as uptime. When you run production workloads on Google Cloud, you might deploy a passive or active standby instance in a second region. With this architecture, the application continues to provide service from the unaffected region if there's a disaster in the primary region. For more information, see [Architecting disaster recovery for cloud outages](/architecture/disaster-recovery) .\n## Practice chaos engineering\nConsider the use of chaos engineering in your test practices. Introduce actual failures into different components of production systems under load in a safe environment. This approach helps to ensure that there's no overall system impact because your service handles failures correctly at each level.\nFailures you inject into the system can include crashing tasks, errors and timeouts on RPCs, or reductions in resource availability. Use random fault injection to test intermittent failures ( ) in service dependencies. These behaviors are hard to detect and mitigate in production.\nChaos engineering ensures that the fallout from such experiments is minimized and contained. Treat such tests as practice for actual outages and use all of the information collected to improve your outage response.\n## What's next\n- [Build efficient alerts](/architecture/framework/reliability/build-efficient-alerts) (next document in this series)\nExplore other categories in the [Architecture Framework](/architecture/framework) such as system design, operational excellence, and security, privacy, and compliance.# Build efficient alerts\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides operational principles to create alerts that help you run reliable services. The more information you have on how your service performs, the more informed your decisions are when there's an issue. Design your alerts for early and accurate detection of all user-impacting system problems, and minimize false positives.\n## Optimize the alert delay\nThere's a balance between alerts that are sent too soon that stress the operations team and alerts that are sent too late and cause long service outages. Tune the alert delay before the monitoring system notifies humans of a problem to minimize time to detect, while maximizing signal versus noise. [Use the error budget consumption rate](https://sre.google/workbook/alerting-on-slos/) to derive the optimal alert configuration.\n## Alert on symptoms rather than causes\nTrigger alerts based on the direct impact to user experience. Noncompliance with global or per-customer SLOs indicates a direct impact. Don't alert on every possible [root cause](https://sre.google/sre-book/monitoring-distributed-systems/#root-cause) of a failure, especially when the impact is limited to a single replica. A well-designed distributed system recovers seamlessly from single-replica failures.\n## Alert on outlier values rather than averages\nWhen monitoring latency, define SLOs and set alerts for (pick two out of three) 90th, 95th, or 99th percentile latency, not for average or 50th percentile latency. Good mean or median latency values can hide unacceptably high values at the 90th percentile or above that cause very bad user experiences. Therefore you should apply this principle of alerting on outlier values when monitoring latency for any critical operation, such as a request-response interaction with a webserver, batch completion in a data processing pipeline, or a read or write operation on a storage service.# Build a collaborative incident management process\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices to manage services and define processes to respond to incidents. Incidents occur in all services, so you need a well-documented process to efficiently respond to these issues and mitigate them.\n## Incident management overview\nIt's inevitable that your well-designed system eventually fails to meet its SLOs. In the absence of an SLO, your customers loosely define what the acceptable service level is themselves from their past experience. Customers escalate to your technical support or similar group, regardless of what's in your SLA.\nTo properly serve your customers, establish and regularly test an incident management plan. The plan can be as short as a single-page checklist with ten items. This process helps your team to reduce time to detect (TTD) and time to mitigate (TTM).\nTTM is preferred as opposed to TTR, where the R for or is often used to mean a full fix versus mitigation. TTM emphasizes fast mitigation to quickly end the customer impact of an outage, and then start the often much longer process to fully fix the problem.\nA well-designed system where operations are excellent increases the time between failures (TBF). In other words, operational principles for reliability, including good incident management, aim to make failures less frequent.\nTo run reliable services, apply the following best practices in your incident management process.\n## Assign clear service ownership\nAll services and their critical dependencies must have clear owners responsible for adherence to their SLOs. If there are reorganizations or team attrition, engineering leads must ensure that ownership is explicitly handed off to a new team, along with the documentation and training as required. The owners of a service must be easily discoverable by other teams.\n## Reduce time to detect (TTD) with well tuned alerts\nBefore you can reduce TTD, review and implement the recommendations in the [build observability into your infrastructure and applications](/architecture/framework/reliability/observability-infrastructure-applications) and [define your reliability goals](/architecture/framework/reliability/define-goals) sections of the Architecture Framework reliability category. For example, disambiguate between application issues and underlying cloud issues.\nA well-tuned set of SLIs alerts your team at the right time without alert overload. For more information, see the [build efficient alerts](/architecture/framework/reliability/build-efficient-alerts) section of the Architecture Framework reliability category or [Tune up your SLI metrics: CRE life lessons](/blog/products/management-tools/tune-up-your-sli-metrics-cre-life-lessons) .\n## Reduce time to mitigate (TTM) with incident management plans and training\nTo reduce TTM, define a documented and well-exercised incident management plan. Have readily available data on what's changed in the environment. Make sure that teams know [generic mitigations](https://www.oreilly.com/content/generic-mitigations/) they can quickly apply to minimize TTM. These mitigation techniques include draining, rolling back changes, upsizing resources, and degrading quality of service.\nAs discussed in another Architecture Framework reliability category document, [create reliable operational processes and tools](/architecture/framework/reliability/create-operational-processes-tools) to support the safe and rapid rollback of changes.\n## Design dashboard layouts and content to minimize TTM\nOrganize your service dashboard layout and navigation so that an operator can determine in a minute or two if the service and all of its critical dependencies are running. To quickly pinpoint potential causes of problems, operators must be able to scan all charts on the dashboard to rapidly look for graphs that change significantly at the time of the alert.\nThe following list of example graphs might be on your dashboard to help troubleshoot issues. Incident responders should be able to glance at them in a single view:\n- Service level indicators, such as successful requests divided by total valid requests\n- Configuration and binary rollouts\n- Requests per second to the system\n- Error responses per second from the system\n- Requests per second from the system to its dependencies\n- Error responses per second to the system from its dependencies\nOther common graphs to help troubleshoot include latency, saturation, request size, response size, query cost, thread pool utilization, and Java virtual machine (JVM) metrics (where applicable). refers to fullness by some limit such as quota or system memory size. looks for regressions due to pool exhaustion.\nTest the placement of these graphs against a few outage scenarios to ensure that the most important graphs are near the top, and that the order of the graphs matches your standard diagnostic workflow. You can also apply machine learning and statistical anomaly detection to surface the right subset of these graphs.\n## Document diagnostic procedures and mitigation for known outage scenarios\nWrite playbooks and link to them from alert notifications. If these documents are accessible from the alert notifications, operators can quickly get the information they need to troubleshoot and mitigate problems.\n## Use blameless postmortems to learn from outages and prevent recurrences\nEstablish a [blameless postmortem culture and an incident review process](https://sre.google/sre-book/postmortem-culture/) . means that your team evaluates and documents what went wrong in an objective manner, without the need to assign blame.\nMistakes are opportunities to learn, not a cause for criticism. Always aim to make the system more resilient so that it can recover quickly from human error, or even better, detect and prevent human error. Extract as much learning as possible from each postmortem and follow up diligently on each postmortem action item in order to make outages less frequent, thereby increasing TBF.\n## Incident management plan example\nProduction issues have been detected, such as through an alert or page, or escalated to me:\n- Should I delegate to someone else?- Yes, if you and your team can't resolve the issue.\n- Is this issue a privacy or security breach?- If yes, delegate to the privacy or security team.\n- Is this issue an emergency or are SLOs at risk?- If in doubt, treat it as an emergency.\n- Should I involve more people?- Yes, if it impacts more than X% of customers or if it takes more than Y minutes to resolve. If in doubt, always involve more people, especially within business hours.\n- Define a primary communications channel, such as IRC, Hangouts Chat, or Slack.\n- Delegate previously defined roles, such as the following:- who is responsible for overall coordination.\n- who is responsible for internal and external communications.\n- who is responsible to mitigate the issue.\n- Define when the incident is over. This decision might require an acknowledgment from a support representative or other similar teams.\n- Collaborate on the blameless postmortem.\n- Attend a postmortem incident review meeting to discuss and staff action items.## Recommendations\nTo apply the guidance in the Architecture Framework to your own environment, follow these recommendations::\n- Establish an incident management plan, and train your teams to use it.\n- To reduce TTD, implement the recommendations to [build observability into your infrastructure and applications](/architecture/framework/reliability/observability-infrastructure-applications) .\n- Build a \"What's changed?\" dashboard that you can glance at when there's an incident.\n- Document query snippets or build a [Looker Studio](https://lookerstudio.google.com/overview) dashboard with frequent log queries.\n- Evaluate [Firebase Remote Config](https://firebase.google.com/products/remote-config) to mitigate rollout issues for mobile applications.\n- [Test failure recovery](/architecture/framework/reliability/create-operational-processes-tools#test_failure_recovery) , including restoring data from backups, to decrease TTM for a subset of your incidents.\n- Design for and test configuration and binary rollbacks.\n- [Replicate data across regions for disaster recovery](/architecture/framework/reliability/design-scale-high-availability#replicate_data_across_regions_for_disaster_recovery) and use [disaster recovery tests](/architecture/framework/reliability/create-operational-processes-tools#conduct_disaster_recovery_tests) to decrease TTM after regional outages.\n- [Design a multi-region architecture for resilience to regional outages](/architecture/framework/reliability/design-scale-high-availability#design_a_multi-region_architecture_for_resilience_to_regional_outages) if the business need for high availability justifies the cost, to increase TBF.## What's next\nLearn more about how to build a collaborative incident management process with the following resources:\n- [Google Cloud status dashboard](https://status.cloud.google.com/) \n- [Incident management at Google](/blog/products/gcp/incident-management-at-google-adventures-in-sre-land) \n- [Data incident response process](/security/incident-response) \n- [SRE book chapter on managing incidents](https://sre.google/sre-book/managing-incidents/) \nExplore other categories in the [Architecture Framework](/architecture/framework) such as system design, operational excellence, and security, privacy, and compliance.# Google Cloud Architecture Framework: Product reliability guides\nThis section in the Architecture Framework has product-specific best practice guidance for reliability, availability, and consistency of some Google Cloud products. The guides also provide recommendations for minimizing and recovering from failures and for scaling your applications well under load.\n**Note:** To learn about the platform-level building blocks of reliability in Google Cloud (zones, regions, and location-scoped resources) and the availability levels that they provide, see [Google Cloud infrastructure reliability guide](/architecture/infra-reliability-guide) . This guide also provides guidelines for assessing the reliability requirements of your workloads, and presents architectural recommendations for building and managing reliable infrastructure in Google Cloud.\nThe product reliability guides are organized under the following areas:\n- Compute- [Compute Engine](/architecture/framework/reliability/product-guides/compute) \n- [Cloud Run](/architecture/framework/reliability/product-guides/run) \n- [Cloud Functions ](/architecture/framework/reliability/product-guides/functions) \n- [Google Kubernetes Engine ](/architecture/framework/reliability/product-guides/kubernetes-engine) \n- Storage and databases- [Cloud Storage](/architecture/framework/reliability/product-guides/storage) \n- [Firestore](/architecture/framework/reliability/product-guides/firestore) \n- [Bigtable](/architecture/framework/reliability/product-guides/bigtable) \n- [Cloud SQL](/architecture/framework/reliability/product-guides/sql) \n- [Spanner](/architecture/framework/reliability/product-guides/spanner) \n- [Filestore](/architecture/framework/reliability/product-guides/filestore) \n- [Memorystore](/architecture/framework/reliability/product-guides/memorystore) \n- Networking- [Cloud DNS](/architecture/framework/reliability/product-guides/dns) \n- [Cloud Load Balancing](/architecture/framework/reliability/product-guides/load-balancing) \n- [Cloud CDN](/architecture/framework/reliability/product-guides/cdn) \n- Data analytics- [BigQuery](/architecture/framework/reliability/product-guides/bigquery) \n- [Dataflow](/architecture/framework/reliability/product-guides/dataflow) \n- [Dataproc](/architecture/framework/reliability/product-guides/dataproc) \n# Compute Engine reliability guide\nCompute Engine is a customizable compute service that enables users to create and run virtual machines on demand on Google's infrastructure.\n## Best practices\n- [Compute Engine API best practices](/compute/docs/api/best-practices) - recommended best practices for using the Compute Engine API and mitigating the effects of API rate limits.\n- [Designing resilient systems](/compute/docs/tutorials/robustsystems) - detailed guidance on how to architect your Compute Engine applications to recover from single-VM failures or reboots, and zonal or regional failures.\n- [How to increase availability by overprovisioning](/compute/docs/instance-groups/regional-migs#overprovision) - add redundant resources to keep your application up and running in the face of capacity losses from zonal or regional disruptions.\n- [Using load balancing for highly available applications](/compute/docs/tutorials/high-availability-load-balancing) - how to use Cloud Load Balancing with Compute Engine to provide high availability, even during a zonal outage.\n- [Best practices for Compute Engine regions selection](/solutions/best-practices-compute-engine-region-selection) - how to choose which Google Cloud regions to use for your Compute Engine resources to optimize latency for your applications while accounting for price/performance tradeoffs.\n- [Best practices for migrating VMs, using Migrate to Virtual Machines](/migrate/virtual-machines/docs/5.0/discover/migrating-vms-migrate-for-compute-engine-best-practices) - how to migrate VMs from a supported source environment to Compute Engine with Migrate to Virtual Machines, including assessment, planning, and carrying out the migration. Also, how to address common issues that might arise with migration.\n- [Patterns for using floating IP addresses in Compute Engine](/solutions/best-practices-floating-ip-addresses) - how to implement shared or virtual IP addresses in a Compute Engine environment by changing the architecture to a pattern for your use case. Patterns include those using load balancing, Google Cloud routes, and autohealing.\n- [Best practices for persistent disk snapshots](/compute/docs/disks/snapshot-best-practices) - create snapshots more quickly and with greater reliability.\n- [Persistent disks and replication](/compute/docs/disks/persistent-disks-replication) - how persistent disks use Colossus for the storage backend, and accessing persistent disks from VMs. Also, monitoring persistent disk latency, replicating persistent disks between regions or zones, and how read and write requests are handled for replicas.\n- [Configure disks to meet performance requirements](/compute/docs/disks/performance) - factors that affect the performance of block storage volumes attached to VM instances.\n- [Image management best practices](/compute/docs/images/image-management-best-practices) - detailed guidance on managing Compute Engine images such as choosing a boot image, customizing images, image lifecycle, and sharing images between projects.\n- [Image families best practices](/compute/docs/images/image-families-best-practices#how-to-use) - the importance of testing the latest image in an image family before using it in production, and how to set up the test procedure.\n- [Best practices for SQL Server instances](/compute/docs/instances/sql-server/best-practices) - best practices for optimizing Compute Engine instances that run Microsoft SQL Server, and optimizing SQL Server Enterprise Edition. In addition, how to use the operating system's default network settings, and operational activities to increase performance and stability.# Cloud Run reliability guide\nCloud Run is a managed compute platform suitable for deploying containerized applications, and is serverless. Cloud Run abstracts away all infrastructure so users can focus on building applications.\n## Best practices\n- [Cloud Run general tips](/run/docs/tips/general) - how to implement a Cloud Run service, start containers quickly, use global variables, and improve container security.\n- [Load testing best practices](/run/docs/about-load-testing) - how to load test Cloud Run services, including addressing concurrency problems before load testing, managing the maximum number of instances, choosing the best region for load testing, and ensuring services scale with load.\n- [Instance scaling](/run/docs/about-instance-autoscaling) - how to scale and limit container instances and minimize response time by keeping some instances idle instead of stopping them.\n- [Using minimum instances](/run/docs/configuring/min-instances) - specify the least number of container instances ready to serve, and when set appropriately high, minimize average response time by reducing the number of cold starts.\n- [Optimizing Java applications for Cloud Run](/run/docs/tips/java) - understand the tradeoffs of some optimizations for Cloud Run services written in Java, and reduce startup time and memory usage.\n- [Optimizing Python applications for Cloud Run](/run/docs/tips/python) - optimize the container image by improving efficiency of the WSGI server, and optimize applications by reducing the number of threads and executing startup tasks in parallel.# Cloud Functions reliability guide\nCloud Functions is a scalable, event-driven, serverless platform to help build and connect services. Cloud Functions can be called via HTTP request or triggered based on background events.\n## Best practices\n- [Cloud Functions best practices](/functions/docs/bestpractices/tips) - guidelines on how to use and interact with Cloud Functions, implement [idempotency](/architecture/framework/reliability/design-scale-high-availability#design_api_calls_and_operational_commands_to_be_retryable) , deploy functions, and optimize performance.# Google Kubernetes Engine reliability guide\nGoogle Kubernetes Engine (GKE) is a system for operating containerized applications in the cloud, at scale. GKE deploys, manages, and provisions resources for your containerized applications. The GKE environment consists of Compute Engine instances grouped together to form a cluster.\n## Best practices\n- [Best practices for operating containers](/solutions/best-practices-for-operating-containers) - how to use logging mechanisms, ensure containers are stateless and immutable, monitor applications, and do liveness and readiness probes.\n- [Best practices for building containers](/solutions/best-practices-for-building-containers) - how to package a single application per container, handle process identifiers (PIDs), optimize for the Docker build cache, and build smaller images for faster upload and download times.\n- [Best practices for Google Kubernetes Engine networking](/kubernetes-engine/docs/best-practices/networking) - use VPC-native clusters for easier scaling, plan IP addresses, scale cluster connectivity, use Google Cloud Armor to block Distributed Denial-of-Service (DDoS) attacks, implement container-native load balancing for lower latency, use the health check functionality of external Application Load Balancers for graceful failover, and use regional clusters to increase the availability of applications in a cluster.\n- [Prepare cloud-based Kubernetes applications](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#prepare_cloud-based_kubernetes_applications) - learn the best practices to plan for application capacity, grow application horizontally or vertically, set resource limits relative to resource requests for memory versus CPU, make containers lean for faster application startup, and limit`Pod`disruption by setting a`Pod Disruption Budget`(PDB). Also, understand how to set up liveness probes and readiness probes for graceful application startup, ensure non-disruptive shutdowns, and implement exponential backoff on retried requests to prevent traffic spikes that overwhelm your application.\n- [GKE multi-tenancy best practices](/kubernetes-engine/docs/best-practices/enterprise-multitenancy) - how to design a multi-tenant cluster architecture for high availability and reliability, use Google Kubernetes Engine (GKE) usage metering for per-tenant usage metrics, provide tenant-specific logs, and provide tenant-specific monitoring.# Cloud Storage reliability guide\nCloud Storage is a durable and highly available object repository with advanced security and sharing capabilities. This service is used for storing and accessing data on Google Cloud infrastructure.\n## Best practices\n- [Best practices for Cloud Storage](/storage/docs/best-practices) - general best practices for Cloud Storage, including tips to maximize availability and minimize latency of your applications, improve the reliability of upload operations, and improve the performance of large-scale data deletions.\n- [Request rate and access distribution guidelines](/storage/docs/request-rate) - how to minimize latency and error responses on read, write, and delete operations at very high request rates by understanding how Cloud Storage auto-scaling works.# Firestore reliability guide\nFirestore is a NoSQL document database that lets you store, sync, and query data for your mobile and web applications, at global scale.\n## Best practices\n- [Firestore best practices](https://cloud.google.com/firestore/docs/best-practices) - how to select your database location for increased reliability, minimize performance pitfalls in indexing, improve the performance of read and write operations, reduce latency for change notifications, and design for scale.# Bigtable reliability guide\nBigtable is a fully managed, scalable, NoSQL database for large analytical and operational workloads. It is designed as a sparsely populated table that can scale to billions of rows and thousands of columns, and supports high read and write throughput at low latency.\n## Best practices\n- [Understand Bigtable performance](/bigtable/docs/performance) - estimating throughput for Bigtable, how to plan Bigtable capacity by looking at throughput and storage use, how enabling replication affects read and write throughput differently, and how Bigtable optimizes data over time.\n- [Bigtable schema design](/bigtable/docs/schema-design) - guidance on designing Bigtable schema, including concepts of key/value store, designing row keys based on planned read requests, handling columns and rows, and special use cases.\n- [Bigtable replication overview](/bigtable/docs/replication-overview) - how to replicate Bigtable across multiple zones or regions, understand performance implications of replication, and how Bigtable resolves conflicts and handles failovers.\n- [About Bigtable backups](/bigtable/docs/backups) - how to save a copy of a table's schema and data with Bigtable Backups, which can help you recover from application-level data corruption or from operator errors, such as accidentally deleting a table.\n# Cloud SQL reliability guide\nCloud SQL is a fully managed relational database service for MySQL, PostgreSQL, and SQL Server. Cloud SQL easily integrates with existing applications and Google Cloud services such as Google Kubernetes Engine and BigQuery.\n## Best practices\n- **Cloud SQL high availability** - an overview of the high availability configuration for Cloud SQL, including handling read replicas and the failover process. Also includes how to control the timing of database maintenance events, and how the use of regional persistent disk in the HA configuration affects database performance. This content is divided into three sections:- [MySQL high availability](/sql/docs/mysql/high-availability) \n- [PostgreSQL high availability](/sql/docs/postgres/high-availability) \n- [SQL Server high availability](/sql/docs/sqlserver/high-availability) \n- **Cloud SQL database disaster recovery** - an overview of the disaster recovery configuration for Cloud SQL using a cross-region read replica.- [MySQL disaster recovery](/sql/docs/mysql/intro-to-cloud-sql-disaster-recovery) \n- [PostgreSQL disaster recovery](/sql/docs/postgres/intro-to-cloud-sql-disaster-recovery) \n- [SQL Server disaster recovery](/sql/docs/sqlserver/intro-to-cloud-sql-disaster-recovery) \n- **General best practices for Cloud SQL** - guidance on configuring instances, data architecture, importing and exporting data, and backup and recovery. This content is divided into three sections:- [MySQL best practices](/sql/docs/mysql/best-practices) \n- [PostgreSQL best practices](/sql/docs/postgres/best-practices) \n- [SQL Server best practices](/sql/docs/sqlserver/best-practices) \n- **Best practices for importing and exporting data** - under what circumstances Cloud Storage buckets cannot be used, compressing data to reduce costs, known limitations with importing and exporting data, how to automate export operations, and troubleshooting import and export operations.- [MySQL best practices](/sql/docs/mysql/import-export) \n- [PostgreSQL best practices](/sql/docs/postgres/import-export) \n- [SQL Server best practices](/sql/docs/sqlserver/import-export) \n# Spanner reliability guide\nSpanner is a distributed SQL database management and storage service, with features such as global transactions and highly available horizontal scaling and transactional consistency.\n## Best practices\n- [Spanner backup and restore](/spanner/docs/backup) - key features of Spanner Backup and Restore, comparison of Backup and Restore with Import and Export, implementation details, and how to control access to Spanner resources.\n- [Regional and multi-region configurations](/spanner/docs/instance-configurations) - description of the two types of instance configurations that Spanner offers: regional configurations and multi-region configurations. The description includes the differences and trade-offs between each configuration.\n- [Autoscaling Spanner](/spanner/docs/autoscaling-overview) - introduction to the Autoscaler tool for Spanner (Autoscaler), an open source tool that you can use as a companion tool to Cloud Spanner. This tool lets you automatically increase or reduce the number of nodes or processing units in one or more Spanner instances based on the utilization metrics of each Spanner instance.\n- [About point-in-time recovery (PITR)](/spanner/docs/pitr) - description of Spanner point-in-time recovery (PITR), a feature that protects against accidental deletion or writes of Spanner data. For example, an operator inadvertently writes data or an application rollout corrupts the database. With PITR, you can recover your data from a point-in-time in the past (up to a maximum of seven days) seamlessly.\n- [Spanner best practices](/spanner/docs/best-practice-list) - guidance on bulk loading, using Data Manipulation Language (DML), designing schema to avoid hotspots, and SQL best practices.# Filestore reliability guide\nFilestore is a managed file storage service for Google Cloud applications, with a filesystem interface and a shared filesystem for data. Filestore offers petabyte-scale online network attached storage (NAS) for Compute Engine and Google Kubernetes Engine instances.\n## Best practices\n- [Filestore performance](/filestore/docs/performance) - performance settings and Compute Engine machine type recommendations, NFS mount options for best performance on Linux client VM instances, and using the [fio](https://linux.die.net/man/1/fio) tool to test performance. Includes recommendations for improved performance across multiple Google Cloud resources.\n- [Filestore backups](/filestore/docs/backups#best_practices) - description of Filestore backups, common use cases, and best practices for creating and using backups.\n- [Filestore snapshots](/filestore/docs/snapshots) - description of Filestore snapshots, common use cases, and best practices for creating and using snapshots.\n- [Filestore networking](/filestore/docs/networking) - networking and IP resource requirements needed to use Filestore.# Memorystore reliability guide\nMemorystore is a fully-managed, in-memory store that provides a managed version of two open source caching solutions: Redis and Memcached. Memorystore is scalable, and automates complex tasks such as provisioning, replication, failover, and patching.\n## Best practices\n- [Redis general best practices](/memorystore/docs/redis/general-best-practices) - guidance on exporting Redis Database (RDB) backups, resource-intensive operations, and operations requiring connection retry. In addition, information on maintenance, memory management, and setting up Serverless VPC Access connector, as well as private services access connection mode, and monitoring and alerts.\n- [Redis memory management best practices](/memorystore/docs/redis/memory-management-best-practices) - memory management concepts such as instance capacity and`Maxmemory`configuration, export, scaling, and version upgrade operations, memory management metrics, and how to resolve an out-of-memory condition.\n- [Redis exponential backoff](/memorystore/docs/redis/exponential-backoff) - how exponential backoff works, an example algorithm, and how maximum backoff and maximum number of retries work.\n- [Memcached best practices](/memorystore/docs/memcached/best-practices) - how to design application for cache misses, connecting directly to nodes' IP addresses, and Memcached [Auto Discovery service](/memorystore/docs/memcached/auto-discovery-overview) . Also, guidance on configuring`max-item-size`parameter, balancing clusters, and using Cloud Monitoring to monitor essential metrics.\n- [Memcached memory management best practices](/memorystore/docs/memcached/memory-management-best-practices) - configuring memory for a Memcached instance,configuration, when to increase, and metrics for memory usage.# Cloud DNS reliability guide\nCloud DNS is a low-latency domain name system that helps register, manage, and serve your domains. Cloud DNS scales to large numbers of DNS zones and records, and millions of DNS records can be created and updated via a user interface.\n## Best practices\n- [Cloud DNS best practices](/dns/docs/best-practices-dns) - learn how to manage private zones, configure DNS forwarding, and create DNS server policies. Includes guidance on using Cloud DNS in a hybrid environment.# Cloud Load Balancing reliability guide\nCloud Load Balancing is a fully distributed, software-defined, managed service for all your traffic. Cloud Load Balancing also provides seamless autoscaling, Layer 4 and Layer 7 load balancing, and support for features such as IPv6 global load balancing.\n## Best practices\n- [Performance best practices](/load-balancing/docs/https/http-load-balancing-best-practices) - how to spread load across application instances to deliver optimal performance. Strategies include backend placement in regions closest to traffic, caching, forwarding rule protocol selection, and configuring session affinity.\n- [Using load balancing for highly available applications](/compute/docs/tutorials/high-availability-load-balancing) - how to use Cloud Load Balancing with Compute Engine to provide high availability, even during a zonal outage.\n# Cloud CDN reliability guide\nCloud CDN (Content Delivery Network) is a service that accelerates internet content delivery by using Google's edge network to bring content as close as possible to the user. Cloud CDN helps reduce latency, cost, and load, making it easier to scale services to users.\n## Best practices\n- [Content delivery best practices](/cdn/docs/best-practices) - how to optimize cache hit ratio using cache keys, ensure HTTP/3 is enabled for top performance, and review monitoring metrics using [Cloud CDN custom monitoring dashboard](/cdn/docs/logging#monitoring_for) . Also, how to review reports on availability, latency, and throughput from third-party performance tests.\n# BigQuery reliability guide\nBigQuery is Google Cloud's data warehouse platform for storing and analyzing data at scale.\n## Best practices\n- [Introduction to reliability](https://cloud.google.com/bigquery/docs/reliability-intro) - reliability best practices and introduction to concepts such as availability, durability, and data consistency.\n- [Availability and durability](/bigquery/docs/availability) - the types of failure domains that can occur in Google Cloud data centers, how BigQuery provides storage redundancy based on data storage location, and why cross-region datasets enhance disaster recovery.\n- [Best practices for multi-tenant workloads on BigQuery](/bigquery/docs/best-practices-for-multi-tenant-workloads-on-bigquery) - common patterns used in multi-tenant data platforms. These patterns include ensuring reliability and isolation for customers of software as a service (SaaS) vendors, important BigQuery quotas and limits for capacity planning, using BigQuery Data Transfer Service to copy relevant datasets into another region, and more.\n- [Use Materialized Views](/bigquery/docs/materialized-views-best-practices) - how to use BigQuery Materialized Views for faster queries at lower cost, including querying materialized views, aligning partitions, and understanding smart-tuning (automatic rewriting of queries).# Dataflow reliability guide\nDataflow is a fully-managed data processing service which enables fast, simplified, streaming data pipeline development using open source Apache Beam libraries. Dataflow minimizes latency, processing time, and cost through autoscaling and batch processing.\n## Best practices\n**Building production-ready data pipelines using Dataflow** - a document series on using Dataflow including planning, developing, deploying, and monitoring Dataflow pipelines.\n- [Overview](https://cloud.google.com/solutions/building-production-ready-data-pipelines-using-dataflow-overview) - introduction to Dataflow pipelines.\n- [Planning](https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-planning) - measuring SLOs, understanding the impact of data sources and sinks on pipeline scalability and performance, and taking high availability, disaster recovery, and network performance into account when specifying regions to run your Dataflow jobs.\n- [Developing and testing](https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-developing-and-testing) - setting up deployment environments, preventing data loss by using dead letter queues for error handling, and reducing latency and cost by minimizing expensive per-element operations. Also, using batching to reduce performance overhead without overloading external services, unfusing inappropriately fused steps so that the steps are separated for better performance, and running end-to-end tests in preproduction to ensure that the pipeline continues to meet your SLOs and other production requirements.\n- [Deploying](https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-deploying) - continuous integration (CI) and continuous delivery and deployment (CD), with special considerations for deploying new versions of streaming pipelines. Also, an example CI/CD pipeline, and some features for optimizing resource usage. Finally, a discussion of high availability, geographic redundancy, and best practices for pipeline reliability, including regional isolation, use of snapshots, handling job submission errors, and recovering from errors and outages impacting running pipelines.\n- [Monitoring](https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-monitoring) - observe service level indicators (SLIs) which are important indicators of pipeline performance, and define and measure service level objectives (SLOs).# Dataproc reliability guide\nDataproc is a fully managed, scalable service for running Apache Hadoop and Spark jobs. With Dataproc, virtual machines can be customized and scaled up and down as needed. Dataproc integrates tightly with Cloud Storage, BigQuery, Bigtable, and other Google Cloud services.\n## Best practices\n- [Dataproc High Availability mode](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/high-availability) - compare Hadoop High Availability (HA) mode with the default non-HA mode in terms of instance names, Apache ZooKeeper, Hadoop Distributed File System (HDFS), and Yet Another Resource Negotiator (YARN). Also, how to create a high availability cluster.\n- [Autoscaling clusters](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling) - when to use Dataproc autoscaling, how to create an autoscaling policy, multi-cluster policy usage, reliability best practices for autoscaling configuration, and metrics and logs.\n- [Dataproc Enhanced Flexibility Mode (EFM)](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/flex) - examples of using Enhanced Flexibility Mode to minimize job progress delays, advanced configuration such as partitioning and parallelism, and YARN graceful decommissioning on EFM clusters.\n- [Graceful decomissioning](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/scaling-clusters#graceful_decommissioning) - using graceful decomissioning to minimize the impact of removing workers from a cluster, how to use this feature with secondary workers, and command examples for graceful decomissioning.\n- [Restartable jobs](https://cloud.google.com/dataproc/docs/concepts/jobs/restartable-jobs) - by using optional settings, you can set jobs to restart on failure to mitigate common types of job failure, including out-of-memory issues and unexpected Compute Engine virtual machine reboots.\n# Google Cloud Architecture Framework: Cost optimization\nThis category in the [Google Cloud Architecture Framework](/architecture/framework) provides design recommendations and describes best practices to help architects, developers, administrators, and other cloud practitioners optimize the cost of workloads in Google Cloud.\nMoving your IT workloads to the cloud can help you to innovate at scale, deliver features faster, and respond to evolving customer needs. To migrate existing workloads or deploy applications built for the cloud, you need a topology that's optimized for security, resilience, operational excellence, cost, and performance.\nIn the cost optimization category of the Architecture Framework, you learn to do the following:\n- [Adopt and implement FinOps](/architecture/framework/cost-optimization/finops) : Strategies to help you encourage employees to consider the cost impact when provisioning and managing resources in Google Cloud.\n- [Monitor and control cost](/architecture/framework/cost-optimization/monitor) : Best practices, tools, and techniques to track and control the cost of your resources in Google Cloud.\n- [Optimize cost: Compute, containers, and serverless](/architecture/framework/cost-optimization/compute) : Service-specific cost-optimization controls for Compute Engine, Google Kubernetes Engine, Cloud Run, Cloud Functions, and App Engine.\n- [Optimize cost: Storage](/architecture/framework/cost-optimization/storage) : Cost-optimization controls for Cloud Storage, Persistent Disk, and Filestore.\n- [Optimize cost: Databases and smart analytics](/architecture/framework/cost-optimization/databases) : Cost-optimization controls for BigQuery, Bigtable, Spanner, Cloud SQL, Dataflow, and Dataproc.\n- [Optimize cost: Networking](/architecture/framework/cost-optimization/networking) : Cost-optimization controls for your networking resources in Google Cloud.\n- [Optimize cost: Cloud operations](/architecture/framework/cost-optimization/cloudops) : Recommendations to help you optimize the cost of monitoring and managing your resources in Google Cloud.# Adopt and implement FinOps\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) outlines strategies to help you consider the cost impact of your actions and decisions when provisioning and managing resources in Google Cloud. It discusses [FinOps](https://www.finops.org/introduction/what-is-finops/) , a practice that combines people, processes, and technology to promote financial accountability and the discipline of cost optimization in an organization, regardless of its size or maturity in the cloud.\nThe guidance in this section is intended for CTOs, CIOs, and executives responsible for controlling their organization's spend in the cloud. The guidance also helps individual cloud operators understand and adopt FinOps.\nEvery [employee](https://www.finops.org/framework/personas/) in your organization can help reduce the cost of your resources in Google Cloud, regardless of role (analyst, architect, developer, or administrator). In teams that have not had to track infrastructure costs in the past, you might have to educate employees about the need for collective responsibility.\nA common model is for a central FinOps team or [Cloud Center of Excellence (CCoE)](https://services.google.com/fh/files/misc/cloud_center_of_excellence.pdf) to standardize the process for optimizing cost across all the cloud workloads. This model assumes that the central team has the required knowledge and expertise to identify high-value opportunities to improve efficiency.\nAlthough centralized cost-control might work well in the initial stages of cloud adoption when usage is low, it doesn't scale well when cloud adoption and usage increase. The central team might struggle with scaling, and project teams might not accept decisions made by anyone outside their teams.\nWe recommend that the central team delegate the decision making for resource optimization to the project teams. The central team can drive broader efforts to encourage the adoption of FinOps across the organization. To enable the individual project teams to practice FinOps, the central team must standardize the process, reporting, and tooling for cost optimization. The central team must work closely with teams that aren't familiar with FinOps practices, and help them consider cost in their decision-making processes. The central team must also act as an intermediary between the finance team and the individual project teams.\nThe next sections describe the design principles that we recommend your central team promote.\n## Encourage individual accountability\nAny employee who creates and uses cloud resources affects the usage and the cost of those resources. For an organization to succeed at implementing FinOps, the central team must help employees transition from viewing cost as someone else's responsibility, to owning cost as their own individual responsibility. With this transition, employees own and make cost decisions that are appropriate for their workloads, team, and the organization. This ownership extends to implementing data-driven cost-optimization actions.\nTo encourage accountability for cost, the central team can take the following actions:\n- Educate users about cost-optimization opportunities and techniques.\n- Reward employees who optimize cost, and celebrate success.\n- Make costs visible across the organization.## Make costs visible\nFor employees to consider cost when provisioning and managing resources in the cloud, they need a complete view of relevant data, as close to real time as possible. Data in reports and dashboards must show the cost and business impact of team members' decisions as the relevant impacts occur. Usage and cost data of other teams can serve as baselines for identifying efficient deployment patterns. This data can help promote a shared understanding of the best ways to use cloud services.\nIf an organization doesn't encourage and promote sharing cost data, employees might be reluctant to share data. Sometimes, for business reasons, an organization might not permit sharing of raw cost data. Even in these cases, we recommend that you avoid a default policy that restricts access to cost information.\nTo make costs visible across the organization, the central team can take the following actions:\n- Use a single, well-defined method for calculating the fully loaded costs of cloud resources. For example, the method could consider the total cloud spend adjusted for purchased discounts and shared costs, like the cost of shared databases.\n- Set up dashboards that enable employees to view their cloud spend in near real time.\n- To motivate individuals in the team to own their costs, allow wide visibility of cloud spending across teams.## Enable collaborative behavior\nEffective cost management for cloud resources requires that teams collaborate to improve their technical and operational processes. A collaborative culture helps teams design cost-effective deployment patterns based on a consistent set of business objectives and factors.\nTo enable collaborative behavior, the central team can take the following actions:\n- Create a workload-onboarding process that helps ensure cost efficiency in the design stage through peer reviews of proposed architectures by other engineers.\n- Create a cross-team knowledge base of cost-efficient architectural patterns.## Establish a blameless culture\nPromote a [culture of learning and growth](/solutions/devops/devops-culture-learning-culture#how_to_implement_a_learning_culture) that makes it safe to take risks, make corrections when required, and innovate. Acknowledge that mistakes, sometimes costly ones, can happen at any stage during the IT design and deployment lifecycle, as in any other part of the business.\nRather than blaming and shaming individuals who have overspent or introduced wastage, promote a blameless culture that helps identify the cause of cost overruns and miscalculations. In this environment, team members are more likely to share their views and experience. Mistakes are anonymized and shared across the business to prevent recurrence.\nDon't confuse a blameless culture with a lack of accountability. Employees continue to be accountable for the decisions they make and the money they spend. But when mistakes occur, the emphasis is on the learning opportunity to prevent the errors from occurring again.\nTo establish a blameless culture, the central team can take the following actions:\n- Run [blameless postmortems](https://sre.google/sre-book/postmortem-culture/) for major cost issues, focusing on the systemic root cause of the issues, rather than the people involved.\n- Celebrate team members who respond to cost overruns and who share lessons learned. Encourage other members in the team to share mistakes, actions taken, and lessons learned.## Focus on business value\nWhile FinOps practices are often focused on cost reduction, the focus for a central team must be on enabling project teams to make decisions that maximize the business value of their cloud resources. It can be tempting to make decisions that reduce cost to a point where the minimum service levels are met. But, such decisions often shift cost to other resources, can lead to higher maintenance cost, and might increase your total cost of ownership. For example, to reduce cost, you might decide to use virtual machines (VMs) instead of a managed service. But, a VM-based solution requires more effort to maintain when compared with a managed service, and so the managed service might offer a higher net business value.\nFinOps practices can provide project teams the visibility and insights that they need to make architectural and operational decisions that maximize the business value of their cloud resources.\nTo help employees focus on business value, the central team can take the following actions:\n- Use managed services and [serverless architectures](/serverless) to reduce the total cost of ownership of your compute resources. For more information, see [Choose a compute platform](/architecture/framework/system-design/compute#platform) .\n- Correlate cloud usage to business-value metrics like cost efficiency, resilience, feature velocity, and innovation that drive cost-optimization decisions. To learn more about business-value metrics, see the [Cloud FinOps whitepaper](/resources/cloud-finops-whitepaper) .\n- Implement [unit costing](https://info.sada.com/whitepaper/next-frontier-cloud-finops) for all your applications and services running in the cloud.## What's next\n- Learn more about FinOps:- [Maximize Business Value with Cloud FinOps](/resources/cloud-finops-whitepaper) \n- [Get started with FinOps on Google Cloud](/resources/cloud-finops-getting-started-whitepaper) \n- [FinOps Foundation](http://finops.org) \n- [Unit costing in the cloud](https://info.sada.com/whitepaper/next-frontier-cloud-finops) \n- [Monitor and control cost](/architecture/framework/cost-optimization/monitor) \n- Optimize cost for Google Cloud services:- [Optimize cost: Compute, containers, and serverless](/architecture/framework/cost-optimization/compute) \n- [Optimize cost: Storage](/architecture/framework/cost-optimization/storage) \n- [Optimize cost: Databases and smart analytics](/architecture/framework/cost-optimization/databases) \n- [Optimize cost: Networking](/architecture/framework/cost-optimization/networking) \n- [Optimize cost: Cloud operations](/architecture/framework/cost-optimization/cloudops) \n- Explore the other categories of the [Google Cloud Architecture Framework](/architecture/framework) # Monitor and control cost\nThis document in [Google Cloud Architecture Framework](/architecture/framework) describes best practices, tools, and techniques to help you track and control the cost of your resources in Google Cloud.\nThe guidance in this section is intended for users who provision or manage resources in the cloud.\n## Cost-management focus areas\nThe cost of your resources in Google Cloud depends on the quantity of resources that you use and the rate at which you're billed for the resources.\nTo manage the cost of cloud resources, we recommend that you focus on the following areas:\n- Cost visibility\n- Resource optimization\n- Rate optimization\n### Cost visibility\nTrack how much you spend and how your resources and services are billed, so that you can analyze the effect of cost on business outcomes. We recommend that you follow the [FinOps operating model](/resources/cloud-finops-getting-started-whitepaper) , which suggests the following actions to make cost information visible across your organization:\n- **Allocate** : Assign an owner for every cost item.\n- **Report** : Make cost data available, consumable, and actionable.\n- **Forecast** : Estimate and track future spend.\n### Resource optimization\nAlign the number and size of your cloud resources to the requirements of your workload. Where feasible, consider using managed services or re-architecting your applications. Typically, individual engineering teams have more context than the central [FinOps](/architecture/framework/cost-optimization/finops) (financial operations) team on opportunities and techniques to optimize resource deployment. We recommend that the FinOps team work with the individual engineering teams to identify resource-optimization opportunities that can be applied across the organization.\n### Rate optimization\nThe FinOps team often makes rate optimization decisions centrally. We recommend that the individual engineering teams work with the central FinOps team to take advantage of deep discounts for reservations, committed usage, Spot VMs, flat-rate pricing, and volume and contract discounting.\n## Design recommendations\nThis section suggests approaches that you can use to monitor and control costs.\n### Consolidate billing and resource management\nTo manage billing and resources in Google Cloud efficiently, we recommend that you use a single billing account for your organization, and use internal chargeback mechanisms to allocate costs. Use multiple billing accounts for loosely structured conglomerates and organizations with entities that don't affect each other. For example, resellers might need distinct accounts for each customer. Using separate billing accounts might also help you meet country-specific tax regulations.\nAnother recommended best practice is to move all the projects that you manage into your organization. We recommend using Resource Manager to build a [resource hierarchy](/resource-manager/docs/cloud-platform-resource-hierarchy#resource-hierarchy-detail) that helps you achieve the following goals:\n- Establish a hierarchy of resource-ownership based on the relationship of each resource to its immediate parent.\n- Control how access policies and cost-allocation tags or labels are attached to and inherited by the resources in your organization.\nIn addition, we recommend that you allocate the cost of shared services proportionally based on consumption. Review and adjust the cost allocation parameters periodically based on changes in your business goals and priorities.\n### Track and allocate cost using tags or labels\nTags and labels are two different methods that you can use to annotate your Google Cloud resources. Tags provide more capabilities than labels. For example, you can implement fine-grained control over resources by creating Identity and Access Management (IAM) policies that are conditional based on whether a tag is attached to a supported resource. In addition, the tags that are associated with a resource are inherited by all the child resources in the hierarchy. For more information about the differences between tags and labels, see [Tags overview](https://cloud.google.com/resource-manager/docs/tags/tags-overview#tags_and_labels) .\nIf you're building a new framework for cost allocation and tracking, we recommend using tags.\nTo categorize cost data at the required granularity, establish a tagging or labeling schema that suits your organization's chargeback mechanism and helps you allocate costs appropriately. You can define tags at the organization or project level. You can assign labels at the project level, and define a set of labels that can be applied by default to all the projects.\nDefine a process to detect and correct tagging and labeling anomalies and unlabeled projects. For example, from [Cloud Asset Inventory](https://console.cloud.google.com/iam-admin/asset-inventory/resources) , you can download an inventory ( `.csv` file) of all the resources in a project and analyze the inventory to identify resources that aren't assigned any tags or labels.\nTo track the cost of shared resources and services (for example, common datastores, multi-tenant clusters, and support subscriptions), consider using a special tag or label to identify projects that contain shared resources.\n### Configure billing access control\nTo control access to Cloud Billing, we recommend that you assign the Billing Account Administrator role to only those users who manage billing contact information. For example, employees in finance, accounting, and operations might need this role.\nTo avoid a single point of failure for billing support, assign the Billing Account Administrator role to multiple users or to a group. Only users with the Billing Account Administrator role can contact support. For detailed guidance, see [Cloud Billing access control examples](/billing/docs/how-to/billing-access#cloud-billing_access-control-examples) and [Important Roles](/billing/docs/concepts#important_roles) .\nMake the following configurations to manage access to billing:\n- To associate a billing account with a project, members need the Billing Account User role on the billing account and the Project Billing Manager role on the project.\n- To enable teams to manually associate billing accounts with projects, you can assign the Project Billing Manager role at the organization level and the Billing Account User role on the billing account. You can automate the association of billing accounts during project creation by assigning the Project Billing Manager and Billing Account User roles to a [service account](/iam/docs/service-accounts) . We recommend that you restrict the Billing Account Creator role or remove all assignments of this role.\n- To prevent outages caused by unintentional changes to the billing status of a project, you can lock the link between the project and its billing account. For more information, see [Secure the link between a project and its billing account](/billing/docs/how-to/secure-project-billing-account-link) .\n### Configure billing reports\nSet up [billing reports](/billing/docs/how-to/reports) to provide data for the key metrics that you need to track. We recommend that you track the following metrics:\n- Cost trends\n- Largest spenders (by project and by product)\n- Areas of irregular spending\n- Key organization-wide insights as follows:- Anomaly detection\n- Trends over time\n- Trends that occur in a set pattern (for example, month-on-month)\n- Cost comparison and benchmark analysis between internal and external workloads\n- Business case tracking and value realization (for example, cloud costs compared with the cost of similar on-premises resources)\n- Validation that Google Cloud bills are as expected and accurate\n### Analyze trends and forecast cost\nCustomize and analyze cost reports using [BigQuery Billing Export](/billing/docs/how-to/export-data-bigquery) , and visualize cost data using [Looker Studio](https://lookerstudio.google.com/overview) . Assess the trend of actual costs and how much you might spend by using the [forecasting tool](/billing/docs/how-to/reports#cost-forecast) .\n### Optimize resource usage and cost\nThis section recommendeds best practices to help you optimize the usage and cost of your resources across Google Cloud services.\nTo prevent overspending, consider configuring default budgets and alerts with high thresholds for all your projects. To help keep within budgets, we recommend that you do the following:\n- Configure [budgets and alerts](/billing/docs/how-to/budgets) for projects where absolute usage limits are necessary (for example, training or sandbox projects).\n- Define budgets based on the financial budgets that you need to track. For example, if a department has an overall cloud budget, set the scope of the Google Cloud budget to include the specific projects that you need to track.\n- To ensure that budgets are maintained, delegate the responsibility for configuring budgets and alerts to the teams that own the workloads.\nTo help optimize costs, we also recommend that you do the following:\n- [Cap API usage](/apis/docs/capping-api-usage) in cases where it has minimal or no business impact. Capping can be useful for sandbox or training projects and for projects with fixed budgets (for example, ad-hoc analytics in BigQuery). Capping doesn't remove all the resources and data from the associated projects.\n- Use [quotas](/docs/quota) to set hard limits that throttle resource deployment. Quotas help you control cost and prevent malicious use or misuse of resources. Quotas are applied at the project level, per resource type and location.\n- View and implement the cost-optimization recommendations in the [Recommendation Hub](/recommender/docs/recommendation-hub/identify-configuration-problems) .\n- Purchase [committed use discounts (CUD)](/docs/cuds) to save money on resources for workloads with predictable resource needs.## Tools and techniques\nThe on-demand provisioning and pay-per-use characteristics of the cloud help you to optimize your IT spend. This section describes tools that Google Cloud provides and techniques that you can use to track and control the cost of your resources in the cloud. Before you use these tools and techniques, review the [basic Cloud Billing concepts](/billing/docs/concepts) .\n### Billing reports\nGoogle Cloud provides [billing reports](/billing/docs/how-to/reports) within the Google Cloud console to help you view your current and forecasted spend. The billing reports enable you to view cost data on a single page, discover and analyze trends, forecast the end-of-period cost, and take corrective action when necessary.\nBilling reports provide the following data:\n- The costs and cost trends for a given period, organized as follows:- By billing account\n- By project\n- By product (for example, Compute Engine)\n- By [SKU](/skus) (for example, static IP addresses)\n- The potential costs if discounts or promotional credits were excluded\n- The forecasted spend\n### Data export to BigQuery\nYou can [export billing reports to BigQuery](/billing/docs/how-to/export-data-bigquery) , and analyze costs using granular and historical views of data, including data that's categorized using labels or tags. You can perform more advanced analyses using BigQuery ML. We recommend that you enable export of billing reports to BigQuery when you create the Cloud Billing account. Your BigQuery dataset contains billing data from the date you set up Cloud Billing export. The dataset doesn't include data for the period before you enabled export.\n**Note:** Exported billing data might be different from the data in billing invoices, due to late-reported data, timestamp variations, errors and adjustments, and taxes. For more information, see [Differences between exported data and invoices](/billing/docs/how-to/export-data-bigquery-tables#differences_between_exported_data_and_invoices) .\nTo visualize cost data, you can create custom dashboards that integrate with BigQuery (example templates: [Looker](/architecture/reference-patterns/overview#understanding-and-optimizing-your-google-cloud-spend) , [Looker Studio](https://github.com/GoogleCloudPlatform/professional-services/tree/main/examples/cost-optimization-dashboard) ).\nYou can use tags and labels as criteria for filtering the exported billing data. The number of labels included in the billing export is limited. Up to a 1,000 label-maps within a a period of one hour are preserved. Labels don't appear in the [invoice PDF or CSV](/billing/docs/how-to/read-invoice) . Consider annotating resources by using tags or labels that indicate the business unit, internal chargeback unit, and other relevant metadata.\n### Billing access control\nYou can [control access to Cloud Billing](/billing/docs/how-to/billing-access) for specific resources by defining Identity and Access Management (IAM) policies for the resources. To grant or limit access to Cloud Billing, you can set an IAM policy at the organization level, the billing account level, or the project level.\n**Note:** Although billing accounts are linked to projects, billing accounts are not parents of projects in IAM. Projects don't inherit permissions from the billing account that they are linked to.\nAccess control for billing and resource management follows the principle of separation of duties. Each user has only the permissions necessary for their business role. The Organization Administrator and Billing Administrator roles don't have the same permissions.\nYou can set billing-related permissions at the billing account level and the organization level. The common roles are Billing Account Administrator, Billing Account User, and Billing Account Viewer.\nWe recommend that you use [invoiced billing](/billing/docs/how-to/invoiced-billing) , or configure a [backup payment method](/billing/docs/how-to/payment-methods) . Maintain [contact and notification settings](/billing/docs/how-to/modify-contacts) for billing and payment.\n### Budgets, alerts, and quotas\n[Budgets](/billing/docs/how-to/budgets) help you track actual Google Cloud costs against planned spending. When you create a budget, you can configure alert rules to trigger email notifications when the actual or forecasted spend exceeds a defined threshold. You can also use budgets to automate cost-control responses.\nBudgets can trigger alerts to inform you about resource usage and cost trends, and prompt you to take cost-optimization actions. However, budgets don't prevent the use or billing of your services when the actual cost reaches or exceeds the budget or threshold. To automatically control cost, you can use budget notifications to programmatically disable Cloud Billing for a project. You can also [limit API usage](/apis/docs/capping-api-usage) to stop incurring cost after a defined usage threshold.\nYou can configure [alerts](/billing/docs/how-to/budgets#email-notifications) for billing accounts and projects. Configure at least one budget for an account.\nTo prevent provisioning resources beyond a predetermined level or to limit the volume of specific operations, you can set [quotas](/docs/quota) at the resource or API level. The following are examples of how you can use quotas:\n- Control the number of API calls per second.\n- Limit the number of VMs created.\n- Restrict the amount of data queried per day in BigQuery.\nProject owners can reduce the amount of quota that can be charged against a quota limit, by using the Service Usage API to apply consumer overrides to specific quota limits. For more information, see [Creating a consumer quota override](/service-usage/docs/manage-quota#create_consumer_quota_override) .\n### Workload efficiency improvement\nWe recommend the following strategies to help make your workloads in Google Cloud cost-efficient:\n- Optimize resource usage by improving product efficiency.\n- Reduce the rate at which you're billed for resources.\n- Control and limit resource usage and spending.\nWhen selecting cost-reduction techniques and Google Cloud features, consider the effort required and the expected savings, as shown in the following graph:\nThe following is a summary of the techniques shown in the preceding graph:\n- The following techniques potentially yield high savings with low effort:- Committed use discounts\n- Autoscaling\n- BigQuery slots\n- The following techniques potentially yield high savings with moderate-to-high effort:- Spot VMs\n- Re-architecting as serverless or containerized applications\n- Re-platforming to use managed services\n- The following techniques potentially yield moderate savings with moderate effort:- Custom machine types\n- Cloud Storage lifecycle management\n- Rightsizing\n- Reclaiming idle resourcesThe techniques explained in the following sections can help you improve the efficiency of your workloads.\nYou can achieve substantial cost savings by refactoring or [re-architecting](/rearchitecting-to-cloud-native-whitepaper) your workload to use Google Cloud products. For example, moving to [serverless services](/serverless/whitepaper) (like Cloud Storage, Cloud Run, BigQuery, and Cloud Functions) that support scaling to zero can help improve efficiency. To assess and compare the cost of these products, you can use the [pricing calculator](/products/calculator) .\nThis technique helps you ensure that the scale of your infrastructure matches the intended usage. This strategy is relevant primarily to infrastructure-as-a-service (IaaS) solutions, where you pay for the underlying infrastructure. For example, you've deployed 50 VMs, but the VMs aren't fully utilized, and you determine that the workloads could run effectively on fewer (or smaller) VMs. In this case, you can remove or resize some of the VMs. Google Cloud provides [rightsizing recommendations](/compute/docs/instances/apply-machine-type-recommendations-for-instances) to help you detect opportunities to save money without affecting performance by provisioning smaller VMs. Rightsizing requires less effort if done during the design phase than after deploying resources to production.\nIf the products you use support dynamic autoscaling, consider designing the workloads to take advantage of autoscaling to get cost and performance benefits. For example, for compute-intensive workloads, you can use managed instance groups in Compute Engine or containerize the applications and deploy them to a Google Kubernetes Engine cluster.\n### Active Assist recommendations\n[Active Assist](/solutions/active-assist) uses data, intelligence, and machine learning to reduce cloud complexity and administrative effort. Active Assist makes it easy to optimize the security, performance, and cost of your cloud topology. It provides [intelligent recommendations](/recommender) for optimizing your costs and usage. You can apply these recommendations for immediate cost savings and greater efficiency.\nThe following are examples of recommendations provided by Active Assist:\n- Compute Engine resource rightsizing: Resize your [VM instances](/compute/docs/instances/apply-sizing-recommendations-for-instances) to optimize for cost and performance based on usage. Identify and delete or back up [idle VMs](/compute/docs/instances/viewing-and-applying-idle-vm-recommendations) and [persistent disks](/compute/docs/disks/viewing-and-applying-idle-pd-recommendations) to optimize your infrastructure cost.\n- Committed-use discount (CUD): Google Cloud analyzes your historical usage, finds the optimal commitment quantity for your workloads, and provides easy-to-understand, actionable recommendations for cost savings. For more information, see [Committed use discount recommender](/docs/cuds-recommender) .\n- Unattended projects: Discover unattended projects in your organization, and remove or reclaim them. For more information, see [Unattended project recommender](/recommender/docs/unattended-project-recommender) .\nFor a complete list, see [Recommenders](/recommender/docs/recommenders) .\n## What's next\n- [Learn about Cloud Billing concepts](/billing/docs/concepts) \n- [Set budgets and budget alerts](/billing/docs/how-to/budgets) \n- Optimize cost for compute services, storage, databases, networking, and operations:- [Optimize cost: Compute, containers, and serverless](/architecture/framework/cost-optimization/compute) \n- [Optimize cost: Storage](/architecture/framework/cost-optimization/storage) \n- [Optimize cost: Databases and smart analytics](/architecture/framework/cost-optimization/databases) \n- [Optimize cost: Networking](/architecture/framework/cost-optimization/networking) \n- [Optimize cost: Cloud operations](/architecture/framework/cost-optimization/cloudops) \n- Explore the other categories of the [Google Cloud Architecture Framework](/architecture/framework) # Optimize cost: Compute, containers, and serverless\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides recommendations to help you optimize the cost of your virtual machines (VMs), containers, and serverless resources in Google Cloud.\nThe guidance in this section is intended for architects, developers, and administrators who are responsible for provisioning and managing compute resources for workloads in the cloud.\nCompute resources are the most important part of your cloud infrastructure. When you migrate your workloads to Google Cloud, a typical first choice is Compute Engine, which lets you provision and manage VMs efficiently in the cloud. Compute Engine offers a wide range of machine types, and is available globally in all the [Google Cloud regions](/about/locations#products-available-by-location) . Compute Engine's predefined and custom machine types let you provision VMs that offer similar compute capacity as your on-premises infrastructure, enabling you to accelerate the migration process. Compute Engine gives you the pricing advantage of paying only for the infrastructure that you use and provides significant savings as you use more compute resources with sustained-use discounts.\nIn addition to Compute Engine, Google Cloud offers containers and serverless compute services. The serverless approach can be more cost-efficient for new services that aren't always running (for example, APIs, data processing, and event processing).\nAlong with general recommendations, this document provides guidance to help you optimize the cost of your compute resources when using the following products:\n- Compute Engine\n- Google Kubernetes Engine (GKE)\n- Cloud Run\n- Cloud Functions\n- App Engine## General recommendations\nThe following recommendations are applicable to all the compute, containers, and serverless services in Google Cloud that are discussed in this section.\n### Track usage and cost\nUse the following tools and techniques to monitor resource usage and cost:\n- View and respond to cost-optimization recommendations in the [Recommendation Hub](/recommender/docs/recommendation-hub/identify-configuration-problems) .\n- Get email notifications for potential increases in resource usage and cost by [configuring budget alerts](/billing/docs/how-to/budgets) .\n- [Manage and respond to alerts programmatically](/billing/docs/how-to/budgets-programmatic-notifications) by using the Pub/Sub and Cloud Functions services.\n### Control resource provisioning\nUse the following recommendations to control the quantity of resources provisioned in the cloud and the location where the resources are created:\n- To help ensure that resource consumption and cost don't exceed the forecast, use resource [quotas](https://console.cloud.google.com/iam-admin/quotas) .\n- Provision resources in the lowest-cost region that meets the latency requirements of your workload. To control where resources are provisioned, you can use the organization policy constraint [gcp.resourceLocations](/resource-manager/docs/organization-policy/org-policy-constraints#constraints-supported-by-multiple-google-cloud-services) .\n### Get discounts for committed use\n[Committed use discounts (CUDs)](/docs/cuds) are ideal for workloads with predictable resource needs. After migrating your workload to Google Cloud, find the baseline for the resources required, and get deeper discounts for committed usage. For example, purchase a one or three-year commitment, and get a substantial discount on Compute Engine VM pricing.\n### Automate cost-tracking using labels\nDefine and assign [labels](/resource-manager/docs/creating-managing-labels) consistently. The following are examples of how you can use labels to automate cost-tracking:\n- For VMs that only developers use during business hours, assign the label `env: development` . You can use Cloud Scheduler to set up a serverless Cloud Function to shut down these VMs after business hours, and restart them when necessary.\n- For an application that has several Cloud Run services and Cloud Functions instances, assign a consistent label to all the [Cloud Run](/run/docs/configuring/labels) and [Cloud Functions](/sdk/gcloud/reference/functions/deploy#--update-labels) resources. Identify the high-cost areas, and take action to reduce cost.\n### Customize billing reports\nConfigure your [Cloud Billing reports](https://console.cloud.google.com/billing/reports) by setting up the required filters and grouping the data as necessary (for example, by projects, services, or labels).\n### Promote a cost-saving culture\nTrain your developers and operators on your cloud infrastructure. Create and promote learning programs using traditional or online classes, discussion groups, peer reviews, pair programming, and cost-saving games. As shown in [Google's DORA research](/devops) , organizational culture is a key driver for improving performance, reducing rework and burnout, and optimizing cost. By giving employees visibility into the cost of their resources, you help them align their priorities and activities with business objectives and constraints.\n## Compute Engine\nThis section provides guidance to help you optimize the cost of your Compute Engine resources. In addition to this guidance, we recommend that you follow the [general recommendations](#general_recommendations) discussed earlier.\n### Understand the billing model\nTo learn about the billing options for Compute Engine, see [Pricing](/compute/all-pricing) .\n### Analyze resource consumption\nTo help you to understand resource consumption in Compute Engine, export usage data to BigQuery. Query the BigQuery datastore to analyze your project's virtual CPU (vCPU) usage trends, and determine the number of vCPUs that you can reclaim. If you've defined thresholds for the number of cores per project, analyze usage trends to spot anomalies and take corrective actions.\n### Reclaim idle resources\nUse the following recommendations to identify and reclaim unused VMs and disks, such as VMs for proof-of-concept projects that have since been deprioritized:\n- Use the [idle VM recommender](/compute/docs/instances/viewing-and-applying-idle-vm-recommendations) to identify inactive VMs and persistent disks based on usage metrics.\n- Before deleting resources, assess the potential impact of the action and plan to recreate the resources if that becomes necessary.\n- Before deleting a VM, consider [taking a snapshot](/compute/docs/disks/create-snapshots) . When you delete a VM, the attached disks are deleted, unless you've selected the **Keep disk** option.\n- When feasible, consider stopping VMs instead of deleting them. When you stop a VM, the instance is terminated, but disks and IP addresses are retained until you detach or delete them.\n### Adjust capacity to match demand\nSchedule your VMs to start and stop automatically. For example, if a VM is used only eight hours a day for five days a week (that's 40 hours in the week), you can potentially reduce costs by 75 percent by stopping the VM during the 128 hours in the week when the VM is not used.\nAutoscale compute capacity based on demand by using [managed instance groups](/compute/docs/autoscaler) . You can autoscale capacity based on the parameters that matter to your business (for example, CPU usage or load-balancing capacity).\n### Choose appropriate machine types\nSize your VMs to match your workload's compute requirements by using the [VM machine type recommender](/compute/docs/instances/apply-machine-type-recommendations-for-instances) .\nFor workloads with predictable resource requirements, tailor the machine type to your needs and save money by using [custom VMs](/compute/docs/instances/creating-instance-with-custom-machine-type#specifications) .\nFor batch-processing workloads that are fault-tolerant, consider using [Spot VMs](/compute/docs/instances/spot) . High-performance computing (HPC), big data, media transcoding, continuous integration and continuous delivery (CI/CD) pipelines, and stateless web applications are examples of workloads that can be deployed on Spot VMs. For an example of how Descartes Labs reduced their analysis costs by using preemptible VMs (the older version of Spot VMs) to process satellite imagery, see the [Descartes Labs case study](/customers/descartes-labs) .\n### Evaluate licensing options\nWhen you migrate third-party workloads to Google Cloud, you might be able to reduce cost by bringing your own licenses (BYOL). For example, to deploy Microsoft Windows Server VMs, instead of using a [premium image](/compute/disks-image-pricing#premiumimages) that incurs additional cost for the third-party license, you can create and use a [custom Windows BYOL image](/compute/docs/images/creating-custom-windows-byol-images) . You then pay only for the VM infrastructure that you use on Google Cloud. This strategy helps you continue to realize value from your existing investments in third-party licenses.\nIf you decide to use a BYOL approach, we recommend that you do the following:\n- Provision the required number of compute CPU cores independently of memory by using [custom machine types](/compute/docs/instances/creating-instance-with-custom-machine-type#extendedmemory) , and limit the third-party licensing cost to the number of CPU cores that you need.\n- Reduce the number of vCPUs per core from 2 to 1 by disabling [simultaneous multithreading](/compute/docs/instances/configuring-simultaneous-multithreading) (SMT), and reduce your licensing costs by 50 percent.\nIf your third-party workloads need dedicated hardware to meet security or compliance requirements, you can bring your own licenses to [sole-tenant nodes](/compute/docs/nodes/bringing-your-own-licenses) .\n## Google Kubernetes Engine\nThis section provides guidance to help you optimize the cost of your GKE resources.\nIn addition to the following recommendations, see the [general recommendations](#general_recommendations) discussed earlier:\n- Use [GKE Autopilot](/kubernetes-engine/docs/concepts/autopilot-overview) to let GKE maximize the efficiency of your cluster's infrastructure. You don't need to monitor the health of your nodes, handle bin-packing, or calculate the capacity that your workloads need.\n- Fine-tune GKE autoscaling by using Horizontal Pod Autoscaler ( [HPA](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#horizontal_pod_autoscaler) ), Vertical Pod Autoscaler ( [VPA](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#vertical_pod_autoscaler) ), Cluster Autoscaler ( [CA](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#cluster_autoscaler) ), or [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) based on your workload's requirements.\n- For batch workloads that aren't sensitive to startup latency, use the [optimization-utilization](/kubernetes-engine/docs/concepts/cluster-autoscaler#autoscaling_profiles) autoscaling profile to help improve the utilization of the cluster.\n- Use [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) to extend the GKE cluster autoscaler, and efficiently create and delete node pools based on the [specifications](https://kubernetes.io/docs/concepts/workloads/pods) of pending pods without over-provisioning.\n- Use separate node pools: a static node pool for static load, and dynamic node pools with cluster autoscaling groups for dynamic loads.\n- Use [Spot VMs](/kubernetes-engine/docs/how-to/spot-vms) for Kubernetes node pools when your pods are fault-tolerant and can terminate gracefully in less than 25 seconds. Combined with the GKE cluster autoscaler, this strategy helps you ensure that the node pool with lower-cost VMs (in this case, the node pool with Spot VMs) scales first.\n- Choose [cost-efficient machine types](/compute#section-6) (for example: [E2](/compute/docs/general-purpose-machines#e2_machine_types) , [N2D](/blog/products/compute/announcing-the-n2d-vm-family-based-on-amd) , [T2D](/blog/products/compute/google-cloud-introduces-tau-vms) ), which provide 20\u201340% higher performance-to-price.\n- Use [GKE usage metering](/kubernetes-engine/docs/how-to/cluster-usage-metering) to analyze your clusters' usage profiles by namespaces and labels. Identify the team or application that's spending the most, the environment or component that caused spikes in usage or cost, and the team that's wasting resources.\n- Use [resource quotas](https://kubernetes.io/docs/concepts/policy/resource-quotas/) in multi-tenant clusters to prevent any tenant from using more than its assigned share of cluster resources.\n- [Schedule automatic downscaling](/architecture/reducing-costs-by-scaling-down-gke-off-hours) of development and test environments after business hours.\n- Follow the [best practices](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke) for running cost-optimized Kubernetes applications on GKE.## Cloud Run\nThis section provides guidance to help you optimize the cost of your Cloud Run resources.\nIn addition to the following recommendations, see the [general recommendations](#general_recommendations) discussed earlier:\n- Adjust the [concurrency setting](/run/docs/about-concurrency) (default: 80) to reduce cost. Cloud Run determines the number of requests to be sent to an instance based on CPU and memory usage. By increasing the request concurrency, you can reduce the number of instances required.\n- [Set a limit](/run/docs/configuring/max-instances) for the number of instances that can be deployed.\n- Estimate the number of instances required by using the [Billable Instance Time](/monitoring/api/metrics_gcp#gcp-run) metric. For example, if the metric shows`100s/s`, around 100 instances were scheduled. Add a 30% buffer to preserve performance; that is, 130 instances for 100s/s of traffic.\n- To reduce the impact of cold starts, configure a [minimum number of instances](/run/docs/configuring/min-instances) . When these instances are idle, they are billed at a tenth of the price.\n- Track [CPU usage](/monitoring/api/metrics_gcp#gcp-run) , and adjust the CPU limits accordingly.\n- Use [ traffic management](/run/docs/rollouts-rollbacks-traffic-migration) to determine a cost-optimal configuration.\n- Consider using [Cloud CDN](/cdn/docs/setting-up-cdn-with-serverless) or Firebase Hosting for serving static assets.\n- For Cloud Run apps that handle requests globally, consider deploying the app to [multiple regions](/run/docs/multiple-regions) , because cross continent data transfer can be expensive. This design is recommended if you use a load balancer and CDN.\n- [Reduce the startup times](/run/docs/tips/general#starting_services_quickly) for your instances, because the startup time is also billable.\n- Purchase [Committed Use Discounts](/run/cud) , and save up to 17% off the on-demand pricing for a one-year commitment.## Cloud Functions\nThis section provides guidance to help you optimize the cost of your Cloud Functions resources.\nIn addition to the following recommendations, see the [general recommendations](#general_recommendations) discussed earlier:\n- Observe [the execution time](/monitoring/api/metrics_gcp#gcp-cloudfunctions) of your functions. Experiment and benchmark to design the smallest function that still meets your required performance threshold.\n- If your Cloud Functions workloads run constantly, consider using GKE or Compute Engine to handle the workloads. Containers or VMs might be lower-cost options for always-running workloads.\n- Limit the [number of function instances](/functions/docs/configuring/max-instances) that can co-exist.\n- Benchmark the runtime performance of the Cloud Functions [programming languages](/functions/docs/writing) against the workload of your function. Programs in compiled languages have longer cold starts, but run faster. Programs in interpreted languages run slower, but have a lower cold-start overhead. Short, simple functions that run frequently might cost less in an interpreted language.\n- [Delete temporary files](/functions/docs/bestpractices/tips#always_delete_temporary_files) written to the local disk, which is an in-memory file system. Temporary files consume memory that's allocated to your function, and sometimes persist between invocations. If you don't delete these files, an out-of-memory error might occur and trigger a cold start, which increases the execution time and cost.## App Engine\nThis section provides guidance to help you optimize the cost of your App Engine resources.\nIn addition to the following recommendations, see the [general recommendations](#general_recommendations) discussed earlier:\n- Set maximum instances based on your traffic and request latency. App Engine usually [scales capacity](/appengine/docs/standard/go/how-instances-are-managed#scaling_types) based on the traffic that the applications receive. You can control cost by limiting the number of instances that App Engine can create.\n- To limit the memory or CPU available for your application, set an [instance class](/appengine/docs/standard#instance_classes) . For CPU-intensive applications, allocate more CPU. Test a few configurations to determine the optimal size.\n- Benchmark your App Engine workload in multiple [programming languages](/appengine/docs) . For example, a workload implemented in one language may need fewer instances and lower cost to complete tasks on time than the same workload programmed in another language.\n- Optimize for fewer cold starts. When possible, reduce CPU-intensive or long-running tasks that occur in the global scope. Try to break down the task into smaller operations that can be \"lazy loaded\" into the context of a request.\n- If you expect bursty traffic, configure a [minimum number of idle instances](/appengine/docs/standard/go/how-instances-are-managed#scaling_types) that are pre-warmed. If you are not expecting traffic, you can configure the minimum idle instances to zero.\n- To balance performance and cost, run an A/B test by [splitting traffic](/appengine/docs/standard/go/splitting-traffic) between two versions, each with a different configuration. Monitor the performance and cost of each version, tune as necessary, and decide the configuration to which traffic should be sent.\n- Configure [request concurrency](/appengine/docs/standard/go/how-requests-are-handled#handling_requests) , and set the maximum concurrent requests higher than the default. The more requests each instance can handle concurrently, the more efficiently you can use existing instances to serve traffic.## What's next\n- Learn about optimizing the cost of GKE resources:- [Run cost-optimized Kubernetes applications on GKE: best practices](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke) \n- [Best practices for reducing GKE over-provisioning](/blog/products/containers-kubernetes/gke-best-practices-to-lessen-over-provisioning) \n- [Optimize resource usage in a multi-tenant GKE cluster using node auto-provisioning](/architecture/optimizing-resources-in-multi-tenant-gke-clusters-with-auto-provisioning) \n- [Reduce costs by scaling down GKE clusters during off-peak hours](/architecture/reducing-costs-by-scaling-down-gke-off-hours) \n- [Run web applications on GKE using cost-optimized Spot VMs](/kubernetes-engine/docs/archive/run-web-applications-on-gke-using-cost-optimized-spot-vms-and-traffic-director) \n- [Estimate your GKE costs early in the development cycle using GitLab](/architecture/estimate-gke-costs-early-using-gitlab) \n- [Video series: GKE Cost Optimization](https://www.youtube.com/playlist?list=PLIivdWyY5sqIUx9ZVsn4BzaIVTRAWYPxi) \n- [Training: Optimize Costs for GKE](https://www.cloudskillsboost.google/quests/157) \n- Learn about optimizing the cost of Cloud Run and Cloud Functions resources:- [Manage cost and reliability in fully managed applications](/blog/products/serverless/managing-cost-and-reliability-serverless-applications) \n- [Optimize for long-term cost management in fully managed applications](/blog/products/serverless/cost-optimization-for-serverless-workloads) \n- [Maximize your Cloud Run investments with new committed use discounts](/blog/products/serverless/introducing-committed-use-discounts-for-cloud-run) \n- Optimize cost for storage, databases, networking, and operations:- [Optimize cost: Storage](/architecture/framework/cost-optimization/storage) \n- [Optimize cost: Databases and smart analytics](/architecture/framework/cost-optimization/databases) \n- [Optimize cost: Networking](/architecture/framework/cost-optimization/networking) \n- [Optimize cost: Cloud operations](/architecture/framework/cost-optimization/cloudops) \n- Explore the other categories of the [Google Cloud Architecture Framework](/architecture/framework) # Optimize cost: Storage\nThis document in [Google Cloud Architecture Framework](/architecture/framework) provides recommendations to help you optimize the usage and cost of your Cloud Storage, Persistent Disk, and Filestore resources.\nThe guidance in this section is intended for architects and administrators responsible for provisioning and managing storage for workloads in the cloud.\n**Note:** For guidance on selecting storage services that are appropriate for your workload, see [Design an optimal storage strategy for your cloud workload](/architecture/storage-advisor) .\n## Cloud Storage\nWhen you plan [Cloud Storage](/storage) for your workloads, consider your requirements for performance, data retention, and access patterns.\n### Storage class\nChoose a [storage class](/storage/docs/storage-classes) that suits the data-retention and access-frequency requirements of your workloads, as recommended in the following table:\n| Storage requirement                                | Recommendation |\n|:-------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------|\n| Data that's accessed frequently (high-throughput analytics or data lakes, websites, streaming videos, and mobile apps).       | Standard storage |\n| Low-cost storage for infrequently accessed data that can be stored for at least 30 days (for example, backups and long-tail multimedia content). | Nearline storage |\n| Infrequently accessed data that can be stored for at least 90 days (for example, data replicas for disaster recovery).       | Coldline storage |\n| Lowest-cost storage for infrequently accessed data that can be stored for at least 365 days (for example, legal and regulatory archives).  | Archive storage |\n**Note:** The Nearline, Coldline, and Archive storage classes cost less than Standard storage, but they incur [higher data retrieval charges](/storage/pricing#retrieval-pricing) and an [early-deletion penalty](/storage/pricing#early-delete) .\n### Location\nSelect the [location](/storage/docs/locations) for your buckets based on your requirements for performance, availability, and data redundancy.\n- **Regions** are recommended when the region is close to your end users. You can select a specific region, and get guaranteed redundancy within the region. Regions offer fast, redundant, and affordable storage for datasets that users within a particular geographical area access frequently.\n- **Multi-regions** provide high availability for distributed users. However, the storage cost is higher than for regions. Multi-region buckets are recommended for content-serving use cases and for low-end analytics workloads.\n- **Dual-regions** provide high availability and data redundancy. Google recommends dual-region buckets for high-performance analytics workloads and for use cases that require true active-active buckets with compute and storage colocated in multiple locations. Dual-regions let you choose where your data is stored, which can help you meet compliance requirements. For example, you can use a dual-region bucket to meet industry-specific requirements regarding the physical distance between copies of your data in the cloud.\n### Lifecycle policies\nOptimize storage cost for your objects in Cloud Storage by defining [lifecycle policies](/storage/docs/lifecycle) . These policies help you save money by automatically downgrading the storage class of specific objects or deleting objects based on [conditions](/storage/docs/lifecycle#conditions) that you set.\nConfigure lifecycle policies based on how frequently objects are accessed and how long you need to retain them. The following are examples of lifecycle policies:\n- Downgrade policy: You expect a dataset to be accessed frequently but for only around three months. To optimize the storage cost for this dataset, use Standard storage, and configure a lifecycle policy to downgrade objects older than 90 days to Coldline storage.\n- Deletion policy: A dataset must be retained for 365 days to meet certain legal requirements and can be deleted after that period. Configure a policy to delete any object that's older than 365 days.To help you ensure that data that needs to be retained for a specific period (for legal or regulatory compliance) is not deleted before that date or time, configure [retention policy locks](/storage/docs/bucket-lock#policy-locks) .\n**Note:** The long-term storage classes (Nearline, Coldline, and Archive) incur [higher data retrieval charges](/storage/pricing#retrieval-pricing) and an [early-deletion penalty](/storage/pricing#early-delete) when compared with Standard storage. If data stored in a long-term storage class needs to be accessed frequently, to avoid increased access charges, you can copy the data to a Standard storage bucket.\n### Accountability\nTo drive accountability for operational charges, network charges, and data-retrieval cost, use the [Requester Pays](/storage/docs/requester-pays) configuration where appropriate. With this configuration, the costs are charged to the department or team that uses the data, rather than the owner.\nDefine and assign [cost-tracking](/blog/topics/cost-management/use-labels-to-gain-visibility-into-gcp-resource-usage-and-spending) labels consistently for all your buckets and objects. Automate labeling when feasible.\n### Redundancy\nUse the following techniques to maintain the required storage redundancy without data duplication:\n- To maintain data resilience with a single source of truth, use a [dual-region or multi-region bucket](/storage/docs/locations) rather than redundant copies of data in different buckets. Dual-region and multi-region buckets provide redundancy across regions. Your data is replicated asynchronously across two or more locations, and is protected against regional outages.\n- If you enable [object versioning](/storage/docs/object-versioning) , consider defining [lifecycle policies](/storage/docs/lifecycle) to remove the oldest version of an object as newer versions become noncurrent. Each noncurrent version of an object is charged at the same rate as the live version of the object.\n- Disable object versioning policies when they are no longer necessary.\n- Review your backup and snapshot [retention policies](/compute/docs/disks/scheduled-snapshots#retention_policy) periodically, and adjust them to avoid unnecessary backups and data retention.## Persistent Disk\nEvery VM instance that you deploy in Compute Engine has a boot disk, and (optionally) one or more data disks. Each disk incurs cost depending on the provisioned size, region, and disk type. Any snapshots you take of your disks incur costs based on the size of the snapshot.\nUse the following design and operational recommendations to help you optimize the cost of your persistent disks:\n- Don't over-allocate disk space. You can't reduce disk capacity after provisioning. Start with a small disk, and increase the size when required. Persistent disks are billed forcapacity, not the data that's stored on the disks.\n- Choose a disk type that matches the [performance characteristics](/compute/docs/disks/performance#type_comparison) of your workload. SSD provides high IOPS and throughput, but costs more than standard persistent disks.\n- Use regional persistent disks only when protecting data against zonal outages is essential. Regional persistent disks are replicated to another zone within the region, so you incur double the cost of equivalent zonal disks.\n- Track the usage of your persistent disks by using Cloud Monitoring, and set up alerts for disks with low usage.\n- Delete disks that you no longer need.\n- For disks that contain data that you might need in the future, consider archiving the data to low-cost Cloud Storage and then deleting the disks.\n- Look for and respond to the recommendations in the [Recommendation Hub](/recommender/docs/recommendation-hub/identify-configuration-problems) .\nConsider also using [Hyperdisks](/compute/docs/disks/hyperdisks) for high-performance storage and [Ephemeral disks (local SSDs)](/compute/docs/disks/local-ssd) for temporary storage.\nDisk snapshots are incremental by default and automatically compressed. Consider the following recommendations for optimizing the cost of your disk snapshots:\n- When feasible, organize your data in separate persistent disks. You can then choose to back up disks selectively, and reduce the cost of disk snapshots.\n- When you create a snapshot, select a [location](/compute/docs/disks/snapshots#selecting_a_storage_location) based on your availability requirements and the associated network costs.\n- If you intend to use a boot-disk snapshot to create multiple VMs, create an image from the snapshot, and then use the image to create your VMs. This approach helps you avoid network charges for data traveling between the location of the snapshot and the location where you restore it.\n- Consider setting up a retention policy to minimize long-term storage costs for disk snapshots.\n- Delete disk snapshots that you no longer need. Each snapshot in a chain might depend on data stored in a previous snapshot. So deleting a snapshot doesn't necessarily delete all the data in the snapshot. To definitively delete data from snapshots, you should delete all the snapshots in the chain.## Filestore\nThe cost of a Filestore instance depends on its service tier, the provisioned capacity, and the region where the instance is provisioned. The following are design and operational recommendations to optimize the cost of your Filestore instances:\n- Select a [service tier](/filestore/docs/service-tiers) and storage type (HDD or SSD) that's appropriate for your storage needs.\n- Don't over-allocate capacity. Start with a small size and increase the size later when required. Filestore billing is based oncapacity, not the stored data.\n- Where feasible, organize your data in separate Filestore instances. You can then choose to back up instances selectively, and reduce the cost of Filestore backups.\n- When choosing the region and zone, consider creating instances in the same zone as the clients. You're billed for data transfer traffic from the zone of the Filestore instance.\n- When you decide the region where Filestore backups should be stored, consider the data transfer charges for storing backups in a different region from the source instance.\n- Track the usage of your Filestore instances by using Cloud Monitoring, and set up alerts for instances with low usage.\n- Scale down the allocated capacity for Filestore instances that have low usage. You can reduce the capacity of instances except for the Basic tier.## What's next\n- [Review best practices for Cloud Storage cost optimization](/blog/products/storage-data-transfer/best-practices-for-cloud-storage-cost-optimization) (blog)\n- Optimize cost for compute services, databases, networking, and operations:- [Optimize cost: Compute, containers, and serverless](/architecture/framework/cost-optimization/compute) \n- [Optimize cost: Databases and smart analytics](/architecture/framework/cost-optimization/databases) \n- [Optimize cost: Networking](/architecture/framework/cost-optimization/networking) \n- [Optimize cost: Cloud operations](/architecture/framework/cost-optimization/cloudops) \n- Explore the other categories of the [Google Cloud Architecture Framework](/architecture/framework) # Optimize cost: Databases and smart analytics\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides recommendations to help you optimize the cost of your databases and analytics workloads in Google Cloud.\nThe guidance in this section is intended for architects, developers, and administrators responsible for provisioning and managing databases and analytics workloads in the cloud.\nThis section includes cost-optimization recommendations for the following products:\n- [Cloud SQL](#sql) \n- [Spanner](#spanner) \n- [Bigtable](#bigtable) \n- [BigQuery](#bigquery) \n- [Dataflow](#dataflow) \n- [Dataproc](#dataproc) ## Cloud SQL\n[Cloud SQL](/sql) is a fully managed relational database for MySQL, PostgreSQL, and SQL Server.\n### Monitor usage\nReview the metrics on the [monitoring dashboard](https://console.cloud.google.com/monitoring/dashboards/resourceList/cloudsql_database) , and validate that your deployment meets the requirements of your workload.\n### Optimize resources\nThe following are recommendations to help you optimize your Cloud SQL resources:\n- Design a high availability and disaster recovery strategy that aligns with your recovery time objective (RTO) and recovery point objective (RPO). Depending on your workload, we recommend the following:- For workloads that need a short RTO and RPO, use the [high-availability configuration](/sql/docs/mysql/configure-ha) and [replicas for regional failover](/sql/docs/mysql/replication/cross-region-replicas) .\n- For workloads that can withstand a longer RTO and RPO, use automated and on-demand [backups](/sql/docs/mysql/backup-recovery/backups) , which can take a little longer to restore after a failure.\n- Provision the database with the minimum required storage capacity.\n- To scale storage capacity automatically as your data grows, enable the [automatic storage increase](/sql/docs/postgres/instance-settings#automatic-storage-increase-2ndgen) feature.\n- Choose a storage type, [solid-state drives (SSD) or hard disk drives (HDD)](/sql/docs/mysql/choosing-ssd-hdd) , that's appropriate for your use case. SSD is the most efficient and cost-effective choice for most use cases. HDD might be appropriate for large datasets (>10 TB) that aren't latency-sensitive or are accessed infrequently.\n### Optimize rates\nConsider purchasing [committed use discounts](/sql/cud#pricing) for workloads with predictable resource needs. You can save 25% of on-demand pricing for a 1-year commitment and 52% for a 3-year commitment.\n## Spanner\n[Spanner](/spanner) is a cloud-native, unlimited-scale, strong-consistency database that offers up to 99.999% availability.\n### Monitor usage\nThe following are recommendations to help you track the usage of your Spanner resources:\n- Monitor your deployment, and configure the node count based on [CPU recommendations](/spanner/docs/cpu-utilization#recommended-max) .\n- Set alerts on your deployments to optimize storage resources. To determine the appropriate configuration, refer to the recommended [limits per node](/spanner/quotas#database_limits) .\n### Optimize resources\nThe following are recommendations to help you optimize your Spanner resources:\n- Run smaller workloads on Spanner at much lower cost by provisioning resources with Processing Units (PUs) versus nodes; [one Spanner node is equal to 1,000 PUs](/spanner/docs/compute-capacity#compute_capacity) .\n- Improve query execution performance by using the [query optimizer](/spanner/docs/query-optimizer/manage-query-optimizer) .\n- Construct SQL statements using [best practices](/spanner/docs/sql-best-practices) for building efficient execution plans.\n- Manage the usage and performance of Spanner deployments by using the [Autoscaler](/architecture/autoscaling-cloud-spanner) tool. The tool monitors instances, adds or removes nodes automatically, and helps you ensure that the instances remain within the recommended CPU and storage limits.\n- Protect against accidental deletion or writes by using [point-in-time recovery (PITR)](/spanner/docs/pitr) . Databases with longer version retention periods (particularly databases that overwrite data frequently) use more system resources and need more nodes.\n- Review your backup strategy, and choose between the following options:- Backup and restore\n- Export and import\n### Optimize rates\nWhen deciding the location of your Spanner nodes, consider the cost differences between Google Cloud regions. For example, a node that's deployed in the `us-central1` region costs considerably less per hour than a node in the `southamerica-east1` region.\n## Bigtable\n[Bigtable](/bigtable) is a cloud-native, wide-column NoSQL store for large scale, low-latency workloads.\n### Monitor usage\nThe following are recommendations to help you track the usage of your Bigtable resources:\n- Analyze [usage metrics](/bigtable/docs/monitoring-instance#console-overview) to identify opportunities for resource optimization.\n- Identify hotspots and hotkeys in your Bigtable cluster by using the [Key Visualizer](/bigtable/docs/keyvis-overview) diagnostic tool.\n### Optimize resources\nThe following are recommendations to help you optimize your Bigtable resources:\n- To help you ensure [CPU and disk usage](/bigtable/docs/monitoring-instance#cpu-and-disk) that provides a balance between latency and storage capacity, evaluate and adjust the node count and size of your Bigtable cluster.\n- Maintain performance at the lowest cost possible by [programmatically scaling](/bigtable/docs/scaling) your Bigtable cluster to adjust the node count automatically.\n- Evaluate the most cost-effective [storage type](/bigtable/docs/choosing-ssd-hdd) (HDD or SSD) for your use case, based on the following considerations:- HDD storage costs less than SSD, but has lower performance.\n- SSD storage costs more than HDD, but provides faster and predictable performance.\nThe cost savings from HDD are minimal, relative to the cost of the nodes in your Bigtable cluster, unless you store large amounts of data. HDD storage is sometimes appropriate for large datasets (>10 TB) that are not latency-sensitive or are accessed infrequently.\n- Remove expired and obsolete data using [garbage collection](/bigtable/docs/garbage-collection) .\n- To avoid hotspots, apply best practices for [row key design](/bigtable/docs/schema-design#row-keys-avoid) .\n- Design a cost-effective [backup plan](/bigtable/docs/backups) that aligns with your RPO.\n- To lower the cluster usage and reduce the node count, consider adding a [capacity cache](/community/tutorials/bigtable-memcached) for cacheable queries by using [Memorystore](/memorystore/docs) .\n### Additional reading\n- [Blog: A primer on Bigtable cost optimization](/blog/products/databases/how-to-save-money-when-using-cloud-databases) \n- [Blog: Best practices for Bigtable performance and cost optimization](/blog/products/databases/check-out-how-to-optimize-database-service-cloud-bigtable) ## BigQuery\n[BigQuery](/bigquery) is a serverless, highly scalable, and cost-effective multicloud data warehouse designed for business agility.\n### Monitor usage\nThe following are recommendations to help you track the usage of your BigQuery resources:\n- [Visualize](/bigquery/docs/visualize-looker-studio) your BigQuery costs segmented by projects and users. Identify the most expensive queries and optimize them.\n- Analyze [slot utilization](/blog/topics/developers-practitioners/monitoring-bigquery-reservations-and-slot-utilization-information_schema) across projects, jobs, and reservations by using`INFORMATION_SCHEMA`metadata tables.\n### Optimize resources\nThe following are recommendations to help you optimize your BigQuery resources:\n- Set up dataset-level, table-level, or partition-level [expirations](/bigquery/docs/updating-datasets#table-expiration) for data, based on your compliance strategy.\n- [Limit query costs](/bigquery/docs/best-practices-costs#limit_query_costs_by_restricting_the_number_of_bytes_billed) by restricting the number of bytes billed per query. To prevent accidental human errors, enable cost control at the user level and project level.\n- [Query only the data](/bigquery/docs/best-practices-costs#avoid_select_) that you need. Avoid full query scans. To explore and understand data semantics, use the no-charge [data preview](/bigquery/docs/best-practices-costs#preview-data) options.\n- To reduce the processing cost and improve performance, [partition](/bigquery/docs/partitioned-tables) and [cluster](/bigquery/docs/clustered-tables) your tables when possible.\n- Filter your [query](/bigquery/query-plan-explanation#background) as early and as often as you can.\n- When processing data from multiple sources (like Bigtable, Cloud Storage, Google Drive, and Cloud SQL), avoid duplicating data, by using a [federated access data model](/bigquery/external-data-sources) and querying data directly from the sources.\n- Take advantage of [BigQuery's backup](/bigquery/docs/availability) instead of duplicating data. See [Disaster recovery scenarios for data](/architecture/dr-scenarios-for-data#managed-database-services-on-gcp) .\n### Optimize rates\nThe following are recommendations to help you reduce the billing rates for your BigQuery resources:\n- Evaluate how you edit data, and take advantage of lower [long-term](/bigquery/docs/best-practices-storage#take_advantage_of_long-term_storage) storage prices.\n- Review the differences between flat-rate and on-demand [pricing](/bigquery/pricing) , and [choose an option](/bigquery/docs/reservations-workload-management) that suits your requirements.\n- Assess whether you can use batch-loading instead of [streaming inserts](/bigquery/streaming-data-into-bigquery) for your data workflows. Use streaming inserts if the data loaded to BigQuery is consumed immediately.\n- To increase performance and reduce the cost of retrieving data, use [cached query results](/bigquery/docs/cached-results) .\n### Additional reading\n- [Controlling BigQuery costs](/bigquery/docs/controlling-costs) \n- [Cost optimization best practices for BigQuery](/blog/products/data-analytics/cost-optimization-best-practices-for-bigquery) \n- [Understanding the principles of cost optimization (PDF)](https://services.google.com/fh/files/misc/understanding_the_principles_of_cost_optimization_2020_whitepaper_google_cloud.pdf) ## Dataflow\n[Dataflow](/dataflow) is a fast and cost-effective serverless service for unified stream and batch data processing.\n### Monitor usage\nThe following are recommendations to help you track the usage of your Dataflow resources:\n- [Predict the cost of Dataflow jobs](/blog/products/data-analytics/predicting-cost-dataflow-job) by running small load experiments, finding your job's optimal performance, and extrapolating the throughput factor.\n- Gain increased visibility into throughput and CPU usage by using [observability dashboards](/blog/products/data-analytics/better-data-pipeline-observability-for-batch-and-stream-processing) .\n- Observe performance, execution, and health-related pipeline metrics using the [Dataflow monitoring interface](/dataflow/docs/guides/using-monitoring-intf) .\n### Optimize resources\nThe following are recommendations to help you optimize your Dataflow resources:\n- Consider [Dataflow Prime](/blog/products/data-analytics/simplify-and-automate-data-processing-with-dataflow-prime) for processing big data efficiently.\n- Reduce batch-processing costs by using [Flexible Resource Scheduling (FlexRS)](/dataflow/docs/guides/flexrs) for autoscaled batched pipelines. FlexRS uses advanced scheduling, Dataflow shuffle, and a combination of preemptible and regular VMs to reduce the cost for batch pipelines.\n- Improve performance by using the in-memory [shuffle service](/dataflow/pricing#dataflow_services) instead of Persistent Disk and worker nodes.\n- For more responsive autoscaling, and to reduce resource consumption, use [Streaming Engine](/dataflow/docs/guides/deploying-a-pipeline#streaming-engine) , which moves pipeline execution out of the worker VMs and into the Dataflow service backend.\n- If the pipeline doesn't need access to and from the internet and other Google Cloud networks, disable public IP addresses. Disabling internet access helps you reduce network costs and improve pipeline security.\n- Follow the best practices for [efficient pipelining with Dataflow](/blog/products/data-analytics/how-to-efficiently-process-both-real-time-and-aggregate-data-with-dataflow) .## Dataproc\n[Dataproc](/dataproc) is a managed Apache Spark and Apache Hadoop service for batch processing, querying, streaming, and machine learning.\nThe following are recommendations to help you optimize the cost of your Dataproc resources:\n- [Choose machine types](/blog/products/data-analytics/optimize-dataproc-costs-using-vm-machine-type) that are appropriate for your workload.\n- Scale automatically to match demand by using [autoscaling clusters](/dataproc/docs/concepts/configuring-clusters/autoscaling) , and pay for only the resources that you need.\n- If a cluster can be deleted when the job is completed, consider provisioning an ephemeral cluster using a [managed-cluster workflow template](/dataproc/docs/concepts/workflows/overview) .\n- To avoid charges for an inactive cluster, use [scheduled deletion](/dataproc/docs/concepts/configuring-clusters/scheduled-deletion) , which lets you delete a cluster after a specified idle period, at a specified time, or after a specified period.\n- Follow the best practices for [building long-running clusters on Dataproc](/blog/products/data-analytics/10-tips-for-building-long-running-clusters-using-cloud-dataproc) .\n- Purchase [committed use discounts](/compute/docs/instances/signing-up-committed-use-discounts) for always-on workloads.## What's next\n- Optimize cost for compute services, storage, networking, and operations:- [Optimize cost: Compute, containers, and serverless](/architecture/framework/cost-optimization/compute) \n- [Optimize cost: Storage](/architecture/framework/cost-optimization/storage) \n- [Optimize cost: Networking](/architecture/framework/cost-optimization/networking) \n- [Optimize cost: Cloud operations](/architecture/framework/cost-optimization/cloudops) \n- Explore the other categories of the [Google Cloud Architecture Framework](/architecture/framework) # Optimize cost: Networking\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides recommendations to help you optimize the cost of your networking resources in Google Cloud.\nThe guidance in this section is intended for architects and administrators responsible for provisioning and managing networking for workloads in the cloud.\n## Design considerations\nA fundamental difference between on-premises and cloud networks is the dynamic, usage-based cost model in the cloud, compared with the fixed cost of networks in traditional data centers.\nWhen planning cloud networks, it's critical to understand the [pricing](/vpc/network-pricing#general) model, which is based on the traffic direction, as follows:\n- You incur no charges for traffic inbound to Google Cloud. Resources that process inbound traffic, like Cloud Load Balancing, incur costs.\n- For data transfer traffic, which includes both traffic between virtual machines (VMs) in Google Cloud and traffic from Google Cloud to the internet and to on-premises hosts, pricing is based on the following factors:- Whether the traffic uses an internal or external IP address\n- Whether the traffic crosses zone or region boundaries\n- Whether the traffic leaves Google Cloud\n- The distance that the traffic travels before leaving Google CloudWhen two VMs or cloud resources within Google Cloud communicate, traffic in each direction is designated as outbound data transfer at the source and inbound data transfer at the destination, and is priced accordingly.\nConsider the following factors for designing cost-optimal cloud networks:\n- Geo-location\n- Network layout\n- Connectivity options\n- Network Service Tiers\n- Logging\nThese factors are discussed in more detail in the following sections.\n### Geo-location\nNetworking costs can vary depending on the Google Cloud region where your resources are provisioned. To analyze network bandwidth between regions, you can use [VPC Flow Logs](/vpc/docs/flow-logs) and the [Network Intelligence Center](https://console.cloud.google.com/net-intelligence) . For traffic flowing between Google Cloud regions, cost can vary depending on the location of the regions even if the traffic doesn't go through the internet.\nBesides the Google Cloud region, consider the zones where your resources are deployed. Depending on availability requirements, you might be able to design your applications to communicate at no cost within a zone through internal IP addresses. When considering a single-zone architecture, weigh any potential savings in networking cost with the impact on availability.\n### Network layout\nAnalyze your network layout, how traffic flows between your applications and users, and the bandwidth consumed by each application or user. The [Network Topology](/network-intelligence-center/docs/network-topology/concepts/overview) tool provides comprehensive visibility into your global Google Cloud deployment and its interaction with the public internet, including an organization-wide view of the topology, and associated network performance metrics. You can identify inefficient deployments, and take necessary actions to optimize your regional and intercontinental data transfer costs.\n### Connectivity options\nWhen you need to push a large volume of data (TBs or PBs) frequently from on-premises environments to Google Cloud, consider using [Dedicated Interconnect](/interconnect/docs/concepts/dedicated-overview) or [Partner Interconnect](/interconnect/partners) . A dedicated connection can be cheaper when compared with costs associated with traversing the public internet or using a VPN.\nUse [Private Google Access](/vpc/docs/private-access-options#pga) when possible to reduce cost and improve your security posture.\n### Network Service Tiers\nGoogle's premium network infrastructure ( [Premium Tier](/network-tiers/docs/overview#premium_tier) ), is used by default for all services. For resources that don't need the high performance and low latency that Premium Tier offers, you can choose [Standard Tier](/network-tiers/docs/overview#standard_tier) , which costs less.\nWhen choosing a service tier, consider the [differences between the tiers](/network-tiers/docs/overview#premium_tier_and_standard_tier_summary) and the limitations of Standard Tier. Fine-tune the network to the needs of your application, and potentially reduce the networking cost for services that can tolerate more latency and don't require an SLA.\n### Logging\n[VPC Flow Logs](/vpc/docs/using-flow-logs#log-sampling) , [Firewall Rule Logging](/vpc/docs/firewall-rules-logging) , and [Cloud NAT logging](/nat/docs/monitoring) let you analyze network logs and identify opportunities to reduce cost.\nFor VPC Flow Logs and Cloud Load Balancing, you can also enable [sampling](/vpc/docs/flow-logs#log-sampling) , which can reduce the volume of logs written to the [database](/logging/docs/export#overview) . You can vary the sampling rate from 1.0 (all log entries are retained) to 0.0 (no logs are kept). For troubleshooting or custom use cases, you can choose to always collect telemetry for a particular VPC network or subnet, or monitor a specific VM Instance or virtual interface.\n## Design recommendations\nTo optimize network traffic, we recommend the following:\n- Design your solutions to bring applications closer to your user base. Use [Cloud CDN](/cdn) to reduce traffic volume and latency, and take advantage of CDN's lower pricing to serve content that you expect many users to access frequently.\n- Avoid synchronizing data globally across regions that are distant from the end user or that can incur high networking costs. If an application is used only within a region, avoid cross-region data processing.\n- Ensure that communication between VMs within a zone is routed through their internal IP addresses, and not routed externally.\n- Reduce data transfer cost and client latency by compressing data output.\n- Analyze spending patterns and identify opportunities to control cost by observing outbound and inbound traffic flows for critical projects using [VPC Flow Logs](/vpc/docs/flow-logs) .\n- When designing your networks in the cloud, consider the trade-off between the high availability that a distributed network offers and the cost savings from centralizing traffic within a single zone or region.\nTo optimize the price that you pay for networking services, we recommend the following:\n- If the server location is not a constraint, assess the cost at different regions, and select the most cost-efficient region. For general outbound traffic, like content served by a group of web servers, prices can vary depending on the region where the servers are provisioned.\n- To reduce the cost of moving high volumes of data frequently to the cloud, use a direct connection between the on-premises and Google Cloud networks. Consider using Dedicated Interconnect or Partner Interconnect.\n- Choose an appropriate [service tier](/network-tiers/docs/overview) for each environment: that is, Standard Tier for development and test environments, and Premium Tier for production.## What's next\n- [Review network pricing information](/vpc/network-pricing#general) \n- [Review best practices for networking cost optimization](/blog/products/networking/networking-cost-optimization-best-practices) \n- Optimize cost for compute services, storage, databases, and operations:- [Optimize cost: Compute, containers, and serverless](/architecture/framework/cost-optimization/compute) \n- [Optimize cost: Storage](/architecture/framework/cost-optimization/storage) \n- [Optimize cost: Databases and smart analytics](/architecture/framework/cost-optimization/databases) \n- [Optimize cost: Cloud operations](/architecture/framework/cost-optimization/cloudops) \n- Explore the other categories of the [Google Cloud Architecture Framework](/architecture/framework) # Optimize cost: Cloud operations\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides recommendations to help you optimize the cost of monitoring and managing your resources in Google Cloud.\nThe guidance in this section is intended for cloud users who are responsible for monitoring and controlling the usage and cost of their organization's resources in the cloud.\nGoogle Cloud Observability is a collection of managed services that you can use to monitor, troubleshoot, and improve the performance of your workloads in Google Cloud. These services include Cloud Monitoring, Cloud Logging, Error Reporting, Cloud Trace, and Cloud Profiler. One of the benefits of managed services in Google Cloud is that the services are usage-based. You pay only for what you use and by the volume of data, with free monthly data-usage allotments, and unlimited access to Google Cloud metrics and audit logs.\n## Cloud Logging\nThe following are recommendations to help you optimize the cost of your Logging operations:\n- Filter billing reports to show Logging costs.\n- Reduce the volume of logs ingested and stored, by [excluding or filtering unnecessary log entries](/logging/docs/exclusions) .\n- Verify whether the exclusion filters are adequate by [monitoring](/monitoring/api/metrics_gcp#gcp-logging) the`billing/bytes_ingested`and`billing/monthly_bytes_ingested`metrics in the Google Cloud console.\n- Offload and [export logs to lower-cost storage](/logging/docs/routing/overview) .\n- When streaming logs from third-party applications, reduce log volumes by using the [logging agent](/logging/docs/agent) on only production instances or by configuring it to send less data.## Cloud Monitoring\nThe following are recommendations to help you optimize the cost of your Monitoring operations:\n- Optimize metrics and label usage by limiting the number of labels. Avoid labels with high cardinality. For example, if you use an IP address as a label, each IP address would have a one-item label series, resulting in numerous labels when you have many VMs.\n- Reduce the volume of detailed metrics for applications that don't require these metrics, or remove the monitoring agent, especially for nonessential environments.\n- Minimize the ingestion volume by reducing the number of custom metrics that your application sends.## Cloud Trace\nThe following are recommendations to help you optimize the cost of your Trace operations:\n- If you use Trace as an export destination for your OpenCensus traces, reduce the volume of traces that are ingested, by using the sampling feature in [OpenCensus](https://opencensus.io/tracing/sampling/) .\n- Limit the usage of Trace, and control cost by using quotas. You can enforce span quotas using the API-specific quota page in the Google Cloud console.## What's next\n- [Optimize costs for Google Cloud Observability](/architecture/stackdriver-cost-optimization) \n- Video: [Manage costs for Google Cloud Observability](https://www.youtube.com/watch?v=vgluz-Tv2qY) \n- Optimize cost for compute services, storage, databases, and networking:- [Optimize cost: Compute, containers, and serverless](/architecture/framework/cost-optimization/compute) \n- [Optimize cost: Storage](/architecture/framework/cost-optimization/storage) \n- [Optimize cost: Databases and smart analytics](/architecture/framework/cost-optimization/databases) \n- [Optimize cost: Networking](/architecture/framework/cost-optimization/networking) \n- Explore the other categories of the [Google Cloud Architecture Framework](/architecture/framework) # Google Cloud Architecture Framework: Performance optimization\nThis category in the [Google Cloud Architecture Framework](/architecture/framework) describes the performance optimization process and best practices to optimize the performance of workloads in Google Cloud.\nThe information in this document is intended for architects, developers, and administrators who plan, design, deploy, and manage workloads in Google Cloud.\nOptimizing the performance of workloads in the cloud can help your organization operate efficiently, improve customer satisfaction, increase revenue, and reduce cost. For example, when the backend processing time of an application decreases, users experience faster response times, which can lead to higher user retention and more revenue.\nThere might be trade-offs between performance and cost. But sometimes, optimizing performance can help you reduce cost. \u200b\u200bFor example, autoscaling helps provide predictable performance when the load increases by ensuring that the resources aren't overloaded. Autoscaling also helps you reduce cost during periods of low load by removing unused resources.\nIn this category of the Architecture Framework, you learn to do the following:\n- [Implement the performance optimization process](/architecture/framework/performance-optimization/process) .\n- [Monitor and analyze performance](/architecture/framework/performance-optimization/monitor) .\n- [Optimize compute performance](/architecture/framework/performance-optimization/compute) .\n- [Optimize storage performance](/architecture/framework/performance-optimization/storage) .\n- [Optimize networking performance](/architecture/framework/performance-optimization/networking) .\n- [Optimize database performance](/architecture/framework/performance-optimization/databases) .\n- [Optimize analytics performance](/architecture/framework/performance-optimization/analytics) .# Performance optimization process\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides an overview of the performance optimization process.\nPerformance optimization is a continuous process, not a one-time activity. The following diagram shows the stages in the performance optimization process:\nThe following is an overview of the stages in the performance optimization process:\n## Define performance requirements\nBefore you start to design and develop the applications that you intend to deploy or migrate to the cloud, determine the performance requirements. Define the requirements as granularly as possible for each layer of the application stack: frontend load balancing, web or applications servers, database, and storage. For example, for the storage layer of the stack, decide on the [throughput](https://wikipedia.org/wiki/Throughput) and I/O operations per second ( [IOPS](https://wikipedia.org/wiki/IOPS) ) that your applications need.\n## Design and deploy your applications\nDesign your applications by using elastic and scalable design patterns that can help you meet the performance requirements. Consider the following guidelines for designing applications that are elastic and scalable:\n- Architect the workloads for optimal content placement.\n- Isolate read and write traffic.\n- Isolate static and dynamic traffic.\n- Implement content caching. Use data caches for internal layers.\n- Use managed services and serverless architectures.\nGoogle Cloud provides [open source tools](/free/docs/measure-compare-performance) that you can use to benchmark the performance of Google Cloud services with other cloud platforms.\n## Monitor and analyze performance\nAfter you deploy your applications, continuously monitor performance by using logs and alerts, analyze the data, and identify performance issues. As your applications grow and evolve, reassess your performance requirements. You might have to redesign some parts of the applications to maintain or improve performance.\n## Optimize performance\nBased on the performance of your applications and changes in requirements, configure the cloud resources to meet the current performance requirements. For example, resize the resources or set up autoscaling. When you configure the resources, evaluate opportunities to use recently released Google Cloud features and services that can help further optimize performance.\nThe performance optimization process doesn't end at this point. Continue the cycle of monitoring performance, reassessing requirements when necessary, and adjusting the cloud resources to maintain and improve performance.\n## What's next\n- [Monitor and analyze performance](/architecture/framework/performance-optimization/monitor) .\n- [Optimize compute performance](/architecture/framework/performance-optimization/compute) .\n- [Optimize storage performance](/architecture/framework/performance-optimization/storage) .\n- [Optimize networking performance](/architecture/framework/performance-optimization/networking) .\n- [Optimize database performance](/architecture/framework/performance-optimization/databases) .\n- [Optimize analytics performance](/architecture/framework/performance-optimization/analytics) .\n# Monitor and analyze performance\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) describes the services in the [Google Cloud Observability](/stackdriver) that you can use to record, monitor, and analyze the performance of your workloads.\n## Monitor performance metrics\nUse [Cloud Monitoring](/monitoring) to analyze trends of performance metrics, analyze the effects of experiments, define alerts for critical metrics, and perform retrospective analyses.\n## Log critical data and events\n[Cloud Logging](/logging/docs) is an integrated logging service that you can use to store, analyze, monitor, and set alerts for log data and events. Cloud Logging can collect logs from the services of Google Cloud and other cloud providers.\n## Analyze code performance\nCode that performs poorly can increase the latency of your applications and the cost of running them. [Cloud Profiler](/profiler/docs) helps you identify and address performance issues by continuously analyzing the performance of CPU-intensive or memory-intensive functions that an application uses.\n## Collect latency data\nIn complex application stacks and microservices-based architectures, assessing [latency](https://wikipedia.org/wiki/Latency_(engineering)) in inter-service communication and identifying performance bottlenecks can be difficult. [Cloud Trace](/trace/docs) and [OpenTelemetry](https://opentelemetry.io/) tools help you collect latency data from your deployments at scale. These tools also help you analyze the latency data efficiently.\n## Monitor network performance\nThe [Performance Dashboard](/network-intelligence-center/docs/performance-dashboard/concepts/overview) of the Network Intelligence Center gives you a comprehensive view of performance metrics for the Google network and the resources in your project. These metrics can help you determine the cause of network-related performance issues. For example, you can identify whether a performance issue is the result of a [problem in your project or the Google network](/network-intelligence-center/docs/performance-dashboard/concepts/use-cases-project#current-diagnostics) .\n## What's next\n- Learn about the [best practices for monitoring your cloud resources](/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging) .\n- Review the best practices for optimizing the performance of your Google Cloud resources:- [Optimize compute performance](/architecture/framework/performance-optimization/compute) .\n- [Optimize storage performance](/architecture/framework/performance-optimization/storage) .\n- [Optimize networking performance](/architecture/framework/performance-optimization/networking) .\n- [Optimize database performance](/architecture/framework/performance-optimization/databases) .\n- [Optimize analytics performance](/architecture/framework/performance-optimization/analytics) .\n# Optimize compute performance\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides recommendations to help you optimize the performance of your Compute Engine, Google Kubernetes Engine (GKE), and serverless resources.\n## Compute Engine\nThis section provides guidance to help you optimize the performance of your [Compute Engine](/compute) resources.\n### Autoscale resources\n[Managed instance groups (MIGs)](/compute/docs/instance-groups) let you scale your stateless apps deployed on Compute Engine VMs efficiently. Autoscaling helps your apps continue to deliver predictable performance when the load increases. In a MIG, a group of Compute Engine VMs is launched based on a template that you define. In the template, you configure an autoscaling policy, which specifies one or more signals that the autoscaler uses to scale the group. The autoscaling signals can be schedule-based, like start time or duration, or based on target metrics such as average CPU utilization. For more information, see [Autoscaling groups of instances](/compute/docs/autoscaler) .\n### Disable SMT\nEach virtual CPU (vCPU) that you allocate to a Compute Engine VM is implemented as a single hardware multithread. By default, two vCPUs share a physical CPU core. This architecture is called [simultaneous multi-threading (SMT)](https://wikipedia.org/wiki/Simultaneous_multithreading) .\nFor workloads that are highly parallel or that perform floating point calculations (such as transcoding, Monte Carlo simulations, genetic sequence analysis, and financial risk modeling), you can improve performance by disabling SMT. For more information, see [Set the number of threads per core](/compute/docs/instances/set-threads-per-core) .\n### Use GPUs\nFor workloads such as machine learning and visualization, you can add [graphics processing units (GPUs)](/compute/docs/gpus) to your VMs. Compute Engine provides NVIDIA GPUs in passthrough mode so that your VMs have direct control over the GPUs and the associated memory. For graphics-intensive workloads such as 3D visualization, you can use NVIDIA RTX virtual workstations. After you deploy the workloads, [monitor the GPU usage](/compute/docs/gpus/monitor-gpus) and review the options for [optimizing GPU performance](/compute/docs/gpus/optimize-gpus) .\n### Use compute-optimized machine types\nWorkloads like gaming, media transcoding, and high performance computing (HPC) require consistently high performance per CPU core. Google recommends that you use [compute-optimized machine types](/compute/docs/compute-optimized-machines) for the VMs that run such workloads. Compute-optimized VMs are built on an architecture that uses features like [non-uniform memory access (NUMA)](https://www.kernel.org/doc/html/latest/mm/numa.html) for optimal and reliable performance.\nTightly coupled HPC workloads have a unique set of requirements for achieving peak efficiency in performance. For more information, see the following documentation:\n- [Parallel file systems for HPC workloads](/architecture/parallel-file-systems-for-hpc) \n- [Architecture: Lustre file system in Google Cloud using DDN EXAScaler](/architecture/lustre-architecture) \n### Choose appropriate storage\nGoogle Cloud offers a wide range of [storage options](/compute/docs/disks) for Compute Engine VMs: Persistent disks, local solid-state drive (SSD) disks, Filestore, and Cloud Storage. For design recommendations and best practices to optimize the performance of each of these storage options, see [Optimize storage performance](/architecture/framework/performance-optimization/storage) .\n## Google Kubernetes Engine\nThis section provides guidance to help you optimize the performance of your [Google Kubernetes Engine (GKE)](/kubernetes-engine) resources.\n### Autoscale resources\nYou can automatically resize the node pools in a GKE cluster to match the current load by using the [cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) feature. Autoscaling helps your apps continue to deliver predictable performance when the load increases. The cluster autoscaler resizes node pools automatically based on the resource requests (rather than actual resource utilization) of the Pods running on the nodes. When you use autoscaling, there can be a trade-off between performance and cost. Review the best practices for configuring [cluster autoscaling](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke#cluster_autoscaler) efficiently.\n**Note:** With [Autopilot clusters](/kubernetes-engine/docs/concepts/autopilot-overview) , you don't need to worry about provisioning nodes or managing node pools. The node pools are automatically provisioned and scaled based on the requirements of your workloads.\n### Use C2D VMs\nYou can improve the performance of compute-intensive containerized workloads by using [C2D machine types](/compute/docs/compute-optimized-machines#c2d_machine_types) . You can add C2D nodes to your GKE clusters by choosing a C2D machine type in your node pools.\n### Disable SMT\n[Simultaneous multi-threading (SMT)](https://wikipedia.org/wiki/Simultaneous_multithreading) can increase application throughput significantly for general computing tasks and for workloads that need high I/O. But for workloads in which both the virtual cores are compute-bound, SMT can cause inconsistent performance. To get better and more predictable performance, you can [disable SMT](/kubernetes-engine/docs/how-to/configure-smt#configure-smt) for your GKE nodes by setting the number of vCPUs per core to 1.\n### Use GPUs\nFor compute-intensive workloads like image recognition and video transcoding, you can accelerate performance by creating node pools that use GPUs. For more information, see [Running GPUs](/kubernetes-engine/docs/how-to/gpus) .\n### Use container-native load balancing\n[Container-native load balancing](/kubernetes-engine/docs/concepts/container-native-load-balancing) enables load balancers to distribute traffic directly and evenly to Pods. This approach provides better network performance and improved visibility into network latency between the load balancer and the Pods. Because of these benefits, container-native load balancing is the recommended solution for load balancing through [Ingress](/kubernetes-engine/docs/concepts/ingress) .\n### Define a compact placement policy\nTightly coupled batch workloads need low network latency between the nodes in the GKE node pool. \u200b\u200bYou can deploy such workloads to [single-zone node pools](/kubernetes-engine/docs/how-to/creating-a-regional-cluster#create-regional-single-zone-nodepool) , and ensure that the nodes are physically close to each other by defining a compact placement policy. For more information, see [Define compact placement for GKE nodes](/kubernetes-engine/docs/how-to/compact-placement) .\n## Serverless compute services\nThis section provides guidance to help you optimize the performance of your serverless compute services in Google Cloud: [Cloud Run](/run) and [Cloud Functions](/functions) . These services provide autoscaling capabilities, where the underlying infrastructure handles scaling automatically. By using these serverless services, you can reduce the effort to scale your microservices and functions, and focus on optimizing performance at the application level.\nFor more information, see the following documentation:\n- [Optimizing performance for Cloud Run services ](/run/docs/tips/general#optimizing_performance) \n- [Optimizing Java applications for Cloud Run](/run/docs/tips/java) \n- [Optimizing performance in Cloud Functions](/functions/docs/bestpractices/tips#performance) ## What's next\nReview the best practices for optimizing the performance of your storage, networking, database, and analytics resources:\n- [Optimize storage performance](/architecture/framework/performance-optimization/storage) .\n- [Optimize networking performance](/architecture/framework/performance-optimization/networking) .\n- [Optimize database performance](/architecture/framework/performance-optimization/databases) .\n- [Optimize analytics performance](/architecture/framework/performance-optimization/analytics) .# Optimize storage performance\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides recommendations to help you optimize the performance of your storage resources in Google Cloud.\n## Cloud Storage\nThis section provides best practices to help you optimize the performance of your [Cloud Storage](/storage) operations.\n### Assess bucket performance\nAssess the performance of your Cloud Storage buckets by using the [gsutil perfdiag](/storage/docs/gsutil/commands/perfdiag) command. This command tests the performance of the specified bucket by sending a series of read and write requests with files of different sizes. You can tune the test to match the usage pattern of your applications. Use the diagnostic report that the command generates to set performance expectations and identify potential bottlenecks.\n### Cache frequently accessed objects\nTo improve the read latency for frequently accessed objects that are publicly accessible, you can configure such objects to be cached. Although caching can improve performance, stale content could be served if a cache has the old version of an object.\n### Scale requests efficiently\nAs the request rate for a bucket increases, Cloud Storage automatically increases the I/O capacity for the bucket by distributing the request load across multiple servers. To achieve optimal performance when scaling requests, follow the [best practices](/storage/docs/request-rate#best-practices) for ramping up request rates and distributing load evenly.\n### Enable multithreading and multiprocessing\nWhen you use `gsutil` to upload numerous small files, you can improve the performance of the operation by using the `-m` option. This option causes the upload request to be implemented as a batched, parallel (that is, multithreaded and multiprocessing) operation. Use this option only when you perform operations over a fast network connection. For more information, see the documentation for the `-m` option in [Global Command-Line Options](/storage/docs/gsutil/addlhelp/GlobalCommandLineOptions#options) .\n### Upload large files as composites\nTo upload large files, you can use a strategy called . With this strategy, the large file is split into chunks, which are uploaded in parallel and then recomposed in the cloud. Parallel composite uploads can be faster than regular upload operations when network bandwidth and disk speed are not limiting factors. However, this strategy has some limitations and cost implications. For more information, see [Parallel composite uploads](/storage/docs/uploads-downloads#parallel-composite-uploads) .\n## Persistent disks and local SSDs\nThis section provides best practices to help you optimize the performance of your [Persistent Disks](/persistent-disk) and [Local SSDs](/local-ssd) that are attached to Compute Engine VMs.\nThe performance of persistent disks and local SSDs depends on the disk type and size, VM machine type, and number of vCPUs. Use the following guidelines to manage the performance of your persistent disks and local SSDs:\n- When you provision block storage for your VMs, choose disk types and disk sizes that are appropriate for your workload. For more information, see [Configure disks to meet performance requirements](/compute/docs/disks/performance) .\n- Benchmark the block storage performance. For more information, see the following documentation:- [Benchmarking persistent disk performance](/compute/docs/disks/benchmarking-pd-performance) \n- [Benchmarking local SSD performance](/compute/docs/disks/benchmarking-local-ssd-performance) \n- Optimize the performance of your persistent disks and local SSDs. For more information, see the following documentation:- [Optimizing persistent disk performance](/compute/docs/disks/optimizing-pd-performance) \n- [Optimizing local SSD performance](/compute/docs/disks/optimizing-local-ssd-performance) \n## Filestore\nThis section provides best practices to help you optimize the performance of your [Filestore](/filestore) instances. You can use Filestore to provision fully managed Network File System (NFS) file servers for Compute Engine VMs and GKE clusters.\n- When you provision a Filestore instance, choose a [service tier](/filestore/docs/performance#expected_performance) that meets the performance and capacity requirements of your workload.\n- For client VMs that run cache-dependent workloads, use a machine type that helps optimize the network performance of the Filestore instance. For more information, see [Recommended client machine type](/filestore/docs/performance#client-machine) .\n- To optimize the performance of Filestore instances for client VMs that run Linux, Google recommends specific NFS mount settings. For more information, see [Linux client mount options](/filestore/docs/performance#linux_client_mount_options) .\n- To minimize network latency, provision your Filestore instances in [regions and zones](/filestore/docs/regions) that are close to where you plan to use the instances.\n- Monitor the performance of your Filestore instances, and set up alerts by using [Cloud Monitoring](/filestore/docs/monitoring-instances) .## What's next\nReview the best practices for optimizing the performance of your compute, networking, database, and analytics resources:\n- [Optimize compute performance](/architecture/framework/performance-optimization/compute) .\n- [Optimize networking performance](/architecture/framework/performance-optimization/networking) .\n- [Optimize database performance](/architecture/framework/performance-optimization/databases) .\n- [Optimize analytics performance](/architecture/framework/performance-optimization/analytics) .# Optimize networking and API performance\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides recommendations to help you optimize the performance of your networking resources and APIs in Google Cloud.\n## Network Service Tiers\nNetwork Service Tiers lets you optimize the network cost and performance of your workloads. You can choose from the following tiers:\n- **Premium Tier** uses Google's highly reliable global backbone to help you achieve minimal packet loss and latency. Traffic enters and leaves the Google network at a global edge point of presence (PoP) that's close to your end user. We recommend using Premium Tier as the default tier for optimal performance. Premium Tier supports both regional and global external IP addresses for VMs and load balancers.\n- **Standard Tier** is available only for resources that use regional external IP addresses. Traffic enters and leaves the Google network at an edge PoP that's closest to the Google Cloud location where your workload runs. The pricing for Standard Tier is lower than Premium Tier. Standard Tier is suitable for traffic that isn't sensitive to packet loss and that doesn't have low latency requirements.\nYou can view the network latency for Standard Tier and Premium Tier for each cloud region in the [Network Intelligence Center Performance Dashboard](/network-intelligence-center/docs/performance-dashboard/concepts/overview) .\n## Jumbo frames\nVirtual Private Cloud (VPC) networks have a default [maximum transmission unit(MTU)](https://www.wikipedia.org/wiki/Maximum_transmission_unit) of 1460 bytes. However, you can configure your VPC networks to to support an MTU of up to `8896` (jumbo frames).\nWith a higher MTU, the network needs fewer packets to send the same amount of data, thus reducing the bandwidth used up by TCP/IP headers. This leads to a higher effective bandwidth for the network.\nFor more information about intra-VPC MTU and the maximum MTU of other connections, see the [Maximum transmission unit](/vpc/docs/mtu) page in the VPC documentation.\n## VM performance\nCompute Engine VMs have a maximum egress bandwidth that in part depends upon the machine type. One aspect of choosing an appropriate machine type is to consider how much traffic you expect the VM to generate.\nThe [Network bandwidth](/compute/docs/network-bandwidth) page contains a discussion and table of network bandwidths for Compute Engine machine types.\nIf your inter-VM bandwidth requirements are very high, consider VMs that support [Tier_1 networking](/compute/docs/networking/configure-vm-with-high-bandwidth-configuration) .\n## Cloud Load Balancing\nThis section provides best practices to help you optimize the performance of your [Cloud Load Balancing](/load-balancing/docs/load-balancing-overview) instances.\n### Deploy applications close to your users\nProvision your application backends close to the location where you expect user traffic to arrive at the load balancer. The closer your users or client applications are to your workload servers, the lower the network latency between the users and the workload. To minimize latency to clients in different parts of the world, you might have to deploy the backends in multiple regions. For more information, see [Best practices for Compute Engine regions selection](/solutions/best-practices-compute-engine-region-selection) .\n### Choose an appropriate load balancer type\nThe [type of load balancer](/load-balancing/docs/choosing-load-balancer#lb-summary) that you choose for your application can determine the latency that your users experience. For information about measuring and optimizing application latency for different load balancer types, see [Optimizing application latency with load balancing](/load-balancing/docs/tutorials/optimize-app-latency) .\n### Enable caching\nTo accelerate content serving, enable caching and [Cloud CDN](/cdn/docs/overview) as part of your default external HTTP load balancer configuration. Make sure that the backend servers are [configured](/cdn/docs/troubleshooting-steps#responses-not-cached) to send the response headers that are necessary for static responses to be cached.\n### Use HTTP when HTTPS isn't necessary\nGoogle automatically [encrypts traffic between proxy load balancers and backends](/load-balancing/docs/ssl-certificates/encryption-to-the-backends#encryption-to-backends) at the packet level. Packet-level encryption makes Layer 7 encryption using HTTPS between the load balancer and the backends redundant for most purposes. Consider using HTTP rather than HTTPS or HTTP/2 for traffic between the load balancer and your backends. By using HTTP, you can also reduce the CPU usage of your backend VMs. However, when the backend is an internet network endpoint group (NEG), use HTTPS or HTTP/2 for traffic between the load balancer and the backend. This helps ensure that your traffic is secure on the public internet. For optimal performance, we recommend benchmarking your application's traffic patterns.\n## Network Intelligence Center\nGoogle Cloud [Network Intelligence Center](/network-intelligence-center) provides a comprehensive view of the performance of the Google Cloud network across all regions. Network Intelligence Center helps you determine whether latency issues are caused by [problems in your project or in the network](/network-intelligence-center/docs/performance-dashboard/concepts/use-cases-google-cloud#gpd-current-diagnostics) . You can also use this information to [select the regions and zones](/network-intelligence-center/docs/performance-dashboard/concepts/use-cases-google-cloud#workload_optimization_planning_for_performance) where you should deploy your workloads to optimize network performance.\nUse the following tools provided by Network Intelligence Center to monitor and analyze network performance for your workloads in Google Cloud:\n- [Performance Dashboard](/network-intelligence-center/docs/performance-dashboard/concepts/overview) shows latency between Google Cloud regions and between individual regions and locations on the internet. Performance Dashboard can help you determine where to place workloads for best latency and help determine when an application issue might be due to underlying network issues.\n- [Network Topology](/network-intelligence-center/docs/network-topology/concepts/overview) shows a visual view of your Virtual Private Cloud (VPC) networks, hybrid connectivity with your on-premises networks, and connectivity to Google-managed services. Network Topology provides real-time operational metrics that you can use to analyze and understand network performance and identify unusual traffic patterns.\n- [Network Analyzer](/network-intelligence-center/docs/network-analyzer/overview) is an automatic configuration monitoring and diagnostics tool. It verifies VPC network configurations for firewall rules, routes, configuration dependencies, and connectivity for services and applications. It helps you identify network failures, and provides root cause analysis and recommendations. Network Analyzer provides prioritized insights to help you analyze problems with network configuration, such as high utilization of IP addresses in a subnet.## API Gateway and Apigee\nThis section provides recommendations to help you optimize the performance of the APIs that you deploy in Google Cloud by using [API Gateway](/api-gateway) and [Apigee](/apigee) .\nAPI Gateway lets you create and manage APIs for Google Cloud serverless backends, including Cloud Functions, Cloud Run, and App Engine. These services are managed services, and they scale automatically. But as the applications that are deployed on these services scale, you might need to increase the [quotas and rate limits](/api-gateway/docs/quotas) for API Gateway.\n**Note:** Exposing your applications or infrastructure to more traffic can cause performance bottlenecks at the next layer in the application stack.\nApigee provides the following analytics dashboards to help you monitor the performance of your managed APIs:\n- [API Proxy Performance Dashboard](/apigee/docs/api-platform/analytics/api-proxy-performance-dashboard) : Monitor API proxy traffic patterns and processing times.\n- [Target Performance Dashboard](/apigee/docs/api-platform/analytics/endpoint-performance-dashboard) : Visualize traffic patterns and performance metrics for API proxy backend targets.\n- [Cache Performance Dashboard](/apigee/docs/api-platform/analytics/cache-performance-dashboard) : Monitor performance metrics for Apigee cache, such as average cache-hit rate and average time in cache.\nIf you use [Apigee Integration](/apigee/docs/api-platform/integration/what-is-apigee-integration) , consider the [system-configuration limits](/apigee/docs/api-platform/integration/system-limits) when you build and manage your integrations.\n## What's next\nReview the best practices for optimizing the performance of your compute, storage, database, and analytics resources:\n- [Optimize compute performance](/architecture/framework/performance-optimization/compute) .\n- [Optimize storage performance](/architecture/framework/performance-optimization/storage) .\n- [Optimize database performance](/architecture/framework/performance-optimization/databases) .\n- [Optimize analytics performance](/architecture/framework/performance-optimization/analytics) .# Optimize database performance\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides recommendations to help you optimize the performance of your databases in Google Cloud.\n## Cloud SQL\nThe following recommendations help you to optimize the performance of your [Cloud SQL](/sql) instances running SQL Server, MySQL, and PostgreSQL databases.\n- For SQL Server databases, Google recommends that you [modify certain parameters](/sql/docs/sqlserver/best-practices#sqlserver_settings_modify) and [retain the default values for some parameters](/sql/docs/sqlserver/best-practices#sqlserver_settings_retain) .\n- When you choose the storage type for MySQL or PostgreSQL databases, consider the [cost-performance tradeoff between SSD and HDD storage](/sql/docs/mysql/choosing-ssd-hdd) .\n- To identify and analyze performance issues with PostgreSQL databases, use the [Cloud SQL Insights](/sql/docs/postgres/using-query-insights) dashboard.\n- To diagnose poor performance when running SQL queries, use the [EXPLAIN](/sql/docs/mysql/diagnose-issues#tips) statement.\nFor more information, see the following documentation:\n- [Optimize performance: SQL Server](/sql/docs/sqlserver/diagnose-issues#performance) \n- [Optimize performance: MySQL](/sql/docs/mysql/diagnose-issues#performance) \n- [Optimize performance: PostgreSQL](/sql/docs/postgres/diagnose-issues#performance) ## Bigtable\nThis section provides recommendations to help you optimize the performance of your [Bigtable](/bigtable/docs/overview) instances.\n### Plan capacity based on performance requirements\nYou can use Bigtable in a broad spectrum of applications, each with a different optimization goal. For example, for batch data-processing jobs, throughput might be more important than latency. For an online service that serves user requests, you might need to prioritize lower latency over throughput. When you plan capacity for your Bigtable clusters, consider the tradeoffs between throughput and latency. For more information, see [Plan your Bigtable capacity](/bigtable/docs/performance#planning-your-capacity) .\n### Follow schema-design best practices\nYour tables can scale to billions of rows and thousands of columns, enabling you to store petabytes of data. When you design the schema for your Bigtable tables, consider the [schema design best practices](/bigtable/docs/schema-design) .\n### Monitor performance and make adjustments\n[Monitor](/bigtable/docs/monitoring-instance) the CPU and disk usage for your instances, analyze the performance of each cluster, and review the sizing recommendations that are shown in the monitoring charts.\n## Spanner\nThis section provides recommendations to help you optimize the performance of your [Spanner](/spanner) instances.\n### Choose a primary key that prevents a hotspot\nA hotspot is a single server that is forced to handle many requests. When you choose the primary key for your database, follow the [schema design best practices](/spanner/docs/schema-design) to prevent a hotspot.\n### Follow best practices for SQL coding\nThe SQL compiler in Spanner converts each declarative SQL statement that you write into an imperative [query execution plan](/spanner/docs/query-execution-plans) . Spanner uses the execution plan to run the SQL statement. When you construct SQL statements, follow [SQL best practices](/spanner/docs/sql-best-practices) to make sure that Spanner uses execution plans that yield optimal performance.\n### Use query options to manage the SQL query optimizer\nSpanner uses a [SQL query optimizer](/spanner/docs/query-optimizer/overview) to transform SQL statements into efficient query execution plans. The query execution plan that the optimizer produces might change slightly when the query optimizer itself evolves, or when the database statistics are updated. You can minimize the potential for performance regression when the query optimizer or the database statistics change by using [query options](/spanner/docs/query-optimizer/manage-query-optimizer) .\n### Visualize and tune the structure of query execution plans\nTo analyze query performance issues, you can visualize and tune the structure of the query execution plans by using the [query plan visualizer](/spanner/docs/tune-query-with-visualizer) .\n### Use operations APIs to manage long-running operations\nFor certain method calls, Spanner creates long-running operations, which might take a substantial amount of time to complete. For example, when you [restore a database](/spanner/docs/reference/rest/v1/projects.instances.databases/restore) , Spanner creates a long-running operation to track restore progress. To help you monitor and manage long-running operations, Spanner provides operations APIs. For more information, see [Managing long-running operations](/spanner/docs/manage-long-running-operations) .\n### Follow best practices for bulk loading\nSpanner supports several options for loading large amounts of data in bulk. The performance of a bulk-load operation depends on factors such as partitioning, the number of write requests, and the size of each request. To load large amounts of data efficiently, follow [bulk-loading best practices](/spanner/docs/bulk-loading) .\n### Monitor and control CPU utilization\nThe CPU utilization of your Spanner instance can affect request latencies. An overloaded backend server can cause higher request latencies. Spanner provides [CPU utilization metrics](/spanner/docs/cpu-utilization) to help you [investigate high CPU utilization](/spanner/docs/introspection/investigate-cpu-utilization) . For performance-sensitive applications, you might need to r [educe CPU utilization by increasing the compute capacity](/spanner/docs/cpu-utilization#reduce) .\n### Analyze and solve latency issues\nWhen a client makes a remote procedure call to Spanner, the API request is first prepared by the client libraries. The request then passes through the [Google Front End](/docs/security/infrastructure/design#google-frontend-service) and the Cloud Spanner API frontend before it reaches the Spanner database. To analyze and solve latency issues, you must [measure and analyze the latency](/spanner/docs/latency-metrics) for each segment of the path that the API request traverses. For more information, see [Spanner end-to-end latency guide](/spanner/docs/latency-guide) .\n### Launch applications after the database reaches the warm state\nAs your Spanner database grows, it divides the key space of your data into [splits](/spanner/docs/schema-and-data-model#database-splits) . Each split is a range of rows that contains a subset of your table. To balance the overall load on the database, Spanner dynamically moves individual splits independently and assigns them to different servers. When the splits are distributed across multiple servers, the database is considered to be in a state. A database that's warm can maximize parallelism and deliver improved performance. Before you launch your applications, we recommend that you [warm up your database](/spanner/docs/pre-warm-database) with test data loads.\n## What's next\nReview the best practices for optimizing the performance of your compute, storage, networking, and analytics resources:\n- [Optimize compute performance](/architecture/framework/performance-optimization/compute) .\n- [Optimize storage performance](/architecture/framework/performance-optimization/storage) .\n- [Optimize networking performance](/architecture/framework/performance-optimization/networking) .\n- [Optimize analytics performance](/architecture/framework/performance-optimization/analytics) .# Optimize analytics performance\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides recommendations to help you optimize the performance of your analytics workloads in Google Cloud.\n## BigQuery\nThis section provides recommendations to help you optimize the performance of queries in [BigQuery](/bigquery) .\n### Optimize query design\nQuery performance depends on factors like the number of bytes that your queries read and write, and the volume of data that's passed between [slots](/bigquery/docs/slots) . To optimize the performance of your queries in BigQuery, apply the best practices that are described in the following documentation:\n- [Introduction to optimizing query performance](/bigquery/docs/best-practices-performance-overview) \n- [Managing input data and data sources](/bigquery/docs/best-practices-performance-input) \n- [Optimizing communication between slots](/bigquery/docs/best-practices-performance-communication) \n- [Optimize query computation](/bigquery/docs/best-practices-performance-compute) \n- [Manage query outputs](/bigquery/docs/best-practices-performance-output) \n- [Avoiding SQL anti-patterns](/bigquery/docs/best-practices-performance-patterns) \n### Define and use materialized views efficiently\nTo improve the performance of workloads that use common and repeated queries, you can use [materialized views](/bigquery/docs/materialized-views-intro) . There are [limits](/bigquery/docs/materialized-views-intro#limitations) to the number of materialized views that you can create. Don't create a separate materialized view for every permutation of a query. Instead, define materialized views that you can use for multiple patterns of queries.\n### Improve JOIN performance\nYou can use [materialized views](/bigquery/docs/materialized-views-intro) to reduce the cost and latency of a query that performs aggregation on top of a [JOIN](/bigquery/docs/reference/standard-sql/query-syntax#join_types) . Consider a case where you join a large fact table with a few small dimension tables, and then perform an [aggregation](/bigquery/docs/reference/standard-sql/aggregate-function-calls) on top of the join. It might be practical to rewrite the query to first perform the aggregation on top of the fact table with foreign keys as grouping keys. Then, join the result with the dimension tables. Finally, perform a post-aggregation.\n## Dataflow\nThis section provides recommendations to help you optimize query performance of your [Dataflow](/dataflow) pipelines.\nWhen you create and deploy pipelines, you can configure execution parameters, like the Compute Engine machine type that should be used for the Dataflow worker VMs. For more information, see [Pipeline options](/dataflow/docs/reference/pipeline-options) .\nAfter you deploy pipelines, Dataflow manages the Compute Engine and Cloud Storage resources that are necessary to run your jobs. In addition, the following features of Dataflow help optimize the performance of the pipelines:\n- **Parallelization** : Dataflow automatically partitions your data and distributes your worker code to Compute Engine instances for parallel processing. For more information, see [parallelization and distribution](/dataflow/docs/guides/deploying-a-pipeline#parallelization-and-distribution) .\n- **Optimization** : Dataflow uses your pipeline code to create an execution graph that represents [PCollection](https://beam.apache.org/documentation/programming-guide/#pcollections) objects and [transforms](https://beam.apache.org/documentation/programming-guide/#transforms) in the pipeline. It then optimizes the graph for the most efficient performance and resource usage. Dataflow also automatically optimizes potentially costly operations, such as data aggregations. For more information, see [Fusion optimization](/dataflow/docs/guides/deploying-a-pipeline#fusion-optimization) and [Combine optimization](/dataflow/docs/guides/deploying-a-pipeline#combine-optimization) .\n- **Automatic tuning** : Dataflow dynamically optimizes jobs while they are running by using [Horizontal Autoscaling](/dataflow/docs/guides/deploying-a-pipeline#horizontal-autoscaling) , [Vertical Autoscaling](/dataflow/docs/guides/deploying-a-pipeline#vertical-autoscaling) , and [Dynamic Work Rebalancing](/dataflow/docs/guides/deploying-a-pipeline#dynamic-work-rebalancing) .\nYou can monitor the performance of Dataflow pipelines by using the web-based [monitoring interface](/dataflow/docs/guides/using-monitoring-intf) or the [Dataflow gcloud CLI](/dataflow/docs/guides/using-command-line-intf) .\n**Note:** You can also use Cloud Profiler to identify the parts of the pipeline code that consume the most resources. For more information, see [Monitoring pipeline performance](/dataflow/docs/guides/profiling-a-pipeline) .\n## Dataproc\nThis section describes best practices to optimize the performance of your [Dataproc](/dataproc) clusters.\n### Autoscale clusters\nTo ensure that your Dataproc clusters deliver predictable performance, you can enable autoscaling. Dataproc uses [Hadoop YARN memory metrics](/dataproc/docs/concepts/configuring-clusters/autoscaling#hadoop_yarn_metrics) and an autoscaling policy that you define to automatically adjust the number of worker VMs in a cluster. For more information about how to use and configure autoscaling, see [Autoscaling clusters](/dataproc/docs/concepts/configuring-clusters/autoscaling) .\n### Provision appropriate storage\nChoose an appropriate storage option for your Dataproc cluster based on your performance and cost requirements:\n- If you need a low-cost Hadoop-compatible file system (HCFS) that Hadoop and Spark jobs can read from and write to with minimal changes, use Cloud Storage. The data stored in Cloud Storage is persistent, and can be accessed by other Dataproc clusters and other products such as BigQuery.\n- If you need a low-latency [Hadoop Distributed File System (HDFS)](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html) for your Dataproc cluster, use Compute Engine persistent disks attached to the worker nodes. The data stored in HDFS storage is transient, and the storage cost is higher than the Cloud Storage option.\n- To get the performance advantage of Compute Engine persistent disks and the cost and durability benefits of Cloud Storage, you can combine both of the storage options. For example, you can store your source and final datasets in Cloud Storage, and provision limited HDFS capacity for the intermediate datasets. When you decide on the size and type of the disks for HDFS storage, consider the recommendations in the [Persistent disks and local SSDs](/architecture/framework/performance-optimization/storage#persistent-disks-and-local-ssds) section.\n### Reduce latency when using Cloud Storage\nTo reduce latency when you access data that's stored in Cloud Storage, we recommend the following:\n- Create your Cloud Storage bucket in the same region as the Dataproc cluster.\n- Disable`auto.purge`for Apache Hive-managed tables stored in Cloud Storage.\n- When using Spark SQL, consider creating Dataproc clusters with the latest versions of available [images](/dataproc/docs/concepts/versioning) . By using the latest version, you can avoid performance issues that might remain in older versions, such as [slow INSERT OVERWRITE performance](https://issues.apache.org/jira/browse/SPARK-18107) in Spark 2.x.\n- To minimize the possibility of writing many files with varying or small sizes to Cloud Storage, you can configure the [Spark SQL parameters](/dataproc/docs/concepts/configuring-clusters/flex#advanced_configuration) `spark.sql.shuffle.partitions`and`spark.default.parallelism`or the Hadoop parameter`mapreduce.job.reduces`.\n### Monitor and adjust storage load and capacity\nThe persistent disks attached to the worker nodes in a Dataproc cluster hold [shuffle](/dataproc/docs/support/spark-job-tuning#configure_partitioning_and_shuffling) data. To perform optimally, the worker nodes need sufficient disk space. If the nodes don't have sufficient disk space, the nodes are marked as `UNHEALTHY` in the [YARN NodeManager](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/NodeManager.html) log. If this issue occurs, either increase the disk size for the affected nodes, or run fewer jobs concurrently.\n### Enable EFM\nWhen worker nodes are removed from a running Dataproc cluster, such as due to downscaling or preemption, shuffle data might be lost. To minimize job delays in such scenarios, we recommend that you enable [Enhanced Flexibility Mode (EFM)](/dataproc/docs/concepts/configuring-clusters/flex) for clusters that use [preemptible VMs](/dataproc/docs/concepts/compute/preemptible-vms) or that only autoscale the secondary worker group.\n## What's next\nReview the best practices for optimizing the performance of your compute, storage, networking, and database resources:\n- [Optimize compute performance](/architecture/framework/performance-optimization/compute) .\n- [Optimize storage performance](/architecture/framework/performance-optimization/storage) .\n- [Optimize networking performance](/architecture/framework/performance-optimization/networking) .\n- [Optimize database performance](/architecture/framework/performance-optimization/databases) .# What's new in the Architecture Framework\nThis document lists significant changes to the Google Cloud Architecture Framework.\n## November 28, 2023\n- Reliability category:- Reorganized the content to improve readability and consistency:\n- [Define SLOs](/architecture/framework/reliability/defining-SLOs) : Moved \"Terminology\" section to new page: [Terminology](/architecture/framework/reliability/terminology) \n- [Adopt SLOs](/architecture/framework/reliability/adopting-slos) : Moved \"SLOs and Alerts\" section to new page: [SLOs and Alerts](/architecture/framework/reliability/slo-and-alerts) \n## November 9, 2023\n- System design category:- Added guidance to help cloud architects to [choose deployment archetypes](/architecture/framework/system-design/archetypes) for workloads in Google Cloud.\n## September 8, 2023\n- Cost optimization category:- Added information about using tags for cost allocation and governance.\n- Updated the guidance for identifying labeling anomalies.\nFor more information, see [Track and allocate cost using tags or labels](/architecture/framework/cost-optimization/monitor#track_and_allocate_cost_using_tags_or_labels) .## August 28, 2023\n- System design category:- Updated the list of [AI and ML services](/architecture/framework/system-design/ai-ml) in Google Cloud.\n## August 23, 2023\n- Cost optimization category:- Added guidance about optimizing [Spanner resource usage](/architecture/framework/cost-optimization/databases#spanner) for small workloads by using Processing Units instead of nodes.\n## August 18, 2023\n- Security, privacy, and compliance category:- Added [HIPAA-related guidance](/architecture/framework/security/compliance) .\n- Operational excellence category:- Updated the best practices for [planning for peak traffic events](/architecture/framework/operational-excellence/plan-for-peak-traffic-and-launch-events) .\n## August 9, 2023\n- Reliability category:- Added links to reliability-related documentation for the following products:- [Dataproc](/architecture/framework/reliability/product-guides/dataproc) \n- [Bigtable](/architecture/framework/reliability/product-guides/bigtable) \n- [Cloud SQL](/architecture/framework/reliability/product-guides/sql) \n- [Spanner](/architecture/framework/reliability/product-guides/spanner) ## July 13, 2023\n- System design:- Added AlloyDB for PostgreSQL to the list of [database services](/architecture/framework/system-design/databases) in Google Cloud.\n- Cost optimization:- Added guidance about Google Cloud Hyperdisk and local SSDs in the [Persistent Disk](/architecture/framework/cost-optimization/storage#pd) section.\n## June 23, 2023\n- Performance optimization:- Added guidance about optimizing bandwidth usage by using [jumbo frames](/architecture/framework/performance-optimization/networking#jumbo_frames) and [appropriate machine types for VMs](/architecture/framework/performance-optimization/networking#vm_performance) .\n## June 15, 2023\n- Security, privacy, and compliance:- Added guidance about [securing connections to on-premises and multicloud environments](/architecture/framework/security/network-security#secure_connections_to_your_on-premises_or_multi-cloud_environments) by using Cross-Cloud Interconnect.\n- Added guidance about [securing the perimeter of cloud workloads](/architecture/framework/security/network-security#secure_your_perimeter) by using firewall policies and rules and Secure Web Proxy.\n## March 30, 2023\n- Added two additional SLO articles to the framework:- [Define SLOs](/architecture/framework/reliability/defining-SLOs) \n- [Adopt SLOs](/architecture/framework/reliability/adopting-slos) \n## September 16, 2022\n- Major expansion of the [performance optimization](/architecture/framework/performance-optimization) category.## August 10, 2022\n- Changes in the [System design](/architecture/framework/system-design) category:- Added [core principles of system design](/architecture/framework/system-design/principles) .\n- Added [recommendations](/architecture/framework/system-design) to use Google Cloud consulting services and engage with our partners.\n## August 4, 2022\n- Added information about the following capabilities in the [cost optimization](/architecture/framework/cost-optimization) category:- Preventing outages due to billing issues by locking the link between a project and its billing account. For more information, see [Configure billing access control](/architecture/framework/cost-optimization/monitor#configure_billing_access_control) .\n- Reducing quota by using the Service Usage API. For more information, see [Budgets, alerts, and quotas](/architecture/framework/cost-optimization/monitor#budgets_alerts_and_quotas) .\n- Identifying and removing unattended projects. For more information, see [Active Assist recommendations](/architecture/framework/cost-optimization/monitor#active_assist_recommendations) .\n## July 13, 2022\n- Changes in the [operational excellence](/architecture/framework/operational-excellence) category:- Added a best practice on how to [build a center of excellence](/architecture/framework/operational-excellence/establish-cloud-support-and-escalation-processes#build_centers_of_excellence) .\n- Added a best practice on how to [set up an audit trail](/architecture/framework/operational-excellence/set-up-monitoring-alerting-logging#set_up_an_audit_trail) .\n## June 27, 2022\n- Added information about [shared responsibilities and shared fate inGoogle Cloud](/architecture/framework/security/shared-responsibility-shared-fate) in the [Security](/architecture/framework/security) category.## June 13, 2022\n- Added recommendations to help you design cloud workloads for [environmental sustainability](/architecture/framework/system-design/sustainability) in the [system design](/architecture/framework/system-design) category.## June 1, 2022\n- Added information about Spot VMs in the [Optimize cost: Compute, containers, and serverless](/architecture/framework/cost-optimization/compute#compute) section.## May 7, 2022\n- Added information about dual-region locations in the [cost-optimization best practices](/architecture/framework/cost-optimization/storage#location) for Cloud Storage.## May 4, 2022\n- Added [product reliability guides](/architecture/framework/reliability/product-guides) . These guides include reliability best practices for the following products:- **Compute:**  [Compute Engine](/architecture/framework/reliability/product-guides/compute) , [Cloud Run](/architecture/framework/reliability/product-guides/run) , [Cloud Functions](/architecture/framework/reliability/product-guides/functions) , and [Google Kubernetes Engine](/architecture/framework/reliability/product-guides/kubernetes-engine) \n- **Storage and databases:**  [Cloud Storage](/architecture/framework/reliability/product-guides/storage) , [Firestore](/architecture/framework/reliability/product-guides/firestore) \n- **Networking:**  [Cloud DNS](/architecture/framework/reliability/product-guides/dns) and [Cloud Load Balancing](/architecture/framework/reliability/product-guides/load-balancing) \n- **Data analytics:**  [BigQuery](/architecture/framework/reliability/product-guides/bigquery) \n## February 25, 2022\n- Changes to the [security](/architecture/framework/security) category:- Updated [compliance](/architecture/framework/security/compliance) best practices to discuss automation.\n## December 15, 2021\n- Launch of the [Architecture Framework space in the Google Cloud Community](https://www.googlecloudcommunity.com/gc/Architecture-Framework-Private/ct-p/cloud-architecture-framework) .## October 25, 2021\n- Changes in the [reliability](/architecture/framework/reliability) category:- Added best practices for [replicating data across regions for disaster recovery](/architecture/framework/reliability/design-scale-high-availability#replicate_data_across_regions_for_disaster_recovery) and [designing a multi-region architecture for resilience to regional outages](/architecture/framework/reliability/design-scale-high-availability#design_a_multi-region_architecture_for_resilience_to_regional_outages) .\n- Added an example of how to [calculate error budgets](/architecture/framework/reliability/principles#error_budget) .\n- Added a best practice for [choosing good names for applications and services](/architecture/framework/reliability/create-operational-processes-tools#choose_good_names_for_applications_and_services) .\n## October 7, 2021\n- Major refresh of all the categories.\n- Anna Berenberg and Brad Calder, [Deployment Archetypes for Cloud Applications](https://dl.acm.org/doi/10.1145/3498336) , ACM Computing Surveys, Volume 55, Issue 3, Article No.: 61, pp 1-48 [\u21a9](#fnref1)", "guide": "Docs"}