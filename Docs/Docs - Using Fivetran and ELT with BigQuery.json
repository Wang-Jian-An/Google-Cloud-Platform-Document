{"title": "Docs - Using Fivetran and ELT with BigQuery", "url": "https://cloud.google.com/architecture/partners/using-fivetran-and-elt-with-bigquery?hl=zh-cn", "abstract": "# Docs - Using Fivetran and ELT with BigQuery\nLast reviewed 2022-12-02 UTC\nBy Charles Wang, Product Evangelist, and Brenner Heintz, Technical Product Marketing Manager [Fivetran](https://fivetran.com/)\nThis article describes how your organization can benefit from replacing extract, transform, and load (ETL) with extract, load, and transform (ELT) by using Fivetran and [BigQuery](/bigquery) . It's intended for analysts, data scientists, and data engineers whose work directly involves or depends on data pipelines.\n", "content": "## Legacy ETL, meet modern ELT pipelines\nThe concept of [ETL](https://wikipedia.org/wiki/Extract,_transform,_load) has existed since the 1970s and has been combined with data warehousing for nearly as long. Today, ETL is still widely used, but it suffers from limitations that were accepted at a time when computation, storage, and internet bandwidth were scarce and expensive.\nIn particular, some downsides to ETL pipelines are as follows:\n- **Complex** - data pipelines tend to run on custom code, even when supported by data integration and management tools.\n- **Brittle and risky** - upstream schema changes and changing analytics needs can require extensive revisions of the code base.\n- **Inaccessible** - smaller organizations without dedicated data engineers are often forced to sample their data or conduct manual, ad hoc reporting.\nThese challenges make effective analytics and business intelligence costly and labor-intensive for even well-staffed organizations, and entirely inaccessible to smaller and understaffed ones.\n## Accelerate analytics with automated ELT on Google Cloud\nFivetran offers a data pipeline tool predicated on the extract-load-transform (ELT) doctrine instead of ETL. Switching the order of the loading and transformation stages\u2014and using a standardized tool\u2014addresses each of the three major shortcomings of ETL. The following are the benefits of ELT:\n- **The pipeline is simplified** - the responsibility of transforming the data shifts downstream to analysts instead of data engineers.\n- **The pipeline is more resilient and less risky** - changing analytics needs no longer affect the upstream work of data engineers, because data transformation happens in the data warehouse.\n- **The pipeline is more accessible** - it's less labor-intensive to maintain. Because the pipeline is greatly simplified and intrinsically more resilient, it's now possible for an outside party to build and maintain a standardized tool to sell to multiple customers. By purchasing a standardized tool, you essentially outsource and automate the extract and load stages of the pipeline.\nUsing Fivetran to perform ELT enables you to preserve the labor of data scientists and engineers while assuming the increasingly trivial cost of storage and computation. After the data is warehoused, your analysts can use SQL to transform the data based on their reporting needs.\n## How Fivetran powers the modern data stack\nThe following diagram shows how you can interact with the Fivetran data connector tool.\nThe basic product that Fivetran offers is a data connector. Each connector file, or database, normalizes and applies some light cleaning to the data, and then routes the data to a data warehouse. The following diagram illustrates the data stack.\nWith Fivetran, you can choose from over 200 prebuilt connectors to data sources like applications, databases, event logs, files, and cloud functions. [See the list of all of our connectors here.](https://www.fivetran.com/connectors)\nConnectors for well-known business apps such as Salesforce, Google Ads, and Oracle organize the data into standardized schemas before loading them to your data warehouse. Standardizing the schemas lets you\u2014and anyone else who uses the same connectors\u2014use the same analytics code snippets, because your data is all structured exactly the same way. You can also transform your data using SQL using [Fivetran Transformations with dbt core](https://www.fivetran.com/transformations) .\nFor custom and obscure APIs and file formats, you can use [Fivetran's Google Cloud Function connectors](https://www.fivetran.com/connectors/google-cloud-functions) to write modules containing the necessary code.\nOnce you've moved your data from sources through the Fivetran data pipeline to the data warehouse, users can access it through their preferred business and data visualization tool.\n## What makes Fivetran different from other ELT tools?- Automatic adjustment to schema and API changes\n- Zero-maintenance, fully-managed data replication\n- In-warehouse transformations, with custom logic and views\n- More than 200 prebuilt connectors\n- Analysis-ready schemas\n- Rapid data access through incremental loads\n- Complete data and schema replication\n- Support for slowly changing dimensions (Type 2 data)\n- Rapid, automated recovery from failure (idempotency)## Using Fivetran with Google Cloud\nFivetran doesn't support on-premises data warehouses, so data warehouse in the current context refers strictly to the cloud. One of the signature benefits of cloud data warehousing is the ability to recruit decentralized computation resources on demand. [BigQuery](/bigquery) is a serverless infrastructure capable of spooling up additional resources without explicit instruction from the user. You don't have to worry about buying excess capacity or designing data infrastructure and workflows to even out the use of computational resources over time.\nYou can access Fivetran connectors to your data sources through the BigQuery interface. To see the full list of available connectors, go to [Google Cloud Marketplace](https://console.cloud.google.com/marketplace/browse?q=fivetran) .\nTo get started, select the appropriate connector, enter your credentials for your data sources and follow the directions to set up in no more than a few minutes.\n## Why shouldn't you build it yourself?\nIf ELT simplifies data engineering, then why should you purchase a tool rather than build one from the ground up to perfectly suit your needs?\nOne reason is the sheer expense in time from construction and maintenance. Based on our experience, you will likely spend at least five weeks to build a connector, and another four weeks per year to maintain it as the API endpoint periodically updates. A relatively common complement of five connectors can thus demand 45 person-weeks in the first year, virtually one year's salary for a full-time data engineer\u2013not to mention the downtime associated with maintenance.\nMoreover, the data engineer in question is unlikely to be an expert in the idiosyncrasies of each particular data source. Some APIs are poorly documented, [extremely complex](https://fivetran.com/blog/netsuite-connector-article) , or ignore best practices. You should also consider the effect such work has on morale, as data pipelines are generally considered [tedious and thankless](https://www.quora.com/Whats-the-most-tedious-part-of-building-ETLs-and-or-data-pipelines) to build, yet often end up in the purview of people who would rather do other things, especially analysts and data scientists.\nYou can solve these problems with a standardized, off-the-shelf solution that performs its job with minimal supervision.\n## What's next\n- Read why the days of [on-premises data warehouses are numbered](https://fivetran.com/blog/microstrategy-world-2019) .\n- [Things to consider before moving to a data lake](https://fivetran.com/blog/when-to-adopt-a-data-lake) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Docs"}