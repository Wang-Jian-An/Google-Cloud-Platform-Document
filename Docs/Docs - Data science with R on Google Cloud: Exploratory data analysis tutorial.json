{"title": "Docs - Data science with R on Google Cloud: Exploratory data analysis tutorial", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Docs - Data science with R on Google Cloud: Exploratory data analysis tutorial\nThis tutorial shows you how to get started with data science at scale with R on Google Cloud. This is intended for those who have some experience with R and with Jupyter notebooks, and who are comfortable with SQL.\nThis tutorial focuses on performing exploratory data analysis using Vertex AI Workbench user-managed notebooks and BigQuery. You can find the code for this tutorial in a [Jupyter notebook](https://github.com/GoogleCloudPlatform/ml-on-gcp/blob/master/tutorials/R/01_EDA-with-R-and-BigQuery.ipynb) that's on GitHub.", "content": "## OverviewR is one of the most widely used programming languages for statistical modeling. It has a large and active community of data scientists and machine learning (ML) professionals. With over 15,000 packages in the open-source repository of the [Comprehensive R Archive Network (CRAN)](https://cran.r-project.org/) , R has tools for all statistical data analysis applications, ML, and visualization. R has experienced steady growth in the last two decades due to its expressiveness of its syntax, and because of how comprehensive its data and ML libraries are.\nAs a data scientist, you might want to know how you can make use of your skill set by using R, and how you can also harness the advantages of the scalable, fully managed cloud services for ML.## ArchitectureIn this tutorial, you use [user-managed notebooks](/vertex-ai/docs/workbench/user-managed) as the data science environment to perform exploratory data analysis (EDA). You use R on data that you extract as part of this tutorial from BigQuery, Google's serverless, highly scalable, and cost-effective cloud data warehouse. After you analyze and process the data, the transformed data is stored in Cloud Storage for further ML tasks. This flow is shown in the following diagram:\n### Data for the tutorialThe dataset used in this tutorial is the [BigQuery natality dataset](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=samples&t=natality&page=table&_ga=2.99329886.-1705629017.1551465326&_gac=1.109796023.1561476396.CI2rz-z4hOMCFc6RhQods4oEXA) . This public dataset includes information about more than 137 million births registered in the United States from 1969 to 2008.\nThis tutorial focuses on EDA and on visualization using R and BigQuery. The tutorial sets you up for a machine-learning goal of predicting a baby's weight given a number of factors about the pregnancy and about the baby's mother, although that task is not covered in this tutorial.\n### User-managed notebooksVertex AI Workbench user-managed notebooks is a service that offers an integrated [JupyterLab](https://jupyterlab.readthedocs.io/en/stable/) environment, with the following features:- **One-click deployment** . You can use a single click to start a JupyterLab instance that's preconfigured with the latest machine-learning and data-science frameworks.\n- **Scale on demand** . You can start with a small machine configuration (for example, 4\u00a0vCPUs and 15\u00a0GB of RAM, as in this tutorial), and when your data gets too big for one machine, you can scale up by adding CPUs, RAM, and GPUs.\n- **Google Cloud integration** . Vertex AI Workbench user-managed notebooks instances are integrated with Google Cloud services like [BigQuery](/bigquery) . This integration makes it easy to go from data ingestion to preprocessing and exploration.\n- **Pay-per-use pricing** . There are no minimum fees or up-front commitments. See [pricing for Vertex AI Workbenchuser-managed notebooks](/vertex-ai/pricing) . You also pay for the Google Cloud resources that you use with the user-managed notebooks instance.\nUser-managed notebooks runs on [Deep Learning VM Images](/deep-learning-vm) . These images are optimized to support ML frameworks like PyTorch and TensorFlow. This tutorial supports creating a user-managed notebooks instance that has R 3.6.\n### Working with BigQuery using R [BigQuery](/bigquery) doesn't require infrastructure management, so you can focus on uncovering meaningful insights. BigQuery lets you use familiar SQL to work with your data, so you don't need a database administrator. You can use BigQuery to analyze large amounts of data at scale, and to prepare datasets for ML using BigQuery's rich SQL analytical capabilities.\nTo query BigQuery data using R, you can use [bigrquery](https://cran.r-project.org/web/packages/bigrquery/index.html) , an open-source R library. The bigrquery package provides the following levels of abstraction on top of BigQuery:- The low-level API provides thin wrappers over the underlying [BigQuery REST API](/bigquery/docs/reference/rest) .\n- The [DBI interface](http://www.r-dbi.org/) wraps the low-level API and makes working with BigQuery similar to working with any other database system. This is the most convenient layer if you want to run SQL queries in BigQuery or upload less than 100\u00a0MB.\n- The [dbplyr](http://dbplyr.tidyverse.org/) interface lets you treat BigQuery tables like in-memory data frames. This is the most convenient layer if you don't want to write SQL, but instead want [dbplyr](https://cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html) to write it for you.\nThis tutorial uses the low-level API from bigrquery, without requiring DBI or dbplyr.## Objectives\n- Create a user-managed notebooks instance that has R support.\n- Query and analyze data from BigQuery using the bigrquery R library.\n- Prepare and store data for ML in Cloud Storage.\n## CostsIn this document, you use the following billable components of Google Cloud:- [BigQuery](/bigquery/pricing) \n- [Vertex AI Workbench user-managed notebooks](/vertex-ai/pricing) instance. You are also charged for resources used within notebooks, including compute resources, BigQuery, and API requests.\n- [Cloud Storage](/storage/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . ## Before you begin\n## Creating a user-managed notebooks instance with RThe first step is to create a user-managed notebooks instance that you can use for this tutorial.- In the Google Cloud console, go to the **Notebooks** page. [Go to Notebooks](https://console.cloud.google.com/ai-platform/notebooks/instances) \n- On the **User-managed notebooks** tab, click add_box **New notebook** .\n- Select **R 3.6** . \n- For this tutorial, leave all the default values and click **Create** : The user-managed notebooks instance can take up to 90 seconds to start. When it's ready, you see it listed in the **Notebook instances** pane with an **Open JupyterLab** link next to the instance name: \n## Opening JupyterLabTo go through the tutorial in the notebook, you need to open the JupyterLab environment, clone the [ml-on-gcp](https://github.com/GoogleCloudPlatform/ml-on-gcp) GitHub repository, and then open the notebook.- In the instances list, click **Open Jupyterlab** . This opens the JupyterLab environment in your browser. \n- To launch a terminal tab, click **Terminal** in the Launcher.\n- In the terminal, clone the [ml-on-gcp](https://github.com/GoogleCloudPlatform/ml-on-gcp) GitHub repository:```\ngit clone https://github.com/GoogleCloudPlatform/ml-on-gcp.git\n```When the command finishes, you see the `ml-on-gcp` folder in the file browser.\n- In the file browser, open `ml-on-gcp` , then `tutorials` , and then `R.`The result of cloning looks like the following: \n## Opening the notebook and setting up RThe R libraries that you need for this tutorial, including bigrquery, are installed in R notebooks by default. As part of this procedure, you import them to make them available to the notebook.- In the file browser, open the `01-EDA-with-R-and-BigQuery.ipynb` notebook.This notebook covers the exploratory data analysis tutorial with R and BigQuery. From this point in the tutorial, you work in the notebook, and you run the code you see within the Jupyter notebook itself.\n- Import the R libraries that you need for this tutorial:```\nlibrary(bigrquery) # used for querying BigQuerylibrary(ggplot2) # used for visualizationlibrary(dplyr) # used for data wrangling\n```\n- Authenticate `bigrquery` using out-of-band authentication:```\nbq_auth(use_oob = True)\n```\n- Set a variable to the name of the project you use for this tutorial:```\n# Set the project IDPROJECT_ID <- \"gcp-data-science-demo\"\n```\n- Set a variable to the name of the Cloud Storage bucket:```\nBUCKET_NAME <- \"bucket-name\"\n```Replace with a name that's globally unique.You use the bucket later to store the output data.\n## Querying data from BigQueryIn this section of the tutorial, you read the results of executing a BigQuery SQL statement into R and take a preliminary look at the data.- Create a BigQuery SQL statement that extracts some possible predictors and the target prediction variable for a sample of births since 2000:```\nsql_query <- \"\u00a0 \u00a0 SELECT\u00a0 \u00a0 \u00a0 ROUND(weight_pounds, 2) AS weight_pounds,\u00a0 \u00a0 \u00a0 is_male,\u00a0 \u00a0 \u00a0 mother_age,\u00a0 \u00a0 \u00a0 plurality,\u00a0 \u00a0 \u00a0 gestation_weeks,\u00a0 \u00a0 \u00a0 cigarette_use,\u00a0 \u00a0 \u00a0 alcohol_use,\u00a0 \u00a0 \u00a0 CAST(ABS(FARM_FINGERPRINT(CONCAT(\u00a0 \u00a0 \u00a0 \u00a0 CAST(YEAR AS STRING), CAST(month AS STRING),\u00a0 \u00a0 \u00a0 \u00a0 CAST(weight_pounds AS STRING)))\u00a0 \u00a0 \u00a0 \u00a0 ) AS STRING) AS key\u00a0 \u00a0 FROM\u00a0 \u00a0 \u00a0 \u00a0 publicdata.samples.natality\u00a0 \u00a0 WHERE\u00a0 \u00a0 \u00a0 year > 2000\u00a0 \u00a0 \u00a0 AND weight_pounds > 0\u00a0 \u00a0 \u00a0 AND mother_age > 0\u00a0 \u00a0 \u00a0 AND plurality > 0\u00a0 \u00a0 \u00a0 AND gestation_weeks > 0\u00a0 \u00a0 \u00a0 AND month > 0\u00a0 \u00a0 LIMIT %s\"\n```The `key` column is a generated row identifier based on the concatenated values of the `year` , `month` , and `weight_pounds` columns.\n- Run the query and retrieve the data as an in-memory `data frame` object:```\nsample_size <- 10000sql_query <- sprintf(sql_query, sample_size)natality_data <- bq_table_download(\u00a0 \u00a0 bq_project_query(\u00a0 \u00a0 \u00a0 \u00a0 PROJECT_ID,\u00a0 \u00a0 \u00a0 \u00a0 query=sql_query\u00a0 \u00a0 ))\n```\n- View the retrieved results:```\nhead(natality_data)\n```The output is similar to the following: \n- View the number of rows and data types of each column:```\nstr(natality_data)\n```The output is similar to the following:```\nClasses \u2018tbl_df\u2019, \u2018tbl\u2019 and 'data.frame': 10000 obs. of 8 variables:\n $ weight_pounds : num 7.75 7.4 6.88 9.38 6.98 7.87 6.69 8.05 5.69 9.22 ...\n $ is_male  : logi FALSE TRUE TRUE TRUE FALSE TRUE ...\n $ mother_age  : int 47 44 42 43 42 43 42 43 45 44 ...\n $ plurality  : int 1 1 1 1 1 1 1 1 1 1 ...\n $ gestation_weeks: int 41 39 38 39 38 40 35 40 38 39 ...\n $ cigarette_use : logi NA NA NA NA NA NA ...\n $ alcohol_use : logi FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ key   : chr \"3579741977144949713\" \"8004866792019451772\" \"7407363968024554640\" \"3354974946785669169\" ...\n```\n- View a summary of the retrieved data:```\nsummary(natality_data)\n```The output is similar to the following:```\n weight_pounds  is_male   mother_age  plurality\n Min. : 0.620 Mode :logical Min. :13.0 Min. :1.000\n 1st Qu.: 6.620 FALSE:4825  1st Qu.:22.0 1st Qu.:1.000\n Median : 7.370 TRUE :5175  Median :27.0 Median :1.000\n Mean : 7.274     Mean :27.3 Mean :1.038\n 3rd Qu.: 8.110     3rd Qu.:32.0 3rd Qu.:1.000\n Max. :11.440     Max. :51.0 Max. :4.000\n gestation_weeks cigarette_use alcohol_use   key\n Min. :18.00 Mode :logical Mode :logical Length:10000\n 1st Qu.:38.00 FALSE:580  FALSE:8284  Class :character\n Median :39.00 TRUE :83  TRUE :144  Mode :character\n Mean :38.68 NA's :9337  NA's :1572\n 3rd Qu.:40.00\n Max. :47.00\n```\n## Visualizing data using ggplot2In this section, you use the [ggplot2](https://ggplot2.tidyverse.org/) library in R to study some of the variables from the natality data set.- Display the distribution of the `weight_pounds` values using a histogram:```\nggplot(\u00a0 \u00a0 data = natality_data,\u00a0 \u00a0 aes(x = weight_pounds)) + geom_histogram(bins = 200)\n```The resulting plot is similar to the following: \n- Display the relationship between `gestation_weeks` and `weight_pounds` using a scatter plot:```\nggplot(\u00a0 \u00a0 data = natality_data,\u00a0 \u00a0 aes(x = gestation_weeks, y = weight_pounds)) + geom_point() + geom_smooth(method = \"lm\")\n```The resulting plot is similar to the following: \n## Processing the data in BigQuery from RWhen you're working with large datasets, we recommend that you perform as much analysis as possible (aggregation, filtering, joining, computing columns, and so on) in BigQuery and then retrieve the results. Performing these tasks in R is less efficient. Using BigQuery for analysis takes advantage of the scalability and performance of BigQuery, and makes sure that the returned results can fit into memory in R.- Create a function that finds the number of records and the average weight for each value of the chosen column:```\nget_distinct_values <- function(column_name) {\u00a0 \u00a0 query <- paste0(\u00a0 \u00a0 \u00a0 \u00a0 'SELECT ', column_name, ',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 COUNT(1) AS num_babies,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AVG(weight_pounds) AS avg_wt\u00a0 \u00a0 \u00a0 \u00a0 FROM publicdata.samples.natality\u00a0 \u00a0 \u00a0 \u00a0 WHERE year > 2000\u00a0 \u00a0 \u00a0 \u00a0 GROUP BY ', column_name)\u00a0 \u00a0 bq_table_download(\u00a0 \u00a0 \u00a0 \u00a0 bq_project_query(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PROJECT_ID,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 query = query\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )}\n```\n- Invoke this function using the `mother_age` column and then look at the number of babies and average weight by mother age:```\ndf <- get_distinct_values('mother_age')ggplot(data = df, aes(x = mother_age, y = num_babies)) + geom_line()ggplot(data = df, aes(x = mother_age, y = avg_wt)) + geom_line()\n```The output of the first `ggplot` command is as follows, showing the number of babies born by mother's age. The output of the second `ggplot` command is as follows, showing the the average weight of babies by mother's age. \nTo see more visualization examples, refer to the notebook.## Saving data as CSV filesThe next task is to save extracted data from BigQuery as CSV files in Cloud Storage so you can use it for further ML tasks.- Load training and evaluation data from BigQuery into R:```\n# Prepare training and evaluation data from BigQuerysample_size <- 10000sql_query <- sprintf(sql_query, sample_size)train_query <- paste('SELECT * FROM (', sql_query,\u00a0 ') WHERE MOD(CAST(key AS INT64), 100) <= 75')eval_query <- paste('SELECT * FROM (', sql_query,\u00a0 ') WHERE MOD(CAST(key AS INT64), 100) > 75')# Load training data to data frametrain_data <- bq_table_download(\u00a0 \u00a0 bq_project_query(\u00a0 \u00a0 \u00a0 \u00a0 PROJECT_ID,\u00a0 \u00a0 \u00a0 \u00a0 query = train_query\u00a0 \u00a0 ))# Load evaluation data to data frameeval_data <- bq_table_download(\u00a0 \u00a0 bq_project_query(\u00a0 \u00a0 \u00a0 \u00a0 PROJECT_ID,\u00a0 \u00a0 \u00a0 \u00a0 query = eval_query\u00a0 \u00a0 ))\n```\n- Write the data to a local CSV file:```\n# Write data frames to local CSV files, without headers or row namesdir.create(file.path('data'), showWarnings = FALSE)write.table(train_data, \"data/train_data.csv\",\u00a0 \u00a0row.names = FALSE, col.names = FALSE, sep = \",\")write.table(eval_data, \"data/eval_data.csv\",\u00a0 \u00a0row.names = FALSE, col.names = FALSE, sep = \",\")\n```\n- Upload the CSV files to Cloud Storage by wrapping [gsutil](https://github.com/GoogleCloudPlatform/gsutil/) commands that are passed to the system:```\n# Upload CSV data to Cloud Storage by passing gsutil commands to systemgcs_url <- paste0(\"gs://\", BUCKET_NAME, \"/\")command <- paste(\"gsutil mb\", gcs_url)system(command)gcs_data_dir <- paste0(\"gs://\", BUCKET_NAME, \"/data\")command <- paste(\"gsutil cp data/*_data.csv\", gcs_data_dir)system(command)command <- paste(\"gsutil ls -l\", gcs_data_dir)system(command, intern = TRUE)\n```Another option for this step is to use the [googleCloudStorageR](https://cran.r-project.org/web/packages/googleCloudStorageR/vignettes/googleCloudStorageR.html) library to do this by using the Cloud Storage JSON API.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this tutorial, you should remove them.\n### Delete the projectThe easiest way to eliminate billing is to delete the project you created for the tutorial.- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Learn more about how you can use BigQuery data in your R notebooks in the [bigrquery documentation](https://bigrquery.r-dbi.org/) .\n- Learn about best practices for ML engineering in [Rules of ML](https://developers.google.com/machine-learning/guides/rules-of-ml/) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Docs"}