{"title": "Docs - Optimize your database", "url": "https://cloud.google.com/architecture/framework/system-design/databases", "abstract": "# Docs - Optimize your database\nLast reviewed 2023-08-28 UTC\nThis document in the [Google Cloud Architecture Framework](/architecture/framework) provides best practices to deploy your system based on database design. You learn how to design, migrate, and scale databases, encrypt database information, manage licensing, and monitor your database for events.\n", "content": "## Key services\nThis document in the Architecture Framework system design category provides best practices that include various Google Cloud database services. The following table provides a high-level overview of these services:\n| Google Cloud service  | Description                                                                                             |\n|:---------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Cloud SQL     | A fully managed database service that lets you set up, maintain, manage, and administer your relational databases that use Cloud SQL for PostgreSQL, Cloud SQL for MySQL, and Cloud SQL for SQL Server. Cloud SQL offers high performance and scalability. Hosted on Google Cloud, Cloud SQL provides a database infrastructure for applications running anywhere.       |\n| Bigtable     | A table that can scale to billions of rows and thousands of columns, letting you store up to petabytes of data. A single value in each row is indexed; this value is known as the row key. Use Bigtable to store very large amounts of single-keyed data with very low latency. It supports high read and write throughput at low latency, and it is a data source for MapReduce operations. |\n| Spanner     | A scalable, globally distributed, enterprise database service built for the cloud that includes relational database structure and non-relational horizontal scale. This combination delivers high-performance transactions and consistency across rows, regions, and continents. Spanner provides a 99.999% availability SLA, no planned downtime, and enterprise-grade security.   |\n| Memorystore    | A fully managed Redis service for Google Cloud. Applications that run on Google Cloud can increase performance by using the highly available, scalable, secure Redis service without managing complex Redis deployments.                                          |\n| Firestore     | A NoSQL document database built for automatic scaling, high performance, and application development. Although the Firestore interface has many of the same features as traditional databases, it is a NoSQL database and it describes relationships between data objects differently.                          |\n| Firebase Realtime Database | A cloud-hosted database. Firebase stores data as JSON and it synchronizes in real time to every connected client. When you build cross-platform apps with Google, iOS, Android, and JavaScript SDKs, all of your clients share one real-time database instance and automatically receive updates with the newest data.                  |\n| Open source databases  | Google partners offer different open source databases, including MongoDB, MariaDB, and Redis.                                                                        |\n| AlloyDB for PostgreSQL  | A fully managed PostgreSQL-compatible database service for demanding enterprise workloads. Provides up to 4x faster performance for transactional workloads and up to 100x faster analytical queries when compared to standard PostgreSQL. AlloyDB for PostgreSQL simplifies management with machine learning-enabled autopilot systems.              |\n## Database selection\nThis section provides best practices for choosing a database to support your system.\n### Consider using a managed database service\nEvaluate Google Cloud [managed database services](/products/databases) before you install your own database or database cluster. Installing your own database involves maintenance overhead including installing patches and updates, and managing daily operational activities like monitoring and performing backups.\nUse functional and non-functional application requirements to drive database selection. Consider low latency access, time series data processing, disaster recovery, and mobile client synchronization.\nTo migrate databases, use one of the products described in the following table:\n| Database migration product | Description                               |\n|:-----------------------------|:------------------------------------------------------------------------------------------------------------------------------------|\n| Cloud SQL     | A regional service that supports read replicas in remote regions, low-latency reads, and disaster recovery.       |\n| Spanner      | A multi-regional offering providing external consistency, global replication, and a five nines service level agreement (SLA).  |\n| Bigtable      | A fully managed, scalable NoSQL database service for large analytical and operational workloads with up to 99.999% availability. |\n| Memorystore     | A fully managed database service that provides a managed version of two popular open source caching solutions: Redis and Memcached. |\n| Firebase Realtime Database | The Firebase Realtime Database is a cloud-hosted NoSQL database that lets you store and sync data between your users in real time. |\n| Firestore     | A NoSQL document database built for automatic scaling, high performance, and ease of application development.      |\n| Open source     | Alternative database options including MongoDB and MariaDB.                   |\n## Database migration\nTo ensure that users experience zero application downtime when you migrate existing workloads to Google Cloud, it's important to choose database technologies that support your requirements. For information about database migration options and best practices, see [Database migration solutions](/solutions/database-migration) and [Best practices for homogeneous database migrations](https://cloud.google.com/blog/products/databases/tips-for-migrating-across-compatible-database-engines) .\nPlanning for a database migration includes the following:\n- Assessment and discovery of the current database.\n- Definitions of migration success criteria.\n- Environment setup for migration and the target database.\n- Creation of the schema in the target database.\n- Migration of the data into the target database.\n- Validation of the migration to verify that all the data is migrated correctly and is present in the database.\n- Creation of rollback strategy.\n### Choose a migration strategy\nSelecting the appropriate target database is one of the keys to a successful migration. The following table provides migration options for some use cases:\n| Use case                    | Recommendation                                |\n|:---------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------|\n| New development in Google Cloud.              | Select one of the managed databases that's built for the cloud\u2014Cloud SQL, Spanner, Bigtable, or Firestore\u2014to meet your use-case requirements. |\n| Lift-and-shift migration.                | Choose a compatible, managed-database service like Cloud SQL, MYSQL, PostgreSQL, or SQLServer.            |\n| Your application requires granular access to a database that CloudSQL doesn't support. | Run your database on Compute Engine VMs.                          |\n### Use Memorystore to support your caching database layer\n[Memorystore](/memorystore/docs) is a fully managed Redis and Memcached database that supports submilliseconds latency. Memorystore is fully compatible with open source [Redis](https://redis.io/) and [Memcached](https://memcached.org/) . If you use these caching databases in your applications, you can use Memorystore without making application-level changes in your code.\n### Use Bare Metal servers to run an Oracle database\nIf your workloads require an Oracle database, use [Bare Metal servers](/bare-metal/docs) provided by Google Cloud. This approach fits into a [lift-and-shift](/architecture/hybrid-multicloud-patterns/adopt#migration_and_modernization) migration strategy.\nIf you want to move your workload to Google Cloud and modernize after your baseline workload is working, consider using managed database options like [Spanner](/spanner/docs) , [Bigtable](/bigtable/docs) , and [Firestore](https://firebase.google.com/docs/firestore) .\nDatabases built for the cloud are modern managed databases which are built from the bottom up on the cloud infrastructure. These databases provide unique default capabilities like scalability and high availability, which are difficult to achieve if you run your own database.\n### Modernize your database\nPlan your database strategy early in the system design process, whether you're designing a new application in the cloud or you're migrating an existing database to the cloud. Google Cloud provides managed database options for open source databases such as [Cloud SQL for MySQL](/sql/docs/mysql/connect-instance-cloud-shell) and [Cloud SQL for PostgreSQL](/sql/docs/postgres) . We recommend that you use the migration as an opportunity to modernize your database and prepare it to support future business needs.\n### Use fixed databases with off-the-shelf applications\n[Commercial off-the-shelf (COTS)](https://wikipedia.org/wiki/Commercial_off-the-shelf) applications require a fixed type of database and fixed configuration. Lift and shift is usually the most appropriate migration approach for COTS applications.\n### Verify your team's database migration skill set\nChoose a cloud database-migration approach based on your team's database migration capabilities and skill sets. Use [Google Cloud Partner Advantage](/partners) to find a partner to support you throughout your migration journey.\n### Design your database to meet HA and DR requirements\nWhen you design your databases to meet high availability (HA) and disaster recovery (DR) requirements, evaluate the tradeoffs between reliability and cost. Database services that are built for the cloud create multiple copies of your data within a region or in multiple regions, depending upon the database and configuration.\nSome Google Cloud services have multi-regional variants, such as [BigQuery](/bigquery/docs) and [Spanner](/spanner/docs) . To be resilient against regional failures, use these multi-regional services in your design where possible.\nIf you design your database on Compute Engine VMs instead of using managed databases on Google Cloud, ensure that you run multiple copies of your databases. For more information, see [Design for scale and high availability](/architecture/framework/reliability/design-scale-high-availability#design_a_multi-region_architecture_for_disaster_recovery) in the Reliability category.\n### Specify cloud regions to support data residency\nData residency describes where your data physically resides at rest. Consider choosing specific cloud regions to deploy your databases based on your data residency requirements.\nIf you deploy your databases in multiple regions, there might be data replication between them depending on how you configure them. Select the configuration that keeps your data within the desired regions at rest. Some databases, like Spanner, offer default multi-regional replication. You can also enforce data residency by setting an [organization policy](/resource-manager/docs/organization-policy/overview) that includes the resource locations [constraints](/resource-manager/docs/organization-policy/understanding-constraints) . For more information, see [Restricting Resource Locations](/resource-manager/docs/organization-policy/defining-locations) .\n### Include disaster recovery in data residency design\nInclude Recovery Time Objective (RTO) and Recovery Point Objective (RPO) in your data residency plans, and consider the trade-off between RTO/RPO and costs of the disaster recovery solution. Smaller RTO/RPO numbers result in higher costs. If you want your system to recover faster from disruptions, your system will cost more to run. Also, factor customer happiness into your disaster recovery approach to make sure that your reliability investments are appropriate. For more information, see [100% reliability is the wrong target](/architecture/framework/reliability/principles#100_reliability_is_the_wrong_target) and [Disaster recovery planning guide](/architecture/dr-scenarios-planning-guide) .\n### Make your database Google Cloud-compliant\nWhen you choose a database for your workload, ensure that the selected service meets compliance for the geographic region that you are operating in and where your data is physically stored. For more information about Google's certifications and compliance standards, see [Compliance offerings](/security/compliance/offerings) .\n## Encryption\nThis section provides best practices for identifying encryption requirements and choosing an encryption key strategy to support your system.\n### Determine encryption requirements\nYour encryption requirements depend on several factors, including company security policies and compliance requirements. All data that is stored in Google Cloud is encrypted at rest by default, without any action required by you, using AES256. For more information, see [Encryption at rest in Google Cloud](/security/encryption/default-encryption) .\n### Choose an encryption key strategy\nDecide if you want to manage encryption keys yourself or if you want to use a managed service. Google Cloud supports both scenarios. If you want a fully managed service to manage your encryption keys on Google Cloud, choose [Cloud Key Management Service (Cloud KMS)](/kms/docs) . If you want to manage your encryption keys to maintain more control over a key's lifecycle, use [Customer-managed encryption keys (CMEK)](/kms/docs/cmek) .\nTo create and manage your encryption keys outside of Google Cloud, choose one of the following options:\n- If you use a partner solution to manage your keys, use [Cloud External Key Manager](/kms/docs/ekm) .\n- If you manage your keys on-premises and if you want to use those keys to encrypt the data on Google Cloud, [import](/kms/docs/key-import) those keys into [Cloud KMS](/kms/docs) either as KMS keys or Hardware Key Module (HSM) keys. Use those keys to encrypt your data on Google Cloud.## Database design and scaling\nThis section provides best practices for designing and scaling a database to support your system.\n### Use monitoring metrics to assess scaling needs\nUse metrics from existing monitoring tools and environments to establish a baseline understanding of database size and scaling requirements\u2014for example, right-sizing and designing scaling strategies for your database instance.\nFor new database designs, determine scaling numbers based on expected load and traffic patterns from the serving application. For more information, see [Monitoring Cloud SQL instances](/sql/docs/mysql/monitor-instance) , [Monitoring with Cloud Monitoring](/spanner/docs/monitoring-cloud) , and [Monitoring an instance](/bigtable/docs/monitoring-instance) .\n## Networking and access\nThis section provides best practices for managing networking and access to support your system.\n### Run databases inside a private network\nRun your databases inside your private network and grant restricted access only from the clients who need to interact with the database. You can [create Cloud SQL instances inside a VPC](/sql/docs/mysql/private-ip) . Google Cloud also provides [VPC Service Controls for Cloud SQL](/sql/docs/mysql/admin-api/configure-service-controls) , [Spanner, and Bigtable databases](/vpc-service-controls/docs/supported-products) to ensure that access to these resources is restricted only to clients within authorized VPC networks.\n### Grant minimum privileges to users\n[Identity and Access Management (IAM)](/iam/docs) controls access to Google Cloud services, including database services. To minimize the risk of unauthorized access, grant the least number of privileges to your users. For application-level access to your databases, use service accounts with the least number of privileges.\n## Automation and right-sizing\nThis section provides best practices for defining automation and right-sizing to support your system.\n### Define database instances as code\nOne of the benefits of migrating to Google Cloud is the ability to automate your infrastructure and other aspects of your workload like compute and database layers. [Google Deployment Manager](/deployment-manager/docs) and third-party tools like [Terraform Cloud](https://www.terraform.io/cloud) let you define your database instances as code, which lets you apply a consistent and repeatable approach to creating and updating your databases.\n### Use Liquibase to version control your database\nGoogle database services like Cloud SQL and [Spanner](/spanner/docs/use-liquibase) support [Liquibase](https://www.liquibase.org/) , an open source version control tool for databases. Liquibase helps you to track your database schema changes, roll back schema changes, and perform repeatable migrations.\n### Test and tune your database to support scaling\nPerform load tests on your database instance and tune it based on the test results to meet your application's requirements. Determine the initial scale of your database by load testing key performance indicators (KPI) or by using monitoring KPIs derived from your current database.\nWhen you create database instances, start with a size that is based on the testing results or historical monitoring metrics. Test your database instances with the expected load in the cloud. Then fine-tune the instances until you get the desired results for the expected load on your database instances.\n### Choose the right database for your scaling requirements\nScaling databases is different from scaling compute layer components. Databases have state; when one instance of your database isn't able to handle the load, consider the appropriate strategy to scale your database instances. Scaling strategies vary depending on the database type.\nUse the following table to learn about Google products that address scaling use cases.\n| Use case                                                                     | Recommended product | Description                              |\n|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------|:----------------------------------------------------------------------------------------------------------------------------------|\n| Horizontally scale your database instance by adding nodes to your database when you need to scale up the serving capacity and storage.                                     | Spanner    | A relational database that's built for the cloud.                     |\n| Add nodes to scale your database.                                                               | Bigtable    | Fully managed NoSQL big data database service.                     |\n| Automatically handle database scaling.                                                             | Firestore    | Flexible, scalable database for mobile, web, and server development.                |\n| To serve more queries, vertically scale up Cloud SQL database instances to give them more compute and memory capacity. In Cloud SQL, the storage layer is decoupled from the database instance. You can choose to scale your storage layer automatically whenever it approaches capacity. | Cloud SQL    | Fully managed database service that helps you set up, maintain, manage, and administer your relational databases on Google Cloud. |\n## Operations\nThis section provides best practices for operations to support your system.\n### Use Cloud Monitoring to monitor and set up alerts for your database\nUse [Cloud Monitoring](/monitoring/docs) to monitor your database instances and set up alerts to notify appropriate teams of events. For information about efficient alerting best practices, see [Build efficient alerts](/architecture/framework/reliability/build-efficient-alerts) .\nAll databases that are built for the cloud provide logging and monitoring metrics. Each service provides a dashboard to visualize logging and monitoring metrics. The monitoring metrics for all services integrate with [Google Cloud Observability](/stackdriver/docs) . [Spanner](/spanner/docs) provides query introspection tools like the [Key Visualizer](/spanner/docs/key-visualizer) for debugging and root cause analysis. The Key Visualizer provides the following capabilities:\n- Helps you analyze Spanner usage patterns by generating visual reports for your databases. The reports display usage patterns by ranges of rows over time.\n- Provides insights into usage patterns at scale.\n[Bigtable](/bigtable/docs) also provides a [Key Visualizer](/bigtable/docs/keyvis-overview) diagnostic tool that helps you to analyze Bigtable instance usage patterns.\n## Licensing\nThis section provides best practices for licensing to support your system.\n### Choose between on-demand licenses and existing licenses\nIf you use [Cloud SQL for SQL Server](/sql/docs/sqlserver/introduction) , [bringing your own licenses](/compute/docs/nodes/bringing-your-own-licenses) isn't supported; your licensing costs are based on per-core hour usage.\nIf you want to use existing [Cloud SQL for SQL Server](/sql/docs/sqlserver) licenses, consider running Cloud SQL for SQL Server on Compute VMs. For more information, see [Microsoft licenses](/compute/docs/instances/windows/ms-licensing) and [Choosing between on-demand licenses and bringing existing licenses](/compute/docs/instances/windows/ms-licensing#flowchart) .\nIf you use Oracle and if you're migrating to the [Bare Metal Solution for Oracle](/bare-metal) , you can bring your own licenses. For more information, see [Plan for Bare Metal Solution](/bare-metal/docs/bms-planning) .\n## Migration timelines, methodology, and toolsets\nThis section provides best practices for planning and supporting your database migration to support your system.\n### Determine database modernization readiness\nAssess whether your organization is ready to modernize your databases and use databases that are built for the cloud.\nConsider database modernization when you plan workload migration timelines, because modernization is likely to impact your application side.\n### Involve relevant stakeholders in migration planning\nTo migrate a database, you complete the following tasks:\n- Set up the target databases.\n- Convert the schema.\n- Set up data replication between the source and target database.\n- Debug issues as they arise during the migration.\n- Establish network connectivity between the application layer and the database.\n- Implement target database security.\n- Ensure that the applications connect to the target databases.\nThese tasks often require different skill sets and multiple teams collaborate across your organization to complete the migration. When you plan the migration, include stakeholders from all teams, such as app developers, database administrators, and infrastructure and security teams.\nIf your team lacks skills to support this type of migration, Google's partners can help you perform your migrations. For more information, see [Google Cloud Partner Advantage](/partners) .\n### Identify tool sets for homogeneous and heterogeneous migrations\nA is a database migration between the source and target databases of the same database technology. A is a migration whose target database is different from the source database.\nHeterogeneous migrations usually involve additional steps of schema conversion from the source database to the target database engine type. Your database teams need to assess the challenges involved in the schema conversion, because they depend on the complexity of the source database schema.\n### Test and validate each step in data migration\nData migrations involve multiple steps. To minimize migration errors, test and validate each step in the migration before moving to the next step. The following factors drive the migration process:\n- Whether the migration is homogeneous or heterogeneous.\n- What type of tools and skill sets you have to perform the migration.\n- For heterogeneous migrations, your experience with the target database engine.\n### Determine continuous data replication requirements\nCreate a plan to migrate the data initially and then continuously replicate the data from the source to the target database. Continue replication until the target is stabilized and the application is completely migrated to the new database. This plan helps you to identify potential downtime during the database switch and plan accordingly.\nIf you plan to migrate database engines from [Cloud SQL](/sql/docs) , [Cloud SQL for MySQL](/sql/docs/mysql) , or [Cloud SQL for PostgreSQL](/sql/docs/postgres) , use [Database Migration Service](/database-migration) to automate this process in a fully managed way. For information about third-party tools that support other types of migrations, see [Cloud Marketplace](/marketplace) .\n## Recommendations\nTo apply the guidance in the Architecture Framework to your own environment, we recommend that you do the following:\n- Multi-tenancy for databases involves storing data from multiple customers on a shared piece of infrastructure, in this case a database. If you offer a software-as-a-service (SaaS) based offering to your customers, make sure that you understand how you can logically isolate datasets that belong to different customers, and support their access requirements. Also, evaluate your requirements based on levels of separation.For relational databases such as [Spanner](/spanner/docs) and [Cloud SQL](/sql/docs) , there are multiple approaches, such as isolating tenants' data at the database-instance level, database level, schema level, or the database-table level. Like other design decisions, there is a tradeoff between the degree of isolation and other factors such as cost and performance. [IAM](/iam/docs) policies control access to your database instances.\n- Choose the right database for your data model requirements.\n- Choose key values to avoid key hotspotting. A is a location within a table that receives many more access requests than other locations. For more information about hotspots, see [Schema design best practices](/spanner/docs/schema-design) .\n- Shard your database instance whenever possible.\n- Use connection-management best practices, such as connection pooling and exponential backoff.\n- Avoid very large transactions.\n- Design and test your application's response to maintenance updates on databases.\n- Secure and isolate connections to your database.\n- Size your database and growth expectations to ensure that the database supports your requirements.\n- Test your HA and DR failover strategies.\n- Perform backups and restore as well as exports and imports so that you're familiar with the process.\n### Cloud SQL recommendations\n- Use private IP address networking (VPC). For additional security, consider the following:- Use [Cloud SQL Auth proxy](/sql/docs/mysql/sql-proxy) to support private networking.\n- Restrict public IP address access [constraints/sql.restrictPublicIp](/sql/docs/mysql/connection-org-policy#connection-constraints) .\n- If you need public IP address networking, consider the following:- Use the built-in firewall with a limited or narrow IP address list and ensure that Cloud SQL instances require that incoming connections use SSL. For more information, see [Configuring SSL/TLS certificates](/sql/docs/mysql/configure-ssl-instance) .\n- For additional security, consider the following:- Don't grant general access; instead, use Cloud SQL Auth proxy.\n- Restrict authorized networks [constraints/sql.restrictAuthorizedNetworks](/sql/docs/mysql/connection-org-policy#connection-constraints) .\n- Use limited privileges for database users.## What's next\nLearn [data analytics best practices](/architecture/framework/system-design/data-analytics) , including the following:\n- Learn [core data analytics principles](/architecture/framework/system-design/data-analytics#core) and [key Google Cloud services](/architecture/framework/system-design/data-analytics#key) .\n- Learn about the [data lifecycle](/architecture/framework/system-design/data-analytics#lifecycle) .\n- Learn how to [ingest data](/architecture/framework/system-design/data-analytics#ingest) .\n- Choose and manage [data storage](/architecture/framework/system-design/data-analytics#data-storage) .\n- [Process and transform data](/architecture/framework/system-design/data-analytics#transform) .\nExplore other categories in the [Architecture Framework](/architecture/framework) such as reliability, operational excellence, and security, privacy, and compliance.", "guide": "Docs"}