{"title": "Docs - Building an ML vision analytics solution with Dataflow and Cloud Vision API", "url": "https://cloud.google.com/architecture/building-a-vision-analytics-solution", "abstract": "# Docs - Building an ML vision analytics solution with Dataflow and Cloud Vision API\nLast reviewed 2021-02-10 UTC\nIn this tutorial, you'll learn how to deploy a [Dataflow](/dataflow) pipeline to process large-scale image files with [Cloud Vision](/vision/docs) . Dataflow stores the results in BigQuery so that you can use them to train [BigQuery ML pre-built models](/bigquery-ml/docs/introduction) .\nThe Dataflow pipeline you create in the tutorial can handle images in large quantities. It's only limited by your Vision quota. You can increase your Vision quota based on your scale requirements.\nThe tutorial is intended for data engineers and data scientists. It assumes you have basic knowledge of building Dataflow pipelines using [ Apache Beam's Java SDK](https://beam.apache.org/documentation/sdks/java/) , BigQuery Standard SQL, and basic shell scripting. It also assumes that you are familiar with Vision.", "content": "## Objectives\n- Create an image metadata ingestion pipeline with Pub/Sub notifications for Cloud Storage.\n- Use Dataflow to deploy a real-time vision analytics pipeline.\n- Use Vision to analyze images for a set of feature types.\n- Analyze and train data with BigQuery ML.\n## Costs\nIn this document, you use the following billable components of Google Cloud:- [BigQuery](/bigquery/pricing) \n- [Cloud Storage](/storage/pricing) \n- [Dataflow](/dataflow/pricing) \n- [Cloud Vision API](/vision/pricing) \n- [Pub/Sub](/pubsub/pricing) To generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- In the Google Cloud console, on the project selector page,   select or [create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . **Note** : If you don't plan to keep the  resources that you create in this procedure, create a project instead of  selecting an existing project. After you finish these steps, you can  delete the project, removing all resources associated with the project. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- In Cloud Shell, enable the Dataflow, Container Registry, and Vision APIs.```\ngcloud services enable dataflow.googleapis.com \\containerregistry.googleapis.com vision.googleapis.com\n```\n- Set some environment variables. (Replace with one of the [available Dataflow regions](/dataflow/docs/concepts/regional-endpoints) . `us-central1` , for example).```\nexport PROJECT=$(gcloud config get-value project)export REGION=REGION\n```\n- Clone the tutorial's Git repository:```\ngit clone https://github.com/GoogleCloudPlatform/dataflow-vision-analytics.git\n```\n- Go to the repository's root folder:```\ncd dataflow-vision-analytics\n```\n## Reference architectureThe following diagram illustrates the flow of the system that you build in this tutorial.\n \nAs shown in the diagram, the flow is as follows:- Clients upload image files to a Cloud Storage bucket.\n- For each file upload, the system automatically notifies the client by publishing a message to Pub/Sub.\n- For each new notification, the Dataflow pipeline does the following:- Reads file metadata from the Pub/Sub message.\n- Sends each segment to Vision API for annotation processing.\n- Stores all annotations in a BigQuery table for further analysis.## Creating a Pub/Sub notification for Cloud StorageIn this section, you create a [Pub/Sub notification for Cloud Storage](/storage/docs/pubsub-notifications) . This notification publishes metadata for the image file that is uploaded in the bucket. Based on the metadata, the Dataflow pipeline starts processing the request.- In Cloud Shell, create a Pub/Sub topic:```\nexport GCS_NOTIFICATION_TOPIC=\"gcs-notification-topic\"gcloud pubsub topics create ${GCS_NOTIFICATION_TOPIC}\n```\n- Create a Pub/Sub subscription for the topic:```\nexport \u00a0GCS_NOTIFICATION_SUBSCRIPTION=\"gcs-notification-subscription\"gcloud pubsub subscriptions create \u00a0${GCS_NOTIFICATION_SUBSCRIPTION} \u00a0--topic=${GCS_NOTIFICATION_TOPIC}\n```\n- Create a bucket to store the input image files:```\nexport IMAGE_BUCKET=${PROJECT}-imagesgsutil mb -c standard -l ${REGION} gs://${IMAGE_BUCKET}\n```\n- Create a Pub/Sub notification for the bucket:```\ngsutil notification create -t ${GCS_NOTIFICATION_TOPIC} \\\u00a0 -f json gs://${IMAGE_BUCKET}\n```\nNow that you have configured notifications, the system sends a Pub/Sub message to the topic that you created. This action occurs every time you upload a file to the bucket.## Creating a BigQuery datasetIn this section, you create a [BigQuery](/bigquery/docs) dataset to store the results output by the Dataflow pipeline. The pipeline automatically creates tables based on [vision feature types](/vision/docs/features-list) .- In Cloud Shell, create a BigQuery dataset:```\nexport BIGQUERY_DATASET=\"vision_analytics\"bq mk -d --location=US ${BIGQUERY_DATASET}\n```\n## Creating a Dataflow Flex TemplateIn this section, you create Apache Beam pipeline code and then run the Dataflow pipeline as a Dataflow job using a [Dataflow Flex Template](/dataflow/docs/guides/templates/using-flex-templates) .- In Cloud Shell, build the Apache Beam pipeline's code:```\ngradle build\n```\n- Create a Docker image for the Dataflow Flex Template:```\ngcloud auth configure-dockergradle jib \\\u00a0 --image=gcr.io/${PROJECT}/dataflow-vision-analytics:latest\n```\n- Create a Cloud Storage bucket to store the Dataflow Flex Template:```\nexport DATAFLOW_TEMPLATE_BUCKET=${PROJECT}-dataflow-template-configgsutil mb -c standard -l ${REGION} \\\u00a0 gs://${DATAFLOW_TEMPLATE_BUCKET}\n```\n- Upload the template's JSON configuration file to the bucket:```\ncat << EOF | gsutil cp - gs://${DATAFLOW_TEMPLATE_BUCKET}/dynamic_template_vision_analytics.json{\u00a0 \"image\": \"gcr.io/${PROJECT}/dataflow-vision-analytics:latest\",\u00a0 \"sdk_info\": {\"language\": \"JAVA\"}}EOF\n```\n## Running the Dataflow pipeline for a set of Vision featuresThe parameters listed in the following table are specific to this Dataflow pipeline.\nRefer to [the Dataflow documentation](/dataflow/docs/guides/specifying-exec-params) for the complete list of standard Dataflow execution parameters.\n| Parameter                                    | Description                                      |\n|:-------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| windowInterval                                   | The window time interval (in seconds) for outputting results to BigQuery and Pub/Sub. The default is 5.               |\n| batchSize                                    | The number of images to include in a request to the Vision API. The default is 1. You can increase it to a maximum of 16.           |\n| subscriberId                                   | The ID of the Pub/Sub subscription that receives input Cloud Storage notifications.                    |\n| keyRange                                    | The parameter that enables you to improve processing performance for large data sets. A higher value means increased parallelism among workers. The default is 1. |\n| visionApiProjectId                                  | The project ID to use for the Vision API.                               |\n| datasetName                                   | The reference of the output BigQuery dataset.                              |\n| features                                    | A list of image-processing features.                                |\n| labelAnnottationTable, landmarkAnnotationTable, logoAnnotationTable, faceAnnotationTable, imagePropertiesTable, cropHintAnnotationTable, errorLogTable | String parameters with table names for various annotations. The default values are provided for each table.              |- In Cloud Shell, define a job name for the Dataflow pipeline:```\nexport JOB_NAME=vision-analytics-pipeline-1\n```\n- Create a file with parameters for the Dataflow pipeline:```\nPARAMETERS=params.yamlcat << EOF > ${PARAMETERS}--parameters:\u00a0 autoscalingAlgorithm: THROUGHPUT_BASED\u00a0 enableStreamingEngine: \"true\"\u00a0 subscriberId: projects/${PROJECT}/subscriptions/${GCS_NOTIFICATION_SUBSCRIPTION}\u00a0 visionApiProjectId: ${PROJECT}\u00a0 features: IMAGE_PROPERTIES,LABEL_DETECTION,LANDMARK_DETECTION,LOGO_DETECTION,CROP_HINTS,FACE_DETECTION\u00a0 datasetName: ${BIGQUERY_DATASET}EOF\n```\n- Run the Dataflow pipeline to process images for these feature types: `IMAGE_PROPERTIES, LABEL_DETECTION, LANDMARK_DETECTION, LOGO_DETECTION, CROP_HINTS,FACE_DETECTION` .```\ngcloud dataflow flex-template run ${JOB_NAME} \\--project=${PROJECT} \\--region=${REGION} \\--template-file-gcs-location=gs://${DATAFLOW_TEMPLATE_BUCKET}/dynamic_template_vision_analytics.json \\--flags-file ${PARAMETERS}\n```This command uses the parameters listed in the preceding table.\n- Retrieve the ID of the running Dataflow job:```\nJOB_ID=$(gcloud dataflow jobs list --filter \"name:${JOB_NAME}\" --format \"value(id)\" --status active)\n```\n- Display the URL of the Dataflow job's web page:```\necho \"https://console.cloud.google.com/dataflow/jobs/${REGION}/${JOB_ID}\"\n```\n- Open the displayed URL in a new browser tab. After a few seconds, the graph for the Dataflow job appears:The Dataflow pipeline is now running and waiting to receive input notifications from Pub/Sub.\n- In Cloud Shell, trigger the Dataflow pipeline by uploading some test files into the input bucket:```\ngsutil cp gs://df-vision-ai-test-data/bali.jpeg gs://${IMAGE_BUCKET}gsutil cp gs://df-vision-ai-test-data/faces.jpeg gs://${IMAGE_BUCKET}gsutil cp gs://df-vision-ai-test-data/bubble.jpeg gs://${IMAGE_BUCKET}gsutil cp gs://df-vision-ai-test-data/setagaya.jpeg gs://${IMAGE_BUCKET}gsutil cp gs://df-vision-ai-test-data/st_basils.jpeg gs://${IMAGE_BUCKET}\n```\n- In the Google Cloud console, review the custom counters in the Dataflow (in the right panel of the Dataflow job) and verify that it processed all five images: \n- In Cloud Shell, validate that the tables were automatically created:```\nbq query \"select table_name, table_type from \\${BIGQUERY_DATASET}.INFORMATION_SCHEMA.TABLES\"\n```The output is as follows:```\n+----------------------+------------+\n|  table_name  | table_type |\n+----------------------+------------+\n| face_annotation  | BASE TABLE |\n| label_annotation  | BASE TABLE |\n| crop_hint_annotation | BASE TABLE |\n| landmark_annotation | BASE TABLE |\n| image_properties  | BASE TABLE |\n+----------------------+------------+\n```\n- View the schema for the `landmark_annotation` table. If requested, the `LANDMARK_DETECTION` feature captures [the attributes returned from the API call](/vision/docs/reference/rest/v1/AnnotateImageResponse#EntityAnnotation) .```\nbq show --schema --format=prettyjson ${BIGQUERY_DATASET}.landmark_annotation\n```The output is as follows:```\n[ {\n \"mode\": \"REQUIRED\",\n \"name\": \"gcs_uri\",\n \"type\": \"STRING\"\n },\n {\n \"mode\": \"NULLABLE\",\n \"name\": \"mid\",\n \"type\": \"STRING\"\n },\n {\n \"mode\": \"REQUIRED\",\n \"name\": \"description\",\n \"type\": \"STRING\"\n },\n {\n \"mode\": \"REQUIRED\",\n \"name\": \"score\",\n \"type\": \"FLOAT\"\n },\n {\n \"fields\": [  {\n  \"fields\": [   {\n   \"mode\": \"REQUIRED\",\n   \"name\": \"x\",\n   \"type\": \"FLOAT\"\n   },\n   {\n   \"mode\": \"REQUIRED\",\n   \"name\": \"y\",\n   \"type\": \"FLOAT\"\n   }\n  ],\n  \"mode\": \"REPEATED\",\n  \"name\": \"vertices\",\n  \"type\": \"RECORD\"\n  }\n ],\n \"mode\": \"NULLABLE\",\n \"name\": \"bounding_poly\",\n \"type\": \"RECORD\"\n },\n {\n \"mode\": \"REPEATED\",\n \"name\": \"locations\",\n \"type\": \"GEOGRAPHY\"\n },\n {\n \"mode\": \"REQUIRED\",\n \"name\": \"transaction_timestamp\",\n \"type\": \"TIMESTAMP\"\n }\n]\n```\n- Stop the pipeline:```\ngcloud dataflow jobs drain ${JOB_ID} \\--region ${REGION}\n```Although there are no more Pub/Sub notifications to process, the streaming pipeline you created continues to run until you enter this command.\n## Analyzing a Flickr30K datasetIn this section, you analyze a [flickr30K dataset](https://www.kaggle.com/hsankesara/flickr-image-dataset) for label and landmark detection.- In Cloud Shell, define a new job name:```\nexport JOB_NAME=vision-analytics-pipeline-2\n```\n- Change the Dataflow pipeline parameters so that it's optimized for a large dataset. The `batchSize` and `keyRange` are increased to allow higher throughput. Dataflow will scale the number of workers as needed:```\ncat <<EOF > ${PARAMETERS}--parameters:\u00a0 autoscalingAlgorithm: THROUGHPUT_BASED\u00a0 enableStreamingEngine: \"true\"\u00a0 subscriberId: projects/${PROJECT}/subscriptions/${GCS_NOTIFICATION_SUBSCRIPTION}\u00a0 visionApiProjectId: ${PROJECT}\u00a0 features: LABEL_DETECTION,LANDMARK_DETECTION\u00a0 datasetName: ${BIGQUERY_DATASET}\u00a0 batchSize: \"16\"\u00a0 windowInterval: \"5\"\u00a0 keyRange: \"2\"EOF\n```\n- Run the pipeline:```\ngcloud dataflow flex-template run ${JOB_NAME} \\--project=${PROJECT} \\--region=${REGION} \\--template-file-gcs-location=gs://${DATAFLOW_TEMPLATE_BUCKET}/dynamic_template_vision_analytics.json \\--flags-file ${PARAMETERS}\n```\n- Upload the dataset into an input bucket:```\ngsutil -m \u00a0cp gs://df-vision-ai-test-data/* \u00a0gs://${IMAGE_BUCKET}\n```\n- Retrieve the ID of the running Dataflow job:```\nJOB_ID=$(gcloud dataflow jobs list --filter \"name:${JOB_NAME}\" --region ${REGION} --format \"value(id)\" --status active)\n```\n- Display the URL of the Dataflow job's web page:```\necho \"https://console.cloud.google.com/dataflow/jobs/${REGION}/${JOB_ID}\"\n```\n- Open the displayed URL in a new browser tab.\n- In the Google Cloud console, validate the custom counters in the Dataflow to ensure that all files are processed. All the files normally process in less than 30 minutes.\n- Filter by custom counters under **Process Annotations** .The output is as follows: The `processedFiles` metric (31,935) matches the total number of images that were uploaded in the bucket (total file count is 31,936). However, the `numberOfRequests` metric (1,997) is lower than the number of files that went through the pipeline. This difference is because the pipeline batches up to 16 files per request, as shown in the values of the `batchSizeDistribution_*` metrics.\n- Shut down the pipeline:```\nJOB_ID=$(gcloud dataflow jobs list --filter \"name:${JOB_NAME}\"--region ${REGION}--format \"value(id)\"--status active) \\gcloud dataflow jobs drain ${JOB_ID} \\--region ${REGION}\n```\n- In the Google Cloud console, go to the BigQuery **Query editor** page. [Go to Query editor](https://console.cloud.google.com/bigquery) \n- Find the most likely label for each file:```\nSELECT\u00a0 SPLIT(gcs_uri,'/')[OFFSET(3)] file,\u00a0 description,\u00a0 scoreFROM (\u00a0 SELECT\u00a0 \u00a0 gcs_uri,\u00a0 \u00a0 description,\u00a0 \u00a0 score,\u00a0 \u00a0 ROW_NUMBER() OVER (PARTITION BY gcs_uri ORDER BY score DESC )AS row_num\u00a0 FROM\u00a0 \u00a0 \u00a0`vision_analytics.label_annotation`)WHERE\u00a0 row_num = 1ORDER BY\u00a0 gcs_uri DESC\n```The output is as follows. You can see from the response that is the most likely description for the `st_basils.jpeg` file. \n- Find the top 10 labels and their max scores:```\nSELECT\u00a0 description,\u00a0 COUNT(*) AS found,\u00a0 MAX(score) AS max_scoreFROM\u00a0 `vision_analytics.label_annotation`GROUP BY\u00a0 descriptionORDER BY\u00a0 found DESCLIMIT 10\n```The final output looks similar to the following: \n- Find the top 10 popular landmarks:```\nSELECT\u00a0 description,\u00a0 COUNT(*) AS count,\u00a0 MAX(score) AS max_scoreFROM\u00a0 `vision_analytics.landmark_annotation`WHERE\u00a0 LENGTH(description)>0GROUP BY\u00a0 descriptionORDER BY\u00a0 count DESCLIMIT 10\n```The output is as follows. You can see from the response that Times Square appears to be the most popular destination. \n- Find any image that has a waterfall:```\nSELECT\u00a0 SPLIT(gcs_uri,'/')[OFFSET(3)] file,\u00a0 description,\u00a0 scoreFROM\u00a0 `vision_analytics.landmark_annotation`WHERE\u00a0 LOWER(description) LIKE '%fall%'ORDER BY score DESC\n```The output is as follows. It only contains images of waterfalls.\n- Find an image of a landmark within 3 kilometers of the Colosseum in Rome (the `ST_GEOPOINT` function uses the Colosseum's longitude and latitude):```\nWITH\u00a0 landmarksWithDistances AS (\u00a0 SELECT\u00a0 \u00a0 gcs_uri,\u00a0 \u00a0 description,\u00a0 \u00a0 location,\u00a0 \u00a0 ST_DISTANCE(location,\u00a0 \u00a0 \u00a0 ST_GEOGPOINT(12.492231,\u00a0 \u00a0 \u00a0 \u00a0 41.890222)) distance_in_meters,\u00a0 FROM\u00a0 \u00a0 `vision_analytics.landmark_annotation` landmarks\u00a0 CROSS JOIN\u00a0 \u00a0 UNNEST(landmarks.locations) AS location )SELECT\u00a0 SPLIT(gcs_uri,\"/\")[OFFSET(3)] file,\u00a0 description,\u00a0 \u00a0 ROUND(distance_in_meters) distance_in_meters,\u00a0 location,\u00a0 CONCAT(\"https://storage.cloud.google.com/\", SUBSTR(gcs_uri, 6)) AS image_urlFROM\u00a0 landmarksWithDistancesWHERE\u00a0 distance_in_meters < 3000ORDER BY\u00a0 distance_in_metersLIMIT\u00a0 100\n```The output is as follows. You can see that several popular destinations appear in these images:The same image can contain multiple locations of the same landmark. This functionality is described in [the Vision API documentation](/vision/docs/reference/rest/v1/AnnotateImageResponse?hl=pl#EntityAnnotation) . Because one location can indicate the location of the scene in the image, multiple [LocationInfo](/vision/docs/reference/rest/v1/AnnotateImageResponse?hl=pl#locationinfo) elements can be present. Another location can indicate where the image was taken. Location information is usually present for landmarks.You can visualize the data in [BigQuery Geo Viz](https://bigquerygeoviz.appspot.com/) by pasting in the previous query. When you select a point on the map, you see its details. The `Image_url` attribute contains the link to the image file that you can open in a browser.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the Google Cloud projectThe easiest way to eliminate billing is to delete the Google Cloud project you created for the tutorial.- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Learn more about [smart analytics reference patterns](/solutions/smart-analytics/reference-patterns/overview) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Docs"}