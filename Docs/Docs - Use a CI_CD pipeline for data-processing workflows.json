{"title": "Docs - Use a CI/CD pipeline for data-processing workflows", "url": "https://cloud.google.com/architecture/cicd-pipeline-for-data-processing", "abstract": "# Docs - Use a CI/CD pipeline for data-processing workflows\nLast reviewed 2023-05-12 UTC\nThis document describes how to set up a continuous integration/continuous deployment (CI/CD) pipeline for processing data by implementing CI/CD methods with managed products on Google Cloud. Data scientists and analysts can adapt the methodologies from CI/CD practices to help to ensure high quality, maintainability, and adaptability of the data processes and workflows. The methods that you can apply are as follows:\n- Version control of source code.\n- Automatic building, testing, and deployment of apps.\n- Environment isolation and separation from production.\n- Replicable procedures for environment setup.\nThis document is intended for data scientists and analysts who build recurrent running data-processing jobs to help structure their research and development (R&D) to systematically and automatically maintain data-processing workloads.\n", "content": "## Architecture\nThe following diagram shows a detailed view of the CI/CD pipeline steps.\nThe deployments to the test and production environments are separated into two different Cloud Build pipelines\u2013a test and a production pipeline.\nIn the preceding diagram, the test pipeline consists of the following steps:\n- A developer commits code changes to the Cloud Source Repositories.\n- Code changes trigger a test build in Cloud Build.\n- Cloud Build builds the self-executing JAR file and deploys it to the test JAR bucket on Cloud Storage.\n- Cloud Build deploys the test files to the test-file buckets on Cloud Storage.\n- Cloud Build sets the variable in Cloud Composer to reference the newly deployed JAR file.\n- Cloud Build tests the data-processing workflow [Directed Acyclic Graph](https://airflow.apache.org/concepts.html) (DAG) and deploys it to the Cloud Composer bucket on Cloud Storage.\n- The workflow DAG file is deployed to Cloud Composer.\n- Cloud Build triggers the newly deployed data-processing workflow to run.\n- When the data-processing workflow integration test has passed, a message is published to Pub/Sub which contains a reference to the latest self-executing JAR (obtained from the Airflow variables) in the message\u2019s data field.\nIn the preceding diagram, the production pipeline consists of the following steps:\n- The production deployment pipeline is triggered when a message is published to a Pub/Sub topic.\n- A developer manually approves the production deployment pipeline and the build is run.\n- Cloud Build copies the latest self-executing JAR file from the test JAR bucket to the production JAR bucket on Cloud Storage.\n- Cloud Build tests the production data-processing workflow DAG and deploys it to the Cloud Composer bucket on Cloud Storage.\n- The production workflow DAG file is deployed to Cloud Composer.\nIn this reference architecture document, the production data-processing workflow is deployed to the same Cloud Composer environment as the test workflow, to give a consolidated view of all data-processing workflows. For the purposes of this reference architecture, the environments are separated by using different Cloud Storage buckets to hold the input and output data.\nTo completely separate the environments, you need multiple Cloud Composer environments created in different projects, which are by default separated from each other. This separation helps to secure your production environment. This approach is outside the scope of this tutorial. For more information about how to access resources across multiple Google Cloud projects, see [Setting service account permissions](/build/docs/securing-builds/set-service-account-permissions) .\n### The data-processing workflow\nThe instructions for how Cloud Composer runs the data-processing workflow are defined in a [Directed Acyclic Graph](https://airflow.apache.org/concepts.html) (DAG) written in Python. In the DAG, all the steps of the data-processing workflow are defined together with the dependencies between them.\nThe CI/CD pipeline automatically deploys the DAG definition from Cloud Source Repositories to Cloud Composer in each build. This process ensures that Cloud Composer is always up to date with the latest workflow definition without needing any human intervention.\nIn the DAG definition for the test environment, an end-to-end test step is defined in addition to the data-processing workflow. The test step helps make sure that the data-processing workflow runs correctly.\nThe data-processing workflow is illustrated in the following diagram.\nThe data-processing workflow consists of the following steps:\n- Run the WordCount data process in Dataflow.\n- Download the output files from the WordCount process. The WordCount process outputs three files:- `download_result_1`\n- `download_result_2`\n- `download_result_3`\n- Download the reference file, called `download_ref_string` .\n- Verify the result against the reference file. This integration test aggregates all three results and compares the aggregated results with the reference file.\n- Publish a message to Pub/Sub after the integration test has passed.\nUsing a task-orchestration framework such as Cloud Composer to manage the data-processing workflow helps alleviate the code complexity of the workflow.\n## Cost optimization\nIn this document, you use the following billable components of Google Cloud:\n- [Cloud Source Repositories](/source-repositories/pricing) \n- [Cloud Build](/build/pricing) \n- [Cloud Composer](/composer/pricing) \n- [Dataflow](/dataflow/pricing) \n- [Cloud Storage](/storage/pricing) ## Deployment\nTo deploy this architecture, see [Deploy a CI/CD pipeline for data-processing workflows](/architecture/cicd-pipeline-for-data-processing/deployment) .\n## What's next\n- Learn more about [GitOps-style continuous delivery with Cloud Build](/kubernetes-engine/docs/tutorials/gitops-cloud-build) .\n- Learn more about [Common Dataflow use-case patterns](/blog/products/gcp/guide-to-common-cloud-dataflow-use-case-patterns-part-1) .\n- Learn more about [Release Engineering](https://landing.google.com/sre/sre-book/chapters/release-engineering/) .\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Docs"}