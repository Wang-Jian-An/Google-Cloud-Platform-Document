{"title": "Docs - Performing a PITR of a PostgreSQL database on Compute Engine", "url": "https://cloud.google.com/architecture/performing-pitr-postgresql-database-compute-engine", "abstract": "# Docs - Performing a PITR of a PostgreSQL database on Compute Engine\nThis tutorial shows how to set up the archiving process, and then perform a\n [point-in-time recovery (PITR)](https://wikipedia.org/wiki/Point-in-time_recovery) \nof a PostgreSQL database running on\n [Compute Engine](/compute/docs) \n.\nIn this tutorial, you create a demonstration database and run an application workload. Then, you configure the archive and backup processes. Next, you learn how to verify the backup, archive, and recovery processes. Finally, you learn how to recover the database to a specific point in time.\nThis tutorial is intended for database administrators, system operators, DevOps professionals, and cloud architects interested in configuring a backup and recovery strategy for PostgreSQL databases.\nThis tutorial assumes that you are familiar with Docker containers and that you are comfortable with Linux commands, PostgreSQL database engines, and Compute Engine.", "content": "## Objectives\n- Set up a backup and archiving process.\n- Perform a PITR.\n- Monitor your backup.\n- Verify a recovery.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Compute Engine](/compute/pricing) \n- [Cloud Storage](/storage/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) At the bottom of the Google Cloud console, a [Cloud Shell](/shell/docs/how-cloud-shell-works) session starts and displays a command-line prompt. Cloud Shell is a shell environment  with the Google Cloud CLI  already installed and with values already set for  your current project. It can take a few seconds for the session to initialize.\n## ConceptsBefore you start the tutorial, review the following PostgreSQL concepts:- **Continuous archiving.** When the database continuously saves sequential transactions to a file.\n- **Write Ahead Log (WAL).** Changes to data files are recorded in the WAL before they are made to the file.\n- **WAL Record.** Each transaction applied to the database is formatted and stored as a WAL record.\n- **Segment files.** Segment files have monotonically increasing filenames, and contain as many WAL records as possible. The file size is configurable, with a default of 16\u00a0MiB. You might choose a larger size if you expect voluminous transactions in size, or count, to lower the aggregate number of generated segment files and to decrease the file management burden.\nFor more information, see [Reliability and the Write-Ahead Log](https://www.postgresql.org/docs/current/wal.html) .\nThe following diagram shows how WALs are persisted in two stages.In the preceding diagram, the first stage of persisting WALs consists of the database engine recording write transactions in the WAL buffer concurrently with the write to a table. When the transaction is committed, the WAL buffer is written (flushed) to disk during the second stage with an append to the WAL segment file.\n **Note:** This tutorial is written for the latest version of Postgres at the time of writing: version 11. In Postgres versions 10 and higher, WALs are created under the `pg_wal` directory. Before version 10, the files were created in the `pg_xlog` directory.## Choosing a PITRA PITR is appropriate for the following scenarios:- **Minimize the recovery point objective (RPO).** The RPO is the maximum time of data loss that is tolerated before it significantly impacts business processes. Saving all transactions in the WALs between backup snapshots drastically decreases the amount of lost data because you have the transactions since the last full backup to apply to the database.\n- **Minimize the recovery time objective (RTO).** The RTO is the amount of time required to recover a database if a destructive event occurs. After you set up binary backups and log archiving, the time required to recover the database can be minimal.\n- **Remediation for a data corruption bug, or an administrative misstep.** If a code release causes catastrophic data corruption, or an unrecoverable mistake is made during routine maintenance, you can recover to before that moment.\nIn some application architectures, such as a microservices architecture, there might be parallel databases that could require independent recoveries. For example, a retail application might have customer data in one database and retail order details and inventory information in other databases. Depending on the overall state of data, one, two, or all databases might need to be recovered in parallel.\nA PITR is not appropriate in the following scenarios:- **RPO is large.** If your disaster recovery policy can tolerate the loss of any transactions received after the recent snapshot, you can avoid additional steps and focus on reducing the time to recover your data.\n- **A complete database recovery is needed.** If your goal is to recover to the most recent transaction, your recovery target is the timestamp of the last persisted transaction. This scenario is a specific case of PITR but semantically this goal is referred to as a\n## Performance considerationsThe archiving process puts additional I/O load on your database server. The additional load is dependent upon the characteristics of your workload, because it is proportionate to the write, update, and delete transaction volume.\nIf you want to reduce the I/O impact that the WAL archive activity might incur on your primary database, you can perform the periodic WAL archives using a read-only replica.\nThis configuration isolates the primary database from the batch-oriented I/O activities related to the transfer of the WAL files. Transactions destined for the read-only replica are transmitted in a constant stream from the primary database, thus exacting a much lower impact upon steady-state throughput.\nFurther, if your production database topology already includes a read-only replica, this configuration doesn't add any additional burden: management, price or otherwise.## Reference architectureThe following diagram illustrates the architecture that you implement in this tutorial.In this tutorial, you create cloud infrastructure to observe a PITR that is using the following components:- A PostgreSQL database server running on Compute Engine.\n- Cloud Storage for storage of snapshots and transaction logs.\nThe following diagram shows the two Docker containers that are launched on the PostgreSQL database virtual machine (VM). As a [separation of concerns](https://wikipedia.org/wiki/Separation_of_concerns) , the database server is running in one of the containers, and the WAL archiver is running in the other container.This diagram shows how the Docker volumes in each container are mapped to Persistent Disk mount points on the host VM.## Setting up environment variablesThe scripts and commands used in this tutorial rely on shell environment variables.- In Cloud Shell, set environment variables for your project, the instance name, and the demonstration PostgreSQL database.```\nexport PROJECT_ID=your-gcp-project\nexport PG_INSTANCE_NAME=instance-pg-pitr\nexport POSTGRES_PASSWORD=PasswordIsThis\nexport POSTGRES_PITR_DEMO_DBNAME=pitr_demo\n```Replace the following:- ``: the name of the project that you created for this tutorial.\n- ``: a secure password for the PostgreSQL database.\n- Set the environment variable for the Google Cloud zone. Replace `` with a [Google Cloud zone](/compute/docs/regions-zones#identifying_a_region_or_zone) .```\nexport ZONE=choose-an-appropriate-zone\nexport REGION=${ZONE%-[a-z]}\n```\n- Set the environment variable for the default [Virtual Private Cloud (VPC)](/vpc/docs) subnet for the region of your zone:```\nexport SUBNETWORK_URI=$(gcloud compute networks subnets \\\u00a0 \u00a0 describe default --format=json --region=$REGION | \\\u00a0 \u00a0 jq --raw-output '.ipCidrRange')\n```\n- Set the environment variable for the Cloud Storage bucket. Replace `` with a unique name for the Cloud Storage bucket where WALs are saved.```\nexport ARCHIVE_BUCKET=archive-bucket\n```\n## Creating a Cloud Storage bucket\n- Create a Cloud Storage bucket to archive the WAL files from the PostgreSQL database:```\ngsutil mb gs://${ARCHIVE_BUCKET}\n```\n## Allowing access to private IP addresses instancesFor the instances used in this tutorial, as in many production use cases, there is no need for the VM instances to obtain public IP addresses. However, the instances require access to the public internet to pull the example container images, and you require access in order to connect by using a secure shell. You configure a network address translation (NAT) gateway and configure [Identity-Aware Proxy (IAP)](/iap/docs) for TCP forwarding.\n### Create a NAT gatewayBecause the VM instances that you create don't have public IP addresses, you create a NAT gateway so that the instances can pull container images from the Docker Hub.- In Cloud Shell, create a [Cloud Router](/router/docs) :```\nexport CLOUD_ROUTER_NAME=${PROJECT_ID}-nat-routergloud compute routers create $CLOUD_ROUTER_NAME \\\u00a0 \u00a0 --network=default --region=$REGION\n```\n- Create the NAT gateway:```\ngcloud compute routers nats create ${PROJECT_ID}-nat-gateway \\\u00a0 \u00a0 --region=$REGION \\\u00a0 \u00a0 --router=$CLOUD_ROUTER_NAME \\\u00a0 \u00a0 --auto-allocate-nat-external-ips \\\u00a0 \u00a0 --nat-all-subnet-ip-ranges\n```\n### Configure IAP for TCP forwardingIAP controls access to your cloud applications and VMs running on Google Cloud. IAP verifies the user identity and context of the request to determine whether a user is allowed to access a VM.- In Cloud Shell, allow traffic from the TCP forwarding net block to the instances in your project:```\nexport IAP_FORWARDING_CIDR=35.235.240.0/20gcloud compute --project=$PROJECT_ID firewall-rules create \\\u00a0 \u00a0 cloud-iap-tcp-forwarding --direction=INGRESS \u00a0\\\u00a0 \u00a0 --priority=1000 --network=default \\\u00a0 \u00a0 --action=ALLOW --rules=all \u00a0\\\u00a0 \u00a0 --source-ranges=$IAP_FORWARDING_CIDR\n```\n- To connect by using a TCP forwarding tunnel, add an [Identity and Access Management (IAM)](/iam/docs) policy binding. Replace `` with the email address that you use to log into Google Cloud console.```\nexport GRANT_EMAIL_ADDRESS=your-email-address\ngcloud projects add-iam-policy-binding $PROJECT_ID \\\n --member=user:$GRANT_EMAIL_ADDRESS \\\n --role=roles/iap.tunnelResourceAccessor\n```\n## Creating the PostgreSQL database infrastructure\n- In Cloud Shell, clone the source repository that contains the configuration scripts, and change the shell context to the local repository:```\ngit clone https://github.com/GoogleCloudPlatform/gcs-postgresql-recovery-tutorialcd gcs-postgresql-recovery-tutorial\n```\n- To create and configure the database VM instance, run the following script:```\ncd bin./create_postgres_instance.sh\n```For this tutorial, this script starts a VM instance in your chosen zone with the container-optimized operating system, and two new attached persistent disks. In this case, you can ignore the warning message returned by the API about poor I/O performance because the scripts create small Persistent Disks.\n## Reviewing the cloud-init configuration [Cloud-init](https://cloudinit.readthedocs.io/en/latest/) is a multi-distribution package that initializes a cloud instance.\nReview the following cloud-init code sample:\n [  yaml/cloud-init.yaml.tpl ](https://github.com/GoogleCloudPlatform/gcs-postgresql-recovery-tutorial/blob/HEAD/yaml/cloud-init.yaml.tpl) [View on GitHub](https://github.com/GoogleCloudPlatform/gcs-postgresql-recovery-tutorial/blob/HEAD/yaml/cloud-init.yaml.tpl) \n```\nwrite_files:- path: /var/tmp/docker-entrypoint-initdb.d/init-pitr-demo-db.sql\u00a0 permissions: 0644\u00a0 owner: root\u00a0 content: |\u00a0 \u00a0 CREATE DATABASE ${POSTGRES_PITR_DEMO_DBNAME};\u00a0 \u00a0 \\c ${POSTGRES_PITR_DEMO_DBNAME}\u00a0 \u00a0 CREATE SCHEMA pitr_db_schema;\u00a0 \u00a0 CREATE TABLE pitr_db_schema.customer\u00a0 \u00a0 \u00a0 \u00a0(id SERIAL NOT NULL,\u00a0 \u00a0 \u00a0 \u00a0 name VARCHAR(255),\u00a0 \u00a0 \u00a0 \u00a0 create_timestamp TIMESTAMP DEFAULT current_timestamp,\u00a0 \u00a0 \u00a0 \u00a0 PRIMARY KEY (id));\u00a0 \u00a0 CREATE TABLE pitr_db_schema.invoice\u00a0 \u00a0 \u00a0 \u00a0(id SERIAL NOT NULL,\u00a0 \u00a0 \u00a0 \u00a0 customer_id INTEGER\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 REFERENCES pitr_db_schema.customer(id),\u00a0 \u00a0 \u00a0 \u00a0 description VARCHAR(1000),\u00a0 \u00a0 \u00a0 \u00a0 create_timestamp TIMESTAMP DEFAULT current_timestamp,\u00a0 \u00a0 \u00a0 \u00a0 PRIMARY KEY (customer_id, id));- path: /etc/systemd/system/postgres.service\u00a0 permissions: 0644\u00a0 owner: root\u00a0 content: |\u00a0 \u00a0 [Unit]\u00a0 \u00a0 Requires=docker.service\u00a0 \u00a0 After=docker.service\u00a0 \u00a0 Description=postgres docker container\u00a0 \u00a0 [Service]\u00a0 \u00a0 TimeoutStartSec=0\u00a0 \u00a0 KillMode=none\u00a0 \u00a0 Restart=always\u00a0 \u00a0 RestartSec=5s\u00a0 \u00a0 ExecStartPre=-/usr/bin/docker kill postgres-db\u00a0 \u00a0 ExecStartPre=-/usr/bin/docker rm -v postgres-db\u00a0 \u00a0 ExecStart=/usr/bin/docker run -u postgres --name postgres-db \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 -v /var/tmp/docker-entrypoint-initdb.d:/docker-entrypoint-initdb.d \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 -v /mnt/disks/data:/var/lib/postgresql/data \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 -v /mnt/disks/wal:/var/lib/postgresql/wal \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 -e PGDATA=/var/lib/postgresql/data/pgdata \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 -e POSTGRES_PASSWORD=${POSTGRES_PASSWORD} \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 -e POSTGRES_INITDB_WALDIR=/var/lib/postgresql/wal/pg_wal \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 -p ${POSTGRES_PORT}:${POSTGRES_PORT} \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0postgres:11-alpine\u00a0 \u00a0 ExecStop=-/usr/bin/docker stop postgres-db\u00a0 \u00a0 ExecStopPost=-/usr/bin/docker rm postgres-db- path: /etc/systemd/system/wal_archive.service\u00a0 permissions: 0644\u00a0 owner: root\u00a0 content: |\u00a0 \u00a0 [Unit]\u00a0 \u00a0 Requires=docker.service postgres.service\u00a0 \u00a0 After=docker.service postgres.service\u00a0 \u00a0 Description=WAL archive docker container\u00a0 \u00a0 [Service]\u00a0 \u00a0 TimeoutStartSec=10min\u00a0 \u00a0 Type=oneshot\u00a0 \u00a0 ExecStart=/usr/bin/docker run --name wal-archive \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 -v /mnt/disks/wal/pg_wal_archive:/mnt/wal_archive \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0google/cloud-sdk:slim gsutil mv /mnt/wal_archive/[0-9A-F]*[0-9A-F] gs://${ARCHIVE_BUCKET}\u00a0 \u00a0 ExecStopPost=-/usr/bin/docker rm wal-archive- path: /etc/systemd/system/wal_archive.timer\u00a0 permissions: 0644\u00a0 owner: root\u00a0 content: |\u00a0 \u00a0 [Unit]\u00a0 \u00a0 Description=Archive WAL to GCS (every 5 minutes)\u00a0 \u00a0 [Timer]\u00a0 \u00a0 OnBootSec=5min\u00a0 \u00a0 OnUnitInactiveSec=5min\u00a0 \u00a0 OnUnitActiveSec=5min\u00a0 \u00a0 [Install]\u00a0 \u00a0 WantedBy=timers.target\n```\nFor this tutorial, cloud-init is used to do the following:- Create two Persistent Disk block storage devices.\n- Create the file systems on the two devices: one for the data and one for the archive logs.\n- Mount the devices at logical mount points on the VM instance which are shared with the Docker containers.\n- Create and then start a`systemd`service (`postgres.service`), which starts a PostgreSQL Docker container with the following:- The persistent disks mounted as volumes.\n- The PostgreSQL port (`5432`) published to the VM host.\n- Create a`/var/tmp/docker-entrypoint-initdb.d/init-pitr-demo-db.sql`file to create a simple set of tables in a demonstration database and schema.\n- Create and start a second`systemd`service (`wal_archive.service`) that runs a Google Cloud CLI Docker container with the WAL disks mounted as a volume. This service backs up archived WAL files to Cloud Storage.\n- Create, enable, and then start a`systemd`timer (`wal_archive.timer`) that periodically runs the`wal_archive.service`.\n- Ensure that the PostgreSQL port (`5432`) is open for the VPC subnet so that the transaction generator can reach the database port.\n## Modify the database instance configurationThe database server is running, but you need to configure network access, and to start the WAL archiving process.\n### Connect to the database VM instance\n- In the Google Cloud console, go to the **VM instances** page. [ Go to VM instance](https://console.cloud.google.com/compute/instances) \n- To open a terminal shell, next to the `instance-pg-pitr` instance that you created, click **SSH** .\n- In the terminal shell, check that the Docker container started: `docker ps`The output is similar to the following:```\nCONTAINER ID IMAGE    COMMAND     CREATED    STATUS    PORTS NAMES\n8bb65d8c1197 postgres:11-alpine \"docker-entrypoint.s\u2026\" About a minute ago Up About a minute   postgres-db\n```If the container is not yet running, wait a moment, and then use the same command to check again.\n### Allow inbound network connections to database **Caution:** These settings might not be the appropriate network settings for your production database and are used only for this tutorial.- In the terminal shell of the `instance-pg-pitr` instance, open the PostgreSQL host-based authentication configuration file for editing:```\nsudoedit /mnt/disks/data/pgdata/pg_hba.conf\n```\n- To remove the default all IP address access to the database, comment out the following line from the end of the file by adding `#` at the beginning of the line. The line in the file looks similar to the following:```\n#host all all all md5\n```\n- To allow password-protected connections from hosts in the `10.0.0.0/8` CIDR block, add the following line to the end of the file:```\nhost \u00a0 \u00a0all \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 all \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 10.0.0.0/8 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 md5\n```This entry enables connectivity from the VPC subnet where the transaction generator is later created.\n- Save and then close the file.\n### Configure WAL archiving\n- In the terminal shell of the `instance-pg-pitr` instance, edit the `postgresql.conf` file:```\nsudoedit /mnt/disks/data/pgdata/postgresql.conf\n```\n- Replace the existing commented-out `archive_mode` , `archive_command` , and `archive_timeout` lines with the following:```\narchive_mode=on\narchive_command = '( ARCHIVE_PATH=/var/lib/postgresql/wal/pg_wal_archive;\ntest ! -f $ARCHIVE_PATH/%f && cp %p $ARCHIVE_PATH/%f.cp && mv\n$ARCHIVE_PATH/%f.cp $ARCHIVE_PATH/%f ) '\narchive_timeout = 120\n``` **Note:** For this tutorial, you use an archive timeout of 120 seconds to demonstrate the creation and archive of the WAL segment files despite a low transaction volume. For more information about these settings, see the [PostgreSQL Write Ahead Log documentation](https://www.postgresql.org/docs/current/runtime-config-wal.html#GUC-ARCHIVE-COMMAND) .When you replace the lines in the modified file, it looks similar to the following code snippet: [  postgres/postgresql.conf.snippet.example ](https://github.com/GoogleCloudPlatform/gcs-postgresql-recovery-tutorial/blob/HEAD/postgres/postgresql.conf.snippet.example) [View on GitHub](https://github.com/GoogleCloudPlatform/gcs-postgresql-recovery-tutorial/blob/HEAD/postgres/postgresql.conf.snippet.example) ```\n.... illustrative snippet start ....# - Archiving -archive_mode=onarchive_command = '( ARCHIVE_PATH=/var/lib/postgresql/wal/pg_wal_archive; \u00a0test ! -f $ARCHIVE_PATH/%f && cp %p $ARCHIVE_PATH/%f.cp && mv $ARCHIVE_PATH/%f.cp $ARCHIVE_PATH/%f ) 'archive_timeout = 120#------------------------------------------------------------------------------# REPLICATION#------------------------------------------------------------------------------.... illustrative snippet end ....\n```\n- Save and then close the file.\n### Apply and verify the configuration changes\n- In the terminal shell of the `instance-pg-pitr` instance, restart the container to apply the changes:```\nsudo systemctl restart postgres\n```\n- Check for the WAL segment files:```\nsudo ls -l /mnt/disks/wal/pg_wal\n```The output is similar to the following:```\ntotal 16388\n-rw------- 1 postgres 70 16777216 Sep 5 23:07 000000010000000000000001\ndrwx------ 2 postgres 70  4096 Sep 5 23:05 archive_status\n```\n- Check for network connectivity to the database:```\nexport LOCAL_IP=127.0.0.1docker exec postgres-db psql -w --host=$LOCAL_IP \\\u00a0 \u00a0 \u00a0 --command='SELECT 1'\n```The output is similar to the following:```\n?column?\n---------  1\n(1 row)\n```\n- Close the SSH connection to the instance.\n## Starting the transaction generator to populate databaseThe following steps start a [Go](https://golang.org/) program that generates transactions for this tutorial. The program runs inside a container on a VM instance.\nThe image for the container is already built and hosted in a project with a public [Container Registry](/container-registry/docs) .\n **Note:** The code is available in the tutorial repository, and a script is included to build the image. If you want to complete the process yourself or change the image, you can use the script to build the image and push it to Container Registry. The steps to modify the script are outside the scope of this tutorial.- In Cloud Shell, change to the transaction generator directory:```\ncd ~/gcs-postgresql-recovery-tutorial/bin\n```\n- Set the environment variables:```\nexport TRANS_GEN_INSTANCE_NAME=instance-trans-genexport POSTGRES_HOST_IP=$(gcloud compute instances describe \u00a0\\\u00a0 \u00a0 --format=json --zone=${ZONE} ${PG_INSTANCE_NAME} | \\\u00a0 \u00a0 jq --raw-output '.networkInterfaces[0].networkIP')\n```\n- To run the transaction generator, start the instance:```\n./run_trans_gen_instance.sh\n```Ignore the warning message about poor I/O performance.\n- Wait a few moments, and check that transactions are reaching the PostgreSQL database:```\ngcloud compute ssh $PG_INSTANCE_NAME \\\u00a0 \u00a0--tunnel-through-iap \\\u00a0 \u00a0--zone=$ZONE \\\u00a0 \u00a0--command=\"docker exec postgres-db psql \\\u00a0 \u00a0--dbname=$POSTGRES_PITR_DEMO_DBNAME \\\u00a0 \u00a0--command='SELECT COUNT(*) FROM pitr_db_schema.customer;'\"\n```The output contains a count greater than 0 when records are added to the database by the transaction generator:```\n count\n------ 413\n(1 row)\n```\n## Configuring the binary snapshot backup scheduleYou can back up persistent disks according to a schedule, and retain them for a time that is defined in the resource policy.\n### Create the snapshot schedule\n- In Cloud Shell, set environment variables:```\nexport ZONE=zone-of-your-instance\nexport SNAPSHOT_SCHEDULE_NAME=pg-disk-schedule\nexport REGION=${ZONE%-[a-z]}\nexport SNAPSHOT_WINDOW_START=$(TZ=\":GMT\" date \"+%H:00\")\nexport SNAPSHOT_RETENTION_DAYS=2\nexport SNAPSHOT_FREQUENCY_HOURS=1\n```Replace with the Google Cloud zone where you previously started the database VM.\n- Create the snapshot schedule:```\ngcloud compute resource-policies create snapshot-schedule \\\u00a0 \u00a0 $SNAPSHOT_SCHEDULE_NAME \\\u00a0 \u00a0 --region=$REGION \\\u00a0 \u00a0 --max-retention-days=$SNAPSHOT_RETENTION_DAYS \\\u00a0 \u00a0 --on-source-disk-delete=apply-retention-policy \\\u00a0 \u00a0 --hourly-schedule=$SNAPSHOT_FREQUENCY_HOURS \\\u00a0 \u00a0 --start-time=$SNAPSHOT_WINDOW_START \\\u00a0 \u00a0 --storage-location=$REGION\n```\n### Attach the snapshot schedule to the disksWhen you ran the script to create an instance, the data and WAL volumes were created as two independent persistent disks. To create persistent disk snapshots according to a defined schedule, you associate a resource policy with each persistent disk. In this case, you want the disk snapshots to occur concurrently, so you use the same policy for both persistent disks attached to the Compute Engine VM.- In Cloud Shell, set environment variables:```\nexport SNAPSHOT_SCHEDULE_NAME=pgdata-disk-schedule\nexport PG_INSTANCE_NAME=instance-pg-pitr\nexport ZONE=zone-of-your-instance\n```\n- Attach the schedule policy to the persistent data disk:```\ngcloud beta compute disks add-resource-policies ${PG_INSTANCE_NAME}-data \\\u00a0 \u00a0 --resource-policies $SNAPSHOT_SCHEDULE_NAME \\\u00a0 \u00a0 --zone $ZONE\n```\n- Attach the schedule policy to the persistent WAL disk:```\ngcloud beta compute disks add-resource-policies ${PG_INSTANCE_NAME}-wal \\\u00a0 \u00a0 --resource-policies $SNAPSHOT_SCHEDULE_NAME \\\u00a0 \u00a0 --zone $ZONE\n```\n## Manually run a snapshot(Optional) Scheduled snapshots happen within the schedule window, so it's unlikely a snapshot is taken immediately when you created the schedule. If you don't want to wait for the scheduled snapshot, you can manually run the initial snapshot.- In Cloud Shell, set environment variables:```\nexport ZONE=zone-of-your-instanceexport PG_INSTANCE_NAME=instance-pg-pitrexport REGION=${ZONE%-[a-z]}\n```\n- Create a snapshot of the two PostgreSQL instance persistent disks:```\ngcloud compute disks snapshot \\\u00a0 \u00a0 ${PG_INSTANCE_NAME}-data ${PG_INSTANCE_NAME}-wal \\\u00a0 \u00a0 --snapshot-names=${PG_INSTANCE_NAME}-data-`date+%s`,${PG_INSTANCE_NAME}-wal-`date +%s` \\\u00a0 \u00a0 --zone=$ZONE --storage-location=$REGION\n```\n- View the snapshots that you created:```\ngcloud compute snapshots list\n```The output is similar to the following:```\nNAME        DISK_SIZE_GB SRC_DISK         STATUS\ninstance-pg-pitr-data-1578339767 200   us-central1-f/disks/instance-pg-pitr-data READY\ninstance-pg-pitr-wal-1578339767 100   us-central1-f/disks/instance-pg-pitr-wal READY\n```\n## Performing a PITRA PITR is often performed to recover data that was lost due to an operational or programmatic misstep.\nIn this section of the tutorial, you perform a database update to simulate a catastrophic loss of data. You then simulate a panicked response, before starting a recovery to the moment before the erroneous command was issued.\n### Ensure that a PITR can be performedBefore performing a PITR, you have to allow sufficient time for the following to run:- The binary backups (disk snapshots)\n- WAL archiving\nFor this tutorial, the `archive_timeout` was set to an atypically low 120 seconds to force frequent WAL file rotation. You also have to wait until at least one scheduled disk snapshot is performed, or you need to [snapshot the disk manually](#manually_run_a_snapshot) .- Check that at least one snapshot was taken:- In the Google Cloud console, go to the **Snapshots** page. [Go to the Snapshots page](https://console.cloud.google.com/compute/snapshots) \n- Verify that there are at least two snapshots\u2014one for the data volume and one for the WAL volume, for example, `instance-pg-pitr--us-central1-a-20190805023535-i3hpw7kn` .\n- Check that the segment files are archived to Cloud Storage:- In the Google Cloud console, go to the **Cloud Storage Browser** page. [Go to the Cloud Storage Browser page](https://console.cloud.google.com/storage/browser) \n- Click **archive-bucket** . \n### Damage the dataTo simulate a catastrophic loss of data, open a command-line shell to the PostgreSQL database, and damage the data in the table populated by the transaction generator.- In the Google Cloud console, go to the **VM instances** page. [Go to the VM instances page](https://console.cloud.google.com/compute/instances) \n- For the **instance-pg-pitr** instance, click **SSH** .\n- In the SSH terminal, run the PostgreSQL terminal-based frontend in the Docker container:```\ndocker exec -it postgres-db psql --dbname=pitr_demo\n```\n- To modify a row in the customer table, submit a SQL DML statement with an intentional typo in the PostgreSQL shell:```\nUPDATE pitr_db_schema.customerSET name = 'New Name for customer id=1';WHERE id = 1;\n```The output is similar to the following:```\nUPDATE 999\npitr_demo=# WHERE id = 1;\nERROR: syntax error at or near \"WHERE\"\nLINE 1: WHERE id = 1;\n  ^\n \n```The error was generated because an extra semicolon was inserted before the `WHERE` clause. All rows in the database were updated. You can now do a PITR to recover the rows that the incorrect statement modified.\n### Determine target time for the recoveryThe first step in a PITR is to determine the target time for the recovery. This time is determined by examining your data to identify a point slightly before the data-damaging event.- In the terminal shell of the `instance-pg-pitr` instance, obtain the maximum timestamp of the damaged rows:```\nSELECT MAX(create_timestamp)::timestamptz\u00a0 FROM pitr_db_schema.customerWHERE name = 'New Name for customer id=1';\n```The output is similar to the following:```\n    max    .\n------------------------------2019-08-05 18:14:58.226511+00\n(1 row)\n```In a production database, the query to determine the recovery target is more complex, especially in cases where the affected table is large, and the indicative column is unindexed.\n- Copy the result; you use the value returned by this query in the next step.\n### Recover the databaseFor this tutorial, a recovery script automates the PITR. We recommend that you have an automated process to recover the database and that you periodically test this process.- In Cloud Shell, change the current working directory to the location of the recovery script:```\ncd ~/gcs-postgresql-recovery-tutorial/bin\n```\n- Set the required environment variables for the script. Replace `` with the query output that you previously copied.```\nexport PROJECT_ID=$(gcloud config get-value project)\nexport PG_INSTANCE_NAME=instance-pg-pitr\nexport POSTGRES_PASSWORD=PasswordIsThis\nexport PG_INSTANCE_NAME=instance-pg-pitr\nexport RECOVER_BUCKET=archive-bucket\nexport PIT_RECOVERY_TARGET=\"YYYY-MM-DD HH:MM:SS.999999+00\"\nexport ZONE=zone-of-your-instance\n```\n- Run the recovery script:```\n./recover_to_point_in_time.sh\n```\n### Understand the recovery scriptThis section provides some details about the input parameters and the steps taken by the script.\nThe script requires that the following environment variables are set:- ``: the recovery target time.\n- ``: the project where the``instance is located.\n- ``: the zone where the``instance is located.\n- ``: the instance where thePostgreSQL instance is running.\n- ``: the Cloud Storage bucket where WAL segment files are archived.\n- ``: the password used for the PostgreSQL database user.\nThe script performs the following steps:- Determines the most recent disk snapshots based on the recovery target date and time.\n- Creates a `cloud-init.yaml` file that is provided to a container-optimized storage VM that runs the PITR database. The `cloud-init.yaml` file creates configuration files and runs several system commands to establish the following environment:- A`gcsfuse`container that mounts the WAL segment file archive bucket as a volume which is then exposed to the host with a [Docker bind mount](https://docs.docker.com/storage/bind-mounts/) .\n- A `postgres-db` container where the database engine runs the following:- The host file system where the persistent disks are attached as volumes.\n- The host file system where the Cloud Storage bucket is attached as a volume.\n- A `recovery.conf` recovery file in the PostgreSQL data directory with the following information:- The target date.\n- The`restore`command: a parameterized copy command that the database uses to copy WAL segment files as needed from the archive file system.`%f`is the segment file and`%p`is the path used by the database for processing files during the recovery.\n- The `archive_` settings are commented out from the `postgresql.conf` settings file to avoid corrupting the WAL archive directory.\n- Starts the PITR instance with the following information:- A name created by combining the`$PG_INSTANCE_NAME`environment variable and the alphanumeric values from the`$PIT_RECOVERY_TARGET`environment variable.\n- Persistent disks created from the previously identified disk snapshots.\nThe following is an example `recovery.conf` file:\n```\nrestore_command = '(test -d /var/lib/postgresql/wal/pg_wal_recover &amp;&amp; cp /var/lib/postgresql/wal/pg_wal_recover/%f %p ) 'recovery_target_time='YYYY-MM-DD HH:MM:SS UTC'recovery_target_inclusive=true\n```\n### Validate the recovery\n- In the Google Cloud console, go to the **VM instances** page. [Go to the VM instances page](https://console.cloud.google.com/compute/instances) \n- For the **instance-pg-pitr-YYYYMMDDHHMMSS** instance, click **SSH** .\n- In the SSH terminals, run the PostgreSQL terminal-based frontend in the Docker container:```\ndocker exec -it postgres-db psql --dbname=pitr_demo\n```If you get the following error, wait a moment for the PostgreSQL container to start, and rerun the command:```\nError: No such container: postgres-db\n```\n- Check the data in the customer table:```\nSELECT * FROM pitr_db_schema.customerWHERE id > (SELECT MAX(id)-10 FROM pitr_db_schema.customer);\n```The output is similar to the following:```\n id |   name   |  create_timestamp\n------+---------------------------+--------------------------- 711 | customer_name_from_golang | 2019-12-06 18:03:51.229444\n 712 | customer_name_from_golang | 2019-12-06 18:03:52.531755\n 713 | customer_name_from_golang | 2019-12-06 18:03:53.555441\n 714 | customer_name_from_golang | 2019-12-06 18:03:54.581872\n 715 | customer_name_from_golang | 2019-12-06 18:03:55.607459\n 716 | customer_name_from_golang | 2019-12-06 18:03:56.633362\n 717 | customer_name_from_golang | 2019-12-06 18:03:57.658523\n 718 | customer_name_from_golang | 2019-12-06 18:03:58.685469\n 719 | customer_name_from_golang | 2019-12-06 18:03:59.706939\n```The name shows the value created by the transaction generator. The final row has a timestamp that is earlier than the recovery target (which you provided to the recovery script in an environment variable). Depending on the number of records that you need to recover, you might have to wait a few moments for all of the rows to update.\n## Clean upThe easiest way to eliminate billing is to delete the Google Cloud project you created for the tutorial. Alternatively, you can delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Learn more about the [Container Optimized Operating System](/container-optimized-os/docs) .\n- Learn more about [cloud-init](https://cloudinit.readthedocs.io/en/latest/) .\n- Learn more about [Cloud Storage Fuse](https://github.com/GoogleCloudPlatform/gcsfuse/) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) . .", "guide": "Docs"}