{"title": "Docs - Build and visualize demand forecast predictions using Datastream, Dataflow, BigQuery ML, and Looker", "url": "https://cloud.google.com/architecture/build-visualize-demand-forecast-prediction-datastream-dataflow-bigqueryml-looker", "abstract": "# Docs - Build and visualize demand forecast predictions using Datastream, Dataflow, BigQuery ML, and Looker\nLast reviewed 2022-05-04 UTC\nThis document shows you how to replicate and process operational data from an Oracle database into Google Cloud in real time. The tutorial also demonstrates how to forecast future demand, and how to visualize this forecast data as it arrives.\nThis tutorial is intended for data engineers and analysts who want to use their operational data. It assumes that you're familiar with writing both SQL queries and user-defined functions (UDFs).\nThis tutorial uses a fictitious retail store named FastFresh to help demonstrate the concepts that it describes. FastFresh specializes in selling fresh produce, and wants to minimize food waste and optimize stock levels across all stores. You use mock sales transactions from FastFresh as the operational data in this tutorial.", "content": "## ArchitectureThe following diagram shows the flow of operational data through Google Cloud.The operational flow shown in the preceding diagram is as follows:- Incoming data from an Oracle source is captured and replicated into Cloud Storage through Datastream.\n- This data is processed and enriched by Dataflow templates, and is then sent to BigQuery.\n- BigQuery ML is used to forecast demand for your data, which is then visualized in Looker.\nGoogle does not provide licenses for Oracle workloads. You are responsible for procuring licenses for the Oracle workloads that you choose to run on Google Cloud, and you are responsible for complying with the terms of these licenses.## Objectives\n- Replicate and process data from Oracle into BigQuery in real time.\n- Run demand forecasting against data that has been replicated and processed from Oracle in BigQuery.\n- Visualize forecasted demand and operational data in Looker in real time.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Datastream](/datastream/pricing) \n- [Cloud Storage](/storage/pricing) \n- [Pub/Sub](/pubsub/pricing) \n- [Dataflow](/dataflow/pricing) \n- [BigQuery (Including BQML)](/bigquery-ml/pricing) \n- [Looker](https://www.looker.com/product/pricing/) \n- [Compute Engine](/compute/all-pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- In the Google Cloud console, on the project selector page,   select or [create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . **Note** : If you don't plan to keep the  resources that you create in this procedure, create a project instead of  selecting an existing project. After you finish these steps, you can  delete the project, removing all resources associated with the project. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- Enable the Compute Engine, Datastream, Dataflow, and Pub/Sub APIs. [Enable the APIs](https://console.cloud.google.com/flows/enableapi?apiid=compute.googleapis.com,datastream.googleapis.com,dataflow.googleapis.com,pubsub.googleapis.com) \n- Make sure that you have developer access to a Looker instance. You need this access to set up your own Looker model and dashboards.To request a trial, see the [Looker free trial demo](https://looker.com/demo/free-trial) .You must also have the role of **Project owner** or **Editor** .\n### Prepare your environment\n- In Cloud Shell, define the following environment variables:```\nexport PROJECT_NAME=\"YOUR_PROJECT_NAME\"export PROJECT_ID=\"YOUR_PROJECT_ID\"export PROJECT_NUMBER=\"YOUR_PROJECT_NUMBER\"export BUCKET_NAME=\"${PROJECT_ID}-oracle_retail\"\n```Replace the following:- ``: the name of your project\n- ``: the ID of your project\n- ``: the number of your project\n- Enter the following:```\ngcloud config set project ${PROJECT_ID}\n```\n- Clone the GitHub tutorial repository which contains the scripts and utilities that you use in this tutorial:```\ngit clone \\https://github.com/caugusto/datastream-bqml-looker-tutorial.git\n```\n- Extract the comma-delimited file containing sample transactions to be loaded into Oracle:```\nbunzip2 \\datastream-bqml-looker-tutorial/sample_data/oracle_data.csv.bz2\n```\n- Create a sample Oracle XE 11g docker instance on Compute Engine by doing the following:- In Cloud Shell, change the directory to `build_docker` :```\n\u00a0 cd datastream-bqml-looker-tutorial/build_docker\n```\n- Run the following `build_orcl.sh` script:```\n\u00a0./build_orcl.sh \\\u00a0-p YOUR_PROJECT_ID \\\u00a0-z GCP_ZONE \\\u00a0-n GCP_NETWORK_NAME \\\u00a0-s GCP_SUBNET_NAME \\\u00a0-f Y \\\u00a0-d Y\n```Replace the following:- ``: Your Google Cloud project ID\n- ``: The zone where the compute instance will be created\n- ``= The network name where VM and firewall entries will be created\n- ``= The network subnet where VM and firewall entries will be created\n- ``= A choice to create the FastFresh schema and`ORDERS`table (Y or N). Use Y for this tutorial.\n- ``= A choice to configure the Oracle database for Datastream usage (Y or N). Use Y for this tutorial.\nThe script does the following:- Creates a new Google Cloud Compute instance.\n- Configures an Oracle 11g XE docker container.\n- Pre-loads the FastFresh schema and the Datastream prerequisites.After the script executes, the `build_orcl.sh` script gives you a summary of the connection details and credentials (DB Host, DB Port, and SID). Make a copy of these details because you use them later in this tutorial.\n- Create a Cloud Storage bucket to store your replicated data:```\ngsutil mb gs://${BUCKET_NAME}\n```Make a copy of the bucket name because you use it in a later step.\n- Configure your bucket to send notifications about object changes to a Pub/Sub topic. This configuration is required by the Dataflow template. Do the following:- Create a new topic called `oracle_retail` :```\ngsutil notification create -t projects/${PROJECT_ID}/topics/oracle_retail -f \\json gs://${BUCKET_NAME}\n```This command creates a new topic called `oracle_retail` which sends notifications about object changes to the Pub/Sub topic.\n- Create a Pub/Sub subscription to receive messages which are sent to the `oracle_retail` topic:```\ngcloud pubsub subscriptions create oracle_retail_sub \\--topic=projects/${PROJECT_ID}/topics/oracle_retail\n```\n- Create a BigQuery dataset named `retail` :```\nbq mk --dataset ${PROJECT_ID}:retail\n```\n- Assign the BigQuery **Admin** role to your Compute Engine service account:```\ngcloud projects add-iam-policy-binding ${PROJECT_ID} \\--member=serviceAccount:${PROJECT_NUMBER}-compute@developer.gserviceaccount.com \\--role='roles/bigquery.admin'\n```\n## Replicate Oracle data to Google Cloud with DatastreamDatastream supports the synchronization of data to Google Cloud databases and storage solutions from sources such as MySQL and Oracle.\nIn this section, you use Datastream to backfill the Oracle FastFresh schema and to replicate updates from the Oracle database to Cloud Storage in real time.\n### Create a stream\n- In Google Cloud console, navigate to Datastream and click **Create Stream** . A form appears.Fill in the form as follows, and then click **Continue** :- **Stream name** : `oracle-cdc`\n- **Stream ID** : `oracle-cdc`\n- **Source type** : `Oracle`\n- **Destination type** : `Cloud Storage`\n- All other fields: Retain the default value\n- In the **Define & Test Source** section, select **Create new connection** profile. A form appears.Fill in the form as follows, and then click **Continue** :- **Connection profile name** : `orcl-retail-source`\n- **Connection profile ID** : `orcl-retail-source`\n- **Hostname** : < `db_host` >\n- **Port** : `1521`\n- **Username** : `datastream`\n- **Password** : `tutorial_datastream`\n- **System Identifier (SID)** : `XE`\n- **Connectivity method** : Select `IP allowlisting` .\n- Click **Run Test** to verify that the source database and Datastream can communicate with each other, and then click **Create & Continue** .You see the **Select Objects to Include** page, which defines the objects to replicate, specific schemas, tables, and columns and be included or excluded.If the test fails, make the necessary changes to the form parameters and then retest.\n- Select the following: **FastFresh** > **Orders** , as shown in the following image:\n- To load existing records, set the **Backfill** mode to **Automatic** , and then click **Continue** .\n- In the **Define Destination** section, select **Create new connection profile** . A form appears.Fill in the form as follows, and then click **Create & Continue** :- Connection Profile Name: `oracle-retail-gcs`\n- Connection Profile ID: `oracle-retail-gcs`\n- Bucket Name: The name of the bucket that you created in [Prepare your environment](#prepare_your_environment) .\n- Keep the **Stream path prefix** blank, and for **Output** format, select JSON. Click **Continue** .\n- On the **Create new connection profile** page, click **Run Validation** , and then click **Create** .The output is similar to the following: \n## Create a Dataflow job using the Datastream to BigQuery templateIn this section, you deploy the Dataflow Datastream to BigQuery streaming template to replicate the changes captured by Datastream into BigQuery.\nYou also extend the functionality of this template by creating and using UDFs.\n### Create a UDF for processing incoming dataYou create a UDF to perform the following operations on both the backfilled data and all new incoming data:- Redact sensitive information such as the customer payment method.\n- Add the Oracle source table to BigQuery for data lineage and discovery purposes.\nThis logic is captured in a JavaScript file that takes the JSON files generated by Datastream as an input parameter.- In the Cloud Shell session, copy and save the following code to a file named `retail_transform.js` :```\nfunction process(inJson) {\u00a0 \u00a0var obj = JSON.parse(inJson),\u00a0 \u00a0includePubsubMessage = obj.data && obj.attributes,\u00a0 \u00a0data = includePubsubMessage ? obj.data : obj;\u00a0 \u00a0data.PAYMENT_METHOD = data.PAYMENT_METHOD.split(':')[0].concat(\"XXX\");\u00a0 \u00a0data.ORACLE_SOURCE = data._metadata_schema.concat('.', data._metadata_table);\u00a0 \u00a0return JSON.stringify(obj);}\n```\n- Create a Cloud Storage bucket to store the `retail_transform.js file` and then upload the JavaScript file to the newly created bucket:```\ngsutil mb gs://js-${BUCKET_NAME}gsutil cp retail_transform.js \\gs://js-${BUCKET_NAME}/utils/retail_transform.js\n```\n### Create a Dataflow job\n- In Cloud Shell, create a dead-letter queue (DLQ) bucket:```\ngsutil mb gs://dlq-${BUCKET_NAME}\n```This bucket is used by Dataflow.\n- Create a service account for the Dataflow execution and assign the account the following roles: `Dataflow Worker` , `Dataflow Admin` , `Pub/Sub Admin` , `BigQuery Data Editor` , `BigQuery Job User` , and `Datastream Admin` .```\ngcloud iam service-accounts create df-tutorialgcloud projects add-iam-policy-binding ${PROJECT_ID} \\--member=\"serviceAccount:df-tutorial@${PROJECT_ID}.iam.gserviceaccount.com\" \\--role=\"roles/dataflow.admin\"gcloud projects add-iam-policy-binding ${PROJECT_ID} \\--member=\"serviceAccount:df-tutorial@${PROJECT_ID}.iam.gserviceaccount.com\" \\--role=\"roles/dataflow.worker\"gcloud projects add-iam-policy-binding ${PROJECT_ID} \\--member=\"serviceAccount:df-tutorial@${PROJECT_ID}.iam.gserviceaccount.com\" \\--role=\"roles/pubsub.admin\"gcloud projects add-iam-policy-binding ${PROJECT_ID} \\--member=\"serviceAccount:df-tutorial@${PROJECT_ID}.iam.gserviceaccount.com\" \\--role=\"roles/bigquery.dataEditor\"gcloud projects add-iam-policy-binding ${PROJECT_ID} \\--member=\"serviceAccount:df-tutorial@${PROJECT_ID}.iam.gserviceaccount.com\" \\--role=\"roles/bigquery.jobUser\"gcloud projects add-iam-policy-binding ${PROJECT_ID} \\--member=\"serviceAccount:df-tutorial@${PROJECT_ID}.iam.gserviceaccount.com\" \\--role=\"roles/datastream.admin\"gcloud projects add-iam-policy-binding ${PROJECT_ID} \\--member=\"serviceAccount:df-tutorial@${PROJECT_ID}.iam.gserviceaccount.com\" \\--role=\"roles/storage.admin\"\n```\n- Create a firewall egress rule to let Dataflow VMs communicate, send, and receive network traffic on TCP ports 12345 and 12346 when autoscale is enabled:```\ngcloud compute firewall-rules create fw-allow-inter-dataflow-comm \\--action=allow \\--direction=ingress \\--network=GCP_NETWORK_NAME \u00a0\\--target-tags=dataflow \\--source-tags=dataflow \\--priority=0 \\--rules tcp:12345-12346\n```\n- Create and run a Dataflow job:```\nexport REGION=us-central1gcloud dataflow flex-template run orders-cdc-template --region ${REGION} \\--template-file-gcs-location \"gs://dataflow-templates/latest/flex/Cloud_Datastream_to_BigQuery\" \\--service-account-email \"df-tutorial@${PROJECT_ID}.iam.gserviceaccount.com\" \\--parameters \\inputFilePattern=\"gs://${BUCKET_NAME}/\",\\gcsPubSubSubscription=\"projects/${PROJECT_ID}/subscriptions/oracle_retail_sub\",\\inputFileFormat=\"json\",\\outputStagingDatasetTemplate=\"retail\",\\outputDatasetTemplate=\"retail\",\\deadLetterQueueDirectory=\"gs://dlq-${BUCKET_NAME}\",\\autoscalingAlgorithm=\"THROUGHPUT_BASED\",\\mergeFrequencyMinutes=1,\\javascriptTextTransformGcsPath=\"gs://js-${BUCKET_NAME}/utils/retail_transform.js\",\\javascriptTextTransformFunctionName=\"process\"\n```Check the Dataflow console to verify that a new streaming job has started.\n- In Cloud Shell, run the following command to start your Datastream stream:```\ngcloud datastream streams update oracle-cdc \\--location=us-central1 --state=RUNNING --update-mask=state\n```\n- Check the Datastream stream status:```\ngcloud datastream streams list \\--location=us-central1\n```Validate that the state shows as `Running` . It may take a few seconds for the new state value to be reflected.Check the Datastream console to validate the progress of the `ORDERS` table backfill.The output is similar to the following: Because this task is an initial load, Datastream reads from the `ORDERS` object. It writes all records to the JSON files located in the Cloud Storage bucket that you specified during the stream creation. It will take about 10 minutes for the backfill task to complete.\n## Analyze your data in BigQueryAfter a few minutes, your backfilled data replicates into BigQuery. Any new incoming data is streamed into your datasets in (near) real time. Each record is processed by the UDF logic that you defined as part of the Dataflow template.\nThe following two new tables in the datasets are created by the Dataflow job:- `ORDERS` : This output table is a replica of the Oracle table and includes the transformations applied to the data as part of the Dataflow template.The output is similar to the following: \n- `ORDERS_log` : This staging table records all the changes from your Oracle source. The table is partitioned, and stores the updated record alongside some metadata change information, such as whether the change is an update, insert, or delete.The output is similar to the following: \nBigQuery lets you see a real-time view of the operational data. You can also run queries such as a comparison of the sales of a particular product across stores in real time, or combining sales and customer data to analyze the spending habits of customers in particular stores.\n### Run queries against your operational data\n- In BigQuery, run the following SQL to query the top three selling products:```\nSELECT product_name, SUM(quantity) as total_salesFROM `retail.ORDERS`GROUP BY product_nameORDER BY total_sales descLIMIT 3\n```The output is similar to the following:\n- In BigQuery, run the following SQL statements to query the number of rows on both the `ORDERS` and `ORDERS_log` tables:```\nSELECT count(*) FROM `hackfast.retail.ORDERS_log`SELECT count(*) FROM `hackfast.retail.ORDERS`\n```With the backfill completed, both statements return the number 520217.\n## Build a demand forecasting model in BigQuery MLBigQuery ML can be used to build and deploy demand forecasting models using the [ARIMA_PLUS](/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create-time-series) algorithm. In this section, you use BigQuery ML to build a model to forecast the demand for products in the store.\n### Prepare your training dataYou use a sample of the data that you backfilled to train the model. In this case, you use data from a one-year period. The training data shows the following:- The name of the product (`product_name`)\n- How many units of each product were sold (`total_sold`)\n- The number of products sold each hour (`hourly_timestamp`)\nDo the following:- In BigQuery, run the following SQL to create and save the training data to a new table named `training_data` :```\nCREATE OR REPLACE TABLE `retail.training_data`AS\u00a0 \u00a0SELECT\u00a0 \u00a0 \u00a0 \u00a0TIMESTAMP_TRUNC(time_of_sale, HOUR) as hourly_timestamp,\u00a0 \u00a0 \u00a0 \u00a0product_name,\u00a0 \u00a0 \u00a0 \u00a0SUM(quantity) AS total_sold\u00a0 \u00a0FROM `retail.ORDERS`\u00a0 \u00a0 \u00a0 \u00a0GROUP BY hourly_timestamp, product_name\u00a0 \u00a0 \u00a0 \u00a0HAVING hourly_timestamp BETWEEN TIMESTAMP_TRUNC('2021-11-22', HOUR) ANDTIMESTAMP_TRUNC('2021-11-28', HOUR)ORDER BY hourly_timestamp\n```\n- Run the following SQL to verify the `training_data` table:```\nSELECT * FROM `retail.training_data` LIMIT 10;\n```The output is similar to the following:\n### Forecast demand\n- In BigQuery, run the following SQL to create a time-series model that uses the `ARIMA_PLUS` algorithm:```\nCREATE OR REPLACE MODEL `retail.arima_plus_model`\u00a0 \u00a0 \u00a0 \u00a0OPTIONS(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0MODEL_TYPE='ARIMA_PLUS',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0TIME_SERIES_TIMESTAMP_COL='hourly_timestamp',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0TIME_SERIES_DATA_COL='total_sold',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0TIME_SERIES_ID_COL='product_name'\u00a0 \u00a0 \u00a0 \u00a0) ASSELECT\u00a0 \u00a0hourly_timestamp,\u00a0 \u00a0product_name,\u00a0 \u00a0total_soldFROM\u00a0`retail.training_data`\n```The [ML.FORECAST](/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-forecast) function is used to forecast the expected demand over a horizon of `n` hours.\n- Run the following SQL to forecast the demand for organic bananas over the next 30 days:```\nSELECT * FROM ML.FORECAST(MODEL `retail.arima_plus_model`, \u00a0STRUCT(720 AS horizon))\n```The output is similar to the following:Because the training data is hourly, the horizon value will use the same unit of time when forecasting (hours). A horizon value of 720 hours will return forecast results over the next 30 days.Because this tutorial uses a small sample dataset, further investigation into the accuracy of the model is out of scope for this tutorial.\n### Create a view for visualization in Looker\n- In BigQuery, run the following SQL query to create a view to unionize the actual and forecasted sales for organic bananas:```\nCREATE OR REPLACE VIEW `retail.orders_forecast` AS (SELECTtimestamp,product_name,SUM(forecast_value) AS forecast,SUM(actual_value) AS actualfrom(SELECT\u00a0 \u00a0TIMESTAMP_TRUNC(TIME_OF_SALE, HOUR) AS timestamp,\u00a0 \u00a0product_name,\u00a0 \u00a0SUM(QUANTITY) as actual_value,\u00a0 \u00a0NULL AS forecast_value\u00a0 \u00a0FROM `retail.ORDERS`\u00a0 \u00a0GROUP BY timestamp, product_nameUNION ALLSELECT\u00a0 \u00a0 \u00a0 \u00a0forecast_timestamp AS timestamp,\u00a0 \u00a0 \u00a0 \u00a0product_name,\u00a0 \u00a0 \u00a0 \u00a0NULL AS actual_value,\u00a0 \u00a0 \u00a0 \u00a0forecast_value,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0FROM ML.FORECAST(MODEL `retail.arima_plus_model`,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0STRUCT(720 AS horizon))\u00a0 \u00a0 \u00a0 \u00a0ORDER BY timestamp)GROUP BY timestamp, product_nameORDER BY timestamp)\n```This view lets Looker query the relevant data when you explore the actual and forecasted data.\n- Run the following SQL to validate the view:```\nSELECT * FROM `retail.orders_forecast`WHERE PRODUCT_NAME='Bag of Organic Bananas'AND TIMESTAMP_TRUNC(timestamp, HOUR) BETWEEN TIMESTAMP_TRUNC('2021-11-28', HOUR) AND TIMESTAMP_TRUNC('2021-11-30', HOUR)LIMIT 100;\n```The output is similar to the following:As an alternative to BigQuery views, you can also use the built-in derived tables capabilities in Looker. These include built-in derived tables and SQL-based derived tables. For more information, see [Derived tables in Looker](https://docs.looker.com/data-modeling/learning-lookml/derived-tables) \n### Visualize operational and forecasted data in LookerIn this section, you visualize the contents of `orders_forecast` view by creating a graph in Looker.\n### Set up a BigQuery connection\n- In Cloud Shell, create a service account with the BigQuery **Data Editor** and BigQuery **Job User** roles:```\ngcloud iam service-accounts create looker-sagcloud projects add-iam-policy-binding ${PROJECT_ID} \\--member=\"serviceAccount:looker-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\--role=\"roles/bigquery.dataEditor\"gcloud projects add-iam-policy-binding ${PROJECT_ID} \\--member=\"serviceAccount:looker-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\--role=\"roles/bigquery.jobUser\"gcloud iam service-accounts keys create looker_sa_key.json \\--iam-account=looker-sa@${PROJECT_ID}.iam.gserviceaccount.com\n```\n- Download the key in JSON format. The key is downloaded to the Cloud Shell directory.\n- Open Looker and navigate to **Admin** > **Database** > **Connections** > **Add Connections** .\n- Fill in the **Connection Settings** form as follows:- **Name:** `rds_cdc_retail`\n- **Dialect:** Google BigQuery Standard SQL\n- **Project Name:** `your_project_name`\n- **Dataset:** `retail`\n- **Service account email:** The name of the account that you created in an earlier step.\n- **Persistent Derived Tables:** Select the checkbox to enable this feature.\n- Upload the service account key that you downloaded to the `Service Account JSON/P12 File` field.\n- Click **Test These Settings** to validate the connection and then click **Create Connection** .\n### Create a new LookML project [LookML](https://docs.looker.com/data-modeling/learning-lookml/what-is-lookml) is a lightweight modeling language that you can use to build models of your data. You use these models to tell Looker how to query your data. In this section, you build a LookML model to reflect the BigQuery dataset schema that you use in this tutorial. Looker constructs SQLs queries against this BigQuery dataset to extract the relevant data for your model.- In the Looker console, click **Go to Develop** > **Manage LookML Projects** > **New LookML Project** .\n- Fill in the form as follows:- **Starting point:** Select **Generate Model** from **Database Schema** \n- **Connection:** `rds_cdc_retail`\n- **Build views from:** Select **All Tables** \nThis project will generate queries using the BigQuery connection that you created in the previous section.\n- Click **Create Project** . You are directed to the **Explore** page. As part of the project creation, model and view files are created that represent the BigQuery tables.The output is similar to the following:The model files describe which tables to use and the relationships between them.The view files describe how to access the fields in your table and let you create customized dimensions, aggregates, and relationships across your data.\n- Copy the following code to the `orders_forecast.view` file and add a new measure to it:```\n\u00a0measure: actual_sum {\u00a0 \u00a0 \u00a0 type: sum\u00a0 \u00a0 \u00a0 sql: ${TABLE}.actual ;;\u00a0 \u00a0 }\u00a0measure: forecast_sum {\u00a0 \u00a0 \u00a0type: sum\u00a0 \u00a0 \u00a0sql: ${TABLE}.forecast ;;\u00a0 \u00a0 \u00a0}\n```This measure summarizes the respective `actual_sum` and `forecast_sum` fields in the BigQuery table.\n- Click **Save Changes** .\n### Create a Looker ExploreIn this section, you select the visualization type for your Looker ML project.\nExplores can be used to run queries and build visualizations against the dimensions and measures defined in the data model. To learn more about Looker Explores, see [Explore parameters](https://docs.looker.com/reference/explore-reference) .- In the Looker console, select **Explore Orders Forecast** from the **orders_forecast.view** drop-down menu.This selection lets you explore and compare the forecasted and actual sales of organic bananas over a period of time.\n- Make the following selections in the Explore (under the **All Fields** tab):- Select the following filters: `Actual Sum` and `Forecast Sum` .\n- Select the filter icon next to **Product Name** and set the following:- **Orders Forecast Product Name:** `is equal to`:`Bag of Organic Bananas`\nThe filter that you see is similar to the following:\n- Expand the **Timestamp** field and select the **Time** dimension\n- In the **Timestamp** field, select the filter icon besides **Date** and set the following filters in the **Orders Forecast Timestamp Date** field:- `is in range`:`2021-11-28`\n- `until (before)`:`2021-11-30`\n- Click **Run** . The results from BigQuery are displayed in columnar format.The output is similar to the following: \n- To visualize these results, click **Visualization** and then **Line Chart** .\n- On the top right, click the settings icon and then click **Save to Dashboard** .\n- Name the tile `Forecasted Data` and then select **New Dashboard** .The output is similar to the following:\n- When you're prompted, name the dashboard `FastFresh Retail Dashboard` .\n- Navigate to the new dashboard by clicking the following link text: **FastFresh Retail Dashboard** .You see the following output:\nYou can continue to explore Looker by adding additional titles to the FastFresh dashboard.\nLooker dashboards support filtering and alerts and can be shared across your organization or scheduled to be periodically sent out to select teams. To enable others in your organization to use the dashboard, you need to push the LookML model to production, however, this is out of the scope of this tutorial.## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Learn how to [connect your Looker instance to BigQuery](https://docs.looker.com/setup-and-management/database-config/google-bigquery) \n- Read about [demand forecasting in BigQuery](/blog/topics/developers-practitioners/how-build-demand-forecasting-models-bigquery-ml) \n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Docs"}