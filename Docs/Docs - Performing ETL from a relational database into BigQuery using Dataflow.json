{"title": "Docs - Performing ETL from a relational database into BigQuery using Dataflow", "url": "https://cloud.google.com/architecture/performing-etl-from-relational-database-into-bigquery", "abstract": "# Docs - Performing ETL from a relational database into BigQuery using Dataflow\nLast reviewed 2022-08-21 UTC\nThis tutorial demonstrates how to use Dataflow to extract, transform, and load (ETL) data from an online transaction processing (OLTP) relational database into BigQuery for analysis.\nThis tutorial is intended for database admins, operations professionals, and cloud architects interested in taking advantage of the analytical query capabilities of [BigQuery](/bigquery) and the batch processing capabilities of [Dataflow](/dataflow) .\nOLTP databases are often relational databases that store information and process transactions for ecommerce sites, software as a service (SaaS) applications, or games. OLTP databases are usually optimized for transactions, which require the [ACID properties](https://wikipedia.org/wiki/ACID_(computer_science)) : atomicity, consistency, isolation, and durability, and typically have highly normalized schemas. In contrast, data warehouses tend to be optimized for data retrieval and analysis, rather than transactions, and typically feature denormalized schemas. Generally, denormalizing data from an OLTP database makes it more useful for analysis in BigQuery.", "content": "## ObjectivesThe tutorial shows two approaches to ETL normalized RDBMS data into denormalized BigQuery data:- Using BigQuery to load and transform the data. Use this approach to perform a one-time load of a small amount of data into BigQuery for analysis. You might also use this approach to prototype your dataset before you automate larger or multiple datasets.\n- Using Dataflow to load, transform, and cleanse the data. Use this approach to load a larger amount of data, load data from multiple data sources, or to load data incrementally or automatically.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Compute Engine](/compute/pricing) \n- [Cloud Storage](/storage/pricing) \n- [Dataflow](/dataflow/pricing) \n- [BigQuery](/bigquery/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n## Using the MusicBrainz datasetThis tutorial relies on JSON snapshots of tables in the MusicBrainz database, which is built on PostgreSQL and contains information about all of the MusicBrainz music. Some elements of the [MusicBrainz schema](https://musicbrainz.org/doc/MusicBrainz_Database/Schema) include:- Artists\n- Release groups\n- Releases\n- Recordings\n- Works\n- Labels\n- Many of the [relationships](https://musicbrainz.org/doc/Relationships) between these entities.\nThe MusicBrainz schema includes three relevant tables: `artist` , `recording` , and `artist_credit_name` . An `artist_credit` represents credit given to the artist for a recording, and the `artist_credit_name` rows link the recording with its corresponding artist through the `artist_credit` value.\nThis tutorial provides the PostgreSQL tables already extracted into newline-delimited JSON format and stored in a public Cloud Storage bucket: `gs://solutions-public-assets/bqetl`\nIf you want to perform this step yourself, you need to have a PostgreSQL database containing the MusicBrainz dataset, and use the following commands to export each of the tables:\n```\nhost=POSTGRES_HOSTuser=POSTGRES_USERdatabase=POSTGRES_DATABASEfor table in artist recording artist_credit_namedo\u00a0 \u00a0 pg_cmd=\"\\\\copy (select row_to_json(r) from (select * from ${table}) r ) to exported_${table}.json\"\u00a0 \u00a0 psql -w -h ${host} -U ${user} -d ${db} -c $pg_cmd\u00a0 \u00a0 # clean up extra '\\' characters\u00a0 \u00a0 sed -i -e 's/\\\\\\\\/\\\\/g' exported_${table}.json done\n```## Approach 1: ETL with BigQueryUse this approach to perform a one-time load of a small amount of data into BigQuery for analysis. You might also use this approach to prototype your dataset before you use automation with larger or multiple datasets.\n### Create a BigQuery datasetTo create a BigQuery dataset, you load the MusicBrainz tables into BigQuery individually, and then you join the tables that you loaded so that each row contains the data linkage that you want. You store the join results in a new BigQuery table. Then you can delete the original tables that you loaded.- In the Google Cloud console, open BigQuery. [OPEN BIGQUERY](https://console.cloud.google.com/bigquery) \n- In the **Explorer** panel, click the menu next to your project name, and then click **Create data set** .\n- In the **Create data set** dialog, complete the following steps:- In the **Data set ID** field, enter`musicbrainz`.\n- Set the **Data Location** to **us** .\n- Click **Create data set** .\n### Import MusicBrainz tablesFor each MusicBrainz table, perform the following steps to add a table to the dataset you created:- In the Google Cloud console BigQuery **Explorer** panel, expand the row with your project name to show the newly created`musicbrainz`dataset.\n- Click the menunext to your`musicbrainz`dataset, and then click **Create Table** .\n- In the **Create Table** dialog, complete the following steps:- In the **Create table from** drop-down list, select **Google Cloud Storage** .\n- In the **Select file from GCS bucket** field, enter the path to the data file:```\nsolutions-public-assets/bqetl/artist.json\n```\n- For **File format** , select **JSONL (Newline Delimited JSON)** .\n- Ensure that **Project** contains your project name.\n- Ensure that **Data set** is `musicbrainz` .\n- For **Table** , enter the table name, `artist` .\n- For **Table type** , leave **Native table** selected.\n- Below the **Schema** section, click to turn on **Edit as Text** .\n- [Download the artist schema file](https://storage.googleapis.com/solutions-public-assets/bqetl/artist_schema.json) and open it in a text editor or viewer.\n- Replace the contents of the **Schema** section with the contents of the schema file you downloaded.\n- Click **Create Table** :\n- Wait a few moments for the load job to complete.\n- When the load has finished, the new table appears under the dataset.\n- Repeat steps 1 - 5 to create the `artist_credit_name` table with the following changes:- Use the following path for the source data file:```\nsolutions-public-assets/bqetl/artist_credit_name.json\n```\n- Use `artist_credit_name` as the **Table** name.\n- Download the [artist_credit_name schema file](https://storage.googleapis.com/solutions-public-assets/bqetl/artist_credit_name_schema.json) and use the contents for the schema.\n- Repeat steps 1 - 5 to create the `recording` table with the following changes:- Use the following path for the source data file:```\nsolutions-public-assets/bqetl/recording.json\n```\n- Use `recording` as the **Table** name.\n- Download the [recording schema file](https://storage.googleapis.com/solutions-public-assets/bqetl/recording_schema.json) . and use the contents for the schema.\n### Manually denormalize the dataTo denormalize the data, join the data into a new BigQuery table that has one row for each artist's recording, together with selected metadata you want retained for analysis.- If the BigQuery query editor is not open in the Google Cloud console click **Compose New Query** .\n- Copy the following query and paste it into the **Query Editor** :```\nSELECT\u00a0 \u00a0 artist.id,\u00a0 \u00a0 artist.gid AS artist_gid,\u00a0 \u00a0 artist.name AS artist_name,\u00a0 \u00a0 artist.area,\u00a0 \u00a0 recording.name AS recording_name,\u00a0 \u00a0 recording.length,\u00a0 \u00a0 recording.gid AS recording_gid,\u00a0 \u00a0 recording.videoFROM\u00a0 \u00a0 `musicbrainz.artist` AS artistINNER JOIN\u00a0 \u00a0 `musicbrainz.artist_credit_name` AS artist_credit_nameON\u00a0 \u00a0 artist.id = artist_credit_name.artistINNER JOIN\u00a0 \u00a0 `musicbrainz.recording` AS recordingON\u00a0 \u00a0 artist_credit_name.artist_credit = recording.artist_credit\n```\n- Click the **More** drop-down list, and then select **Query settings** .\n- In the **Query settings** dialog, complete the following steps:- Select **Set a destination table for query results** .\n- In **Dataset** , enter`musicbrainz`and select the dataset in your project.\n- In **Table id** enter`recordings_by_artists_manual`.\n- For **Destination table write preference** , click **Overwrite table** .\n- Select the **Allow Large Results (no size limit)** checkbox.\n- Click **Save** .\n- Click **Run** .When the query is complete, the data from the query result is organized into songs for each artist in the newly created BigQuery table, and a sample of the results shown in the **Query Results** pane, for example:| Row |  id | artist_gid | artist_name      | area | recording_name          | length | recording_gid | video |\n|------:|-------:|:-------------|:---------------------------------|-------:|:-----------------------------------------------------|---------:|:----------------|:--------|\n|  1 | 97546 | 125ec42a... | unknown       | 240 | Horo Gun Toireamaid H\u00f9gan Fhathast Air    | 174106 | c8bbe048...  | False |\n|  2 | 266317 | 2e7119b5... | Capella Istropolitana   | 189 | Concerto Grosso in D minor, op. 2 no. 3: II. Adagio | 134000 | af0f294d...  | False |\n|  3 | 628060 | 34cd3689... | Conspirare      | 5196 | Liturgy, op. 42: 9. Praise the Lord from the Heavens | 126933 | 8bab920d...  | False |\n|  4 | 423877 | 54401795... | Boys Air Choir     | 1178 | Nunc Dimittis          | 190000 | 111611eb...  | False |\n|  5 | 394456 | 9914f9f9... | L\u2019Orchestre de la Suisse Romande | 23036 | Concert Waltz no. 2, op. 51       | 509960 | b16742d1...  | False |\n## Approach 2: ETL into BigQuery with DataflowIn this section of the tutorial, instead of using the BigQuery UI, you use a sample program to load data into BigQuery by using a Dataflow pipeline. Then, you use the Beam programming model to denormalize and cleanse data to load into BigQuery.\nBefore you begin, review the concepts and the sample code.\n### Review the conceptsAlthough the data is small and can quickly be uploaded by using the BigQuery UI, for the purpose of this tutorial you can also use Dataflow for ETL. Use Dataflow for ETL into BigQuery instead of the BigQuery UI when you are performing massive joins, that is, from around 500-5000 columns of more than 10 TB of data, with the following goals:- You want to clean or transform your data as it's loaded into BigQuery, instead of storing it and joining afterwards. As a result, this approach also has lower storage requirements because data is only stored in BigQuery in its joined and transformed state.\n- You plan to do custom data cleansing (which cannot be simply achieved with SQL).\n- You plan to combine the data with data outside of the OLTP, such as logs or remotely accessed data, during the loading process.\n- You plan to automate testing and deployment of data-loading logic using continuous integration or continuous deployment (CI/CD).\n- You anticipate gradual iteration, enhancement, and improvement of the ETL process over time.\n- You plan to add data incrementally, as opposed to performing a one-time ETL.\nHere's a diagram of the data pipeline that's created by the sample program:In the example code, many of the pipeline steps are grouped or wrapped in convenience methods, given descriptive names, and reused. In the diagram, reused steps are indicated by dashed borders.\n### Review the pipeline codeThe code creates a pipeline that performs the following steps:- Loads each table that you want to be part of the join from the public Cloud Storage bucket into a `PCollection` of strings. Each element comprises the JSON representation of a row of the table. [  src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) ```\npublic static PCollection<String> loadText(Pipeline p, String name) {\u00a0 BQETLOptions options = (BQETLOptions) p.getOptions();\u00a0 String loadingBucket = options.getLoadingBucketURL();\u00a0 String objectToLoad = storedObjectName(loadingBucket, name);\u00a0 return p.apply(name, TextIO.read().from(objectToLoad));}\n```\n- Converts those JSON strings to object representations, `MusicBrainzDataObject` objects, and then organize the object representations by one of the column values, such as a primary or foreign key. [  src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) ```\npublic static PCollection<KV<Long, MusicBrainzDataObject>> loadTableFromText(\u00a0 \u00a0 PCollection<String> text, String name, String keyName) {\u00a0 final String namespacedKeyname = name + \"_\" + keyName;\u00a0 return text.apply(\u00a0 \u00a0 \u00a0 \"load \" + name,\u00a0 \u00a0 \u00a0 MapElements.into(new TypeDescriptor<KV<Long, MusicBrainzDataObject>>() {})\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (String input) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MusicBrainzDataObject datum = JSONReader.readObject(name, input);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Long key = (Long) datum.getColumnValue(namespacedKeyname);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return KV.of(key, datum);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }));}\n```\n- Joins the list based on common artist. The `artist_credit_name` links an artist credit with its recording and includes the artist foreign key. The `artist_credit_name` table is loaded as a list of key value `KV` objects. The `K` member is the artist. [  src/main/java/com/google/cloud/bqetl/BQETLSimple.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/BQETLSimple.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/BQETLSimple.java) ```\nPCollection<MusicBrainzDataObject> artistCredits =\u00a0 \u00a0 MusicBrainzTransforms.innerJoin(\"artists with artist credits\", artists, artistCreditName);\n```\n- Joins the list by using the `MusicBrainzTransforms.innerJoin()` method. [  src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) ```\npublic static PCollection<MusicBrainzDataObject> innerJoin(\u00a0 \u00a0 String name,\u00a0 \u00a0 PCollection<KV<Long, MusicBrainzDataObject>> table1,\u00a0 \u00a0 PCollection<KV<Long, MusicBrainzDataObject>> table2) {\u00a0 final TupleTag<MusicBrainzDataObject> t1 = new TupleTag<MusicBrainzDataObject>() {};\u00a0 final TupleTag<MusicBrainzDataObject> t2 = new TupleTag<MusicBrainzDataObject>() {};\u00a0 PCollection<KV<Long, CoGbkResult>> joinedResult = group(name, table1, table2, t1, t2);\n```- Groups the collections of`KV`objects by the key member on which you want to join. This results in a`PCollection`of`KV`objects with a long key (the`artist.id`column value) and resulting`CoGbkResult`(which stands for combine group by key result). The`CoGbkResult`object is a tuple of lists of objects with the key value in common from the first and second`PCollections`. This tuple is addressable by using the tuple tag formulated for each`PCollection`prior to running the`CoGroupByKey`operation in the`group`method.\n- Merges each matchup of objects into a `MusicBrainzDataObject` object that represents a join result. [  src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) ```\nPCollection<List<MusicBrainzDataObject>> mergedResult =\u00a0 \u00a0 joinedResult.apply(\u00a0 \u00a0 \u00a0 \u00a0 \"merge join results\",\u00a0 \u00a0 \u00a0 \u00a0 MapElements.into(new TypeDescriptor<List<MusicBrainzDataObject>>() {})\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (KV<Long, CoGbkResult> group) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 List<MusicBrainzDataObject> result = new ArrayList<>();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Iterable<MusicBrainzDataObject> leftObjects = group.getValue().getAll(t1);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Iterable<MusicBrainzDataObject> rightObjects = group.getValue().getAll(t2);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 leftObjects.forEach(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (MusicBrainzDataObject l) ->\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rightObjects.forEach(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (MusicBrainzDataObject r) -> result.add(l.duplicate().merge(r))));\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return result;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }));\n```\n- Reorganizes the collection into a list of `KV` objects to begin the next join. This time, the `K` value is the `artist_credit` column, which is used to join with the recording table. [  src/main/java/com/google/cloud/bqetl/BQETLSimple.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/BQETLSimple.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/BQETLSimple.java) ```\nPCollection<KV<Long, MusicBrainzDataObject>> artistCreditNamesByArtistCredit =\u00a0 \u00a0 MusicBrainzTransforms.by(\"artist_credit_name_artist_credit\", artistCredits);\n```\n- Obtains the final resulting collection of `MusicBrainzDataObject` objects by joining that result with the loaded collection of recordings that are organized by `artist_credit.id` . [  src/main/java/com/google/cloud/bqetl/BQETLSimple.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/BQETLSimple.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/BQETLSimple.java) ```\nPCollection<MusicBrainzDataObject> artistRecordings =\u00a0 \u00a0 MusicBrainzTransforms.innerJoin(\u00a0 \u00a0 \u00a0 \u00a0 \"joined recordings\", artistCreditNamesByArtistCredit, recordingsByArtistCredit);\n```\n- Maps the resulting `MusicBrainzDataObjects` objects into `TableRows` . [  src/main/java/com/google/cloud/bqetl/BQETLSimple.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/BQETLSimple.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/BQETLSimple.java) ```\nPCollection<TableRow> tableRows =\u00a0 \u00a0 MusicBrainzTransforms.transformToTableRows(artistRecordings, bqTableSchema);\n```\n- Writes the resulting `TableRows` into BigQuery. [  src/main/java/com/google/cloud/bqetl/BQETLSimple.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/BQETLSimple.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/BQETLSimple.java) ```\ntableRows.apply(\u00a0 \u00a0 \"Write to BigQuery\",\u00a0 \u00a0 BigQueryIO.writeTableRows()\u00a0 \u00a0 \u00a0 \u00a0 .to(options.getBigQueryTablename())\u00a0 \u00a0 \u00a0 \u00a0 .withSchema(bqTableSchema)\u00a0 \u00a0 \u00a0 \u00a0 .withCustomGcsTempLocation(StaticValueProvider.of(options.getTempLocation()))\u00a0 \u00a0 \u00a0 \u00a0 .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_TRUNCATE)\u00a0 \u00a0 \u00a0 \u00a0 .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED));\n```\nFor details about the mechanics of Beam pipeline programming, review the following topics about the [programming model](/dataflow/docs/concepts/beam-programming-model) :- [PCollection](https://beam.apache.org/documentation/programming-guide/#pcollections) \n- [Loading data from text files (including Cloud Storage)](https://beam.apache.org/documentation/programming-guide/#pipeline-io) \n- [Transforms such as ParDo and MapElements`](https://beam.apache.org/documentation/programming-guide/#core-beam-transforms) \n- [Joining and GroupByKey](https://beam.apache.org/documentation/programming-guide/#groupbykey) \n- [BigQuery IO](https://beam.apache.org/documentation/io/built-in/google-bigquery/) \nAfter you review the steps that the code performs, you can run the pipeline.\n### Create a cloud storage bucket\n### Run the pipeline code\n- In the Google Cloud console, open Cloud Shell. [Open Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Set the environment variables for your project and pipeline script```\nexport PROJECT_ID=PROJECT_IDexport REGION=us-central1export DESTINATION_TABLE=recordings_by_artists_dataflowexport DATASET=musicbrainz\n```Replace with the project ID of your Google Cloud project.\n- Make sure that `gcloud` is using the project you created or selected at the beginning of the tutorial:```\ngcloud config set project $PROJECT_ID\n```\n- Following the security principle of least privilege, create a service account for the Dataflow pipeline and grant it only the necessary privileges: the `roles/dataflow.worker` , `roles/bigquery.jobUser` , and the `dataEditor` role on the `musicbrainz` dataset:```\ngcloud iam service-accounts create musicbrainz-dataflowexport SERVICE_ACCOUNT=musicbrainz-dataflow@${PROJECT_ID}.iam.gserviceaccount.comgcloud projects add-iam-policy-binding ${PROJECT_ID} \\\u00a0 \u00a0 --member=serviceAccount:${SERVICE_ACCOUNT} \\\u00a0 \u00a0 --role=roles/dataflow.workergcloud projects add-iam-policy-binding ${PROJECT_ID} \\\u00a0 \u00a0 --member=serviceAccount:${SERVICE_ACCOUNT} \\\u00a0 \u00a0 --role=roles/bigquery.jobUserbq query \u00a0--use_legacy_sql=false \\\u00a0 \u00a0 \"GRANT \\`roles/bigquery.dataEditor\\` ON SCHEMA musicbrainz \u00a0 \u00a0 \u00a0TO 'serviceAccount:${SERVICE_ACCOUNT}'\"\n```\n- Create a bucket for the Dataflow pipeline to use for temporary files, and grant the `musicbrainz-dataflow` service account `Owner` privileges to it:```\nexport DATAFLOW_TEMP_BUCKET=gs://temp-bucket-${PROJECT_ID}gsutil mb -l us ${DATAFLOW_TEMP_BUCKET}gsutil acl ch -u ${SERVICE_ACCOUNT}:O ${DATAFLOW_TEMP_BUCKET}\n```\n- Clone the repository that contains the Dataflow code:```\ngit clone https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample.git\n```\n- Change directory to the sample:```\ncd bigquery-etl-dataflow-sample\n```\n- Compile and run the Dataflow job:```\n./run.sh simple\n```The job should take about 10 minutes to run.\n- To see the progress of the pipeline, in the Google Cloud console, go to the **Dataflow** page. [Go to Dataflow](https://console.cloud.google.com/dataflow) The status of the jobs is shown in the status column. A status of **Succeeded** indicates that the job is complete.\n- (Optional) To see the job graph and details about the steps, click the job name, for example, `etl-into-bigquery-bqetlsimple` .\n- When the job has completed, go to the **BigQuery** page. [Go to BigQuery](https://console.cloud.google.com/bigquery) \n- To run a query on the new table, in the **Query editor** pane, enter the following:```\nSELECT artist_name, artist_gender, artist_area, recording_name, recording_lengthFROM musicbrainz.recordings_by_artists_dataflowWHERE artist_area is NOT NULL\u00a0 \u00a0 \u00a0 AND artist_gender IS NOT NULLLIMIT 1000;\n```The result pane will show a set of results similar to the following:| Row | artist_name | artist_gender | artist_area | recording_name   | recording_length |\n|------:|:--------------|----------------:|--------------:|:-------------------------|-------------------:|\n|  1 | mirin   |    2 |   107 | Sylphia     |    264000 |\n|  2 | mirin   |    2 |   107 | Dependence    |    208000 |\n|  3 | Gaudiburschen |    1 |   81 | Die H\u00e4nde zum Himmel  |    210000 |\n|  4 | Sa4   |    1 |   331 | Ein Tag aus meiner Sicht |    221000 |\n|  5 | Dpat   |    1 |   7326 | Cutthroat    |    249000 |\n|  6 | Dpat   |    1 |   7326 | Deloused     |    178000 |The actual output may differ as the results are not ordered.\n### Cleanse the dataNext, you make a slight change to the Dataflow pipeline so that you can load lookup tables and process them as side inputs, as shown in the following diagram.When you query the resulting BigQuery table, it's difficult to determine from where the artist originates without manually looking up the area numeric ID from the `area` table in the MusicBrainz database. This makes analyzing query results less straightforward than it could be.\nSimilarly, artist genders are shown as IDs, but the entire MusicBrainz gender table consists of only three rows. To fix this, you can add a step in the Dataflow pipeline to use the MusicBrainz `area` and `gender` tables to map the IDs to their proper labels.\nBoth `artist_area` and `artist_gender` tables contain a significantly smaller number of rows than artists or recording data table. The number of elements in the later tables are constrained by the number of geographic areas or genders respectively.\nAs a result, the lookup step uses the Dataflow feature called [side input](https://beam.apache.org/documentation/programming-guide/#side-inputs) .\nSide inputs are loaded as table exports of line-delimited JSON files in the public Cloud Storage bucket containing the musicbrainz dataset, and are used to denormalize the table data in a single step.\n### Review the code that adds side inputs to the pipelineBefore running the pipeline, review the code to get a better understanding of the new steps.\nThis code demonstrates data cleansing with side inputs. The `MusicBrainzTransforms` class provides some added convenience for using side inputs to map foreign key values to labels. The `MusicBrainzTransforms` library provides a method that creates an internal lookup class. The lookup class describes each lookup table and the fields that are replaced with labels and variable length arguments. `keyKey` is the name of the column that contains the key for the lookup and `valueKey` is the name of the column that contains the corresponding label.\n [  src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) \n```\npublic static LookupDescription lookup(\u00a0 \u00a0 String objectName, String keyKey, String valueKey, String... destinationKeys) {\u00a0 return new LookupDescription(objectName, keyKey, valueKey, destinationKeys);}\n```\nEach side input is loaded as a single map object, which is used to look up the corresponding label for an ID.\nFirst, the JSON for the lookup table is initially loaded into `MusicBrainzDataObjects` with an empty namespace and turned into a map from the `Key` column value to the `Value` column value.\n [  src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) \n```\npublic static PCollectionView<Map<Long, String>> loadMapFromText(\u00a0 \u00a0 PCollection<String> text, String name, String keyKey, String valueKey) {\u00a0 // column/Key names are namespaced in MusicBrainzDataObject\u00a0 String keyKeyName = name + \"_\" + keyKey;\u00a0 String valueKeyName = name + \"_\" + valueKey;\u00a0 PCollection<KV<Long, String>> entries =\u00a0 \u00a0 \u00a0 text.apply(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"sideInput_\" + name,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MapElements.into(new TypeDescriptor<KV<Long, String>>() {})\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (String input) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MusicBrainzDataObject object = JSONReader.readObject(name, input);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Long key = (Long) object.getColumnValue(keyKeyName);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String value = (String) object.getColumnValue(valueKeyName);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return KV.of(key, value);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }));\u00a0 return entries.apply(View.asMap());}\n```\nEach of these `Map` objects are put into a `Map` by the value of its `destinationKey` , which is the key to replace with the looked up values.\n [  src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) \n```\nList<SimpleEntry<List<String>, PCollectionView<Map<Long, String>>>> mapSideInputs =\u00a0 \u00a0 new ArrayList<>();for (LookupDescription mapper : mappers) {\u00a0 PCollectionView<Map<Long, String>> mapView =\u00a0 \u00a0 \u00a0 loadMap(text.getPipeline(), mapper.objectName, mapper.keyKey, mapper.valueKey);\u00a0 List<String> destKeyList =\u00a0 \u00a0 \u00a0 mapper.destinationKeys.stream()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .map(destinationKey -> name + \"_\" + destinationKey)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .collect(Collectors.toList());\u00a0 mapSideInputs.add(new SimpleEntry<>(destKeyList, mapView));}\n```\nThen, while transforming the artist objects from JSON, the value of the `destinationKey` (which starts out as a number) is replaced with its label.\n [  src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) \n```\nMap<Long, String> sideInputMap = c.sideInput(mapping.getValue());List<String> keyList = mapping.getKey();keyList.forEach(\u00a0 \u00a0 (String key) -> {\u00a0 \u00a0 \u00a0 Long id = (Long) result.getColumnValue(key);\u00a0 \u00a0 \u00a0 if (id != null) {\u00a0 \u00a0 \u00a0 \u00a0 String label = sideInputMap.get(id);\u00a0 \u00a0 \u00a0 \u00a0 if (label == null) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 label = \"\" + id;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 result.replace(key, label);\n```\nTo add the decoding of the `artist_area` and `artist_gender` fields, complete the following steps:- In Cloud Shell, ensure the environment is set up for the pipeline script:```\nexport PROJECT_ID=PROJECT_IDexport REGION=us-central1export DESTINATION_TABLE=recordings_by_artists_dataflow_sideinputsexport DATASET=musicbrainzexport DATAFLOW_TEMP_BUCKET=gs://temp-bucket-${PROJECT_ID}export SERVICE_ACCOUNT=musicbrainz-dataflow@${PROJECT_ID}.iam.gserviceaccount.com\n```Replace with the project ID of your Google Cloud project.\n- Run the pipeline to create the table with decoded area and artist gender:```\n./run.sh simple-with-lookups\n```\n- As before, to see the progress of the pipeline, go to the **Dataflow** page. [Go to Dataflow](https://console.cloud.google.com/dataflow) The pipeline will take approx 10 minutes to complete.\n- When the job has completed, go to the **BigQuery** page. [Go to BigQuery](https://console.cloud.google.com/bigquery) \n- Perform the same query that includes `artist_area` and `artist_gender` :```\nSELECT artist_name, artist_gender, artist_area, recording_name, recording_length\u00a0 FROM musicbrainz.recordings_by_artists_dataflow_sideinputs\u00a0WHERE artist_area is NOT NULL\u00a0 \u00a0AND artist_gender IS NOT NULL\u00a0LIMIT 1000;\n```In the output, the `artist_area` and `artist_gender` are now decoded:| Row | artist_name | artist_gender | artist_area | recording_name   | recording_length |\n|------:|:--------------|:----------------|:--------------|:-------------------------|-------------------:|\n|  1 | mirin   | Female   | Japan   | Sylphia     |    264000 |\n|  2 | mirin   | Female   | Japan   | Dependence    |    208000 |\n|  3 | Gaudiburschen | Male   | Germany  | Die H\u00e4nde zum Himmel  |    210000 |\n|  4 | Sa4   | Male   | Hamburg  | Ein Tag aus meiner Sicht |    221000 |\n|  5 | Dpat   | Male   | Houston  | Cutthroat    |    249000 |\n|  6 | Dpat   | Male   | Houston  | Deloused     |    178000 |The actual output may differ, because the results are not ordered.\n### Optimize the BigQuery schemaIn the final part of this tutorial, you run a pipeline that generates a more optimal table schema using nested fields.\nTake a moment to review the code that is used to generate this optimized version of the table.\nThe following diagram shows a slightly different Dataflow pipeline that nests the artist's recordings within each artist row, rather than creating duplicate artist rows.The current representation of the data is fairly flat. That is, it includes one row per credited recording that includes all the artist's metadata from the BigQuery schema, and all the recording and `artist_credit_name` metadata. This flat representation has at least two drawbacks:- It repeats the`artist`metadata for every recording credited to an artist, which in turn increases the storage required.\n- When you export the data as JSON, it exports an array that repeats that data, instead of an artist with the nested recording data \u2014 which is probably what you want.\nWithout any performance penalty and without using additional storage, instead of storing one recording per row, you can store recordings as a repeated field in each artist record by making some changes to the Dataflow pipeline.\nInstead of joining the recordings with their artist information by `artist_credit_name.artist` , this alternate pipeline creates a nested list of recordings within an artist object.\n [  src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) \n```\npublic static PCollection<MusicBrainzDataObject> nest(\u00a0 \u00a0 PCollection<KV<Long, MusicBrainzDataObject>> parent,\u00a0 \u00a0 PCollection<KV<Long, MusicBrainzDataObject>> child,\u00a0 \u00a0 String nestingKey) {\u00a0 final TupleTag<MusicBrainzDataObject> parentTag = new TupleTag<MusicBrainzDataObject>() {};\u00a0 final TupleTag<MusicBrainzDataObject> childTag = new TupleTag<MusicBrainzDataObject>() {};\u00a0 PCollection<KV<Long, CoGbkResult>> joinedResult =\u00a0 \u00a0 \u00a0 group(\"nest \" + nestingKey, parent, child, parentTag, childTag);\u00a0 return joinedResult.apply(\u00a0 \u00a0 \u00a0 \"merge join results \" + nestingKey,\u00a0 \u00a0 \u00a0 MapElements.into(new TypeDescriptor<MusicBrainzDataObject>() {})\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .via(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (KV<Long, CoGbkResult> group) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MusicBrainzDataObject parentObject = group.getValue().getOnly(parentTag);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Iterable<MusicBrainzDataObject> children = group.getValue().getAll(childTag);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 List<MusicBrainzDataObject> childList = new ArrayList<>();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 children.forEach(childList::add);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parentObject = parentObject.duplicate();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parentObject.addColumnValue(\"recordings\", childList);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return parentObject;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }));}\n```\nThe BigQuery API has a [maximum row size limit](/bigquery/quotas#query_jobs) of 100\u00a0MB when performing bulk inserts (10\u00a0MB for streaming inserts), so the code limits the number of nested recordings for a given record to 1000 elements to ensure that this limit is not reached. If a given artist has more than 1000 recordings, the code duplicates the row, including the `artist` metadata, and continues nesting the recording data in the duplicate row.\n [  src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java ](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) [View on GitHub](https://github.com/GoogleCloudPlatform/bigquery-etl-dataflow-sample/blob/HEAD/src/main/java/com/google/cloud/bqetl/mbdata/MusicBrainzTransforms.java) \n```\nprivate static List<TableRow> toTableRows(\u00a0 \u00a0 MusicBrainzDataObject mbdo, Map<String, Object> serializableSchema) {\u00a0 TableRow row = new TableRow();\u00a0 List<TableRow> result = new ArrayList<>();\u00a0 Map<String, List<MusicBrainzDataObject>> nestedLists = new HashMap<>();\u00a0 Set<String> keySet = serializableSchema.keySet();\u00a0 /*\u00a0 \u00a0* \u00a0construct a row object without the nested objects\u00a0 \u00a0*/\u00a0 int maxListSize = 0;\u00a0 for (String key : keySet) {\u00a0 \u00a0 Object value = serializableSchema.get(key);\u00a0 \u00a0 Object fieldValue = mbdo.getColumnValue(key);\u00a0 \u00a0 if (fieldValue != null) {\u00a0 \u00a0 \u00a0 if (value instanceof Map) {\u00a0 \u00a0 \u00a0 \u00a0 @SuppressWarnings(\"unchecked\")\u00a0 \u00a0 \u00a0 \u00a0 List<MusicBrainzDataObject> list = (List<MusicBrainzDataObject>) fieldValue;\u00a0 \u00a0 \u00a0 \u00a0 if (list.size() > maxListSize) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 maxListSize = list.size();\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 nestedLists.put(key, list);\u00a0 \u00a0 \u00a0 } else {\u00a0 \u00a0 \u00a0 \u00a0 row.set(key, fieldValue);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\u00a0 /*\u00a0 \u00a0* add the nested objects but break up the nested objects across duplicate rows if nesting\u00a0 \u00a0* limit exceeded\u00a0 \u00a0*/\u00a0 TableRow parent = row.clone();\u00a0 Set<String> listFields = nestedLists.keySet();\u00a0 for (int i = 0; i < maxListSize; i++) {\u00a0 \u00a0 parent = (parent == null ? row.clone() : parent);\u00a0 \u00a0 final TableRow parentRow = parent;\u00a0 \u00a0 nestedLists.forEach(\u00a0 \u00a0 \u00a0 \u00a0 (String key, List<MusicBrainzDataObject> nestedList) -> {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if (nestedList.size() > 0) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if (parentRow.get(key) == null) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parentRow.set(key, new ArrayList<TableRow>());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 @SuppressWarnings(\"unchecked\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 List<TableRow> childRows = (List<TableRow>) parentRow.get(key);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 @SuppressWarnings(\"unchecked\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Map<String, Object> map = (Map<String, Object>) serializableSchema.get(key);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 childRows.add(toChildRow(nestedList.remove(0), map));\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 });\u00a0 \u00a0 if ((i > 0) && (i % BIGQUERY_NESTING_LIMIT == 0)) {\u00a0 \u00a0 \u00a0 result.add(parent);\u00a0 \u00a0 \u00a0 parent = null;\u00a0 \u00a0 }\u00a0 }\u00a0 if (parent != null) {\u00a0 \u00a0 result.add(parent);\u00a0 }\u00a0 return result;}\n```\nThe diagram shows the sources, transformations, and sinks of the pipeline.\n \nIn most cases, the step names are supplied in code as part of the `apply` method call.\nTo create this optimized pipeline, complete the following steps:- In Cloud Shell, ensure the environment is set up for the pipeline script:```\nexport PROJECT_ID=PROJECT_IDexport REGION=us-central1export DESTINATION_TABLE=recordings_by_artists_dataflow_nestedexport DATASET=musicbrainzexport DATAFLOW_TEMP_BUCKET=gs://temp-bucket-${PROJECT_ID}export SERVICE_ACCOUNT=musicbrainz-dataflow@${PROJECT_ID}.iam.gserviceaccount.com\n```\n- Run the pipeline to nest recording rows within artist rows:```\n./run.sh nested\n```\n- As before, to see the progress of the pipeline, go to the **Dataflow** page. [Go to Dataflow](https://console.cloud.google.com/dataflow) The pipeline will take approx 10 minutes to complete.\n- When the job has completed, go to the **BigQuery** page. [Go to BigQuery](https://console.cloud.google.com/bigquery) \n- Query fields from the nested table in BigQuery:```\nSELECT artist_name, artist_gender, artist_area, artist_recordingsFROM musicbrainz.recordings_by_artists_dataflow_nestedWHERE artist_area IS NOT NULL\u00a0 \u00a0 \u00a0 AND artist_gender IS NOT NULLLIMIT 1000;\n```In the output, the `artist_recordings` are shown as nested rows that can be expanded:| Row | artist_name | artist_gender | artist_area | artist_recordings |\n|------:|:--------------|:----------------|:--------------|:--------------------|\n|  1 | mirin   | Female   | Japan   | (5 rows)   |\n|  3 | Gaudiburschen | Male   | Germany  | (1 row)    |\n|  4 | Sa4   | Male   | Hamburg  | (10 rows)   |\n|  6 | Dpat   | Male   | Houston  | (9 rows)   |The actual output may differ as the results are not ordered.\n- Run a query to extract values from the `STRUCT` and use those values to filter the results, for example for artists who have recordings containing the word \"Justin\":```\nSELECT artist_name,\u00a0 \u00a0 \u00a0 \u00a0artist_gender,\u00a0 \u00a0 \u00a0 \u00a0artist_area,\u00a0 \u00a0 \u00a0 \u00a0ARRAY(SELECT artist_credit_name_name\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0FROM UNNEST(recordings_by_artists_dataflow_nested.artist_recordings)) AS artist_credit_name_name,\u00a0 \u00a0 \u00a0 \u00a0ARRAY(SELECT recording_name\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0FROM UNNEST(recordings_by_artists_dataflow_nested.artist_recordings)) AS recording_name\u00a0FROM musicbrainz.recordings_by_artists_dataflow_nested,\u00a0 \u00a0 \u00a0 UNNEST(recordings_by_artists_dataflow_nested.artist_recordings) AS artist_recordings_structWHERE artist_recordings_struct.recording_name LIKE \"%Justin%\"LIMIT 1000;\n```In the output, the `artist_credit_name_name` and `recording_name` are shown as nested rows that can be expanded, for example:| Row | artist_name  | artist_gender | artist_area | artist_credit_name_name | recording_name         |\n|:------|:------------------|:----------------|:--------------|:--------------------------|:-----------------------------------------------|\n| 1  | Damonkenutz  | nan    | nan   | (1 row)     | 1\u00a0Yellowpants\u00a0(Justin\u00a0Martin\u00a0remix)   |\n| 3  | Fabian   | Male   | Germany  | (10+ rows)    | 1\u00a0Heatwave          |\n| .  | nan    | nan    | nan   | nan      | 2\u00a0Starlight\u00a0Love        |\n| .  | nan    | nan    | nan   | nan      | 3\u00a0Dreams\u00a0To\u00a0Wishes        |\n| .  | nan    | nan    | nan   | nan      | 4\u00a0Last\u00a0Flight\u00a0(Justin\u00a0Faust\u00a0remix)    |\n| .  | nan    | nan    | nan   | nan      | ...           |\n| 4  | Digital Punk Boys | nan    | nan   | (6 rows)     | 1\u00a0Come\u00a0True         |\n| .  | nan    | nan    | nan   | nan      | 2\u00a0We\u00a0Are...\u00a0(Punkgirlz\u00a0remix\u00a0by\u00a0Justin\u00a0Famous) |\n| .  | nan    | nan    | nan   | nan      | 3\u00a0Chaos\u00a0(short\u00a0cut)       |\n| .  | nan    | nan    | nan   | nan      | ...           |The actual output may differ as the results are not ordered.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Deleting individual resourcesFollow these steps to delete individual resources, instead of deleting the whole project.\n- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets](https://console.cloud.google.com/storage/browser) \n- Click the checkbox for the bucket that you want to delete.\n- To delete the bucket,  clickdelete **Delete** , and then follow the  instructions.- Open the BigQuery web UI. [Open BIGQUERY](https://bigquery.cloud.google.com/) \n- Select the BigQuery datasets you created during the tutorial.\n- Click **Delete** .\n## What's next\n- Learn more about writing queries for BigQuery. [Querying data](/bigquery/querying-data) explains how to run synchronous and asynchronous queries, create user-defined functions (UDFs), and more.\n- Explore BigQuery syntax. BigQuery uses a SQL-like syntax that is described in the [Query reference (legacy SQL)](/bigquery/query-reference) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Docs"}