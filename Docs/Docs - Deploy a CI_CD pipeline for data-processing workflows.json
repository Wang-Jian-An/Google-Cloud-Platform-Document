{"title": "Docs - Deploy a CI/CD pipeline for data-processing workflows", "url": "https://cloud.google.com/architecture/cicd-pipeline-for-data-processing/deployment", "abstract": "# Docs - Deploy a CI/CD pipeline for data-processing workflows\nLast reviewed 2023-05-12 UTC\nThis document describes how you deploy the architecture in [Use a CI/CD pipeline for data-processing workflows](/architecture/cicd-pipeline-for-data-processing) .\nThis deployment is intended for data scientists and analysts who build recurrent running data-processing jobs to help structure their research and development (R&D) to systematically and automatically maintain data-processing workloads.\nData scientists and analysts can adapt the methodologies from CI/CD practices to help to ensure high quality, maintainability, and adaptability of the data processes and workflows. The methods that you can apply are as follows:\n- Version control of source code.\n- Automatic building, testing, and deployment of apps.\n- Environment isolation and separation from production.\n- Replicable procedures for environment setup.", "content": "## Architecture\nThe following diagram shows a detailed view of the CI/CD pipeline steps for both the test pipeline and the production pipeline.\nIn the preceding diagram, the test pipeline begins when a developer commits code changes to the Cloud Source Repositories and ends when the data-processing workflow integration test passes. At that point, the pipeline publishes a message to Pub/Sub which contains a reference to the latest self-executing Java Archive (JAR) file (obtained from the Airflow variables) in the message\u2019s data field.\nIn the preceding diagram, the production pipeline begins when a message is published to a Pub/Sub topic and ends when the production workflow DAG file is deployed to Cloud Composer.\nIn this deployment guide, you use the following Google Cloud products:\n- [Cloud Build](/build) to create a CI/CD pipeline for building, deploying, and testing a data-processing workflow, and the data processing itself. Cloud Build is a managed service that runs your build on Google Cloud. A build is a series of build steps where each step is run in a Docker container.\n- [Cloud Composer](/composer) to define and run the steps of the workflow, such as starting the data processing, testing and verifying results. Cloud Composer is a managed [Apache Airflow](https://airflow.apache.org/) service, which offers an environment where you can create, schedule, monitor, and manage complex workflows, such as the data-processing workflow in this deployment.\n- [Dataflow](/dataflow) to run the Apache Beam [WordCount](https://beam.apache.org/get-started/wordcount-example/#wordcount-example) example as a sample data process.## Objectives\n- Configure the Cloud Composer environment.\n- Create Cloud Storage buckets for your data.\n- Create the build, test, and production pipelines.\n- Configure the build trigger.## Cost optimization\nIn this document, you use the following billable components of Google Cloud:\n- [Cloud Source Repositories](/source-repositories/pricing) \n- [Cloud Build](/build/pricing) \n- [Cloud Composer](/composer/pricing) \n- [Dataflow](/dataflow/pricing) \n- [Cloud Storage](/storage/pricing) ## Before you begin\n- In the Google Cloud console, on the project selector page, [select or create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . **Note:** If you don't plan to keep the resources that you create in this deployment, create a project instead of selecting an existing project. After you finish this deployment, you can delete the project to remove all resources that are associated with the project. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- Make sure that billing is enabled for your Google Cloud project. Learn how to [check if billing is enabled on a project](/billing/docs/how-to/verify-billing-enabled) .## Sample code\nThe sample code for this deployment is in two folders:\n- The`env-setup`folder contains shell scripts for the initial setup of the Google Cloud environment.\n- The `source-code` folder contains code that is developed over time, needs to be source controlled, and triggers automatic build and test processes. This folder contains the following subfolders:- The`data-processing-code`folder contains the Apache Beam process source code.\n- The`workflow-dag`folder contains the composer DAG definitions for the data-processing workflows with the steps to design, implement, and test the Dataflow process.\n- The`build-pipeline`folder contains two Cloud Build configurations\u2014one for the test pipeline and the other for the production pipeline. This folder also contains a support script for the pipelines.For this deployment, the source code files for data processing and for DAG workflow are in different folders in the same source code repository. In a production environment, the source code files are usually in their own source code repositories and are managed by different teams.\n## Integration and unit tests\nIn addition to the integration test that verifies the data-processing workflow from end to end, there are two unit tests in this deployment. The unit tests are automatic tests on the data-processing code and the data-processing workflow code. The test on the data-processing code is written in Java and runs automatically during the Maven build process. The test on the data-processing workflow code is written in Python and runs as an independent build step.\n## Set up your environment\nIn this deployment, you run all commands in [Cloud Shell](/shell/docs/features) . Cloud Shell appears as a window at the bottom of the Google Cloud console.\n- In the Google Cloud console, open Cloud Shell: [Open Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Clone the sample code repository:```\ngit clone https://github.com/GoogleCloudPlatform/ci-cd-for-data-processing-workflow.git\n```\n- Run a script to set environment variables:```\ncd ~/ci-cd-for-data-processing-workflow/env-setupsource set_env.sh\n```The script sets the following environment variables:- Your Google Cloud project ID\n- Your region and zone\n- The name of your Cloud Storage buckets that are used by the build pipeline and the data-processing workflow.\nBecause environment variables aren't retained between sessions, if your Cloud Shell session shuts down or disconnects while you're working through this deployment, you need to reset the environment variables.## Create the Cloud Composer environment\nIn this deployment, you set up a Cloud Composer environment.\n- In Cloud Shell, add the **Cloud Composer v2 API Service Agent Extension** ( `roles/composer.ServiceAgentV2Ext` ) role to the Cloud Composer Service Agent account:```\ngcloud projects add-iam-policy-binding $GCP_PROJECT_ID \\\u00a0 \u00a0 --member serviceAccount:service-$PROJECT_NUMBER@cloudcomposer-accounts.iam.gserviceaccount.com \\\u00a0 \u00a0 --role roles/composer.ServiceAgentV2Ext\n```\n- In Cloud Shell, create the Cloud Composer environment:```\ngcloud composer environments create $COMPOSER_ENV_NAME \\\u00a0 \u00a0 --location $COMPOSER_REGION \\\u00a0 \u00a0 --image-version composer-2.0.14-airflow-2.2.5\n``` **Note:** It usually takes about 15 minutes to provision the Cloud Composer environment, but it can take up to one hour. Wait until the process completes before continuing.\n- Run a script to set the variables in the Cloud Composer environment. The variables are needed for the data-processing DAGs.```\ncd ~/ci-cd-for-data-processing-workflow/env-setupchmod +x set_composer_variables.sh./set_composer_variables.sh\n```The script sets the following environment variables:- Your Google Cloud project ID\n- Your region and zone\n- The name of your Cloud Storage buckets that are used by the build pipeline and the data-processing workflow.\n### Extract the Cloud Composer environment properties\nCloud Composer uses a Cloud Storage bucket to store DAGs. Moving a DAG definition file to the bucket triggers Cloud Composer to automatically read the files. You created the Cloud Storage bucket for Cloud Composer when you created the Cloud Composer environment. In the following procedure, you extract the URL for the buckets, and then configure your CI/CD pipeline to automatically deploy DAG definitions to the Cloud Storage bucket.\n- In Cloud Shell, export the URL for the bucket as an environment variable:```\nexport COMPOSER_DAG_BUCKET=$(gcloud composer environments describe $COMPOSER_ENV_NAME \\\u00a0 \u00a0 --location $COMPOSER_REGION \\\u00a0 \u00a0 --format=\"get(config.dagGcsPrefix)\")\n```\n- Export the name of the service account that Cloud Composer uses in order to have access to the Cloud Storage buckets:```\nexport COMPOSER_SERVICE_ACCOUNT=$(gcloud composer environments describe $COMPOSER_ENV_NAME \\\u00a0 \u00a0 --location $COMPOSER_REGION \\\u00a0 \u00a0 --format=\"get(config.nodeConfig.serviceAccount)\")\n```## Create the Cloud Storage buckets\nIn this section you create a set of Cloud Storage buckets to store the following:\n- Artifacts of the intermediate steps of the build process.\n- The input and output files for the data-processing workflow.\n- The staging location for the Dataflow jobs to store their binary files.\nTo create the Cloud Storage buckets, complete the following step:\n- In Cloud Shell, create Cloud Storage buckets and give the Cloud Composer service account permission to run the data-processing workflows:```\ncd ~/ci-cd-for-data-processing-workflow/env-setupchmod +x create_buckets.sh./create_buckets.sh\n```## Create the Pub/Sub topic\nIn this section, you create a Pub/Sub topic to receive messages sent from the data-processing workflow integration test in order to automatically trigger the production build pipeline.\n- In the Google Cloud console, go to the **Pub/Sub topics** page. [Go to Topics page](https://console.cloud.google.com/cloudpubsub/topic/list) \n- Click **Create Topic** .\n- To configure the topic, complete the following steps:- For the **Topic ID** , enter`integration-test-complete-topic`.\n- Confirm the **Add a default subscription** option is checked.\n- Leave the remaining options clear.\n- For **Encryption** , select **Google-managed encryption key** .\n- Click **Create Topic** .\n## Push the source code to Cloud Source Repositories\nIn this deployment, you have one source codebase that you need to put into version control. The following step shows how a codebase is developed and changes over time. Whenever changes are pushed to the repository, the pipeline to build, deploy, and test is triggered.\n- In Cloud Shell, push the `source-code` folder to Cloud Source Repositories:```\ngcloud source repos create $SOURCE_CODE_REPOcp -r ~/ci-cd-for-data-processing-workflow/source-code ~/$SOURCE_CODE_REPOcd ~/$SOURCE_CODE_REPOgit initgit remote add google \\\u00a0 \u00a0 https://source.developers.google.com/p/$GCP_PROJECT_ID/r/$SOURCE_CODE_REPOgit add .git commit -m 'initial commit'git push google master\n```These are standard commands to initialize Git in a new directory and push the content to a remote repository.## Create Cloud Build pipelines\nIn this section, you create the build pipelines that build, deploy, and test the data-processing workflow.\n### Grant access to Cloud Build service account\nCloud Build deploys Cloud Composer DAGs and triggers workflows, which are enabled when you add additional access to the Cloud Build service account. For more information about the different roles available when working with Cloud Composer, see the [access control documentation](/composer/docs/how-to/access-control#roles) .\n- In Cloud Shell, add the `composer.admin` role to the Cloud Build service account so the Cloud Build job can set Airflow variables in Cloud Composer:```\ngcloud projects add-iam-policy-binding $GCP_PROJECT_ID \\\u00a0 \u00a0 --member=serviceAccount:$PROJECT_NUMBER@cloudbuild.gserviceaccount.com \\\u00a0 \u00a0 --role=roles/composer.admin\n```\n- Add the `composer.worker` role to the Cloud Build service account so the Cloud Build job can trigger the data workflow in Cloud Composer:```\ngcloud projects add-iam-policy-binding $GCP_PROJECT_ID \\\u00a0 \u00a0 --member=serviceAccount:$PROJECT_NUMBER@cloudbuild.gserviceaccount.com \\\u00a0 \u00a0 --role=roles/composer.worker\n```\n### Create the build and test pipeline\nThe build and test pipeline steps are configured in the [YAML configuration file](/build/docs/configuring-builds/create-basic-configuration) . In this deployment, you use prebuilt [builder images](/build/docs/cloud-builders) for `git` , `maven` , `gsutil` , and `gcloud` to run the tasks in each build step. You use configuration variable [substitutions](/build/docs/build-config#substitutions) to define the environment settings at build time. The source code repository location is defined by variable substitutions, and the locations of Cloud Storage buckets. The build needs this information to deploy the JAR file, test files, and the DAG definition.\n- In Cloud Shell, submit the build pipeline configuration file to create the pipeline in Cloud Build:```\ncd ~/ci-cd-for-data-processing-workflow/source-code/build-pipelinegcloud builds submit --config=build_deploy_test.yaml --substitutions=\\REPO_NAME=$SOURCE_CODE_REPO,\\_DATAFLOW_JAR_BUCKET=$DATAFLOW_JAR_BUCKET_TEST,\\_COMPOSER_INPUT_BUCKET=$INPUT_BUCKET_TEST,\\_COMPOSER_REF_BUCKET=$REF_BUCKET_TEST,\\_COMPOSER_DAG_BUCKET=$COMPOSER_DAG_BUCKET,\\_COMPOSER_ENV_NAME=$COMPOSER_ENV_NAME,\\_COMPOSER_REGION=$COMPOSER_REGION,\\_COMPOSER_DAG_NAME_TEST=$COMPOSER_DAG_NAME_TEST\n```This command instructs Cloud Build to run a build with the following steps:- Build and deploy the WordCount self-executing JAR file.- Check out the source code.\n- Compile the WordCount Beam source code into a self-executing JAR file.\n- Store the JAR file on Cloud Storage where it can be picked up by Cloud Composer to run the WordCount processing job.\n- Deploy and set up the data-processing workflow on Cloud Composer.- Run the unit test on the custom-operator code used by the workflow DAG.\n- Deploy the test input file and the test reference file on Cloud Storage. The test input file is the input for the WordCount processing job. The test reference file is used as a reference to verify the output of the WordCount processing job.\n- Set the Cloud Composer variables to point to the newly built JAR file.\n- Deploy the workflow DAG definition to the Cloud Composer environment.\n- To trigger the test-processing workflow, run the data-processing workflow in the test environment.\n### Verify the build and test pipeline\nAfter you submit the build file, verify the build steps.\n- In the Google Cloud console, go to the **Build History** page to view a list of all past and currently running builds. [Go to Build History page](https://console.cloud.google.com/cloud-build) \n- Click the build that is running.\n- On the **Build details** page, verify that the build steps match the previously described steps. On the **Build details** page, the **Status** field of the build says `Build successful` when the build finishes.\n- In Cloud Shell, verify that the WordCount sample JAR file was copied into the correct bucket:```\ngsutil ls gs://$DATAFLOW_JAR_BUCKET_TEST/dataflow_deployment*.jar\n```The output is similar to the following:```\ngs://\u2026-composer-dataflow-source-test/dataflow_deployment_e88be61e-50a6-4aa0-beac-38d75871757e.jar\n```\n- Get the URL to your Cloud Composer web interface. Make a note of the URL because it's used in the next step.```\ngcloud composer environments describe $COMPOSER_ENV_NAME \\\u00a0 \u00a0 --location $COMPOSER_REGION \\\u00a0 \u00a0 --format=\"get(config.airflowUri)\"\n```\n- Use the URL from the previous step to go to the Cloud Composer UI to verify a successful DAG run. If the **Runs** column shows no information, wait a few minutes and reload the page.- To verify that the data-processing workflow DAG `test_word_count` is deployed and is in running mode, hold the pointer over the light-green circle by **Runs** and verify that it says **Running** .\n- To see the running data-processing workflow as a graph, click the light-green circle, and then on the **Dag Runs** page, click **DagID: test_word_count** .\n- Reload the **Graph View** page to update the state of the current DAG run. It usually takes between 3-5 minutes for the workflow to finish. To verify that the DAG run finishes successfully, hold the pointer over each task to verify that the tooltip says **State: success** . The second to last task, named `do_comparison` , is the integration test that verifies the process output against the reference file.\n- Once the integration test is complete, the last task, named `publish_test_complete` publishes a message to the `integration-test-complete-topic` Pub/Sub topic which will be used to trigger the production build pipeline.- To verify that the published message contains the correct reference to the latest JAR file, we can pull the message from the default `integration-test-complete-topic-sub` Pub/Sub subscription.\n- In the Google Cloud console, go to the **Subscriptions** page. [Go to Subscriptions page](https://console.cloud.google.com/cloudpubsub/subscription/list) \n- Click **integration-test-complete-topic-sub** , select the **Message** tab and click **Pull** \n- The output should be similar to the following: \n### Create the production pipeline\nWhen the test processing workflow runs successfully, you can promote the current version of the workflow to production. There are several ways to deploy the workflow to production:\n- Manually.\n- Automatically triggered when all the tests pass in the test or staging environments.\n- Automatically triggered by a scheduled job.\nIn this deployment, you automatically trigger the production build when all the tests pass in the test environment. For more information about automated approaches, see [Release Engineering](https://landing.google.com/sre/sre-book/chapters/release-engineering/) .\nBefore you implement the automated approach, you verify the production deployment build by doing a manual deployment to production. The production deployment build follows these steps:\n- Copy the WordCount JAR file from the test bucket to the production bucket.\n- Set the Cloud Composer variables for the production workflow to point to the newly promoted JAR file.\n- Deploy the production workflow DAG definition on the Cloud Composer environment and running the workflow.\nVariable substitutions define the name of the latest JAR file that is deployed to production with the Cloud Storage buckets used by the production processing workflow. To create the Cloud Build pipeline that deploys the production airflow workflow, complete the following steps:\n- In Cloud Shell, read the filename of the latest JAR file by printing the Cloud Composer variable for the JAR filename:```\nexport DATAFLOW_JAR_FILE_LATEST=$(gcloud composer environments run $COMPOSER_ENV_NAME \\\u00a0 \u00a0 --location $COMPOSER_REGION variables get -- \\\u00a0 \u00a0 dataflow_jar_file_test 2>&1 | grep -i '.jar')\n```\n- Use the build pipeline configuration file, `deploy_prod.yaml,` to create the pipeline in Cloud Build:```\ncd ~/ci-cd-for-data-processing-workflow/source-code/build-pipelinegcloud builds submit --config=deploy_prod.yaml --substitutions=\\REPO_NAME=$SOURCE_CODE_REPO,\\_DATAFLOW_JAR_BUCKET_TEST=$DATAFLOW_JAR_BUCKET_TEST,\\_DATAFLOW_JAR_FILE_LATEST=$DATAFLOW_JAR_FILE_LATEST,\\_DATAFLOW_JAR_BUCKET_PROD=$DATAFLOW_JAR_BUCKET_PROD,\\_COMPOSER_INPUT_BUCKET=$INPUT_BUCKET_PROD,\\_COMPOSER_ENV_NAME=$COMPOSER_ENV_NAME,\\_COMPOSER_REGION=$COMPOSER_REGION,\\_COMPOSER_DAG_BUCKET=$COMPOSER_DAG_BUCKET,\\_COMPOSER_DAG_NAME_PROD=$COMPOSER_DAG_NAME_PROD\n```\n### Verify the data-processing workflow created by the production pipeline\n- Get the URL for your Cloud Composer UI:```\ngcloud composer environments describe $COMPOSER_ENV_NAME \\\u00a0 \u00a0 --location $COMPOSER_REGION \\\u00a0 \u00a0 --format=\"get(config.airflowUri)\"\n```\n- To verify that the production data-processing workflow DAG is deployed, go to the URL that you retrieved in the previous step and verify that `prod_word_count` DAG is in the list of DAGs.- On the **DAGs** page, in the **prod_word_count** row, click **Trigger DAG** .\n- To update the DAG run status, click the Airflow logo or reload the page. A light-green circle in the **Runs** column indicates the DAG is running.\n- After the run succeeds, hold the pointer over the dark-green circle near the **DAG runs** column and verify that it says **Success** .\n- In Cloud Shell, list the result files in the Cloud Storage bucket:```\ngsutil ls gs://$RESULT_BUCKET_PROD\n```The output is similar to the following:```\ngs://\u2026-composer-result-prod/output-00000-of-00003\ngs://\u2026-composer-result-prod/output-00001-of-00003\ngs://\u2026-composer-result-prod/output-00002-of-00003\n```\n**Note:** Typically, when the production data workflow job execution is either triggered by events, such as publishing Pub/Sub messages, files being stored in buckets, or is scheduled to run regularly, it's important that the deployment job ensures that production data workflow isn't running before you deploy. In a production environment, you can use [dag_state](https://airflow.apache.org/docs/apache-airflow/stable/usage-cli.html) of the [Airflow CLI commands](/composer/docs/how-to/accessing/airflow-cli) to retrieve the status of a DAG run.\n## Create Cloud Build triggers\nIn this section you create the Cloud Build triggers that link the source code changes to the test build process and between the test pipeline and production build pipeline.\n### Configure the test build pipeline trigger\nYou set up a [Cloud Build trigger](/build/docs/running-builds/automate-builds) that triggers a new build when changes are pushed to the master branch of the source repository.\n- In the Google Cloud console, go to the **Build triggers** page. [Go to Build Triggers page](https://console.cloud.google.com/cloud-build/triggers) \n- Click **Create trigger** .\n- To configure trigger settings, complete the following steps:- In the **Name** field, enter`trigger-build-in-test-environment`.\n- In the **Region** drop-down, select **global (non-regional)** .\n- For **Event** , click **Push to a Branch** .\n- For **Source** , select`data-pipeline-source`.\n- In the **Branch name** field, enter`master`.\n- For **Configuration** , click **Cloud Build configuration\nfile (yaml or json)** .\n- For **Location** , click **Repository** .\n- In the **Cloud Build configuration file location** field, enter`build-pipeline/build_deploy_test.yaml`.\n- In Cloud Shell, run the following command to get all the substitution variables needed for the build. Make a note of these values because they are needed in a later step.```\necho \"_COMPOSER_DAG_BUCKET : ${COMPOSER_DAG_BUCKET}_COMPOSER_DAG_NAME_TEST : ${COMPOSER_DAG_NAME_TEST}_COMPOSER_ENV_NAME : ${COMPOSER_ENV_NAME}_COMPOSER_INPUT_BUCKET : ${INPUT_BUCKET_TEST}_COMPOSER_REF_BUCKET : ${REF_BUCKET_TEST}_COMPOSER_REGION : ${COMPOSER_REGION}_DATAFLOW_JAR_BUCKET : ${DATAFLOW_JAR_BUCKET_TEST}\"\n``` **Note:** the output name/value pair is used for the following step.\n- On the **Trigger settings** page, in **Advanced, Substitution variables** , replace the variables with values from your environment that you got from the previous step. Add the following one at a time and click **+ Add item** for each of the name-value pairs.- `_COMPOSER_DAG_BUCKET`\n- `_COMPOSER_DAG_NAME_TEST`\n- `_COMPOSER_ENV_NAME`\n- `_COMPOSER_INPUT_BUCKET`\n- `_COMPOSER_REF_BUCKET`\n- `_COMPOSER_REGION`\n- `_DATAFLOW_JAR_BUCKET` \n- Click **Create** .\n### Configure the production build pipeline trigger\nYou set up a Cloud Build trigger that triggers a production build when the tests have passed in the test environment and a message is published to the `tests-complete` Pub/Sub topic. This trigger includes an approval step where the build needs to be manually approved before the production pipeline is run.\n- In the Google Cloud console, go to the **Build triggers** page. [Go to Build Triggers page](https://console.cloud.google.com/cloud-build/triggers) \n- Click **Create trigger** .\n- To configure trigger settings, complete the following steps:- In the **Name** field, enter`trigger-build-in-prod-environment`.\n- In the **Region** drop-down, select **global (non-regional)** .\n- For **Event** , click **Pub/Sub message** .\n- For **Subscription** , select **integration-test-complete-topic** .\n- For **Source** , select`data-pipeline-source`.\n- For **Revision** , select **Branch** .\n- In the **Branch name** field, enter`master`.\n- For **Configuration** , click **Cloud Build configuration\nfile (yaml or json)** .\n- For **Location** , click **Repository** .\n- In the **Cloud Build configuration file location** field, enter`build-pipeline/deploy_prod.yaml`.\n- In Cloud Shell, run the following command to get all the substitution variables needed for the build. Make a note of these values because they are needed in a later step.```\necho \"_COMPOSER_DAG_BUCKET : ${COMPOSER_DAG_BUCKET}_COMPOSER_DAG_NAME_PROD : ${COMPOSER_DAG_NAME_PROD}_COMPOSER_ENV_NAME : ${COMPOSER_ENV_NAME}_COMPOSER_INPUT_BUCKET : ${INPUT_BUCKET_PROD}_COMPOSER_REGION : ${COMPOSER_REGION}_DATAFLOW_JAR_BUCKET_PROD : ${DATAFLOW_JAR_BUCKET_PROD}_DATAFLOW_JAR_BUCKET_TEST : ${DATAFLOW_JAR_BUCKET_TEST}\"\n``` **Note:** the output name/value pair is used for the following step.\n- On the **Trigger settings** page, in **Advanced, Substitution variables** , replace the variables with values from your environment that you got from the previous step. Add the following one at a time and click **+ Add item** for each of the name-value pairs.- `_COMPOSER_DAG_BUCKET`\n- `_COMPOSER_DAG_NAME_PROD`\n- `_COMPOSER_ENV_NAME`\n- `_COMPOSER_INPUT_BUCKET`\n- `_COMPOSER_REGION`\n- `_DATAFLOW_JAR_BUCKET_PROD`\n- `_DATAFLOW_JAR_BUCKET_TEST`\n- `_DATAFLOW_JAR_FILE_LATEST = $(body.message.data)` \n- For **Approval** , select **Require approval before build executes** .\n- Click **Create** .\n### Test the triggers\nTo test the trigger, you add a new word to the test input file and make the corresponding adjustment to the test reference file. You verify that the build pipeline is triggered by a commit push to Cloud Source Repositories and that the data-processing workflow runs correctly with the updated test files.\n- In Cloud Shell, add a test word at the end of the test file:```\necho \"testword\" >> \u00a0~/$SOURCE_CODE_REPO/workflow-dag/support-files/input.txt\n```\n- Update the test result reference file, `ref.txt` , to match the changes done in the test input file:```\necho \"testword: 1\" >> \u00a0~/$SOURCE_CODE_REPO/workflow-dag/support-files/ref.txt\n```\n- Commit and push changes to Cloud Source Repositories:```\ncd ~/$SOURCE_CODE_REPOgit add .git commit -m 'change in test files'git push google master\n```\n- In the Google Cloud console, go to the **History** page. [GO TO HISTORY PAGE](https://console.cloud.google.com/cloud-build) \n- To verify that a new test build is triggered by the previous push to master branch, on the current running build, the **Ref** column says **master** .\n- In Cloud Shell, get the URL for your Cloud Composer web interface:```\ngcloud composer environments describe $COMPOSER_ENV_NAME \\\u00a0 \u00a0 --location $COMPOSER_REGION --format=\"get(config.airflowUri)\"\n```\n- After the build finishes, go to the URL from the previous command to verify that the `test_word_count` DAG is running.Wait until the DAG run finishes, which is indicated when the light green circle in the **DAG runs** column goes away. It usually takes 3-5 minutes for the process to finish.\n- In Cloud Shell, download the test result files:```\nmkdir ~/result-downloadcd ~/result-downloadgsutil cp gs://$RESULT_BUCKET_TEST/output* .\n```\n- Verify that the newly added word is in one of the result files:```\ngrep testword output*\n```The output is similar to the following:```\noutput-00000-of-00003:testword: 1\n```\n- In the Google Cloud console, go to the **History** page. [Go to History page](https://console.cloud.google.com/cloud-build) \n- Verify that a new production build has been triggered by the completion of the integration test and that the build is awaiting approval.\n- To run the production build pipeline, select the checkbox next to the build, click **Approve** , and then click **Approve** in the confirmation box.\n- After the build finishes, go to the URL from the previous command and trigger the `prod_word_count` DAG manually to run the production pipeline.## Clean up\nThe following sections explain how you can avoid future charges for your Google Cloud project and the Apache Hive and Dataproc resources that you used in this deployment.\n### Delete the Google Cloud project\nTo avoid incurring charges to your Google Cloud account for the resources used in this deployment, you can delete the Google Cloud project.\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Delete the individual resources\nIf you want to keep the project used for this deployment, run the following steps to delete the resources you created.\n- To delete the Cloud Build trigger, complete the following steps:- In the Google Cloud console, go to the **Triggers** page. [Go to Triggers page](https://console.cloud.google.com/cloud-build/triggers) \n- Next to the triggers that you created, click **More** , and then click **Delete** .\n- In Cloud Shell, delete the Cloud Composer environment:```\ngcloud -q composer environments delete $COMPOSER_ENV_NAME \\\u00a0 \u00a0 --location $COMPOSER_REGION\n```\n- Delete the Cloud Storage buckets and their files:```\ngsutil -m rm -r gs://$DATAFLOW_JAR_BUCKET_TEST \\\u00a0 \u00a0 gs://$INPUT_BUCKET_TEST \\\u00a0 \u00a0 gs://$REF_BUCKET_TEST \\\u00a0 \u00a0 gs://$RESULT_BUCKET_TEST \\\u00a0 \u00a0 gs://$DATAFLOW_STAGING_BUCKET_TEST \\\u00a0 \u00a0 gs://$DATAFLOW_JAR_BUCKET_PROD \\\u00a0 \u00a0 gs://$INPUT_BUCKET_PROD \\\u00a0 \u00a0 gs://$RESULT_BUCKET_PROD \\\u00a0 \u00a0 gs://$DATAFLOW_STAGING_BUCKET_PROD\n```\n- To delete the Pub/Sub topic and default subscription, run the following commands in Cloud Shell:```\ngcloud pubsub topics delete integration-test-complete-topicgcloud pubsub subscriptions delete integration-test-complete-topic-sub\n```\n- Delete the repository:```\ngcloud -q source repos delete $SOURCE_CODE_REPO\n```\n- Delete the files and folder you created:```\nrm -rf ~/ci-cd-for-data-processing-workflowrm -rf ~/$SOURCE_CODE_REPOrm -rf ~/result-download\n```\n## What's next\n- Learn more about [GitOps-style continuous delivery with Cloud Build](/kubernetes-engine/docs/tutorials/gitops-cloud-build) .\n- Learn how to [Use a CI/CD pipeline for data-processing workflows](/architecture/cicd-pipeline-for-data-processing) .\n- Learn more about [Common Dataflow use-case patterns](/blog/products/gcp/guide-to-common-cloud-dataflow-use-case-patterns-part-1) .\n- Learn more about [Release Engineering](https://landing.google.com/sre/sre-book/chapters/release-engineering/) .\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Docs"}