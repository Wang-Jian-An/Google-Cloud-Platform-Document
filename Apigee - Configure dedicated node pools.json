{"title": "Apigee - Configure dedicated node pools", "url": "https://cloud.google.com/apigee/docs/api-platform/get-started/what-apigee", "abstract": "# Apigee - Configure dedicated node pools\nYou are currently viewing version 1.1 of the Apigee hybrid documentation. **This version is end of life.** You should upgrade to a newer version. For more information, see [Supported versions](/apigee/docs/hybrid/supported-platforms#supported-versions) .\n", "content": "## \n About node pools\nA [node pools](https://cloud.google.com/kubernetes-engine/docs/concepts/node-pools) is a group of nodes within a cluster that all have the same configuration. Typically, you define separate node pools when you have pods with differing resource requirements. For example, the `apigee-cassandra` pods require persistent storage, while the other Apigee hybrid pods do not.\nThis topic discusses how to configure dedicated node pools for a hybrid installation.\n## \n Using the default nodeSelectors\nThe best practice is to set up two dedicated node pools: one for the Cassandra pods and one for all the other runtime pods. Using default [nodeSelector](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector) configurations, the installer will assign the Cassandra pods to a node pool named `apigee-data` and all the other pods to a node pool named `apigee-runtime` . All you have to do is create node pools with these names, and Apigee hybrid handles the pod scheduling details for you:\n| Default node pool name | Description   |\n|:-------------------------|:-----------------------|\n| apigee-data    | A stateful node pool. |\n| apigee-runtime   | A stateless node pool. |\nFollowing is the default `nodeSelector` configuration. The `apigeeData` property specifies a node pool for the Cassandra pods. The `apigeeRuntime` specifies the node pool for all the other pods. You can override these default settings in your overrides file, as explained later in this topic:\n```\nnodeSelector:\n requiredForScheduling: false\n apigeeRuntime:\n key: \"cloud.google.com/gke-nodepool\"\n value: \"apigee-runtime\"\n apigeeData:\n key: \"cloud.google.com/gke-nodepool\"\n value: \"apigee-data\"\n```\nAgain, to ensure your pods are scheduled on the correct nodes, all you have to do is create two node pools with the names `apigee-data` and `apigee-runtime` .\n## \n The requiredForScheduling property\nThe `nodeSelector` config section has a property called `requiredForScheduling` :\n```\nnodeSelector:\n requiredForScheduling: false\n apigeeRuntime:\n key: \"cloud.google.com/gke-nodepool\"\n value: \"apigee-runtime\"\n apigeeData:\n key: \"cloud.google.com/gke-nodepool\"\n value: \"apigee-data\"\n```\n`false`\n`apigee-runtime`\n`apigee-data`\nIf you set `requiredForScheduling` to `true` , the installation will fail unless there are node pools that match the configured `nodeSelector` keys and values.\nNOTE: The best practice is to set this value to`requiredForScheduling:true`for a  production environment.\n## \n Using custom node pool names\nIf you don't want to use node pools with the default names, you can create node pools with custom names and specify those names in the `nodeSelector` stanza. For example, the following configuration assigns the Cassandra pods to the pool named `my-cassandra-pool` and all other pods to the pool named `my-runtime-pool` :\n```\nnodeSelector:\n requiredForScheduling: false\n apigeeRuntime:\n key: \"cloud.google.com/gke-nodepool\"\n value: \"my-runtime-pool\"\n apigeeData:\n key: \"cloud.google.com/gke-nodepool\"\n value: \"my-cassandra-pool\"\n```\n## \n Overriding the node pool for specific components on GKE\nYou can also override node pool configurations at the individual component level. For example, the following configuration assigns the node pool with the value `apigee-custom` to the `runtime` component:\n```\nruntime:\n nodeSelector:\n key: cloud.google.com/gke-nodepool\n value: apigee-custom\n```\nYou can specify a custom node pool on any of these components:\n- `istio`\n- `mart`\n- `synchronizer`\n- `runtime`\n- `cassandra`\n- `udca`\n- `logger`## GKE node pool configuration\nIn GKE, node pools must have a unique name that you provide when you create the pools, and GKE automatically labels each node with the following:\n```\ncloud.google.com/gke-nodepool=the_node_pool_name\n```\nAs long as you create node pools named `apigee-data` and `apigee-runtime` , no further configuration is required. If you want to use custom node names, see [Using custom node pool names](#using-custom-node-pool-names) .\n## \n Anthos node pool configuration\nApigee hybrid currently is only supported on Anthos 1.1.1. This version of Anthos does not support the node pool feature; therefore, you must manually label the worker nodes as explained below. Perform the following steps once your hybrid cluster is up and running:\n- Run the following command to get a list of the worker nodes in your cluster:```\nkubectl -n apigee get nodes\n```Example output:```\nNAME     STATUS ROLES AGE  VERSION\napigee-092d639a-4hqt Ready  7d  v1.14.6-gke.2\napigee-092d639a-ffd0 Ready  7d  v1.14.6-gke.2\napigee-109b55fc-5tjf Ready  7d  v1.14.6-gke.2\napigee-c2a9203a-8h27 Ready  7d  v1.14.6-gke.2\napigee-c70aedae-t366 Ready  7d  v1.14.6-gke.2\napigee-d349e89b-hv2b Ready  7d  v1.14.6-gke.2\n```\n- Label each node to differentiate between runtime nodes and data nodes.Be sure to choose the nodes so that they are equally distributed among availability zones (AZs).Use this command to label the nodes:```\nkubectl label node node_name key=value\n```For example:```\n$ kubectl label node apigee-092d639a-4hqt apigee.com/apigee-nodepool=apigee-runtime\n$ kubectl label node apigee-092d639a-ffd0 apigee.com/apigee-nodepool=apigee-runtime\n$ kubectl label node apigee-109b55fc-5tjf apigee.com/apigee-nodepool=apigee-runtime\n$ kubectl label node apigee-c2a9203a-8h27 apigee.com/apigee-nodepool=apigee-data\n$ kubectl label node apigee-c70aedae-t366 apigee.com/apigee-nodepool=apigee-data\n$ kubectl label node apigee-d349e89b-hv2b apigee.com/apigee-nodepool=apigee-data\n```## \n Overriding the node pool for specific components on Anthos GKE\nYou can also override node pool configurations at the individual component level for an Anthos GKE installation. For example, the following configuration assigns the node pool with the value `apigee-custom` to the `runtime` component:\n```\nruntime:\n nodeSelector:\n key: apigee.com/apigee-nodepool\n value: apigee-custom\n```\nYou can specify a custom node pool on any of these components:\n- `istio`\n- `mart`\n- `synchronizer`\n- `runtime`\n- `cassandra`\n- `udca`\n- `logger`", "guide": "Apigee"}