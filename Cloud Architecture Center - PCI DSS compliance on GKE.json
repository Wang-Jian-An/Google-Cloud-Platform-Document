{"title": "Cloud Architecture Center - PCI DSS compliance on GKE", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - PCI DSS compliance on GKE\nLast reviewed 2023-10-31 UTC\nThis guide is intended to help you address concerns unique to Google Kubernetes Engine (GKE) applications when you are implementing customer responsibilities for [Payment Card Industry Data Security Standard (PCI DSS)](https://www.pcisecuritystandards.org/about_us/) requirements.\n**Disclaimer** : This guide is for informational purposes only. Google does not intend the information or recommendations in this guide to constitute legal or audit advice. Each customer is responsible for independently evaluating their own particular use of the services as appropriate to support its legal and compliance obligations.\n", "content": "## Introduction to PCI DSS compliance and GKE\nIf you handle payment card data, you must secure it\u2014whether it resides in an on-premises database or in the cloud. PCI DSS was developed to encourage and enhance cardholder data security and facilitate the broad adoption of consistent data security measures globally. PCI DSS provides a baseline of technical and operational requirements designed to protect credit card data. PCI DSS applies to all entities involved in payment card processing\u2014including merchants, processors, acquirers, issuers, and service providers. PCI DSS also applies to all other entities that store, process, or transmit cardholder data (CHD) or sensitive authentication data (SAD), or both.\nContainerized applications have become popular recently with many legacy workloads migrating from a virtual machine (VM)\u2013based architecture to a containerized one. [Google Kubernetes Engine](/kubernetes-engine) is a managed, production-ready environment for deploying containerized applications. It brings Google's latest innovations in developer productivity, resource efficiency, automated operations, and open source flexibility to accelerate your time to market.\nCompliance is a shared responsibility in the cloud. Google Cloud, including GKE (both Autopilot and Standard modes of operation), adheres to PCI DSS requirements. We outline our responsibilities in our [Shared responsibility matrix](https://services.google.com/fh/files/misc/gcp_pci_dss_v4_responsibility_matrix.pdf) .\n### Intended audience\n- Customers who want to bring PCI-compliant workloads to [Google Cloud](/) that involve [GKE](/kubernetes-engine) .\n- Developers, security officers, compliance officers, IT administrators, and other employees who are responsible for implementing controls and ensuring compliance with PCI DSS requirements.\n### Before you begin\nFor the recommendations that follow, you potentially have to use the following:\n- Google Cloud Organization, Folder, and Project resources\n- Identity and Access Management (IAM)\n- Google Kubernetes Engine\n- Google Cloud VPCs\n- Google Cloud Armor\n- The Cloud Data Loss Prevention API (part of Sensitive Data Protection)\n- Identity-Aware Proxy (IAP)\n- Security Command Center\nThis guide is intended for those who are familiar with containers and GKE.\n### Scope\nThis guide identifies the following requirements from PCI DSS that are unique concerns for GKE and supplies guidance for meeting them. It is written against [version 4.0 of the standard](https://docs-prv.pcisecuritystandards.org/PCI%20DSS/Standard/PCI-DSS-v4_0.pdf) . This guide doesn't cover all the requirements in PCI DSS. The information provided in this guide might assist organizations in their pursuit of PCI DSS compliance but it's not comprehensive advice. Organizations can engage a [PCI Qualified Security Assessor (QSA)](https://listings.pcisecuritystandards.org/assessors_and_solutions/qualified_security_assessors) for formal validation.\n| PCI DSS goals         | PCI DSS requirements                                                                                  |\n|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Segment your cardholder data environment  | Sometimes referred to as requirement 0. Alhough it's not a must for PCI compliance, we recommend this requirement to keep the PCI scope limited.                                                   |\n| Build and maintain a secure network and systems | 1. Install and maintain network security controls Requirement 1.1 Requirement 1.1.2 Requirement 1.2 Requirement 1.2.2 Requirement 1.2.5 Requirement 1.3 Requirement 1.3.1 Requirement 1.4 Requirement 1.4.3 Requirement 1.4.5 2. Apply secure configurations to all system components Requirement 2.2 Requirement 2.2.4 Requirement 2.2.5 Requirement 2.2.6 |\n| Protect account data       | 3. Protect stored account data Requirement 3.5 Requirement 3.6 4. Protect cardholder data with strong cryptography during transmission over open, public networks Requirement 4.1                                           |\n| Maintain a vulnerability management program  | 5. Protect all systems and networks from malicious software Requirement 5.2 Requirement 5.2.3 Requirement 5.3 6. Develop and maintain secure systems and software Requirement 6.2 Requirement 6.2.1 Requirement 6.3 Requirement 6.3.1 Requirement 6.4                          |\n| Implement strong access control measures  | 7. Restrict access to system components and cardholder data by business need to know Requirement 7.2 Requirement 7.2.2 8. Identify and authenticate access to system components Requirement 8.2 Requirement 8.2.1 Requirement 8.2.5 Requirement 8.3 Requirement 8.3.1 9. Restrict physical access to cardholder data          |\n| Regularly monitor and test networks    | 10. Log and monitor all access to system components and cardholder data Requirement 10.2 Requirement 10.2.2 11. Test security of systems and networks regularly Requirement 11.3 Requirement 11.3.1 Requirement 11.5 Requirement 11.5.1                              |\n| Maintain an information security policy   | 12. Support information security with organizational policies and programs                                                                     |\n### Terminology\nThis section defines terms used in this guide. For more details, see the [PCI DSS glossary](https://www.pcisecuritystandards.org/pci_security/glossary) .\n## Segment your cardholder data environment\nThe cardholder data environment (CDE) comprises people, processes, and technologies that store, process, or transmit cardholder data or sensitive authentication data. In the context of GKE, the CDE also comprises the following:\n- Systems that provide security services (for example, IAM).\n- Systems that facilitate segmentation (for example, projects, folders, firewalls, virtual private clouds (VPCs), and subnets).\n- Application pods and clusters that store, process, or transmit cardholder data. Without adequate segmentation, your entire cloud footprint can get in scope for PCI DSS.\nTo be considered out of scope for PCI DSS, a system component must be properly isolated from the CDE such that even if the out-of-scope system component were compromised, it could not impact the security of the CDE.\nAn important prerequisite to reduce the scope of the CDE is a clear understanding of business needs and processes related to the storage, processing, and transmission of cardholder data. Restricting cardholder data to as few locations as possible by eliminating unnecessary data and consolidating necessary data might require you to reengineer long-standing business practices.\nYou can properly segment your CDE through a number of means on Google Cloud. This section discusses the following means:\n- Logical segmentation by using the resource hierarchy\n- Network segmentation by using VPCs and subnets\n- Service level segmentation by using VPC\n- Other considerations for any in-scope cluster\n### Logical segmentation using the resource hierarchy\nThere are several ways to isolate your CDE within your organizational structure using Google Cloud's [resource hierarchy](/resource-manager/docs/cloud-platform-resource-hierarchy) . Google Cloud resources are organized hierarchically. The [Organization](/resource-manager/docs/cloud-platform-resource-hierarchy#organizations) resource is the root node in the Google Cloud resource hierarchy. [Folders](/resource-manager/docs/cloud-platform-resource-hierarchy#folders) and [projects](/resource-manager/docs/cloud-platform-resource-hierarchy#projects) fall under the Organization resource. Folders can contain projects and folders. Folders are used to control access to resources in the folder hierarchy through folder-level IAM permissions. They're also used to group similar projects. A project is a trust boundary for all your resources and an IAM enforcement point.\nYou might group all projects that are in PCI scope within a folder to isolate at the folder level. You might also use one project for all in-scope PCI clusters and applications, or you might create a project and cluster for each in-scope PCI application and use them to organize your Google Cloud resources. In any case, we recommend that you keep your in-scope and out-of-scope workloads in different projects.\n### Network segmentation using VPC networks and subnets\nYou can use [Virtual Private Cloud](/vpc) (VPC) and subnets to provision your network and to group and isolate CDE-related resources. VPC is a logical isolation of a section of a public cloud. VPC networks provide scalable and flexible networking for your Compute Engine virtual machine (VM) instances and for the services that leverage VM instances, including GKE. For more details, see the [VPC overview](/vpc/docs/vpc) and refer to the [best practice and reference architectures](/solutions/best-practices-vpc-design) .\nWhile VPC and subnets provide segmentation and create a perimeter to isolate your CDE, [VPC Service Controls](/vpc-service-controls) augments the security perimeter at layer 7. You can use VPC Service Controls to create a perimeter around your in-scope CDE projects. VPC Service Controls gives you the following controls:\n- **Ingress control** . Only authorized identities and clients are allowed into your security perimeter.\n- **Egress control** . Only authorized destinations are allowed for identities and clients within your security perimeter.\nYou can use [Google Cloud Armor](/armor) to create lists of IP addresses to allow or deny access to your [HTTP(S) load balancer](/load-balancing/docs/https) at the edge of the Google Cloud network. By examining IP addresses as close as possible to the user and to malicious traffic, you help prevent malicious traffic from consuming resources or entering your VPC networks.\nUse VPC Service Controls to define a service perimeter around your in-scope projects. This perimeter governs VM-to-service and service-to-service paths, as well as VPC ingress and egress traffic.\n## Build and maintain a secure network and systems\nBuilding and maintaining a secure network encompasses requirements 1 and 2 of PCI DSS.\n### Requirement 1\nInstall and maintain a firewall configuration to protect cardholder data and traffic into and out of the CDE.\nNetworking concepts for containers and GKE differ from those for traditional VMs. Pods can reach each other directly, without NAT, even across nodes. This creates a simple network topology that might be surprising if you're used to managing more complex systems. The first step in network security for GKE is to educate yourself on [these networking concepts](/kubernetes-engine/docs/concepts/network-overview) .\nBefore diving into individual requirements under Requirement 1, you might want to review the following networking concepts in relation to GKE:\n- **Firewall rules** . [Firewall rules](/vpc/docs/firewalls) are used to restrict traffic to your nodes. GKE nodes are provisioned as Compute Engine instances and use the same firewall mechanisms as other instances. Within your network, you can use tags to apply these firewall rules to each instance. Each node pool receives its own set of tags that you can use in rules. By default, each instance belonging to a node pool receives a tag that identifies a specific GKE cluster that this node pool is a part of. This tag is used in firewall rules that GKE creates automatically for you. You can add custom tags at either cluster or node pool creation time by using the `--tags` flag in the Google Cloud CLI.\n- **Network policies** . [Network policies](/kubernetes-engine/docs/how-to/network-policy) let you limit network connections between pods, which can help restrict [network pivoting](https://wikipedia.org/wiki/Exploit_(computer_security)#Pivoting) and lateral movement inside the cluster in the event of a security issue with a pod. To use network policies, you must enable the feature explicitly when creating the GKE cluster. You can enable it on an existing cluster, but it will cause your cluster nodes to restart. The default behavior is that all pod-to-pod communication is always open. Therefore, if you want to segment your network, you need to enforce pod-level networking policies. In GKE, you can define a network policy by using the [Kubernetes Network Policy API](https://kubernetes.io/docs/concepts/services-networking/network-policies/) or by using the `kubectl` tool. These pod-level traffic policy rules determine which pods and services can access one another inside your cluster.\n- **Namespaces** . [Namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/) allow for resource segmentation inside your Kubernetes cluster. Kubernetes comes with a default namespace out of the box, but you can create multiple namespaces within your cluster. Namespaces are logically isolated from each other. They provide scope for pods, services, and deployments in the cluster, so that users interacting with one namespace will not see content in another namespace. However, namespaces within the same cluster don't restrict communication between namespaces; this is where network policies come in. For more information on configuring namespaces, see the [Namespaces Best Practices](/blog/products/gcp/kubernetes-best-practices-organizing-with-namespaces) blog post.\nThe following diagram illustrates the preceding concepts in relation to each other and other GKE components such as cluster, node, and pod.\nProcesses and mechanisms for installing and maintaining network security controls are defined and understood.\nDescribe groups, roles, and responsibilities for managing network components.\nFirst, as you would with most services on Google Cloud, you need to configure [IAM](/iam) roles in order to set up authorization on GKE. When you've set up your IAM roles, you need to add [Kubernetes role-based access control](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) (RBAC) configuration as part of a Kubernetes authorization strategy.\nEssentially, all IAM configuration applies to any Google Cloud resources and all clusters within a project. Kubernetes RBAC configuration applies to the resources in each Kubernetes cluster, and enables fine-grained authorization at the namespace level. With GKE, these approaches to authorization work in parallel, with a user's capabilities effectively representing a union of IAM and RBAC roles assigned to them:\n- Use IAM to control groups, roles, and responsibilities for logical management of network components in GKE.\n- Use Kubernetes RBAC to grant granular permissions to [network policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) within Kubernetes clusters, to control pod-to-pod traffic, and to prevent unauthorized or accidental changes from non-CDE users.\n- Be able to justify for all IAM and RBAC users and permissions. Typically, when QSAs test for controls, they look for a business justification for a sample of IAM and RBAC.Network security controls (NSCs) are configured and maintained.\nFirst, you configure [firewall rules](/vpc/docs/firewalls) on Compute Engine instances that run your GKE nodes. Firewall rules protect these cluster nodes.\nNext, you configure [network policies](/kubernetes-engine/docs/how-to/network-policy) to restrict flows and protect pods in a cluster. A [network policy](/kubernetes-engine/docs/how-to/network-policy) is a specification of how groups of pods are allowed to communicate with each other and with other network endpoints. You can use GKE's network policy enforcement to control the communication between your cluster's pods and services. To further segment your cluster, create multiple namespaces within it. Namespaces are logically isolated from each other. They provide scope for pods, services, and deployments in the cluster, so users interacting with one namespace will not see content in another namespace. However, namespaces within the same cluster don't restrict communication between namespaces; this is where network policies come in. Namespaces allow for resource segmentation inside your Kubernetes cluster. For more information on configuring namespaces, see the [Namespaces Best Practices](/blog/products/gcp/kubernetes-best-practices-organizing-with-namespaces) blog post.\nBy default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to and from pods in that namespace. For example, you can create a default isolation policy for a namespace by creating a network policy that selects all pods but doesn't allow any ingress traffic to those pods.\nAll changes to network connections and to configurations of NSCs are approved and managed in accordance with the change control process defined at Requirement 6.5.1.\nTo treat your networking configurations and infrastructure as code, you need to establish a continuous integration and continuous delivery (CI/CD) pipeline as part of your change-management and change-control processes.\nYou can use [Cloud Deployment Manager](/deployment-manager) or [Terraform](https://github.com/GoogleCloudPlatform/gke-network-policy-demo) templates as part of the CI/CD pipeline to create network policies on your clusters. With Deployment Manager or Terraform, you can treat configuration and infrastructure as code that can reproduce consistent copies of the current production or other environments. Then you are able to write unit tests and other tests to ensure your network changes work as expected. A change control process that includes an approval can be managed through configuration files stored in a version repository.\nWith [Terraform Config Validator](https://github.com/forseti-security/policy-library/blob/master/docs/user_guide.md#how-to-use-terraform-validator) , you can define constraints to enforce security and governance policies. By adding Config Validator to your CI/CD pipeline, you can add a step to any workflow. This step validates a Terraform plan and rejects it if violations are found.\nAll services, protocols, and ports allowed are identified, approved, and have a defined business need.\nFor strong ingress controls for your GKE clusters, you can use [authorized networks](/kubernetes-engine/docs/how-to/authorized-networks) to restrict certain IP ranges that can reach your cluster's control plane. GKE uses both Transport Layer Security (TLS) and authentication to provide secure access to your cluster master endpoint from the public internet. This access gives you the flexibility to administer your cluster from anywhere. By using authorized networks, you can further restrict access to specified sets of IP addresses.\nYou can use [Google Cloud Armor](/armor) to create IP deny lists and allow lists and security policies for GKE hosted applications. In a GKE cluster, incoming traffic is handled by [HTTP(S) Load Balancing](/load-balancing/docs/https) , which is a component of [Cloud Load Balancing](/load-balancing/docs/load-balancing-overview) . Typically, the HTTP(S) load balancer is configured by the [GKE ingress controller](https://github.com/kubernetes/ingress-gce) , which gets configuration information from a Kubernetes [Ingress](/kubernetes-engine/docs/concepts/ingress) object. For more information, see [how to configure Google Cloud Armor policies with GKE](/kubernetes-engine/docs/how-to/cloud-armor-backendconfig) .\nNetwork access to and from the cardholder data environment is restricted.\nTo keep sensitive data private, you can configure private communications between GKE clusters inside your VPC networks and on-premises hybrid deployments by using [VPC Service Controls and Private Google Access](/vpc-service-controls) .\nInbound traffic to the CDE is restricted as follows:- To only traffic that is necessary.\n- All other traffic is specifically denied.\nConsider implementing [Cloud NAT setup with GKE](/nat/docs/gke-example) to limit inbound internet traffic to only that cluster. You can [set up a private cluster](/kubernetes-engine/docs/how-to/private-clusters) for the non-public facing clusters in your CDE. In a private cluster, the nodes have internal RFC 1918 IP addresses only, which ensures that their workloads are isolated from the public internet.\nNetwork connections between trusted and untrusted networks are controlled.\nYou can address this requirement using the same methods listed for Requirement 1.3.\nAnti-spoofing measures are implemented to detect and block forged source IP addresses from entering the trusted network.\nYou implement anti-spoofing measures by using alias IP addresses on GKE pods and clusters to detect and block forged source IP addresses from entering the network. A cluster that uses [alias IP ranges](/vpc/docs/alias-ip) is called a VPC-native cluster.\nThe disclosure of internal IP addresses and routing information is limited to only authorized parties.\nYou can use a [GKE IP masquerade agent](/kubernetes-engine/docs/how-to/ip-masquerade-agent) to do network address translation (NAT) for many-to-one IP address translations on a cluster. Masquerading masks multiple source IP addresses behind a single address.\n### Requirement 2\nApply secure configurations to all system components.\nRequirement 2 specifies how to harden security parameters by removing defaults and vendor supplied credentials. [Hardening your cluster](/kubernetes-engine/docs/how-to/hardening-your-cluster) is a customer responsibility.\nSystem components are configured and managed securely.Ensure that these standards address all known security vulnerabilities and are consistent with industry-accepted system hardening standards. Sources of industry-accepted system hardening standards may include, but are not limited to:- [Center for Internet Security (CIS)](https://www.cisecurity.org/) \n- [International Organization for Standardization (ISO)](https://www.iso.org/home.html) \n- [SysAdmin Audit Network Security (SANS) Institute](https://www.sans.org/) \n- [National Institute of Standards Technology (NIST)](https://www.nist.gov/) Only necessary services, protocols, daemons, and functions are enabled, and all unnecessary functionality is removed or disabled.\nIf any insecure services, protocols, or daemons are present:- Business justification is documented.\n- Additional security features are documented and implemented that reduce the risk of using insecure services, protocols, or daemons.System security parameters are configured to prevent misuse.\nBefore you move containers onto GKE, we recommend the following:\n- Start with a container [managed base image](/artifact-registry/docs/docker/manage-images) that is built, maintained, and vulnerability-checked by a trusted source. Consider creating a set of \"known good\" or \"golden\" base images that your developers can use. A more restrictive option is to use a [distroless image](https://github.com/GoogleContainerTools/distroless) or a [scratch base image](https://docs.docker.com/develop/develop-images/baseimages/#create-a-simple-parent-image-using-scratch) .\n- Use [Artifact Analysis](/container-registry/docs/container-analysis) to scan your container images for vulnerabilities.\n- Establish an internal DevOps/SecOps policy to include only approved, trusted libraries and binaries into the containers.During set up, we recommend the following:\n- Use the default [Container-Optimized OS](/container-optimized-os/docs) as the node image for GKE. Container-Optimized OS is based on Chromium OS and is optimized for [node security](/container-optimized-os/docs/concepts/security) .\n- Enable [auto-upgrading nodes](/kubernetes-engine/docs/how-to/node-auto-upgrades) for the clusters that run your applications. This feature automatically upgrades the node to the Kubernetes version that's running in the managed control plane, providing better stability and security.\n- Enable [auto-repairing nodes](/kubernetes-engine/docs/how-to/node-auto-repair) . When this feature is enabled, GKE periodically checks and uses the node's health status to determine if a node needs to be repaired. If a node requires repair, that node is drained and a new node is created and added to the cluster.\n- Turn on [Cloud Monitoring](/monitoring/kubernetes-engine) and [Cloud Logging](/monitoring/kubernetes-engine/legacy-stackdriver/logging) for visibility of all events, including security events and node health status. Create [Cloud Monitoring alert policies](/monitoring/alerts) to get notified if a security incident occurs.\n- Apply [least privilege service accounts](/kubernetes-engine/docs/how-to/hardening-your-cluster#use_least_privilege_sa) for GKE nodes\n- Review and apply (where applicable) the GKE section in the [Google Cloud CIS Benchmark](https://www.cisecurity.org/blog/new-cis-benchmark-for-google-cloud-computing-platform/) guide. Kubernetes audit logging is already enabled by default, and logs for both requests to`kubectl`and the GKE API are written to Cloud Audit Logs.\n- [Configure audit logging](/kubernetes-engine/docs/how-to/audit-logging) .## Protect account data\nProtecting cardholder data encompasses requirements 3 and 4 of PCI DSS.\n### Requirement 3\nProtect stored account data.\nRequirement 3 of PCI DSS stipulates that protection techniques such as encryption, truncation, masking, and hashing are critical components of cardholder data protection. If an intruder circumvents other security controls and gains access to encrypted data, without the proper cryptographic keys, the data is unreadable and unusable to that person.\nYou might also consider other methods of protecting stored data as potential risk-mitigation opportunities. For example, methods for minimizing risk include not storing cardholder data unless absolutely necessary, truncating cardholder data if the full PAN is not needed, and not sending unprotected PANs using end-user messaging technologies, such as email and instant messaging.\nExamples of systems where CHD might persist as part of your payment processing flows when running on Google Cloud are:\n- Cloud Storage buckets\n- BigQuery instances\n- Datastore\n- Cloud SQL\nBe aware that CHD might be inadvertently stored in email or customer service communication logs. It's prudent to use [Sensitive Data Protection](/sensitive-data-protection) to filter these data streams so that you limit your in-scope environment to the payment processing systems.\nNote that on Google Cloud, data is [encrypted at rest by default](/security/encryption/default-encryption) , and [encrypted in transit by default when it traverses physical boundaries](/security/encryption-in-transit) . No additional configuration is necessary to enable these protections.\nPrimary account number (PAN) is secured wherever it is stored.\nOne mechanism to render PAN data unreadable is tokenization. For more information, see the solution guide on [tokenizing sensitive cardholder data for PCI DSS](/solutions/tokenizing-sensitive-cardholder-data-for-pci-dss) .\nYou can use the [DLP API](/sensitive-data-protection) to scan, discover, and report the cardholder data. Sensitive Data Protection has native support for scanning and classifying 12\u201319-digit PAN data in Cloud Storage, BigQuery, and Datastore. It also has a streaming content API to enable support for additional data sources, custom workloads, and applications. You can also use the DLP API to [truncate (redact) or hash](/blog/products/gcp/take-charge-of-your-sensitive-data-with-the-cloud-dlp-api) the data.\nCryptographic keys used to protect stored account data are secured.\n[Cloud Key Management Service (KMS)](/kms) is a managed storage system for cryptographic keys. It can generate, use, rotate, and destroy cryptographic keys. Although Cloud KMS does not directly [store secrets](/kms/docs/encrypting-application-data) like cardholder data, it can be used to encrypt such data.\nSecrets in the context of Kubernetes are Kubernetes [secret objects](https://kubernetes.io/docs/concepts/configuration/secret/) that let you store and manage sensitive information, such as passwords, tokens, and keys.\nBy default, Google Cloud [encrypts customer content stored at rest](/security/encryption/default-encryption) . GKE handles and manages this default encryption for you without any additional action on your part. [Application-layer secrets encryption](/kubernetes-engine/docs/how-to/encrypting-secrets) provides an additional layer of security for sensitive data such as secrets. Using this functionality, you can provide a key that you manage in [Cloud KMS](/kms/docs) , to encrypt data at the application layer. This protects against attackers who gain access to a copy of the Kubernetes configuration storage instance of your cluster.### Requirement 4\nProtect cardholder data with strong cryptography during transmission over open, public networks.\nThe in-scope data must be encrypted during transmission over networks that are easily accessed by malicious individuals, for example, public networks.\n[Istio](/istio) is an open source service mesh that layers transparently onto existing distributed applications. Istio scalably manages authentication, authorization, and encryption of traffic between microservices. It's a platform that includes APIs that let you integrate into any logging platform, telemetry, or policy system. Istio's feature set lets you efficiently run a distributed microservice architecture and provides a uniform way to secure, connect, and monitor microservices.\nProcesses and mechanisms for protecting cardholder data with strong cryptography during transmission over open, public networks are defined and documented.\nYou can use Istio to create a network of deployed services\u2014with load balancing, service-to-service authentication, and monitoring. You can also use it to deliver secure service-to-service communication in a cluster\u2014with strong identity-based authentication and authorization based on mutual TLS. Mutual TLS (mTLS) is a TLS handshake performed twice, establishing the same level of trust in both directions (as opposed to one-directional client-server trust).\nIstio lets you deploy TLS certificates to each of the GKE pods within an application. Services running on the pod can use mTLS to strongly identify their peer identities. Service-to-service communication is tunneled through client-side and server-side [Envoy](https://www.envoyproxy.io/) proxies. Envoy uses [SPIFFE](https://spiffe.io/) IDs to establish mTLS connections between services. For information on how to deploy Istio on GKE, see the [GKE documentation](/istio/docs/istio-on-gke/overview) . And for information on supported TLS versions, see the Istio [Traffic Management reference](https://istio.io/docs/reference/config/networking/gateway/#Server-TLSOptions) . Use TLS version 1.2 and later.\nIf your application is exposed to the internet, use [GKE HTTP(S) Load Balancing](/kubernetes-engine/docs/concepts/ingress) with ingress routing that is set to use HTTP(S). HTTP(S) Load Balancing, configured by an Ingress object, includes the following features:\n- **Flexible configuration for services** . An Ingress object defines how traffic reaches your services and how the traffic is routed to your application. In addition, an Ingress can provide a single IP address for multiple services in your cluster.\n- **Integration with Google Cloud network services** . An Ingress object can configure Google Cloud features such as [Google-managed SSL certificates (beta)](/load-balancing/docs/ssl-certificates#certificate-types) , [Google Cloud Armor](/kubernetes-engine/docs/how-to/cloud-armor-backendconfig) , [Cloud CDN](/kubernetes-engine/docs/how-to/cdn-backendconfig) , and [Identity-Aware Proxy](/iap/docs/enabling-kubernetes-howto) .\n- **Support for multiple TLS certificates** . An Ingress object can specify the use of multiple TLS certificates for request termination.\nWhen you create an Ingress object, the [GKE ingress controller](https://github.com/kubernetes/ingress-gce) creates a [Cloud HTTP(S) load balancer](/load-balancing/docs/https) and configures it according to the information in the Ingress and its associated Services.\n## Maintain a vulnerability management program\nMaintaining a vulnerability management program encompasses requirements 5 and 6 of PCI DSS.\n### Requirement 5\nProtect all systems and networks from malicious software.\nRequirement 5 of PCI DSS stipulates that antivirus software must be used on all systems commonly affected by malware to protect systems from current and evolving malicious software threats\u2014and containers are no exception.\nMalicious software (malware) is prevented, or detected and addressed..\nYou must implement vulnerability management programs for your container images.\nWe recommend the following actions:\n- Regularly check and apply up-to-date security patches on the containers.\n- Perform regular [vulnerability scanning](/container-registry/docs/get-image-vulnerabilities) against containerized applications and binaries/libraries.\n- Scan images as part of the build pipeline.\n- Subscribe to a [vulnerability intelligence service](https://www.cisa.gov/) to receive up-to-date vulnerability information relevant to the environment and libraries used in the containers.\nGoogle Cloud works with various container security solutions providers to improve security posture within customers' Google Cloud deployments. We recommend leveraging validated security solutions and technologies to increase depth of defense in your GKE environment. For the latest Google Cloud-validated security partners list, see [Security Partners](/security/partners) .\nThe deployed anti-malware solution(s):- Detects all known types of malware.\n- Removes, blocks, or contains all known types of malware.Any system components that are not at risk for malware are evaluated periodically to include the following:- A documented list of all system components not at risk for malware.\n- Identification and evaluation of evolving malware threats for those system components.\n- Confirmation whether such system components continue to not require anti-malware protection.\nThere are many solutions available to perform malware scans, but PCI DSS recognizes that not all systems are equally likely to be vulnerable. It's common for merchants to declare their Linux servers, mainframes, and similar machines as not \"commonly affected by malicious software\" and therefore exempt from 5.2.2. In that case, 5.2.3 applies, and you must implement a system for periodic threat evaluations.\nKeep in mind that these rules apply to both nodes and pods within a GKE cluster.\nAnti-malware mechanisms and processes are active, maintained, and monitored.\nRequirements 5.2, 5.3, and 11.5 call for antivirus scans and file integrity monitoring (FIM) on any in-scope host. We recommend implementing a solution where all nodes can be scanned by a trusted agent within the cluster or where each node has a scanner that reports up to a single management endpoint.\nFor more information, see the [security overview for GKE](/kubernetes-engine/docs/concepts/security-overview) , and the [security overview for Container-Optimized OS](/container-optimized-os/docs/concepts/security) .\nA common solution to both the antivirus and FIM requirements is to lock down your container so only specific allowed folders have write access. To do this, you [run your containers as a non-root user](https://kubernetes.io/blog/2018/07/18/11-ways-not-to-get-hacked/#8-run-containers-as-a-non-root-user) and use file system permissions to prevent write access to all but the working directories within the container file system. [Disallow privilege escalation](https://kubernetes.io/docs/concepts/policy/pod-security-policy/#privilege-escalation) to avoid circumvention of the file system rules.\n### Requirement 6\nDevelop and maintain secure systems and software.\nRequirement 6 of PCI DSS stipulates that you establish a strong software development lifecycle where security is built in at every step of software development.\nBespoke and custom software are developed securely.\nBespoke and custom software are developed securely, as follows:- Based on industry standards and/or best practices for secure development.\n- In accordance with PCI DSS (for example, secure authentication and logging).\n- Incorporating consideration of information security issues during each stage of the software development lifecycle.\nYou can use [Binary Authorization](/binary-authorization/docs) to help ensure that only trusted containers are deployed to GKE. If you want to enable only images authorized by one or more specific attestors, you can configure Binary Authorization to enforce a [policy](/binary-authorization/docs/key-concepts#policies) with rules that require [attestations](/binary-authorization/docs/key-concepts#attestations) based on vulnerability scan results. You can also write policies that require one or more trusted parties (called \"attestors\") to approve of an image before it can be deployed. For a multi-stage deployment pipeline where images progress from development to testing to production clusters, you can use attestors to ensure that all required processes have completed before software moves to the next stage.\nAt deployment time, Binary Authorization enforces your policy by checking that the container image has passed all required constraints\u2014including that all required attestors have verified that the image is ready for deployment. If the image passes, the service allows it to be deployed. Otherwise, deployment is blocked and the image can't be deployed until it's compliant.\nFor more information on Binary Authorization, see [Set up for GKE.](/binary-authorization/docs/setting-up)\nIn an emergency, you can bypass a Binary Authorization policy by using the [breakglass workflow](/blog/products/identity-security/deploy-only-what-you-trust-introducing-binary-authorization-for-google-kubernetes-engine) . All breakglass incidents are recorded in [Cloud Audit Logs](/logging/docs/audit) .\n[GKE Sandbox](/kubernetes-engine/docs/concepts/sandbox-pods) reduces the need for the container to interact directly with the host, shrinking the attack surface for host compromise, and restricting the movement of malicious actors.\nSecurity vulnerabilities are identified and addressed.\nSecurity vulnerabilities are identified and managed as follows:- New security vulnerabilities are identified using industry-recognized sources for security vulnerability information, including alerts from international and national computer emergency response teams (CERTs).\n- Vulnerabilities are assigned a risk ranking based on industry best practices and consideration of potential impact.\n- Risk rankings identify, at a minimum, all vulnerabilities considered to be a high-risk or critical to the environment.\n- Vulnerabilities for bespoke and custom, and third-party software (for example operating systems and databases) are covered.\nSecurity in the cloud is a shared responsibility between the cloud provider and the customer.\nIn GKE, Google manages the control plane, which includes the master VMs, the API server, and other components running on those VMs, as well as the `etcd` database. This includes upgrades and patching, scaling, and repairs, all backed by a service-level objective (SLO). For the nodes' operating system, such as Container-Optimized OS or Ubuntu, GKE promptly makes any patches to these images available. If you have auto-upgrade enabled, these patches are automatically deployed. (This is the base layer of your container\u2014it's not the same as the operating system running in your containers.)\nFor more information on the GKE shared responsibility model, see [Exploring container security: the shared responsibility model in GKE](/blog/products/containers-kubernetes/exploring-container-security-the-shared-responsibility-model-in-gke-container-security-shared-responsibility-model-gke) .\nGoogle provides several security services to help build security into your CI/CD pipeline. To identify vulnerabilities in your container images, you can use [Google Artifact Analysis Vulnerability Scanning](/container-registry/docs/container-analysis#vulnerability_scanning) . When a container image is pushed to [Google Container Registry (GCR)](/container-registry) , vulnerability scanning automatically scans images for known vulnerabilities and exposures from known [CVE](https://wikipedia.org/wiki/Common_Vulnerabilities_and_Exposures) sources. Vulnerabilities are assigned severity levels (critical, high, medium, low, and minimal) based on [CVSS scores](https://www.first.org/cvss/specification-document) .\nPublic-facing web applications are protected against attacks.\n[Web Security Scanner](/security-scanner) allows you to scan publicly facing App Engine, Compute Engine, and GKE web applications for common vulnerabilities ranging from cross-site scripting and misconfigurations to vulnerable resources. Scans can be performed on demand and scheduled from the [Google Cloud console](https://console.cloud.google.com/) . Using the Security Scanner APIs, you can automate the scan as part of your security test suite in your application build pipeline.\n## Implement strong access control measures\nImplementing strong access control measures encompasses requirements 7, 8, and 9 of PCI DSS.\n### Requirement 7\nRestrict access to system components and cardholder data by business need to know.\nRequirement 7 focuses on or . PCI DSS defines these as granting access to the least amount of data and providing the fewest privileges that are required in order to perform a job.\nAccess to system components and data is appropriately defined and assigned.\n[IAM](/iam) and [Kubernetes role-based access control (RBAC)](/kubernetes-engine/docs/how-to/role-based-access-control) work together to provide fine-grained access control to your GKE environment. IAM is used to manage user access and permissions of Google Cloud resources in your CDE project. In GKE, you can also use IAM to manage the access and actions that users and service accounts can perform in your clusters, such as creating and deleting clusters.\nKubernetes RBAC allows you to configure fine-grained sets of permissions that define how a given Google Cloud user, Google Cloud service accounts, or group of users ( [Google Groups](/kubernetes-engine/docs/how-to/role-based-access-control#google-groups-for-gke) ) can interact with any Kubernetes object in your cluster, or in a specific namespace of your cluster. Examples of RBAC permissions include editing deployments or configmaps, deleting pods, or viewing logs from a pod. You grant users or services limited IAM permissions, such as [Google Kubernetes Engine Cluster Viewer](/kubernetes-engine/docs/how-to/iam#predefined) or [custom roles](/kubernetes-engine/docs/how-to/iam#custom_roles) , then apply Kubernetes RBAC RoleBindings as appropriate.\n[Cloud Identity Aware Proxy (IAP)](/iap/docs/enabling-kubernetes-howto) can be integrated through ingress for GKE to control application-level access for employees or people who require access to your PCI applications.\nAdditionally, you can use [Organization policies](/resource-manager/docs/organization-policy/creating-managing-policies) to restrict the APIs and services that are available within a project.\nAccess is assigned to users, including privileged users, based on:- Job classification and function.\n- Least privileges necessary to perform job responsibilities.\nAlong with making sure users and service accounts adhere to the principle of least privilege, containers should too. A best practice when running a container is to run the process with a non-root user. You can accomplish and enforce this practice by using the [PodSecurity admission controller](/kubernetes-engine/docs/how-to/podsecurityadmission) .\nPodSecurity is a Kubernetes admission controller that lets you apply Pod Security Standards to Pods running on your GKE clusters. Pod Security Standards are predefined security policies that cover the high-level needs of Pod security in Kubernetes. These policies range from being highly permissive to highly restrictive. PodSecurity replaces the former PodSecurityPolicy admission controller that was removed in Kubernetes v1.25. Instructions are available for [migrating](/kubernetes-engine/docs/how-to/migrate-podsecuritypolicy) from PodSecurityPolicy to the PodSecurity admission controller.\n### Requirement 8\nIdentify users and authenticate access to system components\nRequirement 8 specifies that a unique ID must be assigned to each person who has access to in-scope PCI systems to ensure that each individual is uniquely accountable for their actions.\nUser identification and related accounts for users and administrators are strictly managed throughout an account's lifecycle.\nAll users are assigned a unique ID before access to system components or cardholder data is allowed.\nAccess for terminated users is immediately revoked.\nBoth IAM and Kubernetes RBAC can be used to control access to your GKE cluster, and in both cases you can grant permissions to a user. We recommend that the users tie back to your [existing identity system](/blog/products/identity-security/identity-and-authentication-the-google-cloud-way) , so that you can manage user accounts and policies in one location.\nStrong authentication for users and administrators is established and managed.\nAll user access to system components for users and administrators is authenticated via at least one of the following authentication factors:- Something you know, such as a password or passphrase.\n- Something you have, such as a token device or smart card.\n- Something you are, such as a biometric element.\nCertificates are bound to a user's identity when [they authenticate to kubectl](/kubernetes-engine/docs/how-to/cluster-access-for-kubectl#authentication) . All GKE clusters are configured to accept Google Cloud user and service account identities, by validating the credentials and retrieving the email address associated with the user or service account identity. As a result, the credentials for those accounts must include the `userinfo.email` OAuth scope in order to successfully authenticate.\n### Requirement 9\nRestrict physical access to cardholder data.\nGoogle is [responsible](https://services.google.com/fh/files/misc/gcp_pci_dss_v4_responsibility_matrix.pdf) for physical security controls on all Google data centers underlying Google Cloud.\n## Regularly monitor and test networks\nRegularly monitoring and testing networks encompasses requirements 10 and 11 of PCI DSS.\n### Requirement 10\nLog and monitor all access to system components and cardholder data.\nAudit logs are implemented to support the detection of anomalies and suspicious activity, and the forensic analysis of events.\nKubernetes clusters have [Kubernetes audit logging](https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/) enabled by default, which keeps a chronological record of calls that have been made to the Kubernetes API server. Kubernetes audit log entries are useful for investigating suspicious API requests, for collecting statistics, or for creating monitoring alerts for unwanted API calls.\nGKE clusters integrate a default configuration for [GKE audit logging](/kubernetes-engine/docs/how-to/audit-logging) with [Cloud Audit Logs](/logging/docs/audit) and [Logging](/logging/docs) . You can see Kubernetes audit log entries in your Google Cloud project.\nIn addition to entries written by Kubernetes, your project's audit logs have entries written by GKE.\nTo differentiate your CDE and non-CDE workloads, we recommend that you add labels to your GKE pods that will percolate into metrics and logs emitted from those workloads.\nAudit logs record the following details for each auditable event:- User identification\n- Type of event\n- Date and time\n- Success or failure indication\n- Origination of event\n- Identity or name of affected data, system component, resource, or service (for example, name and protocol)\nEvery audit log entry in Logging is an object of type [LogEntry](/logging/docs/reference/v2/rest/v2/LogEntry) that contains the following fields:\n- A payload, which is of the`protoPayload`type. The payload of each audit log entry is an object of type [AuditLog](/logging/docs/reference/audit/auditlog/rest/Shared.Types/AuditLog) . You can find the user identity in the`AuthenticationInfo`field of`AuditLog`objects.\n- The specific event, which you can find in the`methodName`field of`AuditLog`.\n- A timestamp.\n- The event status, which you can find in the`response`objects in the`AuditLog`object.\n- The operation request, which you can find in the`request`and`requestMetadata`objects in the`AuditLog`object.\n- The service that is going to be performed, which you can find in the`AuditData`object in`serviceData`.\n### Requirement 11\nTest security of systems and networks regularly.\nExternal and internal vulnerabilities are regularly identified, prioritized, and addressed.\nInternal vulnerability scans are performed as follows:- At least once every three months.\n- High-risk and critical vulnerabilities (per the entity's vulnerability risk rankings defined at Requirement 6.3.1) are resolved.\n- Rescans are performed that confirm all high-risk and critical vulnerabilities (as noted above) have been resolved.\n- Scan tool is kept up to date with latest vulnerability information.\n- Scans are performed by qualified personnel and organizational independence of the tester exists.\nArtifact Analysis [vulnerability scanning](/container-registry/docs/container-analysis#vulnerability_scanning) performs the following types of vulnerability scanning for the images in Container Registry:\n- **Initial scanning** . When you first activate the Artifact Analysis API, it scans your images in Container Registry and extracts package manager, image basis, and vulnerability occurrences for the images.\n- **Incremental scanning** . Artifact Analysis scans new images when they're uploaded to Container Registry.\n- **Continuous analysis** : As Artifact Analysis receives new and updated vulnerability information from vulnerability sources, it reruns analysis of containers to keep the list of vulnerability occurrences for already scanned images up to date.Network intrusions and unexpected file changes are detected and responded to.\nIntrusion-detection and/or intrusion prevention techniques are used to detect and/or prevent intrusions into the network as follows:- All traffic is monitored at the perimeter of the CDE.\n- All traffic is monitored at critical points in the CDE.\n- Personnel are alerted to suspected compromises.\n- All intrusion-detection and prevention engines, baselines, and signatures are kept up to date.\nGoogle Cloud [Packet Mirroring](/vpc/docs/packet-mirroring#enterprise_security) can be used with [Cloud IDS](/intrusion-detection-system) to detect network intrusions. Google Cloud packet mirroring forwards all network traffic from your Compute Engine VMs or Google Cloud clusters to a designated address. Cloud IDS can consume this mirrored traffic to detect a wide range of threats including exploit attempts, port scans, buffer overflows, protocol fragmentation, command and control (C2) traffic, and malware.\n[Security Command Center](/security-command-center) gives you centralized visibility into the security state of Google Cloud services (including GKE) and assets across your whole organization, which makes it easier to prevent, detect, and respond to threats. By using Security Command Center, you can see when high-risk threats such as malware, cryptomining, unauthorized access to Google Cloud resources, outgoing DDoS attacks, port scanning, and brute-force SSH have been detected based on your Cloud Logging logs.\n## Maintain an information security policy\nA strong security policy sets the security tone and informs people what is expected of them. In this case, \"people\" refers to full-time and part-time employees, temporary employees, contractors, and consultants who have access to your CDE.\n### Requirement 12\nSupport information security with organizational policies and programs.\nFor information about requirement 12, see the [Google Cloud PCI Shared Responsibility Matrix](https://services.google.com/fh/files/misc/gcp_pci_dss_v4_responsibility_matrix.pdf) .\n## Cleaning up\nIf you used any resources while following this article\u2014for example, if you started new VMs or used the Terraform scripts\u2014you can avoid incurring charges to your Google Cloud account by deleting the project where you used those resources.\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Learn more about [PCI Data Security Standard compliance](/solutions/pci-dss-compliance-in-gcp) .\n- Try the [Terraform Starter Kit](https://github.com/GoogleCloudPlatform/terraform-pci-starter) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}