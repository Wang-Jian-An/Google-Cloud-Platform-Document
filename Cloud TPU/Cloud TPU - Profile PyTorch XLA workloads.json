{"title": "Cloud TPU - Profile PyTorch XLA workloads", "url": "https://cloud.google.com/tpu/docs/pytorch-xla-performance-profiling-tpu-vm", "abstract": "# Cloud TPU - Profile PyTorch XLA workloads\n# Profile PyTorch XLA workloads\nProfiling is a way to analyze and improve the performance of models. Although there is much more to it, sometimes it helps to think of profiling as timing operations and parts of the code that run on both devices (TPUs) and hosts (CPUs). This guide provides a quick overview of how to profile your code for training or inference. For more information on how to analyze generated profiles, please refer to the following guides.\n- [PyTorch XLA performance debugging on TPU VMs - part 1](https://cloud.google.com/blog/topics/developers-practitioners/pytorchxla-performance-debugging-tpu-vm-part-1) \n- [PyTorch XLA performance debugging on TPU VMs - part 2](https://cloud.google.com/blog/topics/developers-practitioners/pytorchxla-performance-debugging-cloud-tpu-vm-part-ii) \n- [PyTorch XLA performance debugging on TPU VMs - part 3](https://cloud.google.com/blog/topics/developers-practitioners/pytorchxla-performance-debugging-cloud-tpu-vm-part-iii) ", "content": "## Get Started\n### Create a TPU\n- Export environment variables:```\n$ export TPU_NAME=your_tpu_name$ export ZONE=us-central2-b$ export PROJECT_ID=project-id$ export ACCELERATOR_TYPE=v4-8$ export RUNTIME_VERSION=tpu-vm-v4-pt-2.0\n```\n- Launch the TPU resources```\n$ gcloud compute tpus tpu-vm create ${TPU_NAME} \\--zone us-central2-b \\--accelerator-type ${ACCELERATOR_TYPE} \\--version ${RUNTIME_VERSION} \\--project $PROJECT_ID \\--subnetwork=tpusubnet\n```\n- Move your code to your home directory on the TPU VM using the [gcloud scp](https://cloud.google.com/sdk/gcloud/reference/compute/tpus/tpu-vm/scp) command. For example:```\n$ gcloud compute tpus tpu-vm scp my-code-file ${TPU_NAME}: --zone ${ZONE}\n```## Profiling\n**Note:** A v4-8 includes 4 TPU devices. Each worker profiles only one TPU device out of the 4 available devices. Profiling all 4 TPU devices for each worker will be supported soon.\nA profile can be captured manually through `capture_profile.py` or programmatically from within the training script using the `torch_xla.debug.profiler` APIs.\n### Starting the Profile Server\nIn order to capture a profile, a profile server must be running within the training script. Start a server with a port number of your choice, for example `9012` as shown in the following command.\n**Note:** You can refer to this [example script](https://github.com/pytorch/xla/blob/master/test/test_profile_mp_mnist.py) that includes the following steps.\n```\nimport torch_xla.debug.profiler as xpserver = xp.start_server(9012)\n```\nThe server can be started right at the beginning of your `main` function.\n**Note:** If you are using the [Lightning](https://www.pytorchlightning.ai/index.html) module, instead of starting a server, pass `profiler=\"xla\"` to the `trainer` , and it will automatically start the server at port 9012.\nYou can now capture profiles as described in the following section. The script profiles everything that happens on one TPU device.\n### Adding Traces\nIf you would also like to profile operations on the host machine, you can add `xp.StepTrace` or `xp.Trace` in your code. These functions trace the Python code on the host machine. (You can think of this as measuring how much time it takes to execute the Python code on the host (CPU) before passing the \"graph\" to the TPU device. So it is mostly useful for analysing tracing overhead). You can add this inside the training loop where the code processes batches of data, for example,\n```\nfor step, batch in enumerate(train_dataloader):\u00a0 \u00a0 with xp.StepTrace('Training_step', step_num=step): \u00a0 \u00a0 \u00a0 \u00a0...\n```\nor wrap individual parts of the code with\n```\n\u00a0with xp.Trace('loss'): \u00a0 \u00a0 loss = ...\n```\n**Important:** Avoid wrapping `xm.mark_step()` with `xp.Trace` . The code will crash in that case with an error message similar to: `\"RuntimeError: Expecting scope to be empty but it is train_loop.\"` `xp.StepTrace` will not have this issue since it automatically adds `xm.mark_step()` when exiting the scope. Moreover, you may get this error when you use [torchdynamo](https://pytorch.org/docs/stable/torch.compiler_deepdive.html) because torchdynamo may internally invoke `xm.mark_step` . The current workaround is not to use `xp.Trace` .\nIf you are using Lighting you can skip adding traces as it is done automatically in some parts of the code. However if you want to add additional traces, you are welcome to insert them inside the training loop.\nYou will be able to capture device activity after the initial compilation; wait until the model starts its training or inference steps.\n### Manual Capture\nThe `capture_profile.py` script from the Pytorch XLA repository enables quickly capturing a profile. You can do this by copying the [capture profile file](https://raw.githubusercontent.com/pytorch/xla/master/scripts/capture_profile.py) directly to your TPU VM. The following command copies it to the home directory.\n```\n$ gcloud compute tpus tpu-vm ssh ${TPU_NAME} \\--zone us-central2-b \\--worker=all \\--command=\"wget https://raw.githubusercontent.com/pytorch/xla/master/scripts/capture_profile.py\"\n```\nWhile training is running, execute the following to capture a profile:\n```\n$ gcloud compute tpus tpu-vm ssh ${TPU_NAME} \\--zone us-central2-b \\--worker=all \\--command=\"python3 capture_profile.py --service_addr \"localhost:9012\" --logdir ~/profiles/ --duration_ms 2000\"\n```\nThis command saves `.xplane.pb` files in the `logdir` . You can change the logging directory `~/profiles/` to your preferred location and name. It is also possible to directly save in the Cloud Storage bucket. To do that, set `logdir` to be `gs://your_bucket_name/` .\n### Programmatic Capture\nRather than capturing the profile manually by triggering a script, you can configure your training script to automatically trigger a profile by using the [torch_xla.debug.profiler.trace_detached](https://github.com/pytorch/xla/blob/6d73ca8ecf8e5ef7a76296ac95dbbbe78878f6b3/torch_xla/debug/profiler.py#L93) API within your train script.\n**Note:** The `trace_detached` API is only available in nightly builds or after the 2.2 release. To programmatically capture in earlier torch_xla versions, you can copy the body of the method to create the background capture thread directly.\nAs an example, to automatically capture a profile at a specific epoch and step, you can configure your training script to consume `PROFILE_STEP` , `PROFILE_EPOCH` , and `PROFILE_LOGDIR` environment variables:\n```\nimport osimport torch_xla.debug.profiler as xp# Within the training script, read the step and epoch to profile from the# environment.profile_step = int(os.environ.get('PROFILE_STEP', -1))profile_epoch = int(os.environ.get('PROFILE_EPOCH', -1))...for epoch in range(num_epoch):\u00a0 \u00a0...\u00a0 \u00a0for step, data in enumerate(epoch_dataloader):\u00a0 \u00a0 \u00a0 if epoch == profile_epoch and step == profile_step:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0profile_logdir = os.environ['PROFILE_LOGDIR']\u00a0 \u00a0 \u00a0 \u00a0 \u00a0# Use trace_detached to capture the profile from a background thread\u00a0 \u00a0 \u00a0 \u00a0 \u00a0xp.trace_detached('localhost:9012', profile_logdir)\u00a0 \u00a0 \u00a0 ...\n```\nThis will save the `.xplane.pb` files in the directory specified by the `PROFILE_LOGDIR` environment variable.\n### Analysis in TensorBoard\nTo further analyze profiles you can use [TensorBoard](https://www.tensorflow.org/tensorboard) with the [TPU TensorBoard plug-in](/tpu/docs/profile-tpu-vm#install-tensorboard) either on the same or on another machine (recommended).\nTo run TensorBoard on a remote machine, connect to it using SSH and enable port forwarding. For example,\n```\n$ ssh -L 6006:localhost:6006 remote server address\n```\nor\n```\n$ gcloud compute tpus tpu-vm ssh $TPU_NAME --zone=$ZONE --ssh-flag=\"-4 -L 6006:localhost:6006\"\n```\nOn your remote machine, install the required packages and launch TensorBoard (assuming you have profiles on that machine under `~/profiles/` ). If you stored the profiles in another directory or Cloud Storage bucket, make sure to specify paths correctly, for example, `gs://your_bucket_name/profiles` .\n```\n(vm)$ pip install tensorflow tensorboard-plugin-profile \n```\n```\n(vm)$ tensorboard --logdir ~/profiles/ --port 6006\n```\n**Note:** It is important to have `.xplane` files in the same nested structure that is generated when you capture a profile. For example: `~/profiles/plugins/profile/2023_04_10_21_40_22/localhost_9012.xplane.pb`\n**Note:** If you get \"duplicate plugins\" error, this could be because there are multiple packages with TensorBoard. Uninstall all those packages and run the `pip install` commands again. For example, you might need to run:\n```\n(vm)$ pip uninstall tensorflow tf-nightly tensorboard tb-nightly tbp-nightly\n```\nIn your local browser go to: [http://localhost:6006/](http://localhost:6006/) and choose `PROFILE` from the drop-down menu to load your profiles.\nRefer to [TPU tools](/tpu/docs/cloud-tpu-tools) for information on the TensorBoard tools and how to interpret the output.", "guide": "Cloud TPU"}