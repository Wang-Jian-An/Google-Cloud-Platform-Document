{"title": "Cloud TPU - Training Resnet50 on Cloud TPU with PyTorch", "url": "https://cloud.google.com/tpu/docs/tutorials/resnet-pytorch", "abstract": "# Cloud TPU - Training Resnet50 on Cloud TPU with PyTorch\nThis tutorial shows you how to train the ResNet-50 model on a Cloud TPU device with PyTorch. You can apply the same pattern to other TPU-optimised image classification models that use PyTorch and the ImageNet dataset.\nThe model in this tutorial is based on [Deep Residual Learning for ImageRecognition](https://arxiv.org/pdf/1512.03385.pdf) , which first introduces the residual network (ResNet) architecture. The tutorial uses the 50-layer variant, ResNet-50, and demonstrates training the model using [PyTorch/XLA](https://github.com/pytorch/xla) .\n **Warning:** This tutorial uses a third-party dataset. Google provides no representation, warranty, or other guarantees about the validity, or any other aspects of this dataset.\n", "content": "## Objectives\n- Prepare the dataset.\n- Run the training job.\n- Verify the output results.\n## CostsIn this document, you use the following billable components of Google Cloud:- Compute Engine\n- Cloud TPU\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \n## Before you beginBefore starting this tutorial, check that your Google Cloud project is correctly set up.- This walkthrough uses billable components of Google Cloud. Check the [Cloud TPU pricing page](/tpu/docs/pricing) to  estimate your costs. Be sure to [clean up](#clean_up) resources you create when you've finished with them to avoid unnecessary  charges.\n## Create a TPU VM\n- Open a Cloud Shell window. [Open Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Create a TPU VM```\ngcloud compute tpus tpu-vm create your-tpu-name \\--accelerator-type=v4-8 \\--version=tpu-ubuntu2204-base \\--zone=us-central2-b \\--project=your-project\n``` **Note:** The first time you run a command in a new Cloud Shell VM, an `Authorize Cloud Shell` page is displayed. Click `Authorize` at the bottom of the page to allow `gcloud` to make Google Cloud API calls with your credentials.\n- Connect to your TPU VM using SSH:```\ngcloud compute tpus tpu-vm ssh \u00a0your-tpu-name --zone=us-central2-b \u00a0\n```\n- Install PyTorch/XLA on your TPU VM:```\n(vm)$ pip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 torchvision -f https://storage.googleapis.com/libtpu-releases/index.html\n```\n- Clone the [PyTorch/XLA github repo](https://github.com/pytorch/xla) ```\n(vm)$ git clone --depth=1 --branch r2.2 https://github.com/pytorch/xla.git\n```\n- Run the training script with fake data```\n(vm) $ PJRT_DEVICE=TPU python3 xla/test/test_train_mp_imagenet.py --fake_data --batch_size=256 --num_epochs=1\n```\nIf you are able to train the model using fake data, you can try training on real data, such as ImageNet. For instructions on downloading [ImageNet](https://image-net.org/index.php) , see [Downloading ImageNet](/tpu/docs/imagenet-setup) . In the training script command, the `--datadir` flag specifies the location of the dataset on which to train. The following command assumes the ImageNet dataset is located in `~/imagenet` .\n```\n\u00a0 \u00a0(vm) $ PJRT_DEVICE=TPU python3 xla/test/test_train_mp_imagenet.py \u00a0--datadir=~/imagenet --batch_size=256 --num_epochs=1\u00a0 \u00a0\n```## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.- Disconnect from the TPU VM:```\n(vm) $ exit\n```Your prompt should now be `username@projectname` , showing you are in the Cloud Shell.\n- Delete your TPU VM.```\n$ gcloud compute tpus tpu-vm delete resnet50-tutorial \\\u00a0 \u00a0--zone=us-central2-b \n```\n## What's nextTry the PyTorch colabs:- [Getting Started with PyTorch on Cloud TPUs](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb) \n- [Training MNIST on TPUs](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/mnist-training.ipynb) \n- [Training ResNet18 on TPUs with Cifar10 dataset](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/resnet18-training.ipynb) \n- [Inference with Pretrained ResNet50 Model](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/resnet50-inference.ipynb) \n- [Fast Neural Style Transfer](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/style_transfer_inference.ipynb) \n- [MultiCore Training AlexNet on Fashion MNIST](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/multi-core-alexnet-fashion-mnist.ipynb) \n- [Single Core Training AlexNet on Fashion MNIST](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/single-core-alexnet-fashion-mnist.ipynb)", "guide": "Cloud TPU"}