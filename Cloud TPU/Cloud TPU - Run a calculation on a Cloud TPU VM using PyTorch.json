{"title": "Cloud TPU - Run a calculation on a Cloud TPU VM using PyTorch", "url": "https://cloud.google.com/tpu/docs/run-calculation-pytorch", "abstract": "# Cloud TPU - Run a calculation on a Cloud TPU VM using PyTorch\n# Run a calculation on a Cloud TPU VM using PyTorch\nThis quickstart shows you how to create a Cloud TPU, install PyTorch and run a simple calculation on a Cloud TPU. For a more in depth tutorial showing you how to train a model on a Cloud TPU see one of the [Cloud TPU PyTorch Tutorials](/tpu/docs/tutorials/pytorch-pod) .\n", "content": "## Before you beginBefore you follow this quickstart, you must create a Google Cloud Platform account, install the Google Cloud CLI. and configure the `gcloud` command. For more information, see [Set up an account and a Cloud TPU project](/tpu/docs/setup-gcp-account) .## Create a Cloud TPU with gcloud **Note:** To determine which TPU software version to specify when launching the TPU VM, refer to [Cloud TPU software versions.](/tpu/docs/supported-tpu-versions#tpu_software_versions) \nTo create a TPU VM in the default user project, network and compute/zone run:\n```\n$ gcloud compute tpus tpu-vm create tpu-name \\\u00a0 \u00a0--zone=us-central1-b \\\u00a0 \u00a0--accelerator-type=v3-8 \\\u00a0 \u00a0--version=tpu-ubuntu2204-base\n```While creating your TPU, you can pass the additional `--network` and `--subnetwork` flags if you want to specify the default network and subnetwork. If you do not want to use the default network, you must pass the `--network` flag. The `--subnetwork` flag is optional and can be used to specify a default subnetwork for whatever network you are using (default or user-specified). See the `gcloud` [API reference page](/sdk/gcloud/reference/compute/tpus/tpu-vm/create) for details on these flags.\n **Note:** If you have more than one project, you must specify the project ID with the `--project` flag.## Connect to your Cloud TPU VM\n```\n\u00a0 \u00a0$ gcloud compute tpus tpu-vm ssh tpu-name --zone=us-central1-b\n```\n## Install PyTorch/XLA on your TPU VM\n```\n\u00a0 \u00a0(vm)$ pip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 torchvision -f https://storage.googleapis.com/libtpu-releases/index.html\u00a0 \u00a0\n```\n## Set TPU runtime configurationEnsure that the PyTorch/XLA runtime uses the TPU.\n```\n\u00a0 \u00a0(vm) $ export PJRT_DEVICE=TPU\n```\n## Perform a simple calculation:\n- Create a file named `tpu-test.py` in the current directory and copy and paste the following script into it.```\nimport torchimport torch_xla.core.xla_model as xmdev = xm.xla_device()t1 = torch.randn(3,3,device=dev)t2 = torch.randn(3,3,device=dev)print(t1 + t2)\n```\n- Run the script:```\n\u00a0 (vm)$ python3 tpu-test.py\n```Output from the script shows the result of the computation:```\ntensor([[-0.2121, \u00a01.5589, -0.6951],\u00a0 \u00a0 \u00a0 \u00a0 [-0.7886, -0.2022, \u00a00.9242],\u00a0 \u00a0 \u00a0 \u00a0 [ 0.8555, -1.8698, \u00a01.4333]], device='xla:1')\n```\n## Clean upTo avoid incurring charges to your Google Cloud account for   the resources used on this page, follow these steps.- Disconnect from the Compute Engine instance, if you have not already done so:```\n(vm)$ exit\n```Your prompt should now be `username@projectname` , showing you are in the Cloud Shell.\n- Delete your Cloud TPU.```\n$ gcloud compute tpus tpu-vm delete tpu-name \\\u00a0 --zone=us-central1-b\n```\nThe output of this command should confirm that your TPU has been deleted.\n## What's nextRead more about Cloud TPU VMs:- [Run PyTorch code on TPU Pod slices](/tpu/docs/pytorch-pods) \n- [Manage TPUs](/tpu/docs/managing-tpus-tpu-vm) \n- [Cloud TPU System Architecture](/tpu/docs/system-architecture-tpu-vm) \n- [PyTorch/XLA documentation](https://pytorch.org/xla/release/1.7/index.html)", "guide": "Cloud TPU"}