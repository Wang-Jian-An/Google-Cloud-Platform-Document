{"title": "Cloud TPU - System architecture", "url": "https://cloud.google.com/tpu/docs/system-architecture-tpu-vm", "abstract": "# Cloud TPU - System architecture\n# System architecture\nTensor Processing Units (TPUs) are application specific integrated circuits (ASICs) designed by Google to accelerate machine learning workloads. Cloud TPU is a Google Cloud service that makes TPUs available as a scalable resource.\nTPUs are designed to perform matrix operations quickly making them ideal for machine learning workloads. You can run machine learning workloads on TPUs using frameworks such as [TensorFlow](https://www.tensorflow.org/) , [Pytorch](/tpu/docs/tutorials/pytorch-pod) , and [JAX](https://jax.readthedocs.io/en/latest/) .\n", "content": "## Cloud TPU terms\nIf you are new to Cloud TPUs, check out the [TPU documentation home](https://cloud.google.com/tpu/docs) . The following sections explain terms and related concepts used in this document.\n### Batch inference\nBatch or offline inference refers to doing inference outside of production pipelines typically on a bulk of inputs. Batch inference is used for offline tasks such as data labeling and also for evaluating the trained model. Latency SLOs are not a priority for batch inference.\n### TPU chip\nA TPU chip contains one or more TensorCores. The number of TensorCores depends on the version of the TPU chip. Each TensorCore consists of one or more matrix-multiply units (MXUs), a vector unit, and a scalar unit.\nAn MXU is composed of 128 x 128 multiply-accumulators in a [systolic array](https://en.wikipedia.org/wiki/Systolic_array) . MXUs provide the bulk of the compute power in a TensorCore. Each MXU is capable of performing 16K multiply-accumulate operations per cycle. All multiplies take [bfloat16](/tpu/docs/bfloat16) inputs, but all accumulations are performed in FP32 number format.\nThe vector unit is used for general computation such as activations and softmax. The scalar unit is used for control flow, calculating memory addresses, and other maintenance operations.\n### TPU cube\nA 4x4x4 topology. This is only applicable to 3D topologies (beginning with the v4 TPU version).\n### Inference\nInference is the process of using a trained model to make predictions on new data. It is used by the [serving](#serving) process.\n### Multislice versus single slice\nMultislice is a group of slices, extending TPU connectivity beyond the inter-chip interconnect (ICI) connections and leveraging the data-center network (DCN) for transmitting data beyond a slice. Data within each slice is still transmitted by ICI. Using this hybrid connectivity, Multislice enables parallelism across slices and lets you use a greater number of TPU cores for a single job than what a single slice can accommodate.\nTPUs can be used to run a job either on a single slice or multiple slices. Refer to the [Multislice introduction](/tpu/docs/multislice-introduction) for more details.\n### Cloud TPU ICI resiliency\nICI resiliency helps improve fault tolerance of optical links and optical circuit switches (OCS) that connect TPUs between [cubes](#cube) . (ICI connections within a cube use copper links that are not impacted). ICI resiliency allows ICI connections to be routed around OCS and optical ICI faults. As a result, it improves the scheduling availability of TPU slices, with the trade-off of temporary degradation in ICI performance.\nSimilar to Cloud TPU v4, ICI resiliency is enabled by default for v5p slices that are one cube or larger:\n- v5p-128 when specifying acclerator type\n- 4x4x4 when specifying accelerator config\n### Queued resource\nA representation of TPU resources, used to enqueue and manage a request for a single-slice or multi-slice TPU environment. See [Queued Resources user guide](/tpu/docs/queued-resources) for more information.\n### Serving\nServing is the process of deploying a trained machine learning model to a production environment where it can be used to make predictions or decisions. Latency and service-level availability are important for serving.\n### Single host and multi host\nA TPU host is a VM that runs on a physical computer connected to TPU hardware. TPU workloads can use one or more host.\nA single-host workload is limited to one TPU VM. A multi-host workload distributes training across multiple TPU VMs.\n### Slices\nA Pod slice is a collection of chips all located inside the same TPU Pod connected by high-speed inter chip interconnects (ICI). Slices are described in terms of chips or TensorCores, depending on the TPU version.\nand also refer to slice shapes.\n### SparseCore\nv5p includes four SparseCores per chip which are Dataflow processors that accelerate models relying on embeddings found in recommendation models.\n### TPU Pod\nA TPU Pod is a contiguous set of TPUs grouped together over a specialized network. The number of TPU chips in a TPU Pod is dependent on the TPU version.\n### TPU VM or worker\nA virtual machine running Linux that has access to the underlying TPUs. A TPU VM is also known as a .\n### TensorCores\nTPU chips have one or two TensorCores to run matrix multiplication. For more information about TensorCores, see this [ACMarticle](https://dl.acm.org/doi/pdf/10.1145/3360307) .\n### Worker\nSee [TPU VM](#tpu-vm) .\n## TPU versions\nThe exact architecture of a TPU chip depends on the TPU version that you use. Each TPU version also supports different slice sizes and configurations. For more information about the system architecture and supported configurations, see the following pages:\n- [TPU v5p](/tpu/docs/v5p) \n- [TPU v5e](/tpu/docs/v5e) \n- [TPU v4](/tpu/docs/v4) \n- [TPU v3](/tpu/docs/v3) \n- [TPU v2](/tpu/docs/v2) \n**Note:** You can run the same code on different versions of TPUs as long as the TPUs have the same number of TensorCores or chips (for example, `v3-128` and `v4-128` ). However, if you change to a TPU type with a larger or smaller number of TensorCores or chips, you will need to perform significant tuning and optimization. For more information, see [Training on TPUPods](/tpu/docs/training-on-tpu-pods) .\n## Cloud TPU VM architectures\nHow you interact with the TPU host (and the TPU board) depends upon the TPU VM architecture you're using: TPU Nodes or TPU VMs.\n### TPU VM architecture\nThe TPU VM architecture lets you directly connect to the VM physically connected to the TPU device using SSH. You have root access to the VM, so you can run arbitrary code. You can access compiler and runtime debug logs and error messages.### TPU Node architecture\n**Important:** TPU v4 and newer are not supported with the TPU Node architecture.\nThe TPU Node architecture consists of a user VM that communicates with the TPU host over gRPC. When using this architecture, you cannot directly access the TPU Host, making it difficult to debug training and TPU errors.\n## What's next\n- [Quickstarts](/tpus/docs/quick-starts) \n- [Tutorials](/tpus/docs/tutorials)", "guide": "Cloud TPU"}