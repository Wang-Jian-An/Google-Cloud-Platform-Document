{"title": "Cloud TPU - Converting an image classification dataset for use with Cloud TPU", "url": "https://cloud.google.com/tpu/docs/classification-data-conversion", "abstract": "# Cloud TPU - Converting an image classification dataset for use with Cloud TPU\nThis tutorial describes how to use the [image classification data convertersample](https://github.com/tensorflow/tpu/tree/master/tools/data_converter/image_classification) script to convert a raw image classification dataset into the [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format used to train Cloud TPU models.\n [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) s make reading large files from Google Cloud Storage more efficient than reading each image as an individual file. You can use TFRecord anywhere you are using a `tf.data.Dataset` pipeline.\nSee the following TensorFlow documents for more information on using TFRecord:- [TFRecord and tf.train.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord) \n- [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) \n- [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data) \n- [PyTorch TFRecord reader and writer](https://github.com/vahidk/tfrecord) \nIf you use the PyTorch or JAX framework, and are not using Google Cloud storage for your dataset storage, you might not get the same advantage from TFRecords.", "content": "## Conversion overviewThe [image classification folder within the data converter repository](https://github.com/tensorflow/tpu/tree/master/tools/data_converter) on GitHub contains the `converter` script, `image_classification_data.py` , and a sample implementation, `simple_example.py` , you can copy and modify to do your own data conversion.\nThe image classification data converter sample defines two classes, `ImageClassificationConfig` and `ImageClassificationBuilder` . These classes are defined in `tpu/tools/data_converter/image_classification_data.py` .\n`ImageClassificationConfig` is an abstract base class. You subclass `ImageClassificationConfig` to define the configuration needed to instantiate an `ImageClassificationBuilder` .\n`ImageClassificationBuilder` is a [TensorFlow dataset builder](https://www.tensorflow.org/datasets/api_docs/python/tfds/builder) for image classification datasets. It is a subclass of [tdfs.core.GeneratorBasedBuilder](https://www.tensorflow.org/datasets/api_docs/python/tfds/core/DatasetBuilder) . It retrieves data examples from your dataset and converts them to TFRecords. The TFRecords are written to a path specified by the `data_dir` parameter to the `__init__` method of `ImageClassificationBuilder` .\nIn [simple_example.py](https://github.com/tensorflow/tpu/blob/master/tools/data_converter/image_classification/simple_example.py) , `SimpleDatasetConfig` subclasses `ImageClassificationConfig` , implementing properties that define the supported modes, number of image classes, and an example generator that yields a dictionary containing image data and an image class for each example in the dataset.\nThe `main()` function creates a dataset of randomly generated image data and instantiates a `SimpleDatasetConfig` object specifying the number of classes and the path to the dataset on disk. Next, `main()` instantiates an `ImageClassificationBuilder` object, passing in the `SimpleDatasetConfig` instance. Finally, `main()` calls `download_and_prepare()` . When this method is called, the `ImageClassificationBuilder` instance uses the data example generator implemented by `SimpleDatasetConfig` to load each example and saves them to a series of TFRecord files.\nFor a more detailed explanation, please see the [Classification Converter Notebook](https://github.com/tensorflow/tpu/blob/master/tools/colab/image_classification_converter.ipynb) .\n## Before you begin## Modifying the data conversion sample to load your datasetTo convert your dataset into TFRecord format, subclass the `ImageClassificationConfig` class defining the following properties:- num_labels  - returns the number of image classes\n- supported_modes - returns a list of modes supported by your dataset     (for example: test, train, and validate)\n- text_label_map - returns a dictionary that models the mapping between a     text class label and an integer class label     (SimpleDatasetConfig does not use this property, because     it does not require a mapping)\n- download_path - the path from which to download your dataset     (SimpleDatasetConfig does not use this property, the     example_generator loads the data from disk)\nImplement the example_generator generator function. This method must yield a dictionary containing the image data and the image class name for each example. `ImageClassificationBuilder` uses the `example_generator()` function to retrieve each example and writes them to disk in TFRecord format.## Running the data conversion sample\n- Create a Cloud Storage bucket using the following command: **Note:** In the following command, replace with the name you want to assign to your bucket.```\ngsutil mb -p ${PROJECT_ID} -c standard -l us-central1 gs://bucket-name\n```\n- Launch a Compute Engine VM using the `gcloud` command.```\n$ gcloud compute tpus execution-groups create \\\u00a0--vm-only \\\u00a0--zone=us-central1-b \\\u00a0--name=imageclassificationconverter \\\u00a0--tf-version=2.5.0\n``` **Note:** If you are not connected to the Compute Engine instance, you can connect by running the following command:```\ngcloud compute ssh imageclassificationconverter --zone=us-central1-b \n```From this point on, a prefix of `(vm)$` means you should run the command on the Compute Engine VM instance.\n- Install required packages.```\n(vm)$ pip3 install opencv-python-headless pillow\n```\n- Create the following environment variables used by the script.```\n(vm)$ export STORAGE_BUCKET=gs://bucket-name\n``````\n(vm)$ export CONVERTED_DIR=$HOME/tfrecords(vm)$ export GENERATED_DATA=$HOME/data(vm)$ export GCS_CONVERTED=$STORAGE_BUCKET/data_converter/image_classification/tfrecords(vm)$ export GCS_RAW=$STORAGE_BUCKET/image_classification/raw(vm)$ export PYTHONPATH=\"$PYTHONPATH:/usr/share/tpu/models\"\n```\n- Change to the `data_converter` directory.```\n(vm)$ cd /usr/share/tpu/tools/data_converter\n```\n## Running the data converter on a fake datasetThe `simple_example.py` script is located in the `image_classification` folder of the data converter sample. Running the script with the following parameters generates a set of fake images and converts them into TFRecords.\n```\n(vm)$ python3 image_classification/simple_example.py \\\u00a0 --num_classes=1000 \\\u00a0 --data_path=$GENERATED_DATA \\\u00a0 --generate=True \\\u00a0 --num_examples_per_class_low=10 \\\u00a0 --num_examples_per_class_high=11 \\\u00a0 --save_dir=$CONVERTED_DIR\n```## Running the data converter on one of our raw datasets\n- Create an environment variable for the location of the raw data.```\n(vm)$ export GCS_RAW=gs://cloud-tpu-test-datasets/data_converter/raw_image_classification\n```\n- Run the `simple_example.py` script.```\n(vm)$ python3 image_classification/simple_example.py \\--num_classes=1000 \\--data_path=$GCS_RAW \\--generate=False \\--save_dir=$CONVERTED_DIR\n```\nThe `simple_example.py` script takes the following parameters:- `num_classes`refers to the number of classes in the dataset. We're using 1000 here to match ImageNet format.\n- `generate`determines whether or not to generate the raw data.\n- `data_path`refers to the path where the data is generated if`generate=True`or the path where the raw data is stored if`generate=False`.\n- `num_examples_per_class_low`and`num_examples_per_class_high`determine how many examples per class to generate. The script generates a random number of examples in this range.\n- `save_dir`refers to where the saved TFRecords are saved. In order to train a model on Cloud TPU, the data must be stored on Cloud Storage. This can be on Cloud Storage or on the VM.\n## Renaming and moving the TFRecords to Cloud StorageThe following example uses the converted data with the ResNet model.- Rename the TFRecords to the same format as ImageNet TFRecords:```\n(vm)$ cd $CONVERTED_DIR/image_classification_builder/Simple/0.1.0/(vm)$ sudo apt install rename \n``````\n(vm)$ rename -v 's/image_classification_builder-(\\w+)\\.tfrecord/$1/g' *\n```\n- Copy the TFRecords to Cloud Storage:```\n(vm)$ gsutil -m cp train* $GCS_CONVERTED(vm)$ gsutil -m cp validation* $GCS_CONVERTED\n```", "guide": "Cloud TPU"}