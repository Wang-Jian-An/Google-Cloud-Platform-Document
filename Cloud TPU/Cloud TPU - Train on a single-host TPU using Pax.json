{"title": "Cloud TPU - Train on a single-host TPU using Pax", "url": "https://cloud.google.com/tpu/docs/tutorials/LLM/single-device-pax", "abstract": "# Cloud TPU - Train on a single-host TPU using Pax\nThis document provides a brief introduction to working with Pax on a single-host TPU (v2-8, v3-8, v4-8).\n [Pax](https://github.com/google/paxml) is a framework to configure and run machine learning experiments on top of JAX. Pax focuses on simplifying ML at scale by sharing infrastructure components with existing ML frameworks and utilizing the [Praxis](https://github.com/google/praxis) modeling library for modularity.\n", "content": "## Objectives\n- Set up TPU resources for training\n- Install Pax on a single-host TPU\n- Train a transformer based SPMD model using Pax\n## Before you beginRun the following commands to configure `gcloud` to use your Cloud TPU project and install components needed to train a model running Pax on a single-host TPU.\n### Install the Google Cloud CLIThe Google Cloud CLI contains tools and libraries for interacting with Google Cloud CLI products and services. If you haven't installed it previously, install it now using the instructions in [Installingthe Google Cloud CLI](https://cloud.google.com/sdk/docs/install) .\n### Configure the gcloud command(Run `gcloud auth list` to see your currently available accounts).\n```\n$ gcloud config set account account\n```\n```\n$ gcloud config set project project-id\n```\n### Enable the Cloud TPU APIEnable the Cloud TPU API using the following `gcloud` command in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) . (You may also enable it from the [Google Cloud console](https://console.cloud.google.com/) ).\n```\n$ gcloud services enable tpu.googleapis.com\n```\nRun the following command to create a service identity (a service account).\n```\n$ gcloud beta services identity create --service tpu.googleapis.com\n```### Create a TPU VMWith Cloud TPU VMs, your model and code run directly on the TPU VM. You SSH directly into the TPU VM. You can run arbitrary code, install packages, view logs, and debug code directly on the TPU VM.\nCreate your TPU VM by running the following command from a Cloud Shell or your computer terminal where the [Google Cloud CLI](https://cloud.google.com/sdk/docs/install) is installed.\nSet the `zone` based on availability in your contract, reference [TPU Regions and Zones](https://cloud.google.com/tpu/docs/regions-zones) if needed.\nSet the `accelerator-type` variable to v2-8, v3-8, or v4-8.\n **Note:** All TPU types are not available in all zones, so refer to [TPU Regions and Zones](https://cloud.google.com/tpu/docs/regions-zones) to see which zones contain the TPU type you will be using.\nSet the `version` variable to `tpu-vm-base` for v2 and v3 TPU versions or `tpu-vm-v4-base` for v4 TPUs.\n```\n$ gcloud compute tpus tpu-vm create tpu-name \\--zone zone \\--accelerator-type accelerator-type \\--version version\n```\n### Connect to your Google Cloud TPU VMSSH into your TPU VM by using the following command:\n```\n$ gcloud compute tpus tpu-vm ssh tpu-name --zone zone\n```\nWhen you are logged into the VM, your shell prompt changes from `username@projectname` to `username@vm-name` :\n### Install Pax on the Google Cloud TPU VMInstall Pax, JAX and `libtpu` on your TPU VM using the following commands:\n```\n(vm)$ python3 -m pip install -U pip \\python3 -m pip install paxml jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n```\n### System checkTest that everything is installed correctly by checking that JAX sees the TPU cores:\n```\n(vm)$ python3 -c \"import jax; print(jax.device_count())\"\n```\nThe number of TPU cores is displayed, this should be 8 if you are using a v2-8 or v3-8, or 4 if you are using a v4-8.\n### Running Pax code on a TPU VMYou can now run any Pax code you wish. The **lm_cloud**  [examples](https://github.com/google/paxml/blob/main/paxml/tasks/lm/params/lm_cloud.py) are a great place to start running models in Pax. For example, the following commands train a [2B parameter](https://github.com/google/paxml/blob/main/paxml/tasks/lm/params/lm_cloud.py#L155) transformer based SPMD language model on synthetic data.\nThe following commands show training output for a SPMD language model. It trains for 300 step in approximately 20 minutes.\n```\n(vm)$ python3 .local/lib/python3.8/site-packages/paxml/main.py \u00a0--exp=tasks.lm.params.lm_cloud.LmCloudSpmd2BLimitSteps --job_log_dir=job_log_dir\n```\n **Note:** can be a directory in the local filesystem, or in Cloud Storage.\n **Note:** This config, `LmCloudSpmd2BLimitSteps` works out of the box for TPU v4-8 ( `ICI_MESH_SHAPE = [1, 4, 1]` ). If you are running on TPU v3-8 or v2-8 override with `ICI_MESH_SHAPE = [1, 8, 1]` .Losses and step times\nsummary tensor at step= `loss` = summary tensor at step= Steps/sec \n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\nWhen you are done with your TPU VM follow these steps to clean up your resources.\nDisconnect from the Compute Engine instance, if you have not already done so:\n```\n(vm)$ exit\n```\nDelete your Cloud TPU.\n```\n$ gcloud compute tpus tpu-vm delete tpu-name \u00a0--zone zone\n```\n## What's nextFor more information about Cloud TPU, see:- [Training on Cloud TPU Pods](https://cloud.google.com/tpu/docs/training-on-tpu-pods) \n- [Train on a TPU Pod slice using Pax](https://cloud.google.com/tpu/docs/pod-slice-pax) \n- [Cloud TPU System Architecture](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm) \n- [Run JAX code on TPU VM](https://cloud.google.com/tpu/docs/run-calculation-jax)", "guide": "Cloud TPU"}