{"title": "Cloud TPU - TPU v5p", "url": "https://cloud.google.com/tpu/docs/v5p?hl=zh-cn", "abstract": "# Cloud TPU - TPU v5p\n# TPU v5p\nThis document describes the architecture and supported configurations of Cloud TPU v5p.\n", "content": "## System architecture\nThis section describes the system architecture specific to the v5p version. Each TensorCore has four Matrix Multiply Units (MXU), a vector unit, and a scalar unit.\nThere are 8960 chips in a single v5p Pod. The largest job that can be scheduled is a 96 cube (6144 chip) job.\nThe following table shows the key specifications for a v5p.\n| Key specifications   | v5p values  |\n|:-----------------------------|:----------------|\n| Peak compute per chip (bf16) | 459 TFLOPs  |\n| HBM2e capacity and bandwidth | 95GB, 2765 GBps |\n| TPU Pod size     | 8960 chips  |\n| Interconnect topology  | 3D Torus  |\n| Interchip Interconnect BW | 4800 Gbps  |\n## Configurations\nA TPU v5p Pod is composed of 8960 chips interconnected with reconfigurable high-speed links. TPU v5p's flexible networking lets you connect the chips in a same-sized slice in multiple ways. When you create a TPU slice using the `gcloud compute tpus tpu-vm create` command, you specify its type and shape using the [AcceleratorType](#using-accelerator-type) or [AcceleratorConfig](#using-accelerator-config) parameters.\nThe following table shows the most common single-slice shapes supported with v5p, plus most (but not all) full cube shapes greater than 1 cube. The maximum v5p shape is 16x16x24 (6144 chips, 96 cubes).\n| 0   | 1   | 2  | 3  | 4    | 5   | 6     |\n|:------------|:----------|:--------|:--------|:--------------|:-----------|:------------------|\n| Slice Shape | VM Size | # Cores | # Chips | # of Machines | # of Cubes | Supports Twisted? |\n| 2x2x1  | Full host | 8  | 4  | 1    | nan  | nan    |\n| 2x2x2  | Full host | 16  | 8  | 2    | nan  | nan    |\n| 2x4x4  | Full host | 64  | 32  | 8    | nan  | nan    |\n| 4x4x4  | Full host | 128  | 64  | 16   | 1   | nan    |\n| 4x4x8  | Full host | 256  | 128  | 32   | 2   | Yes    |\n| 4x8x8  | Full host | 512  | 256  | 64   | 4   | Yes    |\n| 8x8x8  | Full host | 1024 | 512  | 128   | 8   | nan    |\n| 8x8x16  | Full host | 2048 | 1024 | 256   | 16   | Yes    |\n| 8x16x16  | Full host | 4096 | 2048 | 512   | 32   | Yes    |\n| 16x16x16 | Full host | 8192 | 4096 | 1024   | 64   | nan    |\n| 16x16x24 | Full host | 12288 | 6144 | 1536   | 96   | nan    |\nSingle slice training is supported for up to 6144 chips. It is extensible to 18432 chips using Multislice. See the [Cloud TPU Multislice Overview](/tpu/docs/multislice-introduction) for Multislice details.\n### Using the AcceleratorType parameter\nWhen you allocate TPU resources, you use the `--accelerator-type` argument to specify the number of TensorCores in a slice. `--accelerator-type` is a formatted string \" **v** `$VERSION_NUMBER` **p** `-$CORES_COUNT` \". For example, `v5p-32` specifies a v5p TPU slice with 32 TensorCores (16 chips).\nTo provision TPUs for a v5p training job, use one of the following accelerator types in your CLI or TPU API creation request:\n- v5p-8\n- v5p-16\n- v5p-32\n- v5p-64\n- v5p-128 (one full cube/rack)\n- v5p-256 (2 cubes)\n- v5p-512\n- v5p-1024 ... v5p-12288\n### Using the AcceleratorConfig parameter\nFor v5p and later Cloud TPU versions, AcceleratorConfig is used in much the same way it is with Cloud TPU [v4](/tpu/docs/v4#using-accelerator-config) The difference is that instead of specifying the TPU type as `--type=v4` , you specify it as the TPU version you are using (for example, `--type=v5p` for the v5p release).\n### Cloud TPU ICI resiliency\nICI resiliency helps improve fault tolerance of optical links and optical circuit switches (OCS) that connect TPUs between cubes. (ICI connections within a cube use copper links that are not impacted). ICI resiliency allows ICI connections to be routed around OCS and optical ICI faults. As a result, it improves the scheduling availability of TPU slices, with the trade-off of temporary degradation in ICI performance.\nSimilar to Cloud TPU v4, ICI resiliency is enabled by default for v5p slices that are one cube or larger:\n- v5p-128 when specifying accelerator type\n- 4x4x4 when specifying accelerator config\n### VM, host and slice properties\n| 0      | 1                      |\n|:----------------------|:----------------------------------------------------------------------------------------|\n| Property    | Value in a TPU                   |\n| # of v5p chips  | 4                      |\n| # of vCPUs   | 208 (only half is usable if using NUMA binding to avoid cross-NUMA performance penalty) |\n| RAM (GB)    | 448 (only half is usable if using NUMA binding to avoid cross-NUMA performance penalty) |\n| # of NUMA Nodes  | 2                      |\n| NIC Throughput (Gbps) | 200                      |\nRelationship between the number of TensorCores, chips, hosts/VMs, and cubes in a Pod:\n| 0      | 1  | 2  | 3   | 4  |\n|:------------------------|:------|:------|:----------|:------|\n| nan      | Cores | Chips | Hosts/VMs | Cubes |\n| Host     | 8  | 4  | 1   | nan |\n| Cube (aka rack)   | 128 | 64 | 16  | 1  |\n| Largest supported slice | 12288 | 6144 | 1536  | 96 |\n| v5p full Pod   | 17920 | 8960 | 2240  | 140 |", "guide": "Cloud TPU"}