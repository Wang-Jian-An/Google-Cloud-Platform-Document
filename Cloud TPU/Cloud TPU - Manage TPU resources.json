{"title": "Cloud TPU - Manage TPU resources", "url": "https://cloud.google.com/tpu/docs/managing-tpus-tpu-vm", "abstract": "# Cloud TPU - Manage TPU resources\n# Manage TPU resources\nThis page describes how to manage Cloud TPU resources using:\n- The [Google Cloud CLI](https://cloud.google.com/sdk/gcloud/reference/compute/tpus/) , which provides the primary CLI to Google Cloud.\n- The [Google Cloud console](https://console.cloud.google.com/) , which provides an integrated management console for your Google Cloud resources.\nCloud TPU has two VM architectures, TPU Node and TPU VM. The two VM architectures are described in [System Architecture](/tpu/docs/system-architecture) . You can use the `gcloud` commands described in this document with both TPU configurations. The `gcloud` commands you use depend on the TPU configuration you are using. Each `gcloud` command is shown in a tabbed section. Choose the tab for the TPU configuration you want to use and the web page shows the appropriate `gcloud` command. Unless you know you need to use TPU Nodes, we recommend using TPU VMs. For Cloud TPU v4 and later, only the TPU VM architecture is supported.\n**Note:** Not all TPU types are available in all zones. For more information, see [Cloud TPU regions and zones](/tpu/docs/regions-zones) .\n", "content": "## Prerequisites\nBefore you run these procedures, you must install the Google Cloud CLI, create a Google Cloud project, and enable the Cloud TPU API. For instructions, see [Set up a project and enable the Cloud TPU API](/tpu/docs/setup-gcp-account) .\nIf you are using the Google Cloud CLI, you can use the Google Cloud Shell, a Compute Engine VM, or install the Google Cloud CLI locally. The Google Cloud Shell lets you interact with Cloud TPUs without having to install any software. The Google Cloud Shell may disconnect after a period of inactivity. If you're running long-running commands, we recommend installing the Google Cloud CLI on your local machine. For more information on the Google Cloud CLI, see the [gcloud Reference](/sdk/gcloud/reference) .\n**Note:** Billing begins as soon as the Cloud TPU is created, and continues until the time it is deleted. Check the [Cloud TPU pricing page](/tpu/docs/pricing) to estimate your costs. If you are using a dataset that requires a substantial download and processing phase, download the dataset first and then create the Cloud TPU.\n## Provision Cloud TPUs\nYou can provision a Cloud TPU using `gcloud` , the Google Cloud console, or using the Cloud TPU API.\nUsing `gcloud` , there are two methods for provisioning TPUs:\n- Using queued resources:`gcloud alpha compute tpus queued-resources create`\n- Using the Create Node API:`gcloud alpha compute tpus tpu-vm create`\nThe best practice is to provision with [Queued Resources](/tpu/docs/queued-resources) . When you request queued resources, the request is added to a queue maintained by the Cloud TPU service. When the requested resource becomes available, it's assigned to your Google Cloud project for your immediate exclusive use.\nTo create a TPU using queued resources, see [Queued Resources](/tpu/docs/queued-resources) .\nIf you will be using Multislice, see the [Multislice Introduction](/tpu/docs/multislice-introduction#set_up_your_environment) for more information.\nWhen using Multislice, specify the following additional parameters when you request queued resources:\n```\nexport NODE_COUNT=node_countexport NODE_PREFIX=your_tpu_prefix # Optional\n```\n### Create a Cloud TPU using the Create Node API\nTo create a TPU using the Create Node API, you run the [gcloud compute tpus tpu-vm create](/sdk/gcloud/reference) command.\n**Note:** To create a Cloud TPU using reserved quota, use the `--reserved` flag when running the `gcloud compute tpus tpu-vm create` command. For more information, see [create](https://cloud.google.com/sdk/gcloud/reference/compute/tpus/tpu-vm/create) in the `gcloud` reference.\n**Note:** To determine which TPU VM software you should use, see [Cloud TPU software versions](/tpu/docs/supported-tpu-versions#tpu_software_versions) .\nYou can specify TPU configurations in terms of TensorCores or TPU chips. For more information, see the section for the TPU version you are using in [System architecture](/tpu/docs/system-architecture-tpu-vm) .\nThe following command uses a TensorCore-based configuration:\n```\n$ gcloud alpha compute tpus tpu-vm create tpu-name \\\u00a0 --zone=us-central2-b \\\u00a0 --accelerator-type=v4-8 \\\u00a0 --version=tpu-software-version\n```The following command creates a TPU using the Create Node API:\n```\n\u00a0 $ gcloud alpha compute tpus tpu-vm create tpu-name \\\u00a0 \u00a0 --zone=us-central2-b \\\u00a0 \u00a0 --type=v4 \\\u00a0 \u00a0 --topology=2x2x1 \\\u00a0 \u00a0 --version=tpu-software-version\n```Details for all optional flags are shown in the [gcloud API documentation](/sdk/gcloud/reference/compute/tpus/tpu-vm/create) .\nFor more information on supported TPU types and topologies, see [Supported TPU configurations](/tpu/docs/supported-tpu-configurations) .\n **Note:** This example uses TPU v3. The TPU Node architecture is not supported with TPU v4 or later TPU versions.\n```\n$ gcloud compute tpus execution-groups create --name=tpu-name \\\u00a0 --zone=us-central1-a \\\u00a0 --tf-version=2.12.0 \\\u00a0 --machine-type=n1-standard-1 \\\u00a0 --accelerator-type=v3-8\n```\n### Creating a Cloud TPU in the Google Cloud console\n- Navigate to the [Google Cloud console](https://console.cloud.google.com/) .\n- From the navigation menu, select **Compute Engine > TPUs** .\n- Click **CREATE TPU NODE** .\n- In the **Name** box, type a TPU instance name.\n- In the **Zone** box, select the zone in which to create the TPU.\n- Under **TPU settings** , select either **TPU VM architecture** or **TPU node architecture** . The TPU configuration determines whether you create the TPU as a TPU VM or a TPU Node. For more information, see [System Architecture](/tpu/docs/system-architecture-tpu-vm) .\n- For **TPU type** , select the [TPU type](/tpu/docs/types-zones) you want to create.\n- For **TPU software version** , select the software version. When creating a Cloud TPU VM, the TPU software version specifies the version of the TPU runtime to install. When creating a Cloud TPU Node, the TPU software version allows you to choose the ML framework installed on the node's VM. No other settings are required. For more information, see [Supported Models](/tpu/docs/tutorials/supported-models) .\n- Click **CREATE** to create your resources.\n### Creating a Cloud TPU VM using curl\nThe following command uses `curl` to create a TPU.\n```\n$ curl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" -d \"{accelerator_type: 'v4-8', \\\u00a0 runtime_version:'tpu-vm-tf-2.16.1-pjrt', \\\u00a0 network_config: {enable_external_ips: true}, \\\u00a0 shielded_instance_config: { enable_secure_boot: true }}\" \\\u00a0 https://tpu.googleapis.com/v2/projects/project-id/locations/us-central2-b/nodes?node_id=node_name\n```### Run a startup script\nYou can run a startup script on each TPU VM by specifying the `--metadata startup-script` parameter when creating the TPU VM. The following command creates a TPU VM using a startup script.\n```\n$ gcloud compute tpus tpu-vm create tpu-name \\\u00a0 \u00a0 --zone=us-central2-b \\\u00a0 \u00a0 --accelerator-type=tpu-type \\\u00a0 \u00a0 --version=tpu-vm-tf-2.16.1-pjrt \\\u00a0 \u00a0 --metadata startup-script='#! /bin/bash\u00a0 \u00a0 \u00a0 pip3 install numpy\u00a0 \u00a0 \u00a0 EOF'\n```\n## Connecting to a Cloud TPU\nYou can connect to a TPU using SSH.\nWhen using TPU VMs, you must explicitly connect to your TPU VM using SSH.- Connect to your TPU VM over SSH using the [gcloud compute tpus tpu-vm ssh command](/sdk/gcloud/reference/compute/tpus/tpu-vm/ssh) .When you request slices with more than 4 chips, Cloud TPU creates a TPU VM for each group of 4 chips.To install the binaries or run code, you can connect to each TPU VM using the ` [tpu-vm ssh command](/sdk/gcloud/reference/alpha/compute/tpus/tpu-vm/ssh) ` .```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME}\n```To access a specific TPU VM or to install binaries on each TPU VM with SSH, use the `--worker` flag which follows a 0-based index:```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} --worker=1\n```When you have more than one TPU VM, use the `--worker=all` and `--command` flags to run a command on all TPU VMs at the same time. For example:```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \u00a0--project ${PROJECT_ID} \\--zone \u00a0${ZONE} \\--worker=all \\--command='pip install \"jax[tpu]==0.4.20\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'\n```For Multislice, you can either run a command on a single VM (by using the enumerated tpu-name) or use the `--node=all` , `--worker=all` , and `--command` flags to run the command on all TPU VMs of all slices of the Multislice, with an optional [--batch-size](https://cloud.google.com/sdk/gcloud/reference/alpha/compute/tpus/queued-resources/ssh#--batch-size) field.```\ngcloud alpha compute tpus queued-resources ssh ${QUEUED_RESOURCE_ID} \\--project ${PROJECT_ID} --zone ${ZONE} --node=all --worker=all \\--command='pip install \"jax[tpu]==0.4.20\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html' --batch-size=4\n``````\n$ gcloud compute tpus tpu-vm ssh tpu-name --zone=zone\n``` **Note:** When you connect to VMs using the Google Cloud CLI, Compute Engine creates a persistent SSH key for you.\n- Use SSH-in-browser by doing the following:- In the Google Cloud console, go to the **TPUs** page: [Go to TPUs](https://console.cloud.google.com/compute/tpus) \n- In the list of TPU VMs, click **SSH** in the row of the TPU VM that you want to connect to.\n **Note:** When you connect to TPU VMs using the Google Cloud console, Compute Engine creates an ephemeral SSH key for you.\n **Note:** TPU Nodes are only supported on TPU versions v2 and v3.\nBy default, the `gcloud` command you use to create TPU Nodes automatically attempts to SSH into your TPU node. If you are using TPU Nodes and are not connected to the Compute Engine instance by the `gcloud` command, you can connect by running the following:\n```\n$ gcloud compute ssh tpu-name \\\u00a0 --zone=zone\n```\nAfter the TPU VM is created, you can view the logs from the startup script by [connecting to the TPU VM](#tpu-connect) using `SSH` and running:\n```\n$ cat /var/log/syslog | grep startup-script\n```\n## Listing your Cloud TPU resources\nYou can list all of your Cloud TPU in a specified zone.\n### Listing your Cloud TPU resources using gcloud\nThe commands you use depend on whether you are using TPU VMs or TPU Nodes. For more information, see [System Architecture](/tpu/docs/system-architecture-tpu-vm) .\n```\n$ gcloud compute tpus tpu-vm list --zone=zone\n```\n```\n$ gcloud compute tpus execution-groups list --zone=zone\n```\nThis command lists the Cloud TPU resources in the specified zone. If no resources are currently set up, the output will just show dashes for the VM and TPU. If one resource is active and the other is not, you will see a message saying the status is unhealthy. You need to start or restart whichever resource is not running.\n### Listing your Cloud TPU resources in the Google Cloud console\n- Navigate to the [Google Cloud console](https://console.cloud.google.com/) .\n- From the navigation menu, select **Compute Engine > TPUs** . The console displays the TPUs page.## Retrieving information about your Cloud TPU\nYou can retrieve information about a specified Cloud TPU.\n### Retrieve information about a Cloud TPU using gcloud\nThe commands you use depend on whether you are using TPU VMs or TPU Nodes. For more information, see [System Architecture](/tpu/docs/system-architecture-tpu-vm) .\n```\n$ gcloud compute tpus tpu-vm describe tpu-name \\\u00a0 --zone=zone\n```\n```\n$ gcloud compute tpus execution-groups describe tpu-name \\\u00a0 --zone=zone\n```\n### Retrieve information about a Cloud TPU in the Google Cloud console\n- Navigate to the [Google Cloud console](https://console.cloud.google.com/) .\n- From the navigation menu, select **Compute Engine > TPUs** . The console displays the TPUs page.\n- Click the name of your Cloud TPU. The Cloud TPU detail page is displayed.## Stopping your Cloud TPU resources\nYou can stop a single Cloud TPU to stop incurring charges without losing your VM's configuration and software. Stopping TPU Pods or TPUs allocated through the [queued resources](/tpu/docs/queued-resources) API is not supported. To stop incurring charges for TPUs allocated through the queued resources API, you must [delete](#tpu-delete) the TPU.\n### Stopping a Cloud TPU using gcloud\nThe commands you use for stopping a Cloud TPU depend on whether you are using TPU VMs or TPU Nodes. For more information, see [System Architecture](/tpu/docs/system-architecture-tpu-vm) .\n```\n$ gcloud compute tpus tpu-vm stop tpu-name \\--zone=zone\n```\n```\n$ gcloud compute tpus stop tpu-name \\--zone=zone\n```\n### Stopping a Cloud TPU in the Google Cloud console\n- Navigate to the [Google Cloud console](https://console.cloud.google.com/) .\n- From the navigation menu, select **Compute Engine > TPUs** . The console displays the TPUs page.\n- Select the checkbox next to your Cloud TPU and click **Stop** .## Starting your Cloud TPU resources\nYou can start a Cloud TPU when it is stopped.\n### Starting a Cloud TPU using gcloud\nYou can start a stopped Cloud TPU to resume using it.\n**Note:** Starting TPU Pods or TPUs allocated through the [queued resources](/tpu/docs/queued-resources) API are not supported.\nThe command you use for starting a stopped Cloud TPU depend on whether you are using TPU VMs or TPU Nodes. For more information, see [System Architecture](/tpu/docs/system-architecture-tpu-vm) .\n```\n$ gcloud compute tpus tpu-vm start tpu-name --zone=zone\n```\n```\n$ gcloud compute tpus start tpu-name --zone=zone\n```\n### Starting a Cloud TPU in the Google Cloud console\n- Navigate to the [Google Cloud console](https://console.cloud.google.com/) .\n- From the navigation menu, select **Compute Engine > TPUs** . The console displays the TPUs page.\n- Select the checkbox next to your Cloud TPU and click **Start** .## Delete your TPU VM\nThe command you use depends on whether you are using TPU VMs or TPU Nodes. For more information, see [System Architecture](/tpu/docs/system-architecture-tpu-vm) .\nDelete your TPU VM slices at the end of your session.\n```\n\u00a0gcloud compute tpus tpu-vm delete ${TPU_NAME} --project=${PROJECT_ID}\u00a0--zone=${ZONE} --quiet\n``````\n$ gcloud compute tpus execution-groups delete tpu-name \\\u00a0 --zone=zone\n```\n### Deleting a Cloud TPU in the Google Cloud console\n- Navigate to the [Google Cloud console](https://console.cloud.google.com/) .\n- From the navigation menu, select **Compute Engine > TPUs** . The console displays the TPUs page.\n- Select the checkbox next to your Cloud TPU and click **Delete** .## Advanced Configurations\n### Custom Network Resources\nWhen you create the TPU, you can choose to specify the network and/or a subnetwork. You can do this either by submitting a `gcloud` command or a `curl` call.\nTo specify the network or subnetwork in the `gcloud` CLI, use:\n```\n--network [NETWORK] --subnetwork [SUBNETWORK]\n```\nTo specify the network or subnetwork in a `curl` call, use:\n```\nnetwork_config: {network: '[NETWORK]', subnet: '[SUBNETWORK]', enable_external_ips: true}\n```\n### Network\nYou can optionally specify the network to use for the TPU. If not specified, the `default` network is used.\n**Note:** Any custom network should be created before creating the TPU.\n**Valid network formats:**\n```\nhttps://www.googleapis.com/compute/{version}/projects/{proj-id}/global/networks/{network}compute/{version}/projects/{proj-id}/global/networks/{network}compute/{version}/projects/{proj-\n##}/global/networks/{network}projects/{proj-id}/global/networks/{network}projects/{proj-\n##}/global/networks/{network}global/networks/{network}{network}\n```\n### Subnetwork\nYou can specify the subnetwork to use a specific subnetwork. The specified subnetwork needs to be in the **same region** as the zone where the TPU runs.\n**Note:** The custom subnetwork should be created before the TPU is created.\n**Valid Formats:**\n```\nhttps://www.googleapis.com/compute/{version}/projects/{proj-id}/regions/{region}/subnetworks/{subnetwork}compute/{version}/projects/{proj-id}/regions/{region}/subnetworks/{subnetwork}compute/{version}/projects/{proj-\n##}/regions/{region}/subnetworks/{subnetwork}projects/{proj-id}/regions/{region}/subnetworks/{subnetwork}projects/{proj-\n##}/regions/{region}/subnetworks/{subnetwork}regions/{region}/subnetworks/{subnetwork}{subnetwork}\n```\n### Private Google Access\nIn order to SSH into the TPU VMs, you need to either add access configs for the TPU VMs, or turn on the [Private Google Access](https://cloud.google.com/vpc/docs/configure-private-google-access) for the subnetwork to which the TPU VMs are connected.\nTo add access configs, `enable_external_ips` must be set. When you [create a TPU](#tpu-setup) , `enable_external_ips` is set by default. If you want to opt out, specify the following command:\n```\n--internal-ips\n```\nOr use a `curl` call:\n```\nnetwork_config: {enable_external_ips: true}\n```\nAfter you have configured Private Google Access, [connect to the VM via SSH](#tpu-connect) .\n### Custom Service Account\nEach TPU VM has an associated service account it uses to make API requests on your behalf. TPU VMs use this service account to call Cloud TPU APIs, access Cloud Storage and other services. By default, your TPU VM uses the [default Compute Engine service account](/compute/docs/access/service-accounts#default_service_account) .\nYou can specify a custom service account when creating a TPU VM using the `--service-account` flag. The service account must be defined in the same Google Cloud project where you create your TPU VM. Custom service accounts used for TPU VMs must have the [TPU Viewer](/iam/docs/understanding-roles#tpu.viewer) role to call the Cloud TPU API. If the code running in your TPU VM calls other Google Cloud services, it must have the roles necessary to access those services.\nWhen you create a TPU, you can choose to specify a custom service account using the `--service-account` flag. For more information about service accounts, see [Service Accounts](/compute/docs/access/service-accounts#serviceaccount) .\nUse the following commands to specify a custom service account.\n```\n$ gcloud alpha compute tpus tpu-vm create tpu-name \\\u00a0 \u00a0 --zone=us-central2-b \\\u00a0 \u00a0 --accelerator-type=tpu-type \\\u00a0 \u00a0 --version=tpu-vm-tf-2.16.1-pjrt \\\u00a0 \u00a0 --service-account=your-service-account\n```\n```\n$ curl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" -d \"{accelerator_type: 'v4-8', \\\u00a0 runtime_version:'tpu-vm-tf-2.16.1-pjrt', \\\u00a0 network_config: {enable_external_ips: true}, \\\u00a0 shielded_instance_config: { enable_secure_boot: true }}\" \\\u00a0 service_account: {email: 'your-service-account'} \\\u00a0 https://tpu.googleapis.com/v2/projects/project-id/locations/us-central2-b/nodes?node_id=node_name\n```\nTo use a custom service account, you need to authorize the service account for your Google Cloud Storage buckets. For more information, see [Connecting to Cloud Storage buckets](/tpu/docs/storage-buckets#authorize_the_service_account) .\n### Custom VM SSH methods\n- Set up a firewall for SSH.The default network comes preconfigured to allow SSH access to all VMs. If you don't use the default network, or the default network settings were edited, you may need to explicitly enable SSH access by adding a firewall-rule:```\n$ gcloud CLI compute firewall-rules create \\--network=network allow-ssh \\--allow=tcp:22\n```\n- SSH into the TPU VMs.```\n$ gcloud compute tpus tpu-vm ssh tpu-name \\--zone=us-central2-b \\--project=project-id\n``` **Required fields** - `tpu-name`: Name of the TPU node.\n- `zone`: The location of the TPU node. Currently, only`us-central2-b`is supported.\n- `project-id`: The project you created above.\nFor a list of optional fields, see the [gcloud API documentation](/sdk/gcloud/reference/compute/tpus/tpu-vm/ssh) .", "guide": "Cloud TPU"}