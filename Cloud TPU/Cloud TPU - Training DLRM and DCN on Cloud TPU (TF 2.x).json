{"title": "Cloud TPU - Training DLRM and DCN on Cloud TPU (TF 2.x)", "url": "https://cloud.google.com/tpu/docs/tutorials/dlrm-dcn-2.x", "abstract": "# Cloud TPU - Training DLRM and DCN on Cloud TPU (TF 2.x)\nThis tutorial shows how to train DLRM and DCN v2 ranking models which can be used for tasks such as click-through rate (CTR) prediction. See the note in [Set up to run the DLRM or DCN model](#run-model) to see how to set parameters to train either a DLRM or a DCN v2 ranking model.\nThe model inputs are numerical and categorical features, and output is a scalar (for example click probability). The model can be trained and evaluated on Cloud TPU. The deep ranking models are both memory intensive (for embedding tables/lookup) and compute intensive for deep networks (MLPs). TPUs are designed for both.\nThe model uses a TPUEmbedding layer for categorical features. TPU embedding supports large embedding tables with fast lookup, the size of embedding tables scales linearly with the size of a TPU pod. Up to 90 GB embedding tables can be used for TPU v3-8, 5.6 TB for a v3-512 Pod, and 22.4 TB for a v3-2048 TPU Pod.\nThe model code is in the [TensorFlow Recommenders library](https://github.com/tensorflow/recommenders/tree/main/tensorflow_recommenders/experimental/models) , while input pipeline, configuration and training loop is described in the [TensorFlow Model Garden](https://github.com/tensorflow/models/tree/master/official/recommendation/ranking) .\n", "content": "## Objectives\n- Set up the training environment\n- Run the training job using synthetic data\n- Verify the output results\n## CostsIn this document, you use the following billable components of Google Cloud:- Compute Engine\n- Cloud TPU\n- Cloud Storage\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \n## Before you begin **Important:** You can use this tutorial with either the TPU VM or the TPU Node configuration. The two VM architectures are described in [System Architecture](/tpu/docs/system-architecture-tpu-vm) . The gcloud commands you use depend on the TPU configuration you are using. In this tutorial, each gcloud command is shown in a tabbed section. Choose the tab for the TPU configuration you want to use and the web page shows the appropriate gcloud command. Unless you know you need to use TPU Nodes, we recommend using TPU VMs.\nBefore starting this tutorial, check that your Google Cloud project is correctly set up.- This walkthrough uses billable components of Google Cloud. Check the [Cloud TPU pricing page](/tpu/docs/pricing) to  estimate your costs. Be sure to [clean up](#clean-up) resources you create when you've finished with them to avoid unnecessary  charges.\n## Set up your resourcesThis section provides information on setting up Cloud Storage bucket, VM, and Cloud TPU resources used by this tutorial.\n **Important: ** Set up all resources (Compute Engine VM, Cloud TPU, and Cloud Storage bucket) in the same region or zone to reduce network latency and network costs. VMs and TPU nodes are located in [specific zones](/tpu/docs/types-zones#types) , which are subdivisions within a region.- Open a Cloud Shell window. [Open Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Create a variable for your project's ID.```\nexport PROJECT_ID=project-id\n```\n- Configure Google Cloud CLI to use the project where you want to create Cloud TPU. **Note: ** For more information on the`gcloud`command, see the [Google Cloud CLI Reference](/sdk/gcloud/reference) .```\ngcloud config set project ${PROJECT_ID}\n```The first time you run this command in a new Cloud Shell VM, an `Authorize Cloud Shell` page is displayed. Click `Authorize` at the bottom of the page to allow `gcloud` to make API calls with your credentials.\n- Create a Service Account for the Cloud TPU project.```\ngcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID\n```The command returns a Cloud TPU Service Account with following format:```\nservice-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com\n```\n- Create a Cloud Storage bucket using the following command where the `-l` option specifies the region where the bucket should be created. See the [types and zones](/tpu/docs/types-zones) for more details on zones and regions:```\ngsutil mb -p ${PROJECT_ID} -c standard -l europe-west4 gs://bucket-name\n```This Cloud Storage bucket stores the data you use to train your model and the training results. The `gcloud compute tpus execution-groups` tool used in this tutorial sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the [access level permissions](/tpu/docs/storage-buckets) .The bucket location must be in the same region as your Compute Engine (VM) and your Cloud TPU node.\n- Launch a Compute Engine VM and Cloud TPU using the `gcloud` command. The command you use depends on whether you are using a TPU VM or a TPU node. For more information on the two VM architecture, see [System Architecture](/tpu/docs/system-architecture-tpu-vm) . For more information on the `gcloud` command, see the [gcloud Reference](/sdk/gcloud/reference) .\n```\n$ gcloud compute tpus tpu-vm create dlrm-dcn-tutorial \\--zone=europe-west4-a \\--accelerator-type=v3-8 \\--version=tpu-vm-tf-2.16.1-se\n``````\n$ gcloud compute tpus execution-groups create \\--name=dlrm-dcn-tutorial \\--zone=europe-west4-a \\--disk-size=300 \\--machine-type=n1-standard-8 \\--tf-version=2.12.0\n``` **Note:** The first time you run `gcloud compute tpus execution-groups` on a project it can take several minutes to perform startup tasks such as SSH key propagation and API turnup.\n- If you are not automatically logged in to the Compute Engine instance, log in by running the following `ssh` command. When you are logged into the VM, your shell prompt changes from `username@projectname` to `username@vm-name` :\n```\ngcloud compute tpus tpu-vm ssh dlrm-dcn-tutorial --zone=europe-west4-a\n```\n```\ngcloud compute ssh dlrm-dcn-tutorial --zone=europe-west4-a\n```\nAs you continue these instructions, run each command that begins with `(vm)$` in your VM session window.\n## Set Cloud Storage bucket variablesSet up the following environment variables, replacing with the name of your Cloud Storage bucket:\n```\n(vm)$ export STORAGE_BUCKET=gs://bucket-name(vm)$ export PYTHONPATH=\"/usr/share/tpu/models/:${PYTHONPATH}\"(vm)$ export EXPERIMENT_NAME=dlrm-exp\n```\nSet an environment variable for the TPU name.\n```\n(vm)$ export TPU_NAME=local\n```\n```\n(vm)$ export TPU_NAME=dlrm-dcn-tutorial\n```The training application expects your training data to be accessible in Cloud Storage. The training application also uses your Cloud Storage bucket to store checkpoints during training.## Set up to run the DLRM or DCN model with synthetic dataThe model can be trained on various datasets. Two commonly used ones are [Criteo Terabyte](https://labs.criteo.com/2013/12/download-terabyte-click-logs/) and [Criteo Kaggle](https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/) . This tutorial trains on synthetic data by setting the flag `use_synthetic_data=True` .\nThe synthetic dataset is only useful for understanding how to use a Cloud TPU and validating end-to-end performance. The accuracy numbers and saved model won't be meaningful.\nVisit the [Criteo Terabyte](https://labs.criteo.com/2013/12/download-terabyte-click-logs/) and [Criteo Kaggle](https://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/) websites for information on how to download and [preprocess](https://github.com/tensorflow/models/tree/master/official/recommendation/ranking#preprocess-the-data) these datasets.\n **Note:** If you want to monitor the model's output and performance, follow the guide to [setting up TensorBoard](/tpu/docs/tensorboard-setup) .\n- Install required packages.```\n(vm)$ pip3 install tensorflow-recommenders(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt\n```\n- Change to the script directory.\n```\n(vm)$ cd /usr/share/tpu/models/official/recommendation/ranking\n```\n```\n(vm)$ cd /usr/share/models/official/recommendation/ranking\n```\n- Run the training script. This uses a fake, Criteo-like dataset to train the DLRM model. The training takes approximately 20 minutes. **Note:** To train the DLRM model use dot product feature interaction, that is, interaction: 'dot'. To train the DCN v2 model use interaction: 'cross'```\nexport EMBEDDING_DIM=32python3 train.py --mode=train_and_eval \\\u00a0 \u00a0 \u00a0--model_dir=${STORAGE_BUCKET}/model_dirs/${EXPERIMENT_NAME} --params_override=\"\u00a0 \u00a0 \u00a0runtime:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0distribution_strategy: 'tpu'\u00a0 \u00a0 \u00a0task:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0use_synthetic_data: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0train_data:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0input_path: '${DATA_DIR}/train/*'\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0global_batch_size: 16384\u00a0 \u00a0 \u00a0 \u00a0 \u00a0validation_data:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0input_path: '${DATA_DIR}/eval/*'\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0global_batch_size: 16384\u00a0 \u00a0 \u00a0 \u00a0 \u00a0model:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0num_dense_features: 13\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0bottom_mlp: [512,256,${EMBEDDING_DIM}]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0embedding_dim: ${EMBEDDING_DIM}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0top_mlp: [1024,1024,512,256,1]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0interaction: 'dot'\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0vocab_sizes: [39884406, 39043, 17289, 7420, 20263, 3, 7120, 1543, 63,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a038532951, 2953546, 403346, 10, 2208, 11938, 155, 4, 976, 14,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a039979771, 25641295, 39664984, 585935, 12972, 108, 36]\u00a0 \u00a0 \u00a0trainer:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0use_orbit: false\u00a0 \u00a0 \u00a0 \u00a0 \u00a0validation_interval: 1000\u00a0 \u00a0 \u00a0 \u00a0 \u00a0checkpoint_interval: 1000\u00a0 \u00a0 \u00a0 \u00a0 \u00a0validation_steps: 500\u00a0 \u00a0 \u00a0 \u00a0 \u00a0train_steps: 1000\u00a0 \u00a0 \u00a0 \u00a0 \u00a0steps_per_loop: 1000\u00a0 \u00a0 \u00a0\"\n```\nThis training runs for approximately 10 minutes on a v3-8 TPU. When it completes, you will see messages similar to the following:\n```\nI0621 21:32:58.519792 139675269142336 tpu_embedding_v2_utils.py:907] Done with log of TPUEmbeddingConfiguration.\nI0621 21:32:58.540874 139675269142336 tpu_embedding_v2.py:389] Done initializing TPU Embedding engine.\n1000/1000 [==============================] - 335s 335ms/step - auc: 0.7360 - accuracy: 0.6709 - prediction_mean: 0.4984\n- label_mean: 0.4976 - loss: 0.0734 - regularization_loss: 0.0000e+00 - total_loss: 0.0734 - val_auc: 0.7403\n- val_accuracy: 0.6745 - val_prediction_mean: 0.5065 - val_label_mean: 0.4976 - val_loss: 0.0749\n- val_regularization_loss: 0.0000e+00 - val_total_loss: 0.0749\nModel: \"ranking\"\n_________________________________________________________________\nLayer (type)     Output Shape    Param # \n=================================================================\ntpu_embedding (TPUEmbedding) multiple     1   \n_________________________________________________________________\nmlp (MLP)     multiple     154944 \n_________________________________________________________________\nmlp_1 (MLP)     multiple     2131969 \n_________________________________________________________________\ndot_interaction (DotInteract multiple     0   \n_________________________________________________________________\nranking_1 (Ranking)   multiple     0   \n=================================================================\nTotal params: 2,286,914\nTrainable params: 2,286,914\nNon-trainable params: 0\n_________________________________________________________________\nI0621 21:43:54.977140 139675269142336 train.py:177] Train history: {'auc': [0.7359596490859985],\n'accuracy': [0.67094486951828], 'prediction_mean': [0.4983849823474884], 'label_mean': [0.4975697994232178],\n'loss': [0.07338511198759079], 'regularization_loss': [0], 'total_loss': [0.07338511198759079],\n'val_auc': [0.7402724623680115], 'val_accuracy': [0.6744520664215088], 'val_prediction_mean': [0.5064718723297119],\n'val_label_mean': [0.4975748658180237], 'val_loss': [0.07486172765493393],\n'val_regularization_loss': [0], 'val_total_loss': [0.07486172765493393]}\n```\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.- Disconnect from the Compute Engine instance, if you have not already done so:```\n(vm)$ exit\n```Your prompt should now be `username@projectname` , showing you are in the Cloud Shell.\n- Delete your Cloud TPU and Compute Engine resources. The command you use to delete your resources depends upon whether you are using TPU VMs or TPU Nodes. For more information, see [System Architecture](/tpu/docs/system-architecture-tpu-vm) .\n```\n$ gcloud compute tpus tpu-vm delete dlrm-dcn-tutorial \\--zone=europe-west4-a\n```\n```\n$ gcloud compute tpus execution-groups delete dlrm-dcn-tutorial \\--zone=europe-west4-a\n```\n- Verify the resources have been deleted by running `gcloud compute tpus execution-groups list` . The deletion might take several minutes. The output from the following command shouldn't include any of the resources created in this tutorial:\n```\n$ gcloud compute tpus tpu-vm list --zone=europe-west4-a\n```\n```\n$ gcloud compute tpus execution-groups list --zone=europe-west4-a\n```\n- Delete your Cloud Storage bucket using `gsutil` . Replace with the name of your Cloud Storage bucket.```\n$ gsutil rm -r gs://bucket-name\n```\n## What's nextThe TensorFlow Cloud TPU tutorials generally train the model using a sample dataset. The results of this training are not usable for inference. To use a model for inference, you can train the data on a publicly available dataset or your own dataset. TensorFlow models trained on Cloud TPUs generally require datasets to be in [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format.\nYou can use the [dataset conversion toolsample](/tpu/docs/classification-data-conversion) to convert an image classification dataset into TFRecord format. If you are not using an image classification model, you will have to convert your dataset to [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format yourself. For more information, see [TFRecord andtf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord) .\n### Hyperparameter tuningTo improve the model's performance with your dataset, you can tune the model's hyperparameters. You can find information about hyperparameters common to all TPU supported models on [GitHub](https://github.com/tensorflow/tpu/tree/master/models/hyperparameters) . Information about model-specific hyperparameters can be found in the [sourcecode](https://github.com/tensorflow/tpu/tree/master/models/official) for each model. For more information on hyperparameter tuning, see [Overview ofhyperparameter tuning](/vertex-ai/docs/training/hyperparameter-tuning-overview) and [Tunehyperparameters](https://developers.google.com/machine-learning/guides/text-classification/step-5) .\n### InferenceOnce you have trained your model, you can use it for inference (also called prediction). You can use the [Cloud TPU inference convertertool](/tpu/docs/v5e-inference-converter) to prepare and optimize a TensorFlow model for inference on Cloud TPU v5e. For more information about inference on Cloud TPU v5e, see [Cloud TPU v5e inferenceintroduction](/tpu/docs/v5e-inference) .", "guide": "Cloud TPU"}