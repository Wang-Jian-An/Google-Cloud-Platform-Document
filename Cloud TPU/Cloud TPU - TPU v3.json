{"title": "Cloud TPU - TPU v3", "url": "https://cloud.google.com/tpu/docs/v3?hl=zh-cn", "abstract": "# Cloud TPU - TPU v3\n# TPU v3\nThis document describes the architecture and supported configurations of Cloud TPU v3.\n", "content": "## System architecture\nEach v3 TPU chip contains two TensorCores. Each TensorCore has two matrix-multiply units (MXUs), a vector unit, and a scalar unit. The following table shows the key specifications and their values for a v3 TPU Pod.\n| Key specifications   | v3 Pod values  |\n|:-----------------------------|:---------------------|\n| Peak compute per chip  | 123 teraflops (bf16) |\n| HBM2 capacity and bandwidth | 32 GiB, 900 GBps  |\n| Measured min/mean/max power | 123/220/262 W  |\n| TPU Pod size     | 1024 chips   |\n| Interconnect topology  | 2D torus    |\n| Peak compute per Pod   | 126 petaflops (bf16) |\n| All-reduce bandwidth per Pod | 340 TB/s    |\n| Bisection bandwidth per Pod | 6.4 TB/s    |\nThe following diagram illustrates a TPU v3 chip.\nArchitectural details and performance characteristics of TPU v3 are available in [A Domain Specific Supercomputer for Training Deep Neural Networks](https://dl.acm.org/doi/pdf/10.1145/3360307) .\n### Performance benefits of TPU v3 over v2\nThe increased FLOPS per TensorCore and memory capacity in TPU v3 configurations can improve the performance of your models in the following ways:\n- TPU v3 configurations provide significant performance benefits per TensorCore for compute-bound models. Memory-bound models on TPU v2 configurations might not achieve this same performance improvement if they are also memory-bound on TPU v3 configurations.\n- In cases where data does not fit into memory on TPU v2 configurations, TPU v3 can provide improved performance and reduced recomputation of intermediate values (rematerialization).\n- TPU v3 configurations can run new models with batch sizes that did not fit on TPU v2 configurations. For example, TPU v3 might allow deeper ResNet modelz and larger images with RetinaNet.\nModels that are nearly input-bound (\"infeed\") on TPU v2 because training steps are waiting for input might also be input-bound with Cloud TPU v3. The pipeline performance guide can help you resolve infeed issues.\n## Configurations\nA TPU v3 Pod is composed of 1024 chips interconnected with high-speed links. To create a TPU v3 device or Pod slice, use the `--accelerator-type` flag in the TPU creation command ( `gcloud compute tpus tpu-vm` ). You specify the accelerator type by specifying the TPU version and the number of TPU cores. For example, for a single v3 TPU, use `--accelerator-type=v3-8` . For a v3 Pod slice with 128 TensorCores, use `--accelerator-type=v3-128` .\nThe following command shows how to create a v3 TPU Pod slice with 128 TensorCores:\n```\n\u00a0 $ gcloud compute tpus tpu-vm create tpu-name \\\u00a0 \u00a0 --zone=zone \\\u00a0 \u00a0 --accelerator-type=v3-128 \\\u00a0 \u00a0 --version=tpu-vm-tf-2.16.1-pjrt\n```\nThe following table lists the supported v3 TPU types:\n| TPU version | Support ends   |\n|:--------------|:-----------------------|\n| v3-8   | (End date not yet set) |\n| v3-32   | (End date not yet set) |\n| v3-128  | (End date not yet set) |\n| v3-256  | (End date not yet set) |\n| v3-512  | (End date not yet set) |\n| v3-1024  | (End date not yet set) |\n| v3-2048  | (End date not yet set) |\nFor more information about managing TPUs, see [Manage TPUs](/tpu/docs/managing-tpus-tpu-vm) . For more information about the system architecture of Cloud TPU, see [System architecture](/tpu/docs/system-architecture) .", "guide": "Cloud TPU"}