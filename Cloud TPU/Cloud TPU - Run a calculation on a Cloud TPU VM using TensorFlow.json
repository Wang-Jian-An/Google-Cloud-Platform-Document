{"title": "Cloud TPU - Run a calculation on a Cloud TPU VM using TensorFlow", "url": "https://cloud.google.com/tpu/docs/run-calculation-tensorflow", "abstract": "# Cloud TPU - Run a calculation on a Cloud TPU VM using TensorFlow\n# Run a calculation on a Cloud TPU VM using TensorFlow\nThis quickstart shows you how to create a Cloud TPU, install TensorFlow and run a simple calculation on a Cloud TPU. For a more in depth tutorial showing you how to train a model on a Cloud TPU see one of the [Cloud TPU Tutorials](/tpu/docs/tutorials) .\n", "content": "## Before you begin **Important:** You can use this quickstart with either the TPU VM or the TPU Node configuration. The two VM architectures are described in [System Architecture](/tpu/docs/system-architecture-tpu-vm) . The `gcloud` commands you use depend on the TPU configuration you are using. In this quickstart, each gcloud command is shown in a tabbed section. Choose the tab for the TPU configuration you want to use and the web page shows the appropriate `gcloud` command. Unless you know you need to use TPU Nodes, we recommend using TPU VMs.\nBefore you follow this quickstart, you must create a Google Cloud Platform account, install the Google Cloud CLI. and configure the `gcloud` command. For more information, see [Set up an account and a Cloud TPU project](/tpu/docs/setup-gcp-account) .\n## Create a Cloud TPU VM or Node with gcloudLaunch a Compute Engine Cloud TPU using the `gcloud` command. The command you use depends on whether you are using a TPU VM or a TPU node. For more information on the two VM architecture, see [System Architecture](/tpu/docs/system-architecture-tpu-vm) . For more information on the `gcloud` command, see the [gcloud reference](/sdk/gcloud/reference) .\n **Note:** When launching the TPU VM, you can either use the default TPU software version shown in the command or you can refer to [Cloud TPU software versions](/tpu/docs/supported-tpu-versions#tpu_software_versions) to specify another supported TPU software version.\n```\n$ gcloud compute tpus tpu-vm create tpu-name \\--zone=europe-west4-a \\--accelerator-type=v3-8 \\--version=tpu-vm-tf-2.16.1-pjrt\n``````\n$ gcloud compute tpus execution-groups create \\--name=tpu-name \\--zone=europe-west4-a \\--disk-size=300 \\--machine-type=n1-standard-16 \\--tf-version=2.12.0 \\\n```\n **Note:** If you have more than one project, you must specify the project ID with the `--project` flag.## Connect to your Cloud TPU VMWhen using TPU VMs, you must explicitly connect to your TPU VM using SSH. When using TPU Nodes, you should be automatically SSHed into your Compute EngineVM. If you are not automatically connected, use the following command.\n```\n$ gcloud compute tpus tpu-vm ssh tpu-name \\\u00a0 --zone europe-west4-a\n```\n```\n$ gcloud compute ssh tpu-name \\\u00a0 \u00a0 --zone=europe-west4-a\n```## Run a simple example using TensorFlow\n **Note:** This example shows how to run code on a single TPU (for example v2-8 or v3-8). To run code on a larger TPU slice or Pod (for example v2-32+ or v3-32+), see the [Pod example](/tpu/docs/tensorflow-pods) .\nOnce you are connected to the TPU VM, set the following environment variable.\n```\n\u00a0 (vm)$ export TPU_NAME=local\n```\nWhen creating your TPU, if you set the `--version` parameter to a version ending with `-pjrt` , set the following environment variables to enable the PJRT runtime:\n```\n\u00a0 (vm)$ export NEXT_PLUGGABLE_DEVICE_USE_C_API=true\u00a0 (vm)$ export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so\n```\nCreate a file named `tpu-test.py` in the current directory and copy and paste the following script into it.\n```\n\u00a0 import tensorflow as tf\u00a0 print(\"Tensorflow version \" + tf.__version__)\u00a0 @tf.function\u00a0 def add_fn(x,y):\u00a0 \u00a0 z = x + y\u00a0 \u00a0 return z\u00a0 cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\u00a0 tf.config.experimental_connect_to_cluster(cluster_resolver)\u00a0 tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\u00a0 strategy = tf.distribute.TPUStrategy(cluster_resolver)\u00a0 x = tf.constant(1.)\u00a0 y = tf.constant(1.)\u00a0 z = strategy.run(add_fn, args=(x,y))\u00a0 print(z)\n```\nCreate a file named `tpu-test.py` in the current directory and copy and paste the following script into it.\n **Note:** For single TPUs, Slices, and Pods, pass your TPU name to `TPUClusterResolver()` .\n```\nimport tensorflow as tfprint(\"Tensorflow version \" + tf.__version__)tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='your-tpu-name') \u00a0# TPU detectionprint('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])tf.config.experimental_connect_to_cluster(tpu)tf.tpu.experimental.initialize_tpu_system(tpu)strategy = tf.distribute.experimental.TPUStrategy(tpu)@tf.functiondef add_fn(x,y):\u00a0 \u00a0 z = x + y\u00a0 \u00a0 return zx = tf.constant(1.)y = tf.constant(1.)z = strategy.run(add_fn, args=(x,y))print(z)\n```Run this script with the following command:\n```\n(vm)$ python3 tpu-test.py\n```\nThis script performs a simple computation on a each TensorCore of a TPU. The output will look similar to the following:\n```\nPerReplica:{\n 0: tf.Tensor(2.0, shape=(), dtype=float32),\n 1: tf.Tensor(2.0, shape=(), dtype=float32),\n 2: tf.Tensor(2.0, shape=(), dtype=float32),\n 3: tf.Tensor(2.0, shape=(), dtype=float32),\n 4: tf.Tensor(2.0, shape=(), dtype=float32),\n 5: tf.Tensor(2.0, shape=(), dtype=float32),\n 6: tf.Tensor(2.0, shape=(), dtype=float32),\n 7: tf.Tensor(2.0, shape=(), dtype=float32)\n}\n```\n## Clean upTo avoid incurring charges to your Google Cloud account for   the resources used on this page, follow these steps.- Disconnect from the Compute Engine instance, if you have not already done so:```\n(vm)$ exit\n```Your prompt should now be `username@projectname` , showing you are in the Cloud Shell.\n- Delete your Cloud TPU.\n```\n$ gcloud compute tpus tpu-vm delete tpu-name \\--zone=europe-west4-a\n```\n```\n$ gcloud compute tpus execution-groups delete tpu-name \\--zone=europe-west4-a\n```\n- Verify the resources have been deleted by running `gcloud compute tpus tpu-vm list` . The deletion might take several minutes.\n```\n$ gcloud compute tpus tpu-vm list --zone=europe-west4-a\n```\n```\n$ gcloud compute tpus execution-groups list --zone=europe-west4-a\n```## What's nextFor more information about Cloud TPU, see:- [Run TensorFlow code on TPU Pod slices](/tpu/docs/tensorflow-pods) \n- [Manage TPUs](/tpu/docs/managing-tpus-tpu-vm) \n- [Cloud TPU System Architecture](/tpu/docs/system-architecture-tpu-vm)", "guide": "Cloud TPU"}