{"title": "Cloud TPU - Manage maintenance events with Cloud TPU Pods", "url": "https://cloud.google.com/tpu/docs/maintenance-events", "abstract": "# Cloud TPU - Manage maintenance events with Cloud TPU Pods\n# Manage maintenance events with Cloud TPU Pods\n", "content": "## Overview\nTPU Nodes and TPU VMs are instances of Compute Engine VMs with attached TPU hardware. Compute Engine VMs are subject to [Compute Engine VM maintenance events](/compute/docs/instances/setting-instance-scheduling-options#maintenanceevents) . Each TPU is connected to a Compute Engine VM, so using more TPUs (for example, in a TPU Pod) increases the likelihood of one of your VMs encountering a maintenance event.\n**Note:** The differences between TPU VMs and TPU Nodes are described in [System Architecture](/tpu/docs/system-architecture-tpu-vm) .\nThis document discusses various approaches to handle maintenance events for long-running training jobs on Cloud TPUs.\n## Using checkpoints for fast recovery from maintenance events\nCheckpoints are key to short recoveries from maintenance events and should be saved frequently: a good rule of thumb is saving checkpoints approximately every hour. Not checkpointing often enough risks losing a lot of training progress due to maintenance events or other training interruptions.\nCheckpoints generally refer to all of the saved parameters used in training (such as model weights). The time it takes to save a checkpoint can range from the order of seconds to the order of minutes.\nAlthough most maintenance events are automatically recovered and training jobs continue without manual intervention, there might be edge cases where the job does not restart and automatically continue. When this happens, you need to delete and recreate the TPU resources, and restart the training job from a saved checkpoint. For information about how to detect and recover from automatic recovery failures, see [Detect and recover from TPU failures](#detect-failures) .\nThe mechanisms used to save and load checkpoints are different for each ML framework. Supported Cloud TPU models generally have checkpointing built-in. For more information on checkpointing, see : [TensorFlow 2.x](https://www.tensorflow.org/guide/checkpoint) , [PyTorch](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html) , or [JAX/flax](https://flax.readthedocs.io/en/latest/flax.training.html) .\n## Detecting Maintenance Events\nYou can detect if and when a maintenance event occurs on your TPU using the following [gcloud describe](/sdk/gcloud/reference/alpha/compute/tpus/tpu-vm/describe) command:\n```\n$ gcloud compute tpus tpu-vm describe tpu-name --zone=zone \u00a0| grep 'health'\n```\n```\n$ gcloud compute tpus describe tpu-name --zone=zone | grep 'health'\n```\nThe output from this command displays the current state of the TPU and a description of the most recent maintenance event. The output should look similar to the following:\n```\nhealth: HEALTHY\nhealthDescription: The TPU had a maintenance event at 2022-01-26T03:44:36.265703305Z\n```\n### Maintenance event logs\nYou can view historical logs of maintenance events on your TPU in [system event audit logs](/tpu/docs/audit-logs#audited_operations) .\nIn the Google Cloud console navigation menu, click **Compute Engine > VM instances** and search, for example:\n`\"tpu.nodes.terminate\" OR \"tpu.nodes.restart\"`\nWithin your search timeframe, any interruptions and repairs of your TPU workers are displayed. The logs will show the date and time of the event, the type of event, and for \"terminate\" events, the reason for the termination in `protoPayload.metadata.terminateReason` .\n## Handle maintenance events\nThere are several ways you can mitigate maintenance event disruptions.\n- Periodically save checkpointsIn the ideal scenario, when an \"interruption event\" happens, training resumes from the latest checkpoint.\n- Training script retriesThe training script might stop as a result of an \"interruption event\". You can use a `bash` script to continuously retry the training script until training is complete. Each retry should continue from the latest checkpoint, so retry scripts should always be used in conjunction with checkpoints.Production-ready training pipelines should use a resource management system such as Google Kubernetes Engine (GKE). For more information on using Google Kubernetes Engine with the TPU VM architecture, see [Deploy TPU workloads](/kubernetes-engine/docs/how-to/tpus) . For more information about using Google Kubernetes Engine with the TPU Node architecture, see [Run TPU applications on Google Kubernetes Engine](/tpu/docs/kubernetes-engine-setup) . Otherwise, you can implement a `bash` script to continuously retry the training script until completion. For example:With TPU Node:(From your VM) `bash while ! python3 [training command]; do sleep 1; done`With TPU VM:```\nwhile ! gcloud compute tpus tpu-vm ssh ${TPU_NAME} --command \"python3 [training command]\"; do sleep 1; done\n```(Note that you need to run the TPU VM command from a Cloud Shell or from a terminal, not from the TPU VM).\n- Detect and recover from TPU failuresWhen a TPU does not recover from a maintenance event, you can use a recovery script to detect the TPU state and delete and re-create the TPU. An example of this script can be found [here.](https://github.com/tensorflow/tpu/blob/master/tools/retry/retry.sh) See [Managing TPUs](/tpu/docs/creating-deleting-tpus) for details on manually deleting and re-creating TPUs.When creating or re-creating a TPU VM, you can specify a startup script with the `--metadata startup-script` parameter. A startup script runs whenever a TPU VM is created. Refer to [Run standard installation scripts](/tpu/docs/managing-tpus-tpu-vm#startup-scripts) for more information.", "guide": "Cloud TPU"}