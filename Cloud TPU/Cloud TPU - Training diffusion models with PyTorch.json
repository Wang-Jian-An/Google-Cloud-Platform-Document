{"title": "Cloud TPU - Training diffusion models with PyTorch", "url": "https://cloud.google.com/tpu/docs/tutorials/diffusion-pytorch", "abstract": "# Cloud TPU - Training diffusion models with PyTorch\nThis tutorial shows how to train diffusion models on TPUs using PyTorch Lightning and Pytorch XLA.\n **Warning:** This model uses a third-party dataset. Google provides no representation, warranty, or other guarantees about the validity, or any other aspects of this dataset.\n", "content": "## Objectives\n- Create a Cloud TPU\n- Install PyTorch Lightning\n- Clone the diffusion repo\n- Prepare the Imagenette dataset\n- Run the training script\n## CostsIn this document, you use the following billable components of Google Cloud:- Compute Engine\n- Cloud TPU\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \n## Before you beginBefore starting this tutorial, check that your Google Cloud project is correctly set up.- This walkthrough uses billable components of Google Cloud. Check the [Cloud TPU pricing page](/tpu/docs/pricing) to  estimate your costs. Be sure to [clean up](#clean_up) resources you create when you've finished with them to avoid unnecessary  charges.\n## Create a Cloud TPUThese instructions work on both single host and multi-host TPUs. This tutorial uses a v4-128, but it works similarly on all accelerator sizes.\nSet up some environment variables to make the commands easier to use.\n```\nexport ZONE=us-central2-bexport PROJECT_ID=your-project-idexport ACCELERATOR_TYPE=v4-128export RUNTIME_VERSION=tpu-ubuntu2204-baseexport TPU_NAME=your_tpu_name\n```\nCreate a Cloud TPU.\n **Note:** If there is not enough capacity currently available to create the TPU Pod, you can queue your request using queued resources. Queued resources allow you to receive capacity once it becomes available. To request your Cloud TPU resources as queued resources, use the`gcloud alpha compute tpus queued-resources create`command instead. For more information, see [Manage Queued Resources](/tpu/docs/queued-resources) .\n```\ngcloud compute tpus tpu-vm create ${TPU_NAME} \\--zone=${ZONE} \\--accelerator-type=${ACCELERATOR_TYPE} \\--version=${RUNTIME_VERSION} \\--subnetwork=tpusubnet\n```\n **Note:** The following commands run on all TPU VMs concurrently.## Install required software\n- Install required packages along with PyTorch/XLA latest release v2.2.0.```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\--zone=us-central2-b \\--worker=all \\--command=\"sudo apt-get update -y && sudo apt-get install libgl1 -ygit clone https://github.com/pytorch-tpu/stable-diffusion.gitcd stable-diffusionpip install -e .pip install https://github.com/Lightning-AI/lightning/archive/refs/heads/master.zip -Upip install clippip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 torchvision -f https://storage.googleapis.com/libtpu-releases/index.html\"\n```\n- Fix source files to be compatible with torch 2.2 and newer.```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\--zone=us-central2-b \\--worker=all \\--command=\"cd ~/stable-diffusion/sed -i \\'s/from torch._six import string_classes/string_classes = (str, bytes)/g\\' src/taming-transformers/taming/data/utils.pysed -i \\'s/trainer_kwargs\\\\[\\\"callbacks\\\"\\\\]/# trainer_kwargs\\\\[\\\"callbacks\\\"\\\\]/g\\' main_tpu.py\" \n```\n- Download Imagenette (a smaller version of [Imagenet dataset](https://www.image-net.org/) ) and move it to the appropriate directory.```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\--zone us-central2-b \\--worker=all \\--command=\"wget -nv https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgztar -xf \u00a0imagenette2.tgzmkdir -p ~/.cache/autoencoders/data/ILSVRC2012_train/datamkdir -p ~/.cache/autoencoders/data/ILSVRC2012_validation/datamv imagenette2/train/* \u00a0~/.cache/autoencoders/data/ILSVRC2012_train/datamv imagenette2/val/* ~/.cache/autoencoders/data/ILSVRC2012_validation/data\"\n```\n- Download the first stage pretrained model.```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\--zone us-central2-b \\--worker=all \\--command=\"cd ~/stable-diffusion/wget -nv -O models/first_stage_models/vq-f8/model.zip https://ommer-lab.com/files/latent-diffusion/vq-f8.zipcd \u00a0models/first_stage_models/vq-f8/unzip -o model.zip\"\n```\n## Train the modelRun the training with following command:\n```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\--zone us-central2-b \\--worker=all \\--command=\"python3 stable-diffusion/main_tpu.py --train --no-test --base=stable-diffusion/configs/latent-diffusion/cin-ldm-vq-f8-ss.yaml -- data.params.batch_size=32 lightning.trainer.max_epochs=5 model.params.first_stage_config.params.ckpt_path=stable-diffusion/models/first_stage_models/vq-f8/model.ckpt lightning.trainer.enable_checkpointing=False lightning.strategy.sync_module_states=False\"\n```\n## Clean upPerform a cleanup to avoid incurring unnecessary charges to your account after using the resources you created:\nUse Google Cloud CLI to delete the Cloud TPU resource.\n```\n\u00a0 $ \u00a0gcloud compute tpus delete diffusion-tutorial --zone=us-central2-b\u00a0 \n```## What's nextTry the PyTorch colabs:- [Getting Started with PyTorch on Cloud TPUs](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/getting-started.ipynb) \n- [Training MNIST on TPUs](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/mnist-training.ipynb) \n- [Training ResNet18 on TPUs with Cifar10 dataset](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/resnet18-training.ipynb) \n- [Inference with Pretrained ResNet50 Model](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/resnet50-inference.ipynb) \n- [Fast Neural Style Transfer](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/style_transfer_inference.ipynb) \n- [MultiCore Training AlexNet on Fashion MNIST](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/multi-core-alexnet-fashion-mnist.ipynb) \n- [Single Core Training AlexNet on Fashion MNIST](https://colab.sandbox.google.com/github/pytorch/xla/blob/master/contrib/colab/single-core-alexnet-fashion-mnist.ipynb)", "guide": "Cloud TPU"}