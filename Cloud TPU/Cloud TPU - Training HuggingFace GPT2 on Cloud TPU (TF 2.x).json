{"title": "Cloud TPU - Training HuggingFace GPT2 on Cloud TPU (TF 2.x)", "url": "https://cloud.google.com/tpu/docs/tutorials/hf-gpt2", "abstract": "# Cloud TPU - Training HuggingFace GPT2 on Cloud TPU (TF 2.x)\nIf you are not familiar with Cloud TPU, we recommend that you go through the [quickstart](https://cloud.google.com/tpu/docs/quickstart) to learn how to create a TPU VM.\nThis tutorial shows you how to train the HuggingFace GPT2 model on Cloud TPU.\n **Warning:** This tutorial uses a third-party dataset. Google provides no representation, warranty, or other guarantees about the validity, or any other aspects of this dataset.\n", "content": "## Objectives\n- Create a Cloud TPU\n- Install dependencies\n- Run the training job\n## CostsIn this document, you use the following billable components of Google Cloud:- Compute Engine\n- Cloud TPU\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \n## Before you beginBefore starting this tutorial, check that your Google Cloud project is correctly set up.- This walkthrough uses billable components of Google Cloud. Check the [Cloud TPU pricing page](/tpu/docs/pricing) to  estimate your costs. Be sure to [clean up](#clean-up) resources you create when you've finished with them to avoid unnecessary  charges.\n **Important: ** Set up all resources (Cloud TPU VM and Cloud Storage bucket) in the same region/zone to reduce network latency and network costs. TPU VMs are located in [specific zones](/tpu/docs/types-zones#types) , which are subdivisions within a region.\n## Train HuggingFace GPT2 with Cloud TPUs\n- Open a Cloud Shell window. [Open Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Create an environment variable for your project ID.```\nexport PROJECT_ID=your-project-id\n```\n- Configure Google Cloud CLI to use the your Google Cloud project where you want to create a Cloud TPU.```\ngcloud config set project ${PROJECT_ID}\n```The first time you run this command in a new Cloud Shell VM, an `Authorize Cloud Shell` page is displayed. Click `Authorize` at the bottom of the page to allow `gcloud` to make Google Cloud API calls with your credentials.\n- Create a Service Account for the Cloud TPU project.Service accounts allow the Cloud TPU service to access other Google Cloud services.```\n$ gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID\n```The command returns a Cloud TPU Service Account with following format:```\nservice-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com\n```\n### Create a Cloud TPU\n- Create a Cloud TPU VM using the `gcloud` command. The following command creates a `v4-8` TPU. You can also create a TPU Podslice by setting the `--accelerator-type` flag to a Podslice type, for example `v4-32` .```\n$ gcloud compute tpus tpu-vm create hf-gpt2 \\\u00a0 --zone=us-central2-b \\\u00a0 --accelerator-type=v4-8 \\\u00a0 --version=tpu-vm-tf-2.16.1-pjrt\n``` **Note:** If you have more than one Google Cloud project, you must use the `--project` flag to specify the ID of the Google Cloud in which you want to create the Cloud TPU.\n- Connect to the Cloud TPU VM by running the following `ssh` command.```\ngcloud compute tpus tpu-vm ssh hf-gpt2 --zone=us-central2-b\n``` **Note:** When you are connected to the Cloud TPU VM, your shell prompt changes from `username@projectname` to `username@vm-name` : **Key Point:** From this point on, a prefix of **(vm) $** means you should run the command on the Compute Engine VM instance.\n### Install dependencies\n- Clone the HuggingFace Transformers repo:```\n(vm)$ cd /tmp(vm)$ git clone https://github.com/huggingface/transformers.git(vm)$ cd transformers\n```\n- Install dependencies:```\n(vm)$ pip install .(vm)$ pip install -r examples/tensorflow/_tests_requirements.txt(vm)$ cd /tmp/transformers/examples/tensorflow/language-modeling(vm)$ pip install -r requirements.txt\n```\n- Create temp directory:```\n(vm)$ mkdir /tmp/gpt2-wikitext\n```\n- When creating your TPU, if you set the `--version` parameter to a version ending with `-pjrt` , set the following environment variables to enable the PJRT runtime:```\n\u00a0 (vm)$ export NEXT_PLUGGABLE_DEVICE_USE_C_API=true\u00a0 (vm)$ export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so\n```\n### Run training script```\n(vm)$ python3 run_clm.py \\\u00a0 --model_name_or_path distilgpt2 \\\u00a0 --max_train_samples 1000 \\\u00a0 --max_eval_samples 100 \\\u00a0 --num_train_epochs 1 \\\u00a0 --output_dir /tmp/gpt2-wikitext \\\u00a0 --dataset_name wikitext \\\u00a0 --dataset_config_name wikitext-103-raw-v1\n```When the training is complete, a message similar to the following is displayed:\n```\n 125/125 [============================] - ETA: 0s - loss: 3.61762023-07-07 21:38:17.902850: W tensorflow/core/framework/dataset.cc:956] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n 125/125 [============================] - 763s 6s/step - loss: 3.6176 - val_loss: 3.4233\n Configuration saved in /tmp/gpt2-wikitext/config.json\n Configuration saved in /tmp/gpt2-wikitext/generation_config.json\n Model weights saved in /tmp/gpt2-wikitext/tf_model.h5\n D0707 21:38:45.640973681 12027 init.cc:191]       grpc_shutdown starts clean-up now\n \n```### Clean up\n- Disconnect from the TPU VM instance:```\n(vm)$ exit\n```Your prompt should now be `username@projectname` , showing you are in the Cloud Shell.\n- Delete the TPU resource.```\n$ gcloud compute tpus tpu-vm delete hf-gpt2 \\--zone=us-central2-b\n```\n## What's nextTry one of the other [supported reference models](/tpu/docs/tutorials/supported-models) .", "guide": "Cloud TPU"}