{"title": "Cloud TPU - Cloud TPU v5e training", "url": "https://cloud.google.com/tpu/docs/v5e-training", "abstract": "# Cloud TPU - Cloud TPU v5e training\n# Cloud TPU v5e training\nCloud TPU v5e is Google Cloud's latest generation AI accelerator. With a smaller 256-chip footprint per Pod, a v5e is optimized to be the highest value product for transformer, text-to-image, and Convolutional Neural Network (CNN) training, fine-tuning, and serving.\n", "content": "## Cloud TPU v5e concepts, system architecture, and configurations\nIf you are new to Cloud TPUs, check out the [TPU documentation home](/tpu/docs) .\nGeneral Cloud TPU concepts (for example, slices, hosts, chips, and TensorCores), and Cloud TPU system architecture are described in the [Cloud TPU System Architecture](/tpu/docs/system-architecture-tpu-vm) page.\nEach Cloud TPU version requires specific accelerator types for training and inference. These accelerator types are described in the [v5e configurations](/tpu/docs/v5e#tpu-v5e-config) section.\n### Inference\nInference is the process of using a trained model to make predictions on new data. It is used by the [serving](#serving) process.\n### Slices\nA slice represents a collection of chips all located inside the same Pod connected by high-speed inter chip interconnects (ICI). v5e has 2D slice shapes. See the table in the [v5e configurations](/tpu/docs/v5e#tpu-v5e-config) section for supported slice shapes.\nand also refer to slice shapes.\n### Serving\nServing is the process of deploying a trained machine learning model to a production environment where it can be used to make predictions or decisions. Latency and service-level availability are important for serving.\n### Single host versus multi host\nA host is a physical computer (CPU) that runs VMs. A host can run multiple VMs at once.\nSlices using fewer than 8 chips use at most one host. Slices greater than 8 chips have access to more than a single host and can run distributed training using multiple hosts. See the [TPU System Architecture](/tpu/docs/system-architecture-tpu-vm) page for details on slices and chips.\nv5e supports multi-host training and multi-host inference (using [SAX](https://github.com/google/saxml) ).\n### TPU VM\nA virtual machine running Linux that has access to the underlying TPUs. For v5e TPUs, each TPU VM has direct access to 1, 4, or 8 chips depending on the user-specified accelerator type. A TPU VM is also known as a .\n### Worker\nSee [TPU VM](#tpu-vm) .\n## Get started\nFor information about v5e TPU hardware, see [System Architecture](/tpu/docs/system-architecture-tpu-vm) .\n### Securing capacity\nContact [Cloud Sales](https://cloud.google.com/contact) to start using Cloud TPU v5e for your AI workloads.\n### Prepare a Google Cloud Project\n- [Sign in](https://accounts.google.com/Login) to your Google Account. If you haven't already, [sign up for a new account](https://accounts.google.com/SignUp) .\n- In the [Google Cloud console](https://console.cloud.google.com/) , [select](/resource-manager/docs/creating-managing-projects#get_an_existing_project) or [create](/resource-manager/docs/creating-managing-projects#creating_a_project) a Google Cloud project from the project selector page.\n- [Billing setup](/billing/docs) is required for all Google Cloud usage so make sure billing is enabled for your project.\n- Install [gcloud alpha components](/sdk/gcloud/reference/components/install) .\n- If you are a TPU user reusing existing `gcloud alpha` components, update these to ensure that relevant commands and flags are supported:```\ngcloud components update\n```\n- Enable the TPU API through the following `gcloud` command in the Cloud Shell. (You may also enable it [from the Google Cloud console](https://console.cloud.google.com/apis/library/tpu.googleapis.com) .)```\ngcloud services enable tpu.googleapis.com\n```\n- Enable the TPU service account.Service accounts allow the Cloud TPU service to access other Google Cloud services. A user-managed service account is a recommended Google Cloud practice. Follow these guides to [create](/iam/docs/service-accounts-create) and [grant roles](/iam/docs/granting-changing-revoking-access) . The following roles are necessary:- TPU Admin\n- Storage Admin: Needed for accessing Cloud Storage\n- Logs Writer: Needed for writing logs with the Logging API\n- Monitoring Metric Writer: Needed for writing metrics to Cloud Monitoring\n- Configure the project and zone.Your project ID is the name of your project [shown on the Cloud console](/resource-manager/docs/creating-managing-projects#identifying_projects) .```\nexport PROJECT_ID=your-project-idexport ZONE=us-west4-agcloud alpha compute tpus tpu-vm service-identity create --zone=${ZONE}gcloud auth logingcloud config set project ${PROJECT_ID}gcloud config set compute/zone ${ZONE}\n```\n### Provision the Cloud TPU environment\nThe best practice is to provision Cloud TPU v5es as [queued resources](/tpu/docs/queued-resources) using the `queued-resource create` command. However, you can also use the Create Node API ( `gcloud alpha compute tpus tpu-vm create` ) to provision Cloud TPU v5es.\nSet necessary environment variables for TPU creation.\nReplace the variables (in red) in the following list with values you will use for your training or inference job.\n```\nexport PROJECT_ID=your_project_IDexport ACCELERATOR_TYPE=v5litepod-16export ZONE=us-west4-aexport RUNTIME_VERSION=v2-alpha-tpuv5-liteexport SERVICE_ACCOUNT=your_service_accountexport TPU_NAME=your_tpu_nameexport QUEUED_RESOURCE_ID=your_queued_resource_idexport QUOTA_TYPE=quota_typeexport VALID_UNTIL_DURATION=1d\n```\n| 0     | 1                                                      |\n|:---------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Variable    | Description                                                   |\n| PROJECT_ID   | Google Cloud Project Name                                                |\n| ACCELERATOR_TYPE  | See the Accelerator Types section for supported accelerator types.                                     |\n| ZONE     | All capacity is in us-west4-a.                                               |\n| RUNTIME_VERSION  | Use v2-alpha-tpuv5-lite.                                                |\n| SERVICE_ACCOUNT  | This is the address of your service account that you can find in Google Cloud console -> IAM -> Service Accounts. For example: tpu-service-account@myprojectID.iam.gserviceaccount.com        |\n| TPU_NAME    | The user-assigned text ID of the TPU which is created when the queued resource request is allocated.                             |\n| QUEUED_RESOURCE_ID | The user-assigned text ID of the queued resource request. See the queued resources document for information about queued resources.                     |\n| QUOTA_TYPE   | The flag can be either reserved or best-effort. See TPU Quota types for information on the different types of quotas supported by Cloud TPU. best-effort quota is the default, so you don't need to set that value. |\n| VALID_UNTIL_DURATION | The duration for which the request is valid. See queued resources for information about the different valid durations.                         |\n```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--node-id=${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--accelerator-type=${ACCELERATOR_TYPE} \\\u00a0 \u00a0--runtime-version=${RUNTIME_VERSION} \\\u00a0 \u00a0--valid-until-duration=${VALID_UNTIL_DURATION} \\\u00a0 \u00a0--service-account=${SERVICE_ACCOUNT} \\\u00a0 \u00a0--${QUOTA_TYPE}\n```\n**Note:** The `QUOTA_TYPE` flag can be either `reserved` or `best-effort` . See [quotas](/tpu/docs/quota#quota_types) for information on the different types of quotas supported by Cloud TPU.\nIf the queued resource is created successfully, the state within the `response` field will be either `WAITING_FOR_RESOURCES` or `FAILED` . If the queued resource is in the `WAITING_FOR_RESOURCES` state, that means that the queued resource has passed preliminary validation and is awaiting capacity. Once capacity is available the request will transition to `PROVISIONING` . Being in `WAITING_FOR_RESOURCES` state does not mean it is guaranteed that you will get the quota allocated and it may take some time to change from `WAITING_FOR_RESOURCES` status to `ACTIVE` status. If the queued resource is in the `FAILED` state, the failure reason will be in the output. The request will expire if a request is not filled in the `--valid-until-duration` , and the state becomes \"FAILED\".\nYou will be able to access your TPU VM using SSH once your queued resource is in the `ACTIVE` state.\nUse the `[list](/tpu/docs/managing-tpus-tpu-vm)` or `[describe](/tpu/docs/managing-tpus-tpu-vm)` commands to query the status of your queued resource.\n```\ngcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE}\n```\nThe represents the status of a queued resource. The states are defined as:\n| 0      | 1                                             |\n|:----------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| State     | Description                                          |\n| WAITING_FOR_RESOURCES | The queued resource create command has been received and will begin provisioning, as soon as capacity is available.                |\n| PROVISIONING   | The TPU slices are being provisioned.                                    |\n| ACTIVE    | All TPUs are provisioned, and are ready to use. If a startup script is given, it will start executing on all TPUs, when the queued resource state transitions to ACTIVE.   |\n| FAILED    | The slices couldn't be provisioned.                                    |\n| SUSPENDING   | One or more slices are being deleted.                                    |\n| SUSPENDED    | All underlying slices are deleted but the queued resource stays intact, until explicitly deleted. Right now, a suspended queued resource cannot be resumed and should be deleted. |\n| DELETING    | The queued resource is being deleted.                                    |\n### Connect to the TPU VM using SSH\nThe following section describes how you can install binaries on each TPU VM in your TPU slice and run code. In this context, a TPU VM is also known as a .\nSee the [VM Types](/tpu/docs/v5e#v5e-types) section to determine how many VMs your slice will have.\nTo install the binaries or run code, connect to your TPU VM using the [tpu-vm ssh command](/sdk/gcloud/reference/alpha/compute/tpus/tpu-vm/ssh) .\n```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME}\n```\nTo access a specific TPU VM with SSH, use the `--worker` flag which follows a 0-based index:\n```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --worker=1\n```\nIf you have slice shapes greater than 8 chips, you will have multiple VMs in one slice. In this case, use the `--worker=all` flag to run the installation on all TPU VMs without having to connect to each one separately. For example:\n```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='pip install \"jax[tpu]==0.4.16\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'\n```\n### Manage\nAll commands you can use to manage your TPU VMs are described in [Managing TPUs.](/tpu/docs/managing-tpus-tpu-vm)\n### Framework Setup\nThis section describes the general setup process for custom model training using JAX or PyTorch with TPU v5e. TensorFlow support is available in the `tpu-vm-tf-2.15.0-pjrt` and `tpu-vm-tf-2.15.0-pod-pjrt` TPU runtime versions.\nFor inference setup instructions, see [v5e inference introduction](/tpu/docs/v5e-inference) .\nIf you have slice shapes greater than 8 chips, you will have multiple VMs in one slice. In this case, you need to use the `--worker=all` flag to run the installation on all TPU VMs in a single step without using SSH to log into each separately:\n```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \u00a0\\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='pip install \"jax[tpu]==0.4.16\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'\n```\nYou can run the following command to check number of devices (the outputs shown here were produced with a v5litepod-16 slice). This code tests that everything is installed correctly by checking that JAX sees the Cloud TPU TensorCores and can run basic operations:\n```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='python3 -c \"import jax; print(jax.device_count()); print(jax.local_device_count())\"'\n```\nThe output will be similar to the following:\n```\nSSH: Attempting to connect to worker 0...SSH: Attempting to connect to worker 1...SSH: Attempting to connect to worker 2...SSH: Attempting to connect to worker 3...164164164164\n```\n`jax.device\\_count()` shows the total number of chips in the given slice. `jax.local\\_device\\_count()` indicates the count of chips accessible by a single VM in this slice.\n```\n# Check the number of chips in the given slice by summing the count of chips# from all VMs through the# jax.local_device_count() API call.gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='python3 -c \"import jax; xs=jax.numpy.ones(jax.local_device_count()); print(jax.pmap(lambda x: jax.lax.psum(x, \\\"i\\\"), axis_name=\\\"i\\\")(xs))\"'\n```\nThe output will be similar to the following:\n```\nSSH: Attempting to connect to worker 0...SSH: Attempting to connect to worker 1...SSH: Attempting to connect to worker 2...SSH: Attempting to connect to worker 3...[16. 16. 16. 16.][16. 16. 16. 16.][16. 16. 16. 16.][16. 16. 16. 16.]\n```\nTry the [JAX Tutorials](#jax-flax-examples) in this document to get started with v5e training using JAX.\nNote that v5e only supports the [PJRT runtime](https://github.com/pytorch/xla/blob/master/docs/pjrt.md) and PyTorch 2.1+ will use PJRT as the default runtime for all TPU versions.\nThis section describes how to start using PJRT on v5e with PyTorch/XLA with commands for all workers.\n```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \u00a0\\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='\u00a0 \u00a0 \u00a0 sudo apt-get update -y\u00a0 \u00a0 \u00a0 sudo apt-get install libomp5 -y\u00a0 \u00a0 \u00a0 pip3 install mkl mkl-include\u00a0 \u00a0 \u00a0 pip3 install tf-nightly tb-nightly tbp-nightly\u00a0 \u00a0 \u00a0 pip3 install numpy\u00a0 \u00a0 \u00a0 sudo apt-get install libopenblas-dev -y\u00a0 \u00a0 \u00a0 pip3 install torch~=2.1.0 torchvision torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html\u00a0 \u00a0 \u00a0 pip3 install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html'\n```\nIf you failed to install wheels for torch/torch_xla/torchvision and see an error like `pkg_resources.extern.packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier) torch==nightly+20230222` , downgrade your version with this command:\n```\npip3 install setuptools==62.1.0\n```\n**Note:** For models which have sizable, frequent allocations, using `tcmalloc` can significantly improve training time compared to the default `malloc` implementation, so the default `malloc` used on a TPU VM is `tcmalloc` . However, depending on your workload (for example, with DLRM which has very large allocations for its embedding tables) `tcmalloc` may cause a slowdown. In this case you might try to unset the following variable using the default `malloc` instead:\n```\nunset LD_PRELOAD\n```\nThe following is an example using a Python script to do a calculation on a v5e VM:\n```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker all \\\u00a0 \u00a0--command='\u00a0 \u00a0 \u00a0 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.local/lib/\u00a0 \u00a0 \u00a0 export PJRT_DEVICE=TPU_C_API\u00a0 \u00a0 \u00a0 export PT_XLA_DEBUG=0\u00a0 \u00a0 \u00a0 export USE_TORCH=ON\u00a0 \u00a0 \u00a0 unset LD_PRELOAD\u00a0 \u00a0 \u00a0 export TPU_LIBRARY_PATH=$HOME/.local/lib/python3.10/site-packages/libtpu/libtpu.so\u00a0 \u00a0 \u00a0 python3 -c \"import torch; import torch_xla; import torch_xla.core.xla_model as xm; print(xm.xla_device()); dev = xm.xla_device(); t1 = torch.randn(3,3,device=dev); t2 = torch.randn(3,3,device=dev); print(t1 + t2)\"'\n```\nThis generates output similar to the following:\n```\nSSH: Attempting to connect to worker 0...SSH: Attempting to connect to worker 1...xla:0tensor([[ 1.8611, -0.3114, -2.4208],[-1.0731, 0.3422, 3.1445],[ 0.5743, 0.2379, 1.1105]], device='xla:0')xla:0tensor([[ 1.8611, -0.3114, -2.4208],[-1.0731, 0.3422, 3.1445],[ 0.5743, 0.2379, 1.1105]], device='xla:0')\n```\nTry the [PyTorch Tutorials](#pytorch-xla) in this document to get started with v5e training using PyTorch.\nDelete your TPU and queued resource at the end of your session. To delete a queued resource, delete the slice and then the queued resource in 2 steps:\n```\ngcloud alpha compute tpus tpu-vm delete ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quietgcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quiet\n```\nThese two steps can also be used to remove queued resource requests that are in the `FAILED` state.\n### Monitor and Profile\nCloud TPU v5e supports monitoring and profiling using the same methods as previous generations of Cloud TPU. You can read [Profile your model with Cloud TPU tools](/tpu/docs/cloud-tpu-tools) to learn more about profiling and [Monitoring Cloud TPU VMs](/tpu/docs/troubleshooting/tpu-vm-monitoring) to learn more about monitoring.\n## JAX/FLAX examples\n### Train ImageNet on v5e\nThis tutorial describes how to train ImageNet on v5e using fake input data. If you want to use real data, refer to the [README file on GitHub](https://github.com/google/flax/blob/main/examples/imagenet/README.md) .- Create environment variables:```\nexport PROJECT_ID=your_project_IDexport ACCELERATOR_TYPE=v5litepod-16export ZONE=us-west4-aexport RUNTIME_VERSION=v2-alpha-tpuv5-liteexport SERVICE_ACCOUNT=your_service_accountexport TPU_NAME=your_tpu_nameexport QUEUED_RESOURCE_ID=your_queued_resource_idexport QUOTA_TYPE=quota_typeexport VALID_UNTIL_DURATION=1d\n```\n- [Create a TPU resource:](/tpu/docs/queued-resources) ```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--node-id=${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--accelerator-type=${ACCELERATOR_TYPE} \\\u00a0 \u00a0--runtime-version=${RUNTIME_VERSION} \\\u00a0 \u00a0--valid-until-duration=${VALID_UNTIL_DURATION} \\\u00a0 \u00a0--service-account=${SERVICE_ACCOUNT} \\\u00a0 \u00a0--${QUOTA_TYPE}\n``` **Note:** The `QUOTA_TYPE` flag can be either `reserved` or `best-effort` . See [quotas](/tpu/docs/quota#quota_types) for information on the different types of quotas supported by Cloud TPU. **Note:** You will be able to SSH to your TPU VM once your queued resource is in the `ACTIVE` state:```\ngcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \u00a0\\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE}\n```When the QueuedResource is in the `ACTIVE` state, the output will be similar to the following:```\n\u00a0state: ACTIVE\n```\n- Install newest version of JAX and jaxlib:```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='pip install \"jax[tpu]==0.4.16\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'\n``` **Note:** If you see an error suggesting you use jax>=0.4.8, you can safely ignore the message.\n- Clone the ImageNet model and install the corresponding requirements:```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='git clone https://github.com/google/flax.git && cd flax/examples/imagenet && pip install -r requirements.txt && pip install flax==0.7.4'\n```\n- To generate fake data, the model needs information on the dimensions of the dataset. This can be gathered from the ImageNet dataset's metadata:```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --project=${PROJECT_ID} --zone=${ZONE} --worker=all --command='mkdir -p $HOME/flax/.tfds/metadata/imagenet2012/5.1.0 && curl https://raw.githubusercontent.com/tensorflow/datasets/v4.4.0/tensorflow_datasets/testing/metadata/imagenet2012/5.1.0/dataset_info.json --output $HOME/flax/.tfds/metadata/imagenet2012/5.1.0/dataset_info.json'\n```Once all the previous steps are done, you can train the model.\n```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='cd flax/examples/imagenet && JAX_PLATFORMS=tpu python3 imagenet_fake_data_benchmark.py'\n```\nDelete your TPU and queued resource at the end of your session.\n```\ngcloud alpha compute tpus tpu-vm delete ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quiet\n```\n```\ngcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quiet\n```\n### Hugging Face FLAX Models\n[Hugging Face](https://huggingface.co/) models implemented in FLAX work out of the box on Cloud TPU v5e. This section provides instructions for running popular models.\nThis tutorial shows you how to train the [Vision Transformer](https://arxiv.org/abs/2010.11929) (ViT) model from HuggingFace using the Fast AI [imagenette](https://github.com/fastai/imagenette) dataset on Cloud TPU v5e.\nThe ViT model was the first one that successfully trained a Transformer encoder on ImageNet with excellent results compared to convolutional networks. For more information, see the following resources:\n- [ViT overview](https://huggingface.co/docs/transformers/model_doc/vit) \n- [ViT code resource](https://github.com/huggingface/transformers/tree/main/examples/flax/vision) - Create environment variables:```\nexport PROJECT_ID=your_project_IDexport ACCELERATOR_TYPE=v5litepod-16export ZONE=us-west4-aexport RUNTIME_VERSION=v2-alpha-tpuv5-liteexport SERVICE_ACCOUNT=your_service_accountexport TPU_NAME=your_tpu_nameexport QUEUED_RESOURCE_ID=your_queued_resource_idexport QUOTA_TYPE=quota_typeexport VALID_UNTIL_DURATION=1d\n```\n- [Create a TPU resource:](/tpu/docs/queued-resources) ```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--node-id=${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--accelerator-type=${ACCELERATOR_TYPE} \\\u00a0 \u00a0--runtime-version=${RUNTIME_VERSION} \\\u00a0 \u00a0--valid-until-duration=${VALID_UNTIL_DURATION} \\\u00a0 \u00a0--service-account=${SERVICE_ACCOUNT} \\\u00a0 \u00a0--${QUOTA_TYPE}\n``` **Note:** The `QUOTA_TYPE` flag can be either `reserved` or `best-effort` . See [quotas](/tpu/docs/quota#quota_types) for information on the different types of quotas supported by Cloud TPU.You will be able to SSH to your TPU VM once your queued resource is in state `ACTIVE` :```\ngcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \u00a0\\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE}\n```When the queued resource is in the `ACTIVE` state, the output will be similar to the following:```\n\u00a0state: ACTIVE\n```\n- Install JAX and its library:```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='pip install \"jax[tpu]==0.4.16\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'\n```\n- Download Hugging Face [repository](https://github.com/huggingface/transformers.git) and install requirements:```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='git clone https://github.com/huggingface/transformers.git && cd transformers && pip install . && pip install -r examples/flax/_tests_requirements.txt && pip install --upgrade huggingface-hub urllib3 zipp && pip install tensorflow==2.16.1 && pip install -r examples/flax/vision/requirements.txt'\n```\n- Download the Imagenette dataset:```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='cd transformers && wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz && tar -xvzf imagenette2.tgz'\n```Train the model with a pre-mapped buffer at 4GB.\n```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='cd transformers && JAX_PLATFORMS=tpu python3 examples/flax/vision/run_image_classification.py --train_dir \"imagenette2/train\" --validation_dir \"imagenette2/val\" --output_dir \"./vit-imagenette\" --learning_rate 1e-3 --preprocessing_num_workers 32 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --model_name_or_path google/vit-base-patch16-224-in21k --num_train_epochs 3'\n```\nDelete your TPU and queued-resource at the end of your session.\n```\ngcloud alpha compute tpus tpu-vm delete ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quietgcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quiet\n```\nThe training script was run on v5litepod-4, v5litepod-16, and v5litepod-64. The following table shows the throughputs with different accelerator types.\n| 0       | 1   | 2   | 3   |\n|:--------------------------|:------------|:-------------|:-------------|\n| Accelerator type   | v5litepod-4 | v5litepod-16 | v5litepod-64 |\n| Epoch      | 3   | 3   | 3   |\n| Global batch size   | 32   | 128   | 512   |\n| Throughput (examples/sec) | 263.40  | 429.34  | 470.71  |\nThis tutorial shows you how to train the Stable Diffusion model from HuggingFace using the [Pok\u00e9mon](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions) dataset on Cloud TPU v5e.\nThe Stable Diffusion model is a latent text-to-image model that generates photo-realistic images from any text input. For more information, see the following resources:\n- [Stable Diffusion overview](https://huggingface.co/CompVis/stable-diffusion-v1-4) \n- [Stable Diffusion code source ](https://github.com/huggingface/diffusers/tree/main/examples/text_to_image) - Create environment variables:```\nexport PROJECT_ID=your_project_IDexport ACCELERATOR_TYPE=v5litepod-16export ZONE=us-west4-aexport RUNTIME_VERSION=v2-alpha-tpuv5-liteexport SERVICE_ACCOUNT=your_service_accountexport TPU_NAME=your_tpu_nameexport QUEUED_RESOURCE_ID=queued_resource_idexport QUOTA_TYPE=quota_typeexport VALID_UNTIL_DURATION=1d\n```\n- [Create a TPU resource:](/tpu/docs/queued-resources) ```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--node-id=${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--accelerator-type=${ACCELERATOR_TYPE} \\\u00a0 \u00a0--runtime-version=${RUNTIME_VERSION} \\\u00a0 \u00a0--valid-until-duration=${VALID_UNTIL_DURATION} \\\u00a0 \u00a0--service-account=${SERVICE_ACCOUNT} \\\u00a0 \u00a0--${QUOTA_TYPE}\n``` **Note:** The `QUOTA_TYPE` flag can be either `reserved` or `best-effort` . See [quotas](/tpu/docs/quota#quota_types) for information on the different types of quotas supported by Cloud TPU. **Note:** You will be able to SSH to your TPU VM once your queued resource is in the `ACTIVE` state:```\ngcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \u00a0\\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE}\n```When the queued resource is in the `ACTIVE` state, the output will be similar to the following:```\n\u00a0state: ACTIVE\n```\n- Install JAX and its library.```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='pip install \"jax[tpu]==0.4.16\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'\n```\n- Download the HuggingFace [repository](https://github.com/huggingface/diffusers) and install requirements.```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='git clone https://github.com/RissyRan/diffusers.git && cd diffusers && pip install . && pip install tensorflow==2.16.1 clu && pip install -U -r examples/text_to_image/requirements_flax.txt'\n```Train the model with a pre-mapped buffer at 4GB.\n```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='cd diffusers/examples/text_to_image && JAX_PLATFORMS=tpu,cpu python3 train_text_to_image_flax.py --pretrained_model_name_or_path=duongna/stable-diffusion-v1-4-flax --dataset_name=lambdalabs/pokemon-blip-captions --resolution=128 --center_crop --random_flip --train_batch_size=4 --mixed_precision=fp16 --max_train_steps=1500 --learning_rate=1e-05 --max_grad_norm=1 --output_dir=sd-pokemon-model'\n```\nDelete your TPU and queued resource at the end of your session.\n```\ngcloud alpha compute tpus tpu-vm delete ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quietgcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quiet\n```\nThe training script ran on v5litepod-4, v5litepod-16, and v5litepod-64. The following table shows the throughputs.\n| 0       | 1   | 2   | 3   |\n|:--------------------------|:------------|:-------------|:-------------|\n| Accelerator type   | v5litepod-4 | v5litepod-16 | v5litepod-64 |\n| Train Step    | 1500  | 1500   | 1500   |\n| Global batch size   | 32   | 64   | 128   |\n| Throughput (examples/sec) | 36.53  | 43.71  | 49.36  |\n### Train GPT2 on the OSCAR dataset\nThis tutorial shows you how to train the [ GPT2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) model from HuggingFace using the [OSCAR](https://huggingface.co/datasets/oscar) dataset on Cloud TPU v5e.\nThe GPT2 is a transformer model pre-trained on raw texts without human labeling. It was trained to predict the next word in sentences. For more information, see the following resources:\n- [GPT2 overview](https://huggingface.co/gpt2) \n- [GPT2 code source](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling) - Create environment variables:```\nexport PROJECT_ID=your_project_IDexport ACCELERATOR_TYPE=v5litepod-16export ZONE=us-west4-aexport RUNTIME_VERSION=v2-alpha-tpuv5-liteexport SERVICE_ACCOUNT=your_service_accountexport TPU_NAME=your_tpu_nameexport QUEUED_RESOURCE_ID=queued_resource_idexport QUOTA_TYPE=quota_typeexport VALID_UNTIL_DURATION=1d\n```\n- [Create a TPU resource:](/tpu/docs/queued-resources) ```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--node-id=${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--accelerator-type=${ACCELERATOR_TYPE} \\\u00a0 \u00a0--runtime-version=${RUNTIME_VERSION} \\\u00a0 \u00a0--valid-until-duration=${VALID_UNTIL_DURATION} \\\u00a0 \u00a0--service-account=${SERVICE_ACCOUNT} \\\u00a0 \u00a0--${QUOTA_TYPE}\n``` **Note:** The `QUOTA_TYPE` flag can be either `reserved` or `best-effort` . See [quotas](/tpu/docs/quota#quota_types) for information on the different types of quotas supported by Cloud TPU. **Note:** You will be able to SSH to your TPU VM once your queued resource is in the `ACTIVE` state:```\ngcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE}\n```When the queued resource is in the `ACTIVE` state, the output will be similar to the following:```\n\u00a0state: ACTIVE\n```\n- Install JAX and its library.```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='pip install \"jax[tpu]==0.4.16\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'\n```\n- Download HuggingFace [repository](https://github.com/huggingface/transformers.git) and install requirements.```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='git clone https://github.com/huggingface/transformers.git && cd transformers && pip install . && pip install -r examples/flax/_tests_requirements.txt && pip install --upgrade huggingface-hub urllib3 zipp && pip install tensorflow && pip install -r examples/flax/language-modeling/requirements.txt'\n```\n- Download configs to train the model.```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='cd transformers/examples/flax/language-modeling && gsutil cp -r gs://cloud-tpu-tpuvm-artifacts/v5litepod-preview/jax/gpt .'\n```Train the model with a pre-mapped buffer at 4GB.\n```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='cd transformers/examples/flax/language-modeling && TPU_PREMAPPED_BUFFER_SIZE=4294967296 JAX_PLATFORMS=tpu python3 run_clm_flax.py --output_dir=./gpt --model_type=gpt2 --config_name=./gpt --tokenizer_name=./gpt --dataset_name=oscar --dataset_config_name=unshuffled_deduplicated_no --do_train --do_eval --block_size=512 --per_device_train_batch_size=4 --per_device_eval_batch_size=4 --learning_rate=5e-3 --warmup_steps=1000 --adam_beta1=0.9 --adam_beta2=0.98 --weight_decay=0.01 --overwrite_output_dir --num_train_epochs=3 --logging_steps=500 --eval_steps=2500'\n```\nDelete your TPU and queued-resource at the end of your session.\n```\ngcloud alpha compute tpus tpu-vm delete ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quietgcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quiet\n```\nThe training script ran on v5litepod-4, v5litepod-16, and v5litepod-64. The following table shows the throughputs.\n| 0       | 1   | 2   | 3   |\n|:--------------------------|:------------|:-------------|:-------------|\n| nan      | v5litepod-4 | v5litepod-16 | v5litepod-64 |\n| Epoch      | 3   | 3   | 3   |\n| Global batch size   | 64   | 64   | 64   |\n| Throughput (examples/sec) | 74.60  | 72.97  | 72.62  |\n## PyTorch/XLA\n### Train ResNet using the PJRT runtime\nPyTorch/XLA is migrating from XRT to PjRt from PyTorch 2.0+. Here are the updated instructions to setup v5e for PyTorch/XLA training workloads.- Create environment variables:```\nexport PROJECT_ID=your_project_IDexport ACCELERATOR_TYPE=v5litepod-16export ZONE=us-west4-aexport RUNTIME_VERSION=v2-alpha-tpuv5-liteexport SERVICE_ACCOUNT=your_service_accountexport TPU_NAME=tpu-nameexport QUEUED_RESOURCE_ID=queued_resource_idexport QUOTA_TYPE=quota_typeexport VALID_UNTIL_DURATION=1d\n```\n- [Create a TPU resource:](/tpu/docs/queued-resources) ```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--node-id=${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--accelerator-type=${ACCELERATOR_TYPE} \\\u00a0 \u00a0--runtime-version=${RUNTIME_VERSION} \\\u00a0 \u00a0--valid-until-duration=${VALID_UNTIL_DURATION} \\\u00a0 \u00a0--service-account=${SERVICE_ACCOUNT} \\\u00a0 \u00a0--{QUOTA_TYPE}\n``` **Note:** The `QUOTA_TYPE` flag can be either `reserved` or `best-effort` . See [quotas](/tpu/docs/quota#quota_types) for information on the different types of quotas supported by Cloud TPU. **Note:** You will be able to SSH to your TPU VM once your QueuedResource is in `ACTIVE` state:```\ngcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE}\n```When the queued resource is in the `ACTIVE` state, the output will be similar to the following:```\n\u00a0state: ACTIVE\n```\n- Install Torch/XLA specific dependencies```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='\u00a0 \u00a0 \u00a0 sudo apt-get update -y\u00a0 \u00a0 \u00a0 sudo apt-get install libomp5 -y\u00a0 \u00a0 \u00a0 pip3 install mkl mkl-include\u00a0 \u00a0 \u00a0 pip3 install tf-nightly tb-nightly tbp-nightly\u00a0 \u00a0 \u00a0 pip3 install numpy\u00a0 \u00a0 \u00a0 sudo apt-get install libopenblas-dev -y\u00a0 \u00a0 \u00a0 pip3 install torch~=2.1.0 torchvision torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html\u00a0 \u00a0 \u00a0 pip3 install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html'\n``````\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='\u00a0 \u00a0 \u00a0 date\u00a0 \u00a0 \u00a0 export PJRT_DEVICE=TPU_C_API\u00a0 \u00a0 \u00a0 export PT_XLA_DEBUG=0\u00a0 \u00a0 \u00a0 export USE_TORCH=ON\u00a0 \u00a0 \u00a0 export XLA_USE_BF16=1\u00a0 \u00a0 \u00a0 export LIBTPU_INIT_ARGS=--xla_jf_auto_cross_replica_sharding\u00a0 \u00a0 \u00a0 export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\u00a0 \u00a0 \u00a0 export TPU_LIBRARY_PATH=$HOME/.local/lib/python3.10/site-packages/libtpu/libtpu.so\u00a0 \u00a0 \u00a0 git clone https://github.com/pytorch/xla.git\u00a0 \u00a0 \u00a0 cd xla/\u00a0 \u00a0 \u00a0 git reset --hard caf5168785c081cd7eb60b49fe4fffeb894c39d9\u00a0 \u00a0 \u00a0 python3 test/test_train_mp_imagenet.py --model=resnet50 \u00a0--fake_data --num_epochs=1 \u2014num_workers=16 \u00a0--log_steps=300 --batch_size=64 --profile'\n```\nDelete your TPU and queued-resource at the end of your session.\n```\ngcloud alpha compute tpus tpu-vm delete ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quietgcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quiet\n```\nThe following table shows the benchmark throughputs.\n| 0    | 1       |\n|:-----------------|:-----------------------------|\n| Accelerator type | Throughput (examples/second) |\n| v5litepod-4  | 4240 ex/s     |\n| v5litepod-16  | 10,810 ex/s     |\n| v5litepod-64  | 46,154 ex/s     |\n### Train GPT2 on v5e\nThis tutorial will cover how to run GPT2 on v5e using HuggingFace [repository](https://github.com/huggingface/transformers.git) on PyTorch/XLA using the [wikitext dataset.](https://huggingface.co/datasets/wikitext)- Create environment variables:```\nexport PROJECT_ID=your_project_IDexport ACCELERATOR_TYPE=v5litepod-16export ZONE=us-west4-aexport RUNTIME_VERSION=v2-alpha-tpuv5-liteexport SERVICE_ACCOUNT=your_service_accountexport TPU_NAME=your_tpu_nameexport QUEUED_RESOURCE_ID=queued_resource_idexport QUOTA_TYPE=quota_typeexport VALID_UNTIL_DURATION=1d\n```\n- [Create a TPU resource:](/tpu/docs/queued-resources) ```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--node-id=${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--accelerator-type=${ACCELERATOR_TYPE} \\\u00a0 \u00a0--runtime-version=${RUNTIME_VERSION} \\\u00a0 \u00a0--valid-until-duration=${VALID_UNTIL_DURATION} \\\u00a0 \u00a0--service-account=${SERVICE_ACCOUNT} \\\u00a0 \u00a0--${QUOTA_TYPE}\n``` **Note:** The `QUOTA_TYPE` flag is only needed for reserved resources, not for best-effort resources. See [quotas](/tpu/docs/quota#quota_types) for information on the different types of quotas supported by Cloud TPU. **Note:** You will be able to SSH to your TPU VM once your QueuedResource is in `ACTIVE` state:```\ngcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE}\n```When the queued resource is in the `ACTIVE` state, the output will be similar to the following:```\nstate: ACTIVE\n```\n- Install torch/xla dependencies.```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='\u00a0 \u00a0 \u00a0 sudo apt-get -y update\u00a0 \u00a0 \u00a0 sudo apt install -y libopenblas-base\u00a0 \u00a0 \u00a0 pip3 install torchvision\u00a0 \u00a0 \u00a0 pip3 uninstall -y torch\u00a0 \u00a0 \u00a0 pip3 install torch~=2.1.0 torchvision torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html\u00a0 \u00a0 \u00a0 pip3 install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html'\n```\n- Download HuggingFace [repo](https://github.com/huggingface/transformers.git) and install requirements.```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='\u00a0 \u00a0 \u00a0 git clone https://github.com/pytorch/xla.git\u00a0 \u00a0 \u00a0 pip install --upgrade accelerate\u00a0 \u00a0 \u00a0 git clone https://github.com/huggingface/transformers.git\u00a0 \u00a0 \u00a0 cd transformers\u00a0 \u00a0 \u00a0 git checkout ebdb185befaa821304d461ed6aa20a17e4dc3aa2\u00a0 \u00a0 \u00a0 pip install .\u00a0 \u00a0 \u00a0 git log -1\u00a0 \u00a0 \u00a0 pip install datasets evaluate scikit-learn\u00a0 \u00a0 \u00a0 '\n```\n- Download configs of the pre-trained model.```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='\u00a0 \u00a0 \u00a0 gsutil cp -r gs://cloud-tpu-tpuvm-artifacts/config/xl-ml-test/pytorch/gpt2/my_config_2.json transformers/examples/pytorch/language-modeling/\u00a0 \u00a0 \u00a0 gsutil cp gs://cloud-tpu-tpuvm-artifacts/config/xl-ml-test/pytorch/gpt2/fsdp_config.json transformers/examples/pytorch/language-modeling/'\n```Train the 2B model using a batch size of 16.\n```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='\u00a0 \u00a0 \u00a0 export PJRT_DEVICE=TPU_C_API\u00a0 \u00a0 \u00a0 cd transformers/\u00a0 \u00a0 \u00a0 export LD_LIBRARY_PATH=/usr/local/lib/\u00a0 \u00a0 \u00a0 export PT_XLA_DEBUG=0\u00a0 \u00a0 \u00a0 export USE_TORCH=ON\u00a0 \u00a0 \u00a0 python3 examples/pytorch/xla_spawn.py \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--num_cores=4 \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0examples/pytorch/language-modeling/run_clm.py \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--num_train_epochs=3 \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--dataset_name=wikitext \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--dataset_config_name=wikitext-2-raw-v1 \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--per_device_train_batch_size=16 \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--per_device_eval_batch_size=16 \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--do_train \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--do_eval \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--logging_dir=./tensorboard-metrics \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--cache_dir=./cache_dir \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--output_dir=/tmp/test-clm \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--overwrite_output_dir \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--cache_dir=/tmp \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--config_name=examples/pytorch/language-modeling/my_config_2.json \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--tokenizer_name=gpt2 \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--block_size=1024 \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--optim=adafactor \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--adafactor=true \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--save_strategy=no \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--logging_strategy=no \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--fsdp=full_shard \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0--fsdp_config=examples/pytorch/language-modeling/fsdp_config.json'\n```\nDelete your TPU and queued-resource at the end of your session.\n```\ngcloud alpha compute tpus tpu-vm delete ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quietgcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quiet\n```\nThe training script ran on v5litepod-4, v5litepod-16, and v5litepod-64. The following table shows the benchmark throughputs for different accelerator types.\n| 0       | 1   | 2   | 3   |\n|:--------------------------|:------------|:-------------|:-------------|\n| nan      | v5litepod-4 | v5litepod-16 | v5litepod-64 |\n| Epoch      | 3   | 3   | 3   |\n| config     | 600M  | 2B   | 16B   |\n| Global batch size   | 64   | 128   | 256   |\n| Throughput (examples/sec) | 66   | 77   | 31   |\n### Train ViT on v5e\nThis tutorial will cover how to run VIT on v5e using the HuggingFace [repository](https://github.com/huggingface/transformers.git) on PyTorch/XLA on the [cifar10 dataset](https://huggingface.co/datasets/cifar10) .- Create environment variables:```\nexport PROJECT_ID=your_project_IDexport ACCELERATOR_TYPE=v5litepod-16export ZONE=us-west4-aexport RUNTIME_VERSION=v2-alpha-tpuv5-liteexport SERVICE_ACCOUNT=your_service_accountexport TPU_NAME=tpu-nameexport QUEUED_RESOURCE_ID=queued_resource_idexport QUOTA_TYPE=quota_typeexport VALID_UNTIL_DURATION=1d\n```\n- [Create a TPU resource:](/tpu/docs/queued-resources) ```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--node-id=${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--accelerator-type=${ACCELERATOR_TYPE} \\\u00a0 \u00a0--runtime-version=${RUNTIME_VERSION} \\\u00a0 \u00a0--valid-until-duration=${VALID_UNTIL_DURATION} \\\u00a0 \u00a0--service-account=${SERVICE_ACCOUNT} \\\u00a0 \u00a0--${QUOTA_TYPE}\n``` **Note:** The `QUOTA_TYPE` flag can be either `reserved` or `best-effort` . See [quotas](/tpu/docs/quota#quota_types) for information on the different types of quotas supported by Cloud TPU. **Note:** You will be able to SSH to your TPU VM once your QueuedResource is in the `ACTIVE` state:```\n\u00a0gcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE}\n```When the queued resource is in the `ACTIVE` state, the output will be similar to the following:```\n\u00a0state: ACTIVE\n```\n- Install torch/xla dependencies```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \u00a0 \u00a0--command='\u00a0 \u00a0 \u00a0 sudo apt-get update -y\u00a0 \u00a0 \u00a0 sudo apt-get install libomp5 -y\u00a0 \u00a0 \u00a0 pip3 install mkl mkl-include\u00a0 \u00a0 \u00a0 pip3 install tf-nightly tb-nightly tbp-nightly\u00a0 \u00a0 \u00a0 pip3 install numpy\u00a0 \u00a0 \u00a0 sudo apt-get install libopenblas-dev -y\u00a0 \u00a0 \u00a0 pip3 install torch~=2.1.0 torchvision torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html\u00a0 \u00a0 \u00a0 pip3 install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html'\n```\n- Download HuggingFace [repo](https://github.com/huggingface/transformers.git) and install requirements.```\n\u00a0 \u00a0gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command=\"\u00a0 \u00a0 \u00a0 git clone https://github.com/suexu1025/transformers.git vittransformers; \\\u00a0 \u00a0 \u00a0 cd vittransformers; \\\u00a0 \u00a0 \u00a0 pip3 install .; \\\u00a0 \u00a0 \u00a0 pip3 install datasets; \\\u00a0 \u00a0 \u00a0 wget https://github.com/pytorch/xla/blob/master/scripts/capture_profile.py\"\n``````\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--worker=all \\\u00a0 \u00a0--command='\u00a0 \u00a0 \u00a0 export PJRT_DEVICE=TPU_C_API\u00a0 \u00a0 \u00a0 export PT_XLA_DEBUG=0\u00a0 \u00a0 \u00a0 export USE_TORCH=ON\u00a0 \u00a0 \u00a0 export TF_CPP_MIN_LOG_LEVEL=0\u00a0 \u00a0 \u00a0 export XLA_USE_BF16=1\u00a0 \u00a0 \u00a0 export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH\u00a0 \u00a0 \u00a0 export TPU_LIBRARY_PATH=$HOME/.local/lib/python3.10/site-packages/libtpu/libtpu.so\u00a0 \u00a0 \u00a0 cd vittransformers\u00a0 \u00a0 \u00a0 python3 -u examples/pytorch/xla_spawn.py --num_cores 4 examples/pytorch/image-pretraining/run_mae.py --dataset_name=cifar10 \\\u00a0 \u00a0 \u00a0 --remove_unused_columns=False \\\u00a0 \u00a0 \u00a0 --label_names=pixel_values \\\u00a0 \u00a0 \u00a0 --mask_ratio=0.75 \\\u00a0 \u00a0 \u00a0 --norm_pix_loss=True \\\u00a0 \u00a0 \u00a0 --do_train=true \\\u00a0 \u00a0 \u00a0 --do_eval=true \\\u00a0 \u00a0 \u00a0 --base_learning_rate=1.5e-4 \\\u00a0 \u00a0 \u00a0 --lr_scheduler_type=cosine \\\u00a0 \u00a0 \u00a0 --weight_decay=0.05 \\\u00a0 \u00a0 \u00a0 --num_train_epochs=3 \\\u00a0 \u00a0 \u00a0 --warmup_ratio=0.05 \\\u00a0 \u00a0 \u00a0 --per_device_train_batch_size=8 \\\u00a0 \u00a0 \u00a0 --per_device_eval_batch_size=8 \\\u00a0 \u00a0 \u00a0 --logging_strategy=steps \\\u00a0 \u00a0 \u00a0 --logging_steps=30 \\\u00a0 \u00a0 \u00a0 --evaluation_strategy=epoch \\\u00a0 \u00a0 \u00a0 --save_strategy=epoch \\\u00a0 \u00a0 \u00a0 --load_best_model_at_end=True \\\u00a0 \u00a0 \u00a0 --save_total_limit=3 \\\u00a0 \u00a0 \u00a0 --seed=1337 \\\u00a0 \u00a0 \u00a0 --output_dir=MAE \\\u00a0 \u00a0 \u00a0 --overwrite_output_dir=true \\\u00a0 \u00a0 \u00a0 --logging_dir=./tensorboard-metrics \\\u00a0 \u00a0 \u00a0 --tpu_metrics_debug=true'\n```\nDelete your TPU and queued-resource at the end of your session.\n```\ngcloud alpha compute tpus tpu-vm delete ${TPU_NAME}\u00a0 \u00a0--project=${PROJECT_ID}\u00a0 \u00a0--zone=${ZONE}\u00a0 \u00a0--quietgcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID}\u00a0 \u00a0--project=${PROJECT_ID}\u00a0 \u00a0--zone=${ZONE}\u00a0 \u00a0--quiet\n```\nThe following table shows the benchmark throughputs for different accelerator types.\n| 0       | 1   | 2   | 3   |\n|:--------------------------|:------------|:-------------|:-------------|\n| nan      | v5litepod-4 | v5litepod-16 | v5litepod-64 |\n| Epoch      | 3   | 3   | 3   |\n| Global batch size   | 32   | 128   | 512   |\n| Throughput (examples/sec) | 201   | 657   | 2844   |\n## TensorFlow 2.x\n### Train Resnet on a single host v5e\nThis tutorial describes how to train ImageNet on `v5litepod-4` or `v5litepod-8` using a fake dataset. If you want to use a different dataset, refer to [Preparing the dataset](https://github.com/google/flax/blob/main/examples/imagenet/README.md#preparing-the-dataset) .- Create environment variables:```\nexport PROJECT_ID=your-project-IDexport ACCELERATOR_TYPE=v5litepod-4export ZONE=us-east1-cexport RUNTIME_VERSION=tpu-vm-tf-2.15.0-pjrtexport TPU_NAME=your-tpu-nameexport QUEUED_RESOURCE_ID=your-queued-resource-idexport QUOTA_TYPE=quota-type\n````ACCELERATOR_TYPE` can be either `v5litepod-4` or `v5litepod-8` . /\n- [Create a TPU resource:](/tpu/docs/queued-resources) ```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--node-id=${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--accelerator-type=${ACCELERATOR_TYPE} \\\u00a0 \u00a0--runtime-version=${RUNTIME_VERSION} \\\u00a0 \u00a0--${QUOTA_TYPE}\n``` **Note:** You will be able to SSH to your TPU VM once your queued resource is in the `ACTIVE` state. To check the state of your queued resource, use the following command:```\ngcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE}\n```\n- Connect to your TPU using SSH```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \u00a0\\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE}\n```\n- Set some environment variables```\nexport MODELS_REPO=/usr/share/tpu/modelsexport PYTHONPATH=\"${MODELS_REPO}:${PYTHONPATH}\"export MODEL_DIR=gcp-directory-to-store-modelexport DATA_DIR=gs://cloud-tpu-test-datasets/fake_imagenetexport NEXT_PLUGGABLE_DEVICE_USE_C_API=trueexport TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so\n```\n- Change to the models repository directory and install requirements.```\ncd ${MODELS_REPO} && git checkout r2.15.0pip install -r official/requirements.txt\n```Run the training script.\n```\npython3 official/vision/train.py \\\u00a0 \u00a0--tpu=local \\\u00a0 \u00a0--experiment=resnet_imagenet \\\u00a0 \u00a0--mode=train_and_eval \\\u00a0 \u00a0--config_file=official/vision/configs/experiments/image_classification/imagenet_resnet50_tpu.yaml \\\u00a0 \u00a0--model_dir=${MODEL_DIR} \\\u00a0 \u00a0--params_override=\"runtime.distribution_strategy=tpu,task.train_data.input_path=${DATA_DIR}/train*,task.validation_data.input_path=${DATA_DIR}/validation*,task.train_data.global_batch_size=2048,task.validation_data.global_batch_size=2048,trainer.train_steps=100\"\n```- Delete your TPU```\ngcloud alpha compute tpus tpu-vm delete ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quiet\n```\n- Delete your queued resource request```\ngcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quiet\n```\n### Train Resnet on a multi host v5e\nThis tutorial describes how to train ImageNet on `v5litepod-16` or larger using a fake dataset. If you want to use a different dataset, see [Preparing the dataset](https://github.com/google/flax/blob/main/examples/imagenet/README.md#preparing-the-dataset) .\n- Create environment variables:```\nexport PROJECT_ID=your_project_IDexport ACCELERATOR_TYPE=v5litepod-16export ZONE=us-east1-cexport RUNTIME_VERSION=tpu-vm-tf-2.15.0-pod-pjrtexport TPU_NAME=your_tpu_nameexport QUEUED_RESOURCE_ID=your-queued-resource-idexport QUOTA_TYPE=quota-type\n````ACCELERATOR_TYPE` can be either `v5litepod-16` or larger.\n- [Create a TPU resource:](/tpu/docs/queued-resources) ```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--node-id=${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--accelerator-type=${ACCELERATOR_TYPE} \\\u00a0 \u00a0--runtime-version=${RUNTIME_VERSION} \\\u00a0 \u00a0--${QUOTA_TYPE}\n``` **Note:** You will be able to SSH to your TPU VM once your queued resource is in the `ACTIVE` state. To check the state of your queued resource, use the following command:```\ngcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE}\n```\n- Connect to your TPU (worker zero) using SSH```\ngcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} \u00a0\\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE}\n```\n- Set some environment variables```\nexport MODELS_REPO=/usr/share/tpu/modelsexport PYTHONPATH=\"${MODELS_REPO}:${PYTHONPATH}\"export MODEL_DIR=gcp-directory-to-store-modelexport DATA_DIR=gs://cloud-tpu-test-datasets/fake_imagenetexport TPU_LOAD_LIBRARY=0export TPU_NAME=your_tpu_name\n```\n- Change to the models repository directory and install requirements.```\n\u00a0cd $MODELS_REPO && git checkout r2.15.0\u00a0pip install -r official/requirements.txt\n```Run the training script.\n```\npython3 official/vision/train.py \\\u00a0 \u00a0--tpu=${TPU_NAME} \\\u00a0 \u00a0--experiment=resnet_imagenet \\\u00a0 \u00a0--mode=train_and_eval \\\u00a0 \u00a0--model_dir=${MODEL_DIR} \\\u00a0 \u00a0--params_override=\"runtime.distribution_strategy=tpu,task.train_data.input_path=${DATA_DIR}/train*, task.validation_data.input_path=${DATA_DIR}/validation*\"\n```- Delete your TPU```\ngcloud alpha compute tpus tpu-vm delete ${TPU_NAME} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quiet\n```\n- Delete your queued resource request```\ngcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \\\u00a0 \u00a0--project=${PROJECT_ID} \\\u00a0 \u00a0--zone=${ZONE} \\\u00a0 \u00a0--quiet\n```", "guide": "Cloud TPU"}