{"title": "Cloud TPU - Advanced Guide to Inception v3", "url": "https://cloud.google.com/tpu/docs/inception-v3-advanced", "abstract": "# Cloud TPU - Advanced Guide to Inception v3\n# Advanced Guide to Inception v3\nThis document discusses aspects of the [Inception](https://github.com/tensorflow/tpu/tree/master/models/experimental/inception) model and how they come together to make the model run efficiently on Cloud TPU. It is an advanced view of the guide to running Inception v3 on Cloud TPU. Specific changes to the model that led to significant improvements are discussed in more detail. This document supplements the [Inception v3 tutorial](/tpu/docs/tutorials/inception) .\nInception v3 TPU training runs match accuracy curves produced by GPU jobs of similar configuration. The model has been successfully trained on v2-8, v2-128, and v2-512 configurations. The model has attained greater than 78.1% accuracy in about 170 epochs.\nThe code samples shown in this document are meant to be illustrative, a high-level picture of what happens in the actual implementation. Working code can be found on [GitHub](https://github.com/tensorflow/tpu/tree/master/models/experimental/inception) .\n", "content": "## Introduction\nInception v3 is an image recognition model that has been shown to attain greater than 78.1% accuracy on the ImageNet dataset. The model is the culmination of many ideas developed by multiple researchers over the years. It is based on the original paper: [\"Rethinking the Inception Architecture for Computer Vision\"](https://arxiv.org/abs/1512.00567) by Szegedy, et. al.\nThe model itself is made up of symmetric and asymmetric building blocks, including convolutions, average pooling, max pooling, concatenations, dropouts, and fully connected layers. Batch normalization is used extensively throughout the model and applied to activation inputs. Loss is computed using Softmax.\nA high-level diagram of the model is shown in the following screenshot:\n## Estimator API\n**Warning:** TPUEstimator is only available in TensorFlow 1.x. If you are writing a model with TensorFlow 2.x, use [Keras](https://keras.io/about/) instead.\nThe TPU version of Inception v3 is written using [TPUEstimator](https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimator) , an API designed to facilitate development, so that you can focus on the models themselves rather than on the details of the underlying hardware. The API does most of the low-level grunge work necessary for running models on TPUs behind the scenes, while automating common functions, such as saving and restoring checkpoints.\nThe Estimator API enforces separation of model and input portions of the code. You define `model_fn` and `input_fn` functions, corresponding to the model definition and input pipeline. The following code shows the declaration of these functions:\n```\ndef model_fn(features, labels, mode, params):\u00a0 \u00a0 \u00a0\u2026\u00a0 return tpu_estimator.TPUEstimatorSpec(mode=mode, loss=loss, train_op=train_op)def input_fn(params):\u00a0 \u00a0 def parser(serialized_example):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u2026\u00a0 \u00a0 \u00a0 \u00a0 return image, label\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u2026\u00a0 \u00a0images, labels = dataset.make_one_shot_iterator().get_next()\u00a0 \u00a0return images, labels\n```\nTwo key functions provided by the API are `train()` and `evaluate()` used to train and evaluate as shown in the following code:\n```\ndef main(unused_argv):\u00a0 \u2026\u00a0 run_config = tpu_config.RunConfig(\u00a0 \u00a0 \u00a0 master=FLAGS.master,\u00a0 \u00a0 \u00a0 model_dir=FLAGS.model_dir,\u00a0 \u00a0 \u00a0 session_config=tf.ConfigProto(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 allow_soft_placement=True, log_device_placement=True),\u00a0 \u00a0 \u00a0 tpu_config=tpu_config.TPUConfig(FLAGS.iterations, FLAGS.num_shards),)\u00a0 estimator = tpu_estimator.TPUEstimator(\u00a0 \u00a0 \u00a0 model_fn=model_fn,\u00a0 \u00a0 \u00a0 use_tpu=FLAGS.use_tpu,\u00a0 \u00a0 \u00a0 train_batch_size=FLAGS.batch_size,\u00a0 \u00a0 \u00a0 eval_batch_size=FLAGS.batch_size,\u00a0 \u00a0 \u00a0 config=run_config)\u00a0 estimator.train(input_fn=input_fn, max_steps=FLAGS.train_steps)\u00a0 eval_results = inception_classifier.evaluate(\u00a0 \u00a0 \u00a0 input_fn=imagenet_eval.input_fn, steps=eval_steps)\n```\n## ImageNet dataset\nBefore the model can be used to recognize images, it must be trained using a large set of labeled images. [ImageNet](http://www.image-net.org) is a common dataset to use.\nImageNet has over ten million URLs of labeled images. One million of the images also have bounding boxes specifying a more precise location for the labeled objects.\nFor this model, the ImageNet dataset is composed of 1,331,167 images which are split into training and evaluation datasets containing 1,281,167 and 50,000 images, respectively.\nThe training and evaluation datasets are kept separate intentionally. Only images from the training dataset are used to train the model and only images from the evaluation dataset are used to evaluate model accuracy.\nThe model expects images to be stored as TFRecords. For more information about how to convert images from raw JPEG files into TFRecords, see [download_and_preprocess_imagenet.sh](/tpu/docs/imagenet-setup) .\n## Input pipeline\nEach Cloud TPU device has 8 cores and is connected to a host (CPU). Larger slices have multiple hosts. Other larger configurations interact with multiple hosts. For example a v2-256 communicates with 16 hosts.\nHosts retrieve data from the file system or local memory, do whatever data preprocessing is required, and then transfer preprocessed data to the TPU cores. We consider these three phases of data handling done by the host individually and refer to the phases as: 1) , 2) , 3) . A high-level picture of the diagram is shown in the following figure:\nTo yield good performance, the system should be balanced. If the host CPU takes longer than the TPU to complete the three data handling phases, then execution will be host-bound. Both cases are shown in the following diagram:\nThe current implementation of Inception v3 is at the edge of being input-bound. Images are retrieved from the file system, decoded, and then preprocessed. Different types of preprocessing stages are available, ranging from moderate to complex. If we use the most complex of preprocessing stages, the training pipeline will be preprocessing bound. You can attain accuracy greater than 78.1% using a moderately complex preprocessing stage that keeps the model TPU-bound.\nThe model uses [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to handle input pipeline processing. For more information about how to optimize input pipelines, see the [datasets performance guide](https://www.tensorflow.org/guide/data_performance) .\nAlthough you can define a function and pass it to the Estimator API, the class `InputPipeline` encapsulates all required features.\nThe Estimator API makes it straightforward to use this class. You pass it to the `input_fn` parameter of functions `train()` and `evaluate()` , as shown in the following code snippet:\n```\ndef main(unused_argv):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u2026\u00a0 inception_classifier = tpu_estimator.TPUEstimator(\u00a0 \u00a0 \u00a0 model_fn=inception_model_fn,\u00a0 \u00a0 \u00a0 use_tpu=FLAGS.use_tpu,\u00a0 \u00a0 \u00a0 config=run_config,\u00a0 \u00a0 \u00a0 params=params,\u00a0 \u00a0 \u00a0 train_batch_size=FLAGS.train_batch_size,\u00a0 \u00a0 \u00a0 eval_batch_size=eval_batch_size,\u00a0 \u00a0 \u00a0 batch_axis=(batch_axis, 0))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u2026\u00a0 for cycle in range(FLAGS.train_steps // FLAGS.train_steps_per_eval):\u00a0 \u00a0 tf.logging.info('Starting training cycle %d.' % cycle)\u00a0 \u00a0 inception_classifier.train(\u00a0 \u00a0 \u00a0 \u00a0 input_fn=InputPipeline(True), steps=FLAGS.train_steps_per_eval)\u00a0 \u00a0 tf.logging.info('Starting evaluation cycle %d .' % cycle)\u00a0 \u00a0 eval_results = inception_classifier.evaluate(\u00a0 \u00a0 \u00a0 \u00a0 input_fn=InputPipeline(False), steps=eval_steps, hooks=eval_hooks)\u00a0 \u00a0 tf.logging.info('Evaluation results: %s' % eval_results)\n```\nThe main elements of `InputPipeline` are shown in the following code snippet.\n```\nclass InputPipeline(object):\u00a0 def __init__(self, is_training):\u00a0 \u00a0 self.is_training = is_training\u00a0 def __call__(self, params):\u00a0 \u00a0 # Storage\u00a0 \u00a0 file_pattern = os.path.join(\u00a0 \u00a0 \u00a0 \u00a0 FLAGS.data_dir, 'train-*' if self.is_training else 'validation-*')\u00a0 \u00a0 dataset = tf.data.Dataset.list_files(file_pattern)\u00a0 \u00a0 if self.is_training and FLAGS.initial_shuffle_buffer_size > 0:\u00a0 \u00a0 \u00a0 dataset = dataset.shuffle(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 buffer_size=FLAGS.initial_shuffle_buffer_size)\u00a0 \u00a0 if self.is_training:\u00a0 \u00a0 \u00a0 dataset = dataset.repeat()\u00a0 \u00a0 def prefetch_dataset(filename):\u00a0 \u00a0 \u00a0 dataset = tf.data.TFRecordDataset(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 filename, buffer_size=FLAGS.prefetch_dataset_buffer_size)\u00a0 \u00a0 \u00a0 return dataset\u00a0 \u00a0 dataset = dataset.apply(\u00a0 \u00a0 \u00a0 \u00a0 tf.contrib.data.parallel_interleave(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 prefetch_dataset,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cycle_length=FLAGS.num_files_infeed,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sloppy=True))\u00a0 \u00a0 if FLAGS.followup_shuffle_buffer_size > 0:\u00a0 \u00a0 \u00a0 dataset = dataset.shuffle(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 buffer_size=FLAGS.followup_shuffle_buffer_size)\u00a0 \u00a0 # Preprocessing\u00a0 \u00a0 dataset = dataset.map(\u00a0 \u00a0 \u00a0 \u00a0 self.dataset_parser,\u00a0 \u00a0 \u00a0 \u00a0 num_parallel_calls=FLAGS.num_parallel_calls)\u00a0 \u00a0 dataset = dataset.prefetch(batch_size)\u00a0 \u00a0 dataset = dataset.apply(\u00a0 \u00a0 \u00a0 \u00a0 tf.contrib.data.batch_and_drop_remainder(batch_size))\u00a0 \u00a0 dataset = dataset.prefetch(2) \u00a0# Prefetch overlaps in-feed with training\u00a0 \u00a0 images, labels = dataset.make_one_shot_iterator().get_next()\u00a0 \u00a0 # Transfer\u00a0 \u00a0 return images, labels\n```\nThe **storage** section begins with the creation of a dataset and includes the reading of TFRecords from storage (using `tf.data.TFRecordDataset` ). Special purpose functions `repeat()` and `shuffle()` are used as needed. Function `tf.contrib.data.parallel_interleave()` maps function `prefetch_dataset()` across its input to produce nested datasets, and outputs their elements interleaved. It gets elements from `cycle_length` nested datasets in parallel, which increases throughput. The `sloppy` argument relaxes the requirement that the outputs be produced in a deterministic order, and allows the implementation to skip over nested datasets whose elements are not readily available when requested.\nThe **preprocessing** section calls `dataset.map(parser)` which in turn calls the parser function where images are preprocessed. The details of the preprocessing stage are discussed in the next section.\nThe **transfer** section (at the end of the function) includes the line `return images, labels` . TPUEstimator takes the returned values and automatically transfers them to the device.\nThe following figure shows a sample Cloud TPU performance trace of Inception v3. TPU compute time, ignoring any in-feeding stalls, is approximately 815 msecs.\nHost **storage** is written to the trace and shown in the following screenshot:\nHost **preprocessing** , which includes image decoding and a series of image distortion functions is shown in the following screenshot:\nHost/TPU **transfer** is shown in the following screenshot:\n## Preprocessing Stage\nImage preprocessing is a crucial part of the system and can influence the maximum accuracy that the model attains during training. At a minimum, images need to be decoded and resized to fit the model. For Inception, images need to be 299x299x3 pixels.\nHowever, simply decoding and resizing are not enough to get good accuracy. The ImageNet training dataset contains 1,281,167 images. One pass over the set of training images is referred to as an epoch. During training, the model requires several passes through the training dataset to improve its image recognition capabilities. To train Inception v3 to sufficient accuracy, use between 140 and 200 epochs depending on the global batch size.\nIt is useful to continuously alter the images before feeding them to the model so that a particular image is slightly different at every epoch. How to best do this preprocessing of images is as much art as it is science. A well-designed preprocessing stage can significantly boost the recognition capabilities of a model. Too simple a preprocessing stage may create an artificial ceiling on the accuracy that the same model can attain during training.\nInception v3 offers options for the preprocessing stage, ranging from relatively simple and computationally inexpensive to fairly complex and computationally expensive. Two distinct flavors of such can be found in files **vgg_preprocessing.py** and **inception_preprocessing.py** .\nFile **vgg_preprocessing.py** defines a preprocessing stage that has been used successfully to train `resnet` to 75% accuracy, but yields suboptimal results when applied on Inception v3.\nFile **inception_preprocessing.py** contains a preprocessing stage that has been used to train Inception v3 with accuracies between 78.1 and 78.5% when run on TPUs.\nPreprocessing differs depending on whether the model is undergoing training or being used for inference/evaluation.\nAt evaluation time, preprocessing is straightforward: crop a central region of the image and then resize it to the default 299x299 size. The following code snippet shows a preprocessing implementation:\n```\ndef preprocess_for_eval(image, height, width, central_fraction=0.875):\u00a0 with tf.name_scope(scope, 'eval_image', [image, height, width]):\u00a0 \u00a0 if image.dtype != tf.float32:\u00a0 \u00a0 \u00a0 image = tf.image.convert_image_dtype(image, dtype=tf.float32)\u00a0 \u00a0 image = tf.image.central_crop(image, central_fraction=central_fraction)\u00a0 \u00a0 image = tf.expand_dims(image, 0)\u00a0 \u00a0 image = tf.image.resize_bilinear(image, [height, width], align_corners=False)\u00a0 \u00a0 image = tf.squeeze(image, [0])\u00a0 \u00a0 image = tf.subtract(image, 0.5)\u00a0 \u00a0 image = tf.multiply(image, 2.0)\u00a0 \u00a0 image.set_shape([height, width, 3])\u00a0 \u00a0 return image\n```\nDuring training, the cropping is randomized: a bounding box is chosen randomly to select a region of the image which is then resized. The resized image is then optionally flipped and its colors are distorted. The following code snippet shows an implementation of these operations:\n```\ndef preprocess_for_train(image, height, width, bbox, fast_mode=True, scope=None):\u00a0 with tf.name_scope(scope, 'distort_image', [image, height, width, bbox]):\u00a0 \u00a0 if bbox is None:\u00a0 \u00a0 \u00a0 bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\u00a0 \u00a0 if image.dtype != tf.float32:\u00a0 \u00a0 \u00a0 image = tf.image.convert_image_dtype(image, dtype=tf.float32)\u00a0 \u00a0 distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\u00a0 \u00a0 distorted_image.set_shape([None, None, 3])\u00a0 \u00a0 num_resize_cases = 1 if fast_mode else 4\u00a0 \u00a0 distorted_image = apply_with_random_selector(\u00a0 \u00a0 \u00a0 \u00a0 distorted_image,\u00a0 \u00a0 \u00a0 \u00a0 lambda x, method: tf.image.resize_images(x, [height, width], method),\u00a0 \u00a0 \u00a0 \u00a0 num_cases=num_resize_cases)\u00a0 \u00a0 distorted_image = tf.image.random_flip_left_right(distorted_image)\u00a0 \u00a0 if FLAGS.use_fast_color_distort:\u00a0 \u00a0 \u00a0 distorted_image = distort_color_fast(distorted_image)\u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 num_distort_cases = 1 if fast_mode else 4\u00a0 \u00a0 \u00a0 distorted_image = apply_with_random_selector(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 distorted_image,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 lambda x, ordering: distort_color(x, ordering, fast_mode),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 num_cases=num_distort_cases)\u00a0 \u00a0 distorted_image = tf.subtract(distorted_image, 0.5)\u00a0 \u00a0 distorted_image = tf.multiply(distorted_image, 2.0)\u00a0 \u00a0 return distorted_image\n```\nFunction `distort_color` is in charge of color alteration. It offers a fast mode where only brightness and saturation are modified. The full mode modifies brightness, saturation, and hue, in a random order.\n```\ndef distort_color(image, color_ordering=0, fast_mode=True, scope=None):\u00a0 with tf.name_scope(scope, 'distort_color', [image]):\u00a0 \u00a0 if fast_mode:\u00a0 \u00a0 \u00a0 if color_ordering == 0:\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_brightness(image, max_delta=32. / 255.)\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\u00a0 \u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_brightness(image, max_delta=32. / 255.)\u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 if color_ordering == 0:\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_brightness(image, max_delta=32. / 255.)\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_hue(image, max_delta=0.2)\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\u00a0 \u00a0 \u00a0 elif color_ordering == 1:\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_brightness(image, max_delta=32. / 255.)\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_hue(image, max_delta=0.2)\u00a0 \u00a0 \u00a0 elif color_ordering == 2:\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_hue(image, max_delta=0.2)\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_brightness(image, max_delta=32. / 255.)\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\u00a0 \u00a0 \u00a0 elif color_ordering == 3:\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_hue(image, max_delta=0.2)\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\u00a0 \u00a0 \u00a0 \u00a0 image = tf.image.random_brightness(image, max_delta=32. / 255.)\u00a0 \u00a0 return tf.clip_by_value(image, 0.0, 1.0)\n```\nFunction `distort_color` is computationally expensive, partly due to the nonlinear RGB to HSV and HSV to RGB conversions that are required in order to access hue and saturation. Both fast and full modes require these conversions and although fast mode is less computationally expensive, it still pushes the model to the CPU-compute-bound region, when enabled.\nAs an alternative, a new function `distort_color_fast` has been added to the list of options. This function maps the image from RGB to YCrCb using the [JPEG conversion](https://en.wikipedia.org/wiki/YCbCr#JPEG_conversion) scheme and randomly alters brightness and the Cr/Cb chromas before mapping back to RGB. The following code snippet shows an implementation of this function:\n```\ndef distort_color_fast(image, scope=None):\u00a0 with tf.name_scope(scope, 'distort_color', [image]):\u00a0 \u00a0 br_delta = random_ops.random_uniform([], -32./255., 32./255., seed=None)\u00a0 \u00a0 cb_factor = random_ops.random_uniform(\u00a0 \u00a0 \u00a0 \u00a0 [], -FLAGS.cb_distortion_range, FLAGS.cb_distortion_range, seed=None)\u00a0 \u00a0 cr_factor = random_ops.random_uniform(\u00a0 \u00a0 \u00a0 \u00a0 [], -FLAGS.cr_distortion_range, FLAGS.cr_distortion_range, seed=None)\u00a0 \u00a0 channels = tf.split(axis=2, num_or_size_splits=3, value=image)\u00a0 \u00a0 red_offset = 1.402 * cr_factor + br_delta\u00a0 \u00a0 green_offset = -0.344136 * cb_factor - 0.714136 * cr_factor + br_delta\u00a0 \u00a0 blue_offset = 1.772 * cb_factor + br_delta\u00a0 \u00a0 channels[0] += red_offset\u00a0 \u00a0 channels[1] += green_offset\u00a0 \u00a0 channels[2] += blue_offset\u00a0 \u00a0 image = tf.concat(axis=2, values=channels)\u00a0 \u00a0 image = tf.clip_by_value(image, 0., 1.)\u00a0 \u00a0 return image\n```\nHere's a sample image that has undergone preprocessing. A randomly chosen region of the image has been selected and colors altered using the `distort_color_fast` function.\nFunction `distort_color_fast` is computationally efficient and still allows training to be TPU execution time bound. In addition, it has been used to train the Inception v3 model to an accuracy greater than 78.1% using batch sizes in the 1,024-16,384 range.\n## Optimizer\nThe current model showcases three flavors of optimizers: SGD, momentum, and RMSProp.\n`Stochastic gradient descent (SGD)` is the simplest update: the weights are nudged in the negative gradient direction. Despite its simplicity, good results can still be obtained on some models. The updates dynamics can be written as:\nw k + 1 = w k \u2212 \u03b1 \u2207 f ( w k )\nMomentum is a popular optimizer that frequently leads to faster convergence than SGD. This optimizer updates weights much like SGD but also adds a component in the direction of the previous update. The following equations describe the updates performed by the momentum optimizer:\nz k + 1 = \u03b2 z k + \u2207 f ( w k )\nw k + 1 = w k \u2212 \u03b1 z k + 1\nwhich can be written as:\nw k + 1 = w k \u2212 \u03b1 \u2207 f ( w k ) + \u03b2 ( w k \u2212 w k \u2212 1 )\nThe last term is the component in the direction of the previous update.\nFor the momentum \u03b2 , we use the value of 0.9.\nRMSprop is a popular optimizer first proposed by Geoff Hinton in one of his [lectures](http://www.cs.toronto.edu/%7Etijmen/csc321/slides/lecture_slides_lec6.pdf) . The following equations describe how the optimizer works:\ng \u2212 2 k + 1 = \u03b1 g \u2212 2 k + ( 1 \u2212 \u03b1 ) g 2 kw k + 1 = \u03b2 w k + \u03b7 \u221a g \u2212 2 k + 1 + \u03f5 \u2207 f ( w k )\nFor Inception v3, tests show RMSProp giving the best results in terms of maximum accuracy and time to attain it, with momentum a close second. Thus RMSprop is set as the default optimizer. The parameters used are: decay \u03b1 = 0.9, momentum \u03b2 = 0.9, and \u03f5 = 1.0.\nThe following code snippet shows how to set these parameters:\n```\nif FLAGS.optimizer == 'sgd':\u00a0 tf.logging.info('Using SGD optimizer')\u00a0 optimizer = tf.train.GradientDescentOptimizer(\u00a0 \u00a0 \u00a0 learning_rate=learning_rate)elif FLAGS.optimizer == 'momentum':\u00a0 tf.logging.info('Using Momentum optimizer')\u00a0 optimizer = tf.train.MomentumOptimizer(\u00a0 \u00a0 \u00a0 learning_rate=learning_rate, momentum=0.9)elif FLAGS.optimizer == 'RMS':\u00a0 tf.logging.info('Using RMS optimizer')\u00a0 optimizer = tf.train.RMSPropOptimizer(\u00a0 \u00a0 \u00a0 learning_rate,\u00a0 \u00a0 \u00a0 RMSPROP_DECAY,\u00a0 \u00a0 \u00a0 momentum=RMSPROP_MOMENTUM,\u00a0 \u00a0 \u00a0 epsilon=RMSPROP_EPSILON)else:\u00a0 tf.logging.fatal('Unknown optimizer:', FLAGS.optimizer)\n```\nWhen running on TPUs and using the Estimator API, the optimizer needs to be wrapped in a `CrossShardOptimizer` function in order to ensure synchronization among the replicas (along with any necessary cross communication). The following code snippet shows how the Inception v3 model wraps the optimizer:\n```\nif FLAGS.use_tpu:\u00a0 \u00a0 optimizer = tpu_optimizer.CrossShardOptimizer(optimizer)update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)with tf.control_dependencies(update_ops):\u00a0 train_op = optimizer.minimize(loss, global_step=global_step)\n```\n## Exponential moving average\nWhile training, the trainable parameters are updated during backpropagation according to the optimizer's update rules. The equations describing these rules were discussed in the previous section and repeated here for convenience:\n\u03b8 k + 1 = \u03b8 k \u2212 \u03b1 \u2207 f ( \u03b8 k ) ( S G D )\n\u03b8 k + 1 = \u03b8 k \u2212 \u03b1 z k + 1 ( m o m e n t u m )\n\u03b8 k + 1 = \u03b2 \u03b8 k + \u03b7 \u221a g k + 1 + \u03f5 \u2212 2 \u2207 f ( \u03b8 k ) ( R M S p r o p )\n[Exponential movingaverage](https://en.wikipedia.org/wiki/Exponential_smoothing) (also known as exponential smoothing) is an optional post-processing step that is applied to the updated weights and can sometimes lead to noticeable improvements in performance. TensorFlow provides the function [tf.train.ExponentialMovingAverage](https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage) that computes the ema \u02c6 \u03b8 of weight \u03b8 using the formula:\n^ \u03b8 t = \u03b1 \u02c6 \u03b8 t \u2212 1 + ( 1 \u2212 \u03b1 ) \u03b8 t\nwhere \u03b1 is a decay factor (close to 1.0). In the Inception v3 model, \u03b1 is set to 0.995.\nEven though this calculation is an Infinite Impulse Response (IIR) filter, the decay factor establishes an effective window where most of the energy (or relevant samples) resides, as shown in the following diagram:\nWe can rewrite the filter equation, as follows:\n\u02c6 \u03b8 t + T + 1 = \u03b1 ( 1 \u2212 \u03b1 ) ( \u03b8 t + T + \u03b1 \u03b8 t + T \u2212 1 + . . . + \u03b1 t + T \u03b8 0 )\nwhere we used \u02c6 \u03b8 \u2212 1 = 0 .\nThe \u03b1 k values decay with increasing , so only a subset of the samples will have a sizable influence on \u02c6 \u03b8 t + T + 1 . The rule of thumb for the decay factor value is: 1 1 \u2212 \u03b1 , which corresponds to \u03b1 = 200 for =0.995.\nWe first get a collection of trainable variables and then use the `apply()` method to create shadow variables for each trained variable. The following code snippet shows the Inception v3 model implementation:\n```\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)with tf.control_dependencies(update_ops):\u00a0 train_op = optimizer.minimize(loss, global_step=global_step)if FLAGS.moving_average:\u00a0 ema = tf.train.ExponentialMovingAverage(\u00a0 \u00a0 \u00a0 decay=MOVING_AVERAGE_DECAY, num_updates=global_step)\u00a0 variables_to_average = (tf.trainable_variables() +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tf.moving_average_variables())\u00a0 with tf.control_dependencies([train_op]), tf.name_scope('moving_average'):\u00a0 \u00a0 train_op = ema.apply(variables_to_average)\n```\nWe'd like to use the ema variables during evaluation. We define class `LoadEMAHook` that applies method `variables_to_restore()` to the checkpoint file to evaluate using the shadow variable names:\n```\nclass LoadEMAHook(tf.train.SessionRunHook):\u00a0 def __init__(self, model_dir):\u00a0 \u00a0 super(LoadEMAHook, self).__init__()\u00a0 \u00a0 self._model_dir = model_dir\u00a0 def begin(self):\u00a0 \u00a0 ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\u00a0 \u00a0 variables_to_restore = ema.variables_to_restore()\u00a0 \u00a0 self._load_ema = tf.contrib.framework.assign_from_checkpoint_fn(\u00a0 \u00a0 \u00a0 \u00a0 tf.train.latest_checkpoint(self._model_dir), variables_to_restore)\u00a0 def after_create_session(self, sess, coord):\u00a0 \u00a0 tf.logging.info('Reloading EMA...')\u00a0 \u00a0 self._load_ema(sess)\n```\nThe `hooks` function is passed to `evaluate()` as shown in the following code snippet:\n```\nif FLAGS.moving_average:\u00a0 \u00a0 eval_hooks = [LoadEMAHook(FLAGS.model_dir)]else:\u00a0 \u00a0 eval_hooks = []\u00a0 \u00a0 \u2026eval_results = inception_classifier.evaluate(\u00a0 \u00a0 input_fn=InputPipeline(False), steps=eval_steps, hooks=eval_hooks)\n```\n## Batch normalization\n[Batch normalization](https://arxiv.org/abs/1502.03167) is a widely used technique for normalizing input features on models that can lead to substantial reduction in convergence time. It is one of the more popular and useful algorithmic improvements in machine learning of recent years and is used across a wide range of models, including Inception v3.\nActivation inputs are normalized by subtracting the mean and dividing by the standard deviation. To keep things balanced in the presence of back propagation, two trainable parameters are introduced in every layer. Normalized outputs \u02c6 x undergo a subsequent operation \u03b3 \u02c6 x + \u03b2 , where \u03b3 and \u03b2 are a sort of standard deviation and mean learned by the model itself.\nThe full set of equations is in the [paper](https://arxiv.org/abs/1502.03167) and is repeated here for convenience:\n**Input** : Values of x over a mini-batch: \u03a6 = { x 1.. m } Parameters to be learned: \u03b3 , \u03b2\n**Output** : { y i = B N \u03b3 , \u03b2 ( x i ) }\n\u03bc \u03d5 \u2190 1 m m \u2211 i = 1 x i ( m i n i \u2212 b a t c h m e a n )\n\u03c3 \u03d5 2 \u2190 1 m m \u2211 i = 1 ( x i \u2212 \u03bc \u03d5 ) 2 ( m i n i \u2212 b a t c h v a r i a n c e )\n^ x i \u2190 x i \u2212 \u03bc \u03d5 \u221a \u03c3 2 \u03d5 + \u03f5 ( n o r m a l i z e )\ny i \u2190 \u03b3 ^ x i + \u03b2 \u2261 B N \u03b3 , \u03b2 ( x i ) ( s c a l e a n d s h i f t )\nNormalization happens during training, but come evaluation time, we'd like the model to behave in a deterministic fashion: the classification result of an image should depend solely on the input image and not the set of images that are being fed to the model. Thus, we need to fix \u03bc and \u03c3 2 and use values that represent the image population statistics.\nThe model computes moving averages of the mean and variance over the minibatches:\n\u02c6 \u03bc i = \u03b1 \u02c6 \u03bc t \u2212 1 + ( 1 \u2212 \u03b1 ) \u03bc t\n\u02c6 \u03c3 t 2 = \u03b1 \u02c6 \u03c3 2 t \u2212 1 + ( 1 \u2212 \u03b1 ) \u03c3 t 2\nIn the specific case of Inception v3, a sensible decay factor had been obtained (using hyperparameter tuning) for use in GPUs. We would like to use this value on TPUs as well, but in order to do that, we need to make some adjustments.\nBatch normalization moving mean and variance are both calculated using a loss pass filter, as shown in the following equation (here, y t represents moving mean or variance):\ny t = \u03b1 y t \u2212 1 + ( 1 \u2212 \u03b1 ) x t\n(1)\nIn an 8x1 GPU (synchronous) job, each replica reads the current moving mean and updates it. The current replica must write the new moving variable before the next replica can read it.\nWhen there are 8 replicas, the set of operations for an ensemble update is as follows:\ny t = \u03b1 y t \u2212 1 + ( 1 \u2212 \u03b1 ) x t\ny t + 1 = \u03b1 y t + ( 1 \u2212 \u03b1 ) x t + 1\ny t + 2 = \u03b1 y t + 1 + ( 1 \u2212 \u03b1 ) x t + 2\ny t + 3 = \u03b1 y t + 2 + ( 1 \u2212 \u03b1 ) x t + 3\ny t + 4 = \u03b1 y t + 3 + ( 1 \u2212 \u03b1 ) x t + 4\ny t + 5 = \u03b1 y t + 4 + ( 1 \u2212 \u03b1 ) x t + 5\ny t + 6 = \u03b1 y t + 5 + ( 1 \u2212 \u03b1 ) x t + 6\ny t + 7 = \u03b1 y t + 6 + ( 1 \u2212 \u03b1 ) x t + 7\nThis set of 8 sequential updates can be written as:\ny t + 7 = \u03b1 8 y t \u2212 1 + ( 1 \u2212 \u03b1 ) 7 \u2211 k = 0 \u03b1 7 \u2212 k x t + k\n(2)\nIn the current moving moment calculation implementation on TPUs, each shard performs calculations independently and there is no cross-shard communication. Batches are distributed to every shard and each of them processes 1/8th of the total number of batches (when there are 8 shards).\nAlthough each shard computes the moving moments (that is, mean and variance), only the results from shard 0 are communicated back to the host CPU. So, effectively, only one replica is doing the moving mean/variance update:\nz t = \u03b2 z t \u2212 1 + ( 1 \u2212 \u03b2 ) u t\n(3)\nand this update happens at 1/8th the rate of its sequential counterpart. In order to compare GPU and TPU update equations, we need to align the respective timescales. Specifically, the set of operations that make up a set of 8 sequential updates on the GPU should be compared against a single update on the TPU as illustrated in the following diagram:\nLet us show the equations with the modified time indexes:\ny t = \u03b1 8 y t \u2212 1 + ( 1 \u2212 \u03b1 ) 7 \u2211 k = 0 \u03b1 7 \u2212 k x t \u2212 k / 8 ( G P U )\nz t = \u03b2 z t \u2212 1 + ( 1 \u2212 \u03b2 ) u t ( T P U )\nIf we make the assumption that 8 mini batches (normalized across all relevant dimensions) yield similar values within the GPU 8-minibatch sequential update, then we can approximate these equations as follows:\ny t = \u03b1 8 y t \u2212 1 + ( 1 \u2212 \u03b1 ) 7 \u2211 k = 0 \u03b1 7 \u2212 k ^ x t = \u03b1 8 y t \u2212 1 + ( 1 \u2212 \u03b1 8 ) ^ x t ( G P U )\nz t = \u03b2 z t \u2212 1 + ( 1 \u2212 \u03b2 ) u t ( T P U )\nTo match the effect of a given decay factor on the GPU, we modify the decay factor on the TPU accordingly. Specifically, we set \u03b2 = \u03b1 8 .\nFor Inception v3, the decay value used in the GPU is \u03b1 =0.9997, which translates to a TPU decay value of \u03b2 =0.9976.\n## Learning rate adaptation\nAs batch sizes become larger, training becomes more difficult. Different techniques continue to be proposed to allow efficient training for large batch sizes (see [here](https://arxiv.org/abs/1709.05011) , [here](https://arxiv.org/abs/1706.02677) , and [here](https://arxiv.org/abs/1711.04325) , for example).\nOne of these techniques is increasing the learning rate gradually (also called ramp-up). Ramp-up was used to train the model to greater than 78.1% accuracy for batch sizes ranging from 4,096 to 16,384. For Inception v3, the learning rate is first set to about 10% of what would normally be the starting learning rate. The learning rate remains constant at this low value for a specified (small) number of 'cold epochs', and then begins a linear increase for a specified number of 'warm-up epochs'. At the end the 'warm-up epochs', the learning rate intersects with the normal exponential decay learning. This is illustrated in the following diagram.\nThe following code snippet shows how to do this:\n```\ninitial_learning_rate = FLAGS.learning_rate * FLAGS.train_batch_size / 256if FLAGS.use_learning_rate_warmup:\u00a0 warmup_decay = FLAGS.learning_rate_decay**(\u00a0 \u00a0 (FLAGS.warmup_epochs + FLAGS.cold_epochs) /\u00a0 \u00a0 FLAGS.learning_rate_decay_epochs)\u00a0 adj_initial_learning_rate = initial_learning_rate * warmup_decayfinal_learning_rate = 0.0001 * initial_learning_ratetrain_op = Noneif training_active:\u00a0 batches_per_epoch = _NUM_TRAIN_IMAGES / FLAGS.train_batch_size\u00a0 global_step = tf.train.get_or_create_global_step()\u00a0 current_epoch = tf.cast(\u00a0 \u00a0 (tf.cast(global_step, tf.float32) / batches_per_epoch), tf.int32)\u00a0 learning_rate = tf.train.exponential_decay(\u00a0 \u00a0 learning_rate=initial_learning_rate,\u00a0 \u00a0 global_step=global_step,\u00a0 \u00a0 decay_steps=int(FLAGS.learning_rate_decay_epochs * batches_per_epoch),\u00a0 \u00a0 decay_rate=FLAGS.learning_rate_decay,\u00a0 \u00a0 staircase=True)\u00a0 if FLAGS.use_learning_rate_warmup:\u00a0 \u00a0 wlr = 0.1 * adj_initial_learning_rate\u00a0 \u00a0 wlr_height = tf.cast(\u00a0 \u00a0 \u00a0 0.9 * adj_initial_learning_rate /\u00a0 \u00a0 \u00a0 (FLAGS.warmup_epochs + FLAGS.learning_rate_decay_epochs - 1),\u00a0 \u00a0 \u00a0 tf.float32)\u00a0 \u00a0 epoch_offset = tf.cast(FLAGS.cold_epochs - 1, tf.int32)\u00a0 \u00a0 exp_decay_start = (FLAGS.warmup_epochs + FLAGS.cold_epochs +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0FLAGS.learning_rate_decay_epochs)\u00a0 \u00a0 lin_inc_lr = tf.add(\u00a0 \u00a0 \u00a0 wlr, tf.multiply(\u00a0 \u00a0 \u00a0 \u00a0 tf.cast(tf.subtract(current_epoch, epoch_offset), tf.float32),\u00a0 \u00a0 \u00a0 \u00a0 wlr_height))\u00a0 \u00a0 learning_rate = tf.where(\u00a0 \u00a0 \u00a0 tf.greater_equal(current_epoch, FLAGS.cold_epochs),\u00a0 \u00a0 \u00a0 (tf.where(tf.greater_equal(current_epoch, exp_decay_start),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 learning_rate, lin_inc_lr)),\u00a0 \u00a0 \u00a0 \u00a0wlr)\u00a0 # Set a minimum boundary for the learning rate.\u00a0 learning_rate = tf.maximum(\u00a0 \u00a0 \u00a0 learning_rate, final_learning_rate, name='learning_rate')\n```", "guide": "Cloud TPU"}