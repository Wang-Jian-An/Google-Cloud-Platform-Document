{"title": "Cloud TPU - TPU configurations", "url": "https://cloud.google.com/tpu/docs/supported-tpu-configurations", "abstract": "# Cloud TPU - TPU configurations\n# TPU configurations\n", "content": "## v5p TPU configurations\nA TPU v5p Pod is composed of 8960 chips interconnected with reconfigurable high-speed links. TPU v5p's flexible networking lets you connect the chips in a same-sized slice in multiple ways. When you create a TPU slice using the `gcloud compute tpus tpu-vm create` command, you specify its type and shape using the [AcceleratorType](#using-accelerator-type) or [AcceleratorConfig](#using-accelerator-config) parameters.\nThe following table shows the most common single-slice shapes supported with v5p, plus most (but not all) full cube shapes greater than 1 cube. The maximum v5p shape is 16x16x24 (6144 chips, 96 cubes).\n| 0   | 1   | 2  | 3  | 4    | 5   | 6     |\n|:------------|:----------|:--------|:--------|:--------------|:-----------|:------------------|\n| Slice Shape | VM Size | # Cores | # Chips | # of Machines | # of Cubes | Supports Twisted? |\n| 2x2x1  | Full host | 8  | 4  | 1    | nan  | nan    |\n| 2x2x2  | Full host | 16  | 8  | 2    | nan  | nan    |\n| 2x4x4  | Full host | 64  | 32  | 8    | nan  | nan    |\n| 4x4x4  | Full host | 128  | 64  | 16   | 1   | nan    |\n| 4x4x8  | Full host | 256  | 128  | 32   | 2   | Yes    |\n| 4x8x8  | Full host | 512  | 256  | 64   | 4   | Yes    |\n| 8x8x8  | Full host | 1024 | 512  | 128   | 8   | nan    |\n| 8x8x16  | Full host | 2048 | 1024 | 256   | 16   | Yes    |\n| 8x16x16  | Full host | 4096 | 2048 | 512   | 32   | Yes    |\n| 16x16x16 | Full host | 8192 | 4096 | 1024   | 64   | nan    |\n| 16x16x24 | Full host | 12288 | 6144 | 1536   | 96   | nan    |\nSingle slice training is supported for up to 6144 chips. It is extensible to 18432 chips using Multislice. See the [Cloud TPU Multislice Overview](/tpu/docs/multislice-introduction) for Multislice details.\n### Using the AcceleratorType parameter\nWhen you allocate TPU resources, you use the `--accelerator-type` argument to specify the number of TensorCores in a slice. `--accelerator-type` is a formatted string \" **v** `$VERSION_NUMBER` **p** `-$CORES_COUNT` \". For example, `v5p-32` specifies a v5p TPU slice with 32 TensorCores (16 chips).\nTo provision TPUs for a v5p training job, use one of the following accelerator types in your CLI or TPU API creation request:\n- v5p-8\n- v5p-16\n- v5p-32\n- v5p-64\n- v5p-128 (one full cube/rack)\n- v5p-256 (2 cubes)\n- v5p-512\n- v5p-1024 ... v5p-12288\n### Using the AcceleratorConfig parameter\nFor v5p and later Cloud TPU versions, [AcceleratorConfig](/tpu/docs/supported-tpu-configurations#using-accelerator-config) is used in much the same way it is with Cloud TPU [v4](#v4-using-accelerator-config) The difference is that instead of specifying the TPU type as `--type=v4` , you specify it as the TPU version you are using (for example, `--type=v5p` for the v5p release).\n### Cloud TPU ICI resiliency\nICI resiliency helps improve fault tolerance of optical links and optical circuit switches (OCS) that connect TPUs between cubes. (ICI connections within a cube use copper links that are not impacted). ICI resiliency allows ICI connections to be routed around OCS and optical ICI faults. As a result, it improves the scheduling availability of TPU slices, with the trade-off of temporary degradation in ICI performance.\nSimilar to Cloud TPU v4, ICI resiliency is enabled by default for v5p slices that are one cube or larger:\n- v5p-128 when specifying acclerator type\n- 4x4x4 when specifying accelerator config\n### VM, host and slice properties\n| 0      | 1                      |\n|:----------------------|:----------------------------------------------------------------------------------------|\n| Property    | Value in a TPU                   |\n| # of v5p chips  | 4                      |\n| # of vCPUs   | 208 (only half is usable if using NUMA binding to avoid cross-NUMA performance penalty) |\n| RAM (GB)    | 448 (only half is usable if using NUMA binding to avoid cross-NUMA performance penalty) |\n| # of NUMA Nodes  | 2                      |\n| NIC Throughput (Gbps) | 200                      |\nRelationship between the number of TensorCores, chips, hosts/VMs, and cubes in a Pod:\n| 0      | 1  | 2  | 3   | 4  |\n|:------------------------|:------|:------|:----------|:------|\n| nan      | Cores | Chips | Hosts/VMs | Cubes |\n| Host     | 8  | 4  | 1   | nan |\n| Cube (aka rack)   | 128 | 64 | 16  | 1  |\n| Largest supported slice | 12288 | 6144 | 1536  | 96 |\n| v5p full Pod   | 17920 | 8960 | 2240  | 140 |\n## TPU v5e configurations\nCloud TPU v5e is a combined training and inference (serving) product. To differentiate between a training and an inference environment, use the `AcceleratorType` or `AcceleratorConfig` flags with the TPU API or the `--machine-type` flag [when creating a GKE nodepool](/kubernetes-engine/docs/how-to/tpus#create-node-pool) .\nTraining jobs are optimized for throughput and availability while serving jobs are optimized for latency. So, a training job on TPUs provisioned for serving could have lower availability and similarly, a serving job executed on TPUs provisioned for training could have higher latency.\nYou use `AcceleratorType` to specify the number of TensorCores you want to use. You specify the `AcceleratorType` when creating a TPU using the gcloud CLI or the [Google Cloud console](https://console.cloud.google.com/) . The value you specify for `AcceleratorType` is a string with the format: `v$VERSION_NUMBER-$CHIP_COUNT` .\nYou can also use `AcceleratorConfig` to specify the number of TensorCores you want to use. However, because there are no custom 2D topology variants for TPU v5e, there is no difference between using `AcceleratorConfig` and `AcceleratorType` .\nTo configure a TPU v5e using `AcceleratorConfig` , use the `--version` and the `--topology` flags. Set `--version` to the TPU version you want to use and `--topology` to the physical arrangement of the TPU chips in the slice. The value you specify for `AcceleratorConfig` is a string with the format `AxB` , where `A` and `B` are the chip counts in each direction.\nThe following 2D slice shapes are supported for v5e:\n| 0  | 1     | 2    |\n|:---------|:--------------------|:----------------|\n| Topology | Number of TPU chips | Number of Hosts |\n| 1x1  | 1     | 1/8    |\n| 2x2  | 4     | 1/2    |\n| 2x4  | 8     | 1    |\n| 4x4  | 16     | 2    |\n| 4x8  | 32     | 4    |\n| 8x8  | 64     | 8    |\n| 8x16  | 128     | 16    |\n| 16x16 | 256     | 32    |\nEach TPU VM in a v5e TPU slice contains 1, 4 or 8 chips. In 4-chip and smaller slices, all TPU chips share the same Non Uniform Memory Access (NUMA) node.\nFor 8-chip v5e TPU VMs, CPU-TPU communication will be more efficient within NUMA partitions. For example, in the following figure, `CPU0-Chip0` communication will be faster than `CPU0-Chip4` communication.### Cloud TPU v5e types for serving\nSingle-host serving is supported for up to 8 v5e chips. The following configurations are supported: 1x1, 2x2 and 2x4 slices. Each slice has 1, 4 and 8 chips respectively.\nTo provision TPUs for a serving job, use one of the following accelerator types in your CLI or API TPU creation request:\n| AcceleratorType (TPU API) | Machine type (GKE API) |\n|:----------------------------|:-------------------------|\n| v5litepod-1     | ct5lp-hightpu-1t   |\n| v5litepod-4     | ct5lp-hightpu-4t   |\n| v5litepod-8     | ct5lp-hightpu-8t   |\nServing on more than 8 v5e chips, also called multi-host serving, is supported using [Sax](https://github.com/google/saxml) . For more information, see [Large Language Model Serving](/tpu/docs/v5e-inference#large_language_model_serving) .\n### Cloud TPU v5e types for training\nTraining is supported for up to 256 chips.\nTo provision TPUs for a v5e training job, use one of the following accelerator types in your CLI or API TPU creation request:\n| AcceleratorType (TPU API) | Machine type (GKE API) | Topology |\n|:----------------------------|:-------------------------|:-----------|\n| v5litepod-16    | ct5lp-hightpu-4t   | 4x4  |\n| v5litepod-32    | ct5lp-hightpu-4t   | 4x8  |\n| v5litepod-64    | ct5lp-hightpu-4t   | 8x8  |\n| v5litepod-128    | ct5lp-hightpu-4t   | 8x16  |\n| v5litepod-256    | ct5lp-hightpu-4t   | 16x16  |\n### v5e TPU VM type comparison:\n| 0    | 1     | 2      | 3      |\n|:----------------|:---------------------|:-----------------------|:-----------------------|\n| VM Type   | n2d-48-24-v5lite-tpu | n2d-192-112-v5lite-tpu | n2d-384-224-v5lite-tpu |\n| # of v5e chips | 1     | 4      | 8      |\n| # of vCPUs  | 24     | 112     | 224     |\n| RAM (GB)  | 48     | 192     | 384     |\n| # of NUMA Nodes | 1     | 1      | 2      |\n| Applies to  | v5litepod-1   | v5litepod-4   | v5litepod-8   |\n| Disruption  | High     | Medium     | Low     |\nTo make space for workloads that require more chips, schedulers may preempt VMs with fewer chips. So 8-chip VMs are likely to preempt 1 and 4-chip VMs.\n## TPU v4 configurations\nA TPU v4 Pod is composed of 4096 chips interconnected with reconfigurable high-speed links. TPU v4's flexible networking lets you connect the chips in a same-sized Pod slice in multiple ways. When you create a TPU Pod slice, you specify the TPU version and the number of TPU resources you require. When you create a TPU v4 Pod slice, you can specify its type and size in one of two ways: `AcceleratorType` and `AcceleratorConfig` .\n**Note:** `AcceleratorType` and `AcceleratorConfig` are mutually exclusive; you can only use one when creating a TPU.\n### Using AcceleratorType\nUse AcceleratorType when you are not specifying a topology. To configure v4 TPUs using `AcceleratorType` , use the `--accelerator-type` flag when creating your TPU Pod slice. Set `--accelerator-type` to a string that contains the TPU version and the number of TensorCores you want to use. For example, to create a v4 Pod slice with 32 TensorCores, you would use `--accelerator-type=v4-32` .\nThe following command creates a v4 TPU Pod slice with 512 TensorCores using the `--accelerator-type` flag:\n```\n\u00a0 $ gcloud compute tpus tpu-vm create tpu-name\u00a0 \u00a0 --zone=zone\u00a0 \u00a0 --accelerator-type=v4-512\u00a0 \u00a0 --version=tpu-vm-tf-2.16.1-pod-pjrt\n```\nThe number after the TPU version ( `v4` ) specifies the number of TensorCores. There are two TensorCores in a v4 TPU, so the number of TPU chips would be 512/2 = 256.\n### Using AcceleratorConfig\nUse `AcceleratorConfig` when you want to customize the [physical topology](/tpu/docs/types-topologies#v4-variants) of your TPU slice. This is generally required for performance tuning with Pod slices greater than 256 chips.\nTo configure v4 TPUs using `AcceleratorConfig` , use the `--version` and the `--topology` flags. Set `--version` to the TPU version you want to use and `--topology` to the physical arrangement of the TPU chips in the Pod slice.\nYou specify a TPU topology using a 3-tuple, AxBxC where A<=B<=C and A, B, C are either all <= 4 or are all integer multiples of 4. The values A, B, and C are the chip counts in each of the three dimensions. For example, to create a v4 Pod slice with 16 chips, you would set `--version=v4` and `--topology=2x2x4` .\nThe following command creates a v4 TPU Pod slice with 128 TPU chips arranged in a 4x4x8 array:\n```\n\u00a0 $ gcloud compute tpus tpu-vm create tpu-name\u00a0 \u00a0 --zone=zone\u00a0 \u00a0 --type=v4\u00a0 \u00a0 --topology=4x4x8\u00a0 \u00a0 --version=tpu-vm-tf-2.16.1-pod-pjrt\n```\nTopologies where 2A=B=C or 2A=2B=C also have [topology variants](#v4-variants) optimized for all-to-all communication, for example, 4\u00d74\u00d78, 8\u00d78\u00d716, and 12\u00d712\u00d724. These are known as topologies.\nThe following illustrations show some common TPU v4 topologies.\nLarger Pod slices can be built from one or more 4x4x4 \"cubes\" of chips.\nSome v4 3D torus slice shapes have the option to use what is known as a topology. For example two v4 cubes can be arranged as a 4x4x8 slice or 4x4x8_twisted. Twisted topologies offer significantly higher bisection bandwidth. Increased bisection bandwidth is useful for workloads that use global communication patterns. Twisted topologies can improve performance for most models, with large [TPU embedding workloads](https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/embedding/TPUEmbedding) benefiting the most.\nFor workloads that use data parallelism as the only parallelism strategy, twisted topologies might perform slightly better. For LLMs, performance using a twisted topology can vary depending on the type of parallelism (DP, MP, etc.). Best practice is to train your LLM with and without a twisted topology to determine which provides the best performance for your model. Some experiments on the [FSDP MaxText model](https://github.com/google/maxtext#getting-started) have seen 1-2 MFU improvements using a twisted topology.\nThe primary benefit of twisted topologies is that it transforms an asymmetric torus topology (for example, 4\u00d74\u00d78) into a closely related symmetric topology. The symmetric topology has many benefits:\n- Improved load balancing\n- Higher bisection bandwidth\n- Shorter packet routes\nThese benefits ultimately translate into improved performance for many global communication patterns.\nThe TPU software supports twisted tori on slices where the size of each dimension is either equal to or twice the size of the smallest dimension. For example, 4x4x8, 4\u00d78\u00d78, or 12x12x24.\nAs an example, consider this 4\u00d72 torus topology with TPUs labeled with their (X,Y) coordinates in the slice:\nThe edges in this topology graph are shown as undirected edges for clarity. In practice, each edge is a bidirectional connection between TPUs. We refer to the edges between one side of this grid and the opposite side as wrap-around edges, as noted in the diagram.\nBy twisting this topology, we end up a completely symmetric 4\u00d72 twisted torus topology:\nAll that has changed between this diagram and the previous one is the Y wrap-around edges. Instead of connecting to another TPU with the same X coordinate, they have been shifted to connect to the TPU with coordinate X+2 mod 4.\nThe same idea generalizes to different dimension sizes and different numbers of dimensions. The resulting network is symmetric, as long as each dimension is equal to or twice the size of the smallest dimension.\nThe following table shows the supported twisted topologies and a theoretical increase in bisection bandwidth with them versus untwisted topologies.\n| Twisted Topology | Theoretical increase in bisection bandwidth versus a non-twisted torus |\n|:-------------------|:--------------------------------------------------------------------------|\n| 4\u00d74\u00d78_twisted  | ~70%                  |\n| 8x8x16_twisted  | nan                  |\n| 12\u00d712\u00d724_twisted | nan                  |\n| 4\u00d78\u00d78_twisted  | ~40%                  |\n| 8\u00d716\u00d716_twisted | nan                  |\n**Note:** The twisted topology won't be used automatically unless the suffix `_twisted` is specified with `--topology` flag. For example, to specify a 4x4x8 twisted topology, you would specify the `topology` flag as: `4x4x8_twisted` .\n### TPU v4 Topology variants\nSome topologies containing the same number of chips can be arranged in different ways. For example, a TPU Pod slice with 512 chips (1024 TensorCores) can be configured using the following topologies: 4x4x32, 4x8x16, or 8x8x8. A TPU Pod slice with 2048 chips (4096 TensorCores) offers even more topology options: 4x4x128, 4x8x64, 4x16x32, and 8x16x16. A TPU Pod slice with 2048 chips (4096 TensorCores) offers even more topology options: 4x4x128, 4x8x64, 4x16x32, and 8x16x16.\nThe default topology associated with a given chip count is the one that's most similar to a cube (see [v4 Topology](/tpu/docs/system-architecture-tpu-vm#twisted-tori) ). This shape is likely the best choice for data-parallel ML training. Other topologies can be useful for workloads with multiple kinds of parallelism (for example, model and data parallelism, or spatial partitioning of a simulation). These workloads perform best if the topology is matched to the parallelism used. For example, placing 4-way model parallelism on the X dimension and 256-way data parallelism on the Y and Z dimensions matches a 4x16x16 topology.\nModels with multiple dimensions of parallelism perform best with their parallelism dimensions mapped to TPU topology dimensions. These are usually data+model parallel Large Language Models (LLMs). For example, for a TPU v4 Pod slice with topology 8x16x16, the TPU topology dimensions are 8, 16 and 16. It is more performant to use 8-way or 16-way model parallelism (mapped to one of the physical TPU topology dimensions). A 4-way model parallelism would be sub-optimal with this topology, since it's not aligned with any of the TPU topology dimensions, but it would be optimal with a 4x16x32 topology on the same number of chips.\nTPU v4 configurations consist of two groups, those with topologies smaller than 64 chips (small topologies), and those with topologies greater than 64 chips (large topologies).\nCloud TPU supports the following TPU v4 Pod slices smaller than 64 chips, a 4x4x4 cube. You can create these small v4 topologies using either their TensorCore-based name (for example, v4-32), or their topology (for example, 2x2x4):\n| 0        | 1    | 2  |\n|:---------------------------------|:----------------|:---------|\n| Name (based on TensorCore count) | Number of chips | Topology |\n| v4-8        | 4    | 2x2x1 |\n| v4-16       | 8    | 2x2x2 |\n| v4-32       | 16    | 2x2x4 |\n| v4-64       | 32    | 2x4x4 |\nTPU v4 Pod slices are available in increments of 64 chips, with shapes that are multiples of 4 on all three dimensions. The dimensions must also be in increasing order. Several examples are shown in the following table. A few of these topologies are \"custom\" topologies that can only be created using the `--type` and `--topology` flags because there is more than one way to arrange the chips.\nThe following command creates a v4 TPU Pod slice with 512 TPU chips arranged in a 8x8x8 array:\n```\n\u00a0 $ gcloud compute tpus tpu-vm create tpu-name\u00a0 \u00a0 --zone=zone\u00a0 \u00a0 --type=v4\u00a0 \u00a0 --topology=8x8x8\u00a0 \u00a0 --version=tpu-vm-tf-2.16.1-pod-pjrt\n```\nYou can create a v4 TPU Pod slice with the same number of TensorCores using `--accelerator-type` :\n```\n\u00a0 $ gcloud compute tpus tpu-vm create tpu-name\u00a0 \u00a0 --zone=zone\u00a0 \u00a0 --accelerator-type=v4-1024\u00a0 \u00a0 --version=tpu-vm-tf-2.16.1-pod-pjrt\n```\n| 0            | 1    | 2  |\n|:-----------------------------------------------|:----------------|:---------|\n| Name (based on TensorCore count)    | Number of chips | Topology |\n| v4-128           | 64    | 4x4x4 |\n| v4-256           | 128    | 4x4x8 |\n| v4-512           | 256    | 4x8x8 |\n| N/A - must use the --type and --topology flags | 256    | 4x4x16 |\n| v4-1024          | 512    | 8x8x8 |\n| v4-1536          | 768    | 8x8x12 |\n| v4-2048          | 1024   | 8x8x16 |\n| N/A - must use --type and --topology flags  | 1024   | 4x16x16 |\n| v4-4096          | 2048   | 8x16x16 |\n| \u2026            | \u2026    | \u2026  |\n## TPU v3 configurations\nA TPU v3 Pod is composed of 1024 chips interconnected with high-speed links. To create a TPU v3 device or Pod slice, use the `--accelerator-type` flag for the gcloud compute tpus tpu-vm command. You specify the accelerator type by specifying the TPU version and the number of TPU cores. For a single v3 TPU, use `--accelerator-type=v3-8` . For a v3 Pod slice with 128 TensorCores, use `--accelerator-type=v3-128` .\nThe following command shows how to create a v3 TPU Pod slice with 128 TensorCores:\n```\n\u00a0 $ gcloud compute tpus tpu-vm create tpu-name\u00a0 \u00a0 --zone=zone\u00a0 \u00a0 --accelerator-type=v3-128\u00a0 \u00a0 --version=tpu-vm-tf-2.16.1-pjrt\n```\nThe following table lists the supported v3 TPU types:\n| TPU version | Support ends   |\n|:--------------|:-----------------------|\n| v3-8   | (End date not yet set) |\n| v3-32   | (End date not yet set) |\n| v3-128  | (End date not yet set) |\n| v3-256  | (End date not yet set) |\n| v3-512  | (End date not yet set) |\n| v3-1024  | (End date not yet set) |\n| v3-2048  | (End date not yet set) |\nFor more information about managing TPUs, see [Manage TPUs](/tpu/docs/managing-tpus-tpu-vm) . For more information about the different versions of Cloud TPU, see [System architecture](/tpu/docs/system-architecture) .\n## TPU v2 configurations\nA TPU v2 Pod is composed of 512 chips interconnected with reconfigurable high-speed links. To create a TPU v2 Pod slice, use the `--accelerator-type` flag for the gcloud compute tpus tpu-vm command. You specify the accelerator type by specifying the TPU version and the number of TPU cores. For a single v2 TPU, use `--accelerator-type=v2-8` . For a v2 Pod slice with 128 TensorCores, use `--accelerator-type=v2-128` .\nThe following command shows how to create a v2 TPU Pod slice with 128 TensorCores:\n```\n\u00a0 $ gcloud compute tpus tpu-vm create tpu-name\u00a0 \u00a0 --zone=zone\u00a0 \u00a0 --accelerator-type=v2-128\u00a0 \u00a0 --version=tpu-vm-tf-2.16.1-pjrt\n```\nFor more information about managing TPUs, see [Manage TPUs](/tpu/docs/managing-tpus-tpu-vm) . For more information about the different versions of Cloud TPU, see [System architecture](/tpu/docs/system-architecture) .\nThe following table lists the supported v2 TPU types\n| TPU version | Support ends   |\n|:--------------|:-----------------------|\n| v2-8   | (End date not yet set) |\n| v2-32   | (End date not yet set) |\n| v2-128  | (End date not yet set) |\n| v2-256  | (End date not yet set) |\n| v2-512  | (End date not yet set) |\n## TPU type compatibility\nYou can change the TPU type to another TPU type that has the same number of TensorCores or chips (for example, `v3-128` and `v4-128` ) and run your training script without code changes. However, if you change to a TPU type with a larger or smaller number TensorCores, you will need to perform significant tuning and optimization. For more information, see [Training on TPU Pods](/tpu/docs/training-on-tpu-pods) .\n## TPU VM software versions\nThis section describes the TPU software versions you should use for a TPU with the [TPU VM](/tpu/docs/system-architecture-tpu-vm#tpu-vm-arch) architecture. For the [TPUNode](/tpu/docs/system-architecture-tpu-vm#tpu-node-arch) architecture, see [TPU Node software versions](#tpu-node-versions) .\nTPU software versions are available for TensorFlow, PyTorch, and JAX frameworks.\n### TensorFlow\nUse the TPU software version that matches the version of TensorFlow with which your model was written.\nBeginning with TensorFlow 2.15.0, you must also specify either the stream executor (SE) runtime or the PJRT runtime. For example, if you are using TensorFlow 2.16.1 with the PJRT runtime, use the `tpu-vm-tf-2.16.1-pjrt` TPU software version. Versions prior to TensorFlow 2.15.0 only support stream executor. For more information about PJRT, see [TensorFlow PJRT support](#tf-pjrt-support) .\nThe current supported TensorFlow TPU VM software versions are:\n- tpu-vm-tf-2.16.1-pjrt\n- tpu-vm-tf-2.16.1-se\n- tpu-vm-tf-2.15.0-pjrt\n- tpu-vm-tf-2.15.0-se\n- tpu-vm-tf-2.14.1\n- tpu-vm-tf-2.14.0\n- tpu-vm-tf-2.13.1\n- tpu-vm-tf-2.13.0\n- tpu-vm-tf-2.12.1\n- tpu-vm-tf-2.12.0\n- tpu-vm-tf-2.11.1\n- tpu-vm-tf-2.11.0\n- tpu-vm-tf-2.10.1\n- tpu-vm-tf-2.10.0\n- tpu-vm-tf-2.9.3\n- tpu-vm-tf-2.9.1\n- tpu-vm-tf-2.8.4\n- tpu-vm-tf-2.8.3\n- tpu-vm-tf-2.8.0\n- tpu-vm-tf-2.7.4\n- tpu-vm-tf-2.7.3\n**Note:** If you're using TensorFlow 2.10.0 or earlier on TPU v4, you must use [v4-specific TensorFlow versions](#tf-2-10-0-and-earlier-v4) .\n**Note:** If you are using a Pod slice, append `-pod` after the TensorFlow version number. For example, `tpu-vm-tf-2.16.1-pod-pjrt` .\nFor more information on TensorFlow patch versions, see [SupportedTensorFlow patch versions](/tpu/docs/supported-versions) .\nBeginning with TensorFlow 2.15.0, you can use the PJRT interface for TensorFlow on TPU. PJRT features automatic device memory defragmentation and simplifies the integration of hardware with frameworks. For more information about PJRT, see [PJRT: Simplifying ML Hardware and FrameworkIntegration](https://opensource.googleblog.com/2023/05/pjrt-simplifying-ml-hardware-and-framework-integration.html) on the Google Open Source Blog.\n**Important:** TPU v5e and future TPU generations only support PJRT. PJRT is only supported on the TPU VM architecture.\nNot all features of TPU v2, v3, and v4 have been migrated to the PJRT runtime. The following table describes which features are supported on PJRT or steam executor.\n| Accelerator | Feature          | Supported on PJRT        | Supported on stream executor |\n|:--------------|:-----------------------------------------------|:------------------------------------------------|:-------------------------------|\n| TPU v2-v4  | Dense compute (no TPU embedding API)   | Yes            | Yes       |\n| TPU v2-v4  | Dense compute API + TPU embedding API   | No            | Yes       |\n| TPU v2-v4  | tf.summary/tf.print with soft device placement | No            | Yes       |\n| TPU v5e  | Dense compute (no TPU embedding API)   | Yes            | No        |\n| TPU v5e  | TPU embedding API        | N/A - TPU v5e doesn't support TPU embedding API | nan       |\n| TPU v5p  | Dense compute (no TPU embedding API)   | Yes            | No        |\n| TPU v5p  | TPU embedding API        | Yes            | No        |\nIf you are training a model on TPU v4 with TensorFlow, TensorFlow versions 2.10.0 and earlier use `v4` -specific versions shown in the following table. If the TensorFlow version you're using is not shown in the table, follow the guidance in the [TensorFlow](#tensorflow-vm) section.\n| TensorFlow version | TPU software version       |\n|:---------------------|:---------------------------------------------|\n| 2.10.0    | tpu-vm-tf-2.10.0-v4, tpu-vm-tf-2.10.0-pod-v4 |\n| 2.9.3    | tpu-vm-tf-2.9.3-v4, tpu-vm-tf-2.9.3-pod-v4 |\n| 2.9.2    | tpu-vm-tf-2.9.2-v4, tpu-vm-tf-2.9.2-pod-v4 |\n| 2.9.1    | tpu-vm-tf-2.9.1-v4, tpu-vm-tf-2.9.1-pod-v4 |\nTPU VMs are created with TensorFlow and the corresponding Libtpu library preinstalled. If you are creating your own VM image, specify the following TensorFlow TPU software versions and corresponding `libtpu` versions:\n| TensorFlow version | libtpu.so version |\n|:---------------------|:--------------------|\n| 2.16.1    | 1.10.1    |\n| 2.15.0    | 1.9.0    |\n| 2.14.1    | 1.8.1    |\n| 2.14.0    | 1.8.0    |\n| 2.13.1    | 1.7.1    |\n| 2.13.0    | 1.7.0    |\n| 2.12.1    | 1.6.1    |\n| 2.12.0    | 1.6.0    |\n| 2.11.1    | 1.5.1    |\n| 2.11.0    | 1.5.0    |\n| 2.10.1    | 1.4.1    |\n| 2.10.0    | 1.4.0    |\n| 2.9.3    | 1.3.2    |\n| 2.9.1    | 1.3.0    |\n| 2.8.3    | 1.2.3    |\n| 2.8.*    | 1.2.0    |\n| 2.7.3    | 1.1.2    |\n### PyTorch\nUse the TPU software version that matches the version of PyTorch with which your model was written. For example, if you are using PyTorch 1.13 and TPU v2 or v3, use the `tpu-vm-pt-1.13` TPU software version. If you are using TPU v4 use the `tpu-vm-v4-pt-1.13` TPU software version. The same TPU software version is used for TPU Pods (for example, `v2-32` , `v3-128` , `v4-32` ). The current supported TPU software versions are:- tpu-vm-pt-2.0 (pytorch-2.0)\n- tpu-vm-pt-1.13 (pytorch-1.13)\n- tpu-vm-pt-1.12 (pytorch-1.12)\n- tpu-vm-pt-1.11 (pytorch-1.11)\n- tpu-vm-pt-1.10 (pytorch-1.10)\n- v2-alpha (pytorch-1.8.1)- tpu-vm-v4-pt-2.0 (pytorch-2.0)\n- tpu-vm-v4-pt-1.13 (pytorch-1.13)- v2-alpha-tpuv5 (pytorch-2.0)\nWhen you create a TPU VM, the latest version of PyTorch is preinstalled on the TPU VM. The correct version of libtpu.so is automatically installed when you install PyTorch.\nTo change the current PyTorch software version, see [Changing PyTorch version](/tpu/docs/pytorch-xla-ug-tpu-vm#changing_pytorch_version) .\n### JAX\nYou must manually install JAX on your TPU VM. There is no JAX-specific TPU software (runtime) version for TPU v2 and v3. For later TPU versions, use the following software versions:\n- TPU v4: tpu-vm-v4-base\n- TPU v5e: v2-alpha-tpuv5\n- TPU v5p: v2-alpha-tpuv5\nThe correct version of libtpu.so is automatically installed when you install JAX.\n## TPU Node software versions\nThis section describes the TPU software versions you should use for a TPU with the [TPU Node](/tpu/docs/system-architecture-tpu-vm#tpu-node-arch) architecture. For the [TPUVM](/tpu/docs/system-architecture-tpu-vm#tpu-vm-arch) architecture, see [TPU VM softwareversions](#tpu-vm-versions) .\nTPU software versions are available for TensorFlow, PyTorch, and JAX frameworks.\n### TensorFlow\nUse the TPU software version that matches the version of TensorFlow with which your model was written. For example, if you are using TensorFlow 2.12.0, use the `2.12.0` TPU software version. The TensorFlow specific TPU software versions are:\n- 2.12.1\n- 2.12.0\n- 2.11.1\n- 2.11.0\n- 2.10.1\n- 2.10.0\n- 2.9.3\n- 2.9.1\n- 2.8.4\n- 2.8.2\n- 2.7.3\nFor more information on TensorFlow patch versions, see [SupportedTensorFlow patch versions](/tpu/docs/supported-patches) .\nWhen you create a TPU Node, the latest version of TensorFlow is preinstalled on the TPU Node.\n### PyTorch\nUse the TPU software version that matches the version of PyTorch with which your model was written. For example, if you are using PyTorch 1.9, use the `pytorch-1.9` software version.\nThe PyTorch specific TPU software versions are:\n- pytorch-2.0\n- pytorch-1.13\n- pytorch-1.12\n- pytorch-1.11\n- pytorch-1.10\n- pytorch-1.9\n- pytorch-1.8\n- pytorch-1.7\n- pytorch-1.6\n- pytorch-nightly\nWhen you create a TPU Node, the latest version of PyTorch is preinstalled on the TPU Node.\n### JAX\nYou must manually install JAX on your TPU VM, so there is no pre-installed JAX-specific TPU software version. You can use any of the software versions listed for TensorFlow.\n## What's next\n- Learn more about TPU architecture in the [SystemArchitecture](/tpu/docs/system-architecture-tpu-vm) page.\n- See [When to use TPUs](/tpu/docs/tpus#when_to_use_tpus) to learn about the types of models that are well suited to Cloud TPU.", "guide": "Cloud TPU"}