{"title": "Cloud TPU - Run JAX code on TPU Pod slices", "url": "https://cloud.google.com/tpu/docs/jax-pods", "abstract": "# Cloud TPU - Run JAX code on TPU Pod slices\n# Run JAX code on TPU Pod slices\nAfter you have your JAX code running on a single TPU board, you can scale up your code by running it on a [TPU Pod slice](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#tpu_slices) . TPU Pod slices are multiple TPU boards connected to each other over dedicated high-speed network connections. This document is an introduction to running JAX code on TPU Pod slices; for more in-depth information, see [Using JAX in multi-host and multi-process environments](https://jax.readthedocs.io/en/latest/multi_process.html) .\n[Using an NFS for data storage](/tpu/docs/training-on-tpu-pods#nfs-data)\n", "content": "## Create a TPU Pod slice\nBefore running the commands in this document, make sure you have followed the instructions in [Set up an account and Cloud TPU project](https://cloud.google.com/tpu/docs/setup-gcp-account) . Run the following commands on your local machine.\nCreate a TPU Pod slice using the `gcloud` command. For example, to create a v4-32 Pod slice use the following command:\n```\n$ gcloud compute tpus tpu-vm create tpu-name \u00a0\\\u00a0 --zone=us-central2-b \\\u00a0 --accelerator-type=v4-32 \u00a0\\\u00a0 --version=tpu-ubuntu2204-base \n```\n## Install JAX on the Pod slice\nAfter creating the TPU Pod slice, you must install JAX on all hosts in the TPU Pod slice. You can install JAX on all hosts with a single command using the `--worker=all` option:\n```\n\u00a0 gcloud compute tpus tpu-vm ssh tpu-name \\\u00a0 --zone=us-central2-b --worker=all --command=\"pip install \\\u00a0 --upgrade 'jax[tpu]>0.3.0' \\\u00a0 -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\"\n```\n## Run JAX code on the Pod slice\nTo run JAX code on a TPU Pod slice, you must run the code **on each host in theTPU Pod slice** . The `jax.device_count()` call stops responding until it is called on each host in the Pod slice. The following example illustrates how to run a simple JAX calculation on a TPU Pod slice.\n### Prepare the code\nYou need `gcloud` version >= 344.0.0 (for the ` [scp](/sdk/gcloud/reference/alpha/compute/tpus/tpu-vm/scp) ` command). Use `gcloud --version` to check your `gcloud` version, and run `gcloud components upgrade` , if needed.\nCreate a file called `example.py` with the following code:\n```\n# The following code snippet will be run on all TPU hostsimport jax# The total number of TPU cores in the Poddevice_count = jax.device_count()# The number of TPU cores attached to this hostlocal_device_count = jax.local_device_count()# The psum is performed over all mapped devices across the Podxs = jax.numpy.ones(jax.local_device_count())r = jax.pmap(lambda x: jax.lax.psum(x, 'i'), axis_name='i')(xs)# Print from a single host to avoid duplicated outputif jax.process_index() == 0:\u00a0 \u00a0 print('global device count:', jax.device_count())\u00a0 \u00a0 print('local device count:', jax.local_device_count())\u00a0 \u00a0 print('pmap result:', r)\n```\n### Copy example.py to all TPU worker VMs in the Pod slice\n```\n$ gcloud compute tpus tpu-vm scp example.py tpu-name: \\\u00a0 --worker=all \\\u00a0 --zone=us-central2-b\n```\nIf you have not previously used the `scp` command, you might see an error similar to the following:\n```\nERROR: (gcloud.alpha.compute.tpus.tpu-vm.scp) SSH Key is not present in the SSH\nagent. Please run `ssh-add /.../.ssh/google_compute_engine` to add it, and try\nagain.\n```\nTo resolve the error, run the `ssh-add` command as displayed in the error message and rerun the command.\n### Run the code on the Pod slice\nLaunch the `example.py` program on every VM:\n```\n$ gcloud compute tpus tpu-vm ssh tpu-name \\\u00a0 --zone=us-central2-b \\\u00a0 --worker=all \\\u00a0 --command=\"python3 example.py\"\n```\n### Output (produced with a v4-32 Pod slice):\n```\nglobal device count: 16local device count: 4pmap result: [16. 16. 16. 16.]\n```\n## Clean up\nWhen you are done, you can release your TPU VM resources using the `gcloud` command:\n```\n$ gcloud compute tpus tpu-vm delete tpu-name \\\u00a0 --zone=us-central2-b\n```", "guide": "Cloud TPU"}