{"title": "Cloud TPU - Training transformer on Cloud TPU (TF 2.x)", "url": "https://cloud.google.com/tpu/docs/tutorials/transformer-2.x", "abstract": "# Cloud TPU - Training transformer on Cloud TPU (TF 2.x)\nIf you are not familiar with Cloud TPU, it is strongly recommended that you go through the [quickstart](https://cloud.google.com/tpu/docs/quickstart) to learn how to create a TPU VM.\nThis tutorial shows you how to train a Transformer model on Cloud TPU. Transformer is a neural network architecture that solves sequence to sequence problems using attention mechanisms. Unlike traditional neural seq2seq models, Transformer does not involve recurrent connections. The attention mechanism learns dependencies between tokens in two sequences. Since attention weights apply to all tokens in the sequences, the Transformer model is able to easily capture long-distance dependencies.\nTransformer's overall structure follows the standard encoder-decoder pattern. The encoder uses self-attention to compute a representation of the input sequence. The decoder generates the output sequence one token at a time, taking the encoder output and previous decoder-output tokens as inputs.\nThe model also applies embeddings on the input and output tokens, and adds a constant positional encoding. The positional encoding adds information about the position of each token.\n **Warning:** This tutorial uses a third-party dataset. Google provides no representation, warranty, or other guarantees about the validity, or any other aspects of this dataset.\n", "content": "## Objectives\n- Create a Cloud Storage bucket to hold your dataset and model output.\n- Download and pre process the dataset used to train the model.\n- Run the training job.\n- Verify the output results.\n## CostsIn this document, you use the following billable components of Google Cloud:- Compute Engine\n- Cloud TPU\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \n## Before you beginBefore starting this tutorial, check that your Google Cloud project is correctly set up.- This walkthrough uses billable components of Google Cloud. Check the [Cloud TPU pricing page](/tpu/docs/pricing) to  estimate your costs. Be sure to [clean up](#clean-up) resources you create when you've finished with them to avoid unnecessary  charges.\n## Train with a single Cloud TPU device **Note:** Do you to train the model using a Cloud TPU Pod? See [ TPU Pod training.](#pod-training) \nThis section provides information on setting up a Cloud Storage bucket and a TPU VM for single device training.\n **Important: ** Set up all resources (Cloud TPU VM and Cloud Storage bucket) in the same region/zone to reduce network latency and network costs. TPU VMs are located in [specific zones](/tpu/docs/types-zones#types) , which are subdivisions within a region.- Open a Cloud Shell window. [Open Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Create an environment variable for your project ID.```\nexport PROJECT_ID=project-id\n```\n- Configure Google Cloud CLI to use the your Google Cloud project where you want to create a Cloud TPU.```\ngcloud config set project ${PROJECT_ID}\n```The first time you run this command in a new Cloud Shell VM, an `Authorize Cloud Shell` page is displayed. Click `Authorize` at the bottom of the page to allow `gcloud` to make Google Cloud API calls with your credentials.\n- Create a Service Account for the Cloud TPU project.Service accounts allow the Cloud TPU service to access other Google Cloud services.```\n$ gcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID\n```The command returns a Cloud TPU Service Account with following format:```\nservice-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com\n```\n- Create a Cloud Storage bucket using the following command:```\n$ gsutil mb -p ${PROJECT_ID} -c standard -l us-central2 gs://bucket-name\n```This Cloud Storage bucket stores the data you use to train your model and the training results. The `gcloud` command used in this tutorial to set up the TPU also sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the [access level permissions](/tpu/docs/storage-buckets) .\n### Train the Transformer model on a single Cloud TPU\n- Launch a Cloud TPU VM using the `gcloud` command.```\n\u00a0 $ gcloud compute tpus tpu-vm create transformer-tutorial \\\u00a0 \u00a0 \u00a0--zone=us-central2-b \\\u00a0 \u00a0 \u00a0--accelerator-type=v4-8 \\\u00a0 \u00a0 \u00a0--version=tpu-vm-tf-2.16.1-pjrt\n``` **Note:** If you have more than one Google Cloud project, you must use the `--project` flag to specify the ID of the Google Cloud in which you want to create the Cloud TPU. **Note:** The first time you run `gcloud compute tpus` on a project it takes approximately 5 minutes to perform startup tasks such as SSH key propagation and API turnup.\n- Connect to the Cloud TPU VM by running the following `ssh` command. **Note:** When you are connected to the Cloud TPU VM, your shell prompt changes from `username@projectname` to `username@vm-name` :```\ngcloud compute tpus tpu-vm ssh transformer-tutorial --zone=us-central2-b\n``` **Key Point:** From this point on, a prefix of **(vm) $** means you should run the command on the Compute Engine VM instance.\n- Export environment variables.```\n\u00a0 (vm)$ export STORAGE_BUCKET=gs://bucket-name\u00a0 (vm)$ export SENTENCEPIECE_MODEL=sentencepiece\u00a0 (vm)$ export SENTENCEPIECE_MODEL_PATH=${STORAGE_BUCKET}/${SENTENCEPIECE_MODEL}.model\u00a0 (vm)$ export TFDS_DIR=${STORAGE_BUCKET}/tfds\u00a0 (vm)$ export PARAM_SET=big\u00a0 (vm)$ export TPU_NAME=local\u00a0 (vm)$ export MODEL_DIR=${STORAGE_BUCKET}/transformer/model_${PARAM_SET}\u00a0 (vm)$ \u00a0export PYTHONPATH=\"/usr/share/tpu/models:$PYTHONPATH\"\n```\n- When creating your TPU, if you set the `--version` parameter to a version ending with `-pjrt` , set the following environment variables to enable the PJRT runtime:```\n\u00a0 (vm)$ export NEXT_PLUGGABLE_DEVICE_USE_C_API=true\u00a0 (vm)$ export TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so\n```\n- Install Tensorflow requirements.```\n(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt\n```\n- Download and preprocess the dataset```\n\u00a0 (vm)$ python3 -c \"import tensorflow_datasets as tfds; tfds.load('wmt14_translate/de-en', split='train+validation', shuffle_files=True, download=True)\"\u00a0 (vm)$ python3 /usr/share/tpu/models/official/nlp/data/train_sentencepiece.py --output_model_path=${SENTENCEPIECE_MODEL}\n```\n- Copy the dataset to the Cloud Storage bucket```\n\u00a0 (vm)$ gsutil -m cp ${SENTENCEPIECE_MODEL}.model ${STORAGE_BUCKET}\u00a0 (vm)$ gsutil -m cp ${SENTENCEPIECE_MODEL}.vocab ${STORAGE_BUCKET}\u00a0 (vm)$ gsutil -m cp -r tensorflow_datasets/wmt14_translate ${TFDS_DIR}/wmt14_translate\n```\n- Navigate to the training directory```\n(vm)$ cd /usr/share/tpu/models/\n```\n- Run the training script```\n(vm)$ python3 official/nlp/train.py \\\u00a0 --tpu=${TPU_NAME} \\\u00a0 --experiment=wmt_transformer/large \\\u00a0 --mode=train_and_eval \\\u00a0 --model_dir=${MODEL_DIR} \\\u00a0 --params_override=\"runtime.distribution_strategy=tpu, task.train_data.tfds_data_dir=${TFDS_DIR}, task.validation_data.tfds_data_dir=${TFDS_DIR}, task.sentencepiece_model_path=${SENTENCEPIECE_MODEL_PATH}, trainer.train_steps=10000, trainer.validation_interval=10000\"\n```By default, the model will evaluate after every 10,000 steps. You can increase the number of training steps or specify how often to run evaluations by setting these parameters:- `train.train_steps`: The total number of training steps to run.\n- `trainer.validation_interval`: The number of training steps to run between evaluations.\nTraining and evaluation takes approximately 20 minutes on a v4-8 Cloud TPU. When the training and evaluation complete, a message similar to the following appears:```\nI0208 20:57:19.309512 140039467895872 controller.py:310] eval | step: 10000 | eval time: 69.2 sec | output: \n{'bleu_score': 19.204771518707275,\n 'sacrebleu_score': 18.307039308307356,\n 'validation_loss': 2.0654342}\n eval | step: 10000 | eval time: 69.2 sec | output: \n{'bleu_score': 19.204771518707275,\n 'sacrebleu_score': 18.307039308307356,\n 'validation_loss': 2.0654342}\n```You have now completed single-device training. Use the following steps to delete your single-device TPU resources.\n- Disconnect from the Compute Engine instance:```\n(vm)$ exit\n```Your prompt should now be `username@projectname` , showing you are in the Cloud Shell.\n- Delete the TPU resource.```\n\u00a0 $ gcloud compute tpus tpu-vm delete transformer-tutorial \\\u00a0 \u00a0 --zone=us-central2-b\u00a0 \n```\nAt this point, you can either conclude this tutorial and [clean up](#cleanup) , or you can continue and explore running the model on Cloud TPU Pods.## Scale your model with Cloud TPU PodsTraining your model on Cloud TPU Pods may require some changes to your training script. For information, see [Training on TPU Pods](/tpu/docs/training-on-tpu-pods) .\n### TPU Pod training **Important: ** If you have already set up a Cloud TPU project, Service Account, storage bucket, and dataset for single device training, you can skip to [launching TPU resources](#launch-tpuvm-resources) .\n **Important: ** Set up your Cloud TPU resources and your Cloud Storage bucket in the same region/zone to reduce network latency and network costs. Cloud TPUs are located in [specific zones](/tpu/docs/types-zones#types) , which are subdivisions within a region.- Open a Cloud Shell window. [Open Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Create a variable for your project ID.```\nexport PROJECT_ID=project-id\n```\n- Configure Google Cloud CLI to use the project where you want to create a Cloud TPU.```\ngcloud config set project ${PROJECT_ID}\n```The first time you run this command in a new Cloud Shell VM, an `Authorize Cloud Shell` page is displayed. Click `Authorize` at the bottom of the page to allow `gcloud` to make API calls with your credentials.\n- Create a Service Account for the Cloud TPU project.```\ngcloud beta services identity create --service tpu.googleapis.com --project $PROJECT_ID\n```The command returns a Cloud TPU Service Account with following format:```\nservice-PROJECT_NUMBER@cloud-tpu.iam.gserviceaccount.com\n```\n- Create a Cloud Storage bucket using the following command or use a bucket you created earlier for your project:```\ngsutil mb -p ${PROJECT_ID} -c standard -l us-central1 gs://bucket-name\n```This Cloud Storage bucket stores the data you use to train your model and the training results. The `gcloud` command used in this tutorial sets up default permissions for the Cloud TPU Service Account you set up in the previous step. If you want finer-grain permissions, review the [access level permissions](/tpu/docs/storage-buckets) . **Important:** If you created a new Cloud Storage bucket, you will need to download and preprocess the dataset. For more information, see [Prepare thedataset](#prepare-data) .\n### Launch the TPU VM resources\n- Launch a TPU VM Pod using the `gcloud` command. This tutorial specifies a v4-32 Pod. For other Pod options, see [TPU types](/tpu/docs/types-topologies)  [available TPU types page](/tpu/docs/regions-zones) . **Note:** If there is not enough capacity currently available to create the TPU Pod, you can queue your request using queued resources. Queued resources allow you to receive capacity once it becomes available. To request your Cloud TPU resources as queued resources, use the`gcloud alpha compute tpus queued-resources create`command instead. For more information, see [Manage Queued Resources](/tpu/docs/queued-resources) .```\n$ gcloud compute tpus tpu-vm create transformer-tutorial \\\u00a0 --zone=us-central2-b \\\u00a0 --accelerator-type=v3-32 \\\u00a0 --version=tpu-vm-tf-2.16.1-pod-pjrt\n```\n- Connect to the TPU VM by running the following `ssh` command. When you are logged into the VM, your shell prompt changes from `username@projectname` to `username@vm-name` :```\ngcloud compute tpus tpu-vm ssh transformer-tutorial --zone=us-central2-b\n```\n- Install TensorFlow requirements.```\n(vm)$ pip3 install -r /usr/share/tpu/models/official/requirements.txt\n```\n### Set up and start the Pod training\n- Export Cloud TPU setup variables:```\n(vm)$ export PYTHONPATH=\"/usr/share/tpu/models:$PYTHONPATH\"(vm)$ export STORAGE_BUCKET=gs://bucket-name(vm)$ export SENTENCEPIECE_MODEL=sentencepiece(vm)$ export SENTENCEPIECE_MODEL_PATH=${STORAGE_BUCKET}/${SENTENCEPIECE_MODEL}.model(vm)$ export TFDS_DIR=${STORAGE_BUCKET}/tfds(vm)$ export TPU_NAME=transformer-tutorial(vm)$ export PARAM_SET=big(vm)$ export MODEL_DIR=${STORAGE_BUCKET}/transformer/model_${PARAM_SET} (vm)$ export TPU_LOAD_LIBRARY=0\n```\n- Download the dataset **Note:** Skip this step if you have already downloaded the dataset.```\n(vm)$ python3 -c \"import tensorflow_datasets as tfds; tfds.load('wmt14_translate/de-en', split='train+validation', shuffle_files=True, download=True)\"(vm)$ python3 /usr/share/tpu/models/official/nlp/data/train_sentencepiece.py --output_model_path=${SENTENCEPIECE_MODEL}\n```\n- Copy the dataset to Cloud Storage bucket **Note:** Skip this step if you have already downloaded and copied the dataset to a Cloud Storage bucket.```\n(vm)$ gsutil -m cp ${SENTENCEPIECE_MODEL}.model ${STORAGE_BUCKET}(vm)$ gsutil -m cp ${SENTENCEPIECE_MODEL}.vocab ${STORAGE_BUCKET}(vm)$ gsutil -m cp -r tensorflow_datasets/wmt14_translate ${TFDS_DIR}/wmt14_translate\n```\n- Change to the training directory:```\n(vm)$ cd /usr/share/tpu/models/\n```\n- Run the training script:```\n(vm)$ python3 official/nlp/train.py \u00a0 --tpu=${TPU_NAME} \u00a0 --experiment=wmt_transformer/large \u00a0 --mode=train_and_eval \u00a0 --model_dir=${MODEL_DIR} \u00a0 --params_override=\"runtime.distribution_strategy=tpu, task.train_data.tfds_data_dir=${TFDS_DIR}, task.validation_data.tfds_data_dir=${TFDS_DIR}, task.sentencepiece_model_path=${SENTENCEPIECE_MODEL_PATH}, trainer.train_steps=10000, trainer.validation_interval=10000\"\u00a0 \n```\nBy default, the model will evaluate after every 10000 steps. In order to train to convergence, change `train_steps` to 200000. You can increase the number of training steps or specify how often to run evaluations by setting these parameters:- `trainer.train_steps`: Sets the total number of training steps to run.\n- `trainer.validation_interval`: Sets the number of training steps to run between evaluations.\nTraining and evaluation takes approximately 14 minutes on a v4-32 Cloud TPU. When the training and evaluation complete, messages similar to the following appear:\n```\nI0209 22:19:49.143219 139751309618240 controller.py:310] eval | step: 10000 | eval time: 73.6 sec | output: \n {'bleu_score': 19.401752948760986,\n  'sacrebleu_score': 18.442741330886378,\n  'validation_loss': 2.0558002}\n eval | step: 10000 | eval time: 73.6 sec | output: \n {'bleu_score': 19.401752948760986,\n  'sacrebleu_score': 18.442741330886378,\n  'validation_loss': 2.0558002}\n```\n **Important:** When you have completed the training and evaluation, be sure to [Clean up](#cleanup) so you are not charged for additional TPU usage.\nThis training script trains for 20000 steps and runs evaluation every 2000 steps. This particular training and evaluation takes approximately 8 minutes on a v3-32 Cloud TPU Pod. When the training and evaluation complete, a message similar to the following appears:\n```\nINFO:tensorflow:Writing to file /tmp/tmpdmlanxcf\nI0218 21:09:19.100718 140509661046592 translate.py:184] Writing to file /tmp/tmpdmlanxcf\nI0218 21:09:28.043537 140509661046592 transformer_main.py:118] Bleu score (uncased): 1.799112930893898\nI0218 21:09:28.043911 140509661046592 transformer_main.py:119] Bleu score (cased): 1.730366237461567\n```\nIn order to train to convergence, change `train_steps` to 200000. You can increase the number of training steps or specify how often to run evaluations by setting these parameters:- `--train_steps`: Sets the total number of training steps to run.\n- `--steps_between_evals`: Number of training steps to run between evaluations.\nWhen the training and evaluation complete, a message similar to the following appears:\n```\n0509 00:27:59.984464 140553148962624 translate.py:184] Writing to file /tmp/tmp_rk3m8jp\nI0509 00:28:11.189308 140553148962624 transformer_main.py:119] Bleu score (uncased): 1.3239131309092045\nI0509 00:28:11.189623 140553148962624 transformer_main.py:120] Bleu score (cased): 1.2855342589318752\n```\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.- Disconnect from the Compute Engine instance, if you have not already done so:```\n(vm)$ exit\n```Your prompt should now be `username@projectname` , showing you are in the Cloud Shell.\n- Delete your Cloud TPU and Compute Engine resources.```\n$ gcloud compute tpus tpu-vm delete transformer-tutorial \\\u00a0 --zone=us-central2-b\n```\n- Run `gsutil` as shown, replacing with the name of the Cloud Storage bucket you created for this tutorial: **Caution:** All training data will be lost when you delete your bucket, so only do this step when you are finished running the tutorial.```\n$ gsutil rm -r gs://bucket-name\n```## What's nextThe TensorFlow Cloud TPU tutorials generally train the model using a sample dataset. The results of this training are not usable for inference. To use a model for inference, you can train the data on a publicly available dataset or your own dataset. TensorFlow models trained on Cloud TPUs generally require datasets to be in [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format.\nYou can use the [dataset conversion toolsample](/tpu/docs/classification-data-conversion) to convert an image classification dataset into TFRecord format. If you are not using an image classification model, you will have to convert your dataset to [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) format yourself. For more information, see [TFRecord andtf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord) .\n### Hyperparameter tuningTo improve the model's performance with your dataset, you can tune the model's hyperparameters. You can find information about hyperparameters common to all TPU supported models on [GitHub](https://github.com/tensorflow/tpu/tree/master/models/hyperparameters) . Information about model-specific hyperparameters can be found in the [sourcecode](https://github.com/tensorflow/tpu/tree/master/models/official) for each model. For more information on hyperparameter tuning, see [Overview ofhyperparameter tuning](/vertex-ai/docs/training/hyperparameter-tuning-overview) and [Tunehyperparameters](https://developers.google.com/machine-learning/guides/text-classification/step-5) .\n### InferenceOnce you have trained your model, you can use it for inference (also called prediction). You can use the [Cloud TPU inference convertertool](/tpu/docs/v5e-inference-converter) to prepare and optimize a TensorFlow model for inference on Cloud TPU v5e. For more information about inference on Cloud TPU v5e, see [Cloud TPU v5e inferenceintroduction](/tpu/docs/v5e-inference) .", "guide": "Cloud TPU"}