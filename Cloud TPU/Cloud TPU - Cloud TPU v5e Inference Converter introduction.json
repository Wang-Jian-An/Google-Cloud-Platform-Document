{"title": "Cloud TPU - Cloud TPU v5e Inference Converter introduction", "url": "https://cloud.google.com/tpu/docs/v5e-inference-converter", "abstract": "# Cloud TPU - Cloud TPU v5e Inference Converter introduction\n# Cloud TPU v5e Inference Converter introduction\n# Introduction\nCloud TPU Inference Converter prepares and optimizes a TensorFlow 2 (TF2) model for TPU inference. The converter runs in a local or TPU VM shell. The TPU VM shell is recommended because it comes preinstalled with the command line tools needed for the converter. It takes an exported [SavedModel](https://www.tensorflow.org/guide/saved_model) and performs the following steps:\n- TPU Conversion: It adds`TPUPartitionedCall`and other TPU ops to the model to make it servable on the TPU. By default, a model exported for inference doesn't have such ops and cannot be served on the TPU, even if it was trained on the TPU.\n- Batching: It adds batching ops to the model to enable in-graph batching for better throughput.\n- BFloat16 Conversion: It converts the data format of the model from`float32`to`bfloat16`for better computational performance and lower High Bandwidth Memory (HBM) usage on the TPU.\n- IO Shape Optimization: It optimizes the tensor shapes for data transferred between the CPU and TPU to improve bandwidth utilization.\nWhen exporting a model, users create function aliases for any functions they would like to run on the TPU. They pass these functions to the Converter and the Converter places them on the TPU and optimizes them.\nThe Cloud TPU Inference Converter is available as a Docker image which can be executed in any environment with Docker installed.\nEstimated time to complete the steps shown above: ~20 min - 30 min\n", "content": "## Prerequisites\n- The model must be a TF2 model and exported in the [SavedModel](https://www.tensorflow.org/guide/saved_model) format.\n- The model must have a function alias for the TPU function. See the [code example](#function-alias) for how to do this. The following examples uses`tpu_func`as the TPU function alias.\n- Make sure your machine's CPU supports Advanced Vector eXtensions (AVX) instructions, as the Tensorflow library (the dependency of the Cloud TPU Inference Converter) is compiled to use AVX instructions. [Most CPUs](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX) have the AVX support.- You can run`lscpu | grep avx`to check whether the AVX instruction set is supported.\n## Before you begin\nBefore you begin setup, do the following:\n- [Create a new project](https://console.cloud.google.com/projectselector2/home/dashboard) : In the Google Cloud console, on the project selector page, select or create a Cloud project.\n- [Set up a TPU VM](/tpu/docs/managing-tpus-tpu-vm#creating_a_cloud_tpu) : Create a new TPU VM using Google Cloud console or `gcloud` , or use an existing TPU VM to run inference with the converted model on the TPU VM.- Make sure the TPU VM image is TensorFlow based. For example,`--version=tpu-vm-tf-2.11.0`.\n- The converted model will be loaded and served on this TPU VM.\n- Ensure you have the command line tools you need to use Cloud TPU Inference Converter. You can install the Google Cloud SDK and Docker locally or use a TPU VM which has this software installed by default. You use these tools to interact with the Converter image.Connect to the instance with SSH using the following command:```\ngcloud compute tpus tpu-vm ssh ${tpu-name} --zone ${zone} --project ${project-id}\n```## Environment Setup\nSet up your environment from your TPU VM shell or from your local shell.\n- In your TPU VM shell, run the following commands to allow non-root docker usage:```\nsudo usermod -a -G docker ${USER}newgrp docker\n```\n- Initialize your Docker Credential helpers:```\ngcloud auth configure-docker \\\u00a0 us-docker.pkg.dev\n```\nIn your local shell, set up the environment using the following steps:- Install the [Cloud SDK](https://cloud.google.com/sdk/install) , which includes the `gcloud` command-line tool.\n- Install [Docker](https://docs.docker.com/engine/install/) :\n- Allow non-root Docker usage:```\nsudo usermod -a -G docker ${USER}newgrp docker\n```\n- Login in to your environment:```\ngcloud auth login\n```\n- Initialize your Docker Credential helpers:```\ngcloud auth configure-docker \\\u00a0 \u00a0 us-docker.pkg.dev\n```\n- Pull the Inference Converter Docker image:```\n\u00a0 CONVERTER_IMAGE=us-docker.pkg.dev/cloud-tpu-images/inference/tpu-inference-converter-cli:2.13.0\u00a0 docker pull ${CONVERTER_IMAGE}\u00a0 \n```## Converter Image\nThe Image is for doing one-time model conversions. Set the model paths and adjust the [converter options](#converter-options) to fit your needs. The [Usage Examples](#usage-examples) section provides several common use cases.\n**Note:** In the following code, a Cloud Storage bucket can be used for `input_model_dir` and `output_model_dir` without specifying `mount` in the `docker` command.\n```\ndocker run \\--mount type=bind,source=${MODEL_PATH},target=/tmp/input,readonly \\--mount type=bind,source=${CONVERTED_MODEL_PATH},target=/tmp/output \\${CONVERTER_IMAGE} \\--input_model_dir=/tmp/input \\--output_model_dir=/tmp/output \\--converter_options_string='\u00a0 \u00a0 tpu_functions {\u00a0 \u00a0 \u00a0 function_alias: \"tpu_func\"\u00a0 \u00a0 }\u00a0 \u00a0 batch_options {\u00a0 \u00a0 \u00a0 num_batch_threads: 2\u00a0 \u00a0 \u00a0 max_batch_size: 8\u00a0 \u00a0 \u00a0 batch_timeout_micros: 5000\u00a0 \u00a0 \u00a0 allowed_batch_sizes: 2\u00a0 \u00a0 \u00a0 allowed_batch_sizes: 4\u00a0 \u00a0 \u00a0 allowed_batch_sizes: 8\u00a0 \u00a0 \u00a0 max_enqueued_batches: 10\u00a0 \u00a0 }'\n```\n## Inference with the converted model in TPU VM\n```\n# Initialize the TPUresolver = tf.distribute.cluster_resolver.TPUClusterResolver(\"local\")tf.config.experimental_connect_to_cluster(resolver)tf.tpu.experimental.initialize_tpu_system(resolver)# Load the modelmodel = tf.saved_model.load(${CONVERTED_MODEL_PATH})# Find the signature function for servingserving_signature = 'serving_default' # Change the serving signature if neededserving_fn = model.signatures[serving_signature]# Run the inference using requests.results = serving_fn(**inputs)logging.info(\"Serving results: %s\", str(results))\n```\n## Usage Examples\n### Add a function alias for the TPU function\n- Find or create a function in your model that wraps everything you want to run on the TPU. If`@tf.function`doesn't exist, add it.\n- When saving the model, provide SaveOptions like below to give`model.tpu_func`an alias`func_on_tpu`.\n- You can pass this function alias to the converter.\n```\nclass ToyModel(tf.keras.Model):\u00a0 @tf.function(\u00a0 \u00a0 \u00a0 input_signature=[tf.TensorSpec(shape=[None, 10], dtype=tf.float32)])\u00a0 def tpu_func(self, x):\u00a0 \u00a0 return x * 1.0model = ToyModel()save_options = tf.saved_model.SaveOptions(function_aliases={\u00a0 \u00a0 'func_on_tpu': model.tpu_func,})tf.saved_model.save(model, model_dir, options=save_options)\n```\n### Convert a model with multiple TPU functions\nYou can put multiple functions on the TPU. Simply create multiple function aliases and pass them in `converter_options_string` to the converter.\n```\ntpu_functions {\u00a0 function_alias: \"tpu_func_1\"}tpu_functions {\u00a0 function_alias: \"tpu_func_2\"}\n```\n### Quantization\n**Important:** This feature is experimental and may be subject to change without notice. Users should carefully evaluate the performance of the quantized model before using it in production. TPU v5e support for quantization is provided in TensorFlow.\nQuantization is a technique that reduces the precision of the numbers used to represent a model's parameters. This results in a smaller model size and faster computation. A quantized model provides gains in inference throughput as well as smaller memory usage and storage size, at the cost of small accuracy drops.\nThe new Post-Training Quantization feature in TensorFlow that targets TPU, is developed from the similar existing feature in TensorFlow Lite that is used to target mobile and edge devices. To learn more about quantization in general, you can take a look at [TensorFlow Lite's document](https://www.tensorflow.org/lite/performance/model_optimization#quantization) .\nThis section defines concepts specifically related to quantization with the Inference Converter.\nConcepts related to other TPU configurations (for example, slices, hosts, chips, and TensorCores) are described in the [TPU System Architecture](/tpu/docs/system-architecture-tpu-vm) page.\n- **Post-training quantization (PTQ)** : PTQ is a technique that reduces the size and computational complexity of a neural network model without significantly affecting its accuracy. PTQ works by converting the floating-point weights and activations of a trained model to lower-precision integers, such as 8-bit or 16-bit integers. This can cause a significant reduction in model size and inference latency, while only incurring a small loss in accuracy.\n- **Calibration** : The calibration step for quantization is the process of collecting statistics on the range of values that the weights and activations of a neural network model take. This information is used to determine the quantization parameters for the model, which are the values that will be used to convert the floating-point weights and activations to integers.\n- **Representative Dataset** : A representative dataset for quantization is a small dataset that represents the actual input data for the model. It is used during the calibration step of quantization to collect statistics on the range of values that the weights and activations of the model will take. The representative dataset should satisfy the following properties:- It should properly represent the actual inputs to the model during inference. This means that it should cover the range of values that the model is likely to see in the real world.\n- It should collectively flow through each branch of conditionals (such as`tf.cond`), if there are any. This is important because the quantization process needs to be able to handle all possible inputs to the model, even if they are not explicitly represented in the representative dataset.\n- It should be large enough to collect enough statistics and reduce error. As a rule of thumb, it is recommended to use more than 200 representative samples.\nThe representative dataset can be a subset of the training dataset, or it can be a separate dataset that is specifically designed to be representative of the real-world inputs to the model. The choice of which dataset to use depends on the specific application.\n- **Static Range Quantization (SRQ)** : SRQ determines the range of values for the weights and activations of a neural network model once, during the calibration step. This means that the same range of values is used for all inputs to the model. This can be less accurate than dynamic range quantization, especially for models with a wide range of input values. However, static range quantization requires less computation at run time than dynamic range quantization.\n- **Dynamic Range Quantization (DRQ)** : DRQ determines the range of values for the weights and activations of a neural network model for each input. This allows the model to adapt to the range of values of the input data, which can improve accuracy. However, dynamic range quantization requires more computation at run time than static range quantization.| 0      | 1                    | 2                    |\n|:------------------------|:------------------------------------------------------------------------------|:------------------------------------------------------------------------------|\n| Feature     | Static range quantization              | Dynamic range quantization             |\n| Range of values   | Determined once, during calibration           | Determined for each input              |\n| Accuracy    | Can be less accurate, especially for models with a wide range of input values | Can be more accurate, especially for models with a wide range of input values |\n| Complexity    | Simpler                  | More complex                 |\n| Computation at run time | Less computation                | More computation                |\n- **Weight-only Quantization** : Weight-only quantization is a type of quantization that only quantizes the weights of a neural network model, while leaving the activations in floating point. This can be a good option for models that are sensitive to accuracy, as it can help to preserve the accuracy of the model.Quantization can be applied by configuring and setting [QuantizationOptions](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/mlir/quantization/tensorflow/quantization_options.proto) to the converter options. Notable options are:\n- tags: Collection of tags identifying the [MetaGraphDef](https://www.tensorflow.org/guide/saved_model#details_of_the_savedmodel_command_line_interface) within the [SavedModel](https://www.tensorflow.org/guide/saved_model#running_a_savedmodel_in_tensorflow_serving) to quantize. No need to specify if you have only one`MetaGraphDef`.\n- signature_keys: Sequence of keys identifying` [SignatureDef](https://www.tensorflow.org/guide/saved_model#details_of_the_savedmodel_command_line_interface) `containing inputs and outputs. If not specified, [\"serving_default\"] is used.\n- quantization_method: Quantization method to apply. If not specified,`STATIC_RANGE`quantization will be applied.\n- op_set: Should be kept as XLA. It is currently the default option, no need to specify.\n- representative_datasets: Specify the dataset used for calibrating the quantization parameters.A representative dataset is essentially an iterable of samples. Where a sample is a map of: `{input_key: input_value}` . For example:\n```\nrepresentative_dataset = [{\"x\": tf.random.uniform(shape=(3, 3))}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _ in range(256)]\n```\nThe representative datasets should be saved as `TFRecord` files using the `TfRecordRepresentativeDatasetSaver` class currently available in the tf-nightly pip package. For example:\n```\n# Assumed tf-nightly installed.import tensorflow as tfrepresentative_dataset = [{\"x\": tf.random.uniform(shape=(3, 3))}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _ in range(256)]tf.quantization.experimental.TfRecordRepresentativeDatasetSaver(\u00a0 \u00a0 \u00a0 \u00a0path_map={'serving_default': '/tmp/representative_dataset_path'}\u00a0 \u00a0 ).save({'serving_default': representative_dataset})\n```\nThe following example quantizes the model with the signature key of `serving_default` and function alias of `tpu_func` :\n**Note:** In the following code, set `CONVERTER_IMAGE` as `us-docker.pkg.dev/cloud-tpu-images/inference/tpu-inference-converter-cli:nightly` to use the nightly build.\n```\ndocker run \\\u00a0 --mount type=bind,source=${MODEL_PATH},target=/tmp/input,readonly \\\u00a0 --mount type=bind,source=${CONVERTED_MODEL_PATH},target=/tmp/output \\\u00a0 ${CONVERTER_IMAGE} \\\u00a0 --input_model_dir=/tmp/input \\\u00a0 --output_model_dir=/tmp/output \\\u00a0 --converter_options_string=' \\\u00a0 \u00a0 tpu_functions { \\\u00a0 \u00a0 \u00a0 function_alias: \"tpu_func\" \\\u00a0 \u00a0 } \\\u00a0 \u00a0 external_feature_configs { \\\u00a0 \u00a0 \u00a0 quantization_options { \\\u00a0 \u00a0 \u00a0 \u00a0 signature_keys: \"serving_default\" \\\u00a0 \u00a0 \u00a0 \u00a0 representative_datasets: { \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: \"serving_default\" \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: { \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tfrecord_file_path: \"${TF_RECORD_FILE}\" \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 } \\\u00a0 \u00a0 \u00a0 \u00a0 } \\\u00a0 \u00a0 \u00a0 } \\\u00a0 \u00a0 } '\n```\n### Add batching\nThe Converter can be used to add batching to a model. For a description of the batching options that can be tuned, see [Definition of batching options](#batching-options) .\nBy default, the Converter will batch any TPU functions in the model. It can also batch user-provided [signatures](#signature-batching) and [functions](#function-batching) which can further improve performance. Any TPU function, user-provided function or signature that is batched, must meet the batching op's [strict shape requirements](#strict-shape-requirements) .\nThe Converter can also [update](#update-batching-options) existing batching options. The following is an example of how to add batching to a model. For more information on batching, see [Batching deep dive](#batching-deep-dive) .\n```\nbatch_options {\u00a0 num_batch_threads: 2\u00a0 max_batch_size: 8\u00a0 batch_timeout_micros: 5000\u00a0 allowed_batch_sizes: 2\u00a0 allowed_batch_sizes: 4\u00a0 allowed_batch_sizes: 8\u00a0 max_enqueued_batches: 10}\n```\n### Disable bfloat16 and IO shape optimizations\nBFloat16 and IO Shape Optimizations are enabled by default. If they don't work well with your model, they can be disabled.\n```\n# Disable both optimizationsdisable_default_optimizations: true# Or disable them individuallyio_shape_optimization: DISABLEDbfloat16_optimization: DISABLED\n```\n### Conversion Report\nYou can find this conversion report from the log after running the Inference Converter. Below is an example.\n```\n-------- Conversion Report --------TPU cost of the model: 96.67% (2034/2104)CPU cost of the model: \u00a03.33% (70/2104)Cost breakdown================================% \u00a0 \u00a0 \u00a0 \u00a0 Cost \u00a0 \u00a0Name--------------------------------3.33 \u00a0 \u00a0 \u00a070 \u00a0 \u00a0 \u00a0[CPU cost]48.34 \u00a0 \u00a0 1017 \u00a0 \u00a0tpu_func_148.34 \u00a0 \u00a0 1017 \u00a0 \u00a0tpu_func_2-------------------------------```\nThis report estimates the computational cost of the output model on CPU and TPU, and further breaks down the TPU cost to each function, which should reflect your selection of the TPU functions in the converter options.\nIf you want to better utilize the TPU, you may want to experiment with the model structure and adjust the converter options.\n**Note:** In the current conversion report, all ops in the TPU function(s) are counted as TPU cost.\n## FAQs\n### Which function(s) should I place on the TPU?\nIt is best to put as much of your model on the TPU as possible, because the vast majority of ops execute faster on the TPU.\nIf your model does not contain any TPU-incompatible op, strings or sparse tensors, putting the entire model on the TPU is usually the best strategy. And you can do it by finding or creating a function that wraps the entire model, creating a function alias for it, and passing that to the Converter.\nIf your model contains parts that cannot work on the TPU (e.g.,TPU-incompatible ops, strings or sparse tensors), the choice of TPU functions depends on where the incompatible part is.\n- If it's at the beginning or the end of the model, you can refactor the model to keep it on the CPU. Examples are string pre- and post-processing stages. For more information about moving code to the CPU, see, [\"How do I move a part of the model to CPU?\"](#move-model-to-cpu) It shows a typical way to refactor the model.\n- If it's in the middle of the model it's better to split the model into three parts and contain all the TPU-incompatible ops in the middle part, and make it run on the CPU.\n- If it is a sparse tensor, consider calling`tf.sparse.to_dense`on the CPU and passing the resulting dense tensor to the TPU portion of the model.\nAnother factor to consider is the HBM usage. Embedding tables can use a lot of HBM. If they grow beyond the hardware limitation of the TPU, they have to be put on the CPU, along with the lookup ops.\nWhenever possible, only one TPU function should exist under one signature. If the structure of your model requires calling multiple TPU functions per incoming inference request, you should be aware of the added latency of sending tensors between CPU and TPU.\nA good way to evaluate the selection of TPU functions is to check the [Conversion Report](#conversion-report) . It shows the percentage of computation that was placed on the TPU, and a breakdown of the cost of each TPU function.\n### How do I move a part of the model to CPU?\nIf your model contains parts that cannot be served on the TPU, you need to refactor the model to move them to the CPU. Here is a toy example. The model is a language model with a preprocessing stage. The code for layer definitions and functions are omitted for simplicity.\n```\nclass LanguageModel(tf.keras.Model):\u00a0 @tf.function\u00a0 def model_func(self, input_string):\u00a0 \u00a0 word_ids = self.preprocess(input_string)\u00a0 \u00a0 return self.bert_layer(word_ids)\n```\nThis model cannot be directly served on the TPU for two reasons. First, the parameter is a string. Second, the `preprocess` function may contain many string ops. Both are not TPU-compatible.\nTo refactor this model, you can create another function called `tpu_func` to host the computational-intensive `bert_layer` . Then create a function alias for `tpu_func` and pass it to the Converter. In this way, everything inside `tpu_func` will run on the TPU, and everything left in `model_func` will run on the CPU.\n```\nclass LanguageModel(tf.keras.Model):\u00a0 @tf.function\u00a0 def tpu_func(self, word_ids):\u00a0 \u00a0 return self.bert_layer(word_ids)\u00a0 @tf.function\u00a0 def model_func(self, input_string):\u00a0 \u00a0 word_ids = self.preprocess(input_string)\u00a0 \u00a0 return self.tpu_func(word_ids)\n```\n### What should I do if the model has TPU-incompatible ops, strings or sparse tensors?\nMost of the standard TensorFlow ops are supported on the TPU, but a few including sparse tensors and strings are not supported. The Converter doesn't check for TPU-incompatible ops. So a model containing such ops can pass the conversion. But when running it for inference, errors like below will occur.\n```\n'tf.StringToNumber' op isn't compilable for TPU device.\n```\nIf your model has TPU-incompatible ops, they should be put outside the TPU function. Moreover, string is an unsupported data format on the TPU. So string-typed variables shouldn't be placed in the TPU function. And the parameters and return values of the TPU function shouldn't be string-typed as well. Similarly, avoid placing sparse tensors in the TPU function including in its parameters and return values.\nIt's usually not hard to refactor out the incompatible part of the model and move it to the CPU. Here is an [example](#move-model-to-cpu) .\n### How to support custom ops in the model?\nIf custom ops are used in your model, the Converter may not recognize them and fail to convert the model. This is because the op library of the custom op, which contains the complete definition of the op, isn't linked to the Converter.\nAs currently the converter code is not open-sourced yet, the converter cannot be built with custom op.\n### What should I do if I have a TensorFlow 1 model?\nThe Converter does not support TensorFlow 1 models. TensorFlow 1 models should be migrated to TensorFlow 2.\n### Do I need to enable the MLIR bridge when running my model?\nMost converted models can be run with either the newer TF2XLA MLIR bridge or the original TF2XLA bridge.\n### How do I convert a model that has already been exported without a function alias?\nIf a model was exported without a function alias, the easiest way is to export it again and [create a function alias](#function-alias) . If reexporting is not an option, it is still possible to convert the model by providing a `concrete_function_name` . However, identifying the correct `concrete_function_name` does require some detective work.\nFunction aliases are a mapping from a user defined string to a concrete function name. They make it easier to refer to a specific function in the model. The Converter accepts both function aliases and raw concrete function names.\nConcrete function names can be found by examining the `saved_model.pb` .\nThe following example shows how to put a concrete function called `__inference_serve_24` on the TPU.\n```\nsudo docker run \\--mount type=bind,source=${MODEL_PATH},target=/tmp/input,readonly \\--mount type=bind,source=${CONVERTED_MODEL_PATH},target=/tmp/output \\${CONVERTER_IMAGE} \\--input_model_dir=/tmp/input \\--output_model_dir=/tmp/output \\--converter_options_string='\u00a0 \u00a0 tpu_functions {\u00a0 \u00a0 \u00a0 concrete_function_name: \"__inference_serve_24\"\u00a0 \u00a0 }'\n```\n### How do I resolve a compile time constant constraint error?\nFor both training and inference, XLA requires the inputs to certain ops have a known shape at TPU compile time. This means that when XLA compiles the TPU portion of the program, the inputs to these ops must have a statically known shape.\nThere are two ways to resolve this issue.\n- The best option is to update the op's inputs to have a statically known shape by the time XLA compiles the TPU program. This compilation happens right before the TPU portion of the model is run. This means that the shape should be statically known by the time the`TpuFunction`is about to run.\n- Another option is to modify the`TpuFunction`to no longer include the problematic op.\n### Why am I getting a batching shape error?\nBatching has [strict shape requirements](#strict-shape-requirements) that allow incoming requests to be batched along their 0th dimension (aka the batching dimension). These shape requirements come from the TensorFlow batching op and cannot be relaxed.\nFailure to meet these requirements will result in errors like:\n- Batching input tensors must have at least one dimension.\n- Dimensions of inputs should match.\n- Batching input tensors supplied in a given op invocation must have equal 0th-dimension size.\n- Batched output tensor's 0th dimension does not equal the sum of the 0th dimension sizes of the input tensors.\nTo meet these requirements, consider providing a different [function](#function-batching) or [signature](#signature-batching) to batch. It may also be necessary to modify existing functions to meet these requirements.\nIf a function is being batched, make sure its `@tf.function` 's input_signature's shapes all have None in the 0th dimension. If a signature is being batched, make sure that all its inputs have -1 in the 0th dimension.\nFor a complete explanation on why these errors are happening and how to resolve them, see [Batching Deep Dive](#batching-deep-dive) .\n## Known Issues\n### TPU function cannot indirectly call another TPU function\nWhile the Converter can handle most function calling scenarios across the CPU-TPU boundary, there is one rare edge case it would fail. It is when a TPU function indirectly calls another TPU function.\nThis is because the Converter modifies the direct caller of a TPU function from calling the TPU function itself to calling a TPU call stub. The call stub contains ops that can only work on the CPU. When a TPU function calls any function that eventually calls the direct caller, those CPU ops could be brought on the TPU to execute, which will generate missing kernel errors. Note this case is different from a TPU function directly calling another TPU function. In this case, the Converter doesn't modify either function to call the call stub, so it can work.\nIn the Converter, we have implemented the detection of this scenario. If you see the following error, that means your model has hit this edge case:\n```\nUnable to place both \"__inference_tpu_func_2_46\" and \"__inference_tpu_func_4_68\"on the TPU because \"__inference_tpu_func_2_46\" indirectly calls\"__inference_tpu_func_4_68\". This behavior is unsupported because it can causeinvalid graphs to be generated.\n```\nThe general solution is to refactor the model to avoid such a function calling scenario. If you find that difficult to do, contact the Google support team to discuss more.\n## Reference\n### Converter Options in Protobuf format\n```\nmessage ConverterOptions {\u00a0 // TPU conversion options.\u00a0 repeated TpuFunction tpu_functions = 1;\u00a0 // The state of an optimization.\u00a0 enum State {\u00a0 \u00a0 // When state is set to default, the optimization will perform its\u00a0 \u00a0 // default behavior. For some optimizations this is disabled and for others\u00a0 \u00a0 // it is enabled. To check a specific optimization, read the optimization's\u00a0 \u00a0 // description.\u00a0 \u00a0 DEFAULT = 0;\u00a0 \u00a0 // Enabled.\u00a0 \u00a0 ENABLED = 1;\u00a0 \u00a0 // Disabled.\u00a0 \u00a0 DISABLED = 2;\u00a0 }\u00a0 // Batch options to apply to the TPU Subgraph.\u00a0 //\u00a0 // At the moment, only one batch option is supported. This field will be\u00a0 // expanded to support batching on a per function and/or per signature basis.\u00a0 //\u00a0 //\u00a0 // If not specified, no batching will be done.\u00a0 repeated BatchOptions batch_options = 100;\u00a0 // Global flag to disable all optimizations that are enabled by default.\u00a0 // When enabled, all optimizations that run by default are disabled. If a\u00a0 // default optimization is explicitly enabled, this flag will have no affect\u00a0 // on that optimization.\u00a0 //\u00a0 // This flag defaults to false.\u00a0 bool disable_default_optimizations = 202;\u00a0 // If enabled, apply an optimization that reshapes the tensors going into\u00a0 // and out of the TPU. This reshape operation improves performance by reducing\u00a0 // the transfer time to and from the TPU.\u00a0 //\u00a0 // This optimization is incompatible with input_shape_opt which is disabled.\u00a0 // by default. If input_shape_opt is enabled, this option should be\u00a0 // disabled.\u00a0 //\u00a0 // This optimization defaults to enabled.\u00a0 State io_shape_optimization = 200;\u00a0 // If enabled, apply an optimization that updates float variables and float\u00a0 // ops on the TPU to bfloat16. This optimization improves performance and\u00a0 // throughtput by reducing HBM usage and taking advantage of TPU support for\u00a0 // bfloat16.\u00a0 //\u00a0 // This optimization may cause a loss of accuracy for some models. If an\u00a0 // unacceptable loss of accuracy is detected, disable this optimization.\u00a0 //\u00a0 // This optimization defaults to enabled.\u00a0 State bfloat16_optimization = 201;\u00a0 BFloat16OptimizationOptions bfloat16_optimization_options = 203;\u00a0 // The settings for XLA sharding. If set, XLA sharding is enabled.\u00a0 XlaShardingOptions xla_sharding_options = 204;}message TpuFunction {\u00a0 // The function(s) that should be placed on the TPU. Only provide a given\u00a0 // function once. Duplicates will result in errors. For example, if\u00a0 // you provide a specific function using function_alias don't also provide the\u00a0 // same function via concrete_function_name or jit_compile_functions.\u00a0 oneof name {\u00a0 \u00a0 // The name of the function alias associated with the function that\u00a0 \u00a0 // should be placed on the TPU. Function aliases are created during model\u00a0 \u00a0 // export using the tf.saved_model.SaveOptions.\u00a0 \u00a0 //\u00a0 \u00a0 // This is a recommended way to specify which function should be placed\u00a0 \u00a0 // on the TPU.\u00a0 \u00a0 string function_alias = 1;\u00a0 \u00a0 // The name of the concrete function that should be placed on the TPU. This\u00a0 \u00a0 // is the name of the function as it found in the GraphDef and the\u00a0 \u00a0 // FunctionDefLibrary.\u00a0 \u00a0 //\u00a0 \u00a0 // This is NOT the recommended way to specify which function should be\u00a0 \u00a0 // placed on the TPU because concrete function names change every time a\u00a0 \u00a0 // model is exported.\u00a0 \u00a0 string concrete_function_name = 3;\u00a0 \u00a0 // The name of the signature to be placed on the TPU. The user must make\u00a0 \u00a0 // sure there is no TPU-incompatible op under the entire signature.\u00a0 \u00a0 string signature_name = 5;\u00a0 \u00a0 // When jit_compile_functions is set to True, all jit compiled functions\u00a0 \u00a0 // are placed on the TPU.\u00a0 \u00a0 //\u00a0 \u00a0 // To use this option, decorate the relevant function(s) with\u00a0 \u00a0 // @tf.function(jit_compile=True), before exporting. Then set this flag to\u00a0 \u00a0 // True. The converter will find all functions that were tagged with\u00a0 \u00a0 // jit_compile=True and place them on the TPU.\u00a0 \u00a0 //\u00a0 \u00a0 // When using this option, all other settings for the TpuFunction\u00a0 \u00a0 // will apply to all functions tagged with\u00a0 \u00a0 // jit_compile=True.\u00a0 \u00a0 //\u00a0 \u00a0 // This option will place all jit_compile=True functions on the TPU.\u00a0 \u00a0 // If only some jit_compile=True functions should be placed on the TPU,\u00a0 \u00a0 // use function_alias or concrete_function_name.\u00a0 \u00a0 bool jit_compile_functions = 4;\u00a0 }}message BatchOptions {\u00a0 // Number of scheduling threads for processing batches of work. Determines\u00a0 // the number of batches processed in parallel. This should be roughly in line\u00a0 // with the number of TPU cores available.\u00a0 int32 num_batch_threads = 1;\u00a0 // The maximum allowed batch size.\u00a0 int32 max_batch_size = 2;\u00a0 // Maximum number of microseconds to wait before outputting an incomplete\u00a0 // batch.\u00a0 int32 batch_timeout_micros = 3;\u00a0 // Optional list of allowed batch sizes. If left empty,\u00a0 // does nothing. Otherwise, supplies a list of batch sizes, causing the op\u00a0 // to pad batches up to one of those sizes. The entries must increase\u00a0 // monotonically, and the final entry must equal max_batch_size.\u00a0 repeated int32 allowed_batch_sizes = 4;\u00a0 // Maximum number of batches enqueued for processing before requests are\u00a0 // failed fast.\u00a0 int32 max_enqueued_batches = 5;\u00a0 // If set, disables large batch splitting which is an efficiency improvement\u00a0 // on batching to reduce padding inefficiency.\u00a0 bool disable_large_batch_splitting = 6;\u00a0 // Experimental features of batching. Everything inside is subject to change.\u00a0 message Experimental {\u00a0 \u00a0 // The component to be batched.\u00a0 \u00a0 // 1. Unset if it's for all TPU subgraphs.\u00a0 \u00a0 // 2. Set function_alias or concrete_function_name if it's for a function.\u00a0 \u00a0 // 3. Set signature_name if it's for a signature.\u00a0 \u00a0 oneof batch_component {\u00a0 \u00a0 \u00a0 // The function alias associated with the function. Function alias is\u00a0 \u00a0 \u00a0 // created during model export using the tf.saved_model.SaveOptions, and is\u00a0 \u00a0 \u00a0 // the recommended way to specify functions.\u00a0 \u00a0 \u00a0 string function_alias = 1;\u00a0 \u00a0 \u00a0 // The concreate name of the function. This is the name of the function as\u00a0 \u00a0 \u00a0 // it found in the GraphDef and the FunctionDefLibrary. This is NOT the\u00a0 \u00a0 \u00a0 // recommended way to specify functions, because concrete function names\u00a0 \u00a0 \u00a0 // change every time a model is exported.\u00a0 \u00a0 \u00a0 string concrete_function_name = 2;\u00a0 \u00a0 \u00a0 // The name of the signature.\u00a0 \u00a0 \u00a0 string signature_name = 3;\u00a0 \u00a0 }\u00a0 }\u00a0 Experimental experimental = 7;}message BFloat16OptimizationOptions {\u00a0 // Indicates where the BFloat16 optimization should be applied.\u00a0 enum Scope {\u00a0 \u00a0 // The scope currently defaults to TPU.\u00a0 \u00a0 DEFAULT = 0;\u00a0 \u00a0 // Apply the bfloat16 optimization to TPU computation.\u00a0 \u00a0 TPU = 1;\u00a0 \u00a0 // Apply the bfloat16 optimization to the entire model including CPU\u00a0 \u00a0 // computations.\u00a0 \u00a0 ALL = 2;\u00a0 }\u00a0 // This field indicates where the bfloat16 optimization should be applied.\u00a0 //\u00a0 // The scope defaults to TPU.\u00a0 Scope scope = 1;\u00a0 // If set, the normal safety checks are skipped. For example, if the model\u00a0 // already contains bfloat16 ops, the bfloat16 optimization will error because\u00a0 // pre-existing bfloat16 ops can cause issues with the optimization. By\u00a0 // setting this flag, the bfloat16 optimization will skip the check.\u00a0 //\u00a0 // This is an advanced feature and not recommended for almost all models.\u00a0 //\u00a0 // This flag is off by default.\u00a0 bool skip_safety_checks = 2;\u00a0 // Ops that should not be converted to bfloat16.\u00a0 // Inputs into these ops will be cast to float32, and outputs from these ops\u00a0 // will be cast back to bfloat16.\u00a0 repeated string filterlist = 3;}message XlaShardingOptions {\u00a0 // num_cores_per_replica for TPUReplicateMetadata.\u00a0 //\u00a0 // This is the number of cores you wish to split your model into using XLA\u00a0 // SPMD.\u00a0 int32 num_cores_per_replica = 1;\u00a0 // (optional) device_assignment for TPUReplicateMetadata.\u00a0 //\u00a0 // This is in a flattened [x, y, z, core] format (for\u00a0 // example, core 1 of the chip\u00a0 // located in 2,3,0 will be stored as [2,3,0,1]).\u00a0 //\u00a0 // If this is not specified, then the device assignments will utilize the same\u00a0 // topology as specified in the topology attribute.\u00a0 repeated int32 device_assignment = 2;\u00a0 // A serialized string of tensorflow.tpu.TopologyProto objects, used for\u00a0 // the topology attribute in TPUReplicateMetadata.\u00a0 //\u00a0 // You must specify the mesh_shape and device_coordinates attributes in\u00a0 // the topology object.\u00a0 //\u00a0 // This option is required for num_cores_per_replica > 1 cases due to\u00a0 // ambiguity of num_cores_per_replica, for example,\u00a0 // pf_1x2x1 with megacore and df_1x1\u00a0 // both have num_cores_per_replica = 2, but topology is (1,2,1,1) for pf and\u00a0 // (1,1,1,2) for df.\u00a0 // - For pf_1x2x1, mesh shape and device_coordinates looks like:\u00a0 // \u00a0 mesh_shape = [1,2,1,1]\u00a0 // \u00a0 device_coordinates=flatten([0,0,0,0], [0,1,0,0])\u00a0 // - For df_1x1, mesh shape and device_coordinates looks like:\u00a0 // \u00a0 mesh_shape = [1,1,1,2]\u00a0 // \u00a0 device_coordinates=flatten([0,0,0,0], [0,0,0,1])\u00a0 // - For df_2x2, mesh shape and device_coordinates looks like:\u00a0 // \u00a0 mesh_shape = [2,2,1,2]\u00a0 // \u00a0 device_coordinates=flatten(\u00a0 // \u00a0 \u00a0[0,0,0,0],[0,0,0,1],[0,1,0,0],[0,1,0,1]\u00a0 // \u00a0 \u00a0[1,0,0,0],[1,0,0,1],[1,1,0,0],[1,1,0,1])\u00a0 bytes topology = 3;}\n```\n## Batching Deep Dive\nBatching is used to improve the throughput and TPU utilization. It allows multiple requests to be processed at the same time. During training, batching can be done using `tf.data` . During inference, it is typically done by adding an op in the graph that batches incoming requests. The op waits until it has enough requests or a timeout is reached before it generates a large batch from the individual requests. See [Definition of batching options](#batching-options) for more information about the different batching options that can be tuned, including batch sizes and timeouts.\nBy default, the Converter inserts the batching op directly before the TPU computation. It wraps the user-provided TPU function(s) and any preexisting TPU computation in the model with batching op(s). It is possible to override this default behavior by telling the Converter which [functions](#function-batching) and/or [signatures](#signature-batching) should be batched.\nThe following example shows how to add the default batching.\n```\nbatch_options {\u00a0 num_batch_threads: 2\u00a0 max_batch_size: 8\u00a0 batch_timeout_micros: 5000\u00a0 allowed_batch_sizes: 2\u00a0 allowed_batch_sizes: 4\u00a0 allowed_batch_sizes: 8\u00a0 max_enqueued_batches: 10}\n```\n### Signature batching\n**Note:** This feature is still in the experimental stage. After testing, the field will be moved out of the experimental wrapper.\nSignature batching batches the entire model starting at the signature's inputs and going to the signature's outputs. Unlike the Converter's default batching behavior, signature batching batches both the TPU computation and the CPU computation. This gives 10% to 20% performance gain during inference on some models.\nLike all batching, Signature batching does have [strict shape requirements](#strict-shape-requirements) . To help ensure these shape requirements are met, signature inputs should have shapes that have at least two dimensions. The first dimension is batch size and should have a size of -1. For example, `(-1, 4)` , `(-1)` or `(-1, 128, 4, 10)` are all valid input shapes. If this is not possible, consider using the default batching behavior or [function batching](#function-batching) .\nTo use signature batching provide the signature name(s) as `signature_name` (s) using the `BatchOptions` .\n```\nbatch_options {\u00a0 num_batch_threads: 2\u00a0 max_batch_size: 8\u00a0 batch_timeout_micros: 5000\u00a0 allowed_batch_sizes: 2\u00a0 allowed_batch_sizes: 4\u00a0 allowed_batch_sizes: 8\u00a0 max_enqueued_batches: 10\u00a0 experimental {\u00a0 \u00a0 signature_name: \"serving_default\"\u00a0 }}\n```\n### Function batching\n**Note:** This feature is still in the experimental stage. After testing, the field will be moved out of the experimental wrapper.\nFunction batching can be used to tell the Converter which function(s) should be batched. By default the Converter will batch all TPU functions. Function batching overrides this default behavior.\nFunction batching can be used to batch CPU computation. Many models see a performance improvement when their CPU computation is batched. The best way to batch CPU computation is using signature batching however it may not work for some models. In those cases, function batching can be used to batch part of the CPU computation in addition to the TPU computation. Note that the batching op cannot run on the TPU so any batching function that is provided must be called on the CPU.\nFunction batching can also be used to satisfy the [strict shape requirements](#strict-shape-requirements) imposed by the batching op. In cases when the TPU function(s) don't meet the batching op's shape requirements, function batching can be used to tell the Converter to batch different function(s).\nTo use this, generate a `function_alias` for the function that should be batched. You can do this by finding or creating a function in your model that wraps everything you want batched. Make sure this function meets the [strict shape requirements](#strict-shape-requirements) imposed by the batching op. Add `@tf.function` if it doesn't have one already. It is important to provide the `input_signature` to the `@tf.function` . The 0th dimension should be `None` because it is the batch dimension so it cannot be a fixed size. For example, `[None, 4]` , `[None]` or `[None, 128, 4, 10]` are all valid input shapes. When saving the model, provide `SaveOptions` like those shown below to give `model.batch_func` an alias \" `batch_func` \". Then you can pass this function alias to the converter.\n```\nclass ToyModel(tf.keras.Model):\u00a0 @tf.function(input_signature=[tf.TensorSpec(shape=[None, 10],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 dtype=tf.float32)])\u00a0 def batch_func(self, x):\u00a0 \u00a0 return x * 1.0\u00a0 ...model = ToyModel()save_options = tf.saved_model.SaveOptions(function_aliases={\u00a0 \u00a0 'batch_func': model.batch_func,})tf.saved_model.save(model, model_dir, options=save_options)\n```\nNext, pass the `function_alias` (s) using the BatchOptions.\n```\nbatch_options {\u00a0 num_batch_threads: 2\u00a0 max_batch_size: 8\u00a0 batch_timeout_micros: 5000\u00a0 allowed_batch_sizes: 2\u00a0 allowed_batch_sizes: 4\u00a0 allowed_batch_sizes: 8\u00a0 max_enqueued_batches: 10\u00a0 experimental {\u00a0 \u00a0 function_alias: \"batch_func\"\u00a0 }}\n```\n### Definition of batching options\n- `num_batch_threads`: (integer) Number of scheduling threads for processing batches of work. Determines the number of batches processed in parallel. This should be roughly in line with the number of TPU cores available.\n- `max_batch_size`: (integer) Maximum allowed batch size. Can be larger than`allowed_batch_sizes`to utilize large batch splitting.\n- `batch_timeout_micros`: (integer) Maximum number of microseconds to wait before outputting an incomplete batch.\n- `allowed_batch_sizes`: (list of integers) If the list is not empty, it will pad batches up to the nearest size in the list. The list must be monotonically increasing and the final element must be lower than or equal to`max_batch_size`.\n- `max_enqueued_batches`: (integer) Maximum number of batches enqueued for processing before requests are failed fast.\n### Updating existing batching options\nYou can add or update batching options by running the Docker image specifying batch_options and setting `disable_default_optimizations` to true using the `--converter_options_string` flag. The batch options will be applied to every TPU function or pre-existing batching op.\n**Note:** You cannot update batching options by setting `tpu_functions` together using the `--converter_options_string` .\n```\nbatch_options {\u00a0 num_batch_threads: 2\u00a0 max_batch_size: 8\u00a0 batch_timeout_micros: 5000\u00a0 allowed_batch_sizes: 2\u00a0 allowed_batch_sizes: 4\u00a0 allowed_batch_sizes: 8\u00a0 max_enqueued_batches: 10}disable_default_optimizations=True\n```\n### Batching shape requirements\nBatches are created by concatenating input tensors across requests along their batch (0th) dimension. The output tensors are split along their 0th dimension. In order to perform these operations, the batching op has strict shape requirements for its inputs and outputs.\nTo understand these requirements, it is helpful to first understand how batching is performed. In the example below, we are batching a simple `tf.matmul` op.\n```\ndef my_func(A, B)\u00a0 \u00a0 return tf.matmul(A, B)\n```\nThe first inference request produces the inputs A and B with the shapes `(1, 3, 2)` and `(1, 2, 4)` respectively. The second inference request produces the inputs A and B with the shapes `(2, 3, 2)` and `(2, 2, 4)` .\nThe batching timeout is reached. The model supports a batch size of 3 so inference requests #1 and #2 are batched together without any padding. The batched tensors are formed by concatenating the requests #1 and #2 along the batch (0th) dimension. Since #1's A has a shape of `(1, 3, 2)` and #2's A has a shape of `(2, 3, 2)` , when they are concatenated along the batch (0th) dimension, the resulting shape is `(3, 3, 2)` .\nThe `tf.matmul` is executed and it produces an output with the shape `(3, 3, 4)` .\nThe output of the `tf.matmul` is batched so it needs to be split back into separate requests. The batching op does this by splitting along the batch (0th) dimension of each output tensor. It decides how to split the 0th dimension based on the shape of the original inputs. Since request #1's shapes have a 0th dimension of 1, its output has a 0th dimension of 1 for a shape of `(1, 3, 4)` . Since request #2's shapes have a 0th dimension of 2, its output has a 0th dimension of 2 for a shape of `(2, 3, 4)` .\nIn order to perform the input concatenating and output splitting described above, the batching op has the following shape requirements:\n- Inputs to batching cannot be scalars. In order to concatenate along the 0th dimension, the tensors have to have at least two dimensions.In the walkthrough above. Neither A nor B are scalars.Failure to meet this requirement will cause an error like: `Batching input tensors must have at least one dimension` . A simple fix for this error is to make the scalar a vector.\n- Across different inference requests (for example, different Session run invocations), input tensors with the same name have the same size for each dimension except the 0th dimension. This allows inputs to be cleanly concatenated along their 0th dimension.In the walkthrough above, request #1's A has a shape of `(1, 3, 2)` . This means that any future request must produce a shape with the pattern `(X, 3, 2)` . Request #2 meets this requirement with `(2, 3, 2)` . Similarly, request #1's B has a shape of `(1, 2, 4)` so all future requests must produce a shape with the pattern `(X, 2, 4)` .Failure to meet this requirement will cause an error like: `Dimensions of inputs should match` .\n- For a given inference request, all inputs must have the same 0th dimension size. If different input tensors to the batching op have different 0th dimensions, the batching op does not know how to split the output tensors.In the walkthrough above, request #1's tensors all have a 0th dimension size of 1. This lets the batching op know that its output should have a 0th dimension size of 1. Similarly request #2's tensors have a 0th dimension size of 2, so its output will have a 0th dimension size of 2. When the batching op splits the final shape of `(3, 3, 4)` , it produces `(1, 3, 4)` for request #1 and `(2, 3, 4)` for request #2.Failure to meet this requirement will result in errors like: `Batching input tensors supplied in a given op invocation must have equal 0th-dimension size` .\n- The 0th dimension size of each output tensor's shape must be the sum of all the input tensors' 0th dimension size (plus any padding introduced by the batching op to meet the next largest `allowed_batch_size` ). This allows the batching op to split the output tensors along their 0th dimension based on the 0th dimension of the input tensors.In the walkthrough above, the input tensors have a 0th dimension of 1 from request #1 and 2 from request #2. Therefore, each output tensor must have a 0th dimension of 3 because 1+2=3. The output tensor `(3, 3, 4)` meets this requirement. If 3 had not been a valid batch size but 4 was, the batching op would have had to pad the 0th dimension of the inputs from 3 to 4. In this case, each output tensor would have to have a 0th dimension size of 4.Failure to meet this requirement will result in an error like: `Batched output tensor's 0th dimension does not equal the sum of the 0th dimension sizes of the input tensors` .To meet these requirements, consider providing a different [function](#function-batching) or [signature](#signature-batching) to batch. It may also be necessary to modify existing functions to meet these requirements.\nIf a [function](#function-batching) is being batched, make sure its `@tf.function` 's input_signature's shapes all have `None` in the 0th dimension (aka the batch dimension). If a [signature](#signature-batching) is being batched, make sure that all its inputs have -1 in the 0th dimension.\nThe BatchFunction op does not support `SparseTensors` as inputs or outputs. Internally, each sparse tensor is represented as three separate tensors that can have different 0th dimension sizes.", "guide": "Cloud TPU"}