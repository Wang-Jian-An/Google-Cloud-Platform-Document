{"title": "Cloud TPU - Run a calculation on a Cloud TPU VM using JAX", "url": "https://cloud.google.com/tpu/docs/run-calculation-jax", "abstract": "# Cloud TPU - Run a calculation on a Cloud TPU VM using JAX\n# Run a calculation on a Cloud TPU VM using JAX\nThis document provides a brief introduction to working with JAX and Cloud TPU.\nBefore you follow this quickstart, you must create a Google Cloud Platform account, install the Google Cloud CLI, and configure the `gcloud` command. For more information, see [Set up an account and a Cloud TPU project](/tpu/docs/setup-gcp-account) .\n", "content": "## Install the Google Cloud CLI\nThe Google Cloud CLI contains tools and libraries for interacting with Google Cloud products and services. For more information, see [Installing the Google Cloud CLI](/sdk/docs/install) .\n## Configure the gcloud command\nRun the following commands to configure `gcloud` to use your Google Cloud project and install components needed for the TPU VM preview.\n```\n\u00a0 $ gcloud config set account your-email-account\u00a0 $ gcloud config set project your-project-id\n```\n## Enable the Cloud TPU API\n- Enable the Cloud TPU API using the following `gcloud` command in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) . (You may also enable it from the [Google Cloud console](https://console.cloud.google.com/) ).```\n$ gcloud services enable tpu.googleapis.com\n```\n- Run the following command to create a service identity.```\n$ gcloud beta services identity create --service tpu.googleapis.com\n```## Create a Cloud TPU VM with gcloud\nWith Cloud TPU VMs, your model and code run directly on the TPU host machine. You SSH directly into the TPU host. You can run arbitrary code, install packages, view logs, and debug code directly on the TPU Host.\n- Create your TPU VM by running the following command from a Cloud Shell or your computer terminal where the [Google Cloud CLI](/sdk/docs/install) is installed.```\n(vm)$ gcloud compute tpus tpu-vm create tpu-name \\--zone=us-central2-b \\--accelerator-type=v4-8 \\--version=tpu-ubuntu2204-base\n```## Connect to your Cloud TPU VM\nSSH into your TPU VM by using the following command:\n```\n$ gcloud compute tpus tpu-vm ssh tpu-name --zone=us-central2-b\n```## Install JAX on your Cloud TPU VM\n```\n(vm)$ pip install jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n```\n## System check\nVerify that JAX can access the TPU and can run basic operations:\n### Start the Python 3 interpreter:\n```\n(vm)$ python3\n```\n```\n>>> import jax\n```\n### Display the number of TPU cores available:\n```\n>>> jax.device_count()\n```\nThe number of TPU cores is displayed. If you are using a v4 TPU, this should be `4` . If you are using a v2 or v3 TPU, this should be `8` .\n### Perform a simple calculation:\n```\n>>> jax.numpy.add(1, 1)\n```\nThe result of the numpy add is displayed:\nOutput from the command:\n```\nArray(2, dtype=int32, weak_type=true)\n```### Exit the Python interpreter:\n```\n>>> exit()\n```\n## Running JAX code on a TPU VM\nYou can now run any JAX code you want. The [flax examples](https://github.com/google/flax/tree/master/examples) are a great place to start with running standard ML models in JAX. For example, to train a basic MNIST convolutional network:\n- Install Flax examples dependencies```\n(vm)$ pip install --upgrade clu(vm)$ pip install tensorflow(vm)$ pip install tensorflow_datasets\n```\n- Install FLAX```\n(vm)$ git clone https://github.com/google/flax.git(vm)$ pip install --user flax\n```\n- Run the FLAX MNIST training script```\n(vm)$ cd flax/examples/mnist(vm)$ python3 main.py --workdir=/tmp/mnist \\--config=configs/default.py \\--config.learning_rate=0.05 \\--config.num_epochs=5\n```\nThe script downloads the dataset and starts training. The script output should look like this:\n```\n\u00a0 0214 18:00:50.660087 140369022753856 train.py:146] epoch: \u00a01, train_loss: 0.2421, train_accuracy: 92.97, test_loss: 0.0615, test_accuracy: 97.88\u00a0 I0214 18:00:52.015867 140369022753856 train.py:146] epoch: \u00a02, train_loss: 0.0594, train_accuracy: 98.16, test_loss: 0.0412, test_accuracy: 98.72\u00a0 I0214 18:00:53.377511 140369022753856 train.py:146] epoch: \u00a03, train_loss: 0.0418, train_accuracy: 98.72, test_loss: 0.0296, test_accuracy: 99.04\u00a0 I0214 18:00:54.727168 140369022753856 train.py:146] epoch: \u00a04, train_loss: 0.0305, train_accuracy: 99.06, test_loss: 0.0257, test_accuracy: 99.15\u00a0 I0214 18:00:56.082807 140369022753856 train.py:146] epoch: \u00a05, train_loss: 0.0252, train_accuracy: 99.20, test_loss: 0.0263, test_accuracy: 99.18\n```\n## Clean up\nWhen you are done with your TPU VM follow these steps to clean up your resources.\n- Disconnect from the Compute Engine instance, if you have not already done so:```\n(vm)$ exit\n```\n- Delete your Cloud TPU.```\n$ gcloud compute tpus tpu-vm delete tpu-name \\\u00a0 --zone=us-central2-b\n```\n- Verify the resources have been deleted by running the following command. Make sure your TPU is no longer listed. The deletion might take several minutes.```\n$ gcloud compute tpus tpu-vm list \\\u00a0 --zone=us-central2-b\n```## Performance Notes\nHere are a few important details that are particularly relevant to using TPUs in JAX.\n### Padding\nOne of the most common causes for slow performance on TPUs is introducing inadvertent padding:\n- Arrays in the Cloud TPU are tiled. This entails padding one of the dimensions to a multiple of 8, and a different dimension to a multiple of 128.\n- The matrix multiplication unit performs best with pairs of large matrices that minimize the need for padding.\n### bfloat16 dtype\nBy default, matrix multiplication in JAX on TPUs uses [bfloat16](/tpu/docs/bfloat16) with float32 accumulation. This can be controlled with the precision argument on relevant jax.numpy function calls (matmul, dot, einsum, etc). In particular:\n- `precision=jax.lax.Precision.DEFAULT`: uses mixed bfloat16 precision (fastest)\n- `precision=jax.lax.Precision.HIGH`: uses multiple MXU passes to achieve higher precision\n- `precision=jax.lax.Precision.HIGHEST`: uses even more MXU passes to achieve full float32 precision\nJAX also adds the bfloat16 dtype, which you can use to explicitly cast arrays to `bfloat16` , for example, `jax.numpy.array(x, dtype=jax.numpy.bfloat16)` .\n## Running JAX in a Colab\nWhen you run JAX code in a Colab notebook, Colab automatically creates a legacy TPU node. TPU nodes have a different architecture. For more information, see [System Architecture](/tpu/docs/system-architecture-tpu-vm) .\n## What's next\nFor more information about Cloud TPU, see:\n- [Run JAX code on TPU Pod slices](/tpu/docs/jax-pods) \n- [Manage TPUs](/tpu/docs/managing-tpus-tpu-vm) \n- [Cloud TPU System Architecture](/tpu/docs/system-architecture-tpu-vm)", "guide": "Cloud TPU"}