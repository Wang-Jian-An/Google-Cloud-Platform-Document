{"title": "Cloud TPU - Cloud TPU v5p training", "url": "https://cloud.google.com/tpu/docs/v5p-training", "abstract": "# Cloud TPU - Cloud TPU v5p training\n# Cloud TPU v5p training\nCloud TPU v5p is Google Cloud's fifth generation Cloud TPU and the successor to the v4 TPU. v5p is optimized for large scale training and to be a leading platform for the development of foundational LLMs, diffusion models, and generative AI. At a high level, v5p provides up to 2x the performance of v4, while also packing 2x more TPUs into a Pod (6k largest slice versus 3k in v4), resulting in up to 4x performance at a Pod-level. It also runs at a higher clock frequency (1.75Ghz vs. 1.05Ghz), adds SparseCore for large scale embeddings, and triples High Bandwidth Memory (HBM) capacity.\n", "content": "## Cloud TPU v5p concepts\nIf you are new to Cloud TPUs, check out the [TPU documentation home](/tpu/docs) .\nCloud TPU concepts (for example, slices, hosts, and TensorCores) and Cloud TPU system architecture for all Cloud TPU versions are described in the [Cloud TPU System Architecture](/tpu/docs/system-architecture-tpu-vm) page.\nEach Cloud TPU version requires specific accelerator types for training or inference. These accelerator types are described in [TPU configurations](/tpu/docs/supported-tpu-configurations) .\n### Manage TPU resources\nAll commands you can use to manage your TPU VMs are described in [Managing TPUs](/tpu/docs/managing-tpus-tpu-vm) or [Queued resources user guide](/tpu/docs/queued-resources) for managing queued resources.\n### Framework Setup\nThis section describes the general setup process for model training using JAX or PyTorch with TPU v5p.\nIf you have slice shapes greater than 4 chips, you will have multiple VMs in one slice. In this case, you need to use the `--worker=all` flag to run the installation on all TPU VMs using a single command:\n```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \u00a0--project ${PROJECT_ID} \\--zone ${ZONE} \\--worker=all \\--command='pip install \"jax[tpu]==0.4.20\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'\n```\nYou can run the following command to check number of devices (the outputs shown here were produced with a v5p-32 slice). This code tests that everything is installed correctly by checking that JAX sees the Cloud TPU TensorCores and can run basic operations:\n```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\--project ${PROJECT_ID} \\--zone ${ZONE} \\--worker=all \\--command='python3 -c \"import jax; print(jax.device_count()); print(jax.local_device_count())\"'\n```\nThe output will be similar to the following:\n```\nSSH: Attempting to connect to worker 0...\nSSH: Attempting to connect to worker 1...\nSSH: Attempting to connect to worker 2...\nSSH: Attempting to connect to worker 3...\n16\n4\n16\n4\n16\n4\n16\n4\n```\n`jax.device_count()` shows the total number of chips in the given slice. `jax.local_device_count()` indicates the count of chips accessible by a single VM in this slice.\n```\n# Check the number of chips in the given slice by summing the count of chips# from all VMs through the# jax.local_device_count() API call.gcloud compute tpus tpu-vm ssh ${TPU_NAME} \\--project ${PROJECT_ID} \\--zone ${ZONE} \\--worker=all \\--command='python3 -c \"import jax; xs=jax.numpy.ones(jax.local_device_count()); print(jax.pmap(lambda x: jax.lax.psum(x, \\\"i\\\"), axis_name=\\\"i\\\")(xs))\"'\n```\nThe output will be similar to the following:\n```\nSSH: Attempting to connect to worker 0...\nSSH: Attempting to connect to worker 1...\nSSH: Attempting to connect to worker 2...\nSSH: Attempting to connect to worker 3...\n[16. 16. 16. 16.]\n[16. 16. 16. 16.]\n[16. 16. 16. 16.]\n[16. 16. 16. 16.]\n```\nUse `--node=all` to run the command on all Multislice workers.\n```\ngcloud alpha compute tpus queued-resources ssh ${QUEUED_RESOURCE_ID} \\--project ${PROJECT_ID} --zone ${ZONE} --node=all --worker=all \\--command='python3 -c \"import jax; print(jax.device_count()); print(jax.local_device_count())\"'\n```\nTry the [JAX tutorials](#jax-tutorials) in this document to get started with v5p training using JAX.\nThe [PJRT runtime](https://github.com/pytorch/xla/blob/master/docs/pjrt.md) is the only supported runtime for v5p, and PyTorch 2.1+ uses PJRT as the default runtime for all TPU versions. This section describes how to start using PJRT on v5p Pods with PyTorch/XLA 2.2.0 for all workers.\n```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \u00a0\\--project ${PROJECT_ID} \\--zone ${ZONE} \\--worker=all \\--command='sudo apt-get updatesudo apt-get install libopenblas-dev -ypip3 install numpypip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 -f https://storage.googleapis.com/libtpu-releases/index.html'\n```\nUse a Python script with PJRT to do a validation of the installation to show available TPU devices (the outputs shown here were produced with a v5p-32 slice).\n```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\--project ${PROJECT_ID} --zone ${ZONE} --worker=all \\--command='PJRT_DEVICE=TPU python3 -c \"import torch_xla.core.xla_model as xm; print(xm.get_xla_supported_devices(\\\"TPU\\\"))\"'\n```\n```\nSSH: Attempting to connect to worker 0...\nSSH: Attempting to connect to worker 1...\nSSH: Attempting to connect to worker 2...\nSSH: Attempting to connect to worker 3...\n['xla:0', 'xla:1', 'xla:2', 'xla:3']\n['xla:0', 'xla:1', 'xla:2', 'xla:3']\n['xla:0', 'xla:1', 'xla:2', 'xla:3']\n['xla:0', 'xla:1', 'xla:2', 'xla:3']\n```\nUse `--node=all` to run the command on all Multislice workers.\n```\ngcloud alpha compute tpus queued-resources ssh ${QUEUED_RESOURCE_ID} \\--project ${PROJECT_ID} --zone ${ZONE} --node=all --worker=all \\--command='PJRT_DEVICE=TPU python3 -c \"import torch_xla.core.xla_model as xm; print(xm.get_xla_supported_devices(\\\"TPU\\\"))\"'\n```\nTry the [PyTorch tutorials](#pytorch-xla) in this document to get started with v5p training using PyTorch.\n## Monitor and profile\nCloud TPU v5p supports monitoring and profiling using the same methods as previous generations of Cloud TPU. You can read [Profile your model with Cloud TPU tools](/tpu/docs/profile-tpu-vm) to learn more about profiling and [Monitoring Cloud TPU VMs](/tpu/docs/troubleshooting/tpu-vm-monitoring) to learn more about monitoring.\n## Training tutorials\nThis section focuses on training tutorials for a single slice. Adapting these tutorials to Multislice training can be achieved by adding the `--node=all` flag to SSH commands. For details and best practices, refer to the [Multislice introduction.](/tpu/docs/multislice-introduction#set_up_your_environment)\n**Note:** Before running any of these tutorials, you need to [set up your Cloud TPUenvironment](/tpu/docs/setup-gcp-account#set-up-project) . Once you have set up your TPU environment, you are ready to run the following tutorials.\n- [Diffusion 2.1](#stable-diffusion) \n- [MaxText](#maxtext) \n- [ResNet on a single host v5p](#resnet-tf-single-host) \n- [ResNet on a multi-host v5p](#resnet-tf-multi-host) \n### JAX tutorials\nThis tutorial shows you how to train the Stable Diffusion model from HuggingFace using the [Pok\u00e9mon](https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions) dataset on Cloud TPU v5p.\nThe Stable Diffusion model is a latent text-to-image model that generates photo-realistic images from any text input. For more information, see the following resources:\n- [Stable Diffusion overview](https://huggingface.co/CompVis/stable-diffusion-v1-4) \n- [Stable Diffusion code source ](https://github.com/huggingface/diffusers/tree/main/examples/text_to_image) \n**Note:** Before running this tutorial, you need to [set up your Cloud TPUenvironment](/tpu/docs/setup-gcp-account#set-up-project) .- Create environment variables:```\nexport PROJECT_ID=your_project_IDexport ACCELERATOR_TYPE=v5p-32export ZONE=us-east5-aexport RUNTIME_VERSION=v2-alpha-tpuv5export SERVICE_ACCOUNT=your_service_accountexport TPU_NAME=your_tpu_nameexport QUEUED_RESOURCE_ID=queued_resource_idexport QUOTA_TYPE=quota_typeexport VALID_UNTIL_DURATION=1d\n```\n- [Create a TPU resource:](/tpu/docs/queued-resources) ```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\--node-id ${TPU_NAME} \\--project ${PROJECT_ID} \\--zone ${ZONE} \\--accelerator-type ${ACCELERATOR_TYPE} \\--runtime-version ${RUNTIME_VERSION} \\--valid-until-duration ${VALID_UNTIL_DURATION} \\--service-account ${SERVICE_ACCOUNT} \\--${QUOTA_TYPE}\n``` **Note:** The `QUOTA_TYPE` flag can be `reserved` or `best-effort` . `on-demand` is the default quota and does not need to be specified. See [quotas](/tpu/docs/quota#quota_types) for information on the different types of quotas supported by Cloud TPU. **Note:** You will be able to SSH to your TPU VM once your queued resource is in the `ACTIVE` state. Check the state of your queued resource by running the following command:```\ngcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \u00a0\\--project ${PROJECT_ID} --zone ${ZONE}\n```When the queued resource is in the `ACTIVE` state, the output will be similar to the following:```\nstate: ACTIVE\n```\n- Install JAX and its dependencies.```\n# compatible with v5p: only jax version 0.4.19 and later \\# jax 0.4.19 requires py 3.10 \\gcloud compute tpus tpu-vm ssh ${TPU_NAME} \\--project=${PROJECT_ID} --zone=${ZONE} --worker=all \\--command='pip install \"jax[tpu]==0.4.20\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html'\n```\n- Download HuggingFace [repository](https://github.com/huggingface/diffusers) and install requirements.```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\--project=${PROJECT_ID} \\--zone=${ZONE} \\--worker=all \\--command='git clone https://github.com/huggingface/diffusers.git && cd diffusers && pip install . && pip install tensorflow clu && pip install -U -r examples/text_to_image/requirements_flax.txt'\n```\n- Train the modelTrain the model with a pre-mapped buffer at 4GB.```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} --project=${PROJECT_ID} \\--zone=${ZONE} \\--worker=all \\--command='export PATH=$PATH:$HOME/.local/bin && cd diffusers/examples/text_to_image && JAX_PLATFORMS=tpu,cpu python3 train_text_to_image_flax.py --pretrained_model_name_or_path=stabilityai/stable-diffusion-2-1 --dataset_name=lambdalabs/pokemon-blip-captions --resolution=256 --center_crop --random_flip --train_batch_size=1 --mixed_precision=bf16 --max_train_steps=150 --learning_rate=1e-05 --max_grad_norm=1 --output_dir=sd-pokemon-model --from_pt'\n```Delete your TPU and queued resource request at the end of your session or to remove queued resource requests that are in the \"FAILED\" state. To delete a queued resource, delete the slice(s) and then the queued resource request in 2 steps:\n```\n\u00a0 \u00a0gcloud compute tpus tpu-vm delete ${TPU_NAME} --project=${PROJECT_ID}\u00a0 \u00a0--zone=${ZONE} --quiet\n```\n```\n\u00a0 \u00a0gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID}\u00a0 \u00a0--project ${PROJECT_ID} --zone ${ZONE} --quiet\n```\nOr, use `--force` to delete the slice(s) and the queued resource request in a single step:\n```\n# With --forcegcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID}--project ${PROJECT_ID} --zone ${ZONE} --quiet --force\n```\nThe Stable Diffusion training script ran on v5p-8, v5p-32, and v5p-128. The following table shows the throughput.\n| 0       | 1  | 2  | 3  |\n|:--------------------------|:------|:-------|:--------|\n| nan      | v5p-8 | v5p-32 | v5p-128 |\n| Train Step    | 150 | 150 | 150  |\n| Global batch size   | 32 | 64  | 64  |\n| Throughput (examples/sec) | 12.10 | 18.08 | 19.10 |\nThis tutorial shows you how to train the [MaxText](https://github.com/google/maxtext) model using a synthetic dataset on Cloud TPU.\nMaxText is a high performance, arbitrarily scalable, open source, well-tested LLM written in pure Python/JAX targeting Cloud TPUs. MaxText empowers researchers and developers with an accessible and adaptable tool for advancing the frontiers of Natural Language Processing (NLP) research and development.\nBefore running this tutorial, you need to [set up your Cloud TPU environment](/tpu/docs/setup-gcp-account#set-up-project) .\n- Set up environment variables```\nexport PROJECT_ID=your_project_IDexport TPU_NAME=your_tpu_name # user defined TPU nameexport ACCELERATOR_TYPE=v5p-256export ZONE=us-east5-aexport RUNTIME_VERSION=v2-alpha-tpuv5export RUN_NAME=your_experiment_run_name # user defined name for this runexport GCS_BUCKET_NAME=your_bucket_name # Output cloud folder. Should start with gs://export MAXTEXT_OUTPUT_PATH=${GCS_BUCKET_NAME}/your_experiment_output_pathexport NUM_SLICES=1 # Update the value to a number >1 for Multislice.\n```Optional setup recommended for Multislice:```\nexport NETWORK_NAME=your_network_nameexport FIREWALL_RULE_NAME=your_firewall_rule_name\n```If you're running Multislice workloads and want optimal network performance, consider creating a dedicated network with a Maximum Transmission Unit (MTU) of 8896 bytes and configuring appropriate firewall rules. While optional, this step can significantly improve performance, especially when scaling up the number of slices over the data-center network (DCN). Note creating a network requires `compute.networks.create` permission in the project. The following examples show how to create a dedicated network and firewall rule.Create a dedicated network:```\ngcloud compute networks create ${NETWORK_NAME} \\--mtu=8896 \\--project=${PROJECT_ID} \\--subnet-mode=auto \\--bgp-routing-mode=regional\n```Create a firewall rule:```\ngcloud compute firewall-rules create ${FIREWALL_RULE_NAME} \\--network ${NETWORK_NAME} --allow tcp,icmp,udp --project=${PROJECT_ID}\n```\n- Clone the MaxText repository```\ngit clone https://github.com/google/maxtext.git\n```\n- Train the modelThe following sections describe two options for training MaxText.If you want a script to manage the entire workflow, from provisioning Cloud TPUs and installing dependencies to running your model and tearing down resources, you can use `multihost_job.py` .```\ncd maxtext && python3 multihost_job.py --PROJECT=${PROJECT_ID} --ZONE=${ZONE} \\--NUM_SLICES=${NUM_SLICES} --TPU_TYPE=${ACCELERATOR_TYPE} \\--VERSION=${RUNTIME_VERSION} --RUN_NAME=${RUN_NAME} #user defined run name \\--BUCKET_NAME=${GCS_BUCKET_NAME} \\ #used to store logs and configs--COMMAND=\"bash setup.sh && bash MaxText/configs/experimental/64b.sh RUN_NAME=${RUN_NAME} OUTPUT_PATH=${MAXTEXT_OUTPUT_PATH} PLATFORM=gce\"\n``` **Note:** Add --CQR_EXTRA_ARGS=\"--network=${NETWORK_NAME}\" if you're using a custom network.After initiating the script, you should see a message similar to the following in the log. The log location is referenced in the output message. Click the first link to access the logs of all workers once TPU provisioning is complete.```\n-----------------------------------\nmultihost_job finished running, TPUs are starting up to run your job remotely.\nLogs for your job are displayed here:\nhttps://console.cloud.google.com/logs/query;query=resource.type%3D%22gce_instance%22%20AND%0Alog_id%2528%22_log%22%2529;?project=PROJECT_ID\nTo see the output of a single host, you may edit the slice and worker\nnumber in the `log_file_path` property here:\nhttps://console.cloud.google.com/logs/query;query=resource.type%3D%22gce_instance%22%20AND%0Alog_id%2528%22RUN_NAME_log%22%2529%20AND%0Alabels.%22agent.googleapis.com%2Flog_file_path%22%3D%20%22%2FRUN_NAME%2Fmain_command_log_slice_0_worker_0%22;?project=PROJECT_ID\nWhen your job is finished, the main command log is in your Cloud Storage\nbucket:\nhttps://console.cloud.google.com/storage/browser/YOUR_BUCKET_NAME/RUN_NAME?project=PROJECT_ID\nView the status of the created TPUs using:\ngcloud alpha compute tpus queued-resources list --filter=RUN_NAME\n--zone=ZONE --project=PROJECT_ID\n```To run the training script multiple times on a provisioned Cloud TPU, use the `multihost_runner.py` script to use the resource.\n- Set up variables to create a TPU.```\nexport SERVICE_ACCOUNT=your_service_accountexport TPU_NAME=your_tpu_nameexport QUEUED_RESOURCE_ID=your_queued_resource_idexport VALID_DURATION=1dexport QUOTA_TYPE=quota_type\n``` **Note:** The following flags are only needed if you are using Multislice:```\n--node-count ${NODE_COUNT} \\--node-prefix ${NODE_PREFIX} # optional, the default is QUEUED_RESOURCE_ID\n``` **Note:** If you specify `--node-count` and `--node-prefix` , do not also specify `--node-id` when you create the TPU resource in the following step; an error will be generated if all three flags are specified.\n- Create a TPU resource.```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\--node-id ${TPU_NAME} \\--project ${PROJECT_ID} \\--zone ${ZONE} \\--accelerator-type ${ACCELERATOR_TYPE} \\--runtime-version ${RUNTIME_VERSION} \\--valid-until-duration ${VALID_DURATION} \\--service-account ${SERVICE_ACCOUNT} \\--${QUOTA_TYPE}\n``` **Note:** The `QUOTA_TYPE` flag can be `reserved` or `best-effort` . `on-demand` is the default quota and does not need to be specified. See [quotas](/tpu/docs/quota#quota_types) for information on the different types of quotas supported by Cloud TPU.You will be able to connect to your TPU VMs using SSH once your `QueuedResource` is in state `ACTIVE` :Use the ` [describe](/tpu/docs/queued-resources#retrieve_state_and_diagnostic_information_about_a_queued_resource_request) ` command to query the status of your queued resource.```\ngcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID}--project ${PROJECT_ID} --zone ${ZONE}\n```When your queued resource is in the ACTIVE state, the output will be similar to the following:```\n state: ACTIVE\n```\n- Connect to your TPU using SSH```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \u00a0\\\u00a0 --project ${PROJECT_ID} \\\u00a0 --zone ${ZONE}\n```\n- Install dependencies **Note:** Once you have used `ssh` to connect to your TPU VM, you need to set the variables ${TPU_NAME} and ${MAXTEXT_OUTPUT_PATH} to use them in the TPU VM:```\nexport TPU_NAME=your_tpu_nameexport MAXTEXT_OUTPUT_PATH=output-path\n``````\ncd maxtext && python3 multihost_runner.py --TPU_PREFIX=${TPU_NAME} \\--COMMAND='bash setup.sh'\n```\n- Run the model with various [configuration scripts](https://github.com/google/maxtext/tree/main/MaxText/configs/experimental) , such as 32b.sh, 64b.sh. If you are running the script from a TPU VM, you need to add the flag `--INTERNAL_IP=true` .```\npython3 multihost_runner.py --TPU_PREFIX=${TPU_NAME} \\--COMMAND=\"bash MaxText/configs/experimental/64b.sh RUN_NAME=${RUN_NAME}OUTPUT_PATH=${MAXTEXT_OUTPUT_PATH} PLATFORM=gce\"\n``` **Note:** Set `--TPU_PREFIX=${NODE_PREFIX}` for Multislice instead of `--TPU_PREFIX=${TPU_NAME}` .[Delete your TPU and queued resources](#clean-up) .\nThe MaxText training script was run from 32B to 1160B with bf16 precision. The results of these runs are shown in the following table.\n| No. of params | Accelerator Type | TFLOP/chip/sec | Model flops utilization(MFU) |\n|:----------------|:-------------------|-----------------:|:-------------------------------|\n| 32B    | v5p-128   |    328 | 71.47%       |\n| 64B    | v5p-128   |    323 | 70.31%       |\n| 128B   | v5p-256   |    315 | 68.68%       |\n| 128B   | v5p-512   |    315 | 68.53%       |\n| 256B   | v5p-1024   |    316 | 68.82%       |\n| 512B   | v5p-1024   |    294 | 63.99%       |\n| 1024B   | v5p-2048   |    249 | 64.05%       |\n| 1024B   | v5p-4096   |    297 | 64.80%       |\n| 1160B   | v5p-7680   |    295 | 64.27%       |\n| 1160B   | v5p-12288   |    304 | 66.23%       |\nThe 256B parameter model has been tested on v5p-512 and v5p-1024 using both bf16 and int8 weights. The following table displays the results of these tests.\n| 0       | 1  | 2  | 3  | 4  |\n|:-----------------------------|:---------|:---------|:---------|:---------|\n| nan       | v5p-512 | v5p-512 | v5p-1024 | v5p-1024 |\n| Global batch size(tokens) | 5.24E+05 | 5.24E+05 | 1.05E+06 | 1.05E+06 |\n| Precision     | bf16  | int8  | bf16  | int8  |\n| TFLOP/chip/sec    | 307  | 408  | 308  | 414  |\n| Model flops utilization(MFU) | 66.98% | 88.85% | 67.09% | 90.23% |\n### TensorFlow tutorials\nThis tutorial describes how to train ImageNet on a `v5p-8` TPU using a fake dataset. If you want to use a different dataset, refer to [Preparing the dataset](https://github.com/google/flax/blob/main/examples/imagenet/README.md#preparing-the-dataset) .\n**Note:** Before running this tutorial, you need to [set up your Cloud TPU environment](/tpu/docs/setup-gcp-account#set-up-project) .- Create environment variables:```\nexport PROJECT_ID=your-project-IDexport ACCELERATOR_TYPE=v5p-8export ZONE=us-east1-cexport RUNTIME_VERSION=tpu-vm-tf-2.16.1-pjrtexport TPU_NAME=your-tpu-nameexport QUEUED_RESOURCE_ID=your-queued-resource-idexport QUOTA_TYPE=quota-type\n```For this tutorial, use `v5p-8` as the `ACCELERATOR_TYPE` .\n- [Create a TPU resource:](/tpu/docs/queued-resources) ```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\\u00a0 --node-id ${TPU_NAME} \\\u00a0 --project ${PROJECT_ID} \\\u00a0 --zone ${ZONE} \\\u00a0 --accelerator-type ${ACCELERATOR_TYPE} \\\u00a0 --runtime-version ${RUNTIME_VERSION} \\\u00a0 --${QUOTA_TYPE}\n``` **Note:** You will be able to connect to your TPU VM using SSH once your queued resource is in the `ACTIVE` state. To check the state of your queued resource, use the following command:```\ngcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \\\u00a0 --project ${PROJECT_ID} \\\u00a0 --zone ${ZONE}\n```\n- Connect to your TPU using SSH```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \u00a0\\\u00a0 --project ${PROJECT_ID} \\\u00a0 --zone ${ZONE}\n```\n- Set some environment variables```\nexport MODELS_REPO=/usr/share/tpu/modelsexport PYTHONPATH=\"${MODELS_REPO}:${PYTHONPATH}\"export MODEL_DIR=gcp-directory-to-store-modelexport DATA_DIR=gs://cloud-tpu-test-datasets/fake_imagenetexport NEXT_PLUGGABLE_DEVICE_USE_C_API=trueexport TF_PLUGGABLE_DEVICE_LIBRARY_PATH=/lib/libtpu.so\n```\n- Change to the models repository directory and install requirements.```\ncd ${MODELS_REPO} && git checkout r2.15.0pip install -r official/requirements.txt\n```- Run the training script.```\npython3 official/vision/train.py \\\u00a0 --tpu=local \\\u00a0 --experiment=resnet_imagenet \\\u00a0 --mode=train_and_eval \\\u00a0 --config_file=official/vision/configs/experiments/image_classification/imagenet_resnet50_tpu.yaml \\\u00a0 --model_dir=${MODEL_DIR} \\\u00a0 --params_override=\"runtime.distribution_strategy=tpu,task.train_data.input_path=${DATA_DIR}/train*,task.validation_data.input_path=${DATA_DIR}/validation*,task.train_data.global_batch_size=2048,task.validation_data.global_batch_size=2048,trainer.train_steps=100\"\n```[Delete your TPU and queued resources](#clean-up) .\nThis tutorial describes how to train ImageNet on `v5p-16` or larger using a fake dataset. If you want to use a different dataset, see [Preparing the dataset](https://github.com/google/flax/blob/main/examples/imagenet/README.md#preparing-the-dataset) .\n**Note:** Before running this tutorial, you need to [set up your Cloud TPU environment](/tpu/docs/setup-gcp-account#set-up-project) .\n- Create environment variables:```\nexport PROJECT_ID=your_project_IDexport TPU_NAME=your_tpu_nameexport ZONE=us-east1-cexport ACCELERATOR_TYPE=v5p-16export RUNTIME_VERSION=tpu-vm-tf-2.16.1-pod-pjrtexport QUEUED_RESOURCE_ID=your-queued-resource-idexport QUOTA_TYPE=quota-type\n````ACCELERATOR_TYPE` can be either `v5p-16` or larger.\n- [Create a TPU resource:](/tpu/docs/queued-resources) ```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\\u00a0 --node-id ${TPU_NAME} \\\u00a0 --project ${PROJECT_ID} \\\u00a0 --zone ${ZONE} \\\u00a0 --accelerator-type ${ACCELERATOR_TYPE} \\\u00a0 --runtime-version ${RUNTIME_VERSION} \\\u00a0 --${QUOTA_TYPE}\n``` **Note:** You will be able to connect to your TPU VM using SSH once your queued resource is in the `ACTIVE` state.Use the ` [describe](/tpu/docs/queued-resources#retrieve_state_and_diagnostic_information_about_a_queued_resource_request) ` command to query the status of your queued resource:```\ngcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \\\u00a0 --project ${PROJECT_ID} \\\u00a0 --zone ${ZONE}\n```\n- Connect to your TPU (worker zero) using SSH```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \u00a0\\\u00a0 --project ${PROJECT_ID} \\\u00a0 --zone ${ZONE}\n```\n- Set some environment variables```\nexport TPU_NAME=your_tpu_nameexport MODELS_REPO=/usr/share/tpu/modelsexport PYTHONPATH=\"${MODELS_REPO}:${PYTHONPATH}\"export MODEL_DIR=gcp-directory-to-store-modelexport DATA_DIR=gs://cloud-tpu-test-datasets/fake_imagenetexport TPU_LOAD_LIBRARY=0\n```\n- Change to the models repository directory and install requirements.```\ncd $MODELS_REPO && git checkout r2.15.0pip install -r official/requirements.txt\n```- Run the training script.```\npython3 official/vision/train.py \\\u00a0 --tpu=${TPU_NAME} \\\u00a0 --experiment=resnet_imagenet \\\u00a0 --mode=train_and_eval \\\u00a0 --config_file=official/vision/configs/experiments/image_classification/imagenet_resnet50_tpu.yaml \\\u00a0 --model_dir=${MODEL_DIR} \\\u00a0 --params_override=\"runtime.distribution_strategy=tpu,task.train_data.input_path=${DATA_DIR}/train*,task.validation_data.input_path=${DATA_DIR}/validation*,task.train_data.global_batch_size=2048,task.validation_data.global_batch_size=2048,trainer.train_steps=100\"\n```[Delete your TPU and queued resources](#clean-up) .\n## PyTorch/XLA\n### Llama 2\nThis tutorial will cover how to train the Llama 2 7B model on v5p using a fork of the HuggingFace repository on PyTorch/XLA with General and Scalable Parallelization for ML Computation Graphs (GSPMD).- Create variables for project ID, accelerator type, zone, runtime version, and TPU name.```\nexport PROJECT_ID=your_project_IDexport ACCELERATOR_TYPE=v5p-8export ZONE=us-east5-aexport RUNTIME_VERSION=v2-alpha-tpuv5export SERVICE_ACCOUNT=your_service_accountexport TPU_NAME=your_tpu_nameexport QUEUED_RESOURCE_ID=your_queued_resource_idexport QUOTA_TYPE=quota_typeexport VALID_DURATION=1d\n```\n- Create a TPU resource```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\--node-id ${TPU_NAME} \\--project ${PROJECT_ID} \\--zone ${ZONE} \\--accelerator-type ${ACCELERATOR_TYPE} \\--runtime-version ${RUNTIME_VERSION} \\--valid-until-duration ${VALID_DURATION} \\--service-account ${SERVICE_ACCOUNT} \\--${QUOTA_TYPE}\n``` **Note:** The `QUOTA_TYPE` flag can be `reserved` or `best-effort` . `on-demand` is the default quota and does not need to be specified. See [quotas](/tpu/docs/quota#quota_types) for information on the different types of quotas supported by Cloud TPU.You will be able to connect to your TPU VM using SSH once your `QueuedResource` is in the `ACTIVE` state:Use the ` [describe](/tpu/docs/queued-resources#retrieve_state_and_diagnostic_information_about_a_queued_resource_request) ` command to query the status of your queued resource.```\ngcloud alpha compute tpus queued-resources describe ${QUEUED_RESOURCE_ID} \\--project ${PROJECT_ID} \\--zone ${ZONE}\n```When your queued resource is in the ACTIVE state, the output will be similar to the following:```\n state: ACTIVE\n``` **Note:** If you're running in a multislice environment, use `gcloud alpha compute tpus queued-resources ssh --node=all ...` instead of `gcloud compute tpus tpu-vm ssh ...` for your commands. This ensures all your TPU chips are properly utilized.\n- Install Pytorch/XLA and required dependencies.```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \u00a0\\--project ${PROJECT_ID} \\--zone \u00a0${ZONE} \\--worker=all \\--command='sudo apt-get updatesudo apt-get install libopenblas-dev -ypip3 install numpypip3 install typing-extensionspip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 -f https://storage.googleapis.com/libtpu-releases/index.html'\n```\n- Download the HuggingFace repository and install requirements.```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME}--project=${PROJECT_ID} \\--zone=${ZONE} \\--worker=all \\--command='git clone -b llama2-google-next-training https://github.com/pytorch-tpu/transformers.gitcd transformerspip3 install git+file://$PWDpip3 install datasets accelerate evaluate scikit-learn'\n```\n- Download the 7B model config.```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\--project=${PROJECT_ID} \\--zone=${ZONE} \\--worker=all \\--command=\"curl https://huggingface.co/TheBloke/Llama-2-7B-fp16/raw/main/config.json --output ~/config.json\"\n```\n- Train the model```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\--project=${PROJECT_ID} \\--zone=${ZONE} \\--worker=all \\--command='export PJRT_DEVICE=TPUexport XLA_USE_BF16=1export XLA_IR_DEBUG=1export XLA_HLO_DEBUG=1export LIBTPU_INIT_ARGS=\"--xla_enable_async_collective_permute=true--xla_tpu_enable_async_collective_fusion_multiple_steps=true--xla_tpu_enable_async_collective_fusion=true--xla_tpu_overlap_compute_collective_tc=true--xla_enable_async_all_gather=true--xla_jf_spmd_threshold_for_windowed_einsum_mib=0\"export PROFILE_EPOCH=0export PROFILE_STEP=3export PROFILE_DURATION_MS=20000export PROFILE_LOGDIR=/tmp/home/cd transformerspython examples/pytorch/language-modeling/run_clm.py \\\u00a0--tokenizer_name hf-internal-testing/llama-tokenizer \\\u00a0--dataset_name wikitext \\\u00a0--dataset_config_name wikitext-2-raw-v1 \\\u00a0--per_device_train_batch_size 96 \\\u00a0--per_device_eval_batch_size 8 \\\u00a0--num_train_epochs 1 \\\u00a0--do_train \\\u00a0--output_dir /tmp/output \\\u00a0--overwrite_output_dir \\\u00a0--config_name ~/config.json \\\u00a0--save_strategy no \\\u00a0--logging_strategy no \\\u00a0--remove_unused_columns no \\\u00a0--optim adafactor \\\u00a0--torch_dtype bfloat16 \\\u00a0--dataloader_drop_last yes \\\u00a0--block_size 2048 \\\u00a0--spmd_2d_sharding 1 \\\u00a0--spmd_grad_chkpt'\n```\nIf you're running in a multislice environment, you need to set the flag `--spmd_dcn_parallelism` to the number of slices.\nThe [SPMD_USER_GUIDE](https://github.com/pytorch-tpu/transformers/blob/llama2-google-next-training/SPMD_USER_GUIDE.md#steps-to-run-hf-llama-2) provides a more in-depth user guide that explains all the different environment variables and toggles of the HF script. To be noted, the LIBTPU_INIT_ARGS will be incorporated into PyTorch/XLA and on by default in future releases.\n[Delete your TPU and queued resources](#clean-up) .\nThroughput for all three Llama 2 model sizes are included in the following table.\n| 0       | 1  | 2  | 3  |\n|:-----------------------------|:-------|:--------|:--------|\n| nan       | v5p-8 | v5p-128 | v5p-128 |\n| Model size     | 7B  | 13B  | 70B  |\n| Global batch size   | 96  | 1024 | 128  |\n| Sharding mesh shape   | (4, 1) | (64, 1) | (16, 4) |\n| Model flops utilization(MFU) | 56.67% | 55.80% | 51.85% |\n## Support and Feedback\nWe welcome all feedback! To share feedback or request support, fill out the [Cloud TPU Support or Feedback form](https://forms.gle/pLFRKSdWZ97o2o867) .", "guide": "Cloud TPU"}