{"title": "Cloud TPU - Run PyTorch code on TPU Pod slices", "url": "https://cloud.google.com/tpu/docs/pytorch-pods", "abstract": "# Cloud TPU - Run PyTorch code on TPU Pod slices\n# Run PyTorch code on TPU Pod slices\nPyTorch/XLA requires all TPU VMs to be able to access the model code and data. You can use a [startup script](#create-tpu-vm) to download the software needed to distribute the model data to all TPU VMs.\nIf you are connecting your TPU VMs to a [Virtual Private Cloud](/vpc/docs/vpc) (VPC) you must add a firewall rule in your project to allow ingress for ports 8470 - 8479. For more information about adding firewall rules, see [Using firewall rules](/firewall/docs/using-firewalls)\n", "content": "## Set up your environment\n- In the Cloud Shell, run the following command to make sure you are running the current version of `gcloud` :```\n$ gcloud components update\n```If you need to install `gcloud` , use the following command:```\n$ sudo apt install -y google-cloud-sdk\n```\n- Create some environment variables:```\n$ export PROJECT_ID=project-id$ export TPU_NAME=tpu-name$ export ZONE=us-central2-b$ export RUNTIME_VERSION=tpu-ubuntu2204-base$ export ACCELERATOR_TYPE=v4-32\n```## Create the TPU VM\n```\n$ gcloud compute tpus tpu-vm create ${TPU_NAME} \\--zone=${ZONE} \\--project=${PROJECT_ID} \\--accelerator-type=${ACCELERATOR_TYPE} \\--version ${RUNTIME_VERSION}\n```\n## Configure and run the training script\n- Add your SSH certificate to your project:```\nssh-add ~/.ssh/google_compute_engine\n```\n- Install PyTorch/XLA on all TPU VM workers```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 --zone=${ZONE} \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --worker=all --command=\"\u00a0 pip install torch~=2.2.0 torch_xla[tpu]~=2.2.0 torchvision -f https://storage.googleapis.com/libtpu-releases/index.html\" \n```\n- Clone XLA on all TPU VM workers```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 --zone=${ZONE} \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --worker=all --command=\"git clone -b r2.2 https://github.com/pytorch/xla.git\"\n```\n- Run the training script on all workers```\ngcloud compute tpus tpu-vm ssh ${TPU_NAME} \\\u00a0 --zone=${ZONE} \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --worker=all \\\u00a0 --command=\"PJRT_DEVICE=TPU python3 ~/xla/test/test_train_mp_imagenet.py \u00a0\\\u00a0 --fake_data \\\u00a0 --model=resnet50 \u00a0\\\u00a0 --num_epochs=1 2>&1 | tee ~/logs.txt\"\u00a0 \n```The training takes about 5 minutes. When it completes, you should see a message similar to the following:```\nEpoch 1 test end 23:49:15, Accuracy=100.00\n10.164.0.11 [0] Max Accuracy: 100.00%\n```## Clean up\nWhen you are done with your TPU VM follow these steps to clean up your resources.\n- Disconnect from the Compute Engine:```\n(vm)$ exit\n```\n- Verify the resources have been deleted by running the following command. Make sure your TPU is no longer listed. The deletion might take several minutes.```\n$ gcloud compute tpus tpu-vm list \\\u00a0 --zone europe-west4-a\n```", "guide": "Cloud TPU"}