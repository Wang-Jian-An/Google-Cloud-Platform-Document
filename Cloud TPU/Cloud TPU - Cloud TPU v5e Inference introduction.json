{"title": "Cloud TPU - Cloud TPU v5e Inference introduction", "url": "https://cloud.google.com/tpu/docs/v5e-inference", "abstract": "# Cloud TPU - Cloud TPU v5e Inference introduction\n# Cloud TPU v5e Inference introduction\nCloud TPU v5e is Google Cloud's latest generation AI accelerator. With a smaller 256-chip footprint per Pod, v5e Pods are optimized for transformer-based, text-to-image and CNN-based training, fine-tuning, and serving.\n", "content": "## Concepts\nIf you are new to Cloud TPUs, check out the [TPU documentation home](/tpu/docs/tpus) .\n### Chips\nThere are 256 chips in a single v5e with 8 chips per host. See [System architecture](/tpu/docs/system-architecture-tpu-vm) for more details.\n### Cores\nTPU chips have one or two TensorCores to run matrix multiplication. Similar to v2 and v3 Pods, v5e has one TensorCore per chip. By contrast, v4 Pods have 2 TensorCores per chip. See [System architecture](/tpu/docs/system-architecture-tpu-vm) for more details on v5e TensorCores. Additional information about TensorCores can be found in this [ACM article](https://dl.acm.org/doi/pdf/10.1145/3360307) .\n### Host\nA host is a physical computer (CPU) that runs VMs. A host can run multiple VMs at once.\n### Batch inference\nBatch or offline inference refers to doing inference outside of production pipelines typically on a bulk of inputs. Batch inference is used for offline tasks such as data labeling and also for evaluating the trained model. Latency SLOs are not a priority for batch inference.\n### Serving\nServing refers to the process of deploying a trained machine learning model to a production environment, where it can be used to make predictions or decisions. Latency SLOs are a priority for serving.\n### Single host versus multi host\nSlices using fewer than 8 chips use at most 1 host. Slices greater than 8 chips, have access to more than a single host and can run distributed training using multiple hosts.\n### Queued resource\nA representation of TPU resources, used to enqueue and manage a request for a single-slice or multi-slice TPU environment. See the [Queued Resources user guide](/tpu/docs/queued-resources) for more information.\n### TPU VM\nA virtual machine running Linux that has access to the underlying TPU's. For v5e TPUs, each TPU VM has direct access to 1, 4, or 8 chips depending on the user-specified accelerator type. A TPU VM is also known as a **worker** .\n## Get started\nContact [Cloud Sales](https://cloud.google.com/contact) to start using Cloud TPU v5e for your AI workloads.\n**Note:** Creating a v5e instance for inference (v5litepod-1, v5litepod-4, v5litepod-8) requires serving quota types: `tpu-v5s-litepod-serving` for on-demand TPUs, `tpu-v5s-litepod-serving-preemptible` for preemptible TPUs, and `tpu-v5s-litepod-serving-reserved` for reserved TPUs.\n### Prepare a Google Cloud project\n- [Sign in](https://accounts.google.com/Login) to your Google Account. If you haven't already, [sign up for a new account](https://accounts.google.com/SignUp) .\n- In the [Cloud Console](https://console.cloud.google.com/) , [select](/resource-manager/docs/creating-managing-projects#get_an_existing_project) or [create](/resource-manager/docs/creating-managing-projects#creating_a_project) a Cloud project from the project selector page.\n- [Billing setup](/billing/docs) is required for all Google Cloud usage so make sure billing is enabled for your project.\n- Install [gcloud alpha components](https://cloud.google.com/sdk/gcloud/reference/components/install) .\n- Enable the TPU API using the following `gcloud` command in Cloud Shell. (You may also enable it [from the Google Cloud Console](https://console.cloud.google.com/apis/library/tpu.googleapis.com) .)```\ngcloud services enable tpu.googleapis.com\n```\n- Enable the TPU service account.Service accounts allow the Cloud TPU service to access other Google Cloud services. A user-managed service account is a [recommended](/run/docs/securing/service-identity) Google Cloud practice. Follow these guides to [create](/iam/docs/service-accounts-create) and [grant](/iam/docs/granting-changing-revoking-access) the following roles to your service account. The following roles are necessary:- TPU Admin\n- Storage Admin\n- Logs Writer\n- Monitoring Metric Writer\n- Configure the project and zone.Your project ID is the name of your project [shown on the Cloudconsole](/resource-manager/docs/creating-managing-projects#identifying_projects) . The default zone for Cloud TPU v5e is `us-west4-a` .```\nexport PROJECT_ID=project-IDexport ZONE=us-west4-agcloud alpha compute tpus tpu-vm service-identity create --zone=${ZONE}gcloud auth logingcloud config set project ${PROJECT}gcloud config set compute/zone ${ZONE}\n```\n- Provision the Cloud TPU v5e environment.A v5e is managed as a [Queued Resource](/tpu/docs/queued-resources) . Capacity can be provisioned using the `queued-resource create` command.Create environment variables for project ID, accelerator type, zone, runtime version, and TPU name.```\nexport PROJECT_ID=project_IDexport ACCELERATOR_TYPE=v5litepod-1export ZONE=us-west4-aexport RUNTIME_VERSION=v2-alpha-tpuv5-liteexport SERVICE_ACCOUNT=service_accountexport TPU_NAME=tpu-nameexport QUEUED_RESOURCE_ID=queued_resource_id\n```\n- Create a TPU resource.```\ngcloud alpha compute tpus queued-resources create ${QUEUED_RESOURCE_ID} \\--node-id ${TPU_NAME} \\--project ${PROJECT_ID} \\--zone ${ZONE} \\--accelerator-type ${ACCELERATOR_TYPE} \\--runtime-version ${RUNTIME_VERSION} \\--service-account ${SERVICE_ACCOUNT} \\--${QUOTA_TYPE}\n``` **Note:** The `QUOTA_TYPE` flag can be either `reserved` or `best-effort` . See [Quota Types](https://cloud.google.com/tpu/docs/quota#quota_types) for information on the different types of quotas supported by Cloud TPU.If you would like to delete the resource you have reserved, you need to delete the resource TPU_NAME first and then also delete the queued resource request.```\ngcloud alpha compute tpus delete $TPU_NAME --zone ${ZONE} --project ${PROJECT_ID}gcloud alpha compute tpus queued-resources delete ${QUEUED_RESOURCE_ID} \\--project ${PROJECT_ID} \\--zone ${ZONE}\n```\n- Connect to your TPU vM using SSHTo run code on your TPU VMs, you need to `SSH` into each TPU VM. In this example, with a v5litepod-1, there is only one TPU VM.```\ngcloud compute config-sshgcloud compute tpus tpu-vm ssh $TPU_NAME --zone $ZONE --project $PROJECT_ID\n```\n### Manage your TPU VMs\nFor all TPU management options for your TPU VMs, see [Managing TPUs](/tpu/docs/managing-tpus-tpu-vm) .\n### Develop and run\nThis section describes the general setup process for custom model inference using JAX or PyTorch on Cloud TPU v5e. TensorFlow support will be enabled soon.\nFor v5e training instructions, refer to the [v5e training guide](/tpu/docs/v5e-training) .\n## Run inference on v5e\nDetails of the inference software stack are covered in the following sections. This document focuses on single-host serving for models trained with JAX, TensorFlow (TF), and PyTorch.\nThe following sections assume that you have already set up your Google Cloud project according to the instructions in [Prepare a Google Cloud project](#prepare-a-project) .\n## JAX model inference and serving\nThe following section walks through the workflow for JAX model inference. There are two paths for JAX inference. This section will cover the production path for JAX models through `jax2tf` and Cloud TPU serving.\n- Use`jax2tf`to convert the model to Cloud TPU 2 and save the model\n- Use the Inference Converter to convert the saved model\n- Use Cloud TPU serving to serve the model\n### Use jax2tf to convert the model and save it\nRefer to [JAX and Cloud TPU interoperation](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md) to convert and save your JAX model to Cloud TPU.\n```\n# Inference functiondef model_jax(params, inputs):\u00a0 return params[0] + params[1] * inputs# Wrap the parameter constants as tf.Variables; this will signal to the model# saving code to save those constants as variables, separate from the# computation graph.params_vars = tf.nest.map_structure(tf.Variable, params)# Build the prediction function by closing over the `params_vars`. If you# instead were to close over `params` your SavedModel would have no variables# and the parameters will be included in the function graph.prediction_tf = lambda inputs: jax2tf.convert(model_jax)(params_vars, inputs)my_model = tf.Module()# Tell the model saver what the variables are.my_model._variables = tf.nest.flatten(params_vars)my_model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)tf.saved_model.save(my_model)\n```\n### Use the Inference Converter to convert the saved model\nThe steps for Inference Converter are described in the [Inference converter guide.](/tpu/docs/v5e-inference-converter)\n### Use Cloud TPU serving\nThe steps for Cloud TPU serving are described in [Cloud TPU serving](#tensorflow-serving) .\n### End-to-end JAX model serving example\nThe following sections show an end-to-end example of serving a JAX model.\nYou need to set up your Docker credentials and pull the Inference Converter and Cloud TPU Serving Docker image. If you have not already done so, run the following commands:\n```\nsudo usermod -a -G docker ${USER}newgrp dockergcloud auth configure-docker \\\u00a0 \u00a0 us-docker.pkg.devdocker pull us-docker.pkg.dev/cloud-tpu-images/inference/tpu-inference-converter-cli:2.13.0docker pull us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0\n```\nDownload the Demo code: SSH to your TPU VM and install the inference Demo code.\n```\ngsutil -m cp -r \\\u00a0 \"gs://cloud-tpu-inference-public/demo\" \\\u00a0 .\n```\nInstall the JAX demo dependencies On your TPU VM, install `requirements.txt` .\n```\npip install -r ./demo/jax/requirements.txt\n```\n### Run JAX BERT end-to-end serving demo\nThe [pretrained BERT model](https://huggingface.co/bert-base-uncased) is from Hugging Face.\n**Note:** The following code can also support TF2 Saved model exported from JAX models. PyTorch Saved model exports are not supported.\n- Export a TPU-compatible TF2 saved model from a Flax BERT model:```\ncd demo/jax/bert\n``````\npython3 export_bert_model.py\n```\n- Launch the Cloud TPU model server container for the model:```\ndocker run -t --rm --privileged -d \\\u00a0-p 8500:8500 -p 8501:8501 \\\u00a0--mount type=bind,source=/tmp/jax/bert_tpu,target=/models/bert \\\u00a0-e MODEL_NAME=bert \\\u00a0us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0\n```Check the model server container log and make sure the gRPC and Http Server is up:```\nCONTAINER_ID=$(docker ps | grep \"tf-serving-tpu\" | awk '{print $1}')docker logs ${CONTAINER_ID}\n```If you see the log ending with the following information, it means the server is ready to serve requests. It takes around 30 seconds.```\n2023-04-08 00:43:10.481682: I tensorflow_serving/model_servers/server.cc:409] Running gRPC ModelServer at 0.0.0.0:8500 ...[warn] getaddrinfo: address family for nodename not supported2023-04-08 00:43:10.520578: I tensorflow_serving/model_servers/server.cc:430] Exporting HTTP/REST API at:localhost:8501 ...[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n```\n- Send the request to the model server.```\npython3 bert_request.py\n```The output will be similar to the following:```\nFor input \"The capital of France is [MASK].\", the result is \". the capital of france is paris..\"For input \"Hello my name [MASK] Jhon, how can I [MASK] you?\", the result is \". hello my name is jhon, how can i help you?.\"\n```\n- Clean up.Make sure to clean up the Docker container before running other demos.```\nCONTAINER_ID=$(docker ps | grep \"tf-serving-tpu\" | awk '{print $1}')docker stop ${CONTAINER_ID}\n```Clean up the model artifacts:```\nsudo rm -rf /tmp/jax/\n```\n### Run JAX Stable Diffusion end-to-end serving demo\nThe [pretrained Stable Diffusion model](https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/bf16) is from Hugging Face.\n- Export TPU-compatible TF2 saved model from Flax Stable Diffusion model: **Note:** The export takes about 3 minutes.```\ncd demo/jax/stable_diffusion\n``````\npython3 export_stable_diffusion_model.py\n```\n- Launch the Cloud TPU model server container for the model:```\ndocker run -t --rm --privileged -d \\\u00a0-p 8500:8500 -p 8501:8501 \\\u00a0--mount type=bind,source=/tmp/jax/stable_diffusion_tpu,target=/models/stable_diffusion \\\u00a0-e MODEL_NAME=stable_diffusion \\\u00a0us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0\n```Check the model server container log and make sure the gRPC and Http Server is up:```\nCONTAINER_ID=$(docker ps | grep \"tf-serving-tpu\" | awk '{print $1}')docker logs ${CONTAINER_ID}\n```If you see the log ending with the following information, it means the server is ready to serve requests. It takes around 2 minutes.```\n2023-04-08 00:43:10.481682: I tensorflow_serving/model_servers/server.cc:409] Running gRPC ModelServer at 0.0.0.0:8500 ...[warn] getaddrinfo: address family for nodename not supported2023-04-08 00:43:10.520578: I tensorflow_serving/model_servers/server.cc:430] Exporting HTTP/REST API at:localhost:8501 ...[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n```\n- Send the request to the model server.```\npython3 stable_diffusion_request.py\n```The prompt is \"Painting of a squirrel skating in New York\" and the output image will be saved as `stable_diffusion_images.jpg` in your current directory.\n- Clean up.Make sure to clean up the Docker container before running other demos.```\nCONTAINER_ID=$(docker ps | grep \"tf-serving-tpu\" | awk '{print $1}')docker stop ${CONTAINER_ID}\n```Clean up the model artifacts```\nsudo rm -rf /tmp/jax/\n``` **Note:** This is a reference example only and is not a performance-optimized version of the model that can be used for benchmarking or production use cases.\n### Cloud TPU model inference and serving\nThe following sections walk through the workflow for Cloud TPU model inference.\n- Use the Inference Converter to convert the model\n- Use Cloud TPU serving to serve the model\n### Inference Converter\nCloud TPU Inference Converter prepares and optimizes a model exported from TensorFlow or JAX for TPU inference. The converter runs in a local shell or in the TPU VM shell. The TPU VM shell is recommended because it comes preinstalled with the command line tools needed for the converter. For more details on the Inference Converter refer to the [Inference Converter User Guide](/tpu/docs/v5e-inference-converter) .\n### Prerequisites\n- The model must be exported from TensorFlow or JAX in the [SavedModel](https://www.tensorflow.org/guide/saved_model) format.\n- The model must have a function alias for the TPU function. See the code examples in [Inference Converter User Guide](/tpu/docs/v5e-inference-converter) for instructions on how to do this. The following examples use `tpu_func` as the TPU function alias.\n- Make sure your machine CPU supports Advanced Vector eXtensions (AVX) instructions, as the TensorFlow library (the dependency of the Cloud TPU Inference Converter) is compiled to use AVX instructions. [Most CPUs](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX) have the AVX support.- You can run`lscpu | grep avx`to check whether the AVX instruction set is supported.\n### Get started\n- Set up TPU VM Environment Set up the environment using the following steps depending on the shell you are using:\n- In your TPU VM shell, run the following commands to allow non-root docker usage:\n```\nsudo usermod -a -G docker ${USER}newgrp docker\n```- Initialize your Docker Credential helpers:\n```\ngcloud auth configure-docker \\\u00a0 us-docker.pkg.dev\n```\nIn your local shell, set up the environment using the following steps:- Install the [Cloud SDK](https://cloud.google.com/sdk/install) , which includes the `gcloud` command-line tool.\n- Install [Docker](https://docs.docker.com/engine/install/) :\n- Allow non-root Docker usage:```\nsudo usermod -a -G docker ${USER}newgrp docker\n```\n- Login in to your environment:```\ngcloud auth login\n```\n- Initialize your Docker Credential helpers:```\ngcloud auth configure-docker \\us-docker.pkg.dev\n```- Pull the Inference Converter Docker image:```\n\u00a0 CONVERTER_IMAGE=us-docker.pkg.dev/cloud-tpu-images/inference/tpu-inference-converter-cli:2.13.0\u00a0 docker pull ${CONVERTER_IMAGE}\u00a0 \n```\n### Converter image\nThe image is for doing one-time model conversions. Set the model paths and adjust the [converter options](/tpu/docs/v5e-inference-converter#converter-options) to fit your needs. The Usage Examples section in the [Inference Converter User Guide](/tpu/docs/v5e-inference-converter) provides several common use cases.\n```\ndocker run \\--mount type=bind,source=${MODEL_PATH},target=/tmp/input,readonly \\--mount type=bind,source=${CONVERTED_MODEL_PATH},target=/tmp/output \\${CONVERTER_IMAGE} \\--input_model_dir=/tmp/input \\--output_model_dir=/tmp/output \\--converter_options_string='\u00a0 \u00a0 tpu_functions {\u00a0 \u00a0 \u00a0 function_alias: \"tpu_func\"\u00a0 \u00a0 }\u00a0 \u00a0 batch_options {\u00a0 \u00a0 \u00a0 num_batch_threads: 2\u00a0 \u00a0 \u00a0 max_batch_size: 8\u00a0 \u00a0 \u00a0 batch_timeout_micros: 5000\u00a0 \u00a0 \u00a0 allowed_batch_sizes: 2\u00a0 \u00a0 \u00a0 allowed_batch_sizes: 4\u00a0 \u00a0 \u00a0 allowed_batch_sizes: 8\u00a0 \u00a0 \u00a0 max_enqueued_batches: 10\u00a0 \u00a0 }'\n```\nThe following section shows how to run this model with a TensorFlow model Server.\n### TensorFlow serving\nThe following instructions demonstrate how you can serve your TensorFlow model on TPU VMs.\nPrerequisites:\nSet up your [Docker credentials](#getting-started-serving) , if you have not already done so:\n- Download the TensorFlow Serving Docker Image for your TPU VM.Set sample environment variables```\nexport YOUR_LOCAL_MODEL_PATH=model-pathexport MODEL_NAME=model-name# Note: this image name may change later.export IMAGE_NAME=us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0\n```Download the Docker image```\ndocker pull ${IMAGE_NAME}\n```\n- Serve your TensorFlow model using the TensorFlow Serving Docker Image on your TPU VM.```\n# PORT 8500 is for gRPC model server and 8501 is for HTTP/REST model server.docker run -t --rm --privileged -d \\\u00a0-p 8500:8500 -p 8501:8501 \\\u00a0--mount type=bind,source=${YOUR_LOCAL_MODEL_PATH},target=/models/${MODEL_NAME} \\\u00a0-e MODEL_NAME=${MODEL_NAME} \\\u00a0${IMAGE_NAME}\n```\n- Follow the Serving Client API to query your model.- [REST Client API](https://www.tensorflow.org/tfx/serving/api_rest) \n- [gRPC Client API](https://github.com/tensorflow/serving/tree/master/tensorflow_serving/apis) \nPrerequisite: Make sure you already set up the Docker credentials and pulled the Inference Converter and TensorFlow Serving Docker image. If not, run the following commands:\n```\nsudo usermod -a -G docker ${USER}newgrp dockergcloud auth configure-docker \\\u00a0 \u00a0 us-docker.pkg.devdocker pull us-docker.pkg.dev/cloud-tpu-images/inference/tpu-inference-converter-cli:2.13.0docker pull us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0\n```\nDownload the Demo code:\n```\ngsutil -m cp -r \\\u00a0 \"gs://cloud-tpu-inference-public/demo\" \\\u00a0 .\n```\nInstall the TensorFlow demo dependencies:\n```\npip install -r ./demo/tf/requirements.txt\n```- Export a TPU-compatible TF2 saved model from the Keras ResNet-50 model.```\ncd demo/tf/resnet-50\n``````\npython3 export_resnet_model.py\n```\n- Launch the TensorFlow model server container for the model.```\ndocker run -t --rm --privileged -d \\\u00a0-p 8500:8500 -p 8501:8501 \\\u00a0--mount type=bind,source=/tmp/tf/resnet_tpu,target=/models/resnet \\\u00a0-e MODEL_NAME=resnet \\\u00a0us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0\n```Check the model server container log and make sure the gRPC and Http Server is up:```\nCONTAINER_ID=$(docker ps | grep \"tf-serving-tpu\" | awk '{print $1}')docker logs ${CONTAINER_ID}\n```If you see the log ending with the following information, it means the server is ready to serve requests. It takes around 30 seconds.```\n2023-04-08 00:43:10.481682: I tensorflow_serving/model_servers/server.cc:409] Running gRPC ModelServer at 0.0.0.0:8500 ...[warn] getaddrinfo: address family for nodename not supported2023-04-08 00:43:10.520578: I tensorflow_serving/model_servers/server.cc:430] Exporting HTTP/REST API at:localhost:8501 ...[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n```\n- Send the request to the model server.The request image is a banana from https://i.imgur.com/j9xCCzn.jpeg .```\npython3 resnet_request.py\n```The output will be similar to the following:```\nPredict result: [[('n07753592', 'banana', 0.94921875), ('n03532672', 'hook', 0.022338867), ('n07749582', 'lemon', 0.005126953)]]\n```\n- Clean up.Make sure to clean up the Docker container before running other demos.```\nCONTAINER_ID=$(docker ps | grep \"tf-serving-tpu\" | awk '{print $1}')docker stop ${CONTAINER_ID}\n```Clean up the model artifacts:```\nsudo rm -rf /tmp/tf/\n```## PyTorch model inference and serving\nThe following sections walk through the workflow for PyTorch Model Inference:\n- Write a Python model handler for loading and inferencing using TorchDynamo and PyTorch/XLA\n- Use TorchModelArchiver to create a model archive\n- Use TorchServe to serve the model\n### TorchDynamo and PyTorch/XLA\n[TorchDynamo](https://github.com/pytorch/torchdynamo) (Dynamo) is a Python-level JIT compiler designed to make unmodified PyTorch programs faster. It provides a clean API for compiler backends to hook into. Its biggest feature is to dynamically modify Python bytecode just before execution. In the PyTorch/XLA 2.0 release, an experimental backend for Dynamo is provided for both inference and training.\nDynamo provides a [Torch FX](https://pytorch.org/docs/stable/fx.html) (FX) graph when it recognizes a model pattern and PyTorch/XLA uses a Lazy Tensor approach to compile the FX graph and return the compiled function. To get more insight regarding the technical details about PyTorch/XLA's dynamo implementation, see the [Pytorch Dev Discussions post](https://dev-discuss.pytorch.org/t/torchdynamo-update-10-integrating-with-pytorch-xla-for-inference-and-training/935) dev-discuss post and the [TorchDynamo documentation](https://github.com/pytorch/xla/blob/r2.0/docs/dynamo.md) . See this [blog](https://pytorch.org/blog/pytorch-2.0-xla/) for more details.\nHere is a small code example of running densenet161 inference with `torch.compile` .\n```\nimport torchimport torchvisionimport torch_xla.core.xla_model as xmdef eval_model(loader):\u00a0 device = xm.xla_device()\u00a0 xla_densenet161 = torchvision.models.densenet161().to(device)\u00a0 xla_densenet161.eval()\u00a0 dynamo_densenet161 = torch.compile(\u00a0 \u00a0 \u00a0 xla_densenet161, backend='torchxla_trace_once')\u00a0 for data, _ in loader:\u00a0 \u00a0 output = dynamo_densenet161(data)\n```\n### TorchServe\nThe Cloud TPU TorchServe Docker Image lets you to serve the PyTorch eager mode model using TorchServe on a TPU VM.\nYou can use the provided `torchserve-tpu` Docker image that is ready for serving your archived pytorch model on a TPU VM.\nSet up authentication for Docker:\n```\nsudo usermod -a -G docker ${USER}newgrp dockergcloud auth configure-docker \\\u00a0 \u00a0 us-docker.pkg.dev\n```\nPull the Cloud TPU TorchServe Docker image to your TPU VM:\n```\nCLOUD_TPU_TORCHSERVE_IMAGE_URL=us-docker.pkg.dev/cloud-tpu-images/inference/torchserve-tpu:v0.9.0-2.1docker pull ${CLOUD_TPU_TORCHSERVE_IMAGE_URL}\n```\nTo get started, you need to provide a model handler, which instructs the TorchServe model server worker to load your model, process the input data and run inference. You can use the [TorchServe default inference handlers](https://pytorch.org/serve/default_handlers.html) ( [source](https://github.com/pytorch/serve/tree/master/ts/torch_handler) ), or develop your own custom model handler following the [base_handler.py](https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py) . You may also need to provide the trained model, and the model definition file.\n**Note:** All TorchServe default model handlers support PyTorch/XLA\nIn the following Densenet 161 example, we use model artifacts and the default image classifier handler provided by TorchServe:\n- [model.py](https://github.com/pytorch/serve/tree/master/examples/image_classifier/densenet_161/model.py) \n- [index_to_name.json](https://github.com/pytorch/serve/tree/master/examples/image_classifier/index_to_name.json) \n- [image_classifier.py](https://github.com/pytorch/serve/tree/master/ts/torch_handler/image_classifier.py) \nThe following example shows the work directory.\n```\nCWD=\"$(pwd)\"WORKDIR=\"${CWD}/densenet_161\"mkdir -p ${WORKDIR}/model-storemkdir -p ${WORKDIR}/logs\n```\n- Download and copy model artifacts from the TorchServe image classifier example:```\ngit clone https://github.com/pytorch/serve.gitcp ${CWD}/serve/examples/image_classifier/densenet_161/model.py ${WORKDIR}cp ${CWD}/serve/examples/image_classifier/index_to_name.json ${WORKDIR}\n```\n- Download the model weights:```\nwget https://download.pytorch.org/models/densenet161-8d451a50.pth -O densenet161-8d451a50.pthmv densenet161-8d451a50.pth ${WORKDIR}\n```\n- Create a TorchServe model config file to use the Dynamo backend:```\necho 'pt2: \"torchxla_trace_once\"' >> ${WORKDIR}/model_config.yaml\n```You should see the files and directories shown in the following example:```\n>> ls ${WORKDIR}model_config.yamlindex_to_name.jsonlogsmodel.pydensenet161-8d451a50.pthmodel-store\n```To serve your PyTorch model with Cloud TPU TorchServe, you need to package your model handler and all your model artifacts into a model archive file `(*.mar)` using [Torch Model Archiver](https://github.com/pytorch/serve/blob/master/model-archiver/README.md) .\nGenerate a model archive file with torch-model-archiver:\n```\nMODEL_NAME=Densenet161docker run \\\u00a0 \u00a0 --privileged \u00a0\\\u00a0 \u00a0 --shm-size 16G \\\u00a0 \u00a0 --name torch-model-archiver \\\u00a0 \u00a0 -it \\\u00a0 \u00a0 -d \\\u00a0 \u00a0 --rm \\\u00a0 \u00a0 --mount type=bind,source=${WORKDIR},target=/home/model-server/ \\\u00a0 \u00a0 ${CLOUD_TPU_TORCHSERVE_IMAGE_URL} \\\u00a0 \u00a0 torch-model-archiver \\\u00a0 \u00a0 \u00a0 \u00a0 --model-name ${MODEL_NAME} \\\u00a0 \u00a0 \u00a0 \u00a0 --version 1.0 \\\u00a0 \u00a0 \u00a0 \u00a0 --model-file model.py \\\u00a0 \u00a0 \u00a0 \u00a0 --serialized-file densenet161-8d451a50.pth \\\u00a0 \u00a0 \u00a0 \u00a0 --handler image_classifier \\\u00a0 \u00a0 \u00a0 \u00a0 --export-path model-store \\\u00a0 \u00a0 \u00a0 \u00a0 --extra-files index_to_name.json \\\u00a0 \u00a0 \u00a0 \u00a0 --config-file model_config.yaml\n```\nYou should see the model archive file generated in the model-store directory:\n```\n>> ls ${WORKDIR}/model-storeDensenet161.mar\n```\nNow you have the model archive file, you can start the TorchServe model server and serve inference requests.\n- Start the TorchServe model server:```\ndocker run \\\u00a0 \u00a0--privileged \u00a0\\\u00a0 \u00a0--shm-size 16G \\\u00a0 \u00a0--name torchserve-tpu \\\u00a0 \u00a0-it \\\u00a0 \u00a0-d \\\u00a0 \u00a0--rm \\\u00a0 \u00a0-p 7070:7070 \\\u00a0 \u00a0-p 7071:7071 \\\u00a0 \u00a0-p 8080:8080 \\\u00a0 \u00a0-p 8081:8081 \\\u00a0 \u00a0-p 8082:8082 \\\u00a0 \u00a0-p 9001:9001 \\\u00a0 \u00a0-p 9012:9012 \\\u00a0 \u00a0--mount type=bind,source=${WORKDIR}/model-store,target=/home/model-server/model-store \\\u00a0 \u00a0--mount type=bind,source=${WORKDIR}/logs,target=/home/model-server/logs \\\u00a0 \u00a0${CLOUD_TPU_TORCHSERVE_IMAGE_URL} \\\u00a0 \u00a0torchserve \\\u00a0 \u00a0 \u00a0 \u00a0--start \\\u00a0 \u00a0 \u00a0 \u00a0--ncs \\\u00a0 \u00a0 \u00a0 \u00a0--models ${MODEL_NAME}.mar \\\u00a0 \u00a0 \u00a0 \u00a0--ts-config /home/model-server/config.properties\n```\n- Query model server health:```\ncurl http://localhost:8080/ping\n```If the model server is up and running, you will see:```\n{\u00a0\"status\": \"Healthy\"}\n```To query the default versions of the current registered model use:```\ncurl http://localhost:8081/models\n```You should see the registered model:```\n{\u00a0\"models\": [\u00a0 \u00a0{\u00a0 \u00a0 \u00a0\"modelName\": \"Densenet161\",\u00a0 \u00a0 \u00a0\"modelUrl\": \"Densenet161.mar\"\u00a0 \u00a0}\u00a0]}\n```To download an image for inference use:```\ncurl -O https://raw.githubusercontent.com/pytorch/serve/master/docs/images/kitten_small.jpgmv kitten_small.jpg ${WORKDIR}\n```To send an inference request to the model server use:```\ncurl http://localhost:8080/predictions/${MODEL_NAME} -T ${WORKDIR}/kitten_small.jpg\n```You should see a response similar to the following:```\n{\u00a0\"tabby\": 0.47878125309944153,\u00a0\"lynx\": 0.20393909513950348,\u00a0\"tiger_cat\": 0.16572578251361847,\u00a0\"tiger\": 0.061157409101724625,\u00a0\"Egyptian_cat\": 0.04997897148132324}\n```\n- Model server logsUse the following commands to access the logs:```\nls ${WORKDIR}/logs/cat ${WORKDIR}/logs/model_log.log\n```You should see the following message in your log:```\n\"Compiled model with backend torchxla\\_trace\\_once\"\n```Stop the Docker container:\n```\nrm -rf serverm -rf ${WORKDIR}docker stop torch-model-archiverdocker stop torchserve-tpu\n```\n## Profiling\nAfter setting up the inference, profilers can be used to analyze the performance and TPU utilization. See the following documents for more information about profiling:\n- [Profiling on Cloud TPU](/tpu/docs/cloud-tpu-tools) \n- [TensorFlow profiling](https://www.tensorflow.org/guide/profiler) \n- [PyTorch profiling](/tpu/docs/pytorch-xla-performance-profiling-tpu-vm) \n- [JAX profiling](https://jax.readthedocs.io/en/latest/profiling.html#profiling-jax-programs) ## Support and feedback\nWe welcome all feedback! To share feedback or request support, reach out to us at the [Cloud TPU feedback form](https://forms.gle/pLFRKSdWZ97o2o867) or by emailing [cloudtpu-support@google.com](mailto:cloudtpu-support@google.com)\n## Terms\nAll information Google has provided to you regarding this software release is Google's confidential information and subject to the confidentiality provisions in the [Google Cloud Platform Terms ofService](https://cloud.google.com/terms) (or other agreement governing your use of Google Cloud Platform).", "guide": "Cloud TPU"}