{"title": "Cloud TPU - Cloud TPU v5e Inference introduction", "url": "https://cloud.google.com/tpu/docs/v5e-inference", "abstract": "# Cloud TPU - Cloud TPU v5e Inference introduction\n# Cloud TPU v5e Inference introduction\n**Note:** If you are new to Cloud TPUs, we recommend reading [Introduction to Cloud TPU](/tpu/docs/intro-to-tpu) .\n", "content": "## Overview and benefits\nCloud TPU v5e is a Google-developed AI accelerator optimized for transformer-based, text-to-image and CNN-based training, fine-tuning, and serving (inference). TPU v5e slices can contain up to 256 chips.\nServing refers to the process of deploying a trained machine learning model to a production environment, where it can be used for inference. Latency SLOs are a priority for serving.\nThis document discusses serving a model on a TPU. TPU slices with 8 or less chips have one TPU VM or host and are called TPUs.\n## Get started\nYou will need quota for v5e TPUs. On-demand TPUs require `tpu-v5s-litepod-serving` quota. Reserved TPUs require `tpu-v5s-litepod-serving-reserved` quota. For more information, contact [Cloud Sales](https://cloud.google.com/contact) .\nYou will need a Google Cloud account and project to use Cloud TPU. For more information, see [Set up a Cloud TPU environment](/tpu/docs/setup-gcp-account)\n**Note:** To use TPU v5e, you must install the [gcloud alpha components](https://cloud.google.com/sdk/gcloud/reference/components/install) .\nYou provision v5e TPUs using [Queued resources](/tpu/docs/queued-resources) . For more information on available v5e configurations for serving, see [Cloud TPU v5e types for serving](/tpu/docs/v5e#v5e-types-for-serving) .\n## Cloud TPU model inference and serving\nHow you serve a model for inference depends on the ML framework your model was written with. TPU v5e supports serving models written in JAX, TensorFlow, and PyTorch.\n## JAX model inference and serving\nTo serve a model on a TPU VM, you need to:\n- Serialize your model in TensorFlow [SavedModel](https://www.tensorflow.org/guide/saved_model) format\n- Use the Inference Converter to prepare the saved model for serving\n- Use TensorFlow Serving to serve the model\n### SavedModel format\nA SavedModel contains a complete TensorFlow program, including trained parameters and computation. It does not require the original model building code to run.\nIf your model was written in JAX, you will need to use `jax2tf` to serialize your model in the SavedModel format.\n### Inference Converter\nCloud TPU Inference Converter prepares and optimizes a model exported in [SavedModel](https://www.tensorflow.org/guide/saved_model) format for TPU inference. You can run the inference converter in a local shell or your TPU VM. We recommend using your TPU VM shell because it has all the command line tools needed for running the converter. For more information about the Inference Converter, see the [Inference Converter User Guide](/tpu/docs/v5e-inference-converter) .- Your model must be exported from TensorFlow or JAX in the [SavedModel](https://www.tensorflow.org/guide/saved_model) format.\n- You must define a function alias for the TPU function. For more information, see the [Inference Converter User Guide](/tpu/docs/v5e-inference-converter) . The examples in this guide use `tpu_func` as the TPU function alias.\n- Make sure your machine CPU supports Advanced Vector eXtensions (AVX) instructions, as the TensorFlow library (the dependency of the Cloud TPU Inference Converter) is compiled to use AVX instructions. [Most CPUs](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions#CPUs_with_AVX) have the AVX support. **Note:** You can run `lscpu | grep avx` to check whether the AVX instruction set is supported.\n### JAX model inference and serving\nThis section describes how to serve JAX models using `jax2tf` and TensorFlow Serving.\n- Use`jax2tf`to serialize your model into the SavedModel format\n- Use the Inference Converter to prepare your saved model for serving\n- Use TensorFlow Serving to serve the modelThe following Python function shows how to use `jax2tf` within your model code:\n```\n# Inference functiondef model_jax(params, inputs):\u00a0 return params[0] + params[1] * inputs# Wrap the parameter constants as tf.Variables; this will signal to the model# saving code to save those constants as variables, separate from the# computation graph.params_vars = tf.nest.map_structure(tf.Variable, params)# Build the prediction function by closing over the `params_vars`. If you# instead were to close over `params` your SavedModel would have no variables# and the parameters will be included in the function graph.prediction_tf = lambda inputs: jax2tf.convert(model_jax)(params_vars, inputs)my_model = tf.Module()# Tell the model saver what the variables are.my_model._variables = tf.nest.flatten(params_vars)my_model.f = tf.function(prediction_tf, jit_compile=True, autograph=False)tf.saved_model.save(my_model)\n```\nFor more information about `jax2tf` , see [JAX and Cloud TPU interoperation](https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md) .\nInstructions for using the Inference Converter are described in the [Inference converter guide.](/tpu/docs/v5e-inference-converter)\nInstructions for using TensorFlow Serving are described in [TensorFlow serving](#tensorflow-serving) .\n- Set up your Docker credentials and pull the Inference Converter and Cloud TPU Serving Docker image:```\nsudo usermod -a -G docker ${USER}newgrp dockergcloud auth configure-docker \\\u00a0 \u00a0us-docker.pkg.devdocker pull us-docker.pkg.dev/cloud-tpu-images/inference/tpu-inference-converter-cli:2.13.0docker pull us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0\n```\n- Connect to your TPU VM with SSH and install the inference demo code:```\ngsutil -m cp -r \\\"gs://cloud-tpu-inference-public/demo\" \\.\n```\n- Install the JAX demo dependencies:```\npip install -r ./demo/jax/requirements.txt\n```You can download the [pretrained BERT model](https://huggingface.co/bert-base-uncased) from Hugging Face.\n**Note:** PyTorch saved model exports are not supported.\n- Export a TPU-compatible TensorFlow saved model from a Flax BERT model:```\ncd demo/jax/bertpython3 export_bert_model.py\n```\n- Start the Cloud TPU model server container:```\ndocker run -t --rm --privileged -d \\\u00a0 -p 8500:8500 -p 8501:8501 \\\u00a0 --mount type=bind,source=/tmp/jax/bert_tpu,target=/models/bert \\\u00a0 -e MODEL_NAME=bert \\\u00a0 us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0\n```About 30 seconds after the container is started, check the model server container log and make sure the gRPC and HTTP servers are up:```\nCONTAINER_ID=$(docker ps | grep \"tf-serving-tpu\" | awk '{print $1}')docker logs ${CONTAINER_ID}\n```If you see a log entry ending with the following information, the server is ready to serve requests.```\n2023-04-08 00:43:10.481682: I tensorflow_serving/model_servers/server.cc:409] Running gRPC ModelServer at 0.0.0.0:8500 ...[warn] getaddrinfo: address family for nodename not supported2023-04-08 00:43:10.520578: I tensorflow_serving/model_servers/server.cc:430] Exporting HTTP/REST API at:localhost:8501 ...[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n```\n- Send an inference request to the model server.```\npython3 bert_request.py\n```The output will be similar to the following:```\nFor input \"The capital of France is [MASK].\", the result is \". the capital of france is paris..\"For input \"Hello my name [MASK] Jhon, how can I [MASK] you?\", the result is \". hello my name is jhon, how can i help you?.\"\n```\n- Clean up.Make sure to clean up the Docker container before running other demos.```\nCONTAINER_ID=$(docker ps | grep \"tf-serving-tpu\" | awk '{print $1}')docker stop ${CONTAINER_ID}\n```Clean up the model artifacts:```\nsudo rm -rf /tmp/jax/\n```You can download [pretrained Stable Diffusion model](https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/bf16) from Hugging Face.\n- Download the Stable Diffusion model in a TPU-compatible TF2 saved model format: **Note:** The download takes about 3 minutes.```\ncd demo/jax/stable_diffusionpython3 export_stable_diffusion_model.py\n```\n- Start the Cloud TPU model server container for the model:```\ndocker run -t --rm --privileged -d \\\u00a0 -p 8500:8500 -p 8501:8501 \\\u00a0 --mount type=bind,source=/tmp/jax/stable_diffusion_tpu,target=/models/stable_diffusion \\\u00a0 -e MODEL_NAME=stable_diffusion \\\u00a0 us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0\n```After about two minutes, check the model server container log to make sure the gRPC and HTTP servers are running:```\nCONTAINER_ID=$(docker ps | grep \"tf-serving-tpu\" | awk '{print $1}')docker logs ${CONTAINER_ID}\n```If you see the log ending with the following information, it means the servers are ready to serve requests.```\n2023-04-08 00:43:10.481682: I tensorflow_serving/model_servers/server.cc:409] Running gRPC ModelServer at 0.0.0.0:8500 ...[warn] getaddrinfo: address family for nodename not supported2023-04-08 00:43:10.520578: I tensorflow_serving/model_servers/server.cc:430] Exporting HTTP/REST API at:localhost:8501 ...[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n```\n- Send a request to the model server.```\npython3 stable_diffusion_request.py\n```This script sends \"Painting of a squirrel skating in New York\" as the prompt. The output image will be saved as `stable_diffusion_images.jpg` in your current directory.\n- Clean up.Make sure to clean up the Docker container before running other demos.```\nCONTAINER_ID=$(docker ps | grep \"tf-serving-tpu\" | awk '{print $1}')docker stop ${CONTAINER_ID}\n```Clean up the model artifacts```\nsudo rm -rf /tmp/jax/\n```## TensorFlow Serving\nThe following instructions demonstrate how you can serve your TensorFlow model on TPU VMs.\n### TensorFlow serving workflow\n- Download the TensorFlow Serving Docker image for your TPU VM.Set sample environment variables```\nexport YOUR_LOCAL_MODEL_PATH=model-pathexport MODEL_NAME=model-name# Note: this image name may change later.export IMAGE_NAME=us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0\n```Download the Docker image```\ndocker pull ${IMAGE_NAME}\n```\n- Set up the Docker credentials and pull the Inference Converter and TensorFlow Serving Docker image.```\nsudo usermod -a -G docker ${USER}newgrp dockergcloud auth configure-docker \\\u00a0 \u00a0us-docker.pkg.devdocker pull us-docker.pkg.dev/cloud-tpu-images/inference/tpu-inference-converter-cli:2.13.0docker pull us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0\n```\n- Download the demo code:```\ngsutil -m cp -r \\\"gs://cloud-tpu-inference-public/demo\" \\.\n```\n- Install the TensorFlow demo dependencies:```\npip install -r ./demo/tf/requirements.txt\n```\n- Serve your TensorFlow model using the TensorFlow Serving Docker image on your TPU VM.```\n# PORT 8500 is for gRPC model server and 8501 is for HTTP/REST model server.docker run -t --rm --privileged -d \\\u00a0 -p 8500:8500 -p 8501:8501 \\\u00a0 --mount type=bind,source=${YOUR_LOCAL_MODEL_PATH},target=/models/${MODEL_NAME} \\\u00a0 -e MODEL_NAME=${MODEL_NAME} \\\u00a0 ${IMAGE_NAME}\n```\n- Use the Serving Client API to query your model.- [REST Client API](https://www.tensorflow.org/tfx/serving/api_rest) \n- [gRPC Client API](https://github.com/tensorflow/serving/tree/master/tensorflow_serving/apis) \n### Run TensorFlow ResNet-50 Serving demo\n- Export a TPU-compatible TF2 saved model from the Keras ResNet-50 model.```\ncd demo/tf/resnet-50python3 export_resnet_model.py\n```\n- Launch the TensorFlow model server container for the model.```\ndocker run -t --rm --privileged -d \\\u00a0 -p 8500:8500 -p 8501:8501 \\\u00a0 --mount type=bind,source=/tmp/tf/resnet_tpu,target=/models/resnet \\\u00a0 -e MODEL_NAME=resnet \\\u00a0 us-docker.pkg.dev/cloud-tpu-images/inference/tf-serving-tpu:2.13.0\n```Check the model server container log and make sure the gRPC and HTTP Server is up:```\nCONTAINER_ID=$(docker ps | grep \"tf-serving-tpu\" | awk '{print $1}')docker logs ${CONTAINER_ID}\n```If you see the log ending with the following information, it means the server is ready to serve requests. It takes around 30 seconds.```\n2023-04-08 00:43:10.481682: I tensorflow_serving/model_servers/server.cc:409] Running gRPC ModelServer at 0.0.0.0:8500 ...[warn] getaddrinfo: address family for nodename not supported2023-04-08 00:43:10.520578: I tensorflow_serving/model_servers/server.cc:430] Exporting HTTP/REST API at:localhost:8501 ...[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n```\n- Send the request to the model server.The request image is a banana from https://i.imgur.com/j9xCCzn.jpeg .```\npython3 resnet_request.py\n```The output will be similar to the following:```\nPredict result: [[('n07753592', 'banana', 0.94921875), ('n03532672', 'hook', 0.022338867), ('n07749582', 'lemon', 0.005126953)]]\n```\n- Clean up.Make sure to clean up the Docker container before running other demos.```\nCONTAINER_ID=$(docker ps | grep \"tf-serving-tpu\" | awk '{print $1}')docker stop ${CONTAINER_ID}\n```Clean up the model artifacts:```\nsudo rm -rf /tmp/tf/\n```## PyTorch model inference and serving\nFor models written with PyTorch, the workflow is:\n- Write a Python model handler for loading and inferencing using`TorchDynamo`and PyTorch/XLA\n- Use`TorchModelArchiver`to create a model archive\n- Use`TorchServe`to serve the model\n### TorchDynamo and PyTorch/XLA\n[TorchDynamo](https://github.com/pytorch/torchdynamo) (Dynamo) is a Python-level JIT compiler designed to make PyTorch programs faster. It provides a clean API for compiler backends to hook into. It dynamically modifies Python bytecode just before execution. In the PyTorch/XLA 2.0 release, there is an experimental backend for inference and training using Dynamo.\nDynamo provides a [Torch FX](https://pytorch.org/docs/stable/fx.html) (FX) graph when it recognizes a model pattern and PyTorch/XLA uses a lazy tensor approach to compile the FX graph and return the compiled function. For more information about Dynamo, see:\n- [Pytorch Dev Discussions post](https://dev-discuss.pytorch.org/t/torchdynamo-update-10-integrating-with-pytorch-xla-for-inference-and-training/935) \n- [TorchDynamo documentation](https://github.com/pytorch/xla/blob/r2.0/docs/dynamo.md) \n- [PyTorch 2.0 & XLA](https://pytorch.org/blog/pytorch-2.0-xla/) for more details\nHere is a small code example of running densenet161 inference with `torch.compile` .\n```\nimport torchimport torchvisionimport torch_xla.core.xla_model as xmdef eval_model(loader):\u00a0 device = xm.xla_device()\u00a0 xla_densenet161 = torchvision.models.densenet161().to(device)\u00a0 xla_densenet161.eval()\u00a0 dynamo_densenet161 = torch.compile(\u00a0 \u00a0 \u00a0 xla_densenet161, backend='torchxla_trace_once')\u00a0 for data, _ in loader:\u00a0 \u00a0 output = dynamo_densenet161(data)\n```\n### TorchServe\nYou can use the provided `torchserve-tpu` Docker image for serving your archived pytorch model on a TPU VM.\nSet up authentication for Docker:\n```\nsudo usermod -a -G docker ${USER}newgrp dockergcloud auth configure-docker \\\u00a0 \u00a0 us-docker.pkg.dev\n```\nPull the Cloud TPU TorchServe Docker image to your TPU VM:\n```\nCLOUD_TPU_TORCHSERVE_IMAGE_URL=us-docker.pkg.dev/cloud-tpu-images/inference/torchserve-tpu:v0.9.0-2.1docker pull ${CLOUD_TPU_TORCHSERVE_IMAGE_URL}\n```\nTo get started, you need to provide a model handler, which instructs the TorchServe model server worker to load your model, process the input data and run inference. You can use the [TorchServe default inference handlers](https://pytorch.org/serve/default_handlers.html) ( [source](https://github.com/pytorch/serve/tree/master/ts/torch_handler) ), or develop your own custom model handler following the [base_handler.py](https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py) . You might also need to provide the trained model, and the model definition file.\n**Note:** All TorchServe default model handlers support PyTorch/XLA\nIn the following Densenet 161 example, we use model artifacts and the default image classifier handler provided by TorchServe:\n- [model.py](https://github.com/pytorch/serve/tree/master/examples/image_classifier/densenet_161/model.py) \n- [index_to_name.json](https://github.com/pytorch/serve/tree/master/examples/image_classifier/index_to_name.json) \n- [image_classifier.py](https://github.com/pytorch/serve/tree/master/ts/torch_handler/image_classifier.py) \n- Configure some environment variables:```\nCWD=\"$(pwd)\"WORKDIR=\"${CWD}/densenet_161\"mkdir -p ${WORKDIR}/model-storemkdir -p ${WORKDIR}/logs\n```\n- Download and copy model artifacts from the TorchServe image classifier example:```\ngit clone https://github.com/pytorch/serve.gitcp ${CWD}/serve/examples/image_classifier/densenet_161/model.py ${WORKDIR}cp ${CWD}/serve/examples/image_classifier/index_to_name.json ${WORKDIR}\n```\n- Download the model weights:```\nwget https://download.pytorch.org/models/densenet161-8d451a50.pth -O densenet161-8d451a50.pthmv densenet161-8d451a50.pth ${WORKDIR}\n```\n- Create a TorchServe model config file to use the Dynamo backend:```\necho 'pt2: \"torchxla_trace_once\"' >> ${WORKDIR}/model_config.yaml\n```You should see the following files and directories:```\n>> ls ${WORKDIR}model_config.yamlindex_to_name.jsonlogsmodel.pydensenet161-8d451a50.pthmodel-store\n```To serve your PyTorch model with Cloud TPU TorchServe, you need to package your model handler and all your model artifacts into a model archive file `(*.mar)` using [Torch Model Archiver](https://github.com/pytorch/serve/blob/master/model-archiver/README.md) .\nGenerate a model archive file with torch-model-archiver:\n```\nMODEL_NAME=Densenet161docker run \\\u00a0 \u00a0 --privileged \u00a0\\\u00a0 \u00a0 --shm-size 16G \\\u00a0 \u00a0 --name torch-model-archiver \\\u00a0 \u00a0 -it \\\u00a0 \u00a0 -d \\\u00a0 \u00a0 --rm \\\u00a0 \u00a0 --mount type=bind,source=${WORKDIR},target=/home/model-server/ \\\u00a0 \u00a0 ${CLOUD_TPU_TORCHSERVE_IMAGE_URL} \\\u00a0 \u00a0 torch-model-archiver \\\u00a0 \u00a0 \u00a0 \u00a0 --model-name ${MODEL_NAME} \\\u00a0 \u00a0 \u00a0 \u00a0 --version 1.0 \\\u00a0 \u00a0 \u00a0 \u00a0 --model-file model.py \\\u00a0 \u00a0 \u00a0 \u00a0 --serialized-file densenet161-8d451a50.pth \\\u00a0 \u00a0 \u00a0 \u00a0 --handler image_classifier \\\u00a0 \u00a0 \u00a0 \u00a0 --export-path model-store \\\u00a0 \u00a0 \u00a0 \u00a0 --extra-files index_to_name.json \\\u00a0 \u00a0 \u00a0 \u00a0 --config-file model_config.yaml\n```\nYou should see the model archive file generated in the model-store directory:\n```\n>> ls ${WORKDIR}/model-storeDensenet161.mar\n```\nNow you have the model archive file, you can start the TorchServe model server and serve inference requests.\n- Start the TorchServe model server:```\ndocker run \\\u00a0 \u00a0 --privileged \u00a0\\\u00a0 \u00a0 --shm-size 16G \\\u00a0 \u00a0 --name torchserve-tpu \\\u00a0 \u00a0 -it \\\u00a0 \u00a0 -d \\\u00a0 \u00a0 --rm \\\u00a0 \u00a0 -p 7070:7070 \\\u00a0 \u00a0 -p 7071:7071 \\\u00a0 \u00a0 -p 8080:8080 \\\u00a0 \u00a0 -p 8081:8081 \\\u00a0 \u00a0 -p 8082:8082 \\\u00a0 \u00a0 -p 9001:9001 \\\u00a0 \u00a0 -p 9012:9012 \\\u00a0 \u00a0 --mount type=bind,source=${WORKDIR}/model-store,target=/home/model-server/model-store \\\u00a0 \u00a0 --mount type=bind,source=${WORKDIR}/logs,target=/home/model-server/logs \\\u00a0 \u00a0 ${CLOUD_TPU_TORCHSERVE_IMAGE_URL} \\\u00a0 \u00a0 torchserve \\\u00a0 \u00a0 \u00a0 \u00a0 --start \\\u00a0 \u00a0 \u00a0 \u00a0 --ncs \\\u00a0 \u00a0 \u00a0 \u00a0 --models ${MODEL_NAME}.mar \\\u00a0 \u00a0 \u00a0 \u00a0 --ts-config /home/model-server/config.properties\n```\n- Query model server health:```\ncurl http://localhost:8080/ping\n```If the model server is up and running, you will see:```\n{\u00a0 \"status\": \"Healthy\"}\n```To query the default versions of the current registered model use:```\ncurl http://localhost:8081/models\n```You should see the registered model:```\n{\u00a0 \"models\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"modelName\": \"Densenet161\",\u00a0 \u00a0 \u00a0 \"modelUrl\": \"Densenet161.mar\"\u00a0 \u00a0 }\u00a0 ]}\n```To download an image for inference use:```\ncurl -O https://raw.githubusercontent.com/pytorch/serve/master/docs/images/kitten_small.jpgmv kitten_small.jpg ${WORKDIR}\n```To send an inference request to the model server use:```\ncurl http://localhost:8080/predictions/${MODEL_NAME} -T ${WORKDIR}/kitten_small.jpg\n```You should see a response similar to the following:```\n{\u00a0 \"tabby\": 0.47878125309944153,\u00a0 \"lynx\": 0.20393909513950348,\u00a0 \"tiger_cat\": 0.16572578251361847,\u00a0 \"tiger\": 0.061157409101724625,\u00a0 \"Egyptian_cat\": 0.04997897148132324}\n```\n- Model server logsUse the following commands to access the logs:```\nls ${WORKDIR}/logs/cat ${WORKDIR}/logs/model_log.log\n```You should see the following message in your log:```\n\"Compiled model with backend torchxla\\_trace\\_once\"\n```Stop the Docker container:\n```\nrm -rf serverm -rf ${WORKDIR}docker stop torch-model-archiverdocker stop torchserve-tpu\n```\n## Profiling\nAfter setting up inference, you can use profilers to analyze the performance and TPU utilization. For more information about profiling, see:\n- [Profiling on Cloud TPU](/tpu/docs/cloud-tpu-tools) \n- [TensorFlow profiling](https://www.tensorflow.org/guide/profiler) \n- [PyTorch profiling](/tpu/docs/pytorch-xla-performance-profiling-tpu-vm) \n- [JAX profiling](https://jax.readthedocs.io/en/latest/profiling.html#profiling-jax-programs)", "guide": "Cloud TPU"}