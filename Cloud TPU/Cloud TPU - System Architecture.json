{"title": "Cloud TPU - System Architecture", "url": "https://cloud.google.com/tpu/docs/system-architecture-tpu-vm", "abstract": "# Cloud TPU - System Architecture\n# System Architecture\nTensor Processing Units (TPUs) are application specific integrated circuits (ASICs) designed by Google to accelerate machine learning workloads. Cloud TPU is a Google Cloud service that makes TPUs available as a scalable resource.\nTPUs are designed to perform matrix operations quickly making them ideal for machine learning workloads. You can run machine learning workloads on TPUs using frameworks such as [TensorFlow](https://www.tensorflow.org/) , [Pytorch](/tpu/docs/tutorials/pytorch-pod) , and [JAX](https://jax.readthedocs.io/en/latest/) .\n", "content": "## Cloud TPU terms\nIf you are new to Cloud TPUs, check out the [TPU documentation home](https://cloud.google.com/tpu/docs) . The following sections explain terms and related concepts used in this document.\n### Batch inference\nBatch or offline inference refers to doing inference outside of production pipelines typically on a bulk of inputs. Batch inference is used for offline tasks such as data labeling and also for evaluating the trained model. Latency SLOs are not a priority for batch inference.\n### TPU chip\nA TPU chip contains one or more TensorCores. The number of TensorCores depends on the version of the TPU chip. Each TensorCore consists of one or more matrix-multiply units (MXUs), a vector unit, and a scalar unit.\nAn MXU is composed of 128 x 128 multiply-accumulators in a [systolic array](https://en.wikipedia.org/wiki/Systolic_array) . MXUs provide the bulk of the compute power in a TensorCore. Each MXU is capable of performing 16K multiply-accumulate operations per cycle. All multiplies take [bfloat16](/tpu/docs/bfloat16) inputs, but all accumulations are performed in FP32 number format.\nThe vector unit is used for general computation such as activations and softmax. The scalar unit is used for control flow, calculating memory addresses, and other maintenance operations.\n### TPU cube\nA 4x4x4 topology. This is only applicable to 3D topologies (beginning with the v4 TPU version).\n### Inference\nInference is the process of using a trained model to make predictions on new data. It is used by the [serving](#serving) process.\n### Multislice versus single slice\nMultislice is a group of slices, extending TPU connectivity beyond the inter-chip interconnect (ICI) connections and leveraging the data-center network (DCN) for transmitting data beyond a slice. Data within each slice is still transmitted by ICI. Using this hybrid connectivity, Multislice enables parallelism across slices and lets you use a greater number of TPU cores for a single job than what a single slice can accommodate.\nTPUs can be used to run a job either on a single slice or multiple slices. Refer to the [Multislice introduction](/tpu/docs/multislice-introduction) for more details.\n### Cloud TPU ICI resiliency\nICI resiliency helps improve fault tolerance of optical links and optical circuit switches (OCS) that connect TPUs between [cubes](#cube) . (ICI connections within a cube use copper links that are not impacted). ICI resiliency allows ICI connections to be routed around OCS and optical ICI faults. As a result, it improves the scheduling availability of TPU slices, with the trade-off of temporary degradation in ICI performance.\nSimilar to Cloud TPU v4, ICI resiliency is enabled by default for v5p slices that are one cube or larger:\n- v5p-128 when specifying acclerator type\n- 4x4x4 when specifying accelerator config\n### Queued resource\nA representation of TPU resources, used to enqueue and manage a request for a single-slice or multi-slice TPU environment. See [Queued Resources user guide](/tpu/docs/queued-resources) for more information.\n### Serving\nServing is the process of deploying a trained machine learning model to a production environment where it can be used to make predictions or decisions. Latency and service-level availability are important for serving.\n### Single host and multi host\nA TPU host is a VM that runs on a physical computer connected to TPU hardware. TPU workloads can use one or more host.\nA single-host workload is limited to one TPU VM and can access 1, 4, or 8 TPU chips. A multi-host TPU v5e workload can access 8, 12, 16, 32, 64, 128, or 256 TPU chips with one TPU VM for every four TPU chips. Multi-host workloads distribute training across multiple TPU VMs.\nTPU v5e supports single and multi-host training and single host inference. Multi-host inference is supported using [Sax](https://github.com/google/saxml) . For more information, see [Large Language Model Serving](/tpu/docs/v5e-inference#large_language_model_serving) .\n### Slices\nA Pod slice is a collection of chips all located inside the same TPU Pod connected by high-speed inter chip interconnects (ICI).\nv5p has 3D slice shapes. See the table in the [Accelerator Types](/tpu/docs/supported-tpu-configurations#tpu-v5p-config) section for a list of supported slice shapes.\nv5e slices are described with 2D slice shapes. Each number in the slice shape corresponds to the number of v5e in one dimension. For example, `4x2` describes an arrangement of 8 v5e chips in a 4 x 2 grid. See the table in the [Accelerator Types](/tpu/docs/supported-tpu-configurations#tpu-v5e-config) section for a list of supported slice shapes for v5e.\nTPU v4 and [v5p slices](/tpu/docs/system-architecture-tpu-vm#v5p-configuration) can be described in terms of v4 or v5p chips. v4 or v5p slices are described with 3D shapes. Each number in the slice shape corresponds to the number of v4 or v5p in a dimension. For example `4x4x8` describes an arrangement of 128 chips in a 4 x 4 x 8 cube.\nv4 and v5p slices can also be described in terms of the number of in the slice. For example, `v4-128` describes a v4 slice with 128 TensorCores. v4 slices are available with 16, 32, 64, 128, 256, 512, 1024, 2048, or 4096 TensorCores. The section [v5p slices](/tpu/docs/supported-tpu-configurations#accelerator-types) shows a list of v5p slices and the number of TensorCores in each slice.\nTPU v3 slices are described in terms of TensorCores and are available with 32, 128, 512, 1024, or 2048 TensorCores. TPU v2 slices are also described in terms of TensorCores and are available with 32, 128, 256, or 512 TensorCores.\nand also refer to slice shapes.\n### SparseCore\nv5p includes four SparseCores per chip which are Dataflow processors that accelerate models relying on embeddings found in recommendation models.\n### TPU Pod\nA TPU Pod is a contiguous set of TPUs grouped together over a specialized network. The number of TPU chips in a TPU Pod is dependent on the TPU version.\n### TPU VM or worker\nA virtual machine running Linux that has access to the underlying TPUs. For v5e TPUs, each TPU VM has direct access to 1, 4, or 8 chips depending on the user-specified accelerator type. For v4 and earlier, each TPU VM has access to 4 TPU chips. A TPU VM is also known as a .\n### TensorCores\nTPU chips have one or two TensorCores to run matrix multiplication. TPU v5e has one TensorCore per chip. TPU v2, v3, v4, and v5p have two TensorCores per chip. For more information about TensorCores, see this [ACMarticle](https://dl.acm.org/doi/pdf/10.1145/3360307) .\n### Worker\nSee [TPU VM](#tpu-vm) .\n## TPU versions\nThe exact layout of a TPU depends on the [TPU version](/tpu/docs/supported-tpu-versions) that you use. Architectural details and performance characteristics of TPU v2 and v3 are available in [A Domain Specific Supercomputer for Training Deep Neural Networks](https://dl.acm.org/doi/pdf/10.1145/3360307) . Architectural details for other TPU versions are shown in the following sections.\n### TPU v5p architecture\nThis section describes the system architecture specific to the v5p version.\nEach TensorCore has four Matrix Multiply Units (MXU), a vector unit, and a scalar unit.\nThere are 8960 chips in a single v5p Pod. The largest job that can be scheduled is a 96 cube (6144 chip) job.\nThe following table shows the key specifications for a v5p.\n| Key specifications   | v5p values  |\n|:-----------------------------|:----------------|\n| Peak compute per chip (bf16) | 459 TFLOPs  |\n| HBM2e capacity and bandwidth | 95GB, 2765 GBps |\n| TPU Pod size     | 8960 chips  |\n| Interconnect topology  | 3D Torus  |\n| Interchip Interconnect BW | 4800 Gbps  |\n### TPU v5e\nEach v5e chip contains one TensorCore. Each TensorCore has 4 Matrix Multiply Units (MXU), a vector unit, and a scalar unit.\nThe following diagram illustrates a TPU v5e chip.\nThe following table shows the key chip specifications and their values for v5e.\n| 0       | 1    |\n|:-----------------------------|:----------------|\n| Key chip specifications  | v5e values  |\n| Peak compute per chip (bf16) | 197 TFLOPs  |\n| Peak compute per chip (Int8) | 393 TFLOPs  |\n| HBM2 capacity and bandwidth | 16 GB, 819 GBps |\n| Interchip Interconnect BW | 1600 Gbps  |\nThe following table shows Pod specifications and their values for v5e.\n| 0          | 1     |\n|:--------------------------------------|:------------------|\n| Key Pod specifications    | v5e values  |\n| TPU Pod size       | 256 chips   |\n| Interconnect topology     | 2D Torus   |\n| Peak compute per Pod     | 100 PetaOps(Int8) |\n| All-reduce bandwidth per Pod   | 51.2 TB/s   |\n| Bisection bandwidth per Pod   | 1.6 TB/s   |\n| Data center network bandwidth per Pod | 6.4 Tbps   |\n### TPU v4\nEach TPU v4 chip contains two TensorCores. Each TensorCore has four MXUs, a vector unit, and a scalar unit. The following table shows the key specifications for a v4 TPU Pod.\n| Key specifications   | v4 Pod values    |\n|:-----------------------------|:-----------------------------|\n| Peak compute per chip  | 275 teraflops (bf16 or int8) |\n| HBM2 capacity and bandwidth | 32 GiB, 1200 GBps   |\n| Measured min/mean/max power | 90/170/192 W     |\n| TPU Pod size     | 4096 chips     |\n| Interconnect topology  | 3D mesh      |\n| Peak compute per Pod   | 1.1 exaflops (bf16 or int8) |\n| All-reduce bandwidth per Pod | 1.1 PB/s      |\n| Bisection bandwidth per Pod | 24 TB/s      |\nThe following diagram illustrates a TPU v4 chip.\nv4 TPUs have a direct connection to the nearest neighboring chips in 3 dimensions, resulting in a 3D mesh of networking connections. When the slice is equal or larger than a single cube, the connections can be configured as a 3D torus. In general, the performance of a 3D configuration will be better than a 3D mesh configuration.\n### TPU v3\nEach v3 TPU chip contains two TensorCores. Each TensorCore has two MXUs, a vector unit, and a scalar unit. The following table shows the key specifications and their values for a v3 TPU Pod.\n| Key specifications   | v3 Pod values  |\n|:-----------------------------|:---------------------|\n| Peak compute per chip  | 123 teraflops (bf16) |\n| HBM2 capacity and bandwidth | 32 GiB, 900 GBps  |\n| Measured min/mean/max power | 123/220/262 W  |\n| TPU Pod size     | 1024 chips   |\n| Interconnect topology  | 2D torus    |\n| Peak compute per Pod   | 126 petaflops (bf16) |\n| All-reduce bandwidth per Pod | 340 TB/s    |\n| Bisection bandwidth per Pod | 6.4 TB/s    |\nThe following diagram illustrates a TPU v3 chip.### Performance benefits of TPU v4 over v3\nThis section describes the performance benefits of TPU v4\nNon Uniform Memory Access (NUMA) is a computer memory architecture for machines that have multiple CPUs. Each CPU has direct access to a block of high-speed memory. A CPU and it's memory is called a NUMA node. NUMA nodes are connected to NUMA nodes that are directly adjacent to each other. A CPU from one NUMA node can access memory in another NUMA node, but this access is slower than accessing memory within a NUMA node.\nSoftware running on a multi-CPU machine can place data needed by a CPU within its NUMA node, increasing memory throughput. For more information about NUMA, see [Non Uniform Memory Access](https://en.wikipedia.org/wiki/Non-uniform_memory_access) on Wikipedia.\nYou can take advantage of NUMA-locality benefits by binding your training script to NUMA Node 0.\nTo enable NUMA node binding:\n- Install the numactl command line tool.```\n\u00a0$ sudo apt-get update\u00a0$ sudo apt-get install numactl\n```\n- Use `numactl --cpunodebind=0` when launching your training script. This binds your script code to NUMA Node 0.```\n\u00a0$ numactl --cpunodebind=0 python3 your-training-script\n```\nEnable NUMA node binding if:\n- If your workload has a heavy dependence on CPU workloads (for example, image classification, recommendation workloads) regardless of framework.\n- If you are using a TPU runtime version without a -pod suffix (for example,`tpu-vm-tf-2.10.0-v4`).\nOther memory system differences:\n- v4 TPU chips have a unified 32-GiB HBM memory space across the entire chip, enabling better coordination between the two on-chip TensorCores.\n- Improved HBM performance using latest memory standards and speeds.\n- Improved DMA performance profile with built-in support for high-performance striding at 512B granularities.- Twice the number of MXUs and a higher clock rate delivering 275 max TFLOPS.\n- 2x transposition and permutation bandwidth.\n- Load-store memory access model for Common Memory (Cmem).\n- Faster MXU weight loading bandwidth and 8-bit mode support to allow lower batch sizes and improved inference latency.Six interconnect links per chip to enable network topologies that have smaller network diameters.- x16 PCIE gen3 interface to host (direct connect).\n- Improved security model.\n- Improved energy efficiency.\n### Performance benefits of TPU v3 over v2\nThe increased FLOPS per TensorCore and memory capacity in TPU v3 configurations can improve the performance of your models in the following ways:\n- TPU v3 configurations provide significant performance benefits per TensorCore for compute-bound models. Memory-bound models on TPU v2 configurations might not achieve this same performance improvement if they are also memory-bound on TPU v3 configurations.\n- In cases where data does not fit into memory on TPU v2 configurations, TPU v3 can provide improved performance and reduced recomputation of intermediate values (rematerialization).\n- TPU v3 configurations can run new models with batch sizes that did not fit on TPU v2 configurations. For example, TPU v3 might allow deeper ResNets and larger images with RetinaNet.\nModels that are nearly input-bound (\"infeed\") on TPU v2 because training steps are waiting for input might also be input-bound with Cloud TPU v3. The pipeline performance guide can help you resolve infeed issues.\n## Cloud TPU VM Architectures\nHow you interact with the TPU host (and the TPU board) depends upon the TPU VM architecture you're using: TPU Nodes or TPU VMs.\n### TPU Node Architecture\n**Important:** TPU v4 is not supported with the TPU Node architecture.\nThe TPU Node architecture consists of a user VM that communicates with the TPU host over gRPC. When using this architecture, you cannot directly access the TPU Host, making it difficult to debug training and TPU errors.### TPU VM Architecture\nThe TPU VM architecture lets you directly connect to the VM physically connected to the TPU device using SSH. You have root access to the VM, so you can run arbitrary code. You can access compiler and runtime debug logs and error messages.\n## What's next\n- [Quickstarts](/tpus/docs/quick-starts) \n- [Tutorials](/tpus/docs/tutorials)", "guide": "Cloud TPU"}