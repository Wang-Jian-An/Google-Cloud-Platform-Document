{"title": "Google Kubernetes Engine (GKE) - Migrate x86 application on GKE to multi-arch with Arm", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview", "abstract": "# Google Kubernetes Engine (GKE) - Migrate x86 application on GKE to multi-arch with Arm\n**Note** : This tutorial provides instructions for working with this app: Docker. The instructions might not represent newer versions of the app. For more information, see the documentation: [Docker](https://docs.docker.com/) .\nThis tutorial describes how to migrate an application built for nodes using an x86 (Intel or AMD) processor in a Google Kubernetes Engine (GKE) cluster to a multi-architecture (multi-arch) application that runs on either x86 or Arm nodes. The intended audience for this tutorial is Platform Admins, App Operators, and App Developers who want to run their existing x86-compatible workloads on Arm.\nWith GKE clusters, you can run workloads on Arm nodes using the [Tau T2A Arm machine series](/compute/docs/general-purpose-machines#t2a_machines) . T2A nodes can run in your GKE cluster just like any other node using x86 (Intel or AMD) processors. They are a good choice for scale-out and compute-intensive workloads.\nTo learn more, see [Arm workloads on GKE](/kubernetes-engine/docs/concepts/arm-on-gke) .\nThis tutorial assumes that you are familiar with Kubernetes and Docker. The tutorial uses Google Kubernetes Engine and Artifact Registry.", "content": "## ObjectivesIn this tutorial, you will complete the following tasks:- Store container images with Docker in Artifact Registry.\n- Deploy an x86-compatible workload to a GKE cluster.\n- Rebuild an x86-compatible workload to run on Arm.\n- Add an Arm node pool to an existing cluster.\n- Deploy an Arm-compatible workload to run on an Arm node.\n- Build a multi-arch image to run a workload across multiple architectures.\n- Run workloads across multiple architectures in one GKE cluster.\n## CostsIn this document, you use the following billable components of Google Cloud:- [GKE](/kubernetes-engine/pricing) \n- [Artifact Registry](/artifact-registry/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\nTake the following steps to enable the Kubernetes Engine API:\nWhen you finish this tutorial, you can avoid continued billing by deleting the resources you created. See [Clean up](#clean-up) for more details.## Launch Cloud ShellIn this tutorial you will use [Cloud Shell](/shell/docs) , which is a shell environment for managing resources hosted on Google Cloud.\nCloud Shell comes preinstalled with the [Google Cloud CLI](/sdk/gcloud) and [kubectl](https://kubernetes.io/docs/reference/kubectl/) command-line tool. The gcloud CLI provides the primary command-line interface for Google Cloud, and `kubectl` provides the primary command-line interface for running commands against Kubernetes clusters.\nLaunch Cloud Shell:- Go to the Google Cloud console. [Google Cloud console](https://console.cloud.google.com/) \n- From the upper-right corner of the console, click the **Activate Cloud Shell** button: \nA Cloud Shell session appears inside the console. You use this shell to run `gcloud` and `kubectl` commands.## Prepare your environmentIn this section, you prepare your environment to follow the tutorial.\n### Set the default settings for the gcloud CLISet environment variables for your project ID, the zone, and the name of your new cluster.\n **Caution:** The following environment variables are used throughout the commands of this tutorial. You might need to set the environment variables again if you close the Cloud Shell.\n```\nexport PROJECT_ID=PROJECT_IDexport ZONE=us-central1-aexport CLUSTER_NAME=my-cluster\n```\nReplace `` with the project ID you chose for this tutorial in the [Before you begin](#before-you-begin) section.\nIn this tutorial, you create resources in us-central1-a. To see a complete list of where the Tau T2A machine series is available, see [Available regions and zones](/compute/docs/regions-zones#available) .\n### Clone the git repositoryThis tutorial uses resources from the [Arm on GKE GitHub repository](https://github.com/GoogleCloudPlatform/gke-arm) .- Clone the repository:```\ngit clone https://github.com/GoogleCloudPlatform/gke-arm\n```\n- Change your current working directory to the `gke-arm/migrate-x86-app-to-multi-arch/` from the repository cloned in the previous step:```\ncd gke-arm/migrate-x86-app-to-multi-arch/\n```\n## Create a GKE cluster and deploy the x86 applicationIn the first part of this tutorial, you create a cluster with x86 nodes and deploy an x86 application. The example application is a service which responds to HTTP requests. It is built with the Golang programming language.\nThis setup represents what a typical cluster environment might look like, using x86-compatible applications and x86 nodes.\n### Create a GKE clusterFirst, create a GKE using nodes with x86 processors. With this configuration, you create a typical cluster environment to run x86 applications.\nCreate the cluster:\n```\ngcloud container clusters create $CLUSTER_NAME \\\u00a0 \u00a0 --release-channel=rapid \\\u00a0 \u00a0 --zone=$ZONE \\\u00a0 \u00a0 --machine-type=e2-standard-2 \\\u00a0 \u00a0 --num-nodes=1 \\\u00a0 \u00a0 --async\n```\nThis cluster has autoscaling disabled in order to demonstrate specific functionality in later steps.\nIt might take several minutes to finish creating the cluster. The `--async` flag lets this operation run in the background while you complete the next steps.\nYou can [create clusters with only Arm nodes](/kubernetes-engine/docs/how-to/create-arm-clusters-nodes#create-cluster-arm-node-pool) , however for this tutorial you will create a cluster with only x86 nodes first to learn about the process of making x86-only applications compatible with Arm.\n### Create the Artifact Registry Docker repository\n- Create a repository in Artifact Registry to store Docker images:```\ngcloud artifacts repositories create docker-repo \\\u00a0 \u00a0 \u00a0 --repository-format=docker \\\u00a0 \u00a0 \u00a0 --location=us-central1 \\\u00a0 \u00a0 \u00a0 --description=\"Docker repository\"\n```\n- Configure the Docker command-line tool to authenticate to this repository in Artifact Registry:```\ngcloud auth configure-docker us-central1-docker.pkg.dev\n```\n### Build the x86 image and push it to Artifact Registry\n- Build the x86-compatible version of the application:```\ndocker build -t us-central1-docker.pkg.dev/$PROJECT_ID/docker-repo/x86-hello:v0.0.1 . \n```\n- Push the image to Artifact Registry:```\ndocker push us-central1-docker.pkg.dev/$PROJECT_ID/docker-repo/x86-hello:v0.0.1\n```\n### Deploy the x86 application\n- Check that the cluster is ready by running the following script:```\nechoecho -ne \"Waiting for GKE cluster to finish provisioning\"gke_status=\"\"while [ -z $gke_status ]; do\u00a0 \u00a0sleep 2\u00a0 \u00a0echo -ne '.' \u00a0 \u00a0 \u00a0gke_status=$(gcloud container clusters list --format=\"value(STATUS)\" --filter=\"NAME=$CLUSTER_NAME AND STATUS=RUNNING\")doneechoecho \"GKE Cluster '$CLUSTER_NAME' is $gke_status\" echo\n```When the cluster is ready, the output should be similar to the following:```\nGKE Cluster 'my-cluster' is RUNNING\n```\n- Retrieve the cluster credentials so that `kubectl` can connect to the Kubernetes API for the cluster:```\ngcloud container clusters get-credentials $CLUSTER_NAME --zone $ZONE --project $PROJECT_ID\n```\n- Update the image using [kustomize](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/) and deploy the x86 application:```\n$(cd k8s/overlays/x86 && kustomize edit set image hello=us-central1-docker.pkg.dev/$PROJECT_ID/docker-repo/x86-hello:v0.0.1) kubectl apply -k k8s/overlays/x86\n```\n- Deploy a Service to expose the application to the Internet:```\nkubectl apply -f k8s/hello-service.yaml\n```\n- Check that the external IP address for the Service, `hello-service` , is finished provisioning:```\nechoecho -ne \"Waiting for External IP to be provisioned\"external_ip=\"\"while [ -z $external_ip ]; do\u00a0 \u00a0sleep 2\u00a0 \u00a0echo -ne '.'\u00a0 \u00a0external_ip=$(kubectl get svc hello-service --template=\"{{range .status.loadBalancer.ingress}}{{.ip}}{{end}}\")doneechoecho \"External IP: $external_ip\"echo\n```After the external IP address is provisioned, the output should be similar to the following:```\nExternal IP: 203.0.113.0\n```\n- Make an HTTP request to test that the deployment works as expected:```\ncurl -w '\\n' http://$external_ip\n```The output is similar to the following:```\nHello from NODE:gke-my-cluster-default-pool-32019863-b41t, POD:x86-hello-deployment-6b7b456dd5-mwfkd, CPU PLATFORM:linux/amd64\n```The output shows that this x86-compatible deployment is running on a node in the default node pool on the `amd64` architecture. The nodes in the default node pool of your cluster have x86 (either Intel or AMD) processors.\n## Add Arm nodes to the clusterIn the next part of this tutorial, add Arm nodes to your existing cluster. These nodes are where the Arm-compatible version of your application is deployed when it's rebuilt to run on Arm.\n### CheckpointSo far you've accomplished the following objectives:- create a GKE cluster using x86 nodes.\n- store an x86-compatible container image with Docker in Artifact Registry.\n- deploy an x86-compatible workload to a GKE cluster.\nYou've configured a cluster environment with x86 nodes and an x86-compatible workload. This configuration is similar to your existing cluster environments if you don't currently use Arm nodes and Arm-compatible workloads.\n### Add an Arm node pool to your clusterAdd an Arm node pool to your existing cluster:\n```\ngcloud container node-pools create arm-pool \\\u00a0 \u00a0 --cluster $CLUSTER_NAME \\\u00a0 \u00a0 --zone $ZONE \\\u00a0 \u00a0 --machine-type=t2a-standard-2 \\\u00a0 \u00a0 --num-nodes=1\n```\nThe `t2a-standard-2` machine type is an Arm VM from the [Tau T2A machine series (Preview)](/compute/docs/general-purpose-machines#t2a_machines) .\nYou create a node pool with Arm nodes in the same way as creating a node pool with x86 nodes. After this node pool is created, you will have both x86 nodes and Arm nodes running in this cluster.\nTo learn more about adding Arm node pools to existing clusters, see [Add an Arm node pool to a GKE cluster](/kubernetes-engine/docs/how-to/create-arm-clusters-nodes#create-arm-node-pool) .\n### Scale up the existing application running on x86-based nodesNodes of multiple architecture types can work seamlessly together in one cluster. GKE doesn't schedule existing workloads running on x86 nodes to Arm nodes in the cluster because a taint is automatically placed on Arm nodes. You can see this by scaling up your existing application.- Update the workload, scaling it up to 6 replicas:```\n$(cd k8s/overlays/x86_increase_replicas && kustomize edit set image hello=us-central1-docker.pkg.dev/$PROJECT_ID/docker-repo/x86-hello:v0.0.1) kubectl apply -k k8s/overlays/x86_increase_replicas/\n```\n- Wait 30 seconds, then run the following command to check the status of the deployment:```\nkubectl get pods -l=\"app=hello\" --field-selector=\"status.phase=Pending\"\n```The output should look similar to the following:```\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0READY \u00a0 STATUS \u00a0 \u00a0RESTARTS \u00a0 AGEx86-hello-deployment-6b7b456dd5-6tkxd \u00a0 0/1 \u00a0 \u00a0 Pending \u00a0 0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a040sx86-hello-deployment-6b7b456dd5-k95b7 \u00a0 0/1 \u00a0 \u00a0 Pending \u00a0 0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a040sx86-hello-deployment-6b7b456dd5-kc876 \u00a0 0/1 \u00a0 \u00a0 Pending \u00a0 0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a040s\n```This output shows Pods with a Pending status as there is no room left on the x86-based nodes. Since Cluster Autoscaler is disabled and the Arm nodes are tainted, the workloads will not be deployed on any of the available Arm nodes. This taint prevents GKE from scheduling x86 workloads on Arm nodes. To deploy to Arm nodes, you must indicate that the deployment is compatible with Arm nodes.\n- Check the Pods that are in the Running state:```\nkubectl get pods -l=\"app=hello\" --field-selector=\"status.phase=Running\" -o wide\n```The output should look similar to the following:```\nNAME         READY STATUS RESTARTS AGE IP   NODE          NOMINATED NODE READINESS GATES\nx86-hello-deployment-6b7b456dd5-cjclz 1/1  Running 0   62s 10.100.0.17 gke-my-cluster-default-pool-32019863-b41t <none>   <none>\nx86-hello-deployment-6b7b456dd5-mwfkd 1/1  Running 0   34m 10.100.0.11 gke-my-cluster-default-pool-32019863-b41t <none>   <none>\nx86-hello-deployment-6b7b456dd5-n56rg 1/1  Running 0   62s 10.100.0.16 gke-my-cluster-default-pool-32019863-b41t <none>   <none>\n```In this output, the `NODE` column indicates that all Pods from the deployment are running only in the default-pool, meaning that the x86-compatible Pods are only scheduled to the x86 nodes. The original Pod that was already scheduled before the creation of the Arm node pool is still running on the same node.\n- Run the following command to access the service and see the output:```\nfor i in $(seq 1 6); do curl -w '\\n' http://$external_ip; done\n```The output is similar to the following:```\nHello from NODE:gke-my-cluster-default-pool-32019863-b41t, POD:x86-hello-deployment-6b7b456dd5-cjclz, CPU PLATFORM:linux/amd64\nHello from NODE:gke-my-cluster-default-pool-32019863-b41t, POD:x86-hello-deployment-6b7b456dd5-cjclz, CPU PLATFORM:linux/amd64\nHello from NODE:gke-my-cluster-default-pool-32019863-b41t, POD:x86-hello-deployment-6b7b456dd5-n56rg, CPU PLATFORM:linux/amd64\nHello from NODE:gke-my-cluster-default-pool-32019863-b41t, POD:x86-hello-deployment-6b7b456dd5-n56rg, CPU PLATFORM:linux/amd64\nHello from NODE:gke-my-cluster-default-pool-32019863-b41t, POD:x86-hello-deployment-6b7b456dd5-cjclz, CPU PLATFORM:linux/amd64\nHello from NODE:gke-my-cluster-default-pool-32019863-b41t, POD:x86-hello-deployment-6b7b456dd5-cjclz, CPU PLATFORM:linux/amd64\n```This output shows that all Pods serving requests are running on x86 nodes. Some Pods cannot respond because they are still in the Pending state as there is no space on the existing x86 nodes and they will not be scheduled to Arm nodes.\n## Rebuild your application to run on ArmIn the previous section, you added an Arm node pool to your existing cluster. However, when you scaled up the existing x86 application, it did not schedule any of the workloads to the Arm nodes. In this section, you rebuild your application to be Arm-compatible, so that this application can run on the Arm nodes in the cluster.\nFor this example, accomplish these steps by using [docker build](https://docs.docker.com/engine/reference/commandline/build/) . This two-step approach includes:- **First stage** : Build the code to Arm.\n- **Second stage** : Copy the executable to a lean container.\nAfter following these steps, you will have an Arm-compatible image in addition to the x86-compatible image.\nThe second step of copying the executable to another container follows one of the best practices for building a container, which is to [build the smallest image possible](/architecture/best-practices-for-building-containers#build-the-smallest-image-possible) .\nThis tutorial uses an example application built with the Golang programming language. With Golang, you can cross-compile an application to different operating systems and CPU platforms by providing environment variables, `GOOS` and `GOARCH` , respectively.- Run `cat Dockerfile_arm` to see the Dockerfile written for Arm:```\n#\n# Build: 1st stage\n#\nFROM golang:1.18-alpine as builder \nWORKDIR /app\nCOPY go.mod .\nCOPY hello.go .\nRUN GOARCH=arm64 go build -o /hello && \\\n apk add --update --no-cache file && \\\n file /hello\n```The snippet shown here shows just the first stage. In the file, both stages are included.In this file, setting `GOARCH=arm64` instructs the Go compiler to build the application for the Arm instruction set. You do not need to set `GOOS` because the base image in the first stage is a Linux Alpine image.\n- Build the code for Arm, and push it to Artifact Registry:```\ndocker build -t us-central1-docker.pkg.dev/$PROJECT_ID/docker-repo/arm-hello:v0.0.1 -f Dockerfile_arm .docker push us-central1-docker.pkg.dev/$PROJECT_ID/docker-repo/arm-hello:v0.0.1\n```\n### Deploy the Arm version of your applicationNow that the application is built to run on Arm nodes, you can deploy it to the Arm nodes in your cluster.- Inspect the `add_arm_support.yaml` by running `cat k8s/overlays/arm/add_arm_support.yaml` :The output is similar to the following:```\n nodeSelector:\n  kubernetes.io/arch: arm64\n```This `nodeSelector` specifies that the workload should run only on the Arm nodes. When you use the `nodeSelector` , GKE [adds a toleration](/kubernetes-engine/docs/how-to/prepare-arm-workloads-for-deployment#add-toleration) that matches the taint on Arm nodes, letting GKE schedule the workload on those nodes. To learn more about setting this field, see [Prepare an Arm workload for deployment](/kubernetes-engine/docs/how-to/prepare-arm-workloads-for-deployment) .\n- Deploy one replica of the Arm-compatible version of the application:```\n$(cd k8s/overlays/arm && kustomize edit set image hello=us-central1-docker.pkg.dev/$PROJECT_ID/docker-repo/arm-hello:v0.0.1) kubectl apply -k k8s/overlays/arm\n```\n- Wait 5 seconds, then check that the Arm deployment is answering `curl` requests:```\nfor i in $(seq 1 6); do curl -w '\\n' http://$external_ip; done\n```The output is similar to the following:```\nHello from NODE:gke-my-cluster-default-pool-32019863-b41t, POD:x86-hello-deployment-6b7b456dd5-n56rg, CPU PLATFORM:linux/amd64Hello from NODE:gke-my-cluster-default-pool-32019863-b41t, POD:x86-hello-deployment-6b7b456dd5-n56rg, CPU PLATFORM:linux/amd64Hello from NODE:gke-my-cluster-default-pool-32019863-b41t, POD:x86-hello-deployment-6b7b456dd5-mwfkd, CPU PLATFORM:linux/amd64Hello from NODE:gke-my-cluster-default-pool-32019863-b41t, POD:x86-hello-deployment-6b7b456dd5-mwfkd, CPU PLATFORM:linux/amd64Hello from NODE:gke-my-cluster-arm-pool-e172cff7-shwc, POD:arm-hello-deployment-69b4b6bdcc-n5l28, CPU PLATFORM:linux/arm64Hello from NODE:gke-my-cluster-default-pool-32019863-b41t, POD:x86-hello-deployment-6b7b456dd5-n56rg, CPU PLATFORM:linux/amd64\n```This output should include responses from both the x86-compatible and Arm-compatible applications responding to the `curl` request.\n## Build a multi-architecture image to run a workload across architecturesWhile you can use the strategy described in the previous section and deploy separate workloads for x86 and Arm, this would require you to maintain and keep organized two build processes and two container images.\nIdeally, you want to build and run your application seamlessly across both x86 and Arm platforms. We recommend this approach. To run your application with one manifest across multiple architecture platforms, you need to use multi-architecture (multi-arch) images. To learn more about multi-architecture images, see [Build multi-arch images for Arm workloads](/kubernetes-engine/docs/how-to/build-multi-arch-for-arm) .\nTo use multi-architecture images, you must ensure that your application meets the following prerequisites:- Your application does not have any architecture platform-specific dependencies.\n- All dependencies must be built for multi-architecture or, at minimum, the targeted platforms.\nThe example application used in this tutorial meets both of these prerequisites. However, we recommend testing your own applications when building their multi-arch images before deploying them to production.\n### Build and push multi-architecture imagesYou can build multi-arch images with [Docker Buildx](https://docs.docker.com/buildx/working-with-buildx/) if your workload fulfills the following prerequisites:- The base image supports multiple architectures. Check this by running [docker manifest inspect](https://docs.docker.com/engine/reference/commandline/manifest_inspect/) on the base image and checking the list of architecture platforms. See an example of how to inspect an image at the end of this section.\n- The application does not require special build steps for each architecture platform. If special steps were required, Buildx might not be sufficient. You would need to have a separate Dockerfile for each platform and create the manifest manually with [docker manifest create](https://docs.docker.com/engine/reference/commandline/manifest_create/) .\nThe example application's base image is Alpine, which supports multiple architectures. There are also no architecture platform-specific steps, so you can build the multi-arch image with Buildx.- Inspect the Dockerfile by running `cat Dockerfile` :```\n# This is a multi-stage Dockerfile. \n# 1st stage builds the app in the target platform\n# 2nd stage create a lean image coping the binary from the 1st stage\n#\n# Build: 1st stage\n#\nFROM golang:1.18-alpine as builder \nARG BUILDPLATFORM \nARG TARGETPLATFORM\nRUN echo \"I am running on $BUILDPLATFORM, building for $TARGETPLATFORM\" \nWORKDIR /app\nCOPY go.mod .\nCOPY hello.go .\nRUN go build -o /hello && \\\n apk add --update --no-cache file && \\\n file /hello \n#\n# Release: 2nd stage\n#\nFROM alpine\nWORKDIR /\nCOPY --from=builder /hello /hello\nCMD [ \"/hello\" ]\n```This Dockerfile defines two stages: the build stage and release stage. You use the same Dockerfile used for building the x86 application. If you follow [best practices for building containers](/architecture/best-practices-for-building-containers) , you might be able to rebuild your own container images without changing anything.\n- Run the following command to create and use a new `docker buildx` builder:```\ndocker buildx create --name multiarch --use --bootstrap\n```Now that you have created this new builder, you can build and push an image that is compatible with both `linux/amd64` and `linux/arm64` by using the `--platform` flag. For each platform provided with the flag, Buildx builds an image in the target platform. When Buildx builds the `linux/arm64` image, it downloads `arm64` base images. In the first stage, it builds the binary on the `arm64 golang:1.18-alpine` image for `arm64` . In the second stage, the `arm64` Alpine Linux image is downloaded and the binary is copied to a layer of that image.\n- Build and push the image:```\ndocker buildx build -t us-central1-docker.pkg.dev/$PROJECT_ID/docker-repo/multiarch-hello:v0.0.1 -f Dockerfile --platform linux/amd64,linux/arm64 --push .\n```The output is similar to the following:```\n=> [linux/arm64 builder x/x] ..\n=> [linux/amd64 builder x/x] ..\n```This output shows that two images are generated, one for `linux/arm64` and one for `linux/amd64` .\n- Inspect the manifest of your new multi-arch image:```\ndocker manifest inspect us-central1-docker.pkg.dev/$PROJECT_ID/docker-repo/multiarch-hello:v0.0.1\n```The output is similar to the following:```\n{\n \"schemaVersion\": 2,\n \"mediaType\": \"application/vnd.docker.distribution.manifest.list.v2+json\",\n \"manifests\": [  {\n   \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n   \"size\": 739,\n   \"digest\": \"sha256:dfcf8febd94d61809bca8313850a5af9113ad7d4741edec1362099c9b7d423fc\",\n   \"platform\": {\n   \"architecture\": \"amd64\",\n   \"os\": \"linux\"\n   }\n  },\n  {\n   \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n   \"size\": 739,\n   \"digest\": \"sha256:90b637d85a93c3dc03fc7a97d1fd640013c3f98c7c362d1156560bbd01f6a419\",\n   \"platform\": {\n   \"architecture\": \"arm64\",\n   \"os\": \"linux\"\n   }\n  }\n ]\n```In this output, the `manifests` section includes two manifests, one with the `amd64` platform architecture, and the other with the `arm64` platform architecture.When you deploy this container image to your cluster, GKE automatically downloads only the image that matches the node's architecture.\n### Deploy the multi-arch version of your application\n- Before you deploy the multi-arch image, delete the original workloads:```\nkubectl delete deploy x86-hello-deployment arm-hello-deployment\n```\n- Inspect the `add_multiarch_support.yaml` kustomize overlay by running `cat k8s/overlays/multiarch/add_multiarch_support.yaml` :The output includes the following toleration set:```\n tolerations:\n  - key: kubernetes.io/arch\n   operator: Equal\n   value: arm64\n   effect: NoSchedule\n```This toleration allows the workload to run on the Arm nodes in your cluster, since the toleration matches the taint set on all Arm nodes. As this workload can now run on any node in the cluster, only the toleration is needed. With just the toleration, GKE can schedule the workload to both x86 and Arm nodes. If you want to specify where GKE can schedule workloads, use node selectors and node affinity rules. To learn more about setting these fields, see [Prepare an Arm workload for deployment](/kubernetes-engine/docs/how-to/prepare-arm-workloads-for-deployment) .\n- Deploy the multi-arch container image with 6 replicas:```\n$(cd k8s/overlays/multiarch && kustomize edit set image hello=us-central1-docker.pkg.dev/$PROJECT_ID/docker-repo/multiarch-hello:v0.0.1) kubectl apply -k k8s/overlays/multiarch\n```\n- Wait 10 seconds, then confirm that all of the replicas of the application are running:```\nkubectl get pods -l=\"app=hello\" -o wide\n```The output is similar to the following:```\nNAME           READY STATUS RESTARTS AGE IP   NODE          NOMINATED NODE READINESS GATES\nmultiarch-hello-deployment-65bfd784d-5xrrr 1/1  Running 0   95s 10.100.1.5 gke-my-cluster-arm-pool-e172cff7-shwc  <none>   <none>\nmultiarch-hello-deployment-65bfd784d-7h94b 1/1  Running 0   95s 10.100.1.4 gke-my-cluster-arm-pool-e172cff7-shwc  <none>   <none>\nmultiarch-hello-deployment-65bfd784d-7qbkz 1/1  Running 0   95s 10.100.1.7 gke-my-cluster-arm-pool-e172cff7-shwc  <none>   <none>\nmultiarch-hello-deployment-65bfd784d-7wqb6 1/1  Running 0   95s 10.100.1.6 gke-my-cluster-arm-pool-e172cff7-shwc  <none>   <none>\nmultiarch-hello-deployment-65bfd784d-h2g2k 1/1  Running 0   95s 10.100.0.19 gke-my-cluster-default-pool-32019863-b41t <none>   <none>\nmultiarch-hello-deployment-65bfd784d-lc9dc 1/1  Running 0   95s 10.100.0.18 gke-my-cluster-default-pool-32019863-b41t <none>   <none>\n```This output includes a `NODE` column that indicates the Pods are running on both nodes in the Arm node pool and others in the default (x86) node pool.\n- Run the following command to access the service and see the output:```\nfor i in $(seq 1 6); do curl -w '\\n' http://$external_ip; done\n```The output is similar to the following:```\nHello from NODE:gke-my-cluster-arm-pool-e172cff7-shwc, POD:multiarch-hello-deployment-65bfd784d-7qbkz, CPU PLATFORM:linux/arm64\nHello from NODE:gke-my-cluster-default-pool-32019863-b41t, POD:multiarch-hello-deployment-65bfd784d-lc9dc, CPU PLATFORM:linux/amd64\nHello from NODE:gke-my-cluster-arm-pool-e172cff7-shwc, POD:multiarch-hello-deployment-65bfd784d-5xrrr, CPU PLATFORM:linux/arm64\nHello from NODE:gke-my-cluster-arm-pool-e172cff7-shwc, POD:multiarch-hello-deployment-65bfd784d-7wqb6, CPU PLATFORM:linux/arm64\nHello from NODE:gke-my-cluster-arm-pool-e172cff7-shwc, POD:multiarch-hello-deployment-65bfd784d-7h94b, CPU PLATFORM:linux/arm64\nHello from NODE:gke-my-cluster-arm-pool-e172cff7-shwc, POD:multiarch-hello-deployment-65bfd784d-7wqb6, CPU PLATFORM:linux/arm64\n```You should see that Pods running across architecture platforms are answering the requests. **Note:** It is possible that you run this command and receive only responses from Pods running on one architecture platform. If this occurs, run the command once or twice more and you should responses from both architecture platforms.\nYou built and deployed a multi-arch image to seamlessly run a workload across multiple architectures.## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\nAfter you finish the tutorial, you can clean up the resources that you created to reduce quota usage and stop billing charges. The following sections describe how to delete or turn off these resources.\n### Delete the project\nThe easiest way to eliminate billing is to delete the project that you created for the tutorial.\nTo delete the project:\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.### Delete the service, cluster, and repositoryIf you don't want to delete the entire project, delete the cluster and repository that you created for the tutorial:- Delete the application's Service by running [kubectl delete](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#delete) :```\nkubectl delete service hello-service\n```This command deletes the Compute Engine load balancer that you created when you exposed the Deployment.\n- Delete your cluster by running [gcloud container clusters delete](/sdk/gcloud/reference/container/clusters/delete) :```\ngcloud container clusters delete $CLUSTER_NAME --zone $ZONE\n```\n- Delete the repository:```\ngcloud artifacts repositories delete docker-repo \u2014location=us-central1 --async\n```\n## What's next\n- [Arm workloads on GKE](/kubernetes-engine/docs/concepts/arm-on-gke) \n- [Create clusters and node pools with Arm nodes](/kubernetes-engine/docs/how-to/create-arm-clusters-nodes) \n- [Build multi-architecture images for Arm workloads](/kubernetes-engine/docs/how-to/build-multi-arch-for-arm) \n- [Prepare an Arm workload for deployment](/kubernetes-engine/docs/how-to/prepare-arm-workloads-for-deployment) \n- [Prepare Autopilot workloads on Arm architecture](/kubernetes-engine/docs/how-to/autopilot-arm-workloads) \n- [Best practices for running cost-optimized Kubernetes applications on GKE](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke) \n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Google Kubernetes Engine (GKE)"}