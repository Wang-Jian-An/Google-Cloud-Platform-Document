{"title": "Cloud Architecture Center - Transferring data from Amazon S3 to Cloud Storage using VPC Service Controls and Storage Transfer Service", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Transferring data from Amazon S3 to Cloud Storage using VPC Service Controls and Storage Transfer Service\nThis tutorial describes how to [harden](https://wikipedia.org/wiki/Hardening_(computing)) data transfers from Amazon Simple Storage Service (Amazon S3) to [Cloud Storage](/storage/docs) using [Storage Transfer Service](/storage-transfer/docs) with a [VPC Service Controls](/vpc-service-controls) perimeter. This tutorial is intended for data owners who have data that resides in Amazon S3, and who want to process or migrate that data securely to Google Cloud.\nThis tutorial assumes that you're familiar with Amazon Web Services (AWS) and the fundamentals of working with data in object stores. This tutorial applies a service account-based method of controlling access by using [Access Context Manager](/access-context-manager/docs) . For more advanced access levels beyond the service account-based method, see [Creating an access level](/access-context-manager/docs/create-basic-access-level) .", "content": "## ArchitectureThe following diagram outlines the VPC Service Controls architecture.In the preceding diagram, VPC Service Controls explicitly denies communication between Google Cloud services unless both projects are in the controlled perimeter.## Objectives\n- Configure AWS access.\n- Create VPC Service Controls perimeter.\n- Create an access policy and access level by using Access Context Manager.\n- Use Storage Transfer Service to move data between Amazon S3 and Cloud Storage.\n- Schedule Storage Transfer Service to retrieve data on a schedule.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Cloud Storage](/storage/pricing) \nThere are no extra costs to use Storage Transfer Service; however, Cloud Storage pricing and external provider costs apply when using Storage Transfer Service.\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .\nIn addition to Google Cloud resources, this tutorial uses the following Amazon Web Services (AWS) resources, which might have costs:- [Amazon S3](https://calculator.s3.amazonaws.com/index.html) \n## Before you begin- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) At the bottom of the Google Cloud console, a [Cloud Shell](/shell/docs/how-cloud-shell-works) session starts and displays a command-line prompt. Cloud Shell is a shell environment  with the Google Cloud CLI  already installed and with values already set for  your current project. It can take a few seconds for the session to initialize.\n- In the Google Cloud console, go to the **IAM and Admin** page to give your account the role of Storage Admin and Access Context Manager Admin. [Go to the IAM and Admin page](https://console.cloud.google.com/iam-admin/iam) \n- The [Storage Admin role](/storage/docs/access-control/iam-roles#standard-roles) has the following permissions:- `firebase.projects.get`\n- `resourcemanager.projects.get`\n- `resourcemanager.projects.list`\n- `storage.buckets.*`\n- `storage.objects.*`- The [Access Context Manager Admin role](/access-context-manager/docs/access-control) has the following permissions:- `accesscontextmanager.accessLevels.*`\n- `accesscontextmanager.accessPolicies.*`\n- `accesscontextmanager.accessPolicies.setIamPolicy`\n- `accesscontextmanager.accessPolicies.update`\n- `accesscontextmanager.accessZones.*`\n- `accesscontextmanager.policies.*`\n- `accesscontextmanager.servicePerimeters.*`\n- `resourcemanager.organizations.get`\n## Configuring AWS accessIn this tutorial, you work with existing AWS Identity and Access Management (AWS IAM) users to create an AWS IAM policy to interface with StorageTransfer Service. These policies and users are needed to authenticate your connection to Google Cloud and to help secure your data in transit. This tutorial requires that you have an Amazon S3 bucket to transfer data from; you can use an existing Amazon S3 bucket or you can [create a new bucket](https://docs.aws.amazon.com/quickstarts/latest/s3backup/step-1-create-bucket.html) . You can use a test or sandbox AWS account to avoid affecting production resources in the same account.\n### Create an AWS IAM policy for Storage Transfer Service and apply it to your bucket\n- In the AWS Management Console, go to the **IAM** page.\n- Click **Policies** , and then click **Create Policy** .\n- In the visual editor, click **IAM Policy** .\n- Click **S3** .\n- Select the following **Access Level** checkboxes:- **List** \n- **Read** \n- **Write** \n- In the **Resources** pane, click **Specific** .\n- In the **Bucket** pane, click **Add ARN** .\n- In the **Bucket Name** field, enter the name of the bucket where you're transferring data from.\n- Click **Review Policy** and enter a name such as `transfer-user-policy` .\n- Click **Create Policy** .\n### Add AWS IAM users to your AWS IAM policy\n- In the AWS Management Console, go to the **IAM** page.\n- Click **Users** , and then click **Add User** .\n- In the **Name** field, enter`transfer-user`.\n- For **Access Type** , click **Programmatic Access** and attach the`transfer-user-policy`that you created for that user.\n- After you create the user, make a note of your access ID and secret key pair because it's used later in the tutorial.\n- Click **Save** .\n## Creating a Cloud Storage bucketBefore you can enable your VPC Service Controls perimeter, you need to create a Cloud Storage bucket.- In the Google Cloud console, go to the **Cloud Storage Browser** . [Go to the Cloud Storage Browser page](https://console.cloud.google.com/storage/browser) \n- Click **Create bucket** .\n- In the **Name** field, enter a name, such as `` `-destination-bucket` where `` represents your Google Cloud project ID.\n- For the **Default storage class** for the bucket, click **Regionalstorage** .\n- In the **Location** drop-down list, click a region where the bucket data is stored.\n- Click **Create** .\n## Finding the name of your transfer operations service accountStorage Transfer Service uses a Google-managed service account to communicate with Cloud Storage and Pub/Sub resources within your project. You need to determine the name of your service account because it is used later in this tutorial. If you haven't used Storage Transfer Service before, the following creates the Storage Transfer Service service account for you. For more information about Google-managed service accounts, see [Service accounts](/iam/docs/service-account-types#google-managed) .- To determine the name of your service account, go to the [Storage Transfer Service API](/storage-transfer/docs/reference/rest/v1/googleServiceAccounts/get) page.\n- In the **String** field, enter your Google Cloud project ID.The name of the service account is typically in the following format: `project-` `` `@storage-transfer-service.iam.gserviceaccount.com`\n **Note:** The service project that hosts the transfers doesn't need to be in the same project as the destination project for the data. If you're managing large numbers of transfer operations to different VPCs, you can break those out into their own project. For the purposes of this tutorial, the data is contained within a single project.## Creating your access policy in Access Context ManagerAn [access policy](/vpc-service-controls/docs/service-perimeters#create_an_access_policy) collects the service perimeters and access levels you create for your organization. An organization can only have one access policy.- In the Google Cloud console, go to the **Settings** page. [Go to the Settings page](https://console.cloud.google.com/iam-admin/settings) \n- Make a note of your Google Cloud project ID and the organization name.\n- In Cloud Shell, create a policy:```\ngcloud access-context-manager policies create \\\n --organization organization-id --title policy-title\n```- ``is the organization ID that you found earlier.\n- ``is the title of the perimeter. For example,`Example-Company-Access-Policy`.\nThe output is as follows:```\nCreate request issued\nWaiting for operation [accessPolicies/policy-title/create/policy-number] to complete...done.\nCreated.\n````` represents a unique ID assigned to the policy title.\n## Creating your VPC Service Controls perimeterWhen you create the VPC Service Controls perimeter, you start with no traffic allowed in. Then, you create an explicit access level to allow the transfer operation to send data into the controlled perimeter.- In the Google Cloud console, go to the **VPC Service Controls** page. [Go to the VPC Service Controls page](https://console.cloud.google.com/security/service-perimeter?_ga=2.156225208.-264738467.1556819595) \n- Click **New Perimeter** .\n- In the **Name** field, enter a name for the perimeter, such as `data-transfer-perimeter` .\n- Leave **Regular Perimeter** selected.\n- Click **Add project** and add the project that you created through this tutorial to the list of projects to protect.\n- Click **Cloud Storage API** .\n- Leave **Access Levels** at the default value.\n- Click **Save** .\n## Creating an access level in the access policyIn this section, you limit access into the VPC through the [service account](/access-context-manager/docs/create-basic-access-level#members-example) .- In Cloud Shell, create a YAML file called `conditions.yaml` that lists the principals that you want to provide access to:```\n - members:\n  - serviceAccount:project-project-number@storage-transfer-service.iam.gserviceaccount.com\n  - user:sysadmin@example.com\n \n``` **Note:** For this tutorial, you replace `` with your username on the access control list to illustrate an end-to-end solution. In production, we recommend that you remove user accounts from this step.\n- Create the access level:```\ngcloud access-context-manager levels create name \\\n --title title \\\n --basic-level-spec ~./conditions.yaml \\\n --combine-function=OR \\\n --policy=policy-id\n```- ``is the unique name for the access level. It must begin with a letter and include only letters, numbers, and underscores.\n- ``is a title that is unique to the policy, such as`trusted-identity-ingest`.\n- ``is the ID (number) of your organization's access policy.\n- `combine-function`is set to`OR`. The default value`AND`requires thatconditions be met before an access level is granted. The`OR`value gives the principals access even if other conditions, such as IP address or those inherited from other required access levels aren't met.\nThe output is similar to the following:```\nCreate request issued for: name\nWaiting for operation [accessPolicies/policy-id/accessLevels/name/create/access-level-number] to complete...done.\nCreated level name.\n``` represents a unique ID assigned to the access level.\n## Binding the access level to VPC Service Controls\n- In the Google Cloud console, go to **VPC Service Controls** . [Go to the VPC Service Controls page](https://console.cloud.google.com/security/service-perimeter?_ga=2.156225208.-264738467.1556819595) \n- Click **Edit** for **VPC Service Control** .\n- Click **Access Level** and select the **trusted-identity-ingest** access level.\n- Click **Save** .\nNow the only operations that are allowed in the controlled perimeter are from the service account that you defined.## Initiating the transfer\n- In the Google Cloud console, go to the **Transfer** page. [Go to the Transfer page](https://console.cloud.google.com/transfer/cloud) \n- Click **Create transfer** .\n- Click **Amazon S3 bucket** .\n- In the **Amazon S3 bucket** field, enter the source Amazon S3 bucket name as it appears in the AWS Management Console.\n- Enter the **Access key ID** and **Secret key** associated with the Amazon S3 bucket. You copied these values at the beginning of this tutorial.\n- In **Select destination** , enter the name of the bucket that you created in your perimeter, such as `` `-destination-bucket` .\n- For **Configure transfer** , schedule your transfer job to **Run now** .\n- Optional: Edit the transfer job name.\n- For **Description** use a unique, descriptive name to help you identify your transfer job later.\n- Click **Create** .\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .\n- See more advanced ways to enable access levels by using [Access Context Manager](/vpc-service-controls/docs)", "guide": "Cloud Architecture Center"}