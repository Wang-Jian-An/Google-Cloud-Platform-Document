{"title": "Cloud Architecture Center - Deploy a cold recoverable web server with persistent disk snapshots", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Deploy a cold recoverable web server with persistent disk snapshots\nLast reviewed 2021-08-04 UTC\nThe document describes how to deploy a cold failover topology for a web server by using a [managed instance group](/compute/docs/instance-groups) and [persistent disk snapshots](/compute/docs/disks/snapshots) . This document is intended for architects and people who work in operations and administrative teams.\nYou create a managed instance group that runs a single VM with a persistent disk that stores data. Scheduled snapshots of the persistent disk minimize data loss in a failover scenario. An [external Application Load Balancer](/load-balancing/docs/load-balancing-overview) directs users to the VM that runs in the managed instance group, as shown in the following diagram:If there's an instance failure, the managed instance group tries to recreate the VM in the same zone. If the failure is at the zone level, Cloud Monitoring or similar can let you know there's a problem and you manually create another managed instance group in another zone or region. In either failover scenario, the platform uses the latest persistent disk snapshot to create a replacement disk and attach it to the new VM in the instance group.\nIn this document, you use the external IP address of the VM or the load balancer to view a basic page on the web server. This approach lets you test the cold failover pattern if you don't have a registered domain name, and without any DNS changes. In a production environment, [create and configure a Cloud DNS zone and record](/dns/docs/set-up-dns-records-domain-name) that resolves to the external IP address assigned to the load balancer.\nThis pattern balances the cost difference of running multiple VMs or regional persistent disks with maintaining a certain level of data protection. Your costs are lower as you run one VM and persistent disk, but there's a risk of data loss as the persistent disk snapshots are only taken at a set interval. To reduce your potential data loss, consider deploying a [cold recoverable web server that uses regional persistent disks](/architecture/cold-recoverable-apps-regional-persistent-disks) instead.\nThe following table outlines some high-level differences in data protection options for cold recoverable approaches that use regional persistent disks or persistent disk snapshots. For more information, see [High availability options using persistent disks](/compute/docs/disks/high-availability-regional-persistent-disk) .\n| Unnamed: 0         | Regional persistent disks                    | Persistent disk snapshots                                           |\n|:-------------------------------------------|:------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Data loss - recovery point objective (RPO) | Zero for a single failure, such as sustained outage in a zone or network disconnect.     | Any data since the last snapshot was taken, which is typically one hour or more. The potential data loss depends on your snapshot schedule that controls how frequently snapshots are taken.   |\n| Recovery time objective (RTO)    | Deployment time for a new VM, plus several seconds for the regional persistent disk to be reattached. | Deployment time for a new VM, plus time to create a new persistent disk from the latest snapshot. The disk create time depends on the size of the snapshot, and could take tens of minutes or hours. |\n| Cost          | Storage costs double as the regional persistent disk is replicated continuously to another zone.  | You only pay for the amount of snapshot space consumed.                                    |\n| nan          | For more information, see Disks and images pricing.             | For more information, see Disks and images pricing.                                     |", "content": "## Objectives\n- Create a managed instance group to run a VM with a persistent disk.\n- Configure a snapshot schedule to take regular snapshots of the persistent disk.\n- Create an instance template and startup script.\n- Create and configure an external Application Load Balancer.\n- Test the cold web server failover with a replacement managed instance group.\n## Costs\nIn this document, you use the following billable components of Google Cloud:- [Compute Engine](https://cloud.google.com/compute/vm-instance-pricing) \n- [Networking](https://cloud.google.com/vpc/pricing) \n- [Persistent Disk](https://cloud.google.com/compute/disks-image-pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . ## Before you begin- You can run the Google Cloud CLI in the [Google Cloud console](https://console.cloud.google.com/) without installing the Google Cloud CLI. To run the gcloud CLI in the Google Cloud console, use Cloud Shell.## Prepare the environmentIn this section, you define some variables for your resource names and locations. These variables are used by the Google Cloud CLI commands as you deploy the resources.\nThroughout this document, unless otherwise noted, you enter all commands in Cloud Shell or your local development environment.- Replace `` with your own project ID. If required, provide your own name suffix for resources, such as `` .Specify a region, such as `` , and two zones within that region, such as `` and `` . These zones define where the initial persistent disk and managed instance group is deployed and where you can manually fail over to if needed.```\nPROJECT_ID=PROJECT_IDNAME_SUFFIX=appREGION=us-central1ZONE1=us-central1-aZONE2=us-central1-f\n```\n## Create a VPC and subnetTo provide network access to the VMs, create a Virtual Private Cloud (VPC) and subnet. As the managed instance group works across zones within a single region, only one subnet is created. For more information on the advantages of the custom subnet mode to manage IP address ranges in use in your environment, see [Use custom mode VPC networks](/solutions/best-practices-vpc-design#custom-mode) .- Create the VPC with a custom subnet mode:```\ngcloud compute networks create network-$NAME_SUFFIX \\\u00a0 \u00a0 --subnet-mode=custom\n```If you see a Cloud Shell prompt, authorize this first request to make API calls.\n- Create a subnet in the new VPC. Define your own address range, such as `` , that fits in your network range:```\ngcloud compute networks subnets create subnet-$NAME_SUFFIX-$REGION \\\u00a0 \u00a0 --network=network-$NAME_SUFFIX \\\u00a0 \u00a0 --range=10.1.0.0/20 \\\u00a0 \u00a0 --region=$REGION\n```\n## Create firewall rules\n- Create firewall rules to allow web traffic and health checks for the load balancer and managed instance groups:```\ngcloud compute firewall-rules create allow-http-$NAME_SUFFIX \\\u00a0 \u00a0 --network=network-$NAME_SUFFIX \\\u00a0 \u00a0 --direction=INGRESS \\\u00a0 \u00a0 --priority=1000 \\\u00a0 \u00a0 --action=ALLOW \\\u00a0 \u00a0 --rules=tcp:80 \\\u00a0 \u00a0 --source-ranges=0.0.0.0/0 \\\u00a0 \u00a0 --target-tags=http-servergcloud compute firewall-rules create allow-health-check-$NAME_SUFFIX \\\u00a0 \u00a0 --network=network-$NAME_SUFFIX \\\u00a0 \u00a0 --action=allow \\\u00a0 \u00a0 --direction=ingress \\\u00a0 \u00a0 --source-ranges=130.211.0.0/22,35.191.0.0/16 \\\u00a0 \u00a0 --target-tags=allow-health-check \\\u00a0 \u00a0 --rules=tcp:80\n```The HTTP rule allows traffic to any VM where the `http-server` tag is applied, and from any source using the `0.0.0.0/0` range. For the [health check rule](/load-balancing/docs/health-checks#firewall_rules) , default ranges for Google Cloud are set to allow the platform to correctly check the health of resources.\n- To allow SSH traffic for the initial configuration of a base VM image, scope the firewall rule to your environment using the `--source-range` parameter. You might need to work with your network team to determine what source ranges your organization uses. **Caution:** We don't recommend using a broad `0.0.0.0/0` range that would allow all traffic. To scope traffic to a single IP address, use a network mask, such as `35.230.62.163/32` .Replace `` with your own IP address scopes:```\ngcloud compute firewall-rules create allow-ssh-$NAME_SUFFIX \\\u00a0 \u00a0 --network=network-$NAME_SUFFIX \\\u00a0 \u00a0 --direction=INGRESS \\\u00a0 \u00a0 --priority=1000 \\\u00a0 \u00a0 --action=ALLOW \\\u00a0 \u00a0 --rules=tcp:22 \\\u00a0 \u00a0 --source-ranges=IP_ADDRESS_SCOPE\n```\n- After you create the firewall rules, verify that the three rules have been added:```\ngcloud compute firewall-rules list \\\u00a0 \u00a0 --project=$PROJECT_ID \\\u00a0 \u00a0 --filter=\"NETWORK=network-$NAME_SUFFIX\"\n```The following example output shows the three rules have been correctly created:```\nNAME     NETWORK  DIRECTION PRIORITY ALLOW\nallow-health-check-app network-app INGRESS 1000  tcp:80\nallow-http-app   network-app INGRESS 1000  tcp:80\nallow-ssh-app   network-app INGRESS 1000  tcp:22\n```\n## Create and configure a base VM imageTo create identical VMs that you deploy without additional configuration, you use a custom VM image. This image captures the OS and Apache configuration, and is used to create each VM in the managed instance group in the next steps.\nYou use a persistent disk to store the application data. In this document, you use a basic Apache website to serve the application. Later in this document, you create a snapshot schedule that's attached to this persistent disk to create automated disk snapshots.\n **Note:** In this document, you manually create and configure the base VM and images. In production deployments, use your existing deployment tools and pipelines to automate as much of this process as possible. For more information, see [Managing infrastructure as code with Terraform, Cloud Build, and GitOps](/architecture/managing-infrastructure-as-code) .\nOn the VM, you create a basic `index.html` file on the persistent disk and mount it to `/var/www/example.com` . An Apache configuration file at `/etc/apache2/sites-available/example.com.conf` serves web content from the mounted persistent disk location.\nThe following diagram shows the basic HTML page served by Apache that's stored on the persistent disk:You build this environment in the following steps.- Create a 10 GiB SSD. Understand your storage needs and the associated costs of paying for the provisioned space, not consumed space. For more information, see [persistent disk pricing](https://cloud.google.com/compute/vm-instance-pricing#disk) .```\ngcloud compute disks create disk-$NAME_SUFFIX \\\u00a0 \u00a0 --zone $ZONE1 \\\u00a0 \u00a0 --size=10 \\\u00a0 \u00a0 --type=pd-ssd\n```\n- Create a base VM with the attached persistent disk:```\ngcloud compute instances create vm-base-$NAME_SUFFIX \\\u00a0 \u00a0 --zone=$ZONE1 \\\u00a0 \u00a0 --machine-type=n1-standard-1 \\\u00a0 \u00a0 --subnet=subnet-$NAME_SUFFIX-$REGION \\\u00a0 \u00a0 --tags=http-server \\\u00a0 \u00a0 --image=debian-10-buster-v20210721 \\\u00a0 \u00a0 --image-project=debian-cloud \\\u00a0 \u00a0 --boot-disk-size=10GB \\\u00a0 \u00a0 --boot-disk-type=pd-balanced \\\u00a0 \u00a0 --boot-disk-device-name=vm-base-$NAME_SUFFIX \\\u00a0 \u00a0 --disk=mode=rw,name=disk-$NAME_SUFFIX,device-name=disk-$NAME_SUFFIX\n```You use parameters defined at the start of this document to name the VM and connect to the correct subnet. Names are also assigned from the parameters for the boot disk and data disk.\n- To install and configure the simple website, first connect to the base VM using SSH:```\ngcloud compute ssh vm-base-$NAME_SUFFIX --zone=$ZONE1\n```\n- In your SSH session to the VM, create a script to configure the VM in an editor of your choice. The following example uses [Nano](https://www.nano-editor.org/) as the editor:```\nnano configure-vm.sh\n```Paste the following configuration script into the file. Update the `` variable to match the value set at the start of this document, such as :```\n#!/bin/bashNAME_SUFFIX=app# Create directory for the basic website filessudo mkdir -p /var/www/example.comsudo chmod a+w /var/www/example.comsudo chown -R www-data: /var/www/example.com# Find the disk name, then format and mount itDISK_NAME=\"google-disk-$NAME_SUFFIX\"DISK_PATH=\"$(find /dev/disk/by-id -name \"${DISK_NAME}\" | xargs -I '{}' readlink -f '{}')\"sudo mkfs.ext4 -m 0 -E lazy_itable_init=0,lazy_journal_init=0,discard $DISK_PATHsudo mount -o discard,defaults $DISK_PATH /var/www/example.com# Install Apache, additional utilities, and cloud-initsudo apt-get update && sudo apt-get -y install apache2 moreutils cloud-init# Write out a basic HTML file to the mounted persistent disksudo tee -a /var/www/example.com/index.html >/dev/null <<'EOF'<!doctype html><html lang=en><head><meta charset=utf-8>\u00a0 \u00a0 <title>HA / DR example</title></head><body>\u00a0 \u00a0 <p>Welcome to a test web server with persistent disk snapshots!</p></body></html>EOF# Write out an Apache configuration filesudo tee -a /etc/apache2/sites-available/example.com.conf >/dev/null <<'EOF'<VirtualHost *:80>\u00a0 \u00a0 \u00a0 \u00a0 ServerName www.example.com\u00a0 \u00a0 \u00a0 \u00a0 ServerAdmin webmaster@localhost\u00a0 \u00a0 \u00a0 \u00a0 DocumentRoot /var/www/example.com\u00a0 \u00a0 \u00a0 \u00a0 ErrorLog ${APACHE_LOG_DIR}/error.log\u00a0 \u00a0 \u00a0 \u00a0 CustomLog ${APACHE_LOG_DIR}/access.log combined</VirtualHost>EOF# Enable the Apache configuration file and reload servicesudo a2dissite 000-defaultsudo a2ensite example.com.confsudo systemctl reload apache2\n```\n- Write out the file and exit your editor. For example, in Nano you use `Ctrl-O` to write out the file, then exit with `Ctrl-X` .\n- Make the configuration script executable, then run it:```\nchmod +x configure-vm.sh./configure-vm.sh\n```\n- If there's an instance failure and the managed instance group needs to create a replacement from this base VM, the application data must be available. The following steps should automatically run on each new VM:- Get some info from the metadata server.\n- Get the latest snapshot for the persistent disk.\n- Create a disk from this latest snapshot.\n- Attach the new disk to the VM.\n- Mount the disk within the VM.\nCreate a startup script named `app-startup.sh` that performs these steps required for the VM. This startup script is applied to an instance template in a following step.```\nsudo mkdir /opt/cloud-init-scriptssudo tee -a /opt/cloud-init-scripts/app-startup.sh >/dev/null <<'EOF'#!/bin/bash# Install jq and get an access token for API requestsapt-get install -y jqOAUTH_TOKEN=$(curl \"http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token\" \\\u00a0 \u00a0 -H \"Metadata-Flavor: Google\" --silent | jq -r '.access_token')# Make a request against the metadata server to determine the project name,# instance name, and what zone it's running inZONE_INFO=$(curl http://metadata.google.internal/computeMetadata/v1/instance/zone \\\u00a0 \u00a0 -H \"Metadata-Flavor: Google\" --silent)PROJECT_NAME=$(curl http://metadata.google.internal/computeMetadata/v1/instance/zone \\\u00a0 \u00a0 -H \"Metadata-Flavor: Google\" --silent | awk -v FS=\"/\" '{print $2}')ZONE_NAME=$(curl http://metadata.google.internal/computeMetadata/v1/instance/zone \\\u00a0 \u00a0 -H \"Metadata-Flavor: Google\" --silent | sed 's:.*/::')INSTANCE_NAME=$(curl http://metadata.google.internal/computeMetadata/v1/instance/name \\\u00a0 \u00a0 -H \"Metadata-Flavor: Google\" --silent)# Get the latest snapshot of the app diskLATEST_SNAPSHOT=$(curl -X GET -H \"Authorization: Bearer $OAUTH_TOKEN\" \\\u00a0 \u00a0 https://compute.googleapis.com/compute/v1/projects/$PROJECT_NAME/global/snapshots \\\u00a0 \u00a0 --silent | jq -s '.[].items[] | select(.name | contains(\"disk-$NAME\")) | .name' \\\u00a0 \u00a0 | sort -r | head -n 1 | tr -d '\"')# Create a persistent disk using the latest persistent disk snapshotcurl -X POST -H \"Authorization: Bearer $OAUTH_TOKEN\" -H \"Content-Type: application/json; charset=utf-8\" \\\u00a0 \u00a0 https://compute.googleapis.com/compute/v1/$ZONE_INFO/disks \\\u00a0 \u00a0 --data '{\"name\":\"'$LATEST_SNAPSHOT'-restored\",\"sizeGb\":\"10\",\"type\":\"zones/'$ZONE_NAME'/diskTypes/pd-ssd\",\"sourceSnapshot\":\"https://www.googleapis.com/compute/v1/projects/'$PROJECT_NAME'/global/snapshots/'$LATEST_SNAPSHOT'\"}'# Wait for the persistent disk to be created from the disk snapshotDISK_STATUS=$(curl -X GET -H \"Authorization: Bearer $OAUTH_TOKEN\" \\\u00a0 \u00a0 https://compute.googleapis.com/compute/v1/projects/$PROJECT_NAME/zones/$ZONE_NAME/disks/$LATEST_SNAPSHOT-restored \\\u00a0 \u00a0 --silent | jq -r .status)while [ $DISK_STATUS != \"READY\" ]do\u00a0 \u00a0 sleep 2\u00a0 \u00a0 DISK_STATUS=$(curl -X GET -H \"Authorization: Bearer $OAUTH_TOKEN\" \\\u00a0 \u00a0 \u00a0 \u00a0 https://compute.googleapis.com/compute/v1/projects/$PROJECT_NAME/zones/$ZONE_NAME/disks/$LATEST_SNAPSHOT-restored \\\u00a0 \u00a0 \u00a0 \u00a0 --silent | jq -r .status)done# Attach the new persistent disk created from the snapshot to the VMcurl -X POST -H \"Authorization: Bearer $OAUTH_TOKEN\" -H \"Content-Type: application/json; charset=utf-8\" \\\u00a0 \u00a0 https://compute.googleapis.com/compute/v1/$ZONE_INFO/instances/$INSTANCE_NAME/attachDisk \\\u00a0 \u00a0 --data '{ \"source\": \"/compute/v1/'$ZONE_INFO'/disks/'$LATEST_SNAPSHOT'-restored\"}'# Wait for the persistent disk to be attached before mountingATTACH_STATUS=$(curl -X GET -H \"Authorization: Bearer $OAUTH_TOKEN\" \\\u00a0 \u00a0 https://compute.googleapis.com/compute/v1/projects/$PROJECT_NAME/zones/$ZONE_NAME/instances/$INSTANCE_NAME \\\u00a0 \u00a0 --silent | jq '.disks[] | select(.source | contains(\"disk-\"))')while [ -z \"$ATTACH_STATUS\" ]do\u00a0 \u00a0 sleep 2\u00a0 \u00a0 ATTACH_STATUS=$(curl -X GET -H \"Authorization: Bearer $OAUTH_TOKEN\" GET \\\u00a0 \u00a0 \u00a0 \u00a0 https://compute.googleapis.com/compute/v1/projects/$PROJECT_NAME/zones/$ZONE_NAME/instances/$INSTANCE_NAME \\\u00a0 \u00a0 \u00a0 \u00a0 --silent | jq '.disks[] | select(.source | contains(\"disk-\"))')done# With the disk attached, mount the disk and restart Apacheecho UUID=`blkid -s UUID -o value /dev/sdb` /var/www/example.com ext4 discard,defaults,nofail 0 2 \\\u00a0 \u00a0 | tee -a /etc/fstabmount -asystemctl reload apache2# Remove jq so it's not left on the VMapt-get remove -y jqEOF\n```\n- To apply the `` variable you defined at the start of the document into the startup script, such as `` , use the `envsubst` command:```\nexport NAME=appenvsubst '$NAME' < \"/opt/cloud-init-scripts/app-startup.sh\" \\\u00a0 \u00a0 | sudo sponge \"/opt/cloud-init-scripts/app-startup.sh\"\n```\n- Exit the SSH session to the VM:```\nexit\n```\n- Get the IP address of the VM and use `curl` to see the basic web page:```\ncurl $(gcloud compute instances describe vm-base-$NAME_SUFFIX \\\u00a0 \u00a0 --zone $ZONE1 \\\u00a0 \u00a0 --format=\"value(networkInterfaces.accessConfigs.[0].natIP)\")\n```The basic website is returned, as shown in the following example output:```\n<!doctype html>\n<html lang=en>\n<head>\n<meta charset=utf-8>\n <title>HA / DR example</title>\n</head>\n<body>\n <p>Welcome to a test web server with persistent disk snapshots!</p>\n</body>\n</html>\n```This step confirms that Apache is configured correctly, and the page is loaded from the attached persistent disk. In the following sections, you create an image using this base VM and configure an instance template with a startup script.\n## Create a persistent disk snapshot scheduleTo make sure that VMs created in the managed instance group always have the latest data from the persistent disk, you create a snapshot schedule. This schedule takes automatic snapshots of a persistent disk at defined times, and controls how long to retain the snapshots. The following image shows how this snapshot process works:Think about your application needs and business goals in how often you should take snapshots - for example, a static website needs less frequent snapshots than an active application writing data to disk.\nFor more information on how to determine the best approach for your own applications and which recovery method to use, see the [disaster recovery planning guide](/architecture/dr-scenarios-planning-guide) .- In this scenario, you use a snapshot schedule to create regular persistent disk snapshots. You define this snapshot schedule in a [resource policy](/compute/docs/reference/rest/v1/resourcePolicies) . Resource policies let you define actions to run, and attach them to resources in your environment.In this resource policy, you define a schedule to create a snapshot with the following settings:- Take a snapshot every 4 hours, starting at 22:00 UTC\n- Retain the snapshots for 1 day\nConfigure this schedule as needed for your environment, such as the start time and how often you want to take the snapshots:```\ngcloud compute resource-policies create snapshot-schedule snapshot-schedule-$NAME_SUFFIX \\\u00a0 \u00a0 --description \"Snapshot persistent disk every 4 hours\" \\\u00a0 \u00a0 --max-retention-days 1 \\\u00a0 \u00a0 --start-time 22:00 \\\u00a0 \u00a0 --hourly-schedule 4 \\\u00a0 \u00a0 --region $REGION\n```For more information, see how to [use scheduled snapshots for persistent disks](/compute/docs/disks/scheduled-snapshots) .\n- To use the snapshot schedule, attach the resource policy to your persistent disk. Specify the name of your persistent disk and the resource policy created in the previous step:```\ngcloud compute disks add-resource-policies disk-$NAME_SUFFIX \\\u00a0 \u00a0 --resource-policies snapshot-schedule-$NAME_SUFFIX \\\u00a0 \u00a0 --zone $ZONE1\n```\n- You can't complete the rest of this document and see the managed instance group in action until the first disk snapshot has been created. Manually create a disk snapshot now, and let the resource policy snapshot schedule create additional snapshots as defined:```\ngcloud compute disks snapshot disk-$NAME_SUFFIX \\\u00a0 \u00a0 --zone=$ZONE1 \\\u00a0 \u00a0 --snapshot-names=disk-$NAME_SUFFIX-$(date \"+%Y%m%d%H%M%S\")\n```\n## Create a service accountEach VM in the managed instance group created in the next steps needs to run a startup script. This startup script creates a persistent disk from a snapshot, then attaches it to the VM. As a best security practice, create a new service account with only the permissions required to perform these disk operations. You then assign this service account to the VM.- Create a service account to use with the VMs in the managed instance group:```\ngcloud iam service-accounts create instance-sa-$NAME_SUFFIX \\\u00a0 \u00a0 --description=\"Service account for HA/DR example\" \\\u00a0 \u00a0 --display-name=\"HA/DR for VM instances\"\n```\n- Create a custom role and assign only the permissions required to perform the disk management tasks. The following permissions are required:- `compute.snapshots.list`\n- `compute.snapshots.useReadOnly`\n- `compute.disks.get`\n- `compute.disks.create`\n- `compute.instances.get`\n- `compute.instances.attachDisk`\n- `compute.disks.use`\n```\ngcloud iam roles create instance_snapshot_management_$NAME_SUFFIX \\\u00a0 \u00a0 --project=$PROJECT_ID \\\u00a0 \u00a0 --title=\"Snapshot management for VM instances\" \\\u00a0 \u00a0 --description=\"Custom role to allow an instance to create a persistent disk from a snapshot and attach to VM.\" \\\u00a0 \u00a0 --permissions=compute.snapshots.list,compute.snapshots.useReadOnly,compute.disks.get,compute.disks.create,compute.instances.get,compute.instances.attachDisk,compute.disks.use \\\u00a0 \u00a0 --stage=GA\n```\n- Add the required role bindings for the new service account:```\ngcloud projects add-iam-policy-binding $PROJECT_ID \\\u00a0 \u00a0 --member=\"serviceAccount:instance-sa-$NAME_SUFFIX@$PROJECT_ID.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role=\"projects/$PROJECT_ID/roles/instance_snapshot_management_$NAME_SUFFIX\"gcloud projects add-iam-policy-binding $PROJECT_ID \\\u00a0 \u00a0 --member=\"serviceAccount:instance-sa-$NAME_SUFFIX@$PROJECT_ID.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role=\"roles/iam.serviceAccountUser\"\n```\n## Create a VM image and instance templateTo create identical VMs that can be automatically deployed without additional configuration required, you use a custom VM image. This image captures the OS and Apache configuration. Each VM created in the managed instance group in the next steps uses this image.- Before you can create an image, you must stop the VM:```\ngcloud compute instances stop vm-base-$NAME_SUFFIX --zone=$ZONE1\n```\n- Create an image of the base VM configured in the previous section:```\ngcloud compute images create image-$NAME_SUFFIX \\\u00a0 \u00a0 --source-disk=vm-base-$NAME_SUFFIX \\\u00a0 \u00a0 --source-disk-zone=$ZONE1 \\\u00a0 \u00a0 --storage-location=$REGION\n```\n- You use [cloud-init](https://cloudinit.readthedocs.io/) to run the previous startup script the first time a VM in the managed instance groups boots. A regular startup script applied to the VM runs every time the VM boots, such as if you the VM reboots after updates.Create a cloud-init configuration file to use with the instance template:```\ntee -a cloud-init.yaml >/dev/null <<'EOF'#cloud-configruncmd:\u00a0- [ bash, /opt/cloud-init-scripts/app-startup.sh ]EOF\n```\n- Create an instance template that applies the cloud-init configuration to run the startup script that creates a disk from snapshot then attaches and mounts the disk to the VM:```\ngcloud compute instance-templates create template-$NAME_SUFFIX \\\u00a0 \u00a0 --machine-type=n1-standard-1 \\\u00a0 \u00a0 --subnet=projects/$PROJECT_ID/regions/$REGION/subnetworks/subnet-$NAME_SUFFIX-$REGION \\\u00a0 \u00a0 --tags=http-server \\\u00a0 \u00a0 --image=image-$NAME_SUFFIX \\\u00a0 \u00a0 --scopes cloud-platform \\\u00a0 \u00a0 --service-account=\"instance-sa-$NAME_SUFFIX@$PROJECT_ID.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --metadata-from-file user-data=cloud-init.yaml\n```\n## Create a managed instance groupA managed instance group runs the VMs. The managed instance group runs in a defined zone, and monitors the health of the VMs. If there's a failure and the VM stops running, the managed instance group tries to recreate another VM in the same zone and creates a persistent disk from the latest snapshot. If the failure is at the zone level, you must manually perform the cold failover and create another managed instance group in a different zone. The same custom image and instance template automatically configures the VM in an identical way.- Create a health check to monitor the VMs in the managed instance group. This health check makes sure the VM responds on port 80. For your own applications, monitor the appropriate ports to check the VM health.```\ngcloud compute health-checks create http http-basic-check-$NAME_SUFFIX --port 80\n```\n- Create a managed instance group with only one VM. This single VM boots and creates a persistent disk from the latest snapshot, then mounts it and starts to serve web traffic.```\ngcloud compute instance-groups managed create instance-group-$NAME_SUFFIX-$ZONE1 \\\u00a0 \u00a0 --base-instance-name=instance-vm-$NAME_SUFFIX \\\u00a0 \u00a0 --template=template-$NAME_SUFFIX \\\u00a0 \u00a0 --size=1 \\\u00a0 \u00a0 --zone=$ZONE1 \\\u00a0 \u00a0 --health-check=http-basic-check-$NAME_SUFFIX\n```\n## Create and configure a load balancerFor users to access your website, you need to allow traffic through to the VMs that run in the managed instance group. You also want to automatically redirect traffic to new VMs if there's a zone failure in a managed instance group.\nIn the following section, you create an [external load balancer](/load-balancing/docs/load-balancing-overview) with a backend service for HTTP traffic on port 80, use the health check created in the previous steps, and map an external IP address through to the backend service.\nFor more information, see [How to set up a simple external HTTP load balancer](/load-balancing/docs/https/ext-http-lb-simple) .- Create and configure the load balancer for your application:```\n# Configure port rules for HTTP port 80gcloud compute instance-groups set-named-ports \\\u00a0 \u00a0 instance-group-$NAME_SUFFIX-$ZONE1 \\\u00a0 \u00a0 --named-ports http:80 \\\u00a0 \u00a0 --zone $ZONE1# Create a backend service and add the managed instance group to itgcloud compute backend-services create \\\u00a0 \u00a0 web-backend-service-$NAME_SUFFIX \\\u00a0 \u00a0 --protocol=HTTP \\\u00a0 \u00a0 --port-name=http \\\u00a0 \u00a0 --health-checks=http-basic-check-$NAME_SUFFIX \\\u00a0 \u00a0 --globalgcloud compute backend-services add-backend \\\u00a0 \u00a0 web-backend-service-$NAME_SUFFIX \\\u00a0 \u00a0 --instance-group=instance-group-$NAME_SUFFIX-$ZONE1 \\\u00a0 \u00a0 --instance-group-zone=$ZONE1 \\\u00a0 \u00a0 --global# Create a URL map for the backend servicegcloud compute url-maps create web-map-http-$NAME_SUFFIX \\\u00a0 \u00a0 --default-service web-backend-service-$NAME_SUFFIX# Configure forwarding for the HTTP trafficgcloud compute target-http-proxies create \\\u00a0 \u00a0 http-lb-proxy-$NAME_SUFFIX \\\u00a0 \u00a0 --url-map web-map-http-$NAME_SUFFIXgcloud compute forwarding-rules create \\\u00a0 \u00a0 http-content-rule-$NAME_SUFFIX \\\u00a0 \u00a0 --global \\\u00a0 \u00a0 --target-http-proxy=http-lb-proxy-$NAME_SUFFIX \\\u00a0 \u00a0 --ports=80\n```\n- Get the IP address of the forwarding rule for the web traffic:```\nIP_ADDRESS=$(gcloud compute forwarding-rules describe http-content-rule-$NAME_SUFFIX \\\u00a0 \u00a0 --global \\\u00a0 \u00a0 --format=\"value(IPAddress)\")\n```\n- Use `curl` , or open your web browser, to view the website using the IP address of the load balancer from the previous step:```\ncurl $IP_ADDRESS\n```It takes a few minutes for the load balancer to finish deploying and to correctly direct traffic to your backend. An HTTP 404 or 502 error is returned if the load balancer is still deploying. If needed, wait a few minutes and try to access the website again.The basic website is returned, as shown in the following example output:```\n<!doctype html>\n<html lang=en>\n<head>\n<meta charset=utf-8>\n <title>HA / DR example</title>\n</head>\n<body>\n <p>Welcome to a Compute Engine website with warm failover to Cloud Storage!</p>\n</body>\n</html>\n```\n## Simulate a zone failure and recoveryReview the resource deployments before simulating a failure at the zone level. All of the resources have been created to support the following environment:\n- One VM runs in a managed instance group with an attached persistent disk that stores a basic website.\n- Snapshots are regularly taken of the persistent disk using a resource policy snapshot schedule.\n- A startup script is applied to an instance template so any VMs created in the managed instance group create a persistent disk from the last disk snapshot and attach it.\n- A health check monitors the status of the VM inside the managed instance group.\n- The external Application Load Balancer directs users to the VM that runs in the managed instance group.\n- If the VM fails, the managed instance group tries to recreate a VM in the same zone. If the failure is at the zone level, you must manually create a replacement managed instance group in another, working zone.\nIn a production environment, you might get an alert using [Cloud Monitoring](/monitoring/alerts) or other monitoring solution when there's a problem. This alert prompts a human to understand the scope of the failure before you manually create a replacement managed instance group in another, working zone. An alternative approach is to use your monitoring solution to automatically respond to outages with the managed instance group.\nWhen you or your monitoring solution determine the most appropriate action is to fail over, create a replacement managed instance group. In this document, you manually create this replacement resource.- To simulate a failure at the zone level, delete the load balancer backend and managed instance group:```\ngcloud compute backend-services remove-backend \\\u00a0 \u00a0 web-backend-service-$NAME_SUFFIX \\\u00a0 \u00a0 --instance-group=instance-group-$NAME_SUFFIX-$ZONE1 \\\u00a0 \u00a0 --instance-group-zone=$ZONE1 \\\u00a0 \u00a0 --globalgcloud compute instance-groups managed delete instance-group-$NAME_SUFFIX-$ZONE1 \\\u00a0 \u00a0 \u00a0 --zone=$ZONE1\n```When prompted, confirm the request to delete the managed instance group.In a production environment, your monitoring system generates an alert to now prompt for cold failover action.\n- Use `curl` or your web browser again to access the IP address of the load balancer:```\ncurl $IP_ADDRESS --max-time 5\n```The `curl` request fails as there are no healthy targets for the load balancer.\n- To simulate the cold failover, create a managed instance group in a different zone:```\ngcloud compute instance-groups managed create instance-group-$NAME_SUFFIX-$ZONE2 \\\u00a0 \u00a0 --template=template-$NAME_SUFFIX \\\u00a0 \u00a0 --size=1 \\\u00a0 \u00a0 --zone=$ZONE2 \\\u00a0 \u00a0 --health-check=http-basic-check-$NAME_SUFFIX\n```The VM image, instance template, and persistent disk maintain all the configuration for the application instance.\n- Update the load balancer to add the new managed instance group and VM:```\ngcloud compute instance-groups set-named-ports \\\u00a0 \u00a0 instance-group-$NAME_SUFFIX-$ZONE2 \\\u00a0 \u00a0 --named-ports http:80 \\\u00a0 \u00a0 --zone $ZONE2gcloud compute backend-services add-backend \\\u00a0 \u00a0 web-backend-service-$NAME_SUFFIX \\\u00a0 \u00a0 --instance-group=instance-group-$NAME_SUFFIX-$ZONE2 \\\u00a0 \u00a0 --instance-group-zone=$ZONE2 \\\u00a0 \u00a0 --global\n```\n- Use `curl` or your web browser one more time to access the IP address of the load balancer that directs traffic to the VM that runs in the managed instance group:```\ncurl $IP_ADDRESS\n```It takes a few minutes for the VM to finish deploying and restore data from the latest persistent disk snapshot. An HTTP 404 or 502 error is returned if the VM is still deploying, and the default Apache is displayed if still restoring data. If needed, wait a few minutes and try to access the website again.The following example response shows the web page correctly running on the VM:```\n<!doctype html>\n<html lang=en>\n<head>\n<meta charset=utf-8>\n <title>HA / DR example</title>\n</head>\n<body>\n <p>Welcome to a test web server with persistent disk snapshots!</p>\n</body>\n</html>\n```\n- Check the health status of the managed instance group:```\ngcloud compute instance-groups managed list-instances instance-group-$NAME_SUFFIX-$ZONE2 \\\u00a0 \u00a0 --zone $ZONE2\n```The following example output shows the status of the VM as `RUNNING` and `HEALTHY` :```\nNAME    ZONE   STATUS HEALTH_STATE ACTION\ninstance-vm-app us-central1-f RUNNING HEALTHY  NONE\n```\n- To verify the attached persist disk was created from a snapshot, look at the the source. Specify the instance `` displayed from the previous `list-instances` command.```\ngcloud compute instances describe NAME \\\u00a0 \u00a0 --zone=$ZONE2 \\\u00a0 \u00a0 --format=\"value(disks.[1].source)\"\n```The following example output shows the persistent disk is named . The suffix is added by the startup script to the name of the latest disk snapshot:```\nhttps://www.googleapis.com/compute/v1/projects/project/zones/us-central1-f/disks/disk-app-us-central1-a-20210630165529-umopkt17-restored\n```\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\nTo delete the individual resources created in this document, complete the following steps.- Delete the load balancer configuration:```\ngcloud compute forwarding-rules delete \\\u00a0 \u00a0 http-content-rule-$NAME_SUFFIX --global --quietgcloud compute target-http-proxies delete \\\u00a0 \u00a0 http-lb-proxy-$NAME_SUFFIX --quietgcloud compute url-maps delete web-map-http-$NAME_SUFFIX --quietgcloud compute backend-services delete \\\u00a0 \u00a0 web-backend-service-$NAME_SUFFIX --global --quiet\n```\n- Delete the managed instance group and health check:```\ngcloud compute instance-groups managed delete instance-group-$NAME_SUFFIX-$ZONE2 \\\u00a0 \u00a0 --zone=$ZONE2 --quietgcloud compute health-checks delete http-basic-check-$NAME_SUFFIX --quiet\n```\n- Delete the instance template, cloud-init configuration, images, base VM, persistent disk, and snapshot schedule.```\ngcloud compute instance-templates delete template-$NAME_SUFFIX --quietrm cloud-init.yamlgcloud compute images delete image-$NAME_SUFFIX --quietgcloud compute instances delete vm-base-$NAME_SUFFIX --zone=$ZONE1 --quietgcloud compute disks delete disk-$NAME_SUFFIX --zone=$ZONE1 --quietgcloud compute resource-policies delete \\\u00a0 \u00a0 snapshot-schedule-$NAME_SUFFIX --region $REGION --quiet\n```\n- List and then delete snapshots and disks created by the instances:```\ngcloud compute disks list --filter=\"name:disk-$NAME_SUFFIX\" \\\u00a0 \u00a0 --uri | xargs gcloud compute disks deletegcloud compute snapshots list --filter=\"name:disk-$NAME_SUFFIX\" \\\u00a0 \u00a0 --uri | xargs gcloud compute snapshots delete\n```\n- Delete the custom role and service account:```\ngcloud iam roles delete instance_snapshot_management_$NAME_SUFFIX \\\u00a0 --project=$PROJECT_ID --quietgcloud iam service-accounts delete \\\u00a0 \u00a0 instance-sa-$NAME_SUFFIX@$PROJECT_ID.iam.gserviceaccount.com --quiet\n```\n- Delete the firewall rules:```\ngcloud compute firewall-rules delete allow-health-check-$NAME_SUFFIX --quietgcloud compute firewall-rules delete allow-ssh-$NAME_SUFFIX --quietgcloud compute firewall-rules delete allow-http-$NAME_SUFFIX --quiet\n```\n- Delete the subnet and VPC:```\ngcloud compute networks subnets delete \\\u00a0 \u00a0 subnet-$NAME_SUFFIX-$REGION --region=$REGION --quietgcloud compute networks delete network-$NAME_SUFFIX --quiet\n```\n## What's next\n- For an alternative approach to reduce your potential data loss, see [Deploy cold recoverable application that uses regional persistent disks](/architecture/cold-recoverable-apps-regional-persistent-disks) .\n- To learn how how to determine the best approach for your own applications and which recovery method to use, see the [Disaster recovery planning guide](/architecture/dr-scenarios-planning-guide) .\n- To see other patterns for applications, such as cold and hot failover, see [Disaster recovery scenarios for applications](/architecture/dr-scenarios-for-applications) .\n- For more ways to handle scale and availability, see the [Patterns for scalable and resilient apps](/architecture/scalable-and-resilient-apps) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}