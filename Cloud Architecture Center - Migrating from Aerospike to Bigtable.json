{"title": "Cloud Architecture Center - Migrating from Aerospike to Bigtable", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Migrating from Aerospike to Bigtable\nThis tutorial describes how to migrate data from [Aerospike](https://www.aerospike.com/) to [Bigtable](/bigtable) . The tutorial explains the differences between Aerospike and Bigtable and how to transform your workload to run in Bigtable. It is for database practitioners who are looking for a database service on Google Cloud that is similar to Aerospike. This tutorial assumes that you are familiar with database schemas, data types, the fundamentals of NoSQL, and relational database systems. This tutorial relies on running predefined tasks to perform an example migration. After you finish the tutorial, you can modify the provided code and steps to match your environment.\nBigtable is a petabyte-scale, fully managed [NoSQL database](https://wikipedia.org/wiki/NoSQL) service for large analytical and operational workloads. You can use it as a storage engine for your low-latency and petabyte-scale service with higher availability and durability. You can analyze data in Bigtable using Google Cloud data analytics services like [Dataproc](/dataproc) and [BigQuery](/bigquery/external-data-bigtable) .\nBigtable is ideal for advertising technology (ad tech), financial technology (fintech), and the Internet of Things (IoT) services that are implemented with NoSQL databases like AeroSpike or Cassandra. If you are looking for NoSQL-managed service, use Bigtable.", "content": "## ArchitectureThe following reference architecture diagram shows common components that you can use to migrate data from Aerospike to Bigtable.In the preceding diagram, the data migrates from an on-premises environment using Aerospike to Bigtable on Google Cloud by using two different methods. The first method migrates the data by using batch processing. It starts by moving the Aerospike backup data into a Cloud Storage bucket. When the backup data arrives in Cloud Storage, it triggers Cloud Functions to start a batch extract, transform, and load (ETL) process using Dataflow. The Dataflow job converts the backup data into a Bigtable compatible format and imports the data into the Bigtable instance.\nThe second method migrates the data by using streaming processing. In this method, you connect to Aerospike using a message queue, such as Kafaka using [Aerospike Connect](https://www.aerospike.com/products/connect-for-kafka/) , and transfer messages in real time to [Pub/Sub](/pubsub/docs) on Google Cloud. When the message arrives into a Pub/Sub topic, it is processed by the Dataflow streaming job in real time to convert and import the data into the Bigtable instance.\nWith batch processing, you can efficiently migrate big chunks of data. However, it often requires sufficient cutover downtime, while migrating and updating service for new databases. If you want to minimize cutover downtime, you might consider using streaming processing to migrate data gradually after first batch processing to keep consistency from the backup data until complete graceful cutover. In this document, you can migrate from Aerospike by using batch processing with example applications, including the cutover process.## Comparing Aerospike and BigtableBefore starting your data migration, it's fundamental for you to understand the data model differences between Aerospike and Bigtable.\nThe Bigtable data model is a [distributed, multidimensional, sorted key-value map](/bigtable/docs/overview#storage-model) with rows and column families. By contrast, the Aerospike data model is [a row-oriented database](https://www.aerospike.com/docs/guide/kvs.html) , where every record is uniquely identified by a key. The difference between the models is how they group the attributes of an entity. Bigtable groups related attributes into a column family, while Aerospike groups attributes in a set. Aerospike supports more data types compared to Bigtable. For example, Aerospike supports [integers, strings, lists, and maps](https://www.aerospike.com/docs/guide/data-types.html) . Bigtable treats all data as [raw byte strings](/bigtable/docs/overview#data-types) for most purposes.\nA schema in Aerospike is flexible, and dynamic values in the same [bins](https://www.aerospike.com/docs/architecture/data-model.html) can have different types. Apps that use either Aerospike or Bigtable have similar flexibility and data administration responsibility: apps handle data types and integrity constraints, instead of relying on the database engine.## Bookshelf migrationThe [Bookshelf app](https://github.com/GoogleCloudPlatform/getting-started-python/tree/master/bookshelf) is a web app where users can store information about books and see the list of all the books currently stored in the database. The app uses a book identifier (ID) to search for book information. The app or the database automatically generates these IDs. When a user selects the image of a book, the app's backend loads the details about that book from the database.\nIn this tutorial, you migrate data from the bookshelf app using Aerospike to Bigtable. After the migration, you can access the books from Bigtable.\nThe following diagram shows how the data is migrated from Aerospike to Bigtable:In the preceding diagram, data is migrated in the following way:- You back up data about books from the current Aerospike database and transfer the data to a Cloud Storage bucket.\n- When you upload the backup data to the bucket, it automatically triggers the`as2bt`Dataflow job through Cloud Storage update notifications using Cloud Function.\n- After the data migration is completed by the`as2bt`Dataflow job, you change the database backend from Aerospike to Bigtable so that the bookshelf app loads book data from the Bigtable cluster.\n## Objectives\n- Deploy a tutorial environment for migration from Aerospike to Bigtable.\n- Create an example app backup dataset from Aerospike in Cloud Storage.\n- Use Dataflow to transfer the data schema and migrate it to Bigtable.\n- Change the example app configuration to use Bigtable as a backend.\n- Verify that the bookshelf app is running properly with Bigtable.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Compute Engine](/compute/all-pricing) \n- [Bigtable](/bigtable/pricing) \n- [Pub/Sub](/pubsub/pricing) \n- [Cloud Storage](/storage/pricing) \n- [Dataflow](/dataflow/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nBigtable charges are based on the number of node hours, the amount of data stored, and amount of network bandwidth that you use. To estimate the cost of the Bigtable cluster and other resources, you can use the [pricing calculator](/products/calculator#id=4c1afffb-e7c2-4039-9f60-436d35724526) . The example pricing calculator setup uses three Bigtable nodes instead of a single node. The total estimated cost in the preceding example is more than the actual total cost of this tutorial.\nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- In the Google Cloud console, on the project selector page,   select or [create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . **Note** : If you don't plan to keep the  resources that you create in this procedure, create a project instead of  selecting an existing project. After you finish these steps, you can  delete the project, removing all resources associated with the project. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- Enable the Cloud Resource Manager API. [Enable the API](https://console.cloud.google.com/flows/enableapi?apiid=cloudresourcemanager) Terraform uses the Cloud Resource Manager API to enable the APIs that are required for this tutorial.\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n## Preparing your environmentTo prepare the environment for the Aerospike to Bigtable migration, you run the following tools directly from Cloud Shell:- The Google Cloud CLI\n- The`gsutil`command-line tool\n- The Bigtable command-line tool,`cbt`\n- Terraform\n- Apache Maven\nThese tools are [already available in Cloud Shell](/shell/docs/how-cloud-shell-works#tools) , so you don't need to install these tools again.\n### Configure your project\n- In Cloud Shell, inspect the project ID that Cloud Shell automatically configures. Your command prompt is updated to reflect your currently active project and displays in this format: `` `@cloudshell:~ (` `` `)$`If the project ID isn't configured correctly, you can configure it manually:```\ngcloud config set project <var>PROJECT_ID</var>\n```Replace `` with your Google Cloud project ID.\n- Configure `us-east1` as the region and `us-east1-b` as the zone:```\ngcloud config set compute/region us-east1gcloud config set compute/zone us-east1-b\n```For more information about regions and zones, see [Geography and regions](/docs/geography-and-regions) .\n### Deploy the tutorial environment\n- In Cloud Shell, clone the code repository:```\n\u00a0git clone https://github.com/fakeskimo/as2bt.git/\n```\n- In Cloud Shell, initialize the Terraform working directory:```\ncd \"$HOME\"/as2bt/bookshelf/terraformterraform init\n```\n- Configure [Terraform environment variables](https://www.terraform.io/docs/configuration/variables.html#environment-variables) for deployment:```\nexport TF_VAR_gce_vm_zone=\"$(gcloud config get-value compute/zone)\"export TF_VAR_gcs_bucket_location=\"$(gcloud config get-value compute/region)\"\n```\n- Review the Terraform execution plan:```\nterraform plan\n```The output is similar to the following:```\nTerraform will perform the following actions:\n# google_bigtable_instance.bookshelf_bigtable will be created\n+ resource \"google_bigtable_instance\" \"bookshelf_bigtable\" {\n + display_name = (known after apply)\n + id   = (known after apply)\n + instance_type = \"DEVELOPMENT\"\n + name   = \"bookshelf-bigtable\"\n + project  = (known after apply)\n + cluster {\n  + cluster_id = \"bookshelf-bigtable-cluster\"\n  + storage_type = \"SSD\"\n  + zone   = \"us-east1-b\"\n }\n}\n```\n- (Optional) To visualize which resources with dependencies are deployed by Terraform, draw [graphs](https://www.terraform.io/docs/commands/graph.html) :```\nterraform graph | dot -Tsvg > graph.svg\n```\n- Provision the tutorial environment:```\nterraform apply\n```\n## Verifying the tutorial environment and bookshelf appAfter you provision the environment and before you start the data migration job, you need to verify that all the resources have been deployed and configured. This section explains how to verify the provisioning process and helps you understand what components are configured in the environment.\n### Verify the tutorial environment\n- In Cloud Shell, verify the `bookshelf-aerospike` Compute Engine instance:```\ngcloud compute instances list\n```The output shows that the instance is deployed in the `us-east1-b` zone:```\nNAME     ZONE  MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS\nbookshelf-aerospike us-east1-b n1-standard-2    10.142.0.4 34.74.72.3 RUNNING\n```\n- Verify the `bookshelf-bigtable` Bigtable instance:```\ngcloud bigtable instances list\n```The output is similar to the following:```\nNAME    DISPLAY_NAME  STATE\nbookshelf-bigtable bookshelf-bigtable READY\n```This Bigtable instance is used as the migration target for later steps.\n- Verify that the `bookshelf` Cloud Storage bucket is in the Dataflow pipeline job:```\ngsutil ls -b gs://bookshelf-*\n```Because Cloud Storage bucket names need to be globally unique, the name of the bucket is created with a random suffix. The output is similar to the following:```\ngs://bookshelf-616f60d65a3abe62/\n```\n### Add a book to the Bookshelf app\n- In Cloud Shell, get the external IP address of the `bookshelf-aerospike` instance:```\ngcloud compute instances list --filter=\"name:bookshelf-aerospike\" \\\u00a0 \u00a0 --format=\"value(networkInterfaces[0].accessConfigs.natIP)\"\n```Make a note of the IP address because it's needed in the next step.\n- To open the Bookshelf app, in a web browser, go to `http://` `` `:8080` .Replace `` with the external IP address that you copied from the previous step.\n- To create a new book, click add **Add book** .\n- In the **Add book** window, complete the following fields, and then click **Save** :- In the **Title** field, enter`Aerospike-example`.\n- In the **Author** field, enter`Aerospike-example`.\n- In the **Date Published** field, enter today's date.\n- In the **Description** field, enter`Aerospike-example`.\nThis book is used to verify that the Bookshelf app is using Aerospike as the book storage.\n- In the Bookshelf app URL, make a note of the book ID. For example, if the URL is `34.74.80.160:8080/books/10000` , the book ID is `10000` . \n- In Cloud Shell, use SSH to connect to the `bookshelf-aerospike` instance:```\ngcloud compute ssh bookshelf-aerospike\n```\n- From the `bookshelf-aerospike` instance session, verify that a new book was created with the book ID that you previously noted:```\naql -c 'select * from bookshelf.books where id = \"BOOK_ID\"'\n```The output is similar to the following:```\n+----------------------+----------------------+---------------+----------------------+----------+---------+\n| title    | author    | publishedDate | description   | imageUrl | id  |\n+----------------------+----------------------+---------------+----------------------+----------+---------+\n| \" Aerospike-example\" | \" Aerospike-example\" | \"2000-01-01\" | \" Aerospike-example\" | \"\"  | \"10000\" |\n+----------------------+----------------------+---------------+----------------------+----------+---------+\n1 row in set (0.001 secs)\n```If your book ID isn't listed, repeat the steps to [add a new book](#add_a_book_to_the_bookshelf_app) .\n## Transferring backup data from Aerospike to Cloud Storage\n- In Cloud Shell, from the `bookshelf-aerospike` instance session, create an Aerospike backup file:```\naql -c \"select * from bookshelf.books\" --timeout=-1 --outputmode=json \\`\u00a0 \u00a0 | tail -n +2 | jq -c '.[0] | .[]' \\\u00a0 \u00a0 | gsutil cp - $(gsutil ls -b gs://bookshelf-*)bookshelf-backup.json\n```This command processes the data and creates a backup file through the following process:- Selects book information from Aerospike and prints it out in the JSON prettyprint format.\n- Removes the first two headings from the output and converts the data into Newline delimited JSON ( [ndjson](http://ndjson.org/) ) format by using [jq](https://stedolan.github.io/jq/) , a command-line JSON processor.\n- Uses the`gsutil`command-line tool to upload the data into the Cloud Storage bucket.\n- Verify that the Aerospike backup file is uploaded and exists in the Cloud Storage bucket:```\ngsutil ls gs://bookshelf-*/bookshelf-*\\\u00a0 \u00a0 gs://bookshelf-616f60d65a3abe62/bookshelf-backup.json\n```\n- (Optional) Review the backup file contents from the Cloud Storage bucket:```\ngsutil cat -r 0-1024 gs://bookshelf-*/bookshelf-backup.json | head -n 2\n```The output is similar to the following:```\n{\"title\":\"book_2507\",\"author\":\"write_2507\",\"publishedDate\":\"1970-01-01\",\"imageUrl\":\"https://storage.googleapis.com/aerospike2bt-bookshelf/The_Home_Edit-2019-06-24-044906.jpg\",\"description\":\"test_2507\",\"createdBy\":\"write_2507\",\"createdById\":\"2507_anonymous\",\"id\":\"2507\"}\n{\"title\":\"book_3867\",\"author\":\"write_3867\",\"publishedDate\":\"1970-01-01\",\"imageUrl\":\"https://storage.googleapis.com/aerospike2bt-bookshelf/The_Home_Edit-2019-06-24-044906.jpg\",\"description\":\"test_3867\",\"createdBy\":\"write_3867\",\"createdById\":\"3867_anonymous\",\"id\":\"3867\"}\n```\n- Exit the SSH session and return to Cloud Shell:```\nexit\n```\n## Migrating the backup data to Bigtable using DataflowYou can now migrate the backup data from Cloud Storage to a Bigtable instance. This section explains how to use Dataflow pipelines to migrate data that is compatible with a Bigtable schema.\n### Configure the Dataflow migration job\n- In Cloud Shell, go to the `dataflow` directory in the example code repository:```\ncd \"$HOME\"/as2bt/dataflow/\n```\n- Configure environment variables for a Dataflow job:```\nexport BOOKSHELF_BACKUP_FILE=\"$(gsutil lsgs://bookshelf*/bookshelf-backup.json)\"export BOOKSHELF_DATAFLOW_ZONE=\"$(gcloud config get-value compute/zone)\"\n```\n- Check that the environment variables are configured correctly:```\nenv | grep BOOKSHELF\n```If the environment variables are correctly configured, the output is similar to the following:```\nBOOKSHELF_BACKUP_FILE=gs://bookshelf-616f60d65a3abe62/bookshelf-backup.json\nBOOKSHELF_DATAFLOW_ZONE=us-east1-b\n```\n### Run the Dataflow job\n- In Cloud Shell, migrate data from Cloud Storage to the Bigtable instance:```\n./run_oncloud_json.sh\n```\n- To monitor the backup data migration job, in the Google Cloud console, go to the **Jobs** page. [Go to Jobs](https://console.cloud.google.com/dataflow) Wait until the job successfully completes. When the job successfully completes, the output in Cloud Shell is similar to the following:```\nDataflow SDK version: 2.13.0\nSubmitted job: 2019-12-16_23_24_06-2124083021829446026\n[INFO] -----------------------------------------------------------------------[INFO] BUILD SUCCESS\n[INFO] -----------------------------------------------------------------------[INFO] Total time: 08:20 min\n[INFO] Finished at: 2019-12-17T16:28:08+09:00\n[INFO] -----------------------------------------------------------------------\n```\n### Check the migration job results\n- In Cloud Shell, verify that the backup data was transferred correctly into Bigtable:```\ncbt -instance bookshelf-bigtable lookup books 00001\n```The output is similar to the following:```\n ---------------------------------------00001\n info:author        @ 2019/12/17-16:26:04.434000\n \"Aerospike-example\"\n info:description       @ 2019/12/17-16:26:04.434000\n \"Aerospike-example\"\n info:id         @ 2019/12/17-16:26:04.434000\n \"00001\"\n info:imageUrl       @ 2019/12/17-16:26:04.434000\n \"\"\n info:publishedDate      @ 2019/12/17-16:26:04.434000\n \"2019-10-01\"\n info:title        @ 2019/12/17-16:26:04.434000\n \"Aerospike-example\"\n``` **Note:** The `Aerospike-example` book ID is reversed from `10000` to `00001` to prevent [hot spot problems](/bigtable/docs/schema-design#row-keys) .\n## Changing the bookshelf database from Aerospike to BigtableAfter you successfully migrate data from Aerospike to Bigtable, you can change the Bookshelf app configuration to use Bigtable for storage. When you set up this configuration, new books are saved into the Bigtable instances.\n### Change the Bookshelf app configuration\n- In Cloud Shell, use SSH to connect to the `bookshelf-aerospike` app:```\ngcloud compute ssh bookshelf-aerospike\n```\n- Verify that the current `DATA_BACKEND` configuration is `aerospike` :```\ngrep DATA_BACKEND /opt/app/bookshelf/config.py\n```The output is the following:```\nDATA_BACKEND = 'aerospike'\n```\n- Change the `DATA_BACKEND` configuration from `aerospike` to `bigtable` :```\nsudo sed -i \"s/DATA_BACKEND =.*/DATA_BACKEND = 'bigtable'/g\" /opt/app/bookshelf/config.py\n```\n- Verify that the `DATA_BACKEND` configuration is changed to `bigtable` :```\ngrep DATA_BACKEND /opt/app/bookshelf/config.py\n```The output is the following:```\nDATA_BACKEND = 'bigtable'\n```\n- Restart the Bookshelf app that uses the new `bigtable` backend configuration:```\nsudo supervisorctl restart bookshelf\n```\n- Verify that the Bookshelf app has restarted and is running correctly:```\nsudo supervisorctl status bookshelf\n```The output is similar to the following:```\nbookshelf RUNNING pid 18318, uptime 0:01:00\n```\n### Verify the bookshelf app is using the Bigtable backend\n- In a browser, go to`http://` `` `:8080`.\n- [Add a new book](#add_a_book_to_the_bookshelf_app) named `Bigtable-example` .\n- To verify that the `Bigtable-example` book was created in a Bigtable instance from the Bookshelf app, copy the book ID from the address bar in the browser.\n- In Cloud Shell, look up the `Bigtable-example` book data from a Bigtable instance:```\ncbt -instance bookshelf-bigtable lookup books 7406950188\n```The output is similar to the following:```\n---------------------------------------7406950188\n info:author        @ 2019/12/17-17:28:25.592000\n \"Bigtable-example\"\n info:description       @ 2019/12/17-17:28:25.592000\n \"Bigtable-example\"\n info:id         @ 2019/12/17-17:28:25.592000\n \"7406950188\"\n info:image_url       @ 2019/12/17-17:28:25.592000\n \"\"\n info:published_date      @ 2019/12/17-17:28:25.592000\n \"2019-10-01\"\n info:title        @ 2019/12/17-17:28:25.592000\n \"Bigtable-example\"\n```\nYou have successfully performed a data migration from Aerospike to Bigtable and changed the bookshelf configuration to connect to a Bigtable backend.## Clean upThe easiest way to eliminate billing is to delete the Google Cloud project that you created for the tutorial. Alternatively, you can delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Learn how to design your [Bigtable schema](/bigtable/docs/schema-design) .\n- Read about how to start your [Migration to Google Cloud](/solutions/migration-to-gcp-getting-started) .\n- [Set up a CI/CD pipeline for your data-processing workflow](/solutions/cicd-pipeline-for-data-processing) .\n- Understand which [strategies you have for transferring big datasets](/solutions/migration-to-google-cloud-transferring-your-large-datasets) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}