{"title": "Dialogflow - Detect intent with audio input stream", "url": "https://cloud.google.com/dialogflow/es/docs/how/detect-intent-stream", "abstract": "# Dialogflow - Detect intent with audio input stream\nThis page shows how to stream audio input to a detect intent request using the API. Dialogflow processes the audio and converts it to text before attempting an intent match. This conversion is known as , , , or .\n**Note:** Streaming is supported by the RPC API and client libraries, but it is not supported by the REST API.\n", "content": "## Before you begin\nThis feature is only applicable when using the API for [end-user interactions](/dialogflow/docs/api-overview) . If you are using an [integration](/dialogflow/docs/integrations) , you can skip this guide.\nYou should do the following before reading this guide:\n- Read [Dialogflow basics](/dialogflow/docs/basics) .\n- Perform [setup steps](/dialogflow/docs/quick/setup) .## Create an agent\nIf you have not already created an agent, create one now:- Go to the [Dialogflow ES Console](https://dialogflow.cloud.google.com) .\n- If requested, sign in to the Dialogflow Console.  See [Dialogflow console overview](/dialogflow/docs/console) for more information.\n- Click **Create Agent** in the left sidebar menu.  (If you already have other agents, click the agent name,  scroll to the bottom and click **Create new agent** .)\n- Enter your agent's name, default language, and default time zone.\n- If you have already created a project, enter that project.  If you want to allow the Dialogflow Console to create the project,  select **Create a new Google project** .\n- Click the **Create** button.## Import the example file to your agent\nThe steps in this guide make assumptions about your agent, so you need to [import](/dialogflow/docs/agents-settings#export) an agent prepared for this guide. When importing, these steps use the option, which overwrites all agent settings, intents, and entities.\nTo import the file, follow these steps:\n- Download the [room-booking-agent.zip](/static/dialogflow/es/docs/data/room-booking-agent.zip) file.\n- Go to the [Dialogflow ES Console](https://dialogflow.cloud.google.com) .\n- Select your agent.\n- Click the  settingsbutton  next to the agent name.\n- Select the **Export and Import** tab.\n- Select **Restore From Zip** and follow instructions to restore the zip file that you downloaded.## Streaming basics\nThe [Session](/dialogflow/docs/reference/common-types#sessions) type's `streamingDetectIntent` method returns a bidirectional gRPC streaming object. The available methods for this object vary by language, so see the reference documentation for your client library for details.\nThe streaming object is used to send and receive data concurrently. Using this object, your client streams audio content to Dialogflow, while concurrently listening for a `StreamingDetectIntentResponse` .\nThe `streamingDetectIntent` method has a `query_input.audio_config.single_utterance` parameter that affects speech recognition:\n- If`false`(default), speech recognition does not cease until the client closes the stream.\n- If`true`, Dialogflow will detect a single spoken utterance in input audio. When Dialogflow detects the audio's voice has stopped or paused, it ceases speech recognition and sends a`StreamingDetectIntentResponse`with a recognition result of`END_OF_SINGLE_UTTERANCE`to your client. Any audio sent to Dialogflow on the stream after receipt of`END_OF_SINGLE_UTTERANCE`is ignored by Dialogflow.\nIn bidirectional streaming, a client can the stream object to signal to the server that it won't send more data. For example, in Java and Go, this method is called `closeSend` . It is important to half-close (but not abort) streams in the following situations:\n- Your client has finished sending data.\n- Your client is configured with`single_utterance`set to true, and it receives a`StreamingDetectIntentResponse`with a recognition result of`END_OF_SINGLE_UTTERANCE`.\nAfter closing a stream, your client should start a new request with a new stream as needed.\n## Streaming detect intent\nThe following samples use the [Session](/dialogflow/docs/reference/common-types#sessions) type's `streamingDetectIntent` method to stream audio.\nTo authenticate to Dialogflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/dialogflow/detect_intent/detect_intent.go) \n```\nfunc DetectIntentStream(projectID, sessionID, audioFile, languageCode string) (string, error) {\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 sessionClient, err := dialogflow.NewSessionsClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return \"\", err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer sessionClient.Close()\u00a0 \u00a0 \u00a0 \u00a0 if projectID == \"\" || sessionID == \"\" {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return \"\", errors.New(fmt.Sprintf(\"Received empty project (%s) or session (%s)\", projectID, sessionID))\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 sessionPath := fmt.Sprintf(\"projects/%s/agent/sessions/%s\", projectID, sessionID)\u00a0 \u00a0 \u00a0 \u00a0 // In this example, we hard code the encoding and sample rate for simplicity.\u00a0 \u00a0 \u00a0 \u00a0 audioConfig := dialogflowpb.InputAudioConfig{AudioEncoding: dialogflowpb.AudioEncoding_AUDIO_ENCODING_LINEAR_16, SampleRateHertz: 16000, LanguageCode: languageCode}\u00a0 \u00a0 \u00a0 \u00a0 queryAudioInput := dialogflowpb.QueryInput_AudioConfig{AudioConfig: &audioConfig}\u00a0 \u00a0 \u00a0 \u00a0 queryInput := dialogflowpb.QueryInput{Input: &queryAudioInput}\u00a0 \u00a0 \u00a0 \u00a0 streamer, err := sessionClient.StreamingDetectIntent(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return \"\", err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 f, err := os.Open(audioFile)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return \"\", err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer f.Close()\u00a0 \u00a0 \u00a0 \u00a0 go func() {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 audioBytes := make([]byte, 1024)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 request := dialogflowpb.StreamingDetectIntentRequest{Session: sessionPath, QueryInput: &queryInput}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 err = streamer.Send(&request)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatal(err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 _, err := f.Read(audioBytes)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if err == io.EOF {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 streamer.CloseSend()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatal(err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 request = dialogflowpb.StreamingDetectIntentRequest{InputAudio: audioBytes}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 err = streamer.Send(&request)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatal(err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }()\u00a0 \u00a0 \u00a0 \u00a0 var queryResult *dialogflowpb.QueryResult\u00a0 \u00a0 \u00a0 \u00a0 for {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 response, err := streamer.Recv()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if err == io.EOF {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatal(err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 recognitionResult := response.GetRecognitionResult()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 transcript := recognitionResult.GetTranscript()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Printf(\"Recognition transcript: %s\\n\", transcript)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 queryResult = response.GetQueryResult()\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fulfillmentText := queryResult.GetFulfillmentText()\u00a0 \u00a0 \u00a0 \u00a0 return fulfillmentText, nil}\n```To authenticate to Dialogflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dialogflow/snippets/src/main/java/com/example/dialogflow/DetectIntentStream.java) \n```\nimport com.google.api.gax.rpc.ApiException;import com.google.api.gax.rpc.BidiStream;import com.google.cloud.dialogflow.v2.AudioEncoding;import com.google.cloud.dialogflow.v2.InputAudioConfig;import com.google.cloud.dialogflow.v2.QueryInput;import com.google.cloud.dialogflow.v2.QueryResult;import com.google.cloud.dialogflow.v2.SessionName;import com.google.cloud.dialogflow.v2.SessionsClient;import com.google.cloud.dialogflow.v2.StreamingDetectIntentRequest;import com.google.cloud.dialogflow.v2.StreamingDetectIntentResponse;import com.google.protobuf.ByteString;import java.io.FileInputStream;import java.io.IOException;class DetectIntentStream {\u00a0 // DialogFlow API Detect Intent sample with audio files processes as an audio stream.\u00a0 static void detectIntentStream(String projectId, String audioFilePath, String sessionId)\u00a0 \u00a0 \u00a0 throws IOException, ApiException {\u00a0 \u00a0 // String projectId = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 // String audioFilePath = \"path_to_your_audio_file\";\u00a0 \u00a0 // Using the same `sessionId` between requests allows continuation of the conversation.\u00a0 \u00a0 // String sessionId = \"Identifier of the DetectIntent session\";\u00a0 \u00a0 // Instantiates a client\u00a0 \u00a0 try (SessionsClient sessionsClient = SessionsClient.create()) {\u00a0 \u00a0 \u00a0 // Set the session name using the sessionId (UUID) and projectID (my-project-id)\u00a0 \u00a0 \u00a0 SessionName session = SessionName.of(projectId, sessionId);\u00a0 \u00a0 \u00a0 // Instructs the speech recognizer how to process the audio content.\u00a0 \u00a0 \u00a0 // Note: hard coding audioEncoding and sampleRateHertz for simplicity.\u00a0 \u00a0 \u00a0 // Audio encoding of the audio content sent in the query request.\u00a0 \u00a0 \u00a0 InputAudioConfig inputAudioConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputAudioConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAudioEncoding(AudioEncoding.AUDIO_ENCODING_LINEAR_16)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setLanguageCode(\"en-US\") // languageCode = \"en-US\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setSampleRateHertz(16000) // sampleRateHertz = 16000\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // Build the query with the InputAudioConfig\u00a0 \u00a0 \u00a0 QueryInput queryInput = QueryInput.newBuilder().setAudioConfig(inputAudioConfig).build();\u00a0 \u00a0 \u00a0 // Create the Bidirectional stream\u00a0 \u00a0 \u00a0 BidiStream<StreamingDetectIntentRequest, StreamingDetectIntentResponse> bidiStream =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sessionsClient.streamingDetectIntentCallable().call();\u00a0 \u00a0 \u00a0 // The first request must **only** contain the audio configuration:\u00a0 \u00a0 \u00a0 bidiStream.send(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 StreamingDetectIntentRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setSession(session.toString())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setQueryInput(queryInput)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 try (FileInputStream audioStream = new FileInputStream(audioFilePath)) {\u00a0 \u00a0 \u00a0 \u00a0 // Subsequent requests must **only** contain the audio data.\u00a0 \u00a0 \u00a0 \u00a0 // Following messages: audio chunks. We just read the file in fixed-size chunks. In reality\u00a0 \u00a0 \u00a0 \u00a0 // you would split the user input by time.\u00a0 \u00a0 \u00a0 \u00a0 byte[] buffer = new byte[4096];\u00a0 \u00a0 \u00a0 \u00a0 int bytes;\u00a0 \u00a0 \u00a0 \u00a0 while ((bytes = audioStream.read(buffer)) != -1) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 bidiStream.send(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 StreamingDetectIntentRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputAudio(ByteString.copyFrom(buffer, 0, bytes))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 // Tell the service you are done sending data\u00a0 \u00a0 \u00a0 bidiStream.closeSend();\u00a0 \u00a0 \u00a0 for (StreamingDetectIntentResponse response : bidiStream) {\u00a0 \u00a0 \u00a0 \u00a0 QueryResult queryResult = response.getQueryResult();\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"====================\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"Intent Display Name: %s\\n\", queryResult.getIntent().getDisplayName());\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"Query Text: '%s'\\n\", queryResult.getQueryText());\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Detected Intent: %s (confidence: %f)\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 queryResult.getIntent().getDisplayName(), queryResult.getIntentDetectionConfidence());\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Fulfillment Text: '%s'\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 queryResult.getFulfillmentMessagesCount() > 0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ? queryResult.getFulfillmentMessages(0).getText()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 : \"Triggered Default Fallback Intent\");\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```To authenticate to Dialogflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/dialogflow/detect.js) \n```\nconst fs = require('fs');const util = require('util');const {Transform, pipeline} = require('stream');const {struct} = require('pb-util');const pump = util.promisify(pipeline);// Imports the Dialogflow libraryconst dialogflow = require('@google-cloud/dialogflow');// Instantiates a session clientconst sessionClient = new dialogflow.SessionsClient();// The path to the local file on which to perform speech recognition, e.g.// /path/to/audio.raw const filename = '/path/to/audio.raw';// The encoding of the audio file, e.g. 'AUDIO_ENCODING_LINEAR_16'// const encoding = 'AUDIO_ENCODING_LINEAR_16';// The sample rate of the audio file in hertz, e.g. 16000// const sampleRateHertz = 16000;// The BCP-47 language code to use, e.g. 'en-US'// const languageCode = 'en-US';const sessionPath = sessionClient.projectAgentSessionPath(\u00a0 projectId,\u00a0 sessionId);const initialStreamRequest = {\u00a0 session: sessionPath,\u00a0 queryInput: {\u00a0 \u00a0 audioConfig: {\u00a0 \u00a0 \u00a0 audioEncoding: encoding,\u00a0 \u00a0 \u00a0 sampleRateHertz: sampleRateHertz,\u00a0 \u00a0 \u00a0 languageCode: languageCode,\u00a0 \u00a0 },\u00a0 },};// Create a stream for the streaming request.const detectStream = sessionClient\u00a0 .streamingDetectIntent()\u00a0 .on('error', console.error)\u00a0 .on('data', data => {\u00a0 \u00a0 if (data.recognitionResult) {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `Intermediate transcript: ${data.recognitionResult.transcript}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 } else {\u00a0 \u00a0 \u00a0 console.log('Detected intent:');\u00a0 \u00a0 \u00a0 const result = data.queryResult;\u00a0 \u00a0 \u00a0 // Instantiates a context client\u00a0 \u00a0 \u00a0 const contextClient = new dialogflow.ContextsClient();\u00a0 \u00a0 \u00a0 console.log(` \u00a0Query: ${result.queryText}`);\u00a0 \u00a0 \u00a0 console.log(` \u00a0Response: ${result.fulfillmentText}`);\u00a0 \u00a0 \u00a0 if (result.intent) {\u00a0 \u00a0 \u00a0 \u00a0 console.log(` \u00a0Intent: ${result.intent.displayName}`);\u00a0 \u00a0 \u00a0 } else {\u00a0 \u00a0 \u00a0 \u00a0 console.log(' \u00a0No intent matched.');\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 const parameters = JSON.stringify(struct.decode(result.parameters));\u00a0 \u00a0 \u00a0 console.log(` \u00a0Parameters: ${parameters}`);\u00a0 \u00a0 \u00a0 if (result.outputContexts && result.outputContexts.length) {\u00a0 \u00a0 \u00a0 \u00a0 console.log(' \u00a0Output contexts:');\u00a0 \u00a0 \u00a0 \u00a0 result.outputContexts.forEach(context => {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 const contextId =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 contextClient.matchContextFromProjectAgentSessionContextName(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 context.name\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 const contextParameters = JSON.stringify(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 struct.decode(context.parameters)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(` \u00a0 \u00a0${contextId}`);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(` \u00a0 \u00a0 \u00a0lifespan: ${context.lifespanCount}`);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(` \u00a0 \u00a0 \u00a0parameters: ${contextParameters}`);\u00a0 \u00a0 \u00a0 \u00a0 });\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 });// Write the initial stream request to config for audio input.detectStream.write(initialStreamRequest);// Stream an audio file from disk to the Conversation API, e.g.// \"./resources/audio.raw\"await pump(\u00a0 fs.createReadStream(filename),\u00a0 // Format the audio stream into the request format.\u00a0 new Transform({\u00a0 \u00a0 objectMode: true,\u00a0 \u00a0 transform: (obj, _, next) => {\u00a0 \u00a0 \u00a0 next(null, {inputAudio: obj});\u00a0 \u00a0 },\u00a0 }),\u00a0 detectStream);\n```To authenticate to Dialogflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dialogflow/detect_intent_stream.py) \n```\ndef detect_intent_stream(project_id, session_id, audio_file_path, language_code):\u00a0 \u00a0 \"\"\"Returns the result of detect intent with streaming audio as input.\u00a0 \u00a0 Using the same `session_id` between requests allows continuation\u00a0 \u00a0 of the conversation.\"\"\"\u00a0 \u00a0 from google.cloud import dialogflow\u00a0 \u00a0 session_client = dialogflow.SessionsClient()\u00a0 \u00a0 # Note: hard coding audio_encoding and sample_rate_hertz for simplicity.\u00a0 \u00a0 audio_encoding = dialogflow.AudioEncoding.AUDIO_ENCODING_LINEAR_16\u00a0 \u00a0 sample_rate_hertz = 16000\u00a0 \u00a0 session_path = session_client.session_path(project_id, session_id)\u00a0 \u00a0 print(\"Session path: {}\\n\".format(session_path))\u00a0 \u00a0 def request_generator(audio_config, audio_file_path):\u00a0 \u00a0 \u00a0 \u00a0 query_input = dialogflow.QueryInput(audio_config=audio_config)\u00a0 \u00a0 \u00a0 \u00a0 # The first request contains the configuration.\u00a0 \u00a0 \u00a0 \u00a0 yield dialogflow.StreamingDetectIntentRequest(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 session=session_path, query_input=query_input\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 # Here we are reading small chunks of audio data from a local\u00a0 \u00a0 \u00a0 \u00a0 # audio file. \u00a0In practice these chunks should come from\u00a0 \u00a0 \u00a0 \u00a0 # an audio input device.\u00a0 \u00a0 \u00a0 \u00a0 with open(audio_file_path, \"rb\") as audio_file:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 while True:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 chunk = audio_file.read(4096)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if not chunk:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # The later requests contains audio data.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 yield dialogflow.StreamingDetectIntentRequest(input_audio=chunk)\u00a0 \u00a0 audio_config = dialogflow.InputAudioConfig(\u00a0 \u00a0 \u00a0 \u00a0 audio_encoding=audio_encoding,\u00a0 \u00a0 \u00a0 \u00a0 language_code=language_code,\u00a0 \u00a0 \u00a0 \u00a0 sample_rate_hertz=sample_rate_hertz,\u00a0 \u00a0 )\u00a0 \u00a0 requests = request_generator(audio_config, audio_file_path)\u00a0 \u00a0 responses = session_client.streaming_detect_intent(requests=requests)\u00a0 \u00a0 print(\"=\" * 20)\u00a0 \u00a0 for response in responses:\u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'Intermediate transcript: \"{}\".'.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 response.recognition_result.transcript\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 # Note: The result from the last response is the final transcript along\u00a0 \u00a0 # with the detected content.\u00a0 \u00a0 query_result = response.query_result\u00a0 \u00a0 print(\"=\" * 20)\u00a0 \u00a0 print(\"Query text: {}\".format(query_result.query_text))\u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \"Detected intent: {} (confidence: {})\\n\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 query_result.intent.display_name, query_result.intent_detection_confidence\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 print(\"Fulfillment text: {}\\n\".format(query_result.fulfillment_text))\n```No preface\n **C#** : Please follow the [C# setup instructions](/dialogflow/docs/reference/libraries) on the client libraries page  and then visit the [Dialogflow reference documentation for .NET.](https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.Dialogflow.V2/index.html) \n **PHP** : Please follow the [PHP setup instructions](/dialogflow/docs/reference/libraries) on the client libraries page  and then visit the [Dialogflow reference documentation for PHP.](/php/docs/reference/cloud-dialogflow/latest) \n **Ruby** : Please follow the [Ruby setup instructions](/dialogflow/docs/reference/libraries) on the client libraries page  and then visit the [Dialogflow reference documentation for Ruby.](https://googleapis.dev/ruby/google-cloud-dialogflow/latest/Google/Cloud/Dialogflow.html)\n## Samples\nSee the [samples page](/dialogflow/docs/tutorials/samples) for best practices on streaming from a browser microphone to Dialogflow.", "guide": "Dialogflow"}