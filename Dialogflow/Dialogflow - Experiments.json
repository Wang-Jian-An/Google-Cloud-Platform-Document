{"title": "Dialogflow - Experiments", "url": "https://cloud.google.com/dialogflow/cx/docs/concept/experiments", "abstract": "# Dialogflow - Experiments\nExperiments are used to compare the performance of multiple flow versions ( versions) to a version (normally a production version) while handling live traffic. You can allocate a portion of live traffic to each flow version and monitor the following metrics:\n- **Contained** : Count of sessions that reached [END_SESSION](https://cloud.google.com/dialogflow/cx/docs/concept/handler#symbolic) without triggering other metrics below. Only available to agents using a telephony integration.\n- **Live agent handoff rate** : Count of sessions [handed off to a live agent](/dialogflow/cx/docs/concept/fulfillment#handoff) .\n- **Callback rate** : Count of sessions that were restarted by an end-user. Only available to agents using a telephony integration.\n- **Abandoned rate** : Count of sessions that were abandoned by an end-user. Only available to agents using a telephony integration.\n- **Session end rate** : Count of sessions that reached [END_SESSION](https://cloud.google.com/dialogflow/cx/docs/concept/handler#symbolic) .\n- **Total no-match count** : Total count of occurrences of a [no-match event](/dialogflow/cx/docs/concept/handler#event-built-in) .\n- **Total turn count** : Total number of conversational turns (one end-user input and one agent response is considered a turn).\n- **Average turn count** : Average number of turns.", "content": "## Preparation\nTo prepare for an experiment:\n- Decide which flow will be used for the experiment. You cannot run multiple experiments on a single flow, so ensure that you have partitioned your agent into multiple flows.\n- Create multiple [versions](/dialogflow/cx/docs/concept/version) for your flow. The differences between each version could be small or large, depending on what you want to compare.\n- Decide on the amount of traffic that will be allocated to your experiment. If you are testing minor changes, you might start with a higher amount of traffic. For large changes that may be disruptive, consider allocating a small amount of traffic to your experiment.## Create an experiment\nTo create an experiment:\n- Open the [Dialogflow CX Console](https://dialogflow.cloud.google.com/cx/projects) .\n- Select your GCP project to open the agent selector.\n- Select your agent to open the agent builder.\n- Select the **Manage** tab.\n- Click **Experiments** to open the Experiments panel.\n- Select the **Status** tab.\n- Click **Create** .\n- Enter a description.\n- Select the [environment](/dialogflow/cx/docs/concept/version) that you want to run the experiment from.\n- Select the flow for the experiment.\n- Optionally enter the number of days in which the experiment will automatically stop.\n- Enter the control flow version and the percentage of traffic that will go to the control version.\n- Enter one to four variant flow versions, and the percentage of traffic that will go to the variant version.\n- Optionally, click **Enable auto rollout and steps** for a gradual rollout of traffic to the variant flow. An automated experiment is based on, which are time durations in which a percentage of traffic is increased to the variant flow. Auto rollout only supports one variant flow.- Under **Rollout rules** , you can set one or more conditional rules to determine how the experiment should proceed through the steps.- If you select **Match at least one rule** , the experiment proceeds to the next step if at least one rule and the time duration for the current step are met.\n- If you select **Match all rules** , the experiment proceeds to the next step if all rules and the time duration for the current step are met.\n- If you select **Steps only** , the experiment proceeds according to the time durations for each step.\n- Under **Increase steps** , define a percentage of traffic to allocate to the variant flow and a time duration for each step. The default time duration for each step is 6 hours.\n- Select **Stop conditions** to set one or more conditions under which to stop sending traffic to the variant flow. Note that you cannot restart a stopped experiment.\n- Click **Save** .## Start and stop an experiment\nYou can start a saved experiment or manually stop a running experiment at any time. Stopping an experiment will cancel the traffic allocation and will revert traffic to its original state.\n**Note:** If you stop an experiment while it is [pending](#monitor) , results will not be available. If you stop an experiment while it is [running](#monitor) , results might be inconclusive or missing.\nTo start or stop an experiment:\n- Open the Experiments panel.\n- Select the **Status** tab.\n- Click **Start** or **Stop** for an experiment in the list.## Manage experiments\n**Note:** You can change variant traffic allocation while an experiment is running.\nYou can edit or delete experiments at any time:\n- Open the Experiments panel.\n- Select the **Status** tab.\n- Click the optionmenu for an experiment in the list.\n- Click **Edit** or **Delete** .## Monitor status of experiments\nAll experiments, regardless of their status, can be found on the experiments panel. Experiments can have four different statuses:\n- **Draft** : Experiment has been created, but it has never run.\n- **Pending** : Experiment has started recently, but results are not available yet.\n- **Running** : Experiment is running and interim results are available.\n- **Completed** : Experiment has been completed due to automatically or manually being stopped.## Viewing experiment results\nTo see experiment results:\n- Open the [Dialogflow CX Console](https://dialogflow.cloud.google.com/cx/projects) .\n- Select your GCP project to open the agent selector.\n- Select your agent to open the agent builder.\n- Select the **Manage** tab.\n- Click **Experiments** to open the Experiments panel.\n- Select the **Results** tab.\n- Select an environment and experiment to see the results.\nGreen colored results suggest a favorable outcome, while red suggests a less favorable result. Notice that in some cases, higher/lower numbers are not necessarily better (high abandonment rate / low abandonment rate).\n**Note:** You will see \"no experiment result\" if not enough conversations have been through each variant of the experiment.\n## Limitations\nThe following limitations apply:\n- The [Enable interaction logging](/dialogflow/cx/docs/concept/agent#settings-general) agent setting must be enabled.", "guide": "Dialogflow"}