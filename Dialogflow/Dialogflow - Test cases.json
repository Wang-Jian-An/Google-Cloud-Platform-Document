{"title": "Dialogflow - Test cases", "url": "https://cloud.google.com/dialogflow/cx/docs/concept/test-case", "abstract": "# Dialogflow - Test cases\nYou can use the built-in test feature to uncover bugs and prevent regressions. To test your agent, you create test cases using the [simulator](/dialogflow/cx/docs/concept/console#simulator) to define , then you execute test cases as needed. A test execution verifies that agent responses have not changed for end-user inputs defined in the test case.\nThe instructions below show you how to use the console, but you can also find the same functionality in the API.\n", "content": "## Simulator settings\nWhen you first open the simulator, you need to select an [agent environment or flow versions](/dialogflow/cx/docs/concept/version) and an active [flow](/dialogflow/cx/docs/concept/flow) . In most cases, you should use the draft environment and default start flow.\nYou can also enable or disable webhook calls at any time with the webhook toggle button. Disabling webhooks is useful when defining test cases.\n## Simulator input\nWhen interacting with the simulator, you provide end-user input as text, then press enter or click the send send button. In addition to plain text, you can choose alternate input types with the input input selector:\n- **Parameter** : Inject a [parameter](/dialogflow/cx/docs/concept/parameter) value. You can provide new parameters or provide preset values for existing parameters.\n- **Event** : Invoke an [event](/dialogflow/cx/docs/concept/handler#event) .\n- **DTMF** : Send dual-tone multi-frequency signaling (Touch-Tone) input for telephony interactions.## Create a test case\nTo create a conversation:\n- Open the [Dialogflow CX Console](https://dialogflow.cloud.google.com/cx/projects) .\n- Choose your GCP project.\n- Select your agent.\n- Click **Test Agent** to open the simulator.\n- Chat with the agent to create a conversation that covers the functionality you want to test. For each turn, verify correct values for the triggered intent, the agent response, the active page, and the session parameters.To save a conversation as a test case:\n- Click the savesystem_update_altbutton.\n- Enter a test case display name. Every test case must have a unique display name.\n- Optionally provide a tag name. Tags help you organize your test cases. All tags must start with a \"#\".\n- Optionally provide a note that describes the purpose of the test case.\n- Optionally select parameters you want to track in the test case. A list of suggested parameters are provided. You can also input other parameters to track. If you select tracking parameters, the parameter assertion is checked when running the test case. See more details on the parameter assertion in the [Run test cases](#run) section.\n- Click **Save** to save the test case.## Run test cases\nTo view all test cases for an agent, click **Test Cases** in the **Manage** tab. The test cases table shows the test name, the tags, the latest test time and environment, and the latest test result.\nTo run test cases:\n- Select the test cases you want to run, and click **Run** . Alternatively, you can click **Run all test cases** .\n- Select the environment you want to run the test cases against.\n- The tests start running and you can view the status in the task queue. The test result will be updated when it is completed.\nTo view the test detail result, click the test case. The **golden test case** and the **latest run** conversations are shown side-by-side.\nYou can click on any agent's conversational turn to see the details for that turn. The test engine checks the following types of data turn by turn to evaluate the test result:\n- **Agent dialogue** :For each conversational turn, the agent dialogue is compared from golden to latest run. If there is any difference, a warning is displayed. These differences do not prevent a test from passing, because agent dialogue often varies for the same agent state.\n- **Matched intent** :The matched intent must be the same for each turn for a test to pass.\n- **Current page** :The active page must be the same for each turn for a test to pass.\n- **Session parameters** :If you added tracking parameters when you created the test case, the test engine will check the corresponding session parameters and fail the test if there are missing/unexpected parameters or parameter value mismatches.\nIn some situations, a test case may have an expected failure due to an updated agent. If the conversation in the latest run reflects the expected changes, you can click **Save as golden** to overwrite the .\n## Edit test cases\nTo edit a test case, select the test case from the **Test cases** table, then click the edit edit icon next to the name of the test case. The **Update Test Cases** dialog appears.\nTo edit the metadata and settings for the test case, click the **Settings** tab.\n- You can edit the **Test case name** , **Tags** , and **Note** fields, or add new tracking parameters.\n- Click **Save** .\nTo edit the user input for the test case, click the **User Input** tab.\n- Add, remove, or edit the user inputs in JSON format.\n- Click **Confirm** . An automatic test run begins, and the updated conversation displays after the test run completes.\n- Click **Save** to overwrite the original golden test case, or click **Saveas** to create a new test case with the changes.## View test coverage\nTo view a test coverage report for all test cases, click **Coverage** .\nThe **Coverage** page includes the following tabs:\n- **Transitions** coverage is determined for all [state handlers](/dialogflow/cx/docs/concept/handler) (not including route groups) with a transition target exercised by the test case. The source flow/page and transition target flow/page are listed in the table.\n- **Intents** coverage is determined for all [intents](/dialogflow/cx/docs/concept/intent) that are matched by the test case.\n- **Route groups** coverage is determined for all [routegroups](/dialogflow/cx/docs/concept/handler#route-group) matched by the test case.## Import and export test cases\nTo export test cases:\n- Select test cases and click **Export** or click **Export all test cases** .\n- Click **Download to local file** , or provide a Cloud Storage bucket URI and click **Export to Google Cloud Storage** .\nWhen importing test cases, Dialogflow always creates new test cases for the target agent and does not overwrite any existing test cases. To import test cases:\n- Click **Import** .\n- Choose a local file or provide a Cloud Storage bucket URI.\n**Note:** There are references to the agent resources (intents, pages, transitions, etc.) in test cases. Test cases are validated one by one when being imported. Partial success from importing is supported, and any test case with a mismatched resource will fail.", "guide": "Dialogflow"}