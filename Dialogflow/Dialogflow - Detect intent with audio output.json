{"title": "Dialogflow - Detect intent with audio output", "url": "https://cloud.google.com/dialogflow/es/docs/how/detect-intent-tts", "abstract": "# Dialogflow - Detect intent with audio output\nApplications often need a bot to talk back to the end-user. Dialogflow can use [Cloud Text-to-Speech](/text-to-speech/docs) powered by [DeepMind WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) to generate speech responses from your agent. This conversion from intent text responses to audio is known as , , , or .\nThis guide provides an example that uses audio for both input and output when detecting an intent. This use case is common when developing apps that communicate with users via a purely audio interface.\nFor a list of supported languages, see the **TTS** column on the [Languages](/dialogflow/docs/reference/language) page.\n", "content": "## Before you begin\nThis feature is only applicable when using the API for [end-user interactions](/dialogflow/docs/api-overview) . If you are using an [integration](/dialogflow/docs/integrations) , you can skip this guide.\nYou should do the following before reading this guide:\n- Read [Dialogflow basics](/dialogflow/docs/basics) .\n- Perform [setup steps](/dialogflow/docs/quick/setup) .## Create an agent\nIf you have not already created an agent, create one now:- Go to the [Dialogflow ES Console](https://dialogflow.cloud.google.com) .\n- If requested, sign in to the Dialogflow Console.  See [Dialogflow console overview](/dialogflow/docs/console) for more information.\n- Click **Create Agent** in the left sidebar menu.  (If you already have other agents, click the agent name,  scroll to the bottom and click **Create new agent** .)\n- Enter your agent's name, default language, and default time zone.\n- If you have already created a project, enter that project.  If you want to allow the Dialogflow Console to create the project,  select **Create a new Google project** .\n- Click the **Create** button.## Import the example file to your agent\nThe steps in this guide make assumptions about your agent, so you need to [import](/dialogflow/docs/agents-settings#export) an agent prepared for this guide. When importing, these steps use the option, which overwrites all agent settings, intents, and entities.\nTo import the file, follow these steps:\n- Download the [room-booking-agent.zip](/static/dialogflow/es/docs/data/room-booking-agent.zip) file.\n- Go to the [Dialogflow ES Console](https://dialogflow.cloud.google.com) .\n- Select your agent.\n- Click the  settingsbutton  next to the agent name.\n- Select the **Export and Import** tab.\n- Select **Restore From Zip** and follow instructions to restore the zip file that you downloaded.## Detect intent\nTo detect intent, call the `detectIntent` method on the [Sessions](/dialogflow/docs/reference/common-types#sessions) type.\n### 1. Prepare audio contentDownload the [book-a-room.wav](/static/dialogflow/es/docs/data/book-a-room.wav) sample input_audio file, which says \"book a room\". The audio file must be base64 encoded for this example, so it can be provided in the JSON request below. Here is a Linux example:\n```\nwget https://cloud.google.com/dialogflow/es/docs/data/book-a-room.wavbase64 -w 0 book-a-room.wav > book-a-room.b64\n```\nFor examples on other platforms, see [Embedding Base64 encoded audio](/speech/docs/base64-encoding) in the Cloud Speech API documentation.\n### 2. Make detect intent requestCall the `detectIntent` method on the [Sessions](/dialogflow/docs/reference/common-types#sessions) type and specify base64 encoded audio.\nBefore using any of the request data, make the following replacements:- : your Google Cloud project ID\n- : a session ID\n- : the base64 content from the output file above\nHTTP method and URL:\n```\nPOST https://dialogflow.googleapis.com/v2/projects/PROJECT_ID/agent/sessions/SESSION_ID:detectIntent\n```\nRequest JSON body:\n```\n{\n \"queryInput\": {\n \"audioConfig\": {\n  \"languageCode\": \"en-US\"\n }\n },\n \"outputAudioConfig\" : {\n \"audioEncoding\": \"OUTPUT_AUDIO_ENCODING_LINEAR_16\"\n },\n \"inputAudio\": \"BASE64_AUDIO\"\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"responseId\": \"b7405848-2a3a-4e26-b9c6-c4cf9c9a22ee\",\n \"queryResult\": {\n \"queryText\": \"book a room\",\n \"speechRecognitionConfidence\": 0.8616504,\n \"action\": \"room.reservation\",\n \"parameters\": {\n  \"time\": \"\",\n  \"date\": \"\",\n  \"duration\": \"\",\n  \"guests\": \"\",\n  \"location\": \"\"\n },\n \"fulfillmentText\": \"I can help with that. Where would you like to reserve a room?\",\n \"fulfillmentMessages\": [  {\n  \"text\": {\n   \"text\": [   \"I can help with that. Where would you like to reserve a room?\"\n   ]\n  }\n  }\n ],\n \"intent\": {\n  \"name\": \"projects/PROJECT_ID/agent/intents/e8f6a63e-73da-4a1a-8bfc-857183f71228\",\n  \"displayName\": \"room.reservation\"\n },\n \"intentDetectionConfidence\": 1,\n \"diagnosticInfo\": {},\n \"languageCode\": \"en-us\"\n },\n \"outputAudio\": \"UklGRs6vAgBXQVZFZm10IBAAAAABAAEAwF0AAIC7AA...\"\n}\n```\nNotice that the value of the `queryResult.action` field is `room.reservation` , and the `outputAudio` field contains a large base64 audio string.\n### 3. Play output audioCopy the text from the `outputAudio` field and save it in a file named `output_audio.b64` . This file needs to be converted to audio. Here is a Linux example:\n```\nbase64 -d output_audio.b64 > output_audio.wav\n```\nFor examples on other platforms, see [Decoding Base64-Encoded Audio Content](/text-to-speech/docs/base64-decoding) in the Text-to-speech API documentation.\nYou can now play the `output_audio.wav` audio file and hear that it matches the text from the `queryResult.fulfillmentMessages[1].text.text[0]` field above. The second `fulfillmentMessages` element is chosen, because it is the text response for the default platform.\nTo authenticate to Dialogflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dialogflow/snippets/src/main/java/com/example/dialogflow/DetectIntentWithTextToSpeechResponse.java) \n```\nimport com.google.api.gax.rpc.ApiException;import com.google.cloud.dialogflow.v2.DetectIntentRequest;import com.google.cloud.dialogflow.v2.DetectIntentResponse;import com.google.cloud.dialogflow.v2.OutputAudioConfig;import com.google.cloud.dialogflow.v2.OutputAudioEncoding;import com.google.cloud.dialogflow.v2.QueryInput;import com.google.cloud.dialogflow.v2.QueryResult;import com.google.cloud.dialogflow.v2.SessionName;import com.google.cloud.dialogflow.v2.SessionsClient;import com.google.cloud.dialogflow.v2.TextInput;import com.google.common.collect.Maps;import java.io.IOException;import java.util.List;import java.util.Map;public class DetectIntentWithTextToSpeechResponse {\u00a0 public static Map<String, QueryResult> detectIntentWithTexttoSpeech(\u00a0 \u00a0 \u00a0 String projectId, List<String> texts, String sessionId, String languageCode)\u00a0 \u00a0 \u00a0 throws IOException, ApiException {\u00a0 \u00a0 Map<String, QueryResult> queryResults = Maps.newHashMap();\u00a0 \u00a0 // Instantiates a client\u00a0 \u00a0 try (SessionsClient sessionsClient = SessionsClient.create()) {\u00a0 \u00a0 \u00a0 // Set the session name using the sessionId (UUID) and projectID (my-project-id)\u00a0 \u00a0 \u00a0 SessionName session = SessionName.of(projectId, sessionId);\u00a0 \u00a0 \u00a0 System.out.println(\"Session Path: \" + session.toString());\u00a0 \u00a0 \u00a0 // Detect intents for each text input\u00a0 \u00a0 \u00a0 for (String text : texts) {\u00a0 \u00a0 \u00a0 \u00a0 // Set the text (hello) and language code (en-US) for the query\u00a0 \u00a0 \u00a0 \u00a0 TextInput.Builder textInput =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TextInput.newBuilder().setText(text).setLanguageCode(languageCode);\u00a0 \u00a0 \u00a0 \u00a0 // Build the query with the TextInput\u00a0 \u00a0 \u00a0 \u00a0 QueryInput queryInput = QueryInput.newBuilder().setText(textInput).build();\u00a0 \u00a0 \u00a0 \u00a0 //\u00a0 \u00a0 \u00a0 \u00a0 OutputAudioEncoding audioEncoding = OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_LINEAR_16;\u00a0 \u00a0 \u00a0 \u00a0 int sampleRateHertz = 16000;\u00a0 \u00a0 \u00a0 \u00a0 OutputAudioConfig outputAudioConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 OutputAudioConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAudioEncoding(audioEncoding)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setSampleRateHertz(sampleRateHertz)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 \u00a0 DetectIntentRequest dr =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 DetectIntentRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setQueryInput(queryInput)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setOutputAudioConfig(outputAudioConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setSession(session.toString())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 \u00a0 // Performs the detect intent request\u00a0 \u00a0 \u00a0 \u00a0 DetectIntentResponse response = sessionsClient.detectIntent(dr);\u00a0 \u00a0 \u00a0 \u00a0 // Display the query result\u00a0 \u00a0 \u00a0 \u00a0 QueryResult queryResult = response.getQueryResult();\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"====================\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"Query Text: '%s'\\n\", queryResult.getQueryText());\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Detected Intent: %s (confidence: %f)\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 queryResult.getIntent().getDisplayName(), queryResult.getIntentDetectionConfidence());\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Fulfillment Text: '%s'\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 queryResult.getFulfillmentMessagesCount() > 0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ? queryResult.getFulfillmentMessages(0).getText()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 : \"Triggered Default Fallback Intent\");\u00a0 \u00a0 \u00a0 \u00a0 queryResults.put(text, queryResult);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 \u00a0 return queryResults;\u00a0 }}\n```To authenticate to Dialogflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/dialogflow/detect-intent-TTS-response.v2.js) \n```\n// Imports the Dialogflow client libraryconst dialogflow = require('@google-cloud/dialogflow').v2;// Instantiate a DialogFlow client.const sessionClient = new dialogflow.SessionsClient();/**\u00a0* TODO(developer): Uncomment the following lines before running the sample.\u00a0*/// const projectId = 'ID of GCP project associated with your Dialogflow agent';// const sessionId = `user specific ID of session, e.g. 12345`;// const query = `phrase(s) to pass to detect, e.g. I'd like to reserve a room for six people`;// const languageCode = 'BCP-47 language code, e.g. en-US';// const outputFile = `path for audio output file, e.g. ./resources/myOutput.wav`;// Define session pathconst sessionPath = sessionClient.projectAgentSessionPath(\u00a0 projectId,\u00a0 sessionId);const fs = require('fs');const util = require('util');async function detectIntentwithTTSResponse() {\u00a0 // The audio query request\u00a0 const request = {\u00a0 \u00a0 session: sessionPath,\u00a0 \u00a0 queryInput: {\u00a0 \u00a0 \u00a0 text: {\u00a0 \u00a0 \u00a0 \u00a0 text: query,\u00a0 \u00a0 \u00a0 \u00a0 languageCode: languageCode,\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 \u00a0 outputAudioConfig: {\u00a0 \u00a0 \u00a0 audioEncoding: 'OUTPUT_AUDIO_ENCODING_LINEAR_16',\u00a0 \u00a0 },\u00a0 };\u00a0 sessionClient.detectIntent(request).then(responses => {\u00a0 \u00a0 console.log('Detected intent:');\u00a0 \u00a0 const audioFile = responses[0].outputAudio;\u00a0 \u00a0 util.promisify(fs.writeFile)(outputFile, audioFile, 'binary');\u00a0 \u00a0 console.log(`Audio content written to file: ${outputFile}`);\u00a0 });}detectIntentwithTTSResponse();\n```To authenticate to Dialogflow, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dialogflow/detect_intent_with_texttospeech_response.py) \n```\ndef detect_intent_with_texttospeech_response(\u00a0 \u00a0 project_id, session_id, texts, language_code):\u00a0 \u00a0 \"\"\"Returns the result of detect intent with texts as inputs and includes\u00a0 \u00a0 the response in an audio format.\u00a0 \u00a0 Using the same `session_id` between requests allows continuation\u00a0 \u00a0 of the conversation.\"\"\"\u00a0 \u00a0 from google.cloud import dialogflow\u00a0 \u00a0 session_client = dialogflow.SessionsClient()\u00a0 \u00a0 session_path = session_client.session_path(project_id, session_id)\u00a0 \u00a0 print(\"Session path: {}\\n\".format(session_path))\u00a0 \u00a0 for text in texts:\u00a0 \u00a0 \u00a0 \u00a0 text_input = dialogflow.TextInput(text=text, language_code=language_code)\u00a0 \u00a0 \u00a0 \u00a0 query_input = dialogflow.QueryInput(text=text_input)\u00a0 \u00a0 \u00a0 \u00a0 # Set the query parameters with sentiment analysis\u00a0 \u00a0 \u00a0 \u00a0 output_audio_config = dialogflow.OutputAudioConfig(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 audio_encoding=dialogflow.OutputAudioEncoding.OUTPUT_AUDIO_ENCODING_LINEAR_16\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 request = dialogflow.DetectIntentRequest(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 session=session_path,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 query_input=query_input,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output_audio_config=output_audio_config,\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 response = session_client.detect_intent(request=request)\u00a0 \u00a0 \u00a0 \u00a0 print(\"=\" * 20)\u00a0 \u00a0 \u00a0 \u00a0 print(\"Query text: {}\".format(response.query_result.query_text))\u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Detected intent: {} (confidence: {})\\n\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 response.query_result.intent.display_name,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 response.query_result.intent_detection_confidence,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 print(\"Fulfillment text: {}\\n\".format(response.query_result.fulfillment_text))\u00a0 \u00a0 \u00a0 \u00a0 # The response's audio_content is binary.\u00a0 \u00a0 \u00a0 \u00a0 with open(\"output.wav\", \"wb\") as out:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 out.write(response.output_audio)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print('Audio content written to file \"output.wav\"')\n```\nSee the [Detect intent responses](#response) section for a description of the relevant response fields.\n## Detect intent responses\nThe response for a detect intent request is a `DetectIntentResponse` type.\nNormal detect intent processing controls the content of the `DetectIntentResponse.queryResult.fulfillmentMessages` field.\nThe `DetectIntentResponse.outputAudio` field is populated with audio based on the values of platform responses found in the `DetectIntentResponse.queryResult.fulfillmentMessages` field. If multiple default text responses exist, they will be concatenated when generating audio. If no default platform text responses exist, the generated audio content will be empty.\nThe `DetectIntentResponse.outputAudioConfig` field is populated with audio settings used to generate the output audio.\n## Detect intent from a stream\nWhen detecting intent from a stream, you send requests similar to the example that does not use output audio: [Detecting Intent from a Stream](/dialogflow/docs/how/detect-intent-stream) . However, you supply a [OutputAudioConfig](/dialogflow/docs/reference/rpc/google.cloud.dialogflow.v2#google.cloud.dialogflow.v2.OutputAudioConfig) field to the request. The `output_audio` and `output_audio_config` fields are populated in the very last streaming response that you get from the Dialogflow API server. For more information, see [StreamingDetectIntentRequest](/dialogflow/docs/reference/rpc/google.cloud.dialogflow.v2#google.cloud.dialogflow.v2.StreamingDetectIntentRequest) and [StreamingDetectIntentResponse](/dialogflow/docs/reference/rpc/google.cloud.dialogflow.v2#streamingdetectintentresponse) .\n## Agent settings for speech\nYou can control various aspects of speech synthesis. See [the agent speech settings](/dialogflow/docs/agents-settings#speech) .\n**Note:** API calls that provide output audio config override all existing text-to-speech settings applied to the agent.\n## Use the Dialogflow simulator\nYou can interact with the agent and receive audio responses via the [Dialogflow simulator](/dialogflow/docs/console#simulator) :\n- Follow the [steps above to enable automatic text to speech](#agent-settings) .\n- Type or say \"book a room\" in the simulator.\n- See the **output audio** section at the bottom of the simulator.", "guide": "Dialogflow"}