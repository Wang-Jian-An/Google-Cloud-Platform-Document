{"title": "Dialogflow - Speech adaptation", "url": "https://cloud.google.com/dialogflow/cx/docs/concept/speech-adaptation", "abstract": "# Dialogflow - Speech adaptation\nWhen performing a detect intent request, you can optionally supply [phrase_hints](/dialogflow/cx/docs/reference/rpc/google.cloud.dialogflow.cx.v3#google.cloud.dialogflow.cx.v3.InputAudioConfig.FIELDS.repeated.string.google.cloud.dialogflow.cx.v3.InputAudioConfig.phrase_hints) to provide hints to the speech recognizer. These hints can help with recognition in a specific conversation state.\n**Note:** Providing explicit non-empty `phrase_hints` for a detect intent request overrides explicit speech adaptation configuration in console and the implicit speech context hints generated by auto speech adaptation for input audio (speech-to-text) configuration.\n", "content": "## Auto speech adaptation\nThe auto speech adaptation feature improves the speech recognition accuracy of your agent by automatically using conversation state to pass relevant entities and training phrases as speech context hints for all detect intent requests. This feature is disabled by default.\n### Enable or disable auto speech adaptation\nTo enable or disable auto speech adaptation:\n- Open the [Dialogflow CX Console](https://dialogflow.cloud.google.com/cx/projects) .\n- Choose your GCP project.\n- Select your agent.\n- Click **Agent Settings** .\n- Click the **Speech and IVR** tab.\n- Toggle **Enable auto speech adaptation** on or off.\n- Click **Save** .\nSee the `get` and `patch/update` methods for the `Agent` type.\n### Agent design for speech recognition improvements\nWith auto speech adaptation enabled, you can build your agent in ways to take advantage of it. The following sections explain how speech recognition may be improved with certain changes to your agent's training phrases, and entities.- If you define training phrases with a phrase like \"stuffy nose\", a similar sounding end-user utterance is reliably recognized as \"stuffy nose\" and not \"stuff he knows\".\n- When you have a [required parameter](/dialogflow/cx/docs/concept/parameter#form) that forces Dialogflow into form-filling prompts, auto speech adaptation will strongly bias towards the entity being filled.\nIn all cases, auto speech adaptation is only biasing the speech recognition, not limiting it. For example, even if Dialogflow is prompting a user for a required parameter, users will still be able to trigger other intents such as a top-level \"talk to an agent\" intent.\nIf you define a training phrase that uses the `@sys.number` [system entity](/dialogflow/cx/docs/concept/entity-system) , and the end user says \"I want two\", it may be recognized as \"to\", \"too\", \"2\", or \"two\".\nWith auto speech adaptation enabled, Dialogflow uses the `@sys.number` entity as a hint during speech recognition, and the parameter is more likely to be extracted as \"2\".- If you define a [custom entity](/dialogflow/cx/docs/concept/entity-custom) for product or service names offered by your company, and the end-user mentions these terms in an utterance, they are more likely to be recognized. A training phrase \"I love Dialogflow\", where \"Dialogflow\" is annotated as the @product entity, will tell auto speech adaptation to bias for \"I love Dialogflow\", \"I love Cloud Speech\", and all other entries in the @product entity.\n- It is especially important to define clean entity synonyms when using Dialogflow to detect speech. Imagine you have two @product entity entries, \"Dialogflow\" and \"Dataflow\". Your synonyms for \"Dialogflow\" might be \"Dialogflow\", \"dialogue flow\", \"dialogue builder\", \"Speaktoit\", \"speak to it\", \"API.ai\", \"API dot AI\". These are good synonyms because they cover the most common variations. You don't need to add \"the dialogue flow builder\" because \"dialogue flow\" already covers that.\n**Note:** Why is this important? Consider that you have two entities \"Dialogflow\" and \"Dataflow\", and two synonyms are \"the dialogue flow builder\" and \"Google Cloud Dataflow\". An end-user might very reasonably say \"Google Cloud Dialogflow\", but because there is no \"Google Cloud Dialogflow\" synonym, the speech recognition will likely hear \"Google Cloud Dataflow\" because the entity definitions are biased towards that phrase. Likewise, if someone says \"the dataflow builder\" speech will most likely hear \"the dialogue flow builder\" because it's the only entity defined with \"builder\". Instead, you will get better performance by defining only the key phrases as listed in the bullet above. In summary, be careful to not add generic data to entity definitions as this is what intent training phrases are designed for. A training phrase \"Google Cloud Dataflow\", where \"Dataflow\" is annotated as the @product entity allows auto speech adaptation to listen for \"Google Cloud Dataflow\" and \"Google Cloud Dialogflow\" with equal weight. See [Agent design](/dialogflow/cx/docs/concept/agent-design#input) for more best practices.\n- User utterances with consecutive but distinct number entities can be ambiguous. For example, \"I want two sixteen packs\" might mean 2 quantities of 16 packs, or 216 quantities of packs. Speech adaptation can help disambiguate these cases if you set up entities with spelled-out values:- Define a`quantity`entity with entries:`zero``one``...``twenty`\n- Define a`product`or`size`entity with entries:`sixteen pack``two ounce``...``five liter`\n- Only entity synonyms are used in speech adaptation, so you can define an entity with reference value`1`and single synonym`one`to simplify your fulfillment logic.\n[Regexp entities](/dialogflow/cx/docs/concept/entity-regexp)\nTo recognize these sequences over voice, implement of the requirements below:\nWhile any regular expression can be used to extract entities from text inputs, only certain expressions will tell auto speech adaptation to bias for spelled-out alphanumeric or digit sequences when recognizing speech.\nIn the regexp entity, entry must follow of these rules:\n- Should match some alphanumeric characters, for example:`\\d`,`\\w`,`[a-zA-Z0-9]`\n- Shouldcontain whitespace``or`\\s`, though`\\s*`and`\\s?`are allowed\n- Shouldcontain capture or non-capture groups`()`\n- Shouldtry to match any special characters or punctuation like:`` ~ ! @ # $ % ^ & * ( ) - _ = + , . < > / ? ; ' : \" [ ] { } \\ |`\nThis entry can have character sets `[]` and repetition quantifiers like `*` , `?` , `+` , `{3,5}` .\nSee [Examples](#regexp-examples) .\nMark the regexp entity as a required [form parameter](/dialogflow/cx/docs/concept/parameter#form) , so it can be collected during form filling. This allows auto speech adaptation to strongly bias for sequence recognition instead of trying to recognize an intent and sequence at the same time. Otherwise, \"Where is my package for ABC123\" might be misrecognized as \"Where is my package 4ABC123\".\nDo use the regexp entity for an [intent training phrase annotation](/dialogflow/cx/docs/concept/intent#annot) . This ensures that the parameter is resolved as part of form filling.\nSee [Testing speech adaptation](#testing) .\nFor example, a regexp entity with a single entry `([a-zA-Z0-9]\\s?){5,9}` will not trigger the speech sequence recognizer because it contains a capture group. To fix this, simply add another entry for `[a-zA-Z0-9]{5,9}` . Now you will benefit from the sequence recognizer when matching \"ABC123\", yet the NLU will still match inputs like \"ABC 123\" thanks to the original rule that allows spaces.\nThe following examples of regular expressions adapt for alphanumeric sequences:\n```\n^[A-Za-z0-9]{1,10}$\nWAC\\d+\n215[2-8]{3}[A-Z]+\n[a-zA-Z]\\s?[a-zA-Z]\\s?[0-9]\\s?[0-9]\\s?[0-9]\\s?[a-zA-Z]\\s?[a-zA-Z]\n```\nThe following examples of regular expressions adapt for digit sequences:\n```\n\\d{2,8}\n^[0-9]+$\n2[0-9]{7}\n[2-9]\\d{2}[0-8]{3}\\d{4}\n```\nAuto speech adaptation's built-in support for regexp entities varies by language. Check [Speech class tokens](/speech-to-text/docs/class-tokens) for `$OOV_CLASS_ALPHANUMERIC_SEQUENCE` and `$OOV_CLASS_DIGIT_SEQUENCE` supported languages.\nIf your language is not listed, you can work around this limitation. For example, if you want an employee ID that is three letters followed by three digits to be accurately recognized, you could build your agent with the following entities and parameters:\n- Define a`digit`entity that contains 10 entity entries (with synonyms):`0, 0``1, 1``...``9, 9`\n- Define a`letter`entity that contains 26 entity entries (with synonyms):`A, A``B, B``...``Z, Z`\n- Define a`employee-id`entity that contains a single entity entry (without synonyms):`@letter @letter @letter @digit @digit @digit`\n- Use`@employee-id`as a parameter in a training phrase.\n**Note:** The `employee-id` entity entry requires whitespace between consecutive entities in order to be a valid entity definition. Because of this, a text query of \"ABC123\" is not matched, but a spoken utterance of \"A B C 1 2 3\" is matched because the auto speech adaptation will bias for the spaces.\n## Manual speech adaptation\nManual speech adaptation allows you to manually configure speech adaptation phrases for a flow or a page. It also overrides implicit speech contexts generated by auto speech adaptation when the latter is enabled.\nThe flow level and page level speech adaptation settings have a hierarchical relation, which means that a page inherits speech adaptation settings from the flow level by default and the more fine-grained page level always overrides flow level if the page has a customized setting.\nFor speech adaptation setting, flow level setting and page level setting can be enabled independently. If the flow level adaptation setting is not enabled, you can still choose **Customize** at page level to enable manual speech adaptation for that specific page. Similarly, if you disable manual speech adaptation in flow level setting, pages in the flow with **Customize** selected will not be impacted.\nHowever, flow level setting and page level setting cannot be disabled independently. If a flow has manual speech adaptation enabled, you cannot disable it for a page under the flow through the **Customize** choice. Therefore, if you want to have a mixed usage of manual speech adaptation and auto speech adaptation for pages within a flow, you should not enable manual speech adaptation at flow level and should only use page level adaptation settings instead. You can refer to the table below to understand what combination of flow and page setting you should use for your case of adaptation.\n| Target effect             | Recommended use of adaptation settings            |\n|:--------------------------------------------------------------|:--------------------------------------------------------------------------------------|\n| Disable auto adaptation for a flow       | Flow enabled with no phrase sets (pages within the flow by default use flow setting). |\n| Disable auto adaptation for a page       | Flow disabled and page enabled (Customize chosen) with no phrase sets.    |\n| Only use manual speech adaptation for all pages within a flow | Flow enabled. Customize pages that need use phrase sets different from flow.   |\n| Mix use of auto and manual adaptation within a flow   | Flow disabled. Customize pages that you want to apply manual adaptation.    |\n| Only use auto speech adaptation for all pages within a flow | Flow disabled.                  |\n**Note:** Manual speech adaptation does not yet support multilingual agents.\n### Enable or disable manual speech adaptation\nTo enable or disable manual speech adaptation at flow or page level:\n- Open the [Dialogflow CX Console](https://dialogflow.cloud.google.com/cx/projects) .\n- Choose your GCP project.\n- Hover your mouse over the flow in the **Flows** section.\n- Click the optionsmore_vertbutton.\n- Select **Flow Settings** in the dropdown menu.\n- Select the checkbox **Enable manual speech adaptation** or deselect it.\n- Edit, add or delete phrase sets in the phrase set table\n- Click **Save** .\n- Open the [Dialogflow CX Console](https://dialogflow.cloud.google.com/cx/projects) .\n- Choose your GCP project.\n- Hover your mouse over the page in the **Pages** section.\n- Click the optionsmore_vertbutton.\n- Select **Page Settings** in the dropdown menu.\n- **Use flow level** is chosen by default and when chosen, flow level adaptation phrases will be re-used for this page. You can choose **Customize** to configure adaptation phrases different to flow level settings. Even if manual speech adaptation is disabled at flow level, you can still enable and configure manual speech adaptation for a page in that flow through the **Customize** option.\n- Edit, add or delete phrase set in the adaptation phrase set table\n- Click **Save** .\n### Manual phrase set configuration for speech recognition improvements\nIn an adaptation phrase set, you can define single-word or multi-word phrases with optional references to speech class tokens. For example, you can add phrases like \"great rate\", \"tracking number is $OOV_CLASS_ALPHANUMERIC_SEQUENCE\", or \"$FULLPHONENUM\". These provided phrases increase the probability of them getting transcribed over other phonetically similar phrases. When you add a multi-word phrase without any boost, the bias is applied to both the whole phrase and the continuous portions within the phrase. In general, the number of phrases should be kept small and you should only add phrases that the speech recognition struggles to get right without speech adaptation. If Speech-to-Text can already recognize a phrase correctly, then there's no need to add this phrase into speech adaptation settings. If you see a few phrases that Speech-to-Text often misrecognizes at a page or flow, you can add the correct phrases to its corresponding adaptation settings.\n**Note:** There is a 75 character limit for each speech adaptation phrase, and in total there can be at most 1200 phrases. You can see corresponding validation warning messages in the phrase set edit window when you exceed these limits.\nHere's an example of how you can use speech adaptation to correct recognition issues. Let's say you are designing a phone device trading agent, and the user may either say something including the phrases \"sell phones\" or \"cell phone\" after the agent asks its first question \"what do you need help with?\". Then how can we use speech adaptation to improve recognition accuracy on both phrases?\nIf you include both phrases in the adaptation settings, Speech-to-Text may still be confused, as they sound similar. If you just provide one phrase out of the two, then Speech-to-Text may misrecognize one phrase as the other. To improve speech recognition accuracy for both phrases, you need to provide Speech-to-Text with more context clues to distinguish between when it should hear \"sell phones\" and when it should hear \"cell phone\". For example, you may notice people often use \"sell phones\" as a part of utterances like \"how to sell phones\", \"want to sell phones\" or \"do you sell phones\", whereas \"cell phone\" as a part of utterances like \"purchase cell phone\", \"cell phone bill\", and \"cell phone service\". If you provide these more precise phrases to the model instead of the short original phrases \"cell phone\" and \"sell phones\", Speech-to-Text will learn that \"sell phone\" as a verb phrase is more likely to follow after words like \"how to\", \"want to\" and \"do you\", while \"cell phone\" as a noun phrase is more likely to follow after words like \"purchase\" or be followed by words like \"bill\" or \"service\". Therefore, as a rule of thumb for configuring adaptation phrases, it is usually better to provide more precise phrases like \"how to sell phones\" or \"do you sell phones\" than only including \"sell phone\".\nApart from natural language words, you can also embed references to speech class tokens into a phrase. [Speech class tokens](/speech-to-text/docs/adaptation-model#class_tokens) represent common concepts that usually follow certain format in writing. For example, for the address number in an address like \"123 Main Street\", people would usually expect to see an address number's numerical format \"123\" in an address instead of its fully spelled-out version \"one-hundred twenty-three\". If you expect certain formatting in the transcription results, especially for alphanumeric sequences, please refer to the [list of supported class tokens](/speech-to-text/docs/class-tokens) to see which tokens are available for your language and your use case.\nIf the page already has intent routes or parameters with references to [system entities](/dialogflow/cx/docs/concept/entity-system) , here's a reference table for mappings between common system entities and speech class tokens:\n| System entities  | Speech class tokens     |\n|:---------------------|:-------------------------------------|\n| @sys.date   | $MONTH $DAY $YEAR     |\n| @sys.date-time  | $MONTH $DAY $YEAR     |\n| @sys.date-period  | $MONTH $DAY $YEAR     |\n| @sys.time   | $TIME        |\n| @sys.time-period  | $TIME        |\n| @sys.age    | $OPERAND        |\n| @sys.number   | $OPERAND        |\n| @sys.number-integer | $OPERAND        |\n| @sys.cardinal  | $OPERAND        |\n| @sys.ordinal   | $OPERAND        |\n| @sys.percentage  | $OPERAND        |\n| @sys.duration  | $OPERAND        |\n| @sys.currency-name | $MONEY        |\n| @sys.unit-currency | $MONEY        |\n| @sys.phone-number | $FULLPHONENUM      |\n| @sys.zip-code  | $POSTALCODE or $OOV_CLASS_POSTALCODE |\n| @sys.address   | $ADDRESSNUM $STREET $POSTALCODE  |\n| @sys.street-address | $ADDRESSNUM $STREET $POSTALCODE  |\n| @sys.temperature  | $OOV_CLASS_TEMPERATURE    |\n| @sys.number-sequence | $OOV_CLASS_DIGIT_SEQUENCE   |\n| @sys.flight-number | $OOV_CLASS_ALPHANUMERIC_SEQUENCE  |\nIf adding phrases without the boost value does not provide a strong enough biasing effect, you can use the boost value to further strengthen speech adaptation biasing effect.\nBoost applies additional bias when set to values greater than 0 and no more than 20. When boost is empty or 0, the default biasing effect helps recognize the whole phrase and the continuous portions within the phrase. For example, a non-boosted phrase \" to \" helps recognize that phrase and also similar phrases like \"I \" and \"Hi \".\nWhen positive boost is applied, the biasing effect is stronger, but it only applies to the exact phrase. For example, a boosted phrase \" \" helps recognize \"can you \" but not \"do you sell any phones\".\nFor these reasons, you will get the best results if you provide phrases both with and without boost.\nHigher boost values can result in fewer false negatives, which are cases where the word or phrase occurred in the audio but wasn't correctly recognized by Speech-to-Text (underbiasing). However, boost can also increase the likelihood of false positives; that is, cases where the word or phrase appears in the transcription even though it didn't occur in the audio (overbiasing). You usually need to fine-tune your biasing phrases to find a good trade-off point between the two biasing issues.\nYou can learn more about how to fine-tune boost value for phrases in [Cloud Speech doc about boost](/speech-to-text/docs/adaptation-model#fine-tune_transcription_results_using_boost) .\n## When to use auto or manual speech adaptation\nIn general, if you are not sure if speech adaptation will improve speech recognition quality for your agent (no clear transcription error patterns in mind), you are encouraged to try auto speech adaptation first before resorting to manual speech adaptation. For more nuanced decisions, consider the following factors to decide between auto speech adaptation or manual speech adaptation:\n### 1. Form filling\nAuto speech adaptation works very well with [form filling](/dialogflow/cx/docs/concept/parameter#form) since it uses [ABNF grammar context](/speech-to-text/docs/adaptation-model#abnf_grammars) for the form parameters and enforces grammar rules based on their entity types. Since manual speech adaptation doesn't support [ABNF grammars](/speech-to-text/docs/adaptation-model#abnf_grammars) yet, Auto Speech adaptation is generally preferred over manual speech adaptation for a form filling page. Still for pages with only system entity parameters and simple regexp entities that are supported by [speech class tokens](/speech-to-text/docs/class-tokens) , you can also use manual speech adaptation to achieve biasing effect similar to auto speech adaptation without the need to [tune regexp entities](#regexp-entities) .\n### 2. Page or flow transition complexity\nFor a simple page or flow with a few intent routes, auto speech adaptation will likely generate representative biasing phrases and perform reasonably well.\nHowever, if a page or flow has a large amount of intent routes (for a page, please also consider the number of flow-level routes), or if any of the intents have overly long or short unimportant training phrases (For example, a whole sentence or a single word with only one or two syllables), it is very likely that speech adaptation model won't work well with these phrases. You should first try disabling speech adaptation for the open-ended pages with high complexity by enabling manual speech adaptation with empty phrase sets (empty adaptation override). And after that evaluate whether there are special non-ambiguous phrases that still need to be provided to Speech-to-Text to improve recognition quality.\nAnother symptom of this complexity issue is seeing a wide range of underbiasing or overbiasing issues when auto speech adaptation is enabled. Similar to the case above, you also need to test with speech adaptation disabled for the specific page first. If erroneous behaviors persist after disabling speech adaptation, then you can add the phrases you want to correct towards into speech adaptation settings and even add boost values to further strengthen biasing effects when necessary.\n## Testing speech adaptation\nWhen testing your agent's speech adaptation capabilities for a particular training phrase or entity match, you should not jump directly to testing the match with the first voice utterance of a conversation. You should use only voice or event inputs for the entire conversation prior to the match you want to test. The behavior of your agent when tested in this manner will be similar to the behavior in actual production conversations.\n**Note:** If you are testing the first conversational turn of a telephony agent that normally uses the [WELCOME](/dialogflow/cx/docs/concept/handler#event-built-in) event, you should invoke this event before testing the match.\n## Limitations\nThe following limitations apply:\n- Speech adaptation is not available to all speech models and language combinations. Refer to [Cloud Speech language support page](/speech-to-text/docs/speech-to-text-supported-languages) to verify if \"model adaptation\" is available to your speech model and language combination.\n- Currently manual speech adaptation does not support [custom classes](/speech-to-text/docs/adaptation-model#custom_classes) or [ABNF grammar](/speech-to-text/docs/adaptation-model#abnf_grammars) yet. You can enable auto speech adaptation or use runtime detect intent request to make use of these adaptation features.\n- The same boost value may perform differently for different speech models and languages, so use caution when configuring them manually for agents using multiple languages or speech models. Currently, manual speech adaptation applies to all languages in an agent, so multilingual agents should only use language-agnostic phrases or split each language into a separate agent. Since the default biasing behavior (not providing boost or 0 boost) usually performs reasonably well for all languages and models, you don't need to configure language-specific boost values unless stronger biasing is required for your recognition use case. You can learn more about how to fine-tune boost value in this [Cloud Speech-to-Text guide](/speech-to-text/docs/adaptation-model#boost_basics) .\n- Recognizing long character sequences is challenging. The number of characters that are captured in a single turn is directly related to the quality of your input audio. If you have followed all of the [regexp entity guidelines](#regexp-entities) and tried using relevant speech class tokens in manual speech adaptation settings and are still struggling to capture the entire sequence in a single turn, you may consider some more conversational alternatives:- When validating the sequence against a database, consider cross-referencing other collected parameters like dates, names, or phone numbers to allow for incomplete matches. For example, instead of just asking a user for their order number, also ask for their phone number. Now, when your webhook queries your database for order status, it can rely first on the phone number, then return the closest matching order for that account. This could allow Dialogflow to mishear \"ABC\" as \"AVC\", yet still return the correct order status for the user.\n- For extra long sequences, consider designing a flow that encourages end-users to pause in the middle so that the bot can confirm as you go.", "guide": "Dialogflow"}