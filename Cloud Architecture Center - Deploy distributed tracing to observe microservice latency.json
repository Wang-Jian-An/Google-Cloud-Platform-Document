{"title": "Cloud Architecture Center - Deploy distributed tracing to observe microservice latency", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Deploy distributed tracing to observe microservice latency\nLast reviewed 2023-08-11 UTC\nThis document shows how to deploy the reference architecture that's described in [Use distributed tracing to observe microservice latency](/) . The deployment that's illustrated in this document captures trace information on microservice applications using OpenTelemetry and Cloud Trace.\nThe sample application in this deployment is composed of two microservices that are written in [Go](https://golang.org/) .\nThis document assumes you're familiar with the following:\n- The Go programming language\n- [Google Kubernetes Engine (GKE)](/kubernetes-engine/docs) ", "content": "## Objectives\n- Create a GKE cluster and deploy a sample application.\n- Review OpenTelemetry instrumentation code.\n- Review traces and logs generated by the instrumentation.## Architecture\nThe following diagram shows the architecture that you deploy.\nYou use [Cloud Build](/build) \u2014a fully managed continuous integration, delivery, and deployment platform\u2014to build container images from the sample code and store them in [Artifact Registry](/artifact-registry/docs) . The GKE clusters pull the images from Artifact Registry at deployment time.\nThe frontend service accepts HTTP requests on the `/` URL and calls the backend service. The address of the backend service is defined by an environment variable.\nThe backend service accepts HTTP requests on the `/` URL and makes an outbound call to an external URL as defined in an environment variable. After the external call is completed, the backend service returns the HTTP status call (for example, `200` ) to the caller.\n## Costs\nIn this document, you use the following billable components of Google Cloud:\n- [GKE](/kubernetes-engine/pricing) \n- [Compute Engine](/compute/pricing) \n- [Cloud Trace](/stackdriver/pricing) \n- [Cloud Build](/build/pricing) \n- [Artifact Registry](/artifact-registry/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) .\nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .\n## Before you begin\n## Set up your environment\nIn this section, you set up your environment with the tools that you use throughout the deployment. You run all the terminal commands in this deployment from Cloud Shell.\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) At the bottom of the Google Cloud console, a [Cloud Shell](/shell/docs/how-cloud-shell-works) session starts and displays a command-line prompt. Cloud Shell is a shell environment  with the Google Cloud CLI  already installed and with values already set for  your current project. It can take a few seconds for the session to initialize.\n- Set an environment variable to the ID of your Google Cloud project:```\nexport PROJECT_ID=$(gcloud config list --format 'value(core.project)' 2>/dev/null)\n```\n- Download the required files for this deployment by cloning the  associated Git repository:```\ngit clone https://github.com/GoogleCloudPlatform/kubernetes-engine-samples.git\ncd kubernetes-engine-samples/observability/distributed-tracing\nWORKDIR=$(pwd)\n```You make the repository folder the working directory ( `$WORKDIR` ) from which you do all of the tasks that are related to this deployment. That way, if you don't want to keep the resources, you can delete the folder when you finish the deployment.\n### Install tools\n- In Cloud Shell, install [kubectx and kubens](https://github.com/ahmetb/kubectx) :```\ngit clone https://github.com/ahmetb/kubectx $WORKDIR/kubectx\nexport PATH=$PATH:$WORKDIR/kubectx\n```You use these tools to work with multiple Kubernetes clusters, contexts, and namespaces.\n- In Cloud Shell, install [Apache Bench](https://httpd.apache.org/docs/2.4/programs/ab.html) , an open source load-generation tool:```\nsudo apt-get install apache2-utils\n```## Create a Docker repository\nCreate a Docker repository to store the sample image for this deployment.\n- In the Google Cloud console, open the **Repositories** page. [Open the Repositories page](https://console.cloud.google.com/artifacts) \n- Click **Create Repository** .\n- Specify `distributed-tracing-docker-repo` as the repository name.\n- Choose **Docker** as the format and **Standard** as the mode.\n- Under **Location Type** , select **Region** and then choose the location `us-west1` .\n- Click **Create** .\nThe repository is added to the repository list.- In Cloud Shell, create a new Docker repository named `distributed-tracing-docker-repo` in the location `us-west1` with the description `docker repository` :```\ngcloud artifacts repositories create distributed-tracing-docker-repo --repository-format=docker \\\n--location=us-west1 --description=\"Docker repository for distributed tracing deployment\"\n```\n- Verify that the repository was created:```\ngcloud artifacts repositories list\n```## Create GKE clusters\nIn this section, you create two GKE clusters where you deploy the sample application. GKE clusters are created with write-only access to the Cloud Trace API by default, so you don't need to define access when you create the clusters.\n- In Cloud Shell, create the clusters:```\ngcloud container clusters create backend-cluster \\\n --zone=us-west1-a \\\n --verbosity=none --async\ngcloud container clusters create frontend-cluster \\\n --zone=us-west1-a \\\n --verbosity=none\n```In this example, the clusters are in the `us-west1-a` zone. For more information, see [Geography and regions](/docs/geography-and-regions) .\n- Get the cluster credentials and store them locally:```\ngcloud container clusters get-credentials backend-cluster --zone=us-west1-a\ngcloud container clusters get-credentials frontend-cluster --zone=us-west1-a\n```\n- Rename the contexts of the clusters to make it easier to access them later in the deployment:```\nkubectx backend=gke_${PROJECT_ID}_us-west1-a_backend-cluster\nkubectx frontend=gke_${PROJECT_ID}_us-west1-a_frontend-cluster\n```## Review OpenTelemetry instrumentation\nIn the following sections, you review the code from the `main.go` file in the sample application. This helps you learn how to use context propagation to allow spans from multiple requests to be appended to a single parent trace.\n### Review the imports in the application code\n[  observability/distributed-tracing/main.go ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/observability/distributed-tracing/main.go) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/observability/distributed-tracing/main.go)\n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io/ioutil\"\u00a0 \u00a0 \u00a0 \u00a0 \"log\"\u00a0 \u00a0 \u00a0 \u00a0 \"net/http\"\u00a0 \u00a0 \u00a0 \u00a0 \"os\"\u00a0 \u00a0 \u00a0 \u00a0 \"strconv\"\u00a0 \u00a0 \u00a0 \u00a0 cloudtrace \"github.com/GoogleCloudPlatform/opentelemetry-operations-go/exporter/trace\"\u00a0 \u00a0 \u00a0 \u00a0 \"github.com/gorilla/mux\"\u00a0 \u00a0 \u00a0 \u00a0 \"go.opentelemetry.io/contrib/detectors/gcp\"\u00a0 \u00a0 \u00a0 \u00a0 \"go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp\"\u00a0 \u00a0 \u00a0 \u00a0 \"go.opentelemetry.io/contrib/propagators/autoprop\"\u00a0 \u00a0 \u00a0 \u00a0 \"go.opentelemetry.io/otel\"\u00a0 \u00a0 \u00a0 \u00a0 \"go.opentelemetry.io/otel/sdk/resource\"\u00a0 \u00a0 \u00a0 \u00a0 \"go.opentelemetry.io/otel/sdk/trace\")\n```\nNote the following about the imports:\n- The`go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp`package contains the`otelhttp`plugin, which can instrument an HTTP server or HTTP client. Server instrumentation retrieves the [span context](https://opentelemetry.io/docs/concepts/signals/traces/#span-context) from the HTTP request and records a [span](https://opentelemetry.io/docs/concepts/signals/traces/#spans) for the server's handling of the request. Client instrumentation injects the span context into the outgoing HTTP request and records a span for the time that's spent waiting for a response.\n- The`go.opentelemetry.io/contrib/propagators/autoprop`package provides an implementation of the OpenTelemetry`TextMapPropagator`interface, which is used by`otelhttp`to handle propagation. Propagators determine the format and keys that are used to store the trace context in transports like HTTP. Specifically,`otelhttp`passes HTTP headers to the propagator. The propagator extracts a span context into a Go context from the headers, or it encodes and injects a span context in the Go context into headers (depending on whether it is client or server). By default, the`autoprop`package injects and extracts the span context using [W3C tracecontext](https://www.w3.org/TR/trace-context/) propagation format.\n- The`github.com/GoogleCloudPlatform/opentelemetry-operations-go/exporter/trace`import exports traces to Cloud Trace.\n- The`github.com/gorilla/mux`import is the library that the sample application uses for request handling.\n- The`go.opentelemetry.io/contrib/detectors/gcp`import adds attributes to spans, such as`cloud.availability_zone`, which identify where your application is running inside Google Cloud.\n- The`go.opentelemetry.io/otel`,`go.opentelemetry.io/otel/sdk/trace`, and`go.opentelemetry.io/otel/sdk/resource`imports that are used to set up OpenTelemetry.\n### Review the main function\nThe `main` function sets up trace export to Cloud Trace and uses a [mux router](https://www.gorillatoolkit.org/pkg/mux) to handle requests that are made to the `/` URL.\n[  observability/distributed-tracing/main.go ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/observability/distributed-tracing/main.go) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/observability/distributed-tracing/main.go)\n```\nfunc main() {\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // Set up the Cloud Trace exporter.\u00a0 \u00a0 \u00a0 \u00a0 exporter, err := cloudtrace.New()\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"cloudtrace.New: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Identify your application using resource detection.\u00a0 \u00a0 \u00a0 \u00a0 res, err := resource.New(ctx,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Use the GCP resource detector to detect information about the GKE Cluster.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resource.WithDetectors(gcp.NewDetector()),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resource.WithTelemetrySDK(),\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"resource.New: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 tp := trace.NewTracerProvider(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 trace.WithBatcher(exporter),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 trace.WithResource(res),\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 // Set the global TracerProvider which is used by otelhttp to record spans.\u00a0 \u00a0 \u00a0 \u00a0 otel.SetTracerProvider(tp)\u00a0 \u00a0 \u00a0 \u00a0 // Flush any pending spans on shutdown.\u00a0 \u00a0 \u00a0 \u00a0 defer tp.ForceFlush(ctx)\u00a0 \u00a0 \u00a0 \u00a0 // Set the global Propagators which is used by otelhttp to propagate\u00a0 \u00a0 \u00a0 \u00a0 // context using the w3c traceparent and baggage formats.\u00a0 \u00a0 \u00a0 \u00a0 otel.SetTextMapPropagator(autoprop.NewTextMapPropagator())\u00a0 \u00a0 \u00a0 \u00a0 // Handle incoming request.\u00a0 \u00a0 \u00a0 \u00a0 r := mux.NewRouter()\u00a0 \u00a0 \u00a0 \u00a0 r.HandleFunc(\"/\", mainHandler)\u00a0 \u00a0 \u00a0 \u00a0 var handler http.Handler = r\u00a0 \u00a0 \u00a0 \u00a0 // Use otelhttp to create spans and extract context for incoming http\u00a0 \u00a0 \u00a0 \u00a0 // requests.\u00a0 \u00a0 \u00a0 \u00a0 handler = otelhttp.NewHandler(handler, \"server\")\u00a0 \u00a0 \u00a0 \u00a0 log.Fatal(http.ListenAndServe(fmt.Sprintf(\":%v\", os.Getenv(\"PORT\")), handler))}\n```\nNote the following about this code:\n- You configure an OpenTelemetry [TracerProvider](https://opentelemetry.io/docs/specs/otel/trace/api/#tracerprovider) , which detects attributes when it runs on Google Cloud, and which exports traces to Cloud Trace.\n- You use the`otel.SetTracerProvider`and`otel.SetTextMapPropagators`functions to set the global`TracerProvider`and`Propagator`settings. By default, instrumentation libraries such as`otelhttp`use the globally-registered`TracerProvider`to create spans and the`Propagator`to propagate context.\n- You wrap the HTTP server with`otelhttp.NewHandler`to instrument the HTTP server.\n### Review the mainHandler function\n[  observability/distributed-tracing/main.go ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/observability/distributed-tracing/main.go) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/observability/distributed-tracing/main.go)\n```\nfunc mainHandler(w http.ResponseWriter, r *http.Request) {\u00a0 \u00a0 \u00a0 \u00a0 // Use otelhttp to record a span for the outgoing call, and propagate\u00a0 \u00a0 \u00a0 \u00a0 // context to the destination.\u00a0 \u00a0 \u00a0 \u00a0 destination := os.Getenv(\"DESTINATION_URL\")\u00a0 \u00a0 \u00a0 \u00a0 resp, err := otelhttp.Get(r.Context(), destination)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatal(\"could not fetch remote endpoint\")\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer resp.Body.Close()\u00a0 \u00a0 \u00a0 \u00a0 _, err = ioutil.ReadAll(resp.Body)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"could not read response from %v\", destination)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprint(w, strconv.Itoa(resp.StatusCode))}\n```\nTo capture the latency of outbound requests that are made to the destination, you use the `otelhttp` plugin to make an HTTP request. You also use the `r.Context` function to link the incoming request with the outgoing request, as shown in the following listing:\n```\n// Use otelhttp to record a span for the outgoing call, and propagate// context to the destination.resp, err := otelhttp.Get(r.Context(), destination)\n```\n## Deploy the application\nIn this section, you use Cloud Build to build container images for the backend and frontend services. You then deploy them to their GKE clusters.\n### Build the Docker container\n- In Cloud Shell, submit the build from the working directory:```\ncd $WORKDIR\ngcloud builds submit . --tag us-west1-docker.pkg.dev/$PROJECT_ID/distributed-tracing-docker-repo/backend:latest\n```\n- Confirm that the container image was successfully created and is available in Artifact Registry:```\ngcloud artifacts docker images list us-west1-docker.pkg.dev/$PROJECT_ID/distributed-tracing-docker-repo\n```The container image was successfully created when the output is similar to the following, where `` is the ID of your Google Cloud project:```\nNAME\nus-west1-docker.pkg.dev/PROJECT_ID/distributed-tracing-docker-repo/backend\n```\n### Deploy the backend service\n- In Cloud Shell, set the `kubectx` context to the `backend` cluster:```\nkubectx backend\n```\n- Create the YAML file for the `backend` deployment:```\nexport PROJECT_ID=$(gcloud info --format='value(config.project)')\nenvsubst < backend-deployment.yaml | kubectl apply -f \n```\n- Confirm that the pods are running:```\nkubectl get pods\n```The output displays a `Status` value of `Running` :```\nNAME      READY STATUS RESTARTS AGE\nbackend-645859d95b-7mx95 1/1  Running 0   52s\nbackend-645859d95b-qfdnc 1/1  Running 0   52s\nbackend-645859d95b-zsj5m 1/1  Running 0   52s\n```\n- Expose the `backend` deployment using a load balancer:```\nkubectl expose deployment backend --type=LoadBalancer\n```\n- Get the IP address of the `backend` service:```\nkubectl get services backend\n```The output is similar to the following:```\nNAME  TYPE   CLUSTER-IP  EXTERNAL-IP PORT(S)   AGE\nbackend LoadBalancer 10.11.247.58 34.83.88.143 8080:30714/TCP 70s\n```If the value of the `EXTERNAL-IP` field is `<pending>` , repeat the command until the value is an IP address.\n- Capture the IP address from the previous step in a variable:```\nexport BACKEND_IP=$(kubectl get svc backend -ojson | jq -r '.status.loadBalancer.ingress[].ip')\n```\n### Deploy the frontend service\n- In Cloud Shell, set the `kubectx` context to the backend cluster:```\nkubectx frontend\n```\n- Create the YAML file for the `frontend` deployment:```\nexport PROJECT_ID=$(gcloud info --format='value(config.project)')\nenvsubst < frontend-deployment.yaml | kubectl apply -f \n```\n- Confirm that the pods are running:```\nkubectl get pods\n```The output displays a `Status` value of `Running` :```\nNAME      READY STATUS RESTARTS AGE\nfrontend-747b445499-v7x2w 1/1  Running 0   57s\nfrontend-747b445499-vwtmg 1/1  Running 0   57s\nfrontend-747b445499-w47pf 1/1  Running 0   57s\n```\n- Expose the `frontend` deployment using a load balancer:```\nkubectl expose deployment frontend --type=LoadBalancer\n```\n- Get the IP address of the `frontend` service:```\nkubectl get services frontend\n```The output is similar to the following:```\nNAME  TYPE   CLUSTER-IP  EXTERNAL-IP  PORT(S)   AGE\nfrontend LoadBalancer 10.27.241.93 34.83.111.232 8081:31382/TCP 70s\n```If the value of the `EXTERNAL-IP` field is `<pending>` , repeat the command until the value is an IP address.\n- Capture the IP address from the previous step in a variable:```\nexport FRONTEND_IP=$(kubectl get svc frontend -ojson | jq -r '.status.loadBalancer.ingress[].ip')\n```## Load the application and review traces\nIn this section, you use the Apache Bench utility to create requests for your application. You then review the resulting traces in Cloud Trace.\n- In Cloud Shell, use Apache Bench to generate 1,000 requests using 3 concurrent threads:```\nab -c 3 -n 1000 http://${FRONTEND_IP}:8081/\n```\n- In the Google Cloud console, go to the **Trace List** page. [Go to Trace List](https://console.cloud.google.com/traces/traces) \n- To review the timeline, click one of the URIs that's labeled as `server` . This trace contains four spans that have the following names:- The first`server`span captures the end-to-end latency of handling the HTTP request in the frontend server.\n- The first`HTTP GET`span captures the latency of the GET call that's made by the frontend's client to the backend.\n- The second`server`span captures the end-to-end latency of handling the HTTP request in the backend server.\n- The second`HTTP GET`span captures the latency of the GET call that's made by the backend's client to google.com.\n ## Clean up\nThe easiest way to eliminate billing is to delete the Google Cloud project that you created for the deployment. Alternatively, you can delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.### Delete individual resources\nTo delete individual resources instead of deleting the whole project, run the following commands in Cloud Shell:\n```\ngcloud container clusters delete frontend-cluster --zone=us-west1-a\ngcloud container clusters delete backend-cluster --zone=us-west1-a\ngcloud artifacts repositories delete distributed-tracing-docker-repo --location us-west1\n```\n## What's next\n- Learn about [OpenTelemetry](https://opentelemetry.io/) .\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}