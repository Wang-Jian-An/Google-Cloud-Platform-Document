{"title": "Cloud Architecture Center - Deploy a kernel space NFS caching proxy in Compute Engine", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Deploy a kernel space NFS caching proxy in Compute Engine\nLast reviewed 2023-10-03 UTC\nThis tutorial shows you how to deploy, configure, and test a Linux-based, kernel-space Network File System (NFS) caching proxy in Compute Engine. The architecture that's described in this tutorial is designed for a scenario in which read-only data is synchronized at a byte level from an NFS origin file server (such as an on-premises NFS file server) to Google Cloud, or synchronized on demand from one primary source of truth to multiple read-only replicas.\nThis tutorial assumes that you're familiar with the following:- Building custom versions of the Linux operating system.\n- Installing and configuring software with startup scripts in Compute Engine.\n- Configuring and managing an NFS file system.\nThis architecture doesn't support file locking. The architecture is best suited for pipelines that use unique filenames to track file versions.", "content": "## ArchitectureThe architecture in this tutorial has a kernel-space NFS Daemon (KNFSD) that acts as an NFS proxy and cache. This setup gives your cloud-based compute nodes access to local, fast storage by migrating data when an NFS client requests it. NFS client nodes write data directly back to your NFS origin file server using write-through caching. The following diagram shows this architecture:In this tutorial, you deploy and test the KNFSD proxy system. You create and configure a single NFS server, a single KNFSD proxy, and a single NFS client all in Google Cloud.\nThe KNFSD proxy system works by mounting a volume from the NFS server and re-exporting that volume. The NFS client mounts the re-exported volume from the proxy. When an NFS client requests data, the KNFSD proxy checks its various cache tables to determine whether the data resides locally. If the data is already in the cache, the KNFSD proxy serves it immediately. If the data requested isn't in the cache, the proxy migrates the data, updates its cache tables, and then serves the data. The KNFSD proxy caches both file data and metadata at a byte level, so only the bytes that are used are transferred as they are requested.\nThe KNFSD proxy has two layers of cache: L1 and L2. L1 is the standard block cache of the operating system that resides in RAM. When the volume of data exceeds available RAM, L2 cache is implemented by using [FS-Cache](https://www.kernel.org/doc/Documentation/filesystems/caching/fscache.txt) , a Linux kernel module that caches data locally on disk. In this deployment, you use [Local SSD](/local-ssd) as your L2 cache, although you can configure the system in several ways.\nTo implement the architecture in this tutorial, you use standard NFS tools, which are compatible with NFS versions 2, 3, and 4.## KNFSD deployment in a hybrid architectureIn a hybrid architecture, NFS clients that are running in Google Cloud request data when it's needed. These requests are made to the KNFSD proxy, which serves data from its local cache if present. If the data isn't in the cache, the proxy manages communication back to the on-premises servers. The system can mount single or multiple NFS origin servers. The proxy manages all communication and data migration necessary through a VPN or Dedicated Interconnect back to the on-premises NFS origin servers. The following diagram shows this KNFSD deployment in a hybrid architecture:Hybrid connectivity is beyond the scope of this tutorial. For information about advanced topics such as considerations for deploying to a hybrid architecture, scaling for high performance, and using metrics and dashboards for troubleshooting and tuning, see [Advanced workflow topics](#advanced_workflow_topics) .## Objectives\n- Deploy and test a KNFSD proxy system.\n- Create and configure the following components in Google Cloud:- A custom disk image\n- A KNFSD proxy\n- An NFS server\n- An NFS client\n- Mount an NFS proxy on an NFS client.\n- Copy a file from the NFS server through the NFS proxy to the NFS client.\n## Costs\nIn this document, you use the following billable components of Google Cloud:- [Compute Engine](/compute/all-pricing) \n- [Virtual Private Cloud (VPC)](/vpc/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nFor your usage, consider [Networking Egress costs](/vpc/network-pricing#internet_egress) for data written from Google Cloud back to on-premises storage, and costs for [hybrid connectivity](/hybrid-connectivity) .## Before you beginFor this reference guide, you need a Google Cloud project. You can create a new one, or select a project you already created:- In the Google Cloud console, on the project selector page,   select or [create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- Enable the Compute Engine API. [Enable the API](https://console.cloud.google.com/flows/enableapi?apiid=compute.googleapis.com) \n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Authenticate your login in the Cloud Shell terminal:```\ngcloud auth application-default login\n```The command line guides you through completing the authorization steps.\n- Set environment variables:```\nexport GOOGLE_CLOUD_PROJECT=PROJECT_NAMEgcloud config set project $GOOGLE_CLOUD_PROJECT\n```Replace `` with the name of the project that you created or selected earlier.\nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Download the tutorial configuration files\n- In Cloud Shell, clone the GitHub repository:```\ncd ~/git clone https://github.com/GoogleCloudPlatform/knfsd-cache-utils.git\n```\n- Set the Git tag to a known good version (in this case `v0.9.0` ):```\ncd ~/knfsd-cache-utilsgit checkout tags/v0.9.0\n```\n- Navigate to the `image` directory in your code repository:```\n\u00a0cd ~/knfsd-cache-utils/image\n```\n## Configure your networkFor simplicity of deployment, this tutorial uses the default [VPC](/vpc) network. To let you use SSH to connect to various resources for configuration and monitoring purposes, this tutorial also deploys external IP addresses.\n [Best practices and reference architectures for VPC design](/architecture/best-practices-vpc-design) are beyond the scope of this tutorial. However, when you integrate these resources into a hybrid environment, we recommend that you follow best practices including the following:- Create Compute Engine resources without external IP addresses.\n- [Build internet connectivity for private VMs](/architecture/building-internet-connectivity-for-private-vms) to configure software.\n- Use [Identity-Aware Proxy (IAP) for TCP forwarding](/iap/docs/using-tcp-forwarding) to connect to resources.\nTo configure your network, do the following:- In Cloud Shell, set the following variables:```\nexport BUILD_MACHINE_NETWORK=defaultexport BUILD_MACHINE_SUBNET=default\n```\n## Create the NFS proxy build machineIn this section, you create and then sign in to a VM that acts as your NFS proxy build machine. You then run a provided installation script to update Kernel versions and install all of the necessary software for the KNFSD proxy system. The software installation script can take a few minutes to run, but you only have to run it once.- In Cloud Shell, set the following variables:```\nexport BUILD_MACHINE_NAME=knfsd-build-machineexport BUILD_MACHINE_ZONE=us-central1-aexport IMAGE_FAMILY=knfsd-proxyexport IMAGE_NAME=knfsd-proxy-image\n```\n- Launch the VM instance:```\ngcloud compute instances create $BUILD_MACHINE_NAME \\\u00a0 --zone=$BUILD_MACHINE_ZONE \\\u00a0 --machine-type=n1-standard-16 \\\u00a0 --project=$GOOGLE_CLOUD_PROJECT \\\u00a0 --image=ubuntu-2004-focal-v20220615 \\\u00a0 --image-project=ubuntu-os-cloud \\\u00a0 --network=$BUILD_MACHINE_NETWORK \\\u00a0 --subnet=$BUILD_MACHINE_SUBNET \\\u00a0 --boot-disk-size=20GB \\\u00a0 --boot-disk-type=pd-ssd \\\u00a0 --metadata=serial-port-enable=TRUE\n```You might receive a warning message that indicates a disk size discrepancy. You can ignore this message.\n- Create a tar file of the required software to install, and then copy it to your build machine:```\ntar -czf resources.tgz -C resources .gcloud compute scp resources.tgz build@$BUILD_MACHINE_NAME: \\\u00a0 --zone=$BUILD_MACHINE_ZONE \\\u00a0 --project=$GOOGLE_CLOUD_PROJECT\n``` **Note:** If you get errors when connecting, the instance is still booting. These errors are generic network errors or errors exchanging keys. If you get errors, wait a few moments and then try again.\n- After the VM starts, open an SSH tunnel to it:```\ngcloud compute ssh build@$BUILD_MACHINE_NAME \\\u00a0 --zone=$BUILD_MACHINE_ZONE \\\u00a0 --project=$GOOGLE_CLOUD_PROJECT\n```\n- After the SSH tunnel is established and your command line is targeting the `knfsd-build-machine` instance, run the installation script:```\ntar -zxf resources.tgzsudo bash scripts/1_build_image.sh\n```The script clones the Ubuntu Kernel Code repository, updates the kernel version, and installs additional software. Because there is a repository clone involved, the script can take a long time to complete.\n- After the installation script finishes and displays a `SUCCESS` prompt, reboot the build machine:```\nsudo reboot\n```When your build machine reboots, the following messages are displayed:```\nWARNING: Failed to send all data from [stdin]\nERROR: (gcloud.compute.ssh) [/usr/bin/ssh] exited with return code [255]\n```These errors happen while Cloud Shell reverts from your NFS proxy build machine to your host machine. You can ignore these errors.\n- After the VM reboots, re-open an SSH tunnel to it:```\ngcloud compute ssh $BUILD_MACHINE_NAME \\\u00a0 --zone=$BUILD_MACHINE_ZONE \\\u00a0 --project=$GOOGLE_CLOUD_PROJECT\n```\n- After the SSH tunnel is established, and your command line is targeting the `nfs-proxy-build` instance, switch to `Root` and check your OS version:```\nuname -r\n```The output is similar to the following, indicating that the software updates were successful:```\nlinux <$BUILD_MACHINE_NAME> 5.13.*-gcp ...\n```If the output isn't similar to the preceding example, complete this process to [create the NFS proxy build machine](#create_the_nfs_proxy_build_machine) again.\n- Clean up the local disk and shut down the build machine:```\nsudo bash /home/build/scripts/9_finalize.sh\n```The following warnings will be displayed:```\nuserdel: user build is currently used by process 1431\nuserdel: build mail spool (/var/mail/build) not found\n```These warnings happen while Cloud Shell reverts from your NFS proxy build machine to your host machine. You can ignore these errors.\n## Create the custom disk imageIn this section, you create a [custom image](/compute/docs/images) from the instance. The custom image is stored in a multi-regional Cloud Storage bucket that is located in the United States.- In Cloud Shell, set the following variables:```\nexport IMAGE_NAME=knfsd-imageexport IMAGE_DESCRIPTION=\"first knfsd image from tutorial\"export IMAGE_LOCATION=us\n```\n- Create the disk image:```\ngcloud compute images create $IMAGE_NAME \\\u00a0 --project=$GOOGLE_CLOUD_PROJECT \\\u00a0 --description=\"$IMAGE_DESCRIPTION\" \\\u00a0 --source-disk=$BUILD_MACHINE_NAME \\\u00a0 --source-disk-zone=$BUILD_MACHINE_ZONE \\\u00a0 --storage-location=$IMAGE_LOCATION\n```\n- After the disk image is created, delete the instance:```\ngcloud compute instances delete $BUILD_MACHINE_NAME \\\u00a0 --zone=$BUILD_MACHINE_ZONE\n```\n- When you're prompted to continue, enter `Y` .When you delete the `$BUILD_MACHINE_NAME` instance, you see a prompt that indicates that attached disks on the VM will be deleted. Because you just saved a custom image, you no longer need this temporary disk and it's safe to delete it.\n## Create the NFS origin serverAs mentioned earlier, this architecture is designed to connect cloud-based resources to an on-premises file server. To simplify the process in this tutorial, you create a stand-in resource that runs in your Google Cloud project to simulate this connection. You name the stand-in resource `nfs-server` . The software installation and setup is contained in a startup script. For more information, examine the script, `~/knfsd-cache-utils/tutorial/nfs-server/1_build_nfs-server.sh` .- In Cloud Shell, go to the downloaded `nfs-server` scripts directory:```\ncd ~/knfsd-cache-utils/tutorial\n```\n- Create your stand-in NFS server:```\ngcloud compute \\\u00a0 --project=$GOOGLE_CLOUD_PROJECT instances create nfs-server \\\u00a0 --zone=$BUILD_MACHINE_ZONE \\\u00a0 --machine-type=n1-highcpu-2 \\\u00a0 --maintenance-policy=MIGRATE \\\u00a0 --image-family=ubuntu-2004-lts \\\u00a0 --image-project=ubuntu-os-cloud \\\u00a0 --boot-disk-size=100GB \\\u00a0 --boot-disk-type=pd-standard \\\u00a0 --boot-disk-device-name=nfs-server \\\u00a0 --metadata-from-file startup-script=nfs-server-startup.sh\n```This script can take a few minutes to complete. You might see a warning message that indicates that your disk size is under 200 GB. You can ignore this warning.\n## Create the NFS proxyIn this section, you create the NFS proxy. When the proxy starts, it configures local storage, prepares mount options for the NFS server, and exports the cached results. A provided startup script orchestrates much of this workflow.- In Cloud Shell, set the following variable:```\nexport PROXY_NAME=nfs-proxy\n```\n- Create the `nfs-proxy` VM:```\ngcloud compute instances create $PROXY_NAME \\\u00a0 --machine-type=n1-highmem-16 \\\u00a0 --project=$GOOGLE_CLOUD_PROJECT \\\u00a0 --maintenance-policy=MIGRATE \\\u00a0 --zone=$BUILD_MACHINE_ZONE \\\u00a0 --min-cpu-platform=\"Intel Skylake\" \\\u00a0 --image=$IMAGE_NAME \\\u00a0 --image-project=$GOOGLE_CLOUD_PROJECT \\\u00a0 --boot-disk-size=20GB \\\u00a0 --boot-disk-type=pd-standard \\\u00a0 --boot-disk-device-name=$PROXY_NAME \\\u00a0 --local-ssd=interface=NVME \\\u00a0 --local-ssd=interface=NVME \\\u00a0 --local-ssd=interface=NVME \\\u00a0 --local-ssd=interface=NVME \\\u00a0 --metadata-from-file startup-script=proxy-startup.sh\n```You might see a warning message that your disk size is under 200 GB. You can ignore this warning.The startup script configures NFS mount commands and it lets you tune the system. The settings for NFS version, sync or async, `nocto` , and `actimeo` are some of the variables that you might want to optimize through the startup script. For more information about these settings, see [optimizing your NFS file system](https://www.admin-magazine.com/HPC/Articles/Useful-NFS-Options-for-Tuning-and-Management#:%7E:text=Most%20people%20use%20the%20synchronous%20option%20on%20the%20NFS%20server.&text=Asynchronous%20mode%20allows%20the%20server,responding%20to%20the%20NFS%20client.) .The command in this step defines the `--metadata-from-file` flag, which injects the startup script into your image template. In this tutorial, you use a simple `proxy-startup.sh` script. The script includes some pre-set variables and it doesn't include many options that you might want to use if you integrate into your pipeline. For more advanced use cases, see the [knfsd-cache-utils GitHub repository](https://github.com/GoogleCloudPlatform/knfsd-cache-utils) .\n## Create the NFS clientIn this step, you create a single NFS client (named `nfs-client` ) to stand in for what would likely be a larger [Managed Instance Group (MIG)](/compute/docs/instance-groups) at scale.- In Cloud Shell, create your NFS client:```\ngcloud compute \\\u00a0 --project=$GOOGLE_CLOUD_PROJECT instances create nfs-client \\\u00a0 --zone=$BUILD_MACHINE_ZONE \\\u00a0 --machine-type=n1-highcpu-8 \\\u00a0 --network-tier=PREMIUM \\\u00a0 --maintenance-policy=MIGRATE \\\u00a0 --image-family=ubuntu-2004-lts \\\u00a0 --image-project=ubuntu-os-cloud \\\u00a0 --boot-disk-size=10GB \\\u00a0 --boot-disk-type=pd-standard \\\u00a0 --boot-disk-device-name=nfs-client\n```You might see a warning message that your disk size is under 200 GB. You can ignore this warning.\n## Mount the NFS proxy on the NFS clientIn this step, you open a separate SSH session on your NFS client and then mount the NFS proxy. You use this same shell to test the system in the next section.- In the Google Cloud console, go to the **VM instances** page. [Go to VM instances](https://console.cloud.google.com/compute/instances) \n- To connect to `nfs-client` , in the **Connect** column, click **SSH** .\n- In the `nfs-client` SSH window, install the necessary NFS tools on the `nfs-client` :```\nsudo apt-get install nfs-common -y\n```\n- Create a mount point and mount the NFS proxy:```\nsudo mkdir /datasudo mount -t nfs -o vers=3 nfs-proxy:/data /data\n```\n## Test the systemAll of your resources are now created. In this section, you run a test by copying a file from the NFS server through the NFS proxy to the NFS client. The first time that you run this test, the data comes from the originating server. It can take over one minute.\nThe second time that you run this test, the data is served from a cache that's stored in the Local SSDs of the NFS proxy. In this transfer, it takes much less time to copy data, which validates that caching is accelerating the data transfer.- In the `nfs-client` SSH window that you opened in the previous section, copy the `test` file and view the corresponding output:```\ntime dd if=/data/test.data of=/dev/null iflag=direct bs=1M status=progress\n```The output is similar to the following, which contains a line that shows the file size, time for transfer, and transfer speeds:```\n10737418240 bytes (11 GB, 10 GiB) copied, 88.5224 s, 121 MB/s\nreal 1m28.533s\n```In this transfer, the file is served from the persistent disk of the NFS server, so it's limited by the speed of the NFS server's disk.\n- Run the same command a second time:```\ntime dd if=/data/test.data of=/dev/null iflag=direct bs=1M status=progress\n```The output is similar to the following, which contains a line that shows the file size, time for transfer, and transfer speeds:```\n10737418240 bytes (11 GB, 10 GiB) copied, 9.41952 s, 948 MB/s\nreal 0m9.423s\n```In this transfer, the file is served from the cache in the NFS proxy, so it completes faster.\nYou have now completed deployment and testing of the KNFSD caching proxy.## Advanced workflow topicsThis section includes information about deploying to a hybrid architecture, scaling for high performance, and using metrics and dashboards for troubleshooting and tuning.\n### Performance characteristics and resource sizingAs previously noted, this tutorial uses a single KNFSD proxy. Therefore, scaling the system involves modifying your individual proxy resources to optimize for CPU, RAM, networking, storage capacity, or performance. In this tutorial, you deployed KNFSD on a single Compute Engine VM with the following options:- 16 vCPUs, 104 GB of RAM (`n1-highmem-16`).- With 16 vCPUs and an architecture of Sandy Bridge or newer, you enable a maximum networking speed of 32 Gbps.\n- 10 GB Persistent Disk as a boot disk.\n- 4 local SSD disks. This configuration provides a high speed, 1.5 TB file system.\nAlthough it's beyond the scope of this tutorial, you can scale this architecture by creating multiple KNFSD proxies in a MIG and by using a [TCP load balancer](/load-balancing/docs/tcp) to manage connections between the NFS clients and NFS proxies. For more information, see the [knfsd-cache-utils GitHub repository](https://github.com/GoogleCloudPlatform/knfsd-cache-utils) which contains Terraform scripts, example code for deployment, and various FAQs covering scaling workloads.\n### Considerations for a hybrid deploymentIn many deployments, the bandwidth of your connection from on-premises to the cloud is a major factor to consider when configuring the system. Hybrid connectivity is beyond the scope of this tutorial. For an overview of available options, see the [hybrid connectivity documentation](/hybrid-connectivity) . For guidance on best practices and design patterns, see the series [Build hybrid and multicloud architectures using Google Cloud](/architecture/hybrid-multicloud-patterns) .\n### Exploring metricsDashboards can be useful in providing feedback on metrics for use in performance tuning and overall troubleshooting. Exploring metrics is beyond the scope of this tutorial, however, a metrics dashboard is made available when you deploy the multi-node system that's defined in the [knfsd-cache-utils GitHub repository](https://github.com/GoogleCloudPlatform/knfsd-cache-utils/tree/main/deployment#metrics) .## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the projectThe easiest way to eliminate billing is to delete the project you created for the tutorial.- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Learn more about [image management best practices](/compute/docs/images/create-delete-deprecate-private-images) .\n- Learn more about the [work being done with the Linux developer community on NFS re-exporting](https://www.dneg.com/blog/cloud-rendering-linux-kernel/) .\n- Learn more about the overall management and configuration of [NFS on Ubuntu](https://help.ubuntu.com/community/SettingUpNFSHowTo) .\n- Learn more about scaling a KNFSD deployment using MIGs and a Load Balancer in the [knfsd-cache-utils GitHub repository](https://github.com/GoogleCloudPlatform/knfsd-cache-utils) .\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}