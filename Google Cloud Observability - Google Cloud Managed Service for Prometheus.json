{"title": "Google Cloud Observability - Google Cloud Managed Service for Prometheus", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview", "abstract": "# Google Cloud Observability - Google Cloud Managed Service for Prometheus\nGoogle Cloud Managed Service for Prometheus is Google Cloud's fully managed, multi-cloud, cross-project solution for [Prometheus](https://prometheus.io) metrics. It lets you globally monitor and alert on your workloads, using Prometheus, without having to manually manage and operate Prometheus at scale.\nManaged Service for Prometheus collects metrics from Prometheus exporters and lets you query the data globally using PromQL, meaning that you can keep using any existing [Grafana](https://grafana.com/grafana) dashboards, PromQL-based alerts, and workflows. It is hybrid- and multi-cloud compatible, can monitor Kubernetes, VMs, and serverless workloads on Cloud Run, retains data for 24 months, and maintains portability by staying compatible with upstream Prometheus. You can also supplement your Prometheus monitoring by querying [over 6,500 freemetrics](/monitoring/api/metrics_gcp) in Cloud Monitoring, including [free GKEsystem metrics](/monitoring/api/metrics_kubernetes) , using PromQL.\nThis document gives an overview of the managed service, and further documents describe how to set up and run the service. To receive regular updates about new features and releases, submit the optional [sign-up form](https://docs.google.com/forms/d/e/1FAIpQLScXB4ezLOF_0ifXrz9THQz7w9B4fS0yPkX4Ul1w-gkbPRf2EA/viewform) .\nHear how The Home Depot uses Managed Service for Prometheus to get unified observability across 2,200 stores running on-prem Kubernetes clusters:", "content": "## System overview\nGoogle Cloud Managed Service for Prometheus gives you the familiarity of Prometheus backed by the global, multi-cloud, and cross-project infrastructure of Cloud Monitoring.\nManaged Service for Prometheus is built on top of Monarch, the same [globally scalable data store](https://research.google/pubs/pub50652/) used for Google's own monitoring. Because Managed Service for Prometheus uses the same backend and APIs as [Cloud Monitoring](/monitoring) , both Cloud Monitoring metrics and metrics ingested by Managed Service for Prometheus are queryable by using [PromQL in Cloud Monitoring](/stackdriver/docs/managed-prometheus/query-cm) , [Grafana](/stackdriver/docs/managed-prometheus/query) , or [any other tool thatcan read the Prometheus API](/stackdriver/docs/managed-prometheus/query-api-ui) .\nIn a standard Prometheus deployment, data collection, query evaluation, rule and alert evaluation, and data storage are all handled within a single Prometheus server. Managed Service for Prometheus splits responsibilities for these functions into multiple components:\n- [Data collection](#gmp-data-collection) is handled by managed collectors, self-deployed collectors, the OpenTelemetry Collector, or [the Ops Agent](/monitoring/agent/ops-agent/prometheus) , which scrape local exporters and forward the collected data to Monarch. These collectors can be used for Kubernetes, serverless, and traditional VM workloads and can run everywhere, including other clouds and on-prem deployments.\n- [Query evaluation](#gmp-query-evaluation) is handled by Monarch, which executes queries and unions results across all Google Cloud regions and across up to 1,000 Google Cloud projects.\n- [Rule and alert evaluation](#gmp-rule-evaluation) is handled either by writing [PromQL alerts in Cloud Monitoring](/monitoring/promql/promql-in-alerting) which fully execute in the cloud, or by using locally run and locally configured rule evaluator components which execute rules and alerts against the global Monarch data store and forward any fired alerts to [Prometheus AlertManager](https://prometheus.io/docs/alerting/latest/alertmanager/) .\n- [Data storage](#gmp-data-storage) is handled by Monarch, which stores all Prometheus data for 24 months at no additional cost.\nGrafana connects to the global Monarch data store instead of connecting to individual Prometheus servers. If you have Managed Service for Prometheus collectors configured in all your deployments, then this single Grafana instance gives you a unified view of all your metrics across all your clouds.\n## Data collection\nYou can use Managed Service for Prometheus in one of four modes: with [managed data collection](/stackdriver/docs/managed-prometheus/setup-managed) , with [self-deployed datacollection](/stackdriver/docs/managed-prometheus/setup-unmanaged) , with [the OpenTelemetry Collector](/stackdriver/docs/managed-prometheus/setup-otel) , or with [the Ops Agent](/monitoring/agent/ops-agent/prometheus) .\nManaged Service for Prometheus offers an operator for managed data collection in Kubernetes environments. We recommend that you use managed collection; using it eliminates the complexity of deploying, scaling, sharding, configuring, and maintaining Prometheus servers. Managed collection is supported for both GKE and non-GKE Kubernetes environments.\nWith self-deployed data collection, you manage your Prometheus installation as you always have. The only difference from upstream Prometheus is that you run the Managed Service for Prometheus drop-in replacement binary instead of the upstream Prometheus binary.\nThe OpenTelemetry Collector can be used to scrape Prometheus exporters and send data to Managed Service for Prometheus. OpenTelemetry supports a single-agent strategy for all signals, where one collector can be used for metrics (including Prometheus metrics), logs, and traces in any environment.\nYou can configure the Ops Agent on any Compute Engine instance to scrape and send Prometheus metrics to the global data store. Using an agent greatly simplifies VM discovery and eliminates the need to install, deploy, or configure Prometheus in VM environments.\nIf you have a have a Cloud Run service that writes [Prometheus metrics](https://prometheus.io/docs/concepts/metric_types/) or [OTLP metrics](https://opentelemetry.io/docs/concepts/signals/metrics/) , then you can use a sidecar and Managed Service for Prometheus to send the metrics to Cloud Monitoring.\n- To collect Prometheus metrics from Cloud Run, use the [Prometheus sidecar](/stackdriver/docs/managed-prometheus/cloudrun-sidecar) .\n- To collect OTLP metrics from Cloud Run, use the [OpenTelemetry sidecar](/run/docs/tutorials/custom-metrics-opentelemetry-sidecar) .\nYou can run managed, self-deployed, and OpenTelemetry collectors in on-prem deployments and on any cloud. Collectors running outside of Google Cloud send data to Monarch for long-term storage and global querying.\nWhen choosing between collection options, consider the following:\n- Managed collection:- Google's recommended approach for all Kubernetes environments.\n- Deployed by using the GKE UI, the gcloud CLI, the`kubectl`CLI, or Terraform.\n- Operation of Prometheus\u2014generating scrape configurations, scaling ingestion, scoping rules to the right data, and so forth\u2014is fully handled by the Kubernetes operator.\n- Scraping and rules are configured by using lightweight custom resources (CRs).\n- Good for those who want a more hands-off, fully managed experience.\n- Intuitive migration from [prometheus-operator](https://github.com/prometheus-operator) configs.\n- Supports most current Prometheus use cases.\n- Full assistance from Google Cloud technical support.\n- Self-deployed collection:- A drop-in replacement for the upstream Prometheus binary.\n- You can use your preferred deployment mechanism, like [prometheus-operator](https://github.com/prometheus-operator) or manual deployment.\n- Scraping is configured by using your preferred methods, like annotations or prometheus-operator.\n- Scaling and functional sharding is done manually.\n- Good for quick integration into more complex existing setups. You can reuse your existing configs and run upstream Prometheus and Managed Service for Prometheus side by side.\n- Rules and alerts typically run within individual Prometheus servers, which might be preferable for edge deployments as local rule evaluation does not incur any network traffic.\n- Might support long-tail use cases that aren't yet supported by managed collection, such as [local aggregations](/stackdriver/docs/managed-prometheus/cost-controls#local-aggregation) to reduce cardinality.\n- Limited assistance from Google Cloud technical support.\n- The OpenTelemetry Collector:- A single collector that can collect metrics (including Prometheus metrics) from any environment and send them to any compatible backend. Can also be used to collect logs and traces and send them to any compatible backend, including Cloud Logging and Cloud Trace.\n- Deployed in any compute or Kubernetes environment either manually or by using Terraform. Can be used to send metrics from stateless environments such as Cloud Run.\n- Scraping is configured using Prometheus-like configs in the collector's Prometheus receiver.\n- Supports push-based metric collection patterns.\n- Metadata is injected from any cloud using resource detector processors.\n- Rules and alerts can be executed using a Cloud Monitoring alerting policy or the stand-alone rule evaluator.\n- Best supports cross-signal workflows and features such as exemplars.\n- Limited assistance from Google Cloud technical support.\n- The Ops Agent:- The easiest way to collect and send Prometheus metric data originating from Compute Engine environments, including both Linux and Windows distros.\n- Deployed by using the gcloud CLI, the Compute Engine UI, or Terraform.\n- Scraping is configured using Prometheus-like configs in the Agent's Prometheus receiver, powered by OpenTelemetry.\n- Rules and alerts can be executed using Cloud Monitoring or the stand-alone rule evaluator.\n- Comes bundled with optional Logging agents and [processmetrics](/monitoring/api/metrics_opsagent#agent-processes) .\n- Full assistance from Google Cloud technical support. To get started, see [Get started with managed collection](/stackdriver/docs/managed-prometheus/setup-managed) , [Getstarted with self-deployed collection](/stackdriver/docs/managed-prometheus/setup-unmanaged) , [Get started with theOpenTelemetry Collector](/stackdriver/docs/managed-prometheus/setup-otel) , or [Get started with theOps Agent](/monitoring/agent/ops-agent/prometheus) .If you use the managed service outside of Google Kubernetes Engine or Google Cloud, some additional configuration might be necessary; see [Run managed collection outsideof Google Cloud](/stackdriver/docs/managed-prometheus/setup-managed#gmp-outside-gcp) , [Run self-deployed collection outside ofGoogle Cloud](/stackdriver/docs/managed-prometheus/setup-unmanaged#gmp-outside-gcp) , or [Add OpenTelemetryprocessors](/stackdriver/docs/managed-prometheus/setup-otel#add-processors) .\n## Query evaluation\nManaged Service for Prometheus supports any query UI that can call the Prometheus query API, including Grafana and the Cloud Monitoring UI. Existing Grafana dashboards continue to work when switching from local Prometheus to Managed Service for Prometheus, and you can continue using PromQL found in popular open-source repositories and on community forums.\nYou can use PromQL to query [over 6,500 freemetrics](/monitoring/api/metrics_gcp) in Cloud Monitoring, even without sending data to Managed Service for Prometheus. You can also use PromQL to query [freeKubernetes metrics](/monitoring/api/metrics_kubernetes) , [custom metrics](/monitoring/custom-metrics) and [log-based metrics](/logging/docs/logs-based-metrics) .\nFor information on how to configure Grafana to query Managed Service for Prometheus data, see [Query using Grafana](/stackdriver/docs/managed-prometheus/query) .\nFor information on how to query Cloud Monitoring metrics using PromQL, see [PromQL in Cloud Monitoring](/monitoring/promql) .\n## Rule and alert evaluation\nManaged Service for Prometheus provides both a fully cloud-based alerting pipeline and a stand-alone rule evaluator, both of which evaluate rules against all Monarch data accessible in a [metrics scope](/monitoring/settings#concept-scope) . Evaluating rules against a multi-project metrics scope eliminates the need to co-locate all data of interest on a single Prometheus server or within a single Google Cloud project, and it lets you set IAM permissions on groups of projects.\nBecause all rule evaluation options accept the standard Prometheus `rule_files` format, you can easily migrate to Managed Service for Prometheus by copy-pasting existing rules or by copy-pasting rules found in popular open source repositories. For those using self-deployed collectors, you can continue to evaluate recording rules locally in your collectors. The results of recording and alerting rules are stored in Monarch, just like directly collected metric data. You can also migrate your Prometheus alerting rules to PromQL-based alerting policies in Cloud Monitoring.\nFor alert evaluation with Cloud Monitoring, see [PromQL alerts in Cloud Monitoring](/monitoring/promql/promql-in-alerting) .\nFor rule evaluation with managed collection, see [Managed rule evaluation andalerting](/stackdriver/docs/managed-prometheus/rules-managed) .\nFor rule evaluation with self-deployed collection, the OpenTelemetry Collector, and the Ops Agent, see [Self-deployed rule evaluation andalerting](/stackdriver/docs/managed-prometheus/rules-unmanaged) .\nFor information on reducing cardinality using recording rules on self-deployed collectors, see [Cost controls and attribution](/stackdriver/docs/managed-prometheus/cost-controls#local-aggregation) .\n## Data storage\nAll Managed Service for Prometheus data is stored for 24 months at no additional cost.\nManaged Service for Prometheus supports a minimum scrape interval of 5 seconds. Data is stored at full granularity for 1 week, then is downsampled to 1-minute points for the next 5 weeks, then is downsampled to 10-minute points and stored for the remainder of the retention period.\nManaged Service for Prometheus has no limit on the number of active time series or total time series.\nFor more information, see [Quotas and limits within theCloud Monitoring documentation](/monitoring/quotas) .\n## Billing and quotas\nManaged Service for Prometheus is a Google Cloud product, and billing and usage quotas apply.\n### Billing\nBilling for the service is based primarily on the number of metric samples ingested into storage. There is also a nominal charge for read API calls. Managed Service for Prometheus does not charge for storage or retention of metric data.\n- For current pricing, see [Google Cloud Managed Service for Prometheus pricing summary](/stackdriver/pricing#mgd-prometheus-pricing-summary) .\n- To estimate your bill based on your expected number of time series or your expected samples per second, see the Cloud Operations tab within the [Google Cloud Pricing Calculator](/products/calculator) .\n- For tips on how to lower your bill or determine the sources of high costs, see [Cost controls and attribution](/stackdriver/docs/managed-prometheus/cost-controls) .\n- For information about the rationale for the pricing model, see [Pricing for controllability and predictability](/stackdriver/pricing#sample-pricing-rationale) .\n- For pricing examples, see [Pricing example based on samples ingested](/stackdriver/pricing#pricing_examples_samples) .\n### Quotas\nManaged Service for Prometheus shares ingest and read quotas with Cloud Monitoring. The default ingest quota is 500 QPS per project with up to 200 samples in a single call, equivalent to 100,000 samples per second. The default read quota is 100 QPS per [metrics scope](/monitoring/settings#concept-scope) .\nYou can increase these quotas to support your metric and query volumes. For information about managing quotas and requesting quota increases, see [Working with quotas](/docs/quota) .\n### Terms of Service and compliance\nManaged Service for Prometheus is part of Cloud Monitoring and therefore inherits certain agreements and certifications from Cloud Monitoring, including (but not limited to):\n- The [Google Cloud terms of service](/terms/services) \n- The [Operations Service Level Agreement (SLA)](/operations/sla) \n- [US DISA](/security/compliance/disa) and [FedRAMP](/security/compliance/fedramp) compliance levels\n- [VPC-SC (VPC Service Controls)](/vpc-service-controls/docs/supported-products#table_monitoring) support## What's next\n- Get started with [managed collection](/stackdriver/docs/managed-prometheus/setup-managed) .\n- Get started with [self-deployed collection](/stackdriver/docs/managed-prometheus/setup-unmanaged) .\n- Get started with [the OpenTelemetry Collector](/stackdriver/docs/managed-prometheus/setup-otel) .\n- Get started with [the Ops Agent](/monitoring/agent/ops-agent/prometheus) .\n- [Use PromQL in Cloud Monitoring to query Prometheus metrics](/stackdriver/docs/managed-prometheus/query-cm) .\n- [Use Grafana to query Prometheus metrics](/stackdriver/docs/managed-prometheus/query) .\n- [Query Cloud Monitoring metrics](/monitoring/promql) using PromQL.\n- Read up on [best practices and view architecture diagrams](/stackdriver/docs/managed-prometheus/best-practices/ingest-and-query) .", "guide": "Google Cloud Observability"}