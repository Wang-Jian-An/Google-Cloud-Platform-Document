{"title": "Vertex AI Vision - Build an application", "url": "https://cloud.google.com/vision-ai/docs/build-app?hl=zh-cn", "abstract": "# Vertex AI Vision - Build an application\nThis page shows you how to create an app and combine components - such as input streams, models for analysis, and warehouses for storage - for use on Vertex AI Vision's app platform.\nTo build an app you must consider your data sources, the analysis you want to perform, and how you want to store the results. An end-to-end app can have different forms depending on your use case. Consequently, how you build an app will depend on your objective.\nBefore beginning, assess your use case and objective from a Responsible AI perspective and consider what impact your models and applications could have on your end users when things go wrong. Read more on first steps in [assessing your use case](/inclusive-ml#assess-your-use-case) for fairness. Also ensure that your use of Vertex AI Vision is in compliance with Google Cloud's [Terms of Service](/terms) or an offline variant, and the incorporated URL Terms like Google Cloud's [Acceptable Use Policy](/terms/aup) .\nAs you create your application, keep in mind that AI vision technologies carry the potential to reinforce or introduce unfair bias and to impact fundamental human rights. Developing your application should involve ethical reflection based on your company's values, and legal due diligence for your particular use case including high risk use cases. As with all technologies, you must consider all applicable laws and regulations governing your use of AI/ML technologies in the region you are implementing the technology. Customers are responsible for due diligence required to implement use cases that are considered high risk in a particular region.\n**Use cases:** Based on Google's AI Principles and current product design, we strongly recommend using caution and carefully evaluating the potential benefits and risks of using Vertex AI Vision for the following:- Biometrics-based persistent individual tracking and surveillance.\n- Inferring individual characteristics including but not limited to  gender, age, and race.\n- Decision making without a human in the loop for predictions that could  impact human rights, in sensitive domains including but not limited to  employment, access to public services, healthcare, and safety-critical  contexts.", "content": "## Create a new application\nBefore you can add components like a stream or models to an app you must create the app itself. All apps must contain the following to be deployed:\n- A stream input node (data source).\n- Atone other component node, such as a model or warehouse.\n### Create a new custom application\nCreate an app in the Google Cloud console.- Open the **Applications** tab of the Vertex AI Vision dashboard. [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n- Click the add **Create** button.\n- Enter an app name and choose your region. [Supported regions](/vision-ai/docs/app-supported-regions) .\n- Click **Create** .\n### Create an application with a template\nCreate an app in the Google Cloud console.- Open the **Applications** tab of the Vertex AI Vision dashboard. [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n- Click the add **Create** button.\n- Enter an app name and choose your region. [Supported regions](/vision-ai/docs/app-supported-regions) .\n- Click **Create** .\n- In the application builder page, click the **Application template** node.\n- From the side settings panel, click the **Select model** button.\n- Select a model from the dropdown menu.## Add stream input\nAn application contain a video stream node. You are unable to deploy the application if you don't add a Streams node.\nIf your application is not deployed, your stream input update will be automatically applied to the application. If your app is already deployed you must undeploy it and update it for changes to be applied. See the [update an app](#update-app) section for more details.\nAdd a stream to an app in the Google Cloud console.- Open the **Applications** tab of the Vertex AI Vision dashboard. [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n- Click **View app** for your app.\n- From the side list of components, choose **Video streams** from the list of **Connectors** . The stream node is added to the app graph and a side Video streams menu opens.\n- Select **Add video streams** .\n- If you choose to radio_button_checked **Select from existing streams** , select the existing stream and select **Add streams** .If you choose to radio_button_checked **Register new streams** , add a name for the new stream and select **Add streams** .\nTo add a stream to an app, send a POST request by using the [projects.locations.applications.addStreamInput](/vision-ai/docs/reference/rest/v1/projects.locations.applications/addStreamInput) method.Before using any of the request data, make the following replacements:- : Your Google Cloud [project ID or project number](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- : The [region](/about/locations) where you are using Vertex AI Vision. For example:`us-central1`,`europe-west4`. See [available regions](/vision-ai/docs/warehouse-supported-regions) .\n- : The ID of your target application.\n- : The ID of the target stream.\nHTTP method and URL:\n```\nPOST https://visionai.googleapis.com/v1/projects/PROJECT/locations/LOCATION_ID/applications/APPLICATION_ID:addStreamInput\n```\nRequest JSON body:\n```\n{\n \"applicationStreamInputs\": [  {\n  \"streamWithAnnotation\": {\n   \"stream\": \"projects/PROJECT/locations/LOCATION_ID/clusters/application-cluster-0/streams/STREAM_ID\"\n  }\n  },\n  {\n  \"streamWithAnnotation\": {\n   \"stream\": \"projects/PROJECT/locations/LOCATION_ID/clusters/application-cluster-0/streams/STREAM_ID\"\n  }\n  }\n ]\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://visionai.googleapis.com/v1/projects/PROJECT/locations/LOCATION_ID/applications/APPLICATION_ID:addStreamInput\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://visionai.googleapis.com/v1/projects/PROJECT/locations/LOCATION_ID/applications/APPLICATION_ID:addStreamInput\" | Select-Object -Expand Content\n```\nYou should receive a successful status code (2xx) and an empty response.### Add stream input with node annotations\nWhen you create an application input stream you can add annotations to the stream using the Vertex AI Vision API. You can also add annotations to a node in the Google Cloud console.\nThe following sample adds available annotations ( `STREAM_ANNOTATION_TYPE_ACTIVE_ZONE` and `STREAM_ANNOTATION_TYPE_CROSSING_LINE` ) to an **occupancy count model** .- Open the **Applications** tab of the Vertex AI Vision dashboard. [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n- Select **View app** next to the name of your application from the list.\n- On the application builder page select the model with stream input you want to add annotations to.\n- In the side model setting panel, expand the \"Advanced setting\" section. After expanding the section, click the **Create active zones/lines** button.\n- In the stream gallery view, select an input stream to draw active zones or lines.\n- In the editor view, choose **add multi-point lines** or **add simple polygon** to add annotations. Use the side panel to rename the zone or line name, delete existing zones/lines, or switch line direction. **Warning:** For each stream draw either a crossing-line an active zone. If both lines and zones are annotated Vertex AI Vision API only registers line counts and ignores zones.\nThe following code adds stream annotations.\nBefore using any of the request data, make the following replacements:- : Your Google Cloud [project ID or project number](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- : The [region](/about/locations) where you are using Vertex AI Vision. For example:`us-central1`,`europe-west4`. See [available regions](/vision-ai/docs/warehouse-supported-regions) .\n- : The ID of your target application.\n- : The ID of the target stream.\n- : The name of the target node from the app graph. For example,`builtin-occupancy-count`.\n- : The ID of your target annotation.\n- : The user-specified display name of your target annotation.\n- : One of the available enum values. This type must match the [annotation_payload object](/vision-ai/docs/reference/rest/v1/StreamWithAnnotation#streamannotation) , either`activeZone`or`crossingLine`. Available values are:- `STREAM_ANNOTATION_TYPE_UNSPECIFIED`\n- `STREAM_ANNOTATION_TYPE_ACTIVE_ZONE`\n- `STREAM_ANNOTATION_TYPE_CROSSING_LINE`\n- `normalizedVertices`: Each vertex is specified by x, y coordinate values. Coordinates are normalized float values [0,1] relative to the original image ; 0.0 is X_MIN or Y_MIN, 1.0 is X_MAX or Y_MAX.\nHTTP method and URL:\n```\nPOST https://visionai.googleapis.com/v1/projects/PROJECT/locations/LOCATION_ID/applications/APPLICATION_ID:addStreamInput\n```\nRequest JSON body:\n```\n{\n \"applicationStreamInputs\": [ {\n  \"streamWithAnnotation\": {\n  \"stream\": \"projects/PROJECT/locations/LOCATION_ID/clusters/application-cluster-0/streams/STREAM_ID\",\n  \"nodeAnnotations\": [   {\n   \"node\": \"NODE_NAME\",\n   \"annotations\": [    {\n    \"id\": \"ANNOTATION_ID\",\n    \"displayName\": \"ANNOTATION_DISPLAYNAME\",\n    \"sourceStream\": \"projects/PROJECT/locations/LOCATION_ID/clusters/application-cluster-0/streams/STREAM_ID\",\n    \"type\": ANNOTATION_TYPE,\n    \"activeZone\": {\n     \"normalizedVertices\": {\n     \"x\": 0.07434944,\n     \"y\": 0.18061674\n     },\n     \"normalizedVertices\": {\n     \"x\": 0.64684016,\n     \"y\": 0.16079295\n     },\n     \"normalizedVertices\": {\n     \"x\": 0.6047088,\n     \"y\": 0.92070484\n     },\n     \"normalizedVertices\": {\n     \"x\": 0.1251549,\n     \"y\": 0.76651984\n     }\n    }\n    }\n   ]\n   }\n  ]\n  }\n }\n ]\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://visionai.googleapis.com/v1/projects/PROJECT/locations/LOCATION_ID/applications/APPLICATION_ID:addStreamInput\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://visionai.googleapis.com/v1/projects/PROJECT/locations/LOCATION_ID/applications/APPLICATION_ID:addStreamInput\" | Select-Object -Expand Content\n```\nYou should receive a successful status code (2xx) and an empty response.\n## Remove stream input\nTo remove a stream from an app, send a POST request by using the [projects.locations.applications.removeStreamInput](/vision-ai/docs/reference/rest/v1/projects.locations.applications/removeStreamInput) method.\nBefore using any of the request data, make the following replacements:- : Your Google Cloud [project ID or project number](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- : The [region](/about/locations) where you are using Vertex AI Vision. For example:`us-central1`,`europe-west4`. See [available regions](/vision-ai/docs/warehouse-supported-regions) .\n- : The ID of your target application.\n- : The ID of the target stream.\nHTTP method and URL:\n```\nPOST https://visionai.googleapis.com/v1/projects/PROJECT/locations/LOCATION_ID/applications/APPLICATION_ID:removeStreamInput\n```\nRequest JSON body:\n```\n{\n \"targetStreamInputs\": [  {\n  \"stream\": \"projects/PROJECT/locations/LOCATION_ID/clusters/application-cluster-0/streams/STREAM_ID\"\n  },\n  {\n  \"stream\": \"projects/PROJECT/locations/LOCATION_ID/clusters/application-cluster-0/streams/STREAM_ID\"\n  }\n ]\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://visionai.googleapis.com/v1/projects/PROJECT/locations/LOCATION_ID/applications/APPLICATION_ID:removeStreamInput\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://visionai.googleapis.com/v1/projects/PROJECT/locations/LOCATION_ID/applications/APPLICATION_ID:removeStreamInput\" | Select-Object -Expand Content\n```\nYou should receive a successful status code (2xx) and an empty response.\n## Add a pre-trained Vertex AI Vision model\nAfter you create a app you can add a model and connect them to streams, other models, or a media warehouse.\nThere are two types of models - or . Similarly, can also be two types - or :\n- **Pre-trained models** perform a specific objective, are trained on generalized data, and are ready to use.\n- **User-trained AutoML or custom-trained models** require you to identify and provide sample data, and then train models that are more suited to your unique use case. You train these models using Vertex AI, a product that offers two training options: AutoML trained models and custom-trained models. See [Choose a training method](/vertex-ai/docs/start/training-methods) for more information.Use the following instructions to add a pre-trained Vertex AI Vision model to your app graph.\n- Open the **Applications** tab of the Vertex AI Vision dashboard. [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n- Select **View app** next to the name of your application from the list.\n- On the application builder page select the model you want to add from the **AI models** list. Each model has individual configuration settings.\n- To connect this model to an input source node, select the source node and select add **Add output** .\n- Select the newly created AI model node from the list of **Available nodes** .## Add a user-trained Vertex AI model\nAfter you create a app you can add a model and connect them to streams, other models, or a media warehouse.\nThere are two types of models - or . Similarly, can also be two types - or :\n- **Pre-trained models** perform a specific objective, are trained on generalized data, and are ready to use.\n- **User-trained AutoML or custom-trained models** require you to identify and provide sample data, and then train models that are more suited to your unique use case. You train these models using Vertex AI, a product that offers two training options: AutoML trained models and custom-trained models. See [Choose a training method](/vertex-ai/docs/start/training-methods) for more information.\nVertex AI Vision supports [Vertex AI AutoML vision models](/vertex-ai/docs/training/automl-console) created in October 2022 or later. For models trained before October 2022, compatibility is not guaranteed.### Add a Vertex AI AutoML object detection streaming model\n**Fair-aware:** Review regulations in the region where you are deploying the technology, and existing research or industry guidance in your application domain to learn about policy guidelines and common fairness issues. Read more about [fairness in ML](/inclusive-ml#fairness-in-ml-automl) , including ways to mitigate bias in training datasets, evaluate your custom models for disparities in performance, and things to consider as you use your custom model.\nUse the following instructions to add an Vertex AI AutoML object detection streaming model you train to your app graph.\nBefore you are able to train a Vertex AI AutoML model you must [prepare your object detection data](/vertex-ai/docs/datasets/prepare-image#object-detection) and [create a dataset](/vertex-ai/docs/datasets/create-dataset-console) .\nAfter you create a dataset with a representative collection of data in Vertex AI, you can train a model to use in Vertex AI Vision.\n- In the Google Cloud console, in the Vertex AI dashboard, go to the **Datasets** page. [Go to the Datasets page](https://console.cloud.google.com/vertex-ai/datasets) \n- Click the name of the dataset you want to use to train your model to open its details page.\n- If your data type uses annotation sets, select the annotation set you want to use for this model.\n- Click **Train new model** . **Note:** You can type [model.new](https://model.new) into a browser to go directly to the model creation page.\n- In the **Train new model** page, complete the following steps for your AutoML image object detection model:- Select the model training method. <ul> <li><code translate=\"no\" dir=\"ltr\">AutoML</code> is a good choice for a wide range of use cases.</li> <li><code translate=\"no\" dir=\"ltr\">Seq2seq+</code> is a good choice for experimentation. The algorithm is likely to converge faster than  <code translate=\"no\" dir=\"ltr\">AutoML</code> because its architecture is simpler and it uses a smaller  search space. Our experiments find that Seq2Seq+ performs well with a small time budget and  on datasets smaller than 1&nbsp;GB in size. </li> </ul> Click <b>Continue</b>. In the **Model training method** section, choose radio_button_checked **AutoML** .\n- In the **Choose where to use the model** section, select radio_button_checked **Vertex AI Vision** .\n- Click **Continue** .\n- Fill in values for the **Model details** , **Training options** , and **Compute and pricing** sections. See [Train an AutoML model (Google Cloud console)](/vertex-ai/docs/training/automl-console) for more detailed information.\n- Click **Start Training** .Model training can take many hours, depending on the size and complexity of your data and your training budget, if you specified one. You can close this tab and return to it later. You will receive an email when your model has completed training.\nAfter your model is done training, you can add it to your Vertex AI Vision app.\n- Open the **Applications** tab of the Vertex AI Vision dashboard. [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n- Select **View app** next to the name of your application from the list.\n- On the application builder page select **Add Vertex AI Model** from the side components list. Selecting this option opens a side menu.\n- From the Add Vertex AI Model menu, leave the radio_button_checked **Import atrained model from Vertex AI** option selected, then choose the model from the list of existing Vertex AI AutoML models.\n- After choosing the model to import, select **Add model** .After the **Add model** button is clicked there will be a pending model on the side of the graph builder page. After a few seconds the model is ready to use. **Note** : To see updates to the model's status, refresh the page .\n### Add a Vertex AI custom model\n**Fair-aware:** Review regulations in the region where you are deploying the technology, and existing research or industry guidance in your application domain to learn about policy guidelines and common fairness issues. Read more about [fairness in ML](/inclusive-ml#fairness-in-ml-automl) , including ways to mitigate bias in training datasets, evaluate your custom models for disparities in performance, and things to consider as you use your custom model.\nYou can also import Vertex AI custom-trained models into Vertex AI Vision to use for data analysis. These custom-trained models must have the following properties:\n- The model must be stateless, since the model input is the images that come from different application instances (streams) and might be out of order. If you need stateful streaming processing, you might need to keep states inside your own container.\n- The input size to your model is limited to 1.5MB. Consequently, Vertex AI Vision must compress the original RGB image into a lossy format, such as JPG.\n**Limitation** : The [maxPredictionFps](/vision-ai/docs/reference/rest/v1/ApplicationConfigs#vertexcustomconfig) of your Vertex AI custom-trained model can't be more than 10. The custom-trained model's processing FPS is determined by the model itself and the resources assigned to the model. You must optimize your custom-trained model and assign enough resources, as well as set`maxPredictionFps`lower than the model's processing FPS. For example, if your custom-trained model can only process 5 FPS, you must set`maxPredictionFps`less than 5. You can set this value by selecting your custom-trained model from the app view in the [Google Cloud console](https://console.cloud.google.com/ai/vision-ai/studio) .\nUse the following instructions to add an existing Vertex AI custom-trained video model you trained to your app graph.\nFor information about creating a Vertex AI custom-trained model with custom container, see [Use a custom container for prediction](/vertex-ai/docs/predictions/use-custom-container) .\nWhen you add a Vertex AI custom-trained model you must specify an instances YAML file ( `instances.yaml` ) stored in Cloud Storage. This file specifies the expected input to your model container.\nVertex AI sends the prediction request in a format:\n```\n{\u00a0 \"instances\": [\u00a0 \u00a0 <value>|<simple/nested list>|<object>,\u00a0 \u00a0 ...\u00a0 ]}\n```\nHere, `instances.yaml` defines the schema of the payload. For more information, see [Get online predictions from custom-trained models](/vertex-ai/docs/predictions/get-predictions#request-response-examples) .\nVertex AI Vision only supports custom-trained models with exactly one prediction input. This input type has to be an encoded JPEG string. The schema of the prediction input must be specified with the instances schema YAML file. This schema YAML file must be in the format of an [OpenAPI schema object](https://github.com/OAI/OpenAPI-Specification/blob/main/versions/3.0.2.md#schemaObject) .\nFor example, the following schema will receive the request with the image encoded into a field calling `image_bytes` :\n```\nproperties:\u00a0 image_bytes:\u00a0 \u00a0 type: string\n```\nYour custom model receives prediction input in the following format:\n```\n{\u00a0 \"instances\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"image_bytes\" : {\u00a0 \u00a0 \u00a0 \u00a0 \"b64\": \"BASE64_ENCODED_IMAGE_BYTES\"\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ]}\n```\nBefore importing the model to Vertex AI Vision, verify that your model works correctly with this input.\nWhen you add a Vertex AI custom-trained model you can specify a predictions YAML file ( `predictions.yaml` ) stored in Cloud Storage. This file specifies the output from your model container.\nThis file is optional but recommended to inform Vertex AI Vision the output structure of your model. For example, the following [classification_1.0.0.yaml](https://storage.googleapis.com/google-cloud-aiplatform/schema/predict/prediction/classification_1.0.0.yaml) file describes model output information for an image classification model:\n```\ntitle: Classificationtype: objectdescription: >\u00a0 The predicted AnnotationSpecs.properties:\u00a0 ids:\u00a0 \u00a0 type: array\u00a0 \u00a0 description: >\u00a0 \u00a0 \u00a0 The resource IDs of the AnnotationSpecs that had been identified.\u00a0 \u00a0 items:\u00a0 \u00a0 \u00a0 type: integer\u00a0 \u00a0 \u00a0 format: int64\u00a0 \u00a0 \u00a0 enum: [0] \u00a0# As values of this enum all AnnotationSpec IDs the Model\u00a0 \u00a0 \u00a0 # was trained on will be populated.\u00a0 displayNames:\u00a0 \u00a0 type: array\u00a0 \u00a0 description: >\u00a0 \u00a0 \u00a0 The display names of the AnnotationSpecs that had been identified,\u00a0 \u00a0 \u00a0 order matches the IDs.\u00a0 \u00a0 items:\u00a0 \u00a0 \u00a0 type: string\u00a0 \u00a0 \u00a0 enum: [\"\"] \u00a0# As values of this enum all AnnotationSpec display_names\u00a0 \u00a0 \u00a0 # the Model was trained on will be populated.\u00a0 confidences:\u00a0 \u00a0 type: array\u00a0 \u00a0 description: >\u00a0 \u00a0 \u00a0 The Model's confidences in correctness of the predicted IDs, higher\u00a0 \u00a0 \u00a0 value means higher confidence. Order matches the Ids.\u00a0 \u00a0 items:\u00a0 \u00a0 \u00a0 type: number\u00a0 \u00a0 \u00a0 format: float\u00a0 \u00a0 \u00a0 minimum: 0.0\u00a0 \u00a0 \u00a0 maximum: 1.0\n```\nUse the following sample to add the Vertex AI custom-trained model to your app.\n- Open the **Applications** tab of the Vertex AI Vision dashboard. [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n- Select **View app** next to the name of your application from the list.\n- On the application builder page select **Add Vertex AI Model** from the side components list. Selecting this option opens a side menu.\n- From the Add Vertex AI Model menu, leave the radio_button_checked **Import atrained model from Vertex AI** option selected, then choose the model from the list of existing Vertex AI AutoML models.\n- Provide a name for the model.\n- Specify the instances YAML file in Cloud Storage that defines the format of a single instance used in prediction and explanation requests.\n- Optional: Specify the predictions schema YAML file in Cloud Storage that defines the format of a single prediction or explanation.\n- After providing model name, instances, and predictions information, select **Add model** .\n- After the **Add model** button is clicked there will be a pending model on the side of the graph builder page. After a few seconds the model is ready to use. **Note** : To see updates to the model's status, refresh the page .In addition to image bytes, you have the option to set `attach_application_metadata` to ask the Vertex AI Vision app platform to include the metadata of the application to be sent along to the custom container.\nThe metadata has the following schema:\n```\n'appPlatformMetadata': {\u00a0 \u00a0 \u00a0'application': STRING;\u00a0 \u00a0 \u00a0'instanceId': STRING;\u00a0 \u00a0 \u00a0'node': STRING;\u00a0 \u00a0 \u00a0'processor': STRING;\u00a0 \u00a0}\n```\nTo dynamically control the frame rate that the Vertex Custom Operator sends video frames to vertex custom containers, you can create a Pub/Sub topic. Add it to the `Dynamic Configuration` section of the node settings.\nIf the Pub/Sub topic is configured, the initial frame rate is 0. During video processing, you can send Pub/Sub messages to the Pub/Sub topic in the following format in real time to change the frame rate:\n```\n{\u00a0 \"stream_id\": \"input-stream-id\",\u00a0 \"fps\": 5,}\n```\nThe `stream_id` field should match the ID of the input stream of the application.\nThe default service credential of the custom container has been configured to the Google-owned service account of Vertex AI Vision app platform. To access other Google Cloud services from the container, [grant proper permission](/sdk/gcloud/reference/projects/add-iam-policy-binding) to: `service-<var>PROJECT_NUMBER</var>@gcp-sa-visionai.iam.gserviceaccount.com`\n## Optional. Model event notification with Cloud Functions and Pub/Sub\nIn Vertex AI Vision, models receive media data from devices like cameras, run AI predictions on the data, and produce annotations continuously. Frequently you send that processed data to a data destination (\"data sink\") such as a media warehouse or BigQuery for further analytic jobs. However, you may have a case where some annotations must be handled differently, or the annotation needs are time-sensitive. Integrations with Cloud Functions and Pub/Sub help you address these needs.\nThe following models offer Cloud Functions event generation and Pub/Sub event notification integrations:\n- [Occupancy analytics](/vision-ai/docs/occupancy-analytics-model) model\n- Vertex AI [custom-trained models](#add-vertex-custom-model) \n### Configure Cloud Functions to process model output\nTo trigger event-based notifications, you must first set up Cloud Functions to process model output and generate events.\nYour Cloud Function connects to the model and listens to its output as its post-processing action. The Cloud Function you should return an [AppPlatformCloudFunctionResponse](/vision-ai/docs/reference/rpc/google.cloud.visionai.v1#appplatformcloudfunctionresponse) . The events ( [appplatformeventbody](/vision-ai/docs/reference/rpc/google.cloud.visionai.v1#appplatformeventbody) ) are sent to the Pub/Sub topic you configure in the next step.\nTo view a sample Cloud Function, see [Enable model event notification with Cloud Functions and Pub/Sub](/vision-ai/docs/model-event-notification#sample-oa-function) .\nUse the following instructions to to send the model output stream to your Cloud Function:\n- Open the **Applications** tab of the Vertex AI Vision dashboard. [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n- Select schema **View app** next to the name of your application from the list.\n- Click on the [supported model](#event-supported-model) to open the model details side panel.\n- In the **post-processing** list of the **Event notification** section, select your existing Cloud Function, or create a new one.\n### Enable model event notification with Pub/Sub\nAfter you have set up Cloud Functions to process model output and generate events, you can set up event notification with Pub/Sub. To read messages from a topic, you also need to [Choose and create a Pub/Sub subscription](/pubsub/docs/subscriber) .\n- Open the **Applications** tab of the Vertex AI Vision dashboard. [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n- Select schema **View app** next to the name of your application from the list.\n- Click on the [supported model](#event-supported-model) to open the model details side panel.\n- In the **Event notification** section, select **Set up event notification** .\n- In the **Set up Pub/Sub for event notifications** option window that opens, choose your existing Pub/Sub topic, or create a new one.\n- In the **Frequency** field, set an integer value for the frequency value in seconds a notification for the same type of event can be sent.\n- Click **Set up** .## Connect model output with a downstream node\nAfter you create a model node you can connect its output to another node.\n- Open the **Applications** tab of the Vertex AI Vision dashboard. [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n- Select **View app** next to the name of your application from the list.\n- Click on a model node that outputs data in the application graph builder (for example, the **Occupancy Count** model node).\n- In the side settings panel, click the **Add output** button.\n- Either **Add a new output** and choose a new output node to add, or select an existing **Available Node** to send output to.## Connect output to a data destination\nAfter you create the data ingestion stream and add the processing nodes to your app, you must choose where to send the processed data. This data destination is the end point of your app graph that accepts stream data without producing any stream data. The destination you choose depends on how you use the app output data.\nYou can connect app output data to the following data destinations:\n- **Vision Warehouse** : Store original or analyzed video data that you can view and query.For more information about sending app output data to a Vision Warehouse, see [Connect and store data to a warehouse](/vision-ai/docs/connect-warehouse) .\n- **BigQuery** : Store data in BigQuery to use its offline analytics capabilities.For more information about sending app output data to BigQuery, see [Connect and store data to BigQuery](/vision-ai/docs/connect-bigquery) .\n- **Live stream data** : If you want more granular control to act on real-time analytics, you can receive the live output from your app.For more information about directly streaming app output data, see [Enable live stream output](/vision-ai/docs/enable-stream-output) .## Update an app\n**Undeployed apps**\nMake any modifications to the app (such as adding or removing component nodes) ; Vertex AI Vision will automatically store the changes.\n## What's next\n- Learn how to deploy your app to ingest and analyze data in [Deploy an application](/vision-ai/docs/deploy-app) .\n- Read instructions about how to begin data ingestion from an app's input stream in [Create and manage streams](/vision-ai/docs/create-manage-streams#ingest-videos) .\n- Learn how to list apps and view a deployed app's instances in [Managing applications](/vision-ai/docs/manage-app) .\n- Learn how to read app input data from an ingestion stream or analyzed model output data in [Read stream data](/vision-ai/docs/read-stream) .", "guide": "Vertex AI Vision"}