{"title": "Vertex AI Vision - Object detector guide", "url": "https://cloud.google.com/vision-ai/docs/object-detector-model?hl=zh-cn", "abstract": "# Vertex AI Vision - Object detector guide\nThe **Object detector model** can identify and locate more than 500 types of objects in a video. The model accepts a video stream as input and outputs a [protocol buffer](https://developers.google.com/protocol-buffers) with the detection results to BigQuery. The model runs at one FPS. When you create an app that uses the object detector model, you must direct model output to a BigQuery connector to view prediction output.\n", "content": "## Object detector model app specifications\nUse the following instructions to create a object detector model in the Google Cloud console.\n**Create an app in the Google Cloud console** - To create a object detector app, follow instructions in [Build an application](/vision-ai/docs/build-app) . [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n **Add an object detector model** - When you add model nodes, select the **Object detector** from the list of pre-trained models.\n **Add a BigQuery connector** - To use the output, connect the app to a **BigQuery** connector.For information about using the **BigQuery** connector, see [Connect and store data to BigQuery](/vision-ai/docs/connect-bigquery) . For BigQuery pricing information, see the [BigQuery pricing](/bigquery/pricing) page.\n **View output results in BigQuery** \nAfter the model outputs data to BigQuery, view output annotations in the BigQuery dashboard.\nIf you didn't specify a BigQuery path, you can view the system-created path in the Vertex AI Vision schema **Studio** page.- In the Google Cloud console, open the BigQuery page. [Go to BigQuery](https://console.cloud.google.com/bigquery) \n- Select arrow_drop_down **Expand** next to the target project, dataset name, and application name. \n- In the table detail view, click **Preview** . View results in the **annotation** column. For a description of the output format, see [model output](#model-output) .\nThe application stores results in chronological order. The oldest results are the beginning of the table, while the most recent results are added to the end of the table. To check the latest results, click the page number to go to the last table page.\n## Model output\nThe model outputs bounding boxes, their object labels, and confidence scores for each video frame. The output also contains a timestamp. The rate of the output stream is one frame per second.\nIn the [protocol buffer](https://developers.google.com/protocol-buffers) output example that follows, note the following:\n- Timestamp - The timestamp corresponds to the time for this inference result.\n- Identified boxes - The main detection result that includes box identity, bounding box information, confidence score, and object prediction.\n### Sample annotation output JSON object\n```\n{\n \"currentTime\": \"2022-11-09T02:18:54.777154048Z\",\n \"identifiedBoxes\": [ {\n  \"boxId\":\"0\",\n  \"normalizedBoundingBox\": {\n  \"xmin\": 0.6963465,\n  \"ymin\": 0.23144785,\n  \"width\": 0.23944569,\n  \"height\": 0.3544306\n  },\n  \"confidenceScore\": 0.49874997,\n  \"entity\": {\n  \"labelId\": \"0\",\n  \"labelString\": \"Houseplant\"\n  }\n }\n ]\n}\n```\n### Protocol buffer definition\n```\n// The prediction result protocol buffer for object detectionmessage ObjectDetectionPredictionResult {\u00a0 // Current timestamp\u00a0 protobuf.Timestamp timestamp = 1;\u00a0 // The entity information for annotations from object detection prediction\u00a0 // results\u00a0 message Entity {\u00a0 \u00a0 // Label id\u00a0 \u00a0 int64 label_id = 1;\u00a0 \u00a0 // The human-readable label string\u00a0 \u00a0 string label_string = 2;\u00a0 }\u00a0 // The identified box contains the location and the entity of the object\u00a0 message IdentifiedBox {\u00a0 \u00a0 // An unique id for this box\u00a0 \u00a0 int64 box_id = 1;\u00a0 \u00a0 // Bounding Box in normalized coordinates [0,1]\u00a0 \u00a0 message NormalizedBoundingBox {\u00a0 \u00a0 \u00a0 // Min in x coordinate\u00a0 \u00a0 \u00a0 float xmin = 1;\u00a0 \u00a0 \u00a0 // Min in y coordinate\u00a0 \u00a0 \u00a0 float ymin = 2;\u00a0 \u00a0 \u00a0 // Width of the bounding box\u00a0 \u00a0 \u00a0 float width = 3;\u00a0 \u00a0 \u00a0 // Height of the bounding box\u00a0 \u00a0 \u00a0 float height = 4;\u00a0 \u00a0 }\u00a0 \u00a0 // Bounding Box in the normalized coordinates\u00a0 \u00a0 NormalizedBoundingBox normalized_bounding_box = 2;\u00a0 \u00a0 // Confidence score associated with this bounding box\u00a0 \u00a0 float confidence_score = 3;\u00a0 \u00a0 // Entity of this box\u00a0 \u00a0 Entity entity = 4;\u00a0 }\u00a0 // A list of identified boxes\u00a0 repeated IdentifiedBox identified_boxes = 2;}\n```\n## Best practices and limitations\nTo get the best results when you use the object detector, consider the following when you source data and use the model.\n### Source data recommendations\nRecommended: Make sure the objects in the picture are clear and are not covered or largely obscured by other objects.\nSample image data the object detector is able to process correctly:\n| 0          |\n|:--------------------------------------|\n| Image source: Spacejoy on Unsplash. |\nSending the model this image data returns the following object detection information :\nThe annotations in the following image are for illustrative purposes only. The bounding boxes, labels, and confidence scores are manually drawn and not added by the model or any Google Cloud console tool.\nNot recommended: Avoid image data where the key object items are too small in the frame.\nSample image data the object detector isn't able to process correctly:\n| 0                |\n|:--------------------------------------------------------------|\n| Image source: Bernard Hermant on Unsplash (image cropped). |\nNot recommended: Avoid image data that show the key object items partially or fully covered by other objects.\nSample image data the object detector isn't able to process correctly:\n| 0            |\n|:------------------------------------------------|\n| Image source: \u015eahin Sezer Din\u00e7er on Unsplash. |\n### Limitations\n- **Video resolution** : The recommended maximum input video resolution is 1920 x 1080, and the recommended minimum resolution is 160 x 120.\n- **Lighting** : The model performance is sensitive to lighting conditions. Extreme brightness or darkness might lead to lower detection quality.\n- **Object size** : The object detector has a minimal detectable object size. Make sure the target objects are sufficiently large and visible in your video data.", "guide": "Vertex AI Vision"}