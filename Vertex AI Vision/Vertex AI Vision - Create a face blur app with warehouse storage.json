{"title": "Vertex AI Vision - Create a face blur app with warehouse storage", "url": "https://cloud.google.com/vision-ai/docs/face-blur-tutorial?hl=zh-cn", "abstract": "# Vertex AI Vision - Create a face blur app with warehouse storage\n**Note:** To complete this tutorial you need access to video data with human faces, either Real Time Streaming Protocol ( [RTSP](https://en.wikipedia.org/wiki/Real_Time_Streaming_Protocol) ) video data from a live Internet Protocol ( [IP](https://en.wikipedia.org/wiki/IP_camera) ) camera, or a local video file.\nVertex AI Vision is an AI-powered platform you can use to ingest, analyze, and store video and image data. Vertex AI Vision lets you build and deploy AI applications. You can build end-to-end Vertex AI Vision solutions by leveraging Vertex AI Vision's integration with other product components.\nTo start implementing solutions using the Vertex AI Vision platform, review the following Vertex AI Vision concepts and components:- **Streams** : Represent a video streaming layer from your solution. The stream source can be a live video (for example, an IP camera) or a video file (for example, an MP4 file).\n- **Applications** : Enable the connection between a stream and an AI processor to perform a machine learning operation on the video. For example, you can connect a camera stream to an AI model that counts people passing in front of it.\n- **Media warehouses** : Store the video ingested by streams out to Google Cloud storage. Storing data out to this destination lets you query analysis output and metadata from the AI processors used on data from the ingested streams.\n", "content": "## ObjectivesThis tutorial shows you how to do the following:- Create a data input stream resource.\n- Begin streaming video data into this stream resource.\n- Create an empty application.\n- Add nodes to your application to stream data, modify data, and store data.\n- Deploy your app for you to use.\n- View processed data output in the Google Cloud console.\nBefore you begin this tutorial, you must have a streaming video resource to send data to your Vertex AI Vision app. This resource can be either a local video or an RTSP feed. This video data must contain human faces that the sample app can then blur.## CostsIn this document, you use the following billable components of Google Cloud:- [Vertex AI Vision](/vision-ai/pricing) (Streams -  Data ingested, Streams - Data consumed, Models - Person / face blur,  Warehouse - Video storage)\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- Get the location of your streaming video source, either locally  (for example,`./sample_video.mp4`) or IP address of the live  RTSP feed (for example,`rtsp://192.168.1.180:540`). You need  this information to begin ingesting data into a stream after you create the  stream resource.\n- Installing the`vaictl`tool to ingest data into a stream  (OS: Debian GNU/Linux, CPU architecture: x86_64): **Note** : You can also get a Docker image that has  the Vertex AI Vision SDK and all its dependencies already pre-installed.  For information about getting a Docker image with the  Vertex AI Vision SDK, [Set up a  project and a development environment](/vision-ai/docs/cloud-environment#get-docker) .- Download the required package:```\nwget https://github.com/google/visionai/releases/download/v0.0.5/visionai_0.0-5_amd64.deb\n```\n- After downloading the package, run the following command in the directory   you downloaded the file:```\nsudo apt install ./visionai_0.0-5_amd64.deb\n```\n- Verify installation:```\nvaictl --help\n```## Create a stream\nTo create a streaming video analysis app, you must first create and register a stream resource. As the resource that receives the video data from the users, the stream is required on any scenario you build using Vertex AI Vision.To create a new stream in the Google Cloud console, use the following steps.- Open the **Streams** tab of the Vertex AI Vision dashboard. [Go to the Streams tab](https://console.cloud.google.com/ai/vision-ai/video-streams) \n- Click add **Register** .\n- Enter `input-stream` as the stream name and select the region where you want to create your stream.\n- Click **Register** to create one or more streams. \n## Ingest video into the streamAfter you create a stream resource, you can use the `vaictl` command-line tool to send video data to the stream.\n **Note:** If you installed the `vaictl` tool using the Docker container option, you must [run your container](https://docs.docker.com/engine/reference/commandline/container_run/) and sign in before proceeding with the next steps.If you're testing using a live IP camera, you need to get the IP address of   the camera. You must provide this information with the request, along with   other variable substitutions:- : Your Google Cloud project ID.\n- : Your location ID. For example,`us-central1`. For more information, see [Cloud locations](/about/locations) .\n- : The address of your Real Time Streaming   Protocol ( [RTSP](https://en.wikipedia.org/wiki/Real_Time_Streaming_Protocol) ) feed. For example,`rtsp://192.168.1.180:540`.\nThis command sends an RTSP feed into the stream. You must run this   command in the network that has direct access to the RTSP feed.\n```\nvaictl -p PROJECT_ID \\\n  -l LOCATION_ID \\\n  -c application-cluster-0 \\\n  --service-endpoint visionai.googleapis.com \\\nsend rtsp to streams input-stream --rtsp-uri RTSP_ADDRESS\n  \n```\nIf the command runs successfully, you get the following output:\n```\n[...]\nWaiting for long running operation projects/your-project/locations/us-central1/operations/operation-1651364156981-5dde82db7e4a9-dfb17ca5-1051eb20 \u2819\nI20220430 21:16:28.024988 211449 gstvaisink.cc:417] cluster-id=application-cluster-0\nI20220430 21:16:28.025032 211449 gstvaisink.cc:418] cluster-endpoint=c8khq35ftg78mn61ef50.us-central1.visionai.goog\nI20220430 21:16:28.025040 211449 gstvaisink.cc:419] event-id=ev-1651364114183255223\nI20220430 21:16:28.025048 211449 gstvaisink.cc:420] stream-id=input-stream\nI20220430 21:16:28.025053 211449 gstvaisink.cc:421] series-id=ev-1651364114183255223--input-stream\nI20220430 21:16:28.025060 211449 gstvaisink.cc:422] Sending data\n```You can also send video file data to a stream instead of a live video   feed. This option can be useful if you don't have access to an IP   camera.\nThe only difference in this option is the `vaictl` command   parameters. Instead of passing the IP camera information, pass   the path for the local video file. Make the following variable   substitutions:- : Your Google Cloud project ID.\n- : Your location ID. For example,`us-central1`. [More   information](/about/locations) .\n- : The filename of a local video file.   For example,`my-video.mp4`.\n- `--loop`flag: Optional. Loops file data to simulate   streaming.\nThis command streams a video file to a stream. If using the `--loop` flag, the video is looped into the stream until you   stop the command:\n```\nvaictl -p PROJECT_ID \\\n  -l LOCATION_ID \\\n  -c application-cluster-0 \\\n  --service-endpoint visionai.googleapis.com \\\nsend video-file to streams 'input-stream' --file-path LOCAL_FILE.EXT --loop\n```\nIt might take ~100 seconds between starting the `vaictl` ingest operation and the video appearing in the dashboard.\nAfter the stream ingestion is available, you can see the video feed in the **Streams** tab of the Vertex AI Vision dashboard by selecting the `input-stream` stream.\n [Go to the Streams tab](https://console.cloud.google.com/ai/vision-ai/video-streams) ## Create a face blur applicationAfter you create a stream and ingest data into the stream, it's time to create a Vertex AI Vision app to process the data. An app can be thought of as an automated pipeline that connects the following:- **Data ingestion** : A video feed is ingested into a stream.\n- **Data analysis** : An AI model can be added after the ingestion. Any computer vision operation can be performed on the ingested video information.\n- **Data storage** : The two versions of the video feed (the original stream and the stream processed by the AI model) can be stored in a media warehouse.\nIn the Google Cloud console an app is represented as a graph. Additionally, in Vertex AI Vision an app graph must have at least two nodes: a video source node (stream), and one more node (a processing model or output destination).\n### Create an empty application\nBefore you can populate the app graph, you must first create an empty app.Create an app in the Google Cloud console.- Open the **Applications** tab of the Vertex AI Vision dashboard. [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n- Click the add **Create** button.\n- Enter `person-blur-app` as the app name and choose your region.\n- Click **Create** . ### Add app component nodesAfter you have created the empty application, you can then add the three nodes to the app graph:- **Ingestion node** : The stream resource that's already ingesting data.\n- **Processing node** : The person blur model that acts on ingested data.\n- **Storage node** : The media warehouse that stores processed videos, and also serves as a metadata store. The warehouse allows analytics information to be generated about ingested video data, as well as stores information inferred about the data by the AI models.Add component nodes to your app in the console.- Open the **Applications** tab of the Vertex AI Vision dashboard. [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n- In the `person-blur-app` line, select schema **View graph** . This takes you to the graph visualization of the processing pipeline.\n **Add a data ingestion node** - To add the input stream node, select the **Streams** option in the **Connectors** section of the side menu.\n- In the **Source** section of the **Stream** menu that opens, select add **Add streams** .\n- In the **Add streams** menu, choose radio_button_checked **Select fromexisting streams** and select `person-blur-app` from the list of stream resources.\n- To add the stream to the app graph, click **Add streams** .\n **Add a data processing node** - To add the person blur model node, select the **Person blur** option in the **General processors** section of the side menu.\n- In the \"Person blur\" option menu that opens, leave radio_button_checked **Full occlusion** selected and enable the check_box **Blur faces only** option.\n **Add a data storage node** - To add the output destination (storage) node, select the **Vertex AI Vision's Media Warehouse** option in the **Connectors** section of the side menu.\n- In the **Vertex AI Vision's Media Warehouse** menu, click **Connect warehouse** .\n- In the **Connect warehouse** menu, select radio_button_checked **Create newwarehouse** . Name the warehouse `person-blur-app` , and leave the TTL duration at 14 days.\n- To add the warehouse, click **Create** .\n## Deploy your application\nAfter you have built your end-to-end app with all the necessary components, the last step to using the app is to deploy it.\n- Open the **Applications** tab of the Vertex AI Vision dashboard. [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n- Select **View graph** next to the `person-blur-app` app in the list.\n- From the application graph builder page, click the play_arrow **Deploy** button.\n- In the following confirmation dialog, select **Deploy** .The deploy operation might take several minutes to complete. After deployment finishes, green check marks appear next to the nodes. \n## View processed output data\n- Open the **Warehouses** tab of the Vertex AI Vision dashboard. [Go to the Warehouses tab](https://console.cloud.google.com/ai/vision-ai/media-warehouse) \n- Find the `person-blur-output-storage` warehouse in the list, and click widgets **View assets** . \n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n## What's next\n- Read more about [Responsible AI practices](https://ai.google/responsibilities/responsible-ai-practices/) .\n- Learn about other components you can add to an app in [Build an app](/vision-ai/docs/build-app) .\n- Learn about other output storage and processing options in [Connect app output to a data destination ](/vision-ai/docs/connect-data-destination) .\n- Read about how to [Search Warehouse data in the console](/vision-ai/docs/search-streaming-warehouse) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Vertex AI Vision"}