{"title": "Vertex AI Vision - Connect and store data to BigQuery", "url": "https://cloud.google.com/vision-ai/docs/connect-bigquery?hl=zh-cn", "abstract": "# Vertex AI Vision - Connect and store data to BigQuery\nWhen you add a BigQuery connector to your Vertex AI Vision app all the connected app model outputs will be ingested to the target table.\nYou can either create your own BigQuery table and specify that table when you add a BigQuery connector to the app, or let the Vertex AI Vision app platform automatically create the table.\n**Caution:** Data you ingest into BigQuery is first written to the streaming buffer before they are persisted into the storage. You are not able to use the **Preview** tab to view these rows in the streaming buffer. However, you are still able to read these rows with SQL queries. Use the **Details** tab of the BigQuery table to view metrics about the streaming buffer data.\n", "content": "## Automatic table creation\nIf you let Vertex AI Vision app platform automatically create the table, you can specify this option when you add the BigQuery connector node.\nThe following dataset and table conditions apply if you want to use automatic table creation:\n- Dataset: The automatically created dataset name is`visionai_dataset`.\n- Table: The automatically created table name is`visionai_dataset.` ``.\n- Error handling:- If the table with the same name under the same dataset exists, no automatic creation happens.\n- Open the **Applications** tab of the Vertex AI Vision dashboard. [Go to the Applications tab](https://console.cloud.google.com/ai/vision-ai/applications) \n- Select **View app** next to the name of your application from the list.\n- On the application builder page select **BigQuery** from the **Connectors** section.\n- Leave the **BigQuery path** field empty.\n- Change any other settings.\nTo let the app platform infer a table schema, use the `createDefaultTableIfNotExists` field of the [BigQueryConfig](/vision-ai/docs/reference/rest/v1alpha1/ApplicationConfigs#bigqueryconfig) when you [create](/vision-ai/docs/reference/rest/v1alpha1/projects.locations.applications/create) or [update](/vision-ai/docs/reference/rest/v1alpha1/projects.locations.applications/patch) an app.\n## Manually create and specify a table\nIf you want to manage your output table manually, the table must have the required schema as a subset of the table schema.\nIf the existing table has incompatible schemas, the deployment is rejected.\n### Use the default schema\nIf you use the default schema for model output tables, make sure your table only contains the following required columns in the table. You can directly copy the following schema text when you create the BigQuery table. For more detailed information about creating a BigQuery table, see [Create and use tables](/bigquery/docs/tables) . For more information about schema specification when you create a table, see [Specifying a schema](/bigquery/docs/schemas) .\nUse the following text to describe the schema when you create a table. For information on using the [JSON](/bigquery/docs/reference/standard-sql/data-types#json_type) column type ( `\"type\": \"JSON\"` ), see [Working with JSON data in Standard SQL](/bigquery/docs/reference/standard-sql/json-data) . The JSON column type is recommended for annotation query. You can also use `\"type\" : \"STRING\"` .\n```\n[\u00a0 {\u00a0 \u00a0 \"name\": \"ingestion_time\",\u00a0 \u00a0 \"type\": \"TIMESTAMP\",\u00a0 \u00a0 \"mode\": \"REQUIRED\"\u00a0 },\u00a0{\u00a0 \u00a0\"name\": \"application\",\u00a0 \u00a0\"type\": \"STRING\",\u00a0 \u00a0\"mode\": \"REQUIRED\"\u00a0},\u00a0{\u00a0 \u00a0\"name\": \"instance\",\u00a0 \u00a0\"type\": \"STRING\",\u00a0 \u00a0\"mode\": \"REQUIRED\"\u00a0},\u00a0{\u00a0 \u00a0\"name\": \"node\",\u00a0 \u00a0\"type\": \"STRING\",\u00a0 \u00a0\"mode\": \"REQUIRED\"\u00a0},\u00a0{\u00a0 \u00a0\"name\": \"annotation\",\u00a0 \u00a0\"type\": \"JSON\",\u00a0 \u00a0\"mode\": \"REQUIRED\"\u00a0}]\n```\n**Warning:** The BigQuery JSON column may be unavailable in the console. If you cannot create a JSON column in the console, use the `gcloud` command below, or replace the `\"JSON\"` column with a `\"STRING\"` column.\n- In the Google Cloud console, go to the **BigQuery** page. [Go to BigQuery](https://console.cloud.google.com/bigquery) \n- Select your project.\n- Select more options more_vert .\n- Click **Create table** .\n- In the \"Schema\" section, enable toggle_on **Edit as text** .\nThe following example first creates the request JSON file, then uses the [gcloud alpha bq tables create command](/sdk/gcloud/reference/alpha/bq/tables/create) .- First create the request JSON file:```\necho \"{\n\\\"schema\\\": [ {\n  \\\"name\\\": \\\"ingestion_time\\\",\n  \\\"type\\\": \\\"TIMESTAMP\\\",\n  \\\"mode\\\": \\\"REQUIRED\\\"\n },\n {\n  \\\"name\\\": \\\"application\\\",\n  \\\"type\\\": \\\"STRING\\\",\n  \\\"mode\\\": \\\"REQUIRED\\\"\n },\n {\n  \\\"name\\\": \\\"instance\\\",\n  \\\"type\\\": \\\"STRING\\\",\n  \\\"mode\\\": \\\"REQUIRED\\\"\n },\n {\n  \\\"name\\\": \\\"node\\\",\n  \\\"type\\\": \\\"STRING\\\",\n  \\\"mode\\\": \\\"REQUIRED\\\"\n },\n {\n  \\\"name\\\": \\\"annotation\\\",\n  \\\"type\\\": \\\"JSON\\\",\n  \\\"mode\\\": \\\"REQUIRED\\\"\n }\n]\n}\n\" >> bigquery_schema.json\n```\n- Send the `gcloud` command. Make the following replacements:- : The ID of the table or fully qualified identifier for the table.\n- : The id of the BigQuery dataset.\n```\ngcloud alpha bq tables create TABLE_NAME \\--dataset=DATASET \\--schema-file=./bigquery_schema.json\n```\nSample BigQuery rows generated by a Vertex AI Vision app:\n| ingestion_time    | application | instance | node   | annotation                                 |\n|:------------------------------|:---------------|-----------:|:--------------|:-------------------------------------------------------------------------------------------------------------------------------------------|\n| 2022-05-11 23:3211.911378 UTC | my_application |   5 | just-one-node | {\"bytesFields\": [\"Ig1qdXN0LW9uZS1ub2RIGgE1Eg5teV9hcHBsaWNhdGlvbgjS+YnOzdj3Ag==\"],\"displayNames\":[\"hello\",\"world\"],\"ids\":[\"12345\",\"34567\"]} |\n| 2022-05-11 23:3211.911338 UTC | my_application |   1 | just-one-node | {\"bytesFields\": [\"Ig1qdXN0LW9uZS1ub2RIGgExEg5teV9hcHBsaWNhdGlvbgiq+YnOzdj3Ag==\"],\"displayNames\":[\"hello\",\"world\"],\"ids\":[\"12345\",\"34567\"]} |\n| 2022-05-11 23:3211.911313 UTC | my_application |   4 | just-one-node | {\"bytesFields\": [\"Ig1qdXN0LW9uZS1ub2RIGgE0Eg5teV9hcHBsaWNhdGlvbgiR+YnOzdj3Ag==\"],\"displayNames\":[\"hello\",\"world\"],\"ids\":[\"12345\",\"34567\"]} |\n| 2022-05-11 23:3212.235327 UTC | my_application |   4 | just-one-node | {\"bytesFields\": [\"Ig1qdXN0LW9uZS1ub2RIGgE0Eg5teV9hcHBsaWNhdGlvbgi/3J3Ozdj3Ag==\"],\"displayNames\":[\"hello\",\"world\"],\"ids\":[\"12345\",\"34567\"]} |\n### Use a customized schema\nIf the default schema doesn't work for your use case, you can use Cloud Functions to generate BigQuery rows with a user-defined schema. If you use a custom schema there is no prerequisite for the BigQuery table schema.\n**App graph with BigQuery node selected**\nThe BigQuery connector can be connected to any model that outputs video or proto-based annotation:\n- For video input, the BigQuery connectorextracts the metadata data stored in the stream header and ingests this data to BigQuery as other model annotation outputs. The video itself isstored.\n- If your stream containsmetadata, nothing will be stored to BigQuery.## Query table data\nWith the default BigQuery table schema, you can perform powerful analysis after the table is populated with data.\n### Sample queries\nYou can use the following sample queries in BigQuery to gain insight from Vertex AI Vision models.\nFor example, you can use BigQuery to draw a time-based curve for maximum number of detected people per minute using data from the [Person / vehicle detector model](/vision-ai/docs/person-vehicle-model) with the following query:\n```\nWITH\u00a0nested3 AS(\u00a0WITH\u00a0 \u00a0nested2 AS (\u00a0 \u00a0WITH\u00a0 \u00a0 \u00a0nested AS (\u00a0 \u00a0 \u00a0SELECT\u00a0 \u00a0 \u00a0 \u00a0t.ingestion_time AS ingestion_time,\u00a0 \u00a0 \u00a0 \u00a0JSON_QUERY_ARRAY(t.annotation.stats[\"fullFrameCount\"]) AS counts\u00a0 \u00a0 \u00a0FROM\u00a0 \u00a0 \u00a0 \u00a0`PROJECT_ID.DATASET_NAME.TABLE_NAME` AS t)\u00a0 \u00a0SELECT\u00a0 \u00a0 \u00a0ingestion_time,\u00a0 \u00a0 \u00a0e\u00a0 \u00a0FROM\u00a0 \u00a0 \u00a0nested,\u00a0 \u00a0 \u00a0UNNEST(nested.counts) AS e)\u00a0SELECT\u00a0 \u00a0STRING(TIMESTAMP_TRUNC(nested2.ingestion_time, MINUTE, \"America/Los_Angeles\"),\"America/Los_Angeles\") AS time,\u00a0 \u00a0IFNULL(INT64(nested2.e[\"count\"]), 0) AS person_count\u00a0FROM\u00a0 \u00a0nested2\u00a0WHERE\u00a0 \u00a0JSON_VALUE(nested2.e[\"entity\"][\"labelString\"])=\"Person\")SELECT\u00a0time,\u00a0MAX(person_count)FROM\u00a0nested3GROUP BY\u00a0time\n```\nSimilarly, you can use BigQuery and the crossing line count feature of the [Occupancy analytics model](/vision-ai/docs/occupancy-analytics-model) to create a query that counts the total number of vehicles that pass the crossing line per minute:\n```\nWITH\u00a0nested4 AS (\u00a0WITH\u00a0 \u00a0nested3 AS (\u00a0 \u00a0WITH\u00a0 \u00a0 \u00a0nested2 AS (\u00a0 \u00a0 \u00a0WITH\u00a0 \u00a0 \u00a0 \u00a0nested AS (\u00a0 \u00a0 \u00a0 \u00a0SELECT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0t.ingestion_time AS ingestion_time,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0JSON_QUERY_ARRAY(t.annotation.stats[\"crossingLineCounts\"]) AS lines\u00a0 \u00a0 \u00a0 \u00a0FROM\u00a0 \u00a0 \u00a0 \u00a0 \u00a0`PROJECT_ID.DATASET_NAME.TABLE_NAME` AS t)\u00a0 \u00a0 \u00a0SELECT\u00a0 \u00a0 \u00a0 \u00a0nested.ingestion_time,\u00a0 \u00a0 \u00a0 \u00a0JSON_QUERY_ARRAY(line[\"positiveDirectionCounts\"]) AS entities\u00a0 \u00a0 \u00a0FROM\u00a0 \u00a0 \u00a0 \u00a0nested,\u00a0 \u00a0 \u00a0 \u00a0UNNEST(nested.lines) AS line\u00a0 \u00a0 \u00a0WHERE\u00a0 \u00a0 \u00a0 \u00a0JSON_VALUE(line.annotation.id) = \"LINE_ANNOTATION_ID\")\u00a0 \u00a0SELECT\u00a0 \u00a0 \u00a0ingestion_time,\u00a0 \u00a0 \u00a0entity\u00a0 \u00a0FROM\u00a0 \u00a0 \u00a0nested2,\u00a0 \u00a0 \u00a0UNNEST(nested2.entities) AS entity )\u00a0SELECT\u00a0 \u00a0STRING(TIMESTAMP_TRUNC(nested3.ingestion_time, MINUTE, \"America/Los_Angeles\"),\"America/Los_Angeles\") AS time,\u00a0 \u00a0IFNULL(INT64(nested3.entity[\"count\"]), 0) AS vehicle_count\u00a0FROM\u00a0 \u00a0nested3\u00a0WHERE\u00a0 \u00a0JSON_VALUE(nested3.entity[\"entity\"][\"labelString\"])=\"Vehicle\" )SELECT\u00a0time,\u00a0SUM(vehicle_count)FROM\u00a0nested4GROUP BY\u00a0time\n```\n### Run your query\nAfter you format your Google Standard SQL query, you can use the console to run your query:\n- In the Google Cloud console, open the BigQuery page. [Go to BigQuery](https://console.cloud.google.com/bigquery) \n- Select arrow_drop_down **Expand** next to you dataset name, and select your table name.\n- In the table detail view, click add_box **Compose new query** . \n- Enter a Google Standard SQL query in the **Query editor** text area. For example queries, see [sample queries](#sample-queries) .\n- Optional: To change the data processing location, click **More** , then **Query settings** . Under **Processing location** , click **Auto-select** and choose your data's [location](/bigquery/docs/locations) . Finally, click **Save** to update the query settings.\n- Click **Run** .\nThis creates a query job that writes the output to a temporary table.\n## Cloud Functions integration\nYou can use Cloud Functions triggering additional data processing with your customized BigQuery ingestion. To use Cloud Functions for your customized BigQuery ingestion, do the following:\n- When using the Google Cloud console, select the corresponding cloud function from the dropdown menu of each connected model.\n- When using the Vertex AI Vision API, add one key-value pair to the `cloud_function_mapping` field of [BigQueryConfig](/vision-ai/docs/reference/rest/v1alpha1/ApplicationConfigs#bigqueryconfig) in the BigQuery node. The key is the BigQuery node name and value is the http trigger of the target function.\nTo use Cloud Functions with your customized BigQuery ingestion, the function must meet the following requirements:\n- The Cloud Functions instance has to be created before you create the BigQuery node.\n- Vertex AI Vision API expects to receive an [AppendRowsRequest](/bigquery/docs/reference/storage/rpc/google.cloud.bigquery.storage.v1#appendrowsrequest) annotation returned from Cloud Functions.\n- You must set the [proto_rows.writer_schema](/bigquery/docs/reference/storage/rpc/google.cloud.bigquery.storage.v1#protodata) field for all`CloudFunction`responses ;`write_stream`can be ignored.\n### Cloud Functions integration example\nThe following example shows how to parse occupancy count node output ( `OccupancyCountPredictionResult` ), and extract from it an `ingestion_time` , `person_count` , and `vehicle_count` table schema.\nThe result of the following sample is a BigQuery table with the schema:\n```\n[\u00a0 {\u00a0 \u00a0 \"name\": \"ingestion_time\",\u00a0 \u00a0 \"type\": \"TIMESTAMP\",\u00a0 \u00a0 \"mode\": \"REQUIRED\"\u00a0 },\u00a0 {\u00a0 \u00a0 \"name\": \"person_count\",\u00a0 \u00a0 \"type\": \"INTEGER\",\u00a0 \u00a0 \"mode\": \"NULLABLE\"\u00a0 },\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \"name\": \"vehicle_count\",\u00a0 \u00a0 \"type\": \"INTEGER\",\u00a0 \u00a0 \"mode\": \"NULLABLE\"\u00a0 },]\n```\nUse the following code to create this table:\n- Define a proto (for example, `test_table_schema.proto` ) for table fields you want to write:```\nsyntax = \"proto3\";package visionai.testing;message TestTableSchema {\u00a0 int64 ingestion_time = 1;\u00a0 int32 person_count = 2;\u00a0 int32 vehicle_count = 3;}\n```\n- Compile the proto file to generate the protocol buffer Python file:```\nprotoc -I=./ --python_out=./ ./test_table_schema.proto\n```\n- Import the generated Python file and write the cloud function.\n```\nimport base64import sysfrom flask import jsonifyimport functions_frameworkfrom google.protobuf import descriptor_pb2from google.protobuf.json_format import MessageToDictimport test_table_schema_pb2def table_schema():\u00a0 schema = descriptor_pb2.DescriptorProto()\u00a0 test_table_schema_pb2.DESCRIPTOR.message_types_by_name[\u00a0 \u00a0 \u00a0 'TestTableSchema'].CopyToProto(schema)\u00a0 return schemadef bigquery_append_row_request(row):\u00a0 append_row_request = {}\u00a0 append_row_request['protoRows'] = {\u00a0 \u00a0 \u00a0 'writerSchema': {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'protoDescriptor': MessageToDict(table_schema())\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 'rows': {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'serializedRows':\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 base64.b64encode(row.SerializeToString()).decode('utf-8')\u00a0 \u00a0 \u00a0 }\u00a0 }\u00a0 return append_row_request@functions_framework.httpdef hello_http(request):\u00a0 request_json = request.get_json(silent=False)\u00a0 annotations = []\u00a0 payloads = []\u00a0 if request_json and 'annotations' in request_json:\u00a0 \u00a0 for annotation_with_timestamp in request_json['annotations']:\u00a0 \u00a0 \u00a0 row = test_table_schema_pb2.TestTableSchema()\u00a0 \u00a0 \u00a0 row.person_count = 0\u00a0 \u00a0 \u00a0 row.vehicle_count = 0\u00a0 \u00a0 \u00a0 if 'ingestionTimeMicros' in annotation_with_timestamp:\u00a0 \u00a0 \u00a0 \u00a0 row.ingestion_time = int(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 annotation_with_timestamp['ingestionTimeMicros'])\u00a0 \u00a0 \u00a0 if 'annotation' in annotation_with_timestamp:\u00a0 \u00a0 \u00a0 \u00a0 annotation = annotation_with_timestamp['annotation']\u00a0 \u00a0 \u00a0 \u00a0 if 'stats' in annotation:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 stats = annotation['stats']\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for count in stats['fullFrameCount']:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if count['entity']['labelString'] == 'Person':\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if 'count' in count:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 row.person_count = count['count']\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 elif count['entity']['labelString'] == 'Vehicle':\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if 'count' in count:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 row.vehicle_count = count['count']\u00a0 \u00a0 \u00a0 payloads.append(bigquery_append_row_request(row))\u00a0 for payload in payloads:\u00a0 \u00a0 annotations.append({'annotation': payload})\u00a0 return jsonify(annotations=annotations)\n```\n- To include your dependencies in Cloud Functions, you must also upload the generated `test_table_schema_pb2.py` file and specify `requirements.txt` similar to the following:```\nfunctions-framework==3.*click==7.1.2cloudevents==1.2.0deprecation==2.1.0Flask==1.1.2gunicorn==20.0.4itsdangerous==1.1.0Jinja2==2.11.2MarkupSafe==1.1.1pathtools==0.1.2watchdog==1.0.2Werkzeug==1.0.1protobuf==3.12.2\n```\n- Deploy the cloud function and set the corresponding http trigger in the [BigQueryConfig](/vision-ai/docs/reference/rest/v1alpha1/ApplicationConfigs#bigqueryconfig) .", "guide": "Vertex AI Vision"}