{"title": "Vertex AI - Update and rebuild an active index", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Update and rebuild an active index\n", "content": "## Overview\nWith large search queries, updating your indexes is important to always having the most accurate information. Today you can update your Vector Search indexes using a batch update, which lets you to insert and delete data points through a batch schedule, or with streaming update, which lets you update and query your index within a few seconds.\nAdditionally, you can use `UpdateIndex` to update your important metadata fields, like `display_name` , `description` and `labels` . You can also add optional tags to your index to help with diversifying results or filtering pre index query.\n**Note:** You can invoke `UpdateIndex` to update either the index content or the metadata fields, but not both in a single invocation. For example, if you want to add data points as well as update the description, you need to with two separate invocations.\n## Update a batch index\nTo update the content of an existing `Index` , use the `IndexService.UpdateIndex` method.\nTo replace the existing content of an existing `Index` :\n- Set`Index.metadata.contentsDeltaUri`to the Cloud Storage URI that includes the vectors you want to update.\n- Set`isCompleteOverwrite`to true.\nIf you set the `contentsDeltaUri` field when calling `IndexService.UpdateIndex` , then no other index fields (such as `displayName` , `description` , or `userLabels` ) can be also updated as part of the same call.\n- [Update your index metadata file](/vertex-ai/docs/vector-search/create-manage-index#index-metadata-file) [.](None) \n- [](None) \n- [ Use the ](None) [gcloud ai indexes update command](/sdk/gcloud/reference/ai/indexes/update) .Before using any of the command data below, make the following replacements:- : The local path to the metadata file.\n- : The ID of the index.\n- : The region where you are using Vertex AI.\n- : Your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\nExecute the  following  command:Before using any of the request data, make the following replacements:- : The Cloud Storage directory path of the index content.\n- : The ID of the index.\n- : The region where you are using Vertex AI.\n- : Your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) .\nHTTP method and URL:\n```\nPATCH https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/indexes/INDEX_ID\n```\nRequest JSON body:\n```\n{\n \"metadata\": {\n \"contentsDeltaUri\": \"INPUT_DIR\",\n \"isCompleteOverwrite\": true\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION/indexes/INDEX_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.UpdateIndexOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2022-01-12T23:56:14.480948Z\",\n  \"updateTime\": \"2022-01-12T23:56:14.480948Z\"\n }\n }\n}\n```\nUse these instructions to update a batch index content.\n- In the Vertex AI section of the Google Cloud console, go to  the **Deploy and Use** section. Select **Vector Search** [Go to  Vector Search](https://console.cloud.google.com/vertex-ai/matching-engine/indexes) \n- Select the index you want to update. The **Index info** page opens.\n- Select **Edit Index** . An edit index pane opens.\n- In the Cloud Storage field, search and select the Cloud Storage  folder where your vector data is stored.\n- (Optional) Check the complete overwrite box if you want to overwrite all the existing data.\n- Click **Update** \n- Click **Done** to close out the panel.\nIf the `Index` has any associated deployments (see the `Index.deployed_indexes` field), then when certain changes to the original `Index` are done, the `DeployedIndex` is automatically updated asynchronously in the background to reflect these changes.\nTo check whether the change has been propagated, compare the update index operation finish time and the `DeployedIndex.index_sync_time` .\n## Update a streaming index\nWith streaming updates, you can update and query your index within a few seconds. At this time, you can't use streaming updates on an existing batch update index, you must create a new index. See [Create an index for streaming update](/vertex-ai/docs/vector-search/create-manage-index) to learn more.\nYou are charged $0.45 per GB used for streaming updates. To learn more about pricing, see the [Vertex AI pricing page](/vertex-ai/pricing#vectorsearch) . Streaming updates are directly applied to the deployed indexes in memory, which are then reflected in query results after a short delay.\n**Note:** After you've created an index for streaming updates, you can't perform a batch update on this index. You can perform a complete overwrite to use the index or you can create a new index prepared for batch updates. See [Create an index](/vertex-ai/docs/vector-search/create-manage-index#create-index) to learn more.\n### Upsert data points\nUse these samples to see how to upsert a data point. Remember, `upsert-datapoints` accepts JSON in array format only.\n### Python [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/vector_search/vector_search_stream_update_sample.py) \n```\ndef stream_update_vector_search_index(\u00a0 \u00a0 project: str, location: str, index_name: str, datapoints: Sequence[dict]) -> None:\u00a0 \u00a0 \"\"\"Stream update an existing vector search index\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 project (str): Required. The Project ID\u00a0 \u00a0 \u00a0 location (str): Required. The region name, e.g. \"us-central1\"\u00a0 \u00a0 \u00a0 index_name (str): Required. The index to update. A fully-qualified index\u00a0 \u00a0 \u00a0 \u00a0 resource name or a index ID. \u00a0Example:\u00a0 \u00a0 \u00a0 \u00a0 \"projects/123/locations/us-central1/indexes/my_index_id\" or\u00a0 \u00a0 \u00a0 \u00a0 \"my_index_id\".\u00a0 \u00a0 \u00a0 datapoints: Sequence[dict]: Required. The datapoints to be updated. The dict\u00a0 \u00a0 \u00a0 \u00a0 element should be of the IndexDatapoint type.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 # Initialize the Vertex AI client\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 # Create the index instance from an existing index with stream_update\u00a0 \u00a0 # enabled\u00a0 \u00a0 my_index = aiplatform.MatchingEngineIndex(index_name=index_name)\u00a0 \u00a0 # Upsert the datapoints to the index\u00a0 \u00a0 my_index.upsert_datapoints(datapoints=datapoints)\n```The throughput quota limit relates to the amount of data that is included in an upsert. If the data point ID exists in the index, the embedding is updated, otherwise, a new embedding is added.\n```\n \n DATAPOINT_ID_1=\n DATAPOINT_ID_2=\n curl -H \"Content-Type: application/json\" -H \"Authorization: Bearer `gcloud auth print-access-token`\" https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/indexes/${INDEX_ID}:upsertDatapoints \\\n -d '{datapoints: [{datapoint_id: \"'${DATAPOINT_ID_1}'\", feature_vector: [...]},\n {datapoint_id: \"'${DATAPOINT_ID_2}'\", feature_vector: [...]}]}'\n \n \n```\n### ConsoleUse these instructions to update content to a streaming index.\n- In the Vertex AI section of the Google Cloud console, go to  the **Deploy and Use** section. Select **Vector Search** [Go to  Vector Search](https://console.cloud.google.com/vertex-ai/matching-engine/indexes) \n- Select the index you want to update. The **Index info** page opens.\n- Select **Edit Index** . An edit index pane opens.\n- From the pane, select **Upsert data point** for adding content.\n- Enter the data point ID.\n- Enter the feature vector values for the data point you want to upsert. This field  needs to be comma-separated numbers (ex: 9.32, 0.12, -2.35).\n- Enter the string.\n- Click **Upsert** \n- Click **Done** to close out the panel.\nThe throughput quota limit relates to the amount of data that is included in an upsert. If the data point ID exists in the index, the embedding is updated, otherwise, a new embedding is added.\n### Update dynamic metadata\nThere are many reasons you might need to update streaming restricts or numeric restricts. For example, when dealing with high-volume, fast-moving data, you might want to prioritize certain data streams. Directly updating restricts or numeric restricts lets you refine the focus in real-time, ensuring the most important data is processed or highlighted immediately.\nYou can directly update data point restricts and numeric restricts inside a streaming index without the compaction cost of full update.\nTo perform these metadata-only updates, you need to add the field `update_mask` to the request. The value of `update_mask` must be set to `all_restricts` . The restrict and numeric restrict values set in the data points should be the new values you want to apply in the update.\nThe following example shows how to add restricts to two existing data points.\n```\nDATAPOINT_ID_1=DATAPOINT_ID_2=curl -H \"Content-Type: application/json\" -H \"Authorization: Bearer gcloud auth print-access-token\" https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/indexes/${INDEX_ID}:upsertDatapoints \\-d '{datapoints:[{datapoint_id: \"'${DATAPOINT_ID_1}'\", feature_vector: [...], \u00a0restricts:[{namespace: \"color\", allow_list: [\"red\"]}]},{datapoint_id: \"'${DATAPOINT_ID_2}'\", feature_vector: [...], \u00a0restricts:[{namespace: \"color\", allow_list: [\"red\"]}]}], update_mask: \"all_restricts\"}'\n```\n### Remove data points\nYou might need to remove data points from your streaming index. You can do this using curl or from the Google Cloud console.\nA key use case for deleting a data point from an index is to maintain parity between the index and its real-world source. Consider a bookseller who uses a vector embedding to represent their book inventory for search and recommendation purposes. When a book is sold out or removed from stock, deleting its corresponding data point from the index ensures that search results and recommendations remain accurate and up-to-date.\n```\ncurl -H \"Content-Type: application/json\" -H \"Authorization: Bearer `gcloud auth print-access-token`\" https://{LOCATION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/indexes/{INDEX_ID}:removeDatapoints -d '{datapoint_ids: [\"'{DATAPOINT_ID_1}'\", \"'{DATAPOINT_ID_2}'\"]}'\n```\n### ConsoleUse these instructions to delete a data point from streaming index.\n- In the Vertex AI section of the Google Cloud console, go to  the **Deploy and Use** section. Select **Vector Search** [Go to  Vector Search](https://console.cloud.google.com/vertex-ai/matching-engine/indexes) \n- Select the streaming index you want to update. The **Index info** page opens.\n- Select **Edit Index** . An edit index pane opens.\n- From the pane, select the **Remove data points** tab.\n- Add up to 20 data points by providing a comma delimited list of data point IDs\n- Click **Remove** .\n- Click **Done** to close out the panel.## Compaction\nPeriodically, your index is rebuilt to account for all new updates since your last rebuild. This rebuild, or \"compaction\", improves query performance and reliability. Compactions occur for both streaming updates and batch updates.\n- **Streaming update** : Occurs when the uncompacted data size is > 1 GB or the oldest uncompacted data is at least three days old. You are billed for the cost of rebuilding the index at the same rate of a batch update, in addition to the streaming update costs.\n- **Batch update** : Occurs when the incremental dataset size is > 20% of the base dataset size.\n**Note:** These compactions occur automatically, and when they occur, you receive an email to your account inbox. If you created the index with a service account, you won't receive a compaction notice email.\n## Rebuild and query your index\nYou can send match or batch match requests as usual with the grpc cli, the client library, or the Vertex AI SDK for Python. When you rebuild the query you can expect to see your updates within a few seconds. To learn how to query an index, see [Query indexes to get nearest neighbors.](/vertex-ai/docs/vector-search/query-index-public-endpoint)\n## Optional fields\nWhen you create an index, there are some optional fields you can use to fine-tune your queries.\n### Upsert with restricts\nUpserting your index and adding a restrict is a way of tagging your data points so they are already identified for filtering at query time. You might want to add restrict tags to limit the results that presents on your data before a query is sent. For example, a customer wants to run a query on an index, but wants to make sure the results only display items that match \"red\" in a search for footwear. In the following example, the index is being upserted and is filtering in all red shoes, but denying blue ones. This ensures the search filters in the best specific options from a large and varied index before running.\nIn addition to token restricts, the example uses numeric restricts. In this case, the datapoint is associated with a price of 20, length of 0.3, and width of 0.5. At the time of query, you can use these numeric restricts to filter the results to limit the query results on the values of price, length, and width. For example, this datapoint would appear in a query that filters for price > 25, length < 1, and width < 1.\nTo learn more about filtering, see [Vector Search for Indexing](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/matching_engine/matching_engine_for_indexing.ipynb) .\n```\ncurl -H \"Content-Type: application/json\" -H \"Authorization: Bearer `gcloud auth print-access-token`\" https://${ENDPOINT}/v1/projects/${PROJECT_ID}/locations/us-central1/indexes/${INDEX_ID}:upsertDatapoints \\-d '{datapoints: [\u00a0 {\u00a0 \u00a0 datapoint_id: \"'${DATAPOINT_ID_1}'\",\u00a0 \u00a0 feature_vector: [...],\u00a0 \u00a0 restricts: { namespace: \"color\", allow_list: [\"red\"], deny_list: [\"blue\"]},\u00a0 \u00a0 numeric_restricts: [{namespace: \"price\", value_int: 20}, {namespace: \"length\", value_float: 0.3}, {namespace: \"width\", value_double: 0.5}]\u00a0 }]}'\n```\n### Upsert with crowding\nThe crowding tag limits similar results by improving result diversity. Crowding is a constraint on a neighbor list produced by a nearest neighbor search requiring that no more than some value, of a group of results, return the same value of `crowding_attribute` . As an example, let's say you were back online shopping for shoes. You want to see a wide variety of colors in the results, but maybe want them in a single style, like soccer cleats. You can ask that no more than 3 pairs of shoes with the same color is returned by setting `per_crowding_attribute_num_neighbors` = 3 in your query, assuming you set crowding_attribute to the color of the shoes when inserting the data point.\nThis field represents the allowed maximum number of matches with the same crowding tag.\n```\ncurl -H \"Content-Type: application/json\" -H \"Authorization: Bearer `gcloud auth print-access-token`\" https://${ENDPOINT}/v1/projects/${PROJECT_ID}/locations/us-central1/indexes/${INDEX_ID}:upsertDatapoints \\-d '{datapoints: [\u00a0 {\u00a0 \u00a0 datapoint_id: \"'${DATAPOINT_ID_1}'\",\u00a0 \u00a0 feature_vector: [...],\u00a0 \u00a0 restricts: { namespace: \"type\", allow_list: [\"cleats\"]}\u00a0 \u00a0 crowding_tag: { crowding_attribute: \"red-shoe\"},\u00a0 }]}'\n```\n## What's next\n- Learn how to [Configure indexes](/vertex-ai/docs/vector-search/configuring-indexes) \n- Learn how to [Monitor an index](/vertex-ai/docs/vector-search/monitor)", "guide": "Vertex AI"}