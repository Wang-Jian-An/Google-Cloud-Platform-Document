{"title": "Apigee - Multi-region deployment on AKS", "url": "https://cloud.google.com/apigee/docs/api-platform/get-started/what-apigee", "abstract": "# Apigee - Multi-region deployment on AKS\nYou are currently viewing version 1.1 of the Apigee hybrid documentation. **This version is end of life.** You should upgrade to a newer version. For more information, see [Supported versions](/apigee/docs/hybrid/supported-platforms#supported-versions) .\nThis topic explains how to set up a multi-region deployment for Apigee hybrid on Microsoft\u00ae Azure Kubernetes Service (AKS).\nTopologies for multi-region deployment include the following:\n- **Active-Active** : When you have applications deployed in multiple geographic  locations and you require low latency API response  for your deployments. You have the option to deploy hybrid in multiple geographic  locations nearest to your clients. For example: US West Coast, US East Coast, Europe, APAC.\n- **Active-Passive** : When you have a primary region and a failover or disaster  recovery region.\nThe regions in a multi-region hybrid deployment communicate via Cassandra, as the following image shows:\n", "content": "## Prerequisites\nBefore configuring hybrid for multiple regions, you must complete the following prerequisites:\n- Follow the [hybrid installation guide](/apigee/docs/hybrid/v1.1/precog-overview) for any prerequisites like GCP and org configuration  before moving to cluster setup steps.\nFor detailed information, see [Kubernetes](https://kubernetes.io) documentation.\n**NOTE:** Apigee recommends that you ensure that your servers' times are synchronized.Several features such as expiration and token revocation rely on accurate system times. If you host the runtime components in different datacenters, then be sure that the system times synchronized.You can use a tool such as `ntpdate` to verify that server times are synchronized.\n## \n Create a virtual network in each region\nCreate a virtual network for the multi-region deployment. For example, the following example commands create networks in the Central US and Eastern US regions.\n**NOTE: ** The steps in this section are provided as examples only. Adapt them as needed to suit your requirements.\nExecute this command to create a virtual network in the Eastern US region, with the name `my-hybrid-rg-vnet` :\n```\naz network vnet create \\\n --name my-hybrid-rg-vnet \\\n --location eastus \\\n --resource-group my-hybrid-rg \\\n --address-prefixes 120.38.1.0/24 \\\n --subnet-name my-hybrid-rg-vnet-subnet \\\n --subnet-prefix 120.38.1.0/26\n```\nExecute this command to create a virtual network in the Central US region, with the name `my-hybrid-rg-vnet-ext01` :\n```\naz network vnet create \\\n --name my-hybrid-rg-vnet-ext01 \\\n --location centralus \\\n --resource-group my-hybrid-rg \\\n --address-prefixes 192.138.0.0/24 \\\n --subnet-name my-hybrid-rg-vnet-ext01-subnet \\\n --subnet-prefix 192.138.0.0/26\n```\n## \n Create network peering\nCreate a network peering between the virtual networks.\n**NOTE: ** The steps in this section are provided as examples only. Adapt them as needed to suit your requirements.\n### \n Get the virtual network IDs\nPeerings are established between virtual network IDs. Get the ID of each virtual network with the [ az network vnet show](https://docs.microsoft.com/en-us/cli/azure/network/vnet) command and store the ID in a variable.\nGet the ID of the first virtual network, the one named `my-hybrid-rg-vnet` :\n```\nvNet1Id=$(az network vnet show \\\n --resource-group my-hybrid-rg \\\n --name my-hybrid-rg-vnet \\\n --query id --out tsv)\n```\nGet the ID of the second virtual network, the one named `my-hybrid-rg-vnet-ext01` :\n```\nvNet2Id=$(az network vnet show \\\n --resource-group my-hybrid-rg \\\n --name my-hybrid-rg-vnet-ext01 \\\n --query id \\\n --out tsv)\n```\n### \n Create peering from the first to the second virtual network\nWith the virtual network IDs, you can create a peering from the first virtual netowrk ( `my-hybrid-rg-vnet` ) to the second ( `my-hybrid-rg-vnet-ext01` ), as shown in the following examples:\nIf you do not specify the`--allow-vnet-access`parameter, a peering is established, but no communication can flow through it.\n```\naz network vnet peering create \\\n --name my-hybrid-rg-vnet1-peering \\  # The name of the virtual network peering.\n --resource-group my-hybrid-rg \\\n --vnet-name my-hybrid-rg-vnet \\   # The virtual network name.\n --remote-vnet $vNet2Id \\    # Resource ID of the remote virtual network.\n --allow-vnet-access\n```\nIn the command's output, note that the `peeringState` is . The peering remains in the Initiated state until you create the peering from the second virtual network back to the first.\n```\n{\n ...\n \"peeringState\": \"Initiated\",\n ...\n}\n```\n### \n Create a peering from the second virtual network to the first\nExample command:\n```\naz network vnet peering create \\\n --name my-hybrid-rg-vnet2-peering \\  # The name of the virtual network peering.\n --resource-group my-hybrid-rg \\\n --vnet-name my-hybrid-rg-vnet-ext01 \\  # The virtual network name.\n --remote-vnet $vNet1Id \\     # Resource ID of the remote virtual network.\n --allow-vnet-access\n```\nIn the command's output, note that `peeringState` is . Azure also changes the peering state of the first to second virtual network peering to .\n```\n{\n ...\n \"peeringState\": \"Connected\",\n ...\n}\n```\nYou can also confirm that the peering state for the `my-hybrid-rg-vnet1-peering` to `my-hybrid-rg-vnet2-peering` : peering changed to Connected with the following command:\n```\naz network vnet peering show \\\n --name my-hybrid-rg-vnet1-peering \\\n --resource-group my-hybrid-rg \\\n --vnet-name my-hybrid-rg-vnet \\\n --query peeringState\n```\nExpected output:\n```\nConnected\n```\n## \n Create multi-regional clusters\nSet up Kubernetes clusters in multiple regions with different blocks. See also the [AKS quickstart](/apigee/docs/hybrid/v1.1/install-create-cluster-aks) . Use the locations and virtual network names you created previously.\nOpen Cassandra ports 7000 and 7001 between Kubernetes clusters across all regions (7000  may be used as a backup option during troubleshooting)\n## Configure the multi-region seed host\nThis section describes how to expand the existing Cassandra cluster to a new region. This setup allows the new region to bootstrap the cluster and join the existing data center. Without this configuration, the multi-region Kubernetes clusters would not know about each other.\n- Set the kubectl context to the original cluster before retrieving the seed name:```\nkubectl config use-context original-cluster-name\n```\n- Run the following [kubectl](https://kubernetes.io/docs/reference/kubectl/overview/) command to identify a seed host address for Cassandra in  the current region.A allows a new regional instance to find the  original cluster on the very first startup to learn the topology of the cluster.  The seed host address is designated as the contact point in the cluster.```\nkubectl get pods -o wide -n apigee | grep apigee-cassandra\napigee-cassandra-0 1/1 Running 0 4d17h 120.38.1.9 aks-agentpool-21207753-vmss000000\n```\n- Decide which of the IPs returned from the previous command will be the multi-region seed  host. In this example, where only a single node cassandra cluster is running, the seed host  is`120.38.1.9`.\n- In data center 2, copy your overrides file to a new file whose name includes the cluster  name. For example,`overrides_` `` `.yaml`.\n- In data center 2, configure`cassandra.multiRegionSeedHost`and`cassandra.datacenter`in`overrides_` `` `.yaml`, where`multiRegionSeedHost`is one of the IPs returned by the previous command:```\ncassandra:\n multiRegionSeedHost: seed_host_IP\n datacenter: data_center_name\n rack: rack_name\n```For example:```\ncassandra:\n multiRegionSeedHost: 120.38.1.9\n datacenter: \"centralus\"\n rack: \"ra-1\"\n```\n- In the new data center/region, before you install hybrid, set the same TLS certificates and  credentials in`overrides_` `` `.yaml`as you set in the first region.NOTE: Be sure to use the same Cassandra TLS certificates and credentials in the second  data center as you provided in the original data center. The credentials you set in  the overrides file in the first data center must match the ones you specify in  overrides file in the second data center. For details see [Configuring TLS for Cassandra](/apigee/docs/hybrid/v1.1/cassandra-tls) .## Set up the new region\nAfter you configure the seed host, you can set up the new region.\n**To set up the new region:**\n- Copy your certificate from the existing cluster to the new cluster. The new CA root is   used by Cassandra and other hybrid components for mTLS. Therefore, it is essential to have   consistent certificates across the cluster.- Set the context to the original namespace:```\nkubectl config use-context original-cluster-name\n```\n- Export the current namespace configuration to a file:```\n$ kubectl get namespace -o yaml > apigee-namespace.yaml\n```\n- Export the`apigee-ca`secret to a file:```\nkubectl -n cert-manager get secret apigee-ca -o yaml > apigee-ca.yaml\n```\n- Set the context to the new region's cluster name:```\nkubectl config use-context new-cluster-name\n```\n- Import the namespace configuration to the new cluster.    Be sure to update the \"namespace\" in the file if you're using a different namespace    in the new region:```\nkubectl apply -f apigee-namespace.yaml\n```\n- Import the secret to the new cluster:```\nkubectl -n cert-manager apply -f apigee-ca.yaml\n```\n- Install hybrid in the new region. Be sure that the`overrides_` `` `.yaml`file includes the same TLS certificates that are configured in the first region, as   explained in the previous section.Execute the following two commands to install hybrid in the new region:```\napigeectl init -f overrides_your_cluster_name.yaml\n``````\napigeectl apply -f overrides_your_cluster_name.yaml\n```\n- Expand all apigee keyspaces.The following steps expand the Cassandra data to the new data center:- Open a shell in the Cassandra pod:```\nkubectl run -i --tty --restart=Never --rm --image google/apigee-hybrid-cassandra-client:1.0.0 cqlsh\n``` **NOTE:** If you don't see a command prompt, try    pressing the ENTER key.\n- Connect to the Cassandra server:```\ncqlsh apigee-cassandra-0.apigee-cassandra.apigee.svc.cluster.local -u ddl_user --ssl\nPassword:\nConnected to apigeecluster at apigee-cassandra-0.apigee-cassandra.apigee.svc.cluster.local:9042.\n[cqlsh 5.0.1 | Cassandra 3.11.3 | CQL spec 3.4.4 | Native protocol v4]\nUse HELP for help.\n```\n- Get the available keyspaces:```\nSELECT * from system_schema.keyspaces ;\n keyspace_name    | durable_writes | replication\n----------------------------+----------------+-------------------------------------------------------------------------------------------------------    system_auth |   True | {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '1', 'dc-2': '1'}\n    system_schema |   True |            {'class': 'org.apache.cassandra.locator.LocalStrategy'}\n cache_hybrid_test_7_hybrid |   True |     {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '3'}\n kms_hybrid_test_7_hybrid |   True |     {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '3'}\n kvm_hybrid_test_7_hybrid |   True |     {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '3'}\n   system_distributed |   True | {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '1', 'dc-2': '1'}\n      system |   True |            {'class': 'org.apache.cassandra.locator.LocalStrategy'}\n      perses |   True |     {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '3'}\n quota_hybrid_test_7_hybrid |   True |     {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '3'}\n    system_traces |   True | {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '1', 'dc-2': '1'}\n(10 rows)\n```\n- Update/expand the apigee keyspaces:```\nALTER KEYSPACE cache_hybrid_test_7_hybrid WITH replication = {'class': 'NetworkTopologyStrategy', 'dc-1':3, 'dc-2':3};\nALTER KEYSPACE kms_hybrid_test_7_hybrid WITH replication = {'class': 'NetworkTopologyStrategy', 'dc-1':3, 'dc-2':3};\nALTER KEYSPACE kvm_hybrid_test_7_hybrid WITH replication = {'class': 'NetworkTopologyStrategy', 'dc-1':3, 'dc-2':3};\nALTER KEYSPACE perses WITH replication = {'class': 'NetworkTopologyStrategy', 'dc-1':3, 'dc-2':3};\nALTER KEYSPACE quota_hybrid_test_7_hybrid WITH replication = {'class': 'NetworkTopologyStrategy', 'dc-1':3, 'dc-2':3};\n```\n- Validate the keyspace expansion:```\nSELECT * from system_schema.keyspaces ;\n keyspace_name    | durable_writes | replication\n----------------------------+----------------+-------------------------------------------------------------------------------------------------------    system_auth |   True | {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '1', 'dc-2': '1'}\n    system_schema |   True |            {'class': 'org.apache.cassandra.locator.LocalStrategy'}\n cache_hybrid_test_7_hybrid |   True | {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '3', 'dc-2': '3'}\n kms_hybrid_test_7_hybrid |   True | {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '3', 'dc-2': '3'}\n kvm_hybrid_test_7_hybrid |   True | {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '3', 'dc-2': '3'}\n   system_distributed |   True | {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '1', 'dc-2': '1'}\n      system |   True |            {'class': 'org.apache.cassandra.locator.LocalStrategy'}\n      perses |   True | {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '3', 'dc-2': '3'}\n quota_hybrid_test_7_hybrid |   True | {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '3', 'dc-2': '3'}\n    system_traces |   True | {'class': 'org.apache.cassandra.locator.NetworkTopologyStrategy', 'dc-1': '1', 'dc-2': '1'}\n(10 rows)\nddl@cqlsh>\n```- Run`nodetool rebuild`sequentially on all the nodes in new data center. This may  take a few minutes to a few hours depending on the data size.```\nkubectl exec apigee-cassandra-0 -n apigee -- nodetool rebuild -- dc-1\n```\n- Verify the rebuild processes from the logs. Also, verify the data size  using the`nodetool status`command:```\nkubectl logs apigee-cassandra-0 -f -n apigee\n```The following example shows example log entries:```\nINFO \u00a001:42:24 rebuild from dc: dc-1, (All keyspaces), (All tokens)INFO \u00a001:42:24 [Stream #3a04e810-580d-11e9-a5aa-67071bf82889] Executing streaming plan for RebuildINFO \u00a001:42:24 [Stream #3a04e810-580d-11e9-a5aa-67071bf82889] Starting streaming to /10.12.1.45INFO \u00a001:42:25 [Stream #3a04e810-580d-11e9-a5aa-67071bf82889, ID#0] Beginning stream session with /10.12.1.45INFO \u00a001:42:25 [Stream #3a04e810-580d-11e9-a5aa-67071bf82889] Starting streaming to /10.12.4.36INFO \u00a001:42:25 [Stream #3a04e810-580d-11e9-a5aa-67071bf82889 ID#0] Prepare completed. Receiving 1 files(0.432KiB), sending 0 files(0.000KiB)INFO \u00a001:42:25 [Stream #3a04e810-580d-11e9-a5aa-67071bf82889] Session with /10.12.1.45 is completeINFO \u00a001:42:25 [Stream #3a04e810-580d-11e9-a5aa-67071bf82889, ID#0] Beginning stream session with /10.12.4.36INFO \u00a001:42:25 [Stream #3a04e810-580d-11e9-a5aa-67071bf82889] Starting streaming to /10.12.5.22INFO \u00a001:42:26 [Stream #3a04e810-580d-11e9-a5aa-67071bf82889 ID#0] Prepare completed. Receiving 1 files(0.693KiB), sending 0 files(0.000KiB)INFO \u00a001:42:26 [Stream #3a04e810-580d-11e9-a5aa-67071bf82889] Session with /10.12.4.36 is completeINFO \u00a001:42:26 [Stream #3a04e810-580d-11e9-a5aa-67071bf82889, ID#0] Beginning stream session with /10.12.5.22INFO \u00a001:42:26 [Stream #3a04e810-580d-11e9-a5aa-67071bf82889 ID#0] Prepare completed. Receiving 3 files(0.720KiB), sending 0 files(0.000KiB)INFO \u00a001:42:26 [Stream #3a04e810-580d-11e9-a5aa-67071bf82889] Session with /10.12.5.22 is completeINFO \u00a001:42:26 [Stream #3a04e810-580d-11e9-a5aa-67071bf82889] All sessions completed\n```\n- Update the seed hosts. Remove`multiRegionSeedHost: 10.0.0.11`from`overrides-` `` `.yaml`and reapply.Seed hosts are local cluster members. To boot up a   new region an external seed host is required. Once a region boots up you need   to change the seed hosts back to their local clusters in`overrides.yaml`and then   reapply the configuration.```\napigeectl apply -f overrides-DC_name.yaml\n```", "guide": "Apigee"}