{"title": "Cloud Architecture Center - Design secure deployment pipelines", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Design secure deployment pipelines\nLast reviewed 2023-09-28 UTC\nA deployment pipeline is an automated process that takes code or prebuilt artifacts and deploys them to a test environment or a production environment. Deployment pipelines are commonly used to deploy applications, configuration, or cloud infrastructure (infrastructure as code), and they can play an important role in the overall security posture of a cloud deployment.\nThis guide is intended for DevOps and security engineers and describes best practices for designing secure deployment pipelines based on your confidentiality, integrity, and availability requirements.\n", "content": "## Architecture\nThe following diagram shows the flow of data in a deployment pipeline. It illustrates how you can turn your artifacts into resources.\nDeployment pipelines are often part of a larger continuous integration/continuous deployment (CI/CD) workflow and are typically implemented using one of the following models:\n- **Push model:** In this model, you implement the deployment pipeline using a central CI/CD system such as [Jenkins](https://www.jenkins.io/doc/) or [GitLab](https://docs.gitlab.com/ee/ci/) . This CI/CD system might run on Google Cloud, on-premises, or on a different cloud environment. Often, the same CI/CD system is used to manage multiple deployment pipelines.The push model leads to a centralized architecture with a few CI/CD systems that are used for managing a potentially large number of resources or applications. For example, you might use a single Jenkins or GitLab instance to manage your entire production environment, including all its projects and applications.\n- **Pull model:** In this model, the deployment process is implemented by an agent that is deployed alongside the resource\u2013for example, in the same [Kubernetes](/kubernetes-engine) cluster. The agent pulls artifacts or source code from a centralized location, and deploys them locally. Each agent manages one or two resources.The pull model leads to a more decentralized architecture with a potentially large number of single-purpose agents.\nCompared to manual deployments, consistently using deployment pipelines can have the following benefits:\n- Increased efficiency, because no manual work is required.\n- Increased reliability, because the process is fully automated and repeatable.\n- Increased traceability, because you can trace all deployments to changes in code or to input artifacts.\nTo perform, a deployment pipeline requires access to the resources it manages:\n- A pipeline that deploys infrastructure by using tools like [Terraform](https://www.terraform.io/intro) might need to create, modify, or even delete resources like VM instances, subnets, or Cloud Storage buckets.\n- A pipeline that deploys applications might need to upload new container images to Artifact Registry, and deploy new application versions to [App Engine](/appengine/docs) , [Cloud Run](/run/docs) , or [Google Kubernetes Engine (GKE)](/kubernetes-engine/docs) .\n- A pipeline that manages settings or deploys configuration files might need to modify VM instance metadata, Kubernetes configurations, or modify data in [Cloud Storage](/storage/docs) .\nIf your deployment pipelines aren't properly secured, their access to Google Cloud resources can become a weak spot in your security posture. Weakened security can lead to several kinds of attacks, including the following:\n- **Pipeline poisoning attacks:** Instead of attacking a resource directly, a bad actor might attempt to compromise the deployment pipeline, its configuration, or its underlying infrastructure. Taking advantage of the pipeline's access to Google Cloud, the bad actor could make the pipeline perform malicious actions on Cloud resources, as shown in the following diagram:\n- **Supply chain attacks:** Instead of attacking the deployment pipeline, a bad actor might attempt to compromise or replace pipeline input\u2014including source code, libraries, or container images, as shown in the following diagram:\nTo determine whether your deployment pipelines are appropriately secured, it's insufficient to look only at the allow policies and deny policies of Google Cloud resources in isolation. Instead, you must consider the entire graph of systems that directly or indirectly grant access to a resource. This graph includes the following information:\n- The deployment pipeline, its underlying CI/CD system, and its underlying infrastructure\n- The source code repository, its underlying servers, and its underlying infrastructure\n- Input artifacts, their storage locations, and their underlying infrastructure\n- Systems that produce the input artifacts, and their underlying infrastructure\nComplex input graphs make it difficult to identify user access to resources and systemic weaknesses.\nThe following sections describe best practices for designing deployment pipelines in a way that helps you manage the size of the graph, and reduce the risk of lateral movement and supply chain attacks.\n## Assess security objectives\nYour resources on Google Cloud are likely to vary in how sensitive they are. Some resources might be highly sensitive because they're business critical or confidential. Other resources might be less sensitive because they're ephemeral or only intended for testing purposes.\nTo design a secure deployment pipeline, you must first understand the resources the pipeline needs to access, and how sensitive these resources are. The more sensitive your resources, the more you should focus on securing the pipeline.\nThe resources accessed by deployment pipelines might include:\n- Applications, such as Cloud Run or App Engine\n- Cloud resources, such as VM instances or Cloud Storage buckets\n- Data, such as Cloud Storage objects, BigQuery records, or files\nSome of these resources might have dependencies on other resources, for example:\n- Applications might access data, cloud resources, and other applications.\n- Cloud resources, such as VM instances or Cloud Storage buckets, might contain applications or data.\nAs shown in the preceding diagram, dependencies affect how sensitive a resource is. For example, if you use an application that accesses highly sensitive data, typically you should treat that application as highly sensitive. Similarly, if a cloud resource like a Cloud Storage bucket contains sensitive data, then you typically should treat the bucket as sensitive.\nBecause of these dependencies, it's best to first assess the sensitivity of your data. Once you've assessed your data, you can examine the dependency chain and assess the sensitivity of your Cloud resources and applications.\n### Categorize the sensitivity of your data\nTo understand the sensitivity of the data in your deployment pipeline, consider the following three objectives:\n- **Confidentiality** : You must protect the data from unauthorized access.\n- **Integrity** : You must protect the data against unauthorized modification or deletion.\n- **Availability** : You must ensure that authorized people and systems can access the data in your deployment pipeline.\nFor each of these objectives, ask yourself what would happen if your pipeline was breached:\n- **Confidentiality:** How damaging would it be if data was disclosed to a bad actor, or leaked to the public?\n- **Integrity:** How damaging would it be if data was modified or deleted by a bad actor?\n- **Availability:** How damaging would it be if a bad actor disrupted your data access?\nTo make the results comparable across resources, it's useful to introduce security categories. [Standards for Security Categorization (FIPS-199)](https://nvlpubs.nist.gov/nistpubs/fips/nist.fips.199.pdf) suggests using the following four categories:\n- **High:** Damage would be severe or catastrophic\n- **Moderate:** Damage would be serious\n- **Low:** Damage would be limited\n- **Not applicable:** The standard doesn't apply\nDepending on your environment and context, a different set of categories could be more appropriate.\nThe confidentiality and integrity of pipeline data exist on a spectrum, based on the security categories just discussed. The following subsections contain examples of resources with different confidentiality and integrity measurements:\nThe following resource examples all have low confidentiality:\n- **Low integrity:** Test data\n- **Moderate integrity:** Public web server content, policy constraints for your organization\n- **High integrity:** Container images, disk images, application configurations, access policies (allow and deny lists), liens, access-level dataThe following resource examples all have medium confidentiality:\n- **Low integrity:** Internal web server content\n- **Moderate integrity:** Audit logs\n- **High integrity:** Application configuration filesThe following resource examples all have high confidentiality:\n- **Low integrity:** Usage data and personally identifiable information\n- **Moderate integrity:** Secrets\n- **High integrity:** Financial data, KMS keys\n### Categorize applications based on the data that they access\nWhen an application accesses sensitive data, the application and the deployment pipeline that manages the application can also become sensitive. To qualify that sensitivity, look at the data that the application and the pipeline need to access.\nOnce you've identified [and categorized all data accessed by an application](#categorize-the-sensitivity-of-your-data) , you can use the following categories to initially categorize the application before you design a secure deployment pipeline:\n- **Confidentiality** : Highest category of any data accessed\n- **Integrity** : Highest category of any data accessed\n- **Availability** : Highest category of any data accessed\nThis initial assessment provides guidance, but there might be additional factors to consider\u2014for example:\n- Two sets of data might have low-confidentiality in isolation. But when combined, they could reveal new insights. If an application has access to both sets of data, you might need to categorize it as medium- or high-confidentiality.\n- If an application has access to high-integrity data, then you should typically categorize the application as high-integrity. But if that access is read only, a categorization of high-integrity might be too strict.\nFor details on a formalized approach to categorize applications, see [Guide for Mapping Types of Information and Information Systems to Security Categories (NIST SP 800-60 Vol. 2 Rev1)](https://csrc.nist.gov/publications/detail/sp/800-60/vol-2-rev-1/final) .\n### Categorize cloud resources based on the data and applications they host\nAny data or application that you deploy on Google Cloud is hosted by a Google Cloud resource:\n- An application might be hosted by an App Engine service, a VM instance, or a GKE cluster.\n- Your data might be hosted by a persistent disk, a Cloud Storage bucket, or a BigQuery dataset.\nWhen a cloud resource hosts sensitive data or applications, the resource and the deployment pipeline that manages the resource can also become sensitive. For example, you should consider a Cloud Run service and its deployment pipeline to be as sensitive as the application that it's hosting.\nAfter [categorizing your data](#categorize-the-sensitivity-of-your-data) and [your applications](#categorize-applications-based-on-the-data-that-they-access) , create an initial security category for the application. To do so, determine a level from the following categories:\n- **Confidentiality** : Highest category of any data or application hosted\n- **Integrity** : Highest category of any data or application hosted\n- **Availability** : Highest category of any data or application hosted\nWhen making your initial assessment, don't be too strict\u2014for example:\n- If you encrypt highly confidential data, treat the encryption key as highly confidential. But, you can use a lower security category for the resource containing the data.\n- If you store redundant copies of data, or run redundant instances of the same applications across multiple resources, you can make the category of the resource lower than the category of the data or application it hosts.\n### Constrain the use of deployment pipelines\nIf your deployment pipeline needs to access sensitive Google Cloud resources, you must consider its security posture. The more sensitive the resources, the better you need to attempt to secure the pipeline. However, you might encounter the following practical limitations:\n- When using existing infrastructure or an existing CI/CD system, that infrastructure might constrain the security level you can realistically achieve. For example, your CI/CD system might only support a limited set of security controls, or it might be running on infrastructure that you consider less secure than some of your production environments.\n- When setting up new infrastructure and systems to run your deployment pipeline, securing all components in a way that meets your most stringent security requirements might not be cost effective.\nTo deal with these limitations, it can be useful to set constraints on what scenarios should and shouldn't use deployment pipelines and a particular CI/CD system. For example, the most sensitive deployments are often better handled outside of a deployment pipeline. These deployments could be manual, using a privileged session management system or a privileged access management system, or something else, like tool proxies.\nTo set your constraints, define which access controls you want to enforce based on your resource categories. Consider the guidance offered in the following table:\n| Category of resource | Access controls           |\n|:-----------------------|:---------------------------------------------------------|\n| Low     | No approval required          |\n| Moderate    | Team lead must approve         |\n| High     | Multiple leads must approve and actions must be recorded |\nContrast these requirements with the capabilities of your source code management (SCM) and CI/CD systems by asking the following questions and others:\n- Do your SCM or CI/CD systems support necessary access controls and approval mechanisms?\n- Are the controls protected from being subverted if bad actors attack the underlying infrastructure?\n- Is the configuration that defines the controls appropriately secured?\nDepending on the capabilities and limitations imposed by your SCM or CI/CD systems, you can then define your data and application constraints for your deployment pipelines. Consider the guidance offered in the following table:\n| Category of resource | Constraints                              |\n|:-----------------------|:---------------------------------------------------------------------------------------------------------------------------------|\n| Low     | Deployment pipelines can be used, and developers can self-approve deployments.             |\n| Moderate    | Deployment pipelines can be used, but a team lead has to approve every commit and deployment.         |\n| High     | Don't use deployment pipelines. Instead, administrators have to use a privileged access management system and session recording. |\n## Maintain resource availability\nUsing a deployment pipeline to manage resources can impact the availability of those resources and can introduce new risks:\n- **Causing outages:** A deployment pipeline might push faulty code or configuration files, causing a previously working system to break, or data to become unusable.\n- **Prolonging outages:** To fix an outage, you might need to rerun a deployment pipeline. If the deployment pipeline is broken or unavailable for other reasons, that could prolong the outage.\nA pipeline that can cause or prolong outages poses a denial of service risk: A bad actor might use the deployment pipeline to intentionally cause an outage.\n### Create emergency access procedures\nWhen a deployment pipeline is the only way to deploy or configure an application or resource, pipeline availability can become critical. In extreme cases, where a deployment pipeline is the only way to manage a business-critical application, you might also need to consider the deployment pipeline business-critical.\nBecause deployment pipelines are often made from multiple systems and tools, maintaining a high level of availability can be difficult or uneconomical.\nYou can reduce the influence of deployment pipelines on availability by creating emergency access procedures. For example, create an alternative access path that can be used if the deployment pipeline isn't operational.\nCreating an emergency access procedure typically requires most of the following processes:\n- Maintain one of more user accounts with privileged access to relevant Google Cloud resources.\n- Store the credentials of emergency-access user accounts in a safe location, or use a privileged access management system to broker access.\n- Establish a procedure that authorized employees can follow to access the credentials.\n- Audit and review the use of emergency-access user accounts.\n### Ensure that input artifacts meet your availability demands\nDeployment pipelines typically need to download source code from a central source code repository before they can perform a deployment. If the source code repository isn't available, running the deployment pipeline is likely to fail.\nMany deployment pipelines also depend on third-party artifacts. Such artifacts might include libraries from sources such as npm, Maven Central, or the NuGet Gallery, as well as container base images, and `.deb` , and `.rpm` packages. If one of the third-party sources is unavailable, running the deployment pipeline might fail.\nTo maintain a certain level of availability, you must ensure that the input artifacts of your deployment pipeline all meet the same or higher availability requirements. The following list can help you ensure the availability of input artifacts:\n- Limit the number of sources for input artifacts, particularly third-party sources\n- Maintain a cache of input artifacts that deployment pipelines can use if source systems are unavailable\n### Treat deployment pipelines and their infrastructure like production systems\nDeployment pipelines often serve as the connective tissue between development, staging, and production environments. Depending on the environment, they might implement multiple stages:\n- In the first stage, the deployment pipeline updates a development environment.\n- In the next stage, the deployment pipeline updates a staging environment.\n- In the final stage, the deployment pipeline updates the production environment.\nWhen using a deployment pipeline across multiple environments, ensure that the pipeline meets the availability demands of each environment. Because production environments typically have the highest availability demands, you should treat the deployment pipeline and its underlying infrastructure like a production system. In other words, apply the same access control, security, and quality standards to the infrastructure running your deployment pipelines as you do for your production systems.\n### Limit the scope of deployment pipelines\nThe more resources that a deployment pipeline can access, the more damage it can possibly cause if compromised. A compromised deployment pipeline that has access to multiple projects or even your entire organization could, in the worst case, possibly cause lasting damage to all your data and applications on Google Cloud.\nTo help avoid this worst-case scenario, limit the scope of your deployment pipelines. Define the scope of each deployment pipeline so it only needs access to a relatively small number of resources on Google Cloud:\n- Instead of granting access on the project level, only grant deployment pipelines access to individual resources.\n- Avoid granting access to resources across multiple Google Cloud projects.\n- Split deployment pipelines into multiple stages if they need access to multiple projects or environments. Then, secure the stages individually.## Maintain confidentiality\nA deployment pipeline must maintain the confidentiality of the data it manages. One of the primary risks related to confidentiality is data exfiltration.\nThere are multiple ways in which a bad actor might attempt to use a deployment pipeline to exfiltrate data from your Google Cloud resources. These ways include:\n- **Direct:** A bad actor might modify the deployment pipeline or its configuration so that it extracts data from your Google Cloud resources and then copies it elsewhere.\n- **Indirect:** A bad actor might use the deployment pipeline to deploy compromised code, which then steals data from your Google Cloud environment.\nYou can reduce confidentiality risks by minimizing access to confidential resources. Removing all access to confidential resources might not be practical, however. Therefore, you must design your deployment pipeline to meet the confidentiality demands of the resources it manages. To determine these demands, you can use the following approach:\n- Determine the data, applications, and resources the deployment pipeline needs to access, and categorize them.\n- Find the resource with the highest confidentiality category and use it as an initial category for the deployment pipeline.\nSimilar to the categorization process for applications and cloud resources, this initial assessment isn't always appropriate. For example, you might use a deployment pipeline to create resources that will eventually contain highly confidential information. If you restrict the deployment pipeline so that it can create\u2013but can't read\u2013these resources, then a lower confidentiality category might be sufficient.\nTo maintain confidentiality, the [Bell\u2013LaPadula model](https://en.wikipedia.org/wiki/Bell%E2%80%93LaPadula_model) suggests that a deployment pipeline must not:\n- Consume input artifacts of higher confidentiality\n- Write data to a resource of lower confidentialityAccording to the Bell\u2013LaPadula model, the preceding diagram shows how data should flow in the pipeline to help ensure data confidentiality.\n### Don't let deployment pipelines read data they don't need\nDeployment pipelines often don't need access to data, but they might still have it. Such over-granting of access can result from:\n- Granting incorrect access permissions. A deployment pipeline might be granted access to Cloud Storage on the project level, for example. As a result, the deployment pipeline can access all Cloud Storage buckets in the project, although access to a single bucket might be sufficient.\n- Using an overly permissive role. A deployment pipeline might be granted a role that provides full access to Cloud Storage, for example. However, the permission to create new buckets would suffice.\nThe more data that a pipeline can access, the higher the risk that someone or something can steal your data. To help minimize this risk, avoid granting deployment pipelines access to any data that they don't need. Many deployment pipelines don't need data access at all, because their sole purpose is to manage configuration or software deployments.\n### Don't let deployment pipelines write to locations they don't need\nTo remove data, a bad actor needs access and a way to transfer the data out of your environment. The more storage and network locations a deployment pipeline can send data to, the more likely it is that a bad actor can use one of those locations for exfiltration.\nYou can help reduce risk by limiting the number of network and storage locations where a pipeline can send data:\n- Revoke write access to resources that the pipeline doesn't need, even if the resources don't contain any confidential data.\n- Block internet access, or restrict connections, to an allow-listed set of network locations.\nRestricting outbound access is particularly important for pipelines that you've categorized as moderately confidential or highly confidential because they have access to confidential data or cryptographic key material.\n### Use VPC Service Controls to help prevent compromised deployments from stealing data\nInstead of letting the deployment pipeline perform data exfiltration, a bad actor might attempt to use the deployment pipeline to deploy compromised code. That compromised code can then steal data from within your Google Cloud environment.\nYou can help reduce the risk of such data-theft threats by using [VPC Service Controls](/vpc-service-controls/docs) . VPC Service Controls let you restrict the set of resources and APIs that can be accessed from within certain Google Cloud projects.\n## Maintain integrity\nTo keep your Google Cloud environment secure, you must protect its integrity. This includes:\n- Preventing unauthorized modification or deletion of data or configuration\n- Preventing untrusted code or configuration from being deployed\n- Ensuring that all changes leave a clear audit trail\nDeployment pipelines can help you maintain the integrity of your environment by letting you:\n- Implement approval processes\u2014for example, in the form of code reviews\n- Enforce a consistent process for all configuration or code changes\n- Run automated tests or quick checks before each deployment\nTo be effective, you must try to ensure that bad actors can't undermine or sidestep these measures. To prevent such activity, you must protect the integrity of:\n- The deployment pipeline and its configuration\n- The underlying infrastructure\n- All inputs consumed by the deployment pipeline\nTo prevent the deployment pipeline from becoming vulnerable, try to ensure that the integrity standards of the deployment pipeline match or exceed the integrity demands of the resources it manages. To determine these demands, you can use the following approach:\n- Determine the data, applications, and resources the deployment pipeline needs to access, and categorize them.\n- Find the resource with the highest integrity category and use it as the category for the deployment pipeline.\nTo maintain the integrity of the deployment pipeline, the [Biba model](https://en.wikipedia.org/wiki/Biba_Model) suggests that:\n- The deployment pipeline must not consume input artifacts of lower integrity.\n- The deployment pipeline must not write data to a resource of higher integrity.According to the Biba model, the preceding diagram shows how data should flow in the pipeline to help ensure data integrity.\n### Verify the authenticity of input artifacts\nMany deployment pipelines consume artifacts from third-party sources. Such artifacts might include:\n- Docker base images\n- `.rpm`or`.deb`packages\n- Maven,`.npm`, or NuGet libraries\nA bad actor might attempt to modify your deployment pipeline so that it uses compromised versions of third-party artifacts by:\n- Compromising the repository that stores the artifacts\n- Modifying the deployment pipeline's configuration to use a different source repository\n- Uploading malicious packages with similar names, or names that contain typos\nMany package managers let you verify the authenticity of a package by supporting code-signing. For example, you can use PGP to sign RPM and Maven packages. You can use Authenticode to sign NuGet packages.\nYou can use code-signing to reduce the risk of falling victim to compromised third-party packages by:\n- Requiring that all third-party artifacts are signed\n- Maintaining a curated list of trusted publisher certificates or public keys\n- Letting the deployment pipeline verify the signature of third-party artifacts against the trusted publishers list\nAlternatively, you can verify the hashes of artifacts. You can use this approach for artifacts that don't support code-signing and change infrequently.\n### Ensure that underlying infrastructure meets your integrity demands\nInstead of compromising the deployment pipeline itself, bad actors might attempt to compromise its infrastructure, including:\n- The CI/CD software that runs the deployment pipeline\n- The tools used by the pipeline\u2014for example, Terraform, kubectl, or Docker\n- The operating system and all its components\nBecause the infrastructure that underlies deployment pipelines is often complex and might contain components from various vendors or sources, this type of security breach can be difficult to detect.\nYou can help reduce the risk of compromised infrastructure by:\n- Holding the infrastructure and all its components to the same integrity standards as the deployment pipeline and the Google Cloud resources that it manages\n- Making sure tools come from a trusted source and verifying their authenticity\n- Regularly rebuilding infrastructure from scratch\n- Running the deployment pipeline on [shielded VMs](/shielded-vm) \n### Apply integrity controls in the pipeline\nWhile bad actors are a threat, they aren't the only possible source of software or configuration changes that can impair the integrity of your Google Cloud environment. Such changes can also originate from developers and simply be accidental, due to unawareness, or the result of typos and other mistakes.\nYou can help reduce the risk of inadvertently applying risky changes by configuring deployment pipelines to apply additional integrity controls. Such controls can include:\n- Performing static analysis of code and configuration\n- Requiring all changes to pass a set of rules (policy as code)\n- Limiting the number of changes that can be done at the same time## What's next\n- Learn about our best practices for [using service accounts in deploymentpipelines](/iam/docs/best-practices-for-using-service-accounts-in-deployment-pipelines) .\n- Review our [best practices for securing service accounts](/iam/docs/best-practices-for-securing-service-accounts) .\n- Learn more about [Investigating and responding to threats](/security-command-center/docs/how-to-investigate-threats) .\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}