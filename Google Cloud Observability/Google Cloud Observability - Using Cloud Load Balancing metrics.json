{"title": "Google Cloud Observability - Using Cloud Load Balancing metrics", "url": "https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring/sli-metrics/lb-metrics", "abstract": "# Google Cloud Observability - Using Cloud Load Balancing metrics\nThis page reviews the types of load balancers available from [Cloud Load Balancing](/load-balancing/docs/load-balancing-overview) and describes how to use the Cloud Monitoring metrics exposed by them as SLIs.\nCloud Load Balancing services often provide the first entry point for applications hosted in Google Cloud. Load balancers are automatically instrumented to provide information about traffic, availability, and latency of the Google Cloud services that they expose; therefore, load balancers often act as an excellent source of SLI metrics without the need for application instrumentation.\nWhen getting started, you might choose to focus on availability and latency as the primary dimensions of reliability and create SLIs and SLOs to measure and alert on those dimensions. This page provides implementation examples.\nFor additional information, see the following:\n- [Concepts in service monitoring](/stackdriver/docs/solutions/slo-monitoring) \n- Documentation for [Cloud Load Balancing](/load-balancing/docs/load-balancing-overview) \n- List of [loadbalancing.googleapis.com metric types](https://cloud.google.com/monitoring/api/metrics_gcp#gcp-loadbalancing) \n**Note:** The filter strings in some of these examples have been line-wrapped for readability.\n", "content": "## Availability SLIs and SLOs\nFor non-UDP applications, a [request-based](/stackdriver/docs/solutions/slo-monitoring#slo-type-request) availability SLI is the most appropriate, since service interactions map neatly to requests.\nYou express a request-based availability SLI by using the [TimeSeriesRatio](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#TimeSeriesRatio) structure to set up a ratio of good requests to total requests, as shown in the following availability examples. To arrive at your preferred determination of \"good\" or \"valid\", you filter the metric using its available labels.\n### External layer 7 (HTTP/S) load balancer\nHTTP/S load balancers are used to expose applications that are accessed over HTTP/S and to distribute traffic to resources located in multiple regions.\n[External Application Load Balancer](/load-balancing/docs/https/ext-http-lb-simple) s write metric data to Monitoring using the [https_lb_rule](/monitoring/api/resources#tag_http_lb_rule) monitored-resource type and metric types with the prefix `loadbalancing.googleapis.com` . The metric type that is most relevant to availability SLOs is [https/request_count](/monitoring/api/metrics_gcp#loadbalancing/https/request_count) , which you can filter by using the `response_code_class` metric label.\nIf you choose to not count those requests that result in a 4xx error response code as \"valid\" because they might indicate client errors, rather than service or application errors, you can write the filter for \"total\" like this:\n```\n\"totalServiceFilter\":\u00a0 \"metric.type=\\\"loadbalancing.googleapis.com/https/request_count\\\"\u00a0 \u00a0resource.type=\\\"https_lb_rule\\\"\u00a0 \u00a0resource.label.\\\"url_map_name\\\"=\\\"my-app-lb\\\"\u00a0 \u00a0metric.label.\\\"response_code_class\\\"!=\\\"400\\\"\",\n```\nYou also need to determine how to count \"good\" requests. For example, if you choose to count only those that return a 200 OK success status response code, you can write the filter for \"good\" like this:\n```\n\"goodServiceFilter\":\u00a0 \"metric.type=\\\"loadbalancing.googleapis.com/https/request_count\\\"\u00a0 \u00a0resource.type=\\\"https_lb_rule\\\"\u00a0 \u00a0resource.label.\\\"url_map_name\\\"=\\\"my-app-lb\\\"\u00a0 \u00a0metric.label.\\\"response_code_class\\\"=\\\"200\\\"\",\n```\nYou can then express a request-based SLI like this:\n```\n\"serviceLevelIndicator\": {\u00a0 \"requestBased\": {\u00a0 \u00a0 \"goodTotalRatio\": {\u00a0 \u00a0 \u00a0 \"totalServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"loadbalancing.googleapis.com/https/request_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"https_lb_rule\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"url_map_name\\\"=\\\"my-app-lb\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"response_code_class\\\"!=\\\"400\\\"\",\u00a0 \u00a0 \u00a0 \"goodServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"loadbalancing.googleapis.com/https/request_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"https_lb_rule\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"url_map_name\\\"=\\\"my-app-lb\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"response_code_class\\\"=\\\"200\\\"\",\u00a0 \u00a0 }\u00a0 }},\n```\nFor applications where traffic is served by multiple backends, you might choose to define SLIs for a specific backend. To create an availability SLI for a specific backend, use the [https/backend_request_count](/monitoring/api/metrics_gcp#loadbalancing/https/backend_request_count) metric with the `backend_target_name` resource label in your filters, as shown in this example:\n```\n\"serviceLevelIndicator\": {\u00a0 \"requestBased\": {\u00a0 \u00a0 \"goodTotalRatio\": {\u00a0 \u00a0 \u00a0 \"totalServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"loadbalancing.googleapis.com/https/backend_request_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"https_lb_rule\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"url_map_name\\\"=\\\"my-app-lb\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"backend_target_name\\\"=\\\"my-app-backend\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"response_code_class\\\"!=\\\"400\\\"\",\u00a0 \u00a0 \u00a0 \"goodServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"loadbalancing.googleapis.com/https/backend_request_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"https_lb_rule\\\" resource.label.\\\"url_map_name\\\"=\\\"my-app-lb\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"backend_target_name\\\"=\\\"my-app-backend\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"response_code_class\\\"=\\\"200\\\"\",\u00a0 \u00a0 }\u00a0 }}\n```\n### Internal layer 7 (HTTP/S) load balancer\n[Internal Application Load Balancer](/load-balancing/docs/l7-internal) s write metric data to Monitoring using the [internal_http_lb_rule](/monitoring/api/resources#tag_internal_http_lb_rule) monitored-resource type and metric types with the prefix `loadbalancing.googleapis.com` . The metric type that is most relevant to availability SLOs is [https/internal_request_count](/monitoring/api/metrics_gcp#loadbalancing/https/internal_request_count) , which you can filter by using the `response_code_class` metric label.\nThe following shows an example of a request-based availability SLI:\n```\n\"serviceLevelIndicator\": {\u00a0 \"requestBased\": {\u00a0 \u00a0 \"goodTotalRatio\": {\u00a0 \u00a0 \u00a0 \"totalServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"loadbalancing.googleapis.com/https/internal/request_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"internal_http_lb_rule\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"url_map_name\\\"=\\\"my-internal-lb\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"response_code_class\\\"!=\\\"400\\\"\",\u00a0 \u00a0 \u00a0 \"goodServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"metric.type=\\\"loadbalancing.googleapis.com/https/internal/request_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resource.type=\\\"internal_http_lb_rule\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resource.label.\\\"url_map_name\\\"=\\\"my-internal-lb\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 metric.label.\\\"response_code_class\\\"=\\\"200\\\"\",\u00a0 \u00a0 }\u00a0 }},\n```\n### Layer 3 (TCP) load balancers\nTCP load balancers don't provide request metrics because the applications that use these might not be based on the request-response model. None of the `loadbalancing.googleapis.com` metrics provided by these load balancers lend themselves to good availability SLIs.\nTo create availability SLIs for these load balancers, you must create custom or logs-based metrics. For more information, see [Using custom metrics](/monitoring/custom-metrics) or [Using logs-based metrics](/logging/docs/logs-based-metrics) .\n## Latency SLIs and SLOs\nFor request-response applications, there are two ways to write latency SLOs:\n- As request-based SLOs.\n- As window-based SLOs.\n### Request-based latency SLOs\nA request-based SLO applies a latency threshold and counts the fraction of requests that complete under the threshold within a given compliance window. An example of a request-based SLO is \"99% of requests complete in under 100 ms within a rolling one-hour window\".\nYou express a request-based latency SLI by using a [DistributionCut](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#DistributionCut) structure, as shown in the following latency examples.\nA single request-based SLO can't capture both typical performance and the degradation of user experience, where the \"tail\" or slowest requests see increasingly longer response times. A SLO for typical performance doesn't support understanding tail latency. For a discussion of tail latency, see the section \"Worrying About Your Tail\" in [Chapter 6: Monitoring Distributed Systems](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/) of [Site Reliability Engineering](https://landing.google.com/sre/sre-book/toc/) .\nTo mitigate this limitation, you can write a second SLO to focus specifically on tail latency, for example, \"99.9% of requests complete in under 1000 ms over a rolling 1 hour window\". The combination of the two SLOs capture degradations in both typical user experience and tail latency.\n### Window-based latency SLOs\nA window-based SLO defines a goodness criterion for time period of measurements and computes the ratio of \"good\" intervals to the total number of intervals. An example of a window-based SLO is \"The 95th percentile latency metric is less than 100 ms for at least 99% of one-minute windows, over a 28-day rolling window\":\n- A \"good\" measurement period is a one-minute span in which 95% of the requests have latency under 100 ms.\n- The measure of compliance is the fraction of such \"good\" periods. The service is compliant if this fraction is at least 0.99, calculated over the compliance period.\nYou must use a window-based SLO if the raw metric available to you is a latency percentile; that is, when both of the following are true:\n- The data is bucketed into time periods (for example, into one-minute intervals).\n- The data is expressed in percentile groups (for example, p50, p90, p95, p99).\nFor this kind of data, each percentile group indicates the time dividing the data groups above and below that percentile. For example, a one-minute interval with a p95 latency metric of 89 ms tells you that, for that minute, the service responded to 95% of the requests in 89 ms or less.\n### External Application Load Balancer\nExternal Application Load Balancers use the following primary metric types to capture latency:\n- [https/total_latencies](/monitoring/api/metrics_gcp#loadbalancing/https/total_latencies) : a distribution of the latency calculated from when the request was received by the proxy until the proxy got ACK from client on last response byte. Use when overall user experience is of primary importance.\n- [https/backend_latencies](/monitoring/api/metrics_gcp#loadbalancing/https/backend_latencies) : a distribution of the latency calculated from when the request was sent by the proxy to the backend until the proxy received from the backend the last byte of response. Use to measure latencies of specific backends serving traffic behind the load balancer.\nThese metrics are written against the [https_lb_rule](/monitoring/api/resources#tag_http_lb_rule) monitored-resource type.\n**Note:** Both metrics have value type [DISTRIBUTION](/monitoring/api/ref_v3/rest/v3/TypedValue#Distribution) , which means that they contain histograms representing distribution of values across buckets. The bucket boundaries for these metrics are created using powers of 1.4; that is, the first bucket is 0-1.4, the second is 1.4-1.96, and so forth. If you have very precise alerting requirements, then you might consider this as you set your thresholds, but most of the time, relying on interpolation for thresholds that fall within a bucket is adequate.\nThis example SLO expects that 99% of requests fall between 0 and 100 ms in total latency over a rolling one-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"metric.type=\\\"loadbalancing.googleapis.com/https/total_latencies\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resource.type=\\\"https_lb_rule\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.99,\u00a0 \"rollingPeriod\": \"3600s\",\u00a0 \"displayName\": \"98% requests under 100 ms\"}\n```\nThis example SLO expects that 98% of requests to the \"my-app-backend\" backend target fall between 0 and 100 ms in latency over a rolling one-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"loadbalancing.googleapis.com/https/backend_latencies\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"https_lb_rule\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"backend_target_name\\\"=\\\"my-app-backend\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.98,\u00a0 \"rollingPeriod\": \"3600s\",\u00a0 \"displayName\": \"98% requests under 100 ms\"}\n```\n### Internal Application Load Balancer\nInternal Application Load Balancers use two primary metric types to capture latency:\n- [https/internal/total_latencies](/monitoring/api/metrics_gcp#loadbalancing/https/internal/total_latencies) : a distribution of the latency calculated from when the request was received by the proxy until the proxy got ACK from client on last response byte. Use when overall user experience is of primary importance.\n- [https/internal/backend_latencies](/monitoring/api/metrics_gcp#loadbalancing/https/internal/backend_latencies) : a distribution of the latency calculated from when the request was sent by the proxy to the backend until the proxy received from the backend the last byte of response. Use to measure latencies of specific backends serving traffic behind the load balancer.\nThese metrics are written against the [internal_http_lb_rule](/monitoring/api/resources#tag_internal_http_lb_rule) monitored-resource type.\n**Note:** Both metrics have value type [DISTRIBUTION](/monitoring/api/ref_v3/rest/v3/TypedValue#Distribution) , which means that they contain histograms representing distribution of values across buckets. The bucket boundaries for these metrics are created using powers of 2 with a scale of 1e-06. If you have very precise alerting requirements, then you might consider this as you set your thresholds, but most of the time, relying on interpolation for thresholds that fall within a bucket is adequate.\nThis example SLO expects that 99% of requests fall between 0 and 100 ms in total latency over a rolling one-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"loadbalancing.googleapis.com/https/internal/total_latencies\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"internal_http_lb_rule\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.99,\u00a0 \"rollingPeriod\": \"3600s\",\u00a0 \"displayName\": \"98% requests under 100 ms\"}\n```\nThis example SLO expects that 99% of requests fall between 0 and 100 ms in total latency over a rolling one-hour period.\nThis example SLO expects that 98% of requests to the \"my-internal-backend\" backend target fall between 0 and 100 ms in latency over a rolling one-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"loadbalancing.googleapis.com/https/internal/backend_latencies\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"https_lb_rule\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"backend_target_name\\\"=\\\"my-internal-backend\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.98,\u00a0 \"rollingPeriod\": \"3600s\",\u00a0 \"displayName\": \"98% requests under 100 ms\"}\n```\n### External layer 3 (TCP) load balancer\nExternal TCP load balancers use a single metric type, [l3/external/rtt_latencies](/monitoring/api/metrics_gcp#loadbalancing/l3/external/rtt_latencies) , which records distribution of round-trip time measured over TCP connections for external load-balancer flows.\nThis metric is written against the [tcp_lb_rule](/monitoring/api/resources#tag_tcp_lb_rule) resource.\nThis example SLO expects that 99% of requests fall between 0 and 100 ms in total latency over a rolling one-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"loadbalancing.googleapis.com/l3/external/rtt_latencies\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"tcp_lb_rule\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.99,\u00a0 \"rollingPeriod\": \"3600s\",\u00a0 \"displayName\": \"98% requests under 100 ms\"}\n```\n### Internal layer 3 (TCP) load balancer\nInternal TCP load balancers use a single metric type, [l3/internal/rtt_latencies](/monitoring/api/metrics_gcp#loadbalancing/l3/internal/rtt_latencies) , which records distribution of round-trip time measured over TCP connections for internal load-balancer flows.\nThis metric is written against the [internal_tcp_lb_rule](/monitoring/api/resources#tag_internal_tcp_lb_rule) resource.\nThis example SLO expects that 99% of requests fall between 0 and 100 ms in total latency over a rolling one-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"loadbalancing.googleapis.com/l3/internal/rtt_latencies\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"internal_tcp_lb_rule\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.99,\u00a0 \"rollingPeriod\": \"3600s\",\u00a0 \"displayName\": \"98% requests under 100 ms\"}\n```", "guide": "Google Cloud Observability"}