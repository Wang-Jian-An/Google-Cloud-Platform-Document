{"title": "Google Cloud Observability - Get started with self-deployed collection", "url": "https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-unmanaged", "abstract": "# Google Cloud Observability - Get started with self-deployed collection\nThis document describes how to set up Google Cloud Managed Service for Prometheus with self-deployed collection. An example application is deployed to a Kubernetes cluster and is monitored by a Prometheus server that stores collected metrics in Monarch.\n**Compute Engine users:** If you want to collect Prometheus metrics from Compute Engine VM instances, then the recommended option is to [use the Ops Agent and configure thePrometheus receiver](/stackdriver/docs/solutions/agents/ops-agent/prometheus) .\nThis document shows you how to do the following:\n- Set up your environment and command-line tools.\n- Configure a service account for Workload Identity-enabled clusters.\n- Run the drop-in Prometheus binary on Kubernetes.\n- Control which metrics are ingested into Managed Service for Prometheus.\n- Integrate Managed Service for Prometheus with prometheus-operator setups.\n- Manually compile and run the Prometheus binary.\nWith self-deployed data collection, you manage your Prometheus installation as you always have. The only difference is that you run the Managed Service for Prometheus drop-in replacement binary, `gke.gcr.io/prometheus-engine/prometheus:v2.41.0-gmp.9-gke.0` , instead of the upstream Prometheus binary.\nThe drop-in binary provides additional configuration options with the `--export.*` flags. For more information, see the output of the `--help` option. This document points out the most important options.\n**Managed Service for Prometheus does not support exporting metrics from afederation server or from a server used as a remote-write receiver** . You can replicate all federation server functionality, including reducing ingestion volume by aggregating data before sending to Monarch, by using [filters](#filter-metrics) and [local aggregations](/stackdriver/docs/managed-prometheus/cost-controls#local-aggregation) .\nStreaming data to Managed Service for Prometheus consumes additional resources. If you are self-deploying collectors, then we recommend increasing CPU and memory limits by 5x and adjusting them based on actual usage.\nFor more information about managed and self-deployed data collection, see [Data collection with Managed Service for Prometheus](/stackdriver/docs/managed-prometheus#gmp-data-collection) .\n**Note:** Google Cloud technical support provides limited assistance for  self-deployed collection.\n", "content": "## Before you begin\nThis section describes the configuration needed for the tasks described in this document.\n### Set up projects and tools\nTo use Google Cloud Managed Service for Prometheus, you need the following resources:\n- A Google Cloud project with the Cloud Monitoring API enabled.- If you don't have a Google Cloud project, then do the following:- In the Google Cloud console, go to **New Project** : [Create a New Project](https://console.cloud.google.com/projectcreate) \n- In the **Project Name** field, enter a name for your project and then click **Create** .\n- Go to **Billing** : [Go to Billing](https://console.cloud.google.com/billing) \n- Select the project you just created if it isn't already selected at the top of the page.\n- You are prompted to choose an existing payments profile or to create a new one.\nThe Monitoring API is enabled by default for new projects.\n- If you already have a Google Cloud project, then ensure that the Monitoring API is enabled:- Go to **APIs & services** : [Go to APIs & services](https://console.cloud.google.com/apis/dashboard) \n- Select your project.\n- Click **Enable APIs and Services** .\n- Search for \"Monitoring\".\n- In the search results, click through to \"Cloud Monitoring API\".\n- If \"API enabled\" is not displayed, then click the **Enable** button.\n- A Kubernetes cluster. If you do not have a Kubernetes cluster, then follow the instructions in the [Quickstart forGKE](/kubernetes-engine/docs/deploy-app-cluster) .\nYou also need the following command-line tools:\n- `gcloud`\n- `kubectl`\nThe `gcloud` and `kubectl` tools are part of the Google Cloud CLI. For information about installing them, see [Managing Google Cloud CLI components](/sdk/docs/components) . To see the gcloud CLI components you have installed, run the following command:\n```\ngcloud components list\n```\n### Configure your environment\nTo avoid repeatedly entering your project ID or cluster name, perform the following configuration:\n- Configure the command-line tools as follows:- Configure the gcloud CLI to refer to the ID of your Google Cloud project:```\ngcloud config set project PROJECT_ID\n```\n- Configure the `kubectl` CLI to use your cluster:```\nkubectl config set-cluster CLUSTER_NAME\n```\nFor more information about these tools, see the following:- [gcloud CLI overview](/sdk/gcloud) \n- [kubectl commands](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands) \n### Set up a namespace\nCreate the `` Kubernetes namespace for resources you create as part of the example application:\n```\nkubectl create ns NAMESPACE_NAME\n```\n### Verify service account credentials\n**You can skip this section if your Kubernetes cluster hasWorkload Identity enabled.**\nWhen running on GKE, Managed Service for Prometheus automatically retrieves credentials from the environment based on the Compute Engine default service account. The default service account has the necessary permissions, `monitoring.metricWriter` and `monitoring.viewer` , by default. If you don't use Workload Identity, and you have previously removed either of those roles from the default node service account, you will have to [re-add those missing permissions](/stackdriver/docs/managed-prometheus/troubleshooting#perm-node-svcacct) before continuing.\nIf you are not running on GKE, see [Provide credentials explicitly](#explicit-credentials) .\n### Configure a service account for Workload Identity\n**You can skip this section if your Kubernetes cluster does not haveWorkload Identity enabled.**\nManaged Service for Prometheus captures metric data by using the Cloud Monitoring API. If your cluster is using Workload Identity, you must grant your Kubernetes service account permission to the Monitoring API. This section describes the following:\n- Creating a dedicated [Google Cloud service account](/iam/docs/service-accounts) ,`gmp-test-sa`.\n- Binding the Google Cloud service account to the default [Kubernetesservice account](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/) in a test namespace,``.\n- Granting the necessary permission to the Google Cloud service account.This step appears in several places in the Managed Service for Prometheus documentation. If you have already performed this step as part of a prior task, then you don't need to repeat it. Skip ahead to [Authorize theservice account](#authorize-sa) .\nThe following command sequence creates the `gmp-test-sa` service account and binds it to the default Kubernetes service account in the `` namespace:\n```\ngcloud config set project PROJECT_ID \\\n&&\ngcloud iam service-accounts create gmp-test-sa \\\n&&\ngcloud iam service-accounts add-iam-policy-binding \\\n --role roles/iam.workloadIdentityUser \\\n --member \"serviceAccount:PROJECT_ID.svc.id.goog[NAMESPACE_NAME/default]\" \\\n gmp-test-sa@PROJECT_ID.iam.gserviceaccount.com \\\n&&\nkubectl annotate serviceaccount \\\n --namespace NAMESPACE_NAME \\\n default \\\n iam.gke.io/gcp-service-account=gmp-test-sa@PROJECT_ID.iam.gserviceaccount.com\n```\nIf you are using a different GKE namespace or service account, adjust the commands appropriately.\nGroups of related permissions are collected into , and you grant the roles to a principal, in this example, the Google Cloud service account. For more information about Monitoring roles, see [Access control](/monitoring/access-control) .\nThe following command grants the Google Cloud service account, `gmp-test-sa` , the Monitoring API roles it needs to write metric data.\nIf you have already granted the Google Cloud service account a specific role as part of prior task, then you don't need to do it again.```\ngcloud projects add-iam-policy-binding PROJECT_ID\\\n --member=serviceAccount:gmp-test-sa@PROJECT_ID.iam.gserviceaccount.com \\\n --role=roles/monitoring.metricWriter\n```\nIf you are having trouble getting Workload Identity to work, see the documentation for [verifying your Workload Identity setup](/kubernetes-engine/docs/how-to/workload-identity#verify_the_setup) and the [Workload Identity troubleshooting guide](/kubernetes-engine/docs/troubleshooting/troubleshooting-security#pod_cant_authenticate_to) .\nAs typos and partial copy-pastes are the most common sources of errors when configuring Workload Identity, we **strongly** recommend using the editable variables and clickable copy-paste icons embedded in the code samples in these instructions.\nThe example described in this document binds the Google Cloud service account to the default Kubernetes service account and gives the Google Cloud service account all necessary permissions to use the Monitoring API.\nIn a production environment, you might want to use a finer-grained approach, with a service account for each component, each with minimal permissions. For more information on configuring service accounts for workload-identity management, see [Using Workload Identity](/kubernetes-engine/docs/how-to/workload-identity) .\n## Set up self-deployed collection\nThis section describes how to set up and run an example application that uses self-deployed collection.\n### Deploy the example application\nThe [example application](https://github.com/nilebox/prometheus-example-app) emits the `example_requests_total` counter metric and the `example_random_numbers` histogram metric (among others) on its `metrics` port. The manifest for the application defines three replicas.\nTo deploy the example application, run the following command:\n```\nkubectl -n NAMESPACE_NAME apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/prometheus-engine/v0.8.2/examples/example-app.yaml\n```\n### Run the replacement Prometheus binary\nTo ingest the metric data emitted by the example application, you deploy Google's forked version of the Prometheus server, which is configured to scrape the workload's metrics as well as its own metrics endpoint.\n- To deploy the forked server, run the following command:```\nkubectl -n NAMESPACE_NAME apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/prometheus-engine/v0.8.2/examples/prometheus.yaml\n```This deployed Prometheus server is a thin fork of the upstream Prometheus binary. It behaves like a standard Prometheus server, but it also ingests data into Managed Service for Prometheus.The manifest above provides a basic working example that sends data to the global data store, Monarch. It does not persistently store a local copy of data. For information on how this predefined configuration works and how to extend it, see the open source [Prometheus configuration documentation](https://prometheus.io/docs/prometheus/latest/configuration/configuration/) .The pre-built image only works on Linux nodes. To scrape targets running on Windows nodes, either deploy the server on a Linux node and configure it to scrape endpoints on the Windows nodes or [build the binary for Windowsyourself](#gmp-build-binary) .\n- Verify that the pods for the Prometheus server and the example application deployed successfully:```\nkubectl -n NAMESPACE_NAME get pod\n```If the deployment was successful, then you see output similar to the following:```\nNAME       READY STATUS RESTARTS AGE\nprom-example-84c6f547f5-fglbr 1/1  Running 0   5m\nprom-example-84c6f547f5-jnjp4 1/1  Running 0   5m\nprom-example-84c6f547f5-sqdww 1/1  Running 0   5m\nprometheus-test-0    2/2  Running 1   3m\n```\nIf you are running on GKE, then you can do the following:\n- To query the metrics ingested by the example application, see [Query using Cloud Monitoring](/stackdriver/docs/managed-prometheus/query-cm) or [Query using Grafana](/stackdriver/docs/managed-prometheus/query) .\n- To learn about using prometheus-operator and kube-prometheus with self-deployed collection, and to see how to build and run the binary for the managed service, see [Additional topics for self-deployedcollection](#gmp-unmanaged-addl-topics) .\nIf you are running outside of GKE, then you need to create a service account and authorize it to write your metric data, as described in the following section.\n### Provide credentials explicitly\nWhen running on GKE, the collecting Prometheus server automatically retrieves credentials from the environment based on the node's service account or the Workload Identity setup. In non-GKE Kubernetes clusters, credentials must be explicitly provided to the collecting Prometheus server by using flags or the `GOOGLE_APPLICATION_CREDENTIALS` environment variable.\n- Set the context to your target project:```\ngcloud config set project PROJECT_ID\n```\n- Create a service account:```\ngcloud iam service-accounts create gmp-test-sa\n```This step creates the service account that you might have already created in the [Workload Identity instructions](#gmp-wli-svcacct) .\n- Grant the required permissions to the service account:```\ngcloud projects add-iam-policy-binding PROJECT_ID\\\n --member=serviceAccount:gmp-test-sa@PROJECT_ID.iam.gserviceaccount.com \\\n --role=roles/monitoring.metricWriter\n```\n- Create and download a key for the service account:```\ngcloud iam service-accounts keys create gmp-test-sa-key.json \\\n --iam-account=gmp-test-sa@PROJECT_ID.iam.gserviceaccount.com\n```\n- Add the key file as a secret to your non-GKE cluster:```\nkubectl -n NAMESPACE_NAME create secret generic gmp-test-sa \\\n --from-file=key.json=gmp-test-sa-key.json\n```\n- Open the Prometheus StatefulSet resource for editing:```\nkubectl -n NAMESPACE_NAME edit statefulset prometheus-test\n```- Add the text shown in bold to the resource:```\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n namespace: NAMESPACE_NAME\n name: example\nspec:\n template\n containers:\n - name: prometheus\n  args:\n  - --export.credentials-file=/gmp/key.json\n...\n  volumeMounts:\n  - name: gmp-sa\n  mountPath: /gmp\n  readOnly: true\n...\n volumes:\n - name: gmp-sa\n  secret:\n  secretName: gmp-test-sa\n...\n```\n- Save the file and close the editor. After the change is applied, the pods are re-created and start authenticating to the metric backend with the given service account.\nAlternatively, instead of using the flags set in this example, you can set the key-file path by using the`GOOGLE_APPLICATION_CREDENTIALS`environment variable.\n## Additional topics for self-deployed collection\nThis section describes how to do the following:- Filter the data you export to the managed service.\n- Convert your existing deployment configurations.\n- Run the Prometheus binary in high-availability mode.\n- Build and run the replacement Prometheus binary.\n- Run Managed Service for Prometheus outside of Google Cloud.### Filter exported metrics\nIf you collect a lot of data, you might want to prevent some time series from being sent to Managed Service for Prometheus to keep costs down.You can use regular [metric-relabelingconfigs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config) in your Prometheus scraping configuration. With relabeling configs, you can drop metrics based on label matches at ingestion time.Sometimes, you might want to ingest data locally but not export it to Managed Service for Prometheus. To filter exported metrics, you can use the `--export.match` flag.The flag specifies one or more [PromQL series selectors](https://prometheus.io/docs/prometheus/latest/querying/basics/#time-series-selectors) , and the flag can be used multiple times. A time series is exported to Managed Service for Prometheus if it satisfies all of the selectors in at least one of the flags; that is, when determining eligibility, conditions within a single flag are ANDed while conditions in separate flags are ORed. The following example uses two instances of the flag:```\n./prometheus \\\n --export.match='{job=\"prometheus\"}' \\\n --export.match='{__name__=~\"job:.+\"}' \\\n ...\n```This change causes only metrics for the \"prometheus\" job as well as metrics produced by recording rules that aggregate to the job level (when following naming best practices) to be exported. Samples for all other series are filtered out. By default, no selectors are specified and all time series are exported.The `--export.match` flag has the same semantics as the `match[]` parameter for Prometheus federation. You can therefore migrate federation setups to Managed Service for Prometheus by using the selectors from your federation server directly as flags on the Prometheus servers scraped by your federation Prometheus server. **Exporting metrics from a federation server to the managedservice is not supported** .To include `histogram` -type metrics in a filter, you must specify the `_count` , `_sum` , and `_bucket` metrics. You can also do this with a wildcard matcher, for example the selector `{__name__=~\"histogram_metric_.+\"}` .If you are using the `prometheus-operator` library, then set any `--export.match` flags using the `EXTRA_ARGS` environment variable of the container. For more information, see [Use with prometheus-operator](#prom-managers) .You can combine filter flags with locally run recording rules to \"roll up\" data before sending to Monarch, reducing your cardinality and cost. For more information, see [Cost controls and attribution](/stackdriver/docs/managed-prometheus/cost-controls#local-aggregation) .The Cloud Monitoring **Metrics Management** page provides information that can help you control the amount you spend on chargeable metrics without affecting observability. The **Metrics Management** page reports the following information:- Ingestion volumes for both byte- and sample-based billing, across metric  domains and for individual metrics.\n- Data about labels and cardinality of metrics.\n- Use of metrics in alerting policies and custom dashboards.\n- Rate of metric-write errors.\nFor more information about the **Metrics Management** page, see [View and manage metric usage](/monitoring/docs/metrics-management) .\n### Use with prometheus-operator\nThe Managed Service for Prometheus Prometheus binary can also be used with an existing GKE Prometheus deployment managed by [prometheus-operator](https://github.com/prometheus-operator) .To use the managed service's binary, replace the image specification in the Prometheus resource:```\n apiVersion: monitoring.coreos.com/v1\n kind: Prometheus\n metadata:\n name: NAMESPACE_NAME\n namespace: gmp-system\n spec:\n image: gke.gcr.io/prometheus-engine/prometheus:v2.41.0-gmp.9-gke.0\n ...\n replicas: 1\n serviceAccountName: default\n version: v2.35.0\n ...\n```If you are in a Workload Identity cluster and the namespace or service account in your resource differs, repeat the [Workload Identity instructions](#gmp-wli-svcacct) for the additional namespace and Kubernetes service account pair.When running on a non-GKE Kubernetes cluster, you need to manually provide credentials. To provide credentials, do the following:- Add an appropriate service account key file as a secret, as described in [Provide credentials explicitly](#explicit-credentials) .\n- Modify the Prometheus resource to add the text shown in bold type:```\n apiVersion: monitoring.coreos.com/v1\n kind: Prometheus\n metadata:\n namespace: gmp-test\n name: example\n spec:\n ...\n secrets:\n - gmp-test-sa\n containers:\n - name: prometheus\n  env:\n  - name: GOOGLE_APPLICATION_CREDENTIALS\n  value: /gmp/key.json\n  volumeMounts:\n  - name: secret-gmp-test-sa\n  mountPath: /gmp\n  readOnly: true\n```\nYou can set the `EXTRA_ARGS` environment variable of the container to add additional flags, such as the [metric filtering flags](#filter-metrics) . This is done through an environment variable because the `args` section of the container specification is managed by Prometheus Operator.\n### Use with kube-prometheus\nYou can configure deployments created using the popular [kube-prometheus](https://github.com/prometheus-operator/kube-prometheus#quickstart) library to use Managed Service for Prometheus.Kube-prometheus has some tight internal dependencies on its default namespaces and service accounts, so we recommend only changing the minimum number of fields necessary to send data to Managed Service for Prometheus.Within `manifests/prometheus-prometheus.yaml` , replace the image specification and turn off high-availability collection by reducing `replicas` to 1:```\n apiVersion: monitoring.coreos.com/v1\n kind: Prometheus\n ...\n spec:\n  image: gke.gcr.io/prometheus-engine/prometheus:v2.41.0-gmp.9-gke.0\n  ...\n  replicas: 1\n  version: v2.35.0\n  ...\n \n```If you are running on GKE and have not modified the default service account on the node, applying the modified manifests should immediately start sending data to Managed Service for Prometheus. Otherwise, you might have to configure and apply a service account. When running on GKE and using workload identity, you might have to create and authorize the `prometheus-k8s` service account within the `monitoring` namespace. When running on a non-GKE Kubernetes cluster, follow the instructions in the [prometheus-operator section](#prom-managers) .Note that kube-prometheus collects a lot of metrics by default, most of which are often not necessary in a managed Kubernetes environment like GKE. To save on ingestion costs, you can customize kube-prometheus so that it scrapes only metrics you care about and filter exported metrics aggressively.For more suggestions, see [Cost controls and attribution](/stackdriver/docs/managed-prometheus/cost-controls) .\n### High-availability deployment\nThe replacement Prometheus binary comes with built-in support for highly available collection by using leader election. Replicated Prometheus servers in high-availability mode both collect metrics and evaluate rules as usual, but only one of them sends data to Google Cloud Managed Service for Prometheus.Replicas of the same Prometheus server must always have identical configurations, including the same [external_labels](https://prometheus.io/docs/prometheus/latest/configuration/configuration/) . This requirement differs from other systems, which rely on a special external label, such as `__replica__` , to make replicas explicitly different.The Kubernetes API server is a supported leader-election backend and can be enabled by setting the following flags:```\n./prometheus\n ...\n --export.ha.backend=kube \\\n --export.ha.kube.namespace=LEASE_NAMESPACE \\\n --export.ha.kube.name=LEASE_NAME\n```The and values identify the [Lease resource](https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1) through which leader election takes place. All Prometheus servers pointing at the same resource belong to the same replica set. The Kubernetes service account of the Prometheus deployment needs permission to read and write the respective Lease resource. When running the Prometheus server outside of a Kubernetes cluster, you can provide an explicit config by using the `--export.ha.kube.config` flag.After you do this, you can increase the `replicas` value to 2 or greater.\n### Binary deployments\nIf you want to run in a non-containerized environment, you can build the replacement Prometheus binary directly.If you have an existing process for compiling Prometheus yourself, you can transparently substitute our [GitHub repository](https://github.com/GoogleCloudPlatform/prometheus) into your process. Managed Service for Prometheus has its own version-tag extension to distinguish its releases from upstream releases.To build the plain binary, the Go toolchain and recent versions of NPM/Yarn must be installed on the machine. For more information, see the [upstream buildinstructions](https://github.com/prometheus/prometheus#building-from-source) .- Clone the repository:```\ngit clone https://github.com/GoogleCloudPlatform/prometheus &&\ncd prometheus\n```\n- Check out the desired version tag:```\ngit checkout v2.41.0-gmp.9\n```\n- To create a Managed Service for Prometheus tarball, run the following commands:```\nmake build && make tarball\n```\nThe resulting tarball and binaries are fully compatible with their upstream variants in terms of directory structure and functionality.\n### Limits on creating and updating metrics and labels\nManaged Service for Prometheus enforces a per-minute rate limit on creating new metrics and on adding new metric labels to existing metrics. This rate limit is usually only hit when first integrating with Managed Service for Prometheus, for example when you migrate an existing, mature Prometheus deployment to use self-deployed collection. **Thisis not a rate limit on ingesting data points** . This rate limit only applies when creating never-before-seen metrics or when adding new labels to existing metrics.This quota is fixed, but any issues should automatically resolve as new metrics and metric labels get created up to the per-minute limit.For more information, see [Troubleshooting](/stackdriver/docs/managed-prometheus/troubleshooting#descriptor_limits) .\n### Run self-deployed collection outside of Google Cloud\nIn Compute Engine environments, GKE environments, or on a machine where you ran `gcloud login` with a sufficiently authorized account, you can run self-deployed collection without further configuration. Outside of Google Cloud, you need to explicitly provide credentials, a `project_id` to contain your metrics, and a `location` (Google Cloud region) to store your metrics in. You should also set the `cluster` and `namespace` labels, even if running in a non-Kubernetes environment.You can provide a service account key by using the `--export.credentials-file` flag or the `GOOGLE_APPLICATION_CREDENTIALS` environment variable as described in [Provide credentialsexplicitly](#explicit-credentials) .We recommend choosing `project_id` based on your planned tenancy model for reads. Pick a project to store metrics in based on how you plan to organize reads later with [metrics scopes](/stackdriver/docs/managed-prometheus/query#scoping-intro) . If you don't care, you can put everything into one project.For `location` , we recommend choosing the nearest Google Cloud region to your deployment. The further the chosen Google Cloud region is from your deployment, the more write latency you'll have and the more you'll be affected by potential networking issues. You might want to consult this [list of regions acrossmultiple clouds](https://www.cloudinfrastructuremap.com/) . If you don't care, you can put everything into one Google Cloud region. You can't use `global` as your location.If running in a Kubernetes environment, set the `cluster` and `namespace` values to the local cluster and namespace. If running outside Kubernetes, set them to values that make sense hierarchically. For example, in a VM-based environment running on AWS, set the `cluster` value to `__aws__` and the `namespace` value to the instance ID. You can dynamically fill in the instance ID by using a relabeling rule that calls the local metadata server.For a minimal working example, you can run a local, self-monitoring Prometheus binary with the following command:```\n./prometheus \\\n --config.file=documentation/examples/prometheus.yaml \\\n --export.label.project-id=PROJECT_ID \\\n --export.label.location=REGION \\\n --export.label.cluster=CLUSTER_NAME \\\n```This example assumes you have set the `` variable to a value like `us-central1` , for example.However, we recommend that you set the `export` target labels for the managed service in the `global.external_labels` section of your Prometheus config. For example, in Kubernetes environments you might use the following config:```\nglobal:\n external_labels:\n project_id: PROJECT_ID\n location: REGION\n cluster: CLUSTER_NAME\n namespace: local-testing\nscrape_configs:\n ...\n```Running Managed Service for Prometheus outside of Google Cloud incurs data transfer fees. There are fees to transfer data into Google Cloud, and you might incur fees to transfer data out of another cloud. You can minimize this cost by enabling compression with the `--export.compression=gzip` flag.\n## What's next- [Use PromQL in Cloud Monitoring to query Prometheus metrics](/stackdriver/docs/managed-prometheus/query-cm) .\n- [Use Grafana to query Prometheus metrics](/stackdriver/docs/managed-prometheus/query) .\n- Use [PromQL alerts in Cloud Monitoring](/monitoring/promql/promql-in-alerting) .\n- Set up [managed rule evaluation](/stackdriver/docs/managed-prometheus/rules-managed) .\n- Set up [self-deployed rule evaluation](/stackdriver/docs/managed-prometheus/rules-unmanaged) .\n- Reduce cardinality and cost by [configuring local aggregations](/stackdriver/docs/managed-prometheus/cost-controls#local-aggregation) .\n-", "guide": "Google Cloud Observability"}