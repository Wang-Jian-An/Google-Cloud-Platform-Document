{"title": "Google Cloud Observability - Get started with the OpenTelemetry Collector", "url": "https://cloud.google.com/stackdriver/docs/managed-prometheus/setup-otel", "abstract": "# Google Cloud Observability - Get started with the OpenTelemetry Collector\nThis document describes how to set up the [OpenTelemetryCollector](https://opentelemetry.io/docs/collector/) to scrape standard Prometheus metrics and report those metrics to Google Cloud Managed Service for Prometheus. The OpenTelemetry Collector is an agent that you can deploy yourself and configure to export to Managed Service for Prometheus. The set-up is similar to running Managed Service for Prometheus with self-deployed collection.\nYou might choose the OpenTelemetry Collector over self-deployed collection for the following reasons:\n- The OpenTelemetry Collector allows you to route your telemetry data to multiple backends by configuring different exporters in your pipeline.\n- The Collector also supports signals from metrics, logs, and traces, so by using it you can handle all three signal types in one agent.\n- OpenTelemetry's vendor-agnostic data format (the OpenTelemetry Protocol, or OTLP) supports a strong ecosystem of libraries and pluggable Collector components. This allows for a range of customizability options for receiving, processing, and exporting your data.\nThe trade-off for these benefits is that running an OpenTelemetry Collector requires a self-managed deployment and maintenance approach. Which approach you choose will depend on your specific needs, but in this document we offer recommended guidelines for configuring the OpenTelemetry Collector using Managed Service for Prometheus as a backend.\n**Note:** Google Cloud technical support provides limited assistance for  collection with the OpenTelemetry Collector.\n", "content": "## Before you begin\nThis section describes the configuration needed for the tasks described in this document.\n### Set up projects and tools\nTo use Google Cloud Managed Service for Prometheus, you need the following resources:\n- A Google Cloud project with the Cloud Monitoring API enabled.- If you don't have a Google Cloud project, then do the following:- In the Google Cloud console, go to **New Project** : [Create a New Project](https://console.cloud.google.com/projectcreate) \n- In the **Project Name** field, enter a name for your project and then click **Create** .\n- Go to **Billing** : [Go to Billing](https://console.cloud.google.com/billing) \n- Select the project you just created if it isn't already selected at the top of the page.\n- You are prompted to choose an existing payments profile or to create a new one.\nThe Monitoring API is enabled by default for new projects.\n- If you already have a Google Cloud project, then ensure that the Monitoring API is enabled:- Go to **APIs & services** : [Go to APIs & services](https://console.cloud.google.com/apis/dashboard) \n- Select your project.\n- Click **Enable APIs and Services** .\n- Search for \"Monitoring\".\n- In the search results, click through to \"Cloud Monitoring API\".\n- If \"API enabled\" is not displayed, then click the **Enable** button.\n- A Kubernetes cluster. If you do not have a Kubernetes cluster, then follow the instructions in the [Quickstart forGKE](/kubernetes-engine/docs/deploy-app-cluster) .\nYou also need the following command-line tools:\n- `gcloud`\n- `kubectl`\nThe `gcloud` and `kubectl` tools are part of the Google Cloud CLI. For information about installing them, see [Managing Google Cloud CLI components](/sdk/docs/components) . To see the gcloud CLI components you have installed, run the following command:\n```\ngcloud components list\n```\n### Configure your environment\nTo avoid repeatedly entering your project ID or cluster name, perform the following configuration:\n- Configure the command-line tools as follows:- Configure the gcloud CLI to refer to the ID of your Google Cloud project:```\ngcloud config set project PROJECT_ID\n```\n- Configure the `kubectl` CLI to use your cluster:```\nkubectl config set-cluster CLUSTER_NAME\n```\nFor more information about these tools, see the following:- [gcloud CLI overview](/sdk/gcloud) \n- [kubectl commands](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands) \n### Set up a namespace\nCreate the `` Kubernetes namespace for resources you create as part of the example application:\n```\nkubectl create ns NAMESPACE_NAME\n```\n### Verify service account credentials\n**You can skip this section if your Kubernetes cluster hasWorkload Identity enabled.**\nWhen running on GKE, Managed Service for Prometheus automatically retrieves credentials from the environment based on the Compute Engine default service account. The default service account has the necessary permissions, `monitoring.metricWriter` and `monitoring.viewer` , by default. If you don't use Workload Identity, and you have previously removed either of those roles from the default node service account, you will have to [re-add those missing permissions](/stackdriver/docs/managed-prometheus/troubleshooting#perm-node-svcacct) before continuing.\nIf you are not running on GKE, see [Provide credentials explicitly](#explicit-credentials) .\n### Configure a service account for Workload Identity\n**You can skip this section if your Kubernetes cluster does not haveWorkload Identity enabled.**\nManaged Service for Prometheus captures metric data by using the Cloud Monitoring API. If your cluster is using Workload Identity, you must grant your Kubernetes service account permission to the Monitoring API. This section describes the following:\n- Creating a dedicated [Google Cloud service account](/iam/docs/service-accounts) ,`gmp-test-sa`.\n- Binding the Google Cloud service account to the default [Kubernetesservice account](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/) in a test namespace,``.\n- Granting the necessary permission to the Google Cloud service account.This step appears in several places in the Managed Service for Prometheus documentation. If you have already performed this step as part of a prior task, then you don't need to repeat it. Skip ahead to [Authorize theservice account](#authorize-sa) .\nThe following command sequence creates the `gmp-test-sa` service account and binds it to the default Kubernetes service account in the `` namespace:\n```\ngcloud config set project PROJECT_ID \\\n&&\ngcloud iam service-accounts create gmp-test-sa \\\n&&\ngcloud iam service-accounts add-iam-policy-binding \\\n --role roles/iam.workloadIdentityUser \\\n --member \"serviceAccount:PROJECT_ID.svc.id.goog[NAMESPACE_NAME/default]\" \\\n gmp-test-sa@PROJECT_ID.iam.gserviceaccount.com \\\n&&\nkubectl annotate serviceaccount \\\n --namespace NAMESPACE_NAME \\\n default \\\n iam.gke.io/gcp-service-account=gmp-test-sa@PROJECT_ID.iam.gserviceaccount.com\n```\nIf you are using a different GKE namespace or service account, adjust the commands appropriately.\nGroups of related permissions are collected into , and you grant the roles to a principal, in this example, the Google Cloud service account. For more information about Monitoring roles, see [Access control](/monitoring/access-control) .\nThe following command grants the Google Cloud service account, `gmp-test-sa` , the Monitoring API roles it needs to write metric data.\nIf you have already granted the Google Cloud service account a specific role as part of prior task, then you don't need to do it again.```\ngcloud projects add-iam-policy-binding PROJECT_ID\\\n --member=serviceAccount:gmp-test-sa@PROJECT_ID.iam.gserviceaccount.com \\\n --role=roles/monitoring.metricWriter\n```\nIf you are having trouble getting Workload Identity to work, see the documentation for [verifying your Workload Identity setup](/kubernetes-engine/docs/how-to/workload-identity#verify_the_setup) and the [Workload Identity troubleshooting guide](/kubernetes-engine/docs/troubleshooting/troubleshooting-security#pod_cant_authenticate_to) .\nAs typos and partial copy-pastes are the most common sources of errors when configuring Workload Identity, we **strongly** recommend using the editable variables and clickable copy-paste icons embedded in the code samples in these instructions.\nThe example described in this document binds the Google Cloud service account to the default Kubernetes service account and gives the Google Cloud service account all necessary permissions to use the Monitoring API.\nIn a production environment, you might want to use a finer-grained approach, with a service account for each component, each with minimal permissions. For more information on configuring service accounts for workload-identity management, see [Using Workload Identity](/kubernetes-engine/docs/how-to/workload-identity) .\n## Set up the OpenTelemetry Collector\nThis section guides you through setting up and using the OpenTelemetry collector to scrape metrics from an example application and send the data to Google Cloud Managed Service for Prometheus. For detailed configuration information, see the following sections:\n- [Scrape Prometheus metrics](#scrape-metrics) \n- [Add processors](#add-processors) \n- [Configure the googlemanagedprometheus exporter](#configure-exporter) \nThe OpenTelemetry Collector is analogous to the Managed Service for Prometheus agent binary. The OpenTelemetry community regularly publishes [releases](https://github.com/open-telemetry/opentelemetry-collector-releases/releases) including source code, binaries, and container images.\nYou can either deploy these artifacts on VMs or Kubernetes clusters using the best-practice defaults, or you can use the [collectorbuilder](https://opentelemetry.io/docs/collector/custom-collector/) to build your own collector consisting of only the components you need. To build a collector for use with Managed Service for Prometheus, you need the following components:\n- The [Managed Service for Prometheusexporter](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/googlemanagedprometheusexporter) , which writes your metrics to Managed Service for Prometheus.\n- A receiver to scrape your metrics. This document assumes that you are using the [OpenTelemetry Prometheusreceiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver) , but the Managed Service for Prometheus exporter is compatible with any OpenTelemetry metrics receiver.\n- Processors to batch and mark up your metrics to include important resource identifiers depending on your environment.\nThese components are enabled by using a [configurationfile](https://opentelemetry.io/docs/collector/configuration/) that is passed to the Collector with the `--config` flag.\nThe following sections discuss how to configure each of these components in more detail. This document describes how to run the collector [onGKE](#run-collector-gke) and [elsewhere](#run-off-gcp) .\n### Configure and deploy the Collector\nWhether you are running your collection on Google Cloud or in another environment, you can still configure the OpenTelemetry Collector to export to Managed Service for Prometheus. The biggest difference will be in how you configure the Collector. In non-Google Cloud environments, there may be additional formatting of metric data that is needed for it to be compatible with Managed Service for Prometheus. On Google Cloud, however, much of this formatting can be automatically detected by the Collector.\nYou can copy the following config into a file called `config.yaml` to set up the OpenTelemetry Collector on GKE:\n```\nreceivers:\n prometheus:\n config:\n  scrape_configs:\n  - job_name: 'SCRAPE_JOB_NAME'\n  kubernetes_sd_configs:\n  - role: pod\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]\n   action: keep\n   regex: prom-example\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n   action: replace\n   target_label: __metrics_path__\n   regex: (.+)\n  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n   action: replace\n   regex: (.+):(?:\\d+);(\\d+)\n   replacement: $1:$2\n   target_label: __address__\n  - action: labelmap\n   regex: __meta_kubernetes_pod_label_(.+)\nprocessors:\n resourcedetection:\n detectors: [gcp]\n timeout: 10s\n transform:\n # \"location\", \"cluster\", \"namespace\", \"job\", \"instance\", and \"project_id\" are reserved, and\n # metrics containing these labels will be rejected. Prefix them with exported_ to prevent this.\n metric_statements:\n - context: datapoint\n  statements:\n  - set(attributes[\"exported_location\"], attributes[\"location\"])\n  - delete_key(attributes, \"location\")\n  - set(attributes[\"exported_cluster\"], attributes[\"cluster\"])\n  - delete_key(attributes, \"cluster\")\n  - set(attributes[\"exported_namespace\"], attributes[\"namespace\"])\n  - delete_key(attributes, \"namespace\")\n  - set(attributes[\"exported_job\"], attributes[\"job\"])\n  - delete_key(attributes, \"job\")\n  - set(attributes[\"exported_instance\"], attributes[\"instance\"])\n  - delete_key(attributes, \"instance\")\n  - set(attributes[\"exported_project_id\"], attributes[\"project_id\"])\n  - delete_key(attributes, \"project_id\")\n batch:\n # batch metrics before sending to reduce API usage\n send_batch_max_size: 200\n send_batch_size: 200\n timeout: 5s\n memory_limiter:\n # drop metrics if memory usage gets too high\n check_interval: 1s\n limit_percentage: 65\n spike_limit_percentage: 20\n# Note that the googlemanagedprometheus exporter block is intentionally blank\nexporters:\n googlemanagedprometheus:\nservice:\n pipelines:\n metrics:\n  receivers: [prometheus]\n  processors: [batch, memory_limiter, resourcedetection, transform]\n  exporters: [googlemanagedprometheus]\n```\nThe preceding config uses the [Prometheusreceiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver) and the [Managed Service for Prometheusexporter](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/googlemanagedprometheusexporter) to scrape the metrics endpoints on Kubernetes Pods and export those metrics to Managed Service for Prometheus. The pipeline processors format and batch the data.\nFor more details on what each part of this config does, along with configurations for different platforms, see the detailed sections below on [scraping metrics](#scrape-metrics) and [addingprocessors](#add-processors) .\nWhen running in clusters with an existing Prometheus configuration, replace any `$` characters with `$$` to avoid triggering environment variable substitution. For more information, see [Scrape Prometheus metrics](#scrape-metrics) .\nYou can modify this config based on your environment, provider, and the metrics you want to scrape, but the example config is a recommended starting point for running on GKE.\nRunning the OpenTelemetry Collector outside Google Cloud, such as on-premises or on other cloud providers, is similar to running the Collector on GKE. However, the metrics you scrape are less likely to automatically include data that best formats it for Managed Service for Prometheus. Therefore, you must take extra care to configure the collector to format the metrics so they are compatible with Managed Service for Prometheus.\nYou can the following config into a file called `config.yaml` to set up the OpenTelemetry Collector for deployment on a non-GKE Kubernetes cluster:\n```\nreceivers:\n prometheus:\n config:\n  scrape_configs:\n  - job_name: 'SCRAPE_JOB_NAME'\n  kubernetes_sd_configs:\n  - role: pod\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]\n   action: keep\n   regex: prom-example\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n   action: replace\n   target_label: __metrics_path__\n   regex: (.+)\n  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n   action: replace\n   regex: (.+):(?:\\d+);(\\d+)\n   replacement: $1:$2\n   target_label: __address__\n  - action: labelmap\n   regex: __meta_kubernetes_pod_label_(.+)\nprocessors:\n resource:\n attributes:\n - key: \"cluster\"\n  value: \"CLUSTER_NAME\"\n  action: upsert\n - key: \"namespace\"\n  value: \"NAMESPACE_NAME\"\n  action: upsert\n - key: \"location\"\n  value: \"REGION\"\n  action: upsert\n transform:\n # \"location\", \"cluster\", \"namespace\", \"job\", \"instance\", and \"project_id\" are reserved, and\n # metrics containing these labels will be rejected. Prefix them with exported_ to prevent this.\n metric_statements:\n - context: datapoint\n  statements:\n  - set(attributes[\"exported_location\"], attributes[\"location\"])\n  - delete_key(attributes, \"location\")\n  - set(attributes[\"exported_cluster\"], attributes[\"cluster\"])\n  - delete_key(attributes, \"cluster\")\n  - set(attributes[\"exported_namespace\"], attributes[\"namespace\"])\n  - delete_key(attributes, \"namespace\")\n  - set(attributes[\"exported_job\"], attributes[\"job\"])\n  - delete_key(attributes, \"job\")\n  - set(attributes[\"exported_instance\"], attributes[\"instance\"])\n  - delete_key(attributes, \"instance\")\n  - set(attributes[\"exported_project_id\"], attributes[\"project_id\"])\n  - delete_key(attributes, \"project_id\")\n batch:\n # batch metrics before sending to reduce API usage\n send_batch_max_size: 200\n send_batch_size: 200\n timeout: 5s\n memory_limiter:\n # drop metrics if memory usage gets too high\n check_interval: 1s\n limit_percentage: 65\n spike_limit_percentage: 20\nexporters:\n googlemanagedprometheus:\n project: \"PROJECT_ID\"\nservice:\n pipelines:\n metrics:\n  receivers: [prometheus]\n  processors: [batch, memory_limiter, resource, transform]\n  exporters: [googlemanagedprometheus]\n```\nThis config does the following:\n- Sets up a Kubernetes service discovery scrape config for Prometheus. For more information, see [scraping Prometheus metrics](#scrape-metrics) .\n- Manually sets`cluster`,`namespace`, and`location`resource attributes. For more information about resource attributes, including resource detection for Amazon EKS and Azure AKS, see [Detect resourceattributes](#detect-resource-attributes) .\n- Sets the`project`option in the`googlemanagedprometheus`exporter. For more information about the exporter, see [Configure the googlemanagedprometheus exporter](#configure-exporter) .\nWhen running in clusters with an existing Prometheus configuration, replace any `$` characters with `$$` to avoid triggering environment variable substitution. For more information, see [Scrape Prometheus metrics](#scrape-metrics) .\nFor information about best practices for configuring the Collector on other clouds, see [Amazon EKS](#amazon-eks) or [Azure AKS](#azure-aks) .\n### Deploy the example application\nThe [example application](https://github.com/nilebox/prometheus-example-app) emits the `example_requests_total` counter metric and the `example_random_numbers` histogram metric (among others) on its `metrics` port. The manifest for this example defines three replicas.\nTo deploy the example application, run the following command:\n```\nkubectl -n NAMESPACE_NAME apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/prometheus-engine/v0.8.2/examples/example-app.yaml\n```\n### Create your collector config as a ConfigMap\nAfter you have created your config and placed it in a file called `config.yaml` , use that file to create a Kubernetes ConfigMap based on your `config.yaml` file. When the collector is deployed, it mounts the ConfigMap and loads the file.\nTo create a ConfigMap named `otel-config` with your config, use the following command:\n```\nkubectl -n NAMESPACE_NAME create configmap otel-config --from-file config.yaml\n```\n### Deploy the collector\nCreate a file called `collector-deployment.yaml` with the following content:\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n name: NAMESPACE_NAME:prometheus-test\nrules:\n- apiGroups: [\"\"]\n resources:\n - pods\n verbs: [\"get\", \"list\", \"watch\"]\n--apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n name: NAMESPACE_NAME:prometheus-test\nroleRef:\n apiGroup: rbac.authorization.k8s.io\n kind: ClusterRole\n name: NAMESPACE_NAME:prometheus-test\nsubjects:\n- kind: ServiceAccount\n namespace: NAMESPACE_NAME\n name: default\n--apiVersion: apps/v1\nkind: Deployment\nmetadata:\n name: otel-collector\nspec:\n replicas: 1\n selector:\n matchLabels:\n  app: otel-collector\n template:\n metadata:\n  labels:\n  app: otel-collector\n spec:\n  containers:\n  - name: otel-collector\n  image: otel/opentelemetry-collector-contrib:0.92.0\n  args:\n  - --config\n  - /etc/otel/config.yaml\n  volumeMounts:\n  - mountPath: /etc/otel/\n   name: otel-config\n  volumes:\n  - name: otel-config\n  configMap:\n   name: otel-config\n```\nCreate the Collector deployment in your Kubernetes cluster by running the following command:\n```\nkubectl -n NAMESPACE_NAME create -f collector-deployment.yaml\n```\nAfter the pod starts, it scrapes the sample application and reports metrics to Managed Service for Prometheus.\nFor information about ways to query your data, see [Query using Cloud Monitoring](/stackdriver/docs/managed-prometheus/query-cm) or [Query using Grafana](/stackdriver/docs/managed-prometheus/query) .\n### Provide credentials explicitly\nWhen running on GKE, the OpenTelemetry Collector automatically retrieves credentials from the environment based on the node's service account. In non-GKE Kubernetes clusters, credentials must be explicitly provided to the OpenTelemetry Collector by using flags or the `GOOGLE_APPLICATION_CREDENTIALS` environment variable.\n- Set the context to your target project:```\ngcloud config set project PROJECT_ID\n```\n- Create a service account:```\ngcloud iam service-accounts create gmp-test-sa\n```This step creates the service account that you might have already created in the [Workload Identity instructions](#gmp-wli-svcacct) .\n- Grant the required permissions to the service account:```\ngcloud projects add-iam-policy-binding PROJECT_ID\\\n --member=serviceAccount:gmp-test-sa@PROJECT_ID.iam.gserviceaccount.com \\\n --role=roles/monitoring.metricWriter\n```\n- Create and download a key for the service account:```\ngcloud iam service-accounts keys create gmp-test-sa-key.json \\\n --iam-account=gmp-test-sa@PROJECT_ID.iam.gserviceaccount.com\n```\n- Add the key file as a secret to your non-GKE cluster:```\nkubectl -n NAMESPACE_NAME create secret generic gmp-test-sa \\\n --from-file=key.json=gmp-test-sa-key.json\n```\n- Open the OpenTelemetry Deployment resource for editing:```\nkubectl -n NAMESPACE_NAME edit deployment otel-collector\n```\n- Add the text shown in bold to the resource:```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n namespace: NAMESPACE_NAME\n name: otel-collector\nspec:\n template\n spec:\n  containers:\n  - name: otel-collector\n  env:\n  - name: \"GOOGLE_APPLICATION_CREDENTIALS\"\n   value: \"/gmp/key.json\"\n...\n  volumeMounts:\n  - name: gmp-sa\n   mountPath: /gmp\n   readOnly: true\n...\n  volumes:\n  - name: gmp-sa\n  secret:\n   secretName: gmp-test-sa\n...\n```\n- Save the file and close the editor. After the change is applied, the pods are re-created and start authenticating to the metric backend with the given service account.## Scrape Prometheus metrics\nThis section and the subsequent section provide additional customization information for using the OpenTelemetry Collector. This information might be helpful in certain situations, but none of it is necessary to run the example described in [Set up the OpenTelemetry Collector](#otc-example) .\nIf your applications are already exposing Prometheus endpoints, the OpenTelemetry Collector can scrape those endpoints using the same [scrape configformat](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config) you would use with any standard Prometheus config. To do this, enable the [Prometheusreceiver](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver) in your collector config.\nA simple Prometheus receiver config for Kubernetes pods might look like the following:\n```\nreceivers:\n prometheus:\n config:\n  scrape_configs:\n  - job_name: 'kubernetes-pods'\n  kubernetes_sd_configs:\n  - role: pod\n  relabel_configs:\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n   action: keep\n   regex: true\n  - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n   action: replace\n   target_label: __metrics_path__\n   regex: (.+)\n  - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n   action: replace\n   regex: (.+):(?:\\d+);(\\d+)\n   replacement: $1:$2\n   target_label: __address__\n  - action: labelmap\n   regex: __meta_kubernetes_pod_label_(.+)\nservice:\n pipelines:\n metrics:\n  receivers: [prometheus]\n```\nThis is a simple service discovery-based scrape config that you can modify as needed to scrape your applications.\nWhen running in clusters with an existing Prometheus configuration, replace any `$` characters with `$$` to avoid triggering environment variable substitution. This is especially important to do for the `replacement` value within your `relabel_configs` section. For example, if you have the following `relabel_config` section:\n```\n- source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n action: replace\n regex: (.+):(?:\\d+);(\\d+)\n replacement: $1:$2\n target_label: __address__\n```\nThen rewrite it to be:\n```\n- source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n action: replace\n regex: (.+):(?:\\d+);(\\d+)\n replacement: $$1:$$2\n target_label: __address__\n```\nFor more information, see [the OpenTelemetry documentation](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver#getting-started) .\nNext, we strongly recommend that you use processors to format your metrics. In many cases, processors must be used to properly format your metrics.\n## Add processors\nOpenTelemetry [processors](https://opentelemetry.io/docs/collector/configuration/#processors) modify telemetry data before it is exported. You can use the processors below to ensure that your metrics are written in a format compatible with Managed Service for Prometheus.\n### Detect resource attributes\nThe Managed Service for Prometheus exporter for OpenTelemetry uses the [prometheus_target monitoredresource](https://cloud.google.com/monitoring/api/resources#tag_prometheus_target) to uniquely identify time series data points. The exporter parses the required monitored-resource fields from resource attributes on the metric data points. The fields and the attributes from which the values are scraped are:\n- **project_id** : auto-detected by [Application DefaultCredentials](https://cloud.google.com/docs/authentication/application-default-credentials#personal) ,`gcp.project.id`, or`project`in exporter config (see [configuring theexporter](#configure-exporter) )\n- **location** :`location`,`cloud.availability_zone`,`cloud.region`\n- **cluster** :`cluster`,`k8s.cluster_name`\n- **namespace** :`namespace`,`k8s.namespace_name`\n- **job** :`service.name`+`service.namespace`\n- **instance** :`service.instance.id`\nFailure to set these labels to unique values can result in \"duplicate timeseries\" errors when exporting to Managed Service for Prometheus.\n**Note:** The terms and , when referring to metric data points, represent essentially the same concept in Prometheus and OpenTelemetry, respectively. In this context, a Prometheus metric with the label `foo` will be converted into an OpenTelemetry data point with an attribute `foo` . The specific labels/attributes listed above are converted into attributes, which are another OpenTelemetry concept for identifying data points specific to the source of the data. These resource attributes are then mapped to the monitored resource fields listed.\nThe Prometheus receiver automatically sets the `service.name` attribute based on the `job_name` in the scrape config, and `service.instance.id` attribute based on the scrape target's `instance` . The receiver also sets `k8s.namespace.name` when using `role: pod` in the scrape config.\nWe recommend populating the other attributes automatically using the [resourcedetectionprocessor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor) . However, depending on your environment, some attributes might not be automatically detectable. In this case, you can use other processors to either manually insert these values or parse them from metric labels. The following sections illustration configurations for doing this processing on various platforms\nWhen running OpenTelemetry on GKE, you only need to enable the resource-detection processor to fill out the resource labels. Be sure that your metrics don't already contain any of the reserved resource labels. If this is unavoidable, see [Avoid resource attribute collisions by renamingattributes](#rename-resource-attributes) .\n```\nprocessors:\n resourcedetection:\n detectors: [gcp]\n timeout: 10s\n```\nThis section can be copied directly into your config file, replacing the `processors` section if it already exists.\nThe EKS resource detector does not automatically fill in the `cluster` or `namespace` attributes. You can provide these values manually by using the [resourceprocessor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourceprocessor#resource-processor) , as shown in the following example:\n```\nprocessors:\n resourcedetection:\n detectors: [eks]\n timeout: 10s\n resource:\n attributes:\n - key: \"cluster\"\n  value: \"my-eks-cluster\"\n  action: upsert\n - key: \"namespace\"\n  value: \"my-app\"\n  action: upsert\n```\nYou can also convert these values from metric labels using the `groupbyattrs` processor (see [move metric labels to resource labels](#groupbyattrs) below).\nThe AKS resource detector does not automatically fill in the `cluster` or `namespace` attributes. You can provide these values manually by using the [resourceprocessor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourceprocessor#resource-processor) , as shown in the following example:\n```\nprocessors:\n resourcedetection:\n detectors: [aks]\n timeout: 10s\n resource:\n attributes:\n - key: \"cluster\"\n  value: \"my-eks-cluster\"\n  action: upsert\n - key: \"namespace\"\n  value: \"my-app\"\n  action: upsert\n```\nYou can also convert these values from metric labels by using the `groupbyattrs` processor; see [Move metric labels to resource labels](#groupbyattrs) .\nWith on-premises or non-cloud environments, you probably can't detect any of the necessary resource attributes automatically. In this case, you can emit these labels in your metrics and move them to resource attributes (see [Move metric labels to resource labels](#groupbyattrs) ), or manually set all of the resource attributes as shown in the following example:\n```\nprocessors:\n resource:\n attributes:\n - key: \"cluster\"\n  value: \"my-on-prem-cluster\"\n  action: upsert\n - key: \"namespace\"\n  value: \"my-app\"\n  action: upsert\n - key: \"location\"\n  value: \"us-east-1\"\n  action: upsert\n```\n[Create your collector config as a ConfigMap](#create-configmap) describes how to use the config. That section assumes you have put your config in a file called `config.yaml` .\nThe `project_id` resource attribute can still be automatically set when running the Collector with [Application DefaultCredentials](https://cloud.google.com/docs/authentication/application-default-credentials) . If your Collector does not have access to Application Default Credentials, see [Setting project_id](#set-project-id) .\nAlternatively, you can manually set the resource attributes you need in an environment variable, `OTEL_RESOURCE_ATTRIBUTES` , with a comma-separated list of key/value pairs, for example:\n```\nexport OTEL_RESOURCE_ATTRIBUTES=\"cluster=my-cluster,namespace=my-app,location=us-east-1\"\n```\nThen use the [env resource detectorprocessor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourcedetectionprocessor#environment-variable) to set the resource attributes:\n```\nprocessors:\n resourcedetection:\n detectors: [env]\n```\n### Avoid resource attribute collisions by renaming attributes\nIf your metrics already contain labels that collide with the required resource attributes (such as `location` , `cluster` , or `namespace` ), rename them to avoid the collision. The Prometheus convention is to add the prefix `exported_` to the label name. To add this prefix, use the [transformprocessor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/transformprocessor) .\nThe following `processors` config renames any potential collisions and resolves any conflicting keys from the metric:\n```\nprocessors:\n transform:\n # \"location\", \"cluster\", \"namespace\", \"job\", \"instance\", and \"project_id\" are reserved, and\n # metrics containing these labels will be rejected. Prefix them with exported_ to prevent this.\n metric_statements:\n - context: datapoint\n  statements:\n  - set(attributes[\"exported_location\"], attributes[\"location\"])\n  - delete_key(attributes, \"location\")\n  - set(attributes[\"exported_cluster\"], attributes[\"cluster\"])\n  - delete_key(attributes, \"cluster\")\n  - set(attributes[\"exported_namespace\"], attributes[\"namespace\"])\n  - delete_key(attributes, \"namespace\")\n  - set(attributes[\"exported_job\"], attributes[\"job\"])\n  - delete_key(attributes, \"job\")\n  - set(attributes[\"exported_instance\"], attributes[\"instance\"])\n  - delete_key(attributes, \"instance\")\n  - set(attributes[\"exported_project_id\"], attributes[\"project_id\"])\n  - delete_key(attributes, \"project_id\")\n```\n### Move metric labels to resource labels\nIn some cases, your metrics might be intentionally reporting labels such as `namespace` because your exporter is monitoring multiple namespaces. For example, when running the [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics) exporter.\nIn this scenario, these labels can be moved to resource attributes using the [groupbyattrsprocessor](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/groupbyattrsprocessor) :\n```\nprocessors:\n groupbyattrs:\n keys:\n - namespace\n - cluster\n - location\n```\nIn the above example, given a metric with the labels `namespace` , `cluster` , and/or `location` , those labels will be converted to the matching resource attributes.\n### Limit API requests and memory usage\nTwo other processors, the [batchprocessor](https://github.com/open-telemetry/opentelemetry-collector/tree/main/processor/batchprocessor) and [memory limiterprocessor](https://github.com/open-telemetry/opentelemetry-collector/tree/main/processor/memorylimiterprocessor) allow you to limit the resource consumption of your collector.\nBatching requests lets you define how many data points to send in a single request. Note that Cloud Monitoring has a [limit](https://cloud.google.com/monitoring/quotas) of 200 time series per request. Enable the batch processor by using the following settings:\n```\nprocessors:\n batch:\n # batch metrics before sending to reduce API usage\n send_batch_max_size: 200\n send_batch_size: 200\n timeout: 5s\n```\nWe recommend enabling the memory-limiter processor to prevent your collector from crashing at times of high throughput. Enable the processing by using the following settings:\n```\nprocessors:\n memory_limiter:\n # drop metrics if memory usage gets too high\n check_interval: 1s\n limit_percentage: 65\n spike_limit_percentage: 20\n```\n## Configure the googlemanagedprometheus exporter\nBy default, using the `googlemanagedprometheus` exporter on GKE requires no additional configuration. For many use cases you only need to enable it with an empty block in the `exporters` section:\n```\nexporters:\n googlemanagedprometheus:\n```\nHowever, the exporter does provide some optional configuration settings. The following sections describe the other configuration settings.\n### Setting project_id\nTo associate your time series with a Google Cloud project, the `prometheus_target` monitored resource must have `project_id` set.\nWhen running OpenTelemetry on Google Cloud, the Managed Service for Prometheus exporter defaults to setting this value based on the [Application DefaultCredentials](https://cloud.google.com/docs/authentication/application-default-credentials#personal) it finds. If no credentials are available, or you want to override the default project, you have two options:\n- Set`project`in the exporter config\n- Add a`gcp.project.id`resource attribute to your metrics.\nWe strongly recommend using the default (unset) value for `project_id` rather than explicitly setting it, when possible.\n**Note:** When changing the `project_id` , the Collector's Service Account must have the `roles/monitoring.metricWriter` IAM role for the destination project.\nThe following config excerpt sends metrics to Managed Service for Prometheus in the Google Cloud project `MY_PROJECT` :\n```\nreceivers:\n prometheus:\n config:\n ...\nprocessors:\n resourcedetection:\n detectors: [gcp]\n timeout: 10s\nexporters:\n googlemanagedprometheus:\n project: MY_PROJECT\nservice:\n pipelines:\n metrics:\n  receivers: [prometheus]\n  processors: [resourcedetection]\n  exporters: [googlemanagedprometheus]\n```\nThe only change from previous examples is the new line `project: MY_PROJECT` . This setting is useful if you know that every metric coming through this Collector should be sent to `MY_PROJECT` .\nYou can set project association on a per-metric basis by adding a `gcp.project.id` resource attribute to your metrics. Set the value of the attribute to the name of the project the metric should be associated with.\nFor example, if your metric already has a label `project` , this label can be moved to a resource attribute and renamed to `gcp.project.id` by using processors in the Collector config, as shown in the following example:\n```\nreceivers:\n prometheus:\n config:\n ...\nprocessors:\n resourcedetection:\n detectors: [gcp]\n timeout: 10s\n groupbyattrs:\n keys:\n - project\n resource:\n attributes:\n - key: \"gcp.project.id\"\n  from_attribute: \"project\"\n  action: upsert\nexporters:\n googlemanagedprometheus:\nservice:\n pipelines:\n metrics:\n  receivers: [prometheus]\n  processors: [resourcedetection, groupbyattrs, resource]\n  exporters: [googlemanagedprometheus]\n```\n### Setting client options\nThe `googlemanagedprometheus` exporter uses gRPC clients for Managed Service for Prometheus. Therefore, optional settings are available for configuring the gRPC client:\n- `compression`: Enables gzip compression for gRPC requests, which is useful for minimizing data transfer fees when sending data from other clouds to Managed Service for Prometheus (valid values:`gzip`).\n- `user_agent`: Overrides the user-agent string sent on requests to Cloud Monitoring; only applies to metrics. Defaults to the build and version number of your OpenTelemetry Collector, for example,`opentelemetry-collector-contrib 0.92.0`.\n- `endpoint`: Sets the endpoint to which metric data is going to be sent.\n- `use_insecure`: If true, uses gRPC as the communication transport. Has an effect only when the`endpoint`value is not \"\".\n- `grpc_pool_size`: Sets the size of the connection pool in the gRPC client.\n- `prefix`: Configures the prefix of metrics sent to Managed Service for Prometheus. Defaults to`prometheus.googleapis.com`. Don't change this prefix; doing so causes metrics to not be queryable with PromQL in the Cloud Monitoring UI.\nIn most cases, you don't need to change these values from their defaults. However, you can change them to accommodate special circumstances.\nAll of these settings are set under a `metric` block in the `googlemanagedprometheus` exporter section, as shown in the following example:\n```\nreceivers:\n prometheus:\n config:\n ...\nprocessors:\n resourcedetection:\n detectors: [gcp]\n timeout: 10s\nexporters:\n googlemanagedprometheus:\n metric:\n  compression: gzip\n  user_agent: opentelemetry-collector-contrib 0.92.0\n  endpoint: \"\"\n  use_insecure: false\n  grpc_pool_size: 1\n  prefix: prometheus.googleapis.com\nservice:\n pipelines:\n metrics:\n  receivers: [prometheus]\n  processors: [resourcedetection]\n  exporters: [googlemanagedprometheus]\n```\n## What's next\n- [Use PromQL in Cloud Monitoring to query Prometheus metrics](/stackdriver/docs/managed-prometheus/query-cm) .\n- [Use Grafana to query Prometheus metrics](/stackdriver/docs/managed-prometheus/query) .\n- Set up the [OpenTelemetry Collector as a sidecar agent in Cloud Run](/run/docs/tutorials/custom-metrics-sidecar) .\n- The Cloud Monitoring **Metrics Management** page provides information that can help you control the amount you spend on chargeable metrics without affecting observability. The **Metrics Management** page reports the following information:- Ingestion volumes for both byte- and sample-based billing, across metric  domains and for individual metrics.\n- Data about labels and cardinality of metrics.\n- Use of metrics in alerting policies and custom dashboards.\n- Rate of metric-write errors.\nFor more information about the **Metrics Management** page, see [View and manage metric usage](/monitoring/docs/metrics-management) .", "guide": "Google Cloud Observability"}