{"title": "Google Cloud Observability - Data processing services", "url": "https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring/sli-metrics/data-proc-metrics", "abstract": "# Google Cloud Observability - Data processing services\nThe Google Cloud data services discussed on this page include those that process provided data and output the results of that processing, either in response to a request or continuously. Rather than using availability and latency as the primary SLIs for these services, more appropriate choices are the following:\n- , a measure of how many processing errors the pipeline incurs.\n- , a measure of how quickly data is processed.\nFor further information on data pipelines from the SRE perspective, see [Data Processing Pipelines](https://landing.google.com/sre/workbook/chapters/data-processing/) in the [Site Reliability Engineering Workbook](https://landing.google.com/sre/workbook/toc) .\nYou express a request-based correctness SLI by using the [TimeSeriesRatio](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#TimeSeriesRatio) structure to set up a ratio of items that had processing problems to all items processed. You decide how to filter the metric by using its available labels to arrive at your preferred determination of \"problem\" and \"valid\" totals.\nYou express a request-based freshness SLI by using a [DistributionCut](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#DistributionCut) structure.\n", "content": "## Dataflow\n[Dataflow](/dataflow) is a fully managed streaming analytics service that minimizes latency, processing time, and cost. You can use Dataflow to process data as a stream or in batches using the [Apache Beam SDK](https://beam.apache.org/) .\nFor additional information, see the following:\n- Documentation for [Dataflow](/dataflow/docs) .\n- List of [dataflow.googleapis.com metric types](https://cloud.google.com/monitoring/api/metrics_gcp#gcp-dataflow) .\n### Correctness SLIs\nDataflow writes metric data to Cloud Monitoring using the [dataflow_job](/monitoring/api/resources#tag_dataflow_job) monitored-resource type and the [job/element_count](/monitoring/api/metrics_gcp#dataflow/job/element_count) metric type, which counts the number of elements added to the pcollection so far. Summing across the `job_name` resource label gives you the number of elements to be processed by the job.\nSeparately, you can use the [logging.googleapis.com/log_entry_count](/monitoring/api/metrics_gcp#logging/log_entry_count) metric type with the [dataflow_job](/monitoring/api/resources#tag_dataflow_job) monitored-resource type to count the number of errors logged by a particular job, by using the `severity` metric label.\nYou can use these metrics to express a request-based correctness SLI as a fraction of errors and all processed elements by using a [TimeSeriesRatio](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#TimeSeriesRatio) structure, as shown in the following example:\n```\n\"serviceLevelIndicator\": {\u00a0 \"requestBased\": {\u00a0 \u00a0 \"goodTotalRatio\": {\u00a0 \u00a0 \u00a0 \"totalServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"dataflow.googleapis.com/job/element_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"dataflow_job\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"job_name\\\"=\\\"my_job\\\"\",\u00a0 \u00a0 \u00a0 \"badServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"logging.googleapis.com/log_entry_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"dataflow_job\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"job_name\\\"=\\\"my_job\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"severity\\\"=\\\"error\\\"\",\u00a0 \u00a0 }\u00a0 }}\n```\n### Freshness SLIs\nDataflow also writes metric data to Cloud Monitoring using the [dataflow_job](/monitoring/api/resources#tag_dataflow_job) monitored-resource type and the [job/per_stage_system_lag](/monitoring/api/metrics_gcp#dataflow/job/per_stage_system_lag) metric type, which measures the current maximum duration that an item of data has been processing or awaiting processing.\nYou can express a freshness SLI using this metric by using a [DistributionCut](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#DistributionCut) structure.\nThe following example SLO expects that the oldest data element is processed in under 100 seconds 99% of the time over a rolling one-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"dataflow.googleapis.com/job/per_stage_system_lag\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"dataflow_job\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"job_name\\\"=\\\"my_job\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.99,\u00a0 \"rollingPeriod\": \"3600s\",\u00a0 \"displayName\": \"99% data elements processed under 100 s\"}\n```\nYou can also express a freshness SLI using a [WindowsBasedSli](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#WindowsBasedSli) structure.\nThe following example SLO expects that 99% of five-minute windows over a rolling one-day period see no (zero) elements processed in over 100 seconds:\n```\n{\u00a0 \"displayName\": \"Dataflow - windowed freshness\",\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"windowsBased\": {\u00a0 \u00a0 \u00a0 \"windowPeriod\": \"300s\",\u00a0 \u00a0 \u00a0 \"metricMeanInRange\": {\u00a0 \u00a0 \u00a0 \u00a0 \"timeSeries\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"dataflow.googleapis.com/job/per_stage_system_lag\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"dataflow_job\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"job_name\\\"=\\\"my_job\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": \"0\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": \"100\"\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.99,\u00a0 \"rollingPeriod\": \"86400s\"}\n```\nNote that, for a window to be considered \"good\", the metric cannot exceed the threshold specified in `range` at any point during the evaluation window.\n## Dataproc\n[Dataproc](/dataproc) provides a fully managed, purpose-built cluster that can automatically scale to support any Hadoop or Spark data- or analytics-processing job.\nFor additional information, see the following:\n- Documentation for [Dataproc](/dataproc/docs) .\n- List of [dataproc.googleapis.com metric types](https://cloud.google.com/monitoring/api/metrics_gcp#gcp-dataproc) .\n### Correctness SLIs\nDataproc writes metric data to Cloud Monitoring using the [cloud_dataproc_cluster](/monitoring/api/resources#tag_cloud_dataproc_cluster) monitored-resource type and the following metric types:\n- [cluster/job/submitted_count](/monitoring/api/metrics_gcp#dataproc/cluster/job/submitted_count) , which counts the total number of jobs submitted.\n- [cluster/job/failed_count](/monitoring/api/metrics_gcp#dataproc/cluster/job/failed_count) , which counts the total number of failed jobs.\nYou can use these metrics to express a request-based correctness SLI as a ratio, [TimeSeriesRatio](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#TimeSeriesRatio) , of failed jobs to all submitted jobs, as shown in the following example:\n```\n\"serviceLevelIndicator\": {\u00a0 \"requestBased\": {\u00a0 \u00a0 \"goodTotalRatio\": {\u00a0 \u00a0 \u00a0 \"totalServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"dataproc.googleapis.com/cluster/job/submitted_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"cloud_dataproc_cluster\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"cluster_name\\\"=\\\"my_cluster\\\"\",\u00a0 \u00a0 \u00a0 \"badServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"dataproc.googleapis.com/cluster/job/failed_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"cloud_dataproc_cluster\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"cluster_name\\\"=\\\"my_cluster\\\"\",\u00a0 \u00a0 }\u00a0 }}\n```\n### Freshness SLIs\nDataproc also writes metric data to Cloud Monitoring using the [cloud_dataproc_cluster](/monitoring/api/resources#tag_cloud_dataproc_cluster) monitored-resource type and the following metric types:\n- [cluster/job/duration](/monitoring/api/metrics_gcp#dataproc/cluster/job/duration) , which measures how long jobs stay in processing states. You can filter data on the`state`metric label to identify time spent in specific states. For example, you can create an SLI that measures how long jobs are in the`PENDING`state, to set a maximum allowable wait time before the job begins processing.\n- [cluster/job/completion_time](/monitoring/api/metrics_gcp#dataproc/cluster/job/completion_time) , which measures how long jobs stay in`cluster/job/completion_time`metric. Use when job completion is a well-understood metric or when the volume of data processed by jobs in a cluster doesn't vary, which would affect processing time.\nYou can express a freshness SLI using these metrics by using a [DistributionCut](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#DistributionCut) structure, as shown in the following examples.\nThe following example SLO uses `cluster/job/duration` and expects that 99% of jobs in \"my_cluster\" are in the `PENDING` state for under 100 seconds over a rolling 24-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"dataproc.googleapis.com/cluster/job/duration\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"cloud_dataproc_cluster\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"cluster_name\\\"=\\\"my_cluster\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"state\\\"=\\\"PENDING\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.99,\u00a0 \"rollingPeriod\": \"86400s\",\u00a0 \"displayName\": \"Dataproc pending jobs\"}\n```\nThe following example SLO uses `cluster/job/completion_time` and expects that 99% of jobs in \"my_cluster\" are completed in under 100 seconds over a rolling 24-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"dataproc.googleapis.com/cluster/job/completion_time\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"cloud_dataproc_cluster\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.label.\\\"cluster_name\\\"=\\\"my_cluster\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.99,\u00a0 \"rollingPeriod\": \"86400s\",\u00a0 \"displayName\": \"Dataproc completed jobs\"}\n```", "guide": "Google Cloud Observability"}