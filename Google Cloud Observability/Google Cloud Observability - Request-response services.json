{"title": "Google Cloud Observability - Request-response services", "url": "https://cloud.google.com/stackdriver/docs/solutions/slo-monitoring/sli-metrics/req-resp-metrics", "abstract": "# Google Cloud Observability - Request-response services\nA request-response service is one where a customer explicitly asks the service to do some work and waits for that work to be completed successfully. The most common examples of such services are:\n- Web applications that human users interact with directly by using a browser.\n- Mobile applications that consist of a client application on a user's mobile phone and an API backend that the client interacts with.\n- API backends that are utilized by other services (rather than human users).\nFor all of these services, the common approach is to start with availability (measuring the ratio of successful requests) and latency (measuring the ratio of requests that complete under a time threshold) SLIs. For more information on availability and latency SLIs, see [Concepts in service monitoring](/stackdriver/docs/solutions/slo-monitoring) .\nYou express a request-based availability SLI by using the [TimeSeriesRatio](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#TimeSeriesRatio) structure to set up a ratio of good requests to total requests. You decide how to filter the metric by using its available labels to arrive at your preferred determination of \"good\" or \"valid\".\nYou express a request-based latency SLI by using a [DistributionCut](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#DistributionCut) structure.\n**Note:** The filter strings in some of these examples have been line-wrapped for readability.\n", "content": "## Cloud Endpoints\n[Cloud Endpoints](/endpoints) is a service for managing APIs. It allows you to take an existing API and expose it with authentication, quotas, and monitoring.\nEndpoints is implemented as a proxy in front of the gRPC application server. By measuring the metrics at the proxy, you can correctly handle the case when all backends are unavailable and users are seeing errors. Endpoints writes data to metric types beginning with the prefix `serviceruntime.googleapis.com` .\nFor more information, see the following:\n- Documentation for [Cloud Endpoints](/endpoints/docs) .\n- List of [serviceruntime.googleapis.com metric types](https://cloud.google.com/monitoring/api/metrics_gcp#gcp-serviceruntime) .\n### Availability SLIs\nCloud Endpoints writes metric data to Cloud Monitoring using the [api](/monitoring/api/resources#tag_api) monitored-resource type and the service-runtime [api/request_count](/monitoring/api/metrics_gcp#serviceruntime/api/request_count) metric type, which you can filter by using the `response_code` metric label to count \"good\" and \"total\" requests.\nYou express a request-based availability SLI by creating a [TimeSeriesRatio](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#TimeSeriesRatio) structure for good requests to total requests, as shown in the following example:\n```\n\"serviceLevelIndicator\": {\u00a0 \"requestBased\": {\u00a0 \u00a0 \"goodTotalRatio\": {\u00a0 \u00a0 \u00a0 \"totalServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"serviceruntime.googleapis.com/api/request_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"api\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"response_code_class\\\"!=\\\"4xx\\\"\",\u00a0 \u00a0 \u00a0 \"goodServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"serviceruntime.googleapis.com/api/request_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"api\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0(metric.label.\\\"response_code_class\\\"=\\\"1xx\\\"\" OR\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 metric.label.\\\"response_code_class\\\"=\\\"2xx\\\"\"),\u00a0 \u00a0 }\u00a0 }}\n```\n### Latency SLIs\nCloud Endpoints uses the following primary metric types to capture latency:\n- [api/request_latencies](/monitoring/api/metrics_gcp#serviceruntime/api/request_latencies) : a distribution of latencies in seconds for non-streaming requests. Use whenuser experience is of primary importance.\n- [api/request_latencies_backend](/monitoring/api/metrics_gcp#serviceruntime/api/request_latencies_backend) : a distribution oflatencies in seconds for non-streaming requests. Use to to measure backend latencies directly.\n- [api/request_latencies_overhead](/monitoring/api/metrics_gcp#serviceruntime/api/request_latencies_overhead) : a distribution of request latencies in seconds for non-streaming requests,. Use to measure the overhead introduced by the Endpoints proxy.\nNote that the total request latency is the sum of the backend and overhead latencies:\n```\nrequest_latencies = request_latencies_backend + request_latencies_overhead\n```\nEndpoints writes metric data to Cloud Monitoring using the [api](/monitoring/api/resources#tag_api) monitored-resource type and one of the request-latency metric types. None of these metric types provides a `response_code` or `response_code_class` label; therefore, they report latencies for all requests.\nYou express a request-based latency SLI by using a [DistributionCut](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#DistributionCut) structure, as shown in the following examples.\nThe following example SLO expects that 99% of all requests in the project fall between 0 and 100 ms in total latency over a rolling one-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"serviceruntime.googleapis.com/ap/request_latencies\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"api\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.99,\u00a0 \"rollingPeriod\": \"3600s\",\u00a0 \"displayName\": \"98% requests under 100 ms\"}\n```\nThe following example SLO expects that 98% of requests fall between 0 and 100 ms in backend latency over a rolling one-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"serviceruntime.googleapis.com/api/backend_latencies\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"api\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.98,\u00a0 \"rollingPeriod\": \"3600s\",\u00a0 \"displayName\": \"98% requests under 100 ms\"}\n```\n## Cloud Run\n[Cloud Run](/run) is a fully managed compute platform for deploying and scaling containerized applications quickly and securely. It's intended to abstract away all infrastructure management by responding to changes in traffic by automatically scaling up and down from zero almost instantaneously and only charging you for the exact resources you use.\nFor additional information about Cloud Run observability. see the following:\n- Documentation for [Cloud Run](/run/docs) .\n- List of [run.googleapis.com metric types](https://cloud.google.com/monitoring/api/metrics_gcp#gcp-run) .\n### Availability SLIs\nCloud Run writes metric data to Cloud Monitoring using the [cloud_run_revision](/monitoring/api/resources#tag_cloud_run_revision) monitored-resource type and [request_count](/monitoring/api/metrics_gcp#run/request_count) metric type. You can filter the data by using the `response_code` or the `response_code_class` metric label to count \"good\" and \"total\" requests.\nYou express a request-based availability SLI by creating a [TimeSeriesRatio](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#TimeSeriesRatio) structure for good requests to total requests, as shown in the following example:\n```\n\"serviceLevelIndicator\": {\u00a0 \"requestBased\": {\u00a0 \u00a0 \"goodTotalRatio\": {\u00a0 \u00a0 \u00a0 \"totalServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"run.googleapis.com/request_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"cloud_run_revision\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"response_code_class\\\"!=\\\"4xx\\\"\",\u00a0 \u00a0 \u00a0 \"goodServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"run.googleapis.com/request_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"cloud_run_revision\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0(metric.label.\\\"response_code_class\\\"=\\\"1xx\\\"\" OR\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 metric.label.\\\"response_code_class\\\"=\\\"2xx\\\"\"),\u00a0 \u00a0 \u00a0}\u00a0 }}\n```\n### Latency SLIs\nTo measure latency, Cloud Run writes metric data to Cloud Monitoring using the [cloud_run_revision](/monitoring/api/resources#tag_cloud_run_revision) monitored-resource type and [request_latencies](/monitoring/api/metrics_gcp#run/request_latencies) metric type. The data is a distribution of request latency in milliseconds reaching the revision. You can filter the data by using the `response_code` or the `response_code_class` metric label if you need to explicitly measure the latency of all requests or only the successful requests.\nYou express a request-based latency SLI by using a [DistributionCut](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#DistributionCut) structure. The following example SLO expects that 99% of requests fall between 0 and 100 ms in total latency over a rolling one-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"run.googleapis.com/request_latencies\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"cloud_run_revision\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.99,\u00a0 \"rollingPeriod\": \"3600s\",\u00a0 \"displayName\": \"98% requests under 100 ms\"}\n```\n## Cloud Functions\n[Cloud Functions](/functions) is a scalable pay-as-you-go Functions-as-a-Service offering that runs your code without the need to manage any infrastructure. Functions are used in many architecture patterns to do things like event processing, automation, and serving HTTP/S requests.\nFor information on Cloud Functions observability, see the following:\n- Documentation for [Cloud Functions](/functions/docs) .\n- List of [run.googleapis.com metric types](https://cloud.google.com/monitoring/api/metrics_gcp#gcp-cloudfunctions) .\n### Availability SLIs\nCloud Functions writes metric data to Cloud Monitoring using the [cloud_function](/monitoring/api/resources#tag_cloud_function) monitored-resource type and [function/execution_time](/monitoring/api/metrics_gcp#cloudfunctions/function/execution_time) metric type. You can filter the data by using the `status` metric label to count \"good\" and \"total\" executions.\nYou express a request-based availability SLI by creating a [TimeSeriesRatio](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#TimeSeriesRatio) structure for good requests to total requests, as shown in the following example:\n```\n\"serviceLevelIndicator\": {\u00a0 \"requestBased\": {\u00a0 \u00a0 \"goodTotalRatio\": {\u00a0 \u00a0 \u00a0 \"totalServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"cloudfunctions.googleapis.com/function/execution_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"cloud_function\\\"\",\u00a0 \u00a0 \u00a0 \"goodServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"cloudfunctions.googleapis.com/function/execution_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"cloud_function\\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"status\\\"=\\\"ok\\\"\",\u00a0 \u00a0 \u00a0}\u00a0 }}\n```\n### Latency SLIs\nTo measure latency, Cloud Functions writes metric data to Cloud Monitoring using the [cloud_function](/monitoring/api/resources#tag_cloud_function) monitored-resource type and [function/execution_times](/monitoring/api/metrics_gcp#cloudfunctions/function/execution_times) metric type. The data is a distribution of functions execution times in nanoseconds.\" You can filter the data by using the `status` if you need to explicitly measure the latency of all executions or only the successful executions.\nYou express a request-based latency SLI by using a [DistributionCut](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#DistributionCut) structure. The following example SLO expects that 99% of all Cloud Functions executions fall between 0 and 100 ms in total latency over a rolling one-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"cloudfunctions.googleapis.com/function/execution_times\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"cloud_function\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.99,\u00a0 \"rollingPeriod\": \"3600s\",\u00a0 \"displayName\": \"98% requests under 100 ms\"}\n```\n## App Engine\n[App Engine](/appengine) provides a fully managed serverless platform to build and run applications. You have the choice of two environments, standard or flexible; for more information, see [Choosing an App Engine environments](/appengine/docs/the-appengine-environments) .\nFor more information on App Engine, see the following:\n- Documentation for [App Engine](/appengine/docs) .\n- List of [appengine.googleapis.com metric types](https://cloud.google.com/monitoring/api/metrics_gcp#gcp-appengine) .\n### Availability SLIs\nApp Engine writes metric data to Cloud Monitoring using the [gae_app](/monitoring/api/resources#tag_gae_app) monitored-resource type and the [http/server/response_count](/monitoring/api/metrics_gcp#appengine/http/server/response_count) metric type. You can filter the data by using the `response_code` metric label to count \"good\" and \"total\" responses.\nYou express a request-based availability SLI for App Engine by creating a [TimeSeriesRatio](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#TimeSeriesRatio) structure for good requests to total requests, as shown in the following example:\n```\n\"serviceLevelIndicator\": {\u00a0 \"requestBased\": {\u00a0 \u00a0 \"goodTotalRatio\": {\u00a0 \u00a0 \u00a0 \"totalServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"appengine.googleapis.com/http/server/response_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"gae_app\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"response_code\\\">\\\"499\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"response_code\\\"<\\\"399\\\"\",\u00a0 \u00a0 \u00a0 \"goodServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"appengine.googleapis.com/http/server/response_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"gae_app\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"response_code\\\"<\\\"299\\\"\",\u00a0 \u00a0 \u00a0}\u00a0 }}\n```\n### Latency SLIs\nTo measure latency, App Engine writes metric data to Cloud Monitoring using the [gae_app](/monitoring/api/resources#tag_gae_app) monitored-resource type and the [http/server/response_latencies](/monitoring/api/metrics_gcp#appengine/http/server/response_latencies) metric type. You can filter the data by using the `response_code` metric label to count \"good\" and \"total\" executions.\nYou express a request-based latency SLI for App Engine by using a [DistributionCut](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#DistributionCut) structure. The following example SLO expects that 99% of all requests fall between 0 and 100 ms in total latency over a rolling one-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"appengine.googleapis.com/http/server/response_latencies\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"gae_app\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.99,\u00a0 \"rollingPeriod\": \"3600s\",\u00a0 \"displayName\": \"98% requests under 100 ms\"}\n```\n## GKE and Istio\n[Google Kubernetes Engine (GKE)](/kubernetes-engine) is Google's secured and managed Kubernetes service with four-way auto scaling and multi-cluster support. [Istio](https://istio.io/) is an open-source service mesh that allows you to connect, secure, control, and observe services. Istio can be installed on GKE as an add-on\u2014 [Anthos Service Mesh](/service-mesh/docs/overview) \u2014or by the user from the open source project. In both cases, Istio provides excellent telemetry, including information about traffic, errors, and latency for each service managed by the mesh.\nFor a full list of Istio metrics, see [istio.io metric types](https://cloud.google.com/monitoring/api/metrics_istio) .\n### Availability SLIs\nIstio writes metric data to Cloud Monitoring using the [service/server/request_count](/monitoring/api/metrics_istio#istio/service/server/request_count) metric type and one of the following monitored-resource types:\n- [gce_instance](/monitoring/api/resources#tag_gce_instance) \n- [istio_canonical_service](/monitoring/api/resources#tag_istio_canonical_service) \n- [k8s_container](/monitoring/api/resources#tag_k8s_container) \nYou can filter the data by using the `response_code` metric label to count \"good\" and \"total\" requests. You can also use the `destination_service_name` metric label to count requests for a specific service.\nYou express a request-based availability SLI for a service running on GKE managed by the Istio service mesh by creating a [TimeSeriesRatio](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#TimeSeriesRatio) structure for good requests to total requests, as shown in the following example:\n```\n\"serviceLevelIndicator\": {\u00a0 \"requestBased\": {\u00a0 \u00a0 \"goodTotalRatio\": {\u00a0 \u00a0 \u00a0 \"totalServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"istio.io/service/server/request_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"k8s_container\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"destination_service_name\\\"=\\\"frontend\\\"\",\u00a0 \u00a0 \u00a0 \"goodServiceFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\istio.io/server/response_count\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"k8s_container\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"destination_service_name\\\"=\\\"frontend\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"response_code\\\"<\\\"299\\\"\",\u00a0 \u00a0 }\u00a0 }}\n```\n### Latency SLIs\nTo measure latency, Istio writes metric data to Cloud Monitoring using the [service/server/response_latencies](/monitoring/api/metrics_istio#istio/service/server/response_latencies) metric type and one of the following monitored-resource types:\n- [gce_instance](/monitoring/api/resources#tag_gce_instance) \n- [istio_canonical_service](/monitoring/api/resources#tag_istio_canonical_service) \n- [k8s_container](/monitoring/api/resources#tag_k8s_container) \nYou can filter the data by using the `response_code` metric label to count `\"good\" and \"total\" requests. You can also use the` destination_service_name` metric label to count requests for a specific service.\nYou express a request-based latency SLI for a service running on GKE managed by the Istio service mesh by using a [DistributionCut](/monitoring/api/ref_v3/rest/v3/services.serviceLevelObjectives#DistributionCut) structure. The following example SLO expects that 99% of all requests to the `frontend` service fall between 0 and 100 ms in total latency over a rolling one-hour period:\n```\n{\u00a0 \"serviceLevelIndicator\": {\u00a0 \u00a0 \"requestBased\": {\u00a0 \u00a0 \u00a0 \"distributionCut\": {\u00a0 \u00a0 \u00a0 \u00a0 \"distributionFilter\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"metric.type=\\\"istio.io/server/response_latencies\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0resource.type=\\\"k8s_container\\\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0metric.label.\\\"destination_service_name\\\"=\\\"frontend\\\"\",\u00a0 \u00a0 \u00a0 \u00a0 \"range\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"min\": 0,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"max\": 100\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 \"goal\": 0.99,\u00a0 \"rollingPeriod\": \"3600s\",\u00a0 \"displayName\": \"98% requests under 100 ms\"}\n```", "guide": "Google Cloud Observability"}