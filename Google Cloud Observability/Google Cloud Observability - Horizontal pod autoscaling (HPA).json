{"title": "Google Cloud Observability - Horizontal pod autoscaling (HPA)", "url": "https://cloud.google.com/stackdriver/docs/managed-prometheus/hpa", "abstract": "# Google Cloud Observability - Horizontal pod autoscaling (HPA)\nThis document describes how to enable horizontal pod autoscaling (HPA) for Google Cloud Managed Service for Prometheus. You can enable HPA by doing one of the following:\n- Using the [Custom Metrics Stackdriver Adapter](https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/custom-metrics-stackdriver-adapter) library, developed and supported by Google Cloud.\n- Using the third-party [Prometheus Adapter](https://github.com/kubernetes-sigs/prometheus-adapter/) library.\nYou must choose one approach or the other. You can't use both because their resource definitions overlap, as described in [Troubleshooting](#hpa-troubleshooting) .\n", "content": "## Use the Custom Metrics Stackdriver Adapter\nThe Custom Metrics Stackdriver Adapter supports querying metrics from Managed Service for Prometheus starting with [version v0.13.1 of the adapter](https://github.com/GoogleCloudPlatform/k8s-stackdriver/releases/tag/cm-sd-adapter-v0.13.1) .\nTo set up an example HPA configuration using the Custom Metrics Stackdriver Adapter, do the following:\n- Set up [managed collection](/stackdriver/docs/managed-prometheus/setup-managed) in your cluster.\n- Install Custom Metrics Stackdriver Adapter in your cluster.```\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-stackdriver/master/custom-metrics-stackdriver-adapter/deploy/production/adapter_new_resource_model.yaml\n```\n- Deploy an example Prometheus metrics exporter and an HPA resource:```\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-stackdriver/master/custom-metrics-stackdriver-adapter/examples/prometheus-to-sd/custom-metrics-prometheus-sd.yaml\n```This command deploys an exporter application that emits the metric `foo` and an HPA resource. The HPA scales this application up to 5 replicas to achieve the target value for the metric `foo` .\n- Define a PodMonitoring resource by placing the following configuration in a file named `podmonitoring.yaml` .```\napiVersion: monitoring.googleapis.com/v1kind: PodMonitoringmetadata:\u00a0 name: prom-examplespec:\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 run: custom-metric-prometheus-sd\u00a0 endpoints:\u00a0 - port: 8080\u00a0 \u00a0 interval: 30s\n```\n- Deploy the new PodMonitoring resource:```\nkubectl -n default apply -f podmonitoring.yaml\n```Within a couple of minutes, Managed Service for Prometheus processes the metrics scraped from the exporter and stores them in Cloud Monitoring using a long-form name. Prometheus metrics are stored with the following conventions:- The prefix`prometheus.googleapis.com`.\n- This suffix is usually one of`gauge`,`counter`,`summary`, or`histogram`, although untyped metrics might have the`unknown`or`unknown:counter`suffix. To verify the suffix, look up the metric in Cloud Monitoring by using Metrics Explorer.\n- Update the deployed HPA to query the metric from Cloud Monitoring. The metric `foo` is ingested as `prometheus.googleapis.com/foo/gauge` . To make the metric queryable by the deployed HorizontalPodAutoscaler resource, you use the long-form name in the deployed HPA, but you have to modify it by replacing the all forward slashes ( `/` ) with the pipe character ( `|` ): `prometheus.googleapis.com|foo|gauge` . For more information, see the [Metrics available from Stackdriver section](https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/custom-metrics-stackdriver-adapter#metrics-available-from-stackdriver) of the Custom Metrics Stackdriver Adapter repository.- Update the deployed HPA by running the following command:```\nkubectl edit hpa custom-metric-prometheus-sd\n```\n- Change the value of the `pods.metric.name` field from `foo` to `prometheus.googleapis.com|foo|gauge` . The `spec` section should look like the following:```\nspec:\u00a0 \u00a0maxReplicas: 5\u00a0 \u00a0metrics:\u00a0 \u00a0- pods:\u00a0 \u00a0 \u00a0 \u00a0metric:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0name: prometheus.googleapis.com|foo|gauge\u00a0 \u00a0 \u00a0 \u00a0target:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0averageValue: \"20\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0type: AverageValue\u00a0 \u00a0 \u00a0type: Pods\u00a0 \u00a0minReplicas: 1\n```\nIn this example, the HPA configuration looks for the average value of the metric `prometheus.googleapis.com/foo/gauge` to be `20` . Because the Deployment sets the value of the metric is `40` , the HPA controller increases the number of pods up to the value of the `maxReplicas` ( `5` ) field to try to reduce the average value of the metric across all pods to `20` .The HPA query is scoped to the namespace and cluster in which the HPA resource is installed, so identical metrics in other clusters and namespaces don't affect your autoscaling.\n- To watch the workload scale up, run the following command:```\nkubectl get hpa custom-metric-prometheus-sd --watch\n```The value of the `REPLICAS` field changes from `1` to `5` .```\nNAME       REFERENCE        TARGETS  MINPODS MAXPODS REPLICAS AGE\ncustom-metric-prometheus-sd Deployment/custom-metric-prometheus-sd 40/20   1   5   5   *\n```\n- To scale down the deployment, update the target metric value to be higher than the exported metric value. In this example, the Deployment sets the value of the `prometheus.googleapis.com/foo/gauge` metric to `40` . If you set the target value to a number that is higher than `40` , then the deployment will scale down.For example, use `kubectl edit` to change the value of the `pods.target.averageValue` field in the HPA configuration from `20` to `100` .```\nkubectl edit hpa custom-metric-prometheus-sd\n```Modify the spec section to match the following:```\nspec:\u00a0 maxReplicas: 5\u00a0 metrics:\u00a0 - pods:\u00a0 \u00a0 \u00a0 metric:\u00a0 \u00a0 \u00a0 \u00a0 name: prometheus.googleapis.com|foo|gauge\u00a0 \u00a0 \u00a0 target:\u00a0 \u00a0 \u00a0 \u00a0 averageValue: \"100\"\u00a0 \u00a0 \u00a0 \u00a0 type: AverageValue\u00a0 type: Pods\u00a0 minReplicas: 1\n```\n- To watch the workload scale down, run the following command:```\nkubectl get hpa custom-metric-prometheus-sd --watch\n```The value of the `REPLICAS` field changes from `5` to `1` . By design, this happens more slowly than when scaling the number of pods up:```\nNAME       REFERENCE        TARGETS  MINPODS MAXPODS REPLICAS AGE\ncustom-metric-prometheus-sd Deployment/custom-metric-prometheus-sd 40/100   1   5   1   *\n```\n- To clean up the deployed example, run the following commands:```\nkubectl delete -f https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-stackdriver/master/custom-metrics-stackdriver-adapter/deploy/production/adapter_new_resource_model.yamlkubectl delete -f https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-stackdriver/master/custom-metrics-stackdriver-adapter/examples/prometheus-to-sd/custom-metrics-prometheus-sd.yamlkubectl delete podmonitoring/prom-example\n```\nFor more information, see the [Prometheus example](https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/custom-metrics-stackdriver-adapter/examples/prometheus-to-sd) in the Custom Metrics Stackdriver Adapter repository, or see [Scaling an application](/kubernetes-engine/docs/how-to/scaling-apps) .\n## Use the Prometheus Adapter\nExisting prometheus-adapter configs can be used to autoscale with only a few changes. Configuring prometheus-adapter to scale using Managed Service for Prometheus has two additional restrictions compared to scaling using upstream Prometheus:\n- Queries must be routed through the Prometheus frontend UI proxy, just like when [querying Managed Service for Prometheus using the Prometheus API orUI](/stackdriver/docs/managed-prometheus/query-api-ui) . For prometheus-adapter, you need to [edit the prometheus-adapterDeployment](https://github.com/kubernetes-sigs/prometheus-adapter/blob/7bdb7f14b922298df2de392d5450205467b32404/deploy/manifests/deployment.yaml#L34) to change the `prometheus-url` value as follows:```\n--prometheus-url=http://frontend.NAMESPACE_NAME.svc:9090/\n```where is the namespace where the frontend is deployed.\n- You cannot use a regex matcher on a metric name in the `.seriesQuery` field of the rules config. Instead you must fully specify metric names.\nAs data can take slightly longer to be available within Managed Service for Prometheus compared to upstream Prometheus, configuring overly-eager autoscaling logic might cause unwanted behavior. Although there is no guarantee on data freshness, data is typically available to query 3-7 seconds after it is sent to Managed Service for Prometheus, excluding any network latency.\nAll queries issued by prometheus-adapter are global in scope. This means that if you have applications in two namespaces that emit identically named metrics, an HPA configuration using that metric scales using data from both applications. We recommend always using `namespace` or `cluster` filters in your PromQL to avoid scaling using incorrect data.\nTo set up an example HPA configuration using prometheus-adapter and managed collection, use the following steps:\n- Set up [managed collection](/stackdriver/docs/managed-prometheus/setup-managed) in your cluster.\n- Deploy the [Prometheus frontend UI proxy](/stackdriver/docs/managed-prometheus/query-api-ui#promui-deploy) in your cluster. If you use Workload Identity, you must also [configure andauthorize a service account](/stackdriver/docs/managed-prometheus/query-api-ui#gmp-wli-svcacct) .\n- Deploy the manifests in the [examples/hpa/ directory within theprometheus-engine repo](https://github.com/GoogleCloudPlatform/prometheus-engine/tree/main/examples/hpa) :- `example-app.yaml`: An example deployment and service that emits metrics.\n- `pod-monitoring.yaml`: A resource that configures scraping the example metrics.\n- `hpa.yaml`: The HPA resource that configures scaling for your workload.\n- Ensure `prometheus-adapter` is installed in your cluster. This can be done by deploying the [example install manifest](https://github.com/GoogleCloudPlatform/prometheus-engine/blob/91fa0b34ecbb5094986d1739508a050934f7d434/examples/hpa/prometheus-adapter.yaml) to your cluster. This manifest is configured to:- Query a [frontend proxy](/stackdriver/docs/managed-prometheus/query-api-ui#promui-deploy) deployed in the`default`namespace.\n- Issue PromQL to calculate and return the`http_requests_per_second`metric from the example deployment.\n **Note:** The `http_requests_per_second` metric won't be available until load is generated against the example application. **Note:** You might need to [install an internal firewall rule](/stackdriver/docs/managed-prometheus/troubleshooting#control-plane-firewall) on port `6443` from the control plane to your nodes.\n- Run the following commands, each in a separate terminal session:- Generate HTTP load against the`prometheus-example-app`service:```\nkubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c \"while sleep 0.01; do wget -q -O- http://prometheus-example-app; done\"\n```\n- Watch the horizontal pod autoscaler:```\nkubectl get hpa prometheus-example-app --watch\n```\n- Watch the workload scale up:```\nkubectl get po -lapp.kubernetes.io/name=prometheus-example-app --watch\n```\n- Stop HTTP load generation by using Ctrl+C and watch the workload scale back down.## Troubleshooting\nCustom Metrics Stackdriver Adapter uses resource definitions with the same names as those in the Prometheus Adapter, [prometheus-adapter](https://github.com/kubernetes-sigs/prometheus-adapter/) . This overlap in names means that running more than one adapter in the same cluster [causes errors](https://github.com/kubernetes-sigs/custom-metrics-apiserver/issues/70) .\nInstalling the Prometheus Adapter in a cluster that previously had the Custom Metrics Stackdriver Adapter installed might throw errors such as `FailedGetObjectMetric` due to colliding names. To resolve this, you might have to delete the `v1beta1.external.metrics.k8s.io` , `v1beta1.custom.metrics.k8s.io` , and `v1beta2.custom.metrics.k8s.io` apiservices previously registered by the Custom Metrics Adapter.\nTroubleshooting tips:\n- Some Cloud Monitoring system metrics such as [Pub/Submetrics](/monitoring/api/metrics_gcp#gcp-pubsub) are delayed by 60 seconds or more. As Prometheus Adapter executes queries using the current timestamp, querying these metrics using the Prometheus Adapter might incorrectly result in no data. To query delayed metrics, use the [offset modifier in PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/#offset-modifier) to change your query's time offset by the necessary amount.\n- To verify that the [frontend UI proxy](/stackdriver/docs/managed-prometheus/query-api-ui#promui-deploy) is working as intended and there are no issues with permissions, run the following command in a terminal:```\nkubectl -n NAMESPACE_NAME port-forward svc/frontend 9090\n```Next, open another terminal and run the following command:```\ncurl --silent 'localhost:9090/api/v1/series?match%5B%5D=up'\n```When the frontend UI proxy is working properly, the response in the second terminal is similar to the following:```\ncurl --silent 'localhost:9090/api/v1/series?match%5B%5D=up' | jq .\n{\n \"status\": \"success\",\n \"data\": [  ...\n ]\n}\n```If you receive a 403 error, then then frontend UI proxy isn't properly configured. For information about how to resolve a 403 error, see [configureand authorize a service account](/stackdriver/docs/managed-prometheus/query-api-ui#gmp-wli-svcacct) guide.\n- To verify that the custom metrics apiserver is available, run the following command:```\nkubectl get apiservices.apiregistration.k8s.io v1beta1.custom.metrics.k8s.io\n```When the apiserver is available, the response is similar to the following:```\n$ kubectl get apiservices.apiregistration.k8s.io v1beta1.custom.metrics.k8s.io\nNAME       SERVICE       AVAILABLE AGE\nv1beta1.custom.metrics.k8s.io monitoring/prometheus-adapter True  33m\n```\n- To verify that your HPA is working as intended, run the following command:```\n$ kubectl describe hpa prometheus-example-app\nName:         prometheus-example-app\nNamespace:        default\nLabels:        \nAnnotations:       \nReference:        Deployment/prometheus-example-app\nMetrics:        ( current / target )\n\"http_requests_per_second\" on pods: 11500m / 10\nMin replicas:       1\nMax replicas:       10\nDeployment pods:      2 current / 2 desired\nConditions:\nType   Status Reason    Message\n----   ------ ------    ------AbleToScale  True ReadyForNewScale recommended size matches current size\nScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from pods metric http_requests_per_second\nScalingLimited False DesiredWithinRange the desired count is within the acceptable range\nEvents:\nType  Reason    Age     From      Message\n----  ------    ----     ----      ------Normal SuccessfulRescale 47s     horizontal-pod-autoscaler New size: 2; reason: pods metric http_requests_per_second above target\n```When the response contains a statement like `FailedGetPodsMetric` , then the HPA failing. The following illustrates a response to the `describe` call when the HPA is failing:```\n$ kubectl describe hpa prometheus-example-app\nName:         prometheus-example-app\nNamespace:        default\nReference:        Deployment/prometheus-example-app\nMetrics:        ( current / target )\n \"http_requests_per_second\" on pods: / 10\nMin replicas:       1\nMax replicas:       10\nDeployment pods:      1 current / 1 desired\nConditions:\n Type   Status Reason    Message\n ----   ------ ------    ------ AbleToScale  True ReadyForNewScale  recommended size matches current size\n ScalingActive False FailedGetPodsMetric the HPA was unable to compute the replica count: unable to get metric http_requests_per_second: unable to fetch metrics from custom metrics API: the server could not find the metric http_requests_per_second for pods\n ScalingLimited False DesiredWithinRange the desired count is within the acceptable range\nEvents:\n Type  Reason    Age     From      Message\n ----  ------    ----     ----      ------ Warning FailedGetPodsMetric 104s (x11 over 16m) horizontal-pod-autoscaler unable to get metric http_requests_per_second: unable to fetch metrics from custom metrics API: the server could not find the metric http_requests_per_second for pods\n```When the HPA is failing, make sure you are generating metrics with the `load-generator` . You can check the custom metrics api directly, with the command:```\nkubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/\" | jq .\n```A successful output should look like below:```\n$ kubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/\" | jq .\n {\n \"kind\": \"APIResourceList\",\n \"apiVersion\": \"v1\",\n \"groupVersion\": \"custom.metrics.k8s.io/v1beta1\",\n \"resources\": [  {\n  \"name\": \"namespaces/http_requests_per_second\",\n  \"singularName\": \"\",\n  \"namespaced\": false,\n  \"kind\": \"MetricValueList\",\n  \"verbs\": [  \"get\"\n  ]\n  },\n  {\n  \"name\": \"pods/http_requests_per_second\",\n  \"singularName\": \"\",\n  \"namespaced\": true,\n  \"kind\": \"MetricValueList\",\n  \"verbs\": [  \"get\"\n  ]\n  }\n ]\n }\n```If there are no metrics, there will be no data under `\"resources\"` in the output, for example:```\nkubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/\" | jq .\n{\n\"kind\": \"APIResourceList\",\n\"apiVersion\": \"v1\",\n\"groupVersion\": \"custom.metrics.k8s.io/v1beta1\",\n\"resources\": []\n}\n```", "guide": "Google Cloud Observability"}