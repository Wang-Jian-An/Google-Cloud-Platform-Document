{"title": "Google Cloud Observability - NVIDIA Data Center GPU Manager (DCGM)", "url": "https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/third-party-nvidia", "abstract": "# Google Cloud Observability - NVIDIA Data Center GPU Manager (DCGM)\nThe NVIDIA Data Center GPU Manager integration collects key advanced GPU metrics from DCGM, including Streaming Multiprocessor (SM) block utilization, SM occupancy, SM pipe utilization, PCIe traffic rate, and NVLink traffic rate. For information about the purpose and interpretation of these metrics, see [Profiling Metrics](https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/feature-overview.html#profiling-metrics) in the DCGM feature overview.\nFor more information about the NVIDIA Data Center GPU Manager, see the [DCGMdocumentation](https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/index.html) . This integration is compatible with DCGM version 3.1 and later.\nThe Ops Agent collects DCGM metrics by using NVIDIA's client library [go-dcgm](https://github.com/NVIDIA/go-dcgm) .\n**These metrics are available for Linux systems only.** Metrics are not collected from NVIDIA GPU models K80, P100, and P4.\n", "content": "## Prerequisites\nTo collect DCGM metrics, you must do the following:\n- [Install DCGM](#install-dcgm) .\n- [Install the Ops Agent](/stackdriver/docs/solutions/agents/ops-agent/install-index) .  Only Ops Agent version 2.38.0 or versions 2.41.0 or newer are compatible with GPU monitoring. **Do not installOps Agent versions 2.39.0and 2.40.0 on VMs with attached GPUs.** For more information, see [Agent crashes and report mentions NVIDIA](/stackdriver/docs/solutions/agents/ops-agent/troubleshoot-install-startup#nvidia-crashes) .\n### Install DCGM and verify installation\nYou must install a DCGM version 3.1 and later and ensure that it runs as a privileged service. To install DCGM, see [Installation](https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/getting-started.html#id1) in the DCGM documentation.\nTo verify that DCGM is running correctly, do the following:\n- Check the status of the DCGM service by running the following command:```\nsudo service nvidia-dcgm status\n```If the service is running, the `nvidia-dcgm` service is listed as `active (running)` . The output resembles the following:```\n\u25cf nvidia-dcgm.service - NVIDIA DCGM service\nLoaded: loaded (/usr/lib/systemd/system/nvidia-dcgm.service; disabled; vendor preset: enabled)\nActive: active (running) since Sat 2023-01-07 15:24:29 UTC; 3s ago\nMain PID: 24388 (nv-hostengine)\nTasks: 7 (limit: 14745)\nCGroup: /system.slice/nvidia-dcgm.service\n  \u2514\u250024388 /usr/bin/nv-hostengine -n --service-account nvidia-dcgm\n```\n- Verify that the GPU devices are found by running the following command:```\ndcgmi discovery --list\n```If devices are found, the output resembles the following:```\n1 GPU found.\n+--------+----------------------------------------------------------------------+\n| GPU ID | Device Information             |\n+--------+----------------------------------------------------------------------+\n| 0  | Name: NVIDIA A100-SXM4-40GB           |\n|  | PCI Bus ID: 00000000:00:04.0           |\n|  | Device UUID: GPU-a2d9f5c7-87d3-7d57-3277-e091ad1ba957    |\n+--------+----------------------------------------------------------------------+\n```## Configure the Ops Agent for DCGM\nFollowing the guide for [Configuring the OpsAgent](/stackdriver/docs/solutions/agents/ops-agent/configuration#file-location) , add the required elements to collect telemetry from your DCGM service, and [restart the agent](/stackdriver/docs/solutions/agents/ops-agent/installation#restart) .\n### Example configuration\nThe following commands create the configuration to collect and ingest telemetry for DCGM and restart the Ops Agent:\n```\n# Configures Ops Agent to collect telemetry from the app and restart Ops Agent.\nset -e\n# Create a back up of the existing file so existing configurations are not lost.\nsudo cp /etc/google-cloud-ops-agent/config.yaml /etc/google-cloud-ops-agent/config.yaml.bak\n# Configure the Ops Agent.\nsudo tee /etc/google-cloud-ops-agent/config.yaml > /dev/null << EOF\nmetrics:\n receivers:\n dcgm:\n  type: dcgm\n service:\n pipelines:\n  dcgm:\n  receivers:\n   - dcgm\nEOF\nsudo systemctl restart google-cloud-ops-agent\n```\nAfter running these commands, you can check that the agent restarted. Run the following command and verify that the sub-agent components \"Metrics Agent\" and \"Logging Agent\" are listed as \"active (running)\":\n```\nsudo systemctl status google-cloud-ops-agent\"*\"\n```\n**Note:** If you restarted the Ops Agent by using the command `sudo service google-cloud-ops-agent restart` and the sub-agents did not restart correctly, then use the `systemctl restart` command shown in the configuration commands. For some Deep Learning VM Images, the Ops Agent doesn't restart all the dependencies when you restart with `service restart` .\nIf you are using custom service account instead of the default Compute Engine service account, or if you have a very old Compute Engine VM, then you might need to [authorize the Ops Agent](/stackdriver/docs/solutions/agents/ops-agent/authorization) .\n### Configure metrics collection\nTo ingest metrics from DCGM, you must create a receiver for the metrics that DCGM produces and then create a pipeline for the new receiver.\nThis receiver does not support the use of multiple instances in the configuration, for example, to monitor multiple endpoints. All such instances write to the same time series, and Cloud Monitoring has no way to distinguish among them.\nTo configure a receiver for your `dcgm` metrics, specify the following fields:\n| Field    | Default  | Description           |\n|:--------------------|:---------------|:-----------------------------------------------------|\n| collection_interval | 60s   | A time duration, such as 30s or 5m.     |\n| endpoint   | localhost:5555 | Address of the DCGM service, formatted as host:port. |\n| type    | nan   | This value must be dcgm.        |\nThe following table provides the list of metrics that the Ops Agent collects from the DCGM service. Not all metrics are available for all GPU models. Metrics are not collected from NVIDIA GPU models K80, P100, and P4.\n| ('Metric\\xa0type', 'Kind,\\xa0Type Monitored\\xa0resources')  | ('Metric\\xa0type', 'Labels')         | ('Metric\\xa0type', 'Supported GPU models')         |\n|:---------------------------------------------------------------|:---------------------------------------------------------------|:-----------------------------------------------------------------------------|\n| workload.googleapis.com/dcgm.gpu.profiling.dram_utilization | workload.googleapis.com/dcgm.gpu.profiling.dram_utilization | workload.googleapis.com/dcgm.gpu.profiling.dram_utilization     |\n| GAUGE,\u00a0DOUBLE gce_instance          | gpu_number model uuid           | All except K80, P100, and P4             |\n| workload.googleapis.com/dcgm.gpu.profiling.nvlink_traffic_rate | workload.googleapis.com/dcgm.gpu.profiling.nvlink_traffic_rate | workload.googleapis.com/dcgm.gpu.profiling.nvlink_traffic_rate    |\n| GAUGE,\u00a0INT64 gce_instance          | direction gpu_number model uuid        | All except K80, P100, and P4             |\n| workload.googleapis.com/dcgm.gpu.profiling.pcie_traffic_rate | workload.googleapis.com/dcgm.gpu.profiling.pcie_traffic_rate | workload.googleapis.com/dcgm.gpu.profiling.pcie_traffic_rate     |\n| GAUGE,\u00a0INT64 gce_instance          | direction gpu_number model uuid        | All except K80, P100, and P4             |\n| workload.googleapis.com/dcgm.gpu.profiling.pipe_utilization | workload.googleapis.com/dcgm.gpu.profiling.pipe_utilization | workload.googleapis.com/dcgm.gpu.profiling.pipe_utilization     |\n| GAUGE,\u00a0DOUBLE gce_instance          | gpu_number model pipe uuid          | All except K80, P100, and P4. For L4, the pipe value fp64 is not supported. |\n| workload.googleapis.com/dcgm.gpu.profiling.sm_occupancy  | workload.googleapis.com/dcgm.gpu.profiling.sm_occupancy  | workload.googleapis.com/dcgm.gpu.profiling.sm_occupancy      |\n| GAUGE,\u00a0DOUBLE gce_instance          | gpu_number model uuid           | All except K80, P100, and P4             |\n| workload.googleapis.com/dcgm.gpu.profiling.sm_utilization  | workload.googleapis.com/dcgm.gpu.profiling.sm_utilization  | workload.googleapis.com/dcgm.gpu.profiling.sm_utilization     |\n| GAUGE,\u00a0DOUBLE gce_instance          | gpu_number model uuid           | All except K80, P100, and P4             |\nIn addition, the built-in configuration for the Ops Agent also collects [agent.googleapis.com/gpumetrics](/monitoring/api/metrics_opsagent#agent-gpu) , which are reported by the NVIDIA [Management Library (NVML)](https://developer.nvidia.com/nvidia-management-library-nvml) . You do not need any additional configuration in the Ops Agent to collect these metrics, but you must [create your VM with attached GPUs](https://cloud.google.com/compute/docs/gpus/create-vm-with-gpus) and [install the GPU driver](https://cloud.google.com/compute/docs/gpus/install-drivers-gpu) . For more information, see [About thegpu metrics](/stackdriver/docs/solutions/agents/ops-agent/configuration#receiver-nvml-metrics) .\n## Verify the configuration\nThis section describes how to verify that you correctly configured the NVIDIA DCGM receiver. It might take one or two minutes for the Ops Agent to begin collecting telemetry.\nTo verify that NVIDIA DCGM metrics are being sent to Cloud Monitoring, do the following:\n- In the navigation panel of the Google Cloud console, select **Monitoring** , and then select **Metrics explorer** : [Go to Metrics explorer](https://console.cloud.google.com/monitoring/metrics-explorer) \n- In the toolbar of the query-builder pane, select the button whose name is either **MQL** or **PromQL** .\n- Verify that **MQL** is selected in the **Language** toggle. The language toggle is in the same toolbar that lets you format your query.\n- Enter the following query in the editor, and then click **Run query** :```\nfetch gce_instance\n| metric 'workload.googleapis.com/dcgm.gpu.profiling.sm_utilization'\n| every 1m\n```\n## View dashboard\nTo view your NVIDIA DCGM metrics, you must have a chart or dashboard configured. The NVIDIA DCGM integration includes one or more dashboards for you. Any dashboards are automatically installed after you configure the integration and the Ops Agent has begun collecting metric data.\nYou can also view static previews of dashboards without installing the integration.\nTo view an installed dashboard, do the following:\n- In the navigation panel of the Google Cloud console, select **Monitoring** , and then select **Dashboards** : [Go to Dashboards](https://console.cloud.google.com/monitoring/dashboards) \n- Select the **Dashboard List** tab, and then choose the **Integrations** category.\n- Click the name of the dashboard you want to view.If you have configured an integration but the dashboard has not been installed, then check that the Ops Agent is running. When there is no metric data for a chart in the dashboard, installation of the dashboard fails. After the Ops Agent begins collecting metrics, the dashboard is installed for you.\nTo view a static preview of the dashboard, do the following:\n- In the navigation panel of the Google Cloud console, select **Monitoring** , and then select **Integrations** : [Go to Integrations](https://console.cloud.google.com/monitoring/integrations) \n- Click the **Compute Engine** deployment-platform filter.\n- Locate the entry for     NVIDIA DCGM   and click **View Details** .\n- Select the **Dashboards** tab to see a static preview. If the   dashboard is installed, then you can navigate to it by clicking **View dashboard** .For more information about dashboards in Cloud Monitoring, see [Dashboards and charts](/monitoring/dashboards) .\nFor more information about using the **Integrations** page, see [Manage integrations](/monitoring/agent/integrations) .\n## DCGM limitations, and pausing profiling\nConcurrent usage of DCGM can conflict with usage of some other NVIDIA developer tools, such as Nsight Systems or Nsight Compute. This limitation applies to NVIDIA A100 and earlier GPUs. For more information, see [Profiling Sampling Rate](https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/feature-overview.html#profiling-sampling-rate) in the DCGM feature overiew.\nWhen you need to use tools like Nsight Systems without significant disruption, you can temporarily pause or resume the metrics collection by using the following commands:\n```\ndcgmi profile --pause\ndcgmi profile --resume\n```\nWhen profiling is paused, none of the DCGM metrics that the Ops Agent collects are emitted from the VM.\n## What's next\nFor a walkthrough on how to use Ansible to install the Ops Agent, configure a third-party application, and install a sample dashboard, see the [ Install the Ops Agent to troubleshoot third-party applications](https://www.youtube.com/watch?v=GQgNygd-XJU&t=7s) video.", "guide": "Google Cloud Observability"}