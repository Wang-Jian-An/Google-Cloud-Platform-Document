{"title": "Google Cloud Observability - Configure the Ops Agent", "url": "https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/configuration", "abstract": "# Google Cloud Observability - Configure the Ops Agent\nThis document provides details about the Ops Agent's default and custom configurations. Read this document if any of the following applies to you:\n- You want to change the configuration of the Ops Agent to achieve the following goals:- Turn off the built-in logging or metrics ingestion.- To turn off logging ingestion, see [Example logging serviceconfigurations](#logging-service-examples) .\n- To turn off host-metrics ingestion, see [Example metrics serviceconfigurations](#metrics-service-examples) .\n- Customize the file path of the log files that the agent collects logs from; see [Logging receivers](#logging-receivers) .\n- Customize the structured log format that the agent uses to process the log entries, by parsing the JSON or by using regular expressions; see [Logging processors](#logging-processors) .\n- Change the sampling rate for metrics; see [Metricsreceivers](#metrics-receivers) .\n- Customize which group or groups of metrics to enable. The agent collects all system metrics, like `cpu` and `memory` , by default; see [Metrics processors](#metrics-processors) .\n- Customize how the agent rotates logs; see [Log-rotationconfiguration](#log-rotation) .\n- Collect metrics and logs from supported third-party applications. See [Monitor third-party applications](/stackdriver/docs/solutions/agents/ops-agent/third-party) for the list of supported applications.\n- Use the [Prometheus receiver](/stackdriver/docs/solutions/agents/ops-agent/prometheus) to collect custom metrics.\n- Use the [OpenTelemetry Protocol (OTLP) receiver](/stackdriver/docs/solutions/agents/ops-agent/otlp) to collect custom metrics and traces.\n- You're interested in learning the technical details of the Ops Agent's configuration.", "content": "## Configuration model\nThe Ops Agent uses a built-in default configuration; you can't directly modify this built-in configuration. Instead, you create a file of overrides that are merged with the built-in configuration when the agent restarts.\nThe building blocks of the configuration are as follows:\n- `receivers`: This element describes what is collected by the agent.\n- `processors`: This element describes how the agent can modify the collected information.\n- `service`: This element links receivers and processors together to create data flows, called. The`service`element contains a`pipelines`element, which can contain multiple pipelines.\nThe built-in configuration is made up of these elements, and you use the same elements to override that built-in configuration.\n**Note:** The Ops Agent sends logs to Cloud Logging and metrics to Cloud Monitoring. You can't configure the agent to export logs or metrics to other services. You can, however, configure Cloud Logging to export logs; for more information, see [Route logs to supporteddestinations](/logging/docs/export/configure_export_v2) .\n### Built-in configuration\nThe built-in configuration for the Ops Agent defines the default collection for logs and metrics. The following shows the built-in configuration for Linux and for Windows:\nBy default, the Ops Agent collects file-based `syslog` logs and host metrics.\nFor more information about the metrics collected, see [Metrics ingested by the receivers](#metrics-by-receiver-type) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/ops-agent/blob/HEAD/confgenerator/built-in-config-linux.yaml) \n```\nlogging:\u00a0 receivers:\u00a0 \u00a0 syslog:\u00a0 \u00a0 \u00a0 type: files\u00a0 \u00a0 \u00a0 include_paths:\u00a0 \u00a0 \u00a0 - /var/log/messages\u00a0 \u00a0 \u00a0 - /var/log/syslog\u00a0 service:\u00a0 \u00a0 pipelines:\u00a0 \u00a0 \u00a0 default_pipeline:\u00a0 \u00a0 \u00a0 \u00a0 receivers: [syslog]metrics:\u00a0 receivers:\u00a0 \u00a0 hostmetrics:\u00a0 \u00a0 \u00a0 type: hostmetrics\u00a0 \u00a0 \u00a0 collection_interval: 60s\u00a0 processors:\u00a0 \u00a0 metrics_filter:\u00a0 \u00a0 \u00a0 type: exclude_metrics\u00a0 \u00a0 \u00a0 metrics_pattern: []\u00a0 service:\u00a0 \u00a0 pipelines:\u00a0 \u00a0 \u00a0 default_pipeline:\u00a0 \u00a0 \u00a0 \u00a0 receivers: [hostmetrics]\u00a0 \u00a0 \u00a0 \u00a0 processors: [metrics_filter]\n```\nBy default, the Ops Agent collects Windows event logs from `System` , `Application` , and `Security` channels, as well as host metrics, IIS metrics, and SQL Server metrics.\nFor more information about the metrics collected, see [Metrics ingested by the receivers](#metrics-by-receiver-type) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/ops-agent/blob/HEAD/confgenerator/built-in-config-windows.yaml) \n```\nlogging:\u00a0 receivers:\u00a0 \u00a0 windows_event_log:\u00a0 \u00a0 \u00a0 type: windows_event_log\u00a0 \u00a0 \u00a0 channels: [System, Application, Security]\u00a0 service:\u00a0 \u00a0 pipelines:\u00a0 \u00a0 \u00a0 default_pipeline:\u00a0 \u00a0 \u00a0 \u00a0 receivers: [windows_event_log]metrics:\u00a0 receivers:\u00a0 \u00a0 hostmetrics:\u00a0 \u00a0 \u00a0 type: hostmetrics\u00a0 \u00a0 \u00a0 collection_interval: 60s\u00a0 \u00a0 iis:\u00a0 \u00a0 \u00a0 type: iis\u00a0 \u00a0 \u00a0 collection_interval: 60s\u00a0 \u00a0 mssql:\u00a0 \u00a0 \u00a0 type: mssql\u00a0 \u00a0 \u00a0 collection_interval: 60s\u00a0 processors:\u00a0 \u00a0 metrics_filter:\u00a0 \u00a0 \u00a0 type: exclude_metrics\u00a0 \u00a0 \u00a0 metrics_pattern: []\u00a0 service:\u00a0 \u00a0 pipelines:\u00a0 \u00a0 \u00a0 default_pipeline:\u00a0 \u00a0 \u00a0 \u00a0 receivers: [hostmetrics, iis, mssql]\u00a0 \u00a0 \u00a0 \u00a0 processors: [metrics_filter]\n```\nThese configurations are discussed in more detail in [Logging configuration](#logging-config) and [Metrics configuration](#metrics-config) .\n### User-specified configuration\nTo override the built-in configuration, you add new configuration elements to the user configuration file. Put your configuration for the Ops Agent in the following files:\n- For Linux:`/etc/google-cloud-ops-agent/config.yaml`\n- For Windows:`C:\\Program Files\\Google\\Cloud Operations\\Ops Agent\\config\\config.yaml`\nAny user-specified configuration is merged with the built-in configuration when the agent restarts.\n**Note:** If you make any configuration changes, then you must [restart theagent](/stackdriver/docs/solutions/agents/ops-agent/installation#restart) to apply the updated configurations.\nTo override a built-in receiver, processor, or pipeline, redefine it in your `config.yaml` file by declaring it with the same identifier. Starting with Ops Agent version 2.31.0, you can also configure the agent's log-rotation feature; for more information, see [Configure log rotation in theOps Agent](/stackdriver/docs/solutions/agents/ops-agent/rotate-logs#configure-log-rotation) .\nFor example, the built-in configuration for metrics includes a `hostmetrics` receiver that specifies a 60-second collection interval. To change the collection interval for host metrics to 30 seconds, include a metrics receiver called `hostmetrics` in your `config.yaml` file that sets the `collection_interval` value to 30 seconds, as shown in the following example:\n```\nmetrics:\u00a0 receivers:\u00a0 \u00a0 hostmetrics:\u00a0 \u00a0 \u00a0 type: hostmetrics\u00a0 \u00a0 \u00a0 collection_interval: 30s\n```\nFor other examples of changing the built-in configurations, see [Logging configuration](#logging-config) and [Metrics configuration](#metrics-config) . You can also turn off the collection of logging or metric data. These changes are described in the example [logging serviceconfigurations](#logging-service-examples) and [metricsservice configurations](#metrics-service-examples) .\nYou can use this file to prevent the agent from collecting self logs and sending those logs to Cloud Logging. For more information, see [Collection of self logs](#self-log-collection) .\nYou also configure the agent's log-rotation feature by using this file; for more information, see [Configure log rotation in theOps Agent](/stackdriver/docs/solutions/agents/ops-agent/rotate-logs#configure-log-rotation) .\nYou can't configure the Ops Agent to export logs or metrics to services other than Cloud Logging and Cloud Monitoring.\n## Logging configurations\nThe `logging` configuration uses the [configuration model](#config-intro) described previously:\n- `receivers`: This element describes the data to collect from log files; this data is mapped into a <timestamp, record> model.\n- `processors`: This optional element describes how the agent can modify the collected information.\n- `service`: This element links receivers and processors together to create data flows, called. The`service`element contains a`pipelines`element, which can include multiple pipeline definitions.\nEach receiver and each processor can be used in multiple pipelines.\nThe following sections describe each of these elements.\nThe Ops Agent sends logs to Cloud Logging. You can't configure it to export logs to other services. You can, however, configure Cloud Logging to export logs; for more information, see [Route logs to supporteddestinations](/logging/docs/export/configure_export_v2) .\n## Logging receivers\nThe `receivers` element contains a set of receivers, each identified by a . A receiver describes how to retrieve the logs; for example, by tailing files, by using a TCP port, or from the Windows Event Log.\n### Structure of logging receivers\nEach receiver must have an identifier, , and include a `type` element. The valid types are:\n- `files`: Collect logs by tailing files on disk.\n- `fluent_forward`(Ops Agent versions 2.12.0 and later): Collect logs sent via the Fluent Forward protocol over TCP.\n- `tcp`(Ops Agent versions 2.3.0 and later): Collect logs in JSON format by listening to a TCP port.\n- Linux only:- `syslog`: Collect Syslog messages over TCP or UDP.\n- `systemd_journald`(Ops Agent versions 2.4.0 and later): Collect systemd journal logs from the systemd-journald service.\n- Windows only:- `windows_event_log`: Collect Windows Event Logs using the Windows Event Log API.\n- [Third-party application log receivers](/stackdriver/docs/solutions/agents/ops-agent/third-party) \nThe `receivers` structure looks like the following:\n```\nreceivers:\n RECEIVER_ID:\n type: files\n ...\n RECEIVER_ID_2:\n type: syslog\n ...\n```\nDepending on the value of the `type` element, there might be other configuration options, as follows:\n- `files` receivers:- `include_paths` : Required. A list of filesystem paths to read by tailing each file. A wildcard ( `*` ) can be used in the paths; for example, `/var/log/*.log` (Linux) or `C:\\logs\\*.log` (Windows). **Note:** When specifying wildcards on Windows, you must use `\\` as a separator.For a list of common Linux application log files, see [Common Linux log files](#common-log-files) .\n- `exclude_paths` : Optional. A list of filesystem path patterns to exclude from the set matched by `include_paths` .\n- `record_log_file_path` : Optional. If set to `true` , then the path to the specific file from which the log record was obtained appears in the output log entry as the value of the `agent.googleapis.com/log_file_path` label. When using a wildcard, only the path of the file from which the record was obtained is recorded.\n- `wildcard_refresh_interval` : Optional. The interval at which wildcard file paths in `include_paths` are refreshed. Given as a time duration, for example, `30s` , `2m` . This property might be useful under high logging throughputs where log files are rotated faster than the default interval. If not specified, the default interval is 60 seconds.\n- `fluent_forward` receivers:- `listen_host` : Optional. An IP address to listen on. The default value is `127.0.0.1` .\n- `listen_port` : Optional. A port to listen on. The default value is `24224` .\n- `syslog` receivers (for Linux only):- `transport_protocol` : Optional. Supported values: `tcp` , `udp` . The default is `tcp` .\n- `listen_host` : Optional. An IP address to listen on. The default value is `0.0.0.0` .\n- `listen_port` : Optional. A port to listen on. The default value is `5140` .\n- `tcp` receivers:- `format` : Required. Log format. Supported value: `json` .\n- `listen_host` : Optional. An IP address to listen on. The default value is `127.0.0.1` .\n- `listen_port` : Optional. A port to listen on. The default value is `5170` .\n- `windows_event_log` receivers (for Windows only):- `channels`: Required. A list of Windows Event Log channels from which to read logs.\n- `receiver_version` : Optional. Controls which Windows Event Log API to use. Supported values are `1` and `2` . The default value is `1` . **Caution:** Version `1` is supported for backwards compatibility only. When configuring a new receiver, use version `2` . To read from channels under the \"Applications and Services\" category in Event Viewer, you must use version `2` .\n- `render_as_xml` : Optional. If set to `true` , then all Event Log fields, except for `jsonPayload.Message` and `jsonPayload.StringInserts` , are rendered as an XML document in a string field named `jsonPayload.raw_xml` . The default value is `false` . This cannot be set to `true` when `receiver_version` is `1` .\n### Examples of logging receivers\nSample `files` receiver:\n```\nreceivers:\u00a0 RECEIVER_ID:\u00a0 \u00a0 type: files\u00a0 \u00a0 include_paths: [/var/log/*.log]\u00a0 \u00a0 exclude_paths: [/var/log/not-this-one.log]\u00a0 \u00a0 record_log_file_path: true\n```\nSample `fluent_forward` receiver:\n**Note:** [Tags](https://docs.fluentbit.io/manual/concepts/key-concepts#tag) from incoming records are preserved in the [LogEntry](/logging/docs/reference/v2/rest/v2/LogEntry) 's`logName`field as a dot-separated suffix (`logName`= \"projects//logs/.\").\n```\nreceivers:\u00a0 RECEIVER_ID:\u00a0 \u00a0 type: fluent_forward\u00a0 \u00a0 listen_host: 127.0.0.1\u00a0 \u00a0 listen_port: 24224\n```\nSample `syslog` receiver (Linux only):\n```\nreceivers:\u00a0 RECEIVER_ID:\u00a0 \u00a0 type: syslog\u00a0 \u00a0 transport_protocol: tcp\u00a0 \u00a0 listen_host: 0.0.0.0\u00a0 \u00a0 listen_port: 5140\n```\nSample `tcp` receiver:\n```\nreceivers:\u00a0 RECEIVER_ID:\u00a0 \u00a0 type: tcp\u00a0 \u00a0 format: json\u00a0 \u00a0 listen_host: 127.0.0.1\u00a0 \u00a0 listen_port: 5170\n```\nSample `windows_event_log` receiver (Windows only):\n```\nreceivers:\u00a0 RECEIVER_ID:\u00a0 \u00a0 type: windows_event_log\u00a0 \u00a0 channels: [System,Application,Security]\n```\nSample `windows_event_log` receiver that overrides the built-in receiver to use version `2` :\n```\nreceivers:\u00a0 windows_event_log:\u00a0 \u00a0 type: windows_event_log\u00a0 \u00a0 channels: [System,Application,Security]\u00a0 \u00a0 receiver_version: 2\n```\nSample `systemd_journald` receiver:\n```\nreceivers:\u00a0 RECEIVER_ID:\u00a0 \u00a0 type: systemd_journald\n```\n### Special fields in structured payloads\nFor processors and receivers that can ingest structured data (the `fluent_forward` and `tcp` receivers and the `parse_json` processor), you can set special fields in the input that will map to specific fields in the `LogEntry` object that the agent writes to the Logging API.\nWhen the Ops Agent receives external structured log data, it places top-level fields into the `LogEntry` 's `jsonPayload` field unless the field name is listed in the following table:\n| Record field                                       | LogEntry field |\n|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------|\n| Option 1 \"timestamp\": { \"seconds\": CURRENT_SECONDS, \"nanos\": CURRENT_NANOS, } Option 2 { \"timestampSeconds\": CURRENT_SECONDS, \"timestampNanos\": CURRENT_NANOS, } | timestamp  |\n| receiver_id (not a record field)                                  | logName   |\n| logging.googleapis.com/httpRequest (HttpRequest)                              | httpRequest  |\n| logging.googleapis.com/severity (string)                                | severity   |\n| logging.googleapis.com/labels (struct of string:string)                            | labels   |\n| logging.googleapis.com/operation (struct)                               | operation  |\n| logging.googleapis.com/sourceLocation (struct)                              | sourceLocation |\n| logging.googleapis.com/trace (string)                                | trace   |\n| logging.googleapis.com/spanId (string)                                | spanId   |\nAny remaining structured record fields remain part of the `jsonPayload` structure.\n### Common Linux log files\nThe following table lists common log files for frequently used Linux applications:\n| Application  | Common log files                                                                                                                                                                                                                               |\n|:------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| apache   | For information about Apache log files, see Monitoring third-party applications: Apache Web Server.                                                                                                                                                                                                          |\n| cassandra   | For information about Cassandra log files, see Monitoring third-party applications: Cassandra.                                                                                                                                                                                                           |\n| chef    | /var/log/chef-server/bookshelf/current /var/log/chef-server/chef-expander/current /var/log/chef-server/chef-pedant/http-traffic.log /var/log/chef-server/chef-server-webui/current /var/log/chef-server/chef-solr/current /var/log/chef-server/erchef/current /var/log/chef-server/erchef/erchef.log.1 /var/log/chef-server/nginx/access.log /var/log/chef-server/nginx/error.log /var/log/chef-server/nginx/rewrite-port-80.log /var/log/chef-server/postgresql/current                                                                                                              |\n| gitlab   | /home/git/gitlab/log/application.log /home/git/gitlab/log/githost.log /home/git/gitlab/log/production.log /home/git/gitlab/log/satellites.log /home/git/gitlab/log/sidekiq.log /home/git/gitlab/log/unicorn.stderr.log /home/git/gitlab/log/unicorn.stdout.log /home/git/gitlab-shell/gitlab-shell.log                                                                                                                                                       |\n| jenkins   | /var/log/jenkins/jenkins.log                                                                                                                                                                                                                            |\n| jetty    | /var/log/jetty/out.log /var/log/jetty/*.request.log /var/log/jetty/*.stderrout.log                                                                                                                                                                                                              |\n| joomla   | /var/www/joomla/logs/*.log                                                                                                                                                                                                                            |\n| magento   | /var/www/magento/var/log/exception.log /var/www/magento/var/log/system.log /var/www/magento/var/report/*                                                                                                                                                                                                        |\n| mediawiki   | /var/log/mediawiki/*.log                                                                                                                                                                                                                             |\n| memcached   | For information about Memcached log files, see Monitoring third-party applications: Memcached.                                                                                                                                                                                                           |\n| mongodb   | For information about MongoDB log files, see Monitoring third-party applications: MongoDB.                                                                                                                                                                                                            |\n| mysql    | For information about MySQL log files, see Monitoring third-party applications: MySQL.                                                                                                                                                                                                             |\n| nginx    | For information about nginx log files, see Monitoring third-party applications: nginx.                                                                                                                                                                                                             |\n| postgres   | For information about PostgreSQL log files, see Monitoring third-party applications: PostgreSQL.                                                                                                                                                                                                          |\n| puppet   | /var/log/puppet/http.log /var/log/puppet/masterhttp.log                                                                                                                                                                                                                     |\n| puppet-enterprise | /var/log/pe-activemq/activemq.log /var/log/pe-activemq/wrapper.log /var/log/pe-console-auth/auth.log /var/log/pe-console-auth/cas_client.log /var/log/pe-console-auth/cas.log /var/log/pe-httpd/access.log /var/log/pe-httpd/error.log /var/log/pe-httpd/other_vhosts_access.log /var/log/pe-httpd/puppetdashboard.access.log /var/log/pe-httpd/puppetdashboard.error.log /var/log/pe-httpd/puppetmasteraccess.log /var/log/pe-mcollective/mcollective_audit.log /var/log/pe-mcollective/mcollective.log /var/log/pe-puppet-dashboard/certificate_manager.log /var/log/pe-puppet-dashboard/event-inspector.log /var/log/pe-puppet-dashboard/failed_reports.log /var/log/pe-puppet-dashboard/live-management.log /var/log/pe-puppet-dashboard/mcollective_client.log /var/log/pe-puppet-dashboard/production.log /var/log/pe-puppetdb/pe-puppetdb.log /var/log/pe-puppet/masterhttp.log /var/log/pe-puppet/rails.log |\n| rabbitmq   | For information about RabbitMQ log files, see Monitoring third-party applications: RabbitMQ.                                                                                                                                                                                                           |\n| redis    | For information about Redis log files, see Monitoring third-party applications: Redis.                                                                                                                                                                                                             |\n| redmine   | /var/log/redmine/*.log                                                                                                                                                                                                                             |\n| salt    | /var/log/salt/key /var/log/salt/master /var/log/salt/minion /var/log/salt/syndic.loc                                                                                                                                                                                                             |\n| solr    | For information about Apache Solr log files, see Monitoring third-party applications: Apache Solr.                                                                                                                                                                                                          |\n| sugarcrm   | /var/www/*/sugarcrm.log                                                                                                                                                                                                                             |\n| syslog   | /var/log/syslog /var/log/messages                                                                                                                                                                                                                          |\n| tomcat   | For information about Apache Tomcat log files, see Monitoring third-party applications: Apache Tomcat.                                                                                                                                                                                                         |\n| zookeeper   | For information about Apache ZooKeeper log files, see Monitoring third-party applications: Apache ZooKeeper.                                                                                                                                                                                                       |\n### Default ingested labels\nLogs can contain the following labels by default in the [LogEntry](/logging/docs/reference/v2/rest/v2/LogEntry) :\n| Field             | Sample Value      | Description                                    |\n|:-------------------------------------------------------|:-----------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| labels.\"compute.googleapis.com/resource_name\"   | test_vm       | The name of the virtual machine from which this log originates. Written for all logs.                  |\n| labels.\"logging.googleapis.com/instrumentation_source\" | agent.googleapis.com/apache_access | The value of the receiver type from which thus log originates, prefixed by agent.googleapis.com/. Written only by receivers from third-party integrations. |\n## Logging processors\nThe optional `processors` element contains a set of processing directives, each identified by a . A processor describes how to manipulate the information collected by a receiver.\nEach processor must have a unique identifier and include a `type` element. The valid types are:\n- `parse_json`: Parse JSON-formatted structured logs.\n- `parse_multiline`: Parse multiline logs. (Linux only)\n- `parse_regex`: Parse text-formatted logs via regex patterns to turn them into JSON-formatted structured logs.\n- `exclude_logs`: Exclude logs that match specified rules (starting in 2.9.0).\n- `modify_fields`: Set/transform fields in log entries (starting in 2.14.0).\nThe `processors` structure looks like the following:\n```\nprocessors:\n PROCESSOR_ID:\n type: parse_json\n ...\n PROCESSOR_ID_2:\n type: parse_regex\n ...\n```\nDepending on the value of the `type` element, there are other configuration options, as follows.\n### parse_json processor\n```\nprocessors:\u00a0 PROCESSOR_ID:\u00a0 \u00a0 type: parse_json\u00a0 \u00a0 time_key: \u00a0 \u00a0<field name within jsonPayload>\u00a0 \u00a0 time_format: <strptime format string>\n```\nThe `parse_json` processor parses the input JSON into the `jsonPayload` field of the `LogEntry` . Other parts of the `LogEntry` can be parsed by setting certain [special top-level fields](#special-fields) .\n- `time_key` : Optional. If the log entry provides a field with a timestamp, this option specifies the name of that field. The extracted value is used to set the `timestamp` field of the resulting [LogEntry](/logging/docs/reference/v2/rest/v2/LogEntry) and is removed from the payload.If the `time_key` option is specified, you must also specify the following:- `time_format`: Required if`time_key`is used. This option specifies the format of the`time_key`field so it can be recognized and analyzed properly. For details of the format, see the [strptime(3)](https://linux.die.net/man/3/strptime) guide.\n```\nprocessors:\u00a0 PROCESSOR_ID:\u00a0 \u00a0 type: parse_json\u00a0 \u00a0 time_key: \u00a0 \u00a0time\u00a0 \u00a0 time_format: \"%Y-%m-%dT%H:%M:%S.%L%Z\"\n```\n### parse_multiline processor\n```\nprocessors:\u00a0 PROCESSOR_ID:\u00a0 \u00a0 type: parse_multiline\u00a0 \u00a0 match_any:\u00a0 \u00a0 - type: <type of the exceptions>\u00a0 \u00a0 \u00a0 language: <language name>\n```\n- `match_any` : Required. A list of one or more rules.- `type` : Required. Only a single value is supported:- `language_exceptions`: Allows the processor to concatenate exceptions into one`LogEntry`, based on the value of the`language`option.\n- `language` : Required. Only a single value is supported:- `java`: Concatenates common Java exceptions into one`LogEntry`.\n- `python`: Concatenates common Python exceptions into one`LogEntry`.\n- `go`: Concatenates common Go exceptions into one`LogEntry`.```\nlogging:\u00a0 receivers:\u00a0 \u00a0 custom_file1:\u00a0 \u00a0 \u00a0 type: files\u00a0 \u00a0 \u00a0 include_paths:\u00a0 \u00a0 \u00a0 - /tmp/test-multiline28\u00a0 processors:\u00a0 \u00a0 parse_java_multiline:\u00a0 \u00a0 \u00a0 type: parse_multiline\u00a0 \u00a0 \u00a0 match_any:\u00a0 \u00a0 \u00a0 - type: language_exceptions\u00a0 \u00a0 \u00a0 \u00a0 language: java\u00a0 \u00a0 extract_structure:\u00a0 \u00a0 \u00a0 type: parse_regex\u00a0 \u00a0 \u00a0 field: message\u00a0 \u00a0 \u00a0 regex: \"^(?<time>[\\d-]*T[\\d:.Z]*) (?<severity>[^ ]*) (?<file>[^ :]*):(?<line>[\\d]*) - (?<message>(.|\\\\n)*)$\"\u00a0 \u00a0 \u00a0 time_key: time\u00a0 \u00a0 \u00a0 time_format: \"%Y-%m-%dT%H:%M:%S.%L\"\u00a0 \u00a0 move_severity:\u00a0 \u00a0 \u00a0 type: modify_fields\u00a0 \u00a0 \u00a0 fields:\u00a0 \u00a0 \u00a0 \u00a0 severity:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 move_from: jsonPayload.severity\u00a0 service:\u00a0 \u00a0 pipelines:\u00a0 \u00a0 \u00a0 pipeline1:\u00a0 \u00a0 \u00a0 \u00a0 receivers: [custom_file1]\u00a0 \u00a0 \u00a0 \u00a0 processors: [parse_java_multiline, extract_structure, move_severity]\n```\nIf you use the preceding configuration and have a log file with the following content:\n```\n2022-10-17T22:00:00.187512963Z ERROR HelloWorld:16 - javax.servlet.ServletException: Something bad happened\u00a0 \u00a0 at com.example.myproject.OpenSessionInViewFilter.doFilter(OpenSessionInViewFilter.java:60)\u00a0 \u00a0 at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\u00a0 \u00a0 at com.example.myproject.ExceptionHandlerFilter.doFilter(ExceptionHandlerFilter.java:28)\u00a0 \u00a0 at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\u00a0 \u00a0 at com.example.myproject.OutputBufferFilter.doFilter(OutputBufferFilter.java:33)Caused by: com.example.myproject.MyProjectServletException\u00a0 \u00a0 at com.example.myproject.MyServlet.doPost(MyServlet.java:169)\u00a0 \u00a0 at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)\u00a0 \u00a0 at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\u00a0 \u00a0 at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\u00a0 \u00a0 at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\u00a0 \u00a0 at com.example.myproject.OpenSessionInViewFilter.doFilter(OpenSessionInViewFilter.java:30)\u00a0 \u00a0 ... 27 common frames omitted\n```\nthen the agent ingests the logs into Cloud Logging in the following format:\n```\n{\u00a0 \"insertId\": \"...\",\u00a0 \"jsonPayload\": {\u00a0 \u00a0 \"line\": \"16\",\u00a0 \u00a0 \"message\": \"javax.servlet.ServletException: Something bad happened\\n \u00a0 \u00a0at com.example.myproject.OpenSessionInViewFilter.doFilter(OpenSessionInViewFilter.java:60)\\n \u00a0 \u00a0at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\\n \u00a0 \u00a0at com.example.myproject.ExceptionHandlerFilter.doFilter(ExceptionHandlerFilter.java:28)\\n \u00a0 \u00a0at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1157)\\n \u00a0 \u00a0at com.example.myproject.OutputBufferFilter.doFilter(OutputBufferFilter.java:33)\\nCaused by: com.example.myproject.MyProjectServletException\\n \u00a0 \u00a0at com.example.myproject.MyServlet.doPost(MyServlet.java:169)\\n \u00a0 \u00a0at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)\\n \u00a0 \u00a0at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\\n \u00a0 \u00a0at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)\\n \u00a0 \u00a0at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1166)\\n \u00a0 \u00a0at com.example.myproject.OpenSessionInViewFilter.doFilter(OpenSessionInViewFilter.java:30)\\n \u00a0 \u00a0... 27 common frames omitted\\n\",\u00a0 \u00a0 \"file\": \"HelloWorld\"\u00a0 },\u00a0 \"resource\": {\u00a0 \u00a0 \"type\": \"gce_instance\",\u00a0 \u00a0 \"labels\": {\u00a0 \u00a0 \u00a0 \"instance_id\": \"...\",\u00a0 \u00a0 \u00a0 \"project_id\": \"...\",\u00a0 \u00a0 \u00a0 \"zone\": \"...\"\u00a0 \u00a0 }\u00a0 },\u00a0 \"timestamp\": \"2022-10-17T22:00:00.187512963Z\",\u00a0 \"severity\": \"ERROR\",\u00a0 \"labels\": {\u00a0 \u00a0 \"compute.googleapis.com/resource_name\": \"...\"\u00a0 },\u00a0 \"logName\": \"projects/.../logs/custom_file\",\u00a0 \"receiveTimestamp\": \"2022-10-18T03:12:38.430364391Z\"}\n```\n### parse_regex processor\n```\nprocessors:\u00a0 PROCESSOR_ID:\u00a0 \u00a0 type: parse_regex\u00a0 \u00a0 regex: <regular expression>\u00a0 \u00a0 time_key: \u00a0 \u00a0<field name within jsonPayload>\u00a0 \u00a0 time_format: <format string>\n```\n- `time_key` : Optional. If the log entry provides a field with a timestamp, this option specifies the name of that field. The extracted value is used to set the `timestamp` field of the resulting [LogEntry](/logging/docs/reference/v2/rest/v2/LogEntry) and is removed from the payload.If the `time_key` option is specified, you must also specify the following:- `time_format`: Required if`time_key`is used. This option specifies the format of the`time_key`field so it can be recognized and analyzed properly. For details of the format, see the [strptime(3)](https://linux.die.net/man/3/strptime) guide.\n- `regex` : Required. The regular expression for parsing the field. The expression must include key names for the matched subexpressions; for example, `\"^(?<time>[^ ]*) (?<severity>[^ ]*) (?<msg>.*)$\"` .The text matched by named capture groups will be placed into fields in the `LogEntry` 's `jsonPayload` field. To add additional structure to your logs, use the [modify_fields processor](#logging-processor-modify-fields) .For a set of regular expressions for extracting information from common Linux application log files, see [Common Linux log files](#common-log-files) .```\nprocessors:\u00a0 PROCESSOR_ID:\u00a0 \u00a0 type: parse_regex\u00a0 \u00a0 regex: \u00a0 \u00a0 \u00a0 \"^(?<time>[^ ]*) (?<severity>[^ ]*) (?<msg>.*)$\"\u00a0 \u00a0 time_key: \u00a0 \u00a0time\u00a0 \u00a0 time_format: \"%Y-%m-%dT%H:%M:%S.%L%Z\"\n```\n### exclude_logs processor\n```\ntype: exclude_logsmatch_any:\u00a0 - <filter>\u00a0 - <filter>\n```\nThe top-level configuration for this processor contains a single field, `match_any` , which contains a list of filter rules.\n- `match_any` : Required. A list of one or more rules. If a log entry matches any rule, then the Ops Agent doesn't ingest that entry.The logs that are ingested by Ops Agent follow the [LogEntry structure](/logging/docs/reference/v2/rest/v2/LogEntry) . Field names are case-sensitive. You can only specify rules based on the following fields and their subfields:- `httpRequest`\n- `jsonPayload`\n- `labels`\n- `operation`\n- `severity`\n- `sourceLocation`\n- `trace`\n- `spanId`\nThe following example rule```\nseverity =~ \"(DEBUG|INFO)\"\n```uses a regular expression to exclude all `DEBUG` and `INFO` level logs.Rules follow the [Cloud Logging querylanguage](/logging/docs/view/logging-query-language) syntax but only support a subset of the features that Logging query language supports:- Comparison operators:`=`,`!=`,`:`,`=~`,`!~`. Only string comparisons are supported.\n- Navigation operator:`.`. For example`jsonPayload.message`.\n- Boolean operators:`AND`,`OR`,`NOT`.\n- Grouping expressions with`(``)`.**Note:** Using the`exclude_logs`processor has performance implications, so it's recommended that you avoid using this processor if you can exclude logs from the source or if you can set up [exclusion filters](/logging/docs/routing/overview#exclusions) . If you need to use the`exclude_logs`processor, we recommend the following best practices:- Minimize the number of`exclude_logs`processors.\n- Use exact matches whenever possible instead of using  regular-expression matches.```\nprocessors:\u00a0 PROCESSOR_ID:\u00a0 \u00a0 type: exclude_logs\u00a0 \u00a0 match_any:\u00a0 \u00a0 - '(jsonPayload.message =~ \"log spam 1\" OR jsonPayload.message =~ \"log spam 2\") AND severity = \"ERROR\"'\u00a0 \u00a0 - 'jsonPayload.application = \"foo\" AND severity = \"INFO\"'\n```\n### modify_fields Processor\nThe `modify_fields` processor allows customization of the structure and contents of log entries.\n```\ntype: modify_fieldsfields:\u00a0 <destination field>:\u00a0 \u00a0 # Source\u00a0 \u00a0 move_from: <source field>\u00a0 \u00a0 copy_from: <source field>\u00a0 \u00a0 static_value: <string>\u00a0 \u00a0 \u00a0 \u00a0 # Mutation\u00a0 \u00a0 default_value: <string>\u00a0 \u00a0 map_values:\u00a0 \u00a0 \u00a0 <old value>: <new value>\u00a0 \u00a0 type: {integer|float}\u00a0 \u00a0 omit_if: <filter>\n```\nThe top-level configuration for this processor contains a single field, `fields` , which contains a map of output field names and corresponding translations. For each output field, an optional source and zero or more mutation operations are applied.\nAll field names use [the dot-separated syntax](/logging/docs/view/logging-query-language#comparisons) from the Cloud Logging query language. Filters use the Cloud Logging query language.\nAll transformations are applied in parallel, which means that sources and filters operate on the original input log entry and therefore can not reference the new value of any other fields being modified by the same processor.\n**Source options** : At most one specified source is allowed.\n- No source specifiedIf no source value is specified, the existing value in the destination field will be modified.\n- `move_from: <source field>`The value from `<source field>` will be used as the source for the destination field. Additionally, `<source field>` will be removed from the log entry. If a source field is referenced by both `move_from` and `copy_from` , the source field will still be removed.\n- `copy_from: <source field>`The value from `<source field>` will be used as the source for the destination field. `<source field>` will not be removed from the log entry unless it is also referenced by a `move_from` operation or otherwise modified.\n- `static_value: <string>`The static string `<string>` will be used as the source for the destination field.\n**Mutation options** : Zero or more mutation operators may be applied to a single field. If multiple operators are supplied, they will always be applied in the following order.\n- `default_value: <string>`If the source field did not exist, the output value will be set to `<string>` . If the source field already exists (even if it contains an empty string), the original value is unmodified.\n- `map_values: <map>`If the input value matches one of the keys in `<map>` , the output value will be replaced with the corresponding value from the map.\n- `map_values_exclusive: {true|false}`In case the `<source field>` value does not match any keys specified in the `map_values` pairs, the destination field will be forcefully unset if `map_values_exclusive` is true, or left untouched if `map_values_exclusive` is false.\n- `type: {integer|float}`The input value will be converted to an integer or a float. If the string cannot be converted to a number, the output value will be unset. If the string contains a float but the type is specified as `integer` , the number will be truncated to an integer.Note that the Cloud Logging API uses JSON and therefore it does not support a full 64-bit integer; if a 64-bit (or larger) integer is needed, it must be stored as a string in the log entry.\n- `omit_if: <filter>`If the filter matches the input log record, the output field will be unset. This can be used to remove placeholder values, such as:```\nhttpRequest.referer:\u00a0 move_from: jsonPayload.referer\u00a0 omit_if: httpRequest.referer = \"-\"\n```The `parse_json` processor would transform a JSON file containing\n```\n{\u00a0 \"http_status\": \"400\",\u00a0 \"path\": \"/index.html\",\u00a0 \"referer\": \"-\"}\n```\ninto a [LogEntry structure](/logging/docs/reference/v2/rest/v2/LogEntry) that looks like this:\n```\n{\u00a0 \"jsonPayload\": {\u00a0 \u00a0 \"http_status\": \"400\",\u00a0 \u00a0 \"path\": \"/index.html\",\u00a0 \u00a0 \"referer\": \"-\"\u00a0 }}\n```\nThis could then be transformed with `modify_fields` into this `LogEntry` :\n```\n{\u00a0 \"httpRequest\": {\u00a0 \u00a0 \"status\": 400,\u00a0 \u00a0 \"requestUrl\": \"/index.html\",\u00a0 }}\n```\nusing this Ops agent configuration:\n```\nlogging:\u00a0 receivers:\u00a0 \u00a0 in:\u00a0 \u00a0 \u00a0 type: files\u00a0 \u00a0 \u00a0 include_paths:\u00a0 \u00a0 \u00a0 - /var/log/http.json\u00a0 processors:\u00a0 \u00a0 parse_json:\u00a0 \u00a0 \u00a0 type: parse_json\u00a0 \u00a0 set_http_request:\u00a0 \u00a0 \u00a0 type: modify_fields\u00a0 \u00a0 \u00a0 fields:\u00a0 \u00a0 \u00a0 \u00a0 httpRequest.status:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 move_from: jsonPayload.http_status\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: integer\u00a0 \u00a0 \u00a0 \u00a0 httpRequest.requestUrl:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 move_from: jsonPayload.path\u00a0 \u00a0 \u00a0 \u00a0 httpRequest.referer:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 move_from: jsonPayload.referer\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 omit_if: jsonPayload.referer = \"-\"\u00a0 pipelines:\u00a0 \u00a0 pipeline:\u00a0 \u00a0 \u00a0 receivers: [in]\u00a0 \u00a0 \u00a0 processors: [parse_json, set_http_request]\n```\nThis configuration reads JSON-formatted logs from `/var/log/http.json` and populates part of the `httpRequest` structure from fields in the logs.\n## Logging service\nThe logging service customizes verbosity for the Ops Agent's own logs, and links logging receivers and processors together into pipelines. The `service` section has the following elements:\n- `log_level`\n- `pipelines`\n### Log verbosity level\nThe `log_level` field, available with Ops Agent versions 2.6.0 and later, customizes verbosity for Ops Agent logging submodule's own logs. The default is `info` . Available options are: `error` , `warn` , `info` , `debug` , `trace` .\n**Warning:** Setting the `log_level` to `debug` (or `trace` ) triggers a feedback loop in the logging sub-agent, resulting in a continuous stream of logs. This is a known issue, and is being addressed. In the meantime, set `log_level` to anything above `info` .\nThe following configuration customizes log verbosity for the logging submodule to be `debug` instead:\n```\nlogging:\u00a0 service:\u00a0 \u00a0 log_level: debug\n```\n### Logging pipelines\nThe `pipelines` field can contain multiple pipeline IDs and definitions. Each `pipeline` value consists of the following elements:\n- `receivers` : Required for new pipelines. A list of receiver IDs, as described in [Logging receivers](#logging-receivers) . The order of the receivers IDs in the list doesn't matter. The pipeline collects data from all of the listed receivers.\n- `processors` : Optional. A list of processor IDs, as described in [Logging processors](#logging-processors) . The order of the processor IDs in the list matter. Each record is run through the processors in the listed order.\n### Example logging service configurations\nA `service` configuration has the following structure:\n```\nservice:\n log_level: CUSTOM_LOG_LEVEL\n pipelines:\n PIPELINE_ID:\n  receivers: [...]\n  processors: [...]\n PIPELINE_ID_2:\n  receivers: [...]\n  processors: [...]\n```\nTo stop the agent from collecting and sending either `/var/log/message` or `/var/log/syslog` entries, redefine the default pipeline with an empty `receivers` list and no processors. This configuration does not stop the agent's logging subcomponent, because the agent must be able to collect logs for the monitoring subcomponent. The entire empty logging configuration looks like the following:\n```\nlogging:\u00a0 service:\u00a0 \u00a0 pipelines:\u00a0 \u00a0 \u00a0 default_pipeline:\u00a0 \u00a0 \u00a0 \u00a0 receivers: []\n```\nThe following `service` configuration defines a pipeline with the ID `custom_pipeline` :\n```\nlogging:\u00a0 service:\u00a0 \u00a0 pipelines:\u00a0 \u00a0 \u00a0 custom_pipeline:\u00a0 \u00a0 \u00a0 \u00a0 receivers:\u00a0 \u00a0 \u00a0 \u00a0 - RECEIVER_ID\u00a0 \u00a0 \u00a0 \u00a0 processors:\u00a0 \u00a0 \u00a0 \u00a0 - PROCESSOR_ID\n```\n## Metrics configurations\nThe `metrics` configuration uses the [configuration model](#config-intro) described previously:\n- `receivers`: a list of receiver definitions. A`receiver`describes the source of the metrics; for example, system metrics like`cpu`or`memory`. The receivers in this list can be shared among multiple pipelines.\n- `processors`: a list of processor definitions. A`processor`describes how to modify the metrics collected by a receiver.\n- `service`: contains a`pipelines`section that is a list of`pipeline`definitions. A`pipeline`connects a list of`receivers`and a list of`processors`to form the data flow.\nThe following sections describe each of these elements.\nThe Ops Agent sends metrics to Cloud Monitoring. You can't configure it to export metrics to other services.\n### Metrics receivers\nThe `receivers` element contains a set of receiver definitions. A receiver describes from where to retrieve the metrics, such as like `cpu` and `memory` . A receiver can be shared among multiple pipelines.\nEach receiver must have an identifier, , and include a `type` element. Valid [built-in types](#default) are:\n- `hostmetrics`\n- `iis`(Windows only)\n- `mssql`(Windows only)\nA receiver can also specify the operation `collection_interval` option. The value is in the format of a duration, for example, `30s` or `2m` . The default value is `60s` .\nEach of these receiver types collects a set of metrics; for information about the specific metrics included, see [Metrics ingested by the receivers](#metrics-by-receiver-type) .\nYou can create only one receiver for each type. For example, you can't define two receivers of type `hostmetrics` .\nSome critical workloads might require fast alerting. By reducing the the collection interval for the metrics, you can configure more sensitive alerts. For information on how alerts are evaluated, see [Behavior of metric-based alerting policies](/monitoring/alerts/concepts-indepth) .\nFor example, the following receiver changes the collection interval for host metrics (the receiver ID is `hostmetrics` ) from the default of 60 seconds to 10 seconds:\n```\nmetrics:\u00a0 receivers:\u00a0 \u00a0 hostmetrics:\u00a0 \u00a0 \u00a0 type: hostmetrics\u00a0 \u00a0 \u00a0 collection_interval: 10s\n```\nYou can also override the collection interval for the Windows `iis` and `mssql` metrics receivers using the same technique.\nThe metrics ingested by the Ops Agent have identifiers that begin with the following pattern: `agent.googleapis.com/` `` . The component identifies a set of related metrics; it has values like `cpu` , `network` , and others.\nThe `hostmetrics` receiver ingests the following metric groups. For more information, see the linked section for each group on the [Ops Agent metrics](/monitoring/api/metrics_opsagent) page.\n| Group                   | Metric                                                                                                                         |\n|:------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| cpu                   | CPU load at 1 minute intervals CPU load at 5 minute intervals CPU load at 15 minute intervals CPU usage, with labels for CPU number and CPU state CPU usage percent, with labels for CPU number and CPU state                                                                      |\n| disk                   | Disk bytes read, with label for device Disk bytes written, with label for device Disk I/O time, with label for device Disk weighted I/O time, with label for device Disk pending operations, with label for device Disk merged operations, with labels for device and direction Disk operations, with labels for device and direction Disk operation time, with labels for device and direction Disk usage, with labels for device and state Disk utilization, with labels for device and state |\n| gpu Linux only; see About the gpu metrics for other important information. | Current number of GPU memory bytes used, by state Maximum amount of GPU memory, in bytes, that has been allocated by the process Percentage of time in the process lifetime that one or more kernels has been running on the GPU Percentage of time, since last sample, the GPU has been active                                                  |\n| interface Linux only               | Total count of network errors Total count of packets sent over the network Total number of bytes sent over the network                                                                                             |\n| memory                  | Memory usage, with label for state (buffered, cached, free, slab, used) Memory usage percent, with label for state (buffered, cached, free, slab, used)                                                                                     |\n| network                  | TCP connection count, with labels for port and TCP state                                                                                                             |\n| swap                   | Swap I/O operations, with label for direction Swap bytes used, with labels for device and state Swap percent used, with labels for device and state                                                                                     |\n| pagefile Windows only              | Current percentage of pagefile used by state                                                                                                                |\n| processes                  | Processes count, with label for state Processes forked count Per-process disk read I/O, with labels for process name + others Per-process disk write I/O, with labels for process name + others Per-process RSS usage, with labels for process name + others Per-process VM usage, with labels for process name + others                                           |\n**About the gpu metrics:** The `hostmetrics` receiver collects metrics reported by the NVIDIA [Management Library (NVML)](https://developer.nvidia.com/nvidia-management-library-nvml) as `agent.googleapis.com/gpu` metrics.To collect these metrics, you must [create your VM with attached GPUs](/compute/docs/gpus/create-vm-with-gpus) and [install the GPU driver](/compute/docs/gpus/install-drivers-gpu) . The `hostmetrics` receiver doesn't collect these metrics on VMs with no attached GPUs.Only Ops Agent version 2.38.0 or versions 2.41.0 or newer are compatible with GPU monitoring. **Do not installOps Agent versions 2.39.0and 2.40.0 on VMs with attached GPUs.** For more information, see [Agent crashes and report mentions NVIDIA](/stackdriver/docs/solutions/agents/ops-agent/troubleshoot-install-startup#nvidia-crashes) .You can install or upgrade the NVIDIA GPU driver by using package managers or local installation scripts. When using local installation scripts, the Ops Agent service must be stopped before the driver installation can proceed. To stop the agent, run the following command:```\nsudo systemctl stop google-cloud-ops-agent\n```You must also reboot the VM after installing or upgrading an NVIDIA GPU driver.\nThe `iis` receiver (Windows only) ingests metrics of the `iis` group. For more information, see the [Agent metrics](/monitoring/api/metrics_opsagent#agent-iis) page.\n| Group    | Metric                            |\n|:------------------|:---------------------------------------------------------------------------------------------------------------------|\n| iis Windows only | Currently open connections to IIS Network bytes transferred by IIS Connections opened to IIS Requests made to IIS |\nThe `mssql` receiver (Windows only) ingests metrics of the `mssql` group. For more information, see the [Ops Agent metrics](/monitoring/api/metrics_opsagent#agent-mssql) page.\n| Group    | Metric                              |\n|:--------------------|:-----------------------------------------------------------------------------------------------------------------------------|\n| mssql Windows only | Currently open connections to SQL server SQL server total transactions per second SQL server write transactions per second |\n### Metrics processors\nThe `processor` element contains a set of processor definitions. A processor describes metrics from the receiver type to exclude. The only supported type is `exclude_metrics` , which takes a `metrics_pattern` option. The value is a list of globs that match the [Ops Agent metric types](/monitoring/api/metrics_opsagent) you want to exclude from the group collected by a receiver. For example:\n- To exclude all agent [CPU metrics](/monitoring/api/metrics_opsagent#agent-cpu) , specify`agent.googleapis.com/cpu/*`.\n- To exclude the agent CPU utilization metric, specify`agent.googleapis.com/cpu/utilization`.\n- To exclude the client-side request-count metric from the [metricscollected by the Apache Cassandra third-partyintegration](/monitoring/api/metrics_opsagent#opsagent-cassandra) , specify`workloads.googleapis.com/cassandra.client.request.count`.\n- To exclude all client-side metrics from the [metricscollected by the Apache Cassandra third-partyintegration](/monitoring/api/metrics_opsagent#opsagent-cassandra) , specify`workloads.googleapis.com/cassandra.client.*`.The following example shows the `exclude_metrics` processor supplied in the built-in configurations. This processor supplies an empty `metrics_pattern` value, so it doesn't exclude any metrics.\n```\nprocessors:\u00a0 metrics_filter:\u00a0 \u00a0 type: exclude_metrics\u00a0 \u00a0 metrics_pattern: []\n```\nTo disable the collection of all process metrics by the Ops Agent, add the following to your `config.yaml` file:\n```\nmetrics:\n processors:\n metrics_filter:\n  type: exclude_metrics\n  metrics_pattern:\n  - agent.googleapis.com/processes/*\n```\nThis excludes process metrics from collection in the `metrics_filter` processor that applies to the default pipeline in the `metrics` service.\n### Metrics service\nThe metrics service customizes verbosity for the Ops Agent metrics module's own logs and links metrics receivers and processors together into pipelines. The `service` section has two elements: `log_level` and `pipelines` .\n`log_level` , available with Ops Agent versions 2.6.0 and later, customizes verbosity for Ops Agent metrics submodule's own logs. The default is `info` . Available options are: `error` , `warn` , `info` , `debug` .\nThe `service` section has a single element, `pipelines` , which can contain multiple pipeline IDs and definitions. Each `pipeline` definition consists of the following elements:\n- `receivers` : Required for new pipelines. A list of receiver IDs, as described in [Metrics receivers](#metris-receivers) . The order of the receivers IDs in the list doesn't matter. The pipeline collects data from all of the listed receivers.\n- `processors` : Optional. A list of processor IDs, as described in [Metrics processors](#metris-processors) . The order of the processor IDs in the list matter. Each metric point is run through the processors in the listed order.A `service` configuration has the following structure:\n```\nservice:\n log_level: CUSTOM_LOG_LEVEL\n pipelines:\n PIPELINE_ID:\n  receivers: [...]\n  processors: [...]\n PIPELINE_ID_2:\n  receivers: [...]\n  processors: [...]\n```\nTo turn off the built-in ingestion of host metrics, redefine the default pipeline with an empty `receivers` list and no processors. The entire metrics configuration looks like the following:\n```\nmetrics:\u00a0 service:\u00a0 \u00a0 pipelines:\u00a0 \u00a0 \u00a0 default_pipeline:\u00a0 \u00a0 \u00a0 \u00a0 receivers: []\n```\nThe following example shows the built-in `service` configuration for Windows:\n```\nmetrics:\u00a0 service:\u00a0 \u00a0 pipelines:\u00a0 \u00a0 \u00a0 default_pipeline:\u00a0 \u00a0 \u00a0 \u00a0 receivers:\u00a0 \u00a0 \u00a0 \u00a0 - hostmetrics\u00a0 \u00a0 \u00a0 \u00a0 - iis\u00a0 \u00a0 \u00a0 \u00a0 - mssql\u00a0 \u00a0 \u00a0 \u00a0 processors:\u00a0 \u00a0 \u00a0 \u00a0 - metrics_filter\n```\nThe following `service` configuration customizes log verbosity for the metrics submodule to be `debug` instead:\n```\nmetrics:\u00a0 service:\u00a0 \u00a0 log_level: debug\n```\n## Collection of self logs\nBy default, the Ops Agent's Fluent Bit self logs are sent to Cloud Logging. These logs can include a lot of information, and the additional volume might increase your costs to use Cloud Logging.\nYou can disable the collection of these self logs, starting with Ops Agent version 2.44.0, by using the `default_self_log_file_collection` option.\nTo disable self-log collection, add a `global` section to your user-specified configuration file and set the `default_self_log_file_collection` option to the value `false` :\n```\nlogging: ...\nmetrics: ...\nglobal:\n default_self_log_file_collection: false\n```\n## Log-rotation configuration\nStarting with Ops Agent version 2.31.0, you can also set up the agent's log-rotation feature by using the configuration files. For more information, see [Configure log rotationin the Ops Agent](/stackdriver/docs/solutions/agents/ops-agent/rotate-logs#configure-log-rotation) .", "guide": "Google Cloud Observability"}