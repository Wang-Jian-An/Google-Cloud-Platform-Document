{"title": "Google Cloud Observability - Cost controls and attribution", "url": "https://cloud.google.com/stackdriver/docs/managed-prometheus/cost-controls", "abstract": "# Google Cloud Observability - Cost controls and attribution\nGoogle Cloud Managed Service for Prometheus charges for the number of samples ingested into Cloud Monitoring and for read requests to the Monitoring API. The number of samples ingested is the primary contributor to your cost.\nThis document describes how you can control costs associated with metric ingestion and how to identify sources of high-volume ingestion.\nFor more information about the pricing for Managed Service for Prometheus, see [Managed Service for Prometheus pricing summary](/stackdriver/pricing#mgd-prometheus-pricing-summary) .\n", "content": "## View your bill\nTo view your Google Cloud bill, do the following:\n- In the Google Cloud console, go to the **Billing** page. [Go to Billing](https://console.cloud.google.com/billing) \n- If you have more than one billing account, select **Go to linked billingaccount** to view the current project's billing account. To locate a different billing account, select **Manage billing accounts** and choose the account for which you'd like to get usage reports.\n- In the **Cost management** section of the Billing navigation menu, select **Reports** .\n- From the **Services** menu, select the **Cloud Monitoring** option.\n- From the **SKUs** menu, select the following options:- **Prometheus Samples Ingested** \n- **Monitoring API Requests** The following screenshot shows the billing report for Managed Service for Prometheus from one project:\n## Reduce your costs\nTo reduce the costs associated with using Managed Service for Prometheus, you can do the following:\n- Reduce the number of time series you send to the managed service by filtering the metric data you generate.\n- Reduce the number of samples that you collect by changing the scraping interval.\n- Limit the number of samples from potentially misconfigured high-cardinality metrics.\n### Reduce the number of time series\nOpen source Prometheus documentation rarely recommends filtering metric volume, which is reasonable when costs are bounded by machine costs. But when paying a managed-service provider on a unit basis, sending unlimited data can cause unnecessarily high bills.\nThe exporters included in the [kube-prometheus](https://github.com/prometheus-operator/kube-prometheus) project\u2014the [kube-state-metrics](https://github.com/kubernetes/kube-state-metrics) service in particular\u2014can emit a lot of metric data. For example, the `kube-state-metrics` service emits hundreds of metrics, many of which might be completely valueless to you as a consumer. A fresh three-node cluster using the `kube-prometheus` project sends approximately 900 samples per second to Managed Service for Prometheus. Filtering these extraneous metrics might be enough by itself to get your bill down to an acceptable level.\nTo reduce the number of metrics, you can do the following:\n- Modify your scrape configs to scrape fewer targets.\n- Filter the collected metrics as described in the following:- [Filter exported metrics](/stackdriver/docs/managed-prometheus/setup-managed#filter-metrics) when using managed collection.\n- [Filter exported metrics](/stackdriver/docs/managed-prometheus/setup-unmanaged#filter-metrics) when using self-deployed collection.If you are using the `kube-state-metrics` service, you could add a [Prometheus relabeling rule](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config) with a `keep` action. For managed collection, this rule goes in your [PodMonitoring orClusterPodMonitoring](https://github.com/GoogleCloudPlatform/prometheus-engine/blob/v0.8.2/doc/api.md#relabelingrule) definition. For self-deployed collection, this rule goes in your Prometheus [scrapeconfig](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config) or your [ServiceMonitor](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor) definition (for prometheus-operator).\nFor example, using the following filter on a fresh three-node cluster reduces your sample volume by approximately 125 samples per second:\n```\n metricRelabeling:\n - action: keep\n regex: kube_(daemonset|deployment|pod|namespace|node|statefulset|persistentvolume|horizontalpodautoscaler)_.+\n sourceLabels: [__name__]\n```\nThe previous filter uses a regular expression to specify which metrics to keep based on the name of the metric. For example, metrics whose name begins with `kube_daemonset_` are kept. You can also specify an action of `drop` , which filters out the metrics that match the regular expression.\nSometimes, you might find an entire exporter to be unimportant. For example, the `kube-prometheus` package installs the following service monitors by default, many of which are unnecessary in a managed environment:\n- `alertmanager`\n- `coredns`\n- `grafana`\n- `kube-apiserver`\n- `kube-controller-manager`\n- `kube-scheduler`\n- `kube-state-metrics`\n- `kubelet`\n- `node-exporter`\n- `prometheus`\n- `prometheus-adapter`\n- `prometheus-operator`\nTo reduce the number of metrics that you export, you can delete, disable, or stop scraping the service monitors you don't need. For example, disabling the `kube-apiserver` service monitor on a fresh three-node cluster reduces your sample volume by approximately 200 samples per second.\n### Reduce the number of samples collected\nManaged Service for Prometheus charges on a per-sample basis. You can reduce the number of samples ingested by increasing the length of the sampling period. For example:\n- Changing a 10-second sampling period to a 30-second sampling period can reduce your sample volume by 66%, without much loss of information.\n- Changing a 10-second sampling period to a 60-second sampling period can reduce your sample volume by 83%.\nFor information about how samples are counted and how the sampling period affects the number of samples, see [Pricing examples based on samplesingested](/stackdriver/pricing#pricing_examples_samples) .\nYou can usually set the scraping interval on a per-job or a per-target basis.\nFor managed collection, you set the scrape interval in the [PodMonitoring resource](/stackdriver/docs/managed-prometheus/setup-managed#gmp-pod-monitoring) by using the `interval` field. For self-deployed collection, you set the sampling interval in your [scrapeconfigs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config) , usually by setting an `interval` or `scrape_interval` field.\n### Configure local aggregation (self-deployed collection only)\nIf you are configuring the service by using [self-deployed collection](/stackdriver/docs/managed-prometheus/setup-unmanaged) , for example with kube-prometheus, prometheus-operator, or by manually deploying the image, then you can reduce your samples sent to Managed Service for Prometheus by aggregating high-cardinality metrics locally. You can use recording rules to aggregate away labels such as `instance` and use the [--export.match flag](/stackdriver/docs/managed-prometheus/setup-unmanaged#filter-metrics) or the [EXTRA_ARGS environmentvariable](/stackdriver/docs/managed-prometheus/setup-unmanaged#prom-managers) to only send aggregated data to Monarch.\nFor example, assume you have three metrics, `high_cardinality_metric_1` , `high_cardinality_metric_2` , and `low_cardinality_metric` . You want to reduce the samples sent for `high_cardinality_metric_1` and eliminate all samples sent for `high_cardinality_metric_2` , while keeping all raw data stored locally (perhaps for alerting purposes). Your setup might look something like this:\n- Deploy the Managed Service for Prometheus image.\n- Configure your scrape configs to scrape all raw data into the local server (using as few filters as desired).\n- Configure your recording rules to run local aggregations over `high_cardinality_metric_1` and `high_cardinality_metric_2` , perhaps by aggregating away the `instance` label or any number of metric labels, depending on what provides the best reduction in the number of unneeded time series. You might run a rule that looks like the following, which drops the `instance` label and sums the resulting time series over the remaining labels:```\nrecord: job:high_cardinality_metric_1:sum\nexpr: sum without (instance) (high_cardinality_metric_1)\n```See [aggregation operators in the Prometheus documentation](https://prometheus.io/docs/prometheus/latest/querying/operators/#aggregation-operators) for more aggregation options.\n- Deploy the Managed Service for Prometheus image with the following [filter flag](/stackdriver/docs/managed-prometheus/setup-unmanaged#filter-metrics) , which prevents raw data from the listed metrics from being sent to Monarch:```\n--export.match='{__name__!=\"high_cardinality_metric_1\",__name__!=\"high_cardinality_metric_2\"}'\n```This example `export.match` flag uses comma-separated selectors with the `!=` operator to filter out unwanted raw data. If you add additional recording rules to aggregate other high-cardinality metrics, then you also have to add a new `__name__` selector to the filter so that the raw data is discarded. By using a single flag containing multiple selectors with the `!=` operator to filter out unwanted data, you only need to modify the filter when you create a new aggregation instead of whenever you modify or add a scrape config.Certain deployment methods, such as prometheus-operator, might require you to omit the single quotes surrounding the brackets.\nThis workflow might incur some operational overhead in creating and managing recording rules and `export.match` flags, but it's likely that you can cut a lot of volume by focusing only on metrics with exceptionally high cardinality. For information about identifying which metrics might benefit the most from local pre-aggregation, see [Identify high-volume metrics](#high-volume-metrics) .\n**Do not implement federation when using Managed Service for Prometheus** . This workflow makes using federation servers obsolete, as a single self-deployed Prometheus server can perform any cluster-level aggregations you might need. Federation may cause unexpected effects such as \"unknown\"-typed metrics and doubled ingestion volume.\n### Limit samples from high-cardinality metrics (self-deployed collection only)\nYou can create extremely high-cardinality metrics by adding labels that have a large number of potential values, like a user ID or IP address. Such metrics can generate a very large number of samples. Using labels with a large number of values is typically a misconfiguration. You can guard against high-cardinality metrics in your self-deployed collectors by setting a `sample_limit` value in your [scrape configs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config) .\nIf you use this limit, we recommend that you set it to a very high value, so that it only catches obviously misconfigured metrics. Any samples over the limit are dropped, and it can be very hard to diagnose issues caused by exceeding the limit.\nUsing a sample limit a good way to manage sample ingestion, but the limit can protect you against accidental misconfiguration. For more information, see [Using sample_limit to avoid overload](https://www.robustperception.io/using-sample_limit-to-avoid-overload) .\n## Identify and attribute costs\nYou can use Cloud Monitoring to identify the Prometheus metrics that are writing the largest numbers of samples. These metrics are contributing the most to your costs. After you identify the most expensive metrics, you can modify your scrape configs to filter these metrics appropriately.\nThe Cloud Monitoring **Metrics Management** page provides information that can help you control the amount you spend on chargeable metrics without affecting observability. The **Metrics Management** page reports the following information:\n- Ingestion volumes for both byte- and sample-based billing, across metric  domains and for individual metrics.\n- Data about labels and cardinality of metrics.\n- Use of metrics in alerting policies and custom dashboards.\n- Rate of metric-write errors.To view the **Metrics Management** page, do the following:\n- In the navigation panel of the Google Cloud console, select **Monitoring** , and then select query_stats **Metrics management** : [Go to Metrics management](https://console.cloud.google.com/monitoring/metrics-management) \n- In the toolbar, select your time window. By default, the **Metrics Management** page displays information about the metrics collected  in the previous one day.For more information about the **Metrics Management** page, see [View and manage metric usage](/monitoring/docs/metrics-management) .\nThe following sections describe ways to analyze the number of samples that you are sending to Managed Service for Prometheus and attribute high volume to specific metrics, Kubernetes namespaces, and Google Cloud regions.\n### Identify high-volume metrics\nTo identify the Prometheus metrics with the largest ingestion volumes, do the following:\n- In the navigation panel of the Google Cloud console, select **Monitoring** , and then select query_stats **Metrics management** : [Go to Metrics management](https://console.cloud.google.com/monitoring/metrics-management) \n- On the **Billable samples ingested** scorecard,  click **View charts** .\n- Locate the **Namespace Volume Ingestion** chart, and then click **More chart options** .\n- Select the chart option **View in Metrics Explorer** .\n- In the **Builder** pane of Metrics Explorer, modify the fields  as follows:- In the **Metric** field, verify that the following resource and   and metric are selected: [Metric Ingestion Attribution](/monitoring/api/resources#tag_monitoring.googleapis.com/MetricIngestionAttribution) and [Samples written by attribution id](/monitoring/api/metrics_gcp#collection/attribution/write_sample_count) .\n- For the **Aggregation** field, select`sum`.\n- For the **by** field, select   the following labels:- `attribution_dimension`\n- `metric_type`\n- For the **Filter** field, use`attribution_dimension = namespace`.   You must do this after aggregating by the`attribution_dimension`label.\nThe resulting chart shows the ingestion volumes for each metric type.\n- To see the ingestion volume for each of the metrics, in the  toggle labeled **Chart Table Both** , select **Both** .  The table shows the ingested volume for each metric in the **Value** column.\n- Click the **Value** column header twice to sort the metrics by  descending ingestion volume.The resulting chart, which shows your top metrics by volume ranked by mean, looks like the following screenshot:\n### Identify high-volume namespaces\nTo attribute ingestion volume to specific Kubernetes namespaces, do the following:\n- In the navigation panel of the Google Cloud console, select **Monitoring** , and then select query_stats **Metrics management** : [Go to Metrics management](https://console.cloud.google.com/monitoring/metrics-management) \n- On the **Billable samples ingested** scorecard,  click **View charts** .\n- Locate the **Namespace Volume Ingestion** chart, and then click **More chart options** .\n- Select the chart option **View in Metrics Explorer** .\n- In the **Builder** pane in Metrics Explorer, modify the fields  as follows:- In the **Metric** field, verify that the following resource and   and metric are selected: [Metric Ingestion Attribution](/monitoring/api/resources#tag_monitoring.googleapis.com/MetricIngestionAttribution) and [Samples written by attribution id](/monitoring/api/metrics_gcp#collection/attribution/write_sample_count) .\n- Configure the rest of the query parameters as appropriate:- To correlate overall ingestion volume with namespaces:- For the **Aggregation** field, select`sum`.\n- For the **by** field, select the     following labels:- `attribution_dimension`\n- `attribution_id`\n- For the **Filter** field, use`attribution_dimension = namespace`.- To correlate ingestion volume of individual metrics with    namespaces:- For the **Aggregation** field, select`sum`.\n- For the **by** field, select     the following labels:- `attribution_dimension`\n- `attribution_id`\n- `metric_type`\n- For the **Filter** field, use`attribution_dimension = namespace`.- To identify the namespaces responsible for a specific    high-volume metric:- Identify the metric type for the high-volume metric by     using one of the other examples to identify high-volume     metric types. The metric type is the string in the table     view that begins with`prometheus.googleapis.com/`. For more     information, see [     Identify high-volume metrics](/stackdriver/docs/managed-prometheus/cost-controls#high-volume-metrics) .\n- Restrict the chart data to the identified metric type by     adding a filter for the metric type in the **Filter** field. For example:`metric_type=     prometheus.googleapis.com/container_tasks_state/gauge`.\n- For the **Aggregation** field, select`sum`.\n- For the **by** field, select     the following labels:- `attribution_dimension`\n- `attribution_id`\n- For the **Filter** field, use`attribution_dimension = namespace`.\n- To see ingestion by Google Cloud region, add the`location`label to the **by** field.\n- To see ingestion by Google Cloud project, add the`resource_container`label to the **by** field.", "guide": "Google Cloud Observability"}