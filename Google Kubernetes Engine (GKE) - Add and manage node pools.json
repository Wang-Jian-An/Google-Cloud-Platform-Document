{"title": "Google Kubernetes Engine (GKE) - Add and manage node pools", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview", "abstract": "# Google Kubernetes Engine (GKE) - Add and manage node pools\nThis page shows you how to add and perform operations on node pools running your Google Kubernetes Engine (GKE) [clusters](/kubernetes-engine/docs/concepts/cluster-architecture) . To learn about how node pools work, refer to [About nodepools](/kubernetes-engine/docs/concepts/node-pools) .\nClusters can perform operations, such as node auto-provisioning, on multiple node pools in parallel. You can manually create, update, or delete a node pool while another node pool is already being created, updated, or deleted.\n", "content": "## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n**Note:** OS Login is not supported in public GKE clusters. OS Login is supported in private GKE clusters that run node pool versions 1.20.5 or later. OS Login is explicitly disabled on GKE nodes, even if it is enabled on the Google Cloud project.\n## Add a node pool\nYou can add a new node pool to a GKE Standard cluster using the gcloud CLI or the Google Cloud console. GKE also supports [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning#enable) , which automatically manages the node pools in your cluster based on scaling requirements.\nAs a best practice in both cases, we recommend that you create and use a minimally-privileged Identity and Access Management (IAM) service account for your node pools to use instead of the [Compute Engine default service account](/compute/docs/access/service-accounts#default_service_account) . For instructions to create a minimally-privileged service account, refer to [Hardening your cluster's security](/kubernetes-engine/docs/how-to/hardening-your-cluster#use_least_privilege_sa) .\nTo create a node pool, run the [gcloud container node-pools create](/sdk/gcloud/reference/container/node-pools/create) command:\n```\ngcloud container node-pools create POOL_NAME \\\u00a0 \u00a0 --cluster CLUSTER_NAME \\\u00a0 \u00a0 --service-account SERVICE_ACCOUNT\n```\nReplace the following:- ``: the name of the new node pool.\n- ``: the name of your existing cluster.\n- ``: the name of the IAM service account for your nodes to use. If omitted, the node pool uses the [Compute Engine default service account](/compute/docs/access/service-accounts#default_service_account) .\n- To provision nodes through private IP addresses, add the`--enable-private-nodes`flag. You can use this flag in public clusters that use Private Service Connect. To check if your cluster uses Private Service Connect, see [Public clusters with Private Service Connect](/kubernetes-engine/docs/concepts/network-overview#public-cluster-psc) .\nFor a full list of optional flags you can specify, refer to the [gcloud container node-pools create](/sdk/gcloud/reference/container/node-pools/create) documentation.\nThe output is similar to the following:\n```\nCreating node pool POOL_NAME...done.\nCreated [https://container.googleapis.com/v1/projects/PROJECT_ID/zones/us-central1/clusters/CLUSTER_NAME/nodePools/POOL_NAME].\nNAME: POOL_NAME\nMACHINE_TYPE: e2-medium\nDISK_SIZE_GB: 100\nNODE_VERSION: 1.21.5-gke.1302\n```\nIn this output, you see details about the node pool, such as the machine type and GKE version running on the nodes.\nOccasionally, the node pool is created successfully but the `gcloud` command times out instead of reporting the status from the server. To check the status of all node pools, including those not yet fully provisioned, use the following command:\n```\ngcloud container node-pools list --cluster CLUSTER_NAME\n```\nTo add a node pool to an existing cluster, perform the following steps:- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Click **Add node pool** .\n- Configure your node pool.\n- In the navigation menu, click **Security** .\n- In the **Service account** drop-down menu, select the IAM service account for your node pool to use. By default, the node pool uses the [Compute Engine default service account](/compute/docs/access/service-accounts#default_service_account) .\n- Click **Create** to add the node pool.## View node pools in a cluster\nTo list all the node pools of a cluster, run the [gcloud container node-pools list](/sdk/gcloud/reference/container/clusters/list) command:\n```\ngcloud container node-pools list --cluster CLUSTER_NAME\n```\nTo view details about a specific node pool, run the [gcloud container node-pools describe](/sdk/gcloud/reference/container/clusters/describe) command:\n```\ngcloud container node-pools describe POOL_NAME \\\u00a0 \u00a0 --cluster CLUSTER_NAME\n```\nReplace the following:- ``: the name of the cluster.\n- ``: the name of the node pool to view.\nTo view node pools for a cluster, perform the following steps:- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster.\n- Click the **Nodes** tab.\n- Under **Node Pools** , click the name of the node pool you want to view.## Scale a node pool\nYou can scale your node pools up or down to optimize for performance and cost. With GKE Standard node pools, you can [scale a node pool horizontally](#resizing_a_node_pool) by changing the number of nodes in the node pool, or [scale a node poolvertically](#change-machine-attributes) by changing the machine attribute configuration of the nodes.\n### Horizontally scale by changing the node count\nTo resize a cluster's node pools, run the [gcloud container clusters resize](/sdk/gcloud/reference/container/clusters/resize) command:\n```\ngcloud container clusters resize CLUSTER_NAME \\\u00a0 \u00a0 --node-pool POOL_NAME \\\u00a0 \u00a0 --num-nodes NUM_NODES\n```\nReplace the following:- ``: the name of the cluster to resize.\n- ``: the name of the node pool to resize.\n- ``: the number of nodes in the pool in a zonal cluster. If you use multi-zonal or regional clusters,is the number of nodes for each zone the node pool is in.\nRepeat this command for each node pool. If your cluster has only one node pool, omit the `--node-pool` flag.\nTo resize a cluster's node pools, perform the following steps:- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Click the **Nodes** tab.\n- In the **Node Pools** section, click the name of the node pool that you want to resize.\n- Click **Resize** .\n- In the **Number of nodes** field, enter how many nodes that you want in the node pool, and then click **Resize** .\n- Repeat for each node pool as needed.\n### Vertically scale by changing the node machine attributes\nYou can modify the node pool's configured machine type, disk type, and disk size.\nWhen you edit one or more of these machine attributes, GKE updates the nodes to the new configuration using the [upgradestrategy](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#when-surge-upgrades-are-used) configured for the node pool. If you configure the [blue-greenupgradestrategy](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#blue-green-upgrade-strategy) you can migrate the workloads from the original nodes to the new nodes while being able to roll back the original nodes if the migration fails. [Inspect theupgrade settings of the nodepool](/kubernetes-engine/docs/how-to/node-pool-upgrade-strategies#inspect-upgrade-settings) to ensure that the configured strategy is how you want your nodes to be updated.\nUpdate at least one of the highlighted machine attributes in the following command:\n```\ngcloud container node-pools update POOL_NAME \\\u00a0 \u00a0 --cluster CLUSTER_NAME \\\u00a0 \u00a0 --machine-type MACHINE_TYPE \\\u00a0 \u00a0 --disk-type DISK_TYPE \\\u00a0 \u00a0 --disk-size DISK_SIZE\n```\nOmit any flags for machine attributes that you don't want to change. However, you must use at least one machine attribute flag, as the command otherwise fails.\nReplace the following:\n- ``: the name of the node pool to resize.\n- ``: the name of the cluster to resize.\n- ``: the type of machine to use for nodes. To learn more, see [gcloud container node-pools update](/sdk/gcloud/reference/container/node-pools/update#--machine-type) .\n- ``: the type of the node VM boot disk, must be one of`pd-standard`,`pd-ssd`,`pd-balanced`.\n- ``: the size for node VM boot disks in GB. Defaults to 100GB.## Upgrade a node pool\nBy default, a cluster's nodes have [auto-upgrade](/kubernetes-engine/docs/concepts/cluster-upgrades#node_pool_upgrades) enabled, and it's recommended that you don't [disableit](/kubernetes-engine/docs/how-to/node-auto-upgrades#disable) . Node auto-upgrades ensure that your cluster's control plane and node version remain in sync and in compliance with the [Kubernetes version skewpolicy](https://kubernetes.io/releases/version-skew-policy/) , which ensures that control planes are compatible with nodes up to two minor versions older than the control plane. For example, Kubernetes 1.29 control planes are compatible with Kubernetes 1.27 nodes.\nWith GKE node pool upgrades, you can choose between two configurable upgrade strategies, namely [surge upgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#surge) and [blue-green upgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#blue-green-upgrade-strategy) .\n[Choose a strategy](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies) and [use the parameters to tune the strategy](/kubernetes-engine/docs/how-to/node-pool-upgrade-strategies) to best fit your cluster environment's needs.\nWhile a node is being upgraded, GKE stops scheduling new Pods onto it, and attempts to schedule its running Pods onto other nodes. This is similar to other events that re-create the node, such as enabling or disabling a feature on the node pool.\n**Note:** During automatic or manual node upgrades, [PodDisruptionBudgets (PDBs)](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) and [Pod termination grace period](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination) are respected for a maximum of 1 hour. If Pods running on a node cannot be scheduled onto new nodes within 1 hour, the upgrade is initiated, regardless. If a workload requires more flexibility with graceful termination, we recommend using [blue-green upgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#blue-green-upgrade-strategy) , which provide settings for additional soak time to extend PDB checks beyond the 1 hour default. For more information about what to expect during node termination in general, see the topic about [Pods](https://kubernetes.io/docs/concepts/workloads/pods/#pod-templates) .\nThe upgrade is only complete when all nodes have been recreated and the cluster is in the desired state. When a newly-upgraded node registers with the control plane, GKE marks the node as schedulable.\nNew node instances run the desired Kubernetes version as well as:\n- The [node image](/kubernetes-engine/docs/concepts/node-images) \n- [kubelet](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/) \n- [kube-proxy](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/) \n### Manually upgrade a node pool\nYou can manually upgrade a node pool version to match the version of the control plane or to a previous version that is still available and is compatible with the control plane. You can manually upgrade multiple node pools in parallel, whereas GKE automatically upgrades only one node pool at a time.\nWhen you manually upgrade a node pool, GKE removes any [labels youadded to individual nodes using kubectl](https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node) . To avoid this, [apply labels to node pools](/kubernetes-engine/docs/how-to/update-existing-nodepools#updating_node_labels) instead.\n**Note:** Upgrading a node pool may disrupt workloads running in that node pool. To avoid this, you can create a new node pool with the desired version and migrate the workload. After migration, you can delete the old node pool.\n**Note:** If you upgrade a node pool with an Ingress in an errored state, the instance group does not sync. To work around this issue, first check the status using the `kubectl get ing` command. If the instance group is not synced, you can work around the problem by re-applying the manifest used to create the ingress.\nYou can manually upgrade your node pools to a version compatible with the control plane, using the Google Cloud console or the Google Cloud CLI.\nThe following variables are used in the commands in this section:- ``: the name of the cluster of the node pool to be upgraded.\n- ``: the name of the node pool to be upgraded.\n- ``: the Kubernetes version to which the nodes are upgraded. For example,`--cluster-version=1.7.2`or`cluster-version=latest`.\nUpgrade a node pool:\n```\ngcloud container clusters upgrade CLUSTER_NAME \\\u00a0 --node-pool=NODE_POOL_NAME\n```\nTo specify a different version of GKE on nodes, use the optional `--cluster-version` flag:\n```\ngcloud container clusters upgrade CLUSTER_NAME \\\u00a0 --node-pool=NODE_POOL_NAME \\\u00a0 --cluster-version VERSION\n```\nFor more information about specifying versions, see [Versioning](/kubernetes-engine/versioning) .\nFor more information, refer to the [gcloud container clusters upgrade](/sdk/gcloud/reference/container/clusters/upgrade) documentation.\nTo upgrade a node pool using the Google Cloud console, perform the following steps:- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Next to the cluster you want to edit, click **Actions** , then click **Edit** .\n- On the **Cluster details** page, click the **Nodes** tab.\n- In the **Node Pools** section, click the name of the node pool that you want to upgrade.\n- Click **Edit** .\n- Click **Change** under **Node version** .\n- Select the desired version from the **Node version** drop-down list, then click **Change** .\n **Note:** It may take several minutes for the node version to change.\n## Deploy a Pod to a specific node pool\nYou can explicitly deploy a Pod to a specific node pool by using a `nodeSelector` in your Pod manifest. `nodeSelector` schedules Pods onto nodes with a matching label.\nAll GKE node pools have labels with the following format: `cloud.google.com/gke-nodepool:` `` . Add this label to the `nodeSelector` field in your Pod as shown in the following example:\n```\napiVersion: v1kind: Podmetadata:\u00a0 name: nginx\u00a0 labels:\u00a0 \u00a0 env: testspec:\u00a0 containers:\u00a0 - name: nginx\u00a0 \u00a0 image: nginx\u00a0 \u00a0 imagePullPolicy: IfNotPresent\u00a0 nodeSelector:\u00a0 \u00a0 cloud.google.com/gke-nodepool: POOL_NAME\n```\nFor more information, see [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/) .\nAs an alternative to node selector, you can use node affinity. Use node affinity if you want a \"soft\" rule where the Pod attempts to meet the constraint, but is still scheduled even if the constraint can't be satisfied. For more information, see [Node affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity) . You can also [specify resource requests for the containers](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/) .\n## Downgrade node pools\nYou can downgrade a node pool, for example, to mitigate an unsuccessful node pool upgrade. Review the [limitations](/kubernetes-engine/docs/how-to/upgrading-a-cluster#downgrading-limitations) before downgrading a node pool.\n**Note:** We recommend the [blue-green node upgrade strategy](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#blue-green-upgrade-strategy) if you need to optimize for risk mitigation for node pool upgrades impacting your workloads. With this strategy, you can [rollback](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#rollback-a-blue-green-upgrade) an in-progress upgrade to the original nodes if the upgrade is unsuccessful.\n- Set a [maintenance exclusion](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions#exclusions) for the cluster to prevent the node pool from being automatically upgraded by GKE after being downgraded.\n- To downgrade a node pool, specify an earlier version while following the instructions to [Manually upgrade a node pool](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrade_nodes) .## Delete a node pool\nDeleting a node pool deletes the nodes and all running workloads, not respecting [PodDisruptionBudget](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) settings. To learn more about how this affects your workloads, including interactions with node selectors, see [Deleting nodepools](/kubernetes-engine/docs/concepts/node-pools#drain) .\nTo delete a node pool, run the [gcloud container node-pools delete](/sdk/gcloud/reference/container/clusters/delete) command:\n```\ngcloud container node-pools delete POOL_NAME \\\u00a0 \u00a0 --cluster CLUSTER_NAME\n```\nTo delete a node pool, perform the following steps:- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Click the **Nodes** tab.\n- In the **Node Pools** section, click next to the node pool you want to delete.\n- When prompted to confirm, click **Delete** .## Troubleshoot\nFor troubleshooting information, see [Troubleshoot Standard node pools](/kubernetes-engine/docs/troubleshooting/troubleshoot-node-pools) and [Troubleshoot node registration](/kubernetes-engine/docs/troubleshooting/troubleshoot-node-registration) .\n## What's next\n- [Read about how node pools work](/kubernetes-engine/docs/concepts/node-pools) .\n- [Learn about auto-provisioning node pools](/kubernetes-engine/docs/how-to/node-auto-provisioning) .\n- [Learn how to configure kubelet and sysctl using Node System Configuration](/kubernetes-engine/docs/how-to/node-system-config) .", "guide": "Google Kubernetes Engine (GKE)"}