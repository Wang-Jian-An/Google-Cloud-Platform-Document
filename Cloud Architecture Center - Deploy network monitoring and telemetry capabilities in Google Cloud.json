{"title": "Cloud Architecture Center - Deploy network monitoring and telemetry capabilities in Google Cloud", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Deploy network monitoring and telemetry capabilities in Google Cloud\nLast reviewed 2024-02-13 UTC\nNetwork telemetry collects network traffic data from devices on your network so that the data can be analyzed. Network telemetry lets security operations teams detect network-based threats and hunt for advanced adversaries, which is essential for [autonomic security operations](/solutions/security-analytics-and-operations) . To obtain network telemetry, you need to capture and store network data. This blueprint describes how you can use [Packet Mirroring](/vpc/docs/packet-mirroring) and [Zeek](https://zeek.org/) to capture network data in Google Cloud.\nThis blueprint is intended for security analysts and network administrators who want to mirror network traffic, store this data, and forward it for analysis. This blueprint assumes that you have working knowledge of networking and network monitoring.\nThis blueprint is part of a security blueprint that's made up of the following:\n- A [GitHub repository](https://github.com/GoogleCloudPlatform/terraform-google-network-forensics) that contains a set of Terraform configurations and scripts.\n- A guide to the architecture, design, and security controls that you implement with the blueprint (this document).\nUsing this blueprint, you capture network packets (including network metadata) using Packet Mirroring, transform the network packets into Zeek logs, and then store them in Cloud Logging. The blueprint extracts metadata such as IP addresses, ports, protocols, and Layer 7 headers and requests. Storing network metadata as Zeek logs uses less data volume than storing raw packet data and is therefore more cost effective.\nThis document assumes that you have already configured a foundational set of security controls, as described in the [Google Cloud enterprise foundations guide](/architecture/security-foundations) .\n#", "content": "## Supported use cases\nThis blueprint supports the following use cases:\n- Your security operations center (SOC) requires access to Google Cloud network log data in a centralized location so that they can investigate security incidents. This blueprint translates network packet data into logs that you can forward to your analysis and investigation tools. Analysis and investigation tools include [BigQuery](/bigquery/docs) , [Chronicle](https://chronicle.security/) , [Flowmon](https://www.flowmon.com/) , [ExtraHop](https://www.extrahop.com/) , or security information and event management (SIEM).\n- Your security teams require visibility into Google Cloud networks to perform threat hunting using tools such as Chronicle. You can use this blueprint to create a pipeline for Google Cloud network traffic.\n- You want to demonstrate how your organization meets compliance requirements for network detection and response. For example, your organization must demonstrate compliance with [Memorandum M-21-31](https://www.whitehouse.gov/wp-content/uploads/2021/08/M-21-31-Improving-the-Federal-Governments-Investigative-and-Remediation-Capabilities-Related-to-Cybersecurity-Incidents.pdf) from the United States Office of Management and Budget (OMB).\n- Your network security analysts require long-term network log data. This blueprint supports both long-term monitoring and on-demand monitoring.\nIf you also require [packet capture (pcap)](https://en.wikipedia.org/wiki/Pcap) data, you need to use network protocol analyzer tools (for example, [Wireshark](https://www.wireshark.org/) or [tcpdump](https://www.tcpdump.org/) ). The use of network protocol analyzer tools is not in the scope of this blueprint.\nYou can't deploy this blueprint with [Cloud Intrusion Detection System](/intrusion-detection-system/docs) . Both this solution and Cloud Intrusion Detection System use Packet Mirroring policies, and these policies can only be used by one service at a time.\n### Costs\nThis blueprint can affect your costs because you are adding computing resources and storing significant amounts of data in Cloud Logging. Consider the following when you deploy the blueprint:\n- Each collector virtual machine (VM) in Compute Engine runs as an e2-medium instance.\n- You can control storage costs with the following:- Using Packet Mirroring filters.\n- Not mirroring across zones to avoid inter-zonal egress charges.\n- Storing data only as long as required by your organization.You can use the [Pricing Calculator](/products/calculator) to get an estimate for your computing, logging, and storage costs.\n## Architecture\nThe following architectural diagram shows the infrastructure that you use this blueprint to implement:\nThe architecture shown in the preceding image uses a combination of the following Google Cloud services and features:\n- Two [Virtual Private Cloud (VPC) networks](/vpc/docs/overview) :- A Virtual Private Cloud network for the mirrored sources.\n- A VPC network for the collector instances.\nThese VPC networks must be in the same project.\n- [Compute Engine](/compute/docs) or [Google Kubernetes Engine (GKE)](/kubernetes-engine/docs) instances (called the ) in specific [regions](/compute/docs/regions-zones) and subnets that are the source for the network packets. You identify which instances to mirror sources using one of the following methods:- [Network tags](/vpc/docs/add-remove-network-tags) \n- Compute instance names\n- Subnet name\n- Compute Engine instances that function as the behind an internal passthrough Network Load Balancer, in the same region as the mirrored sources. These instances run the [Zeek-Fluentd Golden Image](https://github.com/GoogleCloudPlatform/terraform-google-network-forensics/tree/aa4ea739243143b82a3d20c84588748044cbf333/packer) or your custom zeek-fluentd image. The VMs are e2-medium and the supported throughput is 4 Gbps.\n- An [internal passthrough Network Load Balancer](/load-balancing/docs/internal) that receives packets from the mirrored sources and forwards them to the collector instances for processing. The forwarding rule for the load balancer uses the `--is-mirroring-collector` flag.\n- [VPC firewall rules](/vpc/docs/firewalls) that permit the following:- Egress from mirrored sources to the internal passthrough Network Load Balancer.\n- Ingress from the collector instances to the mirrored instances.\n- A [Packet Mirroring policy](/vpc/docs/using-packet-mirroring#creating) that defines the region, subnet, mirrored instances, protocols, direction, and forwarding rule. Each region requires its own Packet Mirroring policy.\n- [VPC Network Peering](/vpc/docs/vpc-peering) to permit connectivity using internal IP addresses between highly available Compute Engine VMs across multiple regions. VPC Network Peering allows the mirrored sources to communicate with the internal passthrough Network Load Balancer.\n- A [Cloud Logging](/logging/docs) instance that collects all the packets for storage and retrieval by an analysis and investigation tool.## Understand the security controls that you need\nThis section discusses the security controls within Google Cloud that you can use to help secure the different components of the network monitoring architecture.\n### VPC network security controls\nYou create VPC networks around your mirrored sources and your collectors. When you create the VPC network for the collectors, you delete the [system-generated default route](/vpc/docs/routes#routingpacketsinternet) , which means that all default internet gateway routes are turned off. Turning off default internet gateways helps reduce your network attack surface from external threat attackers.\nYou create subnets in your VPC network for each region. Subnets let you control the flow of traffic between your workloads on Google Cloud and also from external sources. The subnets have [Private Google Access](/vpc/docs/configure-private-google-access) enabled. Private Google Access also helps reduce your network attack surface, while permitting VMs to communicate to Google APIs and services.\nTo permit communication between the VPC networks, you enable VPC Network Peering. VPC Network Peering uses subnet routes for internal IP address connectivity. You import and export [custom routes](/vpc/docs/vpc-peering#importing-exporting-routes) to allow a direct connection between the mirrored sources and the collectors. You must restrict all communication to regional routes because the internal passthrough Network Load Balancer for the collectors doesn't support global routes.\n### Firewall rules\nYou use firewall rules to define the connections that the mirrored sources and collectors can make. You set up an ingress rule to allow for regular uptime [health checks](/load-balancing/docs/health-checks) , an egress rule for all protocols on the mirrored sources, and an ingress rule for all protocols on the collectors.\n### Collector VM security controls\nThe collector VMs are responsible for receiving the packet data. The collector VMs are identical VMs that operate as [managed instance groups (MIGs)](/compute/docs/instance-groups#managed_instance_groups) . You turn on health checks to permit automatic recreation of an unresponsive VM. In addition, you allow the collectors to autoscale based on your usage requirements.\nEach collector VM runs the zeek-fluentd Packer image. This image consists of Zeek, which generates the logs, and Fluentd, which forwards the logs to Cloud Logging. After you deploy the Terraform module, you can update the VM OS and Zeek packages and apply the security controls that are required for your organization.\n### Internal load balancer security controls\nThe internal passthrough Network Load Balancer directs network packet traffic from the mirrored sources to the collector VMs for processing. All the collector VMs must run in the same region as the internal passthrough Network Load Balancer.\nThe forwarding rule for the internal passthrough Network Load Balancer defines that access is possible from all ports, but global access isn't allowed. In addition, the forwarding rule defines this load balancer as a mirroring collector, using the `--is-mirroring-collector` flag.\nYou don't need to set up a load balancer for storage, as each collector VM directly uploads logs to Cloud Logging.\n### Packet Mirroring\nPacket Mirroring requires you to identify the instances that you want to mirror. You can identify the instances that you want to mirror using network tags, instance names, or the subnet that the instances are located in. In addition, you can further filter traffic by using one or more of the following:\n- Layer 4 protocols, such as TCP, UDP, or ICMP.\n- IPv4 CIDR ranges in the IP headers, such as 10.0.0.0/8.\n- Direction of the traffic that you want to mirror, such as ingress, egress, or both.\n### Service accounts and access controls\n[Service accounts](/iam/docs/service-accounts) are identities that Google Cloud can use to run API requests on your behalf. Service accounts ensure that user identities don't have direct access to services.\nTo deploy the Terraform code, you must [impersonate a service account](/iam/docs/impersonating-service-accounts) that has the following roles in the project:\n- [roles/compute.admin](/compute/docs/access/iam#compute.admin) \n- [roles/compute.networkAdmin](/compute/docs/access/iam#compute.networkAdmin) \n- [roles/compute.packetMirroringAdmin](/compute/docs/access/iam#compute.packetMirroringAdmin) \n- [roles/compute.packetMirroringUser](/compute/docs/access/iam#compute.packetMirroringUser) \n- [roles/iam.serviceAccountTokenCreator](/compute/docs/access/iam#iam.serviceAccountTokenCreator) \n- [roles/iam.serviceAccountUser](/compute/docs/access/iam#iam.serviceAccountUser) \n- [roles/logging.logWriter](/logging/docs/access-control#permissions_and_roles) \n- [roles/monitoring.metricWriter](/monitoring/access-control#mon_roles_desc) \n- [roles/storage.admin](/storage/docs/access-control/iam-roles) \nThe collector VMs also require this service account so that they can authenticate to Google Cloud services, get the network packets, and forward them to Cloud Logging.\n### Data retention practices\nYou can specify how long Cloud Logging stores your network logs using [retention rules](/logging/docs/routing/overview#logs-retention) for your [log buckets](/logging/docs/routing/overview#buckets) . To determine how long to store the data, review your organization's regulatory requirements.\n### Logging and auditing\nYou can use [Cloud Monitoring](/monitoring/docs/monitoring-overview) to analyze the performance of the collector VMs and set up alerts for uptime checks and performance conditions such as CPU load.\nYou can track administrator access or changes to the data and configuration using [Cloud Audit Logs](/logging/docs/audit) . Audit logging is supported by [Compute Engine](/compute/docs/logging/audit-logging) , [Cloud Load Balancing](/load-balancing/docs/audit-logging) , and [Cloud Logging](/logging/docs/audit-logging) .\nYou can export monitoring information as follows:\n- To Chronicle for additional analysis. For more information, see [Ingesting Google Cloud Logs in to Chronicle](/security/chronicle) .\n- To a third-party SIEM, using Pub/Sub and Dataflow. For more information, see [Export Google Cloud security data to your SIEM system](/community/tutorials/exporting-security-data-to-your-siem) .## Bringing it all together\nTo implement the architecture described in this document, do the following:\n- Deploy a secure baseline in Google Cloud, as described in the [Google Cloud enterprise foundations blueprint](/architecture/security-foundations) . If you choose not to deploy the enterprise foundations blueprint, ensure that your environment has a similar security baseline in place.\n- Review the [Readme](https://github.com/GoogleCloudPlatform/terraform-google-network-forensics) for the blueprint and ensure that you meet all the prerequisites.\n- In your testing environment, deploy one of the [example network telemetry configurations](https://github.com/GoogleCloudPlatform/terraform-google-network-forensics/tree/main/examples) to see the blueprint in action. As part of your testing process, do the following:- Verify that the Packet Mirroring policies and subnets were created.\n- Verify that you have the [Logs Viewer](/logging/docs/access-control) ( `roles/logging.viewer` ) role and run a [curl](https://curl.se/) command to view your log data. For example:`curl http://example.com/`You should see that log data is stored in Cloud Logging.\n- Use Security Command Center to scan the newly created resources against your [compliance requirements](/security-command-center/docs/concepts-vulnerabilities-findings) .\n- Verify that your system is capturing and storing the appropriate network packets, and fine-tune the performance as necessary.\n- Deploy the blueprint into your production environment.\n- Connect Cloud Logging to your [SIEM](/community/tutorials/exporting-security-data-to-your-siem) or [Chronicle](/security/chronicle) so that your SOC and network security analysts can incorporate the new telemetry into their dashboards.## What's next\n- Work [through the blueprint](https://github.com/GoogleCloudPlatform/terraform-google-network-forensics) .\n- Read about [When to use five telemetry types in security threat monitoring](https://cloud.google.com/blog/products/networking/when-to-use-5-telemetry-types-in-security-threat-monitoring) .\n- Read about [Leveraging Network Telemetry in Google Cloud](https://cloud.google.com/blog/products/networking/open-source-solutions-and-how-tos) .\n- Read about transforming your SOC using [autonomic security operations](https://services.google.com/fh/files/misc/googlecloud_autonomicsecurityoperations_soc10x.pdf) .", "guide": "Cloud Architecture Center"}