{"title": "Dataproc - Dataproc optional Ranger component", "url": "https://cloud.google.com/dataproc/docs/concepts/components/ranger", "abstract": "# Dataproc - Dataproc optional Ranger component\nYou can install additional components like Ranger when you create a Dataproc cluster using the [Optional components](/dataproc/docs/concepts/components/overview#available_optional_components) feature. This page describes the Ranger component.\nThe [Apache Ranger](https://ranger.apache.org/) component is an open source framework to manage permission and auditing for the Hadoop ecosystem. The Ranger admin server and Web UI are available on port `6080` on the cluster's first master node.\n**Also see:**\n- [Using Ranger with Kerberos](/dataproc/docs/concepts/components/ranger-w-kerberos) \n- [Back up and restore a Ranger schema](/dataproc/docs/concepts/components/backup-ranger-schema) \n- [Best practices to use Apache Ranger on Dataproc](https://cloud.google.com/blog/products/data-analytics/running-cloud-managed-spark-and-hadoop-using-ranger) ", "content": "## Install the component\nNote: Before running the gcloud CLI commands on this page, either:- [set the gcloud CLI project ID](/sdk/gcloud/reference/config/set#EXAMPLES) or\n- add the [--project PROJECT_ID](/sdk/gcloud/reference#--project) flag to each gcloud command\nInstall the component when you create a Dataproc cluster. Components can be added to clusters created with Dataproc [version 1.3](/dataproc/docs/concepts/versioning/dataproc-release-1.3) and later. The Ranger component requires the installation of the [Solr](/dataproc/docs/concepts/components/solr) component as shown below.\nSee [Supported Dataproc versions](/dataproc/docs/concepts/versioning/dataproc-versions#supported_cloud_dataproc_versions) for the component version included in each Dataproc image release.\n### Installation steps:\n[](None)\n- Set up your Ranger admin password:- Grant the [Cloud KMS CryptoKey Encrypter/Decrypter role](/kms/docs/reference/permissions-and-roles#predefined_roles) to the cluster [service account](/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_cloud_dataproc) . By default, the cluster service account is set as the [Compute Engine default service account](/compute/docs/access/service-accounts#default_service_account) , which has following form:```\nproject-number-compute@developer.gserviceaccount.com\n```You can [specify a different cluster service account](/dataproc/docs/concepts/configuring-clusters/service-accounts#using_service_accounts) when you create the cluster, below.- **Example:** Grant the Cloud KMS CryptoKey Encrypter/Decrypter role to the Compute Engine default service account:```\ngcloud projects add-iam-policy-binding project-id \\\n\u00a0\u00a0\u00a0\u00a0--member=serviceAccount:project-number-compute@developer.gserviceaccount.com \\\n\u00a0\u00a0\u00a0\u00a0--role=roles/cloudkms.cryptoKeyDecrypter\n```\n- Encrypt your Ranger admin user's password using a [Key Management Service (KMS)](/kms/docs/encrypt-decrypt) key. **Your\npassword must consist of at least 8 characters with a minimum of one\nalphabetic and one numeric character.** - **Example** :- Create the key ring:```\ngcloud kms keyrings create my-keyring --location=global\n```\n- Create the key:```\ngcloud kms keys create my-key \\\n\u00a0\u00a0\u00a0\u00a0--location=global \\\n\u00a0\u00a0\u00a0\u00a0--keyring=my-keyring \\\n\u00a0\u00a0\u00a0\u00a0--purpose=encryption\n```\n- Encrypt your Ranger admin user password:```\necho \"my-ranger-admin-password\" | \\\n\u00a0\u00a0gcloud kms encrypt \\\n\u00a0\u00a0\u00a0\u00a0--location=global \\\n\u00a0\u00a0\u00a0\u00a0--keyring=my-keyring \\\n\u00a0\u00a0\u00a0\u00a0--key=my-key \\\n\u00a0\u00a0\u00a0\u00a0--plaintext-file=- \\\n\u00a0\u00a0\u00a0\u00a0--ciphertext-file=admin-password.encrypted\n```\n- Upload the encrypted password to a [Cloud Storage bucket](/storage/docs/creating-buckets) in your project.- **Example** :```\ngsutil cp admin-password.encrypted gs://my-bucket\n```\n- Create your cluster:- When installing the Ranger component, the [Solr component](/dataproc/docs/concepts/components/solr) must also be installed, as shown below.- The Ranger component relies on the Solr component to store and query its audit logs, which by default uses HDFS as storage. This HDFS data is deleted when the cluster is deleted. To configure the Solr component to store data, including the Ranger audit logs, on Cloud Storage, use the`dataproc:solr.gcs.path=gs://<bucket>` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) when you create your cluster. Cloud Storage data persists after the cluster is deleted.\n- Pass the KMS key and password Cloud Storage URIs to the cluster creation command by setting the`dataproc:ranger.kms.key.uri`and`dataproc:ranger.admin.password.uri` [cluster properties](/dataproc/docs/concepts/configuring-clusters/cluster-properties#dataproc-properties) .\n- Optionally, you can pass in the Ranger database's admin user password through an encrypted Cloud Storage file URI by setting the`dataproc:ranger.db.admin.password.uri` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#dataproc-properties) .\n- By default, the Ranger component uses the MySql database instance running on the cluster's first master node. In the MySQL instance, enable the`log_bin_trust_function_creators`flag by setting the variable to`ON`. Setting this flag controls whether stored function creators can be trusted. After successful cluster creation and Ranger configuration, you can reset the`log_bin_trust_function_creators`to`OFF`.\n- To persist the Ranger database after cluster deletion, use a [Cloud SQL](/sql/docs/mysql) instance as the external MySql Database.- Set the`dataproc:ranger.cloud-sql.instance.connection.name` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#dataproc-properties) to the Cloud SQL instance.\n- Set the`dataproc:ranger.cloud-sql.root.password.uri` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#dataproc-properties) to the Cloud Storage URI of the KMS-key encrypted root password of the Cloud SQL instance.\n- Set the`dataproc:ranger.cloud-sql.use-private-ip` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#dataproc-properties) to indicate whether the connection to the Cloud SQL instance is over private IP.\nThe Ranger component uses [Cloud SQL Proxy](/sql/docs/mysql/sql-proxy) to connect to the Cloud SQL instance. To use the proxy:- Set the`sqlservice.admin`API scope when you create the cluster (see [Authorizing requests with OAuth 2.0](/sql/docs/mysql/admin-api#OAuth2Authorizing) ). If using the`gcloud dataproc cluster create`command, add the`--scopes=default,sql-admin`parameter.\n- Enable [the SQL Admin API](https://console.cloud.google.com/flows/enableapi?apiid=sqladmin.googleapis.com) in your project.\n- Make sure the cluster service account has the [Cloud SQL Editor](/sql/docs/mysql/project-access-control#roles) role.\nTo create a Dataproc cluster that includes the Ranger component,  use the [gcloud dataproc clusters create](/sdk/gcloud/reference/dataproc/clusters/create)  command with the `--optional-components` flag.\nWhen creating the cluster, use [gcloud dataproc clusters create](/sdk/gcloud/reference/dataproc/clusters/create) command with the`--enable-component-gateway`flag, as shown below, to enable connecting to the Ranger Admin Web UI using the [Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) .\n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=SOLR,RANGER \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway \\\n\u00a0\u00a0\u00a0\u00a0--properties=\"dataproc:ranger.kms.key.uri=projects/project-id/locations/global/keyRings/my-keyring/cryptoKeys/my-key,dataproc:ranger.admin.password.uri=gs://my-bucket/admin-password.encrypted\" \\\n\u00a0\u00a0\u00a0\u00a0... other flags\n```Specify the Ranger and Solr components in the [SoftwareConfig.Component](/dataproc/docs/reference/rest/v1/ClusterConfig#Component) field as part of a Dataproc API [clusters.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) request. You must also set the following [cluster properties](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) in the [SoftwareConfig.Component.properties](/static/dataproc/docs/reference/rest/v1/ClusterConfig#SoftwareConfig.FIELDS.properties) field:\n- `dataproc:ranger.kms.key.uri`: \"projects//locations/global/keyRings//cryptoKeys/\"\n- `dataproc:ranger.admin.password.uri`: \"gs:///admin-password.encrypted\"\nUsing the [Dataproc v1 API](/dataproc/docs/reference/rest#rest-resource:-v1.projects.regions.clusters) , set the [EndpointConfig.enableHttpPortAccess](/dataproc/docs/reference/rest/v1/ClusterConfig#EndpointConfig) property to`true`as part of the clusters.create request to enable connecting to the Jupyter notebook Web UI using the [Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) .\n- Enable the component and component gateway.- In the Google Cloud console, open the Dataproc [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page. The Set up cluster panel is selected.\n- In the Components section:- Under Optional components, select Ranger, Solr, and other optional  components to install on your cluster.\n- Under Component Gateway, select Enable component gateway  (see [Viewing and Accessing Component Gateway URLs](/dataproc/docs/concepts/accessing/dataproc-gateways#viewing_and_accessing_component_gateway_urls) ).Click the tab. Under , click **Ranger** to open the Ranger web interface. Login with the Ranger admin username (for example, \"admin\") and password.\n## Ranger Admin logs\nRanger admin logs are available in [Logging](https://console.cloud.google.com/logs) as `ranger-admin-root` logs.", "guide": "Dataproc"}