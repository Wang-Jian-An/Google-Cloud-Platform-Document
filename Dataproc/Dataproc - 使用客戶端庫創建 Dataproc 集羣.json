{"title": "Dataproc - \u4f7f\u7528\u5ba2\u6236\u7aef\u5eab\u5275\u5efa Dataproc \u96c6\u7fa3", "url": "https://cloud.google.com/dataproc/docs/quickstarts/create-cluster-client-libraries?hl=zh-cn", "abstract": "# Dataproc - \u4f7f\u7528\u5ba2\u6236\u7aef\u5eab\u5275\u5efa Dataproc \u96c6\u7fa3\n# \u4f7f\u7528\u5ba2\u6236\u7aef\u5eab\u5275\u5efa Dataproc \u96c6\u7fa3\n\u4e0b\u9762\u5217\u51fa\u7684\u793a\u4f8b\u4ee3\u78bc\u4ecb\u7d39\u77ad\u5982\u4f55\u4f7f\u7528 [Cloud \u5ba2\u6236\u7aef\u5eab](https://cloud.google.com/apis/docs/client-libraries-explained?hl=zh-cn#google_cloud_client_libraries) \u5275\u5efa Dataproc \u96c6\u7fa3\uff0c\u5728\u96c6\u7fa3\u4e0a\u904b\u884c\u4f5c\u696d\uff0c\u7136\u5f8c\u522a\u9664\u96c6\u7fa3\u3002\n\u60a8\u9084\u53ef\u4ee5\u901a\u904e\u4ee5\u4e0b\u65b9\u6cd5\u57f7\u884c\u9019\u4e9b\u4efb\u52d9\uff1a- [\u5feb\u901f\u5165\u9580\uff1a\u4f7f\u7528 API Explorer](https://cloud.google.com/dataproc/docs/quickstarts?hl=zh-cn#quickstarts-using-the-apis-explorer) \u4e2d\u4ecb\u7d39\u7684 API REST \u8acb\u6c42\n- [\u4f7f\u7528 Google Cloud \u63a7\u5236\u6aaf\u5275\u5efa Dataproc \u96c6\u7fa3](https://cloud.google.com/dataproc/docs/quickstarts/create-cluster-console?hl=zh-cn) \u4e2d\u7684 Google Cloud \u63a7\u5236\u6aaf\n- [\u4f7f\u7528 Google Cloud CLI \u5275\u5efa Dataproc \u96c6\u7fa3](https://cloud.google.com/dataproc/docs/quickstarts/create-cluster-gcloud?hl=zh-cn) \u4e2d\u7684 Google Cloud CLI\n", "content": "## \u6e96\u5099\u5de5\u4f5c- Titles in dynamic includes are not used anywhere, and we should avoid paying to translate them## \u904b\u884c\u4ee3\u78bc **\u8a66\u7528\u6f14\u793a** \uff1a\u9ede\u64ca **\u5728 Cloud Shell \u4e2d\u6253\u958b** \u4ee5\u904b\u884c Python Cloud \u5ba2\u6236\u7aef\u5eab\u6f14\u793a\uff0c\u8a72\u6f14\u793a\u6703\u5275\u5efa\u96c6\u7fa3\uff0c\u904b\u884c PySpark \u4f5c\u696d\uff0c\u7136\u5f8c\u522a\u9664\u96c6\u7fa3\u3002\n [\u5728 Cloud Shell \u4e2d\u6253\u958b](https://ssh.cloud.google.com/cloudshell/open?cloudshell_git_repo=https%3A%2F%2Fgithub.com%2Fgoogleapis%2Fpython-dataproc&cloudshell_working_dir=samples%2Fsnippets&tutorial=python-api-walkthrough.md&cloudshell_open_in_editor=submit_job_to_cluster.py&hl=zh-cn) \n- [\u5b89\u88dd\u5ba2\u6236\u7aef\u5eab](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#installing_the_client_library) \u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u8a2d\u7f6e\u958b\u767c\u74b0\u5883](https://cloud.google.com/go/docs/setup?hl=zh-cn) \u3002\n- [\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#setting_up_authentication) \n- \u514b\u9686\u4e26\u904b\u884c\u793a\u4f8b GitHub \u4ee3\u78bc\u3002 **\u9078\u64c7 PySpark \u8f38\u5165\u6587\u4ef6\uff1a** \u4e0b\u9762\u7684 Google \u5ba2\u6236\u7aef\u5eab\u4ee3\u78bc\u793a\u4f8b\u6703\u904b\u884c\u60a8\u6307\u5b9a\u7232\u8f38\u5165\u53c3\u6578\u7684 PySpark \u4f5c\u696d\u3002\u60a8\u53ef\u4ee5\u50b3\u5165 Cloud Storage \u4e2d\u7684\u7c21\u55ae\u201cHello World\u201dPySpark \u61c9\u7528 (`gs://dataproc-examples/pyspark/hello-world/hello-world.py`) \u6216\u8005\u60a8\u81ea\u5df1\u7684 PySpark \u61c9\u7528\u3002\u5982\u9700\u8a73\u7d30\u77ad\u89e3\u5982\u4f55\u5c07\u6587\u4ef6\u4e0a\u50b3\u5230 Cloud Storage\uff0c\u8acb\u53c3\u95b1 [\u4e0a\u50b3\u5c0d\u8c61](https://cloud.google.com/storage/docs/uploading-objects?hl=zh-cn) \u3002\n- \u67e5\u770b\u8f38\u51fa\u3002\u4ee3\u78bc\u6703\u5c07\u4f5c\u696d\u9a45\u52d5\u7a0b\u5e8f\u65e5\u8a8c\u8f38\u51fa\u5230 Cloud Storage \u4e2d\u7684\u9ed8\u8a8d Dataproc [\u66ab\u5b58\u5b58\u5132\u6876](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket?hl=zh-cn) \u3002\u60a8\u53ef\u4ee5\u5728\u9805\u76ee\u7684 Dataproc [\u4f5c\u696d](https://console.cloud.google.com/project/_/dataproc/jobs?hl=zh-cn) \u90e8\u5206\u4e2d\u67e5\u770b\u4f86\u81ea Google Cloud \u63a7\u5236\u6aaf\u7684\u4f5c\u696d\u9a45\u52d5\u7a0b\u5e8f\u8f38\u51fa\u3002\u9ede\u64ca\u201c\u4f5c\u696d\u8a73\u60c5\u201d\u9801\u9762\u4e0a\u7684 **\u4f5c\u696d ID** \u4ee5\u67e5\u770b\u4f5c\u696d\u8f38\u51fa\u3002\n [  dataproc/quickstart/quickstart.go ](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/dataproc/quickstart/quickstart.go) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/dataproc/quickstart/quickstart.go) \n```\n// This quickstart shows how you can use the Dataproc Client library to create a// Dataproc cluster, submit a PySpark job to the cluster, wait for the job to finish// and finally delete the cluster.//// Usage://// \u00a0 \u00a0 \u00a0go build// \u00a0 \u00a0 \u00a0./quickstart --project_id <PROJECT_ID> --region <REGION> \\// \u00a0 \u00a0 \u00a0 \u00a0 \u00a0--cluster_name <CLUSTER_NAME> --job_file_path <GCS_JOB_FILE_PATH>package mainimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"flag\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io/ioutil\"\u00a0 \u00a0 \u00a0 \u00a0 \"log\"\u00a0 \u00a0 \u00a0 \u00a0 \"regexp\"\u00a0 \u00a0 \u00a0 \u00a0 dataproc \"cloud.google.com/go/dataproc/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/dataproc/apiv1/dataprocpb\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/storage\"\u00a0 \u00a0 \u00a0 \u00a0 \"google.golang.org/api/option\")func main() {\u00a0 \u00a0 \u00a0 \u00a0 var projectID, clusterName, region, jobFilePath string\u00a0 \u00a0 \u00a0 \u00a0 flag.StringVar(&projectID, \"project_id\", \"\", \"Cloud Project ID, used for creating resources.\")\u00a0 \u00a0 \u00a0 \u00a0 flag.StringVar(&region, \"region\", \"\", \"Region that resources should be created in.\")\u00a0 \u00a0 \u00a0 \u00a0 flag.StringVar(&clusterName, \"cluster_name\", \"\", \"Name of Cloud Dataproc cluster to create.\")\u00a0 \u00a0 \u00a0 \u00a0 flag.StringVar(&jobFilePath, \"job_file_path\", \"\", \"Path to job file in GCS.\")\u00a0 \u00a0 \u00a0 \u00a0 flag.Parse()\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // Create the cluster client.\u00a0 \u00a0 \u00a0 \u00a0 endpoint := fmt.Sprintf(\"%s-dataproc.googleapis.com:443\", region)\u00a0 \u00a0 \u00a0 \u00a0 clusterClient, err := dataproc.NewClusterControllerClient(ctx, option.WithEndpoint(endpoint))\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"error creating the cluster client: %s\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Create the cluster config.\u00a0 \u00a0 \u00a0 \u00a0 createReq := &dataprocpb.CreateClusterRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ProjectId: projectID,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Region: \u00a0 \u00a0region,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Cluster: &dataprocpb.Cluster{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ProjectId: \u00a0 projectID,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterName: clusterName,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Config: &dataprocpb.ClusterConfig{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MasterConfig: &dataprocpb.InstanceGroupConfig{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 NumInstances: \u00a0 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MachineTypeUri: \"n1-standard-2\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 WorkerConfig: &dataprocpb.InstanceGroupConfig{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 NumInstances: \u00a0 2,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MachineTypeUri: \"n1-standard-2\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Create the cluster.\u00a0 \u00a0 \u00a0 \u00a0 createOp, err := clusterClient.CreateCluster(ctx, createReq)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"error submitting the cluster creation request: %v\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 createResp, err := createOp.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"error creating the cluster: %v\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Defer cluster deletion.\u00a0 \u00a0 \u00a0 \u00a0 defer func() {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 dReq := &dataprocpb.DeleteClusterRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ProjectId: \u00a0 projectID,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Region: \u00a0 \u00a0 \u00a0region,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterName: clusterName,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 deleteOp, err := clusterClient.DeleteCluster(ctx, dReq)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 deleteOp.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"error deleting cluster %q: %v\\n\", clusterName, err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"Cluster %q successfully deleted\\n\", clusterName)\u00a0 \u00a0 \u00a0 \u00a0 }()\u00a0 \u00a0 \u00a0 \u00a0 // Output a success message.\u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"Cluster created successfully: %q\\n\", createResp.ClusterName)\u00a0 \u00a0 \u00a0 \u00a0 // Create the job client.\u00a0 \u00a0 \u00a0 \u00a0 jobClient, err := dataproc.NewJobControllerClient(ctx, option.WithEndpoint(endpoint))\u00a0 \u00a0 \u00a0 \u00a0 // Create the job config.\u00a0 \u00a0 \u00a0 \u00a0 submitJobReq := &dataprocpb.SubmitJobRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ProjectId: projectID,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Region: \u00a0 \u00a0region,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Job: &dataprocpb.Job{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Placement: &dataprocpb.JobPlacement{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterName: clusterName,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TypeJob: &dataprocpb.Job_PysparkJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PysparkJob: &dataprocpb.PySparkJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MainPythonFileUri: jobFilePath,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 submitJobOp, err := jobClient.SubmitJobAsOperation(ctx, submitJobReq)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"error with request to submitting job: %v\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 submitJobResp, err := submitJobOp.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"error submitting job: %v\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 re := regexp.MustCompile(\"gs://(.+?)/(.+)\")\u00a0 \u00a0 \u00a0 \u00a0 matches := re.FindStringSubmatch(submitJobResp.DriverOutputResourceUri)\u00a0 \u00a0 \u00a0 \u00a0 if len(matches) < 3 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"regex error: %s\\n\", submitJobResp.DriverOutputResourceUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Dataproc job outget gets saved to a GCS bucket allocated to it.\u00a0 \u00a0 \u00a0 \u00a0 storageClient, err := storage.NewClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"error creating storage client: %v\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 obj := fmt.Sprintf(\"%s.000000000\", matches[2])\u00a0 \u00a0 \u00a0 \u00a0 reader, err := storageClient.Bucket(matches[1]).Object(obj).NewReader(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"error reading job output: %v\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer reader.Close()\u00a0 \u00a0 \u00a0 \u00a0 body, err := ioutil.ReadAll(reader)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"could not read output from Dataproc Job: %v\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"Job finished successfully: %s\", body)}\n```\n- [\u5b89\u88dd\u5ba2\u6236\u7aef\u5eab](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#installing_the_client_library) \u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u8a2d\u7f6e Java \u958b\u767c\u74b0\u5883](https://cloud.google.com/java/docs/setup?hl=zh-cn) \u3002\n- [\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#setting_up_authentication) \n- \u514b\u9686\u4e26\u904b\u884c\u793a\u4f8b GitHub \u4ee3\u78bc\u3002 **\u9078\u64c7 PySpark \u8f38\u5165\u6587\u4ef6\uff1a** \u4e0b\u9762\u7684 Google \u5ba2\u6236\u7aef\u5eab\u4ee3\u78bc\u793a\u4f8b\u6703\u904b\u884c\u60a8\u6307\u5b9a\u7232\u8f38\u5165\u53c3\u6578\u7684 PySpark \u4f5c\u696d\u3002\u60a8\u53ef\u4ee5\u50b3\u5165 Cloud Storage \u4e2d\u7684\u7c21\u55ae\u201cHello World\u201dPySpark \u61c9\u7528 (`gs://dataproc-examples/pyspark/hello-world/hello-world.py`) \u6216\u8005\u60a8\u81ea\u5df1\u7684 PySpark \u61c9\u7528\u3002\u5982\u9700\u8a73\u7d30\u77ad\u89e3\u5982\u4f55\u5c07\u6587\u4ef6\u4e0a\u50b3\u5230 Cloud Storage\uff0c\u8acb\u53c3\u95b1 [\u4e0a\u50b3\u5c0d\u8c61](https://cloud.google.com/storage/docs/uploading-objects?hl=zh-cn) \u3002\n- \u67e5\u770b\u8f38\u51fa\u3002\u4ee3\u78bc\u6703\u5c07\u4f5c\u696d\u9a45\u52d5\u7a0b\u5e8f\u65e5\u8a8c\u8f38\u51fa\u5230 Cloud Storage \u4e2d\u7684\u9ed8\u8a8d Dataproc [\u66ab\u5b58\u5b58\u5132\u6876](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket?hl=zh-cn) \u3002\u60a8\u53ef\u4ee5\u5728\u9805\u76ee\u7684 Dataproc [\u4f5c\u696d](https://console.cloud.google.com/project/_/dataproc/jobs?hl=zh-cn) \u90e8\u5206\u4e2d\u67e5\u770b\u4f86\u81ea Google Cloud \u63a7\u5236\u6aaf\u7684\u4f5c\u696d\u9a45\u52d5\u7a0b\u5e8f\u8f38\u51fa\u3002\u9ede\u64ca\u201c\u4f5c\u696d\u8a73\u60c5\u201d\u9801\u9762\u4e0a\u7684 **\u4f5c\u696d ID** \u4ee5\u67e5\u770b\u4f5c\u696d\u8f38\u51fa\u3002\n [  dataproc/src/main/java/quickstart.java ](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataproc/src/main/java/Quickstart.java) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataproc/src/main/java/Quickstart.java) \n```\n/* This quickstart sample walks a user through creating a Cloud Dataproc\u00a0* cluster, submitting a PySpark job from Google Cloud Storage to the\u00a0* cluster, reading the output of the job and deleting the cluster, all\u00a0* using the Java client library.\u00a0*\u00a0* Usage:\u00a0* \u00a0 \u00a0 mvn clean package -DskipTests\u00a0*\u00a0* \u00a0 \u00a0 mvn exec:java -Dexec.args=\"<PROJECT_ID> <REGION> <CLUSTER_NAME> <GCS_JOB_FILE_PATH>\"\u00a0*\u00a0* \u00a0 \u00a0 You can also set these arguments in the main function instead of providing them via the CLI.\u00a0*/import com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.dataproc.v1.Cluster;import com.google.cloud.dataproc.v1.ClusterConfig;import com.google.cloud.dataproc.v1.ClusterControllerClient;import com.google.cloud.dataproc.v1.ClusterControllerSettings;import com.google.cloud.dataproc.v1.ClusterOperationMetadata;import com.google.cloud.dataproc.v1.InstanceGroupConfig;import com.google.cloud.dataproc.v1.Job;import com.google.cloud.dataproc.v1.JobControllerClient;import com.google.cloud.dataproc.v1.JobControllerSettings;import com.google.cloud.dataproc.v1.JobMetadata;import com.google.cloud.dataproc.v1.JobPlacement;import com.google.cloud.dataproc.v1.PySparkJob;import com.google.cloud.storage.Blob;import com.google.cloud.storage.Storage;import com.google.cloud.storage.StorageOptions;import com.google.protobuf.Empty;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.regex.Matcher;import java.util.regex.Pattern;public class Quickstart {\u00a0 public static void quickstart(\u00a0 \u00a0 \u00a0 String projectId, String region, String clusterName, String jobFilePath)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 String myEndpoint = String.format(\"%s-dataproc.googleapis.com:443\", region);\u00a0 \u00a0 // Configure the settings for the cluster controller client.\u00a0 \u00a0 ClusterControllerSettings clusterControllerSettings =\u00a0 \u00a0 \u00a0 \u00a0 ClusterControllerSettings.newBuilder().setEndpoint(myEndpoint).build();\u00a0 \u00a0 // Configure the settings for the job controller client.\u00a0 \u00a0 JobControllerSettings jobControllerSettings =\u00a0 \u00a0 \u00a0 \u00a0 JobControllerSettings.newBuilder().setEndpoint(myEndpoint).build();\u00a0 \u00a0 // Create both a cluster controller client and job controller client with the\u00a0 \u00a0 // configured settings. The client only needs to be created once and can be reused for\u00a0 \u00a0 // multiple requests. Using a try-with-resources closes the client, but this can also be done\u00a0 \u00a0 // manually with the .close() method.\u00a0 \u00a0 try (ClusterControllerClient clusterControllerClient =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterControllerClient.create(clusterControllerSettings);\u00a0 \u00a0 \u00a0 \u00a0 JobControllerClient jobControllerClient =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 JobControllerClient.create(jobControllerSettings)) {\u00a0 \u00a0 \u00a0 // Configure the settings for our cluster.\u00a0 \u00a0 \u00a0 InstanceGroupConfig masterConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InstanceGroupConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMachineTypeUri(\"n1-standard-2\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setNumInstances(1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 InstanceGroupConfig workerConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InstanceGroupConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMachineTypeUri(\"n1-standard-2\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setNumInstances(2)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 ClusterConfig clusterConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMasterConfig(masterConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setWorkerConfig(workerConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // Create the cluster object with the desired cluster config.\u00a0 \u00a0 \u00a0 Cluster cluster =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Cluster.newBuilder().setClusterName(clusterName).setConfig(clusterConfig).build();\u00a0 \u00a0 \u00a0 // Create the Cloud Dataproc cluster.\u00a0 \u00a0 \u00a0 OperationFuture<Cluster, ClusterOperationMetadata> createClusterAsyncRequest =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 clusterControllerClient.createClusterAsync(projectId, region, cluster);\u00a0 \u00a0 \u00a0 Cluster clusterResponse = createClusterAsyncRequest.get();\u00a0 \u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\"Cluster created successfully: %s\", clusterResponse.getClusterName()));\u00a0 \u00a0 \u00a0 // Configure the settings for our job.\u00a0 \u00a0 \u00a0 JobPlacement jobPlacement = JobPlacement.newBuilder().setClusterName(clusterName).build();\u00a0 \u00a0 \u00a0 PySparkJob pySparkJob = PySparkJob.newBuilder().setMainPythonFileUri(jobFilePath).build();\u00a0 \u00a0 \u00a0 Job job = Job.newBuilder().setPlacement(jobPlacement).setPysparkJob(pySparkJob).build();\u00a0 \u00a0 \u00a0 // Submit an asynchronous request to execute the job.\u00a0 \u00a0 \u00a0 OperationFuture<Job, JobMetadata> submitJobAsOperationAsyncRequest =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 jobControllerClient.submitJobAsOperationAsync(projectId, region, job);\u00a0 \u00a0 \u00a0 Job jobResponse = submitJobAsOperationAsyncRequest.get();\u00a0 \u00a0 \u00a0 // Print output from Google Cloud Storage.\u00a0 \u00a0 \u00a0 Matcher matches =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Pattern.compile(\"gs://(.*?)/(.*)\").matcher(jobResponse.getDriverOutputResourceUri());\u00a0 \u00a0 \u00a0 matches.matches();\u00a0 \u00a0 \u00a0 Storage storage = StorageOptions.getDefaultInstance().getService();\u00a0 \u00a0 \u00a0 Blob blob = storage.get(matches.group(1), String.format(\"%s.000000000\", matches.group(2)));\u00a0 \u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\"Job finished successfully: %s\", new String(blob.getContent())));\u00a0 \u00a0 \u00a0 // Delete the cluster.\u00a0 \u00a0 \u00a0 OperationFuture<Empty, ClusterOperationMetadata> deleteClusterAsyncRequest =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 clusterControllerClient.deleteClusterAsync(projectId, region, clusterName);\u00a0 \u00a0 \u00a0 deleteClusterAsyncRequest.get();\u00a0 \u00a0 \u00a0 System.out.println(String.format(\"Cluster \\\"%s\\\" successfully deleted.\", clusterName));\u00a0 \u00a0 } catch (ExecutionException e) {\u00a0 \u00a0 \u00a0 System.err.println(String.format(\"quickstart: %s \", e.getMessage()));\u00a0 \u00a0 }\u00a0 }\u00a0 public static void main(String... args) throws IOException, InterruptedException {\u00a0 \u00a0 if (args.length != 4) {\u00a0 \u00a0 \u00a0 System.err.println(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Insufficient number of parameters provided. Please make sure a \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"PROJECT_ID, REGION, CLUSTER_NAME and JOB_FILE_PATH are provided, in this order.\");\u00a0 \u00a0 \u00a0 return;\u00a0 \u00a0 }\u00a0 \u00a0 String projectId = args[0]; // project-id of project to create the cluster in\u00a0 \u00a0 String region = args[1]; // region to create the cluster\u00a0 \u00a0 String clusterName = args[2]; // name of the cluster\u00a0 \u00a0 String jobFilePath = args[3]; // location in GCS of the PySpark job\u00a0 \u00a0 quickstart(projectId, region, clusterName, jobFilePath);\u00a0 }}\n```\n- [\u5b89\u88dd\u5ba2\u6236\u7aef\u5eab](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#installing_the_client_library) \u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u8a2d\u7f6e Node.js \u958b\u767c\u74b0\u5883](https://cloud.google.com/nodejs/docs/setup?hl=zh-cn) \u3002\n- [\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#setting_up_authentication) \n- \u514b\u9686\u4e26\u904b\u884c\u793a\u4f8b GitHub \u4ee3\u78bc\u3002 **\u9078\u64c7 PySpark \u8f38\u5165\u6587\u4ef6\uff1a** \u4e0b\u9762\u7684 Google \u5ba2\u6236\u7aef\u5eab\u4ee3\u78bc\u793a\u4f8b\u6703\u904b\u884c\u60a8\u6307\u5b9a\u7232\u8f38\u5165\u53c3\u6578\u7684 PySpark \u4f5c\u696d\u3002\u60a8\u53ef\u4ee5\u50b3\u5165 Cloud Storage \u4e2d\u7684\u7c21\u55ae\u201cHello World\u201dPySpark \u61c9\u7528 (`gs://dataproc-examples/pyspark/hello-world/hello-world.py`) \u6216\u8005\u60a8\u81ea\u5df1\u7684 PySpark \u61c9\u7528\u3002\u5982\u9700\u8a73\u7d30\u77ad\u89e3\u5982\u4f55\u5c07\u6587\u4ef6\u4e0a\u50b3\u5230 Cloud Storage\uff0c\u8acb\u53c3\u95b1 [\u4e0a\u50b3\u5c0d\u8c61](https://cloud.google.com/storage/docs/uploading-objects?hl=zh-cn) \u3002\n- \u67e5\u770b\u8f38\u51fa\u3002\u4ee3\u78bc\u6703\u5c07\u4f5c\u696d\u9a45\u52d5\u7a0b\u5e8f\u65e5\u8a8c\u8f38\u51fa\u5230 Cloud Storage \u4e2d\u7684\u9ed8\u8a8d Dataproc [\u66ab\u5b58\u5b58\u5132\u6876](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket?hl=zh-cn) \u3002\u60a8\u53ef\u4ee5\u5728\u9805\u76ee\u7684 Dataproc [\u4f5c\u696d](https://console.cloud.google.com/project/_/dataproc/jobs?hl=zh-cn) \u90e8\u5206\u4e2d\u67e5\u770b\u4f86\u81ea Google Cloud \u63a7\u5236\u6aaf\u7684\u4f5c\u696d\u9a45\u52d5\u7a0b\u5e8f\u8f38\u51fa\u3002\u9ede\u64ca\u201c\u4f5c\u696d\u8a73\u60c5\u201d\u9801\u9762\u4e0a\u7684 **\u4f5c\u696d ID** \u4ee5\u67e5\u770b\u4f5c\u696d\u8f38\u51fa\u3002\n [  dataproc/quickstart.js ](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/dataproc/quickstart.js) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/dataproc/quickstart.js) \n```\n// This quickstart sample walks a user through creating a Dataproc// cluster, submitting a PySpark job from Google Cloud Storage to the// cluster, reading the output of the job and deleting the cluster, all// using the Node.js client library.'use strict';function main(projectId, region, clusterName, jobFilePath) {\u00a0 const dataproc = require('@google-cloud/dataproc');\u00a0 const {Storage} = require('@google-cloud/storage');\u00a0 // Create a cluster client with the endpoint set to the desired cluster region\u00a0 const clusterClient = new dataproc.v1.ClusterControllerClient({\u00a0 \u00a0 apiEndpoint: `${region}-dataproc.googleapis.com`,\u00a0 \u00a0 projectId: projectId,\u00a0 });\u00a0 // Create a job client with the endpoint set to the desired cluster region\u00a0 const jobClient = new dataproc.v1.JobControllerClient({\u00a0 \u00a0 apiEndpoint: `${region}-dataproc.googleapis.com`,\u00a0 \u00a0 projectId: projectId,\u00a0 });\u00a0 async function quickstart() {\u00a0 \u00a0 // Create the cluster config\u00a0 \u00a0 const cluster = {\u00a0 \u00a0 \u00a0 projectId: projectId,\u00a0 \u00a0 \u00a0 region: region,\u00a0 \u00a0 \u00a0 cluster: {\u00a0 \u00a0 \u00a0 \u00a0 clusterName: clusterName,\u00a0 \u00a0 \u00a0 \u00a0 config: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 masterConfig: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 numInstances: 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 machineTypeUri: 'n1-standard-2',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 workerConfig: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 numInstances: 2,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 machineTypeUri: 'n1-standard-2',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 };\u00a0 \u00a0 // Create the cluster\u00a0 \u00a0 const [operation] = await clusterClient.createCluster(cluster);\u00a0 \u00a0 const [response] = await operation.promise();\u00a0 \u00a0 // Output a success message\u00a0 \u00a0 console.log(`Cluster created successfully: ${response.clusterName}`);\u00a0 \u00a0 const job = {\u00a0 \u00a0 \u00a0 projectId: projectId,\u00a0 \u00a0 \u00a0 region: region,\u00a0 \u00a0 \u00a0 job: {\u00a0 \u00a0 \u00a0 \u00a0 placement: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 clusterName: clusterName,\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 pysparkJob: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mainPythonFileUri: jobFilePath,\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 };\u00a0 \u00a0 const [jobOperation] = await jobClient.submitJobAsOperation(job);\u00a0 \u00a0 const [jobResponse] = await jobOperation.promise();\u00a0 \u00a0 const matches =\u00a0 \u00a0 \u00a0 jobResponse.driverOutputResourceUri.match('gs://(.*?)/(.*)');\u00a0 \u00a0 const storage = new Storage();\u00a0 \u00a0 const output = await storage\u00a0 \u00a0 \u00a0 .bucket(matches[1])\u00a0 \u00a0 \u00a0 .file(`${matches[2]}.000000000`)\u00a0 \u00a0 \u00a0 .download();\u00a0 \u00a0 // Output a success message.\u00a0 \u00a0 console.log(`Job finished successfully: ${output}`);\u00a0 \u00a0 // Delete the cluster once the job has terminated.\u00a0 \u00a0 const deleteClusterReq = {\u00a0 \u00a0 \u00a0 projectId: projectId,\u00a0 \u00a0 \u00a0 region: region,\u00a0 \u00a0 \u00a0 clusterName: clusterName,\u00a0 \u00a0 };\u00a0 \u00a0 const [deleteOperation] =\u00a0 \u00a0 \u00a0 await clusterClient.deleteCluster(deleteClusterReq);\u00a0 \u00a0 await deleteOperation.promise();\u00a0 \u00a0 // Output a success message\u00a0 \u00a0 console.log(`Cluster ${clusterName} successfully deleted.`);\u00a0 }\u00a0 quickstart();}const args = process.argv.slice(2);if (args.length !== 4) {\u00a0 console.log(\u00a0 \u00a0 'Insufficient number of parameters provided. Please make sure a ' +\u00a0 \u00a0 \u00a0 'PROJECT_ID, REGION, CLUSTER_NAME and JOB_FILE_PATH are provided, in this order.'\u00a0 );}main(...args);\n```\n- [\u5b89\u88dd\u5ba2\u6236\u7aef\u5eab](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#installing_the_client_library) \u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u8a2d\u7f6e Python \u958b\u767c\u74b0\u5883](https://cloud.google.com/python/docs/setup?hl=zh-cn) \u3002\n- [\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#setting_up_authentication) \n- \u514b\u9686\u4e26\u904b\u884c\u793a\u4f8b GitHub \u4ee3\u78bc\u3002 **\u9078\u64c7 PySpark \u8f38\u5165\u6587\u4ef6\uff1a** \u4e0b\u9762\u7684 Google \u5ba2\u6236\u7aef\u5eab\u4ee3\u78bc\u793a\u4f8b\u6703\u904b\u884c\u60a8\u6307\u5b9a\u7232\u8f38\u5165\u53c3\u6578\u7684 PySpark \u4f5c\u696d\u3002\u60a8\u53ef\u4ee5\u50b3\u5165 Cloud Storage \u4e2d\u7684\u7c21\u55ae\u201cHello World\u201dPySpark \u61c9\u7528 (`gs://dataproc-examples/pyspark/hello-world/hello-world.py`) \u6216\u8005\u60a8\u81ea\u5df1\u7684 PySpark \u61c9\u7528\u3002\u5982\u9700\u8a73\u7d30\u77ad\u89e3\u5982\u4f55\u5c07\u6587\u4ef6\u4e0a\u50b3\u5230 Cloud Storage\uff0c\u8acb\u53c3\u95b1 [\u4e0a\u50b3\u5c0d\u8c61](https://cloud.google.com/storage/docs/uploading-objects?hl=zh-cn) \u3002\n- \u67e5\u770b\u8f38\u51fa\u3002\u4ee3\u78bc\u6703\u5c07\u4f5c\u696d\u9a45\u52d5\u7a0b\u5e8f\u65e5\u8a8c\u8f38\u51fa\u5230 Cloud Storage \u4e2d\u7684\u9ed8\u8a8d Dataproc [\u66ab\u5b58\u5b58\u5132\u6876](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket?hl=zh-cn) \u3002\u60a8\u53ef\u4ee5\u5728\u9805\u76ee\u7684 Dataproc [\u4f5c\u696d](https://console.cloud.google.com/project/_/dataproc/jobs?hl=zh-cn) \u90e8\u5206\u4e2d\u67e5\u770b\u4f86\u81ea Google Cloud \u63a7\u5236\u6aaf\u7684\u4f5c\u696d\u9a45\u52d5\u7a0b\u5e8f\u8f38\u51fa\u3002\u9ede\u64ca\u201c\u4f5c\u696d\u8a73\u60c5\u201d\u9801\u9762\u4e0a\u7684 **\u4f5c\u696d ID** \u4ee5\u67e5\u770b\u4f5c\u696d\u8f38\u51fa\u3002\n [  dataproc/snippets/quickstart/quickstart.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataproc/snippets/quickstart/quickstart.py) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataproc/snippets/quickstart/quickstart.py) \n```\n\"\"\"This quickstart sample walks a user through creating a Cloud Dataproccluster, submitting a PySpark job from Google Cloud Storage to thecluster, reading the output of the job and deleting the cluster, allusing the Python client library.Usage:\u00a0 \u00a0 python quickstart.py --project_id <PROJECT_ID> --region <REGION> \\\u00a0 \u00a0 \u00a0 \u00a0 --cluster_name <CLUSTER_NAME> --job_file_path <GCS_JOB_FILE_PATH>\"\"\"import argparseimport refrom google.cloud import dataproc_v1 as dataprocfrom google.cloud import storagedef quickstart(project_id, region, cluster_name, job_file_path):\u00a0 \u00a0 # Create the cluster client.\u00a0 \u00a0 cluster_client = dataproc.ClusterControllerClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options={\"api_endpoint\": \"{}-dataproc.googleapis.com:443\".format(region)}\u00a0 \u00a0 )\u00a0 \u00a0 # Create the cluster config.\u00a0 \u00a0 cluster = {\u00a0 \u00a0 \u00a0 \u00a0 \"project_id\": project_id,\u00a0 \u00a0 \u00a0 \u00a0 \"cluster_name\": cluster_name,\u00a0 \u00a0 \u00a0 \u00a0 \"config\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"master_config\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"num_instances\": 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"machine_type_uri\": \"n1-standard-2\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"disk_config\": {\"boot_disk_size_gb\": 100},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"worker_config\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"num_instances\": 2,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"machine_type_uri\": \"n1-standard-2\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"disk_config\": {\"boot_disk_size_gb\": 100},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 }\u00a0 \u00a0 # Create the cluster.\u00a0 \u00a0 operation = cluster_client.create_cluster(\u00a0 \u00a0 \u00a0 \u00a0 request={\"project_id\": project_id, \"region\": region, \"cluster\": cluster}\u00a0 \u00a0 )\u00a0 \u00a0 result = operation.result()\u00a0 \u00a0 print(\"Cluster created successfully: {}\".format(result.cluster_name))\u00a0 \u00a0 # Create the job client.\u00a0 \u00a0 job_client = dataproc.JobControllerClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options={\"api_endpoint\": \"{}-dataproc.googleapis.com:443\".format(region)}\u00a0 \u00a0 )\u00a0 \u00a0 # Create the job config.\u00a0 \u00a0 job = {\u00a0 \u00a0 \u00a0 \u00a0 \"placement\": {\"cluster_name\": cluster_name},\u00a0 \u00a0 \u00a0 \u00a0 \"pyspark_job\": {\"main_python_file_uri\": job_file_path},\u00a0 \u00a0 }\u00a0 \u00a0 operation = job_client.submit_job_as_operation(\u00a0 \u00a0 \u00a0 \u00a0 request={\"project_id\": project_id, \"region\": region, \"job\": job}\u00a0 \u00a0 )\u00a0 \u00a0 response = operation.result()\u00a0 \u00a0 # Dataproc job output gets saved to the Google Cloud Storage bucket\u00a0 \u00a0 # allocated to the job. Use a regex to obtain the bucket and blob info.\u00a0 \u00a0 matches = re.match(\"gs://(.*?)/(.*)\", response.driver_output_resource_uri)\u00a0 \u00a0 output = (\u00a0 \u00a0 \u00a0 \u00a0 storage.Client()\u00a0 \u00a0 \u00a0 \u00a0 .get_bucket(matches.group(1))\u00a0 \u00a0 \u00a0 \u00a0 .blob(f\"{matches.group(2)}.000000000\")\u00a0 \u00a0 \u00a0 \u00a0 .download_as_bytes()\u00a0 \u00a0 \u00a0 \u00a0 .decode(\"utf-8\")\u00a0 \u00a0 )\u00a0 \u00a0 print(f\"Job finished successfully: {output}\")\u00a0 \u00a0 # Delete the cluster once the job has terminated.\u00a0 \u00a0 operation = cluster_client.delete_cluster(\u00a0 \u00a0 \u00a0 \u00a0 request={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"project_id\": project_id,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"region\": region,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"cluster_name\": cluster_name,\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 )\u00a0 \u00a0 operation.result()\u00a0 \u00a0 print(\"Cluster {} successfully deleted.\".format(cluster_name))if __name__ == \"__main__\":\u00a0 \u00a0 parser = argparse.ArgumentParser(\u00a0 \u00a0 \u00a0 \u00a0 description=__doc__,\u00a0 \u00a0 \u00a0 \u00a0 formatter_class=argparse.RawDescriptionHelpFormatter,\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--project_id\",\u00a0 \u00a0 \u00a0 \u00a0 type=str,\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"Project to use for creating resources.\",\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--region\",\u00a0 \u00a0 \u00a0 \u00a0 type=str,\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"Region where the resources should live.\",\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--cluster_name\",\u00a0 \u00a0 \u00a0 \u00a0 type=str,\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"Name to use for creating a cluster.\",\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--job_file_path\",\u00a0 \u00a0 \u00a0 \u00a0 type=str,\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"Job in GCS to execute against the cluster.\",\u00a0 \u00a0 )\u00a0 \u00a0 args = parser.parse_args()\u00a0 \u00a0 quickstart(args.project_id, args.region, args.cluster_name, args.job_file_path)\n```\n## \u5f8c\u7e8c\u6b65\u9a5f\n- \u8acb\u53c3\u95b1\u95dc\u65bc Dataproc Cloud \u5ba2\u6236\u7aef\u5eab\u7684 [\u5176\u4ed6\u8cc7\u6e90](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#additional_resources) \u3002", "guide": "Dataproc"}