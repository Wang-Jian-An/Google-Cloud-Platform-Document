{"title": "Dataproc - Dataproc optional Jupyter component", "url": "https://cloud.google.com/dataproc/docs/concepts/components/jupyter", "abstract": "# Dataproc - Dataproc optional Jupyter component\nYou can install additional components like Jupyter when you create a Dataproc cluster using the [Optional components](/dataproc/docs/concepts/components/overview#available_optional_components) feature. This page describes the Jupyter component.\nThe [Jupyter](http://jupyter.org/) component is a Web-based notebook for interactive data analytics and supports the [JupyterLab](https://jupyterlab.readthedocs.io/en/stable/index.html) Web UI. The Jupyter Web UI is available on port `8123` on the cluster's first master node.\n- Jupyter can be configured by providing`dataproc:jupyter` [cluster properties](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) . To reduce the risk of remote code execution over unsecured notebook server APIs, the default`dataproc:jupyter.listen.all.interfaces`cluster property setting is`false`, which restricts connections to`localhost (127.0.0.1)`when the [Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) is enabled (Component Gateway activation is required when installing the Jupyter component).\nThe Jupyter notebook provides a Python kernel to run [Spark](https://spark.apache.org/) code, and a PySpark kernel. By default, notebooks are [saved in Cloud Storage](https://github.com/src-d/jgscm) in the Dataproc staging bucket, which is specified by the user or [auto-created](/dataproc/docs/guides/create-cluster#auto-created_staging_bucket) when the cluster is created. The location can be changed at cluster creation time via the [dataproc:jupyter.notebook.gcs.dir](/dataproc/docs/concepts/configuring-clusters/cluster-properties#dataproc-properties) cluster property.\n**Working with data files.** It's easy to use a Jupyter notebook to work with data files that have been [uploaded to Cloud Storage](/storage/docs/uploading-objects) . Since the [Cloud Storage Connector](/dataproc/docs/concepts/connectors/cloud-storage) is pre-installed on a Dataproc cluster, you can reference the files directly in your notebook. Here's an example that accesses CSV files in Cloud Storage (see [Generic Load/Save Functions](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html) ) for more PySpark data load/save examples):```\ndf = spark.read.csv(\"gs://bucket/path/file.csv\")\ndf.show()\n```\n", "content": "## Install Jupyter\nInstall the component when you create a Dataproc cluster. The Jupyter component requires activation of the Dataproc [Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) . When using image version 1.5, installation of the Jupyter component also requires installation of the [Anaconda](/dataproc/docs/concepts/components/anaconda) component.\n- Enable the component.- In the Google Cloud console, open the Dataproc [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page. The **Set up cluster** panel is selected.\n- In the **Components** section:- Under **Optional components** , select the **Jupyter** component, and,    if using image version 1.5, the **Anaconda** component.\n- Under **Component Gateway** , select **Enable component gateway** (see [Viewing and Accessing Component Gateway URLs](/dataproc/docs/concepts/accessing/dataproc-gateways#viewing_and_accessing_component_gateway_urls) ).\nTo create a Dataproc cluster that includes the Jupyter component, use the [gcloud dataproc clusters create](/sdk/gcloud/reference/dataproc/clusters/create)  command with the `--optional-components` flag.\n **Latest default image version example** \nThe following example installs the Jupyter component on a cluster that uses the latest default image version.\n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=JUPYTER \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway \\\n\u00a0\u00a0\u00a0\u00a0... other flags\n```\n **1.5 image version example** \nThe following 1.5 image version example installs both the Jupyter and Anaconda components (Anaconda component installation is required when using image version 1.5).\n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=ANACONDA,JUPYTER \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--image-version=1.5 \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway \\\n\u00a0\u00a0\u00a0\u00a0... other flags\n```The Jupyter component can be installed through the Dataproc API using [SoftwareConfig.Component](/dataproc/docs/reference/rest/v1/ClusterConfig#Component) as part of a [clusters.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) request (Anaconda component installation is also required when using image version 1.5).- Set the [EndpointConfig.enableHttpPortAccess](/dataproc/docs/reference/rest/v1/ClusterConfig#endpointconfig) property to`true`as part of the`clusters.create`request to enable connecting to the Jupyter notebook Web UI using the [Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) .## Open the Jupyter and JupyterLab UIs\nClick the [Google Cloud console Component Gateway links](/dataproc/docs/concepts/accessing/dataproc-gateways#viewing_and_accessing_component_gateway_urls) to open in your local browser the Jupyter notebook or JupyterLab UI running on the cluster master node.\n**Select \"GCS\" or \"Local Disk\" to create a new Jupyter Notebook ineither location.**\n## Attaching GPUs to Master and/or Worker Nodes\nYou can [add GPUs](https://cloud.google.com/dataproc/docs/concepts/compute/gpus) to your cluster's master and worker nodes when using a Jupyter notebook to:\n- Preprocess data in Spark, then collect a [DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) onto the master and run [TensorFlow](https://www.tensorflow.org/) \n- Use Spark to orchestrate TensorFlow runs in parallel\n- Run [Tensorflow-on-YARN](https://github.com/criteo/tf-yarn) \n- Use with other machine learning scenarios that use GPUs", "guide": "Dataproc"}