{"title": "Dataproc - Create a Dataproc cluster by using client libraries", "url": "https://cloud.google.com/dataproc/docs/quickstarts/create-cluster-client-libraries", "abstract": "# Dataproc - Create a Dataproc cluster by using client libraries\n# Create a Dataproc cluster by using client libraries\nThe sample code listed, below, shows you how to use the [Cloud Client Libraries](/apis/docs/client-libraries-explained#google_cloud_client_libraries) to create a Dataproc cluster, run a job on the cluster, then delete the cluster.\nYou can also perform these tasks using:- API REST requests in [Quickstarts Using the API Explorer](/dataproc/docs/quickstarts#quickstarts-using-the-apis-explorer) \n- the Google Cloud console in [Create a Dataproc cluster by using the Google Cloud console](/dataproc/docs/quickstarts/create-cluster-console) \n- the Google Cloud CLI in [Create a Dataproc cluster by using the Google Cloud CLI](/dataproc/docs/quickstarts/create-cluster-gcloud) \n", "content": "## Before you begin\n## Run the Code **Try the walkthrough:** Click **Open in Cloud Shell** to run a Python Cloud Client Libraries walkthrough that creates a cluster, runs a PySpark job, then deletes the cluster.\n [Open in Cloud Shell](https://ssh.cloud.google.com/cloudshell/open?cloudshell_git_repo=https://github.com/googleapis/python-dataproc&cloudshell_working_dir=samples/snippets&tutorial=python-api-walkthrough.md&cloudshell_open_in_editor=submit_job_to_cluster.py) \n- [Install the client library](/dataproc/docs/reference/libraries#installing_the_client_library) For more information, See [Setting up your development environment](/go/docs/setup) .\n- [Set up authentication](/dataproc/docs/reference/libraries#setting_up_authentication) \n- Clone and run the sample GitHub code. **Selecting a PySpark input file:** The Google Client Library code samples, below, run a PySpark job that you specify as an input parameter. You can pass in the simple \"Hello World\" PySpark app located in Cloud Storage at`gs://dataproc-examples/pyspark/hello-world/hello-world.py`or your own PySpark app. See [Uploading objects](/storage/docs/uploading-objects) to learn more about uploading files to Cloud Storage.\n- View the output. The code outputs the job driver log to the default Dataproc [staging bucket](/dataproc/docs/concepts/configuring-clusters/staging-bucket) in Cloud Storage. You can view job driver output from the Google Cloud console in your project's Dataproc [Jobs](https://console.cloud.google.com/project/_/dataproc/jobs) section. Click on the **Job ID** to view job output on the Job details page.\n [  dataproc/quickstart/quickstart.go ](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/dataproc/quickstart/quickstart.go) [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/dataproc/quickstart/quickstart.go) \n```\n// This quickstart shows how you can use the Dataproc Client library to create a// Dataproc cluster, submit a PySpark job to the cluster, wait for the job to finish// and finally delete the cluster.//// Usage://// \u00a0 \u00a0 \u00a0go build// \u00a0 \u00a0 \u00a0./quickstart --project_id <PROJECT_ID> --region <REGION> \\// \u00a0 \u00a0 \u00a0 \u00a0 \u00a0--cluster_name <CLUSTER_NAME> --job_file_path <GCS_JOB_FILE_PATH>package mainimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"flag\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io/ioutil\"\u00a0 \u00a0 \u00a0 \u00a0 \"log\"\u00a0 \u00a0 \u00a0 \u00a0 \"regexp\"\u00a0 \u00a0 \u00a0 \u00a0 dataproc \"cloud.google.com/go/dataproc/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/dataproc/apiv1/dataprocpb\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/storage\"\u00a0 \u00a0 \u00a0 \u00a0 \"google.golang.org/api/option\")func main() {\u00a0 \u00a0 \u00a0 \u00a0 var projectID, clusterName, region, jobFilePath string\u00a0 \u00a0 \u00a0 \u00a0 flag.StringVar(&projectID, \"project_id\", \"\", \"Cloud Project ID, used for creating resources.\")\u00a0 \u00a0 \u00a0 \u00a0 flag.StringVar(&region, \"region\", \"\", \"Region that resources should be created in.\")\u00a0 \u00a0 \u00a0 \u00a0 flag.StringVar(&clusterName, \"cluster_name\", \"\", \"Name of Cloud Dataproc cluster to create.\")\u00a0 \u00a0 \u00a0 \u00a0 flag.StringVar(&jobFilePath, \"job_file_path\", \"\", \"Path to job file in GCS.\")\u00a0 \u00a0 \u00a0 \u00a0 flag.Parse()\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // Create the cluster client.\u00a0 \u00a0 \u00a0 \u00a0 endpoint := fmt.Sprintf(\"%s-dataproc.googleapis.com:443\", region)\u00a0 \u00a0 \u00a0 \u00a0 clusterClient, err := dataproc.NewClusterControllerClient(ctx, option.WithEndpoint(endpoint))\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"error creating the cluster client: %s\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Create the cluster config.\u00a0 \u00a0 \u00a0 \u00a0 createReq := &dataprocpb.CreateClusterRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ProjectId: projectID,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Region: \u00a0 \u00a0region,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Cluster: &dataprocpb.Cluster{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ProjectId: \u00a0 projectID,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterName: clusterName,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Config: &dataprocpb.ClusterConfig{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MasterConfig: &dataprocpb.InstanceGroupConfig{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 NumInstances: \u00a0 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MachineTypeUri: \"n1-standard-2\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 WorkerConfig: &dataprocpb.InstanceGroupConfig{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 NumInstances: \u00a0 2,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MachineTypeUri: \"n1-standard-2\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Create the cluster.\u00a0 \u00a0 \u00a0 \u00a0 createOp, err := clusterClient.CreateCluster(ctx, createReq)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"error submitting the cluster creation request: %v\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 createResp, err := createOp.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"error creating the cluster: %v\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Defer cluster deletion.\u00a0 \u00a0 \u00a0 \u00a0 defer func() {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 dReq := &dataprocpb.DeleteClusterRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ProjectId: \u00a0 projectID,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Region: \u00a0 \u00a0 \u00a0region,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterName: clusterName,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 deleteOp, err := clusterClient.DeleteCluster(ctx, dReq)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 deleteOp.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"error deleting cluster %q: %v\\n\", clusterName, err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"Cluster %q successfully deleted\\n\", clusterName)\u00a0 \u00a0 \u00a0 \u00a0 }()\u00a0 \u00a0 \u00a0 \u00a0 // Output a success message.\u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"Cluster created successfully: %q\\n\", createResp.ClusterName)\u00a0 \u00a0 \u00a0 \u00a0 // Create the job client.\u00a0 \u00a0 \u00a0 \u00a0 jobClient, err := dataproc.NewJobControllerClient(ctx, option.WithEndpoint(endpoint))\u00a0 \u00a0 \u00a0 \u00a0 // Create the job config.\u00a0 \u00a0 \u00a0 \u00a0 submitJobReq := &dataprocpb.SubmitJobRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ProjectId: projectID,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Region: \u00a0 \u00a0region,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Job: &dataprocpb.Job{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Placement: &dataprocpb.JobPlacement{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterName: clusterName,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TypeJob: &dataprocpb.Job_PysparkJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PysparkJob: &dataprocpb.PySparkJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MainPythonFileUri: jobFilePath,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 submitJobOp, err := jobClient.SubmitJobAsOperation(ctx, submitJobReq)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"error with request to submitting job: %v\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 submitJobResp, err := submitJobOp.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"error submitting job: %v\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 re := regexp.MustCompile(\"gs://(.+?)/(.+)\")\u00a0 \u00a0 \u00a0 \u00a0 matches := re.FindStringSubmatch(submitJobResp.DriverOutputResourceUri)\u00a0 \u00a0 \u00a0 \u00a0 if len(matches) < 3 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"regex error: %s\\n\", submitJobResp.DriverOutputResourceUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Dataproc job outget gets saved to a GCS bucket allocated to it.\u00a0 \u00a0 \u00a0 \u00a0 storageClient, err := storage.NewClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"error creating storage client: %v\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 obj := fmt.Sprintf(\"%s.000000000\", matches[2])\u00a0 \u00a0 \u00a0 \u00a0 reader, err := storageClient.Bucket(matches[1]).Object(obj).NewReader(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"error reading job output: %v\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer reader.Close()\u00a0 \u00a0 \u00a0 \u00a0 body, err := ioutil.ReadAll(reader)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"could not read output from Dataproc Job: %v\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fmt.Printf(\"Job finished successfully: %s\", body)}\n```\n- [Install the client library](/dataproc/docs/reference/libraries#installing_the_client_library) For more information, See [Setting Up a Java Development Environment](/java/docs/setup) .\n- [Set up authentication](/dataproc/docs/reference/libraries#setting_up_authentication) \n- Clone and run the sample GitHub code. **Selecting a PySpark input file:** The Google Client Library code samples, below, run a PySpark job that you specify as an input parameter. You can pass in the simple \"Hello World\" PySpark app located in Cloud Storage at`gs://dataproc-examples/pyspark/hello-world/hello-world.py`or your own PySpark app. See [Uploading objects](/storage/docs/uploading-objects) to learn more about uploading files to Cloud Storage.\n- View the output. The code outputs the job driver log to the default Dataproc [staging bucket](/dataproc/docs/concepts/configuring-clusters/staging-bucket) in Cloud Storage. You can view job driver output from the Google Cloud console in your project's Dataproc [Jobs](https://console.cloud.google.com/project/_/dataproc/jobs) section. Click on the **Job ID** to view job output on the Job details page.\n [  dataproc/src/main/java/Quickstart.java ](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataproc/src/main/java/Quickstart.java) [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataproc/src/main/java/Quickstart.java) \n```\n/* This quickstart sample walks a user through creating a Cloud Dataproc\u00a0* cluster, submitting a PySpark job from Google Cloud Storage to the\u00a0* cluster, reading the output of the job and deleting the cluster, all\u00a0* using the Java client library.\u00a0*\u00a0* Usage:\u00a0* \u00a0 \u00a0 mvn clean package -DskipTests\u00a0*\u00a0* \u00a0 \u00a0 mvn exec:java -Dexec.args=\"<PROJECT_ID> <REGION> <CLUSTER_NAME> <GCS_JOB_FILE_PATH>\"\u00a0*\u00a0* \u00a0 \u00a0 You can also set these arguments in the main function instead of providing them via the CLI.\u00a0*/import com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.dataproc.v1.Cluster;import com.google.cloud.dataproc.v1.ClusterConfig;import com.google.cloud.dataproc.v1.ClusterControllerClient;import com.google.cloud.dataproc.v1.ClusterControllerSettings;import com.google.cloud.dataproc.v1.ClusterOperationMetadata;import com.google.cloud.dataproc.v1.InstanceGroupConfig;import com.google.cloud.dataproc.v1.Job;import com.google.cloud.dataproc.v1.JobControllerClient;import com.google.cloud.dataproc.v1.JobControllerSettings;import com.google.cloud.dataproc.v1.JobMetadata;import com.google.cloud.dataproc.v1.JobPlacement;import com.google.cloud.dataproc.v1.PySparkJob;import com.google.cloud.storage.Blob;import com.google.cloud.storage.Storage;import com.google.cloud.storage.StorageOptions;import com.google.protobuf.Empty;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.regex.Matcher;import java.util.regex.Pattern;public class Quickstart {\u00a0 public static void quickstart(\u00a0 \u00a0 \u00a0 String projectId, String region, String clusterName, String jobFilePath)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 String myEndpoint = String.format(\"%s-dataproc.googleapis.com:443\", region);\u00a0 \u00a0 // Configure the settings for the cluster controller client.\u00a0 \u00a0 ClusterControllerSettings clusterControllerSettings =\u00a0 \u00a0 \u00a0 \u00a0 ClusterControllerSettings.newBuilder().setEndpoint(myEndpoint).build();\u00a0 \u00a0 // Configure the settings for the job controller client.\u00a0 \u00a0 JobControllerSettings jobControllerSettings =\u00a0 \u00a0 \u00a0 \u00a0 JobControllerSettings.newBuilder().setEndpoint(myEndpoint).build();\u00a0 \u00a0 // Create both a cluster controller client and job controller client with the\u00a0 \u00a0 // configured settings. The client only needs to be created once and can be reused for\u00a0 \u00a0 // multiple requests. Using a try-with-resources closes the client, but this can also be done\u00a0 \u00a0 // manually with the .close() method.\u00a0 \u00a0 try (ClusterControllerClient clusterControllerClient =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterControllerClient.create(clusterControllerSettings);\u00a0 \u00a0 \u00a0 \u00a0 JobControllerClient jobControllerClient =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 JobControllerClient.create(jobControllerSettings)) {\u00a0 \u00a0 \u00a0 // Configure the settings for our cluster.\u00a0 \u00a0 \u00a0 InstanceGroupConfig masterConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InstanceGroupConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMachineTypeUri(\"n1-standard-2\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setNumInstances(1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 InstanceGroupConfig workerConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InstanceGroupConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMachineTypeUri(\"n1-standard-2\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setNumInstances(2)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 ClusterConfig clusterConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMasterConfig(masterConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setWorkerConfig(workerConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // Create the cluster object with the desired cluster config.\u00a0 \u00a0 \u00a0 Cluster cluster =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Cluster.newBuilder().setClusterName(clusterName).setConfig(clusterConfig).build();\u00a0 \u00a0 \u00a0 // Create the Cloud Dataproc cluster.\u00a0 \u00a0 \u00a0 OperationFuture<Cluster, ClusterOperationMetadata> createClusterAsyncRequest =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 clusterControllerClient.createClusterAsync(projectId, region, cluster);\u00a0 \u00a0 \u00a0 Cluster clusterResponse = createClusterAsyncRequest.get();\u00a0 \u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\"Cluster created successfully: %s\", clusterResponse.getClusterName()));\u00a0 \u00a0 \u00a0 // Configure the settings for our job.\u00a0 \u00a0 \u00a0 JobPlacement jobPlacement = JobPlacement.newBuilder().setClusterName(clusterName).build();\u00a0 \u00a0 \u00a0 PySparkJob pySparkJob = PySparkJob.newBuilder().setMainPythonFileUri(jobFilePath).build();\u00a0 \u00a0 \u00a0 Job job = Job.newBuilder().setPlacement(jobPlacement).setPysparkJob(pySparkJob).build();\u00a0 \u00a0 \u00a0 // Submit an asynchronous request to execute the job.\u00a0 \u00a0 \u00a0 OperationFuture<Job, JobMetadata> submitJobAsOperationAsyncRequest =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 jobControllerClient.submitJobAsOperationAsync(projectId, region, job);\u00a0 \u00a0 \u00a0 Job jobResponse = submitJobAsOperationAsyncRequest.get();\u00a0 \u00a0 \u00a0 // Print output from Google Cloud Storage.\u00a0 \u00a0 \u00a0 Matcher matches =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Pattern.compile(\"gs://(.*?)/(.*)\").matcher(jobResponse.getDriverOutputResourceUri());\u00a0 \u00a0 \u00a0 matches.matches();\u00a0 \u00a0 \u00a0 Storage storage = StorageOptions.getDefaultInstance().getService();\u00a0 \u00a0 \u00a0 Blob blob = storage.get(matches.group(1), String.format(\"%s.000000000\", matches.group(2)));\u00a0 \u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\"Job finished successfully: %s\", new String(blob.getContent())));\u00a0 \u00a0 \u00a0 // Delete the cluster.\u00a0 \u00a0 \u00a0 OperationFuture<Empty, ClusterOperationMetadata> deleteClusterAsyncRequest =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 clusterControllerClient.deleteClusterAsync(projectId, region, clusterName);\u00a0 \u00a0 \u00a0 deleteClusterAsyncRequest.get();\u00a0 \u00a0 \u00a0 System.out.println(String.format(\"Cluster \\\"%s\\\" successfully deleted.\", clusterName));\u00a0 \u00a0 } catch (ExecutionException e) {\u00a0 \u00a0 \u00a0 System.err.println(String.format(\"quickstart: %s \", e.getMessage()));\u00a0 \u00a0 }\u00a0 }\u00a0 public static void main(String... args) throws IOException, InterruptedException {\u00a0 \u00a0 if (args.length != 4) {\u00a0 \u00a0 \u00a0 System.err.println(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Insufficient number of parameters provided. Please make sure a \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"PROJECT_ID, REGION, CLUSTER_NAME and JOB_FILE_PATH are provided, in this order.\");\u00a0 \u00a0 \u00a0 return;\u00a0 \u00a0 }\u00a0 \u00a0 String projectId = args[0]; // project-id of project to create the cluster in\u00a0 \u00a0 String region = args[1]; // region to create the cluster\u00a0 \u00a0 String clusterName = args[2]; // name of the cluster\u00a0 \u00a0 String jobFilePath = args[3]; // location in GCS of the PySpark job\u00a0 \u00a0 quickstart(projectId, region, clusterName, jobFilePath);\u00a0 }}\n```\n- [Install the client library](/dataproc/docs/reference/libraries#installing_the_client_library) For more information, See [Setting up a Node.js development environment](/nodejs/docs/setup) .\n- [Set up authentication](/dataproc/docs/reference/libraries#setting_up_authentication) \n- Clone and run the sample GitHub code. **Selecting a PySpark input file:** The Google Client Library code samples, below, run a PySpark job that you specify as an input parameter. You can pass in the simple \"Hello World\" PySpark app located in Cloud Storage at`gs://dataproc-examples/pyspark/hello-world/hello-world.py`or your own PySpark app. See [Uploading objects](/storage/docs/uploading-objects) to learn more about uploading files to Cloud Storage.\n- View the output. The code outputs the job driver log to the default Dataproc [staging bucket](/dataproc/docs/concepts/configuring-clusters/staging-bucket) in Cloud Storage. You can view job driver output from the Google Cloud console in your project's Dataproc [Jobs](https://console.cloud.google.com/project/_/dataproc/jobs) section. Click on the **Job ID** to view job output on the Job details page.\n [  dataproc/quickstart.js ](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/dataproc/quickstart.js) [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/dataproc/quickstart.js) \n```\n// This quickstart sample walks a user through creating a Dataproc// cluster, submitting a PySpark job from Google Cloud Storage to the// cluster, reading the output of the job and deleting the cluster, all// using the Node.js client library.'use strict';function main(projectId, region, clusterName, jobFilePath) {\u00a0 const dataproc = require('@google-cloud/dataproc');\u00a0 const {Storage} = require('@google-cloud/storage');\u00a0 // Create a cluster client with the endpoint set to the desired cluster region\u00a0 const clusterClient = new dataproc.v1.ClusterControllerClient({\u00a0 \u00a0 apiEndpoint: `${region}-dataproc.googleapis.com`,\u00a0 \u00a0 projectId: projectId,\u00a0 });\u00a0 // Create a job client with the endpoint set to the desired cluster region\u00a0 const jobClient = new dataproc.v1.JobControllerClient({\u00a0 \u00a0 apiEndpoint: `${region}-dataproc.googleapis.com`,\u00a0 \u00a0 projectId: projectId,\u00a0 });\u00a0 async function quickstart() {\u00a0 \u00a0 // Create the cluster config\u00a0 \u00a0 const cluster = {\u00a0 \u00a0 \u00a0 projectId: projectId,\u00a0 \u00a0 \u00a0 region: region,\u00a0 \u00a0 \u00a0 cluster: {\u00a0 \u00a0 \u00a0 \u00a0 clusterName: clusterName,\u00a0 \u00a0 \u00a0 \u00a0 config: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 masterConfig: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 numInstances: 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 machineTypeUri: 'n1-standard-2',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 workerConfig: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 numInstances: 2,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 machineTypeUri: 'n1-standard-2',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 };\u00a0 \u00a0 // Create the cluster\u00a0 \u00a0 const [operation] = await clusterClient.createCluster(cluster);\u00a0 \u00a0 const [response] = await operation.promise();\u00a0 \u00a0 // Output a success message\u00a0 \u00a0 console.log(`Cluster created successfully: ${response.clusterName}`);\u00a0 \u00a0 const job = {\u00a0 \u00a0 \u00a0 projectId: projectId,\u00a0 \u00a0 \u00a0 region: region,\u00a0 \u00a0 \u00a0 job: {\u00a0 \u00a0 \u00a0 \u00a0 placement: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 clusterName: clusterName,\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 pysparkJob: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mainPythonFileUri: jobFilePath,\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 };\u00a0 \u00a0 const [jobOperation] = await jobClient.submitJobAsOperation(job);\u00a0 \u00a0 const [jobResponse] = await jobOperation.promise();\u00a0 \u00a0 const matches =\u00a0 \u00a0 \u00a0 jobResponse.driverOutputResourceUri.match('gs://(.*?)/(.*)');\u00a0 \u00a0 const storage = new Storage();\u00a0 \u00a0 const output = await storage\u00a0 \u00a0 \u00a0 .bucket(matches[1])\u00a0 \u00a0 \u00a0 .file(`${matches[2]}.000000000`)\u00a0 \u00a0 \u00a0 .download();\u00a0 \u00a0 // Output a success message.\u00a0 \u00a0 console.log(`Job finished successfully: ${output}`);\u00a0 \u00a0 // Delete the cluster once the job has terminated.\u00a0 \u00a0 const deleteClusterReq = {\u00a0 \u00a0 \u00a0 projectId: projectId,\u00a0 \u00a0 \u00a0 region: region,\u00a0 \u00a0 \u00a0 clusterName: clusterName,\u00a0 \u00a0 };\u00a0 \u00a0 const [deleteOperation] =\u00a0 \u00a0 \u00a0 await clusterClient.deleteCluster(deleteClusterReq);\u00a0 \u00a0 await deleteOperation.promise();\u00a0 \u00a0 // Output a success message\u00a0 \u00a0 console.log(`Cluster ${clusterName} successfully deleted.`);\u00a0 }\u00a0 quickstart();}const args = process.argv.slice(2);if (args.length !== 4) {\u00a0 console.log(\u00a0 \u00a0 'Insufficient number of parameters provided. Please make sure a ' +\u00a0 \u00a0 \u00a0 'PROJECT_ID, REGION, CLUSTER_NAME and JOB_FILE_PATH are provided, in this order.'\u00a0 );}main(...args);\n```\n- [Install the client library](/dataproc/docs/reference/libraries#installing_the_client_library) For more information, See [Setting Up a Python Development Environment](/python/docs/setup) .\n- [Set up authentication](/dataproc/docs/reference/libraries#setting_up_authentication) \n- Clone and run the sample GitHub code. **Selecting a PySpark input file:** The Google Client Library code samples, below, run a PySpark job that you specify as an input parameter. You can pass in the simple \"Hello World\" PySpark app located in Cloud Storage at`gs://dataproc-examples/pyspark/hello-world/hello-world.py`or your own PySpark app. See [Uploading objects](/storage/docs/uploading-objects) to learn more about uploading files to Cloud Storage.\n- View the output. The code outputs the job driver log to the default Dataproc [staging bucket](/dataproc/docs/concepts/configuring-clusters/staging-bucket) in Cloud Storage. You can view job driver output from the Google Cloud console in your project's Dataproc [Jobs](https://console.cloud.google.com/project/_/dataproc/jobs) section. Click on the **Job ID** to view job output on the Job details page.\n [  dataproc/snippets/quickstart/quickstart.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataproc/snippets/quickstart/quickstart.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataproc/snippets/quickstart/quickstart.py) \n```\n\"\"\"This quickstart sample walks a user through creating a Cloud Dataproccluster, submitting a PySpark job from Google Cloud Storage to thecluster, reading the output of the job and deleting the cluster, allusing the Python client library.Usage:\u00a0 \u00a0 python quickstart.py --project_id <PROJECT_ID> --region <REGION> \\\u00a0 \u00a0 \u00a0 \u00a0 --cluster_name <CLUSTER_NAME> --job_file_path <GCS_JOB_FILE_PATH>\"\"\"import argparseimport refrom google.cloud import dataproc_v1 as dataprocfrom google.cloud import storagedef quickstart(project_id, region, cluster_name, job_file_path):\u00a0 \u00a0 # Create the cluster client.\u00a0 \u00a0 cluster_client = dataproc.ClusterControllerClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options={\"api_endpoint\": \"{}-dataproc.googleapis.com:443\".format(region)}\u00a0 \u00a0 )\u00a0 \u00a0 # Create the cluster config.\u00a0 \u00a0 cluster = {\u00a0 \u00a0 \u00a0 \u00a0 \"project_id\": project_id,\u00a0 \u00a0 \u00a0 \u00a0 \"cluster_name\": cluster_name,\u00a0 \u00a0 \u00a0 \u00a0 \"config\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"master_config\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"num_instances\": 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"machine_type_uri\": \"n1-standard-2\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"disk_config\": {\"boot_disk_size_gb\": 100},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"worker_config\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"num_instances\": 2,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"machine_type_uri\": \"n1-standard-2\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"disk_config\": {\"boot_disk_size_gb\": 100},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 }\u00a0 \u00a0 # Create the cluster.\u00a0 \u00a0 operation = cluster_client.create_cluster(\u00a0 \u00a0 \u00a0 \u00a0 request={\"project_id\": project_id, \"region\": region, \"cluster\": cluster}\u00a0 \u00a0 )\u00a0 \u00a0 result = operation.result()\u00a0 \u00a0 print(\"Cluster created successfully: {}\".format(result.cluster_name))\u00a0 \u00a0 # Create the job client.\u00a0 \u00a0 job_client = dataproc.JobControllerClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options={\"api_endpoint\": \"{}-dataproc.googleapis.com:443\".format(region)}\u00a0 \u00a0 )\u00a0 \u00a0 # Create the job config.\u00a0 \u00a0 job = {\u00a0 \u00a0 \u00a0 \u00a0 \"placement\": {\"cluster_name\": cluster_name},\u00a0 \u00a0 \u00a0 \u00a0 \"pyspark_job\": {\"main_python_file_uri\": job_file_path},\u00a0 \u00a0 }\u00a0 \u00a0 operation = job_client.submit_job_as_operation(\u00a0 \u00a0 \u00a0 \u00a0 request={\"project_id\": project_id, \"region\": region, \"job\": job}\u00a0 \u00a0 )\u00a0 \u00a0 response = operation.result()\u00a0 \u00a0 # Dataproc job output gets saved to the Google Cloud Storage bucket\u00a0 \u00a0 # allocated to the job. Use a regex to obtain the bucket and blob info.\u00a0 \u00a0 matches = re.match(\"gs://(.*?)/(.*)\", response.driver_output_resource_uri)\u00a0 \u00a0 output = (\u00a0 \u00a0 \u00a0 \u00a0 storage.Client()\u00a0 \u00a0 \u00a0 \u00a0 .get_bucket(matches.group(1))\u00a0 \u00a0 \u00a0 \u00a0 .blob(f\"{matches.group(2)}.000000000\")\u00a0 \u00a0 \u00a0 \u00a0 .download_as_bytes()\u00a0 \u00a0 \u00a0 \u00a0 .decode(\"utf-8\")\u00a0 \u00a0 )\u00a0 \u00a0 print(f\"Job finished successfully: {output}\")\u00a0 \u00a0 # Delete the cluster once the job has terminated.\u00a0 \u00a0 operation = cluster_client.delete_cluster(\u00a0 \u00a0 \u00a0 \u00a0 request={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"project_id\": project_id,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"region\": region,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"cluster_name\": cluster_name,\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 )\u00a0 \u00a0 operation.result()\u00a0 \u00a0 print(\"Cluster {} successfully deleted.\".format(cluster_name))if __name__ == \"__main__\":\u00a0 \u00a0 parser = argparse.ArgumentParser(\u00a0 \u00a0 \u00a0 \u00a0 description=__doc__,\u00a0 \u00a0 \u00a0 \u00a0 formatter_class=argparse.RawDescriptionHelpFormatter,\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--project_id\",\u00a0 \u00a0 \u00a0 \u00a0 type=str,\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"Project to use for creating resources.\",\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--region\",\u00a0 \u00a0 \u00a0 \u00a0 type=str,\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"Region where the resources should live.\",\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--cluster_name\",\u00a0 \u00a0 \u00a0 \u00a0 type=str,\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"Name to use for creating a cluster.\",\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\u00a0 \u00a0 \u00a0 \u00a0 \"--job_file_path\",\u00a0 \u00a0 \u00a0 \u00a0 type=str,\u00a0 \u00a0 \u00a0 \u00a0 required=True,\u00a0 \u00a0 \u00a0 \u00a0 help=\"Job in GCS to execute against the cluster.\",\u00a0 \u00a0 )\u00a0 \u00a0 args = parser.parse_args()\u00a0 \u00a0 quickstart(args.project_id, args.region, args.cluster_name, args.job_file_path)\n```\n## What's next\n- See Dataproc Cloud Client Library [Additional resources](/dataproc/docs/reference/libraries#additional_resources) .", "guide": "Dataproc"}