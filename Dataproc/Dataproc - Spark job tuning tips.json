{"title": "Dataproc - Spark job tuning tips", "url": "https://cloud.google.com/dataproc/docs/support/spark-job-tuning", "abstract": "# Dataproc - Spark job tuning tips\nThe following sections provide tips to help you fine tune your Dataproc Spark applications.\n", "content": "## Use ephemeral clusters\nWhen you use the Dataproc \"ephemeral\" cluster model, you create a dedicated cluster for each job, and when the job finishes, you delete the cluster. With the ephemeral model, you can treat storage and compute separately, saving job input and output data in Cloud Storage or BigQuery, using the cluster for compute and temporary data storage only.\n### Persistent cluster pitfalls\nUsing ephemeral one-job clusters avoids the following pitfalls and potential problems associated with using shared and long-running \"persistent\" clusters:\n- Single points of failure: a shared cluster error state can cause all jobs to fail, blocking an entire data pipeline. Investigating and recovering from an error can take hours. Since ephemeral clusters keep temporary in-cluster states only, when an error occurs, they can be quickly deleted and recreated.\n- Difficulty maintaining and migrating cluster states in HDFS, MySQL or local filesystems\n- Resource contentions among jobs that negatively affect SLOs\n- Unresponsive service daemons caused by memory pressure\n- Buildup of logs and temporary files that can exceed disk capacity\n- Upscaling failure due to cluster zone stockout\n- Lack of support for [outdated cluster image versions](/dataproc/docs/concepts/versioning/dataproc-version-clusters#unsupported_dataproc_versions) .\n### Ephemeral cluster benefits\nOn the positive side, ephemeral clusters let you do the following:\n- Configure different IAM permissions for different jobs with different [Dataproc VM service accounts](/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity) .\n- Optimize a cluster's hardware and software configurations for each job, changing cluster configurations as needed.\n- Upgrade image versions in new clusters to get the latest security patches, bug fixes, and optimizations.\n- Troubleshoot issues more quickly on an isolated, single-job cluster.\n- Save costs by paying for ephemeral cluster running time only, not for idle time between jobs on a shared cluster.## Use Spark SQL\nThe [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) DataFrame API is a significant optimization of the RDD API. If you interact with code that uses RDDs, consider reading data as a DataFrame before passing an RDD in the code. In Java or Scala code, consider using the [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) Dataset API as a superset of RDDs and DataFrames.\n## Use Apache Spark 3\n[Dataproc 2.0](/dataproc/docs/concepts/versioning/dataproc-release-2.0) installs Spark 3, which includes the following features and performance improvements:\n- GPU support\n- Ability to read binary files\n- Performance improvements\n- Dynamic Partition Pruning\n- [Adaptive query execution](https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution) , which optimizes Spark jobs in real time\nSpark 3 improvements primarily result from under-the-hood changes, and require minimal user code changes. For considerations when migrating from Spark 2 to Spark 3, see the [Apache Spark documentation](https://spark.apache.org/docs/latest/migration-guide.html) .\n## Use Dynamic Allocation\nApache Spark includes a **Dynamic Allocation** feature that scales the number of Spark executors on workers within a cluster. This feature allows a job to use the full Dataproc cluster even when the cluster scales up. This feature is enabled by default on Dataproc ( `spark.dynamicAllocation.enabled` is set to `true` ). See [Spark Dynamic Allocation](https://spark.apache.org/docs/latest/configuration.html#dynamic-allocation) for more information.\n## Use Dataproc Autoscaling\nDataproc [Autoscaling](/dataproc/docs/concepts/configuring-clusters/autoscaling) dynamically adds and removes Dataproc workers from a cluster to help ensure that Spark jobs have the resources needed to complete quickly.\nIt is a [best practice](/dataproc/docs/concepts/configuring-clusters/autoscaling#avoid_scaling_primary_workers) to configure the autoscaling policy to only scale [secondary workers](/dataproc/docs/concepts/compute/secondary-vms) .\n## Use Dataproc Enhanced Flexibility Mode\nClusters with preemptible VMs or an autoscaling policy may receive FetchFailed exceptions when workers are preempted or removed before they finish serving shuffle data to reducers. This exception can cause task retries and longer job completion times.\nRecommendation: Use Dataproc [Enhanced Flexibility Mode](/dataproc/docs/concepts/configuring-clusters/flex) , which does not store intermediate shuffle data on secondary workers, so that secondary workers can be safely preempted or scaled down.\n## Configure partitioning and shuffling\nSpark stores data in temporary partitions on the cluster. If your application groups or joins DataFrames, it shuffles the data into new partitions according to the grouping and [low-level configuration](#configuring_partitions) .\nData partitioning significantly impacts application performance: too few partitions limits job parallelism and cluster resource utilization; too many partitions slows down the job due to additional partition processing and shuffling.\n### Configuring partitions\nThe following properties govern the number and size of your partitions:\n- `spark.sql.files.maxPartitionBytes` : the maximum size of partitions when you read in data from Cloud Storage. The default is 128 MB, which is sufficiently large for most applications that process less than 100 TB.\n- `spark.sql.shuffle.partitions` : the number of partitions after performing a shuffle. The default is 200, which is appropriate for clusters with less than 100 vCPUs total. Recommendation: Set this to 3x the number of vCPUs in your cluster.\n- `spark.default.parallelism` : the number of partitions returned after performing RDD transformations that require shuffles, such as `join` , `reduceByKey` and `parallelize` . The default is the total number of vCPUs in your cluster. When using RDDs in Spark jobs, you can set this number to 3x your vCPUsWhen using [autoscaling](#use_dataproc_autoscaling) , set these numbers to be 3x the total number of cores of your largest cluster, as defined by the`maxInstances`in the worker configuration of your autoscaling policy.\n### Limit the number of files\nThere is a performance loss when Spark reads a large number small files. Store data in larger file sizes, for example, file sizes in the 256MB\u2013512MB range. Similarly, limit the number of output files (to force a shuffle, see [Avoid unnecessary shuffles](#avoid_unnecessary_shuffles) ).\n### Configure adaptive query execution (Spark 3)\n[Adaptive query execution](https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution) (enabled by default in Dataproc image version 2.0) provides Spark job performance improvements, including:\n- [Coalescing partitions after shuffles](https://spark.apache.org/docs/latest/sql-performance-tuning.html#coalescing-post-shuffle-partitions) \n- [Converting sort-merge joins to broadcast joins](https://spark.apache.org/docs/latest/sql-performance-tuning.html#converting-sort-merge-join-to-broadcast-join) \n- [Optimizations for skew joins](https://spark.apache.org/docs/latest/sql-performance-tuning.html#optimizing-skew-join) .\nAlthough the default configuration settings are sound for most use cases, setting `spark.sql.adaptive.advisoryPartitionSizeInBytes` to `spark.sqlfiles.maxPartitionBytes` (default 128 MB) can be beneficial.\n### Avoid unnecessary shuffles\nSpark allows users to manually trigger a shuffle to re-balance their data with the `repartition` function. Shuffles are expensive, so reshuffling data should be used cautiously. Setting the partition [configurations](#configuring_partitions) appropriately should be sufficient to allow Spark to automatically partition your data.\n**Exception:** When writing column-partitioned data to Cloud Storage, repartitioning on a specific column avoids writing many small files to achieve faster write times.\n```\ndf.repartition(\"col_name\").write().partitionBy(\"col_name\").save(\"gs://...\")\n```\n## Store data in Parquet or Avro\nSpark SQL defaults to reading and writing data in Snappy compressed [Parquet](https://parquet.apache.org/) files. Parquet is in efficient columnar file format that enables Spark to only read the data it needs to execute an application. This is an important advantage when working with large datasets. Other columnar formats, such as [Apache ORC](https://orc.apache.org/) , also perform well.\nFor non-columnar data, [Apache Avro](https://avro.apache.org/docs/current/) provides an efficient binary-row file format. Although typically slower than Parquet, Avro's performance is better than text based formats,such as CSV or JSON.\n## Optimize disk size\nPersistent disks throughput scales with disk size, which can affect the Spark job performance since jobs write metadata and shuffle data to disk. When using standard persistent disks, disk size should be at least 1 terabyte per worker (see [Performance by persistent disk size](/compute/docs/disks/performance#performance_by_disk_size) ).\nTo monitor worker disk throughput in the Google Cloud console:\n- Click the cluster name on the [Clusters](https://console.cloud.google.com/dataproc/clusters) page.\n- Click the VM INSTANCES tab.\n- Click on any worker name.\n- Click the MONITORING tab, then scroll down to Disk Throughput to view worker throughput.\nWhen scaling persistent disk size for optimal performance, disk size can be set significantly larger than the amount of data to be allocated to each worker in order to increase the throughput cap of the workers.\n### Disk considerations\nEphemeral Dataproc clusters, which don't benefit from persistent storage. can use [local SSDs](/dataproc/docs/concepts/compute/dataproc-local-ssds) . Local SSDs are physically attached to the cluster and provide higher throughput than persistent disks (see the [Performance table](/compute/docs/disks/local-ssd#performance) ). Local SSDs are available at a fixed size of 375 gigabytes, but you can add multiple SSDs to increase performance.\nLocal SSDs don't persist data after a cluster is shut down. If you need persistent storage, you can use SSD persistent disks, which provide [higher throughput](/compute/docs/disks/performance#ssd_persistent_disk) for their size than standard persistent disks. SSD persistent disks are also a good choice if partition size will be smaller than 8 KB (however, [avoid small paritiions](#configure_partitioning_and_shuffling) ).\n## Attach GPUs to your cluster\nSpark 3 supports GPUs. Use GPUs with the [RAPIDS initialization action](https://github.com/GoogleCloudDataproc/initialization-actions/tree/master/rapids) to speed up Spark jobs using the [RAPIDS SQL Accelerator](https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/apache-spark-3/) . The [GPU driver initialization action](https://github.com/GoogleCloudDataproc/initialization-actions/tree/master/gpu) to configure a cluster with GPUs.\n## Common job failures and fixes\n### Out of Memory\nExamples:\n- \"Lost executor\"\n- \"java.lang.OutOfMemoryError: GC overhead limit exceeded\"\n- \"Container killed by YARN for exceeding memory limits\"Possible fixes:\n- If using PySpark, raise`spark.executor.memoryOverhead`and lower`spark.executor.memory`.\n- Use [high memory](/compute/docs/machine-types#memory-optimized_machine_type_family) machine types.\n- [Use smaller partitions](#using_smaller_files_increased_partitioning) .### Shuffle Fetch Failures\nExamples:\n- \"FetchFailedException\" (Spark error)\n- \"Failed to connect to...\" (Spark error)\n- \"Failed to fetch\" (MapReduce error)Typically caused by by premature removal of workers which still have shuffle data to serve.\nPossible causes and fixes:\n- Preemptible worker VMs were reclaimed or non-preemptible worker VMs were  removed by the autoscaler. Solution: Use [Enhanced Flexibility Mode](/dataproc/docs/concepts/configuring-clusters/flex) to make secondary workers safely preemptible or scalable.\n- Executor or mapper crashed due to OutOfMemory error. Solution:  increase the memory of executor or mapper.\n- The Spark shuffle service may be overloaded. Solution:  decrease the number of job partitions.### YARN nodes are UNHEALTHY\nExamples (from YARN logs):\n```\n...reported UNHEALTHY with details: 1/1 local-dirs usable space is below\nconfigured utilization percentage/no more usable space\n[ /hadoop/yarn/nm-local-dir : used space above threshold of 90.0% ]\n```\nOften related to insufficient disk space for shuffle data. Diagnose by viewing log files:\n- Open your project's [Clusters](https://console.cloud.google.com/dataproc/clusters) page in the Google Cloud console, then click on the cluster's name.\n- Click VIEW LOGS.\n- Filter logs by`hadoop-yarn-nodemanager`.\n- Search for \"UNHEALTHY\".Possible Fixes:\n- The user cache is stored in the directory specified by the`yarn.nodemanager.local-dirs`property in the`yarn-site.xml file`. This file is located at`/etc/hadoop/conf/yarn-site.xml`. You can check the free space in the`/hadoop/yarn/nm-local-dir`path, and free up space by deleting the`/hadoop/yarn/nm-local-dir/usercache`user cache folder.\n- If the log reports \"UNHEALTHY\" status, recreate your cluster with larger disk space, which will [increase the throughput cap](#optimize_disk_size) .\n### Job fails due to insufficient driver memory\nWhen running jobs in cluster mode, the job fails if the memory size of the master node is significantly larger than the worker node memory size.\nExample from driver logs:\n```\n'Exception in thread \"main\" java.lang.IllegalArgumentException:\nRequired AM memory (32768+3276 MB) is above the max threshold (12288 MB) of this cluster!\nPlease check the values of 'yarn.scheduler.maximum -allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.'\n```\nPossible Fixes:\n- Set`spark:spark.driver.memory`less than`yarn:yarn.scheduler.maximum-allocation-mb`.\n- Use the same machine type for master and worker nodes.## For more information\n- See [Spark Performance Tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html)", "guide": "Dataproc"}