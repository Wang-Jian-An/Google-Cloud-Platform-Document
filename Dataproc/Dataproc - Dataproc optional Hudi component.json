{"title": "Dataproc - Dataproc optional Hudi component", "url": "https://cloud.google.com/dataproc/docs/concepts/components/hudi", "abstract": "# Dataproc - Dataproc optional Hudi component\nYou can install additional components like Hudi when you create a Dataproc cluster using the [Optional components](/dataproc/docs/concepts/components/overview#available_optional_components) feature. This page describes how you can optionally install the Hudi component on a Dataproc cluster.\nWhen installed on a Dataproc cluster, the [Apache Hudi](https://hudi.apache.org/docs/overview) component installs Hudi libraries and configures Spark and Hive in the cluster to work with Hudi.\n", "content": "## Compatible Dataproc image versions\nYou can install the Hudi component on Dataproc clusters created with the following Dataproc image versions:\n- [1.5.80+](/dataproc/docs/concepts/versioning/dataproc-release-1.5) \n- [2.0.54+](/dataproc/docs/concepts/versioning/dataproc-release-2.0) \n- [2.1.2+](/dataproc/docs/concepts/versioning/dataproc-release-2.1) ## Hudi related properties\nWhen you create a Dataproc with Hudi cluster, the following Spark and Hive properties are configured to work with Hudi.\n| Config file       | Property      | Default value                 |\n|:------------------------------------|:--------------------------------|:--------------------------------------------------------------------------------|\n| /etc/spark/conf/spark-defaults.conf | spark.serializer    | org.apache.spark.serializer.KryoSerializer          |\n| /etc/spark/conf/spark-defaults.conf | spark.sql.catalog.spark_catalog | org.apache.spark.sql.hudi.catalog.HoodieCatalog         |\n| /etc/spark/conf/spark-defaults.conf | spark.sql.extensions   | org.apache.spark.sql.hudi.HoodieSparkSessionExtension       |\n| /etc/spark/conf/spark-defaults.conf | spark.driver.extraClassPath  | /usr/lib/hudi/lib/hudi-sparkspark-version-bundle_scala-version-hudi-version.jar |\n| /etc/spark/conf/spark-defaults.conf | spark.executor.extraClassPath | /usr/lib/hudi/lib/hudi-sparkspark-version-bundle_scala-version-hudi-version.jar |\n| /etc/hive/conf/hive-site.xml  | hive.aux.jars.path    | file:///usr/lib/hudi/lib/hudi-hadoop-mr-bundle-version.jar      |\n## Install the component\nInstall the Hudi component when you create a Dataproc cluster.\nThe [Dataproc image release version pages](/dataproc/docs/concepts/versioning/dataproc-version-clusters#supported_dataproc_versions) list the Hudi component version included in each Dataproc image release.\n- Enable the component.- In the Google Cloud console, open the Dataproc [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page. The **Set up cluster** panel is selected.\n- In the **Components** section:- Under **Optional components** , select the **Hudi** component.\nTo create a Dataproc cluster that includes the Hudi component, use the command with the `--optional-components` flag.\n```\ngcloud dataproc clusters create CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=HUDI \\\n\u00a0\u00a0\u00a0\u00a0--image-version=DATAPROC_VERSION \\\n\u00a0\u00a0\u00a0\u00a0--properties=PROPERTIES\n```\nReplace the following:- : Required. The new cluster name.\n- : Required. The [cluster region](/compute/docs/regions-zones#available) .\n- : Optional. You can use this optional this flag to specify a non-default Dataproc image version (see [Default Dataproc image version](/dataproc/docs/concepts/versioning/dataproc-version-clusters#default_dataproc_image_version) ).\n- : Optional. You can use this optional this flag to set [Hudi component properties](https://hudi.apache.org/docs/basic_configurations) , which are specified with the [hudi: file-prefix](/dataproc/docs/concepts/configuring-clusters/cluster-properties#file-prefixed_properties_table) Example:`properties=hudi:hoodie.datasource.write.table.type=COPY_ON_WRITE`).\n- Hudi component version property: You can optionally specify the [dataproc:hudi.version property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#dataproc_service_properties_table) . **Note:** The Hudi component version is set by  Dataproc to be compatible with the Dataproc cluster image version. If  you set this property, cluster creation can fail if the specified version is not  compatible with the cluster image.\n- Spark and Hive properties: Dataproc sets [Hudi-related Spark and Hive](#hudi_related_properties) properties when the cluster is created. You do not need to set them  when creating the cluster or submitting jobs.The Hudi component can be installed through the Dataproc API using [SoftwareConfig.Component](/dataproc/docs/reference/rest/v1/ClusterConfig#Component) as part of a [clusters.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) request.\n## Submit a job to read and write Hudi tables\nAfter [creating a cluster with the Hudi component](#install_the_component) , you can submit Spark and Hive jobs that read and write Hudi tables.\n`gcloud CLI` example:\n```\ngcloud dataproc jobs submit pyspark \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--job-file=JOB_FILE \\\n\u00a0\u00a0\u00a0\u00a0-- JOB_ARGS\n```\n### Sample PySpark job\nThe following PySpark file creates, reads, and writes a Hudi table.\n[  codelabs/spark-hudi/pyspark_hudi_example.py ](https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/HEAD/codelabs/spark-hudi/pyspark_hudi_example.py) [View on GitHub](https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/HEAD/codelabs/spark-hudi/pyspark_hudi_example.py)\n```\n#!/usr/bin/env python\"\"\"Pyspark Hudi test.\"\"\"import sysfrom pyspark.sql import SparkSessiondef create_hudi_table(spark, table_name, table_uri):\u00a0 \"\"\"Creates Hudi table.\"\"\"\u00a0 create_table_sql = f\"\"\"\u00a0 \u00a0 CREATE TABLE IF NOT EXISTS {table_name} (\u00a0 \u00a0 \u00a0 uuid string,\u00a0 \u00a0 \u00a0 begin_lat double,\u00a0 \u00a0 \u00a0 begin_lon double,\u00a0 \u00a0 \u00a0 end_lat double,\u00a0 \u00a0 \u00a0 end_lon double,\u00a0 \u00a0 \u00a0 driver string,\u00a0 \u00a0 \u00a0 rider string,\u00a0 \u00a0 \u00a0 fare double,\u00a0 \u00a0 \u00a0 partitionpath string,\u00a0 \u00a0 \u00a0 ts long\u00a0 \u00a0 ) USING hudi\u00a0 \u00a0 LOCATION '{table_uri}'\u00a0 \u00a0 TBLPROPERTIES (\u00a0 \u00a0 \u00a0 type = 'cow',\u00a0 \u00a0 \u00a0 primaryKey = 'uuid',\u00a0 \u00a0 \u00a0 preCombineField = 'ts'\u00a0 \u00a0 )\u00a0 \u00a0 PARTITIONED BY (partitionpath)\u00a0 \"\"\"\u00a0 spark.sql(create_table_sql)def generate_test_dataframe(spark, n_rows):\u00a0 \"\"\"Generates test dataframe with Hudi's built-in data generator.\"\"\"\u00a0 sc = spark.sparkContext\u00a0 utils = sc._jvm.org.apache.hudi.QuickstartUtils\u00a0 data_generator = utils.DataGenerator()\u00a0 inserts = utils.convertToStringList(data_generator.generateInserts(n_rows))\u00a0 return spark.read.json(sc.parallelize(inserts, 2))def write_hudi_table(table_name, table_uri, df):\u00a0 \"\"\"Writes Hudi table.\"\"\"\u00a0 hudi_options = {\u00a0 \u00a0 \u00a0 'hoodie.table.name': table_name,\u00a0 \u00a0 \u00a0 'hoodie.datasource.write.recordkey.field': 'uuid',\u00a0 \u00a0 \u00a0 'hoodie.datasource.write.partitionpath.field': 'partitionpath',\u00a0 \u00a0 \u00a0 'hoodie.datasource.write.table.name': table_name,\u00a0 \u00a0 \u00a0 'hoodie.datasource.write.operation': 'upsert',\u00a0 \u00a0 \u00a0 'hoodie.datasource.write.precombine.field': 'ts',\u00a0 \u00a0 \u00a0 'hoodie.upsert.shuffle.parallelism': 2,\u00a0 \u00a0 \u00a0 'hoodie.insert.shuffle.parallelism': 2,\u00a0 }\u00a0 df.write.format('hudi').options(**hudi_options).mode('append').save(table_uri)def query_commit_history(spark, table_name, table_uri):\u00a0 tmp_table = f'{table_name}_commit_history'\u00a0 spark.read.format('hudi').load(table_uri).createOrReplaceTempView(tmp_table)\u00a0 query = f\"\"\"\u00a0 \u00a0 SELECT DISTINCT(_hoodie_commit_time)\u00a0 \u00a0 FROM {tmp_table}\u00a0 \u00a0 ORDER BY _hoodie_commit_time\u00a0 \u00a0 DESC\u00a0 \"\"\"\u00a0 return spark.sql(query)def read_hudi_table(spark, table_name, table_uri, commit_ts=''):\u00a0 \"\"\"Reads Hudi table at the given commit timestamp.\"\"\"\u00a0 if commit_ts:\u00a0 \u00a0 options = {'as.of.instant': commit_ts}\u00a0 else:\u00a0 \u00a0 options = {}\u00a0 tmp_table = f'{table_name}_snapshot'\u00a0 spark.read.format('hudi').options(**options).load(\u00a0 \u00a0 \u00a0 table_uri\u00a0 ).createOrReplaceTempView(tmp_table)\u00a0 query = f\"\"\"\u00a0 \u00a0 SELECT _hoodie_commit_time, begin_lat, begin_lon,\u00a0 \u00a0 \u00a0 \u00a0 driver, end_lat, end_lon, fare, partitionpath,\u00a0 \u00a0 \u00a0 \u00a0 rider, ts, uuid\u00a0 \u00a0 FROM {tmp_table}\u00a0 \"\"\"\u00a0 return spark.sql(query)def main():\u00a0 \"\"\"Test create write and read Hudi table.\"\"\"\u00a0 if len(sys.argv) != 3:\u00a0 \u00a0 raise Exception('Expected arguments: <table_name> <table_uri>')\u00a0 table_name = sys.argv[1]\u00a0 table_uri = sys.argv[2]\u00a0 app_name = f'pyspark-hudi-test_{table_name}'\u00a0 print(f'Creating Spark session {app_name} ...')\u00a0 spark = SparkSession.builder.appName(app_name).getOrCreate()\u00a0 spark.sparkContext.setLogLevel('WARN')\u00a0 print(f'Creating Hudi table {table_name} at {table_uri} ...')\u00a0 create_hudi_table(spark, table_name, table_uri)\u00a0 print('Generating test data batch 1...')\u00a0 n_rows1 = 10\u00a0 input_df1 = generate_test_dataframe(spark, n_rows1)\u00a0 input_df1.show(truncate=False)\u00a0 print('Writing Hudi table, batch 1 ...')\u00a0 write_hudi_table(table_name, table_uri, input_df1)\u00a0 print('Generating test data batch 2...')\u00a0 n_rows2 = 10\u00a0 input_df2 = generate_test_dataframe(spark, n_rows2)\u00a0 input_df2.show(truncate=False)\u00a0 print('Writing Hudi table, batch 2 ...')\u00a0 write_hudi_table(table_name, table_uri, input_df2)\u00a0 print('Querying commit history ...')\u00a0 commits_df = query_commit_history(spark, table_name, table_uri)\u00a0 commits_df.show(truncate=False)\u00a0 previous_commit_ts = commits_df.collect()[1]._hoodie_commit_time\u00a0 print('Reading the Hudi table snapshot at the latest commit ...')\u00a0 output_df1 = read_hudi_table(spark, table_name, table_uri)\u00a0 output_df1.show(truncate=False)\u00a0 print(f'Reading the Hudi table snapshot at {previous_commit_ts} ...')\u00a0 output_df2 = read_hudi_table(spark, table_name, table_uri, previous_commit_ts)\u00a0 output_df2.show(truncate=False)\u00a0 print('Stopping Spark session ...')\u00a0 spark.stop()\u00a0 print('All done')main()\n```\nThe following gcloud CLI command submits the sample PySpark file to Dataproc.\n```\ngcloud dataproc jobs submit pyspark \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0gs://BUCKET_NAME/pyspark_hudi_example.py \\\n\u00a0\u00a0\u00a0\u00a0-- TABLE_NAME gs://BUCKET_NAME/TABLE_NAME\n```\n## Use the Hudi CLI\nThe Hudi CLI is located at `/usr/lib/hudi/cli/hudi-cli.sh` on the Dataproc cluster master node. You can use the Hudi CLI to view Hudi table schemas, commits, and statistics, and to manually perform administrative operations, such as schedule compactions (see [Using hudi-cli](https://hudi.apache.org/docs/cli/#using-hudi-cli) ).\nTo start the Hudi CLI and connect to a Hudi table:\n- [SSH into the master node](/dataproc/docs/concepts/accessing/ssh) .\n- Run`/usr/lib/hudi/cli/hudi-cli.sh`. The command prompt changes to`hudi->`.\n- Run`connect --path gs://` `` `/` ``.\n- Run commands, such as`desc`, which describes the table schema, or`commits show`, which shows the commit history.\n- To stop the CLI session, run`exit`.## For more information\n- [Hudi Quick Start Guide](https://hudi.apache.org/docs/quick-start-guide/)", "guide": "Dataproc"}