{"title": "Dataproc - Use the Cloud Storage connector with Apache Spark", "url": "https://cloud.google.com/dataproc/docs/tutorials/gcs-connector-spark-tutorial", "abstract": "# Dataproc - Use the Cloud Storage connector with Apache Spark\nThis tutorial show you how to run example code that uses the\n [Cloud Storage connector](/dataproc/docs/concepts/connectors/cloud-storage#clusters) \nwith\n [Apache Spark](https://spark.apache.org) \n.\n", "content": "## ObjectivesWrite a simple wordcount Spark job in Java, Scala, or Python, then run the job on a Dataproc cluster.\nA Dataproc cluster is pre-installed with the Spark components needed for this tutorial.## CostsIn this document, you use the following billable components of Google Cloud:- Compute Engine\n- Dataproc\n- Cloud Storage\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . ## Before you beginRun the steps below to prepare to run the code in this tutorial.- **Set up your project.** If necessary, set up a project with the Dataproc, Compute Engine, and Cloud Storage APIs enabled and the Google Cloud CLI installed on your local machine.\n- **Create a Cloud Storage bucket.** You need a Cloud Storage to hold tutorial data. If you do not have one ready to use, create a new bucket in your project.- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets page](https://console.cloud.google.com/storage/browser) \n- Click **Create bucket** .\n- On the **Create a bucket** page, enter your bucket information. To go to the next step, click **Continue** .- For **Name your bucket** , enter a name that meets the [bucket naming requirements](/storage/docs/bucket-naming#requirements) .\n- For **Choose where to store your data** , do the following:- Select a **Location type** option.\n- Select a **Location** option.\n- For **Choose a default storage class for your data** , select a [storage class](/storage/docs/storage-classes) .\n- For **Choose how to control access to objects** , select an **Access control** option.\n- For **Advanced settings (optional)** , specify  an [encryption method](/storage/docs/encryption) ,  a [retention policy](/storage/docs/bucket-lock) ,  or [bucket labels](/storage/docs/tags-and-labels#bucket-labels) .\n- Click **Create** .\n- **Set local environment variables.** Set environment variables on your local machine. Set your Google Cloud project-id and the name of the Cloud Storage bucket you will use for this tutorial. Also provide the name and [region](/compute/docs/regions-zones#available) of an existing or new Dataproc cluster. You can create a cluster to use in this tutorial in the next step.```\nPROJECT=project-id\n``````\nBUCKET_NAME=bucket-name\n``````\nCLUSTER=cluster-name\n``````\nREGION=cluster-region Example: \"us-central1\"\n```\n- **Create a Dataproc cluster.** Run the command, below, to create a [single-node](/dataproc/docs/concepts/configuring-clusters/single-node-clusters) Dataproc cluster in the specified [Compute Engine zone](/compute/docs/regions-zones) .```\ngcloud dataproc clusters create ${CLUSTER} \\\n\u00a0\u00a0\u00a0\u00a0--project=${PROJECT} \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--single-node\n```The above command installs the default [cluster image version](/dataproc/docs/concepts/versioning/dataproc-versions#supported_cloud_dataproc_versions) . You can use the [--image-version](/sdk/gcloud/reference/dataproc/clusters/create#--image) flag to select an image version for your cluster. Each image version installs specific versions of Spark and Scala library components. If you [prepare the Spark wordcount job](#prepare_the_spark_wordcount_job) in Java or Scala, you will reference the Spark and Scala versions installed on your cluster when you prepare the job package.\n- **Copy public data to your Cloud Storage bucket.** Copy a public data Shakespeare text snippet into the `input` folder of your Cloud Storage bucket:```\ngsutil cp gs://pub/shakespeare/rose.txt \\\n\u00a0\u00a0\u00a0\u00a0gs://${BUCKET_NAME}/input/rose.txt\n```\n- **Set up a Java (Apache Maven),Scala (SBT), orPythondevelopment environment.** **Use Cloud Shell** . [Cloud Shell](https://cloud.google.com/shell/docs/features#tools) includes tools used in this tutorial, including Apache Maven, Python, and the`gcloud`and`gsutil`Google Cloud CLI command-line tools.\n## Prepare the Spark wordcount jobSelect a tab, below, to follow the steps to prepare a job package or file to submit to your cluster. You can prepare one of the following job types;- **Spark job in Java** using [Apache Maven](https://maven.apache.org/) to build a [JAR](https://en.wikipedia.org/wiki/JAR) package\n- **Spark job in Scala** using [SBT](https://www.scala-sbt.org/) to build a [JAR](https://en.wikipedia.org/wiki/JAR) package\n- **Spark job in Python (PySpark)** \n- **Copy pom.xml file to your local machine.** The following`pom.xml`file specifies Scala and Spark library  dependencies, which are given a`provided`scope to  indicate that the Dataproc cluster will provide these  libraries at runtime. The`pom.xml`file does not specify a  Cloud Storage dependency because the connector implements the standard  HDFS interface. When a Spark job accesses Cloud Storage cluster files  (files with URIs that start with`gs://`), the system  automatically uses the Cloud Storage connector to access the  files in Cloud Storage **Check your cluster image version.** Replace theplaceholders in the file to show the  Spark and Scala library versions used by your cluster's [image version](/dataproc/docs/concepts/versioning/dataproc-versions) .  Note that the`spark-core_`artifact number is the Scala`major.minor`version number.```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?><project xmlns=\"http://maven.apache.org/POM/4.0.0\"\u00a0 \u00a0 xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\u00a0 \u00a0 xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\u00a0 <modelVersion>4.0.0</modelVersion>\u00a0 <groupId>dataproc.codelab</groupId>\u00a0 <artifactId>word-count</artifactId>\u00a0 <version>1.0</version>\u00a0 <properties>\u00a0 \u00a0 <maven.compiler.source>1.8</maven.compiler.source>\u00a0 \u00a0 <maven.compiler.target>1.8</maven.compiler.target>\u00a0 </properties>\u00a0 <dependencies>\u00a0 \u00a0 <dependency>\u00a0 \u00a0 \u00a0 <groupId>org.scala-lang</groupId>\u00a0 \u00a0 \u00a0 <artifactId>scala-library</artifactId>\u00a0 \u00a0 \u00a0 <version>Scala version, for example, 2.11.8</version>\u00a0 \u00a0 \u00a0 <scope>provided</scope>\u00a0 \u00a0 </dependency>\u00a0 \u00a0 <dependency>\u00a0 \u00a0 \u00a0 <groupId>org.apache.spark</groupId>\u00a0 \u00a0 \u00a0 <artifactId>spark-core_Scala major.minor.version, for example, 2.11</artifactId>\u00a0 \u00a0 \u00a0 <version>Spark version, for example, 2.3.1</version>\u00a0 \u00a0 \u00a0 <scope>provided</scope>\u00a0 \u00a0 </dependency>\u00a0 </dependencies></project>\n```\n- **Copy the WordCount.java code listed, below,\n to your local machine.** - Create a set of directories with the path`src/main/java/dataproc/codelab`:```\nmkdir -p src/main/java/dataproc/codelab\n```\n- Copy`WordCount.java`to your local machine into`src/main/java/dataproc/codelab`:```\ncp WordCount.java src/main/java/dataproc/codelab\n```\nWordCount.java is a simple Spark job in Java that reads text files from  Cloud Storage, performs a word count, then writes the text file results  to Cloud Storage.```\npackage dataproc.codelab;import java.util.Arrays;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import scala.Tuple2;public class WordCount {\u00a0 public static void main(String[] args) {\u00a0 \u00a0 if (args.length != 2) {\u00a0 \u00a0 \u00a0 throw new IllegalArgumentException(\"Exactly 2 arguments are required: <inputUri> <outputUri>\");\u00a0 \u00a0 }\u00a0 \u00a0 String inputPath = args[0];\u00a0 \u00a0 String outputPath = args[1];\u00a0 \u00a0 JavaSparkContext sparkContext = new JavaSparkContext(new SparkConf().setAppName(\"Word Count\"));\u00a0 \u00a0 JavaRDD<String> lines = sparkContext.textFile(inputPath);\u00a0 \u00a0 JavaRDD<String> words = lines.flatMap(\u00a0 \u00a0 \u00a0 \u00a0 (String line) -> Arrays.asList(line.split(\" \")).iterator()\u00a0 \u00a0 );\u00a0 \u00a0 JavaPairRDD<String, Integer> wordCounts = words.mapToPair(\u00a0 \u00a0 \u00a0 \u00a0 (String word) -> new Tuple2<>(word, 1)\u00a0 \u00a0 ).reduceByKey(\u00a0 \u00a0 \u00a0 \u00a0 (Integer count1, Integer count2) -> count1 + count2\u00a0 \u00a0 );\u00a0 \u00a0 wordCounts.saveAsTextFile(outputPath);\u00a0 }}\n```\n- **Build the package.** ```\nmvn clean package\n```If the build is successful, a`target/word-count-1.0.jar`is created.\n- **Stage the package to Cloud Storage.** ```\ngsutil cp target/word-count-1.0.jar \\\n\u00a0\u00a0\u00a0\u00a0gs://${BUCKET_NAME}/java/word-count-1.0.jar\n```\n- **Copy build.sbt file to your local machine.** The following`build.sbt`file specifies Scala and Spark library  dependencies, which are given a`provided`scope to  indicate that the Dataproc cluster will provide these  libraries at runtime. The`build.sbt`file does not specify a  Cloud Storage dependency because the connector implements the standard  HDFS interface. When a Spark job accesses Cloud Storage cluster files  (files with URIs that start with`gs://`), the system  automatically uses the Cloud Storage connector to access the  files in Cloud Storage **Check your cluster image verison.** Replace theplaceholders in the file to show the  Spark and Scala library versions used by your cluster's [image version](/dataproc/docs/concepts/versioning/dataproc-versions) .```\nscalaVersion := \"Scala version, for example, 2.11.8\"name := \"word-count\"organization := \"dataproc.codelab\"version := \"1.0\"libraryDependencies ++= Seq(\u00a0 \"org.scala-lang\" % \"scala-library\" % scalaVersion.value % \"provided\",\u00a0 \"org.apache.spark\" %% \"spark-core\" % \"Spark version, for example, 2.3.1\" % \"provided\")\n```\n- **Copy word-count.scala to your local machine.** This is a simple Spark job in Java that reads text files from  Cloud Storage, performs a word count, then writes  the text file results to Cloud Storage.```\npackage dataproc.codelabimport org.apache.spark.SparkContextimport org.apache.spark.SparkConfobject WordCount {\u00a0 def main(args: Array[String]) {\u00a0 \u00a0 if (args.length != 2) {\u00a0 \u00a0 \u00a0 throw new IllegalArgumentException(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Exactly 2 arguments are required: <inputPath> <outputPath>\")\u00a0 \u00a0 }\u00a0 \u00a0 val inputPath = args(0)\u00a0 \u00a0 val outputPath = args(1)\u00a0 \u00a0 val sc = new SparkContext(new SparkConf().setAppName(\"Word Count\"))\u00a0 \u00a0 val lines = sc.textFile(inputPath)\u00a0 \u00a0 val words = lines.flatMap(line => line.split(\" \"))\u00a0 \u00a0 val wordCounts = words.map(word => (word, 1)).reduceByKey(_ + _)\u00a0 \u00a0 wordCounts.saveAsTextFile(outputPath)\u00a0 }}\n```\n- **Build the package.** ```\nsbt clean package\n```If the build is successful, a`target/scala-2.11/word-count_2.11-1.0.jar`is created.\n- **Stage the package to Cloud Storage.** ```\ngsutil cp target/scala-2.11/word-count_2.11-1.0.jar \\\n\u00a0\u00a0\u00a0\u00a0gs://${BUCKET_NAME}/scala/word-count_2.11-1.0.jar\n```\n- **Copy word-count.py to your local machine.** This is a simple Spark job in Python using PySpark that reads text files from  Cloud Storage, performs a word count, then writes  the text file results to Cloud Storage.```\n#!/usr/bin/env pythonimport pysparkimport sysif len(sys.argv) != 3:\u00a0 raise Exception(\"Exactly 2 arguments are required: <inputUri> <outputUri>\")inputUri=sys.argv[1]outputUri=sys.argv[2]sc = pyspark.SparkContext()lines = sc.textFile(sys.argv[1])words = lines.flatMap(lambda line: line.split())wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda count1, count2: count1 + count2)wordCounts.saveAsTextFile(sys.argv[2])\n```## Submit the jobRun the following `gcloud` command to submit the wordcount job to your Dataproc cluster.```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--cluster=${CLUSTER} \\\n\u00a0\u00a0\u00a0\u00a0--class=dataproc.codelab.WordCount \\\n\u00a0\u00a0\u00a0\u00a0--jars=gs://${BUCKET_NAME}/java/word-count-1.0.jar \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0-- gs://${BUCKET_NAME}/input/ gs://${BUCKET_NAME}/output/\n``````\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--cluster=${CLUSTER} \\\n\u00a0\u00a0\u00a0\u00a0--class=dataproc.codelab.WordCount \\\n\u00a0\u00a0\u00a0\u00a0--jars=gs://${BUCKET_NAME}/scala/word-count_2.11-1.0.jar \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0-- gs://${BUCKET_NAME}/input/ gs://${BUCKET_NAME}/output/\n``````\ngcloud dataproc jobs submit pyspark word-count.py \\\n\u00a0\u00a0\u00a0\u00a0--cluster=${CLUSTER} \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0-- gs://${BUCKET_NAME}/input/ gs://${BUCKET_NAME}/output/\n```\n## View the outputAfter the job finishes, run the following gcloud CLI `gsutil` command to view the wordcount output.\n```\ngsutil cat gs://${BUCKET_NAME}/output/*\n```\nThe wordcount output should be similar to the following:\n```\n(a,2)\n(call,1)\n(What's,1)\n(sweet.,1)\n(we,1)\n(as,1)\n(name?,1)\n(any,1)\n(other,1)\n(rose,1)\n(smell,1)\n(name,1)\n(would,1)\n(in,1)\n(which,1)\n(That,1)\n(By,1)\n```\n## Clean up\nAfter you finish the tutorial, you can clean up the resources that you created so that they stop using quota and incurring charges. The following sections describe how to delete or turn off these resources.\n### Deleting the project\nThe easiest way to eliminate billing is to delete the project that you created for the tutorial.\nTo delete the project:\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.### Deleting the Dataproc clusterInstead of deleting your project, you may wish to only [delete your cluster](/dataproc/docs/guides/manage-cluster#deleting_a_cluster) within the project.\n### Deleting the Cloud Storage bucket- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets](https://console.cloud.google.com/storage/browser) \n- Click the checkbox for the bucket that you want to delete.\n- To delete the bucket,  clickdelete **Delete** , and then follow the  instructions.\n- Delete the bucket:\n- ```\ngcloud storage buckets delete BUCKET_NAME\n```\n- **Important:** Your bucket must  be empty before you can delete it.## What's next\n- See [Spark job tuning tips](/dataproc/docs/support/spark-job-tuning)", "guide": "Dataproc"}