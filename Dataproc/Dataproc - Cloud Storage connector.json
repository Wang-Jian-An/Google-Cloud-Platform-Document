{"title": "Dataproc - Cloud Storage connector", "url": "https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage", "abstract": "# Dataproc - Cloud Storage connector\nThe [Cloud Storage](/storage) connector is an [open source Java library](https://github.com/GoogleCloudDataproc/hadoop-connectors/tree/master/gcs) that lets you run [Apache Hadoop](https://hadoop.apache.org) or [Apache Spark](https://spark.apache.org) jobs directly on data in Cloud Storage, and offers a number of benefits over choosing the Hadoop Distributed File System (HDFS).\n**Connector Support.** The Cloud Storage connector is supported by Google Cloud for use with Google Cloud products and use cases, and when used with Dataproc is supported at the same level as Dataproc (see [Get support](/dataproc/docs/support/getting-support) ).\n", "content": "## Benefits of the Cloud Storage connector\n- **Direct data access** \u2013 Store your data in Cloud Storage and access it directly. You do not need to transfer it into HDFS first.\n- **HDFS compatibility** \u2013 You can easily access your data in Cloud Storage using the`gs://`prefix instead of`hdfs://`.\n- **Interoperability** \u2013 Storing data in Cloud Storage enables seamless interoperability between Spark, Hadoop, and Google services.\n- **Data accessibility** \u2013 When you shut down a Hadoop cluster, unlike HDFS, you continue to have access to your data in Cloud Storage.\n- **High data availability** \u2013 Data stored in Cloud Storage is highly available and globally replicated without a loss of performance.\n- **No storage management overhead** \u2013 Unlike HDFS, Cloud Storage requires no routine maintenance, such as checking the file system, or upgrading or rolling back to a previous version of the file system.\n- **Quick startup** \u2013 In HDFS, a MapReduce job can't start until the`NameNode`is out of safe mode, a process that can take from a few seconds to many minutes depending on the size and state of your data. With Cloud Storage, you can start your job as soon as the task nodes start, which leads to significant cost savings over time.## Dataproc cluster Cloud Storage connector setup\nThe Cloud Storage connector is installed by default on all Dataproc cluster nodes in the `/usr/local/share/google/dataproc/lib/` directory.\nThe following sections describe steps you can take to set up the connector successfully on your Dataproc cluster.\n### Service account permissions\nWhen running the connector inside of Compute Engine VMs, including Dataproc clusters, the `google.cloud.auth.service.account.enable` property is set to `false` by default, which means you don't need to manually configure a service account for the connector; it gets service account credentials from the [VM metadata server](/compute/docs/metadata/overview) .\n**Note:** the Dataproc VM service account must have permission to access your Cloud Storage bucket (see [Dataproc VM Service Account (Data Plane identity)](/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity) .\n### Non-default connector versions\nThe default Cloud Storage connector versions used in the latest images installed on Dataproc clusters are listed in the cluster image version pages (see [Supported Dataproc versions](/dataproc/docs/concepts/versioning/dataproc-version-clusters#supported_dataproc_versions) ). If your application depends on a non-default connector version deployed on your cluster, you must either:\n- create a cluster with the`--metadata=GCS_CONNECTOR_VERSION=x.y.z`flag, which updates the connector used by applications running on the cluster to the specified connector version, or\n- include and [relocate](/dataproc/docs/concepts/connectors/%22https:/maven.apache.org/plugins/maven-shade-plugin/examples/class-relocation) the connector classes and connector dependencies for the version you are using into your application's jar. Relocation is necessary to avoid a conflict between the your deployed connector version and the default connector version installed on the Dataproc cluster (see the [Maven dependencies relocation example](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/a839bd8f237197f98808944ca52d03de96518314/gcs/pom.xml#L242-L300) ).## Non-Dataproc clusters\nYou can install and use the Cloud Storage connector on an Apache Hadoop/Spark cluster, for example, to move on-premises HDFS data to [Cloud Storage](/storage) .\n- Download the connector.To download the Cloud Storage connector for Hadoop:- latest version located in Cloud Storage bucket (not recommended for use in production):- [Cloud Storage connector for Hadoop 1.x](https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop1-latest.jar) \n- [Cloud Storage connector for Hadoop 2.x](https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar) \n- [Cloud Storage connector for Hadoop 3.x](https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar) \n- [specific version](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases) from Cloud Storage bucket by substituting Hadoop and Cloud Storage connector versions in the`gcs-connector-` `` `-` `` `.jar`name pattern:- Example:`gs://hadoop-lib/gcs/gcs-connector-hadoop2-2.1.1.jar`\n- [specific version](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases) from the [Apache Maven repository](https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/) (download a shaded jar that has`-shaded`suffix in the name).\n- Install the connector.See [Installing the connector](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/INSTALL.md) on GitHub to to install, configure, and test the Cloud Storage connector.## Use the connector\nThere are multiple ways to access data stored in Cloud Storage:\n- In a Spark (or PySpark) or Hadoop application using the`gs://`prefix.\n- The hadoop shell:`hadoop fs -ls gs://bucket/dir/file`.\n- The [Google Cloud console Cloud Storage browser](/storage/docs/gettingstarted-console) .\n- Using the [gsutil cp](/storage/docs/gsutil/commands/cp) or [gsutil rsync](/storage/docs/gsutil/commands/rsync) commands.\n### Resources\n- [Use the Cloud Storage connector with Apache Spark](/dataproc/docs/tutorials/gcs-connector-spark-tutorial) \n- [Apache Hadoop FileSystem API](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/filesystem/filesystem.html) \n### Java version\nThe Cloud Storage connector requires Java 8.\n### Apache Maven Dependency Information\n```\n<dependency>\n <groupId>com.google.cloud.bigdataoss</groupId>\n <artifactId>gcs-connector</artifactId>\n <version>insert \"hadoopX-X.X.X\" connector version number here</version>\n <scope>provided</scope>\n</dependency>\n```\nor for shaded version:\n```\n<dependency>\n <groupId>com.google.cloud.bigdataoss</groupId>\n <artifactId>gcs-connector</artifactId>\n <version>insert \"hadoopX-X.X.X\" connector version number here</version>\n <scope>provided</scope>\n <classifier>shaded</classifier>\n</dependency>\n```\nFor more detailed information, see the Cloud Storage connector [release notes](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases) and [Javadoc reference](http://www.javadoc.io/doc/com.google.cloud.bigdataoss/gcs-connector/) .\n## What's next\n- Learn more about [Cloud Storage](/storage)", "guide": "Dataproc"}