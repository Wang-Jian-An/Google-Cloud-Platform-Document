{"title": "Dataproc - Write and run Spark Scala jobs on Dataproc", "url": "https://cloud.google.com/dataproc/docs/tutorials/spark-scala", "abstract": "# Dataproc - Write and run Spark Scala jobs on Dataproc\nThis tutorial illustrates different ways to create and submit a Spark Scala job to a Dataproc cluster, including how to:\n- write and compile a Spark Scala \"Hello World\" app on a local machine from the command line using the [Scala REPL](https://docs.scala-lang.org/overviews/) (Read-Evaluate-Print-Loop or interactive interpreter) or the [SBT](http://www.scala-sbt.org/index.html) build tool\n- package compiled Scala classes into a jar file with a manifest\n- submit the Scala jar to a Spark job that runs on your Dataproc cluster\n- examine Scala job output from the Google Cloud console\nThis tutorial also shows you how to:\n- write and run a Spark Scala \"WordCount\" mapreduce job directly on a Dataproc cluster using the `spark-shell` REPL\n- run pre-installed Apache Spark and Hadoop examples on a cluster\nNote that although the command line examples in this tutorial assume a Linux terminal environment, many or most will also run as written in a macOS or Windows terminal window.\n", "content": "## Set up a Google Cloud Platform project\nIf you haven't already done so:\n- [Set up a project](/dataproc/docs/guides/setup-project) \n- [Create a Cloud Storage bucket](https://console.cloud.google.com/project/_/storage) \n- [Create a Dataproc cluster](/dataproc/docs/guides/create-cluster) ## Write and compile Scala code locally\nAs a simple exercise for this tutorial, write a \"Hello World\" Scala app using the [Scala REPL](http://docs.scala-lang.org/overviews/repl/overview.html) or the [SBT](http://www.scala-sbt.org/index.html) command line interface locally on your development machine.\n- [Use Scala](#use_scala) \n- [Use SBT](#use_sbt) \n### Use Scala\n- Download the Scala binaries from the [Scala Install](http://www.scala-lang.org/download/install.html) page\n- Unpack the file, set the `SCALA_HOME` environment variable, and add it to your path, as shown in the [Scala Install](http://www.scala-lang.org/download/install.html) instructions. For example:```\nexport SCALA_HOME=/usr/local/share/scala\nexport PATH=$PATH:$SCALA_HOME/\n```\n- Launch the Scala REPL```\n$ scala\nWelcome to Scala version ...\nType in expressions to have them evaluated.\nType :help for more information.\nscala>\n```\n- Copy and paste `HelloWorld` code into Scala REPL```\nobject HelloWorld {\u00a0 def main(args: Array[String]): Unit = {\u00a0 \u00a0 println(\"Hello, world!\")\u00a0 }}\n```\n- Save `HelloWorld.scala` and exit the REPL```\nscala> :save HelloWorld.scala\nscala> :q\n```\n- Compile with `scalac` ```\n$ scalac HelloWorld.scala\n```\n- List the compiled `.class` files```\n$ ls HelloWorld*.class\nHelloWorld$.class HelloWorld.class\n```\n### Use SBT\n- [Download SBT](http://www.scala-sbt.org/) \n- [Create a \"HelloWorld\" project](http://www.scala-sbt.org/0.12.4/docs/Getting-Started/Hello.html) , as shown below```\n$ mkdir hello\n$ cd hello\n$ echo \\\n'object HelloWorld {def main(args: Array[String]) = println(\"Hello, world!\")}' > \\\nHelloWorld.scala\n```\n- Create an `sbt.build` config file to set the `artifactName` (the name of the [jar file](https://docs.oracle.com/javase/tutorial/deployment/jar/basicsindex.html) that you will generate, below) to \"HelloWorld.jar\" (see [Modifying default artifacts](http://www.scala-sbt.org/0.12.2/docs/Detailed-Topics/Artifacts.html#modifying-default-artifacts) )```\necho \\\n'artifactName := { (sv: ScalaVersion, module: ModuleID, artifact: Artifact) =>\n\"HelloWorld.jar\" }' > \\\nbuild.sbt\n```\n- Launch SBT and run code```\n$ sbt\n[info] Set current project to hello ...\n> run\n... Compiling 1 Scala source to .../hello/target/scala-.../classes...\n... Running HelloWorld\nHello, world!\n[success] Total time: 3 s ...\n```\n- Package code into a [jar file](https://docs.oracle.com/javase/tutorial/deployment/jar/basicsindex.html) with a [manifest that specifies the main class entry point](https://docs.oracle.com/javase/tutorial/deployment/jar/appman.html) ( `HelloWorld` ), then exit```\n> package\n... Packaging .../hello/target/scala-.../HelloWorld.jar ...\n... Done packaging.\n[success] Total time: ...\n> exit\n```## Create a jar\nCreate a [jar file](https://docs.oracle.com/javase/tutorial/deployment/jar/basicsindex.html) with `SBT` or using the [jar](https://docs.oracle.com/javase/tutorial/deployment/jar/build.html) command.\n**Download Java?** To run the [jar](https://docs.oracle.com/javase/8/docs/technotes/tools/unix/jar.html) command, you must have Java SE (Standard Edition) JRE (Java Runtime Environment) installed on your machine\u2014see [Java SE Downloads](http://www.oracle.com/technetwork/java/javase/downloads/index.html) .### Create a jar with SBT\nThe SBT [package](http://www.scala-sbt.org/0.13/docs/Command-Line-Reference.html) command creates a jar file (see [Use SBT](#use_sbt) ).\n### Create a jar manually\n- Change directory (`cd`) into the directory that contains your compiled`HelloWorld*.class`files, then run the following command to package the class files into a jar with a [manifest that specifies the main class entry point](https://docs.oracle.com/javase/tutorial/deployment/jar/appman.html) (`HelloWorld`).```\n$ jar cvfe HelloWorld.jar HelloWorld HelloWorld*.class\nadded manifest\nadding: HelloWorld$.class(in = 637) (out= 403)(deflated 36%)\nadding: HelloWorld.class(in = 586) (out= 482)(deflated 17%)\n```Unpacking and examining the jar's manifest (`MANIFEST.MF`) shows that it lists the`HelloWorld`Main-Class entry point:```\nManifest-Version: ...\nCreated-By: ...\nMain-Class: HelloWorld\n```## Copy jar to Cloud Storage\n- Use the`gsutil`command to copy the jar to a Cloud Storage [bucket](/storage/docs/buckets) in your project\nWhen passing bucket names to`gsutil`, make sure to only specify the bucket name after the`gs://`prefix. For example, if a project contains a \"my-most-unique-bucket-name\" bucket, as shown in the following [Google Cloud console Storage Browser](https://console.cloud.google.com/storage/browser) screenshot,the following command will list the contents of that bucket:```\ngsutil ls gs://my-most-unique-bucket-name\n```\n```\n$ gsutil cp HelloWorld.jar gs://<bucket-name>/\nCopying file://HelloWorld.jar [Content-Type=application/java-archive]...\nUploading gs://bucket-name/HelloWorld.jar:   1.46 KiB/1.46 KiB\n```\n## Submit jar to a Dataproc Spark job\n- Use the [Google Cloud console](https://console.cloud.google.com/dataproc/jobs/jobsSubmit) to submit the jar file to your Dataproc Spark job. Fill in the fields on the **Submit a job** page as follows:- : Select your cluster's name from the cluster list\n- : Spark\n- : Specify the Cloud Storage URI path to your HelloWorld jar ( `gs://your-bucket-name/HelloWorld.jar` ).If your jar does not include a manifest that specifies the entry point to your code (\"Main-Class: HelloWorld\"), the \"Main class or jar\" field should state the name of your Main class (\"HelloWorld\"), and you should fill in the \"Jar files\" field with the URI path to your jar file ( `gs://your-bucket-name/HelloWorld.jar` ).\n- Click **Submit** to start the job. Once the job starts, it is added to the Jobs list.\n- Click the Job ID to open the **Jobs** page, where you can view the job's driver output.## Write and run Spark Scala code using the cluster's spark-shell REPL\nYou may want to develop Scala apps directly on your Dataproc cluster. Hadoop and Spark are pre-installed on Dataproc clusters, and they are configured with the Cloud Storage connector, which allows your code to read and write data directly from and to Cloud Storage.\nThis example shows you how to SSH into your project's Dataproc cluster master node, then use the [spark-shell](http://spark.apache.org/docs/latest/quick-start.html#interactive-analysis-with-the-spark-shell) REPL to create and run a Scala wordcount mapreduce application.\n- SSH into the Dataproc cluster's master node- Go to your project's Dataproc [Clusters](https://console.cloud.google.com/project/_/dataproc/clusters&tab=vm-instances) page in the Google Cloud console, then click on the name of your cluster.\n- On the cluster detail page, select the **VM Instances** tab, then click the SSH selection that appears at the right your cluster's name row.A browser window opens at your home directory on the master node\n- Launch the `spark-shell` ```\n$ spark-shell\n...\nUsing Scala version ...\nType in expressions to have them evaluated.\nType :help for more information.\n...\nSpark context available as sc.\n...\nSQL context available as sqlContext.\nscala>\n```\n- Create an [RDD](http://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds) (Resilient Distributed Dataset) from a Shakespeare text snippet located in public Cloud StorageShakespeare text snippet:```\nWhat's in a name? That which we call a rose\nBy any other name would smell as sweet.\n``````\nscala> val text_file = sc.textFile(\"gs://pub/shakespeare/rose.txt\")\n```\n- Run a wordcount mapreduce on the text, then display the `wordcounts` result```\nscala> val wordCounts = text_file.flatMap(line => line.split(\" \")).map(word =>\n(word, 1)).reduceByKey((a, b) => a + b)\nscala> wordCounts.collect\n... Array((call,1), (What's,1), (sweet.,1), (we,1), (as,1), (name?,1), (any,1), (other,1),\n(rose,1), (smell,1), (name,1), (a,2), (would,1), (in,1), (which,1), (That,1), (By,1))\n```\n- Save the counts in `<bucket-name>/wordcounts-out` in Cloud Storage, then exit the `scala-shell` ```\nscala> wordCounts.saveAsTextFile(\"gs://<bucket-name>/wordcounts-out/\")\nscala> exit\n```\n- Use `gsutil` to list the output files and display the file contents```\n$ gsutil ls gs://bucket-name/wordcounts-out/\ngs://spark-scala-demo-bucket/wordcounts-out/\ngs://spark-scala-demo-bucket/wordcounts-out/_SUCCESS\ngs://spark-scala-demo-bucket/wordcounts-out/part-00000\ngs://spark-scala-demo-bucket/wordcounts-out/part-00001\n```\n- Check `gs://<bucket-name>/wordcounts-out/part-00000` contents```\n$ gsutil cat gs://bucket-name/wordcounts-out/part-00000\n(call,1)\n(What's,1)\n(sweet.,1)\n(we,1)\n(as,1)\n(name?,1)\n(any,1)\n(other,1)\n```## Running Pre-Installed Example code\nThe Dataproc master node contains runnable jar files with standard Apache Hadoop and Spark examples.\n| Jar Type | Master node /usr/lib/ location     | GitHub Source | Apache Docs  |\n|:-----------|:-----------------------------------------------|:----------------|:-------------------|\n| Hadoop  | hadoop-mapreduce/hadoop-mapreduce-examples.jar | source link  | MapReduce Tutorial |\n| Spark  | spark/lib/spark-examples.jar     | source link  | Spark Examples  |\n### Submitting examples to your cluster from the command line\nExamples can be submitted from your local development machine using the Google Cloud CLI `gcloud` command-line tool (see [Using the Google Cloud console](/dataproc/docs/guides/submit-job#using_the_console_name) to submit jobs from the Google Cloud console).\n```\ngcloud dataproc jobs submit hadoop --cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--jars=file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0--class=org.apache.hadoop.examples.WordCount \\\n\u00a0\u00a0\u00a0\u00a0-- URI of input file URI of output file\n```\n```\ngcloud dataproc jobs submit spark --cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0--class=org.apache.spark.examples.JavaWordCount \\\n\u00a0\u00a0\u00a0\u00a0-- URI of input file\n```\n## Shutdown your cluster\nTo avoid ongoing charges, shutdown your cluster and delete the Cloud Storage resources (Cloud Storage bucket and files) used for this tutorial.\nTo shutdown a cluster:\n```\ngcloud dataproc clusters delete cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region\n```\nTo delete the Cloud Storage jar file:\n```\ngsutil rm gs://bucket-name/HelloWorld.jar\n```\nYou can delete a bucket and all of its folders and files with the following command:\n```\ngsutil rm -r gs://bucket-name/\n```\n## What's next\n- Read [Managing Java dependencies for Apache Spark applications on Dataproc](/dataproc/docs/guides/manage-spark-dependencies) .\n- See [Spark job tuning tips](/dataproc/docs/support/spark-job-tuning)", "guide": "Dataproc"}