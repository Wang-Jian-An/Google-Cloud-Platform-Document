{"title": "Dataproc - Dataproc Persistent History Server", "url": "https://cloud.google.com/dataproc/docs/concepts/jobs/history-server", "abstract": "# Dataproc - Dataproc Persistent History Server\n", "content": "## Overview\nThe Dataproc Persistent History Server (PHS) provides web interfaces to view job history for jobs run on active or deleted Dataproc clusters. It is available in Dataproc [image version 1.5](/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions) and later, and runs on a [single node Dataproc cluster](/dataproc/docs/concepts/configuring-clusters/single-node-clusters) . It provides [web interfaces](#component_gateway_web_interfaces) to the following files and data:\n- MapReduce and Spark job history files\n- Flink job history files (see [Dataproc optional Flink component](/dataproc/docs/concepts/components/flink) to create a Dataproc cluster to run Flink jobs)\n- Application Timeline data files created by [YARN Timeline Service v2](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html) and stored in a Bigtable instance.\n- YARN aggregation logsThe Persistent History Server accesses and displays Spark and MapReduce job history files, Flink job history files, and YARN log files written to Cloud Storage during the lifetime of Dataproc job clusters.\n### Limitations\n- A Dataproc PHS cluster lets you view job history files only of Dataproc jobs that ran in the project where the PHS cluster is located. Also, the PHS cluster image version and the Dataproc job cluster(s) image version must match. For example, you can use a Dataproc 2.0 image version PHS cluster to view job history files of jobs that ran on Dataproc 2.0 image version job clusters that were located in the project where the PHS cluster is located.\n- A PHS cluster does not support [Kerberos](/dataproc/docs/concepts/configuring-clusters/security) and [Personal Authentication](/dataproc/docs/concepts/iam/personal-auth) .\nYou can also view job history and data via [Dataproc Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) web interfaces running on a Dataproc cluster. These on-cluster job history files and web interfaces do not persist after the cluster is deleted.\n## Create a Dataproc PHS cluster\nYou can run the following [gcloud dataproc clusters create](/sdk/gcloud/reference/dataproc/clusters/create) command in a local terminal or in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) with the following flags and [cluster properties](/dataproc/docs/concepts/configuring-clusters/cluster-properties#dataproc_service_properties_table) to create a Dataproc Persistent History Server single-node cluster.\nFor information on different ways to create a Dataproc cluster, including using the Google Cloud console, the Cloud Client Libraries, and the Dataproc API, see [How to create a Dataproc cluster](/dataproc/docs/guides/create-cluster#creating_a_cloud_dataproc_cluster) .\n```\ngcloud dataproc clusters create CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--project=PROJECT \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--single-node \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=COMPONENT \\\n\u00a0\u00a0\u00a0\u00a0--properties=PROPERTIES\n```\n- : Specify the name of the PHS cluster.\n- : Specify the project to associate with the PHS cluster. This project should be the same as the project associated with the cluster that runs your jobs (see [Create a Dataproc job cluster](#create_a_job_cluster) ).\n- : Specify a [Compute Engine region](/compute/docs/regions-zones#available) where the PHS cluster will be located.\n- `--single-node`: A PHS cluster is a Dataproc [single node cluster](/dataproc/docs/concepts/configuring-clusters/single-node-clusters) .\n- `--enable-component-gateway`: This flag enables [Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) web interfaces on the PHS cluster.\n- : Use this flag to install one or more [optional components](/dataproc/docs/concepts/components/overview#available_optional_components) on the cluster. **You must specify the FLINK optional component to run the\nFlink HistoryServer Web Service on the PHS cluster to view\nFlink job history files.** \n- . Specify one or more [cluster properties](/dataproc/docs/concepts/configuring-clusters/cluster-properties) .\n- Optionally, add the [--image-version](/sdk/gcloud/reference/dataproc/clusters/create#--image-version) flag to specify the PHS cluster image version. The PHS image version must match the image version of the Dataproc job cluster(s). See [Limitations](#limitations) . **Notes:** - The property value examples in this section use a \"*\" wildcard character to allow the PHS to match multiple directories in the specified bucket written to by different job clusters (but see [Wildcard efficiency considerations](/storage/docs/wildcards#efficiency) ).\n- Separate`--properties`flags are shown in the following examples to aid readability. The recommended practice when using`gcloud dataproc clusters create`to create a Dataproc on Compute Engine cluster is to use one`--properties`flag to specify a list of comma-separated properties (see [cluster properties formatting](/dataproc/docs/concepts/configuring-clusters/cluster-properties#formatting) ).\n **Properties:** - `yarn:yarn.nodemanager.remote-app-log-dir=gs://` `` `/*/yarn-logs`: Add this property to specify the Cloud Storage location where the PHS will access YARN logs written by job clusters.\n- `spark:spark.history.fs.logDirectory=gs://` `` `/*/spark-job-history` : Add this property to enable persistent Spark job history. This property specifies the location where the PHS will access Spark job history logs written by job clusters.In Dataproc **2.0+** clusters, the following two properties must also be set to enable PHS Spark history logs (see [Spark History Server Configuration Options](https://spark.apache.org/docs/latest/monitoring.html#spark-history-server-configuration-options) ). The `spark.history.custom.executor.log.url` value is a literal value that contains {{ PLACEHOLDERS }} for variables that will be set by the Persistent History Server. These variables are not set by users; pass in the property value as shown.```\n--properties=spark:spark.history.custom.executor.log.url.applyIncompleteApplication=false\n``````\n--properties=spark:spark.history.custom.executor.log.url={{YARN_LOG_SERVER_URL}}/{{NM_HOST}}:{{NM_PORT}}/{{CONTAINER_ID}}/{{CONTAINER_ID}}/{{USER}}/{{FILE_NAME}}\n```\n- `mapred:mapreduce.jobhistory.read-only.dir-pattern=gs://` `` `/*/mapreduce-job-history/done` : Add this property to enable persistent MapReduce job history. This property specifies the Cloud Storage location where the PHS will access MapReduce job history logs written by job clusters.\n- `dataproc:yarn.atsv2.bigtable.instance=projects/` `` `/instance_id/` `` : After you [Configure Yarn Timeline Service v2](#configure_yarn_timeline_service_v2) , add this property to use the PHS cluster to view timeline data on the **YARN Application Timeline Service V2** and **Tez** web interfaces (see [Component Gateway web interfaces](#component_gateway_web_interfaces) ).\n- `flink:historyserver.archive.fs.dir=gs://` `` `/*/flink-job-history/completed-jobs` : Use this property to configure the Flink `HistoryServer` to monitor a comma-separated list of directories.\n **Properties examples:** ```\n--properties=spark:spark.history.fs.logDirectory=gs://bucket-name/*/spark-job-history\n``````\n--properties=mapred:mapreduce.jobhistory.read-only.dir-pattern=gs://bucket-name/*/mapreduce-job-history/done\n``````\n--properties=flink:flink.historyserver.archive.fs.dir=gs://bucket-name/*/flink-job-history/completed-jobs\n```## Create a Dataproc job cluster\nYou can run the following command in a local terminal or in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) to create a Dataproc job cluster that runs jobs and writes job history files to a Persistent History Server (PHS).\nFor information on different ways to install, configure, and run Flink on a Dataproc cluster, see [Dataproc optional Flink component](/dataproc/docs/concepts/components/flink) .\n```\ngcloud dataproc clusters create CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--project=PROJECT \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=COMPONENT \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway \\\n\u00a0\u00a0\u00a0\u00a0--properties=PROPERTIES \\\n\u00a0\u00a0\u00a0\u00a0other args ...\n```\n- : Specify the name of the job cluster.\n- : Specify the project associated with the job cluster.\n- : Specify the [Compute Engine region](/compute/docs/regions-zones#available) where the job cluster will be located.\n- `--enable-component-gateway`: This flag enables [Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) web interfaces on the job cluster.\n- : Use this flag to install one or more [optional components](/dataproc/docs/concepts/components/overview#available_optional_components) on the cluster. Specify the`FLINK`optional component to [run Flink jobs](/dataproc/docs/concepts/components/flink#run_flink_jobs) on the cluster.\n- : Add one or more of the following [cluster properties](/dataproc/docs/concepts/configuring-clusters/cluster-properties#dataproc_service_properties_table) to set PHS-related non-default Cloud Storage locations and other job cluster properties. **Notes:** - The property value examples in this section use a \"*\" wildcard character to allow the PHS to match multiple directories in the specified bucket written to by different job clusters (but see [Wildcard efficiency considerations](/storage/docs/wildcards#efficiency) ).\n- Separate`--properties`flags are shown in the following examples to aid readability. The recommended practice when using`gcloud dataproc clusters create`to create a Dataproc on Compute Engine cluster is to use one`--properties`flag to specify a list of comma-separated properties (see [cluster properties formatting](/dataproc/docs/concepts/configuring-clusters/cluster-properties#formatting) ).\n **Properties:** - `yarn:yarn.nodemanager.remote-app-log-dir`: By default, aggregated YARN logs are enabled on Dataproc job clusters and written to the cluster [temp bucket](/dataproc/docs/concepts/configuring-clusters/staging-bucket) . Add this property to specify a different Cloud Storage location where the cluster will write aggregation logs for access by the Persistent History Server.```\n--properties=yarn:yarn.nodemanager.remote-app-log-dir=gs://bucket-name/directory-name/yarn-logs\n```\n- `spark:spark.history.fs.logDirectory`and`spark:spark.eventLog.dir`: By default, Spark job history files are saved in the cluster [temp bucket](/dataproc/docs/concepts/configuring-clusters/staging-bucket) in the`/spark-job-history`directory. You can add these properties to specify different Cloud Storage locations for these files. If both properties are used, they must point to directories in the same bucket.```\n--properties=spark:spark.history.fs.logDirectory=gs://bucket-name/directory-name/spark-job-history\n``````\n--properties=spark:spark.eventLog.dir=gs://bucket-name/directory-name/spark-job-history\n```\n- `mapred:mapreduce.jobhistory.done-dir`and`mapred:mapreduce.jobhistory.intermediate-done-dir`: By default, MapReduce job history files are saved in the cluster [temp bucket](/dataproc/docs/concepts/configuring-clusters/staging-bucket) in the`/mapreduce-job-history/done`and`/mapreduce-job-history/intermediate-done`directories. The intermediate`mapreduce.jobhistory.intermediate-done-dir`location is temporary storage; intermediate files are moved to the`mapreduce.jobhistory.done-dir`location when the MapReduce job completes. You can add these properties to specify different Cloud Storage locations for these files. If both properties are used, they must point to directories in the same bucket.```\n--properties=mapred:mapreduce.jobhistory.done-dir=gs://bucket-name/directory-name/mapreduce-job-history/done\n``````\n--properties=mapred:mapreduce.jobhistory.intermediate-done-dir=gs://bucket-name/directory-name/mapreduce-job-history/intermediate-done\n```\n- `spark:spark.history.fs.gs.outputstream.type`and`spark:spark.history.fs.gs.outputstream.sync.min.interval.ms`: Add these [Cloud Storage connector](/dataproc/docs/concepts/connectors/cloud-storage) properties to change the default behaviour of how the job cluster sends data to Cloud Storage. The default`spark:spark.history.fs.gs.outputstream.type`is`BASIC`, which sends data to Cloud Storage after job completion. You can change this setting to`FLUSHABLE_COMPOSITE`to change flush behavior to copy data to Cloud Storage at regular intervals while the job is running.```\n--properties=spark:spark.history.fs.gs.outputstream.type=FLUSHABLE_COMPOSITE\n```The default`spark:spark.history.fs.gs.outputstream.sync.min.interval.ms`, which controls the frequency at which data is transferred to Cloud Storage, is`5000ms`, and can be changed to a different`ms`time interval:```\n--properties=spark:spark.history.fs.gs.outputstream.sync.min.interval.ms=intervalms\n``` **Note:** To set these properties, the Dataproc job cluster image version must use Cloud Storage connector version 2.2.0 or later. You can check the connector version installed on image versions from the [Dataproc image version list](/dataproc/docs/concepts/versioning/dataproc-versions#supported_dataproc_versions) page.\n- `dataproc:yarn.atsv2.bigtable.instance`: After you [Configure Yarn Timeline Service v2](#configure_yarn_timeline_service_v2) , add this property to write YARN timeline data to the specified Bigtable instance for viewing on the PHS cluster **YARN Application Timeline Service V2** and **Tez** web interfaces. Note: cluster creation will fail if the Bigtable instance does not exist.```\n--properties=dataproc:yarn.atsv2.bigtable.instance=projects/project-id/instance_id/bigtable-instance-id\n```\n- `flink:jobhistory.archive.fs.dir`: The Flink JobManager archives completed Flink jobs by uploading archived job information to a filesystem directory. Use this property to set the archive directory in`flink-conf.yaml`.```\n--properties=flink:jobmanager.archive.fs.dir=gs://bucket-name/job-cluster-1/flink-job-history/completed-jobs\n```\n## Use PHS with Spark batch workloads\nTo use the Persistent History Server with Dataproc Serverless for Spark batch workloads:\n- [Create a PHS cluster](#create_a_phs_cluster) .\n- Select or specify the PHS cluster when you [submit a Spark batch workload](/dataproc-serverless/docs/quickstarts/spark-batch#submit_a_spark_batch_workload) .## Use PHS with Dataproc on Google Kubernetes Engine\nTo use the Persistent History Server with [Dataproc on GKE](/dataproc/docs/guides/dpgke/dataproc-gke-overview) :\n- [Create a PHS cluster](#create_a_phs_cluster) .\n- Select or specify the PHS cluster when you [create a Dataproc on GKE virtual cluster](/dataproc/docs/guides/dpgke/quickstarts/dataproc-gke-quickstart-create-cluster#create_a_virtual_cluster) .## Component Gateway web interfaces\nIn the in the Google Cloud console, from the Dataproc [Clusters](https://console.cloud.google.com/datproc/clusters) page, click PHS cluster name to open the **Cluster details** page. Under the **Web Interfaces** tab, select the Component gateway links to open web interfaces running on the PHS cluster.\n**Note:** These component gateway links are also provided in the Google Cloud console for the job cluster during the lifetime of the job cluster (see [Viewing and Accessing Component Gateway URLs](/dataproc/docs/concepts/accessing/dataproc-gateways#viewing_and_accessing_component_gateway_urls) ).\n**Note:** The PHS cluster **YARN ResourceManager** and **HDFS NameNode** web interfaces do not access and display data.\n### Spark History Server web interface\nThe following screenshot shows the Spark History Server web interface displaying links to Spark jobs run on job-cluster-1 and job-cluster-2 after setting up the job clusters' `spark.history.fs.logDirectory` and `spark:spark.eventLog.dir` and PHS cluster's `spark.history.fs.logDirectory` locations as follows:\n| 0    | 1                 |\n|:--------------|:------------------------------------------------------------------|\n| job-cluster-1 | gs://example-cloud-storage-bucket/job-cluster-1/spark-job-history |\n| job-cluster-2 | gs://example-cloud-storage-bucket/job-cluster-2/spark-job-history |\n| phs-cluster | gs://example-cloud-storage-bucket/*/spark-job-history    |\nYou can list jobs by App Name in the **Spark History Server** web interface by entering an app name in the search box. The app name can be set in one of the following ways (listed by priority):\n- Set inside the application code when creating the spark context\n- Set by the [spark.app.name](https://spark.apache.org/docs/latest/configuration.html#available-properties) property when the job is submitted\n- Set by Dataproc to the full REST resource name for job (`projects/` `` `/regions/` `` `job-id`)\nUsers can input an app or resource name term in the **Search** box to find and list jobs.\nThe Spark History Server web interface provides an **Event Log** button you can click to download Spark event logs. These logs are useful for examining the lifecycle of the Spark application.\nSpark applications are broken down into multiple jobs, which are further broken down into multiple stages. Each stage can have multiple tasks, which are run on executor nodes (workers).- Click a Spark App ID in the web interface to open the Spark Jobs page, which provides an event timeline and summary of jobs within the application.\n- Click a job to open a Job Details page with a Directed Acyclic Graph (DAG) and summary of job stages.\n- Click a stage or use the Stages tab to select a stage to open the Stage Details page.Stage Details includes a DAG visualization, an event timeline, and metrics for the tasks within the stage. You can use this page to troubleshoot issues related to strangled tasks, scheduler delays, and out of memory errors. The DAG visualizer shows the line of code from which the stage is derived, helping you track issues back to the code.\n- Click the Executors tab for information about the Spark application's driver and executor nodes.Important pieces of information on this page include the number of cores and the number of tasks that were run on each executor.If an application runs slowly, look at the size of the shuffle reads and writes; a large amount of data can slow performance.\n### Tez web interface\nTez is the default execution engine for Hive and Pig on Dataproc. Submitting a Hive job on a Dataproc job cluster launches a Tez application (see [Using Apache Hive on Dataproc ](https://cloud.google.com/architecture/using-apache-hive-on-cloud-dataproc) ).\nIf you [configured Yarn Timeline Service v2](#configure_yarn_timeline_service_v2) and set the `dataproc:yarn.atsv2.bigtable.instance` property when you created the PHS and Dataproc job clusters, YARN writes generated Hive and Pig job timeline data to the specified Bigtable instance for retrieval and display on the Tez web interface running on the PHS server.\n**Note:** The **Hive Queries** tab is not supported in the Tez web interface running on the PHS cluster.### YARN Application Timeline V2 web interface\nIf you [configured Yarn Timeline Service v2](#configure_yarn_timeline_service_v2) and set the `dataproc:yarn.atsv2.bigtable.instance` property when you created the PHS and Dataproc job clusters, YARN writes generated job timeline data to the specified Bigtable instance for retrieval and display on the YARN Application Timeline Service web interface running on the PHS server. Dataproc jobs are listed under the **Flow Activity** tab in web interface.\n## Configure Yarn Timeline Service v2\nTo configure Yarn Timeline Service v2, set up a Bigtable instance and, in needed, check service account roles, as follows:\n- [Create a Bigtable instance](/bigtable/docs/creating-instance) .\n- Check service account roles, if needed. The default [VM service account](/dataproc/docs/concepts/configuring-clusters/service-accounts#VM_service_account) used by Dataproc cluster VMs has the permissions needed to create and configure the Bigtable instance for the YARN Timeline Service. If you create your job or PHS cluster with a [custom VM Service account](/dataproc/docs/concepts/configuring-clusters/service-accounts#create_a_cluster_with_a_custom_vm_service_account) , the account must have either the [Bigtable Administrator or Bigtable User role](/bigtable/docs/access-control#roles) .\n### Required table schema\nDataproc PHS support for [YARN Timeline Service v2](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/TimelineServiceV2.html) requires a specific schema created in the Bigtable instance. **Dataproc creates the requiredschema** when a job cluster or PHS cluster is created with the `dataproc:yarn.atsv2.bigtable.instance` property set to point to the Bigtable instance.\n**Note:** If the required bigtable instance schema exists when the `dataproc:yarn.atsv2.bigtable.instance` property is submitted to Dataproc, Dataproc will not create the schema.\nThe following is the required Bigtable instance schema:\n| Tables        | Column families |\n|:------------------------------------|:------------------|\n| prod.timelineservice.application | c,i,m    |\n| prod.timelineservice.app_flow  | m     |\n| prod.timelineservice.entity   | c,i,m    |\n| prod.timelineservice.flowactivity | i     |\n| prod.timelineservice.flowrun  | i     |\n| prod.timelineservice.subapplication | c,i,m    |\n### Bigtable garbage collection\nYou can configure age-based [Bigtable Garbage Collection](https://cloud.google.com/bigtable/docs/garbage-collection) for ATSv2 tables:\n- Install [cbt](https://cloud.google.com/bigtable/docs/cbt-overview) , (including the creation of the `.cbrtc file` ).\n- Create the ATSv2 age-based garbage collection policy:\n```\nexport NUMBER_OF_DAYS = number \\\ncbt setgcpolicy prod.timelineservice.application c maxage=${NUMBER_OF_DAYS} \\\ncbt setgcpolicy prod.timelineservice.application i maxage=${NUMBER_OF_DAYS} \\\ncbt setgcpolicy prod.timelineservice.application m maxage=${NUMBER_OF_DAYS} \\\ncbt setgcpolicy prod.timelineservice.app_flow m maxage=${NUMBER_OF_DAYS} \\\ncbt setgcpolicy prod.timelineservice.entity c maxage=${NUMBER_OF_DAYS} \\\ncbt setgcpolicy prod.timelineservice.entity i maxage=${NUMBER_OF_DAYS} \\\ncbt setgcpolicy prod.timelineservice.entity m maxage=${NUMBER_OF_DAYS} \\\ncbt setgcpolicy prod.timelineservice.flowactivity i maxage=${NUMBER_OF_DAYS} \\\ncbt setgcpolicy prod.timelineservice.flowrun i maxage=${NUMBER_OF_DAYS} \\\ncbt setgcpolicy prod.timelineservice.subapplication c maxage=${NUMBER_OF_DAYS} \\\ncbt setgcpolicy prod.timelineservice.subapplication i maxage=${NUMBER_OF_DAYS} \\\ncbt setgcpolicy prod.timelineservice.subapplication m maxage=${NUMBER_OF_DAYS}\n```\nNotes:\n: Maximum number of days is `30d` .", "guide": "Dataproc"}