{"title": "Dataproc - Initialization actions", "url": "https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/init-actions", "abstract": "# Dataproc - Initialization actions\nWhen [creating a Dataproc cluster](/dataproc/docs/guides/create-cluster) , you can specify initialization actions in executables or scripts that Dataproc will run on all nodes in your Dataproc cluster immediately after the cluster is set up. Initialization actions often set up job dependencies, such as installing Python packages, so that jobs can be submitted to the cluster without having to install dependencies when the jobs are run.\nYou can find sample initialization action scripts at the following locations: Note: **Google does not support these samples** .\n- [GitHub repository](https://github.com/GoogleCloudDataproc/initialization-actions) \n- [Cloud Storage](/storage) \u2014in the regional`gs://goog-dataproc-initialization-actions-` ``public buckets", "content": "## Important considerations and guidelines\n- Don't create production clusters that reference initialization actions located in the `gs://goog-dataproc-initialization-actions-` `` public buckets. These scripts are provided as reference implementations. They are synchronized with ongoing GitHub repository changes, and updates to these scripts can break your cluster creation. Instead, copy the initialization action from the public bucket into a versioned Cloud Storage bucket folder, as shown in the following example:```\nREGION=COMPUTE_REGION\ngsutil cp gs://goog-dataproc-initialization-actions-${REGION}/cloud-sql-proxy/cloud-sql-proxy.sh \\\n\u00a0\u00a0\u00a0\u00a0gs://my-bucket/cloud-sql-proxy/v1.0/cloud-sql-proxy.sh\n```Then, create the cluster by referencing the copy in Cloud Storage:```\ngcloud dataproc clusters create CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--initialization-actions=gs://my-bucket/cloud-sql-proxy/v1.0/cloud-sql-proxy.sh \\\n\u00a0\u00a0\u00a0\u00a0...other flags...\n```\n- Initialization actions are executed on each node in series during cluster creation. They are also executed on each added node when [scaling](/dataproc/docs/concepts/configuring-clusters/scaling-clusters) or [autoscaling](/dataproc/docs/concepts/configuring-clusters/autoscaling) clusters up.\n- When you update initialization actions\u2014for example, when you sync your Cloud Storage initialization actions to changes made to public bucket or GitHub repository initialization actions\u2014create a new (preferably version-named) folder to receive the updated initialization actions. If, instead, you update the initialization action in place, new nodes, such as those added by the autoscaler, will run the updated-in-place initialization action, not the prior-version initialization action that ran on existing nodes. Such initialization action differences can result in inconsistent or broken cluster nodes.\n- Initialization actions run as the `root` user. You **do not** need to use `sudo` .\n- Use **absolute paths** in initialization actions.\n- Use a [shebang line](https://goo.gl/IcHZya) in initialization actions to indicate how the script should be interpreted (such as `#!/bin/bash` or `#!/usr/bin/python` ).\n- If an initialization action terminates with a non-zero exit code, the cluster create operation will report an \"ERROR\" status. To debug the initialization action, use SSH to connect into the cluster's VM instances, and then examine the [logs](#init_actions_logging) . After fixing the initialization action problem, you can delete, then re-create the cluster.\n- If you create a Dataproc cluster with [internal IP addresses only](/dataproc/docs/concepts/configuring-clusters/network#create_a_cloud_dataproc_cluster_with_internal_ip_address_only) , attempts to access `github.com` over the internet in an initialization action will fail unless you have configured routes to direct the traffic through [Cloud NAT](/nat/docs) or a [Cloud VPN](/network-connectivity/docs/vpn) . Without access to the internet, you can enable [Private Google Access](/vpc/docs/private-google-access) and place job dependencies in [Cloud Storage](/storage) ; cluster nodes can download the dependencies from Cloud Storage from internal IPs.\n- You can use [Dataproc custom images](/dataproc/docs/guides/dataproc-images) instead of initialization actions to set up job dependencies.\n- **Initialization processing:** - Pre-2.0 image clusters:- : To allow initialization actions run on masters to write files to HDFS, master node initialization actions do not start until HDFS is writeable (until HDFS has exited safemode and at least two HDFS DataNodes have joined). [](None) \n- : If you set the`dataproc:dataproc.worker.custom.init.actions.mode` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) to`RUN_BEFORE_SERVICES`, each worker runs its initialization actions before it starts its HDFS datanode and YARN nodemanager daemons. Since Dataproc does not run master initialization actions until HDFS is writeable, which requires 2 HDFS datanode daemons to be running, setting this property may increase cluster creation time.\n- 2.0+ image clusters:- : Master node initialization actions may run before HDFS is writeable. If you run initialization actions that stage files in HDFS or depend on the availability of HDFS-dependent services, such as Ranger, set the`dataproc.master.custom.init.actions.mode` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) to`RUN_AFTER_SERVICES`. Note: since this property setting can increase cluster creation time\u2014see the explanation for cluster creation delay for [pre-2.0 image cluster workers](#cluster-delay) \u2014use it only when necessary (as a general practice, rely on the default`RUN_BEFORE_SERVICES`setting for this property).\n- : The`dataproc:dataproc.worker.custom.init.actions.mode` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) is set to`RUN_BEFORE_SERVICES`and **cannot** be passed to the cluster when the cluster is created (you cannot change the property setting). Each worker runs its initialization actions before it starts its HDFS datanode and YARN nodemanager daemons. Since Dataproc does not wait for HDFS to be writeable before running master initialization actions, master and worker initialization actions run in parallel.\n- Recommendations:- Use metadata to determine a node's role to conditionally execute an initialization action on nodes (see [Using cluster metadata](https://github.com/GoogleCloudDataproc/initialization-actions#using-cluster-metadata) ).\n- Fork a copy of an initialization action to a Cloud Storage bucket for stability (see [How initialization actionsare used](https://github.com/GoogleCloudDataproc/initialization-actions#how-initialization-actions-are-used) ).\n- Add retries when you download from the internet to help stabilize the initialization action.## Using initialization actions\nCluster initialization actions can be specified regardless of how you create a cluster:\n- Through the Google Cloud console\n- Using the [gcloud CLI](/sdk/gcloud) \n- Programmatically with the Dataproc [clusters.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) API (see [NodeInitializationAction](/dataproc/docs/reference/rest/v1/ClusterConfig#NodeInitializationAction) )\nWhen creating a cluster with the [gcloud dataproc clusters create](/sdk/gcloud/reference/dataproc/clusters/create) command, specify one or more comma-separated Cloud Storage locations (URIs) of the initialization executables or scripts with the `--initialization-actions` flag. **Note:** Multiple consecutive \"/\"s in a Cloud Storage location URI after the initial \"gs://\", such as \"gs:// /my//object//name\", are not supported. Run `gcloud dataproc clusters create --help` for command information.\n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--initialization-actions=Cloud Storage URI(s) (gs://bucket/...) \\\n\u00a0\u00a0\u00a0\u00a0--initialization-action-timeout=timeout-value (default=10m) \\\n\u00a0\u00a0\u00a0\u00a0... other flags ...\n```\nNotes:\n- Use the`--initialization-action-timeout`flag to specify a timeout period for the initialization action. The default timeout value is 10 minutes. If the initialization executable or script has not completed by the end of the timeout period, Dataproc cancels the initialization action.\n- Use the`dataproc:dataproc.worker.custom.init.actions.mode` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) to run the initialization action on primary workersthe node manager and datanode daemons are started.\n **Let the Google Cloud console construct your cluster create\nrequest** . You can click the **Equivalent REST or command line** links at the bottom of the left panel of the Dataproc [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page to have the Google Cloud console construct an equivalent API REST request or gcloud tool command (Note: the Google Cloud console doesn't include the REST API`executionTimeout`field or the Google Cloud CLI`--initialization-action-timeout`flag).Specify one or more scripts or executables in a [ClusterConfig.initializationActions](/dataproc/docs/reference/rest/v1/ClusterConfig) array as part a [clusters.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) API request.\n **Example** \n```\nPOST /v1/projects/my-project-id/regions/us-central1/clusters/\n{\n \"projectId\": \"my-project-id\",\n \"clusterName\": \"example-cluster\",\n \"config\": {\n \"configBucket\": \"\",\n \"gceClusterConfig\": {\n  \"subnetworkUri\": \"default\",\n  \"zoneUri\": \"us-central1-b\"\n },\n \"masterConfig\": {\n  \"numInstances\": 1,\n  \"machineTypeUri\": \"n1-standard-4\",\n  \"diskConfig\": {\n  \"bootDiskSizeGb\": 500,\n  \"numLocalSsds\": 0\n  }\n },\n \"workerConfig\": {\n  \"numInstances\": 2,\n  \"machineTypeUri\": \"n1-standard-4\",\n  \"diskConfig\": {\n  \"bootDiskSizeGb\": 500,\n  \"numLocalSsds\": 0\n  }\n },\n \"initializationActions\": [  {\n  \"executableFile\": \"gs://cloud-example-bucket/my-init-action.sh\"\n  }\n ]\n }\n}\n```\n **Let the Google Cloud console construct your cluster create\nrequest.** : You can click the **Equivalent REST API or command line** links at the bottom of the left panel of the Dataproc [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page to have the Google Cloud console construct an equivalent API REST request or gcloud tool command (Note: the Google Cloud console doesn't include the REST`executionTimeout`field or the Google Cloud CLI`--initialization-action-timeout`flag).Open the Dataproc [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page, then select the **Customize cluster** panel.\nIn the Initialization actions section, enter the Cloud Storage   bucket location of each initialization action in **Executable file** fields. Click **Browse** to open the Google Cloud console Cloud Storage Browser   page to select a script or executable file. Click **Add Initialization Action** to add each file.\n### Passing arguments to initialization actions\nDataproc sets special [metadata](/dataproc/docs/concepts/configuring-clusters/metadata) values for the instances that run in your clusters. You can set your own [custom metadata](/compute/docs/storing-retrieving-metadata#custom) as a way to pass arguments to initialization actions.\n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--initialization-actions=Cloud Storage URI(s) (gs://bucket/...) \\\n\u00a0\u00a0\u00a0\u00a0--metadata=name1=value1,name2=value2... \\\n\u00a0\u00a0\u00a0\u00a0... other flags ...\n```\nMetadata values can be read within initialization actions as follows:\n```\nvar1=$(/usr/share/google/get_metadata_value attributes/name1)\n```\n### Node selection\nIf you want to limit initialization actions to master, driver or worker nodes, you can add simple node-selection logic to your executable or script.\n```\nROLE=$(/usr/share/google/get_metadata_value attributes/dataproc-role)\nif [[ \"${ROLE}\" == 'Master' ]]; then\n ... master specific actions ...\nelse if [[ \"${ROLE}\" == 'Driver' ]]; then\n ... driver specific actions ...\nelse\n ... worker specific actions ...\nfi\n```\n### Staging binaries\nA common cluster initialization scenario is the staging of job binaries on a cluster to eliminate the need to stage the binaries each time a job is submitted. For example, assume that the following initialization script is stored in `gs://my-bucket/download-job-jar.sh` , a Cloud Storage bucket location:\n```\n#!/bin/bash\nROLE=$(/usr/share/google/get_metadata_value attributes/dataproc-role)\nif [[ \"${ROLE}\" == 'Master' ]]; then\n gsutil cp gs://my-bucket/jobs/sessionalize-logs-1.0.jar home/username\nfi\n```\nThe location of this script can be passed to the `gcloud dataproc clusters create` command:\n```\ngcloud dataproc clusters create my-dataproc-cluster \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--initialization-actions=gs://my-bucket/download-job-jar.sh\n```\nDataproc will run this script on all nodes, and, as a consequence of the script's node-selection logic, will download the jar to the master node. Submitted jobs can then use the pre-staged jar:\n```\ngcloud dataproc jobs submit hadoop \\\n\u00a0\u00a0\u00a0\u00a0--cluster=my-dataproc-cluster \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--jar=file:///home/username/sessionalize-logs-1.0.jar\n```\n### Initialization actions samples\nFrequently used and other sample initialization actions scripts are located in `gs://goog-dataproc-initialization-actions-<REGION>` , a regional public Cloud Storage buckets, and in a [GitHub repository](https://github.com/GoogleCloudDataproc/initialization-actions) . To contribute a script, review the [CONTRIBUTING.md](https://github.com/GoogleCloudDataproc/initialization-actions/blob/master/CONTRIBUTING.md) document, and then file a pull request.\n## Logging\nOutput from the execution of each initialization action is logged for each instance in `/var/log/dataproc-initialization-script-X.log` , where `X` is the zero-based index of each successive initialization action script. For example, if your cluster has two initialization actions, the outputs will be logged in `/var/log/dataproc-initialization-script-0.log` and `/var/log/dataproc-initialization-script-1.log` .\n## What's Next\nExplore [GitHub initialization actions](https://github.com/GoogleCloudDataproc/initialization-actions) .", "guide": "Dataproc"}