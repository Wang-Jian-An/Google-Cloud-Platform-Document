{"title": "Dataproc - Dataproc logs", "url": "https://cloud.google.com/dataproc/docs/guides/logging", "abstract": "# Dataproc - Dataproc logs\nDataproc job and cluster logs can be viewed, searched, filtered, and archived in [Cloud Logging](/logging/docs) .\n- See [Google Cloud Observability pricing](/stackdriver/pricing) to understand your costs.\n- See [Logs retention periods](/logging/quotas#logs_retention_periods) for information on logging retention.\n- See [Logs exclusions](/logging/docs/exclusions) to disable all logs or exclude logs from Logging.\n- See [Routing and storage overview](/logging/docs/routing/overview) to route logs from Logging to Cloud Storage, BigQuery, or Pub/Sub.", "content": "## Component logging levels\nSet Spark, Hadoop, Flink, and other [Dataproc component](/dataproc/docs/concepts/components/overview) logging levels with component-specific log4j [cluster properties](/dataproc/docs/concepts/configuring-clusters/cluster-properties#file-prefixed_properties_table) , such as `hadoop-log4j` , when you [create a cluster](/dataproc/docs/guides/create-cluster) . Cluster-based component logging levels apply to service daemons, such as the YARN ResourceManager, and to jobs that run on the cluster.\nIf log4j properties are not supported for a component, such as the Presto component, write an [initialization action](/dataproc/docs/concepts/configuring-clusters/init-actions) that edits the component's `log4j.properties` or `log4j2.properties` file.\nJob-specific component logging levels: You can also set component logging levels when you [submit a job](/dataproc/docs/guides/submit-job) . These logging levels are applied to the job, and take precedence over logging levels set when you created the cluster. See [Cluster vs. job properties](/dataproc/docs/concepts/configuring-clusters/cluster-properties#cluster_vs_job_properties) for more information.\n**Spark and Hive component version logging levels:**\nThe Spark 3.3.X and Hive 3.X components use log4j2 properties, while previous versions of these components use log4j properties (see [Apache Log4j2](https://logging.apache.org/log4j/2.x/index.html) ). Use a `spark-log4j:` prefix to set Spark logging levels on a cluster.\n- Example: [Dataproc image version 2.0](/dataproc/docs/concepts/versioning/dataproc-release-2.0) with Spark 3.1 to set `log4j.logger.org.apache.spark` :```\ngcloud dataproc clusters create ... \\\n\u00a0\u00a0\u00a0\u00a0--properties spark-log4j:log4j.logger.org.apache.spark=DEBUG\n```\n- Example: [Dataproc image version 2.1](/dataproc/docs/concepts/versioning/dataproc-release-2.1) with Spark 3.3 to set `logger.sparkRoot.level` :```\ngcloud dataproc clusters create ...\\\n\u00a0\u00a0\u00a0\u00a0--properties spark-log4j:logger.sparkRoot.level=debug\n```## Job driver logging levels\nDataproc uses a default [logging level](https://en.wikipedia.org/wiki/Log4j) of `INFO` for job driver programs. You can change this setting for one or more packages with the [gcloud dataproc jobs submit](/sdk/gcloud/reference/dataproc/jobs/submit) `--driver-log-levels` flag.\nExample:\nSet the `DEBUG` logging level when submitting a Spark job that reads Cloud Storage files.\n```\ngcloud dataproc jobs submit spark ...\\\n\u00a0\u00a0\u00a0\u00a0--driver-log-levels org.apache.spark=DEBUG,com.google.cloud.hadoop.gcsio=DEBUG\n```\nExample:\nSet the `root` logger level to `WARN` , `com.example` logger level to `INFO` .\n```\ngcloud dataproc jobs submit hadoop ...\\\n\u00a0\u00a0\u00a0\u00a0--driver-log-levels root=WARN,com.example=INFO\n```\n## Spark executor logging levels\nTo configure Spark executor logging levels:\n- Prepare a log4j config file, and then upload it to Cloud StorageCopy and customize the default`log4j`config located in`/etc/spark/conf/`.\n- Reference your config file when you submit the job.Example:```\ngcloud dataproc jobs submit spark ...\\\n\u00a0\u00a0\u00a0\u00a0--file gs://my-bucket/path/spark-log4j.properties \\\n\u00a0\u00a0\u00a0\u00a0--properties spark.executor.extraJavaOptions=-Dlog4j.configuration=file:spark-log4j.properties\n```\nSpark downloads the Cloud Storage properties file to the job's  local working directory, referenced as `file:<name>` in `-Dlog4j.configuration` .\n## Dataproc job logs in Logging\nSee [Dataproc job output and logs](/dataproc/docs/guides/dataproc-job-output) for information on enabling Dataproc job driver logs in Logging.\n### Access job logs in Logging\nAccess Dataproc job logs using the [Logs Explorer](https://console.cloud.google.com/logs/query) , the [gcloud logging](/sdk/gcloud/reference/logging) command, or the [Logging API](/logging/docs/apis) .\nDataproc Job driver and YARN container logs  are listed under the [Cloud Dataproc Job](/monitoring/api/resources#tag_cloud_dataproc_job) resource.\nExample: Job driver log after running a  Logs Explorer query with the following selections:- **Resource:** `Cloud Dataproc Job`\n- **Log name:** `dataproc.job.driver`\nExample: YARN container log after running a  Logs Explorer query with the following selections:- **Resource:** `Cloud Dataproc Job`\n- **Log name:** `dataproc.job.yarn.container`\nYou can read job log entries using the [gcloud logging read](/sdk/gcloud/reference/logging/read) command. **The resource arguments must be enclosed in quotes (\"...\").** The following command uses cluster labels to filter the returned log entries.\n```\ngcloud logging read \\\n\u00a0\u00a0\u00a0\u00a0\"resource.type=cloud_dataproc_job \\\n\u00a0\u00a0\u00a0\u00a0resource.labels.region=cluster-region \\\n\u00a0\u00a0\u00a0\u00a0resource.labels.job_id=my-job-id\"\n```\n **Sample output (partial):** \n```\njsonPayload:\n class: org.apache.hadoop.hdfs.StateChange\n filename: hadoop-hdfs-namenode-test-dataproc-resize-cluster-20190410-38an-m-0.log\n ,,,\nlogName: projects/project-id/logs/hadoop-hdfs-namenode\n--jsonPayload:\n class: SecurityLogger.org.apache.hadoop.security.authorize.ServiceAuthorizationManager\n filename: cluster-name-dataproc-resize-cluster-20190410-38an-m-0.log\n ...\nlogName: projects/google.com:hadoop-cloud-dev/logs/hadoop-hdfs-namenode\n```You can use the Logging REST API to list log entries (see [entries.list](/logging/docs/reference/v2/rest/v2/entries/list) ).\n## Dataproc cluster logs in Logging\nDataproc exports the following Apache Hadoop, Spark, Hive, Zookeeper, and other Dataproc cluster logs to Cloud Logging.\n| Log Type   | Log Name                                                 | Description                                               |\n|:-------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Master daemon logs | hadoop-hdfs hadoop-hdfs-namenode hadoop-hdfs-secondary namenode hadoop-hdfs-zkfc hadoop-yarn-resourcemanager hadoop-yarn-timelineserver hive-metastore hive-server2 mapred-mapred-historyserver zookeeper | Journal node HDFS namenode HDFS secondary namenode Zookeeper failover controller YARN resource manager YARN timeline server Hive metastore Hive server2 Mapreduce job history server Zookeeper server |\n| Worker daemon logs | hadoop-hdfs-datanode hadoop-yarn-nodemanager                                        | HDFS datanode YARN nodemanager                                          |\n| System logs  | autoscaler google.dataproc.agent google.dataproc.startup                                     | Dataproc autoscaler log Dataproc agent log Dataproc startup script log + initialization action log                         |\n### Access cluster logs in Cloud Logging\nYou can access Dataproc cluster logs using the [Logs Explorer](https://console.cloud.google.com/logs/query) , the [gcloud logging](/sdk/gcloud/reference/logging) command, or the [Logging API](/logging/docs/apis) .\nMake the following query selections to view  cluster logs in the Logs Explorer:- **Resource:** `Cloud Dataproc Cluster`\n- **Log name:** \n- To pre-select a cluster in the Logs Explorer:- Click the cluster name on the [Clusters](https://console.cloud.google.com/dataproc/clusters) page in   Google Cloud console to open the **Cluster details** page.\n- Click **View Logs** .\n- **Use an\nAdvanced Filter to\ntarget cluster logs.** Automatically created cluster labels, which can be used to filter cluster logs, are listed under the Configuration tab in the Dataproc **Cluster details** page in the Google Cloud console.\nYou can read cluster log entries using the [gcloud logging read](/sdk/gcloud/reference/logging/read) command. **The resource arguments must be enclosed in quotes (\"...\").** The following command uses cluster labels to filter the returned log entries.\n```\ngcloud logging read <<'EOF'\n\u00a0\u00a0\u00a0\u00a0\"resource.type=cloud_dataproc_cluster\n\u00a0\u00a0\u00a0\u00a0resource.labels.region=cluster-region\n\u00a0\u00a0\u00a0\u00a0resource.labels.cluster_name=cluster-name\n\u00a0\u00a0\u00a0\u00a0resource.labels.cluster_uuid=cluster-uuid\"\nEOF\n```\n **Sample output (partial):** \n```\njsonPayload:\n class: org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService\n filename: hadoop-yarn-resourcemanager-cluster-name-m.log\n ...\nlogName: projects/project-id/logs/hadoop-yarn-resourcemanager\n--jsonPayload:\n class: org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService\n filename: hadoop-yarn-resourcemanager-component-gateway-cluster-m.log\n ...\nlogName: projects/project-id/logs/hadoop-yarn-resourcemanager\n```\nRun`gcloud dataproc clusters describe` `` `--region=` ``to list cluster labels that you can use to filter logging results.You can use the Logging REST API to list log entries (see [entries.list](/logging/docs/reference/v2/rest/v2/entries/list) ).\n## Permissions\nTo write logs to Logging, the Dataproc VM service account must have the [logging.logWriter role](/logging/docs/access-control) IAM role. The default Dataproc service account has this role. If you use a [custom service account](/dataproc/docs/concepts/configuring-clusters/service-accounts#using_service_accounts) , you must assign this role to the service account.\n## Protecting the logs\nBy default, logs in Logging are encrypted at rest. You can enable [customer-managed encryption keys (CMEK)](/kms/docs/cmek) to encrypt the logs. For more information on CMEK support, see [Manage the keys that protect Log Router data](/logging/docs/routing/managed-encryption) and [Manage the keys that protect Logging storage data](/logging/docs/routing/managed-encryption-storage) .\n## Whats next\n- Explore [Google Cloud Observability](/stackdriver/docs)", "guide": "Dataproc"}