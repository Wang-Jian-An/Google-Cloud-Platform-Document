{"title": "Dataproc - Attaching GPUs to clusters", "url": "https://cloud.google.com/dataproc/docs/concepts/compute/gpus", "abstract": "# Dataproc - Attaching GPUs to clusters\nDataproc provides the ability for graphics processing units (GPUs) to be attached to the master and worker Compute Engine nodes in a Dataproc cluster. You can use these GPUs to accelerate specific workloads on your instances, such as machine learning and data processing.\nFor more information about what you can do with GPUs and what types of GPU hardware are available, read [GPUs on Compute Engine](/compute/docs/gpus) .\nThere are no additional [Dataproc pricing](/dataproc/pricing) charges added to Compute Engine charges for GPUs used in Dataproc clusters.\n", "content": "## Before you begin\n- GPUs require special drivers and software. These items are not pre-installed on Dataproc clusters.\n- Read about [GPU pricing on Compute Engine](/compute/pricing#gpus) to understand the cost to use GPUs in your instances.\n- Read about [restrictions for instances with GPUs](/compute/docs/gpus#restrictions) to learn how these instances function differently from non-GPU instances.\n- Check the [quotas page](https://console.cloud.google.com/compute/quotas) for your project to ensure that you have sufficient GPU quota (`NVIDIA_K80_GPUS`,`NVIDIA_P100_GPUS`, or`NVIDIA_V100_GPUS`) available in your project. If GPUs are not listed on the quotas page or you require additional GPU quota, [request a quota increase](/compute/quotas#requesting_additional_quota) .## Types of GPUs\nDataproc nodes support the following GPU types. You must specify GPU type when attaching GPUs to your Dataproc cluster.\n- `nvidia-tesla-l4`- NVIDIA\u00ae Tesla\u00ae L4\n- `nvidia-tesla-a100`- NVIDIA\u00ae Tesla\u00ae A100\n- `nvidia-tesla-k80`- NVIDIA\u00ae Tesla\u00ae K80\n- `nvidia-tesla-p100`- NVIDIA\u00ae Tesla\u00ae P100\n- `nvidia-tesla-v100`- NVIDIA\u00ae Tesla\u00ae V100\n- `nvidia-tesla-p4`- NVIDIA\u00ae Tesla\u00ae P4\n- `nvidia-tesla-t4`- NVIDIA\u00ae Tesla\u00ae T4\n- `nvidia-tesla-p100-vws`- NVIDIA\u00ae Tesla\u00ae P100 Virtual Workstations\n- `nvidia-tesla-p4-vws`- NVIDIA\u00ae Tesla\u00ae P4 Virtual Workstations\n- `nvidia-tesla-t4-vws`- NVIDIA\u00ae Tesla\u00ae T4 Virtual Workstations## Attaching GPUs to clusters\nAttach GPUs to the master and primary and secondary worker nodes in a Dataproc cluster when creating the cluster using the [\u2011\u2011master-accelerator](/sdk/gcloud/reference/dataproc/clusters/create#--master-accelerator) , [\u2011\u2011worker-accelerator](/sdk/gcloud/reference/dataproc/clusters/create#--worker-accelerator) , and [\u2011\u2011secondary-worker-accelerator](/sdk/gcloud/reference/dataproc/clusters/create#--secondary-worker-accelerator) flags. These flags take the following two values:- the type of GPU to attach to a node, and\n- the number of GPUs to attach to the node.\nThe type of GPU is required, and the number of GPUs is optional (the default is 1 GPU).\n **Example:** \n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--master-accelerator type=nvidia-tesla-k80 \\\n\u00a0\u00a0\u00a0\u00a0--worker-accelerator type=nvidia-tesla-k80,count=4 \\\n\u00a0\u00a0\u00a0\u00a0--secondary-worker-accelerator type=nvidia-tesla-k80,count=4 \\\n\u00a0\u00a0\u00a0\u00a0... other flags\n```\nTo use GPUs in your cluster, you must [install GPU drivers](/compute/docs/gpus/install-drivers-gpu) .Attach GPUs to the master and primary and secondary worker nodes in a Dataproc cluster by filling in the [InstanceGroupConfig.AcceleratorConfig](/dataproc/docs/reference/rest/v1/ClusterConfig#AcceleratorConfig) `acceleratorTypeUri` and `acceleratorCount` fields as part of the [cluster.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) API request.Click CPU PLATFORM AND GPU\u2192GPUs\u2192ADD GPU in the master and worker nodes sections of the Configure nodes panel on the [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page in the Google Cloud console to specify the number of GPUs and GPU type for the nodes.\n## Installing GPU drivers\nGPU drivers are required to utilize any GPUs attached to Dataproc nodes. You can install GPU drivers by following the [instructions for this initialization action](https://github.com/GoogleCloudDataproc/initialization-actions/tree/master/gpu) , which is listed below.\n[  gpu/install_gpu_driver.sh ](https://github.com/GoogleCloudDataproc/initialization-actions/blob/HEAD/gpu/install_gpu_driver.sh) [View on GitHub](https://github.com/GoogleCloudDataproc/initialization-actions/blob/HEAD/gpu/install_gpu_driver.sh)\n```\n#!/bin/bash\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 \u00a0http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS-IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.\n## This script installs NVIDIA GPU drivers and collects GPU utilization metrics.set -euxo pipefailfunction compare_versions_lte {\u00a0 [ \"$1\" = \"$(echo -e \"$1\\n$2\" | sort -V | head -n1)\" ]}function compare_versions_lt() {\u00a0 [ \"$1\" = \"$2\" ] && return 1 || compare_versions_lte $1 $2}function get_metadata_attribute() {\u00a0 local -r attribute_name=$1\u00a0 local -r default_value=$2\u00a0 /usr/share/google/get_metadata_value \"attributes/${attribute_name}\" || echo -n \"${default_value}\"}OS_NAME=$(lsb_release -is | tr '[:upper:]' '[:lower:]')distribution=$(. /etc/os-release;echo $ID$VERSION_ID)readonly OS_NAME# node roleROLE=\"$(/usr/share/google/get_metadata_value attributes/dataproc-role)\"readonly ROLE# CUDA version and Driver version# https://docs.nvidia.com/deeplearning/frameworks/support-matrix/index.htmlreadonly -A DRIVER_FOR_CUDA=([10.1]=\"418.88\" \u00a0 \u00a0[10.2]=\"440.64.00\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [11.0]=\"450.51.06\" [11.1]=\"455.45.01\" [11.2]=\"460.73.01\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [11.5]=\"495.29.05\" [11.6]=\"510.47.03\" [11.7]=\"515.65.01\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [11.8]=\"520.56.06\")readonly -A CUDNN_FOR_CUDA=( [10.1]=\"7.6.4.38\" \u00a0[10.2]=\"7.6.5.32\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [11.0]=\"8.0.4.30\" \u00a0[11.1]=\"8.0.5.39\" \u00a0[11.2]=\"8.1.1.33\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [11.5]=\"8.3.3.40\" \u00a0[11.6]=\"8.4.1.50\" \u00a0[11.7]=\"8.5.0.96\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [11.8]=\"8.6.0.163\")readonly -A NCCL_FOR_CUDA=( \u00a0[10.1]=\"2.4.8\" \u00a0 \u00a0 [10.2]=\"2.5.6\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [11.0]=\"2.7.8\" \u00a0 \u00a0 [11.1]=\"2.8.3\" \u00a0 \u00a0 [11.2]=\"2.8.3\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [11.5]=\"2.11.4\" \u00a0 \u00a0[11.6]=\"2.11.4\" \u00a0 \u00a0[11.7]=\"2.12.12\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [11.8]=\"2.15.5\")readonly -A CUDA_SUBVER=( \u00a0 \u00a0[10.1]=\"10.1.243\" \u00a0[10.2]=\"10.2.89\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [11.0]=\"11.0.3\" \u00a0 \u00a0[11.1]=\"11.1.0\" \u00a0 \u00a0[11.2]=\"11.2.2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [11.5]=\"11.5.2\" \u00a0 \u00a0[11.6]=\"11.6.2\" \u00a0 \u00a0[11.7]=\"11.7.1\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 [11.8]=\"11.8.0\")RUNTIME=$(get_metadata_attribute 'rapids-runtime' 'SPARK')DEFAULT_CUDA_VERSION='11.2'if [[ ${DATAPROC_IMAGE_VERSION} == 2.* ]] && [[ \"${RUNTIME}\" == \"SPARK\" ]]; then\u00a0 DEFAULT_CUDA_VERSION='11.5'fireadonly DEFAULT_CUDA_VERSIONreadonly CUDA_VERSION=$(get_metadata_attribute 'cuda-version' \"${DEFAULT_CUDA_VERSION}\")readonly DEFAULT_NVIDIA_DEBIAN_GPU_DRIVER_VERSION=${DRIVER_FOR_CUDA[\"${CUDA_VERSION}\"]}readonly NVIDIA_DEBIAN_GPU_DRIVER_VERSION=$(get_metadata_attribute 'gpu-driver-version' ${DEFAULT_NVIDIA_DEBIAN_GPU_DRIVER_VERSION})readonly NVIDIA_DEBIAN_GPU_DRIVER_VERSION_PREFIX=${NVIDIA_DEBIAN_GPU_DRIVER_VERSION%%.*}readonly DRIVER=${NVIDIA_DEBIAN_GPU_DRIVER_VERSION_PREFIX}# As of Rocky 8.7, kernel 4.18.0-425 is unable to build older nvidia kernel driversif [[ \"${OS_NAME}\" == \"rocky\" && \u00a0\"${DRIVER}\" < \"510\" ]]; then\u00a0 readonly ROCKY_BINARY_INSTALL=\"true\"fi# Fail early for configurations known to be unsupportedfunction unsupported_error {\u00a0 echo \"Unsupported kernel driver on ${distribution}: '${DRIVER}'\"\u00a0 exit -1}if [[ \"${OS_NAME}\" == \"rocky\" ]]; then\u00a0 KERNEL_SUBVERSION=$(uname -r | awk -F- '{print $2}')\u00a0 if [[ \"${DRIVER}\" < \"460\" && \"${DRIVER}\" != \"450\"\u00a0 \u00a0 \u00a0&& \"${KERNEL_SUBVERSION%%.*}\" > \"305\" ]]; then\u00a0 \u00a0 unsupported_error\u00a0 fielif [[ \"${OS_NAME}\" == \"debian\" ]]; then\u00a0 KERNEL_VERSION=$(uname -r | awk -F- '{print $1}')\u00a0 if [[ \"${DRIVER}\" < \"455\"\u00a0 \u00a0 \u00a0&& $(echo \"${KERNEL_VERSION%.*} > 5.7\" | bc -l) == 1 \u00a0]]; then\u00a0 \u00a0 unsupported_error\u00a0 fifiDEFAULT_NCCL_VERSION=${NCCL_FOR_CUDA[\"${CUDA_VERSION}\"]}if [[ \"${OS_NAME}\" == \"rocky\" ]] \\\u00a0 \u00a0&& (compare_versions_lte \"${DEFAULT_NCCL_VERSION}\" \"2.8.4\") ; then\u00a0 DEFAULT_NCCL_VERSION=\"2.8.4\"fireadonly DEFAULT_NCCL_VERSIONreadonly NCCL_VERSION=$(get_metadata_attribute 'nccl-version' ${DEFAULT_NCCL_VERSION})# Parameters for NVIDIA-provided Debian GPU driverDEFAULT_NVIDIA_DEBIAN_GPU_DRIVER_URL=\"https://download.nvidia.com/XFree86/Linux-x86_64/${NVIDIA_DEBIAN_GPU_DRIVER_VERSION}/NVIDIA-Linux-x86_64-${NVIDIA_DEBIAN_GPU_DRIVER_VERSION}.run\"if [[ \"$(curl -s -I ${DEFAULT_NVIDIA_DEBIAN_GPU_DRIVER_URL} | head -1 | awk '{print $2}')\" != \"200\" ]]; then\u00a0 DEFAULT_NVIDIA_DEBIAN_GPU_DRIVER_URL=\"https://download.nvidia.com/XFree86/Linux-x86_64/${NVIDIA_DEBIAN_GPU_DRIVER_VERSION%.*}/NVIDIA-Linux-x86_64-${NVIDIA_DEBIAN_GPU_DRIVER_VERSION%.*}.run\"fireadonly DEFAULT_NVIDIA_DEBIAN_GPU_DRIVER_URLNVIDIA_DEBIAN_GPU_DRIVER_URL=$(get_metadata_attribute 'gpu-driver-url' \"${DEFAULT_NVIDIA_DEBIAN_GPU_DRIVER_URL}\")readonly NVIDIA_DEBIAN_GPU_DRIVER_URLreadonly NVIDIA_BASE_DL_URL='https://developer.download.nvidia.com/compute'# Parameters for NVIDIA-provided NCCL libraryreadonly DEFAULT_NCCL_REPO_URL=\"${NVIDIA_BASE_DL_URL}/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\"NCCL_REPO_URL=$(get_metadata_attribute 'nccl-repo-url' \"${DEFAULT_NCCL_REPO_URL}\")readonly NCCL_REPO_URLreadonly NCCL_REPO_KEY=\"${NVIDIA_BASE_DL_URL}/machine-learning/repos/ubuntu1804/x86_64/7fa2af80.pub\"readonly -A DEFAULT_NVIDIA_DEBIAN_CUDA_URLS=(\u00a0 [10.1]=\"${NVIDIA_BASE_DL_URL}/cuda/10.1/Prod/local_installers/cuda_10.1.243_418.87.00_linux.run\"\u00a0 [10.2]=\"${NVIDIA_BASE_DL_URL}/cuda/10.2/Prod/local_installers/cuda_10.2.89_440.33.01_linux.run\"\u00a0 [11.0]=\"${NVIDIA_BASE_DL_URL}/cuda/11.0.3/local_installers/cuda_11.0.3_450.51.06_linux.run\"\u00a0 [11.1]=\"${NVIDIA_BASE_DL_URL}/cuda/11.1.0/local_installers/cuda_11.1.0_455.23.05_linux.run\"\u00a0 [11.2]=\"${NVIDIA_BASE_DL_URL}/cuda/11.2.2/local_installers/cuda_11.2.2_460.32.03_linux.run\"\u00a0 [11.5]=\"${NVIDIA_BASE_DL_URL}/cuda/11.5.2/local_installers/cuda_11.5.2_495.29.05_linux.run\"\u00a0 [11.6]=\"${NVIDIA_BASE_DL_URL}/cuda/11.6.2/local_installers/cuda_11.6.2_510.47.03_linux.run\"\u00a0 [11.7]=\"${NVIDIA_BASE_DL_URL}/cuda/11.7.1/local_installers/cuda_11.7.1_515.65.01_linux.run\"\u00a0 [11.8]=\"${NVIDIA_BASE_DL_URL}/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run\")readonly DEFAULT_NVIDIA_DEBIAN_CUDA_URL=${DEFAULT_NVIDIA_DEBIAN_CUDA_URLS[\"${CUDA_VERSION}\"]}NVIDIA_DEBIAN_CUDA_URL=$(get_metadata_attribute 'cuda-url' \"${DEFAULT_NVIDIA_DEBIAN_CUDA_URL}\")readonly NVIDIA_DEBIAN_CUDA_URL# Parameters for NVIDIA-provided Ubuntu GPU driverreadonly NVIDIA_UBUNTU_REPO_URL=\"${NVIDIA_BASE_DL_URL}/cuda/repos/ubuntu1804/x86_64\"readonly NVIDIA_UBUNTU_REPO_KEY_PACKAGE=\"${NVIDIA_UBUNTU_REPO_URL}/cuda-keyring_1.0-1_all.deb\"readonly NVIDIA_UBUNTU_REPO_CUDA_PIN=\"${NVIDIA_UBUNTU_REPO_URL}/cuda-ubuntu1804.pin\"# Parameter for NVIDIA-provided Rocky Linux GPU driverreadonly NVIDIA_ROCKY_REPO_URL=\"${NVIDIA_BASE_DL_URL}/cuda/repos/rhel8/x86_64/cuda-rhel8.repo\"# Parameters for NVIDIA-provided CUDNN libraryDEFAULT_CUDNN_VERSION=${CUDNN_FOR_CUDA[\"${CUDA_VERSION}\"]}if [[ \"${OS_NAME}\" == \"rocky\" ]] \\\u00a0 \u00a0&& (compare_versions_lte \"${DEFAULT_CUDNN_VERSION}\" \"8.0.5.39\") ; then\u00a0 DEFAULT_CUDNN_VERSION=\"8.0.5.39\"fireadonly DEFAULT_CUDNN_VERSIONreadonly CUDNN_VERSION=$(get_metadata_attribute 'cudnn-version' \"${DEFAULT_CUDNN_VERSION}\")CUDNN_TARBALL=\"cudnn-${CUDA_VERSION}-linux-x64-v${CUDNN_VERSION}.tgz\"CUDNN_TARBALL_URL=\"${NVIDIA_BASE_DL_URL}/redist/cudnn/v${CUDNN_VERSION%.*}/${CUDNN_TARBALL}\"if ( compare_versions_lte \"8.3.1.22\" \"${CUDNN_VERSION}\" ); then\u00a0 CUDNN_TARBALL=\"cudnn-linux-x86_64-${CUDNN_VERSION}_cuda${CUDA_VERSION%.*}-archive.tar.xz\"\u00a0 if ( compare_versions_lte \"${CUDNN_VERSION}\" \"8.4.1.50\" ); then\u00a0 \u00a0 CUDNN_TARBALL=\"cudnn-linux-x86_64-${CUDNN_VERSION}_cuda${CUDA_VERSION}-archive.tar.xz\"\u00a0 fi\u00a0 CUDNN_TARBALL_URL=\"${NVIDIA_BASE_DL_URL}/redist/cudnn/v${CUDNN_VERSION%.*}/local_installers/${CUDA_VERSION}/${CUDNN_TARBALL}\"fireadonly CUDNN_TARBALLreadonly CUDNN_TARBALL_URL# Whether to install NVIDIA-provided or OS-provided GPU driverGPU_DRIVER_PROVIDER=$(get_metadata_attribute 'gpu-driver-provider' 'NVIDIA')readonly GPU_DRIVER_PROVIDER# Stackdriver GPU agent parametersreadonly GPU_AGENT_REPO_URL='https://raw.githubusercontent.com/GoogleCloudPlatform/ml-on-gcp/master/dlvm/gcp-gpu-utilization-metrics'# Whether to install GPU monitoring agent that sends GPU metrics to StackdriverINSTALL_GPU_AGENT=$(get_metadata_attribute 'install-gpu-agent' 'false')readonly INSTALL_GPU_AGENT# Dataproc configurationsreadonly HADOOP_CONF_DIR='/etc/hadoop/conf'readonly HIVE_CONF_DIR='/etc/hive/conf'readonly SPARK_CONF_DIR='/etc/spark/conf'NVIDIA_SMI_PATH='/usr/bin'MIG_MAJOR_CAPS=0IS_MIG_ENABLED=0function execute_with_retries() {\u00a0 local -r cmd=$1\u00a0 for ((i = 0; i < 10; i++)); do\u00a0 \u00a0 if eval \"$cmd\"; then\u00a0 \u00a0 \u00a0 return 0\u00a0 \u00a0 fi\u00a0 \u00a0 sleep 5\u00a0 done\u00a0 return 1}function install_nvidia_nccl() {\u00a0 local -r nccl_version=\"${NCCL_VERSION}-1+cuda${CUDA_VERSION}\"\u00a0 if [[ ${OS_NAME} == rocky ]]; then\u00a0 \u00a0 execute_with_retries \"dnf -y -q install libnccl-${nccl_version} libnccl-devel-${nccl_version} libnccl-static-${nccl_version}\"\u00a0 elif [[ ${OS_NAME} == ubuntu ]] || [[ ${OS_NAME} == debian ]]; then\u00a0 \u00a0 curl -fsSL --retry-connrefused --retry 10 --retry-max-time 30 \"${NCCL_REPO_KEY}\" | apt-key add -\u00a0 \u00a0 local tmp_dir\u00a0 \u00a0 tmp_dir=$(mktemp -d -t gpu-init-action-nccl-XXXX)\u00a0 \u00a0 curl -fsSL --retry-connrefused --retry 10 --retry-max-time 30 \\\u00a0 \u00a0 \u00a0 \"${NCCL_REPO_URL}\" -o \"${tmp_dir}/nvidia-ml-repo.deb\"\u00a0 \u00a0 dpkg -i \"${tmp_dir}/nvidia-ml-repo.deb\"\u00a0 \u00a0 execute_with_retries \"apt-get update\"\u00a0 \u00a0 execute_with_retries \\\u00a0 \u00a0 \u00a0 \"apt-get install -y --allow-unauthenticated libnccl2=${nccl_version} libnccl-dev=${nccl_version}\"\u00a0 else\u00a0 \u00a0 echo \"Unsupported OS: '${OS_NAME}'\"\u00a0 \u00a0 exit 1\u00a0 fi}function install_nvidia_cudnn() {\u00a0 local major_version\u00a0 major_version=\"${CUDNN_VERSION%%.*}\"\u00a0 local cudnn_pkg_version\u00a0 cudnn_pkg_version=\"${CUDNN_VERSION}-1+cuda${CUDA_VERSION}\"\u00a0 if [[ ${OS_NAME} == rocky ]]; then\u00a0 \u00a0 if [[ ${major_version} == 8 ]]; then\u00a0 \u00a0 \u00a0 execute_with_retries \"dnf -y -q install libcudnn8-${cudnn_pkg_version} libcudnn8-devel-${cudnn_pkg_version}\"\u00a0 \u00a0 else\u00a0 \u00a0 \u00a0 echo \"Unsupported CUDNN version: '${CUDNN_VERSION}'\"\u00a0 \u00a0 \u00a0 exit 1\u00a0 \u00a0 fi\u00a0 elif [[ ${OS_NAME} == ubuntu ]]; then\u00a0 \u00a0 local -a packages\u00a0 \u00a0 packages=(\u00a0 \u00a0 \u00a0 \"libcudnn${major_version}=${cudnn_pkg_version}\"\u00a0 \u00a0 \u00a0 \"libcudnn${major_version}-dev=${cudnn_pkg_version}\")\u00a0 \u00a0 execute_with_retries \\\u00a0 \u00a0 \u00a0 \"apt-get install -y --no-install-recommends ${packages[*]}\"\u00a0 elif [[ ${OS_NAME} == debian ]]; then\u00a0 \u00a0 local tmp_dir\u00a0 \u00a0 tmp_dir=$(mktemp -d -t gpu-init-action-cudnn-XXXX)\u00a0 \u00a0 curl -fSsL --retry-connrefused --retry 10 --retry-max-time 30 \\\u00a0 \u00a0 \u00a0 \"${CUDNN_TARBALL_URL}\" -o \"${tmp_dir}/${CUDNN_TARBALL}\"\u00a0 \u00a0 if ( compare_versions_lte \"${CUDNN_VERSION}\" \"8.3.0.98\" ); then\u00a0 \u00a0 \u00a0 tar -xzf \"${tmp_dir}/${CUDNN_TARBALL}\" -C /usr/local\u00a0 \u00a0 else\u00a0 \u00a0 \u00a0 ln -sf /usr/local/cuda/targets/x86_64-linux/lib /usr/local/cuda/lib\u00a0 \u00a0 \u00a0 tar -h --no-same-owner --strip-components=1 \\\u00a0 \u00a0 \u00a0 \u00a0 -xJf \"${tmp_dir}/${CUDNN_TARBALL}\" -C /usr/local/cuda\u00a0 \u00a0 fi\u00a0 \u00a0 cat <<'EOF' >>/etc/profile.d/cudnn.shexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH}EOF\u00a0 else\u00a0 \u00a0 echo \"Unsupported OS: '${OS_NAME}'\"\u00a0 \u00a0 exit 1\u00a0 fi\u00a0 ldconfig\u00a0 echo \"NVIDIA cuDNN successfully installed for ${OS_NAME}.\"}# Install NVIDIA GPU driver provided by NVIDIAfunction install_nvidia_gpu_driver() {\u00a0 if [[ ${OS_NAME} == debian ]]; then\u00a0 \u00a0 curl -fsSL --retry-connrefused --retry 10 --retry-max-time 30 \\\u00a0 \u00a0 \u00a0 \"${NVIDIA_UBUNTU_REPO_KEY_PACKAGE}\" -o /tmp/cuda-keyring.deb\u00a0 \u00a0 dpkg -i \"/tmp/cuda-keyring.deb\"\u00a0 \u00a0 curl -fsSL --retry-connrefused --retry 10 --retry-max-time 30 \\\u00a0 \u00a0 \u00a0 \"${NVIDIA_DEBIAN_GPU_DRIVER_URL}\" -o driver.run\u00a0 \u00a0 bash \"./driver.run\" --silent --install-libglvnd\u00a0 \u00a0 curl -fsSL --retry-connrefused --retry 10 --retry-max-time 30 \\\u00a0 \u00a0 \u00a0 \"${NVIDIA_DEBIAN_CUDA_URL}\" -o cuda.run\u00a0 \u00a0 bash \"./cuda.run\" --silent --toolkit --no-opengl-libs\u00a0 elif [[ ${OS_NAME} == ubuntu ]]; then\u00a0 \u00a0 curl -fsSL --retry-connrefused --retry 10 --retry-max-time 30 \\\u00a0 \u00a0 \u00a0 \"${NVIDIA_UBUNTU_REPO_KEY_PACKAGE}\" -o /tmp/cuda-keyring.deb\u00a0 \u00a0 dpkg -i \"/tmp/cuda-keyring.deb\"\u00a0 \u00a0 curl -fsSL --retry-connrefused --retry 10 --retry-max-time 30 \\\u00a0 \u00a0 \u00a0 \"${NVIDIA_UBUNTU_REPO_CUDA_PIN}\" -o /etc/apt/preferences.d/cuda-repository-pin-600\u00a0 \u00a0 add-apt-repository \"deb ${NVIDIA_UBUNTU_REPO_URL} /\"\u00a0 \u00a0 execute_with_retries \"apt-get update\"\u00a0 \u00a0 if [[ -n \"${CUDA_VERSION}\" ]]; then\u00a0 \u00a0 \u00a0 local -r cuda_package=cuda-toolkit-${CUDA_VERSION//./-}\u00a0 \u00a0 else\u00a0 \u00a0 \u00a0 local -r cuda_package=cuda-toolkit\u00a0 \u00a0 fi\u00a0 \u00a0 # Without --no-install-recommends this takes a very long time.\u00a0 \u00a0 execute_with_retries \"apt-get install -y -q --no-install-recommends cuda-drivers-${NVIDIA_DEBIAN_GPU_DRIVER_VERSION_PREFIX}\"\u00a0 \u00a0 execute_with_retries \"apt-get install -y -q --no-install-recommends ${cuda_package}\"\u00a0 elif [[ ${OS_NAME} == rocky ]]; then\u00a0 \u00a0 execute_with_retries \"dnf config-manager --add-repo ${NVIDIA_ROCKY_REPO_URL}\"\u00a0 \u00a0 execute_with_retries \"dnf clean all\"\u00a0 \u00a0 if [[ \"${ROCKY_BINARY_INSTALL}\" == \"true\" ]]; then\u00a0 \u00a0 \u00a0 execute_with_retries \"dnf -y -q module install nvidia-driver\"\u00a0 \u00a0 else\u00a0 \u00a0 \u00a0 execute_with_retries \"dnf -y -q module install nvidia-driver:${NVIDIA_DEBIAN_GPU_DRIVER_VERSION_PREFIX}-dkms\"\u00a0 \u00a0 fi\u00a0 \u00a0 NVIDIA_ROCKY_GPU_DRIVER_VERSION=\"$(ls -d /usr/src/nvidia-* | awk -F\"nvidia-\" '{print $2}')\"\u00a0 \u00a0 execute_with_retries \"dkms build nvidia/${NVIDIA_ROCKY_GPU_DRIVER_VERSION}\"\u00a0 \u00a0 execute_with_retries \"dkms install nvidia/${NVIDIA_ROCKY_GPU_DRIVER_VERSION}\"\u00a0 \u00a0 modprobe nvidia\u00a0 \u00a0 execute_with_retries \"dnf -y -q install cuda-${CUDA_VERSION//./-}\"\u00a0 else\u00a0 \u00a0 echo \"Unsupported OS: '${OS_NAME}'\"\u00a0 \u00a0 exit 1\u00a0 fi\u00a0 ldconfig\u00a0 echo \"NVIDIA GPU driver provided by NVIDIA was installed successfully\"}# Collects 'gpu_utilization' and 'gpu_memory_utilization' metricsfunction install_gpu_agent() {\u00a0 if ! command -v pip; then\u00a0 \u00a0 execute_with_retries \"apt-get install -y -q python-pip\"\u00a0 fi\u00a0 local install_dir=/opt/gpu-utilization-agent\u00a0 mkdir -p \"${install_dir}\"\u00a0 curl -fsSL --retry-connrefused --retry 10 --retry-max-time 30 \\\u00a0 \u00a0 \"${GPU_AGENT_REPO_URL}/requirements.txt\" -o \"${install_dir}/requirements.txt\"\u00a0 curl -fsSL --retry-connrefused --retry 10 --retry-max-time 30 \\\u00a0 \u00a0 \"${GPU_AGENT_REPO_URL}/report_gpu_metrics.py\" -o \"${install_dir}/report_gpu_metrics.py\"\u00a0 pip install -r \"${install_dir}/requirements.txt\"\u00a0 # Generate GPU service.\u00a0 cat <<EOF >/lib/systemd/system/gpu-utilization-agent.service[Unit]Description=GPU Utilization Metric Agent[Service]Type=simplePIDFile=/run/gpu_agent.pidExecStart=/bin/bash --login -c 'python \"${install_dir}/report_gpu_metrics.py\"'User=rootGroup=rootWorkingDirectory=/Restart=always[Install]WantedBy=multi-user.targetEOF\u00a0 # Reload systemd manager configuration\u00a0 systemctl daemon-reload\u00a0 # Enable gpu-utilization-agent service\u00a0 systemctl --no-reload --now enable gpu-utilization-agent.service}function set_hadoop_property() {\u00a0 local -r config_file=$1\u00a0 local -r property=$2\u00a0 local -r value=$3\u00a0 bdconfig set_property \\\u00a0 \u00a0 --configuration_file \"${HADOOP_CONF_DIR}/${config_file}\" \\\u00a0 \u00a0 --name \"${property}\" --value \"${value}\" \\\u00a0 \u00a0 --clobber}function configure_yarn() {\u00a0 if [[ ! -f ${HADOOP_CONF_DIR}/resource-types.xml ]]; then\u00a0 \u00a0 printf '<?xml version=\"1.0\" ?>\\n<configuration/>' >\"${HADOOP_CONF_DIR}/resource-types.xml\"\u00a0 fi\u00a0 set_hadoop_property 'resource-types.xml' 'yarn.resource-types' 'yarn.io/gpu'\u00a0 set_hadoop_property 'capacity-scheduler.xml' \\\u00a0 \u00a0 'yarn.scheduler.capacity.resource-calculator' \\\u00a0 \u00a0 'org.apache.hadoop.yarn.util.resource.DominantResourceCalculator'\u00a0 set_hadoop_property 'yarn-site.xml' 'yarn.resource-types' 'yarn.io/gpu'}# This configuration should be applied only if GPU is attached to the nodefunction configure_yarn_nodemanager() {\u00a0 set_hadoop_property 'yarn-site.xml' 'yarn.nodemanager.resource-plugins' 'yarn.io/gpu'\u00a0 set_hadoop_property 'yarn-site.xml' \\\u00a0 \u00a0 'yarn.nodemanager.resource-plugins.gpu.allowed-gpu-devices' 'auto'\u00a0 set_hadoop_property 'yarn-site.xml' \\\u00a0 \u00a0 'yarn.nodemanager.resource-plugins.gpu.path-to-discovery-executables' $NVIDIA_SMI_PATH\u00a0 set_hadoop_property 'yarn-site.xml' \\\u00a0 \u00a0 'yarn.nodemanager.linux-container-executor.cgroups.mount' 'true'\u00a0 set_hadoop_property 'yarn-site.xml' \\\u00a0 \u00a0 'yarn.nodemanager.linux-container-executor.cgroups.mount-path' '/sys/fs/cgroup'\u00a0 set_hadoop_property 'yarn-site.xml' \\\u00a0 \u00a0 'yarn.nodemanager.linux-container-executor.cgroups.hierarchy' 'yarn'\u00a0 set_hadoop_property 'yarn-site.xml' \\\u00a0 \u00a0 'yarn.nodemanager.container-executor.class' \\\u00a0 \u00a0 'org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor'\u00a0 set_hadoop_property 'yarn-site.xml' 'yarn.nodemanager.linux-container-executor.group' 'yarn'\u00a0 # Fix local dirs access permissions\u00a0 local yarn_local_dirs=()\u00a0 readarray -d ',' yarn_local_dirs < <(bdconfig get_property_value \\\u00a0 \u00a0 --configuration_file \"${HADOOP_CONF_DIR}/yarn-site.xml\" \\\u00a0 \u00a0 --name \"yarn.nodemanager.local-dirs\" 2>/dev/null | tr -d '\\n')\u00a0 chown yarn:yarn -R \"${yarn_local_dirs[@]/,/}\"}function configure_gpu_exclusive_mode() {\u00a0 # check if running spark 3, if not, enable GPU exclusive mode\u00a0 local spark_version\u00a0 spark_version=$(spark-submit --version 2>&1 | sed -n 's/.*version[[:blank:]]\\+\\([0-9]\\+\\.[0-9]\\).*/\\1/p' | head -n1)\u00a0 if [[ ${spark_version} != 3.* ]]; then\u00a0 \u00a0 # include exclusive mode on GPU\u00a0 \u00a0 nvidia-smi -c EXCLUSIVE_PROCESS\u00a0 fi}function fetch_mig_scripts() {\u00a0 mkdir -p /usr/local/yarn-mig-scripts\u00a0 sudo chmod 755 /usr/local/yarn-mig-scripts\u00a0 wget -P /usr/local/yarn-mig-scripts/ https://raw.githubusercontent.com/NVIDIA/spark-rapids-examples/branch-22.10/examples/MIG-Support/yarn-unpatched/scripts/nvidia-smi\u00a0 wget -P /usr/local/yarn-mig-scripts/ https://raw.githubusercontent.com/NVIDIA/spark-rapids-examples/branch-22.10/examples/MIG-Support/yarn-unpatched/scripts/mig2gpu.sh\u00a0 sudo chmod 755 /usr/local/yarn-mig-scripts/*}function configure_gpu_script() {\u00a0 # Download GPU discovery script\u00a0 local -r spark_gpu_script_dir='/usr/lib/spark/scripts/gpu'\u00a0 mkdir -p ${spark_gpu_script_dir}\u00a0 # need to update the getGpusResources.sh script to look for MIG devices since if multiple GPUs nvidia-smi still\u00a0 # lists those because we only disable the specific GIs via CGROUPs. Here we just create it based off of:\u00a0 # https://raw.githubusercontent.com/apache/spark/master/examples/src/main/scripts/getGpusResources.sh\u00a0 echo '#!/usr/bin/env bash\n## Licensed to the Apache Software Foundation (ASF) under one or more# contributor license agreements. \u00a0See the NOTICE file distributed with# this work for additional information regarding copyright ownership.# The ASF licenses this file to You under the Apache License, Version 2.0# (the \"License\"); you may not use this file except in compliance with# the License. \u00a0You may obtain a copy of the License at\n## \u00a0 \u00a0http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.#NUM_MIG_DEVICES=$(nvidia-smi -L | grep MIG | wc -l)ADDRS=$(nvidia-smi --query-gpu=index --format=csv,noheader | sed -e '\\'':a'\\'' -e '\\''N'\\'' -e'\\''$!ba'\\'' -e '\\''s/\\n/\",\"/g'\\'')if [ $NUM_MIG_DEVICES -gt 0 ]; then\u00a0 MIG_INDEX=$(( $NUM_MIG_DEVICES - 1 ))\u00a0 ADDRS=$(seq -s '\\''\",\"'\\'' 0 $MIG_INDEX)fiecho {\\\"name\\\": \\\"gpu\\\", \\\"addresses\\\":[\\\"$ADDRS\\\"]}' > ${spark_gpu_script_dir}/getGpusResources.sh\u00a0 chmod a+rwx -R ${spark_gpu_script_dir}}function configure_gpu_isolation() {\u00a0 # enable GPU isolation\u00a0 sed -i \"s/yarn\\.nodemanager\\.linux\\-container\\-executor\\.group\\=.*$/yarn\\.nodemanager\\.linux\\-container\\-executor\\.group\\=yarn/g\" \"${HADOOP_CONF_DIR}/container-executor.cfg\"\u00a0 if [[ $IS_MIG_ENABLED -ne 0 ]]; then\u00a0 \u00a0 # configure the container-executor.cfg to have major caps\u00a0 \u00a0 printf '\\n[gpu]\\nmodule.enabled=true\\ngpu.major-device-number=%s\\n\\n[cgroups]\\nroot=/sys/fs/cgroup\\nyarn-hierarchy=yarn\\n' $MIG_MAJOR_CAPS >> \"${HADOOP_CONF_DIR}/container-executor.cfg\"\u00a0 \u00a0 printf 'export MIG_AS_GPU_ENABLED=1\\n' >> \"${HADOOP_CONF_DIR}/yarn-env.sh\"\u00a0 \u00a0 printf 'export ENABLE_MIG_GPUS_FOR_CGROUPS=1\\n' >> \"${HADOOP_CONF_DIR}/yarn-env.sh\"\u00a0 else\u00a0 \u00a0 printf '\\n[gpu]\\nmodule.enabled=true\\n[cgroups]\\nroot=/sys/fs/cgroup\\nyarn-hierarchy=yarn\\n' >> \"${HADOOP_CONF_DIR}/container-executor.cfg\"\u00a0 fi\u00a0 # Configure a systemd unit to ensure that permissions are set on restart\u00a0 cat >/etc/systemd/system/dataproc-cgroup-device-permissions.service<<EOF[Unit]Description=Set permissions to allow YARN to access device directories[Service]ExecStart=/bin/bash -c \"chmod a+rwx -R /sys/fs/cgroup/cpu,cpuacct; chmod a+rwx -R /sys/fs/cgroup/devices\"[Install]WantedBy=multi-user.targetEOF\u00a0 systemctl enable dataproc-cgroup-device-permissions\u00a0 systemctl start dataproc-cgroup-device-permissions}function main() {\u00a0 if [[ ${OS_NAME} != debian ]] && [[ ${OS_NAME} != ubuntu ]] && [[ ${OS_NAME} != rocky ]]; then\u00a0 \u00a0 echo \"Unsupported OS: '${OS_NAME}'\"\u00a0 \u00a0 exit 1\u00a0 fi\u00a0 if [[ ${OS_NAME} == debian ]] || [[ ${OS_NAME} == ubuntu ]]; then\u00a0 \u00a0 export DEBIAN_FRONTEND=noninteractive\u00a0 \u00a0 execute_with_retries \"apt-get update\"\u00a0 \u00a0 execute_with_retries \"apt-get install -y -q pciutils\"\u00a0 elif [[ ${OS_NAME} == rocky ]] ; then\u00a0 \u00a0 execute_with_retries \"dnf -y -q update\"\u00a0 \u00a0 execute_with_retries \"dnf -y -q install pciutils\"\u00a0 \u00a0 execute_with_retries \"dnf -y -q install kernel-devel-$(uname -r)\"\u00a0 \u00a0 execute_with_retries \"dnf -y -q install gcc\"\u00a0 fi\u00a0 # This configuration should be ran on all nodes\u00a0 # regardless if they have attached GPUs\u00a0 configure_yarn\u00a0 # Detect NVIDIA GPU\u00a0 if (lspci | grep -q NVIDIA); then\u00a0 \u00a0 # if this is called without the MIG script then the drivers are not installed\u00a0 \u00a0 if (/usr/bin/nvidia-smi --query-gpu=mig.mode.current --format=csv,noheader | uniq | wc -l); then\u00a0 \u00a0 \u00a0 NUM_MIG_GPUS=`/usr/bin/nvidia-smi --query-gpu=mig.mode.current --format=csv,noheader | uniq | wc -l`\u00a0 \u00a0 \u00a0 if [[ $NUM_MIG_GPUS -eq 1 ]]; then\u00a0 \u00a0 \u00a0 \u00a0 if (/usr/bin/nvidia-smi --query-gpu=mig.mode.current --format=csv,noheader | grep Enabled); then\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 IS_MIG_ENABLED=1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 NVIDIA_SMI_PATH='/usr/local/yarn-mig-scripts/'\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MIG_MAJOR_CAPS=`grep nvidia-caps /proc/devices | cut -d ' ' -f 1`\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fetch_mig_scripts\u00a0 \u00a0 \u00a0 \u00a0 fi\u00a0 \u00a0 \u00a0 fi\u00a0 \u00a0 fi\u00a0 \u00a0 if [[ ${OS_NAME} == debian ]] || [[ ${OS_NAME} == ubuntu ]]; then\u00a0 \u00a0 \u00a0 execute_with_retries \"apt-get install -y -q 'linux-headers-$(uname -r)'\"\u00a0 \u00a0 fi\u00a0 \u00a0 # if mig is enabled drivers would have already been installed\u00a0 \u00a0 if [[ $IS_MIG_ENABLED -eq 0 ]]; then\u00a0 \u00a0 \u00a0 install_nvidia_gpu_driver\u00a0 \u00a0 \u00a0 if [[ -n ${CUDNN_VERSION} ]]; then\u00a0 \u00a0 \u00a0 \u00a0 install_nvidia_nccl\u00a0 \u00a0 \u00a0 \u00a0 install_nvidia_cudnn\u00a0 \u00a0 \u00a0 fi\u00a0 \u00a0 \u00a0 #Install GPU metrics collection in Stackdriver if needed\u00a0 \u00a0 \u00a0 if [[ ${INSTALL_GPU_AGENT} == true ]]; then\u00a0 \u00a0 \u00a0 \u00a0 install_gpu_agent\u00a0 \u00a0 \u00a0 \u00a0 echo 'GPU metrics agent successfully deployed.'\u00a0 \u00a0 \u00a0 else\u00a0 \u00a0 \u00a0 \u00a0 echo 'GPU metrics agent will not be installed.'\u00a0 \u00a0 \u00a0 fi\u00a0 \u00a0 \u00a0 configure_gpu_exclusive_mode\u00a0 \u00a0 fi\u00a0 \u00a0 configure_yarn_nodemanager\u00a0 \u00a0 configure_gpu_script\u00a0 \u00a0 configure_gpu_isolation\u00a0 elif [[ \"${ROLE}\" == \"Master\" ]]; then\u00a0 \u00a0 configure_yarn_nodemanager\u00a0 \u00a0 configure_gpu_script\u00a0 fi\u00a0 # Restart YARN services if they are running already\u00a0 if [[ $(systemctl show hadoop-yarn-resourcemanager.service -p SubState --value) == 'running' ]]; then\u00a0 \u00a0 systemctl restart hadoop-yarn-resourcemanager.service\u00a0 fi\u00a0 if [[ $(systemctl show hadoop-yarn-nodemanager.service -p SubState --value) == 'running' ]]; then\u00a0 \u00a0 systemctl restart hadoop-yarn-nodemanager.service\u00a0 fi}main\n```\n## Verifying GPU driver install\nAfter you have finished installing the GPU driver on your Dataproc nodes, you can verify that the driver is functioning properly. SSH into the master node of your Dataproc cluster and run the following command:\n```\nnvidia-smi\n```\nIf the driver is functioning properly, the output will display the driver version and GPU statistics (see [Verifying the GPU driver install](/compute/docs/gpus/install-drivers-gpu#verify-driver-install) ).\n**Note:** The driver may not work correctly after a restart of the VM following a Linux [Unattended Upgrade](https://wiki.debian.org/UnattendedUpgrades) . Possible solutions: You can disable unattended upgrades or exclude kernel updates by editing the unattended upgrades service config.\n## Spark configuration\nWhen you [submit a job](/dataproc/docs/guides/submit-job) to Spark, you can use the `spark.executorEnv` Spark configuration [runtime environment property](https://spark.apache.org/docs/latest/configuration.html#runtime-environment) property with the `LD_PRELOAD` environment variable to preload needed libraries.\nExample:\n```\ngcloud dataproc jobs submit spark --cluster=CLUSTER_NAME \\\n\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0--class=org.apache.spark.examples.SparkPi \\\n\u00a0\u00a0--jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \\\n\u00a0\u00a0--properties=spark.executorEnv.LD_PRELOAD=libnvblas.so,spark.task.resource.gpu.amount=1,spark.executor.resource.gpu.amount=1,spark.executor.resource.gpu.discoveryScript=/usr/lib/spark/scripts/gpu/getGpusResources.sh\n```\n## Example GPU job\nYou can test GPUs on Dataproc by running any of the following jobs, which benefit when run with GPUs:\n- Run one of the [Spark ML examples](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/classification/MultilayerPerceptronClassifier.scala) .\n- Run the following example with`spark-shell`to run a matrix computation:\n```\nimport org.apache.spark.mllib.linalg._\nimport org.apache.spark.mllib.linalg.distributed._\nimport java.util.Random\ndef makeRandomSquareBlockMatrix(rowsPerBlock: Int, nBlocks: Int): BlockMatrix = {\n val range = sc.parallelize(1 to nBlocks)\n val indices = range.cartesian(range)\n return new BlockMatrix(\n  indices.map(\n   ij => (ij, Matrices.rand(rowsPerBlock, rowsPerBlock, new Random()))),\n  rowsPerBlock, rowsPerBlock, 0, 0)\n}\nval N = 1024 * 4\nval n = 2\nval mat1 = makeRandomSquareBlockMatrix(N, n)\nval mat2 = makeRandomSquareBlockMatrix(N, n)\nval mat3 = mat1.multiply(mat2)\nmat3.blocks.persist.count\nprintln(\"Processing complete!\")\n```\n## What's Next\n- [Compute Engine\u2192Attaching GPUs to instances](/compute/docs/gpus/add-gpus) \n- [Compute Engine\u2192GPUs on Compute Engine](/compute/docs/gpus)", "guide": "Dataproc"}