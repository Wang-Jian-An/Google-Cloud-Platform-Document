{"title": "Dataproc - Dataproc monitoring and troubleshooting tools", "url": "https://cloud.google.com/dataproc/docs/support/troubleshoot-monitor", "abstract": "# Dataproc - Dataproc monitoring and troubleshooting tools\n**Objective:** This page introduces you to the tools you can use to troubleshoot and monitor Dataproc clusters and jobs.\n", "content": "## Introduction\nDataproc is a fully managed and highly scalable service for running open-source distributed processing platforms such as Apache Hadoop, Apache Spark, Apache Flink, and Trino. You can use the files and tools discussed in the following sections to troubleshoot and monitor your Dataproc clusters and jobs.\n## Open source web interfaces\nMany Dataproc cluster open source components, such as Apache Hadoop and Apache Spark, provide web interfaces. These interfaces can be used to monitor cluster resources and job performance. For example, you can use the YARN Resource Manager UI to view YARN application resource allocation on a Dataproc cluster.\n: To enable access to component web interfaces available on a cluster, enable the [Dataproc Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) when you create a cluster.\n### Persistent History Server\nOpen Source web interfaces running on a cluster are available when the cluster is running, but they terminate when you delete the cluster. To view cluster and job data after a cluster is deleted, you can create a [Persistent History Server](/dataproc/docs/concepts/jobs/history-server) (PHS).\nExample: You encounter a job error or slowdown that you want to analyze. You stop or delete the job cluster, then view and analyze job history data using your PHS.\nAfter you create a PHS, you enable it on a Dataproc cluster or Dataproc Serverless batch workload when you create the cluster or submit the batch workload. A PHS can access history data for jobs run on multiple clusters, letting you monitor jobs across a project instead of monitoring separate UIs running on different clusters.\n## Dataproc logs\nDataproc collects the logs generated by Apache Hadoop, Spark, Hive, Zookeeper and other open source systems running on your clusters, and sends them to [Logging](/logging/docs) . These logs are grouped based on the source of logs, which allows you to select and view logs of interest to you: for example, YARN NodeManager and Spark Executor logs generated on a cluster are labelled separately. See [Dataproc logs](/dataproc/docs/guides/logging) for more information on Dataproc log contents and options.\n### Cloud Logging\nLogging is a fully-managed, real-time log management system. It provides storage for logs ingested from Google Cloud services and tools to search, filter, and analyze logs at scale. Dataproc clusters generate multiple logs, including [Dataproc service agent](/dataproc/docs/concepts/iam/dataproc-principals#service_agent_control_plane_identity) logs, cluster startup logs, and OSS component logs, such as YARN NodeManager logs.\nLogging is enabled by default on Dataproc clusters and Dataproc Serverless batch workloads. Logs are periodically exported to Logging, where they persist after the cluster is deleted or the workload is completed.\n## Dataproc metrics\n[Dataproc cluster and job metrics](/dataproc/docs/guides/dataproc-metrics#dataproc_resource_metric_collection) , prefixed with `dataproc.googleapis.com/` , consist of time-series data that provide insights into the performance of a cluster, such as CPU utilization or job status. Dataproc [custom metrics](/dataproc/docs/guides/dataproc-metrics#custom_metric_collection) , prefixed with `custom.googleapis.com/` , include metrics emitted by open source systems running on the cluster, such as the YARN `running applications` metric. Gaining insight into Dataproc metrics can help you configure your clusters efficiently. Setting up metric-based alerts can help you recognize and respond to problems quickly.\nUse the Google Cloud console to view [Dataproc metrics](/dataproc/docs/guides/dataproc-metrics) from the [Metrics Explorer](https://console.cloud.google.com/monitoring/metrics-explorer) in Monitoring or from the **Monitoring** tab on the Dataproc **Cluster details** page.\nDataproc cluster and job metrics are collected by default without charge. The collection of [custom metrics](/dataproc/docs/guides/dataproc-metrics#custom_metric_collection) is [charged to customers](/stackdriver/pricing#metrics-chargeable) . You can [enable the collection of custom metrics](/dataproc/docs/guides/dataproc-metrics#enable_custom_metric_collection) when you create a cluster. The collection of Dataproc Serverless [Spark metrics](/dataproc-serverless/docs/concepts/metrics#available_spark_metrics) is enabled by default on Spark batch workloads.\n## Cloud Monitoring\n[Monitoring](/monitoring/docs) uses cluster metadata and metrics, including HDFS, YARN, job, and operation metrics, to provide visibility into the health, performance, and availability of Dataproc clusters and jobs. You can use Monitoring to explore metrics, add charts, build dashboards, and create alerts.\n### Metrics Explorer\nYou can use the [Metrics Explorer](https://console.cloud.google.com/monitoring/metrics-explorer) to view [Dataproc metrics](/dataproc/docs/guides/dataproc-metrics) . Dataproc cluster, job, and serverless batch metrics are listed under the `Cloud Dataproc Cluster` , `Cloud Dataproc Job` , and `Cloud Dataproc Batch` resources. Dataproc custom metrics are listed under the `VM Instances` resource, `Custom` category.\n### Charts\nYou can use Metrics Explorer to [create charts](/monitoring/charts/metrics-explorer) that visualize Dataproc metrics.\nExample: You create a chart to see the number of active Yarn applications running on your clusters, and then add a filter to select visualized metrics by cluster name or region.\n### Dashboards\nYou can [build dashboards](/dataproc/docs/guides/dataproc-metrics#build_a_dashboard) to monitor Dataproc clusters and jobs using metrics from multiple projects and different Google Cloud products. You can build dashboards in the Google Cloud console from the [Dashboards Overview](https://console.cloud.google.com/monitoring/dashboards) page by clicking, creating, and then saving a chart from the [Metrics Explorer](https://console.cloud.google.com/monitoring/metrics-explorer) page.\n### Alerts\nYou can create [Dataproc metric alerts](/dataproc/docs/guides/dataproc-alerts) to receive timely notice of cluster or job issues.\n## For more information\nFor additional guidance, see\n- [Troubleshoot Dataproc error messages](/dataproc/docs/support/troubleshoot-errors) \n- [Diagnose Dataproc clusters](/dataproc/docs/support/diagnose-command) \n- [Dataproc FAQ ](/dataproc/docs/resources/faq)", "guide": "Dataproc"}