{"title": "Dataproc - Dataproc principals", "url": "https://cloud.google.com/dataproc/docs/concepts/iam/dataproc-principals", "abstract": "# Dataproc - Dataproc principals\nWhen you use the Dataproc service to create clusters and run jobs on your clusters, the service sets up the necessary [Dataproc roles and permissions](/dataproc/docs/concepts/iam/iam) in your project to access and use the Google Cloud resources it needs to accomplish these tasks. However, if you do cross-project work, for example to access data in another project, you will need to set up the necessary roles and permissions to access cross-project resources.\nTo help you do cross-project work successfully, this document lists the different principals that use the Dataproc service and the roles that contain the necessary permissions for those principals to access and use Google Cloud resources.\nThere are three principals (identities) that access and use the Dataproc:\n- User Identity\n- Control Plane Identity\n- Data Plane Identity\n", "content": "## Dataproc API User (User identity)\nExample:\nThis is the user that calls the Dataproc service to create clusters, submit jobs, and make other requests to the service. The user is usually an individual, but it can also be a [service account](/iam/docs/understanding-service-accounts) if Dataproc is invoked through an API client or from another Google Cloud service such as Compute Engine, Cloud Functions, or Cloud Composer.\n**Related roles**\n- [Dataproc roles](/dataproc/docs/concepts/iam/iam#roles) , [Project roles](/dataproc/docs/concepts/iam/iam#project_roles) \n**Notes**\n- Dataproc API-submitted jobs run as`root`on Linux.\n- Dataproc clusters inherit project-wide Compute Engine SSH metadata unless explicitly blocked by setting `--metadata=block-project-ssh-keys=true` when you create your cluster (see [Cluster metadata](/dataproc/docs/concepts/configuring-clusters/metadata) ).\n- HDFS user directories are created for each project-level SSH user. These HDFS directories are created at cluster deployment time, and a new (post-deployment) SSH user is not given an HDFS directory on existing clusters.## Dataproc Service Agent (Control Plane identity)\nExample:\nThe Dataproc [Dataproc Service Agent service account](/dataproc/docs/concepts/configuring-clusters/service-accounts#service_agent_account) is used to perform a broad set of system operations on resources located in the project where a Dataproc cluster is created, including:\n- Creation of Compute Engine resources, including VM instances, instance groups, and instance templates\n- `get`and`list`operations to confirm the configuration of resources such as images, firewalls, Dataproc initialization actions, and Cloud Storage buckets\n- Auto-creation of the Dataproc [staging and temp buckets](/dataproc/docs/concepts/configuring-clusters/staging-bucket) if the staging or temp bucket is not specified by the user\n- Writing cluster configuration metadata to the staging bucket\n- Accessing [VPC networks in a host project](/dataproc/docs/concepts/configuring-clusters/network#create_a_cluster_that_uses_a_network_in_another_project) \n**Related roles**\n- [Dataproc Service Agent](/iam/docs/understanding-roles#dataproc.serviceAgent) ## Dataproc VM Service Account (Data Plane identity)\nExample:\nYour application code runs as the [VM service account](/dataproc/docs/concepts/configuring-clusters/service-accounts#VM_service_account) on Dataproc VMs. User jobs are granted the roles (with their associated permissions) of this service account.\nThe VM service account:\n- communicates with the [Dataproc control plane](#service_agent_control_plane_identity) \n- reads and writes data from and to the [Dataproc staging and temp buckets](/dataproc/docs/concepts/configuring-clusters/staging-bucket) \n- As needed by your Dataproc jobs, reads and writes data from and to Cloud Storage, BigQuery, Cloud Logging, and other Google Cloud resources.\n**Related roles**\n- [Dataproc Worker](/dataproc/docs/concepts/iam/iam#roles) \n- [Cloud Storage roles](/storage/docs/access-control/iam-roles) \n- [BigQuery roles](/bigquery/docs/access-control#roles) \n**Note:** For interactive workloads, users can opt to use their user identity to  access Cloud Storage objects in buckets owned by the same project  that contains the cluster (see [Dataproc Personal Cluster Authentication](/dataproc/docs/concepts/iam/personal-auth) ).\n## For more information\n- [Dataproc roles and permissions](/dataproc/docs/concepts/iam/iam) \n- [Service Accounts](/dataproc/docs/concepts/configuring-clusters/service-accounts) \n- [BigQuery Access Control](/bigquery/docs/access-control) \n- [Cloud Storage Access Control Options](/storage/docs/access-control)", "guide": "Dataproc"}