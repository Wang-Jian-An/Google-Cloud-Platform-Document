{"title": "Dataproc - Use Apache Spark with HBase on Dataproc", "url": "https://cloud.google.com/dataproc/docs/tutorials/spark-hbase", "abstract": "# Dataproc - Use Apache Spark with HBase on Dataproc\n**Deprecated:** Starting with Dataproc [version 2.1](/dataproc/docs/concepts/versioning/dataproc-release-2.1) , you can no longer use the optional HBase component. Dataproc [version 1.5](/dataproc/docs/concepts/versioning/dataproc-release-1.5) and Dataproc [version 2.0](/dataproc/docs/concepts/versioning/dataproc-release-2.0) offer a Beta version of HBase with no support. However, due to the ephemeral nature of Dataproc clusters, using HBase is not recommended.\n", "content": "## ObjectivesThis tutorial shows you how to:- Create a Dataproc cluster, installing Apache HBase and Apache ZooKeeper on the cluster\n- Create an HBase table using the HBase shell running on the master node of the Dataproc cluster\n- Use Cloud Shell to submit a Java or PySpark Spark job to the Dataproc service that writes data to, then reads data from, the HBase table\n## CostsIn this document, you use the following billable components of Google Cloud:- [Dataproc](/dataproc/pricing) \n- [Compute Engine](/compute/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . ## Before you beginIf you haven't already done so, create a Google Cloud Platform project.\n## Create a Dataproc cluster\n- Run the following command in a [Cloud Shell](/shell/docs) session terminal to:- Install the [HBase](/dataproc/docs/concepts/components/hbase) and [ZooKeeper](/dataproc/docs/concepts/components/zookeeper) components\n- Provision three worker nodes (three to five workers are recommended to run the code in this tutorial)\n- Enable the [Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) \n- Use image version 2.0\n- Use the`--properties`flag to add the HBase config and HBase library to the Spark driver and executor classpaths.\n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=HBASE,ZOOKEEPER \\\n\u00a0\u00a0\u00a0\u00a0--num-workers=3 \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway \\\n\u00a0\u00a0\u00a0\u00a0--image-version=2.0 \\\n\u00a0\u00a0\u00a0\u00a0--properties='spark:spark.driver.extraClassPath=/etc/hbase/conf:/usr/lib/hbase/*,spark:spark.executor.extraClassPath=/etc/hbase/conf:/usr/lib/hbase/*'\n```\n### Verify connector installation\n- From the Google Cloud console or a Cloud Shell session terminal, [SSH into the Dataproc cluster master node](/dataproc/docs/concepts/accessing/ssh) .\n- Verify the installation of the [Apache HBase Spark connector](https://github.com/apache/hbase-connectors/tree/master/spark) on the master node:```\nls -l /usr/lib/spark/jars | grep hbase-spark\n```Sample output:```\n-rw-r--r-- 1 root root size date time hbase-spark-connector.version.jar\n```\n- Keep the SSH session terminal open to:- [Create an HBase table](#create_an_hbase_table) \n- (Java users): [run commands](#dataproc_hbase_tutorial-java) on the master node of the cluster to determine the versions of components installed on the cluster\n- [Scan your Hbase table](#scan_the_hbase_table) after you [run the code](#run_the_code) ## Create an HBase tableRun the commands listed in this section in the master node SSH session terminal that you opened in the previous step.- Open the HBase shell:```\nhbase shell\n```\n- Create an HBase 'my-table' with a 'cf' column family:```\ncreate 'my_table','cf'\n```- To confirm table creation, in the Google Cloud console, click **HBase** in the [Google Cloud console Component Gateway links](/dataproc/docs/concepts/accessing/dataproc-gateways#viewing_and_accessing_component_gateway_urls) to open the Apache HBase UI.`my-table`is listed in the **Tables** section on the **Home** page.## View the Spark code [  spark-hbase/src/main/java/hbase/SparkHBaseMain.java ](https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/HEAD/spark-hbase/src/main/java/hbase/SparkHBaseMain.java) [View on GitHub](https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/HEAD/spark-hbase/src/main/java/hbase/SparkHBaseMain.java) \n```\npackage hbase;import org.apache.hadoop.hbase.spark.datasources.HBaseTableCatalog;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;import org.apache.spark.sql.SparkSession;import java.io.Serializable;import java.util.Arrays;import java.util.HashMap;import java.util.Map;public class SparkHBaseMain {\u00a0 \u00a0 public static class SampleData implements Serializable {\u00a0 \u00a0 \u00a0 \u00a0 private String key;\u00a0 \u00a0 \u00a0 \u00a0 private String name;\u00a0 \u00a0 \u00a0 \u00a0 public SampleData(String key, String name) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.key = key;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.name = name;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 public SampleData() {\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 public String getName() {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return name;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 public void setName(String name) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.name = name;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 public String getKey() {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return key;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 public void setKey(String key) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.key = key;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 \u00a0 public static void main(String[] args) {\u00a0 \u00a0 \u00a0 \u00a0 // Init SparkSession\u00a0 \u00a0 \u00a0 \u00a0 SparkSession spark = SparkSession\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .builder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .master(\"yarn\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .appName(\"spark-hbase-tutorial\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .getOrCreate();\u00a0 \u00a0 \u00a0 \u00a0 // Data Schema\u00a0 \u00a0 \u00a0 \u00a0 String catalog = \"{\"+\"\\\"table\\\":{\\\"namespace\\\":\\\"default\\\", \\\"name\\\":\\\"my_table\\\"},\" +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\\"rowkey\\\":\\\"key\\\",\" +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\\"columns\\\":{\" +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\\"key\\\":{\\\"cf\\\":\\\"rowkey\\\", \\\"col\\\":\\\"key\\\", \\\"type\\\":\\\"string\\\"},\" +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\\"name\\\":{\\\"cf\\\":\\\"cf\\\", \\\"col\\\":\\\"name\\\", \\\"type\\\":\\\"string\\\"}\" +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"}\" +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"}\";\u00a0 \u00a0 \u00a0 \u00a0 Map<String, String> optionsMap = new HashMap<String, String>();\u00a0 \u00a0 \u00a0 \u00a0 optionsMap.put(HBaseTableCatalog.tableCatalog(), catalog);\u00a0 \u00a0 \u00a0 \u00a0 Dataset<Row> ds= spark.createDataFrame(Arrays.asList(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new SampleData(\"key1\", \"foo\"),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new SampleData(\"key2\", \"bar\")), SampleData.class);\u00a0 \u00a0 \u00a0 \u00a0 // Write to HBase\u00a0 \u00a0 \u00a0 \u00a0 ds.write()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .format(\"org.apache.hadoop.hbase.spark\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .options(optionsMap)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .option(\"hbase.spark.use.hbasecontext\", \"false\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .mode(\"overwrite\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .save();\u00a0 \u00a0 \u00a0 \u00a0 // Read from HBase\u00a0 \u00a0 \u00a0 \u00a0 Dataset dataset = spark.read()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .format(\"org.apache.hadoop.hbase.spark\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .options(optionsMap)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .option(\"hbase.spark.use.hbasecontext\", \"false\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .load();\u00a0 \u00a0 \u00a0 \u00a0 dataset.show();\u00a0 \u00a0 }}\n``` [  spark-hbase/scripts/pyspark-hbase.py ](https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/HEAD/spark-hbase/scripts/pyspark-hbase.py) [View on GitHub](https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/HEAD/spark-hbase/scripts/pyspark-hbase.py) \n```\nfrom pyspark.sql import SparkSession# Initialize Spark Sessionspark = SparkSession \\\u00a0 .builder \\\u00a0 .master('yarn') \\\u00a0 .appName('spark-hbase-tutorial') \\\u00a0 .getOrCreate()data_source_format = ''# Create some test datadf = spark.createDataFrame(\u00a0 \u00a0 [\u00a0 \u00a0 \u00a0 \u00a0 (\"key1\", \"foo\"),\u00a0 \u00a0 \u00a0 \u00a0 (\"key2\", \"bar\"),\u00a0 \u00a0 ],\u00a0 \u00a0 [\"key\", \"name\"])# Define the schema for catalogcatalog = ''.join(\"\"\"{\u00a0 \u00a0 \"table\":{\"namespace\":\"default\", \"name\":\"my_table\"},\u00a0 \u00a0 \"rowkey\":\"key\",\u00a0 \u00a0 \"columns\":{\u00a0 \u00a0 \u00a0 \u00a0 \"key\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\u00a0 \u00a0 \u00a0 \u00a0 \"name\":{\"cf\":\"cf\", \"col\":\"name\", \"type\":\"string\"}\u00a0 \u00a0 }}\"\"\".split())# Write to HBasedf.write.format('org.apache.hadoop.hbase.spark').options(catalog=catalog).option(\"hbase.spark.use.hbasecontext\", \"false\").mode(\"overwrite\").save()# Read from HBaseresult = spark.read.format('org.apache.hadoop.hbase.spark').options(catalog=catalog).option(\"hbase.spark.use.hbasecontext\", \"false\").load()result.show()\n```\n## Run the code\n- Open a [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) session terminal. **Note:** Run the commands listed in this section in a Cloud Shell session terminal. Cloud Shell has the tools required by this tutorial pre-installed, including [gcloud CLI](/sdk/gcloud) , [git](https://git-scm.com/) , [Apache Maven](https://maven.apache.org/download.cgi) , [Java](https://www.java.com/en/) , and [Python](https://www.python.org/) , plus [other tools](/shell/docs/how-cloud-shell-works#tools) .\n- Clone the GitHub [GoogleCloudDataproc/cloud-dataproc](https://github.com/GoogleCloudDataproc/cloud-dataproc) repository into your Cloud Shell session terminal:```\ngit clone https://github.com/GoogleCloudDataproc/cloud-dataproc.git\n```\n- Change to the `cloud-dataproc/spark-hbase` directory:```\ncd cloud-dataproc/spark-hbase\n```Sample output:```\nuser-name@cloudshell:~/cloud-dataproc/spark-hbase (project-id)$\n```\n- Submit the Dataproc job.\n- Set component versions in`pom.xml`file.- The Dataproc [2.0.x release versions](/dataproc/docs/concepts/versioning/dataproc-release-2.0) page lists the Scala, Spark, and HBase component versions installed with the most recent and last four image 2.0 subminor versions.- To find the subminor version of your 2.0 image version cluster, click the cluster name on the [Clusters](https://console.cloud.google.com/dataproc/clusters) page in the Google Cloud console to open the **Cluster details** page, where the cluster **Image version** is listed.\n- Alternatively, you can run the following commands in an [SSH session terminal](/dataproc/docs/concepts/accessing/ssh) from the master node of your cluster to determine component versions:- Check scala version:```\nscala -version\n```\n- Check Spark version (control-D to exit):```\nspark-shell\n```\n- Check HBase version:```\nhbase version\n```\n- Identify the Spark, Scala, and HBase version dependencies in the Maven [pom.xml](https://github.com/apache/hbase-connectors/blob/master/spark/pom.xml) :```\n<properties>\n\u00a0\u00a0<scala.version>scala full version (for example, 2.12.14)</scala.version>\n\u00a0\u00a0<scala.main.version>scala main version (for example, 2.12)</scala.main.version>\n\u00a0\u00a0<spark.version>spark version (for example, 3.1.2)</spark.version>\n\u00a0\u00a0<hbase.client.version>hbase version (for example, 2.2.7)</hbase.client.version>\n\u00a0\u00a0<hbase-spark.version>1.0.0(the current Apache HBase Spark Connector version)>\n</properties>\n```Note: The`hbase-spark.version`is the current Spark HBase connector version; leave this version number unchanged.\n- Edit the`pom.xml`file in the Cloud Shell editor to insert the the correct Scala, Spark, and HBase version numbers. Click **Open Terminal** when you finish editing to return to the Cloud Shell terminal command line.```\ncloudshell edit .\n```\n- Switch to Java 8 in Cloud Shell. This JDK version is needed to build the code (you can ignore any plugin warning messages):```\nsudo update-java-alternatives -s java-1.8.0-openjdk-amd64 && export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n```\n- Verify Java 8 installation:```\njava -version\n```Sample output:```\nopenjdk version \"1.8...\"\n \n```\n- Build the`jar`file:```\nmvn clean package\n```The`.jar`file is placed in the`/target`subdirectory (for example, **target/spark-hbase-1.0-SNAPSHOT.jar** .\n- Submit the job.```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--class=hbase.SparkHBaseMain \\\n\u00a0\u00a0\u00a0\u00a0--jars=target/filename.jar \\\n\u00a0\u00a0\u00a0\u00a0--region=cluster-region \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name\n```- `--jars`: Insert the name of your`.jar`file after \"target/\" and before \".jar\".\n- If you did not set the Spark driver and executor HBase classpaths when you [created your cluster](#create_a_cluster) ,  you must set them with each job submission by including the  following`\u2011\u2011properties`flag in you job submit command:```\n--properties='spark.driver.extraClassPath=/etc/hbase/conf:/usr/lib/hbase/*,spark.executor.extraClassPath=/etc/hbase/conf:/usr/lib/hbase/*'\n  \n```\n- View HBase table output in the Cloud Shell session terminal output:```\nWaiting for job output...\n...\n+----+----+\n| key|name|\n+----+----+\n|key1| foo|\n|key2| bar|\n+----+----+\n```\n- Submit the job.```\ngcloud dataproc jobs submit pyspark scripts/pyspark-hbase.py \\\n\u00a0\u00a0\u00a0\u00a0--region=cluster-region \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name\n```- If you did not set the Spark driver and executor HBase classpaths when you [created your cluster](#create_a_cluster) ,  you must set them with each job submission by including the  following`\u2011\u2011properties`flag in you job submit command:```\n--properties='spark.driver.extraClassPath=/etc/hbase/conf:/usr/lib/hbase/*,spark.executor.extraClassPath=/etc/hbase/conf:/usr/lib/hbase/*'\n  \n```\n- View HBase table output in the Cloud Shell session terminal output:```\nWaiting for job output...\n...\n+----+----+\n| key|name|\n+----+----+\n|key1| foo|\n|key2| bar|\n+----+----+\n```\n### Scan the HBase tableYou can scan the content of your HBase table by running the following commands in the master node SSH session terminal that you opened in [Verify connector installation](#verify_connector_installation) :- Open the HBase shell:```\nhbase shell\n```\n- Scan 'my-table':```\nscan 'my_table'\n```Sample output:```\nROW    COLUMN+CELL\n\u00a0key1    column=cf:name, timestamp=1647364013561, value=foo\n\u00a0key2    column=cf:name, timestamp=1647364012817, value=bar\n2 row(s)\nTook 0.5009 seconds\n```## Clean up\nAfter you finish the tutorial, you can clean up the resources that you created so that they stop using quota and incurring charges. The following sections describe how to delete or turn off these resources.\n### Delete the project\nThe easiest way to eliminate billing is to delete the project that you created for the tutorial.\nTo delete the project:\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.### Delete the cluster\n- To delete your cluster:```\ngcloud dataproc clusters delete cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION}\n```", "guide": "Dataproc"}