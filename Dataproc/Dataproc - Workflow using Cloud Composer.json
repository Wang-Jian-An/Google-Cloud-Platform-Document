{"title": "Dataproc - Workflow using Cloud Composer", "url": "https://cloud.google.com/dataproc/docs/tutorials/workflow-composer", "abstract": "# Dataproc - Workflow using Cloud Composer\n**Objective:** - Create a Dataproc workflow template that runs a [Spark PI job](https://spark.apache.org/examples.html) - - Create an Apache Airflow DAG that [Cloud Composer](/composer/docs) will use to start the workflow at a specific time.\nIn this document, you use the following billable components of Google Cloud:\n- Dataproc\n- Compute Engine\n- Cloud Composer\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) .\n", "content": "## Before you begin\n### Set up your project\n## Create a Dataproc workflow template\nCopy and run the commands listed below in a local terminal window or in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) to create and define a [workflow template](/dataproc/docs/concepts/workflows/using-workflows) .\n- Create the`sparkpi`workflow template.```\ngcloud dataproc workflow-templates create sparkpi \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central1\n  \n```\n- Add the spark job to the`sparkpi`workflow template. The  \"compute\"`step-id`flag identifies the SparkPi job.```\ngcloud dataproc workflow-templates add-job spark \\\n\u00a0\u00a0\u00a0\u00a0--workflow-template=sparkpi \\\n\u00a0\u00a0\u00a0\u00a0--step-id=compute \\\n\u00a0\u00a0\u00a0\u00a0--class=org.apache.spark.examples.SparkPi \\\n\u00a0\u00a0\u00a0\u00a0--jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central1 \\\n\u00a0\u00a0\u00a0\u00a0-- 1000\n  \n```\n- Use a [managed](/dataproc/docs/concepts/workflows/overview#managed_cluster) , [single-node](/dataproc/docs/concepts/configuring-clusters/single-node-clusters) cluster to run the workflow. Dataproc will create the cluster, run the workflow on it, then delete the cluster when the workflow completes.```\ngcloud dataproc workflow-templates set-managed-cluster sparkpi \\\n\u00a0\u00a0\u00a0\u00a0--cluster-name=sparkpi \\\n\u00a0\u00a0\u00a0\u00a0--single-node \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central1\n  \n```\n- Confirm workflow template creation.\nClick on the `sparkpi` name on the Dataproc [Workflows](https://console.cloud.google.com/dataproc/workflows/templates) page in the Google Cloud console to open the **Workflow template details** page. Click on the name of your  workflow template to confirm the `sparkpi` template attributes.Run the following command:\n```\ngcloud dataproc workflow-templates describe sparkpi --region=us-central1\n \n```\n## Create and Upload a DAG to Cloud Storage\n- Create or use an existing [Cloud Composer environment](/composer/docs/how-to/managing/creating#creating_a_new_environment) .\n- Set environment variables.- In the toolbar, click **Admin > Variables** .\n- Click **Create** .\n- Enter the following information:- Key:`project_id`\n- Val:\u2014 your Google Cloud Project ID- Click **Save** .\nEnter the following commands:- `ENVIRONMENT`is the name of the Cloud Composer   environment\n- `LOCATION`is the region where the Cloud Composer   environment is located\n- `PROJECT_ID`is the project ID for the project that contains the Cloud Composer environment\n```\n gcloud composer environments run ENVIRONMENT --location LOCATION variables set -- project_id PROJECT_ID\n \n```\n- Copy the following DAG code locally into a file titled \"composer-dataproc-dag.py\", which uses the [DataprocInstantiateWorkflowTemplateOperator](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html#airflow.providers.google.cloud.operators.dataproc.DataprocInstantiateWorkflowTemplateOperator) . [](None) \n [  composer/workflows/dataproc_workflow_template_instantiate_operator_tutorial.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/composer/workflows/dataproc_workflow_template_instantiate_operator_tutorial.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/composer/workflows/dataproc_workflow_template_instantiate_operator_tutorial.py) \n```\n\"\"\"Example Airflow DAG that kicks off a Cloud Dataproc Template that runs aSpark Pi Job.This DAG relies on an Airflow variablehttps://airflow.apache.org/docs/apache-airflow/stable/concepts/variables.html* project_id - Google Cloud Project ID to use for the Cloud Dataproc Template.\"\"\"import datetimefrom airflow import modelsfrom airflow.providers.google.cloud.operators.dataproc import (\u00a0 \u00a0 DataprocInstantiateWorkflowTemplateOperator,)from airflow.utils.dates import days_agoproject_id = \"{{var.value.project_id}}\"default_args = {\u00a0 \u00a0 # Tell airflow to start one day ago, so that it runs as soon as you upload it\u00a0 \u00a0 \"start_date\": days_ago(1),\u00a0 \u00a0 \"project_id\": project_id,}# Define a DAG (directed acyclic graph) of tasks.# Any task you create within the context manager is automatically added to the# DAG object.with models.DAG(\u00a0 \u00a0 # The id you will see in the DAG airflow page\u00a0 \u00a0 \"dataproc_workflow_dag\",\u00a0 \u00a0 default_args=default_args,\u00a0 \u00a0 # The interval with which to schedule the DAG\u00a0 \u00a0 schedule_interval=datetime.timedelta(days=1), \u00a0# Override to match your needs) as dag:\u00a0 \u00a0 start_template_job = DataprocInstantiateWorkflowTemplateOperator(\u00a0 \u00a0 \u00a0 \u00a0 # The task id of your job\u00a0 \u00a0 \u00a0 \u00a0 task_id=\"dataproc_workflow_dag\",\u00a0 \u00a0 \u00a0 \u00a0 # The template id of your workflow\u00a0 \u00a0 \u00a0 \u00a0 template_id=\"sparkpi\",\u00a0 \u00a0 \u00a0 \u00a0 project_id=project_id,\u00a0 \u00a0 \u00a0 \u00a0 # The region for the template\u00a0 \u00a0 \u00a0 \u00a0 region=\"us-central1\",\u00a0 \u00a0 )\n``` [  composer/airflow_1_samples/dataproc_workflow_template_instantiate_operator_tutorial.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/composer/airflow_1_samples/dataproc_workflow_template_instantiate_operator_tutorial.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/composer/airflow_1_samples/dataproc_workflow_template_instantiate_operator_tutorial.py) \n```\n\"\"\"Example Airflow DAG that kicks off a Cloud Dataproc Template that runs aSpark Pi Job.This DAG relies on an Airflow variablehttps://airflow.apache.org/docs/apache-airflow/stable/concepts/variables.html* project_id - Google Cloud Project ID to use for the Cloud Dataproc Template.\"\"\"import datetimefrom airflow import modelsfrom airflow.contrib.operators import dataproc_operatorfrom airflow.utils.dates import days_agoproject_id = \"{{var.value.project_id}}\"default_args = {\u00a0 \u00a0 # Tell airflow to start one day ago, so that it runs as soon as you upload it\u00a0 \u00a0 \"start_date\": days_ago(1),\u00a0 \u00a0 \"project_id\": project_id,}# Define a DAG (directed acyclic graph) of tasks.# Any task you create within the context manager is automatically added to the# DAG object.with models.DAG(\u00a0 \u00a0 # The id you will see in the DAG airflow page\u00a0 \u00a0 \"dataproc_workflow_dag\",\u00a0 \u00a0 default_args=default_args,\u00a0 \u00a0 # The interval with which to schedule the DAG\u00a0 \u00a0 schedule_interval=datetime.timedelta(days=1), \u00a0# Override to match your needs) as dag:\u00a0 \u00a0 start_template_job = dataproc_operator.DataprocWorkflowTemplateInstantiateOperator(\u00a0 \u00a0 \u00a0 \u00a0 # The task id of your job\u00a0 \u00a0 \u00a0 \u00a0 task_id=\"dataproc_workflow_dag\",\u00a0 \u00a0 \u00a0 \u00a0 # The template id of your workflow\u00a0 \u00a0 \u00a0 \u00a0 template_id=\"sparkpi\",\u00a0 \u00a0 \u00a0 \u00a0 project_id=project_id,\u00a0 \u00a0 \u00a0 \u00a0 # The region for the template\u00a0 \u00a0 \u00a0 \u00a0 # For more info on regions where Dataflow is available see:\u00a0 \u00a0 \u00a0 \u00a0 # https://cloud.google.com/dataflow/docs/resources/locations\u00a0 \u00a0 \u00a0 \u00a0 region=\"us-central1\",\u00a0 \u00a0 )\n```\n- [Upload yourDAG](/composer/docs/how-to/using/managing-dags) to your environment folder in Cloud Storage. After the upload has been completed successfully, click on the **DAGs Folder** link on the Cloud Composer Environment's page.\n### Viewing a task's status\n- Open the [Airflow web interface](/composer/docs/how-to/accessing/airflow-web-interface#accessing_the_web_interface) .\n- On the DAGs page, click the DAG name (for example,`dataproc_workflow_dag`).\n- On the DAGs Details page, click **Graph View** .\n- Check status:- Failed: The task has a red box around it.  You can also hold the pointer over task and look for **State: Failed** .\n- Success: The task has a green box around it.  You can also hold the pointer over the task and check for **State: Success** .Click the Workflows tab to see workflow status.```\ngcloud dataproc operations list \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central1 \\\n\u00a0\u00a0\u00a0\u00a0--filter=\"labels.goog-dataproc-workflow-template-id=sparkpi\"\n \n```\n## Cleaning up\nTo avoid incurring charges to your Google Cloud account, you can delete the resources used in this tutorial:\n- [Delete the Cloud Composer environment.](https://console.cloud.google.com/composer/environments) \n- [Delete the workflow template.](https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#deleting_a_workflow_template) ## What's next\n- See [Overview of Dataproc Workflow Templates](/dataproc/docs/concepts/workflows/overview) \n- See [Workflow scheduling solutions](/dataproc/docs/concepts/workflows/workflow-schedule-solutions)", "guide": "Dataproc"}