{"title": "Dataproc - Dataproc Component Gateway", "url": "https://cloud.google.com/dataproc/docs/concepts/accessing/dataproc-gateways", "abstract": "# Dataproc - Dataproc Component Gateway\nSome of the default open source components included with Google Dataproc clusters, such as [Apache Hadoop](https://hadoop.apache.org/) and [Apache Spark](http://spark.apache.org/) , provide [web interfaces](/dataproc/docs/concepts/accessing/cluster-web-interfaces#available_interfaces_) . These interfaces can be used to manage and monitor cluster resources and facilities, such as the YARN resource manager, the Hadoop Distributed File System (HDFS), MapReduce, and Spark. Component Gateway provides secure access to web endpoints for Dataproc default and [optional components](/dataproc/docs/concepts/components/overview#available_optional_components) .\nClusters created with [supported Dataproc image versions](/dataproc/docs/concepts/versioning/dataproc-version-clusters#supported_dataproc_versions) can enable access to component web interfaces without relying on [SSH tunnels](/dataproc/docs/concepts/accessing/cluster-web-interfaces#create_an_ssh_tunnel) or [modifying firewall rules](/dataproc/docs/concepts/configuring-clusters/network) to allow inbound traffic.\n", "content": "## Considerations\n- Component web interfaces can be accessed by users who have [dataproc.clusters.use](/dataproc/docs/concepts/iam/iam#clusters_permissions) IAM permission. See [Dataproc Roles](/dataproc/docs/concepts/iam/iam#roles) .\n- Component Gateway can be used to access REST APIs, such as [Apache Hadoop YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) and [Apache Livy](https://livy.incubator.apache.org/) , and history servers.\n- When Component Gateway is enabled, Dataproc adds the following services to the cluster's first master node:- [Apache Knox](https://knox.apache.org/) . The default Knox Gateway SSL Certificate is valid for 13 months from the cluster creation date. If it expires, all Component Gateway web interface URls become inactive. To obtain a new certificate, see [How to regenerate the Component Gateway SSL certificate](#how_to_regenerate_the_component_gateway_ssl_certificate) .\n- [Inverting Proxy](https://github.com/google/inverting-proxy) \n- Component gateway does not enable direct access to`node:port`interfaces, but proxies a specific subset of services automatically. If you want to access services on nodes (`node:port`), use an [SSH SOCKS proxy](/dataproc/docs/concepts/accessing/cluster-web-interfaces#create_an_ssh_tunnel) .## Create a cluster with Component Gateway\nTo enable Component Gateway from the Google Cloud console, check the  Component Gateway checkbox in the Components section of the  Set up cluster panel on the Dataproc [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page.Run the gcloud CLI [gcloud dataproc clusters create](/sdk/gcloud/reference/dataproc/clusters/create) command locally in a terminal window or in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) .\n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0other args ...\n```Set the [EndpointConfig.enableHttpPortAccess](/dataproc/docs/reference/rest/v1/ClusterConfig#EndpointConfig) property to `true` as part of a [clusters.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) request.\n**Note:** You can also create a Component Gateway-enabled Dataproc cluster with third party provisioning software, such as HashiCorp's Terraform [google_dataproc_cluster](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/dataproc_cluster#nested_endpoint_config) provider.\n## Use Component Gateway URLs to access web interfaces\nWhen Component Gateway is enabled on a cluster, you can connect to component web interfaces running on the cluster's first master node by clicking links provided on the Google Cloud console. Component Gateway also sets [endpointConfig.httpPorts](/dataproc/docs/reference/rest/v1/ClusterConfig#EndpointConfig) with a map of port names to URLs. As an alternative to using the console, you can use the `gcloud` command-line tool or the Dataproc REST API to view this mapping information, then copy and paste the URL into your browser to connect with the component's UI.\nNavigate to the Dataproc [Clusters](https://console.cloud.google.com/dataproc/clusters) form on Google Cloud console, then select your cluster to open the **Cluster details** form. Click the **Web Interfaces** tab to display a list of Component Gateway links to the web interfaces of [default and optional components](/dataproc/docs/concepts/components/overview) installed on the cluster. Click a link to  open the web interface running on the master node of the cluster  in your local browser.Run the gcloud CLI [gcloud dataproc clusters describe](/sdk/gcloud/reference/dataproc/clusters/describe) command locally in a terminal window or in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) .\n```\ngcloud dataproc clusters describe cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region\n```\n **Sample Output** \n```\n...\nconfig:\n endpointConfig:\n enableHttpPortAccess: true\n httpPorts:\n  HDFS NameNode: https://584bbf70-7a12-4120-b25c-31784c94dbb4-dot-dataproc.google.com/hdfs/\n  MapReduce Job History: https://584bbf70-7a12-4120-b25c-31784c94dbb4-dot-dataproc.google.com/jobhistory/\n  Spark HistoryServer: https://584bbf70-7a12-4120-b25c-31784c94dbb4-dot-dataproc.google.com/sparkhistory/\n  YARN ResourceManager: https://584bbf70-7a12-4120-b25c-31784c94dbb4-dot-dataproc.google.com/yarn/\n  YARN Application Timeline: https://584bbf70-7a12-4120-b25c-31784c94dbb4-dot-dataproc.google.com/apphistory/\n...\n```\nCall\n [clusters.get](/dataproc/docs/reference/rest/v1/projects.regions.clusters/get) \nto get the\n [endpointConfig.httpPorts](/dataproc/docs/reference/rest/v1/ClusterConfig#EndpointConfig) \nmap of port names to URLs.\n## Using Component Gateway with VPC-SC\nComponent Gateway supports [VPC Service Controls](/vpc-service-controls/docs) . For service perimeter enforcement, requests to interfaces through Component Gateway are treated as part of the Dataproc API surface, and any access policies that control permissions for `dataproc.googleapis.com` will also control access to Component Gateway UIs.\nComponent Gateway also supports VPC-SC configurations that rely on [private Google connectivity](/vpc-service-controls/docs/private-connectivity) for Dataproc clusters without external IP addresses, but you must manually configure your network to allow access from the Dataproc master VM to `*.dataproc.cloud.google.com` through the restricted Google virtual IP range `199.36.153.4/30` by doing the following:\n- Follow the instructions to [configure private Google connectivityfor all Google APIs](/vpc-service-controls/docs/set-up-private-connectivity) .\n- Either [Configure DNS with Cloud DNS](#configure_dns_with_cloud_dns) or [configure DNS locally on the Dataproc master node](#configure_dns_locally_on_dataproc_master_node_with_an_initialization_action) to allow access to`*.dataproc.cloud.google.com`.\n### Configure DNS with Cloud DNS\nCreate a Cloud DNS zone that maps traffic destined for `*.dataproc.cloud.google.com` to the restricted Google API virtual IP range.\n- Create a managed private zone for your VPC network.```\ngcloud dns managed-zones create ZONE_NAME \\\n --visibility=private \\\n --networks=https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/NETWORK_NAME \\\n --description=DESCRIPTION \\\n --dns-name=dataproc.cloud.google.com \\\n --project=PROJECT_ID\n```- is a name for the zone that you are creating. For example, `vpc` . This zone name will be used in each of the following steps.\n- is the ID of the project that hosts your VPC network.\n- is the name of your VPC network.\n- is an optional, human-readable description of the managed zone.\n- Start a transaction.```\ngcloud dns record-sets transaction start --zone=ZONE_NAME\n```- is your zone name.\n- Add DNS records.```\ngcloud dns record-sets transaction add --name=*.dataproc.cloud.google.com. \\\n --type=A 199.36.153.4 199.36.153.5 199.36.153.6 199.36.153.7 \\\n --zone=ZONE_NAME \\\n --ttl=300\n```- is your zone name.\n```\ngcloud dns record-sets transaction add --name=dataproc.cloud.google.com. \\\n --type=A 199.36.153.4 199.36.153.5 199.36.153.6 199.36.153.7 \\\n --zone=ZONE_NAME \\\n --ttl=300\n```- is your zone name.\n- Execute the transaction.```\ngcloud dns record-sets transaction execute --zone=ZONE_NAME --project=PROJECT_ID\n```- is your zone name.\n- is the ID of the project that hosts your VPC network.\n### Configure DNS locally on Dataproc master node with an initialization action\nYou can locally configure DNS on Dataproc master nodes to allow private connectivity to `dataproc.cloud.google.com` . This procedure is intended for short-term testing and development. It is not recommended for use in production workloads.\n- Stage the initialization action to Cloud Storage.```\ncat <<EOF >component-gateway-vpc-sc-dns-init-action.sh\n#!/bin/bash\nreadonly ROLE=\"$(/usr/share/google/get_metadata_value attributes/dataproc-role)\"\nif [[ \"${ROLE}\" == 'Master' ]]; then\n readonly PROXY_ENDPOINT=$(grep \"^dataproc.proxy.agent.endpoint=\" \\\n \"/etc/google-dataproc/dataproc.properties\" | \\\n tail -n 1 | cut -d '=' -f 2- | sed -r 's/\\\\([#!=:])/\\1/g')\n readonly HOSTNAME=$(echo ${PROXY_ENDPOINT} | \\\n sed -n -E 's;^https://([^/?#]*).*;\\1;p')\n echo \"199.36.153.4 ${HOSTNAME} # Component Gateway VPC-SC\" >> \"/etc/hosts\"\nfi\nEOF\ngsutil cp component-gateway-vpc-sc-dns-init-action.sh gs://BUCKET/\n```- is a Cloud Storage bucket accessible from the Dataproc cluster.\n- Create a Dataproc cluster with the staged initialization action and Component Gateway enabled.```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--initialization-actions=gs://BUCKET/component-gateway-vpc-sc-dns-init-action.sh \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway \\\n\u00a0\u00a0\u00a0\u00a0other args ...\n```- is the Cloud Storage bucket used in step 1, above.\n## Programmatically using HTTP APIs through Component Gateway\nComponent Gateway is a proxy that incorporates Apache Knox. Endpoints exposed by Apache Knox are available through `https://` `` `/` `` .\nTo authenticate programmatically with Component Gateway, pass the header `Proxy-Authorization` with an [OAuth 2.0 Bearer token](https://tools.ietf.org/html/rfc6750) .\n```\n$ ACCESS_TOKEN=\"$(gcloud auth print-access-token)\"$ curl -H \"Proxy-Authorization: Bearer ${ACCESS_TOKEN}\" \"https://xxxxxxxxxxxxxxx-dot-us-central1.dataproc.googleusercontent.com/yarn/jmx\"{\u00a0 \"beans\" : [ {\u00a0 \u00a0 \"name\" : \"Hadoop:service=ResourceManager,name=RpcActivityForPort8031\",\u00a0 \u00a0 \"modelerType\" : \"RpcActivityForPort8031\",\u00a0 \u00a0 \"tag.port\" : \"8031\",\u00a0 \u00a0 \"tag.Context\" : \"rpc\",\u00a0 \u00a0 \"tag.NumOpenConnectionsPerUser\" : \"{\\\"yarn\\\":2}\",\u00a0 \u00a0 \"tag.Hostname\" : \"demo-cluster-m\",\u00a0 \u00a0 \"ReceivedBytes\" : 1928581096,\u00a0 \u00a0 \"SentBytes\" : 316939850,\u00a0 \u00a0 \"RpcQueueTimeNumOps\" : 7230574,\u00a0 \u00a0 \"RpcQueueTimeAvgTime\" : 0.09090909090909091,\u00a0 \u00a0 \"RpcProcessingTimeNumOps\" : 7230574,\u00a0 \u00a0 \"RpcProcessingTimeAvgTime\" : 0.045454545454545456,...\n```\nComponent Gateway strips out the `Proxy-Authorization` header before forwarding requests to Apache Knox.\nTo find the Component Gateway base url, run: `gcloud dataproc clusters describe` :\n```\n$ gcloud dataproc clusters describe <var>cluster-name</var> &#92;\u00a0 \u00a0 &nbsp;&nbsp;&nbsp;&nbsp;--region=<var>region</var>...\u00a0 endpointConfig:\u00a0 \u00a0 enableHttpPortAccess: true\u00a0 \u00a0 httpPorts:\u00a0 \u00a0 \u00a0 HDFS NameNode: https://xxxxxxxxxxxxxxx-dot-us-central1.dataproc.googleusercontent.com/hdfs/dfshealth.html\u00a0 \u00a0 \u00a0 MapReduce Job History: https://xxxxxxxxxxxxxxx-dot-us-central1.dataproc.googleusercontent.com/jobhistory/\u00a0 \u00a0 \u00a0 Spark History Server: https://xxxxxxxxxxxxxxx-dot-us-central1.dataproc.googleusercontent.com/sparkhistory/\u00a0 \u00a0 \u00a0 Tez: https://xxxxxxxxxxxxxxx-dot-us-central1.dataproc.googleusercontent.com/apphistory/tez-ui/\u00a0 \u00a0 \u00a0 YARN Application Timeline: https://xxxxxxxxxxxxxxx-dot-us-central1.dataproc.googleusercontent.com/apphistory/\u00a0 \u00a0 \u00a0 YARN ResourceManager: https://xxxxxxxxxxxxxxx-dot-us-central1.dataproc.googleusercontent.com/yarn/...\n```\nThe base URL is the scheme and authority portions of the URLs under `httpPorts` . In this example, it is `https://xxxxxxxxxxxxxxx-dot-us-central1.dataproc.googleusercontent.com/` .\n## How to regenerate the Component Gateway SSL certificate\nThe Component Gateway default Knox Gateway SSL Certificate is valid for:\n- 5 years from Dataproc cluster creation date on clusters created with images versions 2.0.93, 2.1.41, 2.2.7 and later.\n- 13 months from Dataproc cluster creation date on clusters created using earlier image versions.\nIf the certificate expires, all Component Gateway web interface URls become inactive.\nIf your organization provided the SSL certificate, obtain a new certificate from the organization, and then replace the old certificate with the new one.\nIf you are using the default self-signed SSL certificate, renew it as follows:\n- Use [SSH](/dataproc/docs/concepts/accessing/ssh) to connect to the Dataproc cluster master node with the `m-0` name suffix.\n- Locate `gateway.jks` in the `/var/lib/knox/security/keystores/gateway.jks` path.```\nkeytool -list -v -keystore /var/lib/knox/security/keystores/gateway.jks\n```\n- Move the `gateway.jks` file to a backup directory.```\nmv /var/lib/knox/security/keystores/gateway.jks /tmp/backup/gateway.jks\n```\n- Create a new self-signed certificate by restarting the Knox service.```\nsystemctl restart knox\n```\n- Verify Component Gateway and Knox status.```\nsystemctl status google-dataproc-component-gateway\nsystemctl status knox\n```## What's Next\n- Create a cluster with [Dataproc components](/dataproc/docs/concepts/components/overview) .", "guide": "Dataproc"}