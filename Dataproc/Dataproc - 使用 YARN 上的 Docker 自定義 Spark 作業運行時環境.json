{"title": "Dataproc - \u4f7f\u7528 YARN \u4e0a\u7684 Docker \u81ea\u5b9a\u7fa9 Spark \u4f5c\u696d\u904b\u884c\u6642\u74b0\u5883", "url": "https://cloud.google.com/dataproc/docs/guides/dataproc-docker-yarn?hl=zh-cn", "abstract": "# Dataproc - \u4f7f\u7528 YARN \u4e0a\u7684 Docker \u81ea\u5b9a\u7fa9 Spark \u4f5c\u696d\u904b\u884c\u6642\u74b0\u5883\n\u85c9\u52a9 Dataproc **YARN \u4e0a\u7684 Docker** \u529f\u80fd\uff0c\u60a8\u53ef\u4ee5\u5275\u5efa\u548c\u4f7f\u7528 Docker \u6620\u50cf\u4f86\u81ea\u5b9a\u7fa9 Spark \u4f5c\u696d\u904b\u884c\u6642\u74b0\u5883\u3002\u6620\u50cf\u53ef\u4ee5\u5305\u542b\u5c0d Java\u3001Python \u548c R \u4f9d\u8cf4\u9805\u4ee5\u53ca\u4f5c\u696d jar \u7684\u81ea\u5b9a\u7fa9\u8a2d\u7f6e\u3002\n#", "content": "## \u9650\u5236\n\u4ee5\u4e0b\u7248\u672c **\u4e0d\u63d0\u4f9b** \u529f\u80fd\u6216\u652f\u6301\u670d\u52d9\uff1a\n- 2.0.49 \u4e4b\u524d\u7684 Dataproc \u6620\u50cf\u7248\u672c\uff08\u5728 1.5 \u6620\u50cf\u4e2d\u4e0d\u53ef\u7528\uff09\n- MapReduce \u4f5c\u696d\uff08\u50c5\u652f\u6301 Spark \u4f5c\u696d\uff09\n- Spark \u5ba2\u6236\u7aef\u6a21\u5f0f\uff08\u50c5\u652f\u6301 Spark \u96c6\u7fa3\u6a21\u5f0f\uff09\n- [Kerberos \u96c6\u7fa3](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/security?hl=zh-cn#create_the_cluster) \uff1a\u5982\u679c\u60a8 [\u5728 YARN \u4e0a\u5275\u5efa Docker \u96c6\u7fa3](#create_a_cluster) \u4e26\u5553\u7528 Kerberos\uff0c\u5247\u96c6\u7fa3\u5275\u5efa\u5931\u6557\u3002\n- \u81ea\u5b9a\u7fa9 JDK\u3001Hadoop \u548c Spark\uff1a\u4f7f\u7528\u7684\u662f\u4e3b\u6a5f JDK\u3001Hadoop \u548c Spark\uff0c\u800c\u4e0d\u662f\u60a8\u7684\u81ea\u5b9a\u7fa9\u8a2d\u7f6e\u3002## \u5275\u5efa Docker \u6620\u50cf\n\u81ea\u5b9a\u7fa9 Spark \u74b0\u5883\u7684\u7b2c\u4e00\u6b65\u662f [\u69cb\u5efa Docker \u6620\u50cf](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/) \u3002\n### Dockerfile\n\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b Dockerfile \u4f5c\u7232\u793a\u4f8b\uff0c\u66f4\u6539\u548c\u6dfb\u52a0\u4ee5\u6eff\u8db3\u81ea\u5df1\u7684\u9700\u6c42\u3002\n```\nFROM debian:10-slim# Suppress interactive prompts.ENV DEBIAN_FRONTEND=noninteractive# Required: Install utilities required by Spark scripts.RUN apt update && apt install -y procps tini# Optional: Add extra jars.ENV SPARK_EXTRA_JARS_DIR=/opt/spark/jars/ENV SPARK_EXTRA_CLASSPATH='/opt/spark/jars/*'RUN mkdir -p \"${SPARK_EXTRA_JARS_DIR}\"COPY *.jar \"${SPARK_EXTRA_JARS_DIR}\"# Optional: Install and configure Miniconda3.ENV CONDA_HOME=/opt/miniconda3ENV PYSPARK_PYTHON=${CONDA_HOME}/bin/pythonENV PYSPARK_DRIVER_PYTHON=${CONDA_HOME}/bin/pythonENV PATH=${CONDA_HOME}/bin:${PATH}COPY Miniconda3-py39_4.10.3-Linux-x86_64.sh .RUN bash Miniconda3-py39_4.10.3-Linux-x86_64.sh -b -p /opt/miniconda3 \\\u00a0 && ${CONDA_HOME}/bin/conda config --system --set always_yes True \\\u00a0 && ${CONDA_HOME}/bin/conda config --system --set auto_update_conda False \\\u00a0 && ${CONDA_HOME}/bin/conda config --system --prepend channels conda-forge \\\u00a0 && ${CONDA_HOME}/bin/conda config --system --set channel_priority strict# Optional: Install Conda packages.\n## The following packages are installed in the default image. It is strongly# recommended to include all of them.\n## Use mamba to install packages quickly.RUN ${CONDA_HOME}/bin/conda install mamba -n base -c conda-forge \\\u00a0 \u00a0 && ${CONDA_HOME}/bin/mamba install \\\u00a0 \u00a0 \u00a0 conda \\\u00a0 \u00a0 \u00a0 cython \\\u00a0 \u00a0 \u00a0 fastavro \\\u00a0 \u00a0 \u00a0 fastparquet \\\u00a0 \u00a0 \u00a0 gcsfs \\\u00a0 \u00a0 \u00a0 google-cloud-bigquery-storage \\\u00a0 \u00a0 \u00a0 google-cloud-bigquery[pandas] \\\u00a0 \u00a0 \u00a0 google-cloud-bigtable \\\u00a0 \u00a0 \u00a0 google-cloud-container \\\u00a0 \u00a0 \u00a0 google-cloud-datacatalog \\\u00a0 \u00a0 \u00a0 google-cloud-dataproc \\\u00a0 \u00a0 \u00a0 google-cloud-datastore \\\u00a0 \u00a0 \u00a0 google-cloud-language \\\u00a0 \u00a0 \u00a0 google-cloud-logging \\\u00a0 \u00a0 \u00a0 google-cloud-monitoring \\\u00a0 \u00a0 \u00a0 google-cloud-pubsub \\\u00a0 \u00a0 \u00a0 google-cloud-redis \\\u00a0 \u00a0 \u00a0 google-cloud-spanner \\\u00a0 \u00a0 \u00a0 google-cloud-speech \\\u00a0 \u00a0 \u00a0 google-cloud-storage \\\u00a0 \u00a0 \u00a0 google-cloud-texttospeech \\\u00a0 \u00a0 \u00a0 google-cloud-translate \\\u00a0 \u00a0 \u00a0 google-cloud-vision \\\u00a0 \u00a0 \u00a0 koalas \\\u00a0 \u00a0 \u00a0 matplotlib \\\u00a0 \u00a0 \u00a0 nltk \\\u00a0 \u00a0 \u00a0 numba \\\u00a0 \u00a0 \u00a0 numpy \\\u00a0 \u00a0 \u00a0 openblas \\\u00a0 \u00a0 \u00a0 orc \\\u00a0 \u00a0 \u00a0 pandas \\\u00a0 \u00a0 \u00a0 pyarrow \\\u00a0 \u00a0 \u00a0 pysal \\\u00a0 \u00a0 \u00a0 pytables \\\u00a0 \u00a0 \u00a0 python \\\u00a0 \u00a0 \u00a0 regex \\\u00a0 \u00a0 \u00a0 requests \\\u00a0 \u00a0 \u00a0 rtree \\\u00a0 \u00a0 \u00a0 scikit-image \\\u00a0 \u00a0 \u00a0 scikit-learn \\\u00a0 \u00a0 \u00a0 scipy \\\u00a0 \u00a0 \u00a0 seaborn \\\u00a0 \u00a0 \u00a0 sqlalchemy \\\u00a0 \u00a0 \u00a0 sympy \\\u00a0 \u00a0 \u00a0 virtualenv# Optional: Add extra Python modules.ENV PYTHONPATH=/opt/python/packagesRUN mkdir -p \"${PYTHONPATH}\"COPY test_util.py \"${PYTHONPATH}\"# Required: Create the 'yarn_docker_user' group/user.# The GID and UID must be 1099. Home directory is required.RUN groupadd -g 1099 yarn_docker_userRUN useradd -u 1099 -g 1099 -d /home/yarn_docker_user -m yarn_docker_userUSER yarn_docker_user\n```\n### \u69cb\u5efa\u548c\u63a8\u9001\u6620\u50cf\n\u4ee5\u4e0b\u662f\u7528\u65bc\u69cb\u5efa\u548c\u63a8\u9001\u793a\u4f8b Docker \u6620\u50cf\u7684\u547d\u4ee4\uff0c\u60a8\u53ef\u4ee5\u6839\u64da\u81ea\u5df1\u7684\u81ea\u5b9a\u7fa9\u5167\u5bb9\u9032\u884c\u66f4\u6539\u3002\n```\n# Increase the version number when there is a change to avoid referencing# a cached older image. Avoid reusing the version number, including the default# `latest` version.IMAGE=gcr.io/my-project/my-image:1.0.1# Download the BigQuery connector.gsutil cp \\\u00a0 gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar .# Download the Miniconda3 installer.wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.10.3-Linux-x86_64.sh# Python module example:cat >test_util.py <<EOFdef hello(name):\u00a0 print(\"hello {}\".format(name))def read_lines(path):\u00a0 with open(path) as f:\u00a0 \u00a0 return f.readlines()EOF# Build and push the image.docker build -t \"${IMAGE}\" .docker push \"${IMAGE}\"\n```\n## \u5275\u5efa Dataproc \u96c6\u7fa3\n[\u5275\u5efa\u53ef\u81ea\u5b9a\u7fa9 Spark \u74b0\u5883\u7684 Docker \u6620\u50cf](#create_a_docker_image) \u5f8c\uff0c\u8acb\u5275\u5efa\u4e00\u500b Dataproc \u96c6\u7fa3\uff0c\u8a72\u96c6\u7fa3\u5c07\u5728\u904b\u884c Spark \u4f5c\u696d\u6642\u4f7f\u7528 Docker \u6620\u50cf\u3002\n```\ngcloud dataproc clusters create CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--image-version=DP_IMAGE \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=DOCKER \\\n\u00a0\u00a0\u00a0\u00a0--properties=dataproc:yarn.docker.enable=true,dataproc:yarn.docker.image=DOCKER_IMAGE \\\n\u00a0\u00a0\u00a0\u00a0other flags\n```\n\u66ff\u63db\u4ee5\u4e0b\u5167\u5bb9\uff1a- \uff1a\u96c6\u7fa3\u540d\u7a31\u3002\n- \uff1a\u96c6\u7fa3\u5340\u57df\u3002\n- \uff1aDataproc \u6620\u50cf\u7248\u672c\u5fc5\u9808\u7232`2.0.49`\u6216\u66f4\u9ad8\u7248\u672c\uff08`--image-version=2.0`\u5c07\u4f7f\u7528\u9ad8\u65bc`2.0.49`\u7684\u5408\u683c\u6b21\u8981\u7248\u672c\uff09\u3002\n- `--optional-components=DOCKER`\uff1a\u5728\u96c6\u7fa3\u4e0a\u5553\u7528 [Docker \u7d44\u4ef6](https://cloud.google.com/dataproc/docs/concepts/components/docker?hl=zh-cn) \u3002\n- `--properties`\u6a19\u8a8c\uff1a- `dataproc:yarn.docker.enable=true`\uff1a\u7528\u65bc\u5553\u7528 YARN \u4e0a\u7684 Dataproc Docker \u529f\u80fd\u7684\u5fc5\u9700\u5c6c\u6027\u3002\n- `dataproc:yarn.docker.image`\uff1a\u60a8\u53ef\u4ee5\u6dfb\u52a0\u7684\u53ef\u9078\u5c6c\u6027\uff0c\u4ee5\u4fbf\u4f7f\u7528\u4ee5\u4e0b Container Registry \u6620\u50cf\u547d\u540d\u683c\u5f0f\u6307\u5b9a [DOCKER_IMAGE](#create_a_docker_image) \uff1a`{hostname}/{project-id}/{image}:{tag}`\u3002\u793a\u4f8b\uff1a```\ndataproc:yarn.docker.image=gcr.io/project-id/image:1.0.1\n``` **\u8981\u6c42** \uff1a\u60a8\u5fc5\u9808\u5728 [Container Registry](https://cloud.google.com/container-registry?hl=zh-cn) \u6216 [Artifact Registry](https://cloud.google.com/artifact-registry?hl=zh-cn) \u4e0a\u8a17\u7ba1 Docker \u6620\u50cf\u3002\uff08Dataproc \u7121\u6cd5\u5f9e\u5176\u4ed6\u8a3b\u518a\u8868\u4e2d\u63d0\u53d6\u5bb9\u5668\uff09\u3002 **\u5efa\u8b70** \uff1a\u5728\u5275\u5efa\u96c6\u7fa3\u6642\u6dfb\u52a0\u6b64\u5c6c\u6027\u4ee5\u7de9\u5b58 Docker \u6620\u50cf\uff0c\u4e26\u907f\u514d\u4e4b\u5f8c\u5728\u63d0\u4ea4\u4f7f\u7528\u8a72\u6620\u50cf\u7684\u4f5c\u696d\u6642\u51fa\u73fe YARN \u8d85\u6642\u3002\n\u5982\u679c\u5c07 `dataproc:yarn.docker.enable` \u8a2d\u7f6e\u7232 `true` \uff0cDataproc \u6703\u66f4\u65b0 Hadoop \u548c Spark \u914d\u7f6e\uff0c\u4ee5\u5728\u96c6\u7fa3\u4e2d\u5553\u7528 Docker on YARN \u529f\u80fd\u3002\u4f8b\u5982\uff0c\u5c07 `spark.submit.deployMode` \u8a2d\u7f6e\u7232 `cluster` \uff0c\u5c07 `spark.yarn.appMasterEnv.YARN_CONTAINER_RUNTIME_DOCKER_MOUNTS` \u548c `spark.executorEnv.YARN_CONTAINER_RUNTIME_DOCKER_MOUNTS` \u8a2d\u7f6e\u7232\u5c07\u76ee\u9304\u5f9e\u4e3b\u6a5f\u88dd\u8f09\u5230\u5bb9\u5668\u4e2d\u3002\n## \u5c07 Spark \u4f5c\u696d\u63d0\u4ea4\u5230\u96c6\u7fa3\n[\u5275\u5efa Dataproc \u96c6\u7fa3](#create_a_cluster) \u5f8c\uff0c\u5c07 Spark \u4f5c\u696d\u63d0\u4ea4\u5230\u4f7f\u7528 Docker \u6620\u50cf\u7684\u96c6\u7fa3\u3002\u672c\u90e8\u5206\u4e2d\u7684\u793a\u4f8b\u5c07 PySpark \u4f5c\u696d\u63d0\u4ea4\u5230\u96c6\u7fa3\u3002\n\u8a2d\u7f6e\u4f5c\u696d\u5c6c\u6027\uff1a\n```\n# Set the Docker image URI.IMAGE=(e.g., gcr.io/my-project/my-image:1.0.1)# Required: Use `#` as the delimiter for properties to avoid conflicts.JOB_PROPERTIES='^#^'# Required: Set Spark properties with the Docker image.JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.yarn.appMasterEnv.YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=${IMAGE}\"JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.executorEnv.YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=${IMAGE}\"# Optional: Add custom jars to Spark classpath. Don't set these properties if# there are no customizations.JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.driver.extraClassPath=/opt/spark/jars/*\"JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.executor.extraClassPath=/opt/spark/jars/*\"# Optional: Set custom PySpark Python path only if there are customizations.JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.pyspark.python=/opt/miniconda3/bin/python\"JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.pyspark.driver.python=/opt/miniconda3/bin/python\"# Optional: Set custom Python module path only if there are customizations.# Since the `PYTHONPATH` environment variable defined in the Dockerfile is# overridden by Spark, it must be set as a job property.JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.yarn.appMasterEnv.PYTHONPATH=/opt/python/packages\"JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.executorEnv.PYTHONPATH=/opt/python/packages\"\n```\n\u5099\u8a3b\uff1a\n- \u5982\u9700\u77ad\u89e3\u76f8\u95dc\u5c6c\u6027\uff0c\u8acb\u53c3\u95b1 [\u4f7f\u7528 Docker \u5bb9\u5668\u5553\u52d5\u61c9\u7528](https://hadoop.apache.org/docs/r3.2.3/hadoop-yarn/hadoop-yarn-site/DockerContainers.html) \u3002\n\u5c07\u4f5c\u696d\u63d0\u4ea4\u5230\u96c6\u7fa3\u3002\n```\ngcloud dataproc jobs submit pyspark PYFILE \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--properties=${JOB_PROPERTIES}\n```\n\u66ff\u63db\u4ee5\u4e0b\u5167\u5bb9\uff1a- \uff1aPySpark \u4f5c\u696d\u6587\u4ef6\u7684\u6587\u4ef6\u8def\u5f91\u3002\u5b83\u53ef\u4ee5\u662f\u672c\u5730\u6587\u4ef6\u8def\u5f91\u6216 Cloud Storage \u4e2d\u6587\u4ef6\u7684 URI (`gs://` `` `/` ``)\u3002\n- \uff1a\u96c6\u7fa3\u540d\u7a31\u3002\n- \uff1a\u96c6\u7fa3\u5340\u57df\u3002", "guide": "Dataproc"}