{"title": "Dataproc - \u63d0\u4ea4\u4f5c\u696d", "url": "https://cloud.google.com/dataproc/docs/guides/submit-job?hl=zh-cn", "abstract": "# Dataproc - \u63d0\u4ea4\u4f5c\u696d\n\u60a8\u53ef\u4ee5\u901a\u904e\u4ee5\u4e0b\u65b9\u5f0f\u5c07\u4f5c\u696d\u63d0\u4ea4\u5230\u73fe\u6709 Dataproc \u96c6\u7fa3\uff1a\u901a\u904e Dataproc API [jobs.submit](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/submit?hl=zh-cn) HTTP \u6216\u7a0b\u5e8f\u5316\u8acb\u6c42\uff0c\u5728\u672c\u5730\u7d42\u7aef\u7a97\u53e3\u6216 [Cloud Shell](https://console.cloud.google.com/?cloudshell=true&hl=zh-cn) \u4e2d\u4f7f\u7528 Google Cloud CLI [gcloud](https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs/submit?hl=zh-cn) \u547d\u4ee4\u884c\u5de5\u5177\uff0c\u6216\u901a\u904e\u5728\u672c\u5730\u700f\u89bd\u5668\u4e2d\u6253\u958b\u7684 [Google Cloud Console](https://console.cloud.google.com/dataproc/jobs/jobsSubmit?hl=zh-cn) \u3002\u60a8\u9084\u53ef\u4ee5 [\u901a\u904e SSH \u9023\u63a5\u5230\u96c6\u7fa3\u4e2d\u7684\u4e3b\u5be6\u4f8b](#ssh_into_the_master_instance) \uff0c\u7136\u5f8c\u7121\u9700\u4f7f\u7528 Dataproc \u670d\u52d9\uff0c\u76f4\u63a5\u5f9e\u5be6\u4f8b\u904b\u884c\u4f5c\u696d\u3002\n**\u4f5c\u696d\u9a45\u52d5\u7a0b\u5e8f\u548c\u65e5\u8a8c\u8f38\u51fa** \uff1a\u5982\u9700\u77ad\u89e3\u5982\u4f55\u67e5\u770b\u4f5c\u696d\u9a45\u52d5\u7a0b\u5e8f\u8f38\u51fa\u548c\u65e5\u8a8c\uff0c\u8acb\u53c3\u95b1 [\u4f5c\u696d\u9a45\u52d5\u7a0b\u5e8f\u8f38\u51fa](https://cloud.google.com/dataproc/docs/concepts/driver-output?hl=zh-cn) \u3002\n**\u4f5c\u696d\u4f75\u767c\uff1a** \u5275\u5efa\u96c6\u7fa3\u6642\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 [dataproc:dataproc.scheduler.max-concurrent-jobs](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/cluster-properties?hl=zh-cn#service_properties) \u96c6\u7fa3\u5c6c\u6027\u914d\u7f6e\u4f75\u767c Dataproc \u4f5c\u696d\u6578\u91cf\u7684\u4e0a\u9650\u3002\u5982\u679c\u672a\u8a2d\u7f6e\u6b64\u5c6c\u6027\u503c\uff0c\u5247\u4f75\u767c\u4f5c\u696d\u6578\u91cf\u4e0a\u9650\u7684\u8a08\u7b97\u516c\u5f0f\u7232`max((masterMemoryMb - 3584) / masterMemoryMbPerJob, 5)`\u3002`masterMemoryMb`\u7531\u4e3b\u865b\u64ec\u6a5f\u7684\u6a5f\u5668\u985e\u578b\u78ba\u5b9a\u3002`masterMemoryMbPerJob`\u9ed8\u8a8d\u7232`1024`\uff0c\u4e0d\u904e\u60a8\u53ef\u4ee5\u5728\u5275\u5efa\u96c6\u7fa3\u6642\u4f7f\u7528 [dataproc:dataproc.scheduler.driver-size-mb](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/cluster-properties?hl=zh-cn#service_properties) \u96c6\u7fa3\u5c6c\u6027\u5c0d\u5176\u9032\u884c\u914d\u7f6e\u3002\n", "content": "## \u63d0\u4ea4 Dataproc \u4f5c\u696d\n\u60a8\u53ef\u4ee5\u6307\u5b9a`file:///`\u8def\u5f91\u4f86\u5f15\u7528\u96c6\u7fa3\u4e3b\u7bc0\u9ede\u4e0a\u7684\u672c\u5730\u6587\u4ef6\u3002\n\u5982\u9700\u5c07\u4f5c\u696d\u63d0\u4ea4\u5230 Dataproc \u96c6\u7fa3\uff0c\u8acb\u5728\u672c\u5730\u7d42\u7aef\u7a97\u53e3\u6216 [Cloud Shell](https://console.cloud.google.com/?cloudshell=true&hl=zh-cn) \u4e2d\u904b\u884c `gcloud` CLI [gcloud dataproc jobs submit](https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs/submit?hl=zh-cn) \u547d\u4ee4\u3002\n```\ngcloud dataproc jobs submit job-command \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0other dataproc-flags \\\n\u00a0\u00a0\u00a0\u00a0-- job-args\n```\n\u60a8\u53ef\u4ee5\u6dfb\u52a0`--cluster-labels`\u6a19\u8a8c\u4ee5\u6307\u5b9a\u4e00\u500b\u6216\u591a\u500b\u96c6\u7fa3\u6a19\u7c64\u3002Dataproc \u6703\u5c07\u4f5c\u696d\u63d0\u4ea4\u5230\u8207\u6307\u5b9a\u96c6\u7fa3\u6a19\u7c64\u5339\u914d\u7684\u96c6\u7fa3\u3002\n **PySpark \u4f5c\u696d\u63d0\u4ea4\u793a\u4f8b** - \u5217\u51fa\u4f4d\u65bc Cloud Storage \u4e2d\u7684\u53ef\u516c\u958b\u8a2a\u554f\u7684`hello-world.py`\u3002```\ngsutil cat gs://dataproc-examples/pyspark/hello-world/hello-world.py\n```\u6587\u4ef6\u5217\u8868\uff1a```\n#!/usr/bin/pythonimport pysparksc = pyspark.SparkContext()rdd = sc.parallelize(['Hello,', 'world!'])words = sorted(rdd.collect())print(words)\n```\n- \u5c07 Pyspark \u4f5c\u696d\u63d0\u4ea4\u5230 Dataproc\u3002```\ngcloud dataproc jobs submit pyspark \\\n\u00a0\u00a0\u00a0\u00a0gs://dataproc-examples/pyspark/hello-world/hello-world.py \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region\n```\u7d42\u7aef\u8f38\u51fa\uff1a```\nWaiting for job output...\n\u2026\n['Hello,', 'world!']\nJob finished successfully.\n```\n **Spark \u4f5c\u696d\u63d0\u4ea4\u793a\u4f8b** - \u904b\u884c\u9810\u5b89\u88dd\u5728 Dataproc \u96c6\u7fa3\u4e3b\u7bc0\u9ede\u4e0a\u7684 SparkPi \u793a\u4f8b\u3002```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--class=org.apache.spark.examples.SparkPi \\\n\u00a0\u00a0\u00a0\u00a0--jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0-- 1000\n```\u7d42\u7aef\u8f38\u51fa\uff1a```\nJob [54825071-ae28-4c5b-85a5-58fae6a597d6] submitted.\nWaiting for job output\u2026\n\u2026\nPi is roughly 3.14177148\n\u2026\nJob finished successfully.\n\u2026\n``` **\u4f5c\u696d\u5982\u4f55\u8a08\u7b97 Pi** \uff1aSpark \u4f5c\u696d\u4f7f\u7528 [Monte Carlo \u65b9\u6cd5](https://en.wikipedia.org/wiki/Monte_Carlo_method) \u4f30\u7b97 Pi \u7684\u503c\u3002  \u5b83\u5728\u5ea7\u6a19\u5e73\u9762\u4e0a\u751f\u6210`x,y`\u9ede\uff0c\u6a21\u64ec\u7531\u4e00\u500b\u55ae\u4f4d\u6b63\u65b9\u5f62\u5305\u570d\u7684\u5713\u3002\u8f38\u5165\u53c3\u6578 (`1000`) \u6c7a\u5b9a\u8981\u751f\u6210\u7684 x,y \u5c0d\u7684\u6578\u91cf\uff1b\u751f\u6210\u7684\u6578\u91cf\u8d8a\u591a\uff0c\u4f30\u7b97\u7684\u6e96\u78ba\u6027\u5c31\u8d8a\u9ad8\u3002\u6b64\u4f30\u7b97\u5229\u7528 Dataproc \u5de5\u4f5c\u5668\u7bc0\u9ede\u57f7\u884c\u4e26\u884c\u8a08\u7b97\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u4f7f\u7528 Monte Carlo \u65b9\u6cd5\u4f30\u7b97 Pi](https://academo.org/demos/estimating-pi-monte-carlo/) \u4e26\u53c3\u95b1 [GitHub \u4e0a\u7684 JavaSparkPi.java](https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java) \u3002\n\u672c\u90e8\u5206\u4ecb\u7d39\u5982\u4f55\u4f7f\u7528 Dataproc [jobs.submit](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs/submit?hl=zh-cn) API \u63d0\u4ea4 Spark \u4f5c\u696d\u4ee5\u8a08\u7b97 `pi` \u7684\u8fd1\u4f3c\u503c\u3002\n\u60a8\u53ef\u4ee5\u5c07`clusterLabels`\u5b57\u6bb5\u6dfb\u52a0\u5230\u5982\u4e0b\u6240\u793a\u7684 API \u8acb\u6c42\u4e2d\uff0c\u4ee5\u6307\u5b9a\u4e00\u500b\u6216\u591a\u500b\u96c6\u7fa3\u6a19\u7c64\u3002Dataproc \u6703\u5c07\u4f5c\u696d\u63d0\u4ea4\u5230\u8207\u6307\u5b9a\u96c6\u7fa3\u6a19\u7c64\u5339\u914d\u7684\u96c6\u7fa3\uff08\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [jobs.submit](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.jobs?hl=zh-cn#JobPlacement.FIELDS.cluster_labels) API\uff09\u3002\n\u5728\u4f7f\u7528\u4efb\u4f55\u8acb\u6c42\u6578\u64da\u4e4b\u524d\uff0c\u8acb\u5148\u9032\u884c\u4ee5\u4e0b\u66ff\u63db\uff1a- \uff1aGCP \u9805\u76ee ID\n- \uff1a [\u96c6\u7fa3\u5730\u5340](https://cloud.google.com/dataproc/docs/guides/create-cluster?hl=zh-cn#cluster-region) \n- \uff1a\u96c6\u7fa3\u540d\u7a31\nHTTP \u65b9\u6cd5\u548c\u7db2\u5740\uff1a\n```\nPOST https://dataproc.googleapis.com/v1/projects/project-id/regions/region/jobs:submit\n```\n\u8acb\u6c42 JSON \u6b63\u6587\uff1a\n```\n{\n \"job\": {\n \"placement\": {\n  \"clusterName\": \"cluster-name\"\n },\n },\n \"sparkJob\": {\n  \"args\": [  \"1000\"\n  ],\n  \"mainClass\": \"org.apache.spark.examples.SparkPi\",\n  \"jarFileUris\": [  \"file:///usr/lib/spark/examples/jars/spark-examples.jar\"\n  ]\n }\n }\n}\n```\n\u5982\u9700\u767c\u9001\u60a8\u7684\u8acb\u6c42\uff0c\u8acb\u5c55\u958b\u4ee5\u4e0b\u9078\u9805\u4e4b\u4e00\uff1a\u60a8\u61c9\u8a72\u6536\u5230\u985e\u4f3c\u4ee5\u4e0b\u5167\u5bb9\u7684 JSON \u97ff\u61c9\uff1a\n```\n{\n \"reference\": {\n \"projectId\": \"project-id\",\n \"jobId\": \"job-id\"\n },\n \"placement\": {\n \"clusterName\": \"cluster-name\",\n \"clusterUuid\": \"cluster-Uuid\"\n },\n \"sparkJob\": {\n \"mainClass\": \"org.apache.spark.examples.SparkPi\",\n \"args\": [  \"1000\"\n ],\n \"jarFileUris\": [  \"file:///usr/lib/spark/examples/jars/spark-examples.jar\"\n ]\n },\n \"status\": {\n \"state\": \"PENDING\",\n \"stateStartTime\": \"2020-10-07T20:16:21.759Z\"\n },\n \"jobUuid\": \"job-Uuid\"\n}\n```\n **\u6ce8\u610f** \uff1a\u60a8\u53ef\u4ee5\u9ede\u64ca Dataproc Cloud Console [\u63d0\u4ea4\u4f5c\u696d](https://console.cloud.google.com/dataproc/jobs/jobsSubmit?hl=zh-cn) \u9801\u9762\u5e95\u90e8\u7684 **\u7b49\u6548 REST** \u93c8\u63a5\uff0c\u8b93 Google Cloud Console \u69cb\u5efa\u4e00\u500b\u7b49\u6548\u7684 API REST \u8acb\u6c42\uff0c\u4ee5\u4fbf\u7528\u5728\u4ee3\u78bc\u4e2d\u5c07\u4f5c\u696d\u63d0\u4ea4\u5230\u96c6\u7fa3\u3002\n\u5728\u700f\u89bd\u5668\u4e2d\u901a\u904e Cloud Console \u6253\u958b [\u63d0\u4ea4\u4f5c\u696d](https://console.cloud.google.com/dataproc/jobs/jobsSubmit?hl=zh-cn) \u9801\u9762\u3002 **Spark \u4f5c\u696d\u793a\u4f8b** \n\u5982\u9700\u63d0\u4ea4\u793a\u4f8b Spark \u4f5c\u696d\uff0c\u8acb\u586b\u5beb **\u63d0\u4ea4\u4f5c\u696d** \u9801\u9762\u4e0a\u7684\u5b57\u6bb5\uff0c\u5982\u4e0b\u6240\u793a\uff08\u6839\u64da\u4e0a\u4e00\u5c4f\u5e55\u622a\u5716\uff09\uff1a- \u5f9e\u96c6\u7fa3\u5217\u8868\u4e2d\u9078\u64c7 **\u96c6\u7fa3** \u540d\u7a31\u3002\n- \u5c07 **\u4f5c\u696d\u985e\u578b** \u8a2d\u7f6e\u7232`Spark`\u3002\n- \u5c07 **\u4e3b\u985e\u6216 jar** \u8a2d\u7f6e\u7232`org.apache.spark.examples.SparkPi`\u3002\n- \u5c07 **\u53c3\u6578** \u8a2d\u7f6e\u7232\u55ae\u500b\u53c3\u6578`1000`\u3002\n- \u5c07`file:///usr/lib/spark/examples/jars/spark-examples.jar`\u6dfb\u52a0\u5230 **Jar \u6587\u4ef6** \uff1a\n- `file:///`\u8868\u793a Hadoop LocalFileSystem \u65b9\u6848\u3002\u5728\u5275\u5efa\u96c6\u7fa3\u6642\uff0cDataproc \u5728\u96c6\u7fa3\u4e3b\u7bc0\u9ede\u4e0a\u5b89\u88dd\u4e86`/usr/lib/spark/examples/jars/spark-examples.jar`\u3002\n- \u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u7232\u5176\u4e2d\u4e00\u500b jar \u6307\u5b9a Cloud Storage \u8def\u5f91 (`gs://` `` `/` `` `.jar`) \u6216 Hadoop \u5206\u4f48\u5f0f\u6587\u4ef6\u7cfb\u7d71\u8def\u5f91 (`hdfs://` `` `.jar`)\u3002\n **\u6dfb\u52a0\u66f4\u591a\u53c3\u6578** \uff1a\u6bcf\u9805\u53c3\u6578\u5fc5\u9808\u8f38\u5165\u55ae\u7368\u7684\u6587\u672c\u6846\u4e2d\u3002\u6309 **<Return>** \u9375\u7232\u5176\u4ed6\u5404\u500b\u53c3\u6578\u6253\u958b\u4e00\u500b\u65b0\u6587\u672c\u6846\u3002\n\u9ede\u64ca **\u63d0\u4ea4** \u4ee5\u5553\u52d5\u4f5c\u696d\u3002\u4f5c\u696d\u5553\u52d5\u5f8c\uff0c\u5c31\u6703\u6dfb\u52a0\u5230\u4f5c\u696d\u5217\u8868\u4e2d\u3002\u9ede\u64ca\u4f5c\u696d ID \u4ee5\u6253\u958b **\u4f5c\u696d** \u9801\u9762\uff0c\u60a8\u53ef\u5728\u6b64\u67e5\u770b\u4f5c\u696d\u7684\u9a45\u52d5\u7a0b\u5e8f\u8f38\u51fa\uff08\u8acb\u53c3\u95b1 [\u8a2a\u554f\u4f5c\u696d\u9a45\u52d5\u7a0b\u5e8f\u8f38\u51fa\u2013CONSOLE](https://cloud.google.com/dataproc/docs/concepts/driver-output?hl=zh-cn#accessing_job_driver_output) \uff09\u3002\u7531\u65bc\u8a72\u4f5c\u696d\u751f\u6210\u7684\u8f38\u51fa\u884c\u9577\u5ea6\u8d85\u51fa\u700f\u89bd\u5668\u7a97\u53e3\u5bec\u5ea6\uff0c\u56e0\u6b64\u60a8\u53ef\u4ee5\u52fe\u9078 **\u63db\u884c** \u6846\uff0c\u4ee5\u5c07\u6240\u6709\u8f38\u51fa\u6587\u672c\u7f6e\u65bc\u8996\u5716\u4e2d\uff0c\u986f\u793a `pi` \u7684\u8a08\u7b97\u7d50\u679c\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528\u5982\u4e0b\u6240\u793a\u7684 [gcloud dataproc jobs wait](https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs/wait?hl=zh-cn) \u547d\u4ee4\uff0c\u5f9e\u547d\u4ee4\u884c\u67e5\u770b\u4f5c\u696d\u7684\u9a45\u52d5\u7a0b\u5e8f\u8f38\u51fa\uff08\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u8a2a\u554f\u4f5c\u696d\u9a45\u52d5\u7a0b\u5e8f\u8f38\u51fa\u2013GCLOUD \u547d\u4ee4](https://cloud.google.com/dataproc/docs/concepts/driver-output?hl=zh-cn#accessing_job_driver_output) \uff09\u3002 \u5c07\u9805\u76ee ID \u8907\u88fd\u4e26\u7c98\u8cbc\u7232 `--project` \u6a19\u8a8c\u7684\u503c\uff0c\u4e26\u5c07\u4f5c\u696d ID\uff08\u986f\u793a\u5728\u201c\u4f5c\u696d\u201d\u5217\u8868\u4e2d\uff09\u4f5c\u7232\u6700\u7d42\u53c3\u6578\u3002\n```\ngcloud dataproc jobs wait job-id \\\n\u00a0\u00a0\u00a0\u00a0--project=project-id \\\n\u00a0\u00a0\u00a0\u00a0--region=region\n```\n\u4ee5\u4e0b\u7232\u4e0a\u8ff0\u63d0\u4ea4\u7684\u793a\u4f8b `SparkPi` \u4f5c\u696d\u7684\u9a45\u52d5\u7a0b\u5e8f\u8f38\u51fa\u7684\u7247\u6bb5\uff1a\n```\n...\n2015-06-25 23:27:23,810 INFO [dag-scheduler-event-loop]\nscheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 0 (reduce at\nSparkPi.scala:35) finished in 21.169 s\n2015-06-25 23:27:23,810 INFO [task-result-getter-3] cluster.YarnScheduler\n(Logging.scala:logInfo(59)) - Removed TaskSet 0.0, whose tasks have all\ncompleted, from pool\n2015-06-25 23:27:23,819 INFO [main] scheduler.DAGScheduler\n(Logging.scala:logInfo(59)) - Job 0 finished: reduce at SparkPi.scala:35,\ntook 21.674931 s\nPi is roughly 3.14189648\n...\nJob [c556b47a-4b46-4a94-9ba2-2dcee31167b2] finished successfully.\ndriverOutputUri:\ngs://sample-staging-bucket/google-cloud-dataproc-metainfo/cfeaa033-749e-48b9-...\n...\n```\n- [\u5b89\u88dd\u5ba2\u6236\u7aef\u5eab](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#installing_the_client_library) \n- [\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#setting_up_authentication) \n- \u904b\u884c\u4ee3\u78bc\u8acb\u53c3\u95b1 [\u8a2d\u7f6e Java \u958b\u767c\u74b0\u5883](https://cloud.google.com/java/docs/setup?hl=zh-cn) \u3002 [samples/snippets/src/main/java/SubmitJob.java](https://github.com/googleapis/java-dataproc/blob/HEAD/samples/snippets/src/main/java/SubmitJob.java) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/googleapis/java-dataproc/blob/HEAD/samples/snippets/src/main/java/SubmitJob.java) ```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.dataproc.v1.Job;import com.google.cloud.dataproc.v1.JobControllerClient;import com.google.cloud.dataproc.v1.JobControllerSettings;import com.google.cloud.dataproc.v1.JobMetadata;import com.google.cloud.dataproc.v1.JobPlacement;import com.google.cloud.dataproc.v1.SparkJob;import com.google.cloud.storage.Blob;import com.google.cloud.storage.Storage;import com.google.cloud.storage.StorageOptions;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.regex.Matcher;import java.util.regex.Pattern;public class SubmitJob {\u00a0 public static void submitJob() throws IOException, InterruptedException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String projectId = \"your-project-id\";\u00a0 \u00a0 String region = \"your-project-region\";\u00a0 \u00a0 String clusterName = \"your-cluster-name\";\u00a0 \u00a0 submitJob(projectId, region, clusterName);\u00a0 }\u00a0 public static void submitJob(String projectId, String region, String clusterName)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 String myEndpoint = String.format(\"%s-dataproc.googleapis.com:443\", region);\u00a0 \u00a0 // Configure the settings for the job controller client.\u00a0 \u00a0 JobControllerSettings jobControllerSettings =\u00a0 \u00a0 \u00a0 \u00a0 JobControllerSettings.newBuilder().setEndpoint(myEndpoint).build();\u00a0 \u00a0 // Create a job controller client with the configured settings. Using a try-with-resources\u00a0 \u00a0 // closes the client,\u00a0 \u00a0 // but this can also be done manually with the .close() method.\u00a0 \u00a0 try (JobControllerClient jobControllerClient =\u00a0 \u00a0 \u00a0 \u00a0 JobControllerClient.create(jobControllerSettings)) {\u00a0 \u00a0 \u00a0 // Configure cluster placement for the job.\u00a0 \u00a0 \u00a0 JobPlacement jobPlacement = JobPlacement.newBuilder().setClusterName(clusterName).build();\u00a0 \u00a0 \u00a0 // Configure Spark job settings.\u00a0 \u00a0 \u00a0 SparkJob sparkJob =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 SparkJob.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMainClass(\"org.apache.spark.examples.SparkPi\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addJarFileUris(\"file:///usr/lib/spark/examples/jars/spark-examples.jar\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addArgs(\"1000\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 Job job = Job.newBuilder().setPlacement(jobPlacement).setSparkJob(sparkJob).build();\u00a0 \u00a0 \u00a0 // Submit an asynchronous request to execute the job.\u00a0 \u00a0 \u00a0 OperationFuture<Job, JobMetadata> submitJobAsOperationAsyncRequest =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 jobControllerClient.submitJobAsOperationAsync(projectId, region, job);\u00a0 \u00a0 \u00a0 Job response = submitJobAsOperationAsyncRequest.get();\u00a0 \u00a0 \u00a0 // Print output from Google Cloud Storage.\u00a0 \u00a0 \u00a0 Matcher matches =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Pattern.compile(\"gs://(.*?)/(.*)\").matcher(response.getDriverOutputResourceUri());\u00a0 \u00a0 \u00a0 matches.matches();\u00a0 \u00a0 \u00a0 Storage storage = StorageOptions.getDefaultInstance().getService();\u00a0 \u00a0 \u00a0 Blob blob = storage.get(matches.group(1), String.format(\"%s.000000000\", matches.group(2)));\u00a0 \u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\"Job finished successfully: %s\", new String(blob.getContent())));\u00a0 \u00a0 } catch (ExecutionException e) {\u00a0 \u00a0 \u00a0 // If the job does not complete successfully, print the error message.\u00a0 \u00a0 \u00a0 System.err.println(String.format(\"submitJob: %s \", e.getMessage()));\u00a0 \u00a0 }\u00a0 }}\n```\n- [\u5b89\u88dd\u5ba2\u6236\u7aef\u5eab](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#installing_the_client_library) \n- [\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#setting_up_authentication) \n- \u904b\u884c\u4ee3\u78bc\u8acb\u53c3\u95b1 [\u8a2d\u7f6e Python \u958b\u767c\u74b0\u5883](https://cloud.google.com/python/docs/setup?hl=zh-cn) \u3002 [samples/snippets/submit_job.py](https://github.com/googleapis/python-dataproc/blob/HEAD/samples/snippets/submit_job.py) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/googleapis/python-dataproc/blob/HEAD/samples/snippets/submit_job.py) ```\nimport refrom google.cloud import dataproc_v1 as dataprocfrom google.cloud import storagedef submit_job(project_id, region, cluster_name):\u00a0 \u00a0 # Create the job client.\u00a0 \u00a0 job_client = dataproc.JobControllerClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options={\"api_endpoint\": \"{}-dataproc.googleapis.com:443\".format(region)}\u00a0 \u00a0 )\u00a0 \u00a0 # Create the job config. 'main_jar_file_uri' can also be a\u00a0 \u00a0 # Google Cloud Storage URL.\u00a0 \u00a0 job = {\u00a0 \u00a0 \u00a0 \u00a0 \"placement\": {\"cluster_name\": cluster_name},\u00a0 \u00a0 \u00a0 \u00a0 \"spark_job\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"main_class\": \"org.apache.spark.examples.SparkPi\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"jar_file_uris\": [\"file:///usr/lib/spark/examples/jars/spark-examples.jar\"],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"args\": [\"1000\"],\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 }\u00a0 \u00a0 operation = job_client.submit_job_as_operation(\u00a0 \u00a0 \u00a0 \u00a0 request={\"project_id\": project_id, \"region\": region, \"job\": job}\u00a0 \u00a0 )\u00a0 \u00a0 response = operation.result()\u00a0 \u00a0 # Dataproc job output gets saved to the Google Cloud Storage bucket\u00a0 \u00a0 # allocated to the job. Use a regex to obtain the bucket and blob info.\u00a0 \u00a0 matches = re.match(\"gs://(.*?)/(.*)\", response.driver_output_resource_uri)\u00a0 \u00a0 output = (\u00a0 \u00a0 \u00a0 \u00a0 storage.Client()\u00a0 \u00a0 \u00a0 \u00a0 .get_bucket(matches.group(1))\u00a0 \u00a0 \u00a0 \u00a0 .blob(f\"{matches.group(2)}.000000000\")\u00a0 \u00a0 \u00a0 \u00a0 .download_as_string()\u00a0 \u00a0 )\u00a0 \u00a0 print(f\"Job finished successfully: {output}\")\n```\n- [\u5b89\u88dd\u5ba2\u6236\u7aef\u5eab](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#installing_the_client_library) \n- [\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#setting_up_authentication) \n- \u904b\u884c\u4ee3\u78bc\u8acb\u53c3\u95b1 [\u8a2d\u7f6e Go \u958b\u767c\u74b0\u5883](https://cloud.google.com/go/docs/setup?hl=zh-cn) \u3002 [  dataproc/submit_job.go ](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/dataproc/submit_job.go) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/dataproc/submit_job.go) ```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 \"io/ioutil\"\u00a0 \u00a0 \u00a0 \u00a0 \"log\"\u00a0 \u00a0 \u00a0 \u00a0 \"regexp\"\u00a0 \u00a0 \u00a0 \u00a0 dataproc \"cloud.google.com/go/dataproc/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/storage\"\u00a0 \u00a0 \u00a0 \u00a0 \"google.golang.org/api/option\"\u00a0 \u00a0 \u00a0 \u00a0 dataprocpb \"google.golang.org/genproto/googleapis/cloud/dataproc/v1\")func submitJob(w io.Writer, projectID, region, clusterName string) error {\u00a0 \u00a0 \u00a0 \u00a0 // projectID := \"your-project-id\"\u00a0 \u00a0 \u00a0 \u00a0 // region := \"us-central1\"\u00a0 \u00a0 \u00a0 \u00a0 // clusterName := \"your-cluster\"\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // Create the job client.\u00a0 \u00a0 \u00a0 \u00a0 endpoint := fmt.Sprintf(\"%s-dataproc.googleapis.com:443\", region)\u00a0 \u00a0 \u00a0 \u00a0 jobClient, err := dataproc.NewJobControllerClient(ctx, option.WithEndpoint(endpoint))\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"error creating the job client: %s\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Create the job config.\u00a0 \u00a0 \u00a0 \u00a0 submitJobReq := &dataprocpb.SubmitJobRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ProjectId: projectID,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Region: \u00a0 \u00a0region,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Job: &dataprocpb.Job{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Placement: &dataprocpb.JobPlacement{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterName: clusterName,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TypeJob: &dataprocpb.Job_SparkJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 SparkJob: &dataprocpb.SparkJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Driver: &dataprocpb.SparkJob_MainClass{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MainClass: \"org.apache.spark.examples.SparkPi\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 JarFileUris: []string{\"file:///usr/lib/spark/examples/jars/spark-examples.jar\"},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Args: \u00a0 \u00a0 \u00a0 \u00a0[]string{\"1000\"},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 submitJobOp, err := jobClient.SubmitJobAsOperation(ctx, submitJobReq)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"error with request to submitting job: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 submitJobResp, err := submitJobOp.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"error submitting job: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 re := regexp.MustCompile(\"gs://(.+?)/(.+)\")\u00a0 \u00a0 \u00a0 \u00a0 matches := re.FindStringSubmatch(submitJobResp.DriverOutputResourceUri)\u00a0 \u00a0 \u00a0 \u00a0 if len(matches) < 3 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"regex error: %s\", submitJobResp.DriverOutputResourceUri)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Dataproc job output gets saved to a GCS bucket allocated to it.\u00a0 \u00a0 \u00a0 \u00a0 storageClient, err := storage.NewClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"error creating storage client: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 obj := fmt.Sprintf(\"%s.000000000\", matches[2])\u00a0 \u00a0 \u00a0 \u00a0 reader, err := storageClient.Bucket(matches[1]).Object(obj).NewReader(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"error reading job output: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer reader.Close()\u00a0 \u00a0 \u00a0 \u00a0 body, err := ioutil.ReadAll(reader)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"could not read output from Dataproc Job: %v\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Job finished successfully: %s\", body)\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```\n- [\u5b89\u88dd\u5ba2\u6236\u7aef\u5eab](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#installing_the_client_library) \n- [\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da](https://cloud.google.com/dataproc/docs/reference/libraries?hl=zh-cn#setting_up_authentication) \n- \u904b\u884c\u4ee3\u78bc\u8acb\u53c3\u95b1 [\u8a2d\u7f6e Node.js \u958b\u767c\u74b0\u5883](https://cloud.google.com/nodejs/docs/setup?hl=zh-cn) \u3002 [  samples/submitJob.js ](https://github.com/googleapis/nodejs-dataproc/blob/HEAD/samples/submitJob.js) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/googleapis/nodejs-dataproc/blob/HEAD/samples/submitJob.js) ```\nconst dataproc = require('@google-cloud/dataproc');const {Storage} = require('@google-cloud/storage');// TODO(developer): Uncomment and set the following variables// projectId = 'YOUR_PROJECT_ID'// region = 'YOUR_CLUSTER_REGION'// clusterName = 'YOUR_CLUSTER_NAME'// Create a client with the endpoint set to the desired cluster regionconst jobClient = new dataproc.v1.JobControllerClient({\u00a0 apiEndpoint: `${region}-dataproc.googleapis.com`,\u00a0 projectId: projectId,});async function submitJob() {\u00a0 const job = {\u00a0 \u00a0 projectId: projectId,\u00a0 \u00a0 region: region,\u00a0 \u00a0 job: {\u00a0 \u00a0 \u00a0 placement: {\u00a0 \u00a0 \u00a0 \u00a0 clusterName: clusterName,\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 sparkJob: {\u00a0 \u00a0 \u00a0 \u00a0 mainClass: 'org.apache.spark.examples.SparkPi',\u00a0 \u00a0 \u00a0 \u00a0 jarFileUris: [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'file:///usr/lib/spark/examples/jars/spark-examples.jar',\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 args: ['1000'],\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 };\u00a0 const [jobOperation] = await jobClient.submitJobAsOperation(job);\u00a0 const [jobResponse] = await jobOperation.promise();\u00a0 const matches =\u00a0 \u00a0 jobResponse.driverOutputResourceUri.match('gs://(.*?)/(.*)');\u00a0 const storage = new Storage();\u00a0 const output = await storage\u00a0 \u00a0 .bucket(matches[1])\u00a0 \u00a0 .file(`${matches[2]}.000000000`)\u00a0 \u00a0 .download();\u00a0 // Output a success message.\u00a0 console.log(`Job finished successfully: ${output}`);\n```\n## \u5728\u96c6\u7fa3\u4e0a\u76f4\u63a5\u63d0\u4ea4\u4f5c\u696d\n\u5982\u679c\u60a8\u60f3\u4e0d\u4f7f\u7528 Dataproc \u670d\u52d9\u800c\u5728\u96c6\u7fa3\u4e0a\u76f4\u63a5\u904b\u884c\u4f5c\u696d\uff0c\u8acb\u901a\u904e SSH \u9023\u63a5\u5230\u96c6\u7fa3\u7684\u4e3b\u7bc0\u9ede\uff0c\u7136\u5f8c\u5728\u4e3b\u7bc0\u9ede\u4e0a\u904b\u884c\u4f5c\u696d\u3002\n### \u901a\u904e SSH \u9023\u63a5\u5230\u4e3b\u5be6\u4f8b\n\u60a8\u53ef\u4ee5\u901a\u904e SSH \u5f9e\u547d\u4ee4\u884c\u6216 Cloud Console \u9023\u63a5\u5230\u96c6\u7fa3\u4e2d\u7684 Compute Engine \u865b\u64ec\u6a5f\u5be6\u4f8b\u3002\n\u5728\u672c\u5730\u7d42\u7aef\u7a97\u53e3\u6216\u5f9e [Cloud Shell](https://cloud.google.com/sdk/gcloud/reference/compute/ssh?hl=zh-cn) \u904b\u884c [gcloud compute ssh](https://console.cloud.google.com/?cloudshell=true&hl=zh-cn) \u547d\u4ee4\uff0c\u901a\u904e SSH \u9023\u63a5\u5230\u96c6\u7fa3\u7684\u4e3b\u7bc0\u9ede\uff08\u4e3b\u7bc0\u9ede\u7684\u9ed8\u8a8d\u540d\u7a31\u7232\u96c6\u7fa3\u540d\u52a0\u4e0a `-m` \u5f8c\u7db4\uff09\u3002\n```\ngcloud compute ssh cluster-name-m \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--project=project-id\n```\n\u4ee5\u4e0b\u4ee3\u78bc\u6bb5\u4f7f\u7528 `gcloud compute ssh` \uff0c\u901a\u904e SSH \u9023\u63a5\u5230 `cluster-1` \u7684\u4e3b\u7bc0\u9ede\u3002\n```\ngcloud compute ssh cluster-1-m \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central-1 \\\n\u00a0\u00a0\u00a0\u00a0--project=my-project-id\n...\nLinux cluster-1-m 4.9.0-8-amd64 #1 SMP Debian 4.9.110-3+deb9u6...\n...\nuser@cluster-1-m:~$\n```\n\u4f7f\u7528 Cloud Console\uff0c\u901a\u904e SSH \u9023\u63a5\u5230\u96c6\u7fa3\u7684\u4e3b\u7bc0\u9ede\uff08\u4e3b\u7bc0\u9ede\u7684\u9ed8\u8a8d\u540d\u7a31\u7232\u96c6\u7fa3\u540d\u52a0\u4e0a\n`-m`\n\u5f8c\u7db4\uff09\u3002\n- \u5728 Cloud Console \u4e2d\uff0c\u8f49\u5230 [\u865b\u64ec\u6a5f\u5be6\u4f8b](https://console.cloud.google.com/compute/instances?hl=zh-cn) \u9801\u9762\u3002\n- \u5728\u865b\u64ec\u6a5f\u5be6\u4f8b\u5217\u8868\u4e2d\uff0c\u5728\u60a8\u5e0c\u671b\u9023\u63a5\u7684\u4e3b\u5be6\u4f8b\u884c\uff08 **-m** \u5f8c\u7db4\uff09\u4e2d\u9ede\u64ca SSH\u3002\n\u6b64\u6642\u6703\u6253\u958b\u4e00\u500b\u700f\u89bd\u5668\u7a97\u53e3\u4e26\u986f\u793a\u4e3b\u7bc0\u9ede\u4e0a\u7684\u4e3b\u76ee\u9304\u3002\n```\nConnected, host fingerprint: ssh-rsa ...\nLinux cluster-1-m 3.16.0-0.bpo.4-amd64 ...\n...\nuser@cluster-1-m:~$\n```\n\u60a8\u9084\u53ef\u4ee5\u5f9e Cloud Console \u4e0a\u96c6\u7fa3\u7684 Dataproc **\u96c6\u7fa3\u8a73\u7d30\u4fe1\u606f** \u9801\u9762\uff0c\u901a\u904e SSH \u9023\u63a5\u5230\u96c6\u7fa3\u7684\u4e3b\u7bc0\u9ede\u3002\n### \u5728\u4e3b\u7bc0\u9ede\u4e0a\u904b\u884c Spark \u4f5c\u696d\n\u5efa\u7acb\u8207\u865b\u64ec\u6a5f\u4e3b\u5be6\u4f8b\u7684 SSH \u9023\u63a5\u5f8c\uff0c\u5728\u7d42\u7aef\u7a97\u53e3\u4e2d\u96c6\u7fa3\u7684\u4e3b\u7bc0\u9ede\u4e0a\u904b\u884c\u547d\u4ee4\uff0c\u4ee5\u57f7\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a\n- \u6253\u958b Spark shell\u3002\n- \u904b\u884c\u4e00\u500b\u7c21\u55ae\u7684 Spark \u4f5c\u696d\uff0c\u7528\u65bc\u8a08\u7b97\u53ef\u516c\u958b\u8a2a\u554f\u7684 Cloud Storage \u6587\u4ef6\u4e2d\u7684 Python\u201chello-world\u201d\u6587\u4ef6\uff08\u4e03\u884c\uff09\u7684\u884c\u6578\u3002\n- \u9000\u51fa shell\u3002```\nuser@cluster-name-m:~$ spark-shell\n...\nscala> sc.textFile(\"gs://dataproc-examples\"\n+ \"/pyspark/hello-world/hello-world.py\").count\n...\nres0: Long = 7\nscala> :quit\n```## \u5728 Dataproc \u4e0a\u904b\u884c bash \u4f5c\u696d\n\u60a8\u53ef\u80fd\u5e0c\u671b\u5c07 bash \u8173\u672c\u4f5c\u7232 Dataproc \u4f5c\u696d\u904b\u884c\uff0c\u56e0\u7232\u4e0d\u652f\u6301\u5c07\u60a8\u4f7f\u7528\u7684\u5f15\u64ce\u7528\u4f5c\u9802\u7d1a Dataproc \u4f5c\u696d\u985e\u578b\uff0c\u6216\u8005\u56e0\u7232\u60a8\u9700\u8981\u5728\u4f7f\u7528\u8173\u672c\u4e2d\u7684 `hadoop` \u6216 `spark-submit` \u5553\u52d5\u4f5c\u696d\u4e4b\u524d\u5148\u9032\u884c\u5176\u4ed6\u8a2d\u7f6e\u6216\u8a08\u7b97\u53c3\u6578\u3002\n### Pig \u793a\u4f8b\n\u5047\u8a2d\u60a8\u5df2\u5c07 hello.sh bash \u8173\u672c\u8907\u88fd\u5230 Cloud Storage\uff1a\n```\ngsutil cp hello.sh gs://${BUCKET}/hello.sh\n```\n\u7531\u65bc `pig fs` \u547d\u4ee4\u4f7f\u7528 Hadoop \u8def\u5f91\uff0c\u56e0\u6b64\u8acb\u5c07\u8173\u672c\u5f9e Cloud Storage \u8907\u88fd\u5230\u6307\u5b9a\u7232 `file:///` \u7684\u76ee\u7684\u5730\uff0c\u4ee5\u78ba\u4fdd\u5b83\u4f4d\u65bc\u672c\u5730\u6587\u4ef6\u7cfb\u7d71\u800c\u975e HDFS \u4e0a\u3002\u5f8c\u7e8c `sh` \u547d\u4ee4\u6703\u81ea\u52d5\u5f15\u7528\u672c\u5730\u6587\u4ef6\u7cfb\u7d71\uff0c\u4e26\u4e14\u4e0d\u9700\u8981 `file:///` \u524d\u7db4\u3002\n```\ngcloud dataproc jobs submit pig --cluster=${CLUSTER} --region=${REGION} \\\u00a0 \u00a0 -e='fs -cp -f gs://${BUCKET}/hello.sh file:///tmp/hello.sh; sh chmod 750 /tmp/hello.sh; sh /tmp/hello.sh'\n```\n\u6216\u8005\uff0c\u7531\u65bc Dataproc \u4f5c\u696d\u63d0\u4ea4 `--jars` \u53c3\u6578\u5c07\u6587\u4ef6\u66ab\u5b58\u5230\u7232\u4f5c\u696d\u751f\u547d\u9031\u671f\u5275\u5efa\u7684\u81e8\u6642\u76ee\u9304\u4e2d\uff0c\u56e0\u6b64\u60a8\u53ef\u4ee5\u5c07 Cloud Storage shell \u8173\u672c\u6307\u5b9a\u7232 `--jars` \u53c3\u6578\uff1a\n```\ngcloud dataproc jobs submit pig --cluster=${CLUSTER} --region=${REGION} \\\u00a0 \u00a0 --jars=gs://${BUCKET}/hello.sh \\\u00a0 \u00a0 -e='sh chmod 750 ${PWD}/hello.sh; sh ${PWD}/hello.sh'\n```\n\u8acb\u6ce8\u610f\uff0c `--jars` \u53c3\u6578\u9084\u53ef\u4ee5\u5f15\u7528\u672c\u5730\u8173\u672c\uff1a\n```\ngcloud dataproc jobs submit pig --cluster=${CLUSTER} --region=${REGION} \\\u00a0 \u00a0 --jars=hello.sh \\\u00a0 \u00a0 -e='sh chmod 750 ${PWD}/hello.sh; sh ${PWD}/hello.sh'\n```", "guide": "Dataproc"}