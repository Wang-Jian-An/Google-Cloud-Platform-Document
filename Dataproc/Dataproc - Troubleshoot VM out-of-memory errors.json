{"title": "Dataproc - Troubleshoot VM out-of-memory errors", "url": "https://cloud.google.com/dataproc/docs/support/troubleshoot-oom-errors", "abstract": "# Dataproc - Troubleshoot VM out-of-memory errors\n**Objective:** This page provides information on Dataproc on Compute Engine VM out-of-memory (OOM) errors, and explains steps you can take to troubleshoot and resolve OOM errors.\n", "content": "## OOM effects\nWhen Dataproc on Compute Engine VMs encounter out-of-memory (OOM) errors:\n- Master and worker VMs freeze for a period of time.\n- Master VMs OOM errors cause jobs to fail with \"task not acquired\" error.\n- Worker VM OOM errors cause a loss of the node on YARN HDFS, which delays Dataproc job execution.## Yarn memory controls\nYarn provides three types of [memory controls](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/NodeManagerCGroupsMemory.html) :\n- Polling based (legacy)\n- Strict\n- Elastic\nBy default, Dataproc doesn't set `yarn.nodemanager.resource.memory.enabled` to enable YARN memory controls, for the following reasons:\n- Strict memory control can cause the termination of containers when there is sufficient memory if the container sizes aren't configured correctly.\n- Elastic memory control requirements can adversely affect job execution.\n- YARN memory controls can fail to prevent OOM errors when processes aggressively consume memory.## Dataproc memory protection\nWhen a Dataproc cluster VM is under memory pressure, Dataproc memory protection terminates processes or containers until the OOM condition is removed.\nDataproc memory protection is provided for following cluster nodes in the following [Dataproc on Compute Engine image versions](/dataproc/docs/concepts/versioning/dataproc-version-clusters) :\n| Role   | 1.5   | 2.0  | 2.1  |\n|:---------------|:--------------|:--------|:--------|\n| Master VM  | 1.5.74+  | 2.0.48+ | all  |\n| Worker VM  | Not Available | 2.0.76+ | 2.1.24+ |\n| Driver Pool VM | Not Available | 2.0.76+ | 2.1.24+ |\n**Recommendation:** Use Dataproc image versions with memory protection to help avoid VM OOM errors.\n### How to identify Dataproc memory protection terminations\n- Processes that Dataproc memory protection terminates exit with code`137`or`143`.\n- Worker node termination:- Dataproc increments the`dataproc.googleapis.com/node/problem_count`cumulative metric, and sets the`reason`to`ProcessKilledDueToMemoryPressure`.\n- If Cloud Logging is enabled, Dataproc writes a`google.dataproc.oom-killer`log with \"A process is killed due to memory pressure: [process name].\"\n- If a YARN container is terminated, Dataproc writes the following message in the YARN resource manager: \"[container id] exited with code 137, which potentially signifies a memory pressure on [node id]\"\n- Master or driver pool node termination: the job driver fails with`Driver received SIGTERM/SIGKILL signal and exited with [INT] code`.## OOM solutions\nThis section offers recommendations for job and container terminations that can result from OOM issues.\n**Job fails with \"Driver received SIGTERM/SIGKILL signal and exited with [INT] code\"**\nRecommendations:\n- If the clusters has a [driver pool](/dataproc/docs/guides/node-groups/dataproc-driver-node-groups) , increase`driver-required-memory-mb`to acutal job memory usage.\n- If the cluster does not have a driver pool, recreate the cluster, lowering the maximum number of concurrent jobs, which is calculated as`(total master memory in MB - 3584MB) / driver-size-mb`. You can lower this number by:- Setting`dataproc:dataproc.scheduler.max-concurrent-jobs`, or\n- Setting`dataproc:dataproc.scheduler.driver-size-mb`to a larger number (the default is`1024MB`).\n- Consider using a master node machine type with additional memory.\n**Container exited with exit code 137 or 143**\nRecommendations:\n- If Dataproc memory protection terminated the container (see [How to identify Dataproc memory protection terminations](#how_to_identify_memory_protection_terminations) ):- Check that container sizes are configured correctly.\n- Consider lowering`yarn.nodemanager.resource.memory-mb`. This property controls the amount of memory used for scheduling YARN containers.\n- If job containers consistently fail, check whether data skew is causing increased usage of specific containers. If so, repartition the job or increase worker size to accommodate additional memory requirements.", "guide": "Dataproc"}