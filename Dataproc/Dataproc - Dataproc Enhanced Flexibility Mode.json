{"title": "Dataproc - Dataproc Enhanced Flexibility Mode", "url": "https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/enhanced-flexibility-mode", "abstract": "# Dataproc - Dataproc Enhanced Flexibility Mode\nDataproc Enhanced Flexibility Mode (EFM) manages shuffle data to minimize job progress delays caused by the removal of nodes from a running cluster. EFM offloads shuffle data in one of two user-selectable modes:\n- Primary-worker shuffle. Mappers write data to primary workers. Workers pull from those remote nodes during the reduce phase. This mode is only available to, and is recommended for, Spark jobs.\n- HCFS (Hadoop Compatible File System) shuffle. Mappers write data to an HCFS implementation ( [HDFS](https://hadoop.apache.org/docs/current1/hdfs_design.html) by default). As with primary worker mode, only primary workers participate in HDFS and HCFS implementations (if HCFS shuffle uses the [Cloud Storage Connector](/dataproc/docs/concepts/connectors/cloud-storage) , data is stored off-cluster). This mode can benefit jobs with small amounts of data, but due to scaling limitations, it is not recommended for larger jobs.\nSince both EFM modes do not store intermediate shuffle data on secondary workers, EFM is well suited to clusters that use [preemptible VMs](/dataproc/docs/concepts/compute/preemptible-vms) or only [autoscale](/dataproc/docs/concepts/configuring-clusters/autoscaling) the secondary worker group.\n[](None) **Limitations:**\n- [Apache Hadoop YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) jobs that do not support AppMaster relocation can fail in Enhanced Flexibility Mode (see [When to wait for AppMasters to finish](#app-master-wait) ).\n- Enhanced Flexibility Mode is **not recommended** :- on a cluster that has primary workers only.\n- Enhanced Flexibility Mode is **not supported** :- when primary worker autoscaling is enabled. In most cases, primary workers will continue to store shuffle data that is not automatically migrated. Downscaling the primary worker group negates EFM benefits.\n- when Spark jobs run on a cluster with graceful decommissioning enabled. Graceful decommissioning and EFM can work at cross purposes since the YARN graceful decommission mechanism keeps the DECOMMISSIONING nodes until all involved applications complete.\n", "content": "## Using Enhanced Flexibility Mode\nEnhanced Flexibility mode is configured per execution engine, and must be configured during cluster creation.\n- The Spark EFM implementation is configured with the `dataproc:efm.spark.shuffle` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties) . Valid property values:- `primary-worker`for primary worker shuffle (recommended)\n- `hcfs`for HCFS-based shuffle. This mode is **deprecated** and is available only on clusters running image version [1.5](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.5) . Not recommended for new workflows.\n- The Hadoop MapReduce implementation is configured with the `dataproc:efm.mapreduce.shuffle` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties) . Valid property values:- `hcfs`**Example:** Create a cluster with primary-worker shuffle for Spark and HCFS shuffle for MapReduce:\n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--properties=dataproc:efm.spark.shuffle=primary-worker \\\n\u00a0\u00a0\u00a0\u00a0--properties=dataproc:efm.mapreduce.shuffle=hcfs \\\n\u00a0\u00a0\u00a0\u00a0--worker-machine-type=n1-highmem-8 \\\n\u00a0\u00a0\u00a0\u00a0--num-workers=25 \\\n\u00a0\u00a0\u00a0\u00a0--num-worker-local-ssds=2 \\\n\u00a0\u00a0\u00a0\u00a0--secondary-worker-type=preemptible \\\n\u00a0\u00a0\u00a0\u00a0--secondary-worker-boot-disk-size=500GB \\\n\u00a0\u00a0\u00a0\u00a0--num-secondary-workers=25\n```\n### Apache Spark example\n- Run a WordCount job against public Shakespeare text using the Spark examples jar on the EFM cluster.```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0--class=org.apache.spark.examples.JavaWordCount \\\n\u00a0\u00a0\u00a0\u00a0-- gs://apache-beam-samples/shakespeare/macbeth.txt\n```\n### Apache Hadoop MapReduce example\n- Run a small teragen job to generate input data in Cloud Storage for a later terasort job using the mapreduce examples jar on the EFM cluster.```\ngcloud dataproc jobs submit hadoop \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--jar=file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0-- teragen 1000 Cloud Storage output URI (for example, gs://terasort/input)\n```\n- Run a terasort job on the data.```\ngcloud dataproc jobs submit hadoop \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--jar=file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0-- terasort gs://terasort/input gs://terasort/output\n```\n### Configuring Local SSDs for primary worker shuffle\nPrimary-worker and HDFS shuffle implementations write intermediate shuffle data to VM-attached disks, and benefit from the additional throughput and IOPS offered by [local SSDs](/compute/docs/disks/local-ssd) . To facilitate resource allocation, target a goal of approximately 1 local SSD partition per 4 vCPUs when configuring primary worker machines.\nTo attach local SSDs, pass the `--num-worker-local-ssds` flag to the [gcloud dataproc clusters create](/sdk/gcloud/reference/dataproc/clusters/create) command.\n**Generally, users will not need Local SSDs on secondary workers.** Adding local SSDs to a cluster's secondary workers (using the`--num-secondary-worker-local-ssds`flag) is often of less importance because secondary workers do not write shuffle data locally. However, since local SSDs improve local disk performance, you may decide to add local SSDs to secondary workers if you expect jobs to be [I/Obound](https://en.wikipedia.org/wiki/I/O_bound) due to local disk use: your job uses significant local disk for scratch space or your partitions are [too large to fit inmemory and will spill to disk](https://spark.apache.org/faq.html) .\n### Secondary worker ratio\nSince secondary workers write their shuffle data to primary workers, your cluster must contain a sufficient number of primary workers with sufficient CPU, memory, and disk resources to accommodate your job's shuffle load. For autoscaling clusters, to prevent the primary group from scaling and causing unwanted behavior, set `minInstances` to the `maxInstances` value in the [autoscaling policy](/dataproc/docs/reference/rest/v1/projects.locations.autoscalingPolicies#AutoscalingPolicy.InstanceGroupAutoscalingPolicyConfig) for the primary worker group.\nConsider starting with a conservative 1:1 ratio of primary to secondary workers (your workflow may be able to tolerate additional secondary workers).\nIf you have a high secondary-to-primary workers ratio (for example, 10:1), monitor the CPU utilization, network, and disk usage of primary workers to determine if they are overloaded. To do this:\n- Go to the [VM instances](https://console.cloud.google.com/compute/instances) page in the Google Cloud console.\n- Click the check box to the left side of primary worker.\n- Click the MONITORING tab to view the primary worker's CPU Utilization, Disk IOPS, Network Bytes, and other metrics.\nIf primary workers are overloaded, consider [scaling up primaryworkers manually](/dataproc/docs/concepts/configuring-clusters/flex#resizing_the_primary_worker_group) .\n### Resizing the primary worker group\nThe primary worker group can be safely scaled up, but downscaling the primary worker group can negatively impact job progress. Operations that downscale the primary worker group should use [graceful decommissioning](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/scaling-clusters#graceful_decommissioning) , which is enabled by setting the `--graceful-decommission-timeout` flag.\n**Autoscaled clusters:** Primary worker group scaling is disabled on EFM clusters with autoscaling policies. To resize the primary worker group on an autoscaled cluster:\n- Disable autoscaling.```\ngcloud dataproc clusters update \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--disable-autoscaling\n```\n- Scale the primary group.```\ngcloud dataproc clusters update \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--num-workers=num-primary-workers \\\n\u00a0\u00a0\u00a0\u00a0--graceful-decommission-timeout=graceful-decommission-timeout # (if downscaling)\n```\n- Re-enable autoscaling:```\ngcloud dataproc clusters update \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--autoscaling-policy=autoscaling-policy\n```\n### Monitoring primary worker disk use\nPrimary workers must have sufficient disk space for the cluster's shuffle data. You can monitor this indirectly through the `remaining HDFS capacity` metric. As local disk fills up, space becomes unavailable for HDFS, and the remaining capacity decreases.\nBy default, when a primary worker's local disk use [exceeds 90% of capacity](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/NodeManager.html#Disk_Checker) , the node will be marked as UNHEALTHY in the YARN node UI. If you experience disk capacity issues, you can delete unused data from HDFS or scale up the primary worker pool.\nNote that intermediate shuffle data is generally not cleaned up until the end of a job. When using primary worker shuffle with Spark, this can take up to 30 minutes after a job's completion.\n## Advanced configuration\n### Partitioning and parallelism\nWhen submitting a MapReduce or Spark job, configure an appropriate level of partitioning. Deciding on the number of input and output partitions for a shuffle stage involves a trade off among different performance characteristics. It is best to experiment with values that work for your job shapes.\n**Input partitions**\nMapReduce and Spark input partitioning are determined by the input data set. When reading files from Cloud Storage, each task processes approximately one \"block size\" worth of data.\n- For Spark SQL jobs, the maximum partition size is controlled by `spark.sql.files.maxPartitionBytes` . Consider increasing it to 1GB: `spark.sql.files.maxPartitionBytes=1073741824` .\n- For MapReduce jobs and Spark RDDs, partition size is typically controlled with `fs.gs.block.size` , which defaults to 128MB. Consider increasing it to 1GB. You can also set `InputFormat` specific properties such as `mapreduce.input.fileinputformat.split.minsize` and `mapreduce.input.fileinputformat.split.maxsize`- For MapReduce jobs:`--properties fs.gs.block.size=1073741824`\n- For Spark RDDs:`--properties spark.hadoop.fs.gs.block.size=1073741824`**Output partitions**\nThe number of tasks in subsequent stages is controlled by several properties. On larger jobs that process more than 1TB, consider having at least 1GB per partition.\n- For MapReduce jobs, the number of output partitions is controlled by `mapreduce.job.reduces` .\n- For Spark SQL, the number of output partitions is controlled by `spark.sql.shuffle.partitions` .\n- For Spark jobs using the RDD API, you can specify the number of output partitions or set `spark.default.parallelism` .\n### Shuffle tuning for primary worker shuffle\nThe most significant property is `--properties yarn:spark.shuffle.io.serverThreads=<num-threads>` . Note that this is a cluster-level YARN property because the Spark shuffle server runs as part of the Node Manager. It defaults to twice (2x) number of cores on the machine (for example, 16 threads on an n1-highmem-8). If \"Shuffle Read Blocked Time\" is larger than 1 second, and primary workers have not reached network, CPU or disk limits, consider increasing the number of shuffle server threads.\nOn larger machine types, consider increasing `spark.shuffle.io.numConnectionsPerPeer` , which defaults to 1. (For example, set it to 5 connections per pair of hosts).\n### Increasing retries\nThe maximum number of attempts permitted for app masters, tasks, and stages can be configured by setting the following properties:\n```\nyarn:yarn.resourcemanager.am.max-attempts\nmapred:mapreduce.map.maxattempts\nmapred:mapreduce.reduce.maxattempts\nspark:spark.task.maxFailures\nspark:spark.stage.maxConsecutiveAttempts\n```\nSince app masters and tasks are more frequently terminated in clusters that use many preemptible VMs or autoscaling without graceful decommissioning, increasing the values of the above properties in those clusters can help (note that [using EFM with Spark and graceful decommissioning isnot supported](#limitations) ).\n### Configuring HDFS For HCFS shuffle\nTo improve the performance of large shuffles, you can decrease lock contention in the NameNode by setting `dfs.namenode.fslock.fair=false` . Note that this risks starving individual requests, but may improve cluster-wide throughput. To further improve NameNode performance, you can attach local SSDs to the master node by setting `--num-master-local-ssds` . You can also add local SSDs to primary workers to improve DataNode performance by setting `--num-worker-local-ssds` .\n### Other Hadoop Compatible File Systems for HCFS shuffle\nBy default, EFM HCFS shuffle data is written to HDFS, but you can use any [Hadoop Compatible File System (HCFS)](https://cwiki.apache.org/confluence/display/HADOOP2/HCFS) . For example, you may decide to write shuffle to Cloud Storage or to a different cluster's HDFS. To specify a file system, you can point `fs.defaultFS` to the target file system when you submit a job to your cluster.\n## YARN graceful decommissioning on EFM clusters\n[YARN Graceful Decommissioning](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/GracefulDecommission.html) can be used to remove nodes quickly with minimal impact on running applications. For autoscaling clusters, the [graceful decommissioning timeout](/dataproc/docs/concepts/configuring-clusters/scaling-clusters#using_graceful_decommissioning) can be set in an [AutoscalingPolicy](/dataproc/docs/concepts/configuring-clusters/autoscaling#create_an_autoscaling_policy) that is attached to the EFM cluster.\n### MapReduce EFM enhancements to graceful decommissioning\n- Since intermediate data is stored in a distributed file system, nodes can be removed from an EFM cluster as soon as all containers running on those nodes have finished. By comparison, nodes are not removed on standard Dataproc clusters until the application has finished.\n- Node removal does not wait for app masters running on a node to finish. When the app master container is terminated, it is rescheduled on another node that is not being decommissioned. Job progress is not lost: the new app master quickly recovers state from the previous app master by reading job history.\n**\nWhen to wait for AppMasters to finish** . If you run applications that do not support AppMaster recovery, your application can wait for AppMasters to finish by setting the following [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties) to **true** when you create the Enhanced Flexibility Mode cluster.```\nyarn:yarn.resourcemanager.decommissioning-nodes-watcher.wait-for-app-masters=true\n```\n**Avoid using graceful decommissioning with Spark.** The YARN graceful decommission mechanism keeps the DECOMMISSIONING nodes until all involved applications complete. This effectively delays or blocks downscaling when Spark jobs run in some scenarios.\n## Using graceful decommissioning on an EFM cluster with MapReduce\n- Create an EFM cluster with an equal number of primary and secondary workers.```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--properties=dataproc:efm.mapreduce.shuffle=hcfs \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--num-workers=5 \\\n\u00a0\u00a0\u00a0\u00a0--num-secondary-workers=5\n```\n- Run a mapreduce job that calculates the value of pi using the mapreduce examples jar on the cluster.```\ngcloud dataproc jobs submit hadoop \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--jar=file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0-- pi 1000 10000000\n```\n- While the job is running, scale down the cluster using graceful decommissioning.```\ngcloud dataproc clusters update cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--num-secondary-workers=0 \\\n\u00a0\u00a0\u00a0\u00a0--graceful-decommission-timeout=1h\n```The nodes will be removed from the cluster quickly before the job finishes, while minimizing loss of job progress. Temporary pauses in job progress can occur due to:- If job progress drops to 0% and then immediately jumps up to the pre-drop value, the app master may have terminated and a new app master recovered its state. This should not significantly affect the progress of the job since failover happens quickly.\n- Since HDFS preserves only complete, not partial, map task outputs, temporary pauses in job progress can occur when a VM is preempted while working on a map task.To speed up node removal, you can scale down the cluster without graceful decommissioning by omitting the`--graceful-decommission-timeout`flag in the above`gcloud`command. Job progress from completed map tasks will be preserved, but partially completed map task output will be lost (the map tasks will be rerun).", "guide": "Dataproc"}