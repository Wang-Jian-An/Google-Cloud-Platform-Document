{"title": "Dataproc - Monte Carlo methods using Dataproc and Apache Spark", "url": "https://cloud.google.com/dataproc/docs/tutorials/monte-carlo-methods-with-hadoop-spark", "abstract": "# Dataproc - Monte Carlo methods using Dataproc and Apache Spark\n[Dataproc](/dataproc) and [Apache Spark](https://spark.apache.org/) provide infrastructure and capacity that you can use to run Monte Carlo simulations written in Java, Python, or Scala.\nMonte Carlo methods can help answer a wide range of questions in business, engineering, science, mathematics, and other fields. By using repeated random sampling to create a probability distribution for a variable, a Monte Carlo simulation can provide answers to questions that might otherwise be impossible to answer. In finance, for example, pricing an equity option requires analyzing the thousands of ways the price of the stock could change over time. Monte Carlo methods provide a way to simulate those stock price changes over a wide range of possible outcomes, while maintaining control over the domain of possible inputs to the problem.\nIn the past, running thousands of simulations could take a very long time and accrue high costs. Dataproc enables you to provision capacity on demand and pay for it by the minute. Apache Spark lets you use clusters of tens, hundreds, or thousands of servers to run simulations in a way that is intuitive and scales to meet your needs. This means that you can run more simulations more quickly, which can help your business innovate faster and manage risk better.\nSecurity is always important when working with financial data. Dataproc runs on Google Cloud, which helps to keep your data [safe, secure, and private](/security/overview) in several ways. For example, all data is encrypted during transmission and when at rest, and Google Cloud is [ISO 27001, SOC3, and PCI compliant](/security/compliance) .", "content": "## Objectives\n- Create a managed Dataproc cluster with [Apache Spark pre-installed](/dataproc/docs/concepts/dataproc-versions#supported_cloud_dataproc_versions) .\n- Run a Monte Carlo simulation using Python that estimates the growth of a stock portfolio over time.\n- Run a Monte Carlo simulation using Scala that simulates how a casino makes money.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Compute Engine](/compute/pricing) \n- [Dataproc](/dataproc/docs/resources/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- Set up a Google Cloud project\n## Creating a Dataproc clusterFollow the steps to [create a Dataproc cluster](/dataproc/docs/guides/create-cluster#creating_a_cloud_dataproc_cluster) from the Google Cloud console. The default cluster settings, which includes two-worker nodes, is sufficient for this tutorial.## Disabling logging for warningsBy default, Apache Spark prints verbose logging in the console window. For the purpose of this tutorial, change the logging level to log only errors. Follow these steps:\n### Use ssh to connect to the Dataproc cluster's primary nodeThe primary node of the Dataproc cluster has the `-m` suffix on its VM name.- In the Google Cloud console, go to the **VM instances** page. [Go to VM instances](https://console.cloud.google.com/compute/instances) \n- In the list of virtual machine instances, click **SSH** in the row of  the instance that you want to connect to.An SSH window opens connected to the primary node.\n```\nConnected, host fingerprint: ssh-rsa 2048 ...\n...\nuser@clusterName-m:~$\n```\n### Change the logging setting\n- From the primary node's home directory, edit `/etc/spark/conf/log4j.properties` .```\nsudo nano /etc/spark/conf/log4j.properties\n```\n- Set `log4j.rootCategory` equal to `ERROR` .```\n# Set only errors to be logged to the consolelog4j.rootCategory=ERROR, consolelog4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n```\n- Save the changes and exit the editor. If you want to enable verbose logging again, reverse the change by restoring the value for `.rootCategory` to its original ( `INFO` ) value.\n## Spark programming languagesSpark supports Python, Scala, and Java as programming languages for standalone applications, and provides interactive interpreters for Python and Scala. The language you choose is a matter of personal preference. This tutorial uses the interactive interpreters because you can experiment by changing the code, trying different input values, and then viewing the results.## Estimating portfolio growthIn finance, Monte Carlo methods are sometimes used to run simulations that try to predict how an investment might perform. By producing random samples of outcomes over a range of probable market conditions, a Monte Carlo simulation can answer questions about how a portfolio might perform on average or in worst-case scenarios.\nFollow these steps to create a simulation that uses Monte Carlo methods to try to estimate the growth of a financial investment based on a few common market factors.\n **Note:** This code is provided only as an example. Don't use this code to make investment decisions.- Start the Python interpreter from the Dataproc primary node.```\npyspark\n```Wait for the Spark prompt `>>>` .\n- Enter the following code. Make sure you maintain the indentation in the function definition.```\nimport randomimport timefrom operator import adddef grow(seed):\u00a0 \u00a0 random.seed(seed)\u00a0 \u00a0 portfolio_value = INVESTMENT_INIT\u00a0 \u00a0 for i in range(TERM):\u00a0 \u00a0 \u00a0 \u00a0 growth = random.normalvariate(MKT_AVG_RETURN, MKT_STD_DEV)\u00a0 \u00a0 \u00a0 \u00a0 portfolio_value += portfolio_value * growth + INVESTMENT_ANN\u00a0 \u00a0 return portfolio_value\n```\n- Press `return` until you see the Spark prompt again.The preceding code defines a function that models what might happen when an investor has an existing retirement account that is invested in the stock market, to which they add additional money each year. The function generates a random return on the investment, as a percentage, every year for the duration of a specified term. The function takes a seed value as a parameter. This value is used to reseed the random number generator, which ensures that the function doesn't get the same list of random numbers each time it runs. The `random.normalvariate` function ensures that random values occur across a [normal distribution](https://wikipedia.org/wiki/Normal_distribution) for the specified mean and standard deviation. The function increases the value of the portfolio by the growth amount, which could be positive or negative, and adds a yearly sum that represents further investment.You define the required constants in an upcoming step.\n- Create many seeds to feed to the function. At the Spark prompt, enter the following code, which generates 10,000 seeds:```\nseeds = sc.parallelize([time.time() + i for i in range(10000)])\n```The result of the `parallelize` operation is a [resilient distributed dataset (RDD)](http://spark.apache.org/docs/1.0.1/programming-guide.html#resilient-distributed-datasets-rdds) , which is a collection of elements that are optimized for parallel processing. In this case, the RDD contains seeds that are based on the current system time.When creating the RDD, Spark slices the data based on the number of workers and cores available. In this case, Spark chooses to use eight slices, one slice for each core. That's fine for this simulation, which has 10,000 items of data. For larger simulations, each slice might be larger than the default limit. In that case, specifying a second parameter to `parallelize` can increase the number slices, which can help to keep the size of each slice manageable, while Spark still takes advantage of all eight cores.\n- Feed the RDD that contains the seeds to the growth function.```\nresults = seeds.map(grow)\n```The `map` method passes each seed in the RDD to the `grow` function and appends each result to a new RDD, which is stored in `results` . Note that this operation, which performs a , doesn't produce its results right away. Spark won't do this work until the results are needed. This is why you can enter code without the constants being defined.\n- Specify some values for the function.```\nINVESTMENT_INIT = 100000 \u00a0# starting amountINVESTMENT_ANN = 10000 \u00a0# yearly new investmentTERM = 30 \u00a0# number of yearsMKT_AVG_RETURN = 0.11 # percentageMKT_STD_DEV = 0.18 \u00a0# standard deviation\n```\n- Call `reduce` to aggregate the values in the RDD. Enter the following code to sum the results in the RDD:```\nsum = results.reduce(add)\n```\n- Estimate and display the average return:```\nprint (sum / 10000.)\n```Be sure to include the dot ( `.` ) character at the end. It signifies floating-point arithmetic.\n- Now change an assumption and see how the results change. For example, you can enter a new value for the market's average return:```\nMKT_AVG_RETURN = 0.07\n```\n- Run the simulation again.```\nprint (sc.parallelize([time.time() + i for i in range(10000)]) \\\u00a0 \u00a0 \u00a0 \u00a0 .map(grow).reduce(add)/10000.)\n```\n- When you're done experimenting, press `CTRL+D` to exit the Python interpreter.\n## Programming a Monte Carlo simulation in ScalaMonte Carlo, of course, is famous as a gambling destination. In this section, you use Scala to create a simulation that models the mathematical advantage that a casino enjoys in a game of chance. The \"house edge\" at a real casino varies widely from game to game; it can be over 20% in [keno](https://wikipedia.org/wiki/Keno) , for example. This tutorial creates a simple game where the house has only a one-percent advantage. Here's how the game works:- The player places a bet, consisting of a number of chips from a bankroll fund.\n- The player rolls a 100-sided die (how cool would that be?).\n- If the result of the roll is a number from 1 to 49, the player wins.\n- For results 50 to 100, the player loses the bet.\nYou can see that this game creates a one-percent disadvantage for the player: in 51 of the 100 possible outcomes for each roll, the player loses.\nFollow these steps to create and run the game:- Start the Scala interpreter from the Dataproc primary node.```\nspark-shell\n```\n- Copy and paste the following code to create the game. Scala doesn't have the same requirements as Python when it comes to indentation, so you can simply copy and paste this code at the `scala>` prompt.```\nval STARTING_FUND = 10val STAKE = 1 \u00a0 // the amount of the betval NUMBER_OF_GAMES = 25def rollDie: Int = {\u00a0 \u00a0 val r = scala.util.Random\u00a0 \u00a0 r.nextInt(99) + 1}def playGame(stake: Int): (Int) = {\u00a0 \u00a0 val faceValue = rollDie\u00a0 \u00a0 if (faceValue < 50)\u00a0 \u00a0 \u00a0 \u00a0 (2*stake)\u00a0 \u00a0 else\u00a0 \u00a0 \u00a0 \u00a0 (0)}// Function to play the game multiple times// Returns the final fund amountdef playSession(\u00a0 \u00a0startingFund: Int = STARTING_FUND,\u00a0 \u00a0stake: Int = STAKE,\u00a0 \u00a0numberOfGames: Int = NUMBER_OF_GAMES):\u00a0 \u00a0(Int) = {\u00a0 \u00a0 // Initialize values\u00a0 \u00a0 var (currentFund, currentStake, currentGame) = (startingFund, 0, 1)\u00a0 \u00a0 // Keep playing until number of games is reached or funds run out\u00a0 \u00a0 while (currentGame <= numberOfGames && currentFund > 0) {\u00a0 \u00a0 \u00a0 \u00a0 // Set the current bet and deduct it from the fund\u00a0 \u00a0 \u00a0 \u00a0 currentStake = math.min(stake, currentFund)\u00a0 \u00a0 \u00a0 \u00a0 currentFund -= currentStake\u00a0 \u00a0 \u00a0 \u00a0 // Play the game\u00a0 \u00a0 \u00a0 \u00a0 val (winnings) = playGame(currentStake)\u00a0 \u00a0 \u00a0 \u00a0 // Add any winnings\u00a0 \u00a0 \u00a0 \u00a0 currentFund += winnings\u00a0 \u00a0 \u00a0 \u00a0 // Increment the loop counter\u00a0 \u00a0 \u00a0 \u00a0 currentGame += 1\u00a0 \u00a0 }\u00a0 \u00a0 (currentFund)}\n```\n- Press `return` until you see the `scala>` prompt.\n- Enter the following code to play the game 25 times, which is the default value for `NUMBER_OF_GAMES` .```\nplaySession()\n```Your bankroll started with a value of 10 units. Is it higher or lower, now?\n- Now simulate 10,000 players betting 100 chips per game. Play 10,000 games in a session. This Monte Carlo simulation calculates the probability of losing all your money before the end of the session. Enter the follow code:```\n(sc.parallelize(1 to 10000, 500)\u00a0 .map(i => playSession(100000, 100, 250000))\u00a0 .map(i => if (i == 0) 1 else 0)\u00a0 .reduce(_+_)/10000.0)\n```Note that the syntax `.reduce(_+_)` is shorthand in Scala for aggregating by using a summing function. It is functionally equivalent to the `.reduce(add)` syntax that you saw in the Python example.The preceding code performs the following steps:- Creates an RDD with the results of playing the session.\n- Replaces bankrupt players' results with the number`1`and nonzero results with the number`0`.\n- Sums the count of bankrupt players.\n- Divides the count by the number of players.\nA typical result might be:```\n0.998\n```Which represents a near guarantee of losing all your money, even though the casino had only a one-percent advantage.\n## Clean up\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- For more on submitting Spark jobs to Dataproc without having to use`ssh`to connect to the cluster, read [Dataproc\u2014Submit a job](/dataproc/docs/guides/submit-job)", "guide": "Dataproc"}