{"title": "Dataproc - Use the Cloud Client Libraries for Python", "url": "https://cloud.google.com/dataproc/docs/tutorials/python-library-example", "abstract": "# Dataproc - Use the Cloud Client Libraries for Python\nThis tutorial includes a [Cloud Shell walkthrough](/shell/docs/tutorials) that uses the [Google Cloud client libraries for Python](/python/docs/reference/dataproc/latest) to programmatically call [Dataproc gRPC APIs](/dataproc/docs/reference/rpc) to create a cluster and submit a job to the cluster.\nThe following sections explain the operation of the walkthrough code contained in the GitHub [GoogleCloudPlatform/python-dataproc](https://github.com/googleapis/python-dataproc/tree/master/samples/snippets) repository.\nThe walkthrough tutorial code makes separate API requests to create a cluster, submit a job to the cluster, then delete the cluster. You can use an [inline workflow](/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.WorkflowTemplateService.InstantiateInlineWorkflowTemplate) to accomplish these tasks with one API request (see [instantiate_inline_workflow_template.py](https://github.com/googleapis/python-dataproc/blob/master/samples/snippets/instantiate_inline_workflow_template.py) for an example).\n", "content": "## Run the Cloud Shell walkthrough\nClick **Open in Cloud Shell** to run the walkthrough.\n[Open in Cloud Shell](https://ssh.cloud.google.com/cloudshell/open?cloudshell_git_repo=https://github.com/googleapis/python-dataproc&cloudshell_working_dir=samples/snippets&tutorial=python-api-walkthrough.md&cloudshell_open_in_editor=submit_job_to_cluster.py)\n## Understand the code\n### Application Default Credentials\nThe [Cloud Shell walkthrough](#run_the_cloud_shell_walkthrough) in this tutorial provides authentication by using your Google Cloud project credentials. When you run code locally, the recommended practice is to use [service account credentials](/docs/authentication/production#obtaining_and_providing_service_account_credentials_manually) to authenticate your code.\n### Create a Dataproc cluster\nThe following values are set to create the cluster:\n- The project in which the cluster will be created\n- The region where the cluster will be created\n- The name of the cluster\n- The cluster config, which specifies one master and two primary workers\nDefault config settings are used for the remaining cluster settings. You can override default cluster config settings. For example, you can add secondary VMs (default = 0) or specify a non-default VPC network for the cluster. For more information, see [CreateCluster](/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.ClusterController.CreateCluster) .\n[  dataproc/snippets/submit_job_to_cluster.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataproc/snippets/submit_job_to_cluster.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataproc/snippets/submit_job_to_cluster.py)\n```\ndef quickstart(project_id, region, cluster_name, gcs_bucket, pyspark_file):\u00a0 \u00a0 # Create the cluster client.\u00a0 \u00a0 cluster_client = dataproc_v1.ClusterControllerClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options={\"api_endpoint\": f\"{region}-dataproc.googleapis.com:443\"}\u00a0 \u00a0 )\u00a0 \u00a0 # Create the cluster config.\u00a0 \u00a0 cluster = {\u00a0 \u00a0 \u00a0 \u00a0 \"project_id\": project_id,\u00a0 \u00a0 \u00a0 \u00a0 \"cluster_name\": cluster_name,\u00a0 \u00a0 \u00a0 \u00a0 \"config\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"master_config\": {\"num_instances\": 1, \"machine_type_uri\": \"n1-standard-2\"},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"worker_config\": {\"num_instances\": 2, \"machine_type_uri\": \"n1-standard-2\"},\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 }\u00a0 \u00a0 # Create the cluster.\u00a0 \u00a0 operation = cluster_client.create_cluster(\u00a0 \u00a0 \u00a0 \u00a0 request={\"project_id\": project_id, \"region\": region, \"cluster\": cluster}\u00a0 \u00a0 )\u00a0 \u00a0 result = operation.result()\u00a0 \u00a0 print(f\"Cluster created successfully: {result.cluster_name}\")\n```### Submit a job\nThe following values are set to submit the job:\n- The project in which the cluster will be created\n- The region where the cluster will be created\n- The job config, which specifies the cluster name and the Cloud Storage filepath (URI) of the PySpark job\nSee [SubmitJob](/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.JobController.SubmitJob) for more information.\n[  dataproc/snippets/submit_job_to_cluster.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataproc/snippets/submit_job_to_cluster.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataproc/snippets/submit_job_to_cluster.py)\n```\n# Create the job client.job_client = dataproc_v1.JobControllerClient(\u00a0 \u00a0 client_options={\"api_endpoint\": f\"{region}-dataproc.googleapis.com:443\"})# Create the job config.job = {\u00a0 \u00a0 \"placement\": {\"cluster_name\": cluster_name},\u00a0 \u00a0 \"pyspark_job\": {\"main_python_file_uri\": f\"gs://{gcs_bucket}/{spark_filename}\"},}operation = job_client.submit_job_as_operation(\u00a0 \u00a0 request={\"project_id\": project_id, \"region\": region, \"job\": job})response = operation.result()# Dataproc job output is saved to the Cloud Storage bucket# allocated to the job. Use regex to obtain the bucket and blob info.matches = re.match(\"gs://(.*?)/(.*)\", response.driver_output_resource_uri)output = (\u00a0 \u00a0 storage.Client()\u00a0 \u00a0 .get_bucket(matches.group(1))\u00a0 \u00a0 .blob(f\"{matches.group(2)}.000000000\")\u00a0 \u00a0 .download_as_bytes()\u00a0 \u00a0 .decode(\"utf-8\"))print(f\"Job finished successfully: {output}\\r\\n\")\n```### Delete the cluster\nThe following values are set to delete the cluster:\n- The project in which the cluster will be created\n- The region where the cluster will be created\n- The name of the cluster\nFor more information, see the [DeleteCluster](/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.JobController.DeleteCluster) .\n[  dataproc/snippets/submit_job_to_cluster.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataproc/snippets/submit_job_to_cluster.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataproc/snippets/submit_job_to_cluster.py)\n```\n# Delete the cluster once the job has terminated.operation = cluster_client.delete_cluster(\u00a0 \u00a0 request={\u00a0 \u00a0 \u00a0 \u00a0 \"project_id\": project_id,\u00a0 \u00a0 \u00a0 \u00a0 \"region\": region,\u00a0 \u00a0 \u00a0 \u00a0 \"cluster_name\": cluster_name,\u00a0 \u00a0 })operation.result()print(f\"Cluster {cluster_name} successfully deleted.\")\n```", "guide": "Dataproc"}