{"title": "Dataproc - Configure Dataproc Hub", "url": "https://cloud.google.com/dataproc/docs/tutorials/dataproc-hub-admins", "abstract": "# Dataproc - Configure Dataproc Hub\nDataproc Hub and  Vertex AI Workbench user-managed notebooks are  deprecated. On January 30, 2025, support for user-managed notebooks  will end and the ability to create user-managed notebooks instances  will be removed. For alternative notebook solutions on Google Cloud, see:- [Install  the Jupyter component on your Dataproc cluster](/dataproc/docs/concepts/components/jupyter#install_jupyter) .\n- [Create  a Dataproc-enabled  Vertex AI Workbench instance](/vertex-ai/docs/workbench/instances/create-dataproc-enabled) .\nDataproc Hub is a customized [JupyterHub](https://jupyterhub.readthedocs.io/en/stable/) server. Admins configure and create Dataproc Hub instances that can spawn single-user Dataproc clusters to host [Jupyter](https://jupyter.org/) and [JupyterLab](https://jupyterlab.readthedocs.io/en/stable/) notebook environments (see [Use Dataproc Hub](/dataproc/docs/tutorials/dataproc-hub-users) ).", "content": "## Objectives\n- Define a Dataproc cluster configuration (or use one of the predefined config files).\n- Set Dataproc Hub instance environment variables.\n- Create a Dataproc Hub instance.\n## Before you beginIf you haven't already done so, create a Google Cloud project and a Cloud Storage bucket.- **Setting up your project** \n- **Creating a Cloud Storage bucket** in your project to hold the data used in this tutorial.- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets page](https://console.cloud.google.com/storage/browser) \n- Click **Create bucket** .\n- On the **Create a bucket** page, enter your bucket information. To go to the next step, click **Continue** .- For **Name your bucket** , enter a name that meets the [bucket naming requirements](/storage/docs/bucket-naming#requirements) .\n- For **Choose where to store your data** , do the following:- Select a **Location type** option.\n- Select a **Location** option.\n- For **Choose a default storage class for your data** , select a [storage class](/storage/docs/storage-classes) .\n- For **Choose how to control access to objects** , select an **Access control** option.\n- For **Advanced settings (optional)** , specify  an [encryption method](/storage/docs/encryption) ,  a [retention policy](/storage/docs/bucket-lock) ,  or [bucket labels](/storage/docs/tags-and-labels#bucket-labels) .\n- Click **Create** .## Define a cluster configurationA Dataproc Hub instance creates a cluster from configuration values contained in a YAML cluster configuration file.\n **Predefined cluster configs:** You can use the following predefined config files located in Cloud Storage:- `example-cluster`config: Sets up a standard Jupyter component  cluster with one master and 2 worker nodes\n- `example-single-node`config: Sets up a Jupyter component  cluster with one node\nTo view these public Cloud Storage config files, run:```\ngsutil cat gs://dataproc-spawner-dist/example-configs/example-cluster.yaml\ngsutil cat gs://dataproc-spawner-dist/example-configs/example-single-node.yaml\n``` **Create a quickstart hub.** To create a hub using the predefined cluster configs, skip to [Create a Dataproc Hubinstance](#create_a_dataproc_hub_instance) . The Cloud Storage location of the predefined cluster configs listed above is the default value of the DATAPROC_CONFIGS environment variable field.\nYour cluster configuration can specify any feature or component available to Dataproc clusters (such as machine type, initialization actions, and optional components). **The cluster image version must be 1.4.13 or higher.** Attempting to spawn a cluster with an image version lower than 1.4.13 will cause an error and fail.\n **Note:** Admins can allow users to override some of the predefined cluster configuration parameters (see `DATAPROC_ALLOW_CUSTOM_CLUSTERS` in [Set Dataproc Hub instance environment values](#set_dataproc_hub_instance_environment_variables) .\n **Sample YAML cluster configuration file** \n```\nclusterName: cluster-name\nconfig:\n gceClusterConfig:\n metadata:\n  'PIP_PACKAGES': 'google-cloud-core>=1.3.0 google-cloud-storage>=1.28.1'\n initializationActions:\n - executableFile: gs://dataproc-initialization-actions/python/pip-install.sh\n softwareConfig:\n imageVersion: 1.5-ubuntu18\n optionalComponents:\n - ANACONDA\n - JUPYTER\n```\nEach configuration must be [saved in Cloud Storage](#save_the_yaml_configuration_file_in) . You can create and save multiple configuration files to give users a choice when they [use Dataproc Hub](/dataproc/docs/tutorials/dataproc-hub-users) to create their Dataproc cluster notebook environment.\nThere are two ways to create a YAML cluster configuration file:- [Create YAML cluster configuration file from the console](#create_yaml_cluster_configuration_file_from_the_console) \n- [Export a YAML cluster configuration file from an existing cluster](#export_a_yaml_cluster_configuration_file_from_an_existing_cluster) \n### Create YAML cluster configuration file from the console\n- Open the [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page in the Google Cloud console, then select and fill in the fields to specify the type of cluster the Dataproc Hub will spawn for users.The region and zone settings will be overridden when the user's cluster is spawned: the spawned cluster's region will be the region where the Dataproc Hub is located, and the user will select a zone within this region.- At the bottom of the left panel, select \"Equivalent REST\".\n- Copy the generated JSON block, excluding the leading POST request line, then paste the JSON block into an online JSON-to-YAML converter (search online for \"Convert JSON to YAML\").Some JSON to YAML converters generate a first line containing \"---\". The inclusion of this line in the YAML file is optional.\n- Copy the converted YAML into a local.yaml file.\n### Export a YAML cluster configuration file from an existing cluster\n- [Create a cluster](/dataproc/docs/guides/create-cluster) that matches your requirements.\n- Export the cluster configuration to a local.yaml file.```\ngcloud dataproc clusters export cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--destination cluster-config-filename.yaml \\\n\u00a0\u00a0\u00a0\u00a0--region region\n \n```\n### Save the YAML configuration file in Cloud StorageCopy your local YAML cluster configuration file to your Cloud Storage bucket.\n```\ngsutil cp cluster-config-filename.yaml gs://bucket-name/\n```## Set Dataproc Hub instance environment variablesThe administrator can set the hub environment variables listed in the table, below, to set attributes of the Dataproc clusters that will be spawned by hub users.\n **Note:** If hub environment variables are not set, Dataproc will use default values, including setting `DATAPROC_CONFIGS` to the Cloud Storage location of the predefined cluster configs (see [Define a cluster configuration](#define_a_cluster_configuration) ).\n| Variable      | Description                                                             | Example                       |\n|:-------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------|\n| NOTEBOOKS_LOCATION    | Cloud Storage bucket or bucket folder that contains user notebooks. The `gs://` prefix is optional. Default: The Dataproc staging bucket.                             | gs://bucket-name/                    |\n| DATAPROC_CONFIGS    | Comma delimited list of strings of the Cloud Storage paths to YAML cluster config files. The `gs://` prefix is optional. Default: gs://dataproc-spawner-dist/example-configs/. which contains predefined example-cluster.yaml and example-single-node.yaml. | gs://cluster-config-filename.yaml                |\n| DATAPROC_LOCATIONS_LIST  | Zone suffixes in region where the Dataproc Hub instance is located. Users can select one of these zones as the zone where their Dataproc cluster will be spawned. Default: \"b\".                    | b,c,d                       |\n| DATAPROC_DEFAULT_SUBNET  | Subnet on which Dataproc Hub instance will spawn Dataproc clusters. Default: the Dataproc Hub instance subnet.                                    | https://www.googleapis.com/compute/v1/projects/project-id/regions/region/subnetworks/subnet-name |\n| DATAPROC_SERVICE_ACCOUNT  | Service account that Dataproc VMs will run as. Default: If not set, the default Dataproc service account is used.                                   | service-account@project-id.iam.gserviceaccount.com            |\n| SPAWNER_DEFAULT_URL   | Whether to show the Jupyter or JupyterLab UI on spawned Dataproc clusters by default. Default: \"/lab\".                                      | `/` or `/lab`, for Jupyter or JupyterLab, respectively.           |\n| DATAPROC_ALLOW_CUSTOM_CLUSTERS | Whether to allow users to customize their Dataproc clusters. Default: false.                                             | \"true\" or \"false\"                    |\n| DATAPROC_MACHINE_TYPES_LIST | List of machine types that users are allowed to choose for their spawned Dataproc clusters, if cluster customization (DATAPROC_ALLOW_CUSTOM_CLUSTERS) is enabled. Default: empty (all machine types are allowed).           | n1-standard-4,n1-standard-8,e2-standard-4,n1-highcpu-4           |\n| NOTEBOOKS_EXAMPLES_LOCATION | Cloud Storage path to notebooks bucket or bucket folder to be downloaded to the spawned Dataproc cluster when the cluster starts. Default: empty.                           | gs://bucket-name/                    |\n### Setting hub environment variablesThere are two ways to set hub environment variables:- [Set hub environment variables from the console](#variables-console) \n- [Set hub environment variables in a text file](#variables-text) \nWhen you [create a Dataproc Hub instance](#create_a_dataproc_hub_instance) from the **User-Managed Notebooks** tab on the **Dataproc\u2192Workbench** page in the Google Cloud console, you can click the **Populate** button to open a **Populate Dataproc Hub** form that allows you to set each environment variable.\n- **Create the file.** You can use a text editor to set Dataproc Hub instance environment variables in a local file. Alternatively, you can create the file by running the following command after filling in placeholder values and changing or adding variables and their values.```\ncat <<EOF > environment-variables-file\nDATAPROC_CONFIGS=gs://bucket/cluster-config-filename.yaml\nNOTEBOOKS_LOCATION=gs://bucket/notebooks\nDATAPROC_LOCATIONS_LIST=b,c\nEOF\n```\n- **Save the file in Cloud Storage.** Copy your local Dataproc Hub instance environment variables file to your Cloud Storage bucket.```\ngsutil cp environment-variable-filename gs://bucket-name/folder-name/\n```\n## Set Identity and Access Management (IAM) rolesDataproc Hub includes the following identities with the following abilities:- Administrator: creates a Dataproc Hub instance\n- Data and ML user: accesses the Dataproc Hub UI\n- Dataproc Hub service account: represents Dataproc Hub\n- Dataproc service account: represents the Dataproc cluster that Dataproc Hub creates.\nEach identity requires specific roles or permissions to perform their associated tasks. The table below summarizes IAM roles and permissions required by each identity.\n| Identity     | Type     | Role or permission        |\n|:---------------------------|:------------------------|:------------------------------------------------|\n| Dataproc Hub administrator | User or Service account | roles/notebooks.admin       |\n| Dataproc Hub user   | User     | notebooks.instances.use, dataproc.clusters.use |\n| Dataproc Hub    | Service account   | roles/dataproc.hubAgent       |\n| Dataproc     | Service account   | roles/dataproc.worker       |\nBy default, the Dataproc Hub instance and Dataproc cluster nodes run as the project [default Compute Engine service account](/compute/docs/access/service-accounts#default_service_account) .## Create a Dataproc Hub instance\n- **Before you begin:** To create a Dataproc Hub instance from the Google Cloud console, your user account must have `compute.instances.create` permission. Also, the service account of the instance\u2014the Compute Engine default service account or your user-specified service account listed in [IAM & admin > Service Accounts](https://console.cloud.google.com/iam-admin/serviceaccounts) (see [Dataproc VM service account](/dataproc/docs/concepts/configuring-clusters/service-accounts#VM_service_account) )\u2014 must have `iam.serviceAccounts.actAs` permission.\n- Go to the **Dataproc\u2192Workbench** page in the Google Cloud console, then select the **User-Managed Notebooks** tab.\n- If not pre-selected as a filter, click in the **Filter** box, then select **Environment:Dataproc Hub\"\".\n- Click **New Notebook\u2192Dataproc Hub** .\n- On the **Create a user-managed notebook** page, provide the following information:- **Notebook name** : Dataproc Hub instance name.\n- **Region** : Select a [region](/compute/docs/regions-zones#available) for the Dataproc Hub instance. Dataproc clusters spawned by this Dataproc Hub instance will also be created in this region.For best performance, select a geographically close region.\n- **Zone** : Select a zone within the selected region.\n- **Environment:** - `Environment`: Select`Dataproc Hub`.\n- `Select a script to run after creation`(optional): You can insert or browse and select an [initialization action](/dataproc/docs/concepts/configuring-clusters/init-actions) script or executable to run on the spawned Dataproc cluster.\n- `Populate Dataproc Hub (optional)`: Click **Populate** to open a form that allows you to set each of the hub environment variables (see [Set Dataproc Hub instance environment variables](#set_dataproc_hub_instance_environment_variables) for a description of each variable). Dataproc uses default values for any unset environment variables. As an alternative, you can set **Metadata** `key:value`pairs to set environment variables (see next item).\n- `Metadata`:- If you created a text file that contains your hub environment variable settings (see [Setting hub environment variables](#setting_hub_environment_variables) ), provide the name of the file as the`key`and the`gs://` `` `/` `` `/` ``Cloud Storage location of the file as the`value`. Dataproc uses default values for any unset environment variables.\n- **Machine configuration:** - `Machine Type`: Select the Compute Engine [machine type](/compute/docs/machine-types) .\n- Set other machine configuration options.\n- **Other Options** :- You can expand and set or replace default values in the **Disks** , **Networking** , **Permission** , **Security** , and **Environment upgrade and system health** sections.\n- Click **Create** to launch the Dataproc Hub instance.\n- The **Open JupyterLab** link for the Dataproc Hub instance becomes active after the instance is created. Users click this link to open the **JupyterHub** server page to configure and create a Dataproc JupyterLab cluster (see [Use Dataproc Hub](/dataproc/docs/tutorials/dataproc-hub-users) ).\n## Clean up\n### Delete the Dataproc Hub instance\n- To delete your Dataproc Hub instance:```\ngcloud compute instances delete --project=${PROJECT} ${INSTANCE_NAME}\n```\n### Delete the bucket\n- To delete the Cloud Storage bucket you created in [Before you begin](#before-you-begin) , including the data files stored in the bucket:```\ngsutil -m rm -r gs://${BUCKET_NAME}\n```\n## What's next\n- [Use Dataproc Hub](/dataproc/docs/tutorials/dataproc-hub-users)", "guide": "Dataproc"}