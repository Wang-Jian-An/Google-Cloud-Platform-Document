{"title": "Dataproc - Use Trino with Dataproc", "url": "https://cloud.google.com/dataproc/docs/tutorials/trino-dataproc", "abstract": "# Dataproc - Use Trino with Dataproc\n[Trino](https://trino.io/) (formerly Presto) is a distributed SQL query engine designed to query large data sets distributed over one or more heterogeneous data sources. Trino can query Hive, MySQL, Kafka and other data sources through connectors. This tutorial shows you how to:- Install the Trino service on a Dataproc cluster\n- Query public data from a Trino client installed on your local machine that communicates with a Trino service on your cluster\n- Run queries from a Java application that communicates with the Trino service on your cluster through the Trino Java JDBC driver.", "content": "## ObjectivesCreate a Dataproc cluster with Trino installed\nPrepare data. This tutorial uses the [Chicago Taxi Trips](/bigquery/public-data/chicago-taxi) public dataset, available in [BigQuery](/bigquery/docs) .- Extract the data from BigQuery\n- Load the data into [Cloud Storage](/storage/docs) as CSV files\n- Transform data:- Expose the data as a Hive external table to make the data queryable by Trino\n- Convert the data from CSV format into Parquet format to make querying faster\nSend Trino CLI or application code queries using an SSH tunnel or Trino JDBC driver, respectively, to the Trino coordinator running on the cluster\nCheck logs and monitor the Trino service through the Trino Web UI\n## CostsIn this document, you use the following billable components of Google Cloud:- [Dataproc](/dataproc/pricing) \n- [Cloud Storage](/storage/pricing) \n- [BigQuery](/bigquery/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . ## Before you begin\nIf you haven't already done so, create a Google Cloud project and a Cloud Storage bucket to hold the data used in this tutorial. 1.\n **Setting up your project** \n1.\n **Creating a Cloud Storage bucket** \nin your project to hold the data used in this tutorial.\n- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets page](https://console.cloud.google.com/storage/browser) \n- Click **Create bucket** .\n- On the **Create a bucket** page, enter your bucket information. To go to the next step, click **Continue** .- For **Name your bucket** , enter a name that meets the [bucket naming requirements](/storage/docs/bucket-naming#requirements) .\n- For **Choose where to store your data** , do the following:- Select a **Location type** option.\n- Select a **Location** option.\n- For **Choose a default storage class for your data** , select a [storage class](/storage/docs/storage-classes) .\n- For **Choose how to control access to objects** , select an **Access control** option.\n- For **Advanced settings (optional)** , specify  an [encryption method](/storage/docs/encryption) ,  a [retention policy](/storage/docs/bucket-lock) ,  or [bucket labels](/storage/docs/tags-and-labels#bucket-labels) .\n- Click **Create** .\n## Create a Dataproc clusterCreate a cluster by running the commands shown in this section from  a terminal window on your local machine.\nCreate a Dataproc cluster using the `optional-components` flag (available on image version 2.1 and later) to install the [Trino optional component](/dataproc/docs/concepts/components/trino) on the cluster and the `enable-component-gateway` flag to enable the [Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) to allow you to access the Trino Web UI from the Google Cloud console.- Set environment variables:- **PROJECT:** your [project ID](https://console.cloud.google.com/) \n- **BUCKET_NAME:** the name of the Cloud Storage bucket  you created in [Before you begin](#before-you-begin) \n- **REGION:** [region](/compute/docs/regions-zones#available) where the cluster used in this tutorial will be created, for example, \"us-west1\"\n- **WORKERS:** 3 - 5 workers are recommended for this tutorial\n```\nexport PROJECT=project-id\nexport WORKERS=number\nexport REGION=region\nexport BUCKET_NAME=bucket-name\n```\n- Run the Google Cloud CLI on your local machine to  create the cluster.```\ngcloud beta dataproc clusters create trino-cluster \\\n\u00a0\u00a0\u00a0\u00a0--project=${PROJECT} \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--num-workers=${WORKERS} \\\n\u00a0\u00a0\u00a0\u00a0--scopes=cloud-platform \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=TRINO \\\n\u00a0\u00a0\u00a0\u00a0--image-version=2.1 \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway\n```Creating a [High-Availability cluster](/dataproc/docs/concepts/configuring-clusters/high-availability) \u2014a cluster with multiple master nodes \u2013 is not recommended since the coordinator is started on master node 0, and any other other master nodes will remain idle.\n## Prepare dataExport the `bigquery-public-data` [chicago_taxi_trips](https://console.cloud.google.com/bigquery) dataset to Cloud Storage as CSV files, then create a Hive external table to reference the data.\n- On your local machine, run the following command to import the taxi data from BigQuery as CSV files without headers into the Cloud Storage bucket you created in [Before you begin](#before-you-begin) .```\nbq\u00a0--location=us extract --destination_format=CSV \\\n\u00a0\u00a0\u00a0\u00a0\u00a0--field_delimiter=',' --print_header=false \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"bigquery-public-data:chicago_taxi_trips.taxi_trips\" \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0gs://${BUCKET_NAME}/chicago_taxi_trips/csv/shard-*.csv\n```\n- Create Hive external tables that are backed by the CSV and Parquet files in your Cloud Storage bucket.\n- Create the Hive external table`chicago_taxi_trips_csv`.```\ngcloud dataproc jobs submit hive \\\n\u00a0\u00a0\u00a0\u00a0--cluster trino-cluster \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--execute \"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0CREATE EXTERNAL TABLE chicago_taxi_trips_csv(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0unique_key STRING,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0taxi_id STRING,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0trip_start_timestamp TIMESTAMP,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0trip_end_timestamp TIMESTAMP,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0trip_seconds INT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0trip_miles FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0pickup_census_tract INT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dropoff_census_tract INT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0pickup_community_area INT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dropoff_community_area INT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0fare FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0tips FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0tolls FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0extras FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0trip_total FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0payment_type STRING,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0company STRING,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0pickup_latitude FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0pickup_longitude FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0pickup_location STRING,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dropoff_latitude FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dropoff_longitude FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dropoff_location STRING)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ROW FORMAT DELIMITED\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0FIELDS TERMINATED BY ','\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0STORED AS TEXTFILE\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0location 'gs://${BUCKET_NAME}/chicago_taxi_trips/csv/';\"\n```\n- Verify the creation   of the Hive external table. **Skip this step to save time and resource usage.** If your cluster has 3 or fewer nodes, this query can take several minutes. You can   skip this query step and only verify the creation of the final Hive Parquet, below,   since that query step is significantly faster.```\ngcloud dataproc jobs submit hive \\\n\u00a0\u00a0\u00a0\u00a0--cluster trino-cluster \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--execute \"SELECT COUNT(*) FROM chicago_taxi_trips_csv;\"\n```\n- Create another Hive external table`chicago_taxi_trips_parquet`with the same columns, but with data stored in [Parquet](https://parquet.apache.org) format  for better query performance.```\ngcloud dataproc jobs submit hive \\\n\u00a0\u00a0\u00a0\u00a0--cluster trino-cluster \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--execute \"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0CREATE EXTERNAL TABLE chicago_taxi_trips_parquet(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0unique_key STRING,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0taxi_id STRING,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0trip_start_timestamp TIMESTAMP,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0trip_end_timestamp TIMESTAMP,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0trip_seconds INT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0trip_miles FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0pickup_census_tract INT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dropoff_census_tract INT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0pickup_community_area INT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dropoff_community_area INT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0fare FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0tips FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0tolls FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0extras FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0trip_total FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0payment_type STRING,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0company STRING,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0pickup_latitude FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0pickup_longitude FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0pickup_location STRING,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dropoff_latitude FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dropoff_longitude FLOAT,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dropoff_location STRING)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0STORED AS PARQUET\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0location 'gs://${BUCKET_NAME}/chicago_taxi_trips/parquet/';\"\n```\n- Load the data from the Hive CSV table into the   Hive Parquet table.```\ngcloud dataproc jobs submit hive \\\n\u00a0\u00a0\u00a0\u00a0--cluster trino-cluster \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--execute \"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0INSERT OVERWRITE TABLE chicago_taxi_trips_parquet\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0SELECT * FROM chicago_taxi_trips_csv;\"\n```\n- Verify that the data loaded correctly.```\ngcloud dataproc jobs submit hive \\\n\u00a0\u00a0\u00a0\u00a0--cluster trino-cluster \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--execute \"SELECT COUNT(*) FROM chicago_taxi_trips_parquet;\"\n```\n## Run queriesYou can run queries locally from the Trino CLI or from an application.\n### Trino CLI queriesThis section demonstrates how to query the Hive Parquet taxi dataset using the Trino CLI.\n- Run the following command on your local machine to SSH into your cluster's master node. The local terminal will stop responding during the execution of the command.```\ngcloud compute ssh trino-cluster-m\n```\n- In SSH terminal window on your cluster's master node, run the Trino CLI, which connects to the Trino server running on the master node.```\ntrino --catalog hive --schema default\n```\n- At the`trino:default`prompt, verify that Trino can find the Hive tables.```\nshow tables;\n``````\nTable\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\n chicago_taxi_trips_csv\n chicago_taxi_trips_parquet\n(2 rows)\n```\n- Run queries from the`trino:default`prompt, and compare the performance of querying Parquet versus CSV data.- **Parquet data query** ```\nselect count(*) from chicago_taxi_trips_parquet where trip_miles > 50;\n``````\n _col0\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\n 117957\n(1 row)\nQuery 20180928_171735_00006_2sz8c, FINISHED, 3 nodes\nSplits: 308 total, 308 done (100.00%)\n0:16 [113M rows, 297MB] [6.91M rows/s, 18.2MB/s]\n```\n- **CSV data query** ```\nselect count(*) from chicago_taxi_trips_csv where trip_miles > 50;\n``````\n_col0\n\u2010\u2010\u2010\u2010\u2010\u2010\u2010\u2010\n 117957\n(1 row)\nQuery 20180928_171936_00009_2sz8c, FINISHED, 3 nodes\nSplits: 881 total, 881 done (100.00%)\n0:47 [113M rows, 41.5GB] [2.42M rows/s, 911MB/s]\n```### Java application queriesTo run queries from a Java application through the Trino Java JDBC driver: 1. Download the [Trino Java JDBC driver](https://trinodb.github.io/docs.trino.io/current/client/jdbc.html) . 1. Add a `trino-jdbc` dependency in [Maven pom.xml](https://maven.apache.org/guides/introduction/introduction-to-the-pom.html) .\n```\n<dependency>\n <groupId>io.trino</groupId>\n <artifactId>trino-jdbc</artifactId>\n <version>376</version>\n</dependency>\n```\n **Sample Java code** \n```\npackage dataproc.codelab.trino;import java.sql.Connection;import java.sql.DriverManager;import java.sql.ResultSet;import java.sql.SQLException;import java.sql.Statement;import java.util.Properties;public class TrinoQuery {\u00a0 private static final String URL = \"jdbc:trino://trino-cluster-m:8080/hive/default\";\u00a0 private static final String SOCKS_PROXY = \"localhost:1080\";\u00a0 private static final String USER = \"user\";\u00a0 private static final String QUERY =\u00a0 \u00a0 \u00a0 \"select count(*) as count from chicago_taxi_trips_parquet where trip_miles > 50\";\u00a0 public static void main(String[] args) {\u00a0 \u00a0 try {\u00a0 \u00a0 \u00a0 Properties properties = new Properties();\u00a0 \u00a0 \u00a0 properties.setProperty(\"user\", USER);\u00a0 \u00a0 \u00a0 properties.setProperty(\"socksProxy\", SOCKS_PROXY);\u00a0 \u00a0 \u00a0 Connection connection = DriverManager.getConnection(URL, properties);\u00a0 \u00a0 \u00a0 try (Statement stmt = connection.createStatement()) {\u00a0 \u00a0 \u00a0 \u00a0 ResultSet rs = stmt.executeQuery(QUERY);\u00a0 \u00a0 \u00a0 \u00a0 while (rs.next()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 int count = rs.getInt(\"count\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"The number of long trips: \" + count);\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 } catch (SQLException e) {\u00a0 \u00a0 \u00a0 e.printStackTrace();\u00a0 \u00a0 }\u00a0 }}\n```\n## Logging and monitoring\n### LoggingThe Trino logs are located at `/var/log/trino/` on the cluster's master and worker nodes.\n### Web UISee [Viewing and Accessing Component Gateway URLs](/dataproc/docs/concepts/accessing/dataproc-gateways#viewing_and_accessing_component_gateway_urls) to open the Trino Web UI running on the cluster's master node in your local browser.\n### MonitoringTrino exposes cluster runtime information through runtime tables. In a Trino session (from the `trino:default` ) prompt, run the following query to view runtime table data:\n```\nselect * FROM system.runtime.nodes;\n```## Clean upAfter you finish the tutorial, you can clean up the resources that you created so that they stop using quota and incurring charges. The following sections describe how to delete or turn off these resources.\n### Delete the project\nThe easiest way to eliminate billing is to delete the project that you created for the tutorial.\nTo delete the project:\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.### Delete the cluster\n- To delete your cluster:```\ngcloud dataproc clusters delete --project=${PROJECT} trino-cluster \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION}\n```\n### Delete the bucket\n- To delete the Cloud Storage bucket you created in [Before you begin](#before-you-begin) , including the data files stored in the bucket:```\ngsutil -m rm -r gs://${BUCKET_NAME}\n```", "guide": "Dataproc"}