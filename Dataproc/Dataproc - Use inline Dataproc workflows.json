{"title": "Dataproc - Use inline Dataproc workflows", "url": "https://cloud.google.com/dataproc/docs/concepts/workflows/inline-workflows", "abstract": "# Dataproc - Use inline Dataproc workflows\nUnlike standard [workflows](/dataproc/docs/concepts/workflows/using-workflows) that instantiate a previously created workflow template resource, inline workflows use a [YAML file](/dataproc/docs/concepts/workflows/using-yamls#instantiate_a_workflow_using_a_yaml_file) or an embedded [WorkflowTemplate](/dataproc/docs/reference/rest/v1/projects.locations.workflowTemplates#WorkflowTemplate) definition to run a workflow.\n**Note:** You cannot pass [parameters](/dataproc/docs/concepts/workflows/workflow-parameters) to an inline workflow.\n", "content": "## Creating and running an inline workflow\nSee [Instantiate a workflow using a YAML file](/dataproc/docs/concepts/workflows/using-yamls#instantiate_a_workflow_using_a_yaml_file) .\nBefore using any of the request data, make the following replacements:- : Google Cloud project ID\n- : [cluster region](/dataproc/docs/guides/create-cluster#cluster-region) ,  such as \"us-central1\"\n- :  Specify a [zone within the cluster's ](/dataproc/docs/reference/rest/v1/ClusterConfig#GceClusterConfig.FIELDS.zone_uri) [region](/dataproc/docs/guides/create-cluster#cluster-region) ,  such as \"us-central1-b\", or leave empty (\"\") to use  Dataproc [Auto Zone placement](/dataproc/docs/concepts/configuring-clusters/auto-zone) \n- : cluster name\nHTTP method and URL:\n```\nPOST https://dataproc.googleapis.com/v1/projects/project-id/regions/region/workflowTemplates:instantiateInline\n```\nRequest JSON body:\n```\n{\n \"jobs\": [ {\n  \"hadoopJob\": {\n  \"mainJarFileUri\": \"file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\",\n  \"args\": [   \"teragen\",\n   \"1000\",\n   \"hdfs:///gen/\"\n  ]\n  },\n  \"stepId\": \"teragen\"\n },\n {\n  \"hadoopJob\": {\n  \"mainJarFileUri\": \"file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\",\n  \"args\": [   \"terasort\",\n   \"hdfs:///gen/\",\n   \"hdfs:///sort/\"\n  ]\n  },\n  \"stepId\": \"terasort\",\n  \"prerequisiteStepIds\": [  \"teragen\"\n  ]\n }\n ],\n \"placement\": {\n \"managedCluster\": {\n  \"clusterName\": \"cluster-name\",\n  \"config\": {\n  \"gceClusterConfig\": {\n   \"zoneUri\": \"zone\"\n  }\n  }\n }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/project-id/regions/region/operations/2fbd0dad-...\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.dataproc.v1.WorkflowMetadata\",\n \"graph\": {\n  \"nodes\": [  {\n   \"stepId\": \"teragen\",\n   \"state\": \"RUNNABLE\"\n  },\n  {\n   \"stepId\": \"terasort\",\n   \"prerequisiteStepIds\": [   \"teragen\"\n   ],\n   \"state\": \"BLOCKED\"\n  }\n  ]\n },\n \"state\": \"PENDING\",\n \"startTime\": \"2020-04-02T22:50:44.826Z\"\n }\n}\n```\nCurrently, the creation of inline workflows is not supported in the Google Cloud console. Workflow templates and instantiated workflows can be viewed from Dataproc **Workflows** page.\n- [Install the client library](/dataproc/docs/reference/libraries#installing_the_client_library) \n- [Set up application default credentials](/dataproc/docs/reference/libraries#setting_up_authentication) \n- Run the codeSee [Setting up your development environment](/go/docs/setup) . [  dataproc/instantiate_inline_workflow_template.go ](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/dataproc/instantiate_inline_workflow_template.go) [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/dataproc/instantiate_inline_workflow_template.go) ```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 dataproc \"cloud.google.com/go/dataproc/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/dataproc/apiv1/dataprocpb\"\u00a0 \u00a0 \u00a0 \u00a0 \"google.golang.org/api/option\")func instantiateInlineWorkflowTemplate(w io.Writer, projectID, region string) error {\u00a0 \u00a0 \u00a0 \u00a0 // projectID := \"your-project-id\"\u00a0 \u00a0 \u00a0 \u00a0 // region := \"us-central1\"\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // Create the cluster client.\u00a0 \u00a0 \u00a0 \u00a0 endpoint := region + \"-dataproc.googleapis.com:443\"\u00a0 \u00a0 \u00a0 \u00a0 workflowTemplateClient, err := dataproc.NewWorkflowTemplateClient(ctx, option.WithEndpoint(endpoint))\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"dataproc.NewWorkflowTemplateClient: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer workflowTemplateClient.Close()\u00a0 \u00a0 \u00a0 \u00a0 // Create jobs for the workflow.\u00a0 \u00a0 \u00a0 \u00a0 teragenJob := &dataprocpb.OrderedJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 JobType: &dataprocpb.OrderedJob_HadoopJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 HadoopJob: &dataprocpb.HadoopJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Driver: &dataprocpb.HadoopJob_MainJarFileUri{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MainJarFileUri: \"file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Args: []string{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"teragen\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"1000\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"hdfs:///gen/\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 StepId: \"teragen\",\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 terasortJob := &dataprocpb.OrderedJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 JobType: &dataprocpb.OrderedJob_HadoopJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 HadoopJob: &dataprocpb.HadoopJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Driver: &dataprocpb.HadoopJob_MainJarFileUri{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MainJarFileUri: \"file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Args: []string{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"terasort\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"hdfs:///gen/\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"hdfs:///sort/\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 StepId: \"terasort\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PrerequisiteStepIds: []string{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"teragen\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Create the cluster placement.\u00a0 \u00a0 \u00a0 \u00a0 clusterPlacement := &dataprocpb.WorkflowTemplatePlacement{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Placement: &dataprocpb.WorkflowTemplatePlacement_ManagedCluster{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ManagedCluster: &dataprocpb.ManagedCluster{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterName: \"my-managed-cluster\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Config: &dataprocpb.ClusterConfig{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 GceClusterConfig: &dataprocpb.GceClusterConfig{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Leave \"ZoneUri\" empty for \"Auto Zone Placement\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // ZoneUri: \"\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ZoneUri: \"us-central1-a\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Create the Instantiate Inline Workflow Template Request.\u00a0 \u00a0 \u00a0 \u00a0 req := &dataprocpb.InstantiateInlineWorkflowTemplateRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Parent: fmt.Sprintf(\"projects/%s/regions/%s\", projectID, region),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Template: &dataprocpb.WorkflowTemplate{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Jobs: []*dataprocpb.OrderedJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 teragenJob,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 terasortJob,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Placement: clusterPlacement,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Create the cluster.\u00a0 \u00a0 \u00a0 \u00a0 op, err := workflowTemplateClient.InstantiateInlineWorkflowTemplate(ctx, req)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"InstantiateInlineWorkflowTemplate: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 if err := op.Wait(ctx); err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"InstantiateInlineWorkflowTemplate.Wait: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Output a success message.\u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Workflow created successfully.\")\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```\n- [Install the client library](/dataproc/docs/reference/libraries#installing_the_client_library) \n- [Set up application default credentials](/dataproc/docs/reference/libraries#setting_up_authentication) \n- Run the codeSee [Setting Up a Java Development Environment](/java/docs/setup) . [  dataproc/src/main/java/InstantiateInlineWorkflowTemplate.java ](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataproc/src/main/java/InstantiateInlineWorkflowTemplate.java) [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataproc/src/main/java/InstantiateInlineWorkflowTemplate.java) ```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.dataproc.v1.ClusterConfig;import com.google.cloud.dataproc.v1.GceClusterConfig;import com.google.cloud.dataproc.v1.HadoopJob;import com.google.cloud.dataproc.v1.ManagedCluster;import com.google.cloud.dataproc.v1.OrderedJob;import com.google.cloud.dataproc.v1.RegionName;import com.google.cloud.dataproc.v1.WorkflowMetadata;import com.google.cloud.dataproc.v1.WorkflowTemplate;import com.google.cloud.dataproc.v1.WorkflowTemplatePlacement;import com.google.cloud.dataproc.v1.WorkflowTemplateServiceClient;import com.google.cloud.dataproc.v1.WorkflowTemplateServiceSettings;import com.google.protobuf.Empty;import java.io.IOException;import java.util.concurrent.ExecutionException;public class InstantiateInlineWorkflowTemplate {\u00a0 public static void instantiateInlineWorkflowTemplate() throws IOException, InterruptedException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String projectId = \"your-project-id\";\u00a0 \u00a0 String region = \"your-project-region\";\u00a0 \u00a0 instantiateInlineWorkflowTemplate(projectId, region);\u00a0 }\u00a0 public static void instantiateInlineWorkflowTemplate(String projectId, String region)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 String myEndpoint = String.format(\"%s-dataproc.googleapis.com:443\", region);\u00a0 \u00a0 // Configure the settings for the workflow template service client.\u00a0 \u00a0 WorkflowTemplateServiceSettings workflowTemplateServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 WorkflowTemplateServiceSettings.newBuilder().setEndpoint(myEndpoint).build();\u00a0 \u00a0 // Create a workflow template service client with the configured settings. The client only\u00a0 \u00a0 // needs to be created once and can be reused for multiple requests. Using a try-with-resources\u00a0 \u00a0 // closes the client, but this can also be done manually with the .close() method.\u00a0 \u00a0 try (WorkflowTemplateServiceClient workflowTemplateServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 WorkflowTemplateServiceClient.create(workflowTemplateServiceSettings)) {\u00a0 \u00a0 \u00a0 // Configure the jobs within the workflow.\u00a0 \u00a0 \u00a0 HadoopJob teragenHadoopJob =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 HadoopJob.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMainJarFileUri(\"file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addArgs(\"teragen\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addArgs(\"1000\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addArgs(\"hdfs:///gen/\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 OrderedJob teragen =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 OrderedJob.newBuilder().setHadoopJob(teragenHadoopJob).setStepId(\"teragen\").build();\u00a0 \u00a0 \u00a0 HadoopJob terasortHadoopJob =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 HadoopJob.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMainJarFileUri(\"file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addArgs(\"terasort\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addArgs(\"hdfs:///gen/\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addArgs(\"hdfs:///sort/\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 OrderedJob terasort =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 OrderedJob.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setHadoopJob(terasortHadoopJob)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addPrerequisiteStepIds(\"teragen\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setStepId(\"terasort\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // Configure the cluster placement for the workflow.\u00a0 \u00a0 \u00a0 // Leave \"ZoneUri\" empty for \"Auto Zone Placement\".\u00a0 \u00a0 \u00a0 // GceClusterConfig gceClusterConfig =\u00a0 \u00a0 \u00a0 // \u00a0 \u00a0 GceClusterConfig.newBuilder().setZoneUri(\"\").build();\u00a0 \u00a0 \u00a0 GceClusterConfig gceClusterConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 GceClusterConfig.newBuilder().setZoneUri(\"us-central1-a\").build();\u00a0 \u00a0 \u00a0 ClusterConfig clusterConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterConfig.newBuilder().setGceClusterConfig(gceClusterConfig).build();\u00a0 \u00a0 \u00a0 ManagedCluster managedCluster =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ManagedCluster.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setClusterName(\"my-managed-cluster\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setConfig(clusterConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 WorkflowTemplatePlacement workflowTemplatePlacement =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 WorkflowTemplatePlacement.newBuilder().setManagedCluster(managedCluster).build();\u00a0 \u00a0 \u00a0 // Create the inline workflow template.\u00a0 \u00a0 \u00a0 WorkflowTemplate workflowTemplate =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 WorkflowTemplate.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addJobs(teragen)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addJobs(terasort)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setPlacement(workflowTemplatePlacement)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // Submit the instantiated inline workflow template request.\u00a0 \u00a0 \u00a0 String parent = RegionName.format(projectId, region);\u00a0 \u00a0 \u00a0 OperationFuture<Empty, WorkflowMetadata> instantiateInlineWorkflowTemplateAsync =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 workflowTemplateServiceClient.instantiateInlineWorkflowTemplateAsync(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 parent, workflowTemplate);\u00a0 \u00a0 \u00a0 instantiateInlineWorkflowTemplateAsync.get();\u00a0 \u00a0 \u00a0 // Print out a success message.\u00a0 \u00a0 \u00a0 System.out.printf(\"Workflow ran successfully.\");\u00a0 \u00a0 } catch (ExecutionException e) {\u00a0 \u00a0 \u00a0 System.err.println(String.format(\"Error running workflow: %s \", e.getMessage()));\u00a0 \u00a0 }\u00a0 }}\n```\n- [Install the client library](/dataproc/docs/reference/libraries#installing_the_client_library) \n- [Set up application default credentials](/dataproc/docs/reference/libraries#setting_up_authentication) \n- Run the code\nSee [Setting up a Node.js development environment](/nodejs/docs/setup) .\n [  dataproc/instantiateInlineWorkflowTemplate.js ](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/dataproc/instantiateInlineWorkflowTemplate.js) [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/dataproc/instantiateInlineWorkflowTemplate.js) \n```\nconst dataproc = require('@google-cloud/dataproc');// TODO(developer): Uncomment and set the following variables// projectId = 'YOUR_PROJECT_ID'// region = 'YOUR_REGION'// Create a client with the endpoint set to the desired regionconst client = new dataproc.v1.WorkflowTemplateServiceClient({\u00a0 apiEndpoint: `${region}-dataproc.googleapis.com`,\u00a0 projectId: projectId,});async function instantiateInlineWorkflowTemplate() {\u00a0 // Create the formatted parent.\u00a0 const parent = client.regionPath(projectId, region);\u00a0 // Create the template\u00a0 const template = {\u00a0 \u00a0 jobs: [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 hadoopJob: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mainJarFileUri:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 args: ['teragen', '1000', 'hdfs:///gen/'],\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 stepId: 'teragen',\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 hadoopJob: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mainJarFileUri:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 args: ['terasort', 'hdfs:///gen/', 'hdfs:///sort/'],\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 stepId: 'terasort',\u00a0 \u00a0 \u00a0 \u00a0 prerequisiteStepIds: ['teragen'],\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 ],\u00a0 \u00a0 placement: {\u00a0 \u00a0 \u00a0 managedCluster: {\u00a0 \u00a0 \u00a0 \u00a0 clusterName: 'my-managed-cluster',\u00a0 \u00a0 \u00a0 \u00a0 config: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 gceClusterConfig: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Leave 'zoneUri' empty for 'Auto Zone Placement'\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // zoneUri: ''\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 zoneUri: 'us-central1-a',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 };\u00a0 const request = {\u00a0 \u00a0 parent: parent,\u00a0 \u00a0 template: template,\u00a0 };\u00a0 // Submit the request to instantiate the workflow from an inline template.\u00a0 const [operation] = await client.instantiateInlineWorkflowTemplate(request);\u00a0 await operation.promise();\u00a0 // Output a success message\u00a0 console.log('Workflow ran successfully.');\n```\n- [Install the client library](/dataproc/docs/reference/libraries#installing_the_client_library) \n- [Set up application default credentials](/dataproc/docs/reference/libraries#setting_up_authentication) \n- Run the codeSee [Setting Up a Python Development Environment](/python/docs/setup) . [  dataproc/snippets/instantiate_inline_workflow_template.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataproc/snippets/instantiate_inline_workflow_template.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataproc/snippets/instantiate_inline_workflow_template.py) ```\nfrom google.cloud import dataproc_v1 as dataprocdef instantiate_inline_workflow_template(project_id, region):\u00a0 \u00a0 \"\"\"This sample walks a user through submitting a workflow\u00a0 \u00a0 for a Cloud Dataproc using the Python client library.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 project_id (string): Project to use for running the workflow.\u00a0 \u00a0 \u00a0 \u00a0 region (string): Region where the workflow resources should live.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 # Create a client with the endpoint set to the desired region.\u00a0 \u00a0 workflow_template_client = dataproc.WorkflowTemplateServiceClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options={\"api_endpoint\": f\"{region}-dataproc.googleapis.com:443\"}\u00a0 \u00a0 )\u00a0 \u00a0 parent = f\"projects/{project_id}/regions/{region}\"\u00a0 \u00a0 template = {\u00a0 \u00a0 \u00a0 \u00a0 \"jobs\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"hadoop_job\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"main_jar_file_uri\": \"file:///usr/lib/hadoop-mapreduce/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"hadoop-mapreduce-examples.jar\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"args\": [\"teragen\", \"1000\", \"hdfs:///gen/\"],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"step_id\": \"teragen\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"hadoop_job\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"main_jar_file_uri\": \"file:///usr/lib/hadoop-mapreduce/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"hadoop-mapreduce-examples.jar\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"args\": [\"terasort\", \"hdfs:///gen/\", \"hdfs:///sort/\"],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"step_id\": \"terasort\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"prerequisite_step_ids\": [\"teragen\"],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \"placement\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"managed_cluster\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"cluster_name\": \"my-managed-cluster\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"config\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gce_cluster_config\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Leave 'zone_uri' empty for 'Auto Zone Placement'\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # 'zone_uri': ''\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"zone_uri\": \"us-central1-a\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 }\u00a0 \u00a0 # Submit the request to instantiate the workflow from an inline template.\u00a0 \u00a0 operation = workflow_template_client.instantiate_inline_workflow_template(\u00a0 \u00a0 \u00a0 \u00a0 request={\"parent\": parent, \"template\": template}\u00a0 \u00a0 )\u00a0 \u00a0 operation.result()\u00a0 \u00a0 # Output a success message.\u00a0 \u00a0 print(\"Workflow ran successfully.\")\n```", "guide": "Dataproc"}