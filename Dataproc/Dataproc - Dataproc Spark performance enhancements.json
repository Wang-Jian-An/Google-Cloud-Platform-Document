{"title": "Dataproc - Dataproc Spark performance enhancements", "url": "https://cloud.google.com/dataproc/docs/guides/performance-enhancements", "abstract": "# Dataproc - Dataproc Spark performance enhancements\nThis document shows you how to enable the Dataproc Spark performance enhancements to help your Dataproc Spark jobs process more data in less time with reduced costs.\nDataproc Spark performance enhancements include:\n- Spark Optimizer enhancements:- Optimizer rules written for better Spark plans\n- Improved performance of the Dataproc BigQuery connector when used in Spark jobs\n- Spark Execution enhancements:- Spark execution engine improvements**Other Dataproc performance improvements:** See Dataproc [cluster caching](/dataproc/docs/concepts/cluster-caching) , which helps reduce the amount of time spent accessing data in Cloud Storage.\n**Note:** The enhancements described in the following sections are available in Dataproc on Compute Engine clusters created with image versions 2.0.69+ and 2.1.17+ and later image releases.\n", "content": "## How to enable Dataproc Spark performance enhancements\nYou can use the Google Cloud console, Google Cloud CLI, and the Dataproc API to enable Dataproc Spark performance enhancements when you create a Dataproc on Compute Engine cluster or when you submit your Spark job to your cluster.\n### Enable enhancements at cluster creationPerform the following steps to enable the Spark optimization and execution enhancements when you create a Dataproc cluster. Enabled enhancements remain in effect for all Spark jobs submitted to the cluster unless you disable the enhancements for a specific job when you submit the job. By default, Dataproc Spark performance enhancements are disabled on a Dataproc cluster.\n **Note:** To enable Dataproc performance enhancements on a job, the job cluster must be created with image versions 2.0.69+ and 2.1.17+ and later image releases. The default **Image Type and Version** used to create a cluster is listed in the **Versioning** section on the **Set up cluster** panel when you open the **Create a Dataproc cluster on Compute Engine** page in the Google Cloud console. By default, the console uses the latest subminor release of this image version (see [Default Dataproc image version](/dataproc/docs/concepts/versioning/dataproc-version-clusters#default_dataproc_image_version) ).- In the Google Cloud console, open the Dataproc [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page.\n- On the **Create Dataproc cluster** form, click **Create** on the **Cluster on Compute Engine** line.\n- On the **Create a Dataproc cluster on Compute Engine** page, click the **Customize cluster** panel, then scroll to the **Cluster properties** section.- To enable Spark optimization enhancements:- Click **+ ADD PROPERTIES** .\n- Select **spark** in the **Prefix** list, then add \"spark.dataproc.enhanced.optimizer.enabled\" in the **Key** field and \"true\" in **Value** field.\n- To enable Spark execution enhancements:- Click **+ ADD PROPERTIES** .\n- Select **spark** in the **Prefix** list, then add \"spark.dataproc.enhanced.execution.enabled\" in the **Key** field and \"true\" in **Value** field.\n- Complete filling in or confirming the other cluster creation fields, then click **Create** .\n### Enable enhancements at job submissionPerform the following steps to enable Spark optimizations and execution enhancements for a Spark job. By default, Dataproc Spark performance enhancements are disabled on Spark jobs unless you enable the enhancements when you create a job cluster or enable them for a specific job.\n **Note:** To enable Dataproc performance enhancements on a job, the job cluster must have been created with image versions 2.0.69+ and 2.1.17+ and later image releases.- In the Google Cloud console, open the Dataproc [Jobs](https://console.cloud.google.com/dataproc/jobs) page.\n- On the **Jobs** page, click **Submit job** , then scroll to the job **Properties** section.- To enable Spark optimization enhancements:- Click **+ ADD PROPERTIES** . Add \"spark.dataproc.enhanced.optimizer.enabled\" in the **Key** field and \"true\" in **Value** field.\n- To enable Spark execution enhancements:- Click **+ ADD PROPERTIES** .\n- Add \"spark.dataproc.enhanced.execution.enabled\" in the **Key** field and \"true\" in **Value** field.\n- Complete filling in or confirming the other job submission fields, then click **Submit** .### Enable enhancements at cluster creationPerform the following steps to enable the Spark optimization and execution enhancements when you create a Dataproc cluster. Enabled enhancements remain in effect for all Spark jobs submitted to the cluster unless you disable the enhancements for a specific job when you submit the job. By default, Dataproc Spark performance enhancements are disabled on a Dataproc cluster.- Run the following [gcloud dataproc clusters create](/sdk/gcloud/reference/dataproc/clusters/create) command locally in a terminal window or in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true%22) .```\ngcloud dataproc clusters create CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--project=PROJECT_ID \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--image-version=IMAGE \\\n\u00a0\u00a0\u00a0\u00a0--properties=PROPERTIES\n```Notes:- : The cluster name, which must be unique within a project. The name must start with a lowercase letter, and can contain up to 51 lowercase letters, numbers, and hyphens. It cannot end with a hyphen. The name of a deleted cluster can be reused.\n- : The project to associate with the cluster.\n- : The [Compute Engine region](/compute/docs/regions-zones#available) where the cluster will be located, such as`us-central1`.- You can add the optional`--zone=` ``flag to specify a zone within the specified region, such as`us-central1-a`. If you do not specify a zone, the Dataproc [autozone placement](/dataproc/docs/concepts/configuring-clusters/auto-zone) feature selects a zone with the specified region.\n- : The Dataproc Spark optimizer and execution performance enhancements are available in Dataproc image versions`2.0.69+`and`2.1.17+`and later releases. If you omit this flag, Dataproc will select the latest subminor version of the default Dataproc on Compute Engine image for the cluster (see [Default Dataproc image version](/dataproc/docs/concepts/versioning/dataproc-version-clusters#default_dataproc_image_version) ).\n- :- To enable Spark optimization enhancements, specify:\n```\nspark:spark.dataproc.enhanced.optimizer.enabled=true\n```- To enable Spark execution enhancements, specify:\n```\nspark:spark.dataproc.enhanced.execution.enabled=true\n```- To enable Spark optimization and execution enhancements, specify:\n```\nspark:spark.dataproc.enhanced.optimizer.enabled=true,spark:spark.dataproc.enhanced.execution.enabled=true\n```\n### Enable enhancements at job submissionPerform the following steps to enable Spark optimizations and execution enhancements for a Spark job. By default, Dataproc Spark performance enhancements are disabled on Spark jobs unless you enable the enhancements when you create a job cluster or enable them for a specific job.\n **Note:** To enable Dataproc performance enhancements on a job, the job cluster must have been created with image versions 2.0.69+ and 2.1.17+ and later image releases.- Run the following [gcloud dataproc jobs submit](/sdk/gcloud/reference/dataproc/jobs/submit) command locally in a terminal window or in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true%22) .```\ngcloud dataproc jobs submit SPARK_JOB_TYPE \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--properties=PROPERTIES\n```Notes:- : Specify`spark`,`pyspark`,`spark-sql`or`spark-r`.\n- : The name of the job where the job will run.\n- : The region where the cluster is located.\n- :- To enable Spark optimization enhancements, specify:\n```\nspark.dataproc.enhanced.optimizer.enabled=true\n```- To enable Spark execution enhancements, specify:\n```\nspark.dataproc.enhanced.execution.enabled=true\n```- To enable Spark optimization and execution enhancements, specify:\n```\nspark.dataproc.enhanced.optimizer.enabled=true,spark.dataproc.enhanced.execution.enabled\n```\n### Enable enhancements at cluster creationPerform the following steps to enable the Spark optimization and execution enhancements when you create a Dataproc cluster. Enabled enhancements remain in effect for all Spark jobs submitted to the cluster unless you disable the enhancements for a specific job when you submit the job. By default, Dataproc Spark performance enhancements are disabled on a Dataproc cluster.\n **Note:** To enable Dataproc performance enhancements on a job, the job cluster be created with image versions 2.0.69+ and 2.1.17+ and later image releases. If you do not specify a [SoftwareConfig.imageVersion](/dataproc/docs/reference/rest/v1/ClusterConfig#softwareconfig) in your `clusters.create` request, Dataproc will select the latest subminor version of the default Dataproc on Compute Engine image for the cluster (see [Default Dataproc image version](/dataproc/docs/concepts/versioning/dataproc-version-clusters#default_dataproc_image_version) ).- Specify the following [SoftwareConfig.properties](/static/dataproc/docs/reference/rest/v1/ClusterConfig#SoftwareConfig.FIELDS.properties) as part of a [clusters.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) request:- To enable Spark optimization enhancements, specify:\n```\n\"spark:spark.dataproc.enhanced.optimizer.enabled\": \"true\"\n```- To enable Spark execution enhancements, specify:\n```\n\"spark:spark.dataproc.enhanced.execution.enabled\": \"true\"\n```- To enable Spark optimization and execution enhancements, specify:\n```\n\"spark:spark.dataproc.enhanced.optimizer.enabled\": \"true\",\"spark:spark.dataproc.enhanced.execution.enabled\": \"true\"\n```\n### Enable enhancements at job submissionPerform the following steps to enable Spark optimizations and execution enhancements for a Spark job. By default, Dataproc Spark performance enhancements are disabled on Spark jobs unless you enable the enhancements when you create a job cluster or enable them for a specific job.\n **Note:** To enable Dataproc performance enhancements on a job, the job cluster must have been created with image versions 2.0.69+ and 2.1.17+ and later image releases.- Specify the following `properties` for a [SparkJob](/dataproc/docs/reference/rest/v1/SparkJob) , [PySparkJob](/dataproc/docs/reference/rest/v1/PySparkJob) , [SparkSqlJob](/dataproc/docs/reference/rest/v1/SparkSqlJob) , or [SparkRJob](/dataproc/docs/reference/rest/v1/SparkRJob) as part of a [jobs.submit](/dataproc/docs/reference/rest/v1/projects.regions.jobs/submit) request:- To enable Spark optimization enhancements, specify:\n```\n\"spark.dataproc.enhanced.optimizer.enabled=true\"\n```- To enable Spark execution enhancements, specify:\n```\n\"spark.dataproc.enhanced.execution.enabled=true\"\n```- To enable Spark optimization and execution enhancements, specify:\n```\n\"spark.dataproc.enhanced.execution.enabled=true,spark.dataproc.enhanced.optimizer.enabled=true\"\n```\n**Note:** You can click **Equivalent Command Line** or **Equivalent REST** at the bottom of the left panel of the [Create a Dataproc cluster on Compute Engine](https://console.cloud.google.com/dataproc/clustersAdd) page in the Google Cloud console to have the console construct an equivalent`gcloud`tool command or API REST request that you can use from the command line or in your code to create a cluster.", "guide": "Dataproc"}