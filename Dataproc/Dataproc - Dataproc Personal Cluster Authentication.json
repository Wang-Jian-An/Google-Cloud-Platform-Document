{"title": "Dataproc - Dataproc Personal Cluster Authentication", "url": "https://cloud.google.com/dataproc/docs/concepts/iam/personal-auth", "abstract": "# Dataproc - Dataproc Personal Cluster Authentication\nWhen you create a Dataproc cluster, you can enable Dataproc Personal Cluster Authentication to allow interactive workloads on the cluster to securely run as your user identity. This means that interactions with other Google Cloud resources such as Cloud Storage will be authenticated as yourself instead of the cluster service account.\n**Note:** Dataproc Personal Cluster Authentication is only supported when calling into Google Cloud services that support [credential access boundaries](/iam/docs/downscoping-short-lived-credentials) .\n", "content": "## Considerations\n- When you create a cluster with Personal Cluster Authentication enabled, the cluster will only be usable by your identity. Other users will not be able to run jobs on the cluster or access [Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) endpoints on the cluster.\n- Clusters with Personal Cluster Authentication enabled block SSH access and Compute Engine features such as startup scripts on all VMs in the cluster.\n- Clusters with Personal Cluster Authentication enabled automatically enable and configure Kerberos on the cluster for secure intra-cluster communication. However, all Kerberos identities on the cluster will interact with Google Cloud resources as the same user.\n- Dataproc Personal Cluster Authentication currently does not support Dataproc [workflows](/dataproc/docs/concepts/workflows/overview) .\n- Dataproc Personal Cluster Authentication is intended only for interactive jobs run by an individual (human) user. Long-running jobs and operations should configure and use an appropriate service account identity.\n- The propagated credentials are downscoped with a [Credential Access Boundary](/iam/docs/downscoping-short-lived-credentials) . The default access boundary is limited to reading and writing Cloud Storage objects in Cloud Storage buckets owned by the same project that contains the cluster. You can define a non-default access boundary when you [enable_an_interactive_session](#downscoped-example) .## Objectives\n- Create a Dataproc cluster with Dataproc Personal Cluster Authentication enabled.\n- Start credential propagation to the cluster.\n- Use a Jupyter notebook on the cluster to run Spark jobs that authenticate with your credentials.## Before You Begin\n### Create a Project\n### Configure the Environment\nConfigure the environment from Cloud Shell or a local terminal:\n- Start a [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) session.\n- Run`gcloud auth login`to obtain valid user credentials.\n## Create a cluster and enable an interactive session\n- Find the email address of your active account in gcloud.```\ngcloud auth list --filter=status=ACTIVE --format=\"value(account)\"\n```\n- Create a cluster.```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--properties=dataproc:dataproc.personal-auth.user=your-email-address \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=ANACONDA,JUPYTER,ZEPPELIN \\\n\u00a0\u00a0\u00a0\u00a0--region=region\n```\n- Enable a credential propagation session for the cluster to start using your personal credentials when interacting with Google Cloud resources.```\ngcloud dataproc clusters enable-personal-auth-session \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0cluster-name\n```Sample output:```\nInjecting initial credentials into the cluster cluster-name...done.\nPeriodically refreshing credentials for cluster cluster-name. This will continue running until the command is interrupted...\n```- [](None) **Downscoped access boundary example** : The following example enables a personal auth session that is more restrictive than the default downscoped credential access boundary. It restricts access to the Dataproc cluster's [staging bucket](/dataproc/docs/concepts/configuring-clusters/staging-bucket) (see [Downscope with Credential Access Boundaries ](/iam/docs/downscoping-short-lived-credentials) for more information).```\ngcloud dataproc clusters enable-personal-auth-session \\\u00a0 \u00a0 --project=PROJECT_ID \\\u00a0 \u00a0 --region=REGION \\\u00a0 \u00a0 --access-boundary=<(echo -n \"{ \\\u00a0 \\\"access_boundary\\\": { \\\u00a0 \u00a0 \\\"accessBoundaryRules\\\": [{ \\\u00a0 \u00a0 \u00a0 \\\"availableResource\\\": \\\"//storage.googleapis.com/projects/_/buckets/$(gcloud dataproc clusters describe --project=PROJECT_ID --region=REGION CLUSTER_NAME --format=\"value(config.configBucket)\")\\\", \\\u00a0 \u00a0 \u00a0 \\\"availablePermissions\\\": [ \\\u00a0 \u00a0 \u00a0 \u00a0 \\\"inRole:roles/storage.objectViewer\\\", \\\u00a0 \u00a0 \u00a0 \u00a0 \\\"inRole:roles/storage.objectCreator\\\", \\\u00a0 \u00a0 \u00a0 \u00a0 \\\"inRole:roles/storage.objectAdmin\\\", \\\u00a0 \u00a0 \u00a0 \u00a0 \\\"inRole:roles/storage.legacyBucketReader\\\" \\\u00a0 \u00a0 \u00a0 ] \\\u00a0 \u00a0 }] \\\u00a0 } \\}\") \\\u00a0 \u00a0CLUSTER_NAME\n```\n- Keep the command running and switch to a new Cloud Shell tab or terminal session. The client will refresh the credentials while the command is running.\n- Type `Ctrl-C` to end the session.\nThe following example creates a cluster with a downscoped credential access boundary.\n## Access Jupyter on the cluster\n- Get cluster details.```\ngcloud dataproc clusters describe cluster-name --region=region\n```The Jupyter Web interface URL is listed in cluster details.```\n...\nJupyterLab: https://UUID-dot-us-central1.dataproc.googleusercontent.com/jupyter/lab/\n...\n```\n- Copy the URL into your local browser to launch the Jupyter UI.\n- Check that personal cluster authentication was successful.- Start a Jupyter terminal.\n- Run`gcloud auth list`\n- Verify that your username is the only active account.\n- In a Jupyter terminal, enable Jupyter to authenticate with Kerberos and submit Spark jobs.```\nkinit -kt /etc/security/keytab/dataproc.service.keytab dataproc/$(hostname -f)\n```- Run`klist`to verify that Jupyter obtained a valid TGT.\n- In a Juypter terminal, use the`gsutil`to create a`rose.txt`file in a Cloud Storage bucket in your project.```\necho \"A rose by any other name would smell as sweet\" > /tmp/rose.txt\n``````\ngsutil cp /tmp/rose.txt gs://bucket-name/rose.txt\n```- Mark the file as private so that only your user account can read from or write to it. Jupyter will use your personal credentials when interacting with Cloud Storage.```\ngsutil acl set private gs://bucket-name/rose.txt\n```\n- Verify your private access.```\ngsutil acl get gs://$BUCKET/rose.txt\n``````\n[{\n\"email\": \"$USER\",\n\"entity\": \"user-$USER\",\n\"role\": \"OWNER\"\n}\n]\n```\n- Click the [Component Gateway Jupyter](/dataproc/docs/concepts/accessing/dataproc-gateways#console_1) link to launch the Jupyter UI.\n- Check that personal cluster authentication was successful.- Start a Jupyter terminal\n- Run`gcloud auth list`\n- Verify that your username is the only active account.\n- In a Jupyter terminal, enable Jupyter to authenticate with Kerberos and submit Spark jobs.```\nkinit -kt /etc/security/keytab/dataproc.service.keytab dataproc/$(hostname -f)\n```- Run`klist`to verify that Jupyter obtained a valid TGT.\n- In a Jupyter terminal, use the`gsutil`to create a`rose.txt`file in a Cloud Storage bucket in your project.```\necho \"A rose by any other name would smell as sweet\" > /tmp/rose.txt\n``````\ngsutil cp /tmp/rose.txt gs://bucket-name/rose.txt\n```- Mark the file as private so that only your user account can read from or write to it. Jupyter will use your personal credentials when interacting with Cloud Storage.```\ngsutil acl set private gs://bucket-name/rose.txt\n```\n- Verify your private access.```\ngsutil acl get gs://bucket-name/rose.txt\n``````\n[{\n\"email\": \"$USER\",\n\"entity\": \"user-$USER\",\n\"role\": \"OWNER\"\n}\n]\n```\n## Run a PySpark job from Jupyter\n- Navigate to a folder, then create a PySpark notebook.\n- Run a basic word count job against the `rose.txt` file you created above.```\ntext_file = sc.textFile(\"gs://bucket-name/rose.txt\")counts = text_file.flatMap(lambda line: line.split(\" \")) \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0.map(lambda word: (word, 1)) \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0.reduceByKey(lambda a, b: a + b)print(counts.collect())\n```Spark is able to read the`rose.txt`file in Cloud Storage because it runs with your user credentials.You can also check the Cloud Storage Bucket [Audit Logs](https://console.cloud.google.com/logs) to verify that the job is accessing Cloud Storage with your identity (see [Cloud Audit Logs with Cloud Storage](/storage/docs/audit-logs) for more information).## Cleanup\n- Delete the Dataproc cluster.```\ngcloud dataproc clusters delete cluster-name --region=region\n```", "guide": "Dataproc"}