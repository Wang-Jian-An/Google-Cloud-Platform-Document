{"title": "Dataproc - Create a Dataproc cluster by using the Google Cloud console", "url": "https://cloud.google.com/dataproc/docs/quickstarts/create-cluster-console", "abstract": "# Dataproc - Create a Dataproc cluster by using the Google Cloud console\n# Create a Dataproc cluster by using the Google Cloud console\nThis page shows you how to use the Google Cloud console to create a Dataproc cluster, run a basic [Apache Spark](http://spark.apache.org/) job in the cluster, and then modify the number of workers in the cluster.To follow step-by-step guidance for this task directly in the Google Cloud console, click **Guide me** :\n [Guide me](https://console.cloud.google.com/?walkthrough_id=dataproc--quickstart-dataproc-console) ", "content": "## Before you begin\n## Create a cluster\n- In the Google Cloud console, go to the Dataproc **Clusters** page. [Go to Clusters](https://console.cloud.google.com/dataproc/clusters) \n- Click **Create cluster** .\n- In the **Create Dataproc cluster** dialog, click **Create** in the **Cluster on Compute engine** row.\n- In the **Cluster Name** field, enter `example-cluster` .\n- In the **Region** and **Zone** lists, select a region and zone.Select a region (for example, `us-east1` or `europe-west1` ) to isolate resources, such as virtual machine (VM) instances and Cloud Storage and metadata storage locations that are utilized by Dataproc, in the region. For more information, see [Available regions and zones](/compute/docs/regions-zones/regions-zones#available) and [Regional endpoints](/dataproc/docs/concepts/regional-endpoints) .\n- For all the other options, use the default settings.\n- To create the cluster, click **Create** .Your new cluster appears in a list on the **Clusters** page. The status is **Provisioning** until the cluster is ready to use, and then the status changes to **Running** . Provisioning the cluster might take a couple of minutes.\n## Submit a Spark jobSubmit a Spark job that estimates a value of Pi:- In the Dataproc navigation menu, click **Jobs** .\n- On the **Jobs** page, click add_box **Submit job** , and then do the following:- In the **Cluster** field, click **Browse** .\n- On the row for **example-cluster** , click **Select** .\n- In the **Job ID** field, use the default setting, or provide an ID that is unique to your Google Cloud project.\n- For **Job type** , select **Spark** .\n- In the **Main class or jar** field, enter`org.apache.spark.examples.SparkPi`.\n- In the **Jar files** field, enter`file:///usr/lib/spark/examples/jars/spark-examples.jar`.\n- In the **Arguments** field, enter `1000` to set the number of tasks. **Note:** The Spark job estimates Pi by using the [Monte Carlo method](https://wikipedia.org/wiki/Monte_Carlo_method) . It generates and points on a coordinate plane that models a circle enclosed by a unit square. The input argument ( `1000` ) determines the number of x-y pairs to generate; the more pairs generated, the greater the accuracy of the estimation. This estimation uses Dataproc worker nodes to parallelize the computation. For more information, see [Estimating Pi using the Monte Carlo Method](https://academo.org/demos/estimating-pi-monte-carlo/) and [JavaSparkPi.java on GitHub](https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java) .\n- Click **Submit** .Your job is displayed on the **Job details** page. The job status is **Running** or **Starting** , and then it changes to **Succeeded** after it's submitted.To avoid scrolling in the output, click **Line wrap: off** . The output is similar to the following:```\nPi is roughly 3.1416759514167594\n```To view job details, click the **Configuration** tab.## Update a clusterUpdate your cluster by changing the number of worker instances:- In the navigation menu, click **Clusters** .\n- In the list of clusters, click **example-cluster** .\n- On the **Cluster details** page, click the **Configuration** tab.Your cluster settings are displayed.\n- Click mode_edit **Edit** .\n- In the **Worker nodes** field, enter `5` .\n- Click **Save** .\nYour cluster is now updated. To decrease the number of worker nodes to the original value, follow the same procedure.## Clean upTo avoid incurring charges to your Google Cloud account for   the resources used on this page, follow these steps.- On the **Cluster details** page for **example-cluster** , clickdelete **Delete** to delete the cluster.\n- To confirm that you want to delete the cluster, click **Delete** .\n## What's next\n- Try this quickstart by using other tools:- [Use the API Explorer](/dataproc/docs/quickstarts/create-cluster-template) .\n- [Use the Google Cloud CLI](/dataproc/docs/quickstarts/create-cluster-gcloud) .\n- Learn how to [create robust firewall rules when you create a project](/dataproc/docs/concepts/configuring-clusters/network) .\n- Learn how to [write and run a Spark Scala job](/dataproc/docs/tutorials/spark-scala) .", "guide": "Dataproc"}