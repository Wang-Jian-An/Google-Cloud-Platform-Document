{"title": "Dataproc - Dataproc job output and logs", "url": "https://cloud.google.com/dataproc/docs/guides/dataproc-job-output", "abstract": "# Dataproc - Dataproc job output and logs\nWhen you [submit a Dataproc job](/dataproc/docs/guides/submit-job) , Dataproc automatically gathers the job output, and makes it available to you. This means you can quickly review job output without having to maintain a connection to the cluster while your jobs run or look through complicated log files.\n**System and cluster logs:** This guide describes how to configure and view job output. See [Dataproc cluster logs in Logging](/dataproc/docs/guides/logging#cluster_logs_in) for information on configuring and viewing Dataproc cluster system and daemon logs.\n", "content": "## Spark logs\nThere are two types of Spark logs: Spark driver logs and Spark executor logs. Spark driver logs contain job output; Spark executor logs contain job executable or launcher output, such as a `spark-submit` \"Submitted application xxx\" message, and can be helpful for debugging job failures.\nThe Dataproc job driver, which is distinct from the Spark driver, is a launcher for many job types. When launching Spark jobs, it runs as a wrapper on the underlying `spark-submit` executable, which launches the Spark driver. The Spark driver runs the job on the Dataproc cluster in Spark [client or cluster mode](https://spark.apache.org/docs/latest/running-on-yarn.html#launching-spark-on-yarn) :\n- `client` mode: the Spark driver runs the job in the `spark-submit` process, and Spark logs are sent to the Dataproc job driver.\n- `cluster` mode: the Spark driver runs the job in a YARN container. Spark driver logs are not available to the Dataproc job driver.## Dataproc and Spark job properties overview\n**Note:** The first two properties in the following table must be set at cluster creation time, and cannot be overridden when a job is submitted.\n| Property              | Value    | Default | Description                                                                                                       |\n|:----------------------------------------------------------------|:------------------|:----------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| dataproc:dataproc.logging.stackdriver.job.driver.enable   | true or false  | false  | Must be set at cluster creation time. When true, job driver output is in Logging, associated with the job resource; when false, job driver output is not in Logging. Note: The following cluster property settings are also required to enable job driver logs in Logging, and are set by default when a cluster is created: dataproc:dataproc.logging.stackdriver.enable=true and dataproc:jobs.file-backed-output.enable=true |\n| dataproc:dataproc.logging.stackdriver.job.yarn.container.enable | true or false  | false  | Must be set at cluster creation time. When true, job YARN container logs are associated with the job resource; when false, job YARN container logs are associated with the cluster resource.                                                          |\n| spark:spark.submit.deployMode         | client or cluster | client | Controls Spark client or cluster mode.                                                                                                 |\n## Spark jobs submitted using the Dataproc jobs API\nThe tables in this section list the effect of different property settings on the destination of Dataproc job driver output when jobs are submitted through the Dataproc `jobs` API, which includes job submission through the Google Cloud console, gcloud CLI, and Cloud Client Libraries.\nThe listed [Dataproc and Spark properties](/dataproc/docs/concepts/configuring-clusters/cluster-properties) can be set with the `--properties` flag when a cluster is created, and will apply to all Spark jobs run on the cluster; Spark properties can also be set with the `--properties` flag (without the \"spark:\" prefix) when a job is submitted to the Dataproc `jobs` API, and will apply only to the job.\n### Dataproc job driver output\nThe following tables list the effect of different property settings on the destination of Dataproc job driver output.\n| dataproc: dataproc.logging.stackdriver.job.driver.enable | Output                                   |\n|:------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------|\n| false (default)            | Streamed to client In Cloud Storage at the Dataproc-generated driverOutputResourceUri Not in Logging           |\n| true              | Streamed to client In Cloud Storage at the Dataproc-generated driverOutputResourceUri In Logging: dataproc.job.driver under the job resource. |\n### Spark driver logs\nThe following tables list the effect of different property settings on the destination of Spark driver logs.\n| spark: spark.submit.deployMode | dataproc: dataproc.logging.stackdriver.job.driver.enable | dataproc: dataproc.logging.stackdriver.job.yarn.container.enable | Driver Output                                 |\n|:----------------------------------|:------------------------------------------------------------|:--------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------|\n| client       | false (default)            | true or false              | Streamed to client In Cloud Storage at the Dataproc-generated driverOutputResourceUri Not in Logging           |\n| client       | true              | true or false              | Streamed to client In Cloud Storage at the Dataproc-generated driverOutputResourceUri In Logging: dataproc.job.driver under the job resource |\n| cluster       | false (default)            | false                | Not streamed to client Not in Cloud Storage In Logging yarn-userlogs under the cluster resource            |\n| cluster       | true              | true                | Not streamed to client Not in Cloud Storage In Logging: dataproc.job.yarn.container under the job resource         |\n### Spark executor logs\nThe following tables list the effect of different property settings on the destination of Spark executor logs.\n| dataproc: dataproc.logging.stackdriver.job.yarn.container.enable | Executor log             |\n|:--------------------------------------------------------------------|:--------------------------------------------------------------|\n| false (default)              | In Logging: yarn-userlogs under the cluster resource   |\n| true                | In Logging dataproc.job.yarn.container under the job resource |\n## Spark jobs submitted without using the Dataproc jobs API\nThis section lists the effect of different property settings on the destination of Spark job logs when jobs are submitted without using the Dataproc `jobs` API, for example when submitting a job directly on a cluster node using `spark-submit` or when using a Jupyter or Zeppelin notebook. These jobs do not have Dataproc job IDs or drivers.\n### Spark driver logs\nThe following tables list the effect of different property settings on the destination of Spark driver logs for jobs not submitted through the Dataproc `jobs` API.\n| spark: spark.submit.deployMode | Driver Output                     |\n|:----------------------------------|:------------------------------------------------------------------------------------------------|\n| client       | Streamed to client Not in Cloud Storage Not in Logging           |\n| cluster       | Not streamed to client Not in Cloud Storage In Logging yarn-userlogs under the cluster resource |\n### Spark executor logs\nWhen Spark jobs are not submitted through the Dataproc `jobs` API, executor logs are in Logging `yarn-userlogs` under the cluster resource.\n## View job output\nYou can access Dataproc job output in the Google Cloud console, the gcloud CLI, Cloud Storage, or Logging.\nTo view job output, go to your project's Dataproc [Jobs](https://console.cloud.google.com/project/_/dataproc/jobs) section, then click on the **Job ID** to view job output.If the job is running, job output periodically refreshes with  new content.When you submit a job with the [gcloud dataproc jobs submit](/sdk/gcloud/reference/dataproc/jobs/submit) command, job output is displayed on the console. You can \"rejoin\" output at a later time, on a different computer, or in a new window by passing your job's ID to the [gcloud dataproc jobs wait](/sdk/gcloud/reference/dataproc/jobs/wait) command. The Job ID is a [GUID](https://wikipedia.org/wiki/Globally_unique_identifier) , such as `5c1754a5-34f7-4553-b667-8a1199cb9cab` . Here's an example.\n```\ngcloud dataproc jobs wait 5c1754a5-34f7-4553-b667-8a1199cb9cab \\\n\u00a0\u00a0\u00a0\u00a0--project my-project-id --region my-cluster-region\n```\n```\nWaiting for job output...\n... INFO gcs.GoogleHadoopFileSystemBase: GHFS version: 1.4.2-hadoop2\n... 16:47:45 INFO client.RMProxy: Connecting to ResourceManager at my-test-cluster-m/\n...\n```Job output is stored in Cloud Storage in  either the [staging bucket](/dataproc/docs/concepts/configuring-clusters/staging-bucket) or the bucket you specified when you created your cluster. A link to  job output in Cloud Storage is provided in the [Job.driverOutputResourceUri](/dataproc/docs/reference/rest/v1beta2/projects.regions.jobs) field returned by:\n- a [jobs.get](/dataproc/docs/reference/rest/v1beta2/projects.regions.jobs/get) API request.\n- a [gcloud dataproc jobs describe](/sdk/gcloud/reference/dataproc/jobs/describe) command.```\n$ gcloud dataproc jobs describe spark-pi\n...\ndriverOutputResourceUri: gs://dataproc-nnn/jobs/spark-pi/driveroutput\n...\n```\n **Cloud Storage Data Retention:** To prevent data loss, job output and logs saved in Cloud Storage remain in storage after a cluster is removed. You must manually delete job output and logs from Cloud Storage.\nSee\n [Dataproc Logs](/dataproc/docs/guides/logging) \nfor information on how to view Dataproc job output  in Logging.", "guide": "Dataproc"}