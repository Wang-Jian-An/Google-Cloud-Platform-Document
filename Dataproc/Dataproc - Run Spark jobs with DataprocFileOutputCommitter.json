{"title": "Dataproc - Run Spark jobs with DataprocFileOutputCommitter", "url": "https://cloud.google.com/dataproc/docs/guides/dataproc-fileoutput-committer", "abstract": "# Dataproc - Run Spark jobs with DataprocFileOutputCommitter\nThe **DataprocFileOutputCommitter** feature is an enhanced version of the open source `FileOutputCommitter` . It enables concurrent writes by Apache Spark jobs to an output location.\n", "content": "## Limitations\nThe `DataprocFileOutputCommitter` feature supports Spark jobs run on Dataproc Compute Engine clusters created with the following image versions:\n- 2.1 image versions 2.1.10 and higher\n- 2.0 image versions 2.0.62 and higher## Use DataprocFileOutputCommitter\nTo use this feature:\n- [Create a Dataproc on Compute Engine cluster](/dataproc/docs/guides/create-cluster#creating_a_cloud_dataproc_cluster) using image versions `2.1.10` or `2.0.62` or higher.\n- Set `spark.hadoop.mapreduce.outputcommitter.factory.class=org.apache.hadoop.mapreduce.lib.output.DataprocFileOutputCommitterFactory` and `spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs=false` as a job property when you [submit a Spark job](/dataproc/docs/guides/submit-job#how_to_submit_a_job) to the cluster.- Google Cloud CLI example:\n```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--properties=spark.hadoop.mapreduce.outputcommitter.factory.class=org.apache.hadoop.mapreduce.lib.output.DataprocFileOutputCommitterFactory,spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs=false \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0other args ...\n```- Code example:\n```\nsc.hadoopConfiguration.set(\"spark.hadoop.mapreduce.outputcommitter.factory.class\",\"org.apache.hadoop.mapreduce.lib.output.DataprocFileOutputCommitterFactory\")\nsc.hadoopConfiguration.set(\"spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\")\n```The Dataproc file output committer must set`spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs=false`to avoid conflicts between success marker files created during concurrent writes. You can also set this property in`spark-defaults.conf`.", "guide": "Dataproc"}