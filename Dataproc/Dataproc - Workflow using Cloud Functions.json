{"title": "Dataproc - Workflow using Cloud Functions", "url": "https://cloud.google.com/dataproc/docs/tutorials/workflow-function", "abstract": "# Dataproc - Workflow using Cloud Functions\n**Objective:** - Create a Dataproc workflow template that runs a wordcount job- - Create a node.js Cloud function to trigger the wordcount workflow when a file is added to Cloud Storage\nWorkflows are \"fire and forget\". If there are no exceptions when the function that triggers the workflow is submitted, Dataproc will execute the workflow to completion. You do not need to wait on workflow completion within the triggering function.\n**Note:** Since the function is triggered by adding a new file to a bucket in Cloud Storage, and since the wordcount binary creates and outputs new files to Cloud Storage, to avoid recursion (wordcount processing its output files), this tutorial uses separate input and output buckets.\n", "content": "## Before you begin\nIf you haven't already done so, set up a Google Cloud project and two ( **2** ) Cloud Storage [buckets](/storage/docs/xml-api/put-bucket-create) .\n### Set up your project\n### Create or use two (2) Cloud Storage buckets in your project\nYou will need two Cloud Storage buckets in you project: one for input files, and one for output.\n- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets page](https://console.cloud.google.com/storage/browser) \n- Click **Create bucket** .\n- On the **Create a bucket** page, enter your bucket information. To go to the next step, click **Continue** .- For **Name your bucket** , enter a name that meets the [bucket naming requirements](/storage/docs/bucket-naming#requirements) .\n- For **Choose where to store your data** , do the following:- Select a **Location type** option.\n- Select a **Location** option.\n- For **Choose a default storage class for your data** , select a [storage class](/storage/docs/storage-classes) .\n- For **Choose how to control access to objects** , select an **Access control** option.\n- For **Advanced settings (optional)** , specify  an [encryption method](/storage/docs/encryption) ,  a [retention policy](/storage/docs/bucket-lock) ,  or [bucket labels](/storage/docs/tags-and-labels#bucket-labels) .\n- Click **Create** .\n## Create a workflow template.\nCopy and run the commands listed below in a local terminal window or in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) to create and define a [workflow template](/dataproc/docs/concepts/workflows/using-workflows) .\nNotes:\n- The commands specify the \"us-central1\" [region](/compute/docs/regions-zones#available) . You can specify  a different region or delete the`--region`flag if you have previously run [gcloud config set compute/region](/sdk/gcloud/reference/config/set#compute) to set the region property.\n- The \"-- \" (dash dash space) sequence passes arguments to the jar file.  The`wordcount` `` `` ``command will run the jar's wordcount application on text files contained in  the Cloud Storage`input_bucket`, then output wordcount  files to an`output_bucket`. You will parameterize the  wordcount input bucket argument to allow your function to supply this  argument.\n- Create the workflow template.```\ngcloud dataproc workflow-templates create wordcount-template \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central1\n```\n- Add the wordcount job to the workflow template.- Specify yourbefore running   the command (your function will supply the input bucket).   After you insert the output-bucket-name, the output   bucket argument should read as follows:`gs://your-output-bucket/wordcount-output\"`.\n- The \"count\" step ID   is required, and identifies the added hadoop job.\n```\ngcloud dataproc workflow-templates add-job hadoop \\\n\u00a0\u00a0\u00a0\u00a0--workflow-template=wordcount-template \\\n\u00a0\u00a0\u00a0\u00a0--step-id=count \\\n\u00a0\u00a0\u00a0\u00a0--jar=file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central1 \\\n\u00a0\u00a0\u00a0\u00a0-- wordcount gs://input-bucket gs://output-bucket-name/wordcount-output\n```\n- Use a [managed](/dataproc/docs/concepts/workflows/overview#managed_cluster) , [single-node](/dataproc/docs/concepts/configuring-clusters/single-node-clusters) cluster to run the workflow. Dataproc will create the  cluster, run the workflow on it, then delete the cluster when the workflow completes.```\ngcloud dataproc workflow-templates set-managed-cluster wordcount-template \\\n\u00a0\u00a0\u00a0\u00a0--cluster-name=wordcount \\\n\u00a0\u00a0\u00a0\u00a0--single-node \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central1\n```\n- Click on the`wordcount-template`name on the Dataproc [Workflows](https://console.cloud.google.com/dataproc/workflows/templates) page in the Google Cloud console to open the **Workflow template details** page. Confirm the wordcount-template  attributes.\n## Parameterize the workflow template.\n[Parameterize](/dataproc/docs/concepts/workflows/workflow-parameters) the input bucket variable to pass to the workflow template.\n- Export the workflow template to a`wordcount.yaml`text  file for parameterization.```\ngcloud dataproc workflow-templates export wordcount-template \\\n\u00a0\u00a0\u00a0\u00a0--destination=wordcount.yaml \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central1\n```\n- Using a text editor, open`wordcount.yaml`, then  add a`parameters`block to the end of YAML file so that  the Cloud Storage INPUT_BUCKET_URI can be  passed as`args[1]`to the wordcount binary when the  workflow is triggered.A sample exported YAML file is shown, below.   You can take one of two approaches to update your template:- Copy then paste the entire file to replace your exported`wordcount.yaml` **after replacing your-output_bucket\n  with your output bucket name** , OR\n- Copy then paste only the`parameters`section to the end of your exported`wordcount.yaml`file.\n.```\njobs:\n- hadoopJob:\n args:\n - wordcount\n - gs://input-bucket\n - gs://your-output-bucket/wordcount-output\n mainJarFileUri: file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\n stepId: count\nplacement:\n managedCluster:\n clusterName: wordcount\n config:\n  softwareConfig:\n  properties:\n   dataproc:dataproc.allow.zero.workers: 'true'\nparameters:\n- name: INPUT_BUCKET_URI\n description: wordcount input bucket URI\n fields:\n - jobs['count'].hadoopJob.args[1]\n```\n- Import the parameterized`wordcount.yaml`text  file. Type 'Y'es when asked to overwrite the template.```\ngcloud dataproc workflow-templates import wordcount-template \\\n\u00a0\u00a0\u00a0\u00a0--source=wordcount.yaml \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central1\n```## Create a Cloud function\n- Open the **Cloud Functions** page in the Google Cloud console, then click CREATE FUNCTION.\n- On the **Create function** page, enter or select the following information:- **Name:** wordcount\n- **Memory allocated:** Keep the default selection.\n- **Trigger:** - Cloud Storage\n- Event Type: Finalize/Create\n- Bucket: Select your input bucket (see [Create a Cloud Storage bucket in your project](#create_a_bucket_in_your_project) ). When a file is added to this bucket, the function will trigger the workflow. The workflow will run the wordcount application, which will process all text files in the bucket.\n- **Source code:** - Inline editor\n- Runtime: Node.js 8\n- `INDEX.JS`tab: Replace the default code snippet with the following code, then **edit the const projectId line to supply\n-your-project-id- (without a leading or trailing \"-\")** .\n```\nconst dataproc = require('@google-cloud/dataproc').v1;exports.startWorkflow = (data) => {\u00a0const projectId = '-your-project-id-'\u00a0const region = 'us-central1'\u00a0const workflowTemplate = 'wordcount-template'const client = new dataproc.WorkflowTemplateServiceClient({\u00a0 \u00a0apiEndpoint: `${region}-dataproc.googleapis.com`,});const file = data;console.log(\"Event: \", file);const inputBucketUri = `gs://${file.bucket}/${file.name}`;const request = {\u00a0 name: client.projectRegionWorkflowTemplatePath(projectId, region, workflowTemplate),\u00a0 parameters: {\"INPUT_BUCKET_URI\": inputBucketUri}};client.instantiateWorkflowTemplate(request)\u00a0 .then(responses => {\u00a0 \u00a0 console.log(\"Launched Dataproc Workflow:\", responses[1]);\u00a0 })\u00a0 .catch(err => {\u00a0 \u00a0 console.error(err);\u00a0 });};\n```- `PACKAGE.JSON`tab: Replace the default code snippet with the following code.\n```\n{\u00a0 \"name\": \"dataproc-workflow\",\u00a0 \"version\": \"1.0.0\",\u00a0 \"dependencies\":{ \"@google-cloud/dataproc\": \">=1.0.0\"}}\n```- Function to execute: Insert: \"startWorkflow\".\n- Click CREATE.\n## Test your function\n- Copy public file `rose.txt` to your bucket to trigger the function. Insert (the bucket used to trigger your function) in the command.```\ngsutil cp gs://pub/shakespeare/rose.txt gs://your-input-bucket-name\n```\n- Wait 30 seconds, then run the following command to verify the function completed successfully.```\ngcloud functions logs read wordcount\n``````\n...\nFunction execution took 1348 ms, finished with status: 'ok'\n```\n- To view the function logs from the [Functions](https://console.cloud.google.com/functions/list) list page in the Google Cloud console, click the `wordcount` function name, then click VIEW LOGS on the **Functiondetails** page.\n- You can view the `wordcount-output` folder in your output bucket from the [Storage browser](https://console.cloud.google.com/storage/browser) page in the Google Cloud console. **Note:** The wordcount job will fail if the `wordcount-output` folder exists.  Before re-running the workflow by re-triggering your function, first  delete the `wordcount-output` folder in your output bucket. .\n- After the workflow completes, job details persist in the Google Cloud console. Click the `count...` job listed on the Dataproc [Jobs](https://console.cloud.google.com/dataproc/jobs) page to view workflow job details.## Cleaning up\nThe workflow in this tutorial deletes its managed cluster when the workflow completes. To avoid recurring costs, you can delete other resources associated with this tutorial.\n### Deleting a project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Deleting Cloud Storage buckets\n- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets](https://console.cloud.google.com/storage/browser) \n- Click the checkbox for the bucket that you want to delete.\n- To delete the bucket,  clickdelete **Delete** , and then follow the  instructions.### Deleting your workflow template\n```\ngcloud dataproc workflow-templates delete wordcount-template \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central1\n```\n### Deleting your Cloud function\nOpen the **Cloud Functions** page in the Google Cloud console, select the box to the left of the `wordcount` function, then click DELETE.\nWhat's next\n- See [Overview of Dataproc Workflow Templates](/dataproc/docs/concepts/workflows/overview) \n- See [Workflow scheduling solutions](/dataproc/docs/concepts/workflows/workflow-schedule-solutions)", "guide": "Dataproc"}