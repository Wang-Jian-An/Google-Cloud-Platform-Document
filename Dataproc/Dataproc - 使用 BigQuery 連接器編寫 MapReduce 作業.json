{"title": "Dataproc - \u4f7f\u7528 BigQuery \u9023\u63a5\u5668\u7de8\u5beb MapReduce \u4f5c\u696d", "url": "https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-mapreduce-example?hl=zh-cn", "abstract": "# Dataproc - \u4f7f\u7528 BigQuery \u9023\u63a5\u5668\u7de8\u5beb MapReduce \u4f5c\u696d\n**\u6ce8\u610f** \uff1a\u6211\u5011\u4e0d\u518d\u7dad\u8b77\u672c\u6559\u7a0b\u4e2d\u6240\u8ff0\u7684\u9069\u7528\u65bc Hadoop MR \u7684 Hadoop BigQuery \u9023\u63a5\u5668\u3002\u4f5c\u7232\u6b64\u5df2\u505c\u7528\u7684\u9023\u63a5\u5668\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 [Spark BigQuery \u9023\u63a5\u5668](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) \u4f86\u63d0\u9ad8\u6027\u80fd\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u5c07 BigQuery \u9023\u63a5\u5668\u8207 Spark \u642d\u914d\u4f7f\u7528](https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example?hl=zh-cn) \u3002\nBigQuery \u9023\u63a5\u5668\u9ed8\u8a8d\u5b89\u88dd\u5728 `/usr/lib/hadoop/lib/` \u4e0b\u7684\u6240\u6709 Dataproc 1.0-1.2 \u96c6\u7fa3\u7bc0\u9ede\u4e0a\u3002Spark \u548c PySpark \u74b0\u5883\u4e2d\u5747\u53ef\u4f7f\u7528\u8a72\u9023\u63a5\u5668\u3002\n**Dataproc \u6620\u50cf\u7248\u672c 1.5 \u53ca\u66f4\u9ad8\u7248\u672c** \uff1a\u9ed8\u8a8d\u60c5\u6cc1\u4e0b\uff0cDataproc [\u6620\u50cf\u7248\u672c 1.5 \u53ca\u66f4\u9ad8\u7248\u672c](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions?hl=zh-cn#supported_cloud_dataproc_versions) \u4e2d\u672a\u5b89\u88dd BigQuery \u9023\u63a5\u5668\u3002\u5982\u9700\u5c07\u8a72\u9023\u63a5\u5668\u7528\u65bc\u9019\u4e9b\u7248\u672c\uff0c\u8acb\u57f7\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a\n- \u901a\u904e [\u521d\u59cb\u5316\u64cd\u4f5c](https://github.com/GoogleCloudDataproc/initialization-actions/tree/master/connectors) \u5b89\u88dd BigQuery \u9023\u63a5\u5668\n- \u63d0\u4ea4\u4f5c\u696d\u6642\uff0c\u5728 `jars` \u53c3\u6578\u4e2d\u6307\u5b9a BigQuery \u9023\u63a5\u5668\uff1a```\n--jars=gs://hadoop-lib/bigquery/bigquery-connector-hadoop3-latest.jar\n```\n- \u5728\u61c9\u7528\u7a0b\u5e8f\u7684 jar-with-dependencies \u4e2d\u5305\u542b BigQuery \u9023\u63a5\u5668\u985e\n**\u907f\u514d\u885d\u7a81** \uff1a\u5982\u679c\u60a8\u7684\u61c9\u7528\u4f7f\u7528\u7684\u9023\u63a5\u5668\u7248\u672c\u8207 [Dataproc \u96c6\u7fa3\u4e0a\u90e8\u7f72\u7684\u9023\u63a5\u5668\u7248\u672c](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions?hl=zh-cn#supported_cloud_dataproc_versions) \u4e0d\u540c\uff0c\u5247\u60a8\u5fc5\u9808\u57f7\u884c\u4ee5\u4e0b\u64cd\u4f5c\u4e4b\u4e00\uff1a\n- \u4f7f\u7528 [\u521d\u59cb\u5316\u64cd\u4f5c](https://github.com/GoogleCloudDataproc/initialization-actions/tree/master/connectors) \u5275\u5efa\u4e00\u500b\u65b0\u96c6\u7fa3\uff0c\u6b64\u64cd\u4f5c\u53ef\u5b89\u88dd\u61c9\u7528\u4f7f\u7528\u7684\u9023\u63a5\u5668\u7248\u672c\uff1b\u6216\u8005\n- \u5c07\u60a8\u6b63\u5728\u4f7f\u7528\u7684\u7248\u672c\u7684\u9023\u63a5\u5668\u985e\u548c\u9023\u63a5\u5668\u4f9d\u8cf4\u9805\u5305\u62ec\u5728\u5167\u4e26 [\u91cd\u65b0\u5b9a\u4f4d](https://maven.apache.org/plugins/maven-shade-plugin/examples/class-relocation.html) \u5230\u61c9\u7528\u7684 Jar \u6587\u4ef6\u4e2d\uff0c\u4ee5\u907f\u514d\u60a8\u7684\u9023\u63a5\u5668\u7248\u672c\u8207\u90e8\u7f72\u5728 Dataproc \u96c6\u7fa3\u4e2d\u7684\u9023\u63a5\u5668\u7248\u672c\u767c\u751f\u885d\u7a81\uff08\u8acb\u53c3\u95b1 [Maven \u4e2d\u7684\u4f9d\u8cf4\u9805\u91cd\u65b0\u5b9a\u4f4d\u793a\u4f8b](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/a839bd8f237197f98808944ca52d03de96518314/bigquery/pom.xml#L227-L286) \uff09\u3002", "content": "## GsonBigQueryInputFormat \u985e\n`BigQueryInputFormat`\u5df2\u91cd\u547d\u540d\u7232`GsonBigQueryInputFormat`\uff0c\u4ee5\u5f37\u8abf\u5176\u57fa\u65bc [Gson](https://github.com/google/gson/blob/master/UserGuide.md) } \u7684\u683c\u5f0f\u3002\n`GsonBigQueryInputFormat` \u901a\u904e\u4ee5\u4e0b\u4e3b\u8981\u64cd\u4f5c\u7232 Hadoop \u63d0\u4f9b\u4e86 JsonObject \u683c\u5f0f\u7684 BigQuery \u5c0d\u8c61\uff1a\n- \u4f7f\u7528\u7528\u6236\u6307\u5b9a\u7684\u67e5\u8a62\u4f86\u9078\u64c7 BigQuery \u5c0d\u8c61\n- \u5728 Hadoop \u7bc0\u9ede\u4e4b\u9593\u5747\u52fb\u62c6\u5206\u67e5\u8a62\u7d50\u679c\n- \u5c07\u62c6\u5206\u7d50\u679c\u89e3\u6790\u7232 java \u5c0d\u8c61\u4ee5\u50b3\u905e\u7d66 mapper\u3002 Hadoop Mapper \u985e\u53ef\u63a5\u6536\u4ee5`JsonObject`\u5f62\u5f0f\u8868\u793a\u7684\u6bcf\u500b\u9078\u5b9a BigQuery \u5c0d\u8c61\u3002\n`BigQueryInputFormat` \u985e\u901a\u904e Hadoop [InputFormat](http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html) \u985e\u7684\u64f4\u5c55\u7a0b\u5e8f\u63d0\u4f9b\u4e86 BigQuery \u8a18\u9304\u7684\u8a2a\u554f\u6b0a\u9650\u3002\u5982\u9700\u4f7f\u7528 BigQueryInputFormat \u985e\uff0c\u8acb\u57f7\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a\n- \u8981\u5728 Hadoop \u914d\u7f6e\u4e2d\u8a2d\u7f6e\u53c3\u6578\uff0c\u60a8\u5fc5\u9808\u5c07\u5e7e\u884c\u4ee3\u78bc\u6dfb\u52a0\u5230\u4e3b\u8981 Hadoop \u4f5c\u696d\u4e2d\u3002\n- \u5fc5\u9808\u5c07 [InputFormat](http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html) \u985e\u8a2d\u7f6e\u7232 `GsonBigQueryInputFormat` \u3002\n\u4ee5\u4e0b\u5404\u90e8\u5206\u4ecb\u7d39\u5982\u4f55\u6eff\u8db3\u9019\u4e9b\u8981\u6c42\u3002\n## \u8f38\u5165\u53c3\u6578\n```\n// Set the job-level projectId.conf.set(BigQueryConfiguration.PROJECT_ID_KEY, projectId);// Configure input parameters.BigQueryConfiguration.configureBigQueryInput(conf, inputQualifiedTableId);// Set InputFormat.job.setInputFormatClass(GsonBigQueryInputFormat.class);\n```\n\u6ce8\u610f\uff1a\n- `job`\u6307\u7684\u662f`org.apache.hadoop.mapreduce.Job`\uff08\u8868\u793a\u8981\u904b\u884c\u7684 Hadoop \u4f5c\u696d\uff09\u3002\n- `conf`\u8868\u793a Hadoop \u4f5c\u696d\u7684`org.apache.hadoop.Configuration`\u3002## Mapper\n`GsonBigQueryInputFormat` \u985e\u5f9e BigQuery \u4e2d\u8b80\u53d6\u6578\u64da\uff0c\u4e26\u4e00\u6b21\u5c07\u4e00\u500b BigQuery \u5c0d\u8c61\u4f5c\u7232\u8f38\u5165\u50b3\u905e\u7d66 Hadoop `Mapper` \u51fd\u6578\u3002\u8f38\u5165\u63a1\u7528\u5305\u542b\u4ee5\u4e0b\u5167\u5bb9\u7684\u9375\u503c\u5c0d\u5f62\u5f0f\uff1a\n- `LongWritable`\uff0c\u8a18\u9304\u7de8\u865f\n- `JsonObject`\uff0cJson \u683c\u5f0f\u7684 BigQuery \u8a18\u9304\n`Mapper` \u63a5\u53d7 `LongWritable` \u548c `JsonObject pair` \u4f5c\u7232\u8f38\u5165\u3002\n\u4ee5\u4e0b\u662f\u7528\u65bc [\u793a\u4f8b WordCount](#completecode) \u4f5c\u696d\u7684 `Mapper` \u4ee3\u78bc\u6bb5\u3002\n```\n\u00a0 // private static final LongWritable ONE = new LongWritable(1);\u00a0 // The configuration key used to specify the BigQuery field name\u00a0 // (\"column name\").\u00a0 public static final String WORDCOUNT_WORD_FIELDNAME_KEY =\u00a0 \u00a0 \u00a0 \"mapred.bq.samples.wordcount.word.key\";\u00a0 // Default value for the configuration entry specified by\u00a0 // WORDCOUNT_WORD_FIELDNAME_KEY. Examples: 'word' in\u00a0 // publicdata:samples.shakespeare or 'repository_name'\u00a0 // in publicdata:samples.github_timeline.\u00a0 public static final String WORDCOUNT_WORD_FIELDNAME_VALUE_DEFAULT = \"word\";\u00a0 /**\u00a0 \u00a0* The mapper function for WordCount.\u00a0 \u00a0*/\u00a0 public static class Map\u00a0 \u00a0 \u00a0 extends Mapper <LongWritable, JsonObject, Text, LongWritable> {\u00a0 \u00a0 private static final LongWritable ONE = new LongWritable(1);\u00a0 \u00a0 private Text word = new Text();\u00a0 \u00a0 private String wordKey;\u00a0 \u00a0 @Override\u00a0 \u00a0 public void setup(Context context)\u00a0 \u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 \u00a0 // Find the runtime-configured key for the field name we're looking for\u00a0 \u00a0 \u00a0 // in the map task.\u00a0 \u00a0 \u00a0 Configuration conf = context.getConfiguration();\u00a0 \u00a0 \u00a0 wordKey = conf.get(WORDCOUNT_WORD_FIELDNAME_KEY,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 WORDCOUNT_WORD_FIELDNAME_VALUE_DEFAULT);\u00a0 \u00a0 }\u00a0 \u00a0 @Override\u00a0 \u00a0 public void map(LongWritable key, JsonObject value, Context context)\u00a0 \u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 \u00a0 JsonElement countElement = value.get(wordKey);\u00a0 \u00a0 \u00a0 if (countElement != null) {\u00a0 \u00a0 \u00a0 \u00a0 String wordInRecord = countElement.getAsString();\u00a0 \u00a0 \u00a0 \u00a0 word.set(wordInRecord);\u00a0 \u00a0 \u00a0 \u00a0 // Write out the key, value pair (write out a value of 1, which will be\u00a0 \u00a0 \u00a0 \u00a0 // added to the total count for this word in the Reducer).\u00a0 \u00a0 \u00a0 \u00a0 context.write(word, ONE);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\n```\n## IndirectBigQueryOutputFormat \u985e\n`IndirectBigQueryOutputFormat` \u5141\u8a31 Hadoop \u5c07 `JsonObject` \u503c\u76f4\u63a5\u5beb\u5165 BigQuery \u8868\u3002\u8a72\u985e\u901a\u904e Hadoop [OutputFormat](http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/OutputFormat.html) \u985e\u7684\u64f4\u5c55\u7a0b\u5e8f\u63d0\u4f9b\u4e86 BigQuery \u8a18\u9304\u7684\u8a2a\u554f\u6b0a\u9650\u3002\u8981\u6b63\u78ba\u4f7f\u7528\u5b83\uff0c\u60a8\u5fc5\u9808\u5728 Hadoop \u914d\u7f6e\u4e2d\u8a2d\u7f6e\u5e7e\u500b\u53c3\u6578\uff0c\u4e26\u4e14\u5fc5\u9808\u5c07 OutputFormat \u985e\u8a2d\u7f6e\u7232 `IndirectBigQueryOutputFormat` \u3002\u8981\u8a2d\u7f6e\u7684\u53c3\u6578\u793a\u4f8b\u4ee5\u53ca\u6b63\u78ba\u4f7f\u7528 `IndirectBigQueryOutputFormat` \u6240\u9700\u7684\u4ee3\u78bc\u884c\u5982\u4e0b\u3002\n`IndirectBigQueryOutputFormat`\u9996\u5148\u5c07\u6240\u6709\u6578\u64da\u7de9\u885d\u5230 Cloud Storage \u81e8\u6642\u8868\u4e2d\uff0c\u7136\u5f8c\u901a\u904e`commitJob`\u5728\u4e00\u6b21\u64cd\u4f5c\u4e2d\u5c07\u6240\u6709\u6578\u64da\u5f9e Cloud Storage \u8907\u88fd\u5230 BigQuery \u4e2d\u3002\u5efa\u8b70\u5c07\u6b64\u547d\u4ee4\u7528\u65bc\u5927\u578b\u4f5c\u696d\uff0c\u56e0\u7232\u6bcf\u500b Hadoop/Spark \u4f5c\u696d\u53ea\u9700\u57f7\u884c\u4e00\u500b BigQuery\u201c\u52a0\u8f09\u201d\u4f5c\u696d\uff0c\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5728\u4f7f\u7528`BigQueryOutputFormat`\u6642\uff0c\u6bcf\u500b Hadoop/Spark \u4efb\u52d9\u90fd\u9700\u8981\u57f7\u884c\u4e00\u500bBigQuery \u4f5c\u696d\u3002\n## \u8f38\u51fa\u53c3\u6578\n```\n\u00a0 \u00a0 // Define the schema we will be using for the output BigQuery table.\u00a0 \u00a0 List<TableFieldSchema> outputTableFieldSchema = new ArrayList<TableFieldSchema>();\u00a0 \u00a0 outputTableFieldSchema.add(new TableFieldSchema().setName(\"Word\").setType(\"STRING\"));\u00a0 \u00a0 outputTableFieldSchema.add(new TableFieldSchema().setName(\"Count\").setType(\"INTEGER\"));\u00a0 \u00a0 TableSchema outputSchema = new TableSchema().setFields(outputTableFieldSchema);\u00a0 \u00a0 // Create the job and get its configuration.\u00a0 \u00a0 Job job = new Job(parser.getConfiguration(), \"wordcount\");\u00a0 \u00a0 Configuration conf = job.getConfiguration();\u00a0 \u00a0 // Set the job-level projectId.\u00a0 \u00a0 conf.set(BigQueryConfiguration.PROJECT_ID_KEY, projectId);\u00a0 \u00a0 // Configure input.\u00a0 \u00a0 BigQueryConfiguration.configureBigQueryInput(conf, inputQualifiedTableId);\u00a0 \u00a0 // Configure output.\u00a0 \u00a0 BigQueryOutputConfiguration.configure(\u00a0 \u00a0 \u00a0 \u00a0 conf,\u00a0 \u00a0 \u00a0 \u00a0 outputQualifiedTableId,\u00a0 \u00a0 \u00a0 \u00a0 outputSchema,\u00a0 \u00a0 \u00a0 \u00a0 outputGcsPath,\u00a0 \u00a0 \u00a0 \u00a0 BigQueryFileFormat.NEWLINE_DELIMITED_JSON,\u00a0 \u00a0 \u00a0 \u00a0 TextOutputFormat.class);\u00a0 \u00a0 // (Optional) Configure the KMS key used to encrypt the output table.\u00a0 \u00a0 BigQueryOutputConfiguration.setKmsKeyName(\u00a0 \u00a0 \u00a0 \u00a0 conf,\u00a0 \u00a0 \u00a0 \u00a0 \"projects/myproject/locations/us-west1/keyRings/r1/cryptoKeys/k1\"););\n```\n## Reducer\n`IndirectBigQueryOutputFormat` \u985e\u5411 BigQuery \u5beb\u5165\u5167\u5bb9\u3002 \u5b83\u5c07\u4e00\u500b\u9375\u548c\u4e00\u500b `JsonObject` \u503c\u4f5c\u7232\u8f38\u5165\uff0c\u4e26\u53ea\u5c07 JsonObject \u503c\u5beb\u5165 BigQuery\uff08\u8a72\u9375\u88ab\u5ffd\u7565\uff09\u3002 `JsonObject` \u61c9\u5305\u542b Json \u683c\u5f0f\u7684 BigQuery \u8a18\u9304\u3002\u7e2e\u6e1b\u5668\u61c9\u8f38\u51fa\u4efb\u610f\u985e\u578b\u7684\u9375\uff08\u5728\u6211\u5011\u7684 [\u793a\u4f8b WordCount](#completecode) \u4f5c\u696d\u4e2d\u4f7f\u7528\u4e86 `NullWritable` \uff09\u548c `JsonObject` \u503c\u5c0d\u3002\u793a\u4f8b WordCount \u4f5c\u696d\u7684 Reducer \u5982\u4e0b\u6240\u793a\u3002\n```\n\u00a0 /**\u00a0 \u00a0* Reducer function for WordCount.\u00a0 \u00a0*/\u00a0 public static class Reduce\u00a0 \u00a0 \u00a0 extends Reducer<Text, LongWritable, JsonObject, NullWritable> {\u00a0 \u00a0 @Override\u00a0 \u00a0 public void reduce(Text key, Iterable<LongWritable> values, Context context)\u00a0 \u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 \u00a0 // Add up the values to get a total number of occurrences of our word.\u00a0 \u00a0 \u00a0 long count = 0;\u00a0 \u00a0 \u00a0 for (LongWritable val : values) {\u00a0 \u00a0 \u00a0 \u00a0 count = count + val.get();\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 JsonObject jsonObject = new JsonObject();\u00a0 \u00a0 \u00a0 jsonObject.addProperty(\"Word\", key.toString());\u00a0 \u00a0 \u00a0 jsonObject.addProperty(\"Count\", count);\u00a0 \u00a0 \u00a0 // Key does not matter.\u00a0 \u00a0 \u00a0 context.write(jsonObject, NullWritable.get());\u00a0 \u00a0 }\u00a0 }\n```\n## \u6e05\u7406\n\u4f5c\u696d\u5b8c\u6210\u5f8c\uff0c\u8acb\u6e05\u7406 Cloud Storage \u5c0e\u51fa\u8def\u5f91\u3002\n```\njob.waitForCompletion(true);GsonBigQueryInputFormat.cleanupJob(job.getConfiguration(), job.getJobID());\n```\n\u60a8\u53ef\u4ee5\u5728 [Google Cloud \u63a7\u5236\u6aaf](https://console.cloud.google.com/bigquery?hl=zh-cn) \u7684 BigQuery \u8f38\u51fa\u8868\u4e2d\u67e5\u770b\u5b57\u6578\u7d71\u8a08\u3002\n## \u793a\u4f8b WordCount \u4f5c\u696d\u7684\u5b8c\u6574\u4ee3\u78bc\n\u4e0b\u9762\u7684\u4ee3\u78bc\u662f\u4e00\u500b\u7c21\u55ae\u7684 WordCount \u4f5c\u696d\u793a\u4f8b\uff0c\u5b83\u5f59\u7e3d\u4e86 BigQuery \u4e2d\u5c0d\u8c61\u7684\u5b57\u6578\u3002\n```\npackage com.google.cloud.hadoop.io.bigquery.samples;import com.google.api.services.bigquery.model.TableFieldSchema;import com.google.api.services.bigquery.model.TableSchema;import com.google.cloud.hadoop.io.bigquery.BigQueryConfiguration;import com.google.cloud.hadoop.io.bigquery.BigQueryFileFormat;import com.google.cloud.hadoop.io.bigquery.GsonBigQueryInputFormat;import com.google.cloud.hadoop.io.bigquery.output.BigQueryOutputConfiguration;import com.google.cloud.hadoop.io.bigquery.output.IndirectBigQueryOutputFormat;import com.google.gson.JsonElement;import com.google.gson.JsonObject;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;import java.io.IOException;import java.util.ArrayList;import java.util.List;/**\u00a0* Sample program to run the Hadoop Wordcount example over tables in BigQuery.\u00a0*/public class WordCount {\u00a0// The configuration key used to specify the BigQuery field name\u00a0 // (\"column name\").\u00a0 public static final String WORDCOUNT_WORD_FIELDNAME_KEY =\u00a0 \u00a0 \u00a0 \"mapred.bq.samples.wordcount.word.key\";\u00a0 // Default value for the configuration entry specified by\u00a0 // WORDCOUNT_WORD_FIELDNAME_KEY. Examples: 'word' in\u00a0 // publicdata:samples.shakespeare or 'repository_name'\u00a0 // in publicdata:samples.github_timeline.\u00a0 public static final String WORDCOUNT_WORD_FIELDNAME_VALUE_DEFAULT = \"word\";\u00a0 // Guava might not be available, so define a null / empty helper:\u00a0 private static boolean isStringNullOrEmpty(String toTest) {\u00a0 \u00a0 return toTest == null || \"\".equals(toTest);\u00a0 }\u00a0 /**\u00a0 \u00a0* The mapper function for WordCount. For input, it consumes a LongWritable\u00a0 \u00a0* and JsonObject as the key and value. These correspond to a row identifier\u00a0 \u00a0* and Json representation of the row's values/columns.\u00a0 \u00a0* For output, it produces Text and a LongWritable as the key and value.\u00a0 \u00a0* These correspond to the word and a count for the number of times it has\u00a0 \u00a0* occurred.\u00a0 \u00a0*/\u00a0 public static class Map\u00a0 \u00a0 \u00a0 extends Mapper <LongWritable, JsonObject, Text, LongWritable> {\u00a0 \u00a0 private static final LongWritable ONE = new LongWritable(1);\u00a0 \u00a0 private Text word = new Text();\u00a0 \u00a0 private String wordKey;\u00a0 \u00a0 @Override\u00a0 \u00a0 public void setup(Context context)\u00a0 \u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 \u00a0 // Find the runtime-configured key for the field name we're looking for in\u00a0 \u00a0 \u00a0 // the map task.\u00a0 \u00a0 \u00a0 Configuration conf = context.getConfiguration();\u00a0 \u00a0 \u00a0 wordKey = conf.get(WORDCOUNT_WORD_FIELDNAME_KEY, WORDCOUNT_WORD_FIELDNAME_VALUE_DEFAULT);\u00a0 \u00a0 }\u00a0 \u00a0 @Override\u00a0 \u00a0 public void map(LongWritable key, JsonObject value, Context context)\u00a0 \u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 \u00a0 JsonElement countElement = value.get(wordKey);\u00a0 \u00a0 \u00a0 if (countElement != null) {\u00a0 \u00a0 \u00a0 \u00a0 String wordInRecord = countElement.getAsString();\u00a0 \u00a0 \u00a0 \u00a0 word.set(wordInRecord);\u00a0 \u00a0 \u00a0 \u00a0 // Write out the key, value pair (write out a value of 1, which will be\u00a0 \u00a0 \u00a0 \u00a0 // added to the total count for this word in the Reducer).\u00a0 \u00a0 \u00a0 \u00a0 context.write(word, ONE);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\u00a0 /**\u00a0 \u00a0* Reducer function for WordCount. For input, it consumes the Text and\u00a0 \u00a0* LongWritable that the mapper produced. For output, it produces a JsonObject\u00a0 \u00a0* and NullWritable. The JsonObject represents the data that will be\u00a0 \u00a0* loaded into BigQuery.\u00a0 \u00a0*/\u00a0 public static class Reduce\u00a0 \u00a0 \u00a0 extends Reducer<Text, LongWritable, JsonObject, NullWritable> {\u00a0 \u00a0 @Override\u00a0 \u00a0 public void reduce(Text key, Iterable<LongWritable> values, Context context)\u00a0 \u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 \u00a0 // Add up the values to get a total number of occurrences of our word.\u00a0 \u00a0 \u00a0 long count = 0;\u00a0 \u00a0 \u00a0 for (LongWritable val : values) {\u00a0 \u00a0 \u00a0 \u00a0 count = count + val.get();\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 JsonObject jsonObject = new JsonObject();\u00a0 \u00a0 \u00a0 jsonObject.addProperty(\"Word\", key.toString());\u00a0 \u00a0 \u00a0 jsonObject.addProperty(\"Count\", count);\u00a0 \u00a0 \u00a0 // Key does not matter.\u00a0 \u00a0 \u00a0 context.write(jsonObject, NullWritable.get());\u00a0 \u00a0 }\u00a0 }\u00a0 /**\u00a0 \u00a0* Configures and runs the main Hadoop job. Takes a String[] of 5 parameters:\u00a0 \u00a0* [ProjectId] [QualifiedInputTableId] [InputTableFieldName]\u00a0 \u00a0* [QualifiedOutputTableId] [GcsOutputPath]\u00a0 \u00a0*\u00a0 \u00a0* ProjectId - Project under which to issue the BigQuery\u00a0 \u00a0* operations. Also serves as the default project for table IDs that don't\u00a0 \u00a0* specify a project for the table.\u00a0 \u00a0*\u00a0 \u00a0* QualifiedInputTableId - Input table ID of the form\u00a0 \u00a0* (Optional ProjectId):[DatasetId].[TableId]\u00a0 \u00a0*\u00a0 \u00a0* InputTableFieldName - Name of the field to count in the\u00a0 \u00a0* input table, e.g., 'word' in publicdata:samples.shakespeare or\u00a0 \u00a0* 'repository_name' in publicdata:samples.github_timeline.\u00a0 \u00a0*\u00a0 \u00a0* QualifiedOutputTableId - Input table ID of the form\u00a0 \u00a0* (Optional ProjectId):[DatasetId].[TableId]\u00a0 \u00a0*\u00a0 \u00a0* GcsOutputPath - The output path to store temporary\u00a0 \u00a0* Cloud Storage data, e.g., gs://bucket/dir/\u00a0 \u00a0*\u00a0 \u00a0* @param args a String[] containing ProjectId, QualifiedInputTableId,\u00a0 \u00a0* \u00a0 \u00a0 InputTableFieldName, QualifiedOutputTableId, and GcsOutputPath.\u00a0 \u00a0* @throws IOException on IO Error.\u00a0 \u00a0* @throws InterruptedException on Interrupt.\u00a0 \u00a0* @throws ClassNotFoundException if not all classes are present.\u00a0 \u00a0*/\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ClassNotFoundException {\u00a0 \u00a0 // GenericOptionsParser is a utility to parse command line arguments\u00a0 \u00a0 // generic to the Hadoop framework. This example doesn't cover the specifics,\u00a0 \u00a0 // but recognizes several standard command line arguments, enabling\u00a0 \u00a0 // applications to easily specify a NameNode, a ResourceManager, additional\u00a0 \u00a0 // configuration resources, etc.\u00a0 \u00a0 GenericOptionsParser parser = new GenericOptionsParser(args);\u00a0 \u00a0 args = parser.getRemainingArgs();\u00a0 \u00a0 // Make sure we have the right parameters.\u00a0 \u00a0 if (args.length != 5) {\u00a0 \u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Usage: hadoop jar bigquery_wordcount.jar [ProjectId] [QualifiedInputTableId] \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"[InputTableFieldName] [QualifiedOutputTableId] [GcsOutputPath]\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" \u00a0 \u00a0ProjectId - Project under which to issue the BigQuery operations. Also serves \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"as the default project for table IDs that don't explicitly specify a project for \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"the table.\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" \u00a0 \u00a0QualifiedInputTableId - Input table ID of the form \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"(Optional ProjectId):[DatasetId].[TableId]\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" \u00a0 \u00a0InputTableFieldName - Name of the field to count in the input table, e.g., \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"'word' in publicdata:samples.shakespeare or 'repository_name' in \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"publicdata:samples.github_timeline.\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" \u00a0 \u00a0QualifiedOutputTableId - Input table ID of the form \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"(Optional ProjectId):[DatasetId].[TableId]\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" \u00a0 \u00a0GcsOutputPath - The output path to store temporary Cloud Storage data, e.g., \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"gs://bucket/dir/\");\u00a0 \u00a0 \u00a0 System.exit(1);\u00a0 \u00a0 }\u00a0 \u00a0 // Get the individual parameters from the command line.\u00a0 \u00a0 String projectId = args[0];\u00a0 \u00a0 String inputQualifiedTableId = args[1];\u00a0 \u00a0 String inputTableFieldId = args[2];\u00a0 \u00a0 String outputQualifiedTableId = args[3];\u00a0 \u00a0 String outputGcsPath = args[4];\u00a0 \u00a0// Define the schema we will be using for the output BigQuery table.\u00a0 \u00a0 List<TableFieldSchema> outputTableFieldSchema = new ArrayList<TableFieldSchema>();\u00a0 \u00a0 outputTableFieldSchema.add(new TableFieldSchema().setName(\"Word\").setType(\"STRING\"));\u00a0 \u00a0 outputTableFieldSchema.add(new TableFieldSchema().setName(\"Count\").setType(\"INTEGER\"));\u00a0 \u00a0 TableSchema outputSchema = new TableSchema().setFields(outputTableFieldSchema);\u00a0 \u00a0 // Create the job and get its configuration.\u00a0 \u00a0 Job job = new Job(parser.getConfiguration(), \"wordcount\");\u00a0 \u00a0 Configuration conf = job.getConfiguration();\u00a0 \u00a0 // Set the job-level projectId.\u00a0 \u00a0 conf.set(BigQueryConfiguration.PROJECT_ID_KEY, projectId);\u00a0 \u00a0 // Configure input.\u00a0 \u00a0 BigQueryConfiguration.configureBigQueryInput(conf, inputQualifiedTableId);\u00a0 \u00a0 // Configure output.\u00a0 \u00a0 BigQueryOutputConfiguration.configure(\u00a0 \u00a0 \u00a0 \u00a0 conf,\u00a0 \u00a0 \u00a0 \u00a0 outputQualifiedTableId,\u00a0 \u00a0 \u00a0 \u00a0 outputSchema,\u00a0 \u00a0 \u00a0 \u00a0 outputGcsPath,\u00a0 \u00a0 \u00a0 \u00a0 BigQueryFileFormat.NEWLINE_DELIMITED_JSON,\u00a0 \u00a0 \u00a0 \u00a0 TextOutputFormat.class);\u00a0 \u00a0 // (Optional) Configure the KMS key used to encrypt the output table.\u00a0 \u00a0 BigQueryOutputConfiguration.setKmsKeyName(\u00a0 \u00a0 \u00a0 \u00a0 conf,\u00a0 \u00a0 \u00a0 \u00a0 \"projects/myproject/locations/us-west1/keyRings/r1/cryptoKeys/k1\");\u00a0 \u00a0 conf.set(WORDCOUNT_WORD_FIELDNAME_KEY, inputTableFieldId);\u00a0 \u00a0 // This helps Hadoop identify the Jar which contains the mapper and reducer\u00a0 \u00a0 // by specifying a class in that Jar. This is required if the jar is being\u00a0 \u00a0 // passed on the command line to Hadoop.\u00a0 \u00a0 job.setJarByClass(WordCount.class);\u00a0 \u00a0 // Tell the job what data the mapper will output.\u00a0 \u00a0 job.setOutputKeyClass(Text.class);\u00a0 \u00a0 job.setOutputValueClass(LongWritable.class);\u00a0 \u00a0 job.setMapperClass(Map.class);\u00a0 \u00a0 job.setReducerClass(Reduce.class);\u00a0 \u00a0 job.setInputFormatClass(GsonBigQueryInputFormat.class);\u00a0 \u00a0 // Instead of using BigQueryOutputFormat, we use the newer\u00a0 \u00a0 // IndirectBigQueryOutputFormat, which works by first buffering all the data\u00a0 \u00a0 // into a Cloud Storage temporary file, and then on commitJob, copies all data from\u00a0 \u00a0 // Cloud Storage into BigQuery in one operation. Its use is recommended for large jobs\u00a0 \u00a0 // since it only requires one BigQuery \"load\" job per Hadoop/Spark job, as\u00a0 \u00a0 // compared to BigQueryOutputFormat, which performs one BigQuery job for each\u00a0 \u00a0 // Hadoop/Spark task.\u00a0 \u00a0 job.setOutputFormatClass(IndirectBigQueryOutputFormat.class);\u00a0 \u00a0 job.waitForCompletion(true);\u00a0 \u00a0 // After the job completes, clean up the Cloud Storage export paths.\u00a0 \u00a0 GsonBigQueryInputFormat.cleanupJob(job.getConfiguration(), job.getJobID());\u00a0 \u00a0 // You can view word counts in the BigQuery output table at\u00a0 \u00a0 // https://console.cloud.google.com/.\u00a0 }}\n```\n## Java \u7248\u672c\nBigQuery \u9023\u63a5\u5668\u9700\u8981 Java 8\u3002\n### Apache Maven \u4f9d\u8cf4\u95dc\u4fc2\u4fe1\u606f\n```\n<dependency>\u00a0 \u00a0 <groupId>com.google.cloud.bigdataoss</groupId>\u00a0 \u00a0 <artifactId>bigquery-connector</artifactId>\u00a0 \u00a0 <version>insert \"hadoopX-X.X.X\" connector version number here</version></dependency>\n```\n\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 BigQuery \u9023\u63a5\u5668 [\u7248\u672c\u8aaa\u660e](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases) \u548c [Javadoc \u53c3\u8003](http://www.javadoc.io/doc/com.google.cloud.bigdataoss/bigquery-connector/) \u3002", "guide": "Dataproc"}