{"title": "Dataproc - Run a genomics analysis in a JupyterLab notebook on Dataproc", "url": "https://cloud.google.com/dataproc/docs/tutorials/genomics-single-cell", "abstract": "# Dataproc - Run a genomics analysis in a JupyterLab notebook on Dataproc\nThis tutorial shows you how to run a single-cell genomics analysis using [Dask](https://dask.org/) , [NVIDIA RAPIDS](https://rapids.ai/) , and GPUs, which you can configure on [Dataproc](/dataproc) . You can configure Dataproc to run Dask either with its standalone scheduler or with YARN for resource management.\nThis tutorial configures Dataproc with a hosted [JupyterLab](https://jupyter.org/) instance to run a notebook featuring a single-cell genomics analysis. Using a Jupyter Notebook on Dataproc lets you combine the interactive capabilities of Jupyter with the workload scaling that Dataproc enables. With Dataproc, you can scale out your workloads from one to many machines, which you can configure with as many GPUs as you need.\nThis tutorial is intended for data scientists and researchers. It assumes that you are experienced with Python and have basic knowledge of the following:- [Dataproc](/dataproc) \n- [Dask](https://dask.org/) \n- [RAPIDS](https://rapids.ai/) \n- [Jupyter Notebooks](https://jupyter.org/) ", "content": "## Objectives\n- Create a Dataproc instance which is configured with GPUs, JupyterLab, and open source components.\n- Run a [notebook](https://github.com/clara-parabricks/rapids-single-cell-examples/blob/rilango/batching/notebooks/1M_brain_gpu_analysis_uvm.ipynb) on Dataproc.\n## CostsIn this document, you use the following billable components of Google Cloud:\n [Dataproc](/dataproc/pricing) \n [Cloud Storage](/storage/pricing) \n [GPUs](/compute/gpus-pricing) To generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- In the Google Cloud console, on the project selector page,   select or [create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . **Note** : If you don't plan to keep the  resources that you create in this procedure, create a project instead of  selecting an existing project. After you finish these steps, you can  delete the project, removing all resources associated with the project. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- Enable the Dataproc API. [Enable the API](https://console.cloud.google.com/flows/enableapi?apiid=dataproc) \n## Prepare your environment\n- Select a [location](/storage/docs/locations) for your resources.```\nREGION=REGION\n```\n- Create a Cloud Storage bucket.```\ngsutil mb BUCKET -l REGION\n```\n- Copy the following [initialization actions](/dataproc/docs/concepts/configuring-clusters/init-actions) to your bucket.```\nSCRIPT_BUCKET=gs://goog-dataproc-initialization-actions-REGION\ngsutil cp ${SCRIPT_BUCKET}/gpu/install_gpu_driver.sh BUCKET/gpu/install_gpu_driver.sh\ngsutil cp ${SCRIPT_BUCKET}/dask/dask.sh BUCKET/dask/dask.sh\ngsutil cp ${SCRIPT_BUCKET}/rapids/rapids.sh BUCKET/rapids/rapids.sh\ngsutil cp ${SCRIPT_BUCKET}/python/pip-install.sh BUCKET/python/pip-install.sh\n```\n## Create a Dataproc cluster with JupyterLab and open source components\n- [Create](/sdk/gcloud/reference/dataproc/clusters/create) a Dataproc cluster.\n```\ngcloud dataproc clusters create CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region REGION \\\n\u00a0\u00a0\u00a0\u00a0--image-version 2.0-ubuntu18 \\\n\u00a0\u00a0\u00a0\u00a0--master-machine-type n1-standard-32 \\\n\u00a0\u00a0\u00a0\u00a0--master-accelerator type=nvidia-tesla-t4,count=4 \\\n\u00a0\u00a0\u00a0\u00a0--initialization-actions\nBUCKET/gpu/install_gpu_driver.sh,BUCKET/dask/dask.sh,BUCKET/rapids/rapids.sh,BUCKET/python/pip-install.sh\n\\\n\u00a0\u00a0\u00a0\u00a0--initialization-action-timeout=60m \\\n\u00a0\u00a0\u00a0\u00a0--metadata\ngpu-driver-provider=NVIDIA,dask-runtime=yarn,rapids-runtime=DASK,rapids-version=21.06,PIP_PACKAGES=\"scanpy==1.8.1,wget\" \\\n\u00a0\u00a0\u00a0\u00a0--optional-components JUPYTER \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway \\\n\u00a0\u00a0\u00a0\u00a0--single-node\n```\nThe cluster has the following properties:- `--region`: the [region](/compute/docs/regions-zones#available) where your cluster is located.\n- `--image-version`:`2.0-ubuntu18`, the [cluster image version](/dataproc/docs/concepts/versioning/dataproc-versions#debian_images) \n- `--master-machine-type`:`n1-standard-32`, the main [machine type](/compute/docs/machine-types) .\n- `--master-accelerator`: the type and count of [GPUs](/dataproc/docs/concepts/compute/gpus) on the main node, four`nvidia-tesla-t4`GPUs.\n- `--initialization-actions`: the Cloud Storage paths to the installation scripts that install GPU drivers, Dask, RAPIDS, and extra dependencies.\n- `--initialization-action-timeout`: the timeout for the initialization actions.\n- `--metadata`: passed to the initialization actions to configure the cluster with NVIDIA GPU drivers, the standalone scheduler for Dask, and RAPIDS version`21.06`.\n- `--optional-components`: configures the cluster with the [Jupyter optional component](/dataproc/docs/concepts/components/jupyter) .\n- `--enable-component-gateway`: allows access to web UIs on the cluster.\n- `--single-node`: configures the cluster as a single node (no workers).\n## Access the Jupyter Notebook\n- Open the **Clusters** page in the Dataproc Google Cloud console. [Open Clusters page](https://console.cloud.google.com/dataproc/clusters) \n- Click your cluster and click the **Web Interfaces** tab.\n- Click **JupyterLab** .\n- Open a [new terminal](https://jupyterlab.readthedocs.io/en/stable/user/terminal.html) in JupyterLab.\n- Clone the `clara-parabricks/rapids-single-cell-examples` [repository](https://github.com/clara-parabricks/rapids-single-cell-examples) and check out the `dataproc/multi-gpu` branch.```\ngit clone https://github.com/clara-parabricks/rapids-single-cell-examples.git\ngit checkout dataproc/multi-gpu\n```\n- In JupyterLab, navigate to the [rapids-single-cell-examples/notebooks](https://github.com/clara-parabricks/rapids-single-cell-examples) repository and open the [1M_brain_gpu_analysis_uvm.ipynb](https://github.com/clara-parabricks/rapids-single-cell-examples/blob/rilango/batching/notebooks/1M_brain_gpu_analysis_uvm.ipynb) Jupyter Notebook.\n- To clear all the outputs in the notebook, select **Edit > Clear All Outputs** \n- Read the instructions in the cells of the notebook. The notebook uses Dask and RAPIDS on Dataproc to guide you through a single-cell RNA-seq workflow on 1 million cells, including processing and visualizing the data. To learn more, see [Accelerating Single Cell Genomic Analysis using RAPIDS](https://developer.nvidia.com/blog/accelerating-single-cell-genomic-analysis-using-rapids/) .\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Delete individual resources\n- [Delete your Dataproc cluster.](/dataproc/docs/guides/manage-cluster#deleting_a_cluster) ```\ngcloud dataproc clusters delete cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region\n```\n- Delete the bucket:```\ngcloud storage buckets delete BUCKET_NAME\n``` **Important:** Your bucket must  be empty before you can delete it.## What's next\n- Discover more about [Dataproc](/dataproc) \n- Learn more about the [Cloud Life Sciences reference architecture](/architecture/genomic-data-processing-reference-architecture) .\n- Explore reference architectures, diagrams, tutorials, and best practices. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Dataproc"}