{"title": "Dataproc - Use the BigQuery connector with Spark", "url": "https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example", "abstract": "# Dataproc - Use the BigQuery connector with Spark\nThe [spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) is used with [Apache Spark](https://spark.apache.org) to read and write data from and to [BigQuery](/bigquery) . This tutorial provides example code that uses the spark-bigquery-connector within a Spark application. For instructions on creating a cluster, see the [Dataproc Quickstarts](/dataproc/docs/quickstarts) .\nThe spark-bigquery-connector takes advantage of the [BigQueryStorage API](/bigquery/docs/reference/storage) when reading data from BigQuery.\n", "content": "## Make the connector available to your application\nYou can make the spark-bigquery-connector available to your application in one of the following ways:\n- Install the spark-bigquery-connector in the Spark jars directory of every node by using the [Dataproc connectors initialization action](https://github.com/GoogleCloudDataproc/initialization-actions/tree/master/connectors) when you create your cluster.\n- Provide the connector URI when you submit your job:- **Google Cloud console:** Use the Spark job`Jars files`item on the Dataproc [Submit a job](https://console.cloud.google.com/dataproc/jobs/jobsSubmit) page.\n- **gcloud CLI:** Use the [gcloud dataproc jobs submit spark --jars flag](/sdk/gcloud/reference/dataproc/jobs/submit/spark#--jars) .\n- **Dataproc API:** Use the [SparkJob.jarFileUris field](/dataproc/docs/reference/rest/v1/SparkJob#FIELDS.jar_file_uris) .\n- Include the jar in your Scala or Java Spark application as a dependency (see [Compiling against the connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector#compiling-against-the-connector) ).\n**Note:** If the connector is not available at runtime, a `ClassNotFoundException` is thrown.\n### How to specify the connector jar URI\nSpark-BigQuery connector versions are listed in the GitHub [GoogleCloudDataproc/spark-bigquery-connector repository](https://github.com/GoogleCloudDataproc/spark-bigquery-connector/releases) .\nSpecify the connector jar by substituting the Scala and connector version information in the following URI string:\n```\ngs://spark-lib/bigquery/spark-bigquery-with-dependencies_SCALA_VERSION-CONNECTOR_VERSION.jar\n```- Use Scala `2.12` with Dataproc image versions `1.5+` ```\ngs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-CONNECTOR_VERSION.jar\n```gcloud CLI example:```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.23.2.jar \\\n\u00a0\u00a0\u00a0\u00a0-- job-args\n```\n- Use Scala `2.11` with Dataproc image versions `1.4` and earlier:```\ngs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.11-CONNECTOR_VERSION.jar\n```gcloud CLI example:```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.11-0.23.2.jar \\\n\u00a0\u00a0\u00a0\u00a0-- job-args\n```\nFor non-production use, you can also point to the`latest`jars, as follows:- Dataproc image version`1.5+`:`--jars=gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar`\n- Dataproc image versions 1.4 and earlier:`--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar`## Calculating costs\nIn this document, you use the following billable components of Google Cloud:\n- Dataproc\n- BigQuery\n- Cloud Storage\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) .\n## Reading and writing data from BigQuery\nThis example reads data from [BigQuery](https://console.cloud.google.com/bigquery) into a Spark DataFrame to perform a word count using the [standard data sourceAPI](https://spark.apache.org/docs/latest/sql-data-sources.html) .\nThe connector writes the data to BigQuery by first buffering all the data into a Cloud Storage temporary table. Then it copies all data from into BigQuery in one operation. The connector attempts to delete the temporary files once the BigQuery load operation has succeeded and once again when the Spark application terminates. If the job fails, remove any remaining temporary Cloud Storage files. Typically, temporary BigQuery files are located in `gs://[bucket]/.spark-bigquery-[jobid]-[UUID]` .\n## Configuring billing\nBy default, the project associated with the credentials or service account is billed for API usage. To bill a different project, set the following configuration: `spark.conf.set(\"parentProject\", \"<BILLED-GCP-PROJECT>\")` .\nIt can also be added to a read/write operation, as follows: `.option(\"parentProject\", \"<BILLED-GCP-PROJECT>\")` .\n## Running the code\nBefore running this example, create a dataset named \"wordcount_dataset\" or change the output dataset in the code to an existing BigQuery dataset in your Google Cloud project.\nUse the [bq](/bigquery/bq-command-line-tool) command to create the `wordcount_dataset` :\n```\nbq mk wordcount_dataset\n```\nUse the [gsutil](/storage/docs/gsutil) command to create a Cloud Storage bucket, which will be used to export to BigQuery:\n```\ngsutil mb gs://[bucket]\n```\n- Examine the code and replace theplaceholder with the Cloud Storage bucket you created earlier. [](None) ```\n/*\u00a0* Remove comment if you are not running in spark-shell.\u00a0*import org.apache.spark.sql.SparkSessionval spark = SparkSession.builder()\u00a0 .appName(\"spark-bigquery-demo\")\u00a0 .getOrCreate()*/// Use the Cloud Storage bucket for temporary BigQuery export data used// by the connector.val bucket = \"[bucket]\"spark.conf.set(\"temporaryGcsBucket\", bucket)// Load data in from BigQuery. See// https://github.com/GoogleCloudDataproc/spark-bigquery-connector/tree/0.17.3#properties// for option information.val wordsDF =\u00a0 (spark.read.format(\"bigquery\")\u00a0 .option(\"table\",\"bigquery-public-data:samples.shakespeare\")\u00a0 .load()\u00a0 .cache())wordsDF.createOrReplaceTempView(\"words\")// Perform word count.val wordCountDF = spark.sql(\u00a0 \"SELECT word, SUM(word_count) AS word_count FROM words GROUP BY word\")wordCountDF.show()wordCountDF.printSchema()// Saving the data to BigQuery.(wordCountDF.write.format(\"bigquery\")\u00a0 .option(\"table\",\"wordcount_dataset.wordcount_output\")\u00a0 .save())\n```\n- Run the code on your cluster- Use SSH to connect to the Dataproc cluster master node\n- Go to the [Dataproc Clusters](https://console.cloud.google.com/dataproc/clusters) page in the Google Cloud console, then click the name of your cluster\n- On the **>Cluster details** page, select the VM Instances tab. Then, click`SSH`to the right of the name of the cluster master nodeA browser window opens at your home directory on the master node```\n Connected, host fingerprint: ssh-rsa 2048 ...\n ...\n user@clusterName-m:~$\n \n```- Create`wordcount.scala`with the pre-installed`vi`,`vim`, or`nano`text editor, then paste in the Scala code from the [Scala code listing](#scala-code-listing) ```\nnano wordcount.scala\n \n```\n- Launch the`spark-shell`REPL.```\n$ spark-shell --jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar\n...\nUsing Scala version ...\nType in expressions to have them evaluated.\nType :help for more information.\n...\nSpark context available as sc.\n...\nSQL context available as sqlContext.\nscala>\n```\n- Run wordcount.scala with the`:load wordcount.scala`command to create the BigQuery`wordcount_output`table. The output listing displays 20 lines from the wordcount output.```\n:load wordcount.scala\n...\n+---------+----------+\n|  word|word_count|\n+---------+----------+\n|  XVII|   2|\n| spoil|  28|\n| Drink|   7|\n|forgetful|   5|\n| Cannot|  46|\n| cures|  10|\n| harder|  13|\n| tresses|   3|\n|  few|  62|\n| steel'd|   5|\n| tripping|   7|\n| travel|  35|\n| ransom|  55|\n|  hope|  366|\n|  By|  816|\n|  some|  1169|\n| those|  508|\n| still|  567|\n|  art|  893|\n| feign|  10|\n+---------+----------+\nonly showing top 20 rows\nroot\n |-- word: string (nullable = false)\n |-- word_count: long (nullable = true)\n```To preview the output table, open the [BigQuery](https://console.cloud.google.com/bigquery) page, select the`wordcount_output`table, and then click **Preview** .- Examine the code and replace theplaceholder with the Cloud Storage bucket you created earlier. [](None) ```\n#!/usr/bin/env python\"\"\"BigQuery I/O PySpark example.\"\"\"from pyspark.sql import SparkSessionspark = SparkSession \\\u00a0 .builder \\\u00a0 .master('yarn') \\\u00a0 .appName('spark-bigquery-demo') \\\u00a0 .getOrCreate()# Use the Cloud Storage bucket for temporary BigQuery export data used# by the connector.bucket = \"[bucket]\"spark.conf.set('temporaryGcsBucket', bucket)# Load data from BigQuery.words = spark.read.format('bigquery') \\\u00a0 .option('table', 'bigquery-public-data:samples.shakespeare') \\\u00a0 .load()words.createOrReplaceTempView('words')# Perform word count.word_count = spark.sql(\u00a0 \u00a0 'SELECT word, SUM(word_count) AS word_count FROM words GROUP BY word')word_count.show()word_count.printSchema()# Save the data to BigQueryword_count.write.format('bigquery') \\\u00a0 .option('table', 'wordcount_dataset.wordcount_output') \\\u00a0 .save()\n```\n- Run the code on your cluster **Use Dataproc to submit the PySpark code** : Instead of running the PySpark code manually from the cluster master instance, you can submit the PySpark file directly to your cluster (see the [Dataproc Quickstarts](/dataproc/docs/quickstarts) ). Here are the steps using the Google Cloud CLI:- Create`wordcount.py`locally in a text editor by copying the PySpark code from the [PySpark code listing](#python-code-listing) \n- Run the PySpark code by submitting the job to your cluster with the`gcloud dataproc jobs submit`command:```\ngcloud dataproc jobs submit pyspark wordcount.py \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--jars=gs://spark-lib/bigquery/spark-bigquery-latest.jar\n```\n- Use SSH to connect to the Dataproc cluster master node\n- Go to the [Dataproc Clusters](https://console.cloud.google.com/dataproc/clusters) page in the Google Cloud console, then click the name of your cluster\n- On the **Cluster details** page, select the VM Instances tab. Then, click`SSH`to the right of the name of the cluster master nodeA browser window opens at your home directory on the master node```\n Connected, host fingerprint: ssh-rsa 2048 ...\n ...\n user@clusterName-m:~$\n \n```- Create`wordcount.py`with the pre-installed`vi`,`vim`, or`nano`text editor, then paste in the PySpark code from the [PySpark code listing](#python-code-listing) ```\nnano wordcount.py\n```\n- Run wordcount with`spark-submit`to create the BigQuery`wordcount_output`table. The output listing displays 20 lines from the wordcount output.```\nspark-submit --jars gs://spark-lib/bigquery/spark-bigquery-latest.jar wordcount.py\n...\n+---------+----------+\n|  word|word_count|\n+---------+----------+\n|  XVII|   2|\n| spoil|  28|\n| Drink|   7|\n|forgetful|   5|\n| Cannot|  46|\n| cures|  10|\n| harder|  13|\n| tresses|   3|\n|  few|  62|\n| steel'd|   5|\n| tripping|   7|\n| travel|  35|\n| ransom|  55|\n|  hope|  366|\n|  By|  816|\n|  some|  1169|\n| those|  508|\n| still|  567|\n|  art|  893|\n| feign|  10|\n+---------+----------+\nonly showing top 20 rows\nroot\n |-- word: string (nullable = false)\n |-- word_count: long (nullable = true)\n```To preview the output table, open the [BigQuery](https://console.cloud.google.com/bigquery) page, select the`wordcount_output`table, and then click **Preview** .## For more information\n- [BigQuery Storage & Spark SQL - Python](https://github.com/tfayyaz/cloud-dataproc/blob/master/notebooks/python/1.2.%20BigQuery%20Storage%20%26%20Spark%20SQL%20-%20Python.ipynb) \n- [Creating a table definition file for an external data source](/bigquery/external-table-definition) \n- [Querying externally partitioned data](/bigquery/docs/hive-partitioned-queries-gcs) \n- [Spark job tuning tips](/dataproc/docs/support/spark-job-tuning)", "guide": "Dataproc"}