{"title": "Dataproc - Hive-BigQuery connector", "url": "https://cloud.google.com/dataproc/docs/concepts/connectors/hive-bigquery", "abstract": "# Dataproc - Hive-BigQuery connector\nThe open source [Hive-BigQuery connector](https://github.com/GoogleCloudDataproc/hive-bigquery-connector) lets your [Apache Hive](https://hive.apache.org/) workloads read and write data from and to [BigQuery](/bigquery) and [BigLake](/biglake) tables. You can store data in BigQuery storage or in open source data formats on Cloud Storage.\nUse the connector to work with Hive and BigQuery together or to migrate your data warehouse from Hive to BigQuery.\nThe Hive-BigQuery connector implements the [Hive Storage Handler API](https://cwiki.apache.org/confluence/display/Hive/StorageHandlers) to allow Hive workloads to integrate with BigQuery and BigLake tables. The Hive execution engine handles compute operations, such as aggregates and joins, and the connector manages interactions with data stored in BigQuery or in BigLake-connected Cloud Storage buckets.\nThe following diagram illustrates how Hive-BigQuery connector fits between the compute and data layers.\n", "content": "## Use cases\nHere are some of the ways the Hive-BigQuery connector can help you in common data-driven scenarios:\n- Data migration. You plan to move your Hive data warehouse to BigQuery, then incrementally translate your Hive queries into BigQuery SQL dialect. You expect the migration to take a significant amount of time due to the size of your data warehouse and the large number of connected applications, and you need to ensure continuity during the migration operations. Here's the workflow:- You move your data to BigQuery\n- Using the connector, you access and run your original Hive queries while you gradually translate the Hive queries to BigQuery ANSI-compliant SQL dialect.\n- After completing the migration and translation, you retire Hive.\n- Hive and BigQuery workflows. You plan to use Hive for some tasks, and BigQuery for workloads that benefit from its features, such as [BigQuery BI Engine](/bigquery/docs/bi-engine-intro) or [BigQuery ML](/bigquery/docs/bqml-introduction) . You use the connector to join Hive tables to your BigQuery tables.\n- Reliance on an open source software (OSS) stack. To avoid vendor lock-in, you use a full OSS stack for your data warehouse. Here's your data plan:- You migrate your data in its original OSS format, such as Avro, Parquet, or ORC, to Cloud Storage buckets using a BigLake connection.\n- You continue to use Hive to execute and process your Hive SQL dialect queries.\n- You use the connector as needed to connect to BigQuery to benefit from the following features:- [Metadata caching](/bigquery/docs/biglake-intro#metadata_caching_for_performance) for query performance\n- [Data loss prevention](/bigquery/docs/scan-with-dlp) \n- [Column-level access control](/bigquery/docs/column-level-security-intro) \n- [Dynamic data masking](/bigquery/docs/column-data-masking-intro) for security and governance at scale.## Features\nYou can use the Hive-BigQuery connector to work with your BigQuery data and accomplish the following tasks:\n- Run queries with MapReduce and Tez execution engines.\n- Create and delete BigQuery tables from Hive.\n- Join BigQuery and BigLake tables with Hive tables.\n- Perform fast reads from BigQuery tables using the [Storage Read API](/bigquery/docs/reference/storage) streams and the [Apache Arrow](https://arrow.apache.org/) format\n- Write data to BigQuery using the following methods:- Direct writes using the BigQuery [Storage Write API in pending mode](/bigquery/docs/write-api-batch) . Use this method for workloads that require low write latency, such as near-real-time dashboards with short refresh time windows.\n- Indirect writes by staging temporary Avro files to Cloud Storage, and then loading the files into a destination table using the [Load Job API](/bigquery/docs/batch-loading-data) . This method is less expensive than the direct method, since BigQuery load jobs don't accrue charges. Since this method is slower, and finds its best use in workloads that aren't time critical\n- Access BigQuery [time-partitioned](/bigquery/docs/partitioned-tables) and [clustered](/bigquery/docs/clustered-tables) tables. The following example defines the relation between a Hive table and a table that is partitioned and clustered in BigQuery.```\nCREATE TABLE my_hive_table (int_val BIGINT, text STRING, ts TIMESTAMP)STORED BY 'com.google.cloud.hive.bigquery.connector.BigQueryStorageHandler'TBLPROPERTIES ('bq.table'='myproject.mydataset.mytable','bq.time.partition.field'='ts','bq.time.partition.type'='MONTH','bq.clustered.fields'='int_val,text');\n```\n- Prune columns to avoid retrieving unnecessary columns from the data layer.\n- Use predicate pushdowns to pre-filter data rows at the BigQuery storage layer. This technique can significantly improve overall query performance by reducing the amount of data traversing the network.\n- Automatically convert Hive data types to BigQuery data types.\n- Read BigQuery [views](/bigquery/docs/views-intro) and [table snapshots](/bigquery/docs/table-snapshots-intro) .\n- Integrate with Spark SQL.\n- Integrate with Apache Pig and HCatalog.## Get started\nSee the instructions to [install and configure the Hive-BigQuery connector on a Hive cluster](https://github.com/GoogleCloudDataproc/hive-bigquery-connector/blob/main/README.md) .", "guide": "Dataproc"}