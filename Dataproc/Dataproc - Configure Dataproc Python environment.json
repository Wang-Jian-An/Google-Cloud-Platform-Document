{"title": "Dataproc - Configure Dataproc Python environment", "url": "https://cloud.google.com/dataproc/docs/tutorials/python-configuration", "abstract": "# Dataproc - Configure Dataproc Python environment\nPySpark jobs on Dataproc are run by a Python interpreter on the cluster. Job code must be compatible at runtime with the Python interpreter's version and dependencies.\n", "content": "## Checking interpreter version and modules\nThe following `check_python_env.py` sample program checks the Linux user running the job, the Python interpreter, and available modules.\n```\nimport getpassimport sysimport impprint('This job is running as \"{}\".'.format(getpass.getuser()))print(sys.executable, sys.version_info)for package in sys.argv[1:]:\u00a0 print(imp.find_module(package))\n```\n**Run the program**\n```\nREGION=region\ngcloud dataproc jobs submit pyspark check_python_env.py \\\n\u00a0\u00a0\u00a0\u00a0--cluster=my-cluster \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0-- pandas scipy\n```\n**Sample output**\n```\nThis job is running as \"root\".\n('/usr/bin/python', sys.version_info(major=2, minor=7, micro=13, releaselevel='final', serial=0))\n(None, '/usr/local/lib/python2.7/dist-packages/pandas', ('', '', 5))\n(None, '/usr/local/lib/python2.7/dist-packages/scipy', ('', '', 5))\n```\n## Dataproc image Python environments\n### Dataproc image version 1.5\nMiniconda3 is installed on Dataproc 1.5 clusters. The default interpreter is Python 3.7 which is located on the VM instance at `/opt/conda/miniconda3/bin/python3.7` , respectively. Python 2.7 is also available at `/usr/bin/python2.7` .\nYou can install Conda and PIP packages in the `base` environment or set up your own Conda environment on the cluster using [Conda-related cluster properties](#using_conda-related_cluster_properties) .\nTo install Anaconda3 instead of Miniconda3, choose the [Anaconda optional component](/dataproc/docs/concepts/components/anaconda) , and install Conda and PIP packages in the `base` environment or set up your own Conda environment on the cluster using [Conda-related cluster properties](#using_conda-related_cluster_properties) .\n**Example**\n```\nREGION=region\ngcloud beta dataproc clusters create my-cluster \\\n\u00a0\u00a0\u00a0\u00a0--image-version=1.5 \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=ANACONDA \\\n\u00a0\u00a0\u00a0\u00a0--properties=^#^dataproc:conda.packages='pytorch==1.0.1,visions==0.7.1'#dataproc:pip.packages='tokenizers==0.10.1,datasets==1.5.0'\n```\nWhen you install the Anaconda3 optional component, Miniconda3 is removed from the cluster and `/opt/conda/anaconda3/bin/python3.6` becomes the default Python interpreter for PySpark jobs. You cannot change the Python interpreter version of the optional component.\nTo use Python 2.7 as the default interpreter on 1.5 clusters, do not use the [Anaconda optional component](/dataproc/docs/concepts/components/anaconda) when creating the cluster. Instead, use the [Conda initialization action](https://github.com/GoogleCloudDataproc/initialization-actions/tree/master/python) to install Miniconda2 and use [Conda-related cluster properties](#using_conda-related_cluster_properties) to install Conda and PIP packages in the `base` environment or set up your own Conda environment on the cluster.\n**Example**\n```\nREGION=region\ngcloud dataproc clusters create my-cluster \\\n\u00a0\u00a0\u00a0\u00a0--image-version=1.5 \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--metadata='MINICONDA_VARIANT=2' \\\n\u00a0\u00a0\u00a0\u00a0--metadata='MINICONDA_VERSION=latest' \\\n\u00a0\u00a0\u00a0\u00a0--initialization-actions=gs://goog-dataproc-initialization-actions-${REGION}/conda/bootstrap-conda.sh \\\n\u00a0\u00a0\u00a0\u00a0--properties=^#^dataproc:conda.packages='pytorch==1.0.1,visions==0.7.1'#dataproc:pip.packages='tokenizers==0.10.1,datasets==1.5.0'\n```\n### Dataproc image version 2.0+\nMiniconda3 is installed on Dataproc 2.0+ clusters. The default Python3 interpreter is located on the VM instance under `/opt/conda/miniconda3/bin/` . The following pages list the Python version included in Dataproc image versions:\n- [2.0](/dataproc/docs/concepts/versioning/dataproc-release-2.0) \n- [2.1](/dataproc/docs/concepts/versioning/dataproc-release-2.1) \n- [2.2](/dataproc/docs/concepts/versioning/dataproc-release-2.2) \nThe non-default Python interpreter from the OS is available under `/usr/bin/` .\nYou can install Conda and PIP packages in the `base` environment or set up your own Conda environment on the cluster using [Conda-related cluster properties](#using_conda-related_cluster_properties) .\n**Example**\n```\nREGION=region\ngcloud dataproc clusters create my-cluster \\\n\u00a0\u00a0\u00a0\u00a0--image-version=2.0 \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--properties=^#^dataproc:conda.packages='pytorch==1.7.1,coverage==5.5'#dataproc:pip.packages='tokenizers==0.10.1,datasets==1.5.0'\n```\n**Note:** Anaconda is not available for Dataproc 2.0 clusters.\n## Choosing a Python interpreter for a job\nIf multiple Python interpreters are installed on your cluster, the system runs `/etc/profile.d/effective-python.sh` , which exports the `PYSPARK_PYTHON` environment variable to choose the default Python interpreter for your PySpark jobs. If you need a non-default Python interpreter for a PySpark job, when you submit the job to your cluster, set the `spark.pyspark.python` and `spark.pyspark.driver.python` properties to the required Python version number (for example, \"python2.7\" or \"python3.6\").\n**Example**\n```\nREGION=region\ngcloud dataproc jobs submit pyspark check_python_env.py \\\n\u00a0\u00a0\u00a0\u00a0--cluster=my-cluster \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--properties=\"spark.pyspark.python=python2.7,spark.pyspark.driver.python=python2.7\"\n```\n## Python with sudo\nIf you SSH into a cluster node that has Miniconda or Anaconda installed, when you run `sudo python --version` , the displayed Python version can be different from the version displayed by `python --version` . This version difference can occur because `sudo` uses the default system Python `/usr/bin/python` , and does not execute `/etc/profile.d/effective-python.sh` to initialize the Python environment. For a consistent experience when using `sudo` , locate the Python path set in `/etc/profile.d/effective-python.sh` , then run the `env` command to set the `PATH` to this Python path. Here is a 1.5 cluster example:\n```\nsudo env PATH=/opt/conda/default/bin:${PATH} python --version\n```\n## Using Conda-related cluster properties\nYou can customize the Conda environment during cluster creation using Conda-related [cluster properties](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) .\nThere are two mutually exclusive ways to customize the Conda environment when you create a Dataproc cluster:\n- Use the `dataproc:conda.env.config.uri` cluster property to create and activate a new Conda environment on the cluster. **or** \n- Use the `dataproc:conda.packages` and `dataproc:pip.packages` cluster properties to add Conda and PIP packages, respectively, to the Conda `base` environment on the cluster.\n**Warning:** Cluster creation can time out depending on the time required to   install the new packages or environment specified by the properties.   The timeout is set at 10 minutes.\n[](None)\n### Conda-related cluster properties\n- **dataproc:conda.env.config.uri:** The absolute path to a Conda environment YAML config file located in Cloud Storage. This file will be used to create and activate a new Conda environment on the cluster. **Note:** The `dataproc:conda.env.config.uri` cluster property cannot be used with the [dataproc:conda.packages](#conda-package-property) or [dataproc:pip.packages](#pip-package-property) cluster properties. **Example:** - Get or create a Conda `environment.yaml` config file. You can [manually create thefile](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#create-env-file-manually) , use an existing file, or [export an existing Conda environment](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#sharing-an-environment) ) into an `environment.yaml` file as shown below.```\nconda env export --name=env-name > environment.yaml\n```\n- Copy the config file to your Cloud Storage bucket.```\ngsutil cp environment.yaml gs://bucket-name/environment.yaml\n```\n- Create the cluster and point to your environment config file in Cloud Storage.```\nREGION=region\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--properties='dataproc:conda.env.config.uri=gs://bucket-name/environment.yaml' \\\n\u00a0\u00a0\u00a0\u00a0... other flags ...\n``` [](None) \n- **dataproc:conda.packages:** A list of Conda packages with specific versions to be installed in the base environment, formatted as `pkg1==v1,pkg2==v2...` . If Conda fails to resolve conflicts with existing packages in the base environment, the conflicting packages will not be installed.Notes:- The `dataproc:conda.packages` and [dataproc:pip.packages](#pip-package-property) cluster properties cannot be used with the [dataproc:conda.env.config.uri](#conda-config-property) cluster property.\n- When specifying multiple packages (separated by a comma), you must specify an alternate delimiter character (see cluster property [Formatting](/dataproc/docs/concepts/configuring-clusters/cluster-properties#formatting) ). The following example specifies \"#\" as the delimiter character to pass multiple, comma-separated, package names to the `dataproc:conda.packages` property.\n **Example:** \n```\nREGION=region\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--properties='^#^dataproc:conda.packages=pytorch==1.7.1,coverage==5.5' \\\n\u00a0\u00a0\u00a0\u00a0... other flags ...\n```\n[](None)\n- **dataproc:pip.packages:** A list of pip packages with specific versions to be installed in the base environment, formatted as `pkg1==v1,pkg2==v2...` . Pip will upgrade existing dependencies only if required. Conflicts can cause the environment to be inconsistent.Notes:- The `dataproc:pip.packages` and [dataproc:conda.packages](#conda-package-property) cluster properties cannot be used with the [dataproc:conda.env.config.uri](#conda-config-property) cluster property.\n- When specifying multiple packages (separated by a comma), you must specify an alternate delimiter character (see cluster property [Formatting](/dataproc/docs/concepts/configuring-clusters/cluster-properties#formatting) ). The following example specifies \"#\" as the delimiter character to pass multiple, comma-separated, package names to the `dataproc:pip.packages` property.\n **Example:** \n```\nREGION=region\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--properties='^#^dataproc:pip.packages=tokenizers==0.10.1,datasets==1.4.1' \\\n\u00a0\u00a0\u00a0\u00a0... other flags ...\n```\n- You can use both `dataproc:conda.packages` and `dataproc:pip.packages` when creating a cluster. **Example:** \n```\nREGION=region\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--image-version=1.5 \\\n\u00a0\u00a0\u00a0\u00a0--metadata='MINICONDA_VARIANT=2' \\\n\u00a0\u00a0\u00a0\u00a0--metadata='MINICONDA_VERSION=latest' \\\n\u00a0\u00a0\u00a0\u00a0--properties=^#^dataproc:conda.packages='pytorch==1.7.1,coverage==5.5'#dataproc:pip.packages='tokenizers==0.10.1,datasets==1.4.1' \\\n\u00a0\u00a0\u00a0\u00a0... other flags ...\n```", "guide": "Dataproc"}