{"title": "Dataproc - Customize your Spark job runtime environment with Docker on YARN", "url": "https://cloud.google.com/dataproc/docs/guides/dataproc-docker-yarn", "abstract": "# Dataproc - Customize your Spark job runtime environment with Docker on YARN\nThe Dataproc **Docker on YARN** feature allows you to create and use a Docker image to customize your Spark job runtime environment. The image can include customizations to Java, Python, and R dependencies, and to your job jar.\n#", "content": "## Limitations\nFeature availability or support is **not available** with:\n- Dataproc image versions prior to 2.0.49 (not available in 1.5 images)\n- MapReduce jobs (only supported for Spark jobs )\n- Spark client mode (only supported with Spark cluster mode)\n- [Kerberos clusters](/dataproc/docs/concepts/configuring-clusters/security#create_the_cluster) : cluster creation fails if you [create a cluster with Docker on YARN](#create_a_cluster) and Kerberos enabled.\n- Customizations of JDK, Hadoop and Spark: the host JDK, Hadoop, and Spark are used, not your customizations.## Create a Docker image\nThe first step to customize your Spark environment is [building a Docker image](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/) .\n### Dockerfile\nYou can use the following Dockerfile as an example, making changes and additions to meet you needs.\n```\nFROM debian:10-slim# Suppress interactive prompts.ENV DEBIAN_FRONTEND=noninteractive# Required: Install utilities required by Spark scripts.RUN apt update && apt install -y procps tini# Optional: Add extra jars.ENV SPARK_EXTRA_JARS_DIR=/opt/spark/jars/ENV SPARK_EXTRA_CLASSPATH='/opt/spark/jars/*'RUN mkdir -p \"${SPARK_EXTRA_JARS_DIR}\"COPY *.jar \"${SPARK_EXTRA_JARS_DIR}\"# Optional: Install and configure Miniconda3.ENV CONDA_HOME=/opt/miniconda3ENV PYSPARK_PYTHON=${CONDA_HOME}/bin/pythonENV PYSPARK_DRIVER_PYTHON=${CONDA_HOME}/bin/pythonENV PATH=${CONDA_HOME}/bin:${PATH}COPY Miniconda3-py39_4.10.3-Linux-x86_64.sh .RUN bash Miniconda3-py39_4.10.3-Linux-x86_64.sh -b -p /opt/miniconda3 \\\u00a0 && ${CONDA_HOME}/bin/conda config --system --set always_yes True \\\u00a0 && ${CONDA_HOME}/bin/conda config --system --set auto_update_conda False \\\u00a0 && ${CONDA_HOME}/bin/conda config --system --prepend channels conda-forge \\\u00a0 && ${CONDA_HOME}/bin/conda config --system --set channel_priority strict# Optional: Install Conda packages.\n## The following packages are installed in the default image. It is strongly# recommended to include all of them.\n## Use mamba to install packages quickly.RUN ${CONDA_HOME}/bin/conda install mamba -n base -c conda-forge \\\u00a0 \u00a0 && ${CONDA_HOME}/bin/mamba install \\\u00a0 \u00a0 \u00a0 conda \\\u00a0 \u00a0 \u00a0 cython \\\u00a0 \u00a0 \u00a0 fastavro \\\u00a0 \u00a0 \u00a0 fastparquet \\\u00a0 \u00a0 \u00a0 gcsfs \\\u00a0 \u00a0 \u00a0 google-cloud-bigquery-storage \\\u00a0 \u00a0 \u00a0 google-cloud-bigquery[pandas] \\\u00a0 \u00a0 \u00a0 google-cloud-bigtable \\\u00a0 \u00a0 \u00a0 google-cloud-container \\\u00a0 \u00a0 \u00a0 google-cloud-datacatalog \\\u00a0 \u00a0 \u00a0 google-cloud-dataproc \\\u00a0 \u00a0 \u00a0 google-cloud-datastore \\\u00a0 \u00a0 \u00a0 google-cloud-language \\\u00a0 \u00a0 \u00a0 google-cloud-logging \\\u00a0 \u00a0 \u00a0 google-cloud-monitoring \\\u00a0 \u00a0 \u00a0 google-cloud-pubsub \\\u00a0 \u00a0 \u00a0 google-cloud-redis \\\u00a0 \u00a0 \u00a0 google-cloud-spanner \\\u00a0 \u00a0 \u00a0 google-cloud-speech \\\u00a0 \u00a0 \u00a0 google-cloud-storage \\\u00a0 \u00a0 \u00a0 google-cloud-texttospeech \\\u00a0 \u00a0 \u00a0 google-cloud-translate \\\u00a0 \u00a0 \u00a0 google-cloud-vision \\\u00a0 \u00a0 \u00a0 koalas \\\u00a0 \u00a0 \u00a0 matplotlib \\\u00a0 \u00a0 \u00a0 nltk \\\u00a0 \u00a0 \u00a0 numba \\\u00a0 \u00a0 \u00a0 numpy \\\u00a0 \u00a0 \u00a0 openblas \\\u00a0 \u00a0 \u00a0 orc \\\u00a0 \u00a0 \u00a0 pandas \\\u00a0 \u00a0 \u00a0 pyarrow \\\u00a0 \u00a0 \u00a0 pysal \\\u00a0 \u00a0 \u00a0 pytables \\\u00a0 \u00a0 \u00a0 python \\\u00a0 \u00a0 \u00a0 regex \\\u00a0 \u00a0 \u00a0 requests \\\u00a0 \u00a0 \u00a0 rtree \\\u00a0 \u00a0 \u00a0 scikit-image \\\u00a0 \u00a0 \u00a0 scikit-learn \\\u00a0 \u00a0 \u00a0 scipy \\\u00a0 \u00a0 \u00a0 seaborn \\\u00a0 \u00a0 \u00a0 sqlalchemy \\\u00a0 \u00a0 \u00a0 sympy \\\u00a0 \u00a0 \u00a0 virtualenv# Optional: Add extra Python modules.ENV PYTHONPATH=/opt/python/packagesRUN mkdir -p \"${PYTHONPATH}\"COPY test_util.py \"${PYTHONPATH}\"# Required: Create the 'yarn_docker_user' group/user.# The GID and UID must be 1099. Home directory is required.RUN groupadd -g 1099 yarn_docker_userRUN useradd -u 1099 -g 1099 -d /home/yarn_docker_user -m yarn_docker_userUSER yarn_docker_user\n```\n### Build and push the image\nThe following is commands for building and pushing the example Docker image, you can make changes according to your customizations.\n```\n# Increase the version number when there is a change to avoid referencing# a cached older image. Avoid reusing the version number, including the default# `latest` version.IMAGE=gcr.io/my-project/my-image:1.0.1# Download the BigQuery connector.gsutil cp \\\u00a0 gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar .# Download the Miniconda3 installer.wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.10.3-Linux-x86_64.sh# Python module example:cat >test_util.py <<EOFdef hello(name):\u00a0 print(\"hello {}\".format(name))def read_lines(path):\u00a0 with open(path) as f:\u00a0 \u00a0 return f.readlines()EOF# Build and push the image.docker build -t \"${IMAGE}\" .docker push \"${IMAGE}\"\n```\n## Create a Dataproc cluster\nAfter [creating a Docker image](#create_a_docker_image) that customizes your Spark environment, create a Dataproc cluster that will use your Docker image when running Spark jobs.\n```\ngcloud dataproc clusters create CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--image-version=DP_IMAGE \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=DOCKER \\\n\u00a0\u00a0\u00a0\u00a0--properties=dataproc:yarn.docker.enable=true,dataproc:yarn.docker.image=DOCKER_IMAGE \\\n\u00a0\u00a0\u00a0\u00a0other flags\n```\nReplace the following;- : The cluster name.\n- : The cluster region.\n- : Dataproc image version must be`2.0.49`or later (`--image-version=2.0`will use a qualified minor version later than`2.0.49`).\n- `--optional-components=DOCKER`: Enables the [Docker component](/dataproc/docs/concepts/components/docker) on the cluster.\n- `--properties`flag:- `dataproc:yarn.docker.enable=true`: Required property to enable the Dataproc Docker on YARN feature.\n- `dataproc:yarn.docker.image`: Optional property that you can add to specify your [DOCKER_IMAGE](#create_a_docker_image) using the following Container Registry image naming format:`{hostname}/{project-id}/{image}:{tag}`.Example:```\ndataproc:yarn.docker.image=gcr.io/project-id/image:1.0.1\n``` **Requirement:** You must host your Docker image on [Container Registry](/container-registry) or [Artifact Registry](/artifact-registry) . (Dataproc cannot fetch containers from other registries). **Recommendation:** Add this property when you create your cluster to cache your Docker image and avoid YARN timeouts later when you submit a job that uses the image.\nWhen `dataproc:yarn.docker.enable` is set to `true` , Dataproc updates Hadoop and Spark configurations to enable the Docker on YARN feature in the cluster. For example, `spark.submit.deployMode` is set to `cluster` , and `spark.yarn.appMasterEnv.YARN_CONTAINER_RUNTIME_DOCKER_MOUNTS` and `spark.executorEnv.YARN_CONTAINER_RUNTIME_DOCKER_MOUNTS` are set to mount directories from the host into the container.\n## Submit a Spark job to the cluster\nAfter [creating a Dataproc cluster](#create_a_cluster) , submit a Spark job to the cluster that uses your Docker image. The example in this section submits a PySpark job to the cluster.\nSet job properties:\n```\n# Set the Docker image URI.IMAGE=(e.g., gcr.io/my-project/my-image:1.0.1)# Required: Use `#` as the delimiter for properties to avoid conflicts.JOB_PROPERTIES='^#^'# Required: Set Spark properties with the Docker image.JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.yarn.appMasterEnv.YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=${IMAGE}\"JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.executorEnv.YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=${IMAGE}\"# Optional: Add custom jars to Spark classpath. Don't set these properties if# there are no customizations.JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.driver.extraClassPath=/opt/spark/jars/*\"JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.executor.extraClassPath=/opt/spark/jars/*\"# Optional: Set custom PySpark Python path only if there are customizations.JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.pyspark.python=/opt/miniconda3/bin/python\"JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.pyspark.driver.python=/opt/miniconda3/bin/python\"# Optional: Set custom Python module path only if there are customizations.# Since the `PYTHONPATH` environment variable defined in the Dockerfile is# overridden by Spark, it must be set as a job property.JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.yarn.appMasterEnv.PYTHONPATH=/opt/python/packages\"JOB_PROPERTIES=\"${JOB_PROPERTIES}#spark.executorEnv.PYTHONPATH=/opt/python/packages\"\n```\nNotes:\n- See [Launching Applications Using Docker Containers](https://hadoop.apache.org/docs/r3.2.3/hadoop-yarn/hadoop-yarn-site/DockerContainers.html) information on related properties.\nSubmit the job to the cluster.\n```\ngcloud dataproc jobs submit pyspark PYFILE \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--properties=${JOB_PROPERTIES}\n```\nReplace the following;- : The file path to your PySpark job file. It can be a local file path or the URI of the file in Cloud Storage (`gs://` `` `/` ``).\n- : The cluster name.\n- : The cluster region.", "guide": "Dataproc"}