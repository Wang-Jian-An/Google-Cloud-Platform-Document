{"title": "Dataproc - \u5728 Dataproc \u4e0a\u5c07 Apache Spark \u8207 HBase \u642d\u914d\u4f7f\u7528", "url": "https://cloud.google.com/dataproc/docs/tutorials/spark-hbase?hl=zh-cn", "abstract": "# Dataproc - \u5728 Dataproc \u4e0a\u5c07 Apache Spark \u8207 HBase \u642d\u914d\u4f7f\u7528\n", "content": "## \u76ee\u6a19\u672c\u6559\u7a0b\u5c07\u4ecb\u7d39\u5982\u4f55\u57f7\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a- \u5275\u5efa Dataproc \u96c6\u7fa3\uff0c\u4e26\u5728\u96c6\u7fa3\u4e0a\u5b89\u88dd Apache HBase \u548c Apache ZooKeeper\n- \u4f7f\u7528\u5728 Dataproc \u96c6\u7fa3\u7684\u4e3b\u7bc0\u9ede\u4e0a\u904b\u884c\u7684 HBase shell \u5275\u5efa HBase \u8868\n- \u4f7f\u7528 Cloud Shell \u5c07 Java \u6216 PySpark Spark \u4f5c\u696d\u63d0\u4ea4\u5230 Dataproc \u670d\u52d9\uff0c\u8a72\u670d\u52d9\u6703\u5c07\u6578\u64da\u5beb\u5165 HBase \u8868\u4e26\u5f9e\u4e2d\u8b80\u53d6\u6578\u64da\n## \u8cbb\u7528\nTitles in dynamic includes are not used anywhere, and we should avoid paying to translate them\u5728\u672c\u6587\u6a94\u4e2d\uff0c\u60a8\u5c07\u4f7f\u7528 Google Cloud \u7684\u4ee5\u4e0b\u6536\u8cbb\u7d44\u4ef6\uff1a- [Dataproc](https://cloud.google.com/dataproc/pricing?hl=zh-cn) \n- [Compute Engine](https://cloud.google.com/compute/pricing?hl=zh-cn) \n\u60a8\u53ef\u4f7f\u7528 [\u50f9\u683c\u8a08\u7b97\u5668](https://cloud.google.com/products/calculator?hl=zh-cn) \u6839\u64da\u60a8\u7684\u9810\u8a08\u4f7f\u7528\u60c5\u6cc1\u4f86\u4f30\u7b97\u8cbb\u7528\u3002 \n## \u6e96\u5099\u5de5\u4f5c\u5982\u679c\u60a8\u5c1a\u672a\u5275\u5efa Google Cloud Platform \u9805\u76ee\uff0c\u8acb\u9996\u5148\u5275\u5efa\u8a72\u9805\u76ee\u3002\n- Titles in dynamic includes are not used anywhere, and we should avoid paying to translate them## \u5275\u5efa Dataproc \u96c6\u7fa3\n- \u5728 [Cloud Shell](https://cloud.google.com/shell/docs?hl=zh-cn) \u6703\u8a71\u7d42\u7aef\u4e2d\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u4ee5\u57f7\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a- \u5b89\u88dd [HBase](https://cloud.google.com/dataproc/docs/concepts/components/hbase?hl=zh-cn) \u548c [ZooKeeper](https://cloud.google.com/dataproc/docs/concepts/components/zookeeper?hl=zh-cn) \u7d44\u4ef6\n- \u9810\u914d\u4e09\u500b\u5de5\u4f5c\u5668\u7bc0\u9ede\uff08\u5efa\u8b70\u904b\u884c\u4e09\u5230\u4e94\u500b\u5de5\u4f5c\u5668\uff09\u904b\u884c\u672c\u6559\u7a0b\u4e2d\u7684\u4ee3\u78bc\n- \u5553\u7528 [\u7d44\u4ef6\u7db2\u95dc](https://cloud.google.com/dataproc/docs/concepts/accessing/dataproc-gateways?hl=zh-cn) \n- \u4f7f\u7528\u6620\u50cf\u7248\u672c 2.0\n- \u4f7f\u7528`--properties`\u6a19\u8a8c\u5c07 HBase \u914d\u7f6e\u548c HBase \u5eab\u6dfb\u52a0\u5230 Spark \u9a45\u52d5\u7a0b\u5e8f\u548c\u57f7\u884c\u7a0b\u5e8f\u985e\u8def\u5f91\u3002\n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=HBASE,ZOOKEEPER \\\n\u00a0\u00a0\u00a0\u00a0--num-workers=3 \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway \\\n\u00a0\u00a0\u00a0\u00a0--image-version=2.0 \\\n\u00a0\u00a0\u00a0\u00a0--properties='spark.driver.extraClassPath=/etc/hbase/conf:/usr/lib/hbase/*,spark.executor.extraClassPath=/etc/hbase/conf:/usr/lib/hbase/*'\n```\n### \u9a57\u8b49\u9023\u63a5\u5668\u5b89\u88dd\n- \u5f9e Cloud Console \u6216 Cloud Shell \u6703\u8a71\u7d42\u7aef [\u901a\u904e SSH \u9023\u63a5\u5230 Dataproc \u96c6\u7fa3\u4e3b\u7bc0\u9ede](https://cloud.google.com/dataproc/docs/concepts/accessing/ssh?hl=zh-cn) \u3002\n- \u9a57\u8b49\u4e3b\u7bc0\u9ede\u4e0a\u662f\u5426\u5b89\u88dd\u4e86 [Apache HBase Spark \u9023\u63a5\u5668](https://github.com/apache/hbase-connectors/tree/master/spark) \uff1a```\nls -l /usr/lib/spark/jars | grep hbase-spark\n```\u793a\u4f8b\u8f38\u51fa\uff1a```\n-rw-r--r-- 1 root root size date time hbase-spark-connector.version.jar\n```\n- \u5c07 SSH \u6703\u8a71\u7d42\u7aef\u4fdd\u6301\u6253\u958b\u72c0\u614b\uff0c\u4ee5\u4fbf\uff1a- [\u5275\u5efa HBase \u8868](#create_an_hbase_table) \n- \uff08Java \u7528\u6236\uff09\u5728\u96c6\u7fa3\u7684\u4e3b\u7bc0\u9ede\u4e0a\u904b\u884c [\u547d\u4ee4](#dataproc_hbase_tutorial-java) \uff0c\u4ee5\u78ba\u5b9a\u96c6\u7fa3\u4e0a\u5b89\u88dd\u7684\u7d44\u4ef6\u7684\u7248\u672c\n- [\u904b\u884c\u4ee3\u78bc](#run_the_code) \u5f8c\uff0c [\u6383\u63cf\u60a8\u7684 HBase \u8868](#scan_the_hbase_table) ## \u5275\u5efa HBase \u8868\u5728\u60a8\u5728\u4e0a\u4e00\u6b65\u4e2d\u6253\u958b\u7684\u4e3b\u7bc0\u9ede SSH \u6703\u8a71\u7d42\u7aef\u4e2d\u904b\u884c\u672c\u90e8\u5206\u5217\u51fa\u7684\u547d\u4ee4\u3002- \u6253\u958b HBase shell\uff1a```\nhbase shell\n```\n- \u5275\u5efa\u4e00\u500b\u5177\u6709\u201c\u5217\u65cf\u201d\u7684 HBase \u8868\uff1a```\ncreate 'my_table','cf'\n```- \u5982\u9700\u78ba\u8a8d\u5275\u5efa\u8868\uff0c\u8acb\u5728 Cloud Console \u4e2d\u9ede\u64ca [Cloud Console \u7d44\u4ef6\u7db2\u95dc\u93c8\u63a5](https://cloud.google.com/dataproc/docs/concepts/accessing/dataproc-gateways?hl=zh-cn#viewing_and_accessing_component_gateway_urls) \u4e2d\u7684 **HBase** \u6253\u958b Apache HBase \u754c\u9762\u3002`my-table`\u5217\u5728 **\u9996\u9801** \u7684 **\u8868** \u90e8\u5206\u3002## \u67e5\u770b Spark \u4ee3\u78bc [  spark-hbase/src/main/java/hbase/SparkHBaseMain.java ](https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/HEAD/spark-hbase/src/main/java/hbase/SparkHBaseMain.java) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/HEAD/spark-hbase/src/main/java/hbase/SparkHBaseMain.java) \n```\npackage hbase;import org.apache.hadoop.hbase.spark.datasources.HBaseTableCatalog;import org.apache.spark.sql.Dataset;import org.apache.spark.sql.Row;import org.apache.spark.sql.SparkSession;import java.io.Serializable;import java.util.Arrays;import java.util.HashMap;import java.util.Map;public class SparkHBaseMain {\u00a0 \u00a0 public static class SampleData implements Serializable {\u00a0 \u00a0 \u00a0 \u00a0 private String key;\u00a0 \u00a0 \u00a0 \u00a0 private String name;\u00a0 \u00a0 \u00a0 \u00a0 public SampleData(String key, String name) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.key = key;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.name = name;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 public SampleData() {\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 public String getName() {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return name;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 public void setName(String name) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.name = name;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 public String getKey() {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return key;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 public void setKey(String key) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 this.key = key;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 \u00a0 public static void main(String[] args) {\u00a0 \u00a0 \u00a0 \u00a0 // Init SparkSession\u00a0 \u00a0 \u00a0 \u00a0 SparkSession spark = SparkSession\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .builder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .master(\"yarn\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .appName(\"spark-hbase-tutorial\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .getOrCreate();\u00a0 \u00a0 \u00a0 \u00a0 // Data Schema\u00a0 \u00a0 \u00a0 \u00a0 String catalog = \"{\"+\"\\\"table\\\":{\\\"namespace\\\":\\\"default\\\", \\\"name\\\":\\\"my_table\\\"},\" +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\\"rowkey\\\":\\\"key\\\",\" +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\\"columns\\\":{\" +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\\"key\\\":{\\\"cf\\\":\\\"rowkey\\\", \\\"col\\\":\\\"key\\\", \\\"type\\\":\\\"string\\\"},\" +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\\"name\\\":{\\\"cf\\\":\\\"cf\\\", \\\"col\\\":\\\"name\\\", \\\"type\\\":\\\"string\\\"}\" +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"}\" +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"}\";\u00a0 \u00a0 \u00a0 \u00a0 Map<String, String> optionsMap = new HashMap<String, String>();\u00a0 \u00a0 \u00a0 \u00a0 optionsMap.put(HBaseTableCatalog.tableCatalog(), catalog);\u00a0 \u00a0 \u00a0 \u00a0 Dataset<Row> ds= spark.createDataFrame(Arrays.asList(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new SampleData(\"key1\", \"foo\"),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 new SampleData(\"key2\", \"bar\")), SampleData.class);\u00a0 \u00a0 \u00a0 \u00a0 // Write to HBase\u00a0 \u00a0 \u00a0 \u00a0 ds.write()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .format(\"org.apache.hadoop.hbase.spark\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .options(optionsMap)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .option(\"hbase.spark.use.hbasecontext\", \"false\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .mode(\"overwrite\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .save();\u00a0 \u00a0 \u00a0 \u00a0 // Read from HBase\u00a0 \u00a0 \u00a0 \u00a0 Dataset dataset = spark.read()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .format(\"org.apache.hadoop.hbase.spark\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .options(optionsMap)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .option(\"hbase.spark.use.hbasecontext\", \"false\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .load();\u00a0 \u00a0 \u00a0 \u00a0 dataset.show();\u00a0 \u00a0 }}\n``` [  spark-hbase/scripts/pyspark-hbase.py ](https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/HEAD/spark-hbase/scripts/pyspark-hbase.py) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/HEAD/spark-hbase/scripts/pyspark-hbase.py) \n```\nfrom pyspark.sql import SparkSession# Initialize Spark Sessionspark = SparkSession \\\u00a0 .builder \\\u00a0 .master('yarn') \\\u00a0 .appName('spark-hbase-tutorial') \\\u00a0 .getOrCreate()data_source_format = ''# Create some test datadf = spark.createDataFrame(\u00a0 \u00a0 [\u00a0 \u00a0 \u00a0 \u00a0 (\"key1\", \"foo\"),\u00a0 \u00a0 \u00a0 \u00a0 (\"key2\", \"bar\"),\u00a0 \u00a0 ],\u00a0 \u00a0 [\"key\", \"name\"])# Define the schema for catalogcatalog = ''.join(\"\"\"{\u00a0 \u00a0 \"table\":{\"namespace\":\"default\", \"name\":\"my_table\"},\u00a0 \u00a0 \"rowkey\":\"key\",\u00a0 \u00a0 \"columns\":{\u00a0 \u00a0 \u00a0 \u00a0 \"key\":{\"cf\":\"rowkey\", \"col\":\"key\", \"type\":\"string\"},\u00a0 \u00a0 \u00a0 \u00a0 \"name\":{\"cf\":\"cf\", \"col\":\"name\", \"type\":\"string\"}\u00a0 \u00a0 }}\"\"\".split())# Write to HBasedf.write.format('org.apache.hadoop.hbase.spark').options(catalog=catalog).option(\"hbase.spark.use.hbasecontext\", \"false\").mode(\"overwrite\").save()# Read from HBaseresult = spark.read.format('org.apache.hadoop.hbase.spark').options(catalog=catalog).option(\"hbase.spark.use.hbasecontext\", \"false\").load()result.show()\n```\n## \u904b\u884c\u4ee3\u78bc\n- \u6253\u958b [Cloud Shell](https://console.cloud.google.com/?cloudshell=true&hl=zh-cn) \u6703\u8a71\u7d42\u7aef\u3002 **\u6ce8\u610f** \uff1a\u5728 Cloud Shell \u6703\u8a71\u7d42\u7aef\u4e2d\u904b\u884c\u672c\u90e8\u5206\u5217\u51fa\u7684\u547d\u4ee4\u3002Cloud Shell \u9810\u5b89\u88dd\u4e86\u672c\u6559\u7a0b\u6240\u9700\u7684\u5de5\u5177\uff0c\u5305\u62ec [gcloud CLI](https://cloud.google.com/sdk/gcloud?hl=zh-cn) \u3001 [git](https://git-scm.com/) \u3001 [Apache Maven](https://maven.apache.org/download.cgi) \u3001 [Java](https://www.java.com/en/) \u548c [Python](https://www.python.org/) \uff0c\u4ee5\u53ca [\u5176\u4ed6\u5de5\u5177](https://cloud.google.com/shell/docs/how-cloud-shell-works?hl=zh-cn#tools) \u3002\n- \u5c07 GitHub [GoogleCloudDataproc/cloud-dataproc](https://github.com/GoogleCloudDataproc/cloud-dataproc) \u4ee3\u78bc\u5eab\u514b\u9686\u5230\u60a8\u7684 Cloud Shell \u6703\u8a71\u7d42\u7aef\u4e2d\uff1a```\ngit clone https://github.com/GoogleCloudDataproc/cloud-dataproc.git\n```\n- \u5207\u63db\u5230 `cloud-dataproc/spark-hbase` \u76ee\u9304\uff1a```\ncd cloud-dataproc/spark-hbase\n```\u793a\u4f8b\u8f38\u51fa\uff1a```\nuser-name@cloudshell:~/cloud-dataproc/spark-hbase (project-id)$\n```\n- \u63d0\u4ea4 Dataproc \u4f5c\u696d\u3002\n- \u5728`pom.xml`\u6587\u4ef6\u4e2d\u8a2d\u7f6e\u7d44\u4ef6\u7248\u672c\u3002- Dataproc [2.0.x \u767c\u4f48\u7248\u672c](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.0?hl=zh-cn) \u9801\u9762\u5217\u51fa\u4e86\u5b89\u88dd\u6709\u6700\u65b0\u548c\u6700\u5f8c\u4e00\u500b\u56db\u500b\u6620\u50cf 2.0 \u6b21\u8981\u7248\u672c\u7684 Scala\u3001Spark \u548c HBase \u7d44\u4ef6\u7248\u672c\u3002- \u5982\u9700\u67e5\u627e 2.0 \u6620\u50cf\u7248\u672c\u7684\u96c6\u7fa3\u7684\u6b21\u8981\u7248\u672c\uff0c\u8acb\u9ede\u64ca Google Cloud Console \u4e2d [\u96c6\u7fa3](https://console.cloud.google.com/dataproc/clusters?hl=zh-cn) \u9801\u9762\u4e0a\u7684\u96c6\u7fa3\u540d\u7a31\uff0c\u6253\u958b **\u96c6\u7fa3\u8a73\u60c5** \u9801\u9762\uff0c\u5176\u4e2d\u5217\u51fa\u4e86\u96c6\u7fa3 **\u6620\u50cf\u7248\u672c** \u3002\n- \u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u5f9e\u96c6\u7fa3\u7684\u4e3b\u7bc0\u9ede\u5728 [SSH \u6703\u8a71\u7d42\u7aef](https://cloud.google.com/dataproc/docs/concepts/accessing/ssh?hl=zh-cn) \u4e2d\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u78ba\u5b9a\u7d44\u4ef6\u7248\u672c\uff1a- \u6aa2\u67e5 scala \u7248\u672c\uff1a```\nscala -version\n```\n- \u6aa2\u67e5 Spark \u7248\u672c\uff08Ctrl+D \u9000\u51fa\uff09\uff1a```\nspark-shell\n```\n- \u6aa2\u67e5 HBase \u7248\u672c\uff1a```\nhbase version\n```\n- \u8b58\u5225 Maven [pom.xml](https://github.com/apache/hbase-connectors/blob/master/spark/pom.xml) \u4e2d\u7684 Spark\u3001Scala \u548c HBase \u7248\u672c\u4f9d\u8cf4\u9805\uff1a```\n<properties>\n\u00a0\u00a0<scala.version>scala full version (for example, 2.12.14)</scala.version>\n\u00a0\u00a0<scala.main.version>scala main version (for example, 2.12)</scala.main.version>\n\u00a0\u00a0<spark.version>spark version (for example, 3.1.2)</spark.version>\n\u00a0\u00a0<hbase.client.version>hbase version (for example, 2.2.7)</hbase.client.version>\n\u00a0\u00a0<hbase-spark.version>1.0.0(the current Apache HBase Spark Connector version)>\n</properties>\n```\u6ce8\u610f\uff1a`hbase-spark.version`\u662f\u7576\u524d\u7684 Spark HBase \u9023\u63a5\u5668\u7248\u672c\uff1b\u6b64\u7248\u672c\u7248\u672c\u865f\u4fdd\u6301\u4e0d\u8b8a\u3002\n- \u5728 Cloud Shell \u7de8\u8f2f\u5668\u4e2d\u4fee\u6539`pom.xml`\u6587\u4ef6\uff0c\u4ee5\u63d2\u5165\u6b63\u78ba\u7684 Scala\u3001Spark \u548c HBase \u7248\u672c\u865f\u3002\u4fee\u6539\u5b8c\u7562\u5f8c\uff0c\u9ede\u64ca **\u6253\u958b\u7d42\u7aef** \u4ee5\u8fd4\u56de Cloud Shell \u7d42\u7aef\u547d\u4ee4\u884c\u3002```\ncloudshell edit .\n```\n- \u5728 Cloud Shell \u4e2d\u5207\u63db\u5230 Java 8\u3002\u69cb\u5efa\u4ee3\u78bc\u9700\u8981\u6b64 JDK \u7248\u672c\uff08\u60a8\u53ef\u4ee5\u5ffd\u7565\u4efb\u4f55\u63d2\u4ef6\u8b66\u544a\u6d88\u606f\uff09\uff1a```\nsudo update-java-alternatives -s java-1.8.0-openjdk-amd64 && export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n```\n- \u9a57\u8b49 Java 8 \u7684\u5b89\u88dd\u60c5\u6cc1\uff1a```\njava -version\n```\u793a\u4f8b\u8f38\u51fa\uff1a```\nopenjdk version \"1.8...\"\n \n```\n- \u69cb\u5efa`jar`\u6587\u4ef6\uff1a```\nmvn clean package\n````.jar`\u6587\u4ef6\u653e\u5728`/target`\u5b50\u76ee\u9304\u4e2d\uff08\u4f8b\u5982 **target/spark-hbase-1.0-SNAPSHOT.jar** \uff09\u3002\n- \u63d0\u4ea4\u4f5c\u696d\u3002```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--class=hbase.SparkHBaseMain \\\n\u00a0\u00a0\u00a0\u00a0--jars=target/filename.jar \\\n\u00a0\u00a0\u00a0\u00a0--region=cluster-region \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name\n```- `--jars`\uff1a\u5c07`.jar`\u6587\u4ef6\u7684\u540d\u7a31\u63d2\u5165\u5230\u201ctarget/\u201d\u548c\u201c.jar\u201d\u4e4b\u524d\u3002\n- \u5982\u679c\u60a8\u5728 [\u5275\u5efa\u96c6\u7fa3](#create_a_cluster) \u6642\u672a\u8a2d\u7f6e Spark \u9a45\u52d5\u7a0b\u5e8f\u548c\u57f7\u884c\u7a0b\u5e8f HBase \u985e\u8def\u5f91\uff0c\u5247\u5fc5\u9808\u5728\u6bcf\u6b21\u4f5c\u696d\u63d0\u4ea4\u6642\u8a2d\u7f6e\u9019\u4e9b\u8b8a\u91cf\uff0c\u65b9\u6cd5\u662f\u5728\u4f5c\u696d\u63d0\u4ea4\u547d\u4ee4\u4e2d\u6dfb\u52a0\u4ee5\u4e0b`\u2011\u2011properties`\u6a19\u8a8c\uff1a```\n--properties='spark.driver.extraClassPath=/etc/hbase/conf:/usr/lib/hbase/*,spark.executor.extraClassPath=/etc/hbase/conf:/usr/lib/hbase/*'\n  \n```\n- \u5728 Cloud Shell \u6703\u8a71\u7d42\u7aef\u8f38\u51fa\u4e2d\u67e5\u770b HBase \u8868\u8f38\u51fa\uff1a```\nWaiting for job output...\n...\n+----+----+\n| key|name|\n+----+----+\n|key1| foo|\n|key2| bar|\n+----+----+\n```\n- \u63d0\u4ea4\u4f5c\u696d\u3002```\ngcloud dataproc jobs submit pyspark scripts/pyspark-hbase.py \\\n\u00a0\u00a0\u00a0\u00a0--region=cluster-region \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name\n```- \u5982\u679c\u60a8\u5728 [\u5275\u5efa\u96c6\u7fa3](#create_a_cluster) \u6642\u672a\u8a2d\u7f6e Spark \u9a45\u52d5\u7a0b\u5e8f\u548c\u57f7\u884c\u7a0b\u5e8f HBase \u985e\u8def\u5f91\uff0c\u5247\u5fc5\u9808\u5728\u6bcf\u6b21\u4f5c\u696d\u63d0\u4ea4\u6642\u8a2d\u7f6e\u9019\u4e9b\u8b8a\u91cf\uff0c\u65b9\u6cd5\u662f\u5728\u4f5c\u696d\u63d0\u4ea4\u547d\u4ee4\u4e2d\u6dfb\u52a0\u4ee5\u4e0b`\u2011\u2011properties`\u6a19\u8a8c\uff1a```\n--properties='spark.driver.extraClassPath=/etc/hbase/conf:/usr/lib/hbase/*,spark.executor.extraClassPath=/etc/hbase/conf:/usr/lib/hbase/*'\n  \n```\n- \u5728 Cloud Shell \u6703\u8a71\u7d42\u7aef\u8f38\u51fa\u4e2d\u67e5\u770b HBase \u8868\u8f38\u51fa\uff1a```\nWaiting for job output...\n...\n+----+----+\n| key|name|\n+----+----+\n|key1| foo|\n|key2| bar|\n+----+----+\n```\n### \u6383\u63cf HBase \u8868\u901a\u904e\u5728 [\u9a57\u8b49\u9023\u63a5\u5668\u5b89\u88dd](#verify_connector_installation) \u4e2d\u6253\u958b\u7684\u4e3b\u7bc0\u9ede SSH \u6703\u8a71\u7d42\u7aef\u4e2d\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u60a8\u53ef\u4ee5\u6383\u63cf HBase \u8868\u7684\u5167\u5bb9\uff1a- \u6253\u958b HBase shell\uff1a```\nhbase shell\n```\n- \u6383\u63cf\u201c\u6211\u7684\u8868\u683c\u201d\uff1a```\nscan 'my_table'\n```\u793a\u4f8b\u8f38\u51fa\uff1a```\nROW    COLUMN+CELL\n\u00a0key1    column=cf:name, timestamp=1647364013561, value=foo\n\u00a0key2    column=cf:name, timestamp=1647364012817, value=bar\n2 row(s)\nTook 0.5009 seconds\n```## \u6e05\u7406\n\u5b8c\u6210\u672c\u6559\u7a0b\u5f8c\uff0c\u60a8\u53ef\u4ee5\u6e05\u7406\u60a8\u5275\u5efa\u7684\u8cc7\u6e90\uff0c\u8b93\u5b83\u5011\u505c\u6b62\u4f7f\u7528\u914d\u984d\uff0c\u4ee5\u514d\u7522\u751f\u8cbb\u7528\u3002\u4ee5\u4e0b\u90e8\u5206\u4ecb\u7d39\u5982\u4f55\u522a\u9664\u6216\u95dc\u9589\u9019\u4e9b\u8cc7\u6e90\u3002\n### \u522a\u9664\u9805\u76ee\n\u82e5\u8981\u907f\u514d\u7522\u751f\u8cbb\u7528\uff0c\u6700\u7c21\u55ae\u7684\u65b9\u6cd5\u662f\u522a\u9664\u60a8\u7232\u672c\u6559\u7a0b\u5275\u5efa\u7684\u9805\u76ee\u3002\n\u5982\u9700\u522a\u9664\u9805\u76ee\uff0c\u8acb\u57f7\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a\n- Titles in dynamic includes are not used anywhere, and we should avoid paying to translate them\n- **\u8b66\u544a** \uff1a\u522a\u9664\u9805\u76ee\u6703\u7522\u751f\u4ee5\u4e0b\u5f71\u97ff- **\u9805\u76ee\u4e2d\u7684\u6240\u6709\u5167\u5bb9\u90fd\u6703\u88ab\u522a\u9664\u3002** \u5982\u679c\u60a8\u5c07\u73fe\u6709\u9805\u76ee\u7528\u65bc\u672c\u6587\u6a94\u4e2d\u7684\u4efb\u52d9\uff0c\u5247\u522a\u9664\u8a72\u9805\u76ee\u5f8c\uff0c\u9084\u5c07\u522a\u9664\u60a8\u5df2\u5728\u8a72\u9805\u76ee\u4e2d\u5b8c\u6210\u7684\u4efb\u4f55\u5176\u4ed6\u5de5\u4f5c\u3002\n- **\u81ea\u5b9a\u7fa9\u9805\u76ee ID \u4e1f\u5931\u3002** \u5275\u5efa\u6b64\u9805\u76ee\u6642\uff0c\u60a8\u53ef\u80fd\u5275\u5efa\u4e86\u8981\u5728\u5c07\u4f86\u4f7f\u7528\u7684\u81ea\u5b9a\u7fa9\u9805\u76ee ID\u3002\u8981\u4fdd\u7559\u4f7f\u7528\u8a72\u9805\u76ee ID \u7684\u7db2\u5740\uff08\u4f8b\u5982`appspot.com`\u7db2\u5740\uff09\uff0c\u8acb\u522a\u9664\u9805\u76ee\u5167\u7684\u6240\u9078\u8cc7\u6e90\uff0c\u800c\u4e0d\u662f\u522a\u9664\u6574\u500b\u9805\u76ee\u3002\n\u5982\u679c\u60a8\u6253\u7b97\u63a2\u7d22\u591a\u500b\u67b6\u69cb\u3001\u6559\u7a0b\u6216\u5feb\u901f\u5165\u9580\uff0c\u5247\u91cd\u8907\u4f7f\u7528\u9805\u76ee\u53ef\u4ee5\u5e6b\u52a9\u60a8\u907f\u514d\u8d85\u51fa\u9805\u76ee\u914d\u984d\u9650\u5236\u3002\n- \u5728 Google Cloud \u63a7\u5236\u6aaf\u4e2d\uff0c\u9032\u5165 **\u7ba1\u7406\u8cc7\u6e90** \u9801\u9762\u3002 [\u8f49\u5230\u201c\u7ba1\u7406\u8cc7\u6e90\u201d](https://console.cloud.google.com/iam-admin/projects?hl=zh-cn) \n- \u5728\u9805\u76ee\u5217\u8868\u4e2d\uff0c\u9078\u64c7\u8981\u522a\u9664\u7684\u9805\u76ee\uff0c\u7136\u5f8c\u9ede\u64ca **\u522a\u9664** \u3002\n- \u5728\u5c0d\u8a71\u6846\u4e2d\u8f38\u5165\u9805\u76ee ID\uff0c\u7136\u5f8c\u9ede\u64ca **\u95dc\u9589** \u4ee5\u522a\u9664\u9805\u76ee\u3002\n### \u522a\u9664\u96c6\u7fa3\n- \u5982\u9700\u522a\u9664\u60a8\u7684\u96c6\u7fa3\uff0c\u8acb\u8f38\u5165\u4ee5\u4e0b\u547d\u4ee4\uff1a```\ngcloud dataproc clusters delete cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION}\n```", "guide": "Dataproc"}