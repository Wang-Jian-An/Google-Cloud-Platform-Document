{"title": "Dataproc - Dataproc \u53ef\u9078 Hudi \u7d44\u4ef6", "url": "https://cloud.google.com/dataproc/docs/concepts/components/hudi?hl=zh-cn", "abstract": "# Dataproc - Dataproc \u53ef\u9078 Hudi \u7d44\u4ef6\n\u4f7f\u7528 [\u53ef\u9078\u7d44\u4ef6](https://cloud.google.com/dataproc/docs/concepts/components/overview?hl=zh-cn#available_optional_components) \u529f\u80fd\u5275\u5efa Dataproc \u96c6\u7fa3\u6642\uff0c\u60a8\u53ef\u4ee5\u5b89\u88dd\u5176\u4ed6\u7d44\u4ef6\uff0c\u4f8b\u5982 Hudi\u3002\u672c\u9801\u9762\u4ecb\u7d39\u5982\u4f55\u5728 Dataproc \u96c6\u7fa3\u4e0a\u9078\u64c7\u6027\u5730\u5b89\u88dd Hudi \u7d44\u4ef6\u3002\n\u5982\u679c\u5b89\u88dd\u5728 Dataproc \u96c6\u7fa3\u4e0a\uff0c [Apache Hudi](https://hudi.apache.org/docs/overview) \u7d44\u4ef6\u6703\u5b89\u88dd Hudi \u5eab\uff0c\u4e26\u5728\u96c6\u7fa3\u4e2d\u5c07 Spark \u548c Hive \u914d\u7f6e\u7232\u8207 Hudi \u642d\u914d\u4f7f\u7528\u3002\n", "content": "## \u517c\u5bb9\u7684 Dataproc \u6620\u50cf\u7248\u672c\n\u60a8\u53ef\u4ee5\u5728\u4f7f\u7528\u4ee5\u4e0b Dataproc \u6620\u50cf\u7248\u672c\u5275\u5efa\u7684 Dataproc \u96c6\u7fa3\u4e0a\u5b89\u88dd Hudi \u7d44\u4ef6\uff1a\n- [1.5.80+](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.5?hl=zh-cn) \n- [2.0.54+](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.0?hl=zh-cn) \n- [2.1.2+](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1?hl=zh-cn) ## \u8207 Hudi \u76f8\u95dc\u7684\u5a92\u9ad4\u8cc7\u6e90\n\u5275\u5efa\u5177\u6709 Hudi \u7684 Dataproc \u96c6\u7fa3\u6642\uff0c\u4ee5\u4e0b Spark \u548c Hive \u5c6c\u6027\u5c07\u914d\u7f6e\u7232\u8207 Hudi \u642d\u914d\u4f7f\u7528\u3002\n| \u914d\u7f6e\u6587\u4ef6       | \u5c6c\u6027       | \u9ed8\u8a8d\u503c                   |\n|:------------------------------------|:--------------------------------|:--------------------------------------------------------------------------------|\n| /etc/spark/conf/spark-defaults.conf | spark.serializer    | org.apache.spark.serializer.KryoSerializer          |\n| /etc/spark/conf/spark-defaults.conf | spark.sql.catalog.spark_catalog | org.apache.spark.sql.hudi.catalog.HoodieCatalog         |\n| /etc/spark/conf/spark-defaults.conf | spark.sql.extensions   | org.apache.spark.sql.hudi.HoodieSparkSessionExtension       |\n| /etc/spark/conf/spark-defaults.conf | spark.driver.extraClassPath  | /usr/lib/hudi/lib/hudi-sparkspark-version-bundle_scala-version-hudi-version.jar |\n| /etc/spark/conf/spark-defaults.conf | spark.executor.extraClassPath | /usr/lib/hudi/lib/hudi-sparkspark-version-bundle_scala-version-hudi-version.jar |\n| /etc/hive/conf/hive-site.xml  | hive.aux.jars.path    | file:///usr/lib/hudi/lib/hudi-hadoop-mr-bundle-version.jar      |\n## \u5b89\u88dd\u7d44\u4ef6\n\u5275\u5efa Dataproc \u96c6\u7fa3\u6642\uff0c\u5b89\u88dd Hudi \u7d44\u4ef6\u3002\n[Dataproc \u6620\u50cf\u767c\u4f48\u7248\u672c\u9801\u9762](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-version-clusters?hl=zh-cn#supported_dataproc_versions) \u5217\u51fa\u4e86\u6bcf\u500b Dataproc \u6620\u50cf\u7248\u672c\u4e2d\u5305\u542b\u7684 Hudi \u7d44\u4ef6\u7248\u672c\u3002\n- \u5553\u7528\u7d44\u4ef6\u3002- \u5728 Google Cloud \u63a7\u5236\u6aaf\u4e2d\uff0c\u6253\u958b Dataproc \u7684 [\u5275\u5efa\u96c6\u7fa3](https://console.cloud.google.com/dataproc/clustersAdd?hl=zh-cn) \u9801\u9762\u3002\u5df2\u9078\u64c7 **\u8a2d\u7f6e\u96c6\u7fa3** \u9762\u677f\u3002\n- \u5728 **\u7d44\u4ef6** \u90e8\u5206\u4e2d\u57f7\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a- \u5728 **\u53ef\u9078\u7d44\u4ef6** \u4e0b\uff0c\u9078\u64c7 **Hudi** \u7d44\u4ef6\u3002\n\u5982\u9700\u5275\u5efa\u5305\u542b Hudi \u7d44\u4ef6\u7684 Dataproc \u96c6\u7fa3\uff0c\u8acb\u4f7f\u7528\u5e36\u6709 `--optional-components` \u6a19\u8a8c\u7684\u547d\u4ee4\u3002\n```\ngcloud dataproc clusters create CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=HUDI \\\n\u00a0\u00a0\u00a0\u00a0--image-version=DATAPROC_VERSION \\\n\u00a0\u00a0\u00a0\u00a0--properties=PROPERTIES\n```\n\u66ff\u63db\u4ee5\u4e0b\u5167\u5bb9\uff1a- \uff1a\u5fc5\u586b\u3002\u65b0\u96c6\u7fa3\u540d\u7a31\u3002\n- \uff1a\u5fc5\u586b\u3002 [\u96c6\u7fa3\u5340\u57df](https://cloud.google.com/compute/docs/regions-zones?hl=zh-cn#available) \u3002\n- \uff1a\u53ef\u9078\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528\u6b64\u53ef\u9078\u6b64\u6a19\u8a8c\u4f86\u6307\u5b9a\u975e\u9ed8\u8a8d Dataproc \u6620\u50cf\u7248\u672c\uff08\u8acb\u53c3\u95b1 [\u9ed8\u8a8d Dataproc \u6620\u50cf\u7248\u672c](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-version-clusters?hl=zh-cn#default_dataproc_image_version) \uff09\u3002\n- \uff1a\u53ef\u9078\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528\u6b64\u53ef\u9078\u6a19\u8a8c\u4f86\u8a2d\u7f6e [Hudi \u7d44\u4ef6\u5c6c\u6027](https://hudi.apache.org/docs/basic_configurations) \uff0c\u9019\u4e9b\u5c6c\u6027\u4f7f\u7528 [hudi: \u6587\u4ef6\u524d\u7db4](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/cluster-properties?hl=zh-cn#file-prefixed_properties_table) \u6307\u5b9a\u3002 \u793a\u4f8b\uff1a`properties=hudi:hoodie.datasource.write.table.type=COPY_ON_WRITE`\u3002\n- Hudi \u7d44\u4ef6\u7248\u672c\u5c6c\u6027\uff1a\u60a8\u53ef\u4ee5\u9078\u64c7\u6307\u5b9a [dataproc:hudi.version \u5c6c\u6027](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/cluster-properties?hl=zh-cn#dataproc_service_properties_table) \u3002 **\u6ce8\u610f** \uff1aDataproc \u5c07 Hudi \u7d44\u4ef6\u7248\u672c\u8a2d\u7f6e\u7232\u8207 Dataproc \u96c6\u7fa3\u6620\u50cf\u7248\u672c\u517c\u5bb9\u3002\u5982\u679c\u60a8\u8a2d\u7f6e\u4e86\u6b64\u5c6c\u6027\uff0c\u90a3\u9ebc\u5982\u679c\u6307\u5b9a\u7684\u7248\u672c\u8207\u96c6\u7fa3\u6620\u50cf\u4e0d\u517c\u5bb9\uff0c\u5247\u96c6\u7fa3\u5275\u5efa\u53ef\u80fd\u6703\u5931\u6557\u3002\n- Spark \u548c Hive \u5c6c\u6027\uff1a\u5275\u5efa\u96c6\u7fa3\u6642\uff0cDataproc \u6703\u8a2d\u7f6e [\u8207 Hudi \u76f8\u95dc\u7684 Spark \u548c Hive](#hudi_related_properties) \u5c6c\u6027\u3002\u5275\u5efa\u96c6\u7fa3\u6216\u63d0\u4ea4\u4f5c\u696d\u6642\u7121\u9700\u8a2d\u7f6e\u5b83\u5011\u3002\u53ef\u4ee5\u901a\u904e Dataproc API \u4f7f\u7528 [SoftwareConfig.Component](https://cloud.google.com/dataproc/docs/reference/rest/v1/ClusterConfig?hl=zh-cn#Component) \u4f5c\u7232 [clusters.create](https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.clusters/create?hl=zh-cn) \u8acb\u6c42\u7684\u4e00\u90e8\u5206\u5b89\u88dd Hudi \u7d44\u4ef6\u3002\n## \u63d0\u4ea4\u4f5c\u696d\u4ee5\u8b80\u53d6\u548c\u5beb\u5165 Hudi \u8868\u683c\n[\u4f7f\u7528 Hudi \u7d44\u4ef6\u5275\u5efa\u96c6\u7fa3](#install_the_component) \u5f8c\uff0c\u60a8\u53ef\u4ee5\u63d0\u4ea4\u8b80\u53d6\u548c\u5beb\u5165 Hudi \u8868\u7684 Spark \u548c Hive \u4f5c\u696d\u3002\n`gcloud CLI` \u793a\u4f8b\uff1a\n```\ngcloud dataproc jobs submit pyspark \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--job-file=JOB_FILE \\\n\u00a0\u00a0\u00a0\u00a0-- JOB_ARGS\n```\n### PySpark \u4f5c\u696d\u793a\u4f8b\n\u4ee5\u4e0b PySpark \u6587\u4ef6\u53ef\u5275\u5efa\u3001\u8b80\u53d6\u548c\u5beb\u5165 Hudi \u8868\u3002\n[  Codelabs/spark-hudi/pyspark_hudi_example.py ](https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/HEAD/codelabs/spark-hudi/pyspark_hudi_example.py) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudDataproc/cloud-dataproc/blob/HEAD/codelabs/spark-hudi/pyspark_hudi_example.py)\n```\n#!/usr/bin/env python\"\"\"Pyspark Hudi test.\"\"\"import sysfrom pyspark.sql import SparkSessiondef create_hudi_table(spark, table_name, table_uri):\u00a0 \"\"\"Creates Hudi table.\"\"\"\u00a0 create_table_sql = f\"\"\"\u00a0 \u00a0 CREATE TABLE IF NOT EXISTS {table_name} (\u00a0 \u00a0 \u00a0 uuid string,\u00a0 \u00a0 \u00a0 begin_lat double,\u00a0 \u00a0 \u00a0 begin_lon double,\u00a0 \u00a0 \u00a0 end_lat double,\u00a0 \u00a0 \u00a0 end_lon double,\u00a0 \u00a0 \u00a0 driver string,\u00a0 \u00a0 \u00a0 rider string,\u00a0 \u00a0 \u00a0 fare double,\u00a0 \u00a0 \u00a0 partitionpath string,\u00a0 \u00a0 \u00a0 ts long\u00a0 \u00a0 ) USING hudi\u00a0 \u00a0 LOCATION '{table_uri}'\u00a0 \u00a0 TBLPROPERTIES (\u00a0 \u00a0 \u00a0 type = 'cow',\u00a0 \u00a0 \u00a0 primaryKey = 'uuid',\u00a0 \u00a0 \u00a0 preCombineField = 'ts'\u00a0 \u00a0 )\u00a0 \u00a0 PARTITIONED BY (partitionpath)\u00a0 \"\"\"\u00a0 spark.sql(create_table_sql)def generate_test_dataframe(spark, n_rows):\u00a0 \"\"\"Generates test dataframe with Hudi's built-in data generator.\"\"\"\u00a0 sc = spark.sparkContext\u00a0 utils = sc._jvm.org.apache.hudi.QuickstartUtils\u00a0 data_generator = utils.DataGenerator()\u00a0 inserts = utils.convertToStringList(data_generator.generateInserts(n_rows))\u00a0 return spark.read.json(sc.parallelize(inserts, 2))def write_hudi_table(table_name, table_uri, df):\u00a0 \"\"\"Writes Hudi table.\"\"\"\u00a0 hudi_options = {\u00a0 \u00a0 \u00a0 'hoodie.table.name': table_name,\u00a0 \u00a0 \u00a0 'hoodie.datasource.write.recordkey.field': 'uuid',\u00a0 \u00a0 \u00a0 'hoodie.datasource.write.partitionpath.field': 'partitionpath',\u00a0 \u00a0 \u00a0 'hoodie.datasource.write.table.name': table_name,\u00a0 \u00a0 \u00a0 'hoodie.datasource.write.operation': 'upsert',\u00a0 \u00a0 \u00a0 'hoodie.datasource.write.precombine.field': 'ts',\u00a0 \u00a0 \u00a0 'hoodie.upsert.shuffle.parallelism': 2,\u00a0 \u00a0 \u00a0 'hoodie.insert.shuffle.parallelism': 2,\u00a0 }\u00a0 df.write.format('hudi').options(**hudi_options).mode('append').save(table_uri)def query_commit_history(spark, table_name, table_uri):\u00a0 tmp_table = f'{table_name}_commit_history'\u00a0 spark.read.format('hudi').load(table_uri).createOrReplaceTempView(tmp_table)\u00a0 query = f\"\"\"\u00a0 \u00a0 SELECT DISTINCT(_hoodie_commit_time)\u00a0 \u00a0 FROM {tmp_table}\u00a0 \u00a0 ORDER BY _hoodie_commit_time\u00a0 \u00a0 DESC\u00a0 \"\"\"\u00a0 return spark.sql(query)def read_hudi_table(spark, table_name, table_uri, commit_ts=''):\u00a0 \"\"\"Reads Hudi table at the given commit timestamp.\"\"\"\u00a0 if commit_ts:\u00a0 \u00a0 options = {'as.of.instant': commit_ts}\u00a0 else:\u00a0 \u00a0 options = {}\u00a0 tmp_table = f'{table_name}_snapshot'\u00a0 spark.read.format('hudi').options(**options).load(\u00a0 \u00a0 \u00a0 table_uri\u00a0 ).createOrReplaceTempView(tmp_table)\u00a0 query = f\"\"\"\u00a0 \u00a0 SELECT _hoodie_commit_time, begin_lat, begin_lon,\u00a0 \u00a0 \u00a0 \u00a0 driver, end_lat, end_lon, fare, partitionpath,\u00a0 \u00a0 \u00a0 \u00a0 rider, ts, uuid\u00a0 \u00a0 FROM {tmp_table}\u00a0 \"\"\"\u00a0 return spark.sql(query)def main():\u00a0 \"\"\"Test create write and read Hudi table.\"\"\"\u00a0 if len(sys.argv) != 3:\u00a0 \u00a0 raise Exception('Expected arguments: <table_name> <table_uri>')\u00a0 table_name = sys.argv[1]\u00a0 table_uri = sys.argv[2]\u00a0 app_name = f'pyspark-hudi-test_{table_name}'\u00a0 print(f'Creating Spark session {app_name} ...')\u00a0 spark = SparkSession.builder.appName(app_name).getOrCreate()\u00a0 spark.sparkContext.setLogLevel('WARN')\u00a0 print(f'Creating Hudi table {table_name} at {table_uri} ...')\u00a0 create_hudi_table(spark, table_name, table_uri)\u00a0 print('Generating test data batch 1...')\u00a0 n_rows1 = 10\u00a0 input_df1 = generate_test_dataframe(spark, n_rows1)\u00a0 input_df1.show(truncate=False)\u00a0 print('Writing Hudi table, batch 1 ...')\u00a0 write_hudi_table(table_name, table_uri, input_df1)\u00a0 print('Generating test data batch 2...')\u00a0 n_rows2 = 10\u00a0 input_df2 = generate_test_dataframe(spark, n_rows2)\u00a0 input_df2.show(truncate=False)\u00a0 print('Writing Hudi table, batch 2 ...')\u00a0 write_hudi_table(table_name, table_uri, input_df2)\u00a0 print('Querying commit history ...')\u00a0 commits_df = query_commit_history(spark, table_name, table_uri)\u00a0 commits_df.show(truncate=False)\u00a0 previous_commit_ts = commits_df.collect()[1]._hoodie_commit_time\u00a0 print('Reading the Hudi table snapshot at the latest commit ...')\u00a0 output_df1 = read_hudi_table(spark, table_name, table_uri)\u00a0 output_df1.show(truncate=False)\u00a0 print(f'Reading the Hudi table snapshot at {previous_commit_ts} ...')\u00a0 output_df2 = read_hudi_table(spark, table_name, table_uri, previous_commit_ts)\u00a0 output_df2.show(truncate=False)\u00a0 print('Stopping Spark session ...')\u00a0 spark.stop()\u00a0 print('All done')main()\n```\n\u4ee5\u4e0b gcloud CLI \u547d\u4ee4\u6703\u5c07\u793a\u4f8b PySpark \u6587\u4ef6\u63d0\u4ea4\u5230 Dataproc\u3002\n```\ngcloud dataproc jobs submit pyspark \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0gs://BUCKET_NAME/pyspark_hudi_example.py \\\n\u00a0\u00a0\u00a0\u00a0-- TABLE_NAME gs://BUCKET_NAME/TABLE_NAME\n```\n## \u4f7f\u7528 Hudi CLI\nHudi CLI \u4f4d\u65bc Dataproc \u96c6\u7fa3\u4e3b\u670d\u52d9\u5668\u7bc0\u9ede\u4e0a\u7684 `/usr/lib/hudi/cli/hudi-cli.sh` \u3002\u60a8\u53ef\u4ee5\u4f7f\u7528 Hudi CLI \u67e5\u770b Hudi \u8868\u67b6\u69cb\u3001\u63d0\u4ea4\u5167\u5bb9\u548c\u7d71\u8a08\u4fe1\u606f\uff0c\u4ee5\u53ca\u624b\u52d5\u57f7\u884c\u7ba1\u7406\u64cd\u4f5c\uff0c\u4f8b\u5982\u5b89\u6392\u6642\u9593\u58d3\u7e2e\uff08\u8acb\u53c3\u95b1 [\u4f7f\u7528 hudi-cli](https://hudi.apache.org/docs/cli/#using-hudi-cli) \uff09\u3002\n\u5982\u9700\u5553\u52d5 Hudi CLI \u4e26\u9023\u63a5\u5230 Hudi \u8868\uff0c\u8acb\u6309\u4ee5\u4e0b\u6b65\u9a5f\u64cd\u4f5c\uff1a\n- [\u901a\u904e SSH \u9023\u63a5\u5230\u4e3b\u7bc0\u9ede](https://cloud.google.com/dataproc/docs/concepts/accessing/ssh?hl=zh-cn) \u3002\n- \u904b\u884c`/usr/lib/hudi/cli/hudi-cli.sh`\u3002\u547d\u4ee4\u63d0\u793a\u7b26\u6703\u66f4\u6539\u7232`hudi->`\u3002\n- \u904b\u884c`connect --path gs://` `` `/` ``\u3002\n- \u904b\u884c\u547d\u4ee4\uff0c\u4f8b\u5982\u63cf\u8ff0\u8868\u67b6\u69cb\u7684`desc`\u6216\u986f\u793a\u63d0\u4ea4\u6b77\u53f2\u8a18\u9304\u7684`commits show`\u3002\n- \u5982\u9700\u505c\u6b62 CLI \u6703\u8a71\uff0c\u8acb\u904b\u884c`exit`\u3002## \u5982\u9700\u6df1\u5165\u77ad\u89e3\n- [Hudi \u5feb\u901f\u5165\u9580\u6307\u5357](https://hudi.apache.org/docs/quick-start-guide/)", "guide": "Dataproc"}