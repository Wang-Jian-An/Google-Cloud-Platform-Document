{"title": "Dataproc - Write a MapReduce job with the BigQuery connector", "url": "https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-mapreduce-example", "abstract": "# Dataproc - Write a MapReduce job with the BigQuery connector\n**Caution:** The Hadoop BigQuery connector for Hadoop MR described in this tutorial is no longer maintained. As a replacement for this discontinued connector, you can use the [Spark BigQuery Connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) for improved performance. For more information, see [Use the BigQuery Connector with Spark](/dataproc/docs/tutorials/bigquery-connector-spark-example) .\nThe Hadoop BigQuery connector is installed by default on all Dataproc 1.0-1.2 cluster nodes under `/usr/lib/hadoop/lib/` . It is available in both Spark and PySpark environments.\n**Dataproc image versions 1.5+:** The BigQuery connector is not installed by default in Dataproc [image versions 1.5 and higher](/dataproc/docs/concepts/versioning/dataproc-versions#supported_cloud_dataproc_versions) . To use it with these versions:\n- Install the BigQuery connector using this [initialization action](https://github.com/GoogleCloudDataproc/initialization-actions/tree/master/connectors) .\n- Specify the BigQuery connector in the `jars` parameter when submitting a job:```\n--jars=gs://hadoop-lib/bigquery/bigquery-connector-hadoop3-latest.jar\n```\n- Include the BigQuery connector classes in the application's jar-with-dependencies.\n**To Avoid Conflicts** : If your application uses a connector version that is different from the [connector version deployed on your Dataproc cluster](/dataproc/docs/concepts/versioning/dataproc-versions#supported_cloud_dataproc_versions) , you must either:\n- Create a new cluster with an [initialization action](https://github.com/GoogleCloudDataproc/initialization-actions/tree/master/connectors) that installs the connector version used by your application, or\n- Include and [relocate](https://maven.apache.org/plugins/maven-shade-plugin/examples/class-relocation.html) the connector classes and connector dependencies for the version you are using into your application's jar to avoid conflict between your connector version and the connector version deployed on your Dataproc cluster (see this [example of dependencies relocation in Maven](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/a839bd8f237197f98808944ca52d03de96518314/bigquery/pom.xml#L227-L286) ).", "content": "## GsonBigQueryInputFormat class\n`BigQueryInputFormat`has been renamed`GsonBigQueryInputFormat`to emphasize its [Gson](https://github.com/google/gson/blob/master/UserGuide.md) }-based format.\n`GsonBigQueryInputFormat` provides Hadoop with the BigQuery objects in a JsonObject format via the following primary operations:\n- Using a user-specified query to select BigQuery objects\n- Splitting the results of the query evenly among Hadoop nodes\n- Parsing the splits into Java objects to pass to the Mapper. The Hadoop Mapper class receives a`JsonObject`representation of each selected BigQuery object.\nThe `BigQueryInputFormat` class provides access to BigQuery records through an extension of the Hadoop [InputFormat](http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html) class. To use the BigQueryInputFormat class:\n- Lines must be added to the main Hadoop job to set parameters in the Hadoop configuration.\n- The [InputFormat](http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/InputFormat.html) class must be set to `GsonBigQueryInputFormat` .\nThe sections, below, show you how to meet these requirements.\n## Input Parameters\n```\n// Set the job-level projectId.conf.set(BigQueryConfiguration.PROJECT_ID_KEY, projectId);// Configure input parameters.BigQueryConfiguration.configureBigQueryInput(conf, inputQualifiedTableId);// Set InputFormat.job.setInputFormatClass(GsonBigQueryInputFormat.class);\n```\nNotes:\n- `job`refers to the`org.apache.hadoop.mapreduce.Job`, the Hadoop job to run.\n- `conf`refers to the`org.apache.hadoop.Configuration`for the Hadoop job.## Mapper\nThe `GsonBigQueryInputFormat` class reads from BigQuery and passes BigQuery objects one at a time as input to the Hadoop `Mapper` function. The inputs take the form of a pair comprising the following:\n- `LongWritable`, the record number\n- `JsonObject`, the Json-formatted BigQuery record\nThe `Mapper` accepts the `LongWritable` and `JsonObject pair` as input.\nHere is a snippet from the `Mapper` for a [sample WordCount](#completecode) job.\n```\n\u00a0 // private static final LongWritable ONE = new LongWritable(1);\u00a0 // The configuration key used to specify the BigQuery field name\u00a0 // (\"column name\").\u00a0 public static final String WORDCOUNT_WORD_FIELDNAME_KEY =\u00a0 \u00a0 \u00a0 \"mapred.bq.samples.wordcount.word.key\";\u00a0 // Default value for the configuration entry specified by\u00a0 // WORDCOUNT_WORD_FIELDNAME_KEY. Examples: 'word' in\u00a0 // publicdata:samples.shakespeare or 'repository_name'\u00a0 // in publicdata:samples.github_timeline.\u00a0 public static final String WORDCOUNT_WORD_FIELDNAME_VALUE_DEFAULT = \"word\";\u00a0 /**\u00a0 \u00a0* The mapper function for WordCount.\u00a0 \u00a0*/\u00a0 public static class Map\u00a0 \u00a0 \u00a0 extends Mapper <LongWritable, JsonObject, Text, LongWritable> {\u00a0 \u00a0 private static final LongWritable ONE = new LongWritable(1);\u00a0 \u00a0 private Text word = new Text();\u00a0 \u00a0 private String wordKey;\u00a0 \u00a0 @Override\u00a0 \u00a0 public void setup(Context context)\u00a0 \u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 \u00a0 // Find the runtime-configured key for the field name we're looking for\u00a0 \u00a0 \u00a0 // in the map task.\u00a0 \u00a0 \u00a0 Configuration conf = context.getConfiguration();\u00a0 \u00a0 \u00a0 wordKey = conf.get(WORDCOUNT_WORD_FIELDNAME_KEY,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 WORDCOUNT_WORD_FIELDNAME_VALUE_DEFAULT);\u00a0 \u00a0 }\u00a0 \u00a0 @Override\u00a0 \u00a0 public void map(LongWritable key, JsonObject value, Context context)\u00a0 \u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 \u00a0 JsonElement countElement = value.get(wordKey);\u00a0 \u00a0 \u00a0 if (countElement != null) {\u00a0 \u00a0 \u00a0 \u00a0 String wordInRecord = countElement.getAsString();\u00a0 \u00a0 \u00a0 \u00a0 word.set(wordInRecord);\u00a0 \u00a0 \u00a0 \u00a0 // Write out the key, value pair (write out a value of 1, which will be\u00a0 \u00a0 \u00a0 \u00a0 // added to the total count for this word in the Reducer).\u00a0 \u00a0 \u00a0 \u00a0 context.write(word, ONE);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\n```\n## IndirectBigQueryOutputFormat class\n`IndirectBigQueryOutputFormat` provides Hadoop with the ability to write `JsonObject` values directly into a BigQuery table. This class provides access to BigQuery records through an extension of the Hadoop [OutputFormat](http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapred/OutputFormat.html) class. To use it correctly, several parameters must be set in the Hadoop configuration, and the OutputFormat class must be set to `IndirectBigQueryOutputFormat` . Below is an example of the parameters to set and the lines of code needed to correctly use `IndirectBigQueryOutputFormat` .\n`IndirectBigQueryOutputFormat`works by first buffering all the data into a Cloud Storage temporary table, and then, on`commitJob`, copies all data from Cloud Storage into BigQuery in one operation. Its use is recommended for large jobs since it only requires one BigQuery \"load\" job per Hadoop/Spark job, as compared to`BigQueryOutputFormat`, which performs one BigQuery job for each Hadoop/Spark task.\n## Output Parameters\n```\n\u00a0 \u00a0 // Define the schema we will be using for the output BigQuery table.\u00a0 \u00a0 List<TableFieldSchema> outputTableFieldSchema = new ArrayList<TableFieldSchema>();\u00a0 \u00a0 outputTableFieldSchema.add(new TableFieldSchema().setName(\"Word\").setType(\"STRING\"));\u00a0 \u00a0 outputTableFieldSchema.add(new TableFieldSchema().setName(\"Count\").setType(\"INTEGER\"));\u00a0 \u00a0 TableSchema outputSchema = new TableSchema().setFields(outputTableFieldSchema);\u00a0 \u00a0 // Create the job and get its configuration.\u00a0 \u00a0 Job job = new Job(parser.getConfiguration(), \"wordcount\");\u00a0 \u00a0 Configuration conf = job.getConfiguration();\u00a0 \u00a0 // Set the job-level projectId.\u00a0 \u00a0 conf.set(BigQueryConfiguration.PROJECT_ID_KEY, projectId);\u00a0 \u00a0 // Configure input.\u00a0 \u00a0 BigQueryConfiguration.configureBigQueryInput(conf, inputQualifiedTableId);\u00a0 \u00a0 // Configure output.\u00a0 \u00a0 BigQueryOutputConfiguration.configure(\u00a0 \u00a0 \u00a0 \u00a0 conf,\u00a0 \u00a0 \u00a0 \u00a0 outputQualifiedTableId,\u00a0 \u00a0 \u00a0 \u00a0 outputSchema,\u00a0 \u00a0 \u00a0 \u00a0 outputGcsPath,\u00a0 \u00a0 \u00a0 \u00a0 BigQueryFileFormat.NEWLINE_DELIMITED_JSON,\u00a0 \u00a0 \u00a0 \u00a0 TextOutputFormat.class);\u00a0 \u00a0 // (Optional) Configure the KMS key used to encrypt the output table.\u00a0 \u00a0 BigQueryOutputConfiguration.setKmsKeyName(\u00a0 \u00a0 \u00a0 \u00a0 conf,\u00a0 \u00a0 \u00a0 \u00a0 \"projects/myproject/locations/us-west1/keyRings/r1/cryptoKeys/k1\"););\n```\n## Reducer\nThe `IndirectBigQueryOutputFormat` class writes to BigQuery. It takes a key and a `JsonObject` value as input and writes only the JsonObject value to BigQuery (the key is ignored). The `JsonObject` should contain a Json-formatted BigQuery record. The Reducer should output a key of any type ( `NullWritable` is used in our [sample WordCount](#completecode) job) and `JsonObject` value pair. The Reducer for the sample WordCount job is shown below.\n```\n\u00a0 /**\u00a0 \u00a0* Reducer function for WordCount.\u00a0 \u00a0*/\u00a0 public static class Reduce\u00a0 \u00a0 \u00a0 extends Reducer<Text, LongWritable, JsonObject, NullWritable> {\u00a0 \u00a0 @Override\u00a0 \u00a0 public void reduce(Text key, Iterable<LongWritable> values, Context context)\u00a0 \u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 \u00a0 // Add up the values to get a total number of occurrences of our word.\u00a0 \u00a0 \u00a0 long count = 0;\u00a0 \u00a0 \u00a0 for (LongWritable val : values) {\u00a0 \u00a0 \u00a0 \u00a0 count = count + val.get();\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 JsonObject jsonObject = new JsonObject();\u00a0 \u00a0 \u00a0 jsonObject.addProperty(\"Word\", key.toString());\u00a0 \u00a0 \u00a0 jsonObject.addProperty(\"Count\", count);\u00a0 \u00a0 \u00a0 // Key does not matter.\u00a0 \u00a0 \u00a0 context.write(jsonObject, NullWritable.get());\u00a0 \u00a0 }\u00a0 }\n```\n## Clean up\nAfter the job completes, clean up Cloud Storage export paths.\n```\njob.waitForCompletion(true);GsonBigQueryInputFormat.cleanupJob(job.getConfiguration(), job.getJobID());\n```\nYou can view word counts in the BigQuery output table in the [Google Cloud console](https://console.cloud.google.com/bigquery) .\n## Complete Code for a sample WordCount job\nThe code below is an example of a simple WordCount job that aggregates word counts from objects in BigQuery.\n```\npackage com.google.cloud.hadoop.io.bigquery.samples;import com.google.api.services.bigquery.model.TableFieldSchema;import com.google.api.services.bigquery.model.TableSchema;import com.google.cloud.hadoop.io.bigquery.BigQueryConfiguration;import com.google.cloud.hadoop.io.bigquery.BigQueryFileFormat;import com.google.cloud.hadoop.io.bigquery.GsonBigQueryInputFormat;import com.google.cloud.hadoop.io.bigquery.output.BigQueryOutputConfiguration;import com.google.cloud.hadoop.io.bigquery.output.IndirectBigQueryOutputFormat;import com.google.gson.JsonElement;import com.google.gson.JsonObject;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;import java.io.IOException;import java.util.ArrayList;import java.util.List;/**\u00a0* Sample program to run the Hadoop Wordcount example over tables in BigQuery.\u00a0*/public class WordCount {\u00a0// The configuration key used to specify the BigQuery field name\u00a0 // (\"column name\").\u00a0 public static final String WORDCOUNT_WORD_FIELDNAME_KEY =\u00a0 \u00a0 \u00a0 \"mapred.bq.samples.wordcount.word.key\";\u00a0 // Default value for the configuration entry specified by\u00a0 // WORDCOUNT_WORD_FIELDNAME_KEY. Examples: 'word' in\u00a0 // publicdata:samples.shakespeare or 'repository_name'\u00a0 // in publicdata:samples.github_timeline.\u00a0 public static final String WORDCOUNT_WORD_FIELDNAME_VALUE_DEFAULT = \"word\";\u00a0 // Guava might not be available, so define a null / empty helper:\u00a0 private static boolean isStringNullOrEmpty(String toTest) {\u00a0 \u00a0 return toTest == null || \"\".equals(toTest);\u00a0 }\u00a0 /**\u00a0 \u00a0* The mapper function for WordCount. For input, it consumes a LongWritable\u00a0 \u00a0* and JsonObject as the key and value. These correspond to a row identifier\u00a0 \u00a0* and Json representation of the row's values/columns.\u00a0 \u00a0* For output, it produces Text and a LongWritable as the key and value.\u00a0 \u00a0* These correspond to the word and a count for the number of times it has\u00a0 \u00a0* occurred.\u00a0 \u00a0*/\u00a0 public static class Map\u00a0 \u00a0 \u00a0 extends Mapper <LongWritable, JsonObject, Text, LongWritable> {\u00a0 \u00a0 private static final LongWritable ONE = new LongWritable(1);\u00a0 \u00a0 private Text word = new Text();\u00a0 \u00a0 private String wordKey;\u00a0 \u00a0 @Override\u00a0 \u00a0 public void setup(Context context)\u00a0 \u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 \u00a0 // Find the runtime-configured key for the field name we're looking for in\u00a0 \u00a0 \u00a0 // the map task.\u00a0 \u00a0 \u00a0 Configuration conf = context.getConfiguration();\u00a0 \u00a0 \u00a0 wordKey = conf.get(WORDCOUNT_WORD_FIELDNAME_KEY, WORDCOUNT_WORD_FIELDNAME_VALUE_DEFAULT);\u00a0 \u00a0 }\u00a0 \u00a0 @Override\u00a0 \u00a0 public void map(LongWritable key, JsonObject value, Context context)\u00a0 \u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 \u00a0 JsonElement countElement = value.get(wordKey);\u00a0 \u00a0 \u00a0 if (countElement != null) {\u00a0 \u00a0 \u00a0 \u00a0 String wordInRecord = countElement.getAsString();\u00a0 \u00a0 \u00a0 \u00a0 word.set(wordInRecord);\u00a0 \u00a0 \u00a0 \u00a0 // Write out the key, value pair (write out a value of 1, which will be\u00a0 \u00a0 \u00a0 \u00a0 // added to the total count for this word in the Reducer).\u00a0 \u00a0 \u00a0 \u00a0 context.write(word, ONE);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\u00a0 /**\u00a0 \u00a0* Reducer function for WordCount. For input, it consumes the Text and\u00a0 \u00a0* LongWritable that the mapper produced. For output, it produces a JsonObject\u00a0 \u00a0* and NullWritable. The JsonObject represents the data that will be\u00a0 \u00a0* loaded into BigQuery.\u00a0 \u00a0*/\u00a0 public static class Reduce\u00a0 \u00a0 \u00a0 extends Reducer<Text, LongWritable, JsonObject, NullWritable> {\u00a0 \u00a0 @Override\u00a0 \u00a0 public void reduce(Text key, Iterable<LongWritable> values, Context context)\u00a0 \u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 \u00a0 // Add up the values to get a total number of occurrences of our word.\u00a0 \u00a0 \u00a0 long count = 0;\u00a0 \u00a0 \u00a0 for (LongWritable val : values) {\u00a0 \u00a0 \u00a0 \u00a0 count = count + val.get();\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 JsonObject jsonObject = new JsonObject();\u00a0 \u00a0 \u00a0 jsonObject.addProperty(\"Word\", key.toString());\u00a0 \u00a0 \u00a0 jsonObject.addProperty(\"Count\", count);\u00a0 \u00a0 \u00a0 // Key does not matter.\u00a0 \u00a0 \u00a0 context.write(jsonObject, NullWritable.get());\u00a0 \u00a0 }\u00a0 }\u00a0 /**\u00a0 \u00a0* Configures and runs the main Hadoop job. Takes a String[] of 5 parameters:\u00a0 \u00a0* [ProjectId] [QualifiedInputTableId] [InputTableFieldName]\u00a0 \u00a0* [QualifiedOutputTableId] [GcsOutputPath]\u00a0 \u00a0*\u00a0 \u00a0* ProjectId - Project under which to issue the BigQuery\u00a0 \u00a0* operations. Also serves as the default project for table IDs that don't\u00a0 \u00a0* specify a project for the table.\u00a0 \u00a0*\u00a0 \u00a0* QualifiedInputTableId - Input table ID of the form\u00a0 \u00a0* (Optional ProjectId):[DatasetId].[TableId]\u00a0 \u00a0*\u00a0 \u00a0* InputTableFieldName - Name of the field to count in the\u00a0 \u00a0* input table, e.g., 'word' in publicdata:samples.shakespeare or\u00a0 \u00a0* 'repository_name' in publicdata:samples.github_timeline.\u00a0 \u00a0*\u00a0 \u00a0* QualifiedOutputTableId - Input table ID of the form\u00a0 \u00a0* (Optional ProjectId):[DatasetId].[TableId]\u00a0 \u00a0*\u00a0 \u00a0* GcsOutputPath - The output path to store temporary\u00a0 \u00a0* Cloud Storage data, e.g., gs://bucket/dir/\u00a0 \u00a0*\u00a0 \u00a0* @param args a String[] containing ProjectId, QualifiedInputTableId,\u00a0 \u00a0* \u00a0 \u00a0 InputTableFieldName, QualifiedOutputTableId, and GcsOutputPath.\u00a0 \u00a0* @throws IOException on IO Error.\u00a0 \u00a0* @throws InterruptedException on Interrupt.\u00a0 \u00a0* @throws ClassNotFoundException if not all classes are present.\u00a0 \u00a0*/\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ClassNotFoundException {\u00a0 \u00a0 // GenericOptionsParser is a utility to parse command line arguments\u00a0 \u00a0 // generic to the Hadoop framework. This example doesn't cover the specifics,\u00a0 \u00a0 // but recognizes several standard command line arguments, enabling\u00a0 \u00a0 // applications to easily specify a NameNode, a ResourceManager, additional\u00a0 \u00a0 // configuration resources, etc.\u00a0 \u00a0 GenericOptionsParser parser = new GenericOptionsParser(args);\u00a0 \u00a0 args = parser.getRemainingArgs();\u00a0 \u00a0 // Make sure we have the right parameters.\u00a0 \u00a0 if (args.length != 5) {\u00a0 \u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Usage: hadoop jar bigquery_wordcount.jar [ProjectId] [QualifiedInputTableId] \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"[InputTableFieldName] [QualifiedOutputTableId] [GcsOutputPath]\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" \u00a0 \u00a0ProjectId - Project under which to issue the BigQuery operations. Also serves \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"as the default project for table IDs that don't explicitly specify a project for \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"the table.\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" \u00a0 \u00a0QualifiedInputTableId - Input table ID of the form \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"(Optional ProjectId):[DatasetId].[TableId]\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" \u00a0 \u00a0InputTableFieldName - Name of the field to count in the input table, e.g., \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"'word' in publicdata:samples.shakespeare or 'repository_name' in \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"publicdata:samples.github_timeline.\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" \u00a0 \u00a0QualifiedOutputTableId - Input table ID of the form \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"(Optional ProjectId):[DatasetId].[TableId]\\n\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \" \u00a0 \u00a0GcsOutputPath - The output path to store temporary Cloud Storage data, e.g., \"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"gs://bucket/dir/\");\u00a0 \u00a0 \u00a0 System.exit(1);\u00a0 \u00a0 }\u00a0 \u00a0 // Get the individual parameters from the command line.\u00a0 \u00a0 String projectId = args[0];\u00a0 \u00a0 String inputQualifiedTableId = args[1];\u00a0 \u00a0 String inputTableFieldId = args[2];\u00a0 \u00a0 String outputQualifiedTableId = args[3];\u00a0 \u00a0 String outputGcsPath = args[4];\u00a0 \u00a0// Define the schema we will be using for the output BigQuery table.\u00a0 \u00a0 List<TableFieldSchema> outputTableFieldSchema = new ArrayList<TableFieldSchema>();\u00a0 \u00a0 outputTableFieldSchema.add(new TableFieldSchema().setName(\"Word\").setType(\"STRING\"));\u00a0 \u00a0 outputTableFieldSchema.add(new TableFieldSchema().setName(\"Count\").setType(\"INTEGER\"));\u00a0 \u00a0 TableSchema outputSchema = new TableSchema().setFields(outputTableFieldSchema);\u00a0 \u00a0 // Create the job and get its configuration.\u00a0 \u00a0 Job job = new Job(parser.getConfiguration(), \"wordcount\");\u00a0 \u00a0 Configuration conf = job.getConfiguration();\u00a0 \u00a0 // Set the job-level projectId.\u00a0 \u00a0 conf.set(BigQueryConfiguration.PROJECT_ID_KEY, projectId);\u00a0 \u00a0 // Configure input.\u00a0 \u00a0 BigQueryConfiguration.configureBigQueryInput(conf, inputQualifiedTableId);\u00a0 \u00a0 // Configure output.\u00a0 \u00a0 BigQueryOutputConfiguration.configure(\u00a0 \u00a0 \u00a0 \u00a0 conf,\u00a0 \u00a0 \u00a0 \u00a0 outputQualifiedTableId,\u00a0 \u00a0 \u00a0 \u00a0 outputSchema,\u00a0 \u00a0 \u00a0 \u00a0 outputGcsPath,\u00a0 \u00a0 \u00a0 \u00a0 BigQueryFileFormat.NEWLINE_DELIMITED_JSON,\u00a0 \u00a0 \u00a0 \u00a0 TextOutputFormat.class);\u00a0 \u00a0 // (Optional) Configure the KMS key used to encrypt the output table.\u00a0 \u00a0 BigQueryOutputConfiguration.setKmsKeyName(\u00a0 \u00a0 \u00a0 \u00a0 conf,\u00a0 \u00a0 \u00a0 \u00a0 \"projects/myproject/locations/us-west1/keyRings/r1/cryptoKeys/k1\");\u00a0 \u00a0 conf.set(WORDCOUNT_WORD_FIELDNAME_KEY, inputTableFieldId);\u00a0 \u00a0 // This helps Hadoop identify the Jar which contains the mapper and reducer\u00a0 \u00a0 // by specifying a class in that Jar. This is required if the jar is being\u00a0 \u00a0 // passed on the command line to Hadoop.\u00a0 \u00a0 job.setJarByClass(WordCount.class);\u00a0 \u00a0 // Tell the job what data the mapper will output.\u00a0 \u00a0 job.setOutputKeyClass(Text.class);\u00a0 \u00a0 job.setOutputValueClass(LongWritable.class);\u00a0 \u00a0 job.setMapperClass(Map.class);\u00a0 \u00a0 job.setReducerClass(Reduce.class);\u00a0 \u00a0 job.setInputFormatClass(GsonBigQueryInputFormat.class);\u00a0 \u00a0 // Instead of using BigQueryOutputFormat, we use the newer\u00a0 \u00a0 // IndirectBigQueryOutputFormat, which works by first buffering all the data\u00a0 \u00a0 // into a Cloud Storage temporary file, and then on commitJob, copies all data from\u00a0 \u00a0 // Cloud Storage into BigQuery in one operation. Its use is recommended for large jobs\u00a0 \u00a0 // since it only requires one BigQuery \"load\" job per Hadoop/Spark job, as\u00a0 \u00a0 // compared to BigQueryOutputFormat, which performs one BigQuery job for each\u00a0 \u00a0 // Hadoop/Spark task.\u00a0 \u00a0 job.setOutputFormatClass(IndirectBigQueryOutputFormat.class);\u00a0 \u00a0 job.waitForCompletion(true);\u00a0 \u00a0 // After the job completes, clean up the Cloud Storage export paths.\u00a0 \u00a0 GsonBigQueryInputFormat.cleanupJob(job.getConfiguration(), job.getJobID());\u00a0 \u00a0 // You can view word counts in the BigQuery output table at\u00a0 \u00a0 // https://console.cloud.google.com/.\u00a0 }}\n```\n## Java version\nThe BigQuery connector requires Java 8.\n### Apache Maven Dependency Information\n```\n<dependency>\u00a0 \u00a0 <groupId>com.google.cloud.bigdataoss</groupId>\u00a0 \u00a0 <artifactId>bigquery-connector</artifactId>\u00a0 \u00a0 <version>insert \"hadoopX-X.X.X\" connector version number here</version></dependency>\n```\nFor detailed information, see the BigQuery connector [release notes](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases) and [Javadoc reference](http://www.javadoc.io/doc/com.google.cloud.bigdataoss/bigquery-connector/) .", "guide": "Dataproc"}