{"title": "Dataproc - Use Ranger with Kerberos", "url": "https://cloud.google.com/dataproc/docs/concepts/components/ranger-w-kerberos", "abstract": "# Dataproc - Use Ranger with Kerberos\nThe following examples create and use a [Kerberos enabled](/dataproc/docs/concepts/configuring-clusters/security) Dataproc cluster with [Ranger](/dataproc/docs/concepts/components/ranger) and [Solr](/dataproc/docs/concepts/components/solr) components to control access by users to Hadoop, YARN, and HIVE resources.\nNotes:\n- The Ranger Web UI can be accessed through the [Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) .\n- In a Ranger with Kerberos cluster, Dataproc maps a Kerberos user to the system user by stripping the Kerberos user's realm and instance. For example, Kerberos principal `user1/cluster-m@MY.REALM` is mapped to system `user1` , and Ranger policies are defined to allow or deny permissions for `user1` .\n- [Set up the Ranger admin password](/dataproc/docs/concepts/components/ranger#installation_steps) .\n- [Set up the Kerberos root principal password](/dataproc/docs/concepts/configuring-clusters/security#set_up_your_kerberos_root_principal_password) .\n- Create the cluster.- The following`gcloud`command can be run in a local terminal window or from a project's [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) .```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=SOLR,RANGER \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway \\\n\u00a0\u00a0\u00a0\u00a0--properties=\"dataproc:ranger.kms.key.uri=projects/project-id/locations/global/keyRings/keyring/cryptoKeys/key,dataproc:ranger.admin.password.uri=gs://bucket/admin-password.encrypted\" \\\n\u00a0\u00a0\u00a0\u00a0--kerberos-root-principal-password-uri=gs://bucket/kerberos-root-principal-password.encrypted \\\n\u00a0\u00a0\u00a0\u00a0--kerberos-kms-key=projects/project-id/locations/global/keyRings/keyring/cryptoKeys/key\n```\n- After the cluster is running, navigate to the Dataproc [Clusters](https://console.cloud.google.com/dataproc/clusters) page on Google Cloud console, then select the cluster's name to open the **Cluster details** page. Click the **Web Interfaces** tab to display a list of Component Gateway links to the web interfaces of [default and optional components](/dataproc/docs/concepts/components/overview) installed on the cluster. Click the Ranger link.\n- Sign in to Ranger by entering the \"admin\" username and the Ranger admin password.\n- The Ranger admin UI opens in a local browser.\nThe following examples create Ranger policies to allow or deny access to two OS users and Kerberos principals:`userone`and`usertwo`.\n#", "content": "## YARN access policy\nThis example creates a Ranger policy to allow and deny user access to the [YARN root.default queue](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/CapacityScheduler.html#Configuration) .\n- Select `yarn-dataproc` from the Ranger Admin UI.\n- On the **yarn-dataproc Policies** page, click **Add New Policy** . On the **Create Policy** page, the following fields are entered or selected:- `Policy Name`: \"yarn-policy-1\"\n- `Queue`: \"root.default\"\n- `Audit Logging`: \"Yes\"\n- `Allow Conditions`:- `Select User`: \"userone\"\n- `Permissions`: \"Select All\" to grant all permissions\n- `Deny Conditions` :- `Select User`: \"usertwo\"\n- `Permissions`: \"Select All\" to deny all permissions\nClick **Add** to save the policy. The policy is listed on the **yarn-dataproc Policies** page:\n- Run a Hadoop mapreduce job in the master SSH session window as userone:```\nuserone@example-cluster-m:~$ hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduced-examples.\njar pi 5 10\n```- The Ranger UI shows that`userone`was allowed to submit the job.\n- Run the Hadoop mapreduce job from the VM master SSH session  window as `usertwo` :```\nusertwo@example-cluster-m:~$ hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduced-examples.\njar pi 5 10\n```- The Ranger UI shows that`usertwo`was denied access to submit the job.\n### HDFS access policy\nThis example creates a Ranger policy to allow and deny user access to the HDFS `/tmp` directory.\n- Select `hadoop-dataproc` from the Ranger Admin UI.\n- On the **hadoop-dataproc Policies** page, click **Add New Policy** . On the **Create Policy** page, the following fields are entered or selected:- `Policy Name`: \"hadoop-policy-1\"\n- `Resource Path`: \"/tmp\"\n- `Audit Logging`: \"Yes\"\n- `Allow Conditions`:- `Select User`: \"userone\"\n- `Permissions`: \"Select All\" to grant all permissions\n- `Deny Conditions` :- `Select User`: \"usertwo\"\n- `Permissions`: \"Select All\" to deny all permissions\nClick **Add** to save the policy. The policy is listed on the **hadoop-dataproc Policies** page:\n- Access the HDFS `/tmp` directory as userone:```\nuserone@example-cluster-m:~$ hadoop fs -ls /tmp\n```- The Ranger UI shows that`userone`was allowed access to the HDFS /tmp directory.\n- Access the HDFS `/tmp` directory as `usertwo` :```\nusertwo@example-cluster-m:~$ hadoop fs -ls /tmp\n```- The Ranger UI shows that`usertwo`was denied access to the HDFS /tmp directory.\n### Hive access policy\nThis example creates a Ranger policy to allow and deny user access to a Hive table.\n- Create a small `employee` table using the hive CLI on the master instance.```\nhive> CREATE TABLE IF NOT EXISTS employee (eid int, name String); INSERT INTO employee VALUES (1 , 'bob') , (2 , 'alice'), (3 , 'john');\n```\n- Select `hive-dataproc` from the Ranger Admin UI.\n- On the **hive-dataproc Policies** page, click **Add New Policy** . On the **Create Policy** page, the following fields are entered or selected:- `Policy Name`: \"hive-policy-1\"\n- `database`: \"default\"\n- `table`: \"employee\"\n- `Hive Column`: \"*\"\n- `Audit Logging`: \"Yes\"\n- `Allow Conditions`:- `Select User`: \"userone\"\n- `Permissions`: \"Select All\" to grant all permissions\n- `Deny Conditions` :- `Select User`: \"usertwo\"\n- `Permissions`: \"Select All\" to deny all permissions\nClick **Add** to save the policy. The policy is listed on the **hive-dataproc Policies** page:\n- Run a query from the VM master SSH session against Hive employee table as userone:```\nuserone@example-cluster-m:~$ beeline -u \"jdbc:hive2://$(hostname -f):10000/default;principal=hive/$(hostname -f)@REALM\" -e \"select * from employee;\"\n```- The userone query succeeds:```\nConnected to: Apache Hive (version 2.3.6)\nDriver: Hive JDBC (version 2.3.6)\nTransaction isolation: TRANSACTION_REPEATABLE_READ\n+---------------+----------------+\n| employee.eid | employee.name |\n+---------------+----------------+\n| 1    | bob   |\n| 2    | alice   |\n| 3    | john   |\n+---------------+----------------+\n3 rows selected (2.033 seconds)\n```\n- Run a query from the VM master SSH session against Hive employee table as usertwo:```\nusertwo@example-cluster-m:~$ beeline -u \"jdbc:hive2://$(hostname -f):10000/default;principal=hive/$(hostname -f)@REALM\" -e \"select * from employee;\"\n```- usertwo is denied access to the table:```\nError: Could not open client transport with JDBC Uri:\n...\nPermission denied: user=usertwo, access=EXECUTE, inode=\"/tmp/hive\"\n```\n### Fine-Grained Hive Access\nRanger supports Masking and Row Level Filters on Hive. This example builds on the previous `hive-policy-1` by adding masking and filter policies.\n- Select `hive-dataproc` from the Ranger Admin UI, then select the **Masking** tab and click **Add New Policy** .- On the **Create Policy** page, the following fields are entered or selected to create a policy to mask (nullify) the employee name column.:- `Policy Name`: \"hive-masking policy\"\n- `database`: \"default\"\n- `table`: \"employee\"\n- `Hive Column`: \"name\"\n- `Audit Logging`: \"Yes\"\n- `Mask Conditions`:- `Select User`: \"userone\"\n- `Access Types`: \"select\" add/edit permissions\n- `Select Masking Option`: \"nullify\"Click **Add** to save the policy.\n- Select `hive-dataproc` from the Ranger Admin UI, then select the **Row Level Filter** tab and click **Add New Policy** .- On the **Create Policy** page, the following fields are entered or selected to create a policy to filter (return) rows where `eid` is not equal to `1` :- `Policy Name`: \"hive-filter policy\"\n- `Hive Database`: \"default\"\n- `Hive Table`: \"employee\"\n- `Audit Logging`: \"Yes\"\n- `Mask Conditions`:- `Select User`: \"userone\"\n- `Access Types`: \"select\" add/edit permissions\n- `Row Level Filter`: \"eid != 1\" filter expressionClick **Add** to save the policy.\n- Repeat the previous query from the VM master SSH session against Hive employee table as userone:```\nuserone@example-cluster-m:~$ beeline -u \"jdbc:hive2://$(hostname -f):10000/default;principal=hive/$(hostname -f)@REALM\" -e \"select * from employee;\"\n```- The query returns with the name column masked out and bob (eid=1) filtered from the results.:```\nTransaction isolation: TRANSACTION_REPEATABLE_READ\n+---------------+----------------+\n| employee.eid | employee.name |\n+---------------+----------------+\n| 2    | NULL   |\n| 3    | NULL   |\n+---------------+----------------+\n2 rows selected (0.47 seconds)\n```", "guide": "Dataproc"}