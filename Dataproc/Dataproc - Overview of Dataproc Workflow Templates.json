{"title": "Dataproc - Overview of Dataproc Workflow Templates", "url": "https://cloud.google.com/dataproc/docs/concepts/workflows/overview", "abstract": "# Dataproc - Overview of Dataproc Workflow Templates\nThe Dataproc [WorkflowTemplates API](/dataproc/docs/reference/rest/v1/projects.regions.workflowTemplates) provides a flexible and easy-to-use mechanism for managing and executing workflows. A Workflow Template is a reusable workflow configuration. It defines a graph of jobs with information on where to run those jobs.\n**Key Points:**\n- [Instantiating a Workflow Template](/dataproc/docs/concepts/workflows/using-workflows#running_a_workflow) launches a Workflow. A Workflow is an operation that runs a [Directed Acyclic Graph (DAG)](https://en.wikipedia.org/wiki/Directed_acyclic_graph) of jobs on a cluster.- If the workflow uses a [managed cluster](#managed_cluster) , it creates the cluster, runs the jobs, and then deletes the cluster when the jobs are finished.\n- If the workflow uses a [cluster selector](#cluster_selector) , it runs jobs on a selected existing cluster.\n- Workflows are ideal for complex job flows. You can create job dependencies so that a job starts only after its dependencies complete successfully.\n- When you [create a workflow template](/dataproc/docs/concepts/workflows/using-workflows#creating_a_template) Dataproc does not create a cluster or submit jobs to a cluster. Dataproc creates or selects a cluster and runs workflow jobs on the cluster when a workflow template is **instantiated** .", "content": "## Kinds of Workflow Templates\n### Managed cluster\nA workflow template can specify a managed cluster. The workflow will create an \"ephemeral\" cluster to run workflow jobs, and then delete the cluster when the workflow is finished.\n### Cluster selector\nA workflow template can specify an existing cluster on which to run workflow jobs by specifying one or more [user labels](/dataproc/docs/concepts/labels) previously attached to the cluster. The workflow will run on a cluster that matches all of the labels. If multiple clusters match all labels, Dataproc selects the cluster with the most YARN available memory to run all workflow jobs. At the end of workflow, Dataproc does not delete the selected cluster. See [Use cluster selectors with workflows](/dataproc/docs/concepts/workflows/cluster-selectors) for more information.\nA workflow can select a specific cluster by matching the`goog-dataproc-cluster-name`label (see [Using Automatically Applied Labels](/dataproc/docs/concepts/workflows/cluster-selectors#using_automatically_applied_labels) ).\n### Parameterized\nIf you will run a workflow template multiple times with different values, use parameters to avoid editing the workflow template for each run:\n- define parameters in the template, then\n- pass different values for the parameters for each run.\nSee [Parameterization of Workflow Templates](/dataproc/docs/concepts/workflows/workflow-parameters) for more information.\n### Inline\nWorkflows can be instantiated inline using the `gcloud` command with [workflow template YAML files](/dataproc/docs/concepts/workflows/using-yamls#instantiate_a_workflow_using_a_yaml_file) or by calling the Dataproc [InstantiateInline](/dataproc/docs/reference/rest/v1/projects.regions.workflowTemplates/instantiateInline) API (see [Using inline Dataproc workflows](/dataproc/docs/concepts/workflows/inline-workflows) ). Inline workflows do not create or modify workflow template resources.\nInline workflows can be useful for rapid prototyping or automation.\n## Workflow Template use cases\n- **Automation of repetitive tasks.** Workflows encapsulate frequently used cluster configurations and jobs.\n- **Transactional fire-and-forget API interaction model.** Workflow Templates replace the steps involved in a typical flow, which include:- creating the cluster\n- submitting jobs\n- polling\n- deleting the cluster\nWorkflow Templates use a single token to track progress from cluster creation to deletion, and automate error handling and recovery. They also simplify the integration of Dataproc with other tools, such as Cloud Functions and Cloud Composer.\n- **Support for ephemeral and long-lived clusters.** A common complexity associated with running Apache Hadoop is tuning and right-sizing clusters. Ephemeral (managed) clusters are easier to configure since they run a single workload. Cluster selectors can be used with longer-lived clusters to repeatedly execute the same workload without incurring the amortized cost of creating and deleting clusters.\n- **Granular IAM security.** Creating Dataproc clusters and submitting jobs require all-or-nothing IAM permissions. Workflow Templates use a per-template [workflowTemplates.instantiate](/dataproc/docs/concepts/iam/iam#workflow_template_permissions) permission, and do not depend on cluster or job permissions.", "guide": "Dataproc"}