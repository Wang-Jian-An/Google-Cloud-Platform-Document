{"title": "Dataproc - Stream a Kafka topic to Hive", "url": "https://cloud.google.com/dataproc/docs/tutorials/dataproc-kafka-tutorial", "abstract": "# Dataproc - Stream a Kafka topic to Hive\n[Apache Kafka](https://kafka.apache.org/documentation/) is an open source distributed streaming platform for real-time data pipelines and data integration. It provides an efficient and scalable streaming system for use in a variety of applications, including:- Real-time analytics\n- Stream processing\n- Log aggregation\n- Distributed messaging\n- Event streaming\nSee [Kafka use cases](https://kafka.apache.org/uses) .", "content": "## Objectives\n- Install Kafka on a [Dataproc HA cluster](/dataproc/docs/concepts/configuring-clusters/high-availability#apache_zookeeper) with ZooKeeper (referred to in this tutorial as a \"Dataproc Kafka cluster\").\n- Create fictitious customer data, then publish the data to a Kafka topic.\n- Create Hive parquet and ORC tables in Cloud Storage to receive streamed Kafka topic data.\n- Submit a PySpark job to subscribe to and stream the Kafka topic into Cloud Storage in parquet and ORC format.\n- Run a query on the streamed Hive table data to count the streamed Kafka messages.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Dataproc](/dataproc/pricing) \n- [Compute Engine](/compute/pricing) \n- [Cloud Storage](/storage/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you beginIf you haven't already done so, create a Google Cloud project.- In the Google Cloud console, go to the Cloud Storage **Buckets** page. [Go to Buckets page](https://console.cloud.google.com/storage/browser) \n- Click **Create bucket** .\n- On the **Create a bucket** page, enter your bucket information. To go to the next step, click **Continue** .- For **Name your bucket** , enter a name that meets the [bucket naming requirements](/storage/docs/bucket-naming#requirements) .\n- For **Choose where to store your data** , do the following:- Select a **Location type** option.\n- Select a **Location** option.\n- For **Choose a default storage class for your data** , select a [storage class](/storage/docs/storage-classes) .\n- For **Choose how to control access to objects** , select an **Access control** option.\n- For **Advanced settings (optional)** , specify  an [encryption method](/storage/docs/encryption) ,  a [retention policy](/storage/docs/bucket-lock) ,  or [bucket labels](/storage/docs/tags-and-labels#bucket-labels) .\n- Click **Create** .\n## Tutorial stepsPerform the following steps to create a Dataproc Kafka cluster to read a Kafka topic into Cloud Storage in parquet OR ORC format.\n **Note:** In a production environment, some of tasks listed in the following sections are performed on separate clusters. For purposes of this tutorial, all tasks are performed on a single Dataproc Kafka cluster.\n### Copy the Kafka installation script to Cloud StorageThe `kafka.sh` [initialization action](/dataproc/docs/tutorials/dataproc/docs/concepts/configuring-clusters/init-actions) script installs Kafka on a Dataproc cluster.- Browse the code. \n- Copy the `kafka.sh` [initialization action](/dataproc/docs/concepts/configuring-clusters/init-actions) script to your Cloud Storage bucket. This script installs Kafka on a Dataproc cluster.- Open [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) , then run the following command:```\ngsutil cp gs://goog-dataproc-initialization-actions-REGION/kafka/kafka.sh gs://BUCKET_NAME/scripts/\n```Make the following replacements:- :`kafka.sh`is stored in public regionally-tagged buckets in Cloud Storage. Specify a geographically close [Compute Engine region](/compute/docs/regions-zones#available) , (example:`us-central1`).\n- : The name of your Cloud Storage bucket.\n **Note:** The `kafka.sh` script is available from the GitHub [initialization-actions/kafka/](https://github.com/GoogleCloudDataproc/initialization-actions/blob/master/kafka/kafka.sh) repository. You can clone or download the script onto your local machine, then modify the source specified in the previous `gsutil cp` example to copy the local script to `gs://` `` `/scripts/` .\n### Create a Dataproc Kafka cluster\n- Open [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) , then run the following [gcloud dataproc clusters create](/sdk/gcloud/reference/dataproc/clusters/create) command to create a Dataproc [HA cluster](/dataproc/docs/concepts/configuring-clusters/high-availability) cluster that installs the Kafka and ZooKeeper components:```\ngcloud dataproc clusters create KAFKA_CLUSTER \\\n\u00a0\u00a0\u00a0\u00a0--project=PROJECT_ID \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--image-version=2.1-debian11 \\\n\u00a0\u00a0\u00a0\u00a0--num-masters=3 \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway \\\n\u00a0\u00a0\u00a0\u00a0--initialization-actions=gs://BUCKET_NAME/scripts/kafka.sh\n```Notes:- : The cluster name, which must be unique within a project. The name must start with a lowercase letter, and can contain up to 51 lowercase letters, numbers, and hyphens. It cannot end with a hyphen. The name of a deleted cluster can be reused.\n- : The project to associate with this cluster.\n- : The [Compute Engine region](/compute/docs/regions-zones#available) where the cluster will be located, such as`us-central1`.- You can add the optional`--zone=` ``flag to specify a zone within the specified region, such as`us-central1-a`. If you do not specify a zone, the Dataproc [autozone placement](/dataproc/docs/concepts/configuring-clusters/auto-zone) feature selects a zone with the specified region.\n- `--image-version`: Dataproc image version [2.1-debian11](/dataproc/docs/concepts/versioning/dataproc-release-2.1) is recommended for this tutorial. Note: Each image version contains a set of pre-installed components, including the Hive component used in this tutorial (see [Supported Dataproc image versions](/dataproc/docs/concepts/versioning/dataproc-version-clusters#supported_dataproc_versions) ).\n- `--num-master`:`3`master nodes create an [HA cluster](/dataproc/docs/concepts/configuring-clusters/high-availability) . The Zookeeper component, which is required by Kafka, is pre-installed on an HA cluster.\n- `--enable-component-gateway`: Enables the [Dataproc Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) .\n- : The name of your Cloud Storage bucket that contains the`/scripts/kafka.sh`initialization script (see [Copy the Kafka installation script to Cloud Storage](#copy_the_kafka_installation_script_to) ).\n### Create a Kafka custdata topicTo create a Kafka topic on the Dataproc Kafka cluster:- Use the [SSH](/dataproc/docs/concepts/accessing/ssh#console) utility to open a terminal window on the cluster master VM.\n- Create a Kafka `custdata` topic.```\n/usr/lib/kafka/bin/kafka-topics.sh \\\n\u00a0\u00a0\u00a0\u00a0--bootstrap-server KAFKA_CLUSTER-w-0:9092 \\\n\u00a0\u00a0\u00a0\u00a0--create --topic custdata\n```Notes:- : Insert the name of your Kafka cluster. `-w-0:9092` signifies the Kafka broker running on port `9092` on the `worker-0` node.\n- You can run the following commands after creating the `custdata` topic:```\n# List all topics.\n/usr/lib/kafka/bin/kafka-topics.sh \\\n\u00a0\u00a0\u00a0\u00a0--bootstrap-server KAFKA_CLUSTER-w-0:9092 \\\n\u00a0\u00a0\u00a0\u00a0--list\n# Consume then display topic data.\n/usr/lib/kafka/bin/kafka-console-consumer.sh \\\n\u00a0\u00a0\u00a0\u00a0--bootstrap-server KAFKA_CLUSTER-w-0:9092 \\\n\u00a0\u00a0\u00a0\u00a0--topic custdata\n# Count the number of messages in the topic.\n/usr/lib/kafka/bin/kafka-run-class.sh kafka.tools.GetOffsetShell \\\n\u00a0\u00a0\u00a0\u00a0--broker-list KAFKA_CLUSTER-w-0:9092 \\\n\u00a0\u00a0\u00a0\u00a0--topic custdata\n# Delete topic.\n/usr/lib/kafka/bin/kafka-topics.sh \\\n\u00a0\u00a0\u00a0\u00a0--bootstrap-server KAFKA_CLUSTER-w-0:9092 \\\n\u00a0\u00a0\u00a0\u00a0--delete --topic custdata\n```\n### Publish content to the Kafka custdata topicThe following script uses the `kafka-console-producer.sh` Kafka tool to generate fictitious customer data in CSV format.- Copy, then paste the script in the SSH terminal on the master node of your Kafka cluster. Press **<return>** to run the script.```\nfor i in {1..10000}; do \\\ncustname=\"cust name${i}\"\nuuid=$(dbus-uuidgen)\nage=$((45 + $RANDOM % 45))\namount=$(echo \"$(( $RANDOM % 99999 )).$(( $RANDOM % 99 ))\")\nmessage=\"${uuid}:${custname},${age},${amount}\"\necho ${message}\ndone | /usr/lib/kafka/bin/kafka-console-producer.sh \\\n--broker-list KAFKA_CLUSTER-w-0:9092 \\\n--topic custdata \\\n--property \"parse.key=true\" \\\n--property \"key.separator=:\"\n```Notes:- : The name of your Kafka cluster.\nYou can publish data to a Kafka topic from many sources, such as web-sites, applications, and devices, in different formats, such as CSV, JSON, text, and Apache Avro serialization format (see [Kafka Use Cases](https://kafka.apache.org/uses) ).\n- Run the following Kafka command to confirm the `custdata` topic contains 10,000 messages.```\n/usr/lib/kafka/bin/kafka-run-class.sh kafka.tools.GetOffsetShell \\\n--broker-list KAFKA_CLUSTER-w-0:9092 \\\n--topic custdata\n```Notes:- : The name of your Kafka cluster.\nExpected output:```\ncustdata:0:10000\n```\n### Create Hive tables in Cloud StorageCreate Hive tables to receive streamed Kafka topic data. Perform the following steps to create `cust_parquet` (parquet) and a `cust_orc` (ORC) Hive tables in your Cloud Storage bucket.- Insert your in the following script, then copy and paste the script into the SSH terminal on your Kafka cluster master node, then press **<return>** to create a `~/hivetables.hql` (Hive Query Language) script.You will run the `~/hivetables.hql` script in the next step to create parquet and ORC Hive tables in your Cloud Storage bucket.```\ncat > ~/hivetables.hql <<EOF\ndrop table if exists cust_parquet;\ncreate external table if not exists cust_parquet\n(uuid string, custname string, age string, amount string)\nrow format delimited fields terminated by ','\nstored as parquet\nlocation \"gs://BUCKET_NAME/tables/cust_parquet\";\ndrop table if exists cust_orc;\ncreate external table if not exists cust_orc\n(uuid string, custname string, age string, amount string)\nrow format delimited fields terminated by ','\nstored as orc\nlocation \"gs://BUCKET_NAME/tables/cust_orc\";\nEOF\n```\n- In the SSH terminal on the master node of your Kafka cluster, submit the `~/hivetables.hql` Hive job to create `cust_parquet` (parquet) and a `cust_orc` (ORC) Hive tables in your Cloud Storage bucket.```\ngcloud dataproc jobs submit hive \\\n\u00a0\u00a0\u00a0\u00a0--cluster=KAFKA_CLUSTER \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0-f ~/hivetables.hql\n```Notes:- The Hive component is pre-installed on the Dataproc Kafka cluster. See [2.1.x release versions](/dataproc/docs/concepts/versioning/dataproc-release-2.1) for a list of the Hive component versions included in recently released 2.1 images.\n- : The name of your Kafka cluster.\n- : The region where your Kafka cluster is located.\n### Stream Kafka custdata to Hive tables\n- Run the following command in the in the SSH terminal on the master node of your Kafka cluster to install the`kafka-python`library. A Kafka client is needed to stream Kafka topic data to Cloud Storage.Typically, the Kafka client runs on a separate machine, but this tutorial uses one cluster for all processes for simplicity and to save costs.```\npip install kafka-python\n```\n- Insert your , then copy then paste the following PySpark code into the SSH terminal on your Kafka cluster master node, and then press **<return>** to create a `streamdata.py` file.The script subscribes to the Kafka `custdata` topic, then streams the data to your Hive tables in Cloud Storage. The output format, which can be parquet or ORC, is passed into the script as a parameter. **Note:** The Kafka topic data streamed to Cloud Storage is in compressed binary format.```\ncat > streamdata.py <<EOF#!/bin/pythonimport sysfrom pyspark.sql.functions import *from pyspark.sql.types import *from pyspark.sql import SparkSessionfrom kafka import KafkaConsumerdef getNameFn (data): return data.split(\",\")[0]def getAgeFn \u00a0(data): return data.split(\",\")[1]def getAmtFn \u00a0(data): return data.split(\",\")[2]def main(cluster, outputfmt):\u00a0 \u00a0 spark = SparkSession.builder.appName(\"APP\").getOrCreate()\u00a0 \u00a0 spark.sparkContext.setLogLevel(\"WARN\")\u00a0 \u00a0 Logger = spark._jvm.org.apache.log4j.Logger\u00a0 \u00a0 logger = Logger.getLogger(__name__)\u00a0 \u00a0 rows = spark.readStream.format(\"kafka\") \\\u00a0 \u00a0 .option(\"kafka.bootstrap.servers\", cluster+\"-w-0:9092\").option(\"subscribe\", \"custdata\") \\\u00a0 \u00a0 .option(\"startingOffsets\", \"earliest\")\\\u00a0 \u00a0 .load()\u00a0 \u00a0 getNameUDF = udf(getNameFn, StringType())\u00a0 \u00a0 getAgeUDF \u00a0= udf(getAgeFn, \u00a0StringType())\u00a0 \u00a0 getAmtUDF \u00a0= udf(getAmtFn, \u00a0StringType())\u00a0 \u00a0 logger.warn(\"Params passed in are cluster name: \" + cluster + \" \u00a0output format(sink): \" + outputfmt)\u00a0 \u00a0 query = rows.select (col(\"key\").cast(\"string\").alias(\"uuid\"),\\\u00a0 \u00a0 \u00a0 \u00a0 getNameUDF \u00a0 \u00a0 \u00a0(col(\"value\").cast(\"string\")).alias(\"custname\"),\\\u00a0 \u00a0 \u00a0 \u00a0 getAgeUDF \u00a0 \u00a0 \u00a0 (col(\"value\").cast(\"string\")).alias(\"age\"),\\\u00a0 \u00a0 \u00a0 \u00a0 getAmtUDF \u00a0 \u00a0 \u00a0 (col(\"value\").cast(\"string\")).alias(\"amount\"))\u00a0 \u00a0 writer = query.writeStream.format(outputfmt)\\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .option(\"path\",\"gs://BUCKET_NAME/tables/cust_\"+outputfmt)\\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .option(\"checkpointLocation\", \"gs://BUCKET_NAME/chkpt/\"+outputfmt+\"wr\") \\\u00a0 \u00a0 \u00a0 \u00a0 .outputMode(\"append\")\\\u00a0 \u00a0 \u00a0 \u00a0 .start()\u00a0 \u00a0 writer.awaitTermination()if __name__==\"__main__\":\u00a0 \u00a0 if len(sys.argv) < 2:\u00a0 \u00a0 \u00a0 \u00a0 print (\"Invalid number of arguments passed \", len(sys.argv))\u00a0 \u00a0 \u00a0 \u00a0 print (\"Usage: \", sys.argv[0], \" cluster \u00a0format\")\u00a0 \u00a0 \u00a0 \u00a0 print (\"e.g.: \u00a0\", sys.argv[0], \" <cluster_name> \u00a0orc\")\u00a0 \u00a0 \u00a0 \u00a0 print (\"e.g.: \u00a0\", sys.argv[0], \" <cluster_name> \u00a0parquet\")\u00a0 \u00a0 main(sys.argv[1], sys.argv[2])EOF\n```\n- In the SSH terminal on the master node of your Kafka cluster, run `spark-submit` to stream data to your Hive tables in Cloud Storage.- Insert the name of your and the output , then copy and paste the following code into the SSH terminal on the master node of your Kafka cluster, and then press **<return>** to run the code and stream the Kafka `custdata` data in parquet format to your Hive tables in Cloud Storage.```\nspark-submit --packages \\\norg.apache.spark:spark-streaming-kafka-0-10_2.12:3.1.3,org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.3 \\\n\u00a0\u00a0\u00a0\u00a0--conf spark.history.fs.gs.outputstream.type=FLUSHABLE_COMPOSITE \\\n\u00a0\u00a0\u00a0\u00a0--conf spark.driver.memory=4096m \\\n\u00a0\u00a0\u00a0\u00a0--conf spark.executor.cores=2 \\\n\u00a0\u00a0\u00a0\u00a0--conf spark.executor.instances=2 \\\n\u00a0\u00a0\u00a0\u00a0--conf spark.executor.memory=6144m \\\n\u00a0\u00a0\u00a0\u00a0streamdata.py KAFKA_CLUSTER FORMAT\n \n```Notes:- : Insert the name of your Kafka cluster.\n- : Specify either`parquet`or`orc`as the output format. You can run the command successively to stream both formats to the Hive tables: for example, in the first invocation, specify`parquet`to stream the Kafka`custdata`topic to the Hive parquet table; then, in second invocation, specify`orc`format to stream`custdata`to the Hive ORC table.\n- After standard output halts in the SSH terminal, which signifies that all of the `custdata` has been streamed, press **<control-c>** in the SSH terminal to stop the process.\n- List the Hive tables in Cloud Storage.```\ngsutil ls -r gs://BUCKET_NAME/tables/*\n```Notes:- : Insert the name of the Cloud Storage bucket that contains your Hive tables (see [Create Hive tables](#create_hive_tables) ).\n### Query streamed data\n- In the SSH terminal on the master node of your Kafka cluster, run the following `hive` command to count the streamed Kafka `custdata` messages in the Hive tables in Cloud Storage.```\nhive -e \"select count(1) from TABLE_NAME\"\n```Notes:- : Specify either`cust_parquet`or`cust_orc`as the Hive table name.\nExpected output snippet:\n```\n...Status: Running (Executing on YARN cluster with App id application_....)----------------------------------------------------------------------------------------------\u00a0 \u00a0 \u00a0 \u00a0 VERTICES \u00a0 \u00a0 \u00a0MODE \u00a0 \u00a0 \u00a0 \u00a0STATUS \u00a0TOTAL \u00a0COMPLETED \u00a0RUNNING \u00a0PENDING \u00a0FAILED \u00a0KILLED \u00a0----------------------------------------------------------------------------------------------Map 1 .......... container \u00a0 \u00a0 SUCCEEDED \u00a0 \u00a0 \u00a01 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01 \u00a0 \u00a0 \u00a0 \u00a00 \u00a0 \u00a0 \u00a0 \u00a00 \u00a0 \u00a0 \u00a0 0 \u00a0 \u00a0 \u00a0 0Reducer 2 ...... container \u00a0 \u00a0 SUCCEEDED \u00a0 \u00a0 \u00a01 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01 \u00a0 \u00a0 \u00a0 \u00a00 \u00a0 \u00a0 \u00a0 \u00a00 \u00a0 \u00a0 \u00a0 0 \u00a0 \u00a0 \u00a0 0----------------------------------------------------------------------------------------------VERTICES: 02/02 \u00a0[==========================>>] 100% \u00a0ELAPSED TIME: 9.89 s \u00a0 \u00a0 ----------------------------------------------------------------------------------------------OK10000Time taken: 21.394 seconds, Fetched: 1 row(s)\n```## Clean up\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- Delete a Google Cloud project:\n- ```\ngcloud projects delete PROJECT_ID\n```\n### Delete resources\n- Delete the bucket:```\ngcloud storage buckets delete BUCKET_NAME\n``` **Important:** Your bucket must  be empty before you can delete it.\n- Delete your Kafka cluster:```\ngcloud dataproc clusters delete KAFKA_CLUSTER \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION}\n```", "guide": "Dataproc"}