{"title": "Dataproc - Dataproc driver node groups", "url": "https://cloud.google.com/dataproc/docs/guides/node-groups/dataproc-driver-node-groups", "abstract": "# Dataproc - Dataproc driver node groups\n", "content": "## Overview\nA Dataproc `NodeGroup` resource is a group of Dataproc cluster nodes that execute an assigned role. This page describes the **driver node group** , which is a group of Compute Engine VMS that are assigned the `Driver` role for the purpose of running job drivers on the Dataproc cluster.\n**Caution:** Driver node groups are typically used on shared, long-running clusters. But, be cautious about using this approach. A shared, long-running cluster typically represents a single point of failure: a cluster that is unhealthy or in an error state can block an entire data pipeline. Instead, use [ephemeral clusters](/dataproc/docs/support/spark-job-tuning#use_ephemeral_clusters) , which exist for the lifetime of a single job.\n## When to use driver node groups\n- Use driver node groups only when you need to run many concurrent jobs on a shared cluster.\n- Increase master node resources before using driver node groups to avoid [driver node group limitations](#limitations_and_considerations) .## How driver nodes help you run concurrents job\nDataproc starts a job driver process on a Dataproc cluster master node for each job. The driver process, in turn, runs an application driver, such as `spark-submit` , as its child process. However, the number of concurrent jobs running on the master is limited by the resources available on master node, and since Dataproc master nodes can't be scaled, a job can fail or get throttled when master node resources are insufficient to run a job.\nDriver node groups are special node groups managed by YARN, so job concurrency is not limited by master node resources. In clusters with a driver node group, application drivers run on driver nodes. Each driver node can run multiple application drivers if the node has sufficient resources.\n## Benefits\nUsing a Dataproc cluster with a driver node group lets you:\n- Horizontally scale job driver resources to run more concurrent jobs\n- Scale driver resources separately from worker resources\n- Obtain faster scaledown on Dataproc 2.0+ and later image clusters. On these clusters, the app master runs within a Spark driver in a driver node group (the`spark.yarn.unmanagedAM.enabled`is set to`true`by default).\n- Customize driver node start-up. You can add`{ROLE} == 'Driver'`in an [initialization script](/dataproc/docs/concepts/configuring-clusters/init-actions) to have the script perform actions for a driver node group in [node selection](/dataproc/docs/concepts/configuring-clusters/init-actions#node_selection) .## Limitations\n- Node groups are not supported in [Dataproc workflow templates](/dataproc/docs/concepts/workflows/overview) .\n- Node group clusters cannot be stopped, restarted, or autoscaled.\n- The MapReduce app master runs on worker nodes. A scale down of worker nodes can be slow if you enable [graceful decommissioning](/dataproc/docs/concepts/configuring-clusters/scaling-clusters#graceful_decommissioning) .\n- Job concurrency is affected by the`dataproc:agent.process.threads.job.max` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#dataproc_service_properties_table) . For example, with three masters and this property set to the default value of`100`, maximum cluster-level job concurrency is`300`.\n**Driver node group compared to Spark cluster mode**\n| Feature    | Spark cluster mode                               | Driver node group                                     |\n|:-----------------------|:-------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Worker node scale down | Long-lived drivers run on the same worker nodes as short-lived containers, making scale down of workers using graceful decommission slow. | Worker nodes scale down more quickly when drivers run on node groups.                        |\n| Streamed driver output | Requires searching in YARN logs to find the node where the driver was scheduled.               | Driver output is streamed to Cloud Storage, and is viewable in the Google Cloud console and in the gcloud dataproc jobs wait command output after a job completes. |\n## Driver node group IAM permissions\nThe following IAM permissions are associated the following Dataproc node group related actions.\n| Permission     | Action                           |\n|:---------------------------|:------------------------------------------------------------------------------------------------------------------|\n| dataproc.nodeGroups.create | Create Dataproc node groups. If a user has dataproc.clusters.create in the project, this permission is granted. |\n| dataproc.nodeGroups.get | Get the details of a Dataproc node group.                   |\n| dataproc.nodeGroups.update | Resize a Dataproc node group.                      |\n## Driver node group operations\nYou can use the gcloud CLI and Dataproc API to create, get, resize, delete, and submit a job to a Dataproc driver node group.\n### Create a driver node group cluster\nA driver node group is associated with one Dataproc cluster. You create a node group as part of [creating a Dataproc cluster](/dataproc/docs/guides/create-cluster#creating_a_cloud_dataproc_cluster) . You can use the gcloud CLI or Dataproc REST API to create a Dataproc cluster with a driver node group.\n**Note:** Dataproc driver node groups are supported in clusters  created with [2.0.52](/dataproc/docs/concepts/versioning/dataproc-release-2.0) and later [image versions](/dataproc/docs/concepts/versioning/dataproc-version-clusters#supported_dataproc_versions) .\n```\ngcloud dataproc clusters create CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--driver-pool-size=SIZE \\\n\u00a0\u00a0\u00a0\u00a0--driver-pool-id=NODE_GROUP_ID\n```\n **Required flags:** - : The cluster name, which must be unique within a project. The name must start with a lowercase letter, and can contain up to 51 lowercase letters, numbers, and hyphens. It cannot end with a hyphen. The name of a deleted cluster can be reused.\n- : The [region](/compute/docs/regions-zones#available) where the cluster will be located.\n- : The number of driver nodes in the node group. The number of nodes needed depend on job load and driver pool machine type. The number ofdriver group nodes is equal to total memory or vCPUs required by job drivers divided by each driver pool's machine memory or vCPUs.\n- : Optional and recommended. The ID must be unique within the cluster. Use this ID to identify the driver group in future operations, such as resizing the node group. If not specified, Dataproc generates the node group ID.\n **Recommended flag:** - `--enable-component-gateway`: Add this flag to enable the [Dataproc Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) , which provides access to the YARN web interface. The YARN UI Application and Scheduler pages display cluster and job status, application queue memory, core capacity, and other metrics.\n **Additional flags:** The following optional `driver-pool` flags can be added to the `gcloud dataproc clusters create` command to customize the node group.\n| Flag        | Default value                                              |\n|:----------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| --driver-pool-id     | A string identifier, generated by the service if not set by the flag. This ID can be used to identify the node group when performing future node pool operations, such as resizing the node group. |\n| --driver-pool-machine-type  | n1-standard-4                                              |\n| --driver-pool-accelerator   | No default. When specifying an accelerator, the GPU type is required; the number of GPUs is optional.                        |\n| --num-driver-pool-local-ssds  | No default                                               |\n| --driver-pool-local-ssd-interface | No default                                               |\n| --driver-pool-boot-disk-type  | pd-standard                                               |\n| --driver-pool-boot-disk-size  | 1000 GB                                                |\n| --driver-pool-min-cpu-platform | AUTOMATIC                                               |Complete a [AuxiliaryNodeGroup](/dataproc/docs/reference/rest/v1/ClusterConfig#AuxiliaryNodeGroup) as part of a Dataproc API [cluster.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) request.\nBefore using any of the request data, make the following replacements:- : Required. Google Cloud project ID.\n- : Required. Dataproc cluster [region](/compute/docs/regions-zones/regions-zones#available) .\n- : Required. The cluster name, which must be unique  within a project. The name must start with a lowercase letter, and can  contain up to 51 lowercase letters, numbers, and hyphens. It cannot end  with a hyphen. The name of a deleted cluster can be reused.\n- : Required. Number of nodes in the node group.\n- :  Optional and recommended. The ID must be unique within the cluster. Use  this ID to identify the driver group in future operations, such as resizing  the node group. If not specified, Dataproc generates  the node group ID.\n **Additional options:** See [NodeGroup](/dataproc/docs/reference/rest/v1/projects.regions.clusters.nodeGroups#resource:-nodegroup) .\nSet the [EndpointConfig.enableHttpPortAccess](/dataproc/docs/reference/rest/v1/ClusterConfig#EndpointConfig) property to`true`to enable the [Dataproc Component Gateway](/dataproc/docs/concepts/accessing/dataproc-gateways) , which provides access to the YARN web interface. The YARN UI Application and Scheduler pages display cluster and job status, application queue memory, core capacity, and other metrics.\nHTTP method and URL:\n```\nPOST https://dataproc.googleapis.com/v1/projects/PROJECT_ID/regions/REGION/clusters\n```\nRequest JSON body:\n```\n{\n \"clusterName\":\"CLUSTER_NAME\",\n \"config\": {\n \"softwareConfig\": {\n  \"imageVersion\":\"\"\n },\n \"endpointConfig\": {\n  \"enableHttpPortAccess\": true\n },\n \"auxiliaryNodeGroups\": [{\n  \"nodeGroup\":{\n   \"roles\":[\"DRIVER\"],\n   \"nodeGroupConfig\": {\n    \"numInstances\": SIZE\n    }\n   },\n  \"nodeGroupId\": \"NODE_GROUP_ID\"\n }]\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"projectId\": \"PROJECT_ID\",\n \"clusterName\": \"CLUSTER_NAME\",\n \"config\": {\n ...\n \"auxiliaryNodeGroups\": [  {\n  \"nodeGroup\": {\n\"name\": \"projects/PROJECT_ID/regions/REGION/clusters/CLUSTER_NAME/nodeGroups/NODE_GROUP_ID\",\n   \"roles\": [   \"DRIVER\"\n   ],\n   \"nodeGroupConfig\": {\n   \"numInstances\": SIZE,\n   \"instanceNames\": [    \"CLUSTER_NAME-np-q1gp\",\n    \"CLUSTER_NAME-np-xfc0\"\n   ],\n   \"imageUri\": \"https://www.googleapis.com/compute/v1/projects/cloud-dataproc-ci/global/images/dataproc-2-0-deb10-...-rc01\",\n   \"machineTypeUri\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/REGION-a/machineTypes/n1-standard-4\",\n   \"diskConfig\": {\n    \"bootDiskSizeGb\": 1000,\n    \"bootDiskType\": \"pd-standard\"\n   },\n   \"managedGroupConfig\": {\n    \"instanceTemplateName\": \"dataproc-2a8224d2-...\",\n    \"instanceGroupManagerName\": \"dataproc-2a8224d2-...\"\n   },\n   \"minCpuPlatform\": \"AUTOMATIC\",\n   \"preemptibility\": \"NON_PREEMPTIBLE\"\n   }\n  },\n  \"nodeGroupId\": \"NODE_GROUP_ID\"\n  }\n ]\n },\n}\n```### Get driver node group cluster metadata\nYou can use the [gcloud dataproc node-groups describe](/sdk/gcloud/reference/dataproc/node-groups/describe) command or the [Dataproc API](/dataproc/docs/reference/rest) to get driver node group metadata.\n```\ngcloud dataproc node-groups describe NODE_GROUP_ID \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION\n```\n **Required flags:** - : You can run`gcloud dataproc clusters describe` ``to list the node group ID.\n- : The cluster name.\n- : The cluster region.\nBefore using any of the request data, make the following replacements:- : Required. Google Cloud project ID.\n- : Required. The cluster region.\n- : Required. The cluster name.\n- : Required. You can run`gcloud dataproc clusters describe` ``to list the node group ID.\nHTTP method and URL:\n```\nGET https://dataproc.googleapis.com/v1/projects/PROJECT_ID/regions/REGION/clusters/CLUSTER_NAMEnodeGroups/Node_GROUP_ID\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_ID/regions/REGION/clusters/CLUSTER_NAME/nodeGroups/NODE_GROUP_ID\",\n \"roles\": [ \"DRIVER\"\n ],\n \"nodeGroupConfig\": {\n \"numInstances\": 5,\n \"imageUri\": \"https://www.googleapis.com/compute/v1/projects/cloud-dataproc-ci/global/images/dataproc-2-0-deb10-...-rc01\",\n \"machineTypeUri\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/REGION-a/machineTypes/n1-standard-4\",\n \"diskConfig\": {\n  \"bootDiskSizeGb\": 1000,\n  \"bootDiskType\": \"pd-standard\"\n },\n \"managedGroupConfig\": {\n  \"instanceTemplateName\": \"dataproc-driver-pool-mcia3j656h2fy\",\n  \"instanceGroupManagerName\": \"dataproc-driver-pool-mcia3j656h2fy\"\n },\n \"minCpuPlatform\": \"AUTOMATIC\",\n \"preemptibility\": \"NON_PREEMPTIBLE\"\n }\n}\n```### Resize a driver node group\nYou can use the [gcloud dataproc node-groups resize](/sdk/gcloud/reference/dataproc/node-groups/resize) command or the [Dataproc API](/dataproc/docs/reference/rest) to add or remove driver nodes from a cluster driver node group.\n```\ngcloud dataproc node-groups resize NODE_GROUP_ID \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--size=SIZE\n```\n **Required flags:** - : You can run`gcloud dataproc clusters describe` ``to list the node group ID.\n- : The cluster name.\n- : The cluster region.\n- : Specify the new number of driver nodes in the node group.\n **Optional flag:** - `--graceful-decommission-timeout=` ``: When scaling down a node group, you can add this flag to specify a [graceful decommissioning](/dataproc/docs/concepts/configuring-clusters/scaling-clusters#graceful_decommissioning) to avoid the immediate termination of job drivers. **Recommendation:** Set a timeout duration that is at least equal to the duration of longest job running on the node group (recovery of failed drivers is not supported).\nExample: gcloud CLI `NodeGroup` scale up command:\n```\ngcloud dataproc node-groups resize NODE_GROUP_ID \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--size=4\n```\nExample: gcloud CLI `NodeGroup` scale down command:\n```\ngcloud dataproc node-groups resize NODE_GROUP_ID \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--size=1 \\\n\u00a0\u00a0\u00a0\u00a0--graceful-decommission-timeout=\"100s\"\n```Before using any of the request data, make the following replacements:- : Required. Google Cloud project ID.\n- : Required. The cluster region.\n- : Required. You can run`gcloud dataproc  clusters describe` ``to list the node group ID.\n- : Required. New number of nodes in the node group.\n- : Optional. When scaling down a node group,  you can add a [gracefulDecommissionTimeout](/dataproc/docs/reference/rest/v1/projects.regions.clusters.nodeGroups/resize#body.request_body.FIELDS.graceful_decommission_timeout) to the request body to avoid the immediate termination of job drivers. **Recommendation:** Set a timeout duration that is at least  equal to the duration of longest job running on the node group (recovery  of failed drivers is not supported).Example:```\n{ \"size\": SIZE,\n \"gracefulDecommissionTimeout\": \"TIMEOUT_DURATION\"\n}\n \n```\nHTTP method and URL:\n```\nPOST https://dataproc.googleapis.com/v1/projects/PROJECT_ID/regions/REGION/clusters/CLUSTER_NAME/nodeGroups/Node_GROUP_ID:resize\n```\nRequest JSON body:\n```\n{\n \"size\": SIZE,\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_ID/regions/REGION/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.dataproc.v1.NodeGroupOperationMetadata\",\n \"nodeGroupId\": \"NODE_GROUP_ID\",\n \"clusterUuid\": \"CLUSTER_UUID\",\n \"status\": {\n  \"state\": \"PENDING\",\n  \"innerState\": \"PENDING\",\n  \"stateStartTime\": \"2022-12-01T23:34:53.064308Z\"\n },\n \"operationType\": \"RESIZE\",\n \"description\": \"Scale \"up or \"down\" a GCE node pool to SIZE nodes.\"\n }\n}\n```### Delete a driver node group cluster\nWhen you [delete a Dataproc cluster](/dataproc/docs/guides/manage-cluster#delete_a_cluster) , node groups associated with the cluster are deleted.\n### Submit a job\nYou can use the [gcloud dataproc jobs submit](/sdk/gcloud/reference/dataproc/jobs/submit) command or the [Dataproc API](/dataproc/docs/reference/rest) to [submit a job to a cluster](/dataproc/docs/guides/submit-job#how_to_submit_a_job) with a driver node group.\n```\ngcloud dataproc jobs submit JOB_COMMAND \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--driver-required-memory-mb=DRIVER_MEMORY \\\n\u00a0\u00a0\u00a0\u00a0--driver-required-vcores=DRIVER_VCORES \\\n\u00a0\u00a0\u00a0\u00a0DATAPROC_FLAGS \\\n\u00a0\u00a0\u00a0\u00a0-- JOB_ARGS\n```\n **Required flags:** - : Specify the [job command](/sdk/gcloud/reference/dataproc/jobs/submit#COMMAND) .\n- : The cluster name.\n- : Amount of job drivers memory in MB needed to run a job (see [Yarn Memory Controls](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/NodeManagerCGroupsMemory.html) ).\n- : The number of vCPUs needed to run a job.\n **Additional flags:** - : Add any additional [gcloud dataproc jobs submit](/sdk/gcloud/reference/dataproc/jobs/submit) flags related to the job type.\n- : Add any arguments (after the`--`to pass to the job.\n **Examples:** You can run the following examples from an [SSH terminal session](/dataproc/docs/concepts/accessing/ssh) on a Dataproc driver node group cluster.- Spark job to estimate value of `pi` :```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--driver-required-memory-mb=2048 \\\n\u00a0\u00a0\u00a0\u00a0--driver-required-vcores=2 \\\n\u00a0\u00a0\u00a0\u00a0--class=org.apache.spark.examples.SparkPi \\\n\u00a0\u00a0\u00a0\u00a0--jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0-- 1000\n```\n- Spark wordcount job:```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--driver-required-memory-mb=2048 \\\n\u00a0\u00a0\u00a0\u00a0--driver-required-vcores=2 \\\n\u00a0\u00a0\u00a0\u00a0--class=org.apache.spark.examples.JavaWordCount \\\n\u00a0\u00a0\u00a0\u00a0--jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0-- 'gs://apache-beam-samples/shakespeare/macbeth.txt'\n```\n- PySpark job to estimate value of `pi` :```\ngcloud dataproc jobs submit pyspark \\\n\u00a0\u00a0\u00a0\u00a0file:///usr/lib/spark/examples/src/main/python/pi.py \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--driver-required-memory-mb=2048 \\\n\u00a0\u00a0\u00a0\u00a0--driver-required-vcores=2 \\\n\u00a0\u00a0\u00a0\u00a0-- 1000\n```\n- Hadoop [TeraGen](https://hadoop.apache.org/docs/r3.2.0/api/org/apache/hadoop/examples/terasort/package-summary.html) MapReduce job:```\ngcloud dataproc jobs submit hadoop \\\n\u00a0\u00a0\u00a0\u00a0--cluster=CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0--driver-required-memory-mb=2048 \\\n\u00a0\u00a0\u00a0\u00a0--driver-required-vcores=2 \\\n\u00a0\u00a0\u00a0\u00a0--jar file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0-- teragen 1000 \\\n\u00a0\u00a0\u00a0\u00a0hdfs:///gen1/test\n```\nBefore using any of the request data, make the following replacements:- : Required. Google Cloud project ID.\n- : Required. Dataproc cluster [region](/compute/docs/regions-zones/regions-zones#available) \n- : Required. The cluster name, which must be unique  within a project. The name must start with a lowercase letter, and can  contain up to 51 lowercase letters, numbers, and hyphens. It cannot end  with a hyphen. The name of a deleted cluster can be reused.\n- : Required. Amount of job drivers memory in MB  needed to run a job (see [Yarn Memory Controls](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/NodeManagerCGroupsMemory.html) ).\n- : Required. The number of vCPUs needed to run a job.\n **Additional fields:** \nAdd additional fields related to the\n [job type](/dataproc/docs/reference/rest/v1/projects.regions.jobs#Job.FIELDS.oneof_type_job) \nand job arguments (the sample request includes fields needed to submit a Spark job that estimates the value of\n`pi`\n).\nHTTP method and URL:\n```\nPOST https://dataproc.googleapis.com/v1/projects/PROJECT_ID/regions/REGION/jobs:submit\n```\nRequest JSON body:\n```\n{\n \"job\": {\n \"placement\": {\n \"clusterName\": \"CLUSTER_NAME\",\n },\n \"driverSchedulingConfig\": {\n  \"memoryMb]\": DRIVER_MEMORY,\n  \"vcores\": DRIVER_VCORES\n },\n \"sparkJob\": {\n  \"jarFileUris\": \"file:///usr/lib/spark/examples/jars/spark-examples.jar\",\n  \"args\": [  \"10000\"\n  ],\n  \"mainClass\": \"org.apache.spark.examples.SparkPi\"\n }\n }\n}```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"reference\": {\n \"projectId\": \"PROJECT_ID\",\n \"jobId\": \"job-id\"\n },\n \"placement\": {\n \"clusterName\": \"CLUSTER_NAME\",\n \"clusterUuid\": \"cluster-Uuid\"\n },\n \"sparkJob\": {\n \"mainClass\": \"org.apache.spark.examples.SparkPi\",\n \"args\": [  \"1000\"\n ],\n \"jarFileUris\": [  \"file:///usr/lib/spark/examples/jars/spark-examples.jar\"\n ]\n },\n \"status\": {\n \"state\": \"PENDING\",\n \"stateStartTime\": \"start-time\"\n },\n \"jobUuid\": \"job-Uuid\"\n}\n```\n## View job logs\nTo view job status and help debug job issues, you can view driver logs using the gcloud CLI or the Google Cloud console.\nJob driver logs are streamed to the gcloud CLI output or Google Cloud console during job execution. Driver logs persist in a the Dataproc cluster [staging bucket](/dataproc/docs/concepts/configuring-clusters/staging-bucket) in Cloud Storage.\nRun the following gcloud CLI command to list the location of driver logs in Cloud Storage:\n```\ngcloud dataproc jobs describe JOB_ID \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION\n \n```\nThe Cloud Storage location of driver logs is listed as the `driverOutputResourceUri` in the command output in the following format:\n```\ndriverOutputResourceUri: gs://CLUSTER_STAGING_BUCKET/google-cloud-dataproc-metainfo/CLUSTER_UUID/jobs/JOB_ID\n```To view node group cluster logs:- [Enable Logging](/dataproc/docs/guides/logging) .\n- You can use the following [Logs Explorer](https://console.cloud.google.com/logs/query) query format to find logs:```\nresource.type=\"cloud_dataproc_cluster\"\nresource.labels.project_id=\"PROJECT_ID\"\nresource.labels.cluster_name=\"CLUSTER_NAME\"\nlog_name=\"projects/PROJECT_ID/logs/LOG_TYPE>\"\n```Replace the following;- : Google Cloud project ID.\n- : The cluster name.\n- :- Yarn user logs:`yarn-userlogs`\n- Yarn resource manager logs:`hadoop-yarn-resourcemanager`\n- Yarn node manager logs:`hadoop-yarn-nodemanager`\n## Monitor metrics\nDataproc node group job drivers run in a `dataproc-driverpool-driver-queue` child queue under a `dataproc-driverpool` partition.\n### Driver node group metrics\nThe following table lists the associated node group driver metrics, which are collected by default for driver node groups.\n| Driver node group metric          | Description                            |\n|:---------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|\n| yarn:ResourceManager:DriverPoolsQueueMetrics:AvailableMB  | The amount available memory in Mebibytes in dataproc-driverpool-driver-queue under the dataproc-driverpool partition. |\n| yarn:ResourceManager:DriverPoolsQueueMetrics:PendingContainers | The number of pending (queued) containers in dataproc-driverpool-driver-queue under the dataproc-driverpool partition. |\n### Child queue metrics\nThe following table lists the child queue metrics. The metrics are collected by default for driver node groups, and can be enabled for collection on any Dataproc clusters.\n| Child queue metric          | Description                       |\n|:---------------------------------------------------------|:----------------------------------------------------------------------------------------------------|\n| nan              | nan                         |\n| yarn:ResourceManager:ChildQueueMetrics:AvailableMB  | The amount of the available memory in Mebibytes in this queue under the default partition.   |\n| yarn:ResourceManager:ChildQueueMetrics:PendingContainers | Number of pending (queued) containers in this queue under the default partition.     |\n| yarn:ResourceManager:ChildQueueMetrics:running_0   | The number of jobs with a runtime between 0 and 60 minutes in this queue under all partitions.  |\n| yarn:ResourceManager:ChildQueueMetrics:running_60  | The number of jobs with a runtime between 60 and 300 minutes in this queue under all partitions. |\n| yarn:ResourceManager:ChildQueueMetrics:running_300  | The number of jobs with a runtime between 300 and 1440 minutes in this queue under all partitions. |\n| yarn:ResourceManager:ChildQueueMetrics:running_1440  | The number of jobs with a runtime greater than 1440 minutes in this queue under all partitions. |\n| yarn:ResourceManager:ChildQueueMetrics:AppsSubmitted  | Number of applications submitted to this queue under all partitions.        |\nTo view `YARN ChildQueueMetrics` and `DriverPoolsQueueMetrics` in the Google Cloud console:\n- Select **VM Instance \u2192 Custom** resources in the [Metrics Explorer](https://console.cloud.google.com/monitoring/metrics-explorer) .\n## Node group job driver debugging\nThis section provides driver node group conditions and errors with recommendations to fix the condition or error.\n### Conditions\n- **Condition:** `yarn:ResourceManager:DriverPoolsQueueMetrics:AvailableMB` is nearing `0` . This indicates that cluster driver pools queue are running out of memory. **Recommendation:** : Scale up the size of the driver pool.\n- **Condition:** `yarn:ResourceManager:DriverPoolsQueueMetrics:PendingContainers` is larger than 0. This can indicate that cluster driver pools queue are running out of memory and YARN is queuing jobs. **Recommendation:** : Scale up the size of the driver pool.\n### Errors\n- **Error:** `Cluster <var>CLUSTER_NAME</var> requires driver scheduling config to run SPARK job because it contains a node pool with role DRIVER. Positive values are required for all driver scheduling config values.` **Recommendation:** Set `driver-required-memory-mb` and `driver-required-vcores` with positive numbers.\n- **Error:** `Container exited with a non-zero exit code 137` . **Recommendation:** Increase `driver-required-memory-mb` to job memory usage.", "guide": "Dataproc"}