{"title": "Dataproc - Ranger Cloud Storage plugin", "url": "https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/ranger-plugin", "abstract": "# Dataproc - Ranger Cloud Storage plugin\nThe Dataproc Ranger Cloud Storage plugin, available with Dataproc image versions 1.5 and 2.0, activates an authorization service on each Dataproc cluster VM. The authorization service evaluates requests from the [Cloud Storage connector](/dataproc/docs/concepts/connectors/cloud-storage) against Ranger policies and, if the request is allowed, returns an access token for the cluster [VM service account](/dataproc/docs/concepts/configuring-clusters/service-accounts#VM_service_account) .\nThe Ranger Cloud Storage plugin relies on [Kerberos](https://web.mit.edu/kerberos/) for authentication, and integrates with Cloud Storage connector support for delegation tokens. Delegation tokens are stored in a [MySQL](https://dev.mysql.com/doc/refman/5.7/en/) database on the cluster master node. The root password for the database is specified through cluster properties when you [create the Dataproc cluster](#create_a_dataproc_cluster) .\nUse the default **KMS symmetric encryption** , which includes message authentication. Don't use asymmetric keys.\n", "content": "## Before you begin\nGrant the [Service Account Token Creator](/iam/docs/understanding-roles#iam.serviceAccountTokenCreator) role and the [IAM Role Admin](/iam/docs/understanding-roles#iam.roleAdmin) role on the [Dataproc VM service account](/dataproc/docs/concepts/configuring-clusters/service-accounts#VM_service_account) in your project.\n## Install the Ranger Cloud Storage plugin\nRun the following commands in a local terminal window or in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) to install the Ranger Cloud Storage plugin when you create a Dataproc cluster.\n### Set environment variables\n```\nexport CLUSTER_NAME=new-cluster-name \\\n\u00a0\u00a0\u00a0\u00a0export REGION=region \\\n\u00a0\u00a0\u00a0\u00a0export KERBEROS_KMS_KEY_URI=Kerberos-KMS-key-URI \\\n\u00a0\u00a0\u00a0\u00a0export KERBEROS_PASSWORD_URI=Kerberos-password-URI \\\n\u00a0\u00a0\u00a0\u00a0export RANGER_ADMIN_PASSWORD_KMS_KEY_URI=Ranger-admin-password-KMS-key-URI \\\n\u00a0\u00a0\u00a0\u00a0export RANGER_ADMIN_PASSWORD_GCS_URI=Ranger-admin-password-GCS-URI \\\n\u00a0\u00a0\u00a0\u00a0export RANGER_GCS_PLUGIN_MYSQL_KMS_KEY_URI=MySQL-root-password-KMS-key-URI \\\n\u00a0\u00a0\u00a0\u00a0export RANGER_GCS_PLUGIN_MYSQL_PASSWORD_URI=MySQL-root-password-GCS-URI\n```\nNotes:\n- CLUSTER_NAME: The name of the new cluster.\n- REGION: The [region](/compute/docs/regions-zones#available) where the cluster will be created, for example,`us-west1`.\n- KERBEROS_KMS_KEY_URI and KERBEROS_PASSWORD_URI: See [Set up your Kerberos root principal password](/dataproc/docs/concepts/configuring-clusters/security#set_up_your_kerberos_root_principal_password) .\n- RANGER_ADMIN_PASSWORD_KMS_KEY_URI and RANGER_ADMIN_PASSWORD_GCS_URI: See [Set up your Ranger admin password](/dataproc/docs/concepts/components/ranger#installation_steps) .\n- RANGER_GCS_PLUGIN_MYSQL_KMS_KEY_URI and RANGER_GCS_PLUGIN_MYSQL_PASSWORD_URI: Set up a MySQL password following the same procedure that you used to [Set up a Ranger admin password](/dataproc/docs/concepts/components/ranger#installation_steps) .\n### Create a Dataproc cluster\nRun the following command to create a Dataproc cluster and install the Ranger Cloud Storage plugin on the cluster.\n```\ngcloud dataproc clusters create ${CLUSTER_NAME} \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--scopes cloud-platform \\\n\u00a0\u00a0\u00a0\u00a0--enable-component-gateway \\\n\u00a0\u00a0\u00a0\u00a0--optional-components=SOLR,RANGER \\\n\u00a0\u00a0\u00a0\u00a0--kerberos-kms-key=${KERBEROS_KMS_KEY_URI} \\\n\u00a0\u00a0\u00a0\u00a0--kerberos-root-principal-password-uri=${KERBEROS_PASSWORD_URI} \\\n\u00a0\u00a0\u00a0\u00a0--properties=\"dataproc:ranger.gcs.plugin.enable=true, \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dataproc:ranger.kms.key.uri=${RANGER_ADMIN_PASSWORD_KMS_KEY_URI}, \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dataproc:ranger.admin.password.uri=${RANGER_ADMIN_PASSWORD_GCS_URI}, \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dataproc:ranger.gcs.plugin.mysql.kms.key.uri=${RANGER_GCS_PLUGIN_MYSQL_KMS_KEY_URI}, \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dataproc:ranger.gcs.plugin.mysql.password.uri=${RANGER_GCS_PLUGIN_MYSQL_PASSWORD_URI}\"\n```\nNotes:\n- **1.5 image version:** If you are creating a 1.5 image version cluster (see [Selecting versions](/dataproc/docs/concepts/versioning/overview#selecting_versions) ), add the`--metadata=GCS_CONNECTOR_VERSION=` ``flag to install the required connector version.\n### Verify Ranger Cloud Storage plugin installation\nAfter the cluster creation completes, a `GCS` service type, named `gcs-dataproc` , appears in the [Ranger admin web interface](/dataproc/docs/concepts/accessing/dataproc-gateways#viewing_and_accessing_component_gateway_urls) .\n## Ranger Cloud Storage plugin default policies\nThe default `gcs-dataproc` service has the following policies:\n- Policies to read from and write to the Dataproc cluster [staging and temp buckets](/dataproc/docs/concepts/configuring-clusters/staging-bucket) \n- An `all - bucket, object-path` policy, which allows all users to access metadata for all objects. This access is required to allow the Cloud Storage connector to perform HCFS ( [Hadoop Compatible Filesystem](https://cwiki.apache.org/confluence/display/HADOOP2/HCFS) ) operations.\n## Usage tips\n### App access to bucket folders\nTo accommodate apps that create intermediate files in Cloud Storage bucket, you can grant `Modify Objects` , `List Objects` , and `Delete Objects` permissions on the Cloud Storage bucket path, then select `recursive` mode to extend the permissions to sub-paths on the specified path.\n**Note:** By default, gcloud CLI jobs have access to all Cloud Storage resources.### Protective measures\nTo help prevent circumvention of the plugin:\n- Grant the [VM service account](/dataproc/docs/concepts/configuring-clusters/service-accounts#VM_service_account) access to the resources in your Cloud Storage buckets to allow it grant access to those resources with down-scoped access tokens (see [IAM permissions for Cloud Storage](/storage/docs/access-control/iam-permissions#object_permissions) ). Also, remove access by users to bucket resources to avoid direct bucket access by users.\n- Disable `sudo` and other means of root access on cluster VMs, including updating the `sudoer` file, to prevent impersonation or changes to authentication and authorization settings. For more information, see the Linux instructions for adding/removing `sudo` user privileges.\n- Use `iptable` to block direct access requests to Cloud Storage from cluster VMs. For example, you can block access to the VM metadata server to prevent access to the VM service account credential or access token used to authenticate and authorize access to Cloud Storage (see [block_vm_metadata_server.sh](https://github.com/GoogleCloudDataproc/initialization-actions/blob/master/ranger/block_vm_metadata_server.sh) , an [initialization script](/dataproc/docs/concepts/configuring-clusters/init-actions) that uses `iptable` rules to block access to VM metadata server).\n### Spark, Hive-on-MapReduce, and Hive-on-Tez jobs\nTo protect sensitive user authentication details and to reduce load on the Key Distribution Center (KDC), the Spark driver does not distribute Kerberos credentials to executors. Instead, the Spark driver obtains a delegation token from the Ranger Cloud Storage plugin, and then distributes the delegation token to executors. Executors use the delegation token to authenticate to the Ranger Cloud Storage plugin, trading it for a Google access token that allows access to Cloud Storage.\nHive-on-MapReduce and Hive-on-Tez jobs also use tokens to access Cloud Storage. Use the following properties to obtain tokens to access specified Cloud Storage buckets when you submit the following job types:\n- **Spark jobs:** ```\n--conf spark.yarn.access.hadoopFileSystems=gs://bucket-name,gs://bucket-name,...\n```\n- **Hive-on-MapReduce jobs:** ```\n--hiveconf \"mapreduce.job.hdfs-servers=gs://bucket-name,gs://bucket-name,...\"\n```\n- **Hive-on-Tez jobs:** ```\n--hiveconf \"tez.job.fs-servers=gs://bucket-name,gs://bucket-name,...\"\n```## Spark job scenario\nA Spark wordcount job fails when run from a terminal window on a Dataproc cluster VM that has the Ranger Cloud Storage plugin installed.\n```\nspark-submit \\\n\u00a0\u00a0\u00a0\u00a0--conf spark.yarn.access.hadoopFileSystems=gs://${FILE_BUCKET} \\\n\u00a0\u00a0\u00a0\u00a0--class org.apache.spark.examples.JavaWordCount \\\n\u00a0\u00a0\u00a0\u00a0/usr/lib/spark/examples/jars/spark-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0gs://bucket-name/wordcount.txt\n```\nNotes:\n- FILE_BUCKET: Cloud Storage bucket for Spark access.\nError output:\n```\nCaused by: com.google.gcs.ranger.client.shaded.io.grpc.StatusRuntimeException: PERMISSION_DENIED:\nAccess denied by Ranger policy: User: '<USER>', Bucket: '<dataproc_temp_bucket>',\nObject Path: 'a97127cf-f543-40c3-9851-32f172acc53b/spark-job-history/', Action: 'LIST_OBJECTS'\n```\nNotes:\n- `spark.yarn.access.hadoopFileSystems=gs://${FILE_BUCKET}`is required in a Kerberos enabled environment.\nError output:\n```\nCaused by: java.lang.RuntimeException: Failed creating a SPNEGO token.\nMake sure that you have run `kinit` and that your Kerberos configuration is correct.\nSee the full Kerberos error message: No valid credentials provided\n(Mechanism level: No valid credentials provided)\n```\nA policy is edited using the **Access Manager** in the **Ranger admin web interface** to add `username` to list of users who have `List Objects` and other `temp` bucket permissions.\nRunning the job generates a new error.\nError output:\n```\ncom.google.gcs.ranger.client.shaded.io.grpc.StatusRuntimeException: PERMISSION_DENIED:\nAccess denied by Ranger policy: User: <USER>, Bucket: '<file-bucket>',\nObject Path: 'wordcount.txt', Action: 'READ_OBJECTS'\n```\nA policy is added to grant the user read access to the `wordcount.text` Cloud Storage path.\nThe job runs and completes successfully.\n```\nINFO com.google.cloud.hadoop.fs.gcs.auth.GcsDelegationTokens:\nUsing delegation token RangerGCSAuthorizationServerSessionToken\nowner=<USER>, renewer=yarn, realUser=, issueDate=1654116824281,\nmaxDate=0, sequenceNumber=0, masterKeyId=0\nthis: 1\nis: 1\na: 1\ntext: 1\nfile: 1\n22/06/01 20:54:13 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped\n```", "guide": "Dataproc"}