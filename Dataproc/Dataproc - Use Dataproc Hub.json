{"title": "Dataproc - Use Dataproc Hub", "url": "https://cloud.google.com/dataproc/docs/tutorials/dataproc-hub-users", "abstract": "# Dataproc - Use Dataproc Hub\nDataproc Hub and  Vertex AI Workbench user-managed notebooks are  deprecated. On January 30, 2025, support for user-managed notebooks  will end and the ability to create user-managed notebooks instances  will be removed. For alternative notebook solutions  on Google Cloud, see:- [Install  the Jupyter component on your Dataproc cluster](/dataproc/docs/concepts/components/jupyter#install_jupyter) .\n- [Create  a Dataproc-enabled  Vertex AI Workbench instance](/vertex-ai/docs/workbench/instances/create-dataproc-enabled) .\n", "content": "## Objectives\n- Use Dataproc Hub to create a single-user JupyterLab notebook environment running on a Dataproc cluster.\n- Create a notebook and run a Spark job on the Dataproc cluster.\n- Delete your cluster and preserve your notebook in Cloud Storage.\n## Before you begin\n- The administrator must grant you`notebooks.instances.use`permission (see [Set Identity and Access Management (IAM) roles](/dataproc/docs/tutorials/dataproc-hub-admins#set_identity_and_access_management_iam_roles) ).\n## Create a Dataproc JupyterLab cluster from Dataproc Hub\n- Select the **User-Managed Notebooks** tab on the **Dataproc\u2192Workbench** page in the Google Cloud console.\n- Click **Open JupyterLab** in the row that lists the Dataproc Hub instance created by the administrator.- If you do not have access to the Google Cloud console, enter the Dataproc Hub instance URL that an administrator shared with you in your web browser.\n- On the **Jupyterhub\u2192Dataproc Options** page, select a cluster configuration and zone. If enabled, specify any customizations, then click **Create** .After the Dataproc cluster is created, you are redirected to the JupyterLab interface running on the cluster.\n## Create a notebook and run a Spark job\n- On the left panel of the JupyterLab interface, click on `GCS` (Cloud Storage).\n- Create a PySpark notebook from the JupyterLab launcher.\n- The PySpark kernel initializes a SparkContext (using the `sc` variable). You can examine the SparkContext and run a Spark job from the notebook.```\nrdd = (sc.parallelize(['lorem', 'ipsum', 'dolor', 'sit', 'amet', 'lorem'])\n  .map(lambda word: (word, 1))\n  .reduceByKey(lambda a, b: a + b))\nprint(rdd.collect())\n```\n- Name and save the notebook. The notebook is saved and remains in Cloud Storage after the Dataproc cluster is deleted.\n## Shut down the Dataproc cluster\n- From the JupyterLab interface, select **File\u2192Hub Control Panel** to open the **Jupyterhub** page.When using Dataproc image versions 1.4 or earlier, navigate to`/hub/home`to access the **Jupyterhub** page.\n- Click **Stop My Cluster** to shut down (delete) the JupyterLab server, which deletes the Dataproc cluster.Stopping the server and deleting the cluster **does not delete the Dataproc Hub instance** . You can click **Start my server** on the **Jupyterhub** (Hub Control Panel) page or select the **Open JupyterLab** link for your Dataproc Hub instance on the **Dataproc\u2192Workbench\u2192User-Managed Notebooks** page in the Google Cloud console to open configure and create another Dataproc JupyterLab cluster.\n## What's next\n- Explore [Spark and Jupyter Notebooks on Dataproc](https://github.com/GoogleCloudDataproc/cloud-dataproc/tree/master/notebooks) on GitHub.", "guide": "Dataproc"}