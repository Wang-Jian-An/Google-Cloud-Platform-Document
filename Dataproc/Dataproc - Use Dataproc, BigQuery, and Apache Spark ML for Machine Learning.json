{"title": "Dataproc - Use Dataproc, BigQuery, and Apache Spark ML for Machine Learning", "url": "https://cloud.google.com/dataproc/docs/tutorials/bigquery-sparkml", "abstract": "# Dataproc - Use Dataproc, BigQuery, and Apache Spark ML for Machine Learning\nThe [BigQuery Connector](/hadoop/bigquery-connector) for [Apache Spark](http://spark.apache.org/) allows Data Scientists to blend the power of [BigQuery](/bigquery/what-is-bigquery) 's seamlessly scalable SQL engine with [Apache Spark\u2019s Machine Learning](https://spark.apache.org/docs/2.0.0/ml-guide.html) capabilities. In this tutorial, we show how to use Dataproc, BigQuery and Apache Spark ML to perform machine learning on a [dataset](http://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes) .\n", "content": "## Objectives\nUse linear regression to build a model of birth weight as a function of five factors:\n- gestation weeks\n- mother's age\n- father's age\n- mother's weight gain during pregnancy\n- [Apgar score](https://en.wikipedia.org/wiki/Apgar_score) \nUse the following tools:- BigQuery, to prepare the linear regression input table, which is written to your Google Cloud project\n- Python, to query and manage data in BigQuery\n- Apache Spark, to access the resulting linear regression table\n- Spark ML, to build and evaluate the model\n- Dataproc PySpark job, to invoke Spark ML functions\n## CostsIn this document, you use the following billable components of Google Cloud:- Compute Engine\n- Dataproc\n- BigQuery\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . ## Before you beginA Dataproc cluster has the Spark components, including Spark ML, installed. To set up a Dataproc cluster and run the code in this example, you will need to do (or have done) the following:- Create a [Dataproc cluster](/dataproc/docs/guides/create-cluster) in your  project. Your cluster should be running a [Dataproc version](/dataproc/docs/concepts/dataproc-versions) with  Spark 2.0 or higher, (includes machine learning libraries).\n## Create a Subset of BigQuery natality dataIn this section, you create a dataset in your project, then create a table in the dataset to which you copy a subset of birth rate data from the publicly available [natality](/bigquery/sample-tables) BigQuery dataset. Later in this tutorial you will use the subset data in this table to predict birth weight as a function of maternal age, paternal age, and gestation weeks.\nYou can create the data subset using the Google Cloud console or running a Python script on your local machine.\n- Create a dataset in your project.- [Go to the BigQuery Web UI](https://console.cloud.google.com/bigquery) .\n- In the left navigation panel, click your project name, then click **CREATE DATASET** .\n- In the **Create dataset** dialog:- For **Dataset ID** , enter \"natality_regression\".\n- For **Data location** , you can choose a [location](/bigquery/docs/dataset-locations) for the dataset. The default value location is`US multi-region`. After a dataset is created, the location cannot be changed.\n- For **Default table expiration** , choose one of the following options:- **Never** (default): You must delete the table manually.\n- **Number of days** : The table will be deleted after the specified number days from its creation time.\n- For **Encryption** , choose one of the following options:- **Google-managed key** (default).\n- **Customer-managed key** : See [Protecting data with Cloud KMS keys](/bigquery/docs/customer-managed-encryption) .\n- Click **Create dataset** .You cannot add a description or a label when you create a dataset using the Web UI. After the dataset is created, you can [add a description](/bigquery/docs/managing-datasets#update-dataset-description) , and [add a label](/bigquery/docs/labels#creating_and_updating_dataset_labels) .\n- Run a query against the public natality dataset, then save the query results in a new table in your dataset.- Copy and paste the following query into the Query Editor, then click Run.```\nSELECT\nweight_pounds,\nmother_age,\nfather_age,\ngestation_weeks,\nweight_gain_pounds,\napgar_5min\nFROM\n`bigquery-public-data.samples.natality`\nWHERE\nweight_pounds IS NOT NULL\nAND mother_age IS NOT NULL\nAND father_age IS NOT NULL\nAND gestation_weeks IS NOT NULL\nAND weight_gain_pounds IS NOT NULL\nAND apgar_5min IS NOT NULL\n```\n- After the query completes (after approximately 1 minute), click **SAVE RESULTS** , then select save options to save the results as a \"regression_input\" BigQuery table in the`natality_regression`dataset in your project.\n- See [Setting Up a Python Development Environment](/python/docs/setup) for instructions on installing Python and the Google Cloud Client Library for Python (needed to run the code). Installing and using a Python `virtualenv` is recommended.\n- Copy and paste the `natality_tutorial.py` code, below, into a `python` shell on your local machine. Press the `<return>` key in the shell to run the code to create a \"natality_regression\" BigQuery dataset in your default Google Cloud project with a \"regression_input\" table that is populated with a subset of the public `natality` data. **Setting up to run the application code locally:** \n- Run`gcloud config list project`to see the name of your default project.\n- Run`gcloud config set project` ``to change the default project.\n- Run`gcloud auth application-default login`to set application credentials to your user account.\n [  samples/snippets/natality_tutorial.py ](https://github.com/googleapis/python-bigquery/blob/HEAD/samples/snippets/natality_tutorial.py) [View on GitHub](https://github.com/googleapis/python-bigquery/blob/HEAD/samples/snippets/natality_tutorial.py) ```\n\"\"\"Create a Google BigQuery linear regression input table.In the code below, the following actions are taken:* A new dataset is created \"natality_regression.\"* A query is run against the public dataset,\u00a0 \u00a0 bigquery-public-data.samples.natality, selecting only the data of\u00a0 \u00a0 interest to the regression, the output of which is stored in a new\u00a0 \u00a0 \"regression_input\" table.* The output table is moved over the wire to the user's default project via\u00a0 \u00a0 the built-in BigQuery Connector for Spark that bridges BigQuery and\u00a0 \u00a0 Cloud Dataproc.\"\"\"from google.cloud import bigquery# Create a new Google BigQuery client using Google Cloud Platform project# defaults.client = bigquery.Client()# Prepare a reference to a new dataset for storing the query results.dataset_id = \"natality_regression\"dataset_id_full = f\"{client.project}.{dataset_id}\"dataset = bigquery.Dataset(dataset_id_full)# Create the new BigQuery dataset.dataset = client.create_dataset(dataset)# Configure the query job.job_config = bigquery.QueryJobConfig()# Set the destination table to where you want to store query results.# As of google-cloud-bigquery 1.11.0, a fully qualified table ID can be# used in place of a TableReference.job_config.destination = f\"{dataset_id_full}.regression_input\"# Set up a query in Standard SQL, which is the default for the BigQuery# Python client library.# The query selects the fields of interest.query = \"\"\"\u00a0 \u00a0 SELECT\u00a0 \u00a0 \u00a0 \u00a0 weight_pounds, mother_age, father_age, gestation_weeks,\u00a0 \u00a0 \u00a0 \u00a0 weight_gain_pounds, apgar_5min\u00a0 \u00a0 FROM\u00a0 \u00a0 \u00a0 \u00a0 `bigquery-public-data.samples.natality`\u00a0 \u00a0 WHERE\u00a0 \u00a0 \u00a0 \u00a0 weight_pounds IS NOT NULL\u00a0 \u00a0 \u00a0 \u00a0 AND mother_age IS NOT NULL\u00a0 \u00a0 \u00a0 \u00a0 AND father_age IS NOT NULL\u00a0 \u00a0 \u00a0 \u00a0 AND gestation_weeks IS NOT NULL\u00a0 \u00a0 \u00a0 \u00a0 AND weight_gain_pounds IS NOT NULL\u00a0 \u00a0 \u00a0 \u00a0 AND apgar_5min IS NOT NULL\"\"\"# Run the query.query_job = client.query(query, job_config=job_config)query_job.result() \u00a0# Waits for the query to finish\n```\n- Confirm the creation of the `natality_regression` dataset and the `regression_input` table.\n## Run a linear regressionIn this section, you'll run a PySpark linear regression by submitting the job to the Dataproc service using the Google Cloud console or by running the `gcloud` command from a local terminal.\n- Copy and paste the following code into a new `natality_sparkml.py` file on your local machine.```\n\"\"\"Run a linear regression using Apache Spark ML.In the following PySpark (Spark Python API) code, we take the following actions:\u00a0 * Load a previously created linear regression (BigQuery) input table\u00a0 \u00a0 into our Cloud Dataproc Spark cluster as an RDD (Resilient\u00a0 \u00a0 Distributed Dataset)\u00a0 * Transform the RDD into a Spark Dataframe\u00a0 * Vectorize the features on which the model will be trained\u00a0 * Compute a linear regression using Spark ML\"\"\"from pyspark.context import SparkContextfrom pyspark.ml.linalg import Vectorsfrom pyspark.ml.regression import LinearRegressionfrom pyspark.sql.session import SparkSession# The imports, above, allow us to access SparkML features specific to linear# regression as well as the Vectors types.# Define a function that collects the features of interest# (mother_age, father_age, and gestation_weeks) into a vector.# Package the vector in a tuple containing the label (`weight_pounds`) for that# row.def vector_from_inputs(r):\u00a0 return (r[\"weight_pounds\"], Vectors.dense(float(r[\"mother_age\"]),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 float(r[\"father_age\"]),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 float(r[\"gestation_weeks\"]),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 float(r[\"weight_gain_pounds\"]),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 float(r[\"apgar_5min\"])))sc = SparkContext()spark = SparkSession(sc)# Read the data from BigQuery as a Spark Dataframe.natality_data = spark.read.format(\"bigquery\").option(\u00a0 \u00a0 \"table\", \"natality_regression.regression_input\").load()# Create a view so that Spark SQL queries can be run against the data.natality_data.createOrReplaceTempView(\"natality\")# As a precaution, run a query in Spark SQL to ensure no NULL values exist.sql_query = \"\"\"SELECT *from natalitywhere weight_pounds is not nulland mother_age is not nulland father_age is not nulland gestation_weeks is not null\"\"\"clean_data = spark.sql(sql_query)# Create an input DataFrame for Spark ML using the above function.training_data = clean_data.rdd.map(vector_from_inputs).toDF([\"label\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"features\"])training_data.cache()# Construct a new LinearRegression object and fit the training data.lr = LinearRegression(maxIter=5, regParam=0.2, solver=\"normal\")model = lr.fit(training_data)# Print the model summary.print(\"Coefficients:\" + str(model.coefficients))print(\"Intercept:\" + str(model.intercept))print(\"R^2:\" + str(model.summary.r2))model.summary.residuals.show()\n```\n- Copy the local `natality_sparkml.py` file to a Cloud Storage bucket in your project.```\ngsutil cp natality_sparkml.py gs://bucket-name\n```Instead of copying the file to a user bucket in your project, you can  copy it to the [staging bucket](/dataproc/docs/concepts/configuring-clusters/staging-bucket) that Dataproc created when you created your  cluster.\n- Run the regression from the Dataproc [Submit a job](https://console.cloud.google.com/dataproc/jobs/jobsSubmit) page.- In the **Main python file** field, insert the `gs://` URI of the Cloud Storage bucket where your copy of the `natality_sparkml.py` file is located.\n- Select `PySpark` as the **Job type** .\n- Insert `gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar` in the **Jar files** field. This makes the spark-bigquery-connector available to the PySpark application at runtime to allow it to read BigQuery data into a Spark DataFrame.The 2.12 jar is compatible with Dataproc clusters created with the 1.5 or later image. If your Dataproc cluster was created with the 1.3 or 1.4 image, specify the 2.11 jar instead (`gs://spark-lib/bigquery/spark-bigquery-latest_2.11.jar`).\n- Fill in the **Job ID** , **Region** , and **Cluster** fields.\n- Click **Submit** to run the job on your cluster.\nWhen the job completes, the linear regression output model summary appears in the Dataproc Job details window.\n **Model Summary Terminology:** - **R^2:** a measure for how much of the  \"signal\" the model captures.\n- **Residuals:** the difference between the prediction of  the model and the actual value for each point that was used to fit the model.\n- Copy and paste the following code into a new `natality_sparkml.py` file on your local machine.```\n\"\"\"Run a linear regression using Apache Spark ML.In the following PySpark (Spark Python API) code, we take the following actions:\u00a0 * Load a previously created linear regression (BigQuery) input table\u00a0 \u00a0 into our Cloud Dataproc Spark cluster as an RDD (Resilient\u00a0 \u00a0 Distributed Dataset)\u00a0 * Transform the RDD into a Spark Dataframe\u00a0 * Vectorize the features on which the model will be trained\u00a0 * Compute a linear regression using Spark ML\"\"\"from pyspark.context import SparkContextfrom pyspark.ml.linalg import Vectorsfrom pyspark.ml.regression import LinearRegressionfrom pyspark.sql.session import SparkSession# The imports, above, allow us to access SparkML features specific to linear# regression as well as the Vectors types.# Define a function that collects the features of interest# (mother_age, father_age, and gestation_weeks) into a vector.# Package the vector in a tuple containing the label (`weight_pounds`) for that# row.def vector_from_inputs(r):\u00a0 return (r[\"weight_pounds\"], Vectors.dense(float(r[\"mother_age\"]),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 float(r[\"father_age\"]),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 float(r[\"gestation_weeks\"]),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 float(r[\"weight_gain_pounds\"]),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 float(r[\"apgar_5min\"])))sc = SparkContext()spark = SparkSession(sc)# Read the data from BigQuery as a Spark Dataframe.natality_data = spark.read.format(\"bigquery\").option(\u00a0 \u00a0 \"table\", \"natality_regression.regression_input\").load()# Create a view so that Spark SQL queries can be run against the data.natality_data.createOrReplaceTempView(\"natality\")# As a precaution, run a query in Spark SQL to ensure no NULL values exist.sql_query = \"\"\"SELECT *from natalitywhere weight_pounds is not nulland mother_age is not nulland father_age is not nulland gestation_weeks is not null\"\"\"clean_data = spark.sql(sql_query)# Create an input DataFrame for Spark ML using the above function.training_data = clean_data.rdd.map(vector_from_inputs).toDF([\"label\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"features\"])training_data.cache()# Construct a new LinearRegression object and fit the training data.lr = LinearRegression(maxIter=5, regParam=0.2, solver=\"normal\")model = lr.fit(training_data)# Print the model summary.print(\"Coefficients:\" + str(model.coefficients))print(\"Intercept:\" + str(model.intercept))print(\"R^2:\" + str(model.summary.r2))model.summary.residuals.show()\n```\n- Copy the local `natality_sparkml.py` file to a Cloud Storage bucket in your project.```\ngsutil cp natality_sparkml.py gs://bucket-name\n```Instead of copying the file to a user bucket in your project, you can  copy it to the [staging bucket](/dataproc/docs/concepts/configuring-clusters/staging-bucket) that Dataproc created when you created your  cluster.\n- Submit the Pyspark job to the Dataproc service by running the `gcloud` command, shown below, from a terminal window on your local machine.- The **--jars** flag value makes the spark-bigquery-connector available to the PySpark jobv at runtime to allow it to read BigQuery data into a Spark DataFrame.```\ngcloud dataproc jobs submit pyspark \\\n\u00a0\u00a0\u00a0\u00a0gs://your-bucket/natality_sparkml.py \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_SCALA_VERSION-CONNECTOR_VERSION.jar\n```The 2.12 jar is compatible with Dataproc clusters created with the 1.5 or later image. If your Dataproc cluster was created with the 1.3 or 1.4 image, specify the 2.11 jar instead. See [Make the connector available to your application](/dataproc/docs/tutorials/bigquery-connector-spark-example#make_the_connector_available_to_your_application) for more information.\nThe linear regression output (model summary) appears in the terminal window when the job completes.\n **Model Summary Terminology:** \n- **R^2:** a measure for how much of the \"signal\" the model captures.\n- **Residuals:** the difference between the prediction of the model and the actual value for each point that was used to fit the model.\n```\n<<< # Print the model summary.\n... print \"Coefficients:\" + str(model.coefficients)\nCoefficients:[0.0166657454602,-0.00296751984046,0.235714392936,0.00213002070133,-0.00048577251587]\n<<< print \"Intercept:\" + str(model.intercept)\nIntercept:-2.26130330748\n<<< print \"R^2:\" + str(model.summary.r2)\nR^2:0.295200579035\n<<< model.summary.residuals.show()\n+--------------------+\n|   residuals|\n+--------------------+\n| -0.7234737533344147|\n| -0.985466980630501|\n| -0.6669710598385468|\n| 1.4162434829714794|\n|-0.09373154375186754|\n|-0.15461747949235072|\n| 0.32659061654192545|\n| 1.5053877697929803|\n| -0.640142797263989|\n| 1.229530260294963|\n|-0.03776160295256...|\n| -0.5160734239126814|\n| -1.5165972740062887|\n| 1.3269085258245008|\n| 1.7604670124710626|\n| 1.2348130901905972|\n| 2.318660276655887|\n| 1.0936947030883175|\n| 1.0169768511417363|\n| -1.7744915698181583|\n+--------------------+\nonly showing top 20 rows.\n \n```\n## Clean up\nAfter you finish the tutorial, you can clean up the resources that you created so that they stop using quota and incurring charges. The following sections describe how to delete or turn off these resources.\n### Deleting the project\nThe easiest way to eliminate billing is to delete the project that you created for the tutorial.\nTo delete the project:\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.### Deleting the Dataproc clusterSee [Delete a cluster](/dataproc/docs/guides/manage-cluster#delete_a_cluster) .## What's next\n- See [Spark job tuning tips](/dataproc/docs/support/spark-job-tuning)", "guide": "Dataproc"}