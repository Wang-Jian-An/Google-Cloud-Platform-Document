{"title": "Dataproc - High Availability Mode", "url": "https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/high-availability", "abstract": "# Dataproc - High Availability Mode\nWhen creating a Dataproc cluster, you can put the cluster into Hadoop High Availability (HA) mode by specifying the number of master instances in the cluster. The number of masters can only be specified at cluster creation time.\nCurrently, Dataproc supports two master configurations:\n- 1 master (default, non HA)\n- 3 masters (Hadoop HA)", "content": "## Comparing default and Hadoop High Availability mode\nDue to the complexity and higher cost of HA mode, use the default mode unless your use case requires HA mode.\n- **Compute Engine failure:** In the rare case of an unexpected Compute Engine failure, Dataproc instances will experience a machine reboot. The default single-master configuration for Dataproc is designed to recover and continue processing new work in such cases, but in-flight jobs will necessarily fail and need to be retried, and HDFS will be inaccessible until the single NameNode fully recovers on reboot. In **HA mode** , [HDFS High Availability](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html) and [YARN High Availability](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html) are configured to allow uninterrupted YARN and HDFS operations despite any single-node failures/reboots.\n- **Job driver termination:** The driver/main program of any jobs you run still represents a potential single point of failure if the correctness of your job depends on the driver program running successfully. Jobs submitted through the Dataproc Jobs API are not considered \"high availability,\" and will still be terminated on failure of the master node that runs the corresponding job driver programs. For individual jobs to be resilient against single-node failures using a HA Cloud Dataproc cluster, the job must either 1) run without a synchronous driver program or 2) it must run the driver program itself inside a YARN container and be written to handle driver-program restarts. See [Launching Spark on YARN](http://spark.apache.org/docs/latest/running-on-yarn.html#launching-spark-on-yarn) for an example of how restartable driver programs can run inside YARN containers for fault tolerance.\n- **Zonal failure:** As is the case with all Dataproc clusters, all nodes in a High Availability cluster reside in the same zone. If there is a failure that impacts all nodes in a zone, the failure will not be mitigated.\n### Instance Names\nThe default master is named `cluster-name-m` ; HA masters are named `cluster-name-m-0` , `cluster-name-m-1` , `cluster-name-m-2` .\n### Apache ZooKeeper\nIn an HA Dataproc cluster, the [Zookeeper component](/dataproc/docs/concepts/components/zookeeper) is automatically installed on cluster master nodes. All masters participate in a ZooKeeper cluster, which enables automatic failover for other Hadoop services.\n### HDFS\nIn a standard Dataproc cluster:\n- `cluster-name-m`runs:- NameNode\n- Secondary NameNodeIn a High Availability Dataproc cluster:\n- `cluster-name-m-0`and`cluster-name-m-1`run:- NameNode\n- ZKFailoverController\n- All masters run JournalNode\n- There is no Secondary NameNode\nPlease see the [HDFS High Availability](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html) documentation for additional details on components.\n### YARN\nIn a standard Dataproc cluster, `cluster-name-m` runs ResourceManager.\nIn a High Availability Dataproc cluster, all masters run ResourceManager.\nPlease see the [YARN High Availability](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerHA.html) documentation for additional details on components.\n## Creating a High Availability cluster\n**gcloud CLI setup:** You must [setup and configure](/sdk/docs/quickstarts) the gcloud CLI to use the Google Cloud CLI.\nTo create an HA cluster with\n [gcloud dataproc clusters create](/sdk/gcloud/reference/dataproc/clusters/create) \n, run the following command:\n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--num-masters=3 \\\n\u00a0\u00a0\u00a0\u00a0... other args\n```\nTo create an HA cluster, use the [clusters.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) API, setting [masterConfig.numInstances](/dataproc/docs/reference/rest/v1/ClusterConfig#InstanceGroupConfig) to `3` .\nAn easy way to construct the JSON body of an HA cluster create request is to create the request from the Dataproc [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page of the Google Cloud console. Select High Availability (3 masters, N workers) in the Cluster type section of the Set up cluster panel, then click the Equivalent REST button at the bottom of the left panel. Here's a snippet of a sample JSON output produced by the console for an HA cluster create request:```\n...\nmasterConfig\": {\n \"numInstances\": 3,\n \"machineTypeUri\": \"n1-standard-4\",\n \"diskConfig\": {\n \"bootDiskSizeGb\": 500,\n \"numLocalSsds\": 0\n }\n}\n...\n```\nTo create an HA cluster, select High Availability (3 masters, N workers) in the Cluster type section of the Set up cluster panel on the Dataproc [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page.", "guide": "Dataproc"}