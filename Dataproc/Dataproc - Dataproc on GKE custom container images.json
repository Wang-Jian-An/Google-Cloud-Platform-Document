{"title": "Dataproc - Dataproc on GKE custom container images", "url": "https://cloud.google.com/dataproc/docs/guides/dpgke/dataproc-gke-custom-images", "abstract": "# Dataproc - Dataproc on GKE custom container images\nYou can specify a custom container image to use with Dataproc on GKE . Your custom container image must use one of the Dataproc on GKE [base Spark images](#base_spark_images) .\n", "content": "## Use a custom container image\nTo use a Dataproc on GKE custom container image, set the `spark.kubernetes.container.image property` when you [create a Dataproc on GKE virtual cluster](/dataproc/docs/guides/dpgke/quickstarts/dataproc-gke-quickstart-create-cluster) or [submit a Spark job](/dataproc/docs/guides/dpgke/quickstarts/dataproc-gke-quickstart-create-cluster#submit_a_spark_job) to the cluster.\n**Note:** The `spark:` file prefix is needed when creating a cluster, but omitted when submitting a job (see [Cluster properties](/dataproc/docs/concepts/configuring-clusters/cluster-properties#formatting) ).\n- gcloud CLI cluster creation example:```\ngcloud dataproc clusters gke create \"${DP_CLUSTER}\" \\\n\u00a0\u00a0\u00a0\u00a0--properties=spark:spark.kubernetes.container.image=custom-image \\\n\u00a0\u00a0\u00a0\u00a0... other args ...\n```\n- gcloud CLI job submit example:```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--properties=spark.kubernetes.container.image=custom-image \\\n\u00a0\u00a0\u00a0\u00a0... other args ...\n```## Custom container image requirements and settings\n### Base images\nYou can use `docker` tools for building customized docker based upon one of the published Dataproc on GKE [base Spark images](#base_spark_images) .\n### Container user\nDataproc on GKE runs Spark containers as the Linux `spark` user with a `1099` UID and a `1099` GID. Use the UID and GID for filesystem permissions. For example, if you add a jar file at `/opt/spark/jars/my-lib.jar` in the image as a workload dependency, you must give the `spark` user read permission to the file.\n### Components\n- **Java:** The `JAVA_HOME` environment variable points to the location of the Java installation. The current default value is `/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64` , which is subject to change (see the [Dataproc release notes](/dataproc/docs/release-notes) for updated information).- If you customize the Java environment, make sure that`JAVA_HOME`is set to the correct location and`PATH`includes the path to binaries.\n- **Python:** Dataproc on GKE [base Spark images](/dataproc/docs/guides/dpgke/dataproc-gke-versions) have Miniconda3 installed at `/opt/conda` . `CONDA_HOME` points to this location, `${CONDA_HOME}/bin` is included in `PATH` , and `PYSPARK_PYTHON` is set to `${CONDA_HOME}/python` .- If you customize Conda, make sure that `CONDA_HOME` points to the Conda home directory , `${CONDA_HOME}/bin` is included in `PATH` , and `PYSPARK_PYTHON` is set to `${CONDA_HOME}/python.`\n- You can install, remove, and update packages in the default base environment, or create a new environment, but it is strongly recommended that the environment include all packages installed in the base environment of the base container image.\n- If you add Python modules, such as a Python script with utility functions, to the container image, include the module directories in `PYTHONPATH` .\n- **Spark:** Spark is installed in `/usr/lib/spark` , and `SPARK_HOME` points to this location. **Spark cannot be customized.** If it is changed, the container image will be rejected or fail to operate correctly.- **Jobs** : You can customize Spark job dependencies. `SPARK_EXTRA_CLASSPATH` defines the extra classpath for Spark JVM processes. Recommendation: put jars under `/opt/spark/jars` , and set `SPARK_EXTRA_CLASSPATH` to `/opt/spark/jars/*` .If you embed the job jar in the image, the recommended directory is `/opt/spark/job` . When you submit the job, you can reference it with a local path, for example, `file:///opt/spark/job/my-spark-job.jar` .\n- **Cloud Storage connector:** The Cloud Storage connector is installed at `/usr/lib/spark/jars` .\n- **Utilities** : The `procps` and `tini` utility packages are required to run Spark. These utilities are included in the [base Spark images](#base_spark_images) , so custom images do not need to re-install them.\n- **Entrypoint:** **Dataproc on GKE ignores anychanges made to the ENTRYPOINT and CMD primitives in thecontainer image.** \n- **Initialization scripts:** you can add an optional initialization script at `/opt/init-script.sh` . An initialization script can download files from Cloud Storage, start a proxy within the container, call other scripts, and perform other startup tasks.The entrypoint script calls the initialization script with all command line args ( `$@` ) before starting the Spark driver, Spark executor, and other processes. The initialization script can select the type of Spark process based on the first arg ( `$1` ): possible values include `spark-submit` for driver containers, and `executor` for executor containers.\n- **Configs:** Spark configs are located under `/etc/spark/conf` . The `SPARK_CONF_DIR` environment variable points to this location.Don't customize Spark configs in the container image. Instead, submit any properties via the Dataproc on GKE API for the following reasons:- Some properties, such as executor memory size, are determined at runtime, not at container image build time; they must be injected by Dataproc on GKE .\n- Dataproc on GKE places restrictions on the properties supplied by users. Dataproc on GKE mounts configs from`configMap`into`/etc/spark/conf`in the container, overriding settings embedded in the image.\n## Base Spark images\nDataproc supports the following base Spark container images:\n- [Spark 2.4](/dataproc/docs/guides/dpgke/dataproc-gke-versions#spark_engine_24) : ${REGION}-docker.pkg.dev/cloud-dataproc/spark/dataproc_1.5\n- [Spark 3.1](/dataproc/docs/guides/dpgke/dataproc-gke-versions#spark_engine_31) : ${REGION}-docker.pkg.dev/cloud-dataproc/spark/dataproc_2.0## Sample custom container image build\n### Sample Dockerfile\n```\nFROM us-central1-docker.pkg.dev/cloud-dataproc/spark/dataproc_2.0:latest# Change to root temporarily so that it has permissions to create dirs and copy# files.USER root# Add a BigQuery connector jar.ENV SPARK_EXTRA_JARS_DIR=/opt/spark/jars/ENV SPARK_EXTRA_CLASSPATH='/opt/spark/jars/*'RUN mkdir -p \"${SPARK_EXTRA_JARS_DIR}\" \\\u00a0 \u00a0 && chown spark:spark \"${SPARK_EXTRA_JARS_DIR}\"COPY --chown=spark:spark \\\u00a0 \u00a0 spark-bigquery-with-dependencies_2.12-0.22.2.jar \"${SPARK_EXTRA_JARS_DIR}\"# Install Cloud Storage client Conda package.RUN \"${CONDA_HOME}/bin/conda\" install google-cloud-storage# Add a custom Python file.ENV PYTHONPATH=/opt/python/packagesRUN mkdir -p \"${PYTHONPATH}\"COPY test_util.py \"${PYTHONPATH}\"# Add an init script.COPY --chown=spark:spark init-script.sh /opt/init-script.sh# (Optional) Set user back to `spark`.USER spark\n```\n### Build the container image\nRun the following commands in the Dockerfile directory\n- Set image (example:`us-central1-docker.pkg.dev/my-project/spark/spark-test-image:latest`) and change to build directory.```\nIMAGE=custom container image \\\n\u00a0\u00a0\u00a0\u00a0BUILD_DIR=$(mktemp -d) \\\n\u00a0\u00a0\u00a0\u00a0cd \"${BUILD_DIR}\"\n```\n- Download the BigQuery connector.```\ngsutil cp \\\n\u00a0\u00a0\u00a0\u00a0gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar .\n```\n- Create a Python example file.```\ncat >test_util.py <<'EOF'\ndef hello(name):\n\u00a0\u00a0print(\"hello {}\".format(name))\ndef read_lines(path):\n\u00a0\u00a0with open(path) as f:\n\u00a0\u00a0\u00a0\u00a0return f.readlines()\nEOF\n```\n- Create an example init script.```\ncat >init-script.sh <<EOF\necho \"hello world\" >/tmp/init-script.out\nEOF\n```\n- Build and push the image.```\ndocker build -t \"${IMAGE}\" . && docker push \"${IMAGE}\"\n```", "guide": "Dataproc"}