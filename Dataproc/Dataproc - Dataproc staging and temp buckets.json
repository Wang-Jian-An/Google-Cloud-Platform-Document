{"title": "Dataproc - Dataproc staging and temp buckets", "url": "https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket", "abstract": "# Dataproc - Dataproc staging and temp buckets\nWhen you create a cluster, HDFS is used as the default filesystem. You can override this behavior by setting the defaultFS as a Cloud Storage [bucket](/storage/docs/buckets) . By default, Dataproc also creates a Cloud Storage staging and a Cloud Storage temp bucket in your project or reuses existing Dataproc-created staging and temp buckets from previous cluster creation requests.\n- Staging bucket: Used to stage cluster job dependencies, [job driver output](/dataproc/docs/guides/dataproc-job-output) , and cluster config files. Also receives output from the gcloud CLI [gcloud dataproc clusters diagnose](/dataproc/docs/support/diagnose-command) command.\n- Temp bucket: Used to store ephemeral cluster and jobs data, such as Spark and MapReduce history files.\nIf you do not specify a staging or temp bucket when you create a cluster, Dataproc sets a [Cloud Storage location in US, ASIA,or EU](/storage/docs/locations#location-mr) for your cluster's staging and temp buckets according to the Compute Engine zone where your cluster is deployed, and then creates and manages these project-level, per-location buckets. Dataproc-created staging and temp buckets are shared among clusters in the same region.\nThe temp bucket contains ephemeral data, and has a TTL of 90 days. The staging bucket, which can contain configuration data and dependency files needed by multiple clusters, does not have a TTL. However, you can [apply a lifecycle rule toyour dependency files](/storage/docs/lifecycle#matchesprefix-suffix) (files with a \".jar\" filename extension located in the staging bucket folder) to schedule the removal of your dependency files when they are no longer needed by your clusters.\nTo locate the default Dataproc staging and temp buckets using the Google Cloud console **Cloud Storage Browser** , filter results using the \"dataproc-staging-\" and \"dataproc-temp-\" prefixes.\n", "content": "## Create your own staging and temp buckets\nInstead of relying on the creation of a default staging and temp bucket, you can specify existing Cloud Storage buckets that Dataproc will use as your cluster's staging and temp bucket.\n**Note:** When you use an [Assured Workloads environment](/assured-workloads/docs/deploy-resource) for regulatory compliance, the cluster, VPC network, and Cloud Storage buckets must be contained within the Assured Workloads environment.\nRun the `gcloud dataproc clusters create` command with the [--bucket](/sdk/gcloud/reference/dataproc/clusters/create#--bucket) and/or [--temp-bucket](/sdk/gcloud/reference/dataproc/clusters/create#--temp-bucket) flags locally in a terminal window or in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) to specify your cluster's staging and/or temp bucket.\n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--bucket=bucket-name \\\n\u00a0\u00a0\u00a0\u00a0--temp-bucket=bucket-name \\\n\u00a0\u00a0\u00a0\u00a0other args ...\n```Use the [ClusterConfig.configBucket](/dataproc/docs/reference/rest/v1/ClusterConfig#FIELDS.config_bucket) and [ClusterConfig.tempBucket](/dataproc/docs/reference/rest/v1/ClusterConfig#FIELDS.temp_bucket) fields in a [clusters.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) request to specify your cluster's staging and temp buckets.In the Google Cloud console, open the Dataproc [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page. Select the Customize cluster panel, then   use the File storage field to specify or select the cluster's staging   bucket.\nNote: Currently, specifying a temp bucket using the Google Cloud console  is not supported.\nDataproc uses a defined folder structure for Cloud Storage buckets attached to clusters. Dataproc also supports attaching more than one cluster to a Cloud Storage bucket. The folder structure used for saving job driver output in Cloud Storage is:\n```\ncloud-storage-bucket-name\n - google-cloud-dataproc-metainfo\n - list of cluster IDs\n  - list of job IDs\n   - list of output logs for a job\n```\nYou can use the `gcloud` command line tool, Dataproc API, or Google Cloud console to list the name of a cluster's staging and temp buckets.\n- \\View cluster details, which includeas the name of the cluster's staging bucket, on the Dataproc [Clusters](https://console.cloud.google.com/project/_/dataproc/clusters) page in the Google Cloud console.\n- On the Google Cloud console **Cloud Storage Browser** page, filter results that contain \"dataproc-temp-\".\nRun the [gcloud dataproc clusters describe](/sdk/gcloud/reference/dataproc/clusters/describe) command locally in a terminal window or in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) . The staging and temp buckets associated with your cluster are listed in the output.\n```\ngcloud dataproc clusters describe cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n...\nclusterName: cluster-name\nclusterUuid: daa40b3f-5ff5-4e89-9bf1-bcbfec ...\nconfig:\n configBucket: dataproc-...\n ...\n tempBucket: dataproc-temp...\n```Call [clusters.get](/dataproc/docs/reference/rest/v1/projects.regions.clusters/get) to list the cluster details, including the name of the cluster's staging and temp buckets.\n```\n{\n \"projectId\": \"vigilant-sunup-163401\",\n \"clusterName\": \"cluster-name\",\n \"config\": {\n \"configBucket\": \"dataproc-...\",\n...\n \"tempBucket\": \"dataproc-temp-...\",\n}\n```\n## defaultFS\nYou can set `core:fs.defaultFS` to a bucket location in Cloud Storage ( `gs://` `` ) to set Cloud Storage as the default filesystem. This also sets `core:fs.gs.reported.permissions` , the reported permission returned by the Cloud Storage connector for all files, to `777` .\n**Note:** When you use an [Assured Workloads environment](/assured-workloads/docs/deploy-resource) for regulatory compliance, the cluster, VPC network, and Cloud Storage buckets must be contained within the Assured Workloads environment.\nIf Cloud Storage is not set as the default filesystem, HDFS will be used, and the `core:fs.gs.reported.permissions` property will return `700` , the default value.\n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--properties=core:fs.defaultFS=gs://defaultFS-bucket-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--bucket=staging-bucket-name \\\n\u00a0\u00a0\u00a0\u00a0--temp-bucket=temp-bucket-name \\\n\u00a0\u00a0\u00a0\u00a0other args ...\n```\n**Note:** Currently, console display of the defaultFS bucket is not supported.", "guide": "Dataproc"}