{"title": "Dataproc - Submit a job", "url": "https://cloud.google.com/dataproc/docs/guides/submit-job", "abstract": "# Dataproc - Submit a job\nYou can submit a job to an existing Dataproc cluster via a Dataproc API [jobs.submit](/dataproc/docs/reference/rest/v1/projects.regions.jobs/submit) HTTP or programmatic request, using the Google Cloud CLI [gcloud](/sdk/gcloud/reference/dataproc/jobs/submit) command-line tool in a local terminal window or in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) , or from the [Google Cloud console](https://console.cloud.google.com/dataproc/jobs/jobsSubmit) opened in a local browser. You can also [SSH into the master instance](#ssh_into_the_master_instance) in your cluster, and then run a job directly from the instance without using the Dataproc service.\n**Job driver and log output:** For information on viewing job driver output and logs, see [Dataproc job output and logs](/dataproc/docs/guides/dataproc-job-output) .\n**Job concurrency: ** You can configure the maximum number of concurrent Dataproc jobs with the [dataproc:dataproc.scheduler.max-concurrent-jobs](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) cluster property when you create a cluster. If this property value is not set, the upper limit on concurrent jobs is calculated as`max((masterMemoryMb - 3584) / masterMemoryMbPerJob, 5)`.`masterMemoryMb`is determined by the master VM's machine type.`masterMemoryMbPerJob`is`1024`by default, but is configurable at cluster creation with the [dataproc:dataproc.scheduler.driver-size-mb](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) cluster property.\n", "content": "## How to submit a job\nYou can specify a`file:///`path to refer to a local file on a cluster's master node.\nOpen the Dataproc [Submit a job](https://console.cloud.google.com/dataproc/jobs/jobsSubmit) page in the Google Cloud console in your browser.\n **Spark job example** \nTo submit a sample Spark job, fill in the fields on the **Submit a job** page, as follows:- Select your **Cluster** name from the cluster list.\n- Set **Job type** to`Spark`.\n- Set **Main class or jar** to`org.apache.spark.examples.SparkPi`.\n- Set **Arguments** to the single argument`1000`.\n- Add`file:///usr/lib/spark/examples/jars/spark-examples.jar`to **Jar files** :\n- `file:///`denotes a Hadoop LocalFileSystem scheme. Dataproc  installed`/usr/lib/spark/examples/jars/spark-examples.jar`on the  cluster's master node when it created the cluster.\n- Alternatively, you can specify a Cloud Storage path  (`gs://` `` `/` `` `.jar`) or a Hadoop Distributed File  System path (`hdfs://` `` `.jar`) to one of your jars.\n **To add more arguments** : Each argument must be entered in a separate text box. Press **<Return>** to open a new text box for each additional argument.\nClick **Submit** to start the job. Once the job starts, it is added to the Jobs list.Click the Job ID to open the **Jobs** page, where you can view the job's driver output. Since this job produces long output lines that exceed the width of the browser window, you can check the **Line wrapping** box to bring all output text within view in order to display the calculated result for `pi` .You can view your job's driver output from the command line using the [gcloud dataproc jobs wait](/sdk/gcloud/reference/dataproc/jobs/wait) command shown below (for more information, see [View job output\u2013GCLOUD COMMAND](/dataproc/docs/concepts/dataproc-job-output#view_job_output) ). Copy and paste your project ID as the value for the `--project` flag and your Job ID (shown on the Jobs list) as the final argument.\n```\ngcloud dataproc jobs wait job-id \\\n\u00a0\u00a0\u00a0\u00a0--project=project-id \\\n\u00a0\u00a0\u00a0\u00a0--region=region\n```\nHere are snippets from the driver output for the sample `SparkPi` job submitted above:\n```\n...\n2015-06-25 23:27:23,810 INFO [dag-scheduler-event-loop]\nscheduler.DAGScheduler (Logging.scala:logInfo(59)) - Stage 0 (reduce at\nSparkPi.scala:35) finished in 21.169 s\n2015-06-25 23:27:23,810 INFO [task-result-getter-3] cluster.YarnScheduler\n(Logging.scala:logInfo(59)) - Removed TaskSet 0.0, whose tasks have all\ncompleted, from pool\n2015-06-25 23:27:23,819 INFO [main] scheduler.DAGScheduler\n(Logging.scala:logInfo(59)) - Job 0 finished: reduce at SparkPi.scala:35,\ntook 21.674931 s\nPi is roughly 3.14189648\n...\nJob [c556b47a-4b46-4a94-9ba2-2dcee31167b2] finished successfully.\ndriverOutputUri:\ngs://sample-staging-bucket/google-cloud-dataproc-metainfo/cfeaa033-749e-48b9-...\n...\n```To submit a job to a Dataproc cluster, run the gcloud CLI [gcloud dataproc jobs submit](/sdk/gcloud/reference/dataproc/jobs/submit) command locally in a terminal window or in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) .\n```\ngcloud dataproc jobs submit job-command \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0other dataproc-flags \\\n\u00a0\u00a0\u00a0\u00a0-- job-args\n```\nYou can add the`--cluster-labels`flag to specify one or more cluster labels. Dataproc will submit the job to a cluster that matches a specified cluster label.\n **PySpark job submit example** - List the publicly accessible`hello-world.py`located in Cloud Storage.```\ngsutil cat gs://dataproc-examples/pyspark/hello-world/hello-world.py\n```File Listing:```\n#!/usr/bin/pythonimport pysparksc = pyspark.SparkContext()rdd = sc.parallelize(['Hello,', 'world!'])words = sorted(rdd.collect())print(words)\n```\n- Submit the Pyspark job to Dataproc.```\ngcloud dataproc jobs submit pyspark \\\n\u00a0\u00a0\u00a0\u00a0gs://dataproc-examples/pyspark/hello-world/hello-world.py \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region\n```Terminal output:```\nWaiting for job output...\n\u2026\n['Hello,', 'world!']\nJob finished successfully.\n```\n **Spark job submit example** - Run the SparkPi example pre-installed on the Dataproc cluster's  master node.```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--cluster=cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--class=org.apache.spark.examples.SparkPi \\\n\u00a0\u00a0\u00a0\u00a0--jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0-- 1000\n```Terminal output:```\nJob [54825071-ae28-4c5b-85a5-58fae6a597d6] submitted.\nWaiting for job output\u2026\n\u2026\nPi is roughly 3.14177148\n\u2026\nJob finished successfully.\n\u2026\n``` **How the job calculates Pi** : The Spark job estimates a value of Pi using the [Monte Carlo method](https://en.wikipedia.org/wiki/Monte_Carlo_method) . It generates`x,y`points on a coordinate plane that models a circle enclosed by a unit square. The input argument (`1000`) determines the number of x,y pairs to generate; the more pairs generated, the greater the accuracy of the estimation. This estimation leverages Dataproc worker nodes to parallelize the computation. For more information, see [Estimating Pi using the Monte Carlo Method](https://academo.org/demos/estimating-pi-monte-carlo/) and see [JavaSparkPi.java on GitHub](https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java) .\nThis section shows how to submit a Spark job to compute the approximate value of `pi` using the Dataproc [jobs.submit](/dataproc/docs/reference/rest/v1/projects.regions.jobs/submit) API.\nYou can add the`clusterLabels`field to the API request shown below to specify one or more cluster labels. Dataproc will submit the job to a cluster that matches a specified cluster label (see the [jobs.submit](/dataproc/docs/reference/rest/v1/projects.regions.jobs#JobPlacement.FIELDS.cluster_labels) API for more information).\nBefore using any of the request data, make the following replacements:- : Google Cloud project ID\n- : [cluster region](/dataproc/docs/guides/create-cluster#cluster-region) \n- : cluster name\nHTTP method and URL:\n```\nPOST https://dataproc.googleapis.com/v1/projects/project-id/regions/region/jobs:submit\n```\nRequest JSON body:\n```\n{\n \"job\": {\n \"placement\": {\n  \"clusterName\": \"cluster-name\"\n },\n },\n \"sparkJob\": {\n  \"args\": [  \"1000\"\n  ],\n  \"mainClass\": \"org.apache.spark.examples.SparkPi\",\n  \"jarFileUris\": [  \"file:///usr/lib/spark/examples/jars/spark-examples.jar\"\n  ]\n }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"reference\": {\n \"projectId\": \"project-id\",\n \"jobId\": \"job-id\"\n },\n \"placement\": {\n \"clusterName\": \"cluster-name\",\n \"clusterUuid\": \"cluster-Uuid\"\n },\n \"sparkJob\": {\n \"mainClass\": \"org.apache.spark.examples.SparkPi\",\n \"args\": [  \"1000\"\n ],\n \"jarFileUris\": [  \"file:///usr/lib/spark/examples/jars/spark-examples.jar\"\n ]\n },\n \"status\": {\n \"state\": \"PENDING\",\n \"stateStartTime\": \"2020-10-07T20:16:21.759Z\"\n },\n \"jobUuid\": \"job-Uuid\"\n}\n```\n **Note:** You can click the **Equivalent REST** link at the bottom of the Dataproc Google Cloud console [Submit a job](https://console.cloud.google.com/dataproc/jobs/jobsSubmit) page to have the Google Cloud console construct an equivalent API REST request to use in your code to submit a job to your cluster.\n- [Install the client library](/dataproc/docs/reference/libraries#installing_the_client_library) \n- [Set up application default credentials](/dataproc/docs/reference/libraries#setting_up_authentication) \n- Run the codeSee [Setting Up a Java Development Environment](/java/docs/setup) . [  dataproc/src/main/java/SubmitJob.java ](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataproc/src/main/java/SubmitJob.java) [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/dataproc/src/main/java/SubmitJob.java) ```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.dataproc.v1.Job;import com.google.cloud.dataproc.v1.JobControllerClient;import com.google.cloud.dataproc.v1.JobControllerSettings;import com.google.cloud.dataproc.v1.JobMetadata;import com.google.cloud.dataproc.v1.JobPlacement;import com.google.cloud.dataproc.v1.SparkJob;import com.google.cloud.storage.Blob;import com.google.cloud.storage.Storage;import com.google.cloud.storage.StorageOptions;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.regex.Matcher;import java.util.regex.Pattern;public class SubmitJob {\u00a0 public static void submitJob() throws IOException, InterruptedException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String projectId = \"your-project-id\";\u00a0 \u00a0 String region = \"your-project-region\";\u00a0 \u00a0 String clusterName = \"your-cluster-name\";\u00a0 \u00a0 submitJob(projectId, region, clusterName);\u00a0 }\u00a0 public static void submitJob(String projectId, String region, String clusterName)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException {\u00a0 \u00a0 String myEndpoint = String.format(\"%s-dataproc.googleapis.com:443\", region);\u00a0 \u00a0 // Configure the settings for the job controller client.\u00a0 \u00a0 JobControllerSettings jobControllerSettings =\u00a0 \u00a0 \u00a0 \u00a0 JobControllerSettings.newBuilder().setEndpoint(myEndpoint).build();\u00a0 \u00a0 // Create a job controller client with the configured settings. Using a try-with-resources\u00a0 \u00a0 // closes the client,\u00a0 \u00a0 // but this can also be done manually with the .close() method.\u00a0 \u00a0 try (JobControllerClient jobControllerClient =\u00a0 \u00a0 \u00a0 \u00a0 JobControllerClient.create(jobControllerSettings)) {\u00a0 \u00a0 \u00a0 // Configure cluster placement for the job.\u00a0 \u00a0 \u00a0 JobPlacement jobPlacement = JobPlacement.newBuilder().setClusterName(clusterName).build();\u00a0 \u00a0 \u00a0 // Configure Spark job settings.\u00a0 \u00a0 \u00a0 SparkJob sparkJob =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 SparkJob.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMainClass(\"org.apache.spark.examples.SparkPi\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addJarFileUris(\"file:///usr/lib/spark/examples/jars/spark-examples.jar\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addArgs(\"1000\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 Job job = Job.newBuilder().setPlacement(jobPlacement).setSparkJob(sparkJob).build();\u00a0 \u00a0 \u00a0 // Submit an asynchronous request to execute the job.\u00a0 \u00a0 \u00a0 OperationFuture<Job, JobMetadata> submitJobAsOperationAsyncRequest =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 jobControllerClient.submitJobAsOperationAsync(projectId, region, job);\u00a0 \u00a0 \u00a0 Job response = submitJobAsOperationAsyncRequest.get();\u00a0 \u00a0 \u00a0 // Print output from Google Cloud Storage.\u00a0 \u00a0 \u00a0 Matcher matches =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Pattern.compile(\"gs://(.*?)/(.*)\").matcher(response.getDriverOutputResourceUri());\u00a0 \u00a0 \u00a0 matches.matches();\u00a0 \u00a0 \u00a0 Storage storage = StorageOptions.getDefaultInstance().getService();\u00a0 \u00a0 \u00a0 Blob blob = storage.get(matches.group(1), String.format(\"%s.000000000\", matches.group(2)));\u00a0 \u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\"Job finished successfully: %s\", new String(blob.getContent())));\u00a0 \u00a0 } catch (ExecutionException e) {\u00a0 \u00a0 \u00a0 // If the job does not complete successfully, print the error message.\u00a0 \u00a0 \u00a0 System.err.println(String.format(\"submitJob: %s \", e.getMessage()));\u00a0 \u00a0 }\u00a0 }}\n```\n- [Install the client library](/dataproc/docs/reference/libraries#installing_the_client_library) \n- [Set up application default credentials](/dataproc/docs/reference/libraries#setting_up_authentication) \n- Run the codeSee [Setting Up a Python Development Environment](/python/docs/setup) . [  dataproc/snippets/submit_job.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataproc/snippets/submit_job.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/dataproc/snippets/submit_job.py) ```\nimport refrom google.cloud import dataproc_v1 as dataprocfrom google.cloud import storagedef submit_job(project_id, region, cluster_name):\u00a0 \u00a0 # Create the job client.\u00a0 \u00a0 job_client = dataproc.JobControllerClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options={\"api_endpoint\": f\"{region}-dataproc.googleapis.com:443\"}\u00a0 \u00a0 )\u00a0 \u00a0 # Create the job config. 'main_jar_file_uri' can also be a\u00a0 \u00a0 # Google Cloud Storage URL.\u00a0 \u00a0 job = {\u00a0 \u00a0 \u00a0 \u00a0 \"placement\": {\"cluster_name\": cluster_name},\u00a0 \u00a0 \u00a0 \u00a0 \"spark_job\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"main_class\": \"org.apache.spark.examples.SparkPi\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"jar_file_uris\": [\"file:///usr/lib/spark/examples/jars/spark-examples.jar\"],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"args\": [\"1000\"],\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 }\u00a0 \u00a0 operation = job_client.submit_job_as_operation(\u00a0 \u00a0 \u00a0 \u00a0 request={\"project_id\": project_id, \"region\": region, \"job\": job}\u00a0 \u00a0 )\u00a0 \u00a0 response = operation.result()\u00a0 \u00a0 # Dataproc job output gets saved to the Google Cloud Storage bucket\u00a0 \u00a0 # allocated to the job. Use a regex to obtain the bucket and blob info.\u00a0 \u00a0 matches = re.match(\"gs://(.*?)/(.*)\", response.driver_output_resource_uri)\u00a0 \u00a0 output = (\u00a0 \u00a0 \u00a0 \u00a0 storage.Client()\u00a0 \u00a0 \u00a0 \u00a0 .get_bucket(matches.group(1))\u00a0 \u00a0 \u00a0 \u00a0 .blob(f\"{matches.group(2)}.000000000\")\u00a0 \u00a0 \u00a0 \u00a0 .download_as_bytes()\u00a0 \u00a0 \u00a0 \u00a0 .decode(\"utf-8\")\u00a0 \u00a0 )\u00a0 \u00a0 print(f\"Job finished successfully: {output}\")\n```\n- [Install the client library](/dataproc/docs/reference/libraries#installing_the_client_library) \n- [Set up application default credentials](/dataproc/docs/reference/libraries#setting_up_authentication) \n- Run the codeSee [Setting Up a Go Development Environment](/go/docs/setup) . [  dataproc/submit_job.go ](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/dataproc/submit_job.go) [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/dataproc/submit_job.go) ```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 \"io/ioutil\"\u00a0 \u00a0 \u00a0 \u00a0 \"log\"\u00a0 \u00a0 \u00a0 \u00a0 \"regexp\"\u00a0 \u00a0 \u00a0 \u00a0 dataproc \"cloud.google.com/go/dataproc/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/dataproc/apiv1/dataprocpb\"\u00a0 \u00a0 \u00a0 \u00a0 \"cloud.google.com/go/storage\"\u00a0 \u00a0 \u00a0 \u00a0 \"google.golang.org/api/option\")func submitJob(w io.Writer, projectID, region, clusterName string) error {\u00a0 \u00a0 \u00a0 \u00a0 // projectID := \"your-project-id\"\u00a0 \u00a0 \u00a0 \u00a0 // region := \"us-central1\"\u00a0 \u00a0 \u00a0 \u00a0 // clusterName := \"your-cluster\"\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // Create the job client.\u00a0 \u00a0 \u00a0 \u00a0 endpoint := fmt.Sprintf(\"%s-dataproc.googleapis.com:443\", region)\u00a0 \u00a0 \u00a0 \u00a0 jobClient, err := dataproc.NewJobControllerClient(ctx, option.WithEndpoint(endpoint))\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 log.Fatalf(\"error creating the job client: %s\\n\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Create the job config.\u00a0 \u00a0 \u00a0 \u00a0 submitJobReq := &dataprocpb.SubmitJobRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ProjectId: projectID,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Region: \u00a0 \u00a0region,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Job: &dataprocpb.Job{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Placement: &dataprocpb.JobPlacement{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClusterName: clusterName,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TypeJob: &dataprocpb.Job_SparkJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 SparkJob: &dataprocpb.SparkJob{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Driver: &dataprocpb.SparkJob_MainClass{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MainClass: \"org.apache.spark.examples.SparkPi\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 JarFileUris: []string{\"file:///usr/lib/spark/examples/jars/spark-examples.jar\"},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Args: \u00a0 \u00a0 \u00a0 \u00a0[]string{\"1000\"},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 submitJobOp, err := jobClient.SubmitJobAsOperation(ctx, submitJobReq)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"error with request to submitting job: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 submitJobResp, err := submitJobOp.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"error submitting job: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 re := regexp.MustCompile(\"gs://(.+?)/(.+)\")\u00a0 \u00a0 \u00a0 \u00a0 matches := re.FindStringSubmatch(submitJobResp.DriverOutputResourceUri)\u00a0 \u00a0 \u00a0 \u00a0 if len(matches) < 3 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"regex error: %s\", submitJobResp.DriverOutputResourceUri)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Dataproc job output gets saved to a GCS bucket allocated to it.\u00a0 \u00a0 \u00a0 \u00a0 storageClient, err := storage.NewClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"error creating storage client: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 obj := fmt.Sprintf(\"%s.000000000\", matches[2])\u00a0 \u00a0 \u00a0 \u00a0 reader, err := storageClient.Bucket(matches[1]).Object(obj).NewReader(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"error reading job output: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer reader.Close()\u00a0 \u00a0 \u00a0 \u00a0 body, err := ioutil.ReadAll(reader)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"could not read output from Dataproc Job: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Job finished successfully: %s\", body)\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```\n- [Install the client library](/dataproc/docs/reference/libraries#installing_the_client_library) \n- [Set up application default credentials](/dataproc/docs/reference/libraries#setting_up_authentication) \n- Run the codeSee [Setting Up a Node.js Development Environment](/nodejs/docs/setup) . [  dataproc/submitJob.js ](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/dataproc/submitJob.js) [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/dataproc/submitJob.js) ```\nconst dataproc = require('@google-cloud/dataproc');const {Storage} = require('@google-cloud/storage');// TODO(developer): Uncomment and set the following variables// projectId = 'YOUR_PROJECT_ID'// region = 'YOUR_CLUSTER_REGION'// clusterName = 'YOUR_CLUSTER_NAME'// Create a client with the endpoint set to the desired cluster regionconst jobClient = new dataproc.v1.JobControllerClient({\u00a0 apiEndpoint: `${region}-dataproc.googleapis.com`,\u00a0 projectId: projectId,});async function submitJob() {\u00a0 const job = {\u00a0 \u00a0 projectId: projectId,\u00a0 \u00a0 region: region,\u00a0 \u00a0 job: {\u00a0 \u00a0 \u00a0 placement: {\u00a0 \u00a0 \u00a0 \u00a0 clusterName: clusterName,\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 sparkJob: {\u00a0 \u00a0 \u00a0 \u00a0 mainClass: 'org.apache.spark.examples.SparkPi',\u00a0 \u00a0 \u00a0 \u00a0 jarFileUris: [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'file:///usr/lib/spark/examples/jars/spark-examples.jar',\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 args: ['1000'],\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 };\u00a0 const [jobOperation] = await jobClient.submitJobAsOperation(job);\u00a0 const [jobResponse] = await jobOperation.promise();\u00a0 const matches =\u00a0 \u00a0 jobResponse.driverOutputResourceUri.match('gs://(.*?)/(.*)');\u00a0 const storage = new Storage();\u00a0 const output = await storage\u00a0 \u00a0 .bucket(matches[1])\u00a0 \u00a0 .file(`${matches[2]}.000000000`)\u00a0 \u00a0 .download();\u00a0 // Output a success message.\u00a0 console.log(`Job finished successfully: ${output}`);\n```\n## Submit a job directly on your cluster\nIf you want to run a job directly on your cluster without using the Dataproc service, [SSH into the master node of your cluster](/dataproc/docs/concepts/accessing/ssh) , then run the job on the master node.\nAfter establishing an SSH connection to the VM master instance, run commands in a terminal window on the cluster's master node to:\n- Open a Spark shell.\n- Run a simple Spark job to count the number of lines in a (seven-line) Python \"hello-world\" file located in a publicly accessible Cloud Storage file.\n- Quit the shell.```\nuser@cluster-name-m:~$ spark-shell\n...\nscala> sc.textFile(\"gs://dataproc-examples\"\n+ \"/pyspark/hello-world/hello-world.py\").count\n...\nres0: Long = 7\nscala> :quit\n```## Run bash jobs on Dataproc\nYou may want to run a bash script as your Dataproc job, either because the engines you use aren't supported as a top-level Dataproc job type or because you need to do additional setup or calculation of arguments before launching a job using `hadoop` or `spark-submit` from your script.\n### Pig example\nAssume you copied an hello.sh bash script into Cloud Storage:\n```\ngsutil cp hello.sh gs://${BUCKET}/hello.sh\n```\nSince the `pig fs` command uses Hadoop paths, copy the script from Cloud Storage to a destination specified as `file:///` to make sure it's on the local filesystem instead of HDFS. The subsequent `sh` commands reference the local filesystem automatically and do not require the `file:///` prefix.\n```\ngcloud dataproc jobs submit pig --cluster=${CLUSTER} --region=${REGION} \\\u00a0 \u00a0 -e='fs -cp -f gs://${BUCKET}/hello.sh file:///tmp/hello.sh; sh chmod 750 /tmp/hello.sh; sh /tmp/hello.sh'\n```\nAlternatively, since the Dataproc jobs submit `--jars` argument stages a file into a temporary directory created for the lifetime of the job, you can specify your Cloud Storage shell script as a `--jars` argument:\n```\ngcloud dataproc jobs submit pig --cluster=${CLUSTER} --region=${REGION} \\\u00a0 \u00a0 --jars=gs://${BUCKET}/hello.sh \\\u00a0 \u00a0 -e='sh chmod 750 ${PWD}/hello.sh; sh ${PWD}/hello.sh'\n```\nNote that the `--jars` argument can also reference a local script:\n```\ngcloud dataproc jobs submit pig --cluster=${CLUSTER} --region=${REGION} \\\u00a0 \u00a0 --jars=hello.sh \\\u00a0 \u00a0 -e='sh chmod 750 ${PWD}/hello.sh; sh ${PWD}/hello.sh'\n```", "guide": "Dataproc"}