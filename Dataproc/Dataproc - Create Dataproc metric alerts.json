{"title": "Dataproc - Create Dataproc metric alerts", "url": "https://cloud.google.com/dataproc/docs/guides/dataproc-alerts", "abstract": "# Dataproc - Create Dataproc metric alerts\nYou can create a Monitoring alert that notifies you when a Dataproc cluster or job metric exceeds a specified threshold.\n", "content": "## Steps to create an alert\nTo create an alert:\n- Open the [Alerting](https://console.cloud.google.com/monitoring/alerting) page in the Google Cloud console.\n- Click **+ Create Policy** to open the **Create alerting policy** page.- Click **Select Metric** .To see all available Dataproc metrics, not only those related to an existing cluster or job, unset \"Show only active resources & metrics\".\n- In the \"Filter by resource or metric name\" input box, type \"dataproc\" to list Dataproc metrics. Navigate through the hierarchy of **Cloud Dataproc** metrics to select a cluster, job, batch, or session metric.\n- Click **Apply** .\n- Click **Next** to open the **Configure alert trigger** pane.\n- Set a threshold value to trigger the alert.\n- Click **Next** to open the **Configure notifications and finalize alert** pane.\n- Set notification channels, documentation, and the alert policy name.\n- Click **Next** to review the alert policy.\n- Click **Create Policy** to create the alert.**Use MQL for custom alerts.** You can select **MQL** on the **Create alerting policy** page to create [Monitoring Query Language alerts](/monitoring/mql/alerts) .\n## Sample alerts\nThis section describes a sample alert for a job submitted to the Dataproc service and an alert for a job run as a YARN application.\n### Long-running Dataproc job alert\nDataproc emits the `dataproc.googleapis.com/job/state` metric, which tracks how long a job has been in different states. This metric is found under the Google Cloud console Metrics Explorer under the **Cloud Dataproc Job** ( [cloud_dataproc_job](/monitoring/api/resources#tag_cloud_dataproc_job) ) resource. You can use this metric to set up an alert that notifies you when the job's `RUNNING` state exceeds a duration threshold.\nThis example uses the [Monitoring Query Language](/monitoring/mql) (MQL) to create an alert policy (see [Creating MQL alerting policies(console)](/monitoring/mql/alerts#create-ql-policy-ui) ).\n```\nfetch cloud_dataproc_job| metric 'dataproc.googleapis.com/job/state'| filter metric.state == 'RUNNING'| group_by [resource.job_id, metric.state], 1m| condition val() == true()\n```\nIn the following example, the alert triggers when a job has been running for more than 30 minutes.\nYou can modify the query by filtering on the `resource.job_id` to apply it to a specific job:\n```\nfetch cloud_dataproc_job| metric 'dataproc.googleapis.com/job/state'| filter (resource.job_id == '1234567890') && (metric.state == 'RUNNING')| group_by [resource.job_id, metric.state], 1m| condition val() == true()\n```\n### Long-running YARN application alert\nThe previous sample shows an alert that is triggered when a Dataproc job runs longer than a specified duration, but it only applies to jobs submitted to the Dataproc service via the Google Cloud console, the Google Cloud CLI, or by direct calls to the Dataproc `jobs` API. You can also use OSS metrics to set up similar alerts that monitor the running time of YARN applications.\nFirst, some background. YARN emits running time metrics into multiple buckets. By default, YARN maintains 60, 300, and 1440 minutes as bucket thresholds and emits 4 metrics, `running_0` , `running_60` , `running_300` and `running_1440` :\n- `running_0` records the number of jobs with a runtime between 0 and 60 minutes.\n- `running_60` records the number of jobs with a runtime between 60 and 300 minutes.\n- `running_300` records the number of jobs with a runtime between 300 and 1440 minutes.\n- `running_1440` records the number of jobs with a runtime greater than 1440 minutes.\nFor example, a job running for 72 minutes will be recorded in `running_60` , but not in `running_0` .\n**Note:** These metrics only provide a count of the number of applications with running times that are within each bucket threshold; they do not identity the names of the applications.\nThese default bucket thresholds can be modified by passing new values to the `yarn:yarn.resourcemanager.metrics.runtime.buckets` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#file-prefixed_properties_table) during Dataproc cluster creation. When defining custom buckets thresholds, you must also define metric overrides. For example, to specify bucket thresholds of 30, 60, and 90 minutes, the `gcloud dataproc clusters create` command should include the following flags:\n- bucket thresholds: `\u2011\u2011properties=yarn:yarn.resourcemanager.metrics.runtime.buckets=30,60,90`\n- metrics overrides: `\u2011\u2011metric-overrides=yarn:ResourceManager:QueueMetrics:running_0, yarn:ResourceManager:QueueMetrics:running_30,yarn:ResourceManager:QueueMetrics:running_60, yarn:ResourceManager:QueueMetrics:running_90`\n**Sample cluster creation command**  [](None)\n```\ngcloud dataproc clusters create test-cluster \\\n --properties ^#^yarn:yarn.resourcemanager.metrics.runtime.buckets=30,60,90 \\\n --metric-sources=yarn \\\n --metric-overrides=yarn:ResourceManager:QueueMetrics:running_0,yarn:ResourceManager:QueueMetrics:running_30,yarn:ResourceManager:QueueMetrics:running_60,yarn:ResourceManager:QueueMetrics:running_90\n```\nThese metrics are listed in the Google Cloud console Metrics Explorer under the **VM Instance** ( [gce_instance](/monitoring/api/resources#tag_gce_instance) ) resource.\n**Note:** Recently defined metrics may not be immediately visible in Cloud Monitoring. Typically, they appear within ten to fifteen minutes after you create a cluster with custom buckets and metric overrides enabled.- [Create a cluster with required buckets and metrics enabled](#sample-cluster-create) .\n- Create an alert policy that triggers when the number of applications in a YARN metric bucket exceed a specified threshold.- Optionally, add a filter to alert on clusters that match a pattern.\n- Configure the threshold for triggering the alert.\n### Failed Dataproc job alert\nYou can also use the `dataproc.googleapis.com/job/state` metric (see [Long-running Dataproc job alert](#long-running_job_alert) ), to alert you when a Dataproc job fails.\nThis example uses the [Monitoring Query Language](/monitoring/mql) (MQL) to create an alert policy (see [Creating MQL alerting policies(console)](/monitoring/mql/alerts#create-ql-policy-ui) ).\n```\nfetch cloud_dataproc_job| metric 'dataproc.googleapis.com/job/state'| filter metric.state == 'ERROR'| group_by [resource.job_id, metric.state], 1m| condition val() == true()\n```\nIn the following example, the alert triggers when any Dataproc job fails in your project.\nYou can modify the query by filtering on the `resource.job_id` to apply it to a specific job:\n```\nfetch cloud_dataproc_job| metric 'dataproc.googleapis.com/job/state'| filter (resource.job_id == '1234567890') && (metric.state == 'ERROR')| group_by [resource.job_id, metric.state], 1m| condition val() == true()\n```\n### Cluster capacity deviation alert\nDataproc emits the `dataproc.googleapis.com/cluster/capacity_deviation` metric, which reports the difference between the expected node count in the cluster and the active YARN node count. You can find this metric in the Google Cloud console [Metrics Explorer](https://console.cloud.google.com/monitoring/metrics-explorer) under the [Cloud Dataproc Cluster](/monitoring/api/resources#tag_cloud_dataproc_cluster) resource. You can use this metric to create an alert that notifies you when cluster capacity deviates from expected capacity for longer than a specified threshold duration.\nThe following operations can cause a temporary under reporting of cluster nodes in the `capacity_deviation` metric. To avoid false positive alerts, set the metric alert threshold to account for these operations:\n- **Cluster creation and updates:** The `capacity_deviation` metric is not emitted during cluster create or update operations.\n- **Cluster initialization actions:** Initialization actions are performed after a node is provisioned.\n- **Secondary worker updates:** Secondary workers are added asynchronously, after the update operation completes.\nThe maximum lookback window for the cluster`capacity_deviation`metric is 7 days. If a cluster update operation does not occur during the previous 7 days, the metric will be empty.\nThis example uses the [Monitoring Query Language](/monitoring/mql) (MQL) to [create an alert policy](/monitoring/mql/alerts#create-ql-policy-ui) .\n```\nfetch cloud_dataproc_cluster| metric 'dataproc.googleapis.com/cluster/capacity_deviation'| every 1m| condition val() <> 0 '1'\n```\nIn the next example, the alert triggers when cluster capacity deviation is non zero for more than 30 minutes.\n## View alerts\nWhen an alert is triggered by a metric threshold condition, Monitoring creates an incident and a corresponding event. You can view incidents from the [Monitoring Alerting](https://console.cloud.google.com/monitoring/alerting) page in the Google Cloud console.\nIf you defined a notification mechanism in the alert policy, such as an email or SMS notification, Monitoring sends a notification of the incident.\n## Whats next\n- See the [Introduction to alerting](/monitoring/alerts) .", "guide": "Dataproc"}