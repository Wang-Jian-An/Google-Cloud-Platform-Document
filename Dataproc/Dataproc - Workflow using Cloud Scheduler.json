{"title": "Dataproc - Workflow using Cloud Scheduler", "url": "https://cloud.google.com/dataproc/docs/tutorials/workflow-scheduler", "abstract": "# Dataproc - Workflow using Cloud Scheduler\n**Objective:** - Create a Dataproc workflow template that runs a [Spark PI job](https://spark.apache.org/examples.html) - - Create a [Cloud Scheduler](/scheduler/docs) job to start the workflow at a specified time.\nIn this document, you use the following billable components of Google Cloud:\n- Dataproc\n- Compute Engine\n- Cloud Scheduler\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) .\n", "content": "## Before you begin\n### Set up your project\n### Create a custom role\n- Open the Open **IAM & Admin \u2192 Roles** page in the Google Cloud console.- Click CREATE ROLE to open the **Create Role** page.\n- Complete the Title, Description, ID, Launch stage fields. Suggestion:  Use \"Dataproc Workflow Template Create\" as the role title.\n- Click ADD PERMISSIONS,- In the **Add Permissions** form, click **Filter** , then select \"Permission\". Complete the filter to read \"Permission: dataproc.workflowTemplates.instantiate\".\n- Click the checkbox to the left of the listed permission, then click ADD.\n- On the Create Role page, click ADD PERMISSIONS again to repeat the previous sub-steps to add the \"iam.serviceAccounts.actAs\" permission to the custom role. The **Create Role** page now lists two permissions.\n- Click CREATE on the **Custom Role** page. The custom role is listed on the **Roles** page.\n### Create a service account\n- In the Google Cloud console, go to the **Service Accounts** page. [Go toService Accounts](https://console.cloud.google.com/iam-admin/serviceaccounts) \n- Select your project.\n- Click add **Create ServiceAccount** .\n- In the **Service account name** field, enter the name `workflow-scheduler` . The Google Cloud console fills in the **Service account ID** field based on this name.\n- Optional: In the **Service account description** field, enter a description for the service account.\n- Click **Create and continue** .\n- Click the **Select a role** field and choose the **Dataproc Workflow TemplateCreate** custom role that you created in the previous step.\n- Click **Continue** .\n- In the **Service account admins role** field, enter your Google account email address.\n- Click **Done** to finish creating the service account.## Create a workflow template.\nCopy and run the commands listed below in a local terminal window or in [Cloud Shell](https://console.cloud.google.com/?cloudshell=true) to create and define a [workflow template](/dataproc/docs/concepts/workflows/using-workflows) .\nNotes:\n- The commands specify the \"us-central1\" [region](/compute/docs/regions-zones#available) . You can specify  a different region or delete the`--region`flag if you have previously run [gcloud config set compute/region](/sdk/gcloud/reference/config/set#compute) to set the region property.\n- The \"-- \" (dash dash space) sequence in the`add-job`command  passes the`1000`argument to the SparkPi job, which specifies  the number of samples to use to estimate the value of Pi.\n- Create the workflow template.```\ngcloud dataproc workflow-templates create sparkpi \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central1\n```\n- Add the spark job to the sparkpi workflow template. The \"compute\" step ID  is required, and identifies the added SparkPi job.```\ngcloud dataproc workflow-templates add-job spark \\\n\u00a0\u00a0\u00a0\u00a0--workflow-template=sparkpi \\\n\u00a0\u00a0\u00a0\u00a0--step-id=compute \\\n\u00a0\u00a0\u00a0\u00a0--class=org.apache.spark.examples.SparkPi \\\n\u00a0\u00a0\u00a0\u00a0--jars=file:///usr/lib/spark/examples/jars/spark-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central1 \\\n\u00a0\u00a0\u00a0\u00a0-- 1000\n```\n- Use a [managed](/dataproc/docs/concepts/workflows/overview#managed_cluster) , [single-node](/dataproc/docs/concepts/configuring-clusters/single-node-clusters) cluster to run the workflow. Dataproc will create the  cluster, run the workflow on it, then delete the cluster when the workflow completes.```\ngcloud dataproc workflow-templates set-managed-cluster sparkpi \\\n\u00a0\u00a0\u00a0\u00a0--cluster-name=sparkpi \\\n\u00a0\u00a0\u00a0\u00a0--single-node \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central1\n```\n- Click on the`sparkpi`name on the Dataproc [Workflows](https://console.cloud.google.com/dataproc/workflows/templates) page in the Google Cloud console to open the **Workflow template details** page. Confirm the sparkpi template  attributes.\n## Create a Cloud Scheduler job\n- Open the **Cloud Scheduler** page in the Google Cloud console (you may need to select your project to open the page). Click CREATE JOB.\n- Enter or select the following job information:- **Select a region:** \"us-central\" or other region where you created your workflow template.\n- **Name:** \"sparkpi\"\n- **Frequency:** \"* * * * *\" selects every minute; \"0 9 * * 1\" selects every Monday at 9 AM. See [Defining the Job Schedule](/scheduler/docs/configuring/cron-job-schedules) for other unix-cron values. **Note:** You will be able to click a RUN NOW button on the Cloud Scheduler **Jobs** in the Google Cloud console to run and test your job regardless of the frequency you set for your job.\n- **Timezone:** Select your. Type \"United States\" to list U.S. timezones.\n- **Target:** \"HTTP\"\n- **URL:** Insert the following URL after inserting. Replace \"us-central1\" if you created your workflow template in a different region. This URL will call the Dataproc [workflowTemplates.instantiate](/dataproc/docs/reference/rest/v1/projects.regions.workflowTemplates/instantiate) API to run your [sparkpi workflow template](#create_a_workflow_template) .```\nhttps://dataproc.googleapis.com/v1/projects/your-project-id/regions/us-central1/workflowTemplates/sparkpi:instantiate?alt=json\n```\n- **HTTP method:** - \"POST\"\n- **Body:** \"{}\"If the scheduled job runs a [parameterized workflow](/dataproc/docs/concepts/workflows/workflow-parameters) , you can pass the parameters to the workflow using a JSON payload in the Body:```\n{\"parameters\": {\"PARAMETER_NAME\": \"VALUE\"}}\n```\n- **Auth header:** - \"Add OAuth token\"\n- **Service account:** Insert theof the [service account that you created for this tutorial](#create_a_service_account_for_this_tutorial) . You can use the following account address after inserting:```\nworkflow-scheduler@your-project-id.iam.gserviceaccount\n```\n- **Scope:** You can ignore this item.\n- Click CREATE.\n## Test your scheduled workflow job\n- On the `sparkpi` job row on the [Cloud Scheduler](https://console.cloud.google.com/cloudscheduler)  **Jobs** page, click RUN NOW.\n- Wait a few minutes, then open the Dataproc **Workflows** page to verify that the sparkpi workflow completed.\n- After the workflow deletes the managed cluster, job details persist in the Google Cloud console. Click the `compute...` job listed on the Dataproc [Jobs](https://console.cloud.google.com/dataproc/jobs) page to view workflow job details.## Cleaning up\nThe workflow in this tutorial deletes its managed cluster when the workflow completes. Keeping the workflow allows you to rerun the workflow and does not incur charges. You can delete other resources created in this tutorial to avoid recurring costs.\n### Deleting a project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Deleting your workflow template\n```\ngcloud dataproc workflow-templates delete sparkpi \\\n\u00a0\u00a0\u00a0\u00a0--region=us-central1\n```\n### Deleting your Cloud Schedule job\nOpen the [Cloud Scheduler](https://console.cloud.google.com/cloudscheduler)  **Jobs** page in the Google Cloud console, select the box to the left of the `sparkpi` function, then click DELETE.\n### Deleting your service account\nOpen the **IAM & Admin \u2192 Service Accounts** page in the Google Cloud console, select the box to the left of the `workflow-scheduler...` service account, then click DELETE.\nWhat's next\n- See [Overview of Dataproc Workflow Templates](/dataproc/docs/concepts/workflows/overview) \n- See [Workflow scheduling solutions](/dataproc/docs/concepts/workflows/workflow-schedule-solutions)", "guide": "Dataproc"}