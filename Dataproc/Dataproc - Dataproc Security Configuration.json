{"title": "Dataproc - Dataproc Security Configuration", "url": "https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/security", "abstract": "# Dataproc - Dataproc Security Configuration\nWhen you create a Dataproc cluster, you can enable [Hadoop Secure Mode](https://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-common/SecureMode.html) via [Kerberos](https://web.mit.edu/kerberos/#what_is) to provide multi-tenancy via user authentication, isolation, and encryption inside a Dataproc cluster.\n**User Authentication and Other Google Cloud Platform Services** . Per-user authentication via Kerberos only applies within the cluster. Interactions with other Google Cloud services, such as Cloud Storage, continue to be authenticated as the service account for the cluster.\n**Note:** When `fs.defaultFS` is set to a Cloud Storage location, HDFS requests do not go through the HDFS NameNode, and `authN` and `authZ` are not performed through Kerberos. Therefore, HDFS will not require a Kerberos ticket.\n", "content": "## Enabling Hadoop Secure Mode via Kerberos\nEnabling Kerberos and Hadoop Secure Mode for a cluster will include the [MIT distribution of Kerberos](https://web.mit.edu/kerberos/#what_is) and configure [Apache Hadoop YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) , [HDFS](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html) , [Hive](https://hive.apache.org/index.html) , [Spark](https://spark.apache.org/) , and related components to use it for authentication.\nEnabling Kerberos creates an on-cluster [Key Distribution Center (KDC)](https://en.wikipedia.org/wiki/Key_distribution_center) , that contains service principals and a root principal. The root principal is the account with administrator permissions to the on-cluster KDC. It can also contain standard user principals or be connected via [cross-realm trust](#cross-realm-trust) to another KDC that contains the user principals.\n## Create a Kerberos cluster\nYou can use the Google Cloud CLI, the Dataproc API, or the Google Cloud console to enable Kerberos on clusters that use Dataproc [image version 1.3](/dataproc/docs/concepts/versioning/dataproc-release-1.3) and later.\n[](None) \nTo automatically configure a new Kerberos Dataproc cluster (image version 1.3 and later), use the [gcloud dataproc clusters create](/sdk/gcloud/reference/dataproc/clusters/create) command.\n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--image-version=2.0 \\\n\u00a0\u00a0\u00a0\u00a0--enable-kerberos\n```\n **Cluster property: ** Instead of using the `--enable-kerberos` flag as shown above, you can automatically configure Kerberos by passing the `--properties \"dataproc:kerberos.beta.automatic-config.enable=true\"` flag to the clusters create command (see [Dataproc service properties](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) ).Kerberos clusters can be created through the [ClusterConfig.SecurityConfig.KerberosConfig](/dataproc/docs/reference/rest/v1/ClusterConfig#kerberosconfig) as part of a [clusters.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) request. You must set `enableKerberos` to `true` .You can automatically configure Kerberos on a new cluster by   selecting \"Enable\" from the Kerberos and Hadoop Secure Mode section of   the Manage security panel on the Dataproc [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page of the Google Cloud console.\n## Create a Kerberos cluster With Your Own Root Principal Password\nFollow the steps below to set up a Kerberos cluster that uses your root principal password.\n### Set up your Kerberos root principal password\nThe Kerberos root principal is the account with administrator permissions to the on-cluster KDC. To securely provide the password for The Kerberos root principal, users can encrypt it with a [Key Management Service (KMS)](/kms/docs/encrypt-decrypt) key, and then store it in a [Google Cloud Storage bucket](/storage/docs/creating-buckets) that the cluster [service account](/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) can access. The cluster service account must be granted the `cloudkms.cryptoKeyDecrypter` [IAM role](/kms/docs/reference/permissions-and-roles) .\n- Grant the Cloud KMS CryptoKey Encrypter/Decrypter role to the [cluster service account](/dataproc/docs/concepts/configuring-clusters/service-accounts#service_accounts_in_dataproc) :```\ngcloud projects add-iam-policy-binding project-id \\\n\u00a0\u00a0\u00a0\u00a0--member serviceAccount:project-number-compute@developer.gserviceaccount.com \\\n\u00a0\u00a0\u00a0\u00a0--role roles/cloudkms.cryptoKeyDecrypter\n```\n- Create a key ring:```\ngcloud kms keyrings create my-keyring --location global\n```\n- Create a key in the key ring:```\ngcloud kms keys create my-key \\\n\u00a0\u00a0\u00a0\u00a0--location global \\\n\u00a0\u00a0\u00a0\u00a0--keyring my-keyring \\\n\u00a0\u00a0\u00a0\u00a0--purpose encryption\n```\n- Encrypt your Kerberos root principal password:```\necho \"my-password\" | \\\n\u00a0\u00a0gcloud kms encrypt \\\n\u00a0\u00a0\u00a0\u00a0--location=global \\\n\u00a0\u00a0\u00a0\u00a0--keyring=my-keyring \\\n\u00a0\u00a0\u00a0\u00a0--key=my-key \\\n\u00a0\u00a0\u00a0\u00a0--plaintext-file=- \\\n\u00a0\u00a0\u00a0\u00a0--ciphertext-file=kerberos-root-principal-password.encrypted\n```- Upload the encrypted password to a [Cloud Storage bucket](/storage/docs/creating-buckets) in your project.- **Example** :```\ngsutil cp kerberos-root-principal-password.encrypted gs://my-bucket\n```\n### Create the cluster\nYou can use the `gcloud` command or the Dataproc API to enable Kerberos on clusters with your own root principal password.\n[](None) \nTo create a Kerberos Dataproc cluster (image version 1.3 and later), use the [gcloud dataproc clusters create](/sdk/gcloud/reference/dataproc/clusters/create) command.\n```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--image-version=2.0 \\\n\u00a0\u00a0\u00a0\u00a0--kerberos-root-principal-password-uri=gs://my-bucket/kerberos-root-principal-password.encrypted \\\n\u00a0\u00a0\u00a0\u00a0--kerberos-kms-key=projects/project-id/locations/global/keyRings/my-keyring/cryptoKeys/my-key\n```\n [](None) \n **Use a YAML (or JSON) config file.** Instead of passing `kerberos-*` flags to the `gcloud` command as shown above, you can place kerberos settings in a YAML (or JSON) config file, then reference the config file to create the kerberos cluster.- Create a config file (see [SSL Certificates](#ssl_certificates) , [Additional Kerberos Settings](#additional_kerberos_settings) , and [Cross-realm trust](#cross-realm_trust) for additional config settings that can be included in the file):```\nroot_principal_password_uri: gs://my-bucket/kerberos-root-principal-password.encryptedkms_key_uri: projects/project-id/locations/global/keyRings/mykeyring/cryptoKeys/my-key\n```\n- Use the following`gcloud`command to create the kerberos cluster:```\ngcloud dataproc clusters create cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--kerberos-config-file=local path to config-file \\\n\u00a0\u00a0\u00a0\u00a0--image-version=2.0\n```\n **Security Considerations.** Dataproc discards the decrypted form of the password after adding the root principal to the KDC. For security purposes, after creating the cluster you may decide to delete the password file and the key used to decrypt the secret, and remove the service account from the `kmsKeyDecrypter` role. Don't do this if you plan on scaling the cluster up, which requires the password file and key, and the service account role.Kerberos clusters can be created through the [ClusterConfig.SecurityConfig.KerberosConfig](/dataproc/docs/reference/rest/v1/ClusterConfig#kerberosconfig) as part of a [clusters.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) request. Set `enableKerberos` to true and set the `rootPrincipalPasswordUri` and `kmsKeyUri` fields.When [creating a cluster](https://console.cloud.google.com/dataproc/clustersAdd) with image version 1.3+, select \"Enable\" from the Kerberos and Hadoop Secure Mode  section of the Manage security panel on the Dataproc [Create a cluster](https://console.cloud.google.com/dataproc/clustersAdd) page of the Google Cloud console,  then complete the security options (discussed in the following sections).\n## OS Login\nOn-cluster KDC management can be performed with the `kadmin` command using the root Kerberos user principal or using `sudo kadmin.local` . [Enable OS Login](/compute/docs/instances/managing-instance-access) to control who can run superuser commands.\n## SSL Certificates\nAs part of enabling Hadoop Secure Mode, Dataproc creates a self-signed certificate to enable cluster SSL encryption. As an alternative, you can provide a certificate for cluster SSL encryption by adding the following settings to the [configuration file](#kerberos-config) when you [create a kerberos cluster](#gcloud-create-kerberos-cluster) :\n- `ssl:keystore_password_uri`: Location in Cloud Storage of the KMS-encrypted file containing the password to the keystore file.\n- `ssl:key_password_uri`: Location in Cloud Storage of the KMS-encrypted file containing the password to the key in the keystore file.\n- `ssl:keystore_uri`: Location in Cloud Storage of the keystore file containing the wildcard certificate and the private key used by cluster nodes.\n- `ssl:truststore_password_uri`: Location in Cloud Storage of the KMS-encrypted file that contains the password to the truststore file.\n- `ssl:truststore_uri`: Location in Cloud Storage of the trust store file containing trusted certificates.\nSample config file:\n```\nroot_principal_password_uri: gs://my-bucket/kerberos-root-principal-password.encryptedkms_key_uri: projects/project-id/locations/global/keyRings/mykeyring/cryptoKeys/my-keyssl:\u00a0 key_password_uri: gs://bucket/key_password.encrypted\u00a0 keystore_password_uri: gs://bucket/keystore_password.encrypted\u00a0 keystore_uri: gs://bucket/keystore.jks\u00a0 truststore_password_uri: gs://bucket/truststore_password.encrypted\u00a0 truststore_uri: gs://bucket/truststore.jks\n```\n[](None)\n## Additional Kerberos Settings\nTo specify a Kerberos realm, [create a kerberos cluster](#gcloud-create-kerberos-cluster) with the following property added in the Kerberos [configuration file](#kerberos-config) :\n- `realm`: The name of the on-cluster Kerberos realm.\nIf this property is not set, the hostnames' domain (in uppercase) will be the realm.\nTo specify the master key of the KDC database, [create a kerberos cluster](#gcloud-create-kerberos-cluster) with the following property added in the Kerberos [configuration file](#kerberos-config) :\n- `kdc_db_key_uri`: Location in Cloud Storage of the KMS-encrypted file containing the KDC database master key.\nIf this property is not set, Dataproc will generate the master key.\nTo specify the ticket granting ticket's maximum lifetime (in hours), [create a kerberos cluster](#gcloud-create-kerberos-cluster) with the following property added in the Kerberos [configuration file](#kerberos-config) :\n- `tgt_lifetime_hours`: Max life time of the ticket granting ticket in hours.\nIf this property is not set, Dataproc will set the ticket granting ticket's life time to 10 hours.\n[](None)\n## Cross-realm trust\nThe KDC on the cluster initially contains only the root administrator principal and service principals. You can add user principals manually or establish a cross-realm trust with an external KDC or Active Directory server that holds user principals. [Cloud VPN](https://cloud.google.com/network-connectivity/docs/vpn/) or [Cloud Interconnect](https://cloud.google.com/network-connectivity/docs/interconnect/) is recommended to connect to an on-premise KDC/Active Directory,.\nTo create a kerberos cluster that supports cross-realm trust, add the settings listed below to the Kerberos [configuration file](#kerberos-config) when you [create a kerberos cluster](#gcloud-create-kerberos-cluster) . Encrypt the shared password with KMS and store it in a Cloud Storage bucket that the cluster service account can access.\n- `cross_realm_trust:admin_server`: hostname/address of the remote admin server.\n- `cross_realm_trust:kdc`: hostname/address of the remote KDC.\n- `cross_realm_trust:realm`: name of the remote realm to be trusted.\n- `cross_realm_trust:shared_password_uri`: Location in Cloud Storage of the KMS-encrypted shared password.\nSample config file:\n```\nroot_principal_password_uri: gs://my-bucket/kerberos-root-principal-password.encryptedkms_key_uri: projects/project-id/locations/global/keyRings/mykeyring/cryptoKeys/my-keycross_realm_trust:\u00a0 admin_server: admin.remote.realm\u00a0 kdc: kdc.remote.realm\u00a0 realm: REMOTE.REALM\u00a0 shared_password_uri: gs://bucket/shared_password.encrypted\n```\nTo enable cross-realm trust to a remote KDC:\n- Add the following in the `/etc/krb5.conf` file in the remote KDC:```\n[realms]DATAPROC.REALM = {\u00a0 kdc = MASTER-NAME-OR-ADDRESS\u00a0 admin_server = MASTER-NAME-OR-ADDRESS}\n```\n- Create the trust user:```\nkadmin -q \"addprinc krbtgt/DATAPROC.REALM@REMOTE.REALM\"\n```\n- When prompted, enter the user's password. The password should match the contents of the encrypted shared password file\nTo enable cross-realm trust with Active Directory, run the following commands in a PowerShell as Administrator:\n- Create a KDC definition in Active Directory.```\nksetup /addkdc DATAPROC.REALM DATAPROC-CLUSTER-MASTER-NAME-OR-ADDRESS\n```\n- Create trust in Active Directory.```\nnetdom trust DATAPROC.REALM /Domain AD.REALM /add /realm /passwordt:TRUST-PASSWORD\n```The password should match the contents of the encrypted shared password file.## dataproc principal\nWhen you submit a job via the Dataproc [jobs API](/dataproc/docs/guides/submit-job#submitting_a_job) to a Dataproc kerberos cluster, it runs as the `dataproc` kerberos principal from the cluster's kerberos realm.\nMulti-tenancy is supported within a Dataproc kerberos cluster if you [submit a job directly](/dataproc/docs/guides/submit-job#submit_a_job_directly_on_your_cluster) , to the cluster, for example via SSH. However, if the job reads or writes to other Google Cloud services, such as Cloud Storage, the job acts as the [cluster's service account](/dataproc/docs/concepts/configuring-clusters/service-accounts) .\n## Default and Custom Cluster Properties\nHadoop secure mode is configured with [properties in config files](https://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-common/SecureMode.html#Configuration) . Dataproc sets default values for these properties.\nYou can override the default properties when you create the cluster with the [gcloud dataproc clusters create](/sdk/gcloud/reference/dataproc/clusters/create#--properties) `--properties` flag or by calling the [clusters.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) API and setting [SoftwareConfig properties](/dataproc/docs/reference/rest/v1/ClusterConfig#softwareconfig) (see [cluster properties examples](/dataproc/docs/concepts/configuring-clusters/cluster-properties#examples) ).\n## High-Availability Mode\nIn [High Availability (HA) mode](/dataproc/docs/concepts/configuring-clusters/high-availability) , a kerberos cluster will have 3 KDCs: one on each master. The KDC running on the \"first\" master ( `$CLUSTER_NAME-m-0` ) will be the Master KDC and also serve as the Admin Server. The Master KDC's database will be synced to the two replica KDCs at 5 minute intervals through a cron job, and the 3 KDCs will serve read traffic.\nKerberos does not natively support real-time replication or automatic failover if the master KDC is down. To perform a manual failover:\n- On all KDC machines, in`/etc/krb5.conf`, change`admin_server`to the new Master's FQDN (Fully Qualified Domain Name). Remove the old Master from the KDC list.\n- On the new Master KDC, set up a cron job to propagate the database.\n- On the new Master KDC, restart the admin_server process (`krb5-admin-server`).\n- On all KDC machines, restart the KDC process (`krb5-kdc`).## Network Configuration\nTo make sure that worker nodes can talk to the KDC and Kerberos Admin Server running on the master(s), verify that the [VPC firewall rules](/vpc/docs/firewalls) allow ingress TCP and UDP traffic on port 88 and ingress TCP traffic on port 749 on the master(s). In High-Availability mode, make sure that VPC firewall rules allow ingress TCP traffic on port 754 on the masters to allow the propagation of changes made to the master KDC. Kerberos requires reverse DNS to be properly set up. Also, for host-based service principal canonicalization, make sure reverse DNS is properly set up for the cluster's network.\n## For More Information\nSee the [MIT Kerberos Documentation](https://web.mit.edu/kerberos/krb5-devel/doc/admin/install_kdc.html) .", "guide": "Dataproc"}