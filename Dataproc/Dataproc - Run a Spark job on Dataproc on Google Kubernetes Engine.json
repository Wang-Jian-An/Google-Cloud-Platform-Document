{"title": "Dataproc - Run a Spark job on Dataproc on Google Kubernetes Engine", "url": "https://cloud.google.com/dataproc/docs/guides/dpgke/quickstarts/dataproc-gke-quickstart-create-cluster", "abstract": "# Dataproc - Run a Spark job on Dataproc on Google Kubernetes Engine\n**Objective:** Create a Dataproc on GKE virtual cluster, then run a Spark job on the cluster.\n", "content": "## Before you begin\n- You must have created a standard (not autopilot) Google Kubernetes Engine (GKE) [zonal or regional cluster](/kubernetes-engine/docs/how-to#creating-clusters) that has [Workload Identity](/kubernetes-engine/docs/how-to/workload-identity) enabled on the cluster. **Performance tip:** Enable [image streaming](/kubernetes-engine/docs/how-to/image-streaming) for faster workload initialization.## Create a Dataproc on GKE virtual cluster\nA Dataproc on GKE virtual cluster is created as the deployment platform for Dataproc components. It's a **virtual** resource, and unlike a Dataproc on Compute Engine cluster, does not include separate Dataproc master and worker VMs.\n- Dataproc on GKE creates node pools within a GKE cluster when you create a Dataproc on GKE virtual cluster.\n- Dataproc on GKE jobs are run as pods on these node pools. The node pools and scheduling of pods on the node pools are managed by GKE.\n- **Create multiple virtual clusters.** You can create and run multiple virtual clusters on a GKE cluster to obtain improved resource utilization by sharing node pools across the virtual clusters.- Each virtual cluster:- is created with separate properties, including Spark engine version and workload identity\n- is isolated within a separate GKE namespace on the GKE cluster\n**Note:** Deletion of one or more Dataproc on GKE clusters does not delete associated node pools. Node pools are deleted when the GKE cluster is deleted (see [Node pool deletion](/dataproc/docs/guides/dpgke/dataproc-gke-nodepools#node_pool_deletion) ).\n- In the Google Cloud console, go to the Dataproc **Clusters** page. [Go to Clusters](https://console.cloud.google.com/dataproc/clusters) \n- Click **Create cluster** .\n- In the **Create Dataproc cluster** dialog, click **Create** in the **Cluster on GKE** row.\n- In the **Set up cluster** panel:- In the **Cluster Name** field, enter a name for the cluster.\n- In the **Region** list, select a [region](/compute/docs/regions-zones/regions-zones#available) for the Dataproc on GKE virtual cluster. This region must be the same region where your existing GKE cluster is located (which you select in the next item).\n- In the **Kubernetes Cluster** field, click **Browse** to select the region where your existing GKE cluster is located.\n- Optional: In the **Cloud Storage staging bucket** field, you can click **Browse** to select an existing Cloud Storage bucket. Dataproc on GKE will stage artifacts in the bucket. Ignore this field to have Dataproc on GKE create a staging bucket.\n- In the left panel, click **Configure Node pools** , then in the **Node pools** panel, click **Add a pool** .- To reuse an existing Dataproc on GKE node pool:- Click **Reuse existing node pool** .\n- Input the name of the existing node pool and select its [Role](/dataproc/docs/reference/rest/v1/GkeClusterConfig#Role) . At least one node pool must have the DEFAULT role.\n- Click **Done** .\n- To create a new Dataproc on GKE node pool:- Click **Create a new node pool** .\n- Input the following node pool values:- Node pool name\n- [Role](/dataproc/docs/reference/rest/v1/projects.regions.clusters#Role) : At least one node pool must have the DEFAULT role.\n- [Location](/dataproc/docs/reference/rest/v1/projects.regions.clusters#GkeNodePoolConfig.FIELDS.locations) : Specify a zone within the Dataproc on GKE cluster region.\n- Node pool [machine type](/dataproc/docs/reference/rest/v1/projects.regions.clusters#GkeNodeConfig.FIELDS.machine_type) \n- [CPU platform](/dataproc/docs/reference/rest/v1/projects.regions.clusters#GkeNodeConfig.FIELDS.min_cpu_platform) \n- [Preemptibility](/dataproc/docs/reference/rest/v1/projects.regions.clusters#GkeNodeConfig.FIELDS.preemptible) \n- [Min](/dataproc/docs/reference/rest/v1/projects.regions.clusters#GkeNodePoolAutoscalingConfig.FIELDS.min_node_count) : Minimum node count.\n- [Max](/dataproc/docs/reference/rest/v1/projects.regions.clusters#GkeNodePoolAutoscalingConfig.FIELDS.max_node_count) : Maximum node count. The maximum node count must be greater than 0.\n- Click **Add a pool** to add more node pools. All node pools must have the location. You can add a total of four node pools.\n- (Optional) If you have set up a [Dataproc Persistent History Server (PHS)](/dataproc/docs/concepts/jobs/history-server#set_up_a_persistent_history_server) to use to view Spark job history, on active and deleted Dataproc on GKE clusters, click **Customize cluster** . Then in the **History server cluster** field, browse for and choose your PHS cluster. The PHS cluster must be located in the same region as the Dataproc on GKE virtual cluster.\n- Click **Create** to create the Dataproc cluster. Your Dataproc on GKE cluster appears in a list on the **Clusters** page. Its status is **Provisioning** until the cluster is ready to use, and then the status changes to **Running** .Set environment variables, then run the [gcloud dataproc clusters gke create](/sdk/gcloud/reference/dataproc/clusters/gke/create) command locally or in Cloud Shell to create a Dataproc on GKE cluster.\n- Set environment variables:```\nDP_CLUSTER=Dataproc on GKE cluster-name \\\n\u00a0\u00a0REGION=region \\\n\u00a0\u00a0GKE_CLUSTER=GKE cluster-name \\\n\u00a0\u00a0BUCKET=Cloud Storage bucket-name \\\n\u00a0\u00a0DP_POOLNAME=node pool-name\n\u00a0\u00a0PHS_CLUSTER=Dataproc PHS server name\n```Notes:- `DP_CLUSTER`: Set the Dataproc virtual cluster name, which must start with a lowercase letter, followed by up to 54 lowercase letters, numbers, or hyphens. It and cannot end with a hyphen.\n- `REGION`: Themust be the same as the region where the GKE cluster is located.\n- `GKE_CLUSTER`: The name of your existing GKE cluster.\n- `BUCKET`: (Optional) You can specify the name of a [Cloud Storage bucket](/storage/docs/creating-buckets) , which Dataproc will use to stage artifacts. If you do not specify a bucket, Dataproc on GKE will create a staging bucket.\n- `DP_POOLNAME`: The name of a [node pool](/kubernetes-engine/docs/concepts/node-pools) to create on the GKE cluster.\n- `PHS_CLUSTER`: (Optional) [Dataproc PHS Server](/dataproc/docs/concepts/jobs/history-server#set_up_a_persistent_history_server) to use to view Spark job history on active and deleted Dataproc on GKE clusters. The PHS cluster must be located in the same region as the Dataproc on GKE virtual cluster.\n- Run the command:```\ngcloud dataproc clusters gke create ${DP_CLUSTER} \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--gke-cluster=${GKE_CLUSTER} \\\n\u00a0\u00a0\u00a0\u00a0--spark-engine-version=latest \\\n\u00a0\u00a0\u00a0\u00a0--staging-bucket=${BUCKET} \\\n\u00a0\u00a0\u00a0\u00a0--pools=\"name=${DP_POOLNAME},roles=default\" \\\n\u00a0\u00a0\u00a0\u00a0--setup-workload-identity \\\n\u00a0\u00a0\u00a0\u00a0--history-server-cluster=${PHS_CLUSTER}\n```Notes:- `--spark-engine-version`: The [Spark image version](/dataproc/docs/guides/dpgke/dataproc-gke-versions) used on the Dataproc cluster. You can use an identifier, such as`3`,`3.1`, or`latest`, or you can specify the full subminor version, such as`3.1-dataproc-5`.\n- `--staging-bucket`: Delete this flag to have Dataproc on GKE create a staging bucket.\n- `--pools`: This flag is used to specify a new or existing node pool that Dataproc will create or use to perform the workload. List [Dataproc on GKE node pool settings](/dataproc/docs/guides/dpgke/dataproc-gke-nodepools#node_pool_settings) , separated by commas, for example:```\n--pools=name=dp-default,roles=default,machineType=e2-standard-4,min=0,max=10\n```You must specify the node pool`name`and`role`. Other node pool settings are optional. You can use multiple`--pools`flags to specify multiple node pools. At least one node pool must have the`default`role. All node pools must have the same location.\n- `--setup-workload-identity`: This flag enables [Workload Identity](/kubernetes-engine/docs/how-to/workload-identity) bindings. These bindings allow the Kubernetes service accounts (KSAs) to act as the default [Dataproc VM Service Account (Data Plane identity)](/dataproc/docs/concepts/iam/dataproc-principals#vm_service_account_data_plane_identity) of the virtual cluster.You need elevated permissions to set workload identity on a Google service account (GSA) (see [Dataproc on Google Kubernetes Engine IAM Permissions](/dataproc/docs/guides/dpgke/dataproc-gke-iam#data_plane_identity) ). To use your own GSA with your Dataproc on GKE virtual cluster, see [Custom IAM configuration](/dataproc/docs/guides/dpgke/dataproc-gke-iam#custom_iam_configuration) .\nComplete a [virtualClusterConfig](/dataproc/docs/reference/rest/v1/projects.regions.clusters#VirtualClusterConfig) as part of a Dataproc API [cluster.create](/dataproc/docs/reference/rest/v1/projects.regions.clusters/create) request.\nBefore using any of the request data, make the following replacements:- : Google Cloud project ID\n- : Dataproc virtual cluster [region](/compute/docs/regions-zones/regions-zones#available) (same region as the existing GKE cluster region)\n- : Dataproc cluster name\n- : GKE cluster name\n- : Node pool name\n- : [Persistent History Server (PHS) cluster name](/dataproc/docs/concepts/jobs/history-server) \n- : (Optional) Staging bucket name. Leave this empty to have Dataproc on GKE create a staging bucket.\nHTTP method and URL:\n```\nPOST https://dataproc.googleapis.com/v1/projects/project-id/regions/region/clusters\n```\nRequest JSON body:\n```\n{\n \"clusterName\":\"DP_CLUSTER\",\n \"projectId\":\"PROJECT\",\n \"virtualClusterConfig\":{\n \"auxiliaryServicesConfig\":{\n  \"sparkHistoryServerConfig\":{\n  \"dataprocCluster\":\"projects/PROJECT/regions/REGION/clusters/PHS_CLUSTER\"\n  }\n },\n \"kubernetesClusterConfig\":{\n  \"gkeClusterConfig\":{\n  \"gkeClusterTarget\":\"projects/PROJECT/locations/REGION/clusters/GKE_CLUSTER\",\n  \"nodePoolTarget\":[   {\n\"nodePool\":\"projects/PROJECT/locations/REGION/clusters/GKE_CLUSTER/nodePools/NODE_POOL\",\n   \"roles\":[    \"DEFAULT\"\n   ]\n   }\n  ]\n  },\n  \"kubernetesSoftwareConfig\":{\n  \"componentVersion\":{\n   \"SPARK\":\"latest\"\n  }\n  }\n },\n \"stagingBucket\":\"BUCKET\"\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"projectId\":\"PROJECT\",\n \"clusterName\":\"DP_CLUSTER\",\n \"status\":{\n \"state\":\"RUNNING\",\n \"stateStartTime\":\"2022-04-01T19:16:39.865716Z\"\n },\n \"clusterUuid\":\"98060b77-...\",\n \"statusHistory\":[ {\n  \"state\":\"CREATING\",\n  \"stateStartTime\":\"2022-04-01T19:14:27.340544Z\"\n }\n ],\n \"labels\":{\n \"goog-dataproc-cluster-name\":\"DP_CLUSTER\",\n \"goog-dataproc-cluster-uuid\":\"98060b77-...\",\n \"goog-dataproc-location\":\"REGION\",\n \"goog-dataproc-environment\":\"prod\"\n },\n \"virtualClusterConfig\":{\n \"stagingBucket\":\"BUCKET\",\n \"kubernetesClusterConfig\":{\n  \"kubernetesNamespace\":\"dp-cluster\",\n  \"gkeClusterConfig\":{\n\"gkeClusterTarget\":\"projects/PROJECT/locations/REGION/clusters/GKE_CLUSTER\",\n  \"nodePoolTarget\":[   {\n\"nodePool\":\"projects/PROJECT/locations/REGION/clusters/GKE_CLUSTER/nodePools/NODE_POOL\",\n   \"roles\":[    \"DEFAULT\"\n   ]\n   }\n  ]\n  },\n  \"kubernetesSoftwareConfig\":{\n  \"componentVersion\":{\n   \"SPARK\":\"3.1-...\"\n  },\n  \"properties\":{\n   \"dpgke:dpgke.unstable.outputOnly.endpoints.sparkHistoryServer\":\"https://...\",\n   \"spark:spark.eventLog.dir\":\"gs://BUCKET/.../spark-job-history\",\n   \"spark:spark.eventLog.enabled\":\"true\"\n  }\n  }\n },\n \"auxiliaryServicesConfig\":{\n  \"sparkHistoryServerConfig\":{\n  \"dataprocCluster\":\"projects/PROJECT/regions/REGION/clusters/PHS_CLUSTER\"\n  }\n }\n }\n```\n## Submit a Spark job\nAfter your Dataproc on GKE virtual cluster is running, [submit a Spark job](/dataproc/docs/guides/submit-job#submit_a_spark_job) using the Google Cloud console, [gcloud CLI](/sdk/gcloud/reference/dataproc/jobs/submit) , or the Dataproc [jobs.submit](/dataproc/docs/reference/rest/v1/projects.regions.jobs/submit) API (by using direct HTTP requests or the [Cloud Client Libraries](/apis/docs/cloud-client-libraries) ).\n**Note:** In the following examples, the job jars are pre-installed and run \"locally\" on the Dataproc virtual cluster.\n**gcloud CLI Spark job example:**\n```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--cluster=${DP_CLUSTER} \\\n\u00a0\u00a0\u00a0\u00a0--class=org.apache.spark.examples.SparkPi \\\n\u00a0\u00a0\u00a0\u00a0--jars=local:///usr/lib/spark/examples/jars/spark-examples.jar \\\n\u00a0\u00a0\u00a0\u00a0-- 1000\n```\n**gcloud CLI PySpark job example:**\n```\ngcloud dataproc jobs submit pyspark \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--cluster=${DP_CLUSTER} \\\n\u00a0\u00a0\u00a0\u00a0local:///usr/lib/spark/examples/src/main/python/pi.py \\\n\u00a0\u00a0\u00a0\u00a0-- 10\n```\n**gcloud CLI SparkR job example:**\n```\ngcloud dataproc jobs submit spark-r \\\n\u00a0\u00a0\u00a0\u00a0--region=${REGION} \\\n\u00a0\u00a0\u00a0\u00a0--cluster=${DP_CLUSTER} \\\n\u00a0\u00a0\u00a0\u00a0local:///usr/lib/spark/examples/src/main/r/dataframe.R\n```\n## Clean up\n- Delete any of the following resources used in this quickstart that you do not want to continue to use.\n- [Delete the Dataproc on GKE cluster](/dataproc/docs/guides/dpgke/dataproc-gke-delete-cluster) .\n- [Delete node pools](/kubernetes-engine/docs/how-to/node-pools#deleting_a_node_pool) used by the Dataproc on GKE cluster.\n- [Delete the GKE cluster](/kubernetes-engine/docs/how-to/deleting-a-cluster) .", "guide": "Dataproc"}