{"title": "Dataproc - Manage Java and Scala dependencies for Apache Spark", "url": "https://cloud.google.com/dataproc/docs/guides/manage-spark-dependencies", "abstract": "# Dataproc - Manage Java and Scala dependencies for Apache Spark\nSpark applications often depend on third-party Java or Scala libraries. Here are recommended approaches to including these dependencies when you submit a Spark job to a Dataproc cluster:\n- When submitting a job from your local machine with the [gcloud dataproc jobs submit](/dataproc/docs/guides/submit-job#submitting_a_job) command, use the `--properties spark.jars.packages=[DEPENDENCIES]` flag.  **Example:** ```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--cluster=my-cluster \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--properties=spark.jars.packages='com.google.cloud:google-cloud-translate:1.35.0,org.apache.bahir:spark-streaming-pubsub_2.11:2.2.0'\n```\n- When [submitting a job directly on your cluster](/dataproc/docs/guides/submit-job#submit_a_job_directly_on_your_cluster) use the `spark-submit` command with the `--packages=[DEPENDENCIES]` parameter.  **Example:** ```\nspark-submit --packages='com.google.cloud:google-cloud-translate:1.35.0,org.apache.bahir:spark-streaming-pubsub_2.11:2.2.0'\n```", "content": "## Avoiding dependency conflicts\nThe above approaches may fail if Spark application dependencies conflict with Hadoop's dependencies. This conflict can arise because Hadoop injects its dependencies into the application's [classpath](https://en.wikipedia.org/wiki/Classpath_(Java)) , so its dependencies take precedence over the application's dependencies. When a conflict occurs, `NoSuchMethodError` or other errors can be generated.\n**Example:**  [Guava](https://github.com/google/guava) is the Google core library for Java that is used by many libraries and frameworks, including Hadoop. A dependency conflict can occur if a job or its dependencies require a version of Guava that is newer than the one used by Hadoop.\nHadoop v3.0 [resolved](https://issues.apache.org/jira/browse/HADOOP-11804) this issue , but applications that rely on earlier Hadoop versions require the following two-part workaround to avoid possible dependency conflicts.\n- Create a single JAR that contains the application's package and all of its dependencies.\n- Relocate the conflicting dependency packages within the uber JAR to prevent their path names from conflicting with those of Hadoop's dependency packages. Instead of modifying your code, use a plugin (see below) to automatically perform this relocation (aka \"shading\") as part of the packaging process.\n### Creating a shaded uber JAR with Maven\n[Maven](https://maven.apache.org/) is a package management tool for building Java applications. The [Maven scala](https://davidb.github.io/scala-maven-plugin/) plugin can be used to build applications written in Scala, the language used by Spark applications. The [Maven shade](http://maven.apache.org/plugins/maven-shade-plugin/) plugin can be used to create a shaded JAR.\nThe following is a sample `pom.xml` configuration file that shades the Guava library, which is located in the `com.google.common` package. This configuration instructs Maven to rename the `com.google.common` package to `repackaged.com.google.common` and to update all references to the classes from the original package.\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?><project xmlns=\"http://maven.apache.org/POM/4.0.0\"\u00a0 xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\u00a0 xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\u00a0 <modelVersion>4.0.0</modelVersion>\u00a0 <properties>\u00a0 \u00a0 <maven.compiler.source>1.8</maven.compiler.source>\u00a0 \u00a0 <maven.compiler.target>1.8</maven.compiler.target>\u00a0 </properties>\u00a0 <groupId><!-- YOUR_GROUP_ID --></groupId>\u00a0 <artifactId><!-- YOUR_ARTIFACT_ID --></artifactId>\u00a0 <version><!-- YOUR_PACKAGE_VERSION --></version>\u00a0 <dependencies>\u00a0 \u00a0 <dependency>\u00a0 \u00a0 \u00a0 <groupId>org.apache.spark</groupId>\u00a0 \u00a0 \u00a0 <artifactId>spark-sql_2.11</artifactId>\u00a0 \u00a0 \u00a0 <version><!-- YOUR_SPARK_VERSION --></version>\u00a0 \u00a0 \u00a0 <scope>provided</scope>\u00a0 \u00a0 </dependency>\u00a0 \u00a0 <!-- YOUR_DEPENDENCIES -->\u00a0 </dependencies>\u00a0 <build>\u00a0 \u00a0 <plugins>\u00a0 \u00a0 \u00a0 <plugin>\u00a0 \u00a0 \u00a0 \u00a0 <groupId>net.alchim31.maven</groupId>\u00a0 \u00a0 \u00a0 \u00a0 <artifactId>scala-maven-plugin</artifactId>\u00a0 \u00a0 \u00a0 \u00a0 <executions>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <execution>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <goals>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <goal>compile</goal>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <goal>testCompile</goal>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </goals>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </execution>\u00a0 \u00a0 \u00a0 \u00a0 </executions>\u00a0 \u00a0 \u00a0 \u00a0 <configuration>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <scalaVersion><!-- YOUR_SCALA_VERSION --></scalaVersion>\u00a0 \u00a0 \u00a0 \u00a0 </configuration>\u00a0 \u00a0 \u00a0 </plugin>\u00a0 \u00a0 \u00a0 <plugin>\u00a0 \u00a0 \u00a0 \u00a0 <groupId>org.apache.maven.plugins</groupId>\u00a0 \u00a0 \u00a0 \u00a0 <artifactId>maven-shade-plugin</artifactId>\u00a0 \u00a0 \u00a0 \u00a0 <executions>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <execution>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <phase>package</phase>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <goals>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <goal>shade</goal>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </goals>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <configuration>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <transformers>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <transformer implementation=\"org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\">\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <mainClass><!-- YOUR_APPLICATION_MAIN_CLASS --></mainClass>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </transformer>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <!-- This is needed if you have dependencies that use Service Loader. Most Google Cloud client libraries do. -->\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <transformer implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"/>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </transformers>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <filters>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <filter>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <artifact>*:*</artifact>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <excludes>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <exclude>META-INF/maven/**</exclude>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <exclude>META-INF/*.SF</exclude>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <exclude>META-INF/*.DSA</exclude>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <exclude>META-INF/*.RSA</exclude>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </excludes>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </filter>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </filters>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <relocations>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <relocation>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <pattern>com</pattern>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <shadedPattern>repackaged.com.google.common</shadedPattern>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <includes>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <include>com.google.common.**</include>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </includes>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </relocation>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </relocations>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </configuration>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 </execution>\u00a0 \u00a0 \u00a0 \u00a0 </executions>\u00a0 \u00a0 \u00a0 </plugin>\u00a0 \u00a0 </plugins>\u00a0 </build></project>\n```\nTo run the build:\n```\nmvn package\n```\n**Notes about pom.xml:**\n- [ManifestResourceTransformer](https://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html#ManifestResourceTransformer) processes attributes in the uber JAR's manifest file (`MANIFEST.MF`). The manifest can also specify the entry point for your application.\n- Spark's [scope](https://maven.apache.org/guides/introduction/introduction-to-dependency-mechanism.html#Dependency_Scope) is`provided`, since Spark is installed on Dataproc.\n- Specify the version of Spark that is installed on your Dataproc cluster (see [Dataproc Version List](/dataproc/docs/concepts/versioning/dataproc-versions) ). If your application requires a Spark version that is different from the version installed on your Dataproc cluster, you can write an [initialization action](/dataproc/docs/concepts/configuring-clusters/init-actions) or construct a [custom image](/dataproc/docs/guides/dataproc-images) that installs the Spark version used by your application.\n- The`<filters>`entry exclude signature files from your dependencies'`META-INF`directories. Without this entry, a`java.lang.SecurityException: Invalid signature file digest for Manifest main attributes`run-time exception can occur because the signature files are invalid in the context of your uber JAR.\n- You may need to shade multiple libraries. To do so, include multiple paths. The next example shades the Guava and Protobuf libraries.```\n<relocation>\u00a0 <pattern>com</pattern>\u00a0 <shadedPattern>repackaged.com</shadedPattern>\u00a0 <includes>\u00a0 \u00a0 <include>com.google.protobuf.**</include>\u00a0 \u00a0 <include>com.google.common.**</include>\u00a0 </includes></relocation>\n```\n### Creating a shaded uber JAR with SBT\n[SBT](https://www.scala-sbt.org/) is a tool for building Scala applications. To create a shaded JAR with SBT, add the [sbt-assembly](https://github.com/sbt/sbt-assembly) plugin to your build definition, first by creating a file called `assembly.sbt` under the `project/` directory:\n```\n\u251c\u2500\u2500 src/\n\u2514\u2500\u2500 build.sbt\n\u2514\u2500\u2500 project/\n \u2514\u2500\u2500 assembly.sbt\n```\n... then by adding the following line in `assembly.sbt` :\n```\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.6\")\n```\nThe following is a sample `build.sbt` configuration file that shades the Guava library, which is located in the `com.google.common package` :\n```\nlazy val commonSettings = Seq(\u00a0organization := \"YOUR_GROUP_ID\",\u00a0name := \"YOUR_ARTIFACT_ID\",\u00a0version := \"YOUR_PACKAGE_VERSION\",\u00a0scalaVersion := \"YOUR_SCALA_VERSION\",)lazy val shaded = (project in file(\".\"))\u00a0.settings(commonSettings)mainClass in (Compile, packageBin) := Some(\"YOUR_APPLICATION_MAIN_CLASS\")libraryDependencies ++= Seq(\u00a0\"org.apache.spark\" % \"spark-sql_2.11\" % \"YOUR_SPARK_VERSION\" % \"provided\",\u00a0// YOUR_DEPENDENCIES)assemblyShadeRules in assembly := Seq(\u00a0 ShadeRule.rename(\"com.google.common.**\" -> \"repackaged.com.google.common.@1\").inAll)\n```\nTo run the build:\n```\nsbt assembly\n```\n**Notes about build.sbt** :\n- The shade rule in the above example may not solve all dependency conflicts because SBT uses strict conflict resolution strategies. Therefore, you may need to provide more granular rules that explicitly merge specific types of conflicting files using`MergeStrategy.first`,`last`,`concat`,`filterDistinctLines`,`rename`, or`discard`strategies. See`sbt-assembly`'s [merge strategy](https://github.com/sbt/sbt-assembly#merge-strategy) for more details.\n- You may need to shade multiple libraries. To do so, include multiple paths. The next example shades the Guava and Protobuf libraries.```\nassemblyShadeRules in assembly := Seq(\u00a0 ShadeRule.rename(\"com.google.common.**\" -> \"repackaged.com.google.common.@1\").inAll,\u00a0 ShadeRule.rename(\"com.google.protobuf.**\" -> \"repackaged.com.google.protobuf.@1\").inAll)\n```## Submitting the uber JAR to Dataproc\nAfter you have created a shaded uber JAR that contains your Spark applications and its dependencies, you are ready to [submit a job](/dataproc/docs/guides/submit-job) to Dataproc.\n## What's next\n- See [spark-translate](https://github.com/GoogleCloudDataproc/cloud-dataproc/tree/master/spark-translate) , a sample Spark application that contains configuration files for both Maven and SBT.\n- [Write and run Spark Scala jobs](/dataproc/docs/tutorials/spark-scala) on Dataproc. quickstart to learn how to write and run Spark Scala jobs on a Dataproc cluster.", "guide": "Dataproc"}