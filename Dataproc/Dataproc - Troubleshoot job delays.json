{"title": "Dataproc - Troubleshoot job delays", "url": "https://cloud.google.com/dataproc/docs/concepts/jobs/troubleshoot-job-delays", "abstract": "# Dataproc - Troubleshoot job delays\nThis page lists common causes of Dataproc job scheduling delays, with information that can help you avoid them.\n", "content": "## Overview\nThe following are common reasons why a Dataproc job is being delayed (throttled):\n- Too many running jobs\n- High system memory usage\n- Not enough free memory\n- Rate limit exceeded\nTypically, the job delay message will be issued in the following format:\n```\nAwaiting execution [SCHEDULER_MESSAGE]\"\n```\nThe following sections provide possible causes and solutions for specific job delay scenarios.\n## Too many running jobs\nScheduler message:\n```\nThrottling job \n### (and maybe others): Too many running jobs (current=xx max=xx)\n```\nCauses:\nThe maximum number of concurrent jobs based on master VM memory is exceeded (the job driver runs on the Dataproc cluster master VM). By default, Dataproc reserves 3.5GB of memory for applications, and allows 1 job per GB.\nExample: The `n1-standard-4` machine type has `15GB` memory. With `3.5GB` reserved for overhead, `11.5GB` remains. Rounding down to an integer, `11GB` is available for up to 11 concurrent jobs.\nSolutions:\n- Monitor log metrics, such as CPU usage and memory, to estimate job requirements.\n- When you create a job cluster:- Use a larger memory machine type for the cluster master VM.\n- If `1GB` per job is more than you need, set the `dataproc:dataproc.scheduler.driver-size-mb` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) to less than `1024` .\n- Set the `dataproc:dataproc.scheduler.max-concurrent-jobs` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) to a value suited to your job requirements.\n## High system memory or not enough free memory\nScheduler message:\n```\nThrottling job xxx_____JOBID_____xxx (and maybe others): High system memory usage (current=xx%)\nThrottling job xxx_____JOBID_____xxx (and maybe others): Not enough free memory (current=xx min=xx)\n```\nCauses:\nBy default, the Dataproc agent throttles job submission when memory use reaches 90% ( `0.9)` . When this limit is reached, new jobs cannot be scheduled.\nThe amount of free memory needed to schedule another job on the cluster is not sufficient.\nSolution:\n- When you create a cluster:- Increase the value of the`dataproc:dataproc.scheduler.max-memory-used` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) . For example, set it above the`0.90`default to`0.95`.Setting the value to`1.0`disables master-memory-utilization job throttling.\n- Increase the value of the`dataproc.scheduler.min-free-memory.mb` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) . The default value is`256`MB.\n## Job rate limit exceeded\nScheduler message:\n```\nThrottling job xxx__JOBID___xxx (and maybe others): Rate limit\n```\nCauses:\nThe Dataproc agent reached the job submission rate limit.\nSolutions:\n- By default, the Dataproc agent job submission is limited at`1.0 QPS`, which you can set to a different value when you create a cluster with the`dataproc:dataproc.scheduler.job-submission-rate` [cluster property](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) .## View job status.\nTo view job status and details, see [Job monitoring and debugging](/dataproc/docs/concepts/jobs/life-of-a-job#job_monitoring_and_debugging) .", "guide": "Dataproc"}