{"title": "Dataproc - Dataproc on GKE \u81ea\u5b9a\u7fa9\u5bb9\u5668\u6620\u50cf", "url": "https://cloud.google.com/dataproc/docs/guides/dpgke/dataproc-gke-custom-images?hl=zh-cn", "abstract": "# Dataproc - Dataproc on GKE \u81ea\u5b9a\u7fa9\u5bb9\u5668\u6620\u50cf\n\u60a8\u53ef\u4ee5\u6307\u5b9a\u8981\u7528\u65bc Dataproc on GKE \u7684\u81ea\u5b9a\u7fa9\u5bb9\u5668\u6620\u50cf\u3002\u60a8\u7684\u81ea\u5b9a\u7fa9\u5bb9\u5668\u6620\u50cf\u5fc5\u9808\u4f7f\u7528\u5176\u4e2d\u4e00\u500b Dataproc on GKE [\u57fa\u790e Spark \u6620\u50cf](#base_spark_images) \u3002\n", "content": "## \u4f7f\u7528\u81ea\u5b9a\u7fa9\u5bb9\u5668\u6620\u50cf\n\u5982\u9700\u4f7f\u7528 Dataproc on GKE \u81ea\u5b9a\u7fa9\u5bb9\u5668\u6620\u50cf\uff0c\u8acb\u5728 [\u5275\u5efa Dataproc on GKE \u865b\u64ec\u96c6\u7fa3](https://cloud.google.com/dataproc/docs/guides/dpgke/quickstarts/dataproc-gke-quickstart-create-cluster?hl=zh-cn) \u6216\u5411\u96c6\u7fa3 [\u63d0\u4ea4 Spark \u4f5c\u696d](https://cloud.google.com/dataproc/docs/guides/dpgke/quickstarts/dataproc-gke-quickstart-create-cluster?hl=zh-cn#submit_a_spark_job) \u6642\u8a2d\u7f6e `spark.kubernetes.container.image property` \u3002\n**\u6ce8\u610f** \uff1a\u5275\u5efa\u96c6\u7fa3\u6642\u9700\u8981 `spark:` \u6587\u4ef6\u524d\u7db4\uff0c\u4f46\u5728\u63d0\u4ea4\u4f5c\u696d\u6642\u53ef\u4ee5\u7701\u7565\u8a72\u6587\u4ef6\uff08\u8acb\u53c3\u95b1 [\u96c6\u7fa3\u5c6c\u6027](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/cluster-properties?hl=zh-cn#formatting) \uff09\u3002\n- gcloud CLI \u96c6\u7fa3\u5275\u5efa\u793a\u4f8b\uff1a```\ngcloud dataproc clusters gke create \"${DP_CLUSTER}\" \\\n\u00a0\u00a0\u00a0\u00a0--properties=spark:spark.kubernetes.container.image=custom-image \\\n\u00a0\u00a0\u00a0\u00a0... other args ...\n```\n- gcloud CLI \u4f5c\u696d\u63d0\u4ea4\u793a\u4f8b\uff1a```\ngcloud dataproc jobs submit spark \\\n\u00a0\u00a0\u00a0\u00a0--properties=spark.kubernetes.container.image=custom-image \\\n\u00a0\u00a0\u00a0\u00a0... other args ...\n```## \u81ea\u5b9a\u7fa9\u5bb9\u5668\u6620\u50cf\u8981\u6c42\u548c\u8a2d\u7f6e\n### \u57fa\u790e\u6620\u50cf\n\u60a8\u53ef\u4ee5\u4f7f\u7528 `docker` \u5de5\u5177\uff0c\u6839\u64da\u5df2\u767c\u4f48\u7684 Dataproc on GKE [\u57fa\u790e Spark \u6620\u50cf](#base_spark_images) \u69cb\u5efa\u81ea\u5b9a\u7fa9 Docker\u3002\n### \u5bb9\u5668\u7528\u6236\nDataproc on GKE \u4ee5 Linux `spark` \u7528\u6236\u8eab\u4efd\u904b\u884c Spark \u5bb9\u5668\uff0c\u5177\u6709 `1099` UID \u548c `1099` GID\u3002\u4f7f\u7528 UID \u548c GID \u7372\u5f97\u6587\u4ef6\u7cfb\u7d71\u6b0a\u9650\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u60a8\u5728\u6620\u50cf\u7684 `/opt/spark/jars/my-lib.jar` \u8655\u6dfb\u52a0\u4e00\u500b jar \u6587\u4ef6\u4f5c\u7232\u5de5\u4f5c\u8ca0\u8f09\u4f9d\u8cf4\u9805\uff0c\u5247\u5fc5\u9808\u5411 `spark` \u7528\u6236\u6388\u4e88\u8a72\u6587\u4ef6\u7684\u8b80\u53d6\u6b0a\u9650\u3002\n### \u7d44\u4ef6\n- **Java** \uff1a `JAVA_HOME` \u74b0\u5883\u8b8a\u91cf\u6307\u5411 Java \u7684\u5b89\u88dd\u4f4d\u7f6e\u3002\u7576\u524d\u9ed8\u8a8d\u503c\u7232 `/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64` \uff0c\u53ef\u80fd\u6703\u767c\u751f\u8b8a\u5316\uff08\u5982\u9700\u77ad\u89e3\u66f4\u65b0\u5f8c\u7684\u4fe1\u606f\uff0c\u8acb\u53c3\u95b1 [Dataproc \u7248\u672c\u8aaa\u660e](https://cloud.google.com/dataproc/docs/release-notes?hl=zh-cn) \uff09\u3002- \u5982\u679c\u60a8\u81ea\u5b9a\u7fa9 Java \u74b0\u5883\uff0c\u8acb\u78ba\u4fdd`JAVA_HOME`\u8a2d\u7f6e\u7232\u6b63\u78ba\u7684\u4f4d\u7f6e\uff0c\u4e26\u4e14`PATH`\u5305\u542b\u4e8c\u9032\u5236\u6587\u4ef6\u7684\u8def\u5f91\u3002\n- **Python** \uff1aGKE \u4e0a\u7684 Dataproc [\u57fa\u790e Spark \u6620\u50cf](https://cloud.google.com/dataproc/docs/guides/dpgke/dataproc-gke-versions?hl=zh-cn) \u5df2\u5c07 Miniconda3 \u5b89\u88dd\u5728 `/opt/conda` \u3002 `CONDA_HOME` \u6307\u5411\u6b64\u4f4d\u7f6e\uff0c `${CONDA_HOME}/bin` \u5305\u542b\u5728 `PATH` \u4e2d\uff0c\u4e26\u4e14 `PYSPARK_PYTHON` \u8a2d\u7f6e\u7232 `${CONDA_HOME}/python` \u3002- \u5982\u679c\u60a8\u81ea\u5b9a\u7fa9 Conda\uff0c\u8acb\u78ba\u4fdd `CONDA_HOME` \u6307\u5411 Conda \u4e3b\u76ee\u9304\uff0c `${CONDA_HOME}/bin` \u5305\u542b\u5728 `PATH` \u4e2d\uff0c\u4e26\u4e14\u5c07 `PYSPARK_PYTHON` \u8a2d\u7f6e\u7232 `${CONDA_HOME}/python.`\n- \u60a8\u53ef\u4ee5\u5728\u9ed8\u8a8d\u57fa\u672c\u74b0\u5883\u4e2d\u5b89\u88dd\u3001\u79fb\u9664\u548c\u66f4\u65b0\u8edf\u4ef6\u5305\uff0c\u4e5f\u53ef\u4ee5\u5275\u5efa\u65b0\u74b0\u5883\uff0c\u4f46\u5f37\u70c8\u5efa\u8b70\u8a72\u74b0\u5883\u5305\u542b\u5728\u57fa\u790e\u5bb9\u5668\u6620\u50cf\u7684\u57fa\u790e\u74b0\u5883\u4e2d\u5b89\u88dd\u7684\u6240\u6709\u8edf\u4ef6\u5305\u3002\n- \u5982\u679c\u5c07 Python \u6a21\u584a\uff08\u4f8b\u5982\u5e36\u6709\u5be6\u7528\u51fd\u6578\u7684 Python \u8173\u672c\uff09\u6dfb\u52a0\u5230\u5bb9\u5668\u6620\u50cf\uff0c\u8acb\u5c07\u6a21\u584a\u76ee\u9304\u5305\u542b\u5728 `PYTHONPATH` \u4e2d\u3002\n- **Spark** \uff1aSpark \u5b89\u88dd\u5728 `/usr/lib/spark` \u4e2d\uff0c\u4e26\u4e14 `SPARK_HOME` \u6307\u5411\u6b64\u4f4d\u7f6e\u3002 **Spark \u7121\u6cd5\u81ea\u5b9a\u7fa9\u3002** \u5982\u679c\u66f4\u6539\uff0c\u5247\u5bb9\u5668\u6620\u50cf\u5c07\u88ab\u62d2\u7d55\u6216\u7121\u6cd5\u6b63\u5e38\u904b\u884c\u3002- **\u4f5c\u696d** \uff1a\u60a8\u53ef\u4ee5\u81ea\u5b9a\u7fa9 Spark \u4f5c\u696d\u4f9d\u8cf4\u9805\u3002 `SPARK_EXTRA_CLASSPATH` \u5b9a\u7fa9\u4e86 Spark JVM \u9032\u7a0b\u7684\u984d\u5916\u985e\u8def\u5f91\u3002\u5efa\u8b70\uff1a\u5c07 jar \u653e\u5728 `/opt/spark/jars` \u4e0b\uff0c\u4e26\u5c07 `SPARK_EXTRA_CLASSPATH` \u8a2d\u7f6e\u7232 `/opt/spark/jars/*` \u3002\u5982\u679c\u60a8\u5728\u6620\u50cf\u4e2d\u5d4c\u5165\u4f5c\u696d jar\uff0c\u5247\u63a8\u85a6\u7684\u76ee\u9304\u7232 `/opt/spark/job` \u3002\u63d0\u4ea4\u4f5c\u696d\u6642\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u672c\u5730\u8def\u5f91\uff08\u4f8b\u5982 `file:///opt/spark/job/my-spark-job.jar` \uff09\u5f15\u7528\u8a72\u4f5c\u696d\u3002\n- **Cloud Storage \u9023\u63a5\u5668** \uff1aCloud Storage \u9023\u63a5\u5668\u5b89\u88dd\u5728 `/usr/lib/spark/jars` \u4e2d\u3002\n- **\u5be6\u7528\u7a0b\u5e8f** \uff1a\u904b\u884c Spark \u9700\u8981 `procps` \u548c `tini` \u5be6\u7528\u7a0b\u5e8f\u8edf\u4ef6\u5305\u3002\u9019\u4e9b\u5be6\u7528\u7a0b\u5e8f\u5305\u542b\u5728 [\u57fa\u790e Spark \u6620\u50cf](#base_spark_images) \u4e2d\uff0c\u56e0\u6b64\u81ea\u5b9a\u7fa9\u6620\u50cf\u7121\u9700\u91cd\u65b0\u5b89\u88dd\u5b83\u5011\u3002\n- **\u5165\u53e3\u9ede** \uff1a **GKE \u4e0a\u7684 Dataproc \u6703\u5ffd\u7565\u5c0d\u5bb9\u5668\u6620\u50cf\u4e2d\u7684 ENTRYPOINT \u548c CMD \u539f\u8a9e\u6240\u505a\u7684\u4efb\u4f55\u66f4\u6539\u3002** \n- **\u521d\u59cb\u5316\u8173\u672c** \uff1a\u60a8\u53ef\u4ee5\u5728 `/opt/init-script.sh` \u4e2d\u6dfb\u52a0\u53ef\u9078\u7684\u521d\u59cb\u5316\u8173\u672c\u3002\u521d\u59cb\u5316\u8173\u672c\u53ef\u4ee5\u5f9e Cloud Storage \u4e0b\u8f09\u6587\u4ef6\u3001\u5728\u5bb9\u5668\u5167\u5553\u52d5\u4ee3\u7406\u3001\u8abf\u7528\u5176\u4ed6\u8173\u672c\uff0c\u4ee5\u53ca\u57f7\u884c\u5176\u4ed6\u5553\u52d5\u4efb\u52d9\u3002\u5728\u5553\u52d5 Spark \u9a45\u52d5\u7a0b\u5e8f\u3001Spark \u57f7\u884c\u5668\u548c\u5176\u4ed6\u9032\u7a0b\u4e4b\u524d\uff0c\u5165\u53e3\u9ede\u8173\u672c\u6703\u4f7f\u7528\u6240\u6709\u547d\u4ee4\u884c\u53c3\u6578 ( `$@` ) \u8abf\u7528\u521d\u59cb\u5316\u8173\u672c\u3002\u521d\u59cb\u5316\u8173\u672c\u53ef\u4ee5\u6839\u64da\u7b2c\u4e00\u500b\u53c3\u6578 ( `$1` ) \u9078\u64c7 Spark \u9032\u7a0b\u7684\u985e\u578b\uff1a\u53ef\u80fd\u7684\u503c\u5305\u62ec\u7528\u65bc\u9a45\u52d5\u7a0b\u5e8f\u5bb9\u5668\u7684 `spark-submit` \u548c\u57f7\u884c\u7a0b\u5e8f\u5bb9\u5668\u7684 `executor` \u3002\n- **\u914d\u7f6e** \uff1aSpark \u914d\u7f6e\u4f4d\u65bc `/etc/spark/conf` \u4e0b\u3002 `SPARK_CONF_DIR` \u74b0\u5883\u8b8a\u91cf\u6307\u5411\u6b64\u4f4d\u7f6e\u3002\u8acb\u52ff\u5728\u5bb9\u5668\u6620\u50cf\u4e2d\u81ea\u5b9a\u7fa9 Spark \u914d\u7f6e\u3002\u61c9\u901a\u904e Dataproc on GKE API \u63d0\u4ea4\u4efb\u4f55\u5c6c\u6027\uff0c\u539f\u56e0\u5982\u4e0b\uff1a- \u67d0\u4e9b\u5c6c\u6027\uff08\u4f8b\u5982\u57f7\u884c\u7a0b\u5e8f\u5167\u5b58\u5927\u5c0f\uff09\u662f\u5728\u904b\u884c\u6642\uff08\u800c\u4e0d\u662f\u5bb9\u5668\u6620\u50cf\u69cb\u5efa\u6642\uff09\u78ba\u5b9a\u7684\uff1b\u5b83\u5011\u5fc5\u9808\u7531 Dataproc on GKE \u6ce8\u5165\u3002\n- Dataproc on GKE \u5c0d\u7528\u6236\u63d0\u4f9b\u7684\u5c6c\u6027\u65bd\u52a0\u4e86\u9650\u5236\u3002Dataproc on GKE \u6703\u5c07\u914d\u7f6e\u5f9e`configMap`\u88dd\u8f09\u5230\u5bb9\u5668\u4e2d\u7684`/etc/spark/conf`\u4e2d\uff0c\u5f9e\u800c\u66ff\u63db\u6620\u50cf\u4e2d\u5d4c\u5165\u7684\u8a2d\u7f6e\u3002\n## \u57fa\u790e Spark \u6620\u50cf\nDataproc \u652f\u6301\u4ee5\u4e0b Spark \u57fa\u790e\u5bb9\u5668\u6620\u50cf\uff1a\n- [Spark 2.4](https://cloud.google.com/dataproc/docs/guides/dpgke/dataproc-gke-versions?hl=zh-cn#spark_engine_24) : ${REGION}-docker.pkg.dev/cloud-dataproc/spark/dataproc_1.5\n- [Spark 3.1](https://cloud.google.com/dataproc/docs/guides/dpgke/dataproc-gke-versions?hl=zh-cn#spark_engine_31) : ${REGION}-docker.pkg.dev/cloud-dataproc/spark/dataproc_2.0## \u81ea\u5b9a\u7fa9\u5bb9\u5668\u6620\u50cf\u69cb\u5efa\u793a\u4f8b\n### \u793a\u4f8b Dockerfile\n```\nFROM us-central1-docker.pkg.dev/cloud-dataproc/spark/dataproc_2.0:latest# Change to root temporarily so that it has permissions to create dirs and copy# files.USER root# Add a BigQuery connector jar.ENV SPARK_EXTRA_JARS_DIR=/opt/spark/jars/ENV SPARK_EXTRA_CLASSPATH='/opt/spark/jars/*'RUN mkdir -p \"${SPARK_EXTRA_JARS_DIR}\" \\\u00a0 \u00a0 && chown spark:spark \"${SPARK_EXTRA_JARS_DIR}\"COPY --chown=spark:spark \\\u00a0 \u00a0 spark-bigquery-with-dependencies_2.12-0.22.2.jar \"${SPARK_EXTRA_JARS_DIR}\"# Install Cloud Storage client Conda package.RUN \"${CONDA_HOME}/bin/conda\" install google-cloud-storage# Add a custom Python file.ENV PYTHONPATH=/opt/python/packagesRUN mkdir -p \"${PYTHONPATH}\"COPY test_util.py \"${PYTHONPATH}\"# Add an init script.COPY --chown=spark:spark init-script.sh /opt/init-script.sh# (Optional) Set user back to `spark`.USER spark\n```\n### \u69cb\u5efa\u5bb9\u5668\u6620\u50cf\n\u5728 Dockerfile \u76ee\u9304\u4e2d\u904b\u884c\u4ee5\u4e0b\u547d\u4ee4\n- \u8a2d\u7f6e\u6620\u50cf\uff08\u793a\u4f8b\uff1a`us-central1-docker.pkg.dev/my-project/spark/spark-test-image:latest`\uff09\u4e26\u66f4\u6539\u7232 build \u76ee\u9304\u3002```\nIMAGE=custom container image \\\n\u00a0\u00a0\u00a0\u00a0BUILD_DIR=$(mktemp -d) \\\n\u00a0\u00a0\u00a0\u00a0cd \"${BUILD_DIR}\"\n```\n- \u4e0b\u8f09 BigQuery \u9023\u63a5\u5668\u3002```\ngsutil cp \\\n\u00a0\u00a0\u00a0\u00a0gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.22.2.jar .\n```\n- \u5275\u5efa\u4e00\u500b Python \u793a\u4f8b\u6587\u4ef6\u3002```\ncat >test_util.py <<'EOF'\ndef hello(name):\n\u00a0\u00a0print(\"hello {}\".format(name))\ndef read_lines(path):\n\u00a0\u00a0with open(path) as f:\n\u00a0\u00a0\u00a0\u00a0return f.readlines()\nEOF\n```\n- \u5275\u5efa\u4e00\u500b\u793a\u4f8b init \u8173\u672c\u3002```\ncat >init-script.sh <<EOF\necho \"hello world\" >/tmp/init-script.out\nEOF\n```\n- \u69cb\u5efa\u4e26\u63a8\u9001\u6620\u50cf\u3002```\ndocker build -t \"${IMAGE}\" . && docker push \"${IMAGE}\"\n```", "guide": "Dataproc"}