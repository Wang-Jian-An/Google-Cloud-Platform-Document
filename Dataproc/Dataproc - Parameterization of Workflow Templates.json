{"title": "Dataproc - Parameterization of Workflow Templates", "url": "https://cloud.google.com/dataproc/docs/concepts/workflows/workflow-parameters", "abstract": "# Dataproc - Parameterization of Workflow Templates\nIf your [workflow template](/dataproc/docs/concepts/workflows/overview) will be run multiple times with different values, you can avoid having to edit the workflow each time by defining parameters in the template (parameterizing the template). Then, you can pass different values for the parameters each time you run the template.\nParameters can be added to your workflow template when you create or update the template.\n", "content": "## Parameterizable Fields\nThe following Dataproc workflow template fields can be parameterized:\n- Labels\n- File URIs\n- [Managed cluster](/dataproc/docs/concepts/workflows/using-workflows#configuring_or_selecting_a_cluster) name. Dataproc will use the user-supplied name as the name prefix, and append random characters to create a unique cluster name. The cluster is deleted at the end of the workflow.\n- Job properties\n- Job arguments\n- Script variables (in HiveJob, SparkSqlJob, and PigJob)\n- Main class (in HadoopJob and SparkJob)\n- Zone (in ClusterSelector)\n- Number of Instances (`numInstances`) in a master or worker instance group.\nParameterizing cluster selector labels allows you to choose the cluster where the workflow will run each time you run the workflow (see [Using cluster selectors with workflows](/dataproc/docs/concepts/workflows/cluster-selectors) ).\n## Parameter attributes\n[Workflow template parameters](/dataproc/docs/reference/rest/v1/projects.locations.workflowTemplates#templateparameter) are defined with the following required and optional attributes:### Field path syntax\nA field path is similar in syntax to a [FieldMask](https://developers.google.com/protocol-buffers/docs/reference/google.protobuf#google.protobuf.FieldMask) . For example, a field path that references the zone field of a workflow template's cluster selector would be specified as `placement.clusterSelector.zone` .\nField paths can reference fields using the following syntax:\n- Managed cluster name:- placement.managedCluster.clusterName\n- Values in maps can be referenced by key, for example: - labels['key']\n- placement.clusterSelector.clusterLabels['key']\n- placement.managedCluster.labels['key']\n- jobs['step-id'].labels['key']\n- Jobs in the jobs list can be referenced by step-id. - jobs['step-id'].hadoopJob.mainJarFileUri\n- jobs['step-id'].hiveJob.queryFileUri\n- jobs['step-id'].pySparkJob.mainPythonFileUri\n- jobs['step-id'].hadoopJob.jarFileUris[0]\n- jobs['step-id'].hadoopJob.archiveUris[0]\n- jobs['step-id'].hadoopJob.fileUris[0]\n- jobs['step-id'].pySparkJob.pythonFileUris[0]\n- Items in repeated fields can be referenced by a zero-based index, for example: \n- jobs['step-id'].sparkJob.args[0]\n- Other examples:\n- jobs['step-id'].hadoopJob.args[0]\n- jobs['step-id'].hadoopJob.mainJarFileUri\n- jobs['step-id'].hadoopJob.properties['key']\n- jobs['step-id'].hiveJob.scriptVariables['key']\n- placement.clusterSelector.zoneMaps and repeated fields cannot be parameterized in their entirety: currently, only individual map values and individual items in repeated fields can be referenced. For example, the following field paths are invalid:\n`placement.clusterSelector.clusterLabels` `jobs['step-id'].sparkJob.args`\n## Parameterizing a workflow template\nYou parameterize a workflow template by defining template parameters with the Dataproc API or the Google Cloud CLI.\nYou can define workflow template parameters by creating, or exporting with the Google Cloud CLI and editing, a workflow template YAML file, then importing the file with the Google Cloud CLI to create or update the template. See [Using YAML files](/dataproc/docs/concepts/workflows/using-yamls) for more information.\n [Example 1: Parameterized managed-cluster template example](None) \n [The following is a sample teragen-terasort ](None)  [managed-cluster workflow](/dataproc/docs/concepts/workflows/using-workflows#configuring_or_selecting_a_cluster) template YAML file with four defined parameters: CLUSTER, NUM_ROWS, GEN_OUT, and SORT_OUT. Two versions are listed: one **BEFORE** and the other **AFTER** parameterization.```\nplacement:\n managedCluster:\n clusterName: my-managed-cluster\n config:\n  gceClusterConfig:\n  zoneUri: us-central1-a\njobs:\n- hadoopJob:\n args:\n - teragen\n - '10000'\n - hdfs:///gen/\n mainJarFileUri: file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\n stepId: teragen\n- hadoopJob:\n args:\n - terasort\n - hdfs:///gen/\n - hdfs:///sort/\n mainJarFileUri: file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\n prerequisiteStepIds:\n - teragen\n stepId: terasort\n``````\nplacement:\n managedCluster:\n clusterName: 'to-be-determined'\n config:\n  gceClusterConfig:\n  zoneUri: us-central1-a\njobs:\n- hadoopJob:\n args:\n - teragen\n - '10000'\n - hdfs:///gen/\n mainJarFileUri: file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\n stepId: teragen\n- hadoopJob:\n args:\n - terasort\n - hdfs:///gen/\n - hdfs:///sort/\n mainJarFileUri: file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\n prerequisiteStepIds:\n - teragen\n stepId: terasort\nparameters:\n- description: The managed cluster name prefix\n fields:\n - placement.managedCluster.clusterName\n name: CLUSTER\n- description: The number of rows to generate\n fields:\n - jobs['teragen'].hadoopJob.args[1]\n name: NUM_ROWS\n validation:\n values:\n  values:\n  - '1000'\n  - '10000'\n  - '100000'\n- description: Output directory for teragen\n fields:\n - jobs['teragen'].hadoopJob.args[2]\n - jobs['terasort'].hadoopJob.args[1]\n name: GEN_OUT\n validation:\n regex:\n  regexes:\n  - hdfs:///.*\n- description: Output directory for terasort\n fields:\n - jobs['terasort'].hadoopJob.args[2]\n name: SORT_OUT\n validation:\n regex:\n  regexes:\n  - hdfs:///.*\n```\n [](None) \n [Example 2: Cluster selector workflow template example](None) \n [](None) \n [The following is a parameterized sample teragen-terasort ](None)  [cluster-selector workflow](/dataproc/docs/concepts/workflows/cluster-selectors#adding_a_cluster_selector_to_a_template) template YAML file with three defined parameters: CLUSTER, NUM_ROWS, and OUTPUT_DIR.\n```\nplacement:\n clusterSelector:\n clusterLabels:\n  goog-dataproc-cluster-name: 'to-be-determined'\njobs:\n - stepId: teragen\n hadoopJob:\n  args:\n  - 'teragen'\n  - 'tbd number of rows'\n  - 'tbd output directory'\nparameters:\n- name: CLUSTER\n fields:\n - placement.clusterSelector.clusterLabels['goog-dataproc-cluster-name']\n- name: NUM_ROWS\n fields:\n - jobs['teragen'].hadoopJob.args[1]\n- name: OUTPUT_DIR\n fields:\n - jobs['teragen'].hadoopJob.args[2]\n```\nAfter creating or editing a YAML file that defines a workflow template with template parameters, use the following gcloud command to import the YAML file to create or update the parameterized template.\n```\ngcloud dataproc workflow-templates import template-ID or template-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--source=template.yaml\n```\nYou can pass either the [WorkflowTemplate](/dataproc/docs/reference/rest/v1/projects.regions.workflowTemplates#resource-workflowtemplate) `id` or the fully qualified template resource `name` (\"projects/ /regions/ /workflowTemplates/ \") to the command. If a template resource with the same template name exists, it will be overwritten (updated) and its version number will be incremented. If a template with the same template name does not exist, it will be created.\nCurrently, you cannot use the Google Cloud CLI to run a workflow directly from a YAML file that defines a parameterized workflow template (see [Instantiate a workflow using a YAML file](/dataproc/docs/concepts/workflows/using-yamls#instantiate_a_workflow_using_a_yaml_file) . You must first import the YAML file, then pass parameters to the workflow template when you instantiate it as explained in this and the following subsections.\nYou can define one or more [WorkflowTemplate.parameters](/dataproc/docs/reference/rest/v1/projects.locations.workflowTemplates#WorkflowTemplate) in a [workflowTemplates.create](/dataproc/docs/reference/rest/v1/projects.locations.workflowTemplates/create) or a [workflowTemplates.update](/dataproc/docs/reference/rest/v1/projects.locations.workflowTemplates/update) API request.\nThe following is a sample `workflowTemplates.create` request to create a teragen-terasort workflow template with four defined parameters: CLUSTER, NUM_ROWS, GEN_OUT, and SORT_OUT.\n```\nPOST https://dataproc.googleapis.com/v1/projects/my-project/locations/us-central1/workflowTemplates\n{\n \"id\": \"my-template\",\n \"jobs\": [ {\n  \"stepId\": \"teragen\",\n  \"hadoopJob\": {\n  \"mainJarFileUri\": \"file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\",\n  \"args\": [   \"teragen\",\n   \"10000\",\n   \"hdfs:///gen/\"\n  ]\n  }\n },\n {\n  \"stepId\": \"terasort\",\n  \"prerequisiteStepIds\": [  \"teragen\"\n  ],\n  \"hadoopJob\": {\n  \"mainJarFileUri\": \"file:///usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar\",\n  \"args\": [   \"terasort\",\n   \"hdfs:///gen/\",\n   \"hdfs:///sort/\"\n  ]\n  }\n }\n ],\n \"parameters\": [ {\n  \"name\": \"CLUSTER\",\n  \"fields\": [  \"placement.managedCluster.clusterName\"\n  ],\n  \"description\": \"The managed cluster name prefix\"\n },\n {\n  \"name\": \"NUM_ROWS\",\n  \"fields\": [  \"jobs['teragen'].hadoopJob.args[1]\"\n  ],\n  \"description\": \"The number of rows to generate\",\n  \"validation\": {\n  \"values\": {\n   \"values\": [   \"1000\",\n   \"10000\",\n   \"100000\"\n   ]\n  }\n  }\n },\n {\n  \"name\": \"GEN_OUT\",\n  \"fields\": [  \"jobs['teragen'].hadoopJob.args[2]\",\n  \"jobs['terasort'].hadoopJob.args[1]\"\n  ],\n  \"description\": \"Output directory for teragen\",\n  \"validation\": {\n  \"regex\": {\n   \"regexes\": [   \"hdfs:///.*\"\n   ]\n  }\n  }\n },\n {\n  \"name\": \"SORT_OUT\",\n  \"fields\": [  \"jobs['terasort'].hadoopJob.args[2]\"\n  ],\n  \"description\": \"Output directory for terasort\",\n  \"validation\": {\n  \"regex\": {\n   \"regexes\": [   \"hdfs:///.*\"\n   ]\n  }\n  }\n }\n ],\n \"placement\": {\n \"managedCluster\": {\n  \"clusterName\": \"to-be-determined\",\n  \"config\": {\n  \"gceClusterConfig\": {\n   \"zoneUri\": \"us-central1-a\"\n  }\n  }\n }\n }\n}\n```\n## Passing Parameters to a parameterized template\nYou can pass a different set of parameter values each time you run a parameterized workflow template. You must provide a value for each parameter defined in the template.\nYou can pass a map of parameter names to values to the [gcloud dataproc workflow-templates instantiate ](/sdk/gcloud/reference/dataproc/workflow-templates/instantiate) command with the `--parameters` flag. All parameter values defined in the template must be supplied. The supplied values will override values specified in the template.\n **Parameterized managed-cluster template example** \n```\ngcloud dataproc workflow-templates instantiate my-template \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--parameters=CLUSTER=cluster,NUM_ROWS=1000,GEN_OUT=hdfs:///gen_20180601/,SORT_OUT=hdfs:///sort_20180601\n```\n **Parameterized cluster-selector template example** \n```\ngcloud dataproc workflow-templates instantiate \\\n --parameters CLUSTER=my-cluster,NUM_ROWS=10000,OUTPUT_DIR=hdfs://some/dir\n \n```You can pass a [parameters map](/dataproc/docs/reference/rest/v1/projects.locations.workflowTemplates/instantiate#request-body) of parameter `names` to `values` to the Dataproc [workflowTemplates.instantiate](/dataproc/docs/reference/rest/v1/projects.locations.workflowTemplates/instantiate) API. All parameter values defined in the template must be supplied. The supplied values will override values specified in the template.\nYou cannot pass parameters to a parameterized workflow template using the [workflowTemplates.instantiateinline](/dataproc/docs/reference/rest/v1/projects.regions.workflowTemplates/instantiateInline) API.\n.\n **Example:** \n```\nPOST https://dataproc.googleapis.com/v1/projects/my-project/regions/us-central1/workflowTemplates/my-template:instantiate\n{\n \"parameters\": {\n \"CLUSTER\": \"clusterA\",\n \"NUM_ROWS\": \"1000\",\n \"GEN_OUT\": \"hdfs:///gen_20180601/\",\n \"SORT_OUT\": \"hdfs:///sort_20180601/\"\n }\n}\n```", "guide": "Dataproc"}