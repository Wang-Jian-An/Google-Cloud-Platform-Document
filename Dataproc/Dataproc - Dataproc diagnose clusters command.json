{"title": "Dataproc - Dataproc diagnose clusters command", "url": "https://cloud.google.com/dataproc/docs/support/diagnose-cluster-command", "abstract": "# Dataproc - Dataproc diagnose clusters command\nYou can run the [gcloud dataproc clusters diagnose](/sdk/gcloud/reference/dataproc/clusters/diagnose) command to collect system, Spark, Hadoop, and Dataproc logs, cluster configuration files, and other information that you can examine or share with [Google support](/dataproc/docs/support/getting-support) to help you troubleshoot a Dataproc cluster or job. The command uploads the [diagnostic data and summary](#diagnostic_summary_and_archive_contents) to the Dataproc [staging bucket](/dataproc/docs/concepts/configuring-clusters/staging-bucket) in Cloud Storage.\n", "content": "## Run the Google Cloud CLI diagnose cluster command\nRun the [gcloud dataproc clusters diagnose](/sdk/gcloud/reference/dataproc/clusters/diagnose) command to create and output the location of the diagnostic archive file.\n```\ngcloud dataproc clusters diagnose CLUSTER_NAME \\\n\u00a0\u00a0\u00a0\u00a0--region=REGION \\\n\u00a0\u00a0\u00a0\u00a0OPTIONAL FLAGS ...\n```\nNotes:\n- : The name of the cluster to diagnose.\n- : The cluster's region, for example,`us-central1`.\n- :- `--job-ids` : You can this flag to collect job driver, Spark event, YARN application, and Spark Lense output logs, in addition to the [default log files](#log_files) , for a specified comma-separated list of job IDs. For MapReduce jobs, only YARN application logs are collected. YARN log aggregation must be enabled for the collection of YARN application logs.\n- `--yarn-application-ids` : You can this flag to collect job driver, Spark event, YARN application, and Spark Lense output logs in addition to the [default log files](#log_files) , for a specified comma-separated list of YARN application IDs. YARN log aggregation must be enabled for the collection of YARN application logs.\n- `--start-time` with `--end-time` : Use both flags to specify a time range, in `%Y-%m-%dT%H:%M:%S.%fZ` format, for the collection of diagnostic data. Specifying a time range also enables the collection of Dataproc autoscaling logs during the time range (by default, Dataproc autoscaling logs are not collected in the diagnostic data).\n- `--tarball-access` = `GOOGLE_DATAPROC_DIAGNOSE` Use this flag to submit or provide access to the diagnostic tar file to the [Google Cloud support](/dataproc/docs/support/getting-support) team. Also provide information to Google Cloud support team as follows:- Cloud Storage path of the diagnostic tar file, **or** \n- Cluster configuration bucket, cluster UUID, and operation ID of the diagnose command## Run the diagnostic script from the cluster master node (if needed)\nThe `gcloud dataproc clusters diagnose` command can fail or time-out if a cluster is in an error state and cannot accept diagnose tasks from the Dataproc server. As an alternative to running the diagnose command, you can [connect to the cluster master node using SSH](/dataproc/docs/concepts/accessing/ssh) , download the diagnostic script, and then run the script locally on the master node.\n```\ngcloud compute ssh HOSTNAME\n```\n```\ngsutil cp gs://dataproc-diagnostic-scripts/diagnostic-script.sh .\n```\n```\nsudo bash diagnostic-script.sh\n```\nThe diagnostic archive tar file is saved in a local directory. The command output lists the location of the tar file with instructions on how to to upload the tar file to a Cloud Storage bucket.\n## How to share diagnostic data\n**Sensitive log information:** If you pass sensitive information in arguments, metadata, or driver output, your logs may contain sensitive information. Be careful when sharing the diagnostic archive.\nTo share the archive:\n- [Download the archive](/storage/docs/gsutil/commands/cp) from Cloud Storage, then share the downloaded archive, or\n- [Change the permissions](/storage/docs/gsutil/commands/acl) on the archive to allow other Google Cloud users or projects to access the file.\nExample: The following command adds read permissions to the archive in a `test-project` :\n```\ngsutil -m acl ch -g test-project:R PATH_TO_ARCHIVE}\n```\n## Diagnostic summary and archive contents\nThe `diagnose` command outputs a diagnostic summary and an archive tar file that contains cluster configuration files, logs, and other files and information. The archive tar file is written to the Dataproc [staging bucket](/dataproc/docs/concepts/configuring-clusters/staging-bucket) in Cloud Storage.\n**Diagnostic summary:** The diagnostic script analyzes collected data, and generates a `summary.txt` at the root of the diagnostic archive. The summary provides an overview of cluster status, including YARN, HDFS, disk, and networking status, and includes warnings to alert you to potential problems.\n**Archive tar file:** The following sections list the files and information contained in the diagnostic archive tar file.\n### Daemons and services information\n| Command executed         | Location in archive   |\n|:---------------------------------------------------|:------------------------------|\n| yarn node -list -all        | /system/yarn-nodes.log  |\n| hdfs dfsadmin -report -live -decommissioning  | /system/hdfs-nodes.log  |\n| hdfs dfs -du -h         | /system/hdfs-du.log   |\n| service --status-all        | /system/service.log   |\n| systemctl --type service       | /system/systemd-services.log |\n| curl \"http://${HOSTNAME}:8088/jmx\"     | /metrics/resource_manager_jmx |\n| curl \"http://${HOSTNAME}:8088/ws/v1/cluster/apps\" | /metrics/yarn_app_info  |\n| curl \"http://${HOSTNAME}:8088/ws/v1/cluster/nodes\" | /metrics/yarn_node_info  |\n| curl \"http://${HOSTNAME}:9870/jmx\"     | /metrics/namenode_jmx   |\n### JVM information\n| Command executed     | Location in archive      |\n|:----------------------------------|:------------------------------------------|\n| jstack -l \"${DATAPROC_AGENT_PID}\" | jstack/agent_${DATAPROC_AGENT_PID}.jstack |\n| jstack -l \"${PRESTO_PID}\"   | jstack/agent_${PRESTO_PID}.jstack   |\n| jstack -l \"${JOB_DRIVER_PID}\"  | jstack/driver_${JOB_DRIVER_PID}.jstack |\n| jinfo \"${DATAPROC_AGENT_PID}\"  | jinfo/agent_${DATAPROC_AGENT_PID}.jstack |\n| jinfo \"${PRESTO_PID}\"    | jinfo/agent_${PRESTO_PID}.jstack   |\n| jinfo \"${JOB_DRIVER_PID}\"   | jinfo/agent_${JOB_DRIVER_PID}.jstack  |\n### Linux system information\n| Command executed   | Location in archive  |\n|:-------------------------|:-------------------------|\n| df -h     | /system/df.log   |\n| ps aux     | /system/ps.log   |\n| free -m     | /system/free.log   |\n| netstat -anp    | /system/netstat.log  |\n| sysctl -a    | /system/sysctl.log  |\n| uptime     | /system/uptime.log  |\n| cat /proc/sys/fs/file-nr | /system/fs-file-nr.log |\n| ping -c 1    | /system/cluster-ping.log |\n### Log files\n| Item(s) included                                      | Location in archive                          |\n|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|\n| All logs in /var/log with the following prefixes in their filename: cloud-sql-proxy dataproc druid gcdp google hadoop hdfs hive knox presto spark syslog yarn zookeeper | Files are placed in the archive logs folder, and keep their original filenames.           |\n| Dataproc node startup logs for each node (master and worker) in your cluster.                       | Files are placed in the archive node_startup folder, which contains separate sub-folders for each machine in the cluster. |\n| Component gateway logs from journalctl -u google-dataproc-component-gateway                        | /logs/google-dataproc-component-gateway.log                    |\n### Configuration files\n| Item(s) included       | Location in archive    |\n|:------------------------------------------|:-----------------------------------|\n| VM metadata        | /conf/dataproc/metadata   |\n| Environment variables in /etc/environment | /conf/dataproc/environment   |\n| Dataproc properties      | /conf/dataproc/dataproc.properties |\n| All files in /etc/google-dataproc/  | /conf/dataproc/     |\n| All files in /etc/hadoop/conf/   | /conf/hadoop/      |\n| All files in /etc/hive/conf/    | /conf/hive/      |\n| All files in /etc/hive-hcatalog/conf/  | /conf/hive-hcatalog/    |\n| All files in /etc/knox/conf/    | /conf/knox/      |\n| All files in /etc/pig/conf/    | /conf/pig/       |\n| All files in /etc/presto/conf/   | /conf/presto/      |\n| All files in /etc/spark/conf/    | /conf/spark/      |\n| All files in /etc/tez/conf/    | /conf/tez/       |\n| All files in /etc/zookeeper/conf/   | /conf/zookeeper/     |", "guide": "Dataproc"}