{"title": "Dataproc - Scale Dataproc clusters", "url": "https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/scaling-clusters", "abstract": "# Dataproc - Scale Dataproc clusters\nAfter creating a Dataproc cluster, you can adjust (\"scale\") the cluster by increasing or decreasing the number of primary or secondary worker nodes (horizontal scaling) in the cluster. You can scale a Dataproc cluster at any time, even when jobs are running on the cluster. You cannot change the machine type of an existing cluster (vertical scaling). To vertically scale, create a cluster using a [supported machine type](/dataproc/docs/concepts/compute/supported-machine-types) , then migrate jobs to the new cluster.\n**Use Dataproc Autoscaling.** Instead of manually scaling clusters, enable [Autoscaling](/dataproc/docs/concepts/configuring-clusters/autoscaling) to have Dataproc set the \"right\" number of workers for your workloads.\nWhy scale a Dataproc cluster?\n- to increase the number of workers to make a job run faster\n- to decrease the number of workers to save money (see [Graceful Decommissioning](#graceful_decommissioning) as an option to use when downsizing a cluster to avoid losing work in progress).\n- to increase the number of nodes to expand available Hadoop Distributed Filesystem (HDFS) storage\n**Note:** You cannot scale a [single-node cluster](/dataproc/docs/concepts/configuring-clusters/single-node-clusters) .\nBecause clusters can be scaled more than once, you might want to increase/decrease the cluster size at one time, and then decrease/increase the size later.\n**Removing workers:** If you remove workers from your cluster, make sure that the new cluster size is sufficient to handle your workload; if it isn't, your jobs may take a long time to complete or may stop responding.\n", "content": "## Using Scaling\nThere are three ways you can scale your Dataproc cluster:\n- Use the [gcloud](/sdk/gcloud/reference/dataproc/clusters/update) command-line tool in the gcloud CLI.\n- Edit the cluster configuration in the [Google Cloud console](#scaling_with_the_developers_console) .\n- Use the [REST API](/dataproc/docs/reference/rest/v1/projects.regions.clusters/patch) .\nNew workers added to a cluster will use the same [machine type](/compute/docs/machine-types) as existing workers. For example, if a cluster is created with workers that use the `n1-standard-8` machine type, new workers will also use the `n1-standard-8` machine type.\n**Note:** Dataproc does not support changing worker size (vCPUs, memory, persistent disk) when you add workers to a cluster.\nYou can scale the number of primary workers or the number of secondary (preemptible) workers, or both. For example, if you only scale the number of preemptible workers, the number of primary workers remains the same.\n**gcloud CLI setup:** You must [setup and configure](/sdk/docs/quickstarts) the gcloud CLI to use the Google Cloud CLI.\nTo scale a cluster with\n [gcloud dataproc clusters update](/sdk/gcloud/reference/dataproc/clusters/update) \n, run the following command.\n```\ngcloud dataproc clusters update cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0[--num-workers and/or --num-secondary-workers]=new-number-of-workers\n```\nwhere\nis the name of the cluster to update, and\nis the updated number of primary and/or secondary worker nodes. For example, to scale a cluster named \"dataproc-1\" to use five primary worker nodes, run the following command.\n```\ngcloud dataproc clusters update dataproc-1 \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--num-workers=5\n...\nWaiting on operation [operations/projects/project-id/operations/...].\nWaiting for cluster update operation...done.\nUpdated [https://dataproc.googleapis.com/...].\nclusterName: my-test-cluster\n...\n masterDiskConfiguration:\n bootDiskSizeGb: 500\n masterName: dataproc-1-m\n numWorkers: 5\n ...\n workers:\n - my-test-cluster-w-0\n - my-test-cluster-w-1\n - my-test-cluster-w-2\n - my-test-cluster-w-3\n - my-test-cluster-w-4\n...\n```See [clusters.patch](/dataproc/docs/reference/rest/v1/projects.regions.clusters/patch) .\n **Example** \n```\nPATCH /v1/projects/project-id/regions/us-central1/clusters/example-cluster?updateMask=config.worker_config.num_instances,config.secondary_worker_config.num_instances\n{\n \"config\": {\n \"workerConfig\": {\n  \"numInstances\": 4\n },\n \"secondaryWorkerConfig\": {\n  \"numInstances\": 2\n }\n },\n \"labels\": null\n}\n```\n **Let the Console construct your cluster update\nrequest.** : You can click the ** Equivalent REST ** link at the bottom of the Google Cloud console Dataproc Cluster details\u2192Configuration page to have the Console construct an equivalent API REST PATCH request.\nAfter a cluster is created, you can scale a cluster by opening the\n **Cluster details** \npage for the cluster from the\n [Google Cloud console](https://console.cloud.google.com/dataproc/clusters) \n **Clusters** \npage, then clicking the\n **Edit** \nbutton on the\n **Configuration** \ntab.\nEnter a new value for the number of\n **Worker nodes** \nand/or\n **Preemptible worker nodes** \n(updated to \"5\" and \" 2\", respectively, in the following screenshot).\nClick\n **Save** \nto update the cluster.\n### How Dataproc selects cluster nodes for removal\nOn clusters created with image versions [1.5.83+](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.5) , [2.0.57+](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.0) , and [2.1.5+](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1) , when scaling down a cluster, Dataproc attempts to minimize the impact of node removal on running YARN applications by first removing inactive, unhealthy, and idle nodes, then removing nodes with the fewest running YARN application masters and running containers.\n## Graceful Decommissioning\nWhen you [downscale a cluster](#using_scaling) , work in progress may stop before completion. If you are using [Dataproc v 1.2 or later](/dataproc/docs/concepts/versioning/dataproc-versions#supported_cloud_dataproc_versions) , you can use Graceful Decommissioning, which incorporates [Graceful Decommission of YARN Nodes](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/GracefulDecommission.html) to finish work in progress on a worker before it is removed from the Cloud Dataproc cluster.\n**Note:** A worker is not removed from a cluster until running YARN applications are finished.\n### Graceful Decommissioning and Secondary Workers\nThe preemptible (secondary) worker group continues to provision or delete workers to reach its expected size even after a cluster scaling operation is marked complete. If you attempt to gracefully decommission a secondary worker and receive an error message similar to the following:  , wait a few minutes then repeat the graceful decommissioning request.\nAlso note:\n- You can **forcefully decommission** preemptible workers   at any time.\n- You gracefully decommission primary workers at any time### Using Graceful Decommissioning\nDataproc Graceful Decommissioning incorporates [Graceful Decommission of YARN Nodes](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/GracefulDecommission.html) to finish work in progress on a worker before it is removed from the Cloud Dataproc cluster. As a default, graceful decommissioning is disabled. You enable it by setting a timeout value when you update your cluster to remove one or more workers from the cluster.\nWhen you update a cluster to remove one or more workers, use the\n [gcloud dataproc clusters update](/sdk/gcloud/reference/dataproc/clusters/update) \ncommand with the\n`--graceful-decommission-timeout`\nflag. The timeout (string) values can be a value of \"0s\" (the default; forceful not graceful decommissioning) or a positive duration relative to the current time (for example, \"3s\"). The maximum duration is 1 day.\n **Durations** : Specify a graceful decommissioning duration as a non-negative integer with one of the following lower-case suffixes: \"s\", \"m\", \"h\", or \"d\" for seconds, minutes, hours, or days, respectively. The maximum duration for graceful decommissioning is \"1d\" (one day). If the suffix is omitted, seconds is assumed.\n```\ngcloud dataproc clusters update cluster-name \\\n\u00a0\u00a0\u00a0\u00a0--region=region \\\n\u00a0\u00a0\u00a0\u00a0--graceful-decommission-timeout=\"timeout-value\" \\\n\u00a0\u00a0\u00a0\u00a0[--num-workers and/or --num-secondary-workers]=decreased-number-of-workers \\\n\u00a0\u00a0\u00a0\u00a0... other args ...\n```\nSee\n [clusters.patch.gracefulDecommissionTimeout](/dataproc/docs/reference/rest/v1/projects.regions.clusters/patch#query-parameters) \n. The timeout (string) values can be a value of \"0\" (the default; forceful not graceful decommissioning) or a duration in seconds (for example, \"3s\"). The maximum duration is 1 day.\nAfter a cluster is created, you can select graceful decommissioning of a cluster by opening the\n **Cluster details** \npage for the cluster from the\n [Google Cloud console](https://console.cloud.google.com/dataproc/clusters) \n **Clusters** \npage, then clicking the\n **Edit** \nbutton on the\n **Configuration** \ntab.\nIn the\n **Graceful Decommissioning** \nsection, check the \"Use graceful decommissioning\" box, then select a timeout value.\nClick\n **Save** \nto update the cluster.\n### How to cancel a graceful decommissioning scaledown operation\nOn Dataproc clusters created with image versions [2.0.57+](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.0) or [2.1.5+](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1) , you can run the [gcloud dataproc operations cancel](/sdk/gcloud/reference/dataproc/operations/cancel) command or issue a Dataproc API [operations.cancel](/dataproc/docs/reference/rest/v1/projects.locations.operations/cancel) request to cancel a graceful decommissioning scaledown operation.\nYou can obtain the resource name, including the ID, of a scaledown operation by running [gcloud dataproc operations list](/sdk/gcloud/reference/dataproc/operations/list) or issuing a Dataproc API [operations.list](/dataproc/docs/reference/rest/v1/projects.locations.operations/list) request.\nWhen you cancel a graceful decommissioning scaledown operation:\n- workers in a `DECOMMISSIONING` state are re-commissioned and become `ACTIVE` at the completion of the operation's cancellation.\n- if the scaledown operation includes label updates, the updates may not take effect.\n**Note:** Cancellation of a scaledown operation is asynchronous. Dataproc makes a best effort to cancel the operation, but success of the cancellation is not guaranteed.\nTo verify the status of the cancellation request, you can run the [gcloud dataproc operations describe](/sdk/gcloud/reference/dataproc/operations/describe) command or issue a Dataproc API [operations.get](/dataproc/docs/reference/rest/v1/projects.locations.operations/get) request. If the cancel operation succeeds, the inner operation status is marked as CANCELLED.", "guide": "Dataproc"}