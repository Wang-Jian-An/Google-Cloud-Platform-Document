{"title": "Dataproc - Life of a Dataproc job", "url": "https://cloud.google.com/dataproc/docs/concepts/jobs/life-of-a-job", "abstract": "# Dataproc - Life of a Dataproc job\nThis page delineates the sequence of steps involved with the submission, execution, and completion of a Dataproc job. It also discusses job throttling and debugging.\n**Key Terms** . Below are key terms and associated actions related to the Dataproc jobs flow.- **Dataproc service** \u2013\u00a0receives job  requests from the user and submits the request to the`dataproc agent`\n- **dataproc agent** \u2013\u00a0runs on the VM,  receives job requests from the Dataproc service, and spawns`driver`\n- **driver** \u2013\u00a0runs customer-supplied code, such as  Hadoop jar, spark-submit, beeline, and pig applications", "content": "## Dataproc jobs flow\n- User submits job to Dataproc.- [JobStatus.State](/dataproc/docs/reference/rest/v1/projects.regions.jobs#State) is marked as`PENDING`.\n- Job waits to be acquired by the`dataproc`agent.- If the job is acquired, [JobStatus.State](/dataproc/docs/reference/rest/v1/projects.regions.jobs#State) is marked as`RUNNING`.\n- If the job is not acquired due to agent failure, Compute Engine network failure, or other cause, the job is marked`ERROR`.\n- Once a job is acquired by the agent, the agent verifies that there are sufficient resources available on the Dataproc cluster's master node to start the driver.- If sufficient resources are not available, the job is delayed (throttled). [JobStatus.Substate](/dataproc/docs/reference/rest/v1/projects.regions.jobs#substate) shows the job as`QUEUED`, and [Job.JobStatus.details](/dataproc/docs/reference/rest/v1/projects.regions.jobs#jobstatus) provides information on the cause of the delay.\n- If sufficient resources are available, the`dataproc`agent starts the job driver process.- At this stage, typically there are one or more applications running in [Apache Hadoop YARN](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) . However, Yarn applications may not start until the driver finishes scanning Cloud Storage directories or performing other start-up job tasks.\n- The`dataproc`agent periodically sends updates to Dataproc on job progress, cluster metrics, and Yarn applications associated with the job (see [Job monitoring and debugging](/dataproc/docs/concepts/jobs/troubleshoot-jobs#job_monitoring_and_debugging) ).\n- Yarn application(s) complete.- Job continues to be reported as`RUNNING`while driver performs any job completion tasks, such as materializing collections.\n- An unhandled or uncaught failure in the Main thread can leave the driver in a zombie state (marked as`RUNNING`without information as to the cause of the failure).\n- Driver exits.`dataproc`agent reports completion to Dataproc.- Dataproc reports job as`DONE`.\n## Job concurrency\nYou can configure the maximum number of concurrent Dataproc jobs with the [dataproc:dataproc.scheduler.max-concurrent-jobs](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) cluster property when you create a cluster. If this property value is not set, the upper limit on concurrent jobs is calculated as `max((masterMemoryMb - 3584) / masterMemoryMbPerJob, 5)` . `masterMemoryMb` is determined by the master VM's machine type. `masterMemoryMbPerJob` is `1024` by default, but is configurable at cluster creation with the [dataproc:dataproc.scheduler.driver-size-mb](/dataproc/docs/concepts/configuring-clusters/cluster-properties#service_properties) cluster property.\nFor information on troubleshooting job delays due to excessive job concurrency and other causes, see [Troubleshoot job delays](/dataproc/docs/concepts/jobs/troubleshoot-job-delays) .\n## Whats next\n- See [Troubleshoot jobs](/dataproc/docs/concepts/jobs/troubleshoot-jobs)", "guide": "Dataproc"}