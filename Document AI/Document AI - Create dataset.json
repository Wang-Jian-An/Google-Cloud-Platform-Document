{"title": "Document AI - Create dataset", "url": "https://cloud.google.com/document-ai/docs/workbench/create-dataset?hl=zh-cn", "abstract": "# Document AI - Create dataset\n# Create dataset\nA labeled dataset of documents is required to train, uptrain, or evaluate a processor version.\nThis page describes how to create a dataset, import documents, and define a schema. To label the imported documents, see [Label documents](/document-ai/docs/workbench/label-documents) .\nThis page assumes you have already [created a processor](https://cloud.google.com/document-ai/docs/create-processor) that supports training, uptraining, or evaluation. If your processor is supported, you see the **Train** tab in the Google Cloud console.\n", "content": "## Create a dataset\nIn your processor's **Train** tab, click **Create Dataset** and select an empty Cloud Storage folder. The folder can either be the root folder in a Cloud Storage bucket or a subfolder.\nMake sure the folder is empty. Document AI manages it for you, so do not manually add documents or modify the contents of the folder. Document AI will store all the dataset content and metadata, including labels and annotations, here.\nOnce created, the location of a dataset cannot be modified. You can go to the **Processor details** tab to see the location of your dataset.\n## Dataset storage options\nYou can choose between two options to save your dataset: Google-managed and a custom location. Follow these steps to provision your storage location.\n### Google-managed storage\n- Toggle advanced options.\n- Keep the default radio group option to **Google-managed** storage.\n- Click **Continue** .\n- Confirm dataset is created successfully and dataset location is **Google-managed location** .\n### Custom storage option\n- Toggle advanced options.\n- Select **I'll specify my own storage location** .\n- Choose a Cloud Storage folder from input component.\n- Click **Continue** .\n### Dataset operations\nThis sample shows you how to use the [processors.updateDataset](/document-ai/docs/reference/rest/v1beta3/projects.locations.processors/updateDataset) method to create a dataset. A dataset resource is a singleton resource in a processor, which means that there is no create resource RPC. Instead, you can use the `updateDataset` RPC to set the preferences. Document AI provides an option to store the dataset documents in a Cloud Storage bucket you provide or to have them automatically managed by Google.\nBefore using any of the request data, make the following replacements:\n- LOCATION: Your processor location\n- PROJECT_ID: Your Google Cloud project ID\n- PROCESSOR_ID: The ID of your custom processor\n- GCS_URI: Your Cloud Storage URI where dataset documents are stored\nFollow next steps to create a dataset request with a Cloud Storage bucket you provide.\nHTTP method\n```\nPATCH https://LOCATION-documentai.googleapis.com/v1beta3/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset\n```\nRequest JSON:\n```\n\u00a0 {\u00a0 \u00a0 \u00a0 \"name\":\"projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset\"\u00a0 \u00a0 \u00a0 \"gcs_managed_config\" {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gcs_prefix\" {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gcs_uri_prefix\": \"GCS_URI\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \"spanner_indexing_config\" {}\u00a0 }\n```\nIn case you want to create the dataset which is Google managed, update the following information:\nHTTP method\n```\nPATCH https://LOCATION-documentai.googleapis.com/v1beta3/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset\n```\nRequest JSON:\n```\n\u00a0 {\u00a0 \u00a0 \u00a0 \"name\":\"projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset\"\u00a0 \u00a0 \u00a0 \"unmanaged_dataset_config\": {}\u00a0 \u00a0 \u00a0 \"spanner_indexing_config\": {}\u00a0 }\n```\nTo send your request, you can use Curl:\n**Note:** The following command assumes that you have logged in to the [gcloud CLI](/sdk/docs/install) with your account by running `gcloud init` or `gcloud auth login` or by using Cloud Shell, which automatically logs you in to the gcloud CLI. You can check the currently active account by running gcloud auth list.\nSave the request body in a file named `request.json` . Execute the following command:\n```\n\u00a0 curl -X PATCH \\\u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 -H \"Content-Type: application/json; charset=utf-8\" \\\u00a0 -d @request.json \\\u00a0 \"https://LOCATION-documentai.googleapis.com/v1beta3/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset\"\n```\nYou should receive a JSON response similar to the following:\n```\n\u00a0 {\u00a0 \u00a0 \u00a0 \"name\": \"projects/PROJECT_ID/locations/LOCATION/operations/OPERATION_ID\"\u00a0 }\n```\n## Import documents\nA newly created dataset is empty. To add documents, click **Import Documents** and select one or more Cloud Storage folders that contain the documents you want to add to your dataset.\n**Note:** Each imported document can contain up to 50 pages.\nIf your Cloud Storage is in a different Google Cloud project, make sure to grant access so that Document AI is allowed to read files from that location. Specifically, you must grant the [Storage Object Viewer](/storage/docs/access-control/iam-roles) role to Document AI's Core Service Agent `service-{project-id}@gcp-sa-prod-dai-core.iam.gserviceaccount.com` . For more information, see [Google-managed service accounts](/iam/docs/service-account-types#google-managed) .\n**Warning:** Make sure the file names do not contain the following unsupported characters `* ? [ ] %` .\nThen, choose one of the following assignment options:\n- Training: assign to training set\n- Test: assign to test set\n- Auto-split: Randomly shuffles documents in training and test set\n- Unassigned: Will not be used in training or evaluation. You can manually assign later\nYou can always modify the assignments later.\nWhen you click **Import** , Document AI imports all of the [supported file types](https://cloud.google.com/document-ai/docs/file-types) as well as JSON [Document](https://cloud.google.com/document-ai/docs/reference/rest/v1/Document) files into the dataset. For JSON [Document](https://cloud.google.com/document-ai/docs/reference/rest/v1/Document) files, Document AI imports the document and converts its `entities` into label instances.\nDocument AI does not modify the import folder or read from the folder after import is complete.\nClick **Activity** at the top of the page to open the **Activity** panel, which lists the files that were successfully imported as well as those that failed to import.\nIf you already have an existing version of your processor, you can select the **Import with auto-labeling** checkbox in the **Import documents** dialog box. The documents will be auto-labeled using the previous processor when they are imported. You cannot train or uptrain on auto-labeled documents, or use them in the test set, without marking them as labeled. After you import auto-labeled documents, manually review and correct the auto-labeled documents. Then, click **Save** to save the corrections and mark the document as labeled. You can then assign the documents as appropriate. See [Auto-labeling](/document-ai/docs/workbench/label-documents#auto-label) .\n### Import documents RPC\nThis sample shows you how to use the [dataset.importDocuments](/document-ai/docs/reference/rest/v1beta3/projects.locations.processors.dataset/importDocuments) method to import documents into the dataset.\nBefore using any of the request data, make the following replacements:\n- LOCATION: Your processor location\n- PROJECT_ID: Your Google Cloud project ID\n- PROCESSOR_ID: The ID of your custom processor\n- GCS_URI: Your Cloud Storage URI where dataset documents are stored\n- DATASET_TYPE: The dataset type to which you want to add documents. The value should be either`DATASET_SPLIT_TRAIN`or`DATASET_SPLIT_TEST`.\n- TRAINING_SPLIT_RATIO: The ratio of documents which you want to autoassign to the training set.\nIf you want to add documents to either training or test dataset:\nHTTP method\n```\nPOST https://LOCATION-documentai.googleapis.com/v1beta3/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset/importDocuments\n```\nRequest JSON:\n```\n\u00a0 {\u00a0 \u00a0 \u00a0 \"batch_documents_import_configs\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"dataset_split\": DATASET_TYPE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"batch_input_config\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gcs_prefix\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gcs_uri_prefix\": GCS_URI\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 }\n```\nIf you want to autosplit the documents between the training and test dataset:\nHTTP method\n```\nPOST https://LOCATION-documentai.googleapis.com/v1beta3/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset/importDocuments\n```\nRequest JSON:\n```\n\u00a0 {\u00a0 \u00a0 \u00a0 \"batch_documents_import_configs\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"auto_split_config\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"training_split_ratio\": TRAINING_SPLIT_RATIO\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"batch_input_config\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gcs_prefix\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gcs_uri_prefix\": \"gs://test_sbindal/pdfs-1-page/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 }\n```\nSave the request body in a file named `request.json` , and execute the following command:\n```\n\u00a0 curl -X POST \\\u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 -H \"Content-Type: application/json; charset=utf-8\" \\\u00a0 -d @request.json \\\u00a0 \"https://LOCATION-documentai.googleapis.com/v1beta3/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset/importDocuments\"\n```\nYou should receive a JSON response similar to the following:\n```\n\u00a0 {\u00a0 \u00a0 \u00a0 \"name\": \"projects/PROJECT_ID/locations/LOCATION/operations/OPERATION_ID\"\u00a0 }\n```\n**Tip:** You can use [ImportDocumentsMetadata](/document-ai/docs/reference/rest/Shared.Types/ImportDocumentsMetadata) to get the status of each document import. We suggest storing all the [DocumentId](/document-ai/docs/reference/rest/Shared.Types/DocumentId) returned as part of import metadata, which can be used to get and delete individual documents from the dataset.\n### Delete documents RPC\nThis sample shows you how to use the [dataset.batchDeleteDocuments](/document-ai/docs/reference/rest/v1beta3/projects.locations.processors.dataset/batchDeleteDocuments) method to delete documents from the dataset.\nBefore using any of the request data, make the following replacements:\n- LOCATION: Your processor location\n- PROJECT_ID: Your Google Cloud project ID\n- PROCESSOR_ID: The id of your custom processor\n- DOCUMENT_ID: The document ID blob returned by `ImportDocuments` request\nHTTP method\n```\nPOST https://LOCATION-documentai.googleapis.com/v1beta3/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset/batchDeleteDocuments\n```\nRequest JSON:\n```\n\u00a0 {\u00a0 \"dataset_documents\": {\u00a0 \u00a0 \u00a0 \"individual_document_ids\": {\u00a0 \u00a0 \u00a0 \"document_ids\": DOCUMENT_ID\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 }\n```\nSave the request body in a file named `request.json` , and execute the following command:\n```\n\u00a0 curl -X POST \\\u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 -H \"Content-Type: application/json; charset=utf-8\" \\\u00a0 -d @request.json \\\u00a0 \"https://LOCATION-documentai.googleapis.com/v1beta3/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset/batchDeleteDocuments\"\n```\nYou should receive a JSON response similar to the following:\n```\n\u00a0 {\u00a0 \u00a0 \u00a0 \"name\": \"projects/PROJECT_ID/locations/LOCATION/operations/OPERATION_ID\"\u00a0 }\n```\n## Assign documents to training or test set\nUnder **Data split** , select documents and assign them to either the Training set, Test set, or Unassigned.### Best practices for test set\nThe quality of your test set determines the quality of your evaluation.\nThe test set should be created at the beginning of the processor development cycle and locked in so that you can track the processor's quality over time.\nWe recommend at least 100 documents per document type for the test set. It is critical to ensure that the **test set is representative** of the types of documents that customers will be using for the model being developed.\nThe test set should be representative of production traffic in terms of frequency. For example, if you are processing W2 forms and expect 70% to be for year 2020 and 30% to be for year 2019, then ~70% of the test set should consist of W2 2020 documents. Such a test set composition ensures appropriate importance is given to each document subtype when evaluating the processor's performance. Also, if you are extracting people's names from international forms, make sure that your test set includes forms from all targeted countries.\n### Best practices for the training set\nAny documents that have already been included in the test set should not be included in the training set.\nUnlike the test set, the training set doesn't need to be as strictly representative of the customer usage in terms of document diversity or frequency. Some labels are more difficult to train than others. Thus, you might get better performance by skewing the training set towards those labels.\nIn the beginning, there isn't a good way to figure out which labels are difficult. You should start with a small, randomly-sampled initial training set using the same approach described for the [test set](#test-set) . This initial training set should contain roughly 10% of the total number of documents that you plan to annotate. Then, you can iteratively evaluate the processor quality (looking for specific error patterns) and add more training data.\n## Define processor schema\nAfter you create a dataset, you can define a processor schema either before or after you import documents.\nThe processor's `schema` defines the labels, such as name and address, to extract from your documents.\nClick **Edit Schema** and then create, edit, enable, and disable labels as necessary.\nMake sure to click **Save** when you are finished.\n**Note:** For the Custom Document Extractor, a maximum of 150 unique labels are supported.\nNotes on schema label management:\n- Once a schema label is created, **the schema label's name cannot be edited** .\n- A schema label can only be edited or deleted when there are no trained processor versions. Only data type and occurrence type can be edited.\n- Disabling a label also does not affect prediction. When you send a processing request, the processor version will extract all labels that were active at the time of training.\n### Get data schema RPC\nThis sample shows you how to use the dataset. [getDatasetSchema](/document-ai/docs/reference/rest/v1beta3/projects.locations.processors.dataset/getDatasetSchema) to get the current schema. `DatasetSchema` is a singleton resource, which is automatically created when you create a dataset resource.\nBefore using any of the request data, make the following replacements:\n- LOCATION: Your processor location.\n- PROJECT_ID: Your Google Cloud project ID.\n- PROCESSOR_ID: The ID of your custom processor.\nHTTP method\n```\nGET https://LOCATION-documentai.googleapis.com/v1beta3/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset/datasetSchema\n```\n```\n\u00a0 curl -X POST \\\u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 -H \"Content-Type: application/json; charset=utf-8\" \\\u00a0 -d @request.json \\\u00a0 \"https://LOCATION-documentai.googleapis.com/v1beta3/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset/datasetSchema\"\n```\nYou should receive a JSON response similar to the following:\n```\n\u00a0 {\u00a0 \u00a0 \u00a0 \"name\": \"projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset/datasetSchema\",\u00a0 \u00a0 \u00a0 \"documentSchema\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"entityTypes\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": $SCHEMA_NAME,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"baseTypes\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"document\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"properties\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": $LABEL_NAME,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"valueType\": $VALUE_TYPE,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"occurrenceType\": $OCCURRENCE_TYPE,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"propertyMetadata\": {}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"entityTypeMetadata\": {}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 }\u00a0 }\n```\n### Update document schema RPC\nThis sample shows you how to use the [dataset.updateDatasetSchema](/document-ai/docs/reference/rest/v1beta3/projects.locations.processors.dataset/updateDatasetSchema) to update the current schema. This example shows you a command to update the dataset schema to have one label. If you desire to add a new label, not delete or update existing labels, then you can call `getDatasetSchema` first and make appropriate changes in its response.\nBefore using any of the request data, make the following replacements:\n- LOCATION: Your processor location\n- PROJECT_ID: Your Google Cloud project ID\n- PROCESSOR_ID: The ID of your custom processor\n- LABEL_NAME: The label name which you want to add\n- DATA_TYPE: The type of the label. You can specify this as`string`,`number`,`currency`,`money`,`datetime`,`address`or`boolean`.\n- OCCURRENCE_TYPE: Describes the number of times this label is expected. Pick an enum value from [here](/document-ai/docs/reference/rest/v1beta3/projects.locations.processors.processorVersions#OccurrenceType) .\nHTTP method\n```\nPATCH https://LOCATION-documentai.googleapis.com/v1beta3/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset/datasetSchema\n```\nRequest JSON:\n```\n\u00a0 {\u00a0 \u00a0 \u00a0 \"document_schema\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"entityTypes\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": $SCHEMA_NAME,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"baseTypes\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"document\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"properties\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": LABEL_NAME,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"valueType\": DATA_TYPE,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"occurrenceType\": OCCURRENCE_TYPE,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"propertyMetadata\": {}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"entityTypeMetadata\": {}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 }\u00a0 }\n```\nSave the request body in a file named `request.json` , and execute the following command:\n```\n\u00a0 curl -X POST \\\u00a0 -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\u00a0 -H \"Content-Type: application/json; charset=utf-8\" \\\u00a0 -d @request.json \\\u00a0 \"https://LOCATION-documentai.googleapis.com/v1beta3/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/dataset/datasetSchema\"\n```\n### Choose label attributes\n**Data type**\n- `Plain text`: a string value.\n- `Number`: a number - integer or floating point.\n- `Money`: a monetary value amount. When labeling, do not include the currency symbol.- When the entity is extracted, it is normalized to [google.type.Money](https://github.com/googleapis/googleapis/blob/master/google/type/money.proto) .\n- `Currency`: a currency symbol.\n- `Datetime`: a date or time value.- When the entity is extracted, it is normalized to the [ISO 8601](https://www.iso.org/iso-8601-date-and-time-format.html) text format.\n- `Address`- a location address.- When the entity is extracted, it is normalized and enriched with [EKG](https://cloud.google.com/document-ai/docs/ekg-enrichment) .\n- `Checkbox`- a`true`or`false`boolean value.\n**Occurrence**\nChoose `REQUIRED` if an entity is expected to always appear in documents of a given type. Choose `OPTIONAL` if there is no such expectation.\nChoose `ONCE` if an entity is expected to have one , even if the same value appears multiple times in the same document. Choose `MULTIPLE` if an entity is expected to have multiple values.\n**Parent and child labels**\nParent/child labels (also known as tabular entities) are used to label data in a table. The following table contains 3 rows and 4 columns.\nYou can define such tables using parent/child labels. In this example, the parent label `line-item` defines a row of the table.\n**Create a parent label**\n- On the **Edit schema** page, click **Create Label** .\n- Select the **This is a parent label** checkbox, and enter the other information. The parent label must have an occurrence of either `optional_multiple` or `require_multiple` so that it can be repeated to capture all the rows in the table.\n- Click **Save** .The parent label appears on the **Edit schema** page, with an **Add Child Label** button next to it.\n**To create a child label**\n- Next to the parent label on the **Edit schema** page, click **Add Child Label** .\n- Enter the information for the child label.\n- Click **Save** .\nRepeat for each child label you want to add.\nThe child labels appear indented under the parent label on the **Edit schema** page.\nParent/child labels are a Preview feature and are only supported only for tables. Nesting depth is limited to 1, meaning that child entities cannot contain other child entities.\n### Create Schema Labels from Labeled Documents\nAutomatically create schema labels by importing pre-labeled [Document](/document-ai/docs/reference/rest/v1/Document) JSON files.\nWhile [Document](/document-ai/docs/reference/rest/v1/Document) import is in progress, newly added schema labels will be added to the Schema Editor. Click 'Edit Schema' to verify or change new schema labels data type and occurrence type. Once confirmed, select desired schema labels and click 'Enable'.\n## Sample Datasets\nTo aid in getting started using Document AI Workbench, datasets are provided in a public Cloud Storage bucket that includes pre-labeled and unlabeled sample [Document](/document-ai/docs/reference/rest/v1/Document) JSON files of multiple document types.\nThese can be used for Uptraining or Custom Document Extractors depending on the document type.\n```\ngs://cloud-samples-data/documentai/Custom/gs://cloud-samples-data/documentai/Custom/1040/gs://cloud-samples-data/documentai/Custom/Invoices/gs://cloud-samples-data/documentai/Custom/Patents/gs://cloud-samples-data/documentai/Custom/Procurement-Splitter/gs://cloud-samples-data/documentai/Custom/W2-redacted/gs://cloud-samples-data/documentai/Custom/W2/gs://cloud-samples-data/documentai/Custom/W9/\n```\n## What's next\n[Label documents](/document-ai/docs/workbench/label-documents)\n[Train a processor](/document-ai/docs/workbench/train-processor)\n[Evaluate processor performance](/document-ai/docs/workbench/evaluate)", "guide": "Document AI"}