{"title": "Document AI - Send a batch process documents request", "url": "https://cloud.google.com/document-ai/docs/samples/documentai-batch-process-document?hl=zh-cn", "abstract": "# Document AI - Send a batch process documents request\nSends a batch (asynchronous) processing request to a processor.", "content": "## Explore furtherFor detailed documentation that includes this code sample, see the following:- [Send a processing request](/document-ai/docs/send-request) \n## Code sampleFor more information, see the [Document AI Java API reference documentation](/java/docs/reference/google-cloud-document-ai/latest/overview) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/document-ai/src/main/java/documentai/v1/BatchProcessDocument.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.api.gax.paging.Page;import com.google.cloud.documentai.v1.BatchDocumentsInputConfig;import com.google.cloud.documentai.v1.BatchProcessMetadata;import com.google.cloud.documentai.v1.BatchProcessRequest;import com.google.cloud.documentai.v1.BatchProcessResponse;import com.google.cloud.documentai.v1.Document;import com.google.cloud.documentai.v1.DocumentOutputConfig;import com.google.cloud.documentai.v1.DocumentOutputConfig.GcsOutputConfig;import com.google.cloud.documentai.v1.DocumentProcessorServiceClient;import com.google.cloud.documentai.v1.DocumentProcessorServiceSettings;import com.google.cloud.documentai.v1.GcsDocument;import com.google.cloud.documentai.v1.GcsDocuments;import com.google.cloud.storage.Blob;import com.google.cloud.storage.BlobId;import com.google.cloud.storage.Bucket;import com.google.cloud.storage.Storage;import com.google.cloud.storage.StorageOptions;import com.google.protobuf.util.JsonFormat;import java.io.File;import java.io.FileReader;import java.io.IOException;import java.util.List;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class BatchProcessDocument {\u00a0 public static void batchProcessDocument()\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, TimeoutException, ExecutionException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String projectId = \"your-project-id\";\u00a0 \u00a0 String location = \"your-project-location\"; // Format is \"us\" or \"eu\".\u00a0 \u00a0 String processerId = \"your-processor-id\";\u00a0 \u00a0 String outputGcsBucketName = \"your-gcs-bucket-name\";\u00a0 \u00a0 String outputGcsPrefix = \"PREFIX\";\u00a0 \u00a0 String inputGcsUri = \"gs://your-gcs-bucket/path/to/input/file.pdf\";\u00a0 \u00a0 batchProcessDocument(\u00a0 \u00a0 \u00a0 \u00a0 projectId, location, processerId, inputGcsUri, outputGcsBucketName, outputGcsPrefix);\u00a0 }\u00a0 public static void batchProcessDocument(\u00a0 \u00a0 \u00a0 String projectId,\u00a0 \u00a0 \u00a0 String location,\u00a0 \u00a0 \u00a0 String processorId,\u00a0 \u00a0 \u00a0 String gcsInputUri,\u00a0 \u00a0 \u00a0 String gcsOutputBucketName,\u00a0 \u00a0 \u00a0 String gcsOutputUriPrefix)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, TimeoutException, ExecutionException {\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs\u00a0 \u00a0 // to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your\u00a0 \u00a0 // requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background\u00a0 \u00a0 // resources.\u00a0 \u00a0 String endpoint = String.format(\"%s-documentai.googleapis.com:443\", location);\u00a0 \u00a0 DocumentProcessorServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 DocumentProcessorServiceSettings.newBuilder().setEndpoint(endpoint).build();\u00a0 \u00a0 try (DocumentProcessorServiceClient client = DocumentProcessorServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 // The full resource name of the processor, e.g.:\u00a0 \u00a0 \u00a0 // projects/project-id/locations/location/processor/processor-id\u00a0 \u00a0 \u00a0 // You must create new processors in the Cloud Console first\u00a0 \u00a0 \u00a0 String name =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\"projects/%s/locations/%s/processors/%s\", projectId, location, processorId);\u00a0 \u00a0 \u00a0 GcsDocument gcsDocument =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 GcsDocument.newBuilder().setGcsUri(gcsInputUri).setMimeType(\"application/pdf\").build();\u00a0 \u00a0 \u00a0 GcsDocuments gcsDocuments = GcsDocuments.newBuilder().addDocuments(gcsDocument).build();\u00a0 \u00a0 \u00a0 BatchDocumentsInputConfig inputConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BatchDocumentsInputConfig.newBuilder().setGcsDocuments(gcsDocuments).build();\u00a0 \u00a0 \u00a0 String fullGcsPath = String.format(\"gs://%s/%s/\", gcsOutputBucketName, gcsOutputUriPrefix);\u00a0 \u00a0 \u00a0 GcsOutputConfig gcsOutputConfig = GcsOutputConfig.newBuilder().setGcsUri(fullGcsPath).build();\u00a0 \u00a0 \u00a0 DocumentOutputConfig documentOutputConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 DocumentOutputConfig.newBuilder().setGcsOutputConfig(gcsOutputConfig).build();\u00a0 \u00a0 \u00a0 // Configure the batch process request.\u00a0 \u00a0 \u00a0 BatchProcessRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BatchProcessRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setName(name)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputDocuments(inputConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDocumentOutputConfig(documentOutputConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 OperationFuture<BatchProcessResponse, BatchProcessMetadata> future =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 client.batchProcessDocumentsAsync(request);\u00a0 \u00a0 \u00a0 // Batch process document using a long-running operation.\u00a0 \u00a0 \u00a0 // You can wait for now, or get results later.\u00a0 \u00a0 \u00a0 // Note: first request to the service takes longer than subsequent\u00a0 \u00a0 \u00a0 // requests.\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to complete...\");\u00a0 \u00a0 \u00a0 future.get();\u00a0 \u00a0 \u00a0 System.out.println(\"Document processing complete.\");\u00a0 \u00a0 \u00a0 Storage storage = StorageOptions.newBuilder().setProjectId(projectId).build().getService();\u00a0 \u00a0 \u00a0 Bucket bucket = storage.get(gcsOutputBucketName);\u00a0 \u00a0 \u00a0 // List all of the files in the Storage bucket.\u00a0 \u00a0 \u00a0 Page<Blob> blobs = bucket.list(Storage.BlobListOption.prefix(gcsOutputUriPrefix + \"/\"));\u00a0 \u00a0 \u00a0 int idx = 0;\u00a0 \u00a0 \u00a0 for (Blob blob : blobs.iterateAll()) {\u00a0 \u00a0 \u00a0 \u00a0 if (!blob.isDirectory()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Fetched file #%d\\n\", ++idx);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Read the results\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Download and store json data in a temp file.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 File tempFile = File.createTempFile(\"file\", \".json\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Blob fileInfo = storage.get(BlobId.of(gcsOutputBucketName, blob.getName()));\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fileInfo.downloadTo(tempFile.toPath());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Parse json file into Document.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 FileReader reader = new FileReader(tempFile);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Document.Builder builder = Document.newBuilder();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 JsonFormat.parser().merge(reader, builder);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Document document = builder.build();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Get all of the document text as one big string.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String text = document.getText();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Read the text recognition output from the processor\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"The document contains the following paragraphs:\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Document.Page page1 = document.getPages(0);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 List<Document.Page.Paragraph> paragraphList = page1.getParagraphsList();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (Document.Page.Paragraph paragraph : paragraphList) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String paragraphText = getText(paragraph.getLayout().getTextAnchor(), text);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Paragraph text:%s\\n\", paragraphText);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Form parsing provides additional output about\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // form-formatted PDFs. You must create a form\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // processor in the Cloud Console to see full field details.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"The following form key/value pairs were detected:\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (Document.Page.FormField field : page1.getFormFieldsList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String fieldName = getText(field.getFieldName().getTextAnchor(), text);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String fieldValue = getText(field.getFieldValue().getTextAnchor(), text);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"Extracted form fields pair:\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t(%s, %s))\", fieldName, fieldValue);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Clean up temp file.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tempFile.deleteOnExit();\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\u00a0 // Extract shards from the text field\u00a0 private static String getText(Document.TextAnchor textAnchor, String text) {\u00a0 \u00a0 if (textAnchor.getTextSegmentsList().size() > 0) {\u00a0 \u00a0 \u00a0 int startIdx = (int) textAnchor.getTextSegments(0).getStartIndex();\u00a0 \u00a0 \u00a0 int endIdx = (int) textAnchor.getTextSegments(0).getEndIndex();\u00a0 \u00a0 \u00a0 return text.substring(startIdx, endIdx);\u00a0 \u00a0 }\u00a0 \u00a0 return \"[NO TEXT]\";\u00a0 }}\n```For more information, see the [Document AI Node.js API reference documentation](/nodejs/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/document-ai/batch-process-document.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*/// const projectId = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION'; // Format is 'us' or 'eu'// const processorId = 'YOUR_PROCESSOR_ID';// const gcsInputUri = 'YOUR_SOURCE_PDF';// const gcsOutputUri = 'YOUR_STORAGE_BUCKET';// const gcsOutputUriPrefix = 'YOUR_STORAGE_PREFIX';// Imports the Google Cloud client libraryconst {DocumentProcessorServiceClient} =\u00a0 require('@google-cloud/documentai').v1;const {Storage} = require('@google-cloud/storage');// Instantiates Document AI, Storage clientsconst client = new DocumentProcessorServiceClient();const storage = new Storage();const {default: PQueue} = require('p-queue');async function batchProcessDocument() {\u00a0 const name = `projects/${projectId}/locations/${location}/processors/${processorId}`;\u00a0 // Configure the batch process request.\u00a0 const request = {\u00a0 \u00a0 name,\u00a0 \u00a0 inputDocuments: {\u00a0 \u00a0 \u00a0 gcsDocuments: {\u00a0 \u00a0 \u00a0 \u00a0 documents: [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 gcsUri: gcsInputUri,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mimeType: 'application/pdf',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 \u00a0 documentOutputConfig: {\u00a0 \u00a0 \u00a0 gcsOutputConfig: {\u00a0 \u00a0 \u00a0 \u00a0 gcsUri: `${gcsOutputUri}/${gcsOutputUriPrefix}/`,\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 };\u00a0 // Batch process document using a long-running operation.\u00a0 // You can wait for now, or get results later.\u00a0 // Note: first request to the service takes longer than subsequent\u00a0 // requests.\u00a0 const [operation] = await client.batchProcessDocuments(request);\u00a0 // Wait for operation to complete.\u00a0 await operation.promise();\u00a0 console.log('Document processing complete.');\u00a0 // Query Storage bucket for the results file(s).\u00a0 const query = {\u00a0 \u00a0 prefix: gcsOutputUriPrefix,\u00a0 };\u00a0 console.log('Fetching results ...');\u00a0 // List all of the files in the Storage bucket\u00a0 const [files] = await storage.bucket(gcsOutputUri).getFiles(query);\u00a0 // Add all asynchronous downloads to queue for execution.\u00a0 const queue = new PQueue({concurrency: 15});\u00a0 const tasks = files.map((fileInfo, index) => async () => {\u00a0 \u00a0 // Get the file as a buffer\u00a0 \u00a0 const [file] = await fileInfo.download();\u00a0 \u00a0 console.log(`Fetched file #${index + 1}:`);\u00a0 \u00a0 // The results stored in the output Storage location\u00a0 \u00a0 // are formatted as a document object.\u00a0 \u00a0 const document = JSON.parse(file.toString());\u00a0 \u00a0 const {text} = document;\u00a0 \u00a0 // Extract shards from the text field\u00a0 \u00a0 const getText = textAnchor => {\u00a0 \u00a0 \u00a0 if (!textAnchor.textSegments || textAnchor.textSegments.length === 0) {\u00a0 \u00a0 \u00a0 \u00a0 return '';\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 // First shard in document doesn't have startIndex property\u00a0 \u00a0 \u00a0 const startIndex = textAnchor.textSegments[0].startIndex || 0;\u00a0 \u00a0 \u00a0 const endIndex = textAnchor.textSegments[0].endIndex;\u00a0 \u00a0 \u00a0 return text.substring(startIndex, endIndex);\u00a0 \u00a0 };\u00a0 \u00a0 // Read the text recognition output from the processor\u00a0 \u00a0 console.log('The document contains the following paragraphs:');\u00a0 \u00a0 const [page1] = document.pages;\u00a0 \u00a0 const {paragraphs} = page1;\u00a0 \u00a0 for (const paragraph of paragraphs) {\u00a0 \u00a0 \u00a0 const paragraphText = getText(paragraph.layout.textAnchor);\u00a0 \u00a0 \u00a0 console.log(`Paragraph text:\\n${paragraphText}`);\u00a0 \u00a0 }\u00a0 \u00a0 // Form parsing provides additional output about\u00a0 \u00a0 // form-formatted PDFs. You \u00a0must create a form\u00a0 \u00a0 // processor in the Cloud Console to see full field details.\u00a0 \u00a0 console.log('\\nThe following form key/value pairs were detected:');\u00a0 \u00a0 const {formFields} = page1;\u00a0 \u00a0 for (const field of formFields) {\u00a0 \u00a0 \u00a0 const fieldName = getText(field.fieldName.textAnchor);\u00a0 \u00a0 \u00a0 const fieldValue = getText(field.fieldValue.textAnchor);\u00a0 \u00a0 \u00a0 console.log('Extracted key value pair:');\u00a0 \u00a0 \u00a0 console.log(`\\t(${fieldName}, ${fieldValue})`);\u00a0 \u00a0 }\u00a0 });\u00a0 await queue.addAll(tasks);}\n```For more information, see the [Document AI Python API reference documentation](/python/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/documentai/snippets/batch_process_documents_sample.py) \n```\nimport refrom typing import Optionalfrom google.api_core.client_options import ClientOptionsfrom google.api_core.exceptions import InternalServerErrorfrom google.api_core.exceptions import RetryErrorfrom google.cloud import documentai \u00a0# type: ignorefrom google.cloud import storage# TODO(developer): Uncomment these variables before running the sample.# project_id = \"YOUR_PROJECT_ID\"# location = \"YOUR_PROCESSOR_LOCATION\" # Format is \"us\" or \"eu\"# processor_id = \"YOUR_PROCESSOR_ID\" # Create processor before running sample# gcs_output_uri = \"YOUR_OUTPUT_URI\" # Must end with a trailing slash `/`. Format: gs://bucket/directory/subdirectory/# processor_version_id = \"YOUR_PROCESSOR_VERSION_ID\" # Optional. Example: pretrained-ocr-v1.0-2020-09-23# TODO(developer): You must specify either `gcs_input_uri` and `mime_type` or `gcs_input_prefix`# gcs_input_uri = \"YOUR_INPUT_URI\" # Format: gs://bucket/directory/file.pdf# input_mime_type = \"application/pdf\"# gcs_input_prefix = \"YOUR_INPUT_URI_PREFIX\" # Format: gs://bucket/directory/# field_mask = \"text,entities,pages.pageNumber\" \u00a0# Optional. The fields to return in the Document object.def batch_process_documents(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 gcs_output_uri: str,\u00a0 \u00a0 processor_version_id: Optional[str] = None,\u00a0 \u00a0 gcs_input_uri: Optional[str] = None,\u00a0 \u00a0 input_mime_type: Optional[str] = None,\u00a0 \u00a0 gcs_input_prefix: Optional[str] = None,\u00a0 \u00a0 field_mask: Optional[str] = None,\u00a0 \u00a0 timeout: int = 400,) -> None:\u00a0 \u00a0 # You must set the `api_endpoint` if you use a location other than \"us\".\u00a0 \u00a0 opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\u00a0 \u00a0 client = documentai.DocumentProcessorServiceClient(client_options=opts)\u00a0 \u00a0 if gcs_input_uri:\u00a0 \u00a0 \u00a0 \u00a0 # Specify specific GCS URIs to process individual documents\u00a0 \u00a0 \u00a0 \u00a0 gcs_document = documentai.GcsDocument(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 gcs_uri=gcs_input_uri, mime_type=input_mime_type\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 # Load GCS Input URI into a List of document files\u00a0 \u00a0 \u00a0 \u00a0 gcs_documents = documentai.GcsDocuments(documents=[gcs_document])\u00a0 \u00a0 \u00a0 \u00a0 input_config = documentai.BatchDocumentsInputConfig(gcs_documents=gcs_documents)\u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 \u00a0 # Specify a GCS URI Prefix to process an entire directory\u00a0 \u00a0 \u00a0 \u00a0 gcs_prefix = documentai.GcsPrefix(gcs_uri_prefix=gcs_input_prefix)\u00a0 \u00a0 \u00a0 \u00a0 input_config = documentai.BatchDocumentsInputConfig(gcs_prefix=gcs_prefix)\u00a0 \u00a0 # Cloud Storage URI for the Output Directory\u00a0 \u00a0 gcs_output_config = documentai.DocumentOutputConfig.GcsOutputConfig(\u00a0 \u00a0 \u00a0 \u00a0 gcs_uri=gcs_output_uri, field_mask=field_mask\u00a0 \u00a0 )\u00a0 \u00a0 # Where to write results\u00a0 \u00a0 output_config = documentai.DocumentOutputConfig(gcs_output_config=gcs_output_config)\u00a0 \u00a0 if processor_version_id:\u00a0 \u00a0 \u00a0 \u00a0 # The full resource name of the processor version, e.g.:\u00a0 \u00a0 \u00a0 \u00a0 # projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}\u00a0 \u00a0 \u00a0 \u00a0 name = client.processor_version_path(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 project_id, location, processor_id, processor_version_id\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 \u00a0 # The full resource name of the processor, e.g.:\u00a0 \u00a0 \u00a0 \u00a0 # projects/{project_id}/locations/{location}/processors/{processor_id}\u00a0 \u00a0 \u00a0 \u00a0 name = client.processor_path(project_id, location, processor_id)\u00a0 \u00a0 request = documentai.BatchProcessRequest(\u00a0 \u00a0 \u00a0 \u00a0 name=name,\u00a0 \u00a0 \u00a0 \u00a0 input_documents=input_config,\u00a0 \u00a0 \u00a0 \u00a0 document_output_config=output_config,\u00a0 \u00a0 )\u00a0 \u00a0 # BatchProcess returns a Long Running Operation (LRO)\u00a0 \u00a0 operation = client.batch_process_documents(request)\u00a0 \u00a0 # Continually polls the operation until it is complete.\u00a0 \u00a0 # This could take some time for larger files\u00a0 \u00a0 # Format: projects/{project_id}/locations/{location}/operations/{operation_id}\u00a0 \u00a0 try:\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Waiting for operation {operation.operation.name} to complete...\")\u00a0 \u00a0 \u00a0 \u00a0 operation.result(timeout=timeout)\u00a0 \u00a0 # Catch exception when operation doesn't finish before timeout\u00a0 \u00a0 except (RetryError, InternalServerError) as e:\u00a0 \u00a0 \u00a0 \u00a0 print(e.message)\u00a0 \u00a0 # NOTE: Can also use callbacks for asynchronous processing\u00a0 \u00a0 #\u00a0 \u00a0 # def my_callback(future):\u00a0 \u00a0 # \u00a0 result = future.result()\u00a0 \u00a0 #\u00a0 \u00a0 # operation.add_done_callback(my_callback)\u00a0 \u00a0 # Once the operation is complete,\u00a0 \u00a0 # get output document information from operation metadata\u00a0 \u00a0 metadata = documentai.BatchProcessMetadata(operation.metadata)\u00a0 \u00a0 if metadata.state != documentai.BatchProcessMetadata.State.SUCCEEDED:\u00a0 \u00a0 \u00a0 \u00a0 raise ValueError(f\"Batch Process Failed: {metadata.state_message}\")\u00a0 \u00a0 storage_client = storage.Client()\u00a0 \u00a0 print(\"Output files:\")\u00a0 \u00a0 # One process per Input Document\u00a0 \u00a0 for process in list(metadata.individual_process_statuses):\u00a0 \u00a0 \u00a0 \u00a0 # output_gcs_destination format: gs://BUCKET/PREFIX/OPERATION_NUMBER/INPUT_FILE_NUMBER/\u00a0 \u00a0 \u00a0 \u00a0 # The Cloud Storage API requires the bucket name and URI prefix separately\u00a0 \u00a0 \u00a0 \u00a0 matches = re.match(r\"gs://(.*?)/(.*)\", process.output_gcs_destination)\u00a0 \u00a0 \u00a0 \u00a0 if not matches:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Could not parse output GCS destination:\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 process.output_gcs_destination,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\u00a0 \u00a0 \u00a0 \u00a0 output_bucket, output_prefix = matches.groups()\u00a0 \u00a0 \u00a0 \u00a0 # Get List of Document Objects from the Output Bucket\u00a0 \u00a0 \u00a0 \u00a0 output_blobs = storage_client.list_blobs(output_bucket, prefix=output_prefix)\u00a0 \u00a0 \u00a0 \u00a0 # Document AI may output multiple JSON files per source file\u00a0 \u00a0 \u00a0 \u00a0 for blob in output_blobs:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Document AI should only output JSON files to GCS\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if blob.content_type != \"application/json\":\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f\"Skipping non-supported file: {blob.name} - Mimetype: {blob.content_type}\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 continue\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Download JSON File as bytes object and convert to Document Object\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(f\"Fetching {blob.name}\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 document = documentai.Document.from_json(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 blob.download_as_bytes(), ignore_unknown_fields=True\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # For a full list of Document object attributes, please reference this page:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # https://cloud.google.com/python/docs/reference/documentai/latest/google.cloud.documentai_v1.types.Document\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Read the text recognition output from the processor\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"The document contains the following text:\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(document.text)\n```\n## What's nextTo search and filter code samples for other Google Cloud products, see the [Google Cloud sample browser](/docs/samples?product=documentai) .", "guide": "Document AI"}