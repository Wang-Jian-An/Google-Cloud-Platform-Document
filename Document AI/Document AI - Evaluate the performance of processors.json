{"title": "Document AI - Evaluate the performance of processors", "url": "https://cloud.google.com/document-ai/docs/workbench/evaluate?hl=zh-cn", "abstract": "# Document AI - Evaluate the performance of processors\n# Evaluate the performance of processors\nDocument AI generates evaluation metrics, such as precision and recall, to help you determine the predictive performance of your processors.\nThese evaluation metrics are generated by comparing the entities returned by the processor (the predictions) against the annotations in the test documents. If your processor does not have a test set, then you must first [create a dataset](/document-ai/docs/workbench/create-dataset) and [label the test documents](/document-ai/docs/workbench/label-documents) .\n", "content": "## Run an evaluation\nAn evaluation is automatically run whenever you train or uptrain a processor version.\nYou can also manually run an evaluation. This is required to generate updated metrics after you've modified the test set, or if you are evaluating a pretrained processor version.\n**Note:** Document AI cannot and does not calculate evaluation metrics for a label if the processor version cannot extract that label (for example, the label was disabled at the time of training) or if the test set does not include annotations for that label. Such labels are not included in aggregated metrics.\n- In the Google Cloud console, go to the **Processors** page and choose your processor. [Go to the Processors page](https://console.cloud.google.com/ai/document-ai/processors) \n- In the **Evaluate & Test** tab, select the **Version** of the processor to evaluate and then click **Run new evaluation** .\nOnce complete, the page contains evaluation metrics for all labels and for each individual label.For more information, see the [Document AI Python API reference documentation](/python/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/documentai/snippets/evaluate_processor_version_sample.py) \n```\nfrom google.api_core.client_options import ClientOptionsfrom google.cloud import documentai \u00a0# type: ignore# TODO(developer): Uncomment these variables before running the sample.# project_id = 'YOUR_PROJECT_ID'# location = 'YOUR_PROCESSOR_LOCATION' # Format is 'us' or 'eu'# processor_id = 'YOUR_PROCESSOR_ID'# processor_version_id = 'YOUR_PROCESSOR_VERSION_ID'# gcs_input_uri = # Format: gs://bucket/directory/def evaluate_processor_version_sample(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version_id: str,\u00a0 \u00a0 gcs_input_uri: str,) -> None:\u00a0 \u00a0 # You must set the api_endpoint if you use a location other than 'us', e.g.:\u00a0 \u00a0 opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\u00a0 \u00a0 client = documentai.DocumentProcessorServiceClient(client_options=opts)\u00a0 \u00a0 # The full resource name of the processor version\u00a0 \u00a0 # e.g. `projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}`\u00a0 \u00a0 name = client.processor_version_path(\u00a0 \u00a0 \u00a0 \u00a0 project_id, location, processor_id, processor_version_id\u00a0 \u00a0 )\u00a0 \u00a0 evaluation_documents = documentai.BatchDocumentsInputConfig(\u00a0 \u00a0 \u00a0 \u00a0 gcs_prefix=documentai.GcsPrefix(gcs_uri_prefix=gcs_input_uri)\u00a0 \u00a0 )\u00a0 \u00a0 # NOTE: Alternatively, specify a list of GCS Documents\u00a0 \u00a0 #\u00a0 \u00a0 # gcs_input_uri = \"gs://bucket/directory/file.pdf\"\u00a0 \u00a0 # input_mime_type = \"application/pdf\"\u00a0 \u00a0 #\u00a0 \u00a0 # gcs_document = documentai.GcsDocument(\u00a0 \u00a0 # \u00a0 \u00a0 gcs_uri=gcs_input_uri, mime_type=input_mime_type\u00a0 \u00a0 # )\u00a0 \u00a0 # gcs_documents = [gcs_document]\u00a0 \u00a0 # evaluation_documents = documentai.BatchDocumentsInputConfig(\u00a0 \u00a0 # \u00a0 \u00a0 gcs_documents=documentai.GcsDocuments(documents=gcs_documents)\u00a0 \u00a0 # )\u00a0 \u00a0 #\u00a0 \u00a0 request = documentai.EvaluateProcessorVersionRequest(\u00a0 \u00a0 \u00a0 \u00a0 processor_version=name,\u00a0 \u00a0 \u00a0 \u00a0 evaluation_documents=evaluation_documents,\u00a0 \u00a0 )\u00a0 \u00a0 # Make EvaluateProcessorVersion request\u00a0 \u00a0 # Continually polls the operation until it is complete.\u00a0 \u00a0 # This could take some time for larger files\u00a0 \u00a0 operation = client.evaluate_processor_version(request=request)\u00a0 \u00a0 # Print operation details\u00a0 \u00a0 # Format: projects/PROJECT_NUMBER/locations/LOCATION/operations/OPERATION_ID\u00a0 \u00a0 print(f\"Waiting for operation {operation.operation.name} to complete...\")\u00a0 \u00a0 # Wait for operation to complete\u00a0 \u00a0 response = documentai.EvaluateProcessorVersionResponse(operation.result())\u00a0 \u00a0 # Once the operation is complete,\u00a0 \u00a0 # Print evaluation ID from operation response\u00a0 \u00a0 print(f\"Evaluation Complete: {response.evaluation}\")\n```\n### Get results of an evaluation\n- In the Google Cloud console, go to the **Processors** page and choose your processor. [Go to the Processors page](https://console.cloud.google.com/ai/document-ai/processors) \n- In the **Evaluate & Test** tab, select the **Version** of the processor to view evaluation.\nOnce complete, the page contains evaluation metrics for all labels and for each individual label.For more information, see the [Document AI Python API reference documentation](/python/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/documentai/snippets/get_evaluation_sample.py) \n```\nfrom google.api_core.client_options import ClientOptionsfrom google.cloud import documentai \u00a0# type: ignore# TODO(developer): Uncomment these variables before running the sample.# project_id = 'YOUR_PROJECT_ID'# location = 'YOUR_PROCESSOR_LOCATION' # Format is 'us' or 'eu'# processor_id = 'YOUR_PROCESSOR_ID' # Create processor before running sample# processor_version_id = 'YOUR_PROCESSOR_VERSION_ID'# evaluation_id = 'YOUR_EVALUATION_ID'def get_evaluation_sample(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version_id: str,\u00a0 \u00a0 evaluation_id: str,) -> None:\u00a0 \u00a0 # You must set the api_endpoint if you use a location other than 'us', e.g.:\u00a0 \u00a0 opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\u00a0 \u00a0 client = documentai.DocumentProcessorServiceClient(client_options=opts)\u00a0 \u00a0 # The full resource name of the evaluation\u00a0 \u00a0 # e.g. `projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}`\u00a0 \u00a0 evaluation_name = client.evaluation_path(\u00a0 \u00a0 \u00a0 \u00a0 project_id, location, processor_id, processor_version_id, evaluation_id\u00a0 \u00a0 )\u00a0 \u00a0 # Make GetEvaluation request\u00a0 \u00a0 evaluation = client.get_evaluation(name=evaluation_name)\u00a0 \u00a0 create_time = evaluation.create_time\u00a0 \u00a0 document_counters = evaluation.document_counters\u00a0 \u00a0 # Print the Evaluation Information\u00a0 \u00a0 # Refer to https://cloud.google.com/document-ai/docs/reference/rest/v1beta3/projects.locations.processors.processorVersions.evaluations\u00a0 \u00a0 # for more information on the available evaluation data\u00a0 \u00a0 print(f\"Create Time: {create_time}\")\u00a0 \u00a0 print(f\"Input Documents: {document_counters.input_documents_count}\")\u00a0 \u00a0 print(f\"\\tInvalid Documents: {document_counters.invalid_documents_count}\")\u00a0 \u00a0 print(f\"\\tFailed Documents: {document_counters.failed_documents_count}\")\u00a0 \u00a0 print(f\"\\tEvaluated Documents: {document_counters.evaluated_documents_count}\")\n```\n### List all evaluations for a processor version\nFor more information, see the [Document AI Python API reference documentation](/python/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/documentai/snippets/list_evaluations_sample.py) \n```\nfrom google.api_core.client_options import ClientOptionsfrom google.cloud import documentai \u00a0# type: ignore# TODO(developer): Uncomment these variables before running the sample.# project_id = 'YOUR_PROJECT_ID'# location = 'YOUR_PROCESSOR_LOCATION' # Format is 'us' or 'eu'# processor_id = 'YOUR_PROCESSOR_ID' # Create processor before running sample# processor_version_id = 'YOUR_PROCESSOR_VERSION_ID'def list_evaluations_sample(\u00a0 \u00a0 project_id: str, location: str, processor_id: str, processor_version_id: str) -> None:\u00a0 \u00a0 # You must set the api_endpoint if you use a location other than 'us', e.g.:\u00a0 \u00a0 opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\u00a0 \u00a0 client = documentai.DocumentProcessorServiceClient(client_options=opts)\u00a0 \u00a0 # The full resource name of the processor version\u00a0 \u00a0 # e.g. `projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}`\u00a0 \u00a0 parent = client.processor_version_path(\u00a0 \u00a0 \u00a0 \u00a0 project_id, location, processor_id, processor_version_id\u00a0 \u00a0 )\u00a0 \u00a0 evaluations = client.list_evaluations(parent=parent)\u00a0 \u00a0 # Print the Evaluation Information\u00a0 \u00a0 # Refer to https://cloud.google.com/document-ai/docs/reference/rest/v1beta3/projects.locations.processors.processorVersions.evaluations\u00a0 \u00a0 # for more information on the available evaluation data\u00a0 \u00a0 print(f\"Evaluations for Processor Version {parent}\")\u00a0 \u00a0 for evaluation in evaluations:\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Name: {evaluation.name}\")\u00a0 \u00a0 \u00a0 \u00a0 print(f\"\\tCreate Time: {evaluation.create_time}\\n\")\n```\n## Evaluation metrics for all labels\nMetrics for **All labels** are computed based on the number of true positives, false positives, and false negatives in the dataset across all labels, and thus, are weighted by the number of times each label appears in the dataset. For definitions of these terms, see [Evaluation metrics for individual labels](#individual-labels) .\n- **Precision:** the proportion of predictions that match the annotations in the test set. Defined as `True Positives / (True Positives + False Positives)`\n- **Recall:** the proportion of annotations in the test set that are correctly predicted. Defined as `True Positives / (True Positives + False Negatives)`\n- **F1 score:** the harmonic mean of precision and recall, which combines precision and recall into a single metric, providing equal weight to both. Defined as `2 * (Precision * Recall) / (Precision + Recall)`\n**Note:** Document AI does not provide a metric for Accuracy. The accuracy metric, often defined as the proportion of instances that are predicted correctly, is less meaningful because 1) not all labels appear in the test set (for example, optional fields) and 2) there may be multiple values for a single label (for example, line items in an invoice). F1, on the other hand, can be considered roughly equivalent to accuracy, but it more meaningfully accommodates these scenarios.\n## Evaluation metrics for individual labels- **True Positives:** the predicted entities that match an annotation in the test document. For more information, see [matching behavior](#matching_behavior) .\n- **False Positives:** the predicted entities that do not match any annotation in the test document.\n- **False Negatives:** the annotations in the test document that do not match any of the predicted entities.- **False Negatives (Below Threshold):** the annotations in the test document that would have matched a predicted entity, but the predicted entity's confidence value is below the specified confidence threshold.**Note:** False positives and false negatives are not mutually exclusive. For example, if a predicted entity has a corresponding, albeit incorrect, annotation in the test set, then there will be one false positive for the predicted entity and one false negative for the annotation. A predicted entity without an annotation is only a false positive. An annotation without an associated prediction is only a false negative.\n**Tip:** If the quality of `checkbox` extraction is not high enough, consider running the documents through the [Form Parser](https://cloud.google.com/document-ai/docs/processors-list#processor_form-parser) . The Form Parser extracts information that could potentially boost the quality of checkbox extraction. Specifically, Form Parser can automatically draw bounding boxes around checkboxes whereas manually drawn bounding boxes might be less consistent. To do this, process your documents using the Form Parser, then [import](/document-ai/docs/workbench/create-dataset#import) the processed documents and annotate them again.\n### Confidence threshold\nThe evaluation logic ignores any predictions with confidence below the specified **Confidence Threshold** , even if the prediction is correct. Document AI provides a list of , which are the annotations that would have a match if the confidence threshold were set lower.\nDocument AI automatically computes the **optimal threshold** , which maximizes the F1 score, and by default, sets the confidence threshold to this optimal value.\n**Note:** If there are multiple optimal threshold values (with the same maximal F1 score), the lowest value is chosen.\nYou are free to choose your own confidence threshold by moving the slider bar. In general, higher confidence threshold results in:\n- higher precision, because the predictions are more likely to be correct.\n- lower recall, because there are fewer predictions.\n### Tabular entities\nThe metrics for a parent label are not calculated by directly averaging the child metrics, but rather, by applying the parent's confidence threshold to all of its child labels and aggregating the results.\nThe optimal threshold for the parent is the [confidence threshold](#Confidence_threshold) value that, when applied to all children, yields the maximum F1 score for the parent.\n## Matching behavior\nA predicted entity matches an annotation if:\n- the type of the predicted entity ( [entity.type](https://cloud.google.com/document-ai/docs/reference/rest/v1/Document#Entity) ) matches the annotation's label name\n- the value of the predicted entity ( [entity.mention_text](https://cloud.google.com/document-ai/docs/reference/rest/v1/Document#Entity) or [entity.normalized_value.text](https://cloud.google.com/document-ai/docs/reference/rest/v1/Document#Entity) ) matches the annotation's text value, subject to [fuzzy matching](#fuzzy_matching) if it is enabled.\nNote that type and text value are all that is used for matching. Other information, such as text anchors and bounding boxes (with the exception of tabular entities described below) are not used.\n**Note:** Some [Identity processors](https://cloud.google.com/document-ai/docs/processors-list#identity_processors) extract a entity, which consists of no text and only a bounding box that indicates where a person's portrait is in an identity document. Because it lacks text, portrait labels and entities are not evaluated.\n### Single-occurrence vs. multi-occurrence labels\nSingle-occurrence labels have one value per document (for example, invoice ID) even if that value is annotated multiple times in the same document (for example, the invoice ID appears in every page of the same document). Even if the multiple annotations have different text, they are considered equal. In other words, if a predicted entity matches any of the annotations, it counts as a match. The extra annotations are considered and do not contribute towards any of the true positive, false positive, or false negative counts.\nMulti-occurrence labels can have multiple, different values. Thus, each predicted entity and annotation is considered and matched separately. If a document contains N annotations for a multi-occurrence label, then there can be N matches with the predicted entities. Each predicted entity and annotation are independently counted as a true positive, false positive, and/or false negative.\n### Fuzzy Matching\nThe **Fuzzy Matching** toggle lets you tighten or relax some of the matching rules to decrease or increase the number of matches.\nFor example, without fuzzy matching, the string `ABC` will not match `abc` due to capitalization. But with fuzzy matching, they will match.\nWhen fuzzy matching is enabled, here are the rule changes:\n- **Whitespace normalization:** removes leading/trailing whitespace and condenses consecutive intermediate whitespaces (including newlines) into single spaces.\n- **Leading/trailing punctuation removal:** removes the following leading/trailing punctuation characters `!,.:;-\"?|` .\n- **Case-insensitive matching:** converts all characters to lowercase.\n- **Money normalization:** For labels with the data type `money` , remove the leading/trailing currency symbols.\n**Note:** You cannot use fuzzy matching on numeric values. For example, `1` and `1.00` do not match, even when fuzzy matching is enabled.\n### Tabular entities\nParent entities and annotations do not have text values and are matched based on the combined bounding boxes of their children. If there is only one predicted parent and one annotated parent, they are automatically matched, regardless of bounding boxes.\nOnce parents are matched, their children are matched as if they were non-tabular entities. If parents are not matched, Document AI will not attempt to match their children. This means that child entities can be considered incorrect, even with the same text contents, if their parent entities are not matched.\nParent / child entities are a Preview feature and only supported for tables with one layer of nesting.\n## Export evaluation metrics\n- In the Google Cloud console, go to the **Processors** page and choose your processor. [Go to the Processors page](https://console.cloud.google.com/ai/document-ai/processors) \n- In the **Evaluate & Test** tab, click **Download Metrics** , to download the evaluation metrics as a JSON file.\n**Note:** The exported metrics are based on the value of the [Confidence threshold](#Confidence_threshold) slider at the time of export.\n## What's next\nOnce you're ready to process documents, see [send a processing request](https://cloud.google.com/document-ai/docs/send-request) .\nOr if the processor's quality is insufficient, [refine your datasets](/document-ai/docs/workbench/label-documents) and [train](/document-ai/docs/workbench/train-processor) another processor version.", "guide": "Document AI"}