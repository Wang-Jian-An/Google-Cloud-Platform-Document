{"title": "Document AI - Use Enterprise Document OCR to process documents", "url": "https://cloud.google.com/document-ai/docs/process-documents-ocr?hl=zh-cn", "abstract": "# Document AI - Use Enterprise Document OCR to process documents\n# Use Enterprise Document OCR to process documentsThis quickstart introduces you to Enterprise Document OCR. It shows you how to optimize document OCR results for your workflow by enabling or disabling any of the available OCR configurations.\n", "content": "## Before you begin\n## Create an Enterprise Document OCR ProcessorFirst, create an Enterprise Document OCR processor. For more information, see [creating and managing processors](/document-ai/docs/create-processor#create-processor) .## OCR configurationsAll OCR configurations can be enabled by setting the respective fields in [ProcessOptions.ocrConfig](/document-ai/docs/reference/rest/v1/ProcessOptions#OcrConfig) in the [ProcessDocumentRequest](/document-ai/docs/reference/rest/v1/projects.locations.processors/process#body.request_body.FIELDS.process_options) or [BatchProcessDocumentsRequest](/document-ai/docs/reference/rest/v1/projects.locations.processors/batchProcess#body.request_body.FIELDS.process_options) .\nFor more information, refer to [Send a processing request](/document-ai/docs/send-request) .\n### Image-quality analysis **Intelligent document-quality analysis** uses machine learning to perform quality assessment of a document based on the readability of its content. This quality assessment is returned as a quality score `[0, 1]` , where `1` means perfect quality. If the quality score detected is lower than `0.5` , a list of negative quality reasons (sorted by the likelihood) is also returned. Likelihood greater than `0.5` is considered a positive detection.\nIf the document is considered to be defective, the API returns the following eight document defect types:- `quality/defect_blurry`\n- `quality/defect_noisy`\n- `quality/defect_dark`\n- `quality/defect_faint`\n- `quality/defect_text_too_small`\n- `quality/defect_document_cutoff`\n- `quality/defect_text_cutoff`\n- `quality/defect_glare`\nThere are some limitations with the current document-quality analysis:- It can return false positive detections with digital documents with no defects. The feature is best used on scanned or photographed documents.\n- Glare defects are local and their presence might not hinder overall document readability.Enable by setting [ProcessOptions.ocrConfig.enableImageQualityScores](/document-ai/docs/reference/rest/v1/ProcessOptions#OcrConfig.FIELDS.enable_image_quality_scores) to `true` in the processing request. This additional feature adds latency comparable to OCR processing to the process call.\n```\n\u00a0 {\u00a0 \u00a0 \"rawDocument\": {\u00a0 \u00a0 \u00a0 \"mimeType\": \"MIME_TYPE\",\u00a0 \u00a0 \u00a0 \"content\": \"IMAGE_CONTENT\"\u00a0 \u00a0 },\u00a0 \u00a0 \"processOptions\": {\u00a0 \u00a0 \u00a0 \"ocrConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \"enableImageQualityScores\": true\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\n```\nThe defect detection results appear in [Document.pages[].imageQualityScores[]](/document-ai/docs/reference/rest/v1/Document#Page.FIELDS.image_quality_scores) .\n```\n\u00a0 {\u00a0 \u00a0 \"pages\": [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \"imageQualityScores\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"qualityScore\": 0.7811847,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"detectedDefects\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"quality/defect_document_cutoff\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"confidence\": 1.0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"quality/defect_glare\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"confidence\": 0.97849524\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"quality/defect_text_cutoff\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"confidence\": 0.5\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 ]\u00a0 }\n```\nRefer to [Sample processor output](/document-ai/docs/output#processor_doc-ocr) for full output examples.\n### Language hintsThe OCR processor supports language hints that you define to improve OCR engine performance. Applying a language hint allows for OCR to optimize for a selected language instead of an inferred language.\nEnable by setting [ProcessOptions.ocrConfig.hints[].languageHints[]](/document-ai/docs/reference/rest/v1/ProcessOptions#Hints.FIELDS.language_hints) with a list of [BCP-47](https://www.rfc-editor.org/info/bcp47) language codes.\n```\n\u00a0 {\u00a0 \u00a0 \"rawDocument\": {\u00a0 \u00a0 \u00a0 \"mimeType\": \"MIME_TYPE\",\u00a0 \u00a0 \u00a0 \"content\": \"IMAGE_CONTENT\"\u00a0 \u00a0 },\u00a0 \u00a0 \"processOptions\": {\u00a0 \u00a0 \u00a0 \"ocrConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \"hints\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"languageHints\": [\"en\", \"es\"]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\n```\nRefer to [Sample processor output](/document-ai/docs/output#processor_doc-ocr) for full output examples.\n### Symbol detectionPopulate data at the symbol (or individual letter) level in the document response.\nEnable by setting [ProcessOptions.ocrConfig.enableSymbol](/document-ai/docs/reference/rest/v1/ProcessOptions#OcrConfig.FIELDS.enable_symbol) to `true` in the processing request.\n```\n\u00a0 {\u00a0 \u00a0 \"rawDocument\": {\u00a0 \u00a0 \u00a0 \"mimeType\": \"MIME_TYPE\",\u00a0 \u00a0 \u00a0 \"content\": \"IMAGE_CONTENT\"\u00a0 \u00a0 },\u00a0 \u00a0 \"processOptions\": {\u00a0 \u00a0 \u00a0 \"ocrConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \"enableSymbol\": true\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\n```\nIf this feature is enabled, the field [Document.pages[].symbols[]](/document-ai/docs/reference/rest/v1/Document#Page.FIELDS.symbols) is populated.\nRefer to [Sample processor output](/document-ai/docs/output#processor_doc-ocr) for full output examples.\n### Native PDF parsingExtract embedded text from digital PDF files. When enabled, if there is manual text, the native digital PDF model is automatically used. If there is non digital text, the optical OCR model is automatically used. The user receives the digital and non digital text results merged together.\nEnable by setting [ProcessOptions.ocrConfig.enableNativePdfParsing](/document-ai/docs/reference/rest/v1/ProcessOptions#OcrConfig.FIELDS.enable_native_pdf_parsing) to `true` in the processing request.\n```\n\u00a0 {\u00a0 \u00a0 \"rawDocument\": {\u00a0 \u00a0 \u00a0 \"mimeType\": \"MIME_TYPE\",\u00a0 \u00a0 \u00a0 \"content\": \"IMAGE_CONTENT\"\u00a0 \u00a0 },\u00a0 \u00a0 \"processOptions\": {\u00a0 \u00a0 \u00a0 \"ocrConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \"enableNativePdfParsing\": true\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\n```\n### Character-in-the-box detectionBy default, Enterprise Document OCR has a detector enabled to improve text-extraction quality of characters that sit within a box. Here is an example:If you're experiencing OCR quality issues with characters inside boxes, you can disable it.\n **Note:** This feature is only available for processor versions 2.0 and above.\nDisable by setting [ProcessOptions.ocrConfig.disableCharacterBoxesDetection](/document-ai/docs/reference/rest/v1/ProcessOptions#OcrConfig.FIELDS.disable_character_boxes_detection) to `true` in the processing request.\n```\n\u00a0 {\u00a0 \u00a0 \"rawDocument\": {\u00a0 \u00a0 \u00a0 \"mimeType\": \"MIME_TYPE\",\u00a0 \u00a0 \u00a0 \"content\": \"IMAGE_CONTENT\"\u00a0 \u00a0 },\u00a0 \u00a0 \"processOptions\": {\u00a0 \u00a0 \u00a0 \"ocrConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \"disableCharacterBoxesDetection\": true\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\n```\n### Legacy layoutIf you require a heuristics layout-detection algorithm, you can enable legacy layout, which serves as an alternative to the current ML-based, layout-detection algorithm. This is not the recommended configuration. Customers can choose the best suitable layout algorithm based on their document workflow.\n **Note:** This feature might change the text ordering in [Document.text](/document-ai/docs/reference/rest/v1/Document#FIELDS.text) .\nEnable by setting [ProcessOptions.ocrConfig.advancedOcrOptions](/document-ai/docs/reference/rest/v1/ProcessOptions#OcrConfig.FIELDS.advanced_ocr_options) to `[\"legacy_layout\"]` in the processing request.\n```\n\u00a0 {\u00a0 \u00a0 \"rawDocument\": {\u00a0 \u00a0 \u00a0 \"mimeType\": \"MIME_TYPE\",\u00a0 \u00a0 \u00a0 \"content\": \"IMAGE_CONTENT\"\u00a0 \u00a0 },\u00a0 \u00a0 \"processOptions\": {\u00a0 \u00a0 \u00a0 \"ocrConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"advancedOcrOptions\": [\"legacy_layout\"]\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\n```## OCR add ons\n### Math OCRMath OCR detects, recognizes, and extracts formulas, such as mathematical equations represented as [LaTeX](https://www.latex-project.org/) along with bounding box coordinates.\nHere is an example of LaTeX representation:- Image detected\n- Conversion to LaTeXEnable by setting [ProcessOptions.ocrConfig.premiumFeatures.enableMathOcr](/document-ai/docs/reference/rest/v1/ProcessOptions#PremiumFeatures.FIELDS.enable_math_ocr) to `true` in the processing request.\n```\n\u00a0 {\u00a0 \u00a0 \"rawDocument\": {\u00a0 \u00a0 \u00a0 \"mimeType\": \"MIME_TYPE\",\u00a0 \u00a0 \u00a0 \"content\": \"IMAGE_CONTENT\"\u00a0 \u00a0 },\u00a0 \u00a0 \"processOptions\": {\u00a0 \u00a0 \u00a0 \"ocrConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"premiumFeatures\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"enableMathOcr\": true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\n```\nThe Math OCR output appears in [Document.pages[].visualElements[]](/document-ai/docs/reference/rest/v1/Document#Page.FIELDS.visual_elements) with `\"type\": \"math_formula\"` .```\n\"visualElements\": [\u00a0 {\u00a0 \u00a0 \"layout\": {\u00a0 \u00a0 \u00a0 \"textAnchor\": {\u00a0 \u00a0 \u00a0 \u00a0 \"textSegments\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"endIndex\": \"46\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \"confidence\": 1,\u00a0 \u00a0 \u00a0 \"boundingPoly\": {\u00a0 \u00a0 \u00a0 \u00a0 \"normalizedVertices\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": 0.14662756,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": 0.27891156\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": 0.9032258,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": 0.27891156\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": 0.9032258,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": 0.8027211\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": 0.14662756,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": 0.8027211\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \"orientation\": \"PAGE_UP\"\u00a0 \u00a0 },\u00a0 \u00a0 \"type\": \"math_formula\"\u00a0 }]\n```\nYou can check the full [Document](/document-ai/docs/reference/rest/v1/Document) JSON output in this [link](https://storage.googleapis.com/cloud-samples-data/documentai/SampleDocuments/OCR_PROCESSOR/pretrained-ocr-v2.0-2023-06-02_math_output.json) .\n### Selection mark extractionIf enabled, the model attempts to extract all [checkboxes](https://en.wikipedia.org/wiki/Checkbox) and [radio buttons](https://en.wikipedia.org/wiki/Radio_button) in the document along with bounding box coordinates.\nEnable by setting [ProcessOptions.ocrConfig.premiumFeatures.enableSelectionMarkDetection](/document-ai/docs/reference/rest/v1/ProcessOptions#PremiumFeatures.FIELDS.enable_selection_mark_detection) to `true` in the processing request.\n```\n\u00a0 {\u00a0 \u00a0 \"rawDocument\": {\u00a0 \u00a0 \u00a0 \"mimeType\": \"MIME_TYPE\",\u00a0 \u00a0 \u00a0 \"content\": \"IMAGE_CONTENT\"\u00a0 \u00a0 },\u00a0 \u00a0 \"processOptions\": {\u00a0 \u00a0 \u00a0 \"ocrConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"premiumFeatures\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"enableSelectionMarkDetection\": true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\n```\nThe checkbox output appears in [Document.pages[].visualElements[]](/document-ai/docs/reference/rest/v1/Document#Page.FIELDS.visual_elements) with `\"type\": \"unfilled_checkbox\"` or `\"type\": \"filled_checkbox\"` .```\n\"visualElements\": [\u00a0 {\u00a0 \u00a0 \"layout\": {\u00a0 \u00a0 \u00a0 \"confidence\": 0.89363575,\u00a0 \u00a0 \u00a0 \"boundingPoly\": {\u00a0 \u00a0 \u00a0 \u00a0 \"vertices\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": 11,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": 24\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": 37,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": 24\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": 37,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": 56\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": 11,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": 56\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \"normalizedVertices\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": 0.017488075,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": 0.38709676\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": 0.05882353,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": 0.38709676\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": 0.05882353,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": 0.9032258\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"x\": 0.017488075,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"y\": 0.9032258\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 },\u00a0 \u00a0 \"type\": \"unfilled_checkbox\"\u00a0 },\u00a0 {\u00a0 \u00a0 \"layout\": {\u00a0 \u00a0 \u00a0 \"confidence\": 0.9148201,\u00a0 \u00a0 \u00a0 \"boundingPoly\": ...\u00a0 \u00a0 },\u00a0 \u00a0 \"type\": \"filled_checkbox\"\u00a0 }],\n```\nYou can check the full [Document](/document-ai/docs/reference/rest/v1/Document) JSON output in this [link](https://storage.googleapis.com/cloud-samples-data/documentai/SampleDocuments/OCR_PROCESSOR/pretrained-ocr-v2.0-2023-06-02_checkbox_output.json) .\n **Note:** Math OCR and selection mark detection are mutually exclusive add-ons. They can't be enabled at the same time.\n### Font-style detectionWith font-style detection enabled, Enterprise Document OCR extracts font attributes, which can be used for better post-processing.\nAt the token (word) level, the following attributes are detected:- Handwriting detection\n- Font style\n- Font size\n- Font type\n- Font color\n- Font weight\n- Letter spacing\n- Bold\n- Italic\n- Underlined\n- Text color ( [RGBa](https://en.wikipedia.org/wiki/RGBA_color_model) )\n- Background color ( [RGBa](https://en.wikipedia.org/wiki/RGBA_color_model) )Enable by setting [ProcessOptions.ocrConfig.premiumFeatures.computeStyleInfo](/document-ai/docs/reference/rest/v1/ProcessOptions#PremiumFeatures.FIELDS.compute_style_info) to `true` in the processing request.\n```\n\u00a0 {\u00a0 \u00a0 \"rawDocument\": {\u00a0 \u00a0 \u00a0 \"mimeType\": \"MIME_TYPE\",\u00a0 \u00a0 \u00a0 \"content\": \"IMAGE_CONTENT\"\u00a0 \u00a0 },\u00a0 \u00a0 \"processOptions\": {\u00a0 \u00a0 \u00a0 \"ocrConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"premiumFeatures\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"computeStyleInfo\": true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\n```\nThe font-style output appears in [Document.pages[].tokens[].styleInfo](/document-ai/docs/reference/rest/v1/Document#Token.FIELDS.style_info) with type [StyleInfo](/document-ai/docs/reference/rest/v1/Document#styleinfo) .```\n\"tokens\": [\u00a0 {\u00a0 \u00a0 \"styleInfo\": {\u00a0 \u00a0 \u00a0 \"fontSize\": 3,\u00a0 \u00a0 \u00a0 \"pixelFontSize\": 13,\u00a0 \u00a0 \u00a0 \"fontType\": \"SANS_SERIF\",\u00a0 \u00a0 \u00a0 \"bold\": true,\u00a0 \u00a0 \u00a0 \"fontWeight\": 564,\u00a0 \u00a0 \u00a0 \"textColor\": {\u00a0 \u00a0 \u00a0 \u00a0 \"red\": 0.16862746,\u00a0 \u00a0 \u00a0 \u00a0 \"green\": 0.16862746,\u00a0 \u00a0 \u00a0 \u00a0 \"blue\": 0.16862746\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \"backgroundColor\": {\u00a0 \u00a0 \u00a0 \u00a0 \"red\": 0.98039216,\u00a0 \u00a0 \u00a0 \u00a0 \"green\": 0.9882353,\u00a0 \u00a0 \u00a0 \u00a0 \"blue\": 0.99215686\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 },\u00a0 ...]\n```\nYou can check the full [Document](/document-ai/docs/reference/rest/v1/Document) JSON output in this [link](https://storage.googleapis.com/cloud-samples-data/documentai/SampleDocuments/OCR_PROCESSOR/pretrained-ocr-v2.0-2023-06-02_font_output.json) .## Specify a page rangeBy default, OCR extracts text and layout information from all pages in the documents. You can select specific page numbers or page ranges and only extract text from those pages.\n **Note:** This feature is only supported for [online processing](/document-ai/docs/send-request#online-process) (synchronous) requests.\nThere are three ways to configure this in [ProcessOptions](/document-ai/docs/reference/rest/v1/ProcessOptions) :- To only process the second and fifth page:\n```\n\u00a0 {\u00a0 \u00a0 \"individualPageSelector\": {\"pages\": [2, 5]}\u00a0 }\n```\n **Note:** Pages are 1-indexed.- To only process the first three pages:\n```\n\u00a0 {\u00a0 \u00a0 \"fromStart\": 3\u00a0 }\n```- To only process the last four pages:\n```\n\u00a0 {\u00a0 \u00a0 \"fromEnd\": 4\u00a0 }\n```\nIn the response, each [Document.pages[].pageNumber](/document-ai/docs/reference/rest/v1/Document#Page.FIELDS.page_number) corresponds the same pages specified in the request.## Convert document objects to Vision AI API formatThe [Document AI Toolbox](/document-ai/docs/toolbox) includes a tool that converts the Document AI API [Document](/document-ai/docs/reference/rest/v1/Document) format to the Vision AI [AnnotateFileResponse](/vision/docs/reference/rest/v1/BatchAnnotateFilesResponse#AnnotateFileResponse) format enables users to compare the responses between the document OCR processor and Vision AI API. Here is some [sample code](/document-ai/docs/toolbox#vision-conversion) .\nKnown discrepancies between the Vision AI API response and Document AI API response and converter:- The Vision AI API response populates only vertices for image request, and populates only normalized_vertices for PDF request. The Document AI response and the converter populates both vertices and`normalized_vertices`.\n- The Vision AI API response populates the`detected_break`in the last symbol of the word. The Document AI API response and the converter populates`detected_break`in the word and the last symbol of the word.\n- The Vision AI API response always populates symbols fields. By default, the Document AI response does not populate symbols fields. To make sure the Document AI response and the converter also gets symbols fields populated, set the`enable_symbol`feature as detailed.\n## Code samplesThe following code samples demonstrate how to send a processing request enabling OCR configurations and add ons, then read and print the fields to the terminal:Before using any of the request data, make the following replacements:- : your processor's [location](/document-ai/docs/regions) , for example:- `us`- United States\n- `eu`- European Union\n- : Your Google Cloud project ID.\n- : the ID of your custom processor.\n- : the processor version identifier. Refer to [Select a processor version](/document-ai/docs/manage-processor#select_a_processor_version) for more information. For example:- `pretrained-TYPE-vX.X-YYYY-MM-DD`\n- `stable`\n- `rc`\n- : A boolean to disable human review (Supported by [Human-in-the-Loop processors](/document-ai/docs/hitl#processors_supported) only.)- `true`- skips human review\n- `false`- enables human review (default)\n- : One of the valid [MIME type](/document-ai/docs/file-types) options.\n- : One of the valid Inline document content, represented as a stream of bytes. For JSON representations, the base64 encoding (ASCII string) of your binary image data. This string should look similar to the following string:- `/9j/4QAYRXhpZgAA...9tAVx/zDQDlGxn//2Q==`\nVisit the [Base64 encode](/document-ai/docs/base64) topic for more information.\n- : Specifies which fields to include in the`Document`output. This is a comma-separated list of fully qualified names of fields in` [FieldMask](https://developers.google.com/protocol-buffers/docs/reference/google.protobuf#google.protobuf.FieldMask) `format.- Example:`text,entities,pages.pageNumber`\n- OCR configurations- : (Boolean) Extracts embedded text from PDFs, if available.\n- : (Boolean) Enables intelligent document quality scores.\n- : (Boolean) Includes symbol (letter) OCR information.\n- : (Boolean) Turn off character box detector in OCR engine.\n- : List of [BCP-47](https://www.rfc-editor.org/info/bcp47) language codes to use for OCR.\n- : A list of advanced OCR options to further fine-tune OCR behavior. Current valid values are:- `legacy_layout`: a heuristics layout detection algorithm, which serves as an alternative to the current ML-based layout detection algorithm.\n- Premium OCR add ons- : (Boolean) Turn on selection mark detector in OCR engine.\n- (Boolean) Turn on font identification model and return font style information.\n- : (Boolean) Turn on the model that can extract LaTeX math formulas.\n- : A list of individual pages to process.- Alternatively, provide field [fromStart](/document-ai/docs/reference/rest/v1/ProcessOptions#FIELDS.from_start) or [fromEnd](/document-ai/docs/reference/rest/v1/ProcessOptions#FIELDS.from_end) to process a specific quantity of pages from the beginning or end of the document.\n\u2020 This content can also be specified using base64-encoded content in the [inlineDocument](/document-ai/docs/reference/rest/v1/projects.locations.processors/process) object.\nHTTP method and URL:\n```\nPOST https://LOCATION-documentai.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/processorVersions/PROCESSOR_VERSION:process\n```\nRequest JSON body:\n```\n{\n \"skipHumanReview\": skipHumanReview,\n \"rawDocument\": {\n \"mimeType\": \"MIME_TYPE\",\n \"content\": \"IMAGE_CONTENT\"\n },\n \"fieldMask\": \"FIELD_MASK\",\n \"processOptions\": {\n \"ocrConfig\": {\n  \"enableNativePdfParsing\": ENABLE_NATIVE_PDF_PARSING,\n  \"enableImageQualityScores\": ENABLE_IMAGE_QUALITY_SCORES,\n  \"enableSymbol\": ENABLE_SYMBOL,\n  \"disableCharacterBoxesDetection\": DISABLE_CHARACTER_BOXES_DETECTION,\n  \"hints\": {\n  \"languageHints\": [   \"LANGUAGE_HINTS\"\n  ]\n  },\n  \"advancedOcrOptions\": [\"ADVANCED_OCR_OPTIONS\"],\n  \"premiumFeatures\": {\n  \"enableSelectionMarkDetection\": ENABLE_SELECTION_MARK_DETECTION,\n  \"computeStyleInfo\": COMPUTE_STYLE_INFO,\n  \"enableMathOcr\": ENABLE_MATH_OCR,\n  }\n },\n \"individualPageSelector\" {\n  \"pages\": [INDIVIDUAL_PAGES]\n }\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-documentai.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/processorVersions/PROCESSOR_VERSION:process\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-documentai.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/processors/PROCESSOR_ID/processorVersions/PROCESSOR_VERSION:process\" | Select-Object -Expand Content\n```\nIf the request is successful, the server returns a `200 OK` HTTP status code and the  response in JSON format. The response body contains an instance of ` [Document](/document-ai/docs/reference/rest/v1/Document) ` .For more information, see the [Document AI Python API reference documentation](/python/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/documentai/snippets/handle_response_sample.py) \n```\nfrom typing import Optional, Sequencefrom google.api_core.client_options import ClientOptionsfrom google.cloud import documentai# TODO(developer): Uncomment these variables before running the sample.# project_id = \"YOUR_PROJECT_ID\"# location = \"YOUR_PROCESSOR_LOCATION\" # Format is \"us\" or \"eu\"# processor_id = \"YOUR_PROCESSOR_ID\" # Create processor before running sample# processor_version = \"rc\" # Refer to https://cloud.google.com/document-ai/docs/manage-processor-versions for more information# file_path = \"/path/to/local/pdf\"# mime_type = \"application/pdf\" # Refer to https://cloud.google.com/document-ai/docs/file-types for supported file typesdef process_document_ocr_sample(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version: str,\u00a0 \u00a0 file_path: str,\u00a0 \u00a0 mime_type: str,) -> None:\u00a0 \u00a0 # Optional: Additional configurations for Document OCR Processor.\u00a0 \u00a0 # For more information: https://cloud.google.com/document-ai/docs/enterprise-document-ocr\u00a0 \u00a0 process_options = documentai.ProcessOptions(\u00a0 \u00a0 \u00a0 \u00a0 ocr_config=documentai.OcrConfig(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enable_native_pdf_parsing=True,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enable_image_quality_scores=True,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enable_symbol=True,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # OCR Add Ons https://cloud.google.com/document-ai/docs/ocr-add-ons\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 premium_features=documentai.OcrConfig.PremiumFeatures(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 compute_style_info=True,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enable_math_ocr=False, \u00a0# Enable to use Math OCR Model\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enable_selection_mark_detection=True,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 # Online processing request to Document AI\u00a0 \u00a0 document = process_document(\u00a0 \u00a0 \u00a0 \u00a0 project_id,\u00a0 \u00a0 \u00a0 \u00a0 location,\u00a0 \u00a0 \u00a0 \u00a0 processor_id,\u00a0 \u00a0 \u00a0 \u00a0 processor_version,\u00a0 \u00a0 \u00a0 \u00a0 file_path,\u00a0 \u00a0 \u00a0 \u00a0 mime_type,\u00a0 \u00a0 \u00a0 \u00a0 process_options=process_options,\u00a0 \u00a0 )\u00a0 \u00a0 text = document.text\u00a0 \u00a0 print(f\"Full document text: {text}\\n\")\u00a0 \u00a0 print(f\"There are {len(document.pages)} page(s) in this document.\\n\")\u00a0 \u00a0 for page in document.pages:\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Page {page.page_number}:\")\u00a0 \u00a0 \u00a0 \u00a0 print_page_dimensions(page.dimension)\u00a0 \u00a0 \u00a0 \u00a0 print_detected_langauges(page.detected_languages)\u00a0 \u00a0 \u00a0 \u00a0 print_blocks(page.blocks, text)\u00a0 \u00a0 \u00a0 \u00a0 print_paragraphs(page.paragraphs, text)\u00a0 \u00a0 \u00a0 \u00a0 print_lines(page.lines, text)\u00a0 \u00a0 \u00a0 \u00a0 print_tokens(page.tokens, text)\u00a0 \u00a0 \u00a0 \u00a0 if page.symbols:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print_symbols(page.symbols, text)\u00a0 \u00a0 \u00a0 \u00a0 if page.image_quality_scores:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print_image_quality_scores(page.image_quality_scores)\u00a0 \u00a0 \u00a0 \u00a0 if page.visual_elements:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print_visual_elements(page.visual_elements, text)def print_page_dimensions(dimension: documentai.Document.Page.Dimension) -> None:\u00a0 \u00a0 print(f\" \u00a0 \u00a0Width: {str(dimension.width)}\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0Height: {str(dimension.height)}\")def print_detected_langauges(\u00a0 \u00a0 detected_languages: Sequence[documentai.Document.Page.DetectedLanguage],) -> None:\u00a0 \u00a0 print(\" \u00a0 \u00a0Detected languages:\")\u00a0 \u00a0 for lang in detected_languages:\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0{lang.language_code} ({lang.confidence:.1%} confidence)\")def print_blocks(blocks: Sequence[documentai.Document.Page.Block], text: str) -> None:\u00a0 \u00a0 print(f\" \u00a0 \u00a0{len(blocks)} blocks detected:\")\u00a0 \u00a0 first_block_text = layout_to_text(blocks[0].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First text block: {repr(first_block_text)}\")\u00a0 \u00a0 last_block_text = layout_to_text(blocks[-1].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0Last text block: {repr(last_block_text)}\")def print_paragraphs(\u00a0 \u00a0 paragraphs: Sequence[documentai.Document.Page.Paragraph], text: str) -> None:\u00a0 \u00a0 print(f\" \u00a0 \u00a0{len(paragraphs)} paragraphs detected:\")\u00a0 \u00a0 first_paragraph_text = layout_to_text(paragraphs[0].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First paragraph text: {repr(first_paragraph_text)}\")\u00a0 \u00a0 last_paragraph_text = layout_to_text(paragraphs[-1].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0Last paragraph text: {repr(last_paragraph_text)}\")def print_lines(lines: Sequence[documentai.Document.Page.Line], text: str) -> None:\u00a0 \u00a0 print(f\" \u00a0 \u00a0{len(lines)} lines detected:\")\u00a0 \u00a0 first_line_text = layout_to_text(lines[0].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First line text: {repr(first_line_text)}\")\u00a0 \u00a0 last_line_text = layout_to_text(lines[-1].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0Last line text: {repr(last_line_text)}\")def print_tokens(tokens: Sequence[documentai.Document.Page.Token], text: str) -> None:\u00a0 \u00a0 print(f\" \u00a0 \u00a0{len(tokens)} tokens detected:\")\u00a0 \u00a0 first_token_text = layout_to_text(tokens[0].layout, text)\u00a0 \u00a0 first_token_break_type = tokens[0].detected_break.type_.name\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First token text: {repr(first_token_text)}\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First token break type: {repr(first_token_break_type)}\")\u00a0 \u00a0 if tokens[0].style_info:\u00a0 \u00a0 \u00a0 \u00a0 print_style_info(tokens[0].style_info)\u00a0 \u00a0 last_token_text = layout_to_text(tokens[-1].layout, text)\u00a0 \u00a0 last_token_break_type = tokens[-1].detected_break.type_.name\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0Last token text: {repr(last_token_text)}\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0Last token break type: {repr(last_token_break_type)}\")\u00a0 \u00a0 if tokens[-1].style_info:\u00a0 \u00a0 \u00a0 \u00a0 print_style_info(tokens[-1].style_info)def print_symbols(\u00a0 \u00a0 symbols: Sequence[documentai.Document.Page.Symbol], text: str) -> None:\u00a0 \u00a0 print(f\" \u00a0 \u00a0{len(symbols)} symbols detected:\")\u00a0 \u00a0 first_symbol_text = layout_to_text(symbols[0].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First symbol text: {repr(first_symbol_text)}\")\u00a0 \u00a0 last_symbol_text = layout_to_text(symbols[-1].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0Last symbol text: {repr(last_symbol_text)}\")def print_image_quality_scores(\u00a0 \u00a0 image_quality_scores: documentai.Document.Page.ImageQualityScores,) -> None:\u00a0 \u00a0 print(f\" \u00a0 \u00a0Quality score: {image_quality_scores.quality_score:.1%}\")\u00a0 \u00a0 print(\" \u00a0 \u00a0Detected defects:\")\u00a0 \u00a0 for detected_defect in image_quality_scores.detected_defects:\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0{detected_defect.type_}: {detected_defect.confidence:.1%}\")def print_style_info(style_info: documentai.Document.Page.Token.StyleInfo) -> None:\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 Only supported in version `pretrained-ocr-v2.0-2023-06-02`\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Font Size: {style_info.font_size}pt\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Font Type: {style_info.font_type}\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Bold: {style_info.bold}\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Italic: {style_info.italic}\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Underlined: {style_info.underlined}\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Handwritten: {style_info.handwritten}\")\u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 f\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Text Color (RGBa): {style_info.text_color.red}, {style_info.text_color.green}, {style_info.text_color.blue}, {style_info.text_color.alpha}\"\u00a0 \u00a0 )def print_visual_elements(\u00a0 \u00a0 visual_elements: Sequence[documentai.Document.Page.VisualElement], text: str) -> None:\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 Only supported in version `pretrained-ocr-v2.0-2023-06-02`\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 checkboxes = [x for x in visual_elements if \"checkbox\" in x.type]\u00a0 \u00a0 math_symbols = [x for x in visual_elements if x.type == \"math_formula\"]\u00a0 \u00a0 if checkboxes:\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0{len(checkboxes)} checkboxes detected:\")\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First checkbox: {repr(checkboxes[0].type)}\")\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0Last checkbox: {repr(checkboxes[-1].type)}\")\u00a0 \u00a0 if math_symbols:\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0{len(math_symbols)} math symbols detected:\")\u00a0 \u00a0 \u00a0 \u00a0 first_math_symbol_text = layout_to_text(math_symbols[0].layout, text)\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First math symbol: {repr(first_math_symbol_text)}\")def process_document(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version: str,\u00a0 \u00a0 file_path: str,\u00a0 \u00a0 mime_type: str,\u00a0 \u00a0 process_options: Optional[documentai.ProcessOptions] = None,) -> documentai.Document:\u00a0 \u00a0 # You must set the `api_endpoint` if you use a location other than \"us\".\u00a0 \u00a0 client = documentai.DocumentProcessorServiceClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options=ClientOptions(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 api_endpoint=f\"{location}-documentai.googleapis.com\"\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 # The full resource name of the processor version, e.g.:\u00a0 \u00a0 # `projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}`\u00a0 \u00a0 # You must create a processor before running this sample.\u00a0 \u00a0 name = client.processor_version_path(\u00a0 \u00a0 \u00a0 \u00a0 project_id, location, processor_id, processor_version\u00a0 \u00a0 )\u00a0 \u00a0 # Read the file into memory\u00a0 \u00a0 with open(file_path, \"rb\") as image:\u00a0 \u00a0 \u00a0 \u00a0 image_content = image.read()\u00a0 \u00a0 # Configure the process request\u00a0 \u00a0 request = documentai.ProcessRequest(\u00a0 \u00a0 \u00a0 \u00a0 name=name,\u00a0 \u00a0 \u00a0 \u00a0 raw_document=documentai.RawDocument(content=image_content, mime_type=mime_type),\u00a0 \u00a0 \u00a0 \u00a0 # Only supported for Document OCR processor\u00a0 \u00a0 \u00a0 \u00a0 process_options=process_options,\u00a0 \u00a0 )\u00a0 \u00a0 result = client.process_document(request=request)\u00a0 \u00a0 # For a full list of `Document` object attributes, reference this page:\u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/reference/rest/v1/Document\u00a0 \u00a0 return result.documentdef layout_to_text(layout: documentai.Document.Page.Layout, text: str) -> str:\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 Document AI identifies text in different parts of the document by their\u00a0 \u00a0 offsets in the entirety of the document\"s text. This function converts\u00a0 \u00a0 offsets to a string.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 # If a text segment spans several lines, it will\u00a0 \u00a0 # be stored in different text segments.\u00a0 \u00a0 return \"\".join(\u00a0 \u00a0 \u00a0 \u00a0 text[int(segment.start_index) : int(segment.end_index)]\u00a0 \u00a0 \u00a0 \u00a0 for segment in layout.text_anchor.text_segments\u00a0 \u00a0 )\n```\n## Clean upTo avoid incurring charges to your Google Cloud account for   the resources used on this page, follow these steps.## What's next", "guide": "Document AI"}