{"title": "Document AI - Handling the processing response", "url": "https://cloud.google.com/document-ai/docs/handle-response?hl=zh-cn", "abstract": "# Document AI - Handling the processing response\n# Handling the processing response\nThe response to a processing request contains a [Document](/document-ai/docs/reference/rest/v1/Document) object that holds everything known about the processed document, including all of the structured information that Document AI was able to extract.\nThis page explains the layout of the `Document` object by providing sample documents and then mapping them to fields. It also provides [Client Library](/document-ai/docs/process-documents-client-libraries) code samples and [Document AI Toolbox SDK](/document-ai/docs/toolbox) code samples. These code samples use online processing, but the `Document` object parsing works the same for batch processing.\n", "content": "## Text, layout, and quality scores\nHere's a sample text document:\nHere's the full document object as returned by the [Document OCR](/document-ai/docs/processors-list#processor_doc-ocr) processor:\n[Download JSON](/static/document-ai/docs/images/document-ocr.json)\nHere are some of the important fields:\n- The `text` field contains the text that is recognized by Document AI. This text doesn't contain any layout structure other than spaces, tabs, and line feeds. This is the only field that stores a document's textual information. Other fields can refer to parts of the text field by position ( `startIndex` and `endIndex` ).```\n{\u00a0 text: \"Sample Document\\nHeading 1\\nLorem ipsum dolor sit amet, ...\"}\n```\n- Each [page](/document-ai/docs/reference/rest/v1/Document#page) in the document object corresponds to a physical page from the sample document. Our sample JSON output contains one page, because our sample document is a single PNG image. **Note:** `pages[].pageNumber` is 1-based, not 0-based.```\n{\u00a0 \"pages:\" [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"pageNumber\": 1,\u00a0 \u00a0 \u00a0 \"dimension\": {\u00a0 \u00a0 \u00a0 \u00a0 \"width\": 679.0,\u00a0 \u00a0 \u00a0 \u00a0 \"height\": 460.0,\u00a0 \u00a0 \u00a0 \u00a0 \"unit\": \"pixels\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 }\u00a0 ]}\n```- The [pages[].detectedLanguages[]](/document-ai/docs/reference/rest/v1/Document#Page.FIELDS.detected_languages) field contains the languages found on a given page, along with the confidence score. You can see the list of supported languages on the [Language support](/document-ai/docs/languages) page.\n```\n{\u00a0 \"pages\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"detectedLanguages\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"confidence\": 0.98009938,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"languageCode\": \"en\"\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"confidence\": 0.01990064,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"languageCode\": \"und\"\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 }\u00a0 ]}\n```\n- Document AI is able to detect some elements in the page, such as the paragraphs and lines. Each element has a corresponding [layout](/document-ai/docs/reference/rest/v1/Document#Layout) that describes its position and text. **Caution: Zero coordinate values omitted.** When the API detects a coordinate (\"x\" or \"y\") value of 0, **that coordinate is omitted in the\n JSON response** . For example, a response with a`BoundingPoly`around the entire image would be:`\"normalizedVertices\": [{}, {\"x\": 1}, {\"x\": 1,\"y\": 1}, {\"y\": 1}]`, or`\"vertices\": [{}, {\"x\": X_MAX}, {\"x\": X_MAX,\"y\": Y_MAX}, {\"y\": Y_MAX}]````\n{\u00a0 \"pages\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"paragraphs\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"layout\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"textAnchor\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"textSegments\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"endIndex\": \"16\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"confidence\": 0.9939527,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"boundingPoly\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"vertices\": [ ... ],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"normalizedVertices\": [ ... ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"orientation\": \"PAGE_UP\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 }\u00a0 ]}\n```- For `boundingPoly` , the top-left corner of the page is the origin `(0,0)` . Positive X values are to the right, and positive Y values are down.\n- `vertices` uses the same coordinates as the original image whereas `normalizedVertices` are in the range `[0,1]` .\n- To draw the `boundingPoly` , draw line segments from one vertex to the next. Then, close the polygon by drawing a line segment from the last vertex back to the first.\n- To help you visualize the document's structure, the following images draw bounding polygons for [page.paragraphs](/document-ai/docs/reference/rest/v1/Document#Paragraph) , [page.lines](/document-ai/docs/reference/rest/v1/Document#Line) , [page.tokens](/document-ai/docs/reference/rest/v1/Document#Token) . \n \n \n- The [document OCR](/document-ai/docs/processors-list#processor_doc-ocr) processor can perform quality assessment of a document based on its readability. This quality assessment is a quality score in `[0, 1]` , where `1` means perfect quality. The quality score is returned in the [Page.imageQualityScores](/document-ai/docs/reference/rest/v1/Document#Page.FIELDS.image_quality_scores) field. All detected defects are listed as `quality/defect_*` and sorted in descending order by confidence value.- You must set the field [processOptions.ocrConfig.enableImageQualityScores](/document-ai/docs/reference/rest/v1/ProcessOptions#OcrConfig.FIELDS.enable_image_quality_scores) to`true`to get this data in the API response.\nHere's a PDF that is too dark and blurry to comfortably read: [Download PDF](/static/document-ai/docs/images/document-quality.pdf) Here's the document quality information as returned by the [document OCR](/document-ai/docs/processors-list#processor_doc-ocr) processor:```\n{\u00a0 \"pages\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"imageQualityScores\": {\u00a0 \u00a0 \u00a0 \u00a0 \"qualityScore\": 0.7811847,\u00a0 \u00a0 \u00a0 \u00a0 \"detectedDefects\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"quality/defect_document_cutoff\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"confidence\": 1.0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"quality/defect_glare\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"confidence\": 0.97849524\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"quality/defect_text_cutoff\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"confidence\": 0.5\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ]}\n```\n### Code samples\nThe following code samples demonstrate how to send a processing request and then read and print the fields to the terminal:\nFor more information, see the [Document AI Java API reference documentation](/java/docs/reference/google-cloud-document-ai/latest/overview) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/document-ai/src/main/java/documentai/v1beta3/ProcessOcrDocument.java) \n```\nimport com.google.cloud.documentai.v1beta3.Document;import com.google.cloud.documentai.v1beta3.DocumentProcessorServiceClient;import com.google.cloud.documentai.v1beta3.DocumentProcessorServiceSettings;import com.google.cloud.documentai.v1beta3.ProcessRequest;import com.google.cloud.documentai.v1beta3.ProcessResponse;import com.google.cloud.documentai.v1beta3.RawDocument;import com.google.protobuf.ByteString;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Paths;import java.util.List;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeoutException;public class ProcessOcrDocument {\u00a0 public static void processOcrDocument()\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String projectId = \"your-project-id\";\u00a0 \u00a0 String location = \"your-project-location\"; // Format is \"us\" or \"eu\".\u00a0 \u00a0 String processerId = \"your-processor-id\";\u00a0 \u00a0 String filePath = \"path/to/input/file.pdf\";\u00a0 \u00a0 processOcrDocument(projectId, location, processerId, filePath);\u00a0 }\u00a0 public static void processOcrDocument(\u00a0 \u00a0 \u00a0 String projectId, String location, String processorId, String filePath)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs\u00a0 \u00a0 // to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your\u00a0 \u00a0 // requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background\u00a0 \u00a0 // resources.\u00a0 \u00a0 String endpoint = String.format(\"%s-documentai.googleapis.com:443\", location);\u00a0 \u00a0 DocumentProcessorServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 DocumentProcessorServiceSettings.newBuilder().setEndpoint(endpoint).build();\u00a0 \u00a0 try (DocumentProcessorServiceClient client = DocumentProcessorServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 // The full resource name of the processor, e.g.:\u00a0 \u00a0 \u00a0 // projects/project-id/locations/location/processor/processor-id\u00a0 \u00a0 \u00a0 // You must create new processors in the Cloud Console first\u00a0 \u00a0 \u00a0 String name =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\"projects/%s/locations/%s/processors/%s\", projectId, location, processorId);\u00a0 \u00a0 \u00a0 // Read the file.\u00a0 \u00a0 \u00a0 byte[] imageFileData = Files.readAllBytes(Paths.get(filePath));\u00a0 \u00a0 \u00a0 // Convert the image data to a Buffer and base64 encode it.\u00a0 \u00a0 \u00a0 ByteString content = ByteString.copyFrom(imageFileData);\u00a0 \u00a0 \u00a0 RawDocument document =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 RawDocument.newBuilder().setContent(content).setMimeType(\"application/pdf\").build();\u00a0 \u00a0 \u00a0 // Configure the process request.\u00a0 \u00a0 \u00a0 ProcessRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ProcessRequest.newBuilder().setName(name).setRawDocument(document).build();\u00a0 \u00a0 \u00a0 // Recognizes text entities in the PDF document\u00a0 \u00a0 \u00a0 ProcessResponse result = client.processDocument(request);\u00a0 \u00a0 \u00a0 Document documentResponse = result.getDocument();\u00a0 \u00a0 \u00a0 System.out.println(\"Document processing complete.\");\u00a0 \u00a0 \u00a0 // Read the text recognition output from the processor\u00a0 \u00a0 \u00a0 // For a full list of Document object attributes,\u00a0 \u00a0 \u00a0 // please reference this page:\u00a0 \u00a0 \u00a0 // https://googleapis.dev/java/google-cloud-document-ai/latest/index.html\u00a0 \u00a0 \u00a0 // Get all of the document text as one big string\u00a0 \u00a0 \u00a0 String text = documentResponse.getText();\u00a0 \u00a0 \u00a0 System.out.printf(\"Full document text: '%s'\\n\", escapeNewlines(text));\u00a0 \u00a0 \u00a0 // Read the text recognition output from the processor\u00a0 \u00a0 \u00a0 List<Document.Page> pages = documentResponse.getPagesList();\u00a0 \u00a0 \u00a0 System.out.printf(\"There are %s page(s) in this document.\\n\", pages.size());\u00a0 \u00a0 \u00a0 for (Document.Page page : pages) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Page %d:\\n\", page.getPageNumber());\u00a0 \u00a0 \u00a0 \u00a0 printPageDimensions(page.getDimension());\u00a0 \u00a0 \u00a0 \u00a0 printDetectedLanguages(page.getDetectedLanguagesList());\u00a0 \u00a0 \u00a0 \u00a0 printParagraphs(page.getParagraphsList(), text);\u00a0 \u00a0 \u00a0 \u00a0 printBlocks(page.getBlocksList(), text);\u00a0 \u00a0 \u00a0 \u00a0 printLines(page.getLinesList(), text);\u00a0 \u00a0 \u00a0 \u00a0 printTokens(page.getTokensList(), text);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\u00a0 private static void printPageDimensions(Document.Page.Dimension dimension) {\u00a0 \u00a0 String unit = dimension.getUnit();\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0Width: %.1f %s\\n\", dimension.getWidth(), unit);\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0Height: %.1f %s\\n\", dimension.getHeight(), unit);\u00a0 }\u00a0 private static void printDetectedLanguages(\u00a0 \u00a0 \u00a0 List<Document.Page.DetectedLanguage> detectedLangauges) {\u00a0 \u00a0 System.out.println(\" \u00a0 \u00a0Detected languages:\");\u00a0 \u00a0 for (Document.Page.DetectedLanguage detectedLanguage : detectedLangauges) {\u00a0 \u00a0 \u00a0 String languageCode = detectedLanguage.getLanguageCode();\u00a0 \u00a0 \u00a0 float confidence = detectedLanguage.getConfidence();\u00a0 \u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0 \u00a0 \u00a0%s (%.2f%%)\\n\", languageCode, confidence * 100.0);\u00a0 \u00a0 }\u00a0 }\u00a0 private static void printParagraphs(List<Document.Page.Paragraph> paragraphs, String text) {\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0%d paragraphs detected:\\n\", paragraphs.size());\u00a0 \u00a0 Document.Page.Paragraph firstParagraph = paragraphs.get(0);\u00a0 \u00a0 String firstParagraphText = getLayoutText(firstParagraph.getLayout().getTextAnchor(), text);\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0 \u00a0 \u00a0First paragraph text: %s\\n\", escapeNewlines(firstParagraphText));\u00a0 \u00a0 Document.Page.Paragraph lastParagraph = paragraphs.get(paragraphs.size() - 1);\u00a0 \u00a0 String lastParagraphText = getLayoutText(lastParagraph.getLayout().getTextAnchor(), text);\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0 \u00a0 \u00a0Last paragraph text: %s\\n\", escapeNewlines(lastParagraphText));\u00a0 }\u00a0 private static void printBlocks(List<Document.Page.Block> blocks, String text) {\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0%d blocks detected:\\n\", blocks.size());\u00a0 \u00a0 Document.Page.Block firstBlock = blocks.get(0);\u00a0 \u00a0 String firstBlockText = getLayoutText(firstBlock.getLayout().getTextAnchor(), text);\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0 \u00a0 \u00a0First block text: %s\\n\", escapeNewlines(firstBlockText));\u00a0 \u00a0 Document.Page.Block lastBlock = blocks.get(blocks.size() - 1);\u00a0 \u00a0 String lastBlockText = getLayoutText(lastBlock.getLayout().getTextAnchor(), text);\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0 \u00a0 \u00a0Last block text: %s\\n\", escapeNewlines(lastBlockText));\u00a0 }\u00a0 private static void printLines(List<Document.Page.Line> lines, String text) {\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0%d lines detected:\\n\", lines.size());\u00a0 \u00a0 Document.Page.Line firstLine = lines.get(0);\u00a0 \u00a0 String firstLineText = getLayoutText(firstLine.getLayout().getTextAnchor(), text);\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0 \u00a0 \u00a0First line text: %s\\n\", escapeNewlines(firstLineText));\u00a0 \u00a0 Document.Page.Line lastLine = lines.get(lines.size() - 1);\u00a0 \u00a0 String lastLineText = getLayoutText(lastLine.getLayout().getTextAnchor(), text);\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0 \u00a0 \u00a0Last line text: %s\\n\", escapeNewlines(lastLineText));\u00a0 }\u00a0 private static void printTokens(List<Document.Page.Token> tokens, String text) {\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0%d tokens detected:\\n\", tokens.size());\u00a0 \u00a0 Document.Page.Token firstToken = tokens.get(0);\u00a0 \u00a0 String firstTokenText = getLayoutText(firstToken.getLayout().getTextAnchor(), text);\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0 \u00a0 \u00a0First token text: %s\\n\", escapeNewlines(firstTokenText));\u00a0 \u00a0 Document.Page.Token lastToken = tokens.get(tokens.size() - 1);\u00a0 \u00a0 String lastTokenText = getLayoutText(lastToken.getLayout().getTextAnchor(), text);\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0 \u00a0 \u00a0Last token text: %s\\n\", escapeNewlines(lastTokenText));\u00a0 }\u00a0 // Extract shards from the text field\u00a0 private static String getLayoutText(Document.TextAnchor textAnchor, String text) {\u00a0 \u00a0 if (textAnchor.getTextSegmentsList().size() > 0) {\u00a0 \u00a0 \u00a0 int startIdx = (int) textAnchor.getTextSegments(0).getStartIndex();\u00a0 \u00a0 \u00a0 int endIdx = (int) textAnchor.getTextSegments(0).getEndIndex();\u00a0 \u00a0 \u00a0 return text.substring(startIdx, endIdx);\u00a0 \u00a0 }\u00a0 \u00a0 return \"[NO TEXT]\";\u00a0 }\u00a0 private static String escapeNewlines(String s) {\u00a0 \u00a0 return s.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\\\\r\");\u00a0 }}\n```For more information, see the [Document AI Node.js API reference documentation](/nodejs/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/document-ai/process-document-ocr.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*/// const projectId = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION'; // Format is 'us' or 'eu'// const processorId = 'YOUR_PROCESSOR_ID'; // Create processor in Cloud Console// const filePath = '/path/to/local/pdf';const {DocumentProcessorServiceClient} =\u00a0 require('@google-cloud/documentai').v1beta3;// Instantiates a clientconst client = new DocumentProcessorServiceClient();async function processDocument() {\u00a0 // The full resource name of the processor, e.g.:\u00a0 // projects/project-id/locations/location/processor/processor-id\u00a0 // You must create new processors in the Cloud Console first\u00a0 const name = `projects/${projectId}/locations/${location}/processors/${processorId}`;\u00a0 // Read the file into memory.\u00a0 const fs = require('fs').promises;\u00a0 const imageFile = await fs.readFile(filePath);\u00a0 // Convert the image data to a Buffer and base64 encode it.\u00a0 const encodedImage = Buffer.from(imageFile).toString('base64');\u00a0 const request = {\u00a0 \u00a0 name,\u00a0 \u00a0 rawDocument: {\u00a0 \u00a0 \u00a0 content: encodedImage,\u00a0 \u00a0 \u00a0 mimeType: 'application/pdf',\u00a0 \u00a0 },\u00a0 };\u00a0 // Recognizes text entities in the PDF document\u00a0 const [result] = await client.processDocument(request);\u00a0 console.log('Document processing complete.');\u00a0 // Read the text recognition output from the processor\u00a0 // For a full list of Document object attributes,\u00a0 // please reference this page: https://googleapis.dev/nodejs/documentai/latest/index.html\u00a0 const {document} = result;\u00a0 const {text} = document;\u00a0 // Read the text recognition output from the processor\u00a0 console.log(`Full document text: ${JSON.stringify(text)}`);\u00a0 console.log(`There are ${document.pages.length} page(s) in this document.`);\u00a0 for (const page of document.pages) {\u00a0 \u00a0 console.log(`Page ${page.pageNumber}`);\u00a0 \u00a0 printPageDimensions(page.dimension);\u00a0 \u00a0 printDetectedLanguages(page.detectedLanguages);\u00a0 \u00a0 printParagraphs(page.paragraphs, text);\u00a0 \u00a0 printBlocks(page.blocks, text);\u00a0 \u00a0 printLines(page.lines, text);\u00a0 \u00a0 printTokens(page.tokens, text);\u00a0 }}const printPageDimensions = dimension => {\u00a0 console.log(` \u00a0 \u00a0Width: ${dimension.width}`);\u00a0 console.log(` \u00a0 \u00a0Height: ${dimension.height}`);};const printDetectedLanguages = detectedLanguages => {\u00a0 console.log(' \u00a0 \u00a0Detected languages:');\u00a0 for (const lang of detectedLanguages) {\u00a0 \u00a0 const code = lang.languageCode;\u00a0 \u00a0 const confPercent = lang.confidence * 100;\u00a0 \u00a0 console.log(` \u00a0 \u00a0 \u00a0 \u00a0${code} (${confPercent.toFixed(2)}% confidence)`);\u00a0 }};const printParagraphs = (paragraphs, text) => {\u00a0 console.log(` \u00a0 \u00a0${paragraphs.length} paragraphs detected:`);\u00a0 const firstParagraphText = getText(paragraphs[0].layout.textAnchor, text);\u00a0 console.log(\u00a0 \u00a0 ` \u00a0 \u00a0 \u00a0 \u00a0First paragraph text: ${JSON.stringify(firstParagraphText)}`\u00a0 );\u00a0 const lastParagraphText = getText(\u00a0 \u00a0 paragraphs[paragraphs.length - 1].layout.textAnchor,\u00a0 \u00a0 text\u00a0 );\u00a0 console.log(\u00a0 \u00a0 ` \u00a0 \u00a0 \u00a0 \u00a0Last paragraph text: ${JSON.stringify(lastParagraphText)}`\u00a0 );};const printBlocks = (blocks, text) => {\u00a0 console.log(` \u00a0 \u00a0${blocks.length} blocks detected:`);\u00a0 const firstBlockText = getText(blocks[0].layout.textAnchor, text);\u00a0 console.log(` \u00a0 \u00a0 \u00a0 \u00a0First block text: ${JSON.stringify(firstBlockText)}`);\u00a0 const lastBlockText = getText(\u00a0 \u00a0 blocks[blocks.length - 1].layout.textAnchor,\u00a0 \u00a0 text\u00a0 );\u00a0 console.log(` \u00a0 \u00a0 \u00a0 \u00a0Last block text: ${JSON.stringify(lastBlockText)}`);};const printLines = (lines, text) => {\u00a0 console.log(` \u00a0 \u00a0${lines.length} lines detected:`);\u00a0 const firstLineText = getText(lines[0].layout.textAnchor, text);\u00a0 console.log(` \u00a0 \u00a0 \u00a0 \u00a0First line text: ${JSON.stringify(firstLineText)}`);\u00a0 const lastLineText = getText(\u00a0 \u00a0 lines[lines.length - 1].layout.textAnchor,\u00a0 \u00a0 text\u00a0 );\u00a0 console.log(` \u00a0 \u00a0 \u00a0 \u00a0Last line text: ${JSON.stringify(lastLineText)}`);};const printTokens = (tokens, text) => {\u00a0 console.log(` \u00a0 \u00a0${tokens.length} tokens detected:`);\u00a0 const firstTokenText = getText(tokens[0].layout.textAnchor, text);\u00a0 console.log(` \u00a0 \u00a0 \u00a0 \u00a0First token text: ${JSON.stringify(firstTokenText)}`);\u00a0 const firstTokenBreakType = tokens[0].detectedBreak.type;\u00a0 console.log(` \u00a0 \u00a0 \u00a0 \u00a0First token break type: ${firstTokenBreakType}`);\u00a0 const lastTokenText = getText(\u00a0 \u00a0 tokens[tokens.length - 1].layout.textAnchor,\u00a0 \u00a0 text\u00a0 );\u00a0 console.log(` \u00a0 \u00a0 \u00a0 \u00a0Last token text: ${JSON.stringify(lastTokenText)}`);\u00a0 const lastTokenBreakType = tokens[tokens.length - 1].detectedBreak.type;\u00a0 console.log(` \u00a0 \u00a0 \u00a0 \u00a0Last token break type: ${lastTokenBreakType}`);};// Extract shards from the text fieldconst getText = (textAnchor, text) => {\u00a0 if (!textAnchor.textSegments || textAnchor.textSegments.length === 0) {\u00a0 \u00a0 return '';\u00a0 }\u00a0 // First shard in document doesn't have startIndex property\u00a0 const startIndex = textAnchor.textSegments[0].startIndex || 0;\u00a0 const endIndex = textAnchor.textSegments[0].endIndex;\u00a0 return text.substring(startIndex, endIndex);};\n```For more information, see the [Document AI Python API reference documentation](/python/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/documentai/snippets/handle_response_sample.py) \n```\nfrom typing import Optional, Sequencefrom google.api_core.client_options import ClientOptionsfrom google.cloud import documentai# TODO(developer): Uncomment these variables before running the sample.# project_id = \"YOUR_PROJECT_ID\"# location = \"YOUR_PROCESSOR_LOCATION\" # Format is \"us\" or \"eu\"# processor_id = \"YOUR_PROCESSOR_ID\" # Create processor before running sample# processor_version = \"rc\" # Refer to https://cloud.google.com/document-ai/docs/manage-processor-versions for more information# file_path = \"/path/to/local/pdf\"# mime_type = \"application/pdf\" # Refer to https://cloud.google.com/document-ai/docs/file-types for supported file typesdef process_document_ocr_sample(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version: str,\u00a0 \u00a0 file_path: str,\u00a0 \u00a0 mime_type: str,) -> None:\u00a0 \u00a0 # Optional: Additional configurations for Document OCR Processor.\u00a0 \u00a0 # For more information: https://cloud.google.com/document-ai/docs/enterprise-document-ocr\u00a0 \u00a0 process_options = documentai.ProcessOptions(\u00a0 \u00a0 \u00a0 \u00a0 ocr_config=documentai.OcrConfig(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enable_native_pdf_parsing=True,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enable_image_quality_scores=True,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enable_symbol=True,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # OCR Add Ons https://cloud.google.com/document-ai/docs/ocr-add-ons\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 premium_features=documentai.OcrConfig.PremiumFeatures(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 compute_style_info=True,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enable_math_ocr=False, \u00a0# Enable to use Math OCR Model\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 enable_selection_mark_detection=True,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 # Online processing request to Document AI\u00a0 \u00a0 document = process_document(\u00a0 \u00a0 \u00a0 \u00a0 project_id,\u00a0 \u00a0 \u00a0 \u00a0 location,\u00a0 \u00a0 \u00a0 \u00a0 processor_id,\u00a0 \u00a0 \u00a0 \u00a0 processor_version,\u00a0 \u00a0 \u00a0 \u00a0 file_path,\u00a0 \u00a0 \u00a0 \u00a0 mime_type,\u00a0 \u00a0 \u00a0 \u00a0 process_options=process_options,\u00a0 \u00a0 )\u00a0 \u00a0 text = document.text\u00a0 \u00a0 print(f\"Full document text: {text}\\n\")\u00a0 \u00a0 print(f\"There are {len(document.pages)} page(s) in this document.\\n\")\u00a0 \u00a0 for page in document.pages:\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Page {page.page_number}:\")\u00a0 \u00a0 \u00a0 \u00a0 print_page_dimensions(page.dimension)\u00a0 \u00a0 \u00a0 \u00a0 print_detected_langauges(page.detected_languages)\u00a0 \u00a0 \u00a0 \u00a0 print_blocks(page.blocks, text)\u00a0 \u00a0 \u00a0 \u00a0 print_paragraphs(page.paragraphs, text)\u00a0 \u00a0 \u00a0 \u00a0 print_lines(page.lines, text)\u00a0 \u00a0 \u00a0 \u00a0 print_tokens(page.tokens, text)\u00a0 \u00a0 \u00a0 \u00a0 if page.symbols:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print_symbols(page.symbols, text)\u00a0 \u00a0 \u00a0 \u00a0 if page.image_quality_scores:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print_image_quality_scores(page.image_quality_scores)\u00a0 \u00a0 \u00a0 \u00a0 if page.visual_elements:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print_visual_elements(page.visual_elements, text)def print_page_dimensions(dimension: documentai.Document.Page.Dimension) -> None:\u00a0 \u00a0 print(f\" \u00a0 \u00a0Width: {str(dimension.width)}\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0Height: {str(dimension.height)}\")def print_detected_langauges(\u00a0 \u00a0 detected_languages: Sequence[documentai.Document.Page.DetectedLanguage],) -> None:\u00a0 \u00a0 print(\" \u00a0 \u00a0Detected languages:\")\u00a0 \u00a0 for lang in detected_languages:\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0{lang.language_code} ({lang.confidence:.1%} confidence)\")def print_blocks(blocks: Sequence[documentai.Document.Page.Block], text: str) -> None:\u00a0 \u00a0 print(f\" \u00a0 \u00a0{len(blocks)} blocks detected:\")\u00a0 \u00a0 first_block_text = layout_to_text(blocks[0].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First text block: {repr(first_block_text)}\")\u00a0 \u00a0 last_block_text = layout_to_text(blocks[-1].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0Last text block: {repr(last_block_text)}\")def print_paragraphs(\u00a0 \u00a0 paragraphs: Sequence[documentai.Document.Page.Paragraph], text: str) -> None:\u00a0 \u00a0 print(f\" \u00a0 \u00a0{len(paragraphs)} paragraphs detected:\")\u00a0 \u00a0 first_paragraph_text = layout_to_text(paragraphs[0].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First paragraph text: {repr(first_paragraph_text)}\")\u00a0 \u00a0 last_paragraph_text = layout_to_text(paragraphs[-1].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0Last paragraph text: {repr(last_paragraph_text)}\")def print_lines(lines: Sequence[documentai.Document.Page.Line], text: str) -> None:\u00a0 \u00a0 print(f\" \u00a0 \u00a0{len(lines)} lines detected:\")\u00a0 \u00a0 first_line_text = layout_to_text(lines[0].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First line text: {repr(first_line_text)}\")\u00a0 \u00a0 last_line_text = layout_to_text(lines[-1].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0Last line text: {repr(last_line_text)}\")def print_tokens(tokens: Sequence[documentai.Document.Page.Token], text: str) -> None:\u00a0 \u00a0 print(f\" \u00a0 \u00a0{len(tokens)} tokens detected:\")\u00a0 \u00a0 first_token_text = layout_to_text(tokens[0].layout, text)\u00a0 \u00a0 first_token_break_type = tokens[0].detected_break.type_.name\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First token text: {repr(first_token_text)}\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First token break type: {repr(first_token_break_type)}\")\u00a0 \u00a0 if tokens[0].style_info:\u00a0 \u00a0 \u00a0 \u00a0 print_style_info(tokens[0].style_info)\u00a0 \u00a0 last_token_text = layout_to_text(tokens[-1].layout, text)\u00a0 \u00a0 last_token_break_type = tokens[-1].detected_break.type_.name\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0Last token text: {repr(last_token_text)}\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0Last token break type: {repr(last_token_break_type)}\")\u00a0 \u00a0 if tokens[-1].style_info:\u00a0 \u00a0 \u00a0 \u00a0 print_style_info(tokens[-1].style_info)def print_symbols(\u00a0 \u00a0 symbols: Sequence[documentai.Document.Page.Symbol], text: str) -> None:\u00a0 \u00a0 print(f\" \u00a0 \u00a0{len(symbols)} symbols detected:\")\u00a0 \u00a0 first_symbol_text = layout_to_text(symbols[0].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First symbol text: {repr(first_symbol_text)}\")\u00a0 \u00a0 last_symbol_text = layout_to_text(symbols[-1].layout, text)\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0Last symbol text: {repr(last_symbol_text)}\")def print_image_quality_scores(\u00a0 \u00a0 image_quality_scores: documentai.Document.Page.ImageQualityScores,) -> None:\u00a0 \u00a0 print(f\" \u00a0 \u00a0Quality score: {image_quality_scores.quality_score:.1%}\")\u00a0 \u00a0 print(\" \u00a0 \u00a0Detected defects:\")\u00a0 \u00a0 for detected_defect in image_quality_scores.detected_defects:\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0{detected_defect.type_}: {detected_defect.confidence:.1%}\")def print_style_info(style_info: documentai.Document.Page.Token.StyleInfo) -> None:\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 Only supported in version `pretrained-ocr-v2.0-2023-06-02`\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Font Size: {style_info.font_size}pt\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Font Type: {style_info.font_type}\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Bold: {style_info.bold}\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Italic: {style_info.italic}\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Underlined: {style_info.underlined}\")\u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Handwritten: {style_info.handwritten}\")\u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 f\" \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Text Color (RGBa): {style_info.text_color.red}, {style_info.text_color.green}, {style_info.text_color.blue}, {style_info.text_color.alpha}\"\u00a0 \u00a0 )def print_visual_elements(\u00a0 \u00a0 visual_elements: Sequence[documentai.Document.Page.VisualElement], text: str) -> None:\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 Only supported in version `pretrained-ocr-v2.0-2023-06-02`\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 checkboxes = [x for x in visual_elements if \"checkbox\" in x.type]\u00a0 \u00a0 math_symbols = [x for x in visual_elements if x.type == \"math_formula\"]\u00a0 \u00a0 if checkboxes:\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0{len(checkboxes)} checkboxes detected:\")\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First checkbox: {repr(checkboxes[0].type)}\")\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0Last checkbox: {repr(checkboxes[-1].type)}\")\u00a0 \u00a0 if math_symbols:\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0{len(math_symbols)} math symbols detected:\")\u00a0 \u00a0 \u00a0 \u00a0 first_math_symbol_text = layout_to_text(math_symbols[0].layout, text)\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0 \u00a0 \u00a0First math symbol: {repr(first_math_symbol_text)}\")def process_document(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version: str,\u00a0 \u00a0 file_path: str,\u00a0 \u00a0 mime_type: str,\u00a0 \u00a0 process_options: Optional[documentai.ProcessOptions] = None,) -> documentai.Document:\u00a0 \u00a0 # You must set the `api_endpoint` if you use a location other than \"us\".\u00a0 \u00a0 client = documentai.DocumentProcessorServiceClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options=ClientOptions(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 api_endpoint=f\"{location}-documentai.googleapis.com\"\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 # The full resource name of the processor version, e.g.:\u00a0 \u00a0 # `projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}`\u00a0 \u00a0 # You must create a processor before running this sample.\u00a0 \u00a0 name = client.processor_version_path(\u00a0 \u00a0 \u00a0 \u00a0 project_id, location, processor_id, processor_version\u00a0 \u00a0 )\u00a0 \u00a0 # Read the file into memory\u00a0 \u00a0 with open(file_path, \"rb\") as image:\u00a0 \u00a0 \u00a0 \u00a0 image_content = image.read()\u00a0 \u00a0 # Configure the process request\u00a0 \u00a0 request = documentai.ProcessRequest(\u00a0 \u00a0 \u00a0 \u00a0 name=name,\u00a0 \u00a0 \u00a0 \u00a0 raw_document=documentai.RawDocument(content=image_content, mime_type=mime_type),\u00a0 \u00a0 \u00a0 \u00a0 # Only supported for Document OCR processor\u00a0 \u00a0 \u00a0 \u00a0 process_options=process_options,\u00a0 \u00a0 )\u00a0 \u00a0 result = client.process_document(request=request)\u00a0 \u00a0 # For a full list of `Document` object attributes, reference this page:\u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/reference/rest/v1/Document\u00a0 \u00a0 return result.documentdef layout_to_text(layout: documentai.Document.Page.Layout, text: str) -> str:\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 Document AI identifies text in different parts of the document by their\u00a0 \u00a0 offsets in the entirety of the document\"s text. This function converts\u00a0 \u00a0 offsets to a string.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 # If a text segment spans several lines, it will\u00a0 \u00a0 # be stored in different text segments.\u00a0 \u00a0 return \"\".join(\u00a0 \u00a0 \u00a0 \u00a0 text[int(segment.start_index) : int(segment.end_index)]\u00a0 \u00a0 \u00a0 \u00a0 for segment in layout.text_anchor.text_segments\u00a0 \u00a0 )\n```\n## Forms and tables\nHere's our sample form:\nHere's the full document object as returned by the [Form Parser](/document-ai/docs/processors-list#processor_form-parser) :\n[Download JSON](/static/document-ai/docs/images/form.json)\nHere are some of the important fields:\n- The Form Parser is able to detect [FormFields](/document-ai/docs/reference/rest/v1/Document#FormField) in the page. Each form field has a name and value.```\n{\u00a0 \"pages:\" [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"formFields\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fieldName\": { ... },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fieldValue\": { ... }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 }\u00a0 ]}\n```\n- Document AI can also detect [Tables](/document-ai/docs/reference/rest/v1/Document#Table) in the page.```\n{\u00a0 \"pages:\" [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"tables\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"layout\": { ... },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"headerRows\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"cells\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"layout\": { ... },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"rowSpan\": 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"colSpan\": 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"layout\": { ... },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"rowSpan\": 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"colSpan\": 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"bodyRows\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"cells\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"layout\": { ... },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"rowSpan\": 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"colSpan\": 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"layout\": { ... },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"rowSpan\": 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"colSpan\": 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 }\u00a0 ]}\n```\n- Starting with processor version `pretrained-form-parser-v2.0-2022-11-10` , the Form Parser can also recognize generic entities. See [Form Parser](/document-ai/docs/form-parser) for more information.\nTo help you visualize the document's structure, the following images draw bounding polygons for [page.formFields](/document-ai/docs/reference/rest/v1/Document#FormField) and [page.tables](/document-ai/docs/reference/rest/v1/Document#Table) .### Code samples\nThe following code samples demonstrate how to send a processing request and then read and print the fields to the terminal:\nFor more information, see the [Document AI Java API reference documentation](/java/docs/reference/google-cloud-document-ai/latest/overview) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/document-ai/src/main/java/documentai/v1beta3/ProcessFormDocument.java) \n```\nimport com.google.cloud.documentai.v1beta3.Document;import com.google.cloud.documentai.v1beta3.DocumentProcessorServiceClient;import com.google.cloud.documentai.v1beta3.DocumentProcessorServiceSettings;import com.google.cloud.documentai.v1beta3.ProcessRequest;import com.google.cloud.documentai.v1beta3.ProcessResponse;import com.google.cloud.documentai.v1beta3.RawDocument;import com.google.protobuf.ByteString;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Paths;import java.util.List;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeoutException;public class ProcessFormDocument {\u00a0 public static void processFormDocument()\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String projectId = \"your-project-id\";\u00a0 \u00a0 String location = \"your-project-location\"; // Format is \"us\" or \"eu\".\u00a0 \u00a0 String processerId = \"your-processor-id\";\u00a0 \u00a0 String filePath = \"path/to/input/file.pdf\";\u00a0 \u00a0 processFormDocument(projectId, location, processerId, filePath);\u00a0 }\u00a0 public static void processFormDocument(\u00a0 \u00a0 \u00a0 String projectId, String location, String processorId, String filePath)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs\u00a0 \u00a0 // to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your\u00a0 \u00a0 // requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background\u00a0 \u00a0 // resources.\u00a0 \u00a0 String endpoint = String.format(\"%s-documentai.googleapis.com:443\", location);\u00a0 \u00a0 DocumentProcessorServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 DocumentProcessorServiceSettings.newBuilder().setEndpoint(endpoint).build();\u00a0 \u00a0 try (DocumentProcessorServiceClient client = DocumentProcessorServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 // The full resource name of the processor, e.g.:\u00a0 \u00a0 \u00a0 // projects/project-id/locations/location/processor/processor-id\u00a0 \u00a0 \u00a0 // You must create new processors in the Cloud Console first\u00a0 \u00a0 \u00a0 String name =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\"projects/%s/locations/%s/processors/%s\", projectId, location, processorId);\u00a0 \u00a0 \u00a0 // Read the file.\u00a0 \u00a0 \u00a0 byte[] imageFileData = Files.readAllBytes(Paths.get(filePath));\u00a0 \u00a0 \u00a0 // Convert the image data to a Buffer and base64 encode it.\u00a0 \u00a0 \u00a0 ByteString content = ByteString.copyFrom(imageFileData);\u00a0 \u00a0 \u00a0 RawDocument document =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 RawDocument.newBuilder().setContent(content).setMimeType(\"application/pdf\").build();\u00a0 \u00a0 \u00a0 // Configure the process request.\u00a0 \u00a0 \u00a0 ProcessRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ProcessRequest.newBuilder().setName(name).setRawDocument(document).build();\u00a0 \u00a0 \u00a0 // Recognizes text entities in the PDF document\u00a0 \u00a0 \u00a0 ProcessResponse result = client.processDocument(request);\u00a0 \u00a0 \u00a0 Document documentResponse = result.getDocument();\u00a0 \u00a0 \u00a0 System.out.println(\"Document processing complete.\");\u00a0 \u00a0 \u00a0 // Read the text recognition output from the processor\u00a0 \u00a0 \u00a0 // For a full list of Document object attributes,\u00a0 \u00a0 \u00a0 // please reference this page:\u00a0 \u00a0 \u00a0 // https://googleapis.dev/java/google-cloud-document-ai/latest/index.html\u00a0 \u00a0 \u00a0 // Get all of the document text as one big string\u00a0 \u00a0 \u00a0 String text = documentResponse.getText();\u00a0 \u00a0 \u00a0 System.out.printf(\"Full document text: '%s'\\n\", removeNewlines(text));\u00a0 \u00a0 \u00a0 // Read the text recognition output from the processor\u00a0 \u00a0 \u00a0 List<Document.Page> pages = documentResponse.getPagesList();\u00a0 \u00a0 \u00a0 System.out.printf(\"There are %s page(s) in this document.\\n\", pages.size());\u00a0 \u00a0 \u00a0 for (Document.Page page : pages) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\n\\n**** Page %d ****\\n\", page.getPageNumber());\u00a0 \u00a0 \u00a0 \u00a0 List<Document.Page.Table> tables = page.getTablesList();\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Found %d table(s):\\n\", tables.size());\u00a0 \u00a0 \u00a0 \u00a0 for (Document.Page.Table table : tables) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 printTableInfo(table, text);\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 List<Document.Page.FormField> formFields = page.getFormFieldsList();\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Found %d form fields:\\n\", formFields.size());\u00a0 \u00a0 \u00a0 \u00a0 for (Document.Page.FormField formField : formFields) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String fieldName = getLayoutText(formField.getFieldName().getTextAnchor(), text);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String fieldValue = getLayoutText(formField.getFieldValue().getTextAnchor(), text);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \" \u00a0 \u00a0* '%s': '%s'\\n\", removeNewlines(fieldName), removeNewlines(fieldValue));\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\u00a0 private static void printTableInfo(Document.Page.Table table, String text) {\u00a0 \u00a0 Document.Page.Table.TableRow firstBodyRow = table.getBodyRows(0);\u00a0 \u00a0 int columnCount = firstBodyRow.getCellsCount();\u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \" \u00a0 \u00a0Table with %d columns and %d rows:\\n\", columnCount, table.getBodyRowsCount());\u00a0 \u00a0 Document.Page.Table.TableRow headerRow = table.getHeaderRows(0);\u00a0 \u00a0 StringBuilder headerRowText = new StringBuilder();\u00a0 \u00a0 for (Document.Page.Table.TableCell cell : headerRow.getCellsList()) {\u00a0 \u00a0 \u00a0 String columnName = getLayoutText(cell.getLayout().getTextAnchor(), text);\u00a0 \u00a0 \u00a0 headerRowText.append(String.format(\"%s | \", removeNewlines(columnName)));\u00a0 \u00a0 }\u00a0 \u00a0 headerRowText.setLength(headerRowText.length() - 3);\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0 \u00a0 \u00a0Collumns: %s\\n\", headerRowText.toString());\u00a0 \u00a0 StringBuilder firstRowText = new StringBuilder();\u00a0 \u00a0 for (Document.Page.Table.TableCell cell : firstBodyRow.getCellsList()) {\u00a0 \u00a0 \u00a0 String cellText = getLayoutText(cell.getLayout().getTextAnchor(), text);\u00a0 \u00a0 \u00a0 firstRowText.append(String.format(\"%s | \", removeNewlines(cellText)));\u00a0 \u00a0 }\u00a0 \u00a0 firstRowText.setLength(firstRowText.length() - 3);\u00a0 \u00a0 System.out.printf(\" \u00a0 \u00a0 \u00a0 \u00a0First row data: %s\\n\", firstRowText.toString());\u00a0 }\u00a0 // Extract shards from the text field\u00a0 private static String getLayoutText(Document.TextAnchor textAnchor, String text) {\u00a0 \u00a0 if (textAnchor.getTextSegmentsList().size() > 0) {\u00a0 \u00a0 \u00a0 int startIdx = (int) textAnchor.getTextSegments(0).getStartIndex();\u00a0 \u00a0 \u00a0 int endIdx = (int) textAnchor.getTextSegments(0).getEndIndex();\u00a0 \u00a0 \u00a0 return text.substring(startIdx, endIdx);\u00a0 \u00a0 }\u00a0 \u00a0 return \"[NO TEXT]\";\u00a0 }\u00a0 private static String removeNewlines(String s) {\u00a0 \u00a0 return s.replace(\"\\n\", \"\").replace(\"\\r\", \"\");\u00a0 }}\n```For more information, see the [Document AI Node.js API reference documentation](/nodejs/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/document-ai/process-document-form.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*/// const projectId = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION'; // Format is 'us' or 'eu'// const processorId = 'YOUR_PROCESSOR_ID'; // Create processor in Cloud Console// const filePath = '/path/to/local/pdf';const {DocumentProcessorServiceClient} =\u00a0 require('@google-cloud/documentai').v1beta3;// Instantiates a clientconst client = new DocumentProcessorServiceClient();async function processDocument() {\u00a0 // The full resource name of the processor, e.g.:\u00a0 // projects/project-id/locations/location/processor/processor-id\u00a0 // You must create new processors in the Cloud Console first\u00a0 const name = `projects/${projectId}/locations/${location}/processors/${processorId}`;\u00a0 // Read the file into memory.\u00a0 const fs = require('fs').promises;\u00a0 const imageFile = await fs.readFile(filePath);\u00a0 // Convert the image data to a Buffer and base64 encode it.\u00a0 const encodedImage = Buffer.from(imageFile).toString('base64');\u00a0 const request = {\u00a0 \u00a0 name,\u00a0 \u00a0 rawDocument: {\u00a0 \u00a0 \u00a0 content: encodedImage,\u00a0 \u00a0 \u00a0 mimeType: 'application/pdf',\u00a0 \u00a0 },\u00a0 };\u00a0 // Recognizes text entities in the PDF document\u00a0 const [result] = await client.processDocument(request);\u00a0 console.log('Document processing complete.');\u00a0 // Read the table and form fields output from the processor\u00a0 // The form processor also contains OCR data. For more information\u00a0 // on how to parse OCR data please see the OCR sample.\u00a0 // For a full list of Document object attributes,\u00a0 // please reference this page: https://googleapis.dev/nodejs/documentai/latest/index.html\u00a0 const {document} = result;\u00a0 const {text} = document;\u00a0 console.log(`Full document text: ${JSON.stringify(text)}`);\u00a0 console.log(`There are ${document.pages.length} page(s) in this document.`);\u00a0 for (const page of document.pages) {\u00a0 \u00a0 console.log(`\\n\\n**** Page ${page.pageNumber} ****`);\u00a0 \u00a0 console.log(`Found ${page.tables.length} table(s):`);\u00a0 \u00a0 for (const table of page.tables) {\u00a0 \u00a0 \u00a0 const numCollumns = table.headerRows[0].cells.length;\u00a0 \u00a0 \u00a0 const numRows = table.bodyRows.length;\u00a0 \u00a0 \u00a0 console.log(`Table with ${numCollumns} columns and ${numRows} rows:`);\u00a0 \u00a0 \u00a0 printTableInfo(table, text);\u00a0 \u00a0 }\u00a0 \u00a0 console.log(`Found ${page.formFields.length} form field(s):`);\u00a0 \u00a0 for (const field of page.formFields) {\u00a0 \u00a0 \u00a0 const fieldName = getText(field.fieldName.textAnchor, text);\u00a0 \u00a0 \u00a0 const fieldValue = getText(field.fieldValue.textAnchor, text);\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\t* ${JSON.stringify(fieldName)}: ${JSON.stringify(fieldValue)}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 }\u00a0 }}const printTableInfo = (table, text) => {\u00a0 // Print header row\u00a0 let headerRowText = '';\u00a0 for (const headerCell of table.headerRows[0].cells) {\u00a0 \u00a0 const headerCellText = getText(headerCell.layout.textAnchor, text);\u00a0 \u00a0 headerRowText += `${JSON.stringify(headerCellText.trim())} | `;\u00a0 }\u00a0 console.log(\u00a0 \u00a0 `Collumns: ${headerRowText.substring(0, headerRowText.length - 3)}`\u00a0 );\u00a0 // Print first body row\u00a0 let bodyRowText = '';\u00a0 for (const bodyCell of table.bodyRows[0].cells) {\u00a0 \u00a0 const bodyCellText = getText(bodyCell.layout.textAnchor, text);\u00a0 \u00a0 bodyRowText += `${JSON.stringify(bodyCellText.trim())} | `;\u00a0 }\u00a0 console.log(\u00a0 \u00a0 `First row data: ${bodyRowText.substring(0, bodyRowText.length - 3)}`\u00a0 );};// Extract shards from the text fieldconst getText = (textAnchor, text) => {\u00a0 if (!textAnchor.textSegments || textAnchor.textSegments.length === 0) {\u00a0 \u00a0 return '';\u00a0 }\u00a0 // First shard in document doesn't have startIndex property\u00a0 const startIndex = textAnchor.textSegments[0].startIndex || 0;\u00a0 const endIndex = textAnchor.textSegments[0].endIndex;\u00a0 return text.substring(startIndex, endIndex);};\n```For more information, see the [Document AI Python API reference documentation](/python/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/documentai/snippets/handle_response_sample.py) \n```\nfrom typing import Optional, Sequencefrom google.api_core.client_options import ClientOptionsfrom google.cloud import documentai# TODO(developer): Uncomment these variables before running the sample.# project_id = \"YOUR_PROJECT_ID\"# location = \"YOUR_PROCESSOR_LOCATION\" # Format is \"us\" or \"eu\"# processor_id = \"YOUR_PROCESSOR_ID\" # Create processor before running sample# processor_version = \"rc\" # Refer to https://cloud.google.com/document-ai/docs/manage-processor-versions for more information# file_path = \"/path/to/local/pdf\"# mime_type = \"application/pdf\" # Refer to https://cloud.google.com/document-ai/docs/file-types for supported file typesdef process_document_form_sample(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version: str,\u00a0 \u00a0 file_path: str,\u00a0 \u00a0 mime_type: str,) -> documentai.Document:\u00a0 \u00a0 # Online processing request to Document AI\u00a0 \u00a0 document = process_document(\u00a0 \u00a0 \u00a0 \u00a0 project_id, location, processor_id, processor_version, file_path, mime_type\u00a0 \u00a0 )\u00a0 \u00a0 # Read the table and form fields output from the processor\u00a0 \u00a0 # The form processor also contains OCR data. For more information\u00a0 \u00a0 # on how to parse OCR data please see the OCR sample.\u00a0 \u00a0 text = document.text\u00a0 \u00a0 print(f\"Full document text: {repr(text)}\\n\")\u00a0 \u00a0 print(f\"There are {len(document.pages)} page(s) in this document.\")\u00a0 \u00a0 # Read the form fields and tables output from the processor\u00a0 \u00a0 for page in document.pages:\u00a0 \u00a0 \u00a0 \u00a0 print(f\"\\n\\n**** Page {page.page_number} ****\")\u00a0 \u00a0 \u00a0 \u00a0 print(f\"\\nFound {len(page.tables)} table(s):\")\u00a0 \u00a0 \u00a0 \u00a0 for table in page.tables:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 num_columns = len(table.header_rows[0].cells)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 num_rows = len(table.body_rows)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(f\"Table with {num_columns} columns and {num_rows} rows:\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Print header rows\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"Columns:\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print_table_rows(table.header_rows, text)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Print body rows\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"Table body data:\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print_table_rows(table.body_rows, text)\u00a0 \u00a0 \u00a0 \u00a0 print(f\"\\nFound {len(page.form_fields)} form field(s):\")\u00a0 \u00a0 \u00a0 \u00a0 for field in page.form_fields:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name = layout_to_text(field.field_name, text)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value = layout_to_text(field.field_value, text)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0* {repr(name.strip())}: {repr(value.strip())}\")\u00a0 \u00a0 # Supported in version `pretrained-form-parser-v2.0-2022-11-10` and later.\u00a0 \u00a0 # For more information: https://cloud.google.com/document-ai/docs/form-parser\u00a0 \u00a0 if document.entities:\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Found {len(document.entities)} generic entities:\")\u00a0 \u00a0 \u00a0 \u00a0 for entity in document.entities:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print_entity(entity)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Print Nested Entities\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for prop in entity.properties:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print_entity(prop)\u00a0 \u00a0 return documentdef print_table_rows(\u00a0 \u00a0 table_rows: Sequence[documentai.Document.Page.Table.TableRow], text: str) -> None:\u00a0 \u00a0 for table_row in table_rows:\u00a0 \u00a0 \u00a0 \u00a0 row_text = \"\"\u00a0 \u00a0 \u00a0 \u00a0 for cell in table_row.cells:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cell_text = layout_to_text(cell.layout, text)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 row_text += f\"{repr(cell_text.strip())} | \"\u00a0 \u00a0 \u00a0 \u00a0 print(row_text)def print_entity(entity: documentai.Document.Entity) -> None:\u00a0 \u00a0 # Fields detected. For a full list of fields for each processor see\u00a0 \u00a0 # the processor documentation:\u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/processors-list\u00a0 \u00a0 key = entity.type_\u00a0 \u00a0 # Some other value formats in addition to text are availible\u00a0 \u00a0 # e.g. dates: `entity.normalized_value.date_value.year`\u00a0 \u00a0 text_value = entity.text_anchor.content\u00a0 \u00a0 confidence = entity.confidence\u00a0 \u00a0 normalized_value = entity.normalized_value.text\u00a0 \u00a0 print(f\" \u00a0 \u00a0* {repr(key)}: {repr(text_value)}({confidence:.1%} confident)\")\u00a0 \u00a0 if normalized_value:\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0* Normalized Value: {repr(normalized_value)}\")def process_document(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version: str,\u00a0 \u00a0 file_path: str,\u00a0 \u00a0 mime_type: str,\u00a0 \u00a0 process_options: Optional[documentai.ProcessOptions] = None,) -> documentai.Document:\u00a0 \u00a0 # You must set the `api_endpoint` if you use a location other than \"us\".\u00a0 \u00a0 client = documentai.DocumentProcessorServiceClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options=ClientOptions(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 api_endpoint=f\"{location}-documentai.googleapis.com\"\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 # The full resource name of the processor version, e.g.:\u00a0 \u00a0 # `projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}`\u00a0 \u00a0 # You must create a processor before running this sample.\u00a0 \u00a0 name = client.processor_version_path(\u00a0 \u00a0 \u00a0 \u00a0 project_id, location, processor_id, processor_version\u00a0 \u00a0 )\u00a0 \u00a0 # Read the file into memory\u00a0 \u00a0 with open(file_path, \"rb\") as image:\u00a0 \u00a0 \u00a0 \u00a0 image_content = image.read()\u00a0 \u00a0 # Configure the process request\u00a0 \u00a0 request = documentai.ProcessRequest(\u00a0 \u00a0 \u00a0 \u00a0 name=name,\u00a0 \u00a0 \u00a0 \u00a0 raw_document=documentai.RawDocument(content=image_content, mime_type=mime_type),\u00a0 \u00a0 \u00a0 \u00a0 # Only supported for Document OCR processor\u00a0 \u00a0 \u00a0 \u00a0 process_options=process_options,\u00a0 \u00a0 )\u00a0 \u00a0 result = client.process_document(request=request)\u00a0 \u00a0 # For a full list of `Document` object attributes, reference this page:\u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/reference/rest/v1/Document\u00a0 \u00a0 return result.documentdef layout_to_text(layout: documentai.Document.Page.Layout, text: str) -> str:\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 Document AI identifies text in different parts of the document by their\u00a0 \u00a0 offsets in the entirety of the document\"s text. This function converts\u00a0 \u00a0 offsets to a string.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 # If a text segment spans several lines, it will\u00a0 \u00a0 # be stored in different text segments.\u00a0 \u00a0 return \"\".join(\u00a0 \u00a0 \u00a0 \u00a0 text[int(segment.start_index) : int(segment.end_index)]\u00a0 \u00a0 \u00a0 \u00a0 for segment in layout.text_anchor.text_segments\u00a0 \u00a0 )\n```\n## Entities, nested entities, and normalized values\nMany of the specialized processors extract structured data that is grounded to a well-defined schema. For example, the [Invoice parser](/document-ai/docs/processors-list#processor_invoice-processor) detects specific fields such as `invoice_date` and `supplier_name` . Here's a sample invoice:\nHere's the full document object as returned by the [Invoice parser](/document-ai/docs/processors-list#processor_invoice-processor) :\n[Download JSON](/static/document-ai/docs/images/invoice.json)\nHere are some of the important parts of the document object:\n- **Detected fields** : [Entities](/document-ai/docs/reference/rest/v1/Document#Entity) contains the fields that the processor was able to detect, for example, the `invoice_date` :```\n{\u00a0\"entities\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"textAnchor\": {\u00a0 \u00a0 \u00a0 \u00a0 \"textSegments\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"startIndex\": \"14\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"endIndex\": \"24\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \"content\": \"2020/01/01\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \"type\": \"invoice_date\",\u00a0 \u00a0 \u00a0 \"confidence\": 0.9938466,\u00a0 \u00a0 \u00a0 \"pageAnchor\": { ... },\u00a0 \u00a0 \u00a0 \"id\": \"2\",\u00a0 \u00a0 \u00a0 \"normalizedValue\": {\u00a0 \u00a0 \u00a0 \u00a0 \"text\": \"2020-01-01\",\u00a0 \u00a0 \u00a0 \u00a0 \"dateValue\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"year\": 2020,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"month\": 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"day\": 1\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ]}\n```For certain fields, the processor also **normalizes** the value. In this example, the date has been normalized from `2020/01/01` to `2020-01-01` .\n- **EKG normalization** : Certain processors and fields also support [EKG normalization](/document-ai/docs/ekg-enrichment) . For example, the original `supplier_name` in the document `Google Singapore` has been normalized against the Knowledge Graph to `Google Asia Pacific, Singapore` . Also notice that because the Knowledge Graph contains information about Google, Document AI infers the `supplier_address` even though it wasn't present in the sample document.```\n{\u00a0 \"entities\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"textAnchor\": {\u00a0 \u00a0 \u00a0 \u00a0 \"textSegments\": [ ... ],\u00a0 \u00a0 \u00a0 \u00a0 \"content\": \"Google Singapore\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \"type\": \"supplier_name\",\u00a0 \u00a0 \u00a0 \"confidence\": 0.39170802,\u00a0 \u00a0 \u00a0 \"pageAnchor\": { ... },\u00a0 \u00a0 \u00a0 \"id\": \"12\",\u00a0 \u00a0 \u00a0 \"normalizedValue\": {\u00a0 \u00a0 \u00a0 \u00a0 \"text\": \"Google Asia Pacific, Singapore\"\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"type\": \"supplier_address\",\u00a0 \u00a0 \u00a0 \"id\": \"17\",\u00a0 \u00a0 \u00a0 \"normalizedValue\": {\u00a0 \u00a0 \u00a0 \u00a0 \"text\": \"70 Pasir Panjang Rd #03-71 Mapletree Business City II Singapore 117371\",\u00a0 \u00a0 \u00a0 \u00a0 \"addressValue\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"regionCode\": \"SG\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"languageCode\": \"en-US\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"postalCode\": \"117371\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"addressLines\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"70 Pasir Panjang Rd\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"#03-71 Mapletree Business City II\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ]}\n```\n- **Nested fields** : In the following example, `line_item` is a field that has two fields: `line_item/description` and `line_item/quantity` .```\n{\u00a0 \"entities\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"textAnchor\": { ... },\u00a0 \u00a0 \u00a0 \"type\": \"line_item\",\u00a0 \u00a0 \u00a0 \"confidence\": 1.0,\u00a0 \u00a0 \u00a0 \"pageAnchor\": { ... },\u00a0 \u00a0 \u00a0 \"id\": \"19\",\u00a0 \u00a0 \u00a0 \"properties\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"textAnchor\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"textSegments\": [ ... ],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": \"Tool A\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"line_item/description\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"confidence\": 0.3461604,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"pageAnchor\": { ... },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"id\": \"20\"\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"textAnchor\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"textSegments\": [ ... ],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"content\": \"500\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"line_item/quantity\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"confidence\": 0.8077843,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"pageAnchor\": { ... },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"id\": \"21\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"normalizedValue\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"text\": \"500\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 }\u00a0 ]}\n``` **Note:** The child field might not include the name of its parent. By convention, some processors prefix the child field with `{parent}/` , for example, `line_item/description` and `line_item/quantity` , but not all processors follow that convention.\n### Code samples\nThe following code samples demonstrate how to send a processing request and then read and print the fields from a specialized processor to the terminal:\nFor more information, see the [Document AI Java API reference documentation](/java/docs/reference/google-cloud-document-ai/latest/overview) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/document-ai/src/main/java/documentai/v1beta3/ProcessSpecializedDocument.java) \n```\nimport com.google.cloud.documentai.v1beta3.Document;import com.google.cloud.documentai.v1beta3.DocumentProcessorServiceClient;import com.google.cloud.documentai.v1beta3.DocumentProcessorServiceSettings;import com.google.cloud.documentai.v1beta3.ProcessRequest;import com.google.cloud.documentai.v1beta3.ProcessResponse;import com.google.cloud.documentai.v1beta3.RawDocument;import com.google.protobuf.ByteString;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Paths;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeoutException;public class ProcessSpecializedDocument {\u00a0 public static void processSpecializedDocument()\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String projectId = \"your-project-id\";\u00a0 \u00a0 String location = \"your-project-location\"; // Format is \"us\" or \"eu\".\u00a0 \u00a0 String processerId = \"your-processor-id\";\u00a0 \u00a0 String filePath = \"path/to/input/file.pdf\";\u00a0 \u00a0 processSpecializedDocument(projectId, location, processerId, filePath);\u00a0 }\u00a0 public static void processSpecializedDocument(\u00a0 \u00a0 \u00a0 String projectId, String location, String processorId, String filePath)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs\u00a0 \u00a0 // to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your\u00a0 \u00a0 // requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background\u00a0 \u00a0 // resources.\u00a0 \u00a0 String endpoint = String.format(\"%s-documentai.googleapis.com:443\", location);\u00a0 \u00a0 DocumentProcessorServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 DocumentProcessorServiceSettings.newBuilder().setEndpoint(endpoint).build();\u00a0 \u00a0 try (DocumentProcessorServiceClient client = DocumentProcessorServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 // The full resource name of the processor, e.g.:\u00a0 \u00a0 \u00a0 // projects/project-id/locations/location/processor/processor-id\u00a0 \u00a0 \u00a0 // You must create new processors in the Cloud Console first\u00a0 \u00a0 \u00a0 String name =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\"projects/%s/locations/%s/processors/%s\", projectId, location, processorId);\u00a0 \u00a0 \u00a0 // Read the file.\u00a0 \u00a0 \u00a0 byte[] imageFileData = Files.readAllBytes(Paths.get(filePath));\u00a0 \u00a0 \u00a0 // Convert the image data to a Buffer and base64 encode it.\u00a0 \u00a0 \u00a0 ByteString content = ByteString.copyFrom(imageFileData);\u00a0 \u00a0 \u00a0 RawDocument document =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 RawDocument.newBuilder().setContent(content).setMimeType(\"application/pdf\").build();\u00a0 \u00a0 \u00a0 // Configure the process request.\u00a0 \u00a0 \u00a0 ProcessRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ProcessRequest.newBuilder().setName(name).setRawDocument(document).build();\u00a0 \u00a0 \u00a0 // Recognizes text entities in the PDF document\u00a0 \u00a0 \u00a0 ProcessResponse result = client.processDocument(request);\u00a0 \u00a0 \u00a0 Document documentResponse = result.getDocument();\u00a0 \u00a0 \u00a0 System.out.println(\"Document processing complete.\");\u00a0 \u00a0 \u00a0 // Read fields specificly from the specalized US drivers license processor:\u00a0 \u00a0 \u00a0 // https://cloud.google.com/document-ai/docs/processors-list#processor_us-driver-license-parser\u00a0 \u00a0 \u00a0 // retriving data from other specalized processors follow a similar pattern.\u00a0 \u00a0 \u00a0 // For a complete list of processors see:\u00a0 \u00a0 \u00a0 // https://cloud.google.com/document-ai/docs/processors-list\u00a0 \u00a0 \u00a0 //\u00a0 \u00a0 \u00a0 // OCR and other data is also present in the quality processor's response.\u00a0 \u00a0 \u00a0 // Please see the OCR and other samples for how to parse other data in the\u00a0 \u00a0 \u00a0 // response.\u00a0 \u00a0 \u00a0 for (Document.Entity entity : documentResponse.getEntitiesList()) {\u00a0 \u00a0 \u00a0 \u00a0 // Fields detected. For a full list of fields for each processor see\u00a0 \u00a0 \u00a0 \u00a0 // the processor documentation:\u00a0 \u00a0 \u00a0 \u00a0 // https://cloud.google.com/document-ai/docs/processors-list\u00a0 \u00a0 \u00a0 \u00a0 String entityType = entity.getType();\u00a0 \u00a0 \u00a0 \u00a0 // some other value formats in addition to text are availible\u00a0 \u00a0 \u00a0 \u00a0 // e.g. dates: `entity.getNormalizedValue().getDateValue().getYear()`\u00a0 \u00a0 \u00a0 \u00a0 // check for normilized value with `entity.hasNormalizedValue()`\u00a0 \u00a0 \u00a0 \u00a0 String entityTextValue = escapeNewlines(entity.getTextAnchor().getContent());\u00a0 \u00a0 \u00a0 \u00a0 float entityConfidence = entity.getConfidence();\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \" \u00a0 \u00a0* %s: %s (%.2f%% confident)\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 entityType, entityTextValue, entityConfidence * 100.0);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\u00a0 private static String escapeNewlines(String s) {\u00a0 \u00a0 return s.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\\\\r\");\u00a0 }}\n```For more information, see the [Document AI Node.js API reference documentation](/nodejs/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/document-ai/process-document-specialized.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*/// const projectId = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION'; // Format is 'us' or 'eu'// const processorId = 'YOUR_PROCESSOR_ID'; // Create processor in Cloud Console// const filePath = '/path/to/local/pdf';const {DocumentProcessorServiceClient} =\u00a0 require('@google-cloud/documentai').v1beta3;// Instantiates a clientconst client = new DocumentProcessorServiceClient();async function processDocument() {\u00a0 // The full resource name of the processor, e.g.:\u00a0 // projects/project-id/locations/location/processor/processor-id\u00a0 // You must create new processors in the Cloud Console first\u00a0 const name = `projects/${projectId}/locations/${location}/processors/${processorId}`;\u00a0 // Read the file into memory.\u00a0 const fs = require('fs').promises;\u00a0 const imageFile = await fs.readFile(filePath);\u00a0 // Convert the image data to a Buffer and base64 encode it.\u00a0 const encodedImage = Buffer.from(imageFile).toString('base64');\u00a0 const request = {\u00a0 \u00a0 name,\u00a0 \u00a0 rawDocument: {\u00a0 \u00a0 \u00a0 content: encodedImage,\u00a0 \u00a0 \u00a0 mimeType: 'application/pdf',\u00a0 \u00a0 },\u00a0 };\u00a0 // Recognizes text entities in the PDF document\u00a0 const [result] = await client.processDocument(request);\u00a0 console.log('Document processing complete.');\u00a0 // Read fields specificly from the specalized US drivers license processor:\u00a0 // https://cloud.google.com/document-ai/docs/processors-list#processor_us-driver-license-parser\u00a0 // retriving data from other specalized processors follow a similar pattern.\u00a0 // For a complete list of processors see:\u00a0 // https://cloud.google.com/document-ai/docs/processors-list\u00a0 //\u00a0 // OCR and other data is also present in the quality processor's response.\u00a0 // Please see the OCR and other samples for how to parse other data in the\u00a0 // response.\u00a0 const {document} = result;\u00a0 for (const entity of document.entities) {\u00a0 \u00a0 // Fields detected. For a full list of fields for each processor see\u00a0 \u00a0 // the processor documentation:\u00a0 \u00a0 // https://cloud.google.com/document-ai/docs/processors-list\u00a0 \u00a0 const key = entity.type;\u00a0 \u00a0 // some other value formats in addition to text are availible\u00a0 \u00a0 // e.g. dates: `entity.normalizedValue.dateValue.year`\u00a0 \u00a0 const textValue =\u00a0 \u00a0 \u00a0 entity.textAnchor !== null ? entity.textAnchor.content : '';\u00a0 \u00a0 const conf = entity.confidence * 100;\u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 `* ${JSON.stringify(key)}: ${JSON.stringify(textValue)}(${conf.toFixed(\u00a0 \u00a0 \u00a0 \u00a0 2\u00a0 \u00a0 \u00a0 )}% confident)`\u00a0 \u00a0 );\u00a0 }}\n```For more information, see the [Document AI Python API reference documentation](/python/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/documentai/snippets/handle_response_sample.py) \n```\nfrom typing import Optional, Sequencefrom google.api_core.client_options import ClientOptionsfrom google.cloud import documentai# TODO(developer): Uncomment these variables before running the sample.# project_id = \"YOUR_PROJECT_ID\"# location = \"YOUR_PROCESSOR_LOCATION\" # Format is \"us\" or \"eu\"# processor_id = \"YOUR_PROCESSOR_ID\" # Create processor before running sample# processor_version = \"rc\" # Refer to https://cloud.google.com/document-ai/docs/manage-processor-versions for more information# file_path = \"/path/to/local/pdf\"# mime_type = \"application/pdf\" # Refer to https://cloud.google.com/document-ai/docs/file-types for supported file typesdef process_document_entity_extraction_sample(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version: str,\u00a0 \u00a0 file_path: str,\u00a0 \u00a0 mime_type: str,) -> None:\u00a0 \u00a0 # Online processing request to Document AI\u00a0 \u00a0 document = process_document(\u00a0 \u00a0 \u00a0 \u00a0 project_id, location, processor_id, processor_version, file_path, mime_type\u00a0 \u00a0 )\u00a0 \u00a0 # Print extracted entities from entity extraction processor output.\u00a0 \u00a0 # For a complete list of processors see:\u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/processors-list\u00a0 \u00a0 #\u00a0 \u00a0 # OCR and other data is also present in the processor's response.\u00a0 \u00a0 # Refer to the OCR samples for how to parse other data in the response.\u00a0 \u00a0 print(f\"Found {len(document.entities)} entities:\")\u00a0 \u00a0 for entity in document.entities:\u00a0 \u00a0 \u00a0 \u00a0 print_entity(entity)\u00a0 \u00a0 \u00a0 \u00a0 # Print Nested Entities (if any)\u00a0 \u00a0 \u00a0 \u00a0 for prop in entity.properties:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print_entity(prop)def print_entity(entity: documentai.Document.Entity) -> None:\u00a0 \u00a0 # Fields detected. For a full list of fields for each processor see\u00a0 \u00a0 # the processor documentation:\u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/processors-list\u00a0 \u00a0 key = entity.type_\u00a0 \u00a0 # Some other value formats in addition to text are availible\u00a0 \u00a0 # e.g. dates: `entity.normalized_value.date_value.year`\u00a0 \u00a0 text_value = entity.text_anchor.content\u00a0 \u00a0 confidence = entity.confidence\u00a0 \u00a0 normalized_value = entity.normalized_value.text\u00a0 \u00a0 print(f\" \u00a0 \u00a0* {repr(key)}: {repr(text_value)}({confidence:.1%} confident)\")\u00a0 \u00a0 if normalized_value:\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0* Normalized Value: {repr(normalized_value)}\")def process_document(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version: str,\u00a0 \u00a0 file_path: str,\u00a0 \u00a0 mime_type: str,\u00a0 \u00a0 process_options: Optional[documentai.ProcessOptions] = None,) -> documentai.Document:\u00a0 \u00a0 # You must set the `api_endpoint` if you use a location other than \"us\".\u00a0 \u00a0 client = documentai.DocumentProcessorServiceClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options=ClientOptions(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 api_endpoint=f\"{location}-documentai.googleapis.com\"\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 # The full resource name of the processor version, e.g.:\u00a0 \u00a0 # `projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}`\u00a0 \u00a0 # You must create a processor before running this sample.\u00a0 \u00a0 name = client.processor_version_path(\u00a0 \u00a0 \u00a0 \u00a0 project_id, location, processor_id, processor_version\u00a0 \u00a0 )\u00a0 \u00a0 # Read the file into memory\u00a0 \u00a0 with open(file_path, \"rb\") as image:\u00a0 \u00a0 \u00a0 \u00a0 image_content = image.read()\u00a0 \u00a0 # Configure the process request\u00a0 \u00a0 request = documentai.ProcessRequest(\u00a0 \u00a0 \u00a0 \u00a0 name=name,\u00a0 \u00a0 \u00a0 \u00a0 raw_document=documentai.RawDocument(content=image_content, mime_type=mime_type),\u00a0 \u00a0 \u00a0 \u00a0 # Only supported for Document OCR processor\u00a0 \u00a0 \u00a0 \u00a0 process_options=process_options,\u00a0 \u00a0 )\u00a0 \u00a0 result = client.process_document(request=request)\u00a0 \u00a0 # For a full list of `Document` object attributes, reference this page:\u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/reference/rest/v1/Document\u00a0 \u00a0 return result.document\n```\n## Custom Document Extractor\nThe [Custom Document Extractor processor](/document-ai/docs/processors-list#processor_cde) can extract custom entities from documents which don't have a pretrained processor available. This can be accomplished through training a custom model or by using Generative AI Foundation Models to extract named entities without any training.\nFor more information, refer to [Create a Custom Document Extractor in the Google Cloud console](/document-ai/docs/workbench/build-custom-processor) .\nIf you train a custom model, the processor can be used in exactly the same way as a pretrained entity extraction processor.\nIf you use a foundation model, you can either create a [processor version](/document-ai/docs/manage-processor-versions) to extract specific entities for every request, or you can configure it on a per-request basis.\nRefer to [Entities, nested entities, and normalized values](#entities) for information about the output structure.\n### Code samples\nIf you are using a custom model or created a processor version using a foundation model, then use the [entity extraction code samples](#entity-extraction-code-samples) .\nThe following code sample demonstrates how to configure specific entities for a foundation model Custom Document Extractor on a per-request basis and print the extracted entities:\nFor more information, see the [Document AI Python API reference documentation](/python/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/documentai/snippets/handle_response_sample_v1beta3.py) \n```\nfrom typing import Optionalfrom google.api_core.client_options import ClientOptionsfrom google.cloud import documentai_v1beta3 as documentai# TODO(developer): Uncomment these variables before running the sample.# project_id = \"YOUR_PROJECT_ID\"# location = \"YOUR_PROCESSOR_LOCATION\" # Format is \"us\" or \"eu\"# processor_id = \"YOUR_PROCESSOR_ID\" # Create processor before running sample# processor_version = \"rc\" # Refer to https://cloud.google.com/document-ai/docs/manage-processor-versions for more information# file_path = \"/path/to/local/pdf\"# mime_type = \"application/pdf\" # Refer to https://cloud.google.com/document-ai/docs/file-types for supported file typesdef process_document_custom_extractor_sample(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version: str,\u00a0 \u00a0 file_path: str,\u00a0 \u00a0 mime_type: str,) -> None:\u00a0 \u00a0 # Entities to extract from Foundation Model CDE\u00a0 \u00a0 properties = [\u00a0 \u00a0 \u00a0 \u00a0 documentai.DocumentSchema.EntityType.Property(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name=\"invoice_id\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value_type=\"string\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 occurrence_type=documentai.DocumentSchema.EntityType.Property.OccurrenceType.REQUIRED_ONCE,\u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 documentai.DocumentSchema.EntityType.Property(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name=\"notes\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value_type=\"string\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 occurrence_type=documentai.DocumentSchema.EntityType.Property.OccurrenceType.REQUIRED_ONCE,\u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 documentai.DocumentSchema.EntityType.Property(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name=\"terms\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value_type=\"string\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 occurrence_type=documentai.DocumentSchema.EntityType.Property.OccurrenceType.REQUIRED_ONCE,\u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 ]\u00a0 \u00a0 # Optional: For Generative AI processors, request different fields than the\u00a0 \u00a0 # schema for a processor version\u00a0 \u00a0 process_options = documentai.ProcessOptions(\u00a0 \u00a0 \u00a0 \u00a0 schema_override=documentai.DocumentSchema(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 display_name=\"CDE Schema\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 description=\"Document Schema for the CDE Processor\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 entity_types=[\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 documentai.DocumentSchema.EntityType(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name=\"custom_extraction_document_type\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 base_types=[\"document\"],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 properties=properties,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 # Online processing request to Document AI\u00a0 \u00a0 document = process_document(\u00a0 \u00a0 \u00a0 \u00a0 project_id,\u00a0 \u00a0 \u00a0 \u00a0 location,\u00a0 \u00a0 \u00a0 \u00a0 processor_id,\u00a0 \u00a0 \u00a0 \u00a0 processor_version,\u00a0 \u00a0 \u00a0 \u00a0 file_path,\u00a0 \u00a0 \u00a0 \u00a0 mime_type,\u00a0 \u00a0 \u00a0 \u00a0 process_options=process_options,\u00a0 \u00a0 )\u00a0 \u00a0 for entity in document.entities:\u00a0 \u00a0 \u00a0 \u00a0 print_entity(entity)\u00a0 \u00a0 \u00a0 \u00a0 # Print Nested Entities (if any)\u00a0 \u00a0 \u00a0 \u00a0 for prop in entity.properties:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print_entity(prop)def print_entity(entity: documentai.Document.Entity) -> None:\u00a0 \u00a0 # Fields detected. For a full list of fields for each processor see\u00a0 \u00a0 # the processor documentation:\u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/processors-list\u00a0 \u00a0 key = entity.type_\u00a0 \u00a0 # Some other value formats in addition to text are availible\u00a0 \u00a0 # e.g. dates: `entity.normalized_value.date_value.year`\u00a0 \u00a0 text_value = entity.text_anchor.content\u00a0 \u00a0 confidence = entity.confidence\u00a0 \u00a0 normalized_value = entity.normalized_value.text\u00a0 \u00a0 print(f\" \u00a0 \u00a0* {repr(key)}: {repr(text_value)}({confidence:.1%} confident)\")\u00a0 \u00a0 if normalized_value:\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0* Normalized Value: {repr(normalized_value)}\")def process_document(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version: str,\u00a0 \u00a0 file_path: str,\u00a0 \u00a0 mime_type: str,\u00a0 \u00a0 process_options: Optional[documentai.ProcessOptions] = None,) -> documentai.Document:\u00a0 \u00a0 # You must set the `api_endpoint` if you use a location other than \"us\".\u00a0 \u00a0 client = documentai.DocumentProcessorServiceClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options=ClientOptions(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 api_endpoint=f\"{location}-documentai.googleapis.com\"\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 # The full resource name of the processor version, e.g.:\u00a0 \u00a0 # `projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}`\u00a0 \u00a0 # You must create a processor before running this sample.\u00a0 \u00a0 name = client.processor_version_path(\u00a0 \u00a0 \u00a0 \u00a0 project_id, location, processor_id, processor_version\u00a0 \u00a0 )\u00a0 \u00a0 # Read the file into memory\u00a0 \u00a0 with open(file_path, \"rb\") as image:\u00a0 \u00a0 \u00a0 \u00a0 image_content = image.read()\u00a0 \u00a0 # Configure the process request\u00a0 \u00a0 request = documentai.ProcessRequest(\u00a0 \u00a0 \u00a0 \u00a0 name=name,\u00a0 \u00a0 \u00a0 \u00a0 raw_document=documentai.RawDocument(content=image_content, mime_type=mime_type),\u00a0 \u00a0 \u00a0 \u00a0 # Only supported for Document OCR processor\u00a0 \u00a0 \u00a0 \u00a0 process_options=process_options,\u00a0 \u00a0 )\u00a0 \u00a0 result = client.process_document(request=request)\u00a0 \u00a0 # For a full list of `Document` object attributes, reference this page:\u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/reference/rest/v1/Document\u00a0 \u00a0 return result.document\n```\n## Summarization\nThe [Summarizer processor](/document-ai/docs/processors-list#processor_SUMMARIZER) uses Generative AI Foundation Models to summarize the extracted text from a document. The length and format of the response can be customized in the following ways:\n- Length- [BRIEF](/document-ai/docs/reference/rest/v1beta3/projects.locations.processors.processorVersions#Length.ENUM_VALUES.BRIEF) : A brief summary of one or two sentences.\n- [MODERATE](/document-ai/docs/reference/rest/v1beta3/projects.locations.processors.processorVersions#Length.ENUM_VALUES.MODERATE) : A paragraph-length summary.\n- [COMPREHENSIVE](/document-ai/docs/reference/rest/v1beta3/projects.locations.processors.processorVersions#Length.ENUM_VALUES.COMPREHENSIVE) : The longest option available.\n- Format- [PARAGRAPH](/document-ai/docs/reference/rest/v1beta3/projects.locations.processors.processorVersions#Format.ENUM_VALUES.PARAGRAPH) : Format the output in paragraphs.\n- [BULLETS](/document-ai/docs/reference/rest/v1beta3/projects.locations.processors.processorVersions#Format.ENUM_VALUES.BULLETS) : Format the output in bullet points.You can either create a [processor version](/document-ai/docs/manage-processor-versions) for a specific length and format, or you can configure it on a per-request basis.\nThe summarized text appears in [Document.entities.normalizedValue.text](/document-ai/docs/reference/rest/v1/Document#NormalizedValue.FIELDS.text) . You can find a full sample output JSON file in [Sample processor output](/document-ai/docs/output#processor_SUMMARIZER) .\nFor more information, refer to [Build a document summarizer in the Google Cloud console](/document-ai/docs/workbench/build-summarizer-processor) .\n### Code samples\nThe following code sample demonstrates how to configure a specific length and format in a processing request and print the summarized text:\nFor more information, see the [Document AI Python API reference documentation](/python/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/documentai/snippets/handle_response_sample_v1beta3.py) \n```\nfrom typing import Optionalfrom google.api_core.client_options import ClientOptionsfrom google.cloud import documentai_v1beta3 as documentai# TODO(developer): Uncomment these variables before running the sample.# project_id = \"YOUR_PROJECT_ID\"# location = \"YOUR_PROCESSOR_LOCATION\" # Format is \"us\" or \"eu\"# processor_id = \"YOUR_PROCESSOR_ID\" # Create processor before running sample# processor_version = \"rc\" # Refer to https://cloud.google.com/document-ai/docs/manage-processor-versions for more information# file_path = \"/path/to/local/pdf\"# mime_type = \"application/pdf\" # Refer to https://cloud.google.com/document-ai/docs/file-types for supported file typesdef process_document_summarizer_sample(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version: str,\u00a0 \u00a0 file_path: str,\u00a0 \u00a0 mime_type: str,) -> None:\u00a0 \u00a0 # For supported options, refer to:\u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/reference/rest/v1beta3/projects.locations.processors.processorVersions#summaryoptions\u00a0 \u00a0 summary_options = documentai.SummaryOptions(\u00a0 \u00a0 \u00a0 \u00a0 length=documentai.SummaryOptions.Length.BRIEF,\u00a0 \u00a0 \u00a0 \u00a0 format=documentai.SummaryOptions.Format.BULLETS,\u00a0 \u00a0 )\u00a0 \u00a0 properties = [\u00a0 \u00a0 \u00a0 \u00a0 documentai.DocumentSchema.EntityType.Property(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name=\"summary\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value_type=\"string\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 occurrence_type=documentai.DocumentSchema.EntityType.Property.OccurrenceType.REQUIRED_ONCE,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 property_metadata=documentai.PropertyMetadata(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 field_extraction_metadata=documentai.FieldExtractionMetadata(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 summary_options=summary_options\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 ]\u00a0 \u00a0 # Optional: Request specific summarization format other than the default\u00a0 \u00a0 # for the processor version.\u00a0 \u00a0 process_options = documentai.ProcessOptions(\u00a0 \u00a0 \u00a0 \u00a0 schema_override=documentai.DocumentSchema(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 entity_types=[\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 documentai.DocumentSchema.EntityType(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name=\"summary_document_type\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 base_types=[\"document\"],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 properties=properties,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 # Online processing request to Document AI\u00a0 \u00a0 document = process_document(\u00a0 \u00a0 \u00a0 \u00a0 project_id,\u00a0 \u00a0 \u00a0 \u00a0 location,\u00a0 \u00a0 \u00a0 \u00a0 processor_id,\u00a0 \u00a0 \u00a0 \u00a0 processor_version,\u00a0 \u00a0 \u00a0 \u00a0 file_path,\u00a0 \u00a0 \u00a0 \u00a0 mime_type,\u00a0 \u00a0 \u00a0 \u00a0 process_options=process_options,\u00a0 \u00a0 )\u00a0 \u00a0 for entity in document.entities:\u00a0 \u00a0 \u00a0 \u00a0 print_entity(entity)\u00a0 \u00a0 \u00a0 \u00a0 # Print Nested Entities (if any)\u00a0 \u00a0 \u00a0 \u00a0 for prop in entity.properties:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print_entity(prop)def print_entity(entity: documentai.Document.Entity) -> None:\u00a0 \u00a0 # Fields detected. For a full list of fields for each processor see\u00a0 \u00a0 # the processor documentation:\u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/processors-list\u00a0 \u00a0 key = entity.type_\u00a0 \u00a0 # Some other value formats in addition to text are availible\u00a0 \u00a0 # e.g. dates: `entity.normalized_value.date_value.year`\u00a0 \u00a0 text_value = entity.text_anchor.content\u00a0 \u00a0 confidence = entity.confidence\u00a0 \u00a0 normalized_value = entity.normalized_value.text\u00a0 \u00a0 print(f\" \u00a0 \u00a0* {repr(key)}: {repr(text_value)}({confidence:.1%} confident)\")\u00a0 \u00a0 if normalized_value:\u00a0 \u00a0 \u00a0 \u00a0 print(f\" \u00a0 \u00a0* Normalized Value: {repr(normalized_value)}\")def process_document(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version: str,\u00a0 \u00a0 file_path: str,\u00a0 \u00a0 mime_type: str,\u00a0 \u00a0 process_options: Optional[documentai.ProcessOptions] = None,) -> documentai.Document:\u00a0 \u00a0 # You must set the `api_endpoint` if you use a location other than \"us\".\u00a0 \u00a0 client = documentai.DocumentProcessorServiceClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options=ClientOptions(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 api_endpoint=f\"{location}-documentai.googleapis.com\"\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 # The full resource name of the processor version, e.g.:\u00a0 \u00a0 # `projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}`\u00a0 \u00a0 # You must create a processor before running this sample.\u00a0 \u00a0 name = client.processor_version_path(\u00a0 \u00a0 \u00a0 \u00a0 project_id, location, processor_id, processor_version\u00a0 \u00a0 )\u00a0 \u00a0 # Read the file into memory\u00a0 \u00a0 with open(file_path, \"rb\") as image:\u00a0 \u00a0 \u00a0 \u00a0 image_content = image.read()\u00a0 \u00a0 # Configure the process request\u00a0 \u00a0 request = documentai.ProcessRequest(\u00a0 \u00a0 \u00a0 \u00a0 name=name,\u00a0 \u00a0 \u00a0 \u00a0 raw_document=documentai.RawDocument(content=image_content, mime_type=mime_type),\u00a0 \u00a0 \u00a0 \u00a0 # Only supported for Document OCR processor\u00a0 \u00a0 \u00a0 \u00a0 process_options=process_options,\u00a0 \u00a0 )\u00a0 \u00a0 result = client.process_document(request=request)\u00a0 \u00a0 # For a full list of `Document` object attributes, reference this page:\u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/reference/rest/v1/Document\u00a0 \u00a0 return result.document\n```\n## Splitting and classification\nHere's a 10-page PDF that contains different types of documents and forms:\n[Download PDF](/static/document-ai/docs/images/splitter-ldai.pdf)\nHere's the full document object as returned by the [Lending Document Splitter & Classifier](/document-ai/docs/processors-list#processor_lending-splitter-classifier) :\n[Download JSON](/static/document-ai/docs/images/splitter-ldai.json)\nEach document that is detected by the splitter is represented by an [entity](/document-ai/docs/reference/rest/v1/Document#Entity) . For example:\n```\n\u00a0 {\u00a0 \u00a0 \"entities\": [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \"textAnchor\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"textSegments\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"startIndex\": \"13936\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"endIndex\": \"21108\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"1040se_2020\",\u00a0 \u00a0 \u00a0 \u00a0 \"confidence\": 0.76257163,\u00a0 \u00a0 \u00a0 \u00a0 \"pageAnchor\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"pageRefs\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"page\": \"6\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"page\": \"7\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 ]\u00a0 }\n```\n- `Entity.pageAnchor` indicates that this document is 2 pages long. Note that `pageRefs[].page` is zero-based and is the index into the `document.pages[]` field. **Caution: Zero pageRefs[].page values omitted.** When the API detects a page value of`\"0\"`, **that coordinate is omitted in the\n JSON response** . For example, a response for the first two pages of a document would be:`\"pageRefs\": [{}, {\"page\": \"1\"}]`\n- `Entity.type` specifies that this document is a 1040 Schedule SE form. For a full list of document types that can be identified, see in the [processor documentation](/document-ai/docs/processors-list) .\nFor more information, see [Document splitters behavior](/document-ai/docs/splitters) .\n### Code samples\nSplitters identify page boundaries, but don't actually split the input document for you. You can use [Document AI Toolbox](/document-ai/docs/toolbox) to physically split a PDF file by using the page boundaries. The following code samples print the page ranges without splitting the PDF:\nFor more information, see the [Document AI Java API reference documentation](/java/docs/reference/google-cloud-document-ai/latest/overview) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/document-ai/src/main/java/documentai/v1beta3/ProcessSplitterDocument.java) \n```\nimport com.google.cloud.documentai.v1beta3.Document;import com.google.cloud.documentai.v1beta3.DocumentProcessorServiceClient;import com.google.cloud.documentai.v1beta3.DocumentProcessorServiceSettings;import com.google.cloud.documentai.v1beta3.ProcessRequest;import com.google.cloud.documentai.v1beta3.ProcessResponse;import com.google.cloud.documentai.v1beta3.RawDocument;import com.google.protobuf.ByteString;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Paths;import java.util.List;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeoutException;public class ProcessSplitterDocument {\u00a0 public static void processSplitterDocument()\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String projectId = \"your-project-id\";\u00a0 \u00a0 String location = \"your-project-location\"; // Format is \"us\" or \"eu\".\u00a0 \u00a0 String processerId = \"your-processor-id\";\u00a0 \u00a0 String filePath = \"path/to/input/file.pdf\";\u00a0 \u00a0 processSplitterDocument(projectId, location, processerId, filePath);\u00a0 }\u00a0 public static void processSplitterDocument(\u00a0 \u00a0 \u00a0 String projectId, String location, String processorId, String filePath)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs\u00a0 \u00a0 // to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your\u00a0 \u00a0 // requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background\u00a0 \u00a0 // resources.\u00a0 \u00a0 String endpoint = String.format(\"%s-documentai.googleapis.com:443\", location);\u00a0 \u00a0 DocumentProcessorServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 DocumentProcessorServiceSettings.newBuilder().setEndpoint(endpoint).build();\u00a0 \u00a0 try (DocumentProcessorServiceClient client = DocumentProcessorServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 // The full resource name of the processor, e.g.:\u00a0 \u00a0 \u00a0 // projects/project-id/locations/location/processor/processor-id\u00a0 \u00a0 \u00a0 // You must create new processors in the Cloud Console first\u00a0 \u00a0 \u00a0 String name =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\"projects/%s/locations/%s/processors/%s\", projectId, location, processorId);\u00a0 \u00a0 \u00a0 // Read the file.\u00a0 \u00a0 \u00a0 byte[] imageFileData = Files.readAllBytes(Paths.get(filePath));\u00a0 \u00a0 \u00a0 // Convert the image data to a Buffer and base64 encode it.\u00a0 \u00a0 \u00a0 ByteString content = ByteString.copyFrom(imageFileData);\u00a0 \u00a0 \u00a0 RawDocument document =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 RawDocument.newBuilder().setContent(content).setMimeType(\"application/pdf\").build();\u00a0 \u00a0 \u00a0 // Configure the process request.\u00a0 \u00a0 \u00a0 ProcessRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ProcessRequest.newBuilder().setName(name).setRawDocument(document).build();\u00a0 \u00a0 \u00a0 // Recognizes text entities in the PDF document\u00a0 \u00a0 \u00a0 ProcessResponse result = client.processDocument(request);\u00a0 \u00a0 \u00a0 Document documentResponse = result.getDocument();\u00a0 \u00a0 \u00a0 System.out.println(\"Document processing complete.\");\u00a0 \u00a0 \u00a0 // Read the splitter output from the document splitter processor:\u00a0 \u00a0 \u00a0 // https://cloud.google.com/document-ai/docs/processors-list#processor_doc-splitter\u00a0 \u00a0 \u00a0 // This processor only provides text for the document and information on how\u00a0 \u00a0 \u00a0 // to split the document on logical boundaries. To identify and extract text,\u00a0 \u00a0 \u00a0 // form elements, and entities please see other processors like the OCR, form,\u00a0 \u00a0 \u00a0 // and specalized processors.\u00a0 \u00a0 \u00a0 List<Document.Entity> entities = documentResponse.getEntitiesList();\u00a0 \u00a0 \u00a0 System.out.printf(\"Found %d subdocuments:\\n\", entities.size());\u00a0 \u00a0 \u00a0 for (Document.Entity entity : entities) {\u00a0 \u00a0 \u00a0 \u00a0 float entityConfidence = entity.getConfidence();\u00a0 \u00a0 \u00a0 \u00a0 String pagesRangeText = pageRefsToString(entity.getPageAnchor().getPageRefsList());\u00a0 \u00a0 \u00a0 \u00a0 String subdocumentType = entity.getType();\u00a0 \u00a0 \u00a0 \u00a0 if (subdocumentType.isEmpty()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"%.2f%% confident that %s a subdocument.\\n\", entityConfidence * 100, pagesRangeText);\u00a0 \u00a0 \u00a0 \u00a0 } else {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"%.2f%% confident that %s a '%s' subdocument.\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 entityConfidence * 100, pagesRangeText, subdocumentType);\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\u00a0 // Converts page reference(s) to a string describing the page or page range.\u00a0 private static String pageRefsToString(List<Document.PageAnchor.PageRef> pageRefs) {\u00a0 \u00a0 if (pageRefs.size() == 1) {\u00a0 \u00a0 \u00a0 return String.format(\"page %d is\", pageRefs.get(0).getPage() + 1);\u00a0 \u00a0 } else {\u00a0 \u00a0 \u00a0 long start = pageRefs.get(0).getPage() + 1;\u00a0 \u00a0 \u00a0 long end = pageRefs.get(1).getPage() + 1;\u00a0 \u00a0 \u00a0 return String.format(\"pages %d to %d are\", start, end);\u00a0 \u00a0 }\u00a0 }}\n```For more information, see the [Document AI Node.js API reference documentation](/nodejs/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/document-ai/process-document-splitter.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*/// const projectId = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION'; // Format is 'us' or 'eu'// const processorId = 'YOUR_PROCESSOR_ID'; // Create processor in Cloud Console// const filePath = '/path/to/local/pdf';const {DocumentProcessorServiceClient} =\u00a0 require('@google-cloud/documentai').v1beta3;// Instantiates a clientconst client = new DocumentProcessorServiceClient();async function processDocument() {\u00a0 // The full resource name of the processor, e.g.:\u00a0 // projects/project-id/locations/location/processor/processor-id\u00a0 // You must create new processors in the Cloud Console first\u00a0 const name = `projects/${projectId}/locations/${location}/processors/${processorId}`;\u00a0 // Read the file into memory.\u00a0 const fs = require('fs').promises;\u00a0 const imageFile = await fs.readFile(filePath);\u00a0 // Convert the image data to a Buffer and base64 encode it.\u00a0 const encodedImage = Buffer.from(imageFile).toString('base64');\u00a0 const request = {\u00a0 \u00a0 name,\u00a0 \u00a0 rawDocument: {\u00a0 \u00a0 \u00a0 content: encodedImage,\u00a0 \u00a0 \u00a0 mimeType: 'application/pdf',\u00a0 \u00a0 },\u00a0 };\u00a0 // Recognizes text entities in the PDF document\u00a0 const [result] = await client.processDocument(request);\u00a0 console.log('Document processing complete.');\u00a0 // Read fields specificly from the specalized US drivers license processor:\u00a0 // https://cloud.google.com/document-ai/docs/processors-list#processor_us-driver-license-parser\u00a0 // retriving data from other specalized processors follow a similar pattern.\u00a0 // For a complete list of processors see:\u00a0 // https://cloud.google.com/document-ai/docs/processors-list\u00a0 //\u00a0 // OCR and other data is also present in the quality processor's response.\u00a0 // Please see the OCR and other samples for how to parse other data in the\u00a0 // response.\u00a0 const {document} = result;\u00a0 console.log(`Found ${document.entities.length} subdocuments:`);\u00a0 for (const entity of document.entities) {\u00a0 \u00a0 const conf = entity.confidence * 100;\u00a0 \u00a0 const pagesRange = pageRefsToRange(entity.pageAnchor.pageRefs);\u00a0 \u00a0 if (entity.type !== '') {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `${conf.toFixed(2)}% confident that ${pagesRange} a \"${\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 entity.type\u00a0 \u00a0 \u00a0 \u00a0 }\" subdocument.`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 } else {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `${conf.toFixed(2)}% confident that ${pagesRange} a subdocument.`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 }\u00a0 }}// Converts a page ref to a string describing the page or page range.const pageRefsToRange = pageRefs => {\u00a0 if (pageRefs.length === 1) {\u00a0 \u00a0 const num = parseInt(pageRefs[0].page) + 1 || 1;\u00a0 \u00a0 return `page ${num} is`;\u00a0 } else {\u00a0 \u00a0 const start = parseInt(pageRefs[0].page) + 1 || 1;\u00a0 \u00a0 const end = parseInt(pageRefs[1].page) + 1;\u00a0 \u00a0 return `pages ${start} to ${end} are`;\u00a0 }};\n```For more information, see the [Document AI Python API reference documentation](/python/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/documentai/snippets/handle_response_sample.py) \n```\nfrom typing import Optional, Sequencefrom google.api_core.client_options import ClientOptionsfrom google.cloud import documentai# TODO(developer): Uncomment these variables before running the sample.# project_id = \"YOUR_PROJECT_ID\"# location = \"YOUR_PROCESSOR_LOCATION\" # Format is \"us\" or \"eu\"# processor_id = \"YOUR_PROCESSOR_ID\" # Create processor before running sample# processor_version = \"rc\" # Refer to https://cloud.google.com/document-ai/docs/manage-processor-versions for more information# file_path = \"/path/to/local/pdf\"# mime_type = \"application/pdf\" # Refer to https://cloud.google.com/document-ai/docs/file-types for supported file typesdef process_document_splitter_sample(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version: str,\u00a0 \u00a0 file_path: str,\u00a0 \u00a0 mime_type: str,) -> None:\u00a0 \u00a0 # Online processing request to Document AI\u00a0 \u00a0 document = process_document(\u00a0 \u00a0 \u00a0 \u00a0 project_id, location, processor_id, processor_version, file_path, mime_type\u00a0 \u00a0 )\u00a0 \u00a0 # Read the splitter output from a document splitter/classifier processor:\u00a0 \u00a0 # e.g. https://cloud.google.com/document-ai/docs/processors-list#processor_procurement-document-splitter\u00a0 \u00a0 # This processor only provides text for the document and information on how\u00a0 \u00a0 # to split the document on logical boundaries. To identify and extract text,\u00a0 \u00a0 # form elements, and entities please see other processors like the OCR, form,\u00a0 \u00a0 # and specalized processors.\u00a0 \u00a0 print(f\"Found {len(document.entities)} subdocuments:\")\u00a0 \u00a0 for entity in document.entities:\u00a0 \u00a0 \u00a0 \u00a0 conf_percent = f\"{entity.confidence:.1%}\"\u00a0 \u00a0 \u00a0 \u00a0 pages_range = page_refs_to_string(entity.page_anchor.page_refs)\u00a0 \u00a0 \u00a0 \u00a0 # Print subdocument type information, if available\u00a0 \u00a0 \u00a0 \u00a0 if entity.type_:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 f\"{conf_percent} confident that {pages_range} a '{entity.type_}' subdocument.\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(f\"{conf_percent} confident that {pages_range} a subdocument.\")def page_refs_to_string(\u00a0 \u00a0 page_refs: Sequence[documentai.Document.PageAnchor.PageRef],) -> str:\u00a0 \u00a0 \"\"\"Converts a page ref to a string describing the page or page range.\"\"\"\u00a0 \u00a0 pages = [str(int(page_ref.page) + 1) for page_ref in page_refs]\u00a0 \u00a0 if len(pages) == 1:\u00a0 \u00a0 \u00a0 \u00a0 return f\"page {pages[0]} is\"\u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 \u00a0 return f\"pages {', '.join(pages)} are\"def process_document(\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 processor_version: str,\u00a0 \u00a0 file_path: str,\u00a0 \u00a0 mime_type: str,\u00a0 \u00a0 process_options: Optional[documentai.ProcessOptions] = None,) -> documentai.Document:\u00a0 \u00a0 # You must set the `api_endpoint` if you use a location other than \"us\".\u00a0 \u00a0 client = documentai.DocumentProcessorServiceClient(\u00a0 \u00a0 \u00a0 \u00a0 client_options=ClientOptions(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 api_endpoint=f\"{location}-documentai.googleapis.com\"\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 # The full resource name of the processor version, e.g.:\u00a0 \u00a0 # `projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}`\u00a0 \u00a0 # You must create a processor before running this sample.\u00a0 \u00a0 name = client.processor_version_path(\u00a0 \u00a0 \u00a0 \u00a0 project_id, location, processor_id, processor_version\u00a0 \u00a0 )\u00a0 \u00a0 # Read the file into memory\u00a0 \u00a0 with open(file_path, \"rb\") as image:\u00a0 \u00a0 \u00a0 \u00a0 image_content = image.read()\u00a0 \u00a0 # Configure the process request\u00a0 \u00a0 request = documentai.ProcessRequest(\u00a0 \u00a0 \u00a0 \u00a0 name=name,\u00a0 \u00a0 \u00a0 \u00a0 raw_document=documentai.RawDocument(content=image_content, mime_type=mime_type),\u00a0 \u00a0 \u00a0 \u00a0 # Only supported for Document OCR processor\u00a0 \u00a0 \u00a0 \u00a0 process_options=process_options,\u00a0 \u00a0 )\u00a0 \u00a0 result = client.process_document(request=request)\u00a0 \u00a0 # For a full list of `Document` object attributes, reference this page:\u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/reference/rest/v1/Document\u00a0 \u00a0 return result.document\n```\n[Document AI Toolbox](/document-ai/docs/toolbox)\n[Document](/document-ai/docs/reference/rest/v1/Document)\nFor more information, see the [Document AI Python API reference documentation](/python/docs/reference/documentai/latest) .\nTo authenticate to Document AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/googleapis/python-documentai-toolbox/blob/HEAD/samples/snippets/split_pdf_sample.py) \n```\nfrom google.cloud.documentai_toolbox import document# TODO(developer): Uncomment these variables before running the sample.# Given a local document.proto or sharded document.proto from a splitter/classifier in path# document_path = \"path/to/local/document.json\"# pdf_path = \"path/to/local/document.pdf\"# output_path = \"resources/output/\"def split_pdf_sample(document_path: str, pdf_path: str, output_path: str) -> None:\u00a0 \u00a0 wrapped_document = document.Document.from_document_path(document_path=document_path)\u00a0 \u00a0 output_files = wrapped_document.split_pdf(\u00a0 \u00a0 \u00a0 \u00a0 pdf_path=pdf_path, output_path=output_path\u00a0 \u00a0 )\u00a0 \u00a0 print(\"Document Successfully Split\")\u00a0 \u00a0 for output_file in output_files:\u00a0 \u00a0 \u00a0 \u00a0 print(output_file)\n```\n## Document AI Toolbox\n**    Experimental     ** This library is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA libraries are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\n[Document AI Toolbox](/document-ai/docs/toolbox) is an SDK for Python that provides utility functions for managing, manipulating, and extracting information from the document response. It creates a \"wrapped\" document object from a processed document response from JSON files in Cloud Storage, local JSON files, or output directly from the [process_document()](/document-ai/docs/reference/rest/v1/projects.locations.processors/process) method.\nIt can perform the following actions:\n- Combine fragmented [Document](/document-ai/docs/reference/rest/v1/Document) JSON files from Batch Processing into a single \"wrapped\" document.\n- Export shards as a unified [Document](/document-ai/docs/reference/rest/v1/Document) .\n- Get [Document](/document-ai/docs/reference/rest/v1/Document) output from:- [Cloud Storage](/storage) \n- [BatchProcessMetadata](/document-ai/docs/reference/rest/Shared.Types/BatchProcessMetadata) \n- [Operation name](/document-ai/docs/reference/rest/Shared.Types/ListOperationsResponse#Operation.FIELDS.name) \n- Access text from [Pages](/document-ai/docs/reference/rest/v1/Document#page) , [Lines](/document-ai/docs/reference/rest/v1/Document#line) , [Paragraphs](/document-ai/docs/reference/rest/v1/Document#paragraph) , [FormFields](/document-ai/docs/reference/rest/v1/Document#formfield) , and [Tables](/document-ai/docs/reference/rest/v1/Document#table) without handling [Layout](/document-ai/docs/reference/rest/v1/Document#Layout) information.\n- Search for a [Pages](/document-ai/docs/reference/rest/v1/Document#page) containing a target string or matching a regular expression.\n- Search for [FormFields](/document-ai/docs/reference/rest/v1/Document#formfield) by name.\n- Search for [Entities](/document-ai/docs/reference/rest/v1/Document#entity) by type.\n- Convert [Tables](/document-ai/docs/reference/rest/v1/Document#table) to a [Pandas](https://pandas.pydata.org/) Dataframe or CSV.\n- Insert [Entities](/document-ai/docs/reference/rest/v1/Document#entity) and [FormFields](/document-ai/docs/reference/rest/v1/Document#formfield) into a [BigQuery](/bigquery) table.\n- Split a PDF file based on [output from a Splitter/Classifier processor](#splitting) .\n- Extract image [Entities](/document-ai/docs/reference/rest/v1/Document#entity) from [Document](/document-ai/docs/reference/rest/v1/Document) [bounding boxes](/document-ai/docs/reference/rest/v1/Document#boundingpoly) .\n- Convert [Documents](/document-ai/docs/reference/rest/v1/Document) to and from commonly used formats:- [Cloud Vision API](/vision) [AnnotateFileResponse](/vision/docs/reference/rest/v1/BatchAnnotateFilesResponse#AnnotateFileResponse) \n- [hOCR](https://en.wikipedia.org/wiki/HOCR) \n- Third-party document processing formats\n- Create batches of documents for processing from a Cloud Storage folder.\n### Code Samples\nThe following code samples demonstrate how to use Document AI Toolbox.\n[View on GitHub](https://github.com/googleapis/python-documentai-toolbox/blob/HEAD/samples/snippets/quickstart_sample.py) \n```\nfrom typing import Optionalfrom google.cloud import documentaifrom google.cloud.documentai_toolbox import documentfrom google.cloud.documentai_toolbox import gcs_utilities# TODO(developer): Uncomment these variables before running the sample.# Given a Document JSON or sharded Document JSON in path gs://bucket/path/to/folder# gcs_bucket_name = \"bucket\"# gcs_prefix = \"path/to/folder\"# Or, given a Document JSON in path gs://bucket/path/to/folder/document.json# gcs_uri = \"gs://bucket/path/to/folder/document.json\"# Or, given a Document JSON in path local/path/to/folder/document.json# document_path = \"local/path/to/folder/document.json\"# Or, given a Document object from Document AI# documentai_document = documentai.Document()# Or, given a BatchProcessMetadata object from Document AI# operation = client.batch_process_documents(request)# operation.result(timeout=timeout)# batch_process_metadata = documentai.BatchProcessMetadata(operation.metadata)# Or, given a BatchProcessOperation name from Document AI# batch_process_operation = \"projects/project_id/locations/location/operations/operation_id\"def quickstart_sample(\u00a0 \u00a0 gcs_bucket_name: Optional[str] = None,\u00a0 \u00a0 gcs_prefix: Optional[str] = None,\u00a0 \u00a0 gcs_uri: Optional[str] = None,\u00a0 \u00a0 document_path: Optional[str] = None,\u00a0 \u00a0 documentai_document: Optional[documentai.Document] = None,\u00a0 \u00a0 batch_process_metadata: Optional[documentai.BatchProcessMetadata] = None,\u00a0 \u00a0 batch_process_operation: Optional[str] = None,) -> document.Document:\u00a0 \u00a0 if gcs_bucket_name and gcs_prefix:\u00a0 \u00a0 \u00a0 \u00a0 # Load from Google Cloud Storage Directory\u00a0 \u00a0 \u00a0 \u00a0 print(\"Document structure in Cloud Storage\")\u00a0 \u00a0 \u00a0 \u00a0 gcs_utilities.print_gcs_document_tree(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 gcs_bucket_name=gcs_bucket_name, gcs_prefix=gcs_prefix\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 wrapped_document = document.Document.from_gcs(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 gcs_bucket_name=gcs_bucket_name, gcs_prefix=gcs_prefix\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 elif gcs_uri:\u00a0 \u00a0 \u00a0 \u00a0 # Load a single Document from a Google Cloud Storage URI\u00a0 \u00a0 \u00a0 \u00a0 wrapped_document = document.Document.from_gcs_uri(gcs_uri=gcs_uri)\u00a0 \u00a0 elif document_path:\u00a0 \u00a0 \u00a0 \u00a0 # Load from local `Document` JSON file\u00a0 \u00a0 \u00a0 \u00a0 wrapped_document = document.Document.from_document_path(document_path)\u00a0 \u00a0 elif documentai_document:\u00a0 \u00a0 \u00a0 \u00a0 # Load from `documentai.Document` object\u00a0 \u00a0 \u00a0 \u00a0 wrapped_document = document.Document.from_documentai_document(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 documentai_document\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 elif batch_process_metadata:\u00a0 \u00a0 \u00a0 \u00a0 # Load Documents from `BatchProcessMetadata` object\u00a0 \u00a0 \u00a0 \u00a0 wrapped_documents = document.Document.from_batch_process_metadata(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 metadata=batch_process_metadata\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 wrapped_document = wrapped_documents[0]\u00a0 \u00a0 elif batch_process_operation:\u00a0 \u00a0 \u00a0 \u00a0 wrapped_documents = document.Document.from_batch_process_operation(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 location=\"us\", operation_name=batch_process_operation\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 wrapped_document = wrapped_documents[0]\u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 \u00a0 raise ValueError(\"No document source provided.\")\u00a0 \u00a0 # For all properties and methods, refer to:\u00a0 \u00a0 # https://cloud.google.com/python/docs/reference/documentai-toolbox/latest/google.cloud.documentai_toolbox.wrappers.document.Document\u00a0 \u00a0 print(\"Document Successfully Loaded!\")\u00a0 \u00a0 print(f\"\\t Number of Pages: {len(wrapped_document.pages)}\")\u00a0 \u00a0 print(f\"\\t Number of Entities: {len(wrapped_document.entities)}\")\u00a0 \u00a0 for page in wrapped_document.pages:\u00a0 \u00a0 \u00a0 \u00a0 print(f\"Page {page.page_number}\")\u00a0 \u00a0 \u00a0 \u00a0 for block in page.blocks:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(block.text)\u00a0 \u00a0 \u00a0 \u00a0 for paragraph in page.paragraphs:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(paragraph.text)\u00a0 \u00a0 \u00a0 \u00a0 for line in page.lines:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(line.text)\u00a0 \u00a0 \u00a0 \u00a0 for token in page.tokens:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(token.text)\u00a0 \u00a0 \u00a0 \u00a0 # Only supported with Form Parser processor\u00a0 \u00a0 \u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/form-parser\u00a0 \u00a0 \u00a0 \u00a0 for form_field in page.form_fields:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(f\"{form_field.field_name} : {form_field.field_value}\")\u00a0 \u00a0 \u00a0 \u00a0 # Only supported with Enterprise Document OCR version `pretrained-ocr-v2.0-2023-06-02`\u00a0 \u00a0 \u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/process-documents-ocr#enable_symbols\u00a0 \u00a0 \u00a0 \u00a0 for symbol in page.symbols:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(symbol.text)\u00a0 \u00a0 \u00a0 \u00a0 # Only supported with Enterprise Document OCR version `pretrained-ocr-v2.0-2023-06-02`\u00a0 \u00a0 \u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/process-documents-ocr#math_ocr\u00a0 \u00a0 \u00a0 \u00a0 for math_formula in page.math_formulas:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(math_formula.text)\u00a0 \u00a0 # Only supported with Entity Extraction processors\u00a0 \u00a0 # https://cloud.google.com/document-ai/docs/processors-list\u00a0 \u00a0 for entity in wrapped_document.entities:\u00a0 \u00a0 \u00a0 \u00a0 print(f\"{entity.type_} : {entity.mention_text}\")\u00a0 \u00a0 \u00a0 \u00a0 if entity.normalized_text:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(f\"\\tNormalized Text: {entity.normalized_text}\")\n``` [View on GitHub](https://github.com/googleapis/python-documentai-toolbox/blob/HEAD/samples/snippets/table_sample.py) \n```\nfrom google.cloud.documentai_toolbox import document# TODO(developer): Uncomment these variables before running the sample.# Given a local document.proto or sharded document.proto in path# document_path = \"path/to/local/document.json\"# output_file_prefix = \"output/table\"def table_sample(document_path: str, output_file_prefix: str) -> None:\u00a0 \u00a0 wrapped_document = document.Document.from_document_path(document_path=document_path)\u00a0 \u00a0 print(\"Tables in Document\")\u00a0 \u00a0 for page in wrapped_document.pages:\u00a0 \u00a0 \u00a0 \u00a0 for table_index, table in enumerate(page.tables):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Convert table to Pandas Dataframe\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Refer to https://pandas.pydata.org/docs/reference/frame.html for all supported methods\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 df = table.to_dataframe()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(df)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output_filename = f\"{output_file_prefix}-{page.page_number}-{table_index}\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Write Dataframe to CSV file\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 df.to_csv(f\"{output_filename}.csv\", index=False)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Write Dataframe to HTML file\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 df.to_html(f\"{output_filename}.html\", index=False)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Write Dataframe to Markdown file\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 df.to_markdown(f\"{output_filename}.md\", index=False)\n``` [View on GitHub](https://github.com/googleapis/python-documentai-toolbox/blob/HEAD/samples/snippets/entities_to_bigquery_sample.py) \n```\nfrom google.cloud.documentai_toolbox import document# TODO(developer): Uncomment these variables before running the sample.# Given a document.proto or sharded document.proto in path gs://bucket/path/to/folder# gcs_bucket_name = \"bucket\"# gcs_prefix = \"path/to/folder\"# dataset_name = \"test_dataset\"# table_name = \"test_table\"# project_id = \"YOUR_PROJECT_ID\"def entities_to_bigquery_sample(\u00a0 \u00a0 gcs_bucket_name: str,\u00a0 \u00a0 gcs_prefix: str,\u00a0 \u00a0 dataset_name: str,\u00a0 \u00a0 table_name: str,\u00a0 \u00a0 project_id: str,) -> None:\u00a0 \u00a0 wrapped_document = document.Document.from_gcs(\u00a0 \u00a0 \u00a0 \u00a0 gcs_bucket_name=gcs_bucket_name, gcs_prefix=gcs_prefix\u00a0 \u00a0 )\u00a0 \u00a0 job = wrapped_document.entities_to_bigquery(\u00a0 \u00a0 \u00a0 \u00a0 dataset_name=dataset_name, table_name=table_name, project_id=project_id\u00a0 \u00a0 )\u00a0 \u00a0 # Also supported:\u00a0 \u00a0 # job = wrapped_document.form_fields_to_bigquery(\u00a0 \u00a0 # \u00a0 \u00a0 dataset_name=dataset_name, table_name=table_name, project_id=project_id\u00a0 \u00a0 # )\u00a0 \u00a0 print(\"Document entities loaded into BigQuery\")\u00a0 \u00a0 print(f\"Job ID: {job.job_id}\")\u00a0 \u00a0 print(f\"Table: {job.destination.path}\")\n``` [View on GitHub](https://github.com/googleapis/python-documentai-toolbox/blob/HEAD/samples/snippets/split_pdf_sample.py) \n```\nfrom google.cloud.documentai_toolbox import document# TODO(developer): Uncomment these variables before running the sample.# Given a local document.proto or sharded document.proto from a splitter/classifier in path# document_path = \"path/to/local/document.json\"# pdf_path = \"path/to/local/document.pdf\"# output_path = \"resources/output/\"def split_pdf_sample(document_path: str, pdf_path: str, output_path: str) -> None:\u00a0 \u00a0 wrapped_document = document.Document.from_document_path(document_path=document_path)\u00a0 \u00a0 output_files = wrapped_document.split_pdf(\u00a0 \u00a0 \u00a0 \u00a0 pdf_path=pdf_path, output_path=output_path\u00a0 \u00a0 )\u00a0 \u00a0 print(\"Document Successfully Split\")\u00a0 \u00a0 for output_file in output_files:\u00a0 \u00a0 \u00a0 \u00a0 print(output_file)\n``` [View on GitHub](https://github.com/googleapis/python-documentai-toolbox/blob/HEAD/samples/snippets/export_images_sample.py) \n```\nfrom google.cloud.documentai_toolbox import document# TODO(developer): Uncomment these variables before running the sample.# Given a local document.proto or sharded document.proto from an identity processor in path# document_path = \"path/to/local/document.json\"# output_path = \"resources/output/\"# output_file_prefix = \"exported_photo\"# output_file_extension = \"png\"def export_images_sample(\u00a0 \u00a0 document_path: str,\u00a0 \u00a0 output_path: str,\u00a0 \u00a0 output_file_prefix: str,\u00a0 \u00a0 output_file_extension: str,) -> None:\u00a0 \u00a0 wrapped_document = document.Document.from_document_path(document_path=document_path)\u00a0 \u00a0 output_files = wrapped_document.export_images(\u00a0 \u00a0 \u00a0 \u00a0 output_path=output_path,\u00a0 \u00a0 \u00a0 \u00a0 output_file_prefix=output_file_prefix,\u00a0 \u00a0 \u00a0 \u00a0 output_file_extension=output_file_extension,\u00a0 \u00a0 )\u00a0 \u00a0 print(\"Images Successfully Exported\")\u00a0 \u00a0 for output_file in output_files:\u00a0 \u00a0 \u00a0 \u00a0 print(output_file)\n``` [View on GitHub](https://github.com/googleapis/python-documentai-toolbox/blob/HEAD/samples/snippets/convert_document_to_vision_sample.py) \n```\nfrom google.cloud.documentai_toolbox import document# TODO(developer): Uncomment these variables before running the sample.# Given a document.proto or sharded document.proto in path gs://bucket/path/to/folder# gcs_bucket_name = \"bucket\"# gcs_prefix = \"path/to/folder\"def convert_document_to_vision_sample(\u00a0 \u00a0 gcs_bucket_name: str,\u00a0 \u00a0 gcs_prefix: str,) -> None:\u00a0 \u00a0 wrapped_document = document.Document.from_gcs(\u00a0 \u00a0 \u00a0 \u00a0 gcs_bucket_name=gcs_bucket_name, gcs_prefix=gcs_prefix\u00a0 \u00a0 )\u00a0 \u00a0 # Converting wrapped_document to vision AnnotateFileResponse\u00a0 \u00a0 annotate_file_response = (\u00a0 \u00a0 \u00a0 \u00a0 wrapped_document.convert_document_to_annotate_file_response()\u00a0 \u00a0 )\u00a0 \u00a0 print(\"Document converted to AnnotateFileResponse!\")\u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 f\"Number of Pages : {len(annotate_file_response.responses[0].full_text_annotation.pages)}\"\u00a0 \u00a0 )\n``` [View on GitHub](https://github.com/googleapis/python-documentai-toolbox/blob/HEAD/samples/snippets/convert_document_to_hocr_sample.py) \n```\nfrom google.cloud.documentai_toolbox import document# TODO(developer): Uncomment these variables before running the sample.# Given a document.proto or sharded document.proto in path gs://bucket/path/to/folder# document_path = \"path/to/local/document.json\"# document_title = \"your-document-title\"def convert_document_to_hocr_sample(document_path: str, document_title: str) -> str:\u00a0 \u00a0 wrapped_document = document.Document.from_document_path(document_path=document_path)\u00a0 \u00a0 # Converting wrapped_document to hOCR format\u00a0 \u00a0 hocr_string = wrapped_document.export_hocr_str(title=document_title)\u00a0 \u00a0 print(\"Document converted to hOCR!\")\u00a0 \u00a0 return hocr_string\n``` [View on GitHub](https://github.com/googleapis/python-documentai-toolbox/blob/HEAD/samples/snippets/convert_external_annotations_sample.py) \n```\nfrom google.cloud.documentai_toolbox import converter# TODO(developer): Uncomment these variables before running the sample.# This sample will convert external annotations to the Document.json format used by Document AI Workbench for training.# To process this the external annotation must have these type of objects:# \u00a0 \u00a0 \u00a0 1) Type# \u00a0 \u00a0 \u00a0 2) Text# \u00a0 \u00a0 \u00a0 3) Bounding Box (bounding boxes must be 1 of the 3 optional types)\n## This is the bare minimum requirement to convert the annotations but for better accuracy you will need to also have:# \u00a0 \u00a0 \u00a0 1) Document width & height\n## Bounding Box Types:# \u00a0 Type 1:# \u00a0 \u00a0 \u00a0 bounding_box:[{\"x\":1,\"y\":2},{\"x\":2,\"y\":2},{\"x\":2,\"y\":3},{\"x\":1,\"y\":3}]# \u00a0 Type 2:# \u00a0 \u00a0 \u00a0 bounding_box:{ \"Width\": 1, \"Height\": 1, \"Left\": 1, \"Top\": 1}# \u00a0 Type 3:# \u00a0 \u00a0 \u00a0 bounding_box: [1,2,2,2,2,3,1,3]\n## \u00a0 Note: If these types are not sufficient you can propose a feature request or contribute the new type and conversion functionality.\n## Given a folders in gcs_input_path with the following structure :\n## gs://path/to/input/folder# \u00a0 \u251c\u2500\u2500test_annotations.json# \u00a0 \u251c\u2500\u2500test_config.json# \u00a0 \u2514\u2500\u2500test.pdf\n## An example of the config is in sample-converter-configs/Azure/form-config.json\n## location = \"us\",# processor_id = \"my_processor_id\"# gcs_input_path = \"gs://path/to/input/folder\"# gcs_output_path = \"gs://path/to/input/folder\"def convert_external_annotations_sample(\u00a0 \u00a0 location: str,\u00a0 \u00a0 processor_id: str,\u00a0 \u00a0 project_id: str,\u00a0 \u00a0 gcs_input_path: str,\u00a0 \u00a0 gcs_output_path: str,) -> None:\u00a0 \u00a0 converter.convert_from_config(\u00a0 \u00a0 \u00a0 \u00a0 project_id=project_id,\u00a0 \u00a0 \u00a0 \u00a0 location=location,\u00a0 \u00a0 \u00a0 \u00a0 processor_id=processor_id,\u00a0 \u00a0 \u00a0 \u00a0 gcs_input_path=gcs_input_path,\u00a0 \u00a0 \u00a0 \u00a0 gcs_output_path=gcs_output_path,\u00a0 \u00a0 )\n``` [View on GitHub](https://github.com/googleapis/python-documentai-toolbox/blob/HEAD/samples/snippets/create_batches_sample.py) \n```\nfrom google.cloud import documentaifrom google.cloud.documentai_toolbox import gcs_utilities# TODO(developer): Uncomment these variables before running the sample.# Given unprocessed documents in path gs://bucket/path/to/folder# gcs_bucket_name = \"bucket\"# gcs_prefix = \"path/to/folder\"# batch_size = 50def create_batches_sample(\u00a0 \u00a0 gcs_bucket_name: str,\u00a0 \u00a0 gcs_prefix: str,\u00a0 \u00a0 batch_size: int = 50,) -> None:\u00a0 \u00a0 # Creating batches of documents for processing\u00a0 \u00a0 batches = gcs_utilities.create_batches(\u00a0 \u00a0 \u00a0 \u00a0 gcs_bucket_name=gcs_bucket_name, gcs_prefix=gcs_prefix, batch_size=batch_size\u00a0 \u00a0 )\u00a0 \u00a0 print(f\"{len(batches)} batch(es) created.\")\u00a0 \u00a0 for batch in batches:\u00a0 \u00a0 \u00a0 \u00a0 print(f\"{len(batch.gcs_documents.documents)} files in batch.\")\u00a0 \u00a0 \u00a0 \u00a0 print(batch.gcs_documents.documents)\u00a0 \u00a0 \u00a0 \u00a0 # Use as input for batch_process_documents()\u00a0 \u00a0 \u00a0 \u00a0 # Refer to https://cloud.google.com/document-ai/docs/send-request\u00a0 \u00a0 \u00a0 \u00a0 # for how to send a batch processing request\u00a0 \u00a0 \u00a0 \u00a0 request = documentai.BatchProcessRequest(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name=\"processor_name\", input_documents=batch\u00a0 \u00a0 \u00a0 \u00a0 )\n``` [View on GitHub](https://github.com/googleapis/python-documentai-toolbox/blob/HEAD/samples/snippets/merge_document_shards_sample.py) \n```\nfrom google.cloud import documentaifrom google.cloud.documentai_toolbox import document# TODO(developer): Uncomment these variables before running the sample.# Given a document.proto or sharded document.proto in path gs://bucket/path/to/folder# gcs_bucket_name = \"bucket\"# gcs_prefix = \"path/to/folder\"# output_file_name = \"path/to/folder/file.json\"def merge_document_shards_sample(\u00a0 \u00a0 gcs_bucket_name: str, gcs_prefix: str, output_file_name: str) -> None:\u00a0 \u00a0 wrapped_document = document.Document.from_gcs(\u00a0 \u00a0 \u00a0 \u00a0 gcs_bucket_name=gcs_bucket_name, gcs_prefix=gcs_prefix\u00a0 \u00a0 )\u00a0 \u00a0 merged_document = wrapped_document.to_merged_documentai_document()\u00a0 \u00a0 with open(output_file_name, \"w\") as f:\u00a0 \u00a0 \u00a0 \u00a0 f.write(documentai.Document.to_json(merged_document))\u00a0 \u00a0 print(f\"Document with {len(wrapped_document.shards)} shards successfully merged.\")\n```", "guide": "Document AI"}