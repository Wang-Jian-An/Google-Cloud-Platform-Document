{"title": "Compute Engine - Maximize GPU network performance with GPUDirect-TCPX", "url": "https://cloud.google.com/compute/docs/instances", "abstract": "# Compute Engine - Maximize GPU network performance with GPUDirect-TCPX\nThe [accelerator-optimized machine family](/compute/docs/accelerator-optimized-machines) is designed by Google Cloud to deliver the needed performance and efficiency for GPU accelerated workloads such as artificial intelligence (AI), machine learning (ML), and high performance computing (HPC).\nThe A3 accelerator-optimized machine series has 208 vCPUs, and up to 1872\u00a0GB of memory. Each A3 VM has eight NVIDIA H100 GPUs attached, which offers 80 GB GPU memory per GPU. These VMs can get up to 1,000\u00a0Gbps of network bandwidth, which makes them ideal for large transformer-based language models, databases, and high performance computing (HPC).\nWhen working with A3 VMs, you can use GPUDirect-TCPX to achieve the lowest possible latency between applications and the network. GPUDirect-TCPX is a custom, remote direct memory access (RDMA) networking stack that increases the network performance of your A3 VMs by allowing data packet payloads to transfer directly from GPU memory to the network interface without having to go through the CPU and system memory. A3 VMs can use GPUDirect-TCPX combined with Google Virtual NIC (gVNIC) to deliver the highest throughput between VMs in a cluster when compared to the A2 or G2 accelerator-optimized machine types.\nThis document shows you how to use set up and test the improved GPU network performance that is available with GPUDirect-TCPX on A3 VMs that use Container-Optimized OS.\n", "content": "## Overview\nTo test network performance with GPUDirect-TCPX, complete the following steps:\n- Set up Virtual Private Cloud (VPC) MTU jumbo frame networks.\n- Create your GPU VMs by using the`cos-105-lts`Container-Optimized OS image.\n- On each VM, install the GPU drivers.\n- On each VM, give the network interface cards (NICs) access to the GPU.\n- Run an NCCL test.## Set up jumbo frame MTU networks\n`a3-highgpu-8g` VMs have [five physical NICs](/compute/docs/gpus/gpu-network-bandwidth#a3_vms) , to get the best performance for the physical NICs, you need to create five Virtual Private Cloud networks and set the MTU to `8244` .\n### Create management network, subnet, and firewall rule\nComplete the following steps to set up the management network:\n- Create the management network by using the [networks create command](/sdk/gcloud/reference/compute/networks/create) :```\ngcloud compute networks create NETWORK_NAME_PREFIX-mgmt-net \\\n --project=PROJECT_ID \\\n --subnet-mode=custom \\\n --mtu=8244\n```\n- Create the management subnet by using the [networks subnets create command](/sdk/gcloud/reference/compute/networks/subnets/create) :```\ngcloud compute networks subnets create NETWORK_NAME_PREFIX-mgmt-sub \\\n --project=PROJECT_ID \\\n --network=NETWORK_NAME_PREFIX-mgmt-net \\\n --region=REGION \\\n --range=192.168.0.0/24\n```\n- Create firewall rules by using the [firewall-rules create command](/sdk/gcloud/reference/compute/firewall-rules/create) .- Create a firewall rule for the management network.```\ngcloud compute firewall-rules create NETWORK_NAME_PREFIX-mgmt-internal \\\n --project=PROJECT_ID \\\n --network=NETWORK_NAME_PREFIX-mgmt-net \\\n --action=ALLOW \\\n --rules=tcp:0-65535,udp:0-65535,icmp \\\n --source-ranges=192.168.0.0/16\n```\n- Create the `tcp:22` firewall rule to limit which source IP addresses can connect to your VM by using SSH.```\ngcloud compute firewall-rules create NETWORK_NAME_PREFIX-mgmt-external-ssh \\\n --project=PROJECT_ID \\\n --network=NETWORK_NAME_PREFIX-mgmt-net \\\n --action=ALLOW \\\n --rules=tcp:22 \\\n --source-ranges=SSH_SOURCE_IP_RANGE\n```\n- Create the `icmp` firewall rule that can be used to check for data transmission issues in the network.```\ngcloud compute firewall-rules create NETWORK_NAME_PREFIX-mgmt-external-ping \\\n --project=PROJECT_ID \\\n --network=NETWORK_NAME_PREFIX-mgmt-net \\\n --action=ALLOW \\\n --rules=icmp \\\n --source-ranges=0.0.0.0/0\n```Replace the following:\n- ``: the name prefix to use for the Virtual Private Cloud networks and subnets.\n- ``: your project ID.\n- ``: the region where you want to create the networks.\n- ``: IP range in CIDR format. This specifies which source IP addresses can connect to your VM by using SSH.\n### Create data networks, subnets, and firewall rule\nUse the following command to create four data networks, each with subnets and firewall rules.\n```\nfor N in $(seq 1 4); do\n gcloud compute networks create NETWORK_NAME_PREFIX-data-net-$N \\\n  --project=PROJECT_ID \\\n  --subnet-mode=custom \\\n  --mtu=8244\n gcloud compute networks subnets create NETWORK_NAME_PREFIX-data-sub-$N \\\n  --project=PROJECT_ID \\\n  --network=NETWORK_NAME_PREFIX-data-net-$N \\\n  --region=REGION \\\n  --range=192.168.$N.0/24\n gcloud compute firewall-rules create NETWORK_NAME_PREFIX-data-internal-$N \\\n  --project=PROJECT_ID \\\n  --network=NETWORK_NAME_PREFIX-data-net-$N \\\n  --action=ALLOW \\\n  --rules=tcp:0-65535,udp:0-65535,icmp \\\n  --source-ranges=192.168.0.0/16\ndone\n```\nFor more information about how to create Virtual Private Cloud networks, see [Create and verify a jumbo frame MTU network](/vpc/docs/configure-jumbo-frame-mtu-vpc) .\n## Create your GPU VMs\nTo test network performance with GPUDirect-TCPX, you need to create at least two A3 VMs.\n- Create each VM by using the `cos-105-lts` Container-Optimized OS image and specifying the virtual MTU networks that were created in the previous step.The VMs must also use the Google Virtual NIC (gVNIC) network interface. For A3 VMs, gVNIC version 1.4.0rc3 or later is required. This driver version is available on the Container-Optimized OS.The first virtual NIC is used as the primary NIC for general networking and storage, the other four virtual NICs are NUMA aligned with two of the eight GPUs on the same PCIe switch.```\ngcloud compute instances create VM_NAME \\\n --project=PROJECT_ID \\\n --zone=ZONE \\\n --machine-type=a3-highgpu-8g \\\n --maintenance-policy=TERMINATE --restart-on-failure \\\n --image-family=cos-105-lts \\\n --image-project=cos-cloud \\\n --boot-disk-size=${BOOT_DISK_SZ:-50} \\\n --metadata=cos-update-strategy=update_disabled \\\n --scopes=https://www.googleapis.com/auth/cloud-platform \\\n --network-interface=nic-type=GVNIC,network=NETWORK_NAME_PREFIX-mgmt-net,subnet=NETWORK_NAME_PREFIX-mgmt-sub \\\n --network-interface=nic-type=GVNIC,network=NETWORK_NAME_PREFIX-data-net-1,subnet=NETWORK_NAME_PREFIX-data-sub-1,no-address \\\n --network-interface=nic-type=GVNIC,network=NETWORK_NAME_PREFIX-data-net-2,subnet=NETWORK_NAME_PREFIX-data-sub-2,no-address \\\n --network-interface=nic-type=GVNIC,network=NETWORK_NAME_PREFIX-data-net-3,subnet=NETWORK_NAME_PREFIX-data-sub-3,no-address \\\n --network-interface=nic-type=GVNIC,network=NETWORK_NAME_PREFIX-data-net-4,subnet=NETWORK_NAME_PREFIX-data-sub-4,no-address\n```Replace the following:- ``: the name of your VM.\n- ``: your project ID.\n- ``: the zone for the VM.\n- ``: the name prefix to use for the Virtual Private Cloud networks and subnets.\n## Install GPU drivers\nOn each A3 VM, complete the following steps.\n- Install the NVIDIA GPU drivers by running the following command:```\nsudo cos-extensions install gpu -- --version=latest\n```\n- Re-mount the path by running the following command:```\nsudo mount --bind /var/lib/nvidia /var/lib/nvidia\nsudo mount -o remount,exec /var/lib/nvidia\n```## Give the NICs access to the GPUs\nOn each A3 VM, give the NICs access to the GPUs by completing the following steps:\n- Configure the registry.- If you are using Container Registry, run the following command:```\ndocker-credential-gcr configure-docker\n```\n- If you are using Artifact Registry, run the following command:```\ndocker-credential-gcr configure-docker --registries us-docker.pkg.dev\n```\n- Configure the receive data path manager. A management service, GPUDirect-TCPX Receive Data Path Manager, needs to run alongside the applications that use GPUDirect-TCPX. To start the service on each Container-Optimized OS VM, run the following command:```\ndocker run --pull=always --rm \\\n --name receive-datapath-manager \\\n --detach \\\n --cap-add=NET_ADMIN --network=host \\\n --volume /var/lib/nvidia/lib64:/usr/local/nvidia/lib64 \\\n --device /dev/nvidia0:/dev/nvidia0 \\\n --device /dev/nvidia1:/dev/nvidia1 \\\n --device /dev/nvidia2:/dev/nvidia2 \\\n --device /dev/nvidia3:/dev/nvidia3 \\\n --device /dev/nvidia4:/dev/nvidia4 \\\n --device /dev/nvidia5:/dev/nvidia5 \\\n --device /dev/nvidia6:/dev/nvidia6 \\\n --device /dev/nvidia7:/dev/nvidia7 \\\n --device /dev/nvidia-uvm:/dev/nvidia-uvm \\\n --device /dev/nvidiactl:/dev/nvidiactl \\\n --env LD_LIBRARY_PATH=/usr/local/nvidia/lib64 \\\n --volume /run/tcpx:/run/tcpx \\\n --entrypoint /tcpgpudmarxd/build/app/tcpgpudmarxd \\\nus-docker.pkg.dev/gce-ai-infra/gpudirect-tcpx/tcpgpudmarxd \\\n --gpu_nic_preset a3vm --gpu_shmem_type fd --uds_path \"/run/tcpx\" --setup_param \"--verbose 128 2 0\"\n```\n- Verify the `receive-datapath-manager` container started.```\ndocker container logs --follow receive-datapath-manager\n```The output should resemble the following:```\nI0000 00:00:1687813309.406064  1 rx_rule_manager.cc:174] Rx Rule Manager server(s) started...\n```\n- To stop viewing the logs, press `ctrl-c` .\n- Install IP table rules.```\nsudo iptables -I INPUT -p tcp -m tcp -j ACCEPT\n```\n- Configure the NVIDIA Collective Communications Library (NCCL) and GPUDirect-TCPX plugin.A specific NCCL library version and GPUDirect-TCPX plugin binary combination are required to use NCCL with GPUDirect-TCPX support. Google Cloud has provided packages that meet this requirement.To install the Google Cloud package, run the following command:```\ndocker run --rm -v /var/lib:/var/lib us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpx/nccl-plugin-gpudirecttcpx install --install-nccl\nsudo mount --bind /var/lib/tcpx /var/lib/tcpx\nsudo mount -o remount,exec /var/lib/tcpx\n```If this command is successful, the `libnccl-net.so` and `libnccl.so` files are placed in the `/var/lib/tcpx/lib64` directory.## Run tests\nOn each A3 VM, run an NCCL test by completing the following steps:\n- Start the container.```\n#!/bin/bash\nfunction run_tcpx_container() {\ndocker run \\\n -u 0 --network=host \\\n --cap-add=IPC_LOCK \\\n --userns=host \\\n --volume /run/tcpx:/tmp \\\n --volume /var/lib/nvidia/lib64:/usr/local/nvidia/lib64 \\\n --volume /var/lib/tcpx/lib64:/usr/local/tcpx/lib64 \\\n --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 \\\n --device /dev/nvidia0:/dev/nvidia0 \\\n --device /dev/nvidia1:/dev/nvidia1 \\\n --device /dev/nvidia2:/dev/nvidia2 \\\n --device /dev/nvidia3:/dev/nvidia3 \\\n --device /dev/nvidia4:/dev/nvidia4 \\\n --device /dev/nvidia5:/dev/nvidia5 \\\n --device /dev/nvidia6:/dev/nvidia6 \\\n --device /dev/nvidia7:/dev/nvidia7 \\\n --device /dev/nvidia-uvm:/dev/nvidia-uvm \\\n --device /dev/nvidiactl:/dev/nvidiactl \\\n --env LD_LIBRARY_PATH=/usr/local/nvidia/lib64:/usr/local/tcpx/lib64 \\\n \"$@\"\n}\n```The preceding command completes the following:- Mounts NVIDIA devices from`/dev`into the container\n- Sets network namespace of the container to the host\n- Sets user namespace of the container to host\n- Adds`CAP_IPC_LOCK`to the capabilities of the container\n- Mounts`/tmp`of the host to`/tmp`of the container\n- Mounts the installation path of NCCL and GPUDirect-TCPX NCCL plugin into the container and add the mounted path to`LD_LIBRARY_PATH`\n- After you start the container, applications that use NCCL can run from inside the container. For example, to run the `run-allgather` test, complete the following steps:- On each A3 VM, run the following:```\n$ run_tcpx_container -it --rm us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpx/nccl-plugin-gpudirecttcpx shell\n```\n- On one VM, run the following commands:- Set up connection between the VMs. Replace `` and `` with the names of each VM.```\n/scripts/init_ssh.sh VM-0 VM-1\npushd /scripts && /scripts/gen_hostfiles.sh VM-0 VM-1; popd\n```This creates a `/scripts/hostfiles2` directory on each VM.\n- Run the script.```\n/scripts/run-allgather.sh 8 eth1,eth2,eth3,eth4 1M 512M 2\n```The `run-allgather` script takes about two minutes to run. At the end of the logs, you'll see the `all-gather` results.If you see the following line in your NCCL logs, this verifies that GPUDirect-TCPX is initialized successfully.```\nNCCL INFO NET/GPUDirectTCPX ver. 3.1.1.\n```", "guide": "Compute Engine"}