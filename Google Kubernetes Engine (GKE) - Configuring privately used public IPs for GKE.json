{"title": "Google Kubernetes Engine (GKE) - Configuring privately used public IPs for GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview", "abstract": "# Google Kubernetes Engine (GKE) - Configuring privately used public IPs for GKE\nLast reviewed 2022-12-19 UTC\n**Warning:** This page is **archived** and is not actively maintained. The commands on this page might not work and could cause disruptions to your cluster.\nThis tutorial shows how to apply privately used public IP (PUPI) addresses to Google Kubernetes Engine (GKE) Pod address blocks. Service consumer organizations that are IPv4-address constrained can use PUPI addresses in service producer virtual private clouds (VPCs) as an address-management option.\nThis document is intended for network architects and GKE system administrators whose companies offer managed services over a GKE infrastructure on Google Cloud.", "content": "## IntroductionSome companies want to deliver managed services to their customers over Kubernetes or GKE clusters on Google Cloud. However, Kubernetes can require many IP addresses for its various components. For some organizations, meeting this requirement is difficult or impossible because they're unable to assign appropriately sized `10.0.0.0/8` , `172.16.0.0/12` , and `192.168.0.0/16` (RFC 1918) [Classless Inter-domain Routing (CIDR) blocks](https://tools.ietf.org/html/rfc4632) .\nOne way to mitigate address exhaustion is to use privately used public IP (PUPI) addresses for the GKE Pod CIDR block. PUPIs are any public IP addresses not owned by Google that a customer can use privately on Google Cloud. The customer doesn't necessarily own these addresses.\nThe following diagram shows a company (producer) that offers a managed service to a customer (consumer).This setup involves the following considerations:- **Primary CIDR block:** A non-PUPI CIDR block that is used for nodes and ILB and must be non-overlapping across VPCs.\n- **Producer secondary CIDR block:** A PUPI CIDR block that is used for Pods (for example,`45.45.0.0/16`).\n- **Consumer secondary CIDR block:** Any other PUPI CIDR block on the customer side (for example,`5.5/16`).\nThe company's managed service is in the producer VPC ( `vpc-producer` ) and is built on a GKE deployment. The company's GKE cluster uses the PUPI `45.0.0.0/8` CIDR block for Pod addresses. The customer's applications are located in the consumer VPC ( `vpc-consumer` ). The customer also has a GKE installation. The GKE cluster in the consumer VPC uses the PUPI `5.0.0.0/8` CIDR block for Pod addresses. The two VPCs are peered with one another. Both VPCs use the RFC 1918 address space for node, service, and load balancing addresses.\nBy default, the consumer VPC ( `vpc-consumer` ) exports all RFC 1918 to the producer VPC ( `vpc-producer` ). Unlike RFC 1918 private addresses and extended private addresses (CGN, Class E), PUPIs aren't automatically advertised to VPC peers by default. If the `vpc-consumer` Pods must communicate with `vpc-producer` , the consumer must enable the VPC peering connection to export PUPI addresses. Likewise, the producer must configure the producer VPC to import PUPI routes over the VPC peering connection.\nThe `vpc-consumer` address space that is exported into `vpc-producer` must not overlap with any RFC 1918 or PUPI address used in `vpc-producer` . The producer must inform the consumer what PUPI CIDR blocks the managed service uses and ensure that the consumer isn't using these blocks. The producer and consumer must also agree and assign non-overlapping address space for internal load balancing (ILB) and node addresses in `vpc-producer` .\n **Note:** PUPIs don't support service networking.\nIn most cases, resources in `vpc-consumer` communicate with services in `vpc-producer` through ILB addresses in the producer cluster. If the producer Pods are required to initiate communication directly with resources in `vpc-consumer` , and PUPI addressing doesn't overlap, then the producer must configure the producer VPC to export the PUPI routes over the VPC peering connection. Likewise, the consumer must configure the VPC peering connection to import routes into `vpc-consumer` . If the consumer VPC already uses the PUPI address, then the producer should instead configure the IP masquerade feature and hide the Pod IP addresses behind the producer node IP addresses.\nThe following table shows the default import and export settings for each VPC. You can modify default VPC peering settings by using the [gcloud compute networks peerings update](/sdk/gcloud/reference/compute/networks/peerings/update) Google Cloud CLI command.\n| PUPI flag                     | Import                       | Export                        |\n|:------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------|\n| Producer side (flags controlled through service networking)        | To turn on: --import-subnet-routes-with-public-ip (through peering) Default behavior: Turned off | To turn off: --no-export-subnet-routes-with-public-ip (through peering) Default behavior: Turned on |\n| Consumer side (owned by customer, not required to be modified through service networking) | Turned off (default)                    | Turned on (default)                     |\nThese settings result in the following:- The producer VPC sees all the customer routes.\n- The consumer VPC doesn't see the PUPI routes configured on the Pod subnet in the producer VPC.\n- Traffic originating from the producer Pods to the`vpc-consumer`network must be translated behind the node addresses in the producer cluster.\n## Qualifications\n- The address range that you select for a PUPI can't be reachable through the internet or be an address that Google owns.\n- The Node IPs and primary range need to be non-overlapping between the two VPCs.\n- If direct Pod-to-Pod communication is required between the customer VPC and the managed service, then the producer Pod IP addresses must be translated behind their corresponding Node IPs.\n## Objectives\n- Configure two VPC networks.\n- Configure one subnet inside each VPC network.\n- Configure a PUPI address range on a secondary address range in each subnet.\n- Establish a VPC peering relationship between the two VPC networks with proper import and export settings.\n- Inspect the routes within each VPC.\n## CostsIn this document, you use the following billable components of Google Cloud:- [GKE](/kubernetes-engine/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . ## Before you begin\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) You complete most of this tutorial from the Cloud Shell terminal using HashiCorp's Terraform and the gcloud CLI.\n- Clone the GitHub repository and change to the local working directory:```\ngit clone https://github.com/GoogleCloudPlatform/terraform-vpc-pupi $HOME/pupi\n```The repository contains all the files that you need to complete this tutorial. For a complete description of each file, see the `README.md` file in the repository.\n- Make all shell scripts executable:```\nsudo chmod 755 $HOME/pupi/*.sh\n```\n## Preparing your environmentIn this section, you install and set up Terraform, and then you set environment variables.\n### Set up Terraform\n- Install Terraform by following the [steps in the HashiCorp documentation](https://learn.hashicorp.com/terraform/getting-started/install.html) .\n- In Cloud Shell, initialize Terraform:```\ncd $HOME/pupiterraform init\n```The output is similar to the following:```\n...\nInitializing provider plugins...\nThe following providers do not have any version constraints in\nconfiguration, so the latest version was installed.\n...\nTerraform has been successfully initialized!\n...\n```As Terraform initializes, it logs progress messages. At the end of the message output, you see a message that Terraform initialized successfully.\n### Set environment variables\n- In Cloud Shell, set the `TF_VAR_org_id` variable:```\nexport TF_VAR_org_id=$(gcloud organizations list | \\\u00a0 \u00a0 awk '/YOUR_ORGANIZATION_NAME/ {print $2}')\n```Replace the following:- ``: the Google Cloud organization name that you want to use for this tutorial\n- Verify that you set the environment variable correctly:```\necho $TF_VAR_org_id\n```The output lists your numeric organization ID and looks similar to the following:```\n...\n123123123123\n...\n```\n- Set the remaining environment variables:```\nsource $HOME/pupi/set_variables.sh\n```This shell script sets basic parameters such as the Google Cloud region, organization ID, and project ID as variables in the shell environment. Terraform uses these variables to configure your Google Cloud resources. You can adjust or change the parameters in the shell script to fit your environment. For a full list of variables, review the `set_variables` shell script.\n- Verify that you set the environment variables correctly:```\nenv | grep TF_\n```The output is similar to the following:```\n...\nTF_VAR_billing_account=QQQQQQ-XAAAAA-E8769\nTF_VAR_org_id=406999999999\nTF_VAR_region1=us-west1\nTF_VAR_region2=us-west2\nTF_VAR_consumer_ilb_ip=10.129.0.200\nTF_VAR_user_account=user@example\nTF_VAR_pid=pupi-pid--999999999\nTF_VAR_zone2=us-west2-b\nTF_VAR_zone1=us-west1-b\n...\n```\n- Create an environment variable file:```\n$HOME/pupi/saveVars.sh\n```This command redirects the environment variables that you created into a file named `TF_ENV_VARS` . Each variable is prepended with the `export` command. If your Cloud Shell session is terminated, you can use this file to reset the variables. These variables are used by the Terraform scripts, Cloud Shell scripts, and the Google Cloud CLI.If you need to reinitialize the variables later, run the following command:```\nsource $HOME/pupi/TF_ENV_VARS\n```\n### Deploy supporting infrastructure\n- In Cloud Shell, deploy the Terraform supporting infrastructure:```\nterraform apply\n```When prompted, enter `yes` to apply either configuration. Terraform can take several minutes to deploy the resources.The `terraform apply` command instructs Terraform to deploy all the solution's components. To understand how the infrastructure is declaratively defined, see the Terraform manifests (files with a `.tf` extension).\n- Establish the VPC peering relationship. Because the feature is in alpha and not supported by Terraform, you use `gcloud` commands to set up peering.```\ngcloud alpha compute networks peerings create consumer \\\u00a0 \u00a0 --project=\"$TF_VAR_pid\" \\\u00a0 \u00a0 --network=consumer \\\u00a0 \u00a0 --peer-network=producergcloud alpha compute networks peerings create producer \\\u00a0 \u00a0 --project=\"$TF_VAR_pid\" \\\u00a0 \u00a0 --network=producer \\\u00a0 \u00a0 --peer-network=consumer \\\u00a0 \u00a0 --no-export-subnet-routes-with-public-ip \\\u00a0 \u00a0 --import-subnet-routes-with-public-ip\n```By default, the consumer VPC exports the PUPI addresses. When you create the producer VPC, you use the following arguments to configure the VPC to import PUPI addresses but not export them:```\n--no-export-subnet-routes-with-public-ip--import-subnet-routes-with-public-ip\n```\n## Inspecting the supporting infrastructureYou now verify that Terraform successfully created the resources by checking whether the resources respond after you send a command.\n### Verify the projects\n- In Cloud Shell, list the project:```\ngcloud projects list | grep pupi-pid\n```The output is similar to the following:```\n...\npupi-pid--1234567899   pupi-test    777999333555\n...\n```In this output, `pupi-test` is the project name, and `pupi-pid-` is the prefix to your project ID.\n- List the API status:```\ngcloud services list --project=$TF_VAR_pid \\\u00a0 \u00a0 | grep -E \"compute|container\"\n```The output is similar to the following:```\n...\ncompute.googleapis.com   Compute Engine API\ncontainer.googleapis.com   Kubernetes Engine API\ncontainerregistry.googleapis.com Container Registry API\n...\n```This output shows that the Compute Engine, GKE, and Container Registry APIs are enabled.\n### Verify the networks and subnetworks\n- In Cloud Shell, verify the producer network and subnetwork:```\ngcloud compute networks describe producer \\\u00a0 \u00a0 --project=$TF_VAR_pidgcloud compute networks subnets describe producer-nodes \\\u00a0 \u00a0 --project=$TF_VAR_pid \\\u00a0 \u00a0 --region=$TF_VAR_region1\n```The output is similar to the following:```\n...\nkind: compute#network\nname: producer\n...\nipCidrRange: 10.128.0.0/24\nkind: compute#isubnetwork\nname: producer-nodes\n...\nsecondaryIpRanges:\n- ipCidrRange: 45.45.45.0/24\n rangeName: producer-pods\n- ipCidrRange: 172.16.45.0/24\n rangeName: producer-cluster\n...\n```This output shows the following:- The network was created with the`10.128.0.0/24`CIDR block.\n- The two subnets were created with the`45.45.45.0/24`and`172.16.45.0/24`CIDR blocks.\n- Verify the consumer network and subnetwork:```\ngcloud compute networks describe consumer \\\u00a0 \u00a0 --project=$TF_VAR_pidgcloud compute networks subnets describe consumer-nodes \\\u00a0 \u00a0 --project=$TF_VAR_pid \\\u00a0 \u00a0 --region=$TF_VAR_region2\n```The output is similar to the following:```\n...\nkind: compute#network\nname: consumer\n...\nipCidrRange: 10.129.0.0/24\nkind: compute#isubnetwork\nname: consumer-nodes\n...\nsecondaryIpRanges:\n- ipCidrRange: 5.5.5.0/24\n rangeName: producer-pods\n- ipCidrRange: 172.16.5.0/24\n rangeName: consumer-cluster\n...\n```This output shows the following:- The network was created using the`10.129.0.0/24`CIDR block.\n- The two subnets were created using the`5.5.5.0/24`and`172.16.5.0/24`CIDR blocks.\n### Verify the GKE cluster and its resources\n- In Cloud Shell, get the cluster credentials:```\ngcloud container clusters get-credentials consumer-cluster \\\u00a0 \u00a0 --project=$TF_VAR_pid \\\u00a0 \u00a0 --zone=$TF_VAR_zone2\n```The output is similar to the following:```\n...\nFetching cluster endpoint and auth data.\nkubeconfig entry generated for consumer-cluster.\n...\n```\n- Verify the cluster:```\ngcloud container clusters list \\\u00a0 \u00a0 --project=$TF_VAR_pid \\\u00a0 \u00a0 --zone=$TF_VAR_zone2\n```The output is similar to the following:```\nNAME    LOCATION MASTER_VERSION MASTER_IP  MACHINE_TYPE NODE_VERSION NUM_NODES STATUS\nconsumer-cluster us-west2-b 1.14.10-gke.17 35.236.104.74 n1-standard-1 1.14.10-gke.17 3   RUNNING\n```This output shows a cluster that is named `consumer-cluster` .\n- Verify the Hello World app:```\nkubectl get deployment my-app\n```The output is similar to the following:```\n...\nNAME  DESIRED CURRENT UP-TO-DATE AVAILABLE AGE\nmy-app 3   3   3   3   118m\n...\n```This output shows a deployment that is named `my-app` .\n- Verify the internal load balancer service:```\nkubectl get service hello-server\n```The output is similar to the following:```\nNAME   TYPE   CLUSTER-IP EXTERNAL-IP PORT(S)   AGE\nhello-server LoadBalancer 172.16.5.99 10.129.0.200 8080:31673/TCP 4d23h\n```This output shows a service that is named `hello-server` .\n## Verifying the solution\n- In Cloud Shell, validate that you successfully created VPC peering:```\ngcloud alpha compute networks peerings list\n```The output is similar to the following:```\nNAME  NETWORK PEER_PROJECT   PEER_NETWORK IMPORT_CUSTOM_ROUTES EXPORT_CUSTOM_ROUTES STATE STATE_DETAILS\nconsumer consumer pupi-pid--1324732197 producer  False     False     ACTIVE [2020-02-26T11:33:16.886-08:00]: Connected.\nproducer producer pupi-pid--1324732197 consumer  False     False     ACTIVE [2020-02-26T11:33:16.886-08:00]: Connected.\n```This output shows peerings that are named `consumer` and `producer` .\n- Validate that the consumer VPC exports PUPI routes:```\ngcloud alpha compute networks peerings list-routes consumer \\\u00a0 \u00a0 --direction=OUTGOING \\\u00a0 \u00a0 --network=consumer \\\u00a0 \u00a0 --region=\"$TF_VAR_region2\"\n```The output is similar to the following:```\nDEST_RANGE  TYPE     NEXT_HOP_REGION PRIORITY STATUS\n10.129.0.0/24 SUBNET_PEERING_ROUTE us-west2   1000  accepted by peer\n172.16.5.0/24 SUBNET_PEERING_ROUTE us-west2   1000  accepted by peer\n5.5.5.0/24  SUBNET_PEERING_ROUTE us-west2   1000  accepted by peer\n```This output shows all three consumer CIDR blocks.\n- Validate the PUPI routes that the producer VCP imported:```\ngcloud alpha compute networks peerings list-routes producer \\\u00a0 \u00a0 --direction=INCOMING \\\u00a0 \u00a0 --network=producer \\\u00a0 \u00a0 --region=\"$TF_VAR_region1\"\n```The output is similar to the following:```\nDEST_RANGE  TYPE     NEXT_HOP_REGION PRIORITY STATUS\n10.129.0.0/24 SUBNET_PEERING_ROUTE us-west2   1000  accepted\n172.16.5.0/24 SUBNET_PEERING_ROUTE us-west2   1000  accepted\n5.5.5.0/24  SUBNET_PEERING_ROUTE us-west2   1000  accepted\n```This output shows all three consumer CIDR blocks.\n- Validate that the GKE Pods have a PUPI address:```\nkubectl get pod -o wide\n```The output is similar to the following:```\nNAME      READY STATUS RESTARTS AGE  IP   NODE            NOMINATED NODE READINESS GATES\nmy-app-594b56d7bc-642d8 1/1  Running 0   4d23h 5.5.5.21 gke-consumer-cluster-default-pool-cd302b68-tccf <none>   <none>\nmy-app-594b56d7bc-chnw8 1/1  Running 0   4d23h 5.5.5.38 gke-consumer-cluster-default-pool-cd302b68-h8v9 <none>   <none>\nmy-app-594b56d7bc-fjvbz 1/1  Running 0   4d23h 5.5.5.20 gke-consumer-cluster-default-pool-cd302b68-tccf <none>   <none>\n```The IP addresses of the Pods fall within the `5.5.5/24` range.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this tutorial:\n### Destroy the infrastructure\n- In Cloud Shell, destroy all of the tutorial's components:```\nterraform destroy\n```When prompted, enter `yes` to destroy the configuration.You might see the following Terraform error:```\n...\n\u2217 google_compute_network.ivpc (destroy): 1 error(s) occurred:\n\u2217 google_compute_network.ivpc: Error waiting for Deleting Network: The network resource 'projects/pupi-pid--1324732197/global/networks/consumer-cluster' is already being used by 'projects/pupi-pid--1324732197/global/firewalls/k8s-05693142c93de80e-node-hc'\n...\n```This error occurs when the command attempts to destroy the VPC network before destroying the GKE firewall rules. If you receive this error, do the following:- Remove the non-default firewall rules from the VPC:```\n$HOME/pupi/k8-fwr.sh\n```The output shows you the firewall rules to be removed. Review the rules and, when prompted, enter `yes` .\n- Re-issue the following command:```\ncd $HOME/pupiterraform destroy\n```\nWhen prompted, enter `yes` to destroy the configuration.\n- Remove the Git repository:```\nrm -rf $HOME/pupi\n```\n## What's next\n- Read the [Configuring private service access](/vpc/docs/configure-private-services-access) guide.\n- Read the [Getting started with the Service Networking API](/service-infrastructure/docs/service-networking/getting-started) guide.\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Google Kubernetes Engine (GKE)"}