{"title": "Cloud Architecture Center - Import logs from Cloud Storage to Cloud Logging", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Import logs from Cloud Storage to Cloud Logging\nLast reviewed 2024-01-02 UTC\nThis reference architecture describes how you can import logs that were previously [exported to Cloud Storage](/architecture/exporting-stackdriver-logging-for-compliance-requirements) back to Cloud Logging.\nThis reference architecture is intended for engineers and developers, including DevOps, site reliability engineers (SREs), and security investigators, who want to configure and run the log importing job. This document assumes you are familiar with running Cloud Run jobs, and how to use Cloud Storage and Cloud Logging.\n", "content": "## Architecture\nThe following diagram shows how Google Cloud services are used in this reference architecture:\nThis workflow includes the following components:\n- **Cloud Storage \nbucket** : Contains the previously exported logs you want to import back to Cloud Logging. Because these logs were previously exported, they're organized in the expected [export format](/logging/docs/export/storage#gcs-organization) .\n- **Cloud Run \njob** : Runs the import logs process:- Reads the objects that store log entries from Cloud Storage.\n- Finds exported logs for the specified log ID, in the requested time range, based on the [organization of the exported logs](/logging/docs/export/storage#gcs-organization) in the Cloud Storage bucket.\n- Converts the objects into Cloud Logging API [LogEntry structures](/logging/docs/reference/v2/rest/v2/LogEntry) . Multiple`LogEntry`structures are aggregated into batches, to reduce Cloud Logging API quota consumption. The architecture handles [quota errors](/docs/quota_detail/troubleshoot) when necessary.\n- Writes the converted log entries to Cloud Logging. If you re-run the same job multiple times, duplicate entries can result. For more information, see [Run the import job](/architecture/import-logs-from-storage-to-logging/deployment#run-import-job) .\n- **Cloud Logging** : Ingests and stores the converted log entries. The log entries are processed as described in the [Routing and storage overview](/logging/docs/routing/overview) .- The Logging [quotas and limits](/logging/quotas) apply, including the Cloud Logging API quotas and limits and a 30-day [retention period](/logging/quotas#logs_retention_periods) . This reference architecture is designed to work with the default write quotas, with a basic retrying mechanism. If your write quota is lower than the default, the implementation might fail.\n- The imported logs aren't included in [log-based metrics](/logging/docs/logs-based-metrics) , because their timestamps are in the past. However, if you opt to [use a label](/architecture/importing_logs_from_gcs_bucket#30-days-limit) , the timestamp records the import time, and the logs are included in the metric data.\n- **BigQuery** : Uses SQL to run analytical queries on imported logs (optional). To import audit logs from Cloud Storage, this architecture [modifies the log IDs](#log-names) ; you must account for this renaming when you query the imported logs.## Use case\nYou might choose to deploy this architecture if your organization requires additional log analysis for incident investigations or other audits of past events. For example, you might want to analyze connections to your databases for the first quarter of the last year, as a part of a database access audit.\n## Design alternatives\nThis section describes alternatives to the default design shown in this reference architecture document.\n### Retention period and imported logs\nCloud Logging requires incoming log entries to have timestamps that don't exceed a 30-day [retention period](/logging/quotas#logs_retention_periods) . Imported log entries with timestamps older than 30 days from the import time are not stored.\nThis architecture validates the date range set in the Cloud Run job to avoid importing logs that are older than 29 days, leaving a one-day safety margin.\nTo import logs older than 29 days, you need to make the [following changes to the implementation code](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/logging/import-logs#importing-log-entries-with-timestamps-older-than-30-days) , and then build a new container image to use in the Cloud Run job configuration.\n- Remove the 30-day validation of the date range\n- Add the original timestamp as a user label to the log entry\n- Reset the timestamp label of the log entry to allow it to be ingested with the current timestamp\nWhen you use this modification, you must use the [labels field](/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.labels) instead of the [timestamp field](/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.timestamp) in your Log Analytics queries. For more information about Log Analytics queries and samples, see [Sample SQL queries](/logging/docs/analyze/examples) .\n## Design considerations\nThe following guidelines can help you to develop an architecture that meets your organization's requirements.\n### Cost optimization\nThe cost for importing logs by using this reference architecture has multiple contributing factors.\nYou use the following billable components of Google Cloud:\n- [Cloud Logging](/stackdriver/pricing#logging-pricing-summary) ( [logs retention period](/logging/quotas#logs_retention_periods) costs apply)\n- [Cloud Run](/run/pricing) \n- [Cloud Storage API](/storage/pricing#operations-pricing) \nConsider the following factors that might increase costs:\n- **Log duplication** : To avoid additional log storage costs, don't run the import job with the same configuration multiple times.\n- **Storage in additional destinations** : To avoid additional log storage costs, disable routing policies at the destination project to prevent log storage in additional locations or forwarding logs to other destinations such as Pub/Sub or BigQuery.\n- **Additional CPU and memory** : If your import job times out, you might need to increase the import job CPU and memory in your [import job configuration](/architecture/import-logs-from-storage-to-logging/deployment#create-the-cloud-run-import-job) . Increasing these values might increase incurred Cloud Run costs.\n- **Additional tasks** : If the expected number of logs to be imported each day within the time range is high, you might need to increase the number of tasks in the [import job configuration](/architecture/import-logs-from-storage-to-logging/deployment#create-the-cloud-run-import-job) . The job will split the time range equally between the tasks, so each task will process a similar number of days from the range concurrently. Increasing the number of tasks might increase incurred Cloud Run costs.\n- **Storage class** : If your Cloud Storage bucket's storage class is other than Standard, such as Nearline, Durable Reduced Availability (DRA), or Coldline, you might incur additional charges.\n- **Data traffic between different locations** : Configure the import job to run in the same location as the Cloud Storage bucket from which you import the logs. Otherwise, [network egress costs](/storage/pricing#network-buckets) might be incurred.\nTo generate a cost estimate based on your projected usage, including Cloud Run jobs, use the [pricing calculator](/products/calculator) .\n### Operational efficiency\nThis section describes considerations for managing analytical queries after the solution is deployed.\nLogs are stored to the project that is defined in the [logName field](/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.log_name) of the log entry. To import the logs to the selected project, this architecture modifies the `logName` field of each imported log. The import logs are stored in the selected project's default log bucket that has the log ID `imported_logs` (unless the project has a log routing policy that changes the storage destination). The original value of the `logName` field is preserved in the [labels field](/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.labels) with the key `original_logName` .\nYou must account for the location of the original `logName` value when you query the imported logs. For more information about Log Analytics queries and samples, see [Sample SQL queries](/logging/docs/analyze/examples) .\n### Performance optimization\nIf the volume of logs that you're importing exceeds Cloud Run capacity limits, the job might time out before the import is complete. To prevent an incomplete data import, consider increasing the `tasks` value in the [import job](#create-the-cloud-run-import-job) . Increasing [CPU](/run/docs/configuring/services/cpu) and [memory](/run/docs/configuring/services/memory-limits) resources can also help improve task performance when you increase the number of tasks.\nAvoid setting a `task-timeout` value greater than `60m` ; for details, see [Using task timeouts greater than one hour](/run/docs/configuring/task-timeout#long-task-timeout) .\n## Deployment\nTo deploy this architecture, see [Deploy a job to import logs from Cloud Storage to Cloud Logging](/architecture/import-logs-from-storage-to-logging/deployment) .\n## What's Next\n- Review the implementation code in the [GitHub repository](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/logging/import-logs) .\n- Learn how to analyze imported logs by using [Log Analytics and SQL](/logging/docs/analyze/query-and-view) .\n- Learn how to [export logs](/architecture/exporting-stackdriver-logging-for-compliance-requirements) to the Cloud Storage bucket.\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .## Contributors\nAuthor: [Leonid Yankulin](https://www.linkedin.com/in/minherz) | Developer Relations Engineer\nOther contributors:\n- [Summit Tuladhar](https://www.linkedin.com/in/summitraj) | Sr. Staff Software Engineer\n- [Wilton Wong](https://www.linkedin.com/in/wiltwong) | Enterprise Architect\n- [Xiang Shen](https://www.linkedin.com/in/xiangshen07) | Cloud Solutions Architect", "guide": "Cloud Architecture Center"}