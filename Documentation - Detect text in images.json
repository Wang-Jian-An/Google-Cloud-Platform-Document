{"title": "Documentation - Detect text in images", "url": "https://cloud.google.com/distributed-cloud/hosted/docs/latest/gdch/overview", "abstract": "# Documentation - Detect text in images\nOptical Character Recognition (OCR) is one of the three Vertex AI pre-trained APIs available on Google Distributed Cloud Hosted (GDCH).\nUse the OCR feature of Vertex AI to detect text in [various file types](#supported-file-types) . Vertex AI detects typed text in a photo image or handwritten text.\n[Learn more about OCR-supported languages](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-ocr-supported-langs) detected by the text-recognition feature of OCR in a single image.\n", "content": "## Examples of files with detected text\nThe examples illustrate how Vertex AI detects and extracts text from images.\n### Road sign photograph\nFigure 1 is a photograph that contains a street or traffic sign. Vertex AI returns a JSON file with the extracted string, individual words, and their bounding boxes.\n**Figure 1.** Road sign photograph where Vertex AI detects words and their bounding boxes.\n### Scanned image of typed text\nFigure 2 is a scanned image of typed text. Vertex AI returns a JSON file containing page, block, paragraph, word, and break information.\n**Figure 2.** Scanned image of typed text where Vertex AI detects information such as words, pages, and paragraphs.\n### Image of handwriting\nFigure 3 is an image of handwritten text. Vertex AI detects and extracts text from these images. For a list of handwriting scripts that are supported for handwriting recognition, see [Handwriting scripts](#handwriting-scripts) .\n**Figure 3.** Handwriting image where Vertex AI detects text.\n## Feature differences from Google Cloud\nThis section describes the differences between OCR on Google Distributed Cloud Hosted (GDCH) and Vision and OCR on Google Cloud.\nThe primary difference is that GDCH Vision only supports OCR. Vision on GDCH doesn't provide other functionality available in Vision on Google Cloud, such as image recognition, facial recognition, and crop hint detection.\nThe following table describes the supported features in GDCH.\n| Feature     | GDCH functionality                                             |\n|:---------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| OCR API methods   | OCR on GDCH supports the following two methods: BatchAnnotateImagesBatchAnnoteFiles                             |\n| Language support   | OCR on GDCH supports a subset of the languages supported on Google Cloud.                                |\n| Asynchronous methods  | AsyncBatchAnnotateFiles                                            |\n| BatchAnnotateFiles method | The following subset of fields supported on Google Cloud are supported on GDCH: typecontentlanguage_hintsmime_typepages If you set any other fields in a request, they are ignored or cause an error. |\n| BatchAnnotateImages method | The following subset of fields supported on Google Cloud are supported on GDCH: typecontentlanguage_hints If you set any other fields in a request, they are ignored or cause an error.    |\n| File location    | In GDCH, you can only process images for OCR if they are stored locally.                                |\n## Use the API to detect text in files\nVertex AI on GDCH supports the following two methods for extracting text from files and images:\n- [BatchAnnotateImages:](#batchannotateimages) Extract text from JPEG and PNG files.\n- [BatchAnnotateFiles:](#batchannotatefiles) Extract text from PDF and TIFF files.\nVertex AI on GDCH doesn't support any other OCR API methods that are supported on Google Cloud.\n### Use the BatchAnnotateImages method\nUse the `BatchAnnotateImages` method to detect and extract text from a batch of JPEG and PNG files. Specify the following fields in the request:\n- **type** : The type of the text to extract. Specify one of two OCR types: `TEXT_DETECTION` or `DOCUMENT_TEXT_DETECTION` .\n- **content** : The images with text to detect. You can only process images that are stored locally in your GDCH environment. The system can't access images available to the public or stored in a Google Cloud bucket. Therefore, the system doesn't support them.\n- **language_hints** : Optional. List of languages to use for the `TEXT_DETECTION` or `DOCUMENT_TEXT_DETECTION` OCR types. In most cases, an empty value yields the best results, because it enables automatic language detection. For languages based on the Latin alphabet, you don't need to set the `language_hints` field. In rare cases, when you know the language of the text in the image, setting a hint improves results. Use the `language_hints` field with caution. If a hint is wrong, it can significantly impede text detection.\nThe `BatchAnnotateImages` method on GDCH supports a subset of the parameters that you can specify when you call `BachAnnotateImages` in Vertex AI on Google Cloud. If you specify any other parameters in a `BatchAnnotateImages` request on GDCH, they are ignored or result in an error.\nFor more information, see `BatchAnnotateImages` in the GDCH API reference.\n### Use the BatchAnnotateFiles method\nUse the `BatchAnnotateFiles` method to detect and extract text from a batch of PDF and TIFF files. Specify the following fields in the request:\n- **type** : The type of the text to extract. Specify one of two OCR types: `TEXT_DETECTION` or `DOCUMENT_TEXT_DETECTION` .\n- **content** : The images with text to detect. You can only process images that are stored locally in your GDCH environment. The system can't access images available to the public or stored in a Google Cloud bucket. Therefore, the system doesn't support them.\n- **language_hints** : Optional. List of languages to use for the `TEXT_DETECTION` or `DOCUMENT_TEXT_DETECTION` OCR types. In most cases, an empty value yields the best results, because it enables automatic language detection. For languages based on the Latin alphabet, you don't need to set the `language_hints` field. In rare cases, when you know the language of the text in the image, setting a hint improves results. Use the `language_hints` field with caution. If a hint is wrong, it can significantly impede text detection.\n- **mime_type** : The type of the file. You must set it to one of the following values:- `application/pdf`\n- `image/tiff`\n- **pages** : Optional. The pages of the file that are processed for text detection. The maximum number of pages that you can specify is five. If you don't specify the number of pages, the first five pages of the file are processed.\nThe `BatchAnnotateFiles` method on GDCH supports a subset of the parameters that you can specify on Google Cloud. If you specify any other parameters in a `BatchAnnotateFiles` request on GDCH, they are ignored or result in an error.\nFor more information, see `BatchAnnotateFiles` in the GDCH API reference.\n## Use the OCR client library\nThe Optical Character Recognition (OCR) API detects text on a local image by sending the contents of an image file as a base64-encoded string in the body of a request.\n### Prerequisites to use the OCR client library\nTo use the OCR client library, make sure the following prerequisites are complete:\n- Install the Vertex AI client libraries. For more information, see [Install Vertex AI client libraries](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-install-libraries) .\n- Get the OCR endpoint. Make a note of the endpoint and use it where you see `` in the following client library sample code. For more information, see [Get theOCR endpoint](#get-endpoints) .\n### Create OCR text detection requests\nTo send an OCR request using Python, first install the [Vertex AI client libraries](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-install-libraries) .\nBefore you send an OCR request using REST, replace the variable with the base64 ASCII string representation of your binary image data. This string begins with characters that look similar to the following string:- `/9j/4QAYRXhpZgAA...9tAVx/zDQDlGxn//2Q==`\nThe sample JSON request body: \n```\n{\n \"requests\": [ {\n  \"image\": {\n  \"content\": BASE64_ENCODED_IMAGE\n  },\n  \"features\": [  {\n   \"type\": \"TEXT_DETECTION\"\n  }\n  ]\n }\n ]\n}\n```\nTo send your request, choose one of the following options:Save the request body in a file named\n`request.json`\n, then run the following command:\n```\ncurl -X POST \\\n-H \"Content-Type: application/json; charset=utf-8\" \\\n-d @request.json \\\n\"OCR_ENDPOINT:v1/images:annotate\"\n```\nSave the request body in a file named\n`request.json`\n, then run the following command:\n```\n$cred = gcloud auth application-default print-access-token\n$headers = @{ \"Authorization\" = \"Bearer $cred\" }\nInvoke-WebRequest `\n -Method POST `\n -Headers $headers `\n -ContentType: \"application/json; charset=utf-8\" `\n -InFile request.json `\n -Uri \"OCR_ENDPOINT/v1/images:annotate\" | Select-Object -Expand Content\n \n```\nOCR and Optical Character Recognition are not formal product names, so don't appear in _product_names.html. However, the PM would like to treat it as a product name for GDCH with title-case in the long form. These variables enforce that for GDCH.\nTo send an OCR request using Python, first install the\n [Vertex AIclient libraries](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-install-libraries) \nand Python 3.7.\nThe Python code sample:```\nimport iofrom google.cloud import visionimport google.authfrom google.auth.transport import requestsfrom google.api_core.client_options import ClientOptions# Get your credentialsdef get_credentials():creds = Nonetry:\u00a0 credentials, project_id = google.auth.default()\u00a0 credentials = credentials.with_gdch_audience('https://OCR_ENDPOINT:443')\u00a0 req = requests.Request()\u00a0 credentials.refresh(req)except Exception as e:\u00a0 print(\"Caught exception\" + str(e))\u00a0 raise ereturn credentials# Use your credentials to access the client librarydef get_vision_client(credentials):opts = ClientOptions(api_endpoint='OCR_ENDPOINT:443')return vision.ImageAnnotatorClient(credentials= credentials, client_options=opts)# Define the function that detects text in an image filedef detect_text(path):credentials \u00a0= get_credentials()client = get_vision_client(credentials)with io.open(path, 'rb') as image_file:\u00a0 content = image_file.read()image = vision.Image(content=content)response = client.text_detection(image=image)texts = response.text_annotationsprint('Texts:')for text in texts:\u00a0 print('\\n\"{}\"'.format(text.description))\u00a0 vertices = (['({},{})'.format(vertex.x, vertex.y)\u00a0 \u00a0 for vertex in text.bounding_poly.vertices])\u00a0 print('bounds: {}'.format(','.join(vertices)))# Add error handling code here.if response.error.message:\u00a0 raise Exception(\u00a0 \u00a0 'Error handling code goes here.'# Call the \"detect_text\" function using the path to your text fileif __name__==\"__main__\": \u00a0 detect_text(\"PATH_TO_IMAGE_FILE\")\n```\n### Optional: Specify the language in a request\nThe `BatchAnnotateFiles` and `BatchAnnotateImages` methods support one or more language hints to specify the language of any text in the image. If you don't specify a language, Vertex AI enables automatic language detection, which usually results in the most accurate results. For languages based on the Latin alphabet, you don't need to set language hints. In rare cases, when the language of the text in the image is known, setting a hint improves the results. However, if the hint is incorrect, it might cause a significant impediment. Text detection returns an error if a specified language isn't one of the [supported languages](#supported-langs) .\nAdd one or more supported languages to the `imageContext.languageHints` request in the `request.json` file to provide a language hint. The following code sample is a demonstration:\n```\n{\u00a0 \"requests\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"image\": {\u00a0 \u00a0 \u00a0 \u00a0 \"content\": BASE64_ENCODED_IMAGE\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \"features\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"type\": \"DOCUMENT_TEXT_DETECTION\"\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"imageContext\": {\u00a0 \u00a0 \u00a0 \u00a0 \"languageHints\": [\"en-t-i0-handwrit\"]\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ]}\n```\n**How do language hints work?** The `languageHint` format uses the following `BCP47` language code formatting guidelines:`language` [\"-\" `script` ] [\"-\" `region` ] *(\"-\" `variant` ) *(\"-\" `extension` ) [\"-\" `privateuse` ].For example, the language hint  \" `en` - `t` - `i0` - `handwrit` \"  specifies English language ( `en` ), transform extension singleton  ( `t` ), input method engine transform extension code  ( `i0` ), and handwriting transform code ( `handwrit` ).  This roughly says the language is \"English transformed from handwriting.\"  You do not need to specify a script code because the \" `en` \"  language implies `Latn` .\n### Get the OCR endpoint\nTo get the endpoint for OCR, see [View service statuses and endpoints](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-api-status) .\n## Handwriting scripts\nThe following scripts are supported for handwriting recognition. To learn which languages use each script, refer to the language tables in this page.\n## OCR limits\nThe following table lists the current limits in Optical Character Recognition (OCR) on Google Distributed Cloud Hosted (GDCH).\n| File limit for OCR  | Value        |\n|:------------------------|:-----------------------------------|\n| Maximum number of pages | 5         |\n| Maximum file size  | 20 MB        |\n| Maximum image size  | 20 million pixels (length x width) |\nSubmitted files for OCR that exceed the maximum number of pages or the maximum file size return an error. Submitted files that exceed the maximum image size are downsized to 20 million pixels.\n## Supported file types for OCR\nThe Optical Character Recognition (OCR) pre-trained API detects and transcribes text from the following file types:\n- PDF\n- TIFF\n- JPG\n- PNG\nYou must store the files locally in your Google Distributed Cloud Hosted (GDCH) environment. You can't access files hosted in Cloud Storage or files that are publicly available for text detection.", "guide": "Documentation"}