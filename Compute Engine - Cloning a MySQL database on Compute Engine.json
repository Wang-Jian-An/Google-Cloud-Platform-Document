{"title": "Compute Engine - Cloning a MySQL database on Compute Engine", "url": "https://cloud.google.com/compute/docs/instances", "abstract": "# Compute Engine - Cloning a MySQL database on Compute Engine\nLast reviewed 2019-10-08 UTC\nThis tutorial shows two ways to clone a [MySQL database](https://wikipedia.org/wiki/MySQL) running on Compute Engine. One method uses [persistent disk snapshots](/compute/docs/disks/create-snapshots) . The other method uses native MySQL export and import, transferring the export file using [Cloud Storage](/storage) . Cloud Storage is the Google Cloud object storage service. It offers a straightforward, security-enhanced, durable, and highly available way to store files.\nCloning is the process of copying a database onto another server. The copy is independent of the source database and is preserved as a point-in-time snapshot. You can use a cloned database for various purposes without putting a load on the production server or risking the integrity of production data. Some of these purposes include the following:- Performing analytical queries.\n- Load testing or integration testing of your apps.\n- Extracting data for populating data warehouses.\n- Running experiments on the data.\nEach cloning method described in this tutorial has advantages and disadvantages. The ideal method for you depends on your situation. The following table highlights some key issues.\n| Issue             | Method 1: Disk snapshots         | Method 2: Export and import using Cloud Storage         |\n|:---------------------------------------------------------|:----------------------------------------------------------|:----------------------------------------------------------------------------------|\n| Additional disk space required on MySQL instances  | No additional disk space required       | Additional space required for storing the export file when creating and restoring |\n| Additional load on source MySQL instances during cloning | No additional load          | Additional load on CPU and I/O when creating and uploading the export file  |\n| Duration of cloning          | Relatively fast for large databases      | Relatively slow for large databases            |\n| Can clone from MySQL instances external to Google Cloud | No              | Yes                    |\n| Complexity            | A complex sequence of commands for attaching cloned disks | A relatively straightforward set of commands for cloning       |\n| Can leverage existing backup systems      | Yes, if backup system uses Google Cloud disk snapshots | Yes, if backup system exports files to Cloud Storage        |\n| Granularity of cloning         | Can clone only entire disks        | Can clone only the specified database            |\n| Data consistency           | Consistent at point of snapshot       | Consistent at point of export              |\n| Can use Cloud SQL as source        | No              | Yes, if the same version is used             |\n| Can use Cloud SQL as destination       | No              | Yes                    |\nThis tutorial assumes you're familiar with the Linux command line and MySQL database administration.", "content": "## Objectives\n- Learn how to run a MySQL database on Google Cloud.\n- Learn how to create a demo database on a secondary disk.\n- Learn how to clone a MySQL database using [Compute Engine disk snapshots](/compute/docs/disks/create-snapshots) .\n- Learn how to clone a MySQL database by transferring an export file using Cloud Storage.\n- Learn how to clone a MySQL database to Cloud SQL by transferring an export file using Cloud Storage.\n## CostsIn this document, you use the following billable components of Google Cloud:- [Compute Engine](/compute/pricing) \n- [Cloud Storage](/storage/pricing) \n- [Cloud SQL](/sql/docs/mysql/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin- Enable the Compute Engine API.\n- [  Enable the API](https://console.cloud.google.com/flows/enableapi?apiid=compute_component) \n## Setting up the environmentTo complete this tutorial, you need to set up your computing environment with the following:- A MySQL instance on Compute Engine (named`mysql-prod`) to represent your production database server.\n- An additional disk (named`mysql-prod-data`) that's attached to your production server for storing your production database.\n- A copy of the [Employees database](https://dev.mysql.com/doc/employee/en/) imported into`mysql-prod`to simulate the production database that you want to clone.\n- A MySQL instance on Compute Engine (named`mysql-test`) to represent your testing database server. You clone your database onto this server.\nThe following diagram illustrates this architecture.\n### Create the production VM instanceTo simulate a production environment, you set up a Compute Engine VM instance running MySQL on Debian Linux.\nThe VM instance for this tutorial uses two disks: a 50 GB disk for the OS and user accounts, and a 100 GB disk for database storage.\nIn Compute Engine, using separate disks offers no [performance](/compute/docs/disks#pdspecs) benefits. Disk performance is determined by the total storage capacity of all disks attached to an instance and by the total number of vCPUs on your VM instance. Therefore, the database and log file can reside on the same disk.\n **Note:** For simplicity in this tutorial, you give the VM instances' default service account full access to all Cloud APIs. In a production environment, it's best to grant access only to required Cloud APIs, or to use a specific service account with limited access.- Open Cloud Shell. [Open Cloud Shell](https://console.cloud.google.com/welcome?cloudshell=true) \n- Set your preferred zone:```\nZONE=us-east1-bREGION=us-east1gcloud config set compute/zone \"${ZONE}\"\n```\n- Create a Compute Engine instance:```\ngcloud compute instances create mysql-prod \\\u00a0 \u00a0 --machine-type=n1-standard-2 \\\u00a0 \u00a0 --scopes=cloud-platform \\\u00a0 \u00a0 --boot-disk-size=50GB \\\u00a0 \u00a0 --boot-disk-device-name=mysql-prod \\\u00a0 \u00a0 --create-disk=\"mode=rw,size=100,type=pd-standard,name=mysql-prod-data,device-name=mysql-prod-data\"\n```This command grants the instance full access to Google Cloud APIs, creates a 100 GB secondary disk, and attaches the disk to the instance. Ignore the disk performance warning because you don't need high performance for this tutorial.\n### Set up the additional diskThe second disk attached to the production instance is for storing your production database. This disk is blank, so you need to partition, format, and mount it.- In the Google Cloud console, go to the **VM instances** page. [Go to the VM instances page](https://console.cloud.google.com/compute/instances) \n- Make sure a green check mark is displayed next to the name of your `mysql-prod` instance, indicating that the instance is ready.\n- Click the **SSH** button next to the `mysql-prod` instance. The browser opens a terminal connection to the instance.\n- In the terminal window, display a list of disks attached to your instance:```\nlsblk\n```The output is the following:```\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT\nsda  8:0 0 50G 0 disk\n\u2514\u2500sda1 8:1 0 50G 0 part /\nsdb  8:16 0 100G 0 disk\n```The disk named `sdb` (100 GB) is your data disk.\n- Format the `sdb` disk and create a single partition with an ext4 file system:```\nsudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard \\\u00a0 \u00a0 /dev/sdb\n```\n- Create the MySQL data directory to be the mount point for the data disk:```\nsudo mkdir -p /var/lib/mysql\n```\n- To automatically mount the disk at the mount point you created, add an entry to the `/etc/fstab` file:```\necho \"UUID=`sudo blkid -s UUID -o value /dev/sdb` /var/lib/mysql ext4 discard,defaults,nofail 0 2\" \\\u00a0 \u00a0| sudo tee -a /etc/fstab\n```\n- Mount the disk:```\nsudo mount -av\n```\n- Remove all files from the data disk so that it's free to be used by MySQL as a data directory:```\nsudo rm -rf /var/lib/mysql/*\n```\n### Install the MySQL serverYou need to download and install MySQL Community Edition. The MySQL data directory is created on the additional disk.- In the SSH session connected to `mysql-prod` , download and install the MySQL configuration package:```\nwget http://repo.mysql.com/mysql-apt-config_0.8.13-1_all.debsudo dpkg -i mysql-apt-config_0.8.13-1_all.deb\n```\n- When you're prompted, select the **MySQL Server & Cluster option** , and then select **mysql-5.7** .\n- In the list, select the **Ok** option to complete the configuration of the package.\n- Refresh the repository cache and install the mysql-community packages:```\nsudo apt-get updatesudo apt-get install -y mysql-community-server mysql-community-client\n```\n- When you're warned that the data directory already exists, select **Ok** .\n- When you're prompted to provide a root password, create and enter a password. Note the password or store it temporarily in a safe place.\n### Download and install the sample database\n- In the SSH session connected to the `mysql-prod` instance, install git:```\nsudo apt-get install -y git\n```\n- Clone the [GitHub repository](https://github.com/datacharmer/test_db) containing the `Employees` database scripts:```\ngit clone https://github.com/datacharmer/test_db.git\n```\n- Change directory to the directory for the `Employees` database script:```\ncd test_db\n```\n- Run the `Employees` database creation script:```\nmysql -u root -p -q < employees.sql\n```When you're prompted, enter the root password that you created earlier.\n- To verify the sample database is functional, you can run a query that counts the number of rows in the `employees` table:```\nmysql -u root -p -e \"select count(*) from employees.employees;\"\n```When you're prompted, enter the root password you that you created earlier.The output is the following:```\n+----------+\n| count(*) |\n+----------+\n| 300024 |\n+----------+\n```\n### Create the test VM instanceIn this section, you create a MySQL VM instance named `mysql-test` as the destination for the cloned database. The configuration of this instance is identical to the production instance. However, you don't create a second data disk; instead, you attach the data disk later in this tutorial.- Open Cloud Shell. [Open Cloud Shell](https://console.cloud.google.com/welcome?cloudshell=true) \n- Create the test MySQL instance:```\ngcloud compute instances create mysql-test \\\u00a0 --machine-type=n1-standard-2 \\\u00a0 --scopes=cloud-platform \\\u00a0 --boot-disk-size=50GB \\\u00a0 --boot-disk-device-name=mysql-test\n```You can ignore the disk performance warning because you don't need high performance for this tutorial.\n### Install the MySQL server on the test VM instanceYou also need to download and install MySQL Community Edition onto the `mysql-test` VM instance.- In the SSH session connected to `mysql-test` , download and install the MySQL configuration package:```\nwget http://repo.mysql.com/mysql-apt-config_0.8.13-1_all.debsudo dpkg -i mysql-apt-config_0.8.13-1_all.deb\n```\n- When you're prompted, select the **MySQL Server & Cluster option** , and then select **mysql-5.7** .\n- In the list, select the **Ok** option to complete the configuration of the package.\n- Refresh the repository cache and install the mysql-community packages:```\nsudo apt-get updatesudo apt-get install -y mysql-community-server mysql-community-client\n```\n- When you're prompted to provide a root password, create and enter a password. Note the password or store it temporarily in a safe place.\n## Cloning the database using Compute Engine disk snapshotsOne way to clone a MySQL database running on Compute Engine is to store the database on a separate data disk and use persistent disk snapshots to create a clone of that disk.\n [Persistent disk snapshots](/compute/docs/disks/create-snapshots) let you get a point-in-time copy of on-disk data. Scheduling disk snapshots is one way to automatically back up your data.\nIn this section of the tutorial, you do the following:- Take a snapshot of the production server's data disk.\n- Create a new disk from the snapshot.\n- Mount the new disk onto the test server.\n- Restart the MySQL server on the test instance so that the server uses the new disk as a data disk.\nThe following diagram shows how a database is cloned by using disk snapshots. **Note:** For simplicity in this tutorial, you create the production and test VM instances in the same project. In a production environment, it's likely these instances would be in separate projects. Disk snapshots can be shared between projects using the [Google Cloud CLI or API](/compute/docs/images/sharing-images-across-projects) .\n### Create the disk snapshot\n- Open Cloud Shell. [Open Cloud Shell](https://console.cloud.google.com/welcome?cloudshell=true) \n- Create a snapshot of your data disk in the same zone as the VM instance:```\ngcloud compute disks snapshot mysql-prod-data \\\u00a0 \u00a0 \u00a0--snapshot-names=mysql-prod-data-snapshot \\\u00a0 \u00a0 \u00a0--zone=\"${ZONE}\"\n```After a few minutes, your snapshot is created.\n### Attach the disk snapshot to the test instanceYou need to create a new data disk from the snapshot you created and then attach it to the `mysql-test` instance.- Open Cloud Shell. [Open Cloud Shell](https://console.cloud.google.com/welcome?cloudshell=true) \n- Create a new persistent disk by using the snapshot of the production disk for its contents:```\ngcloud beta compute disks create mysql-test-data \\\u00a0 \u00a0 \u00a0--size=100GB \\\u00a0 \u00a0 \u00a0--source-snapshot=mysql-prod-data-snapshot \\\u00a0 \u00a0 \u00a0--zone=\"${ZONE}\"\n```\n- Attach the new disk to your `mysql-test` instance with read-write permissions:```\ngcloud compute instances attach-disk mysql-test \\\u00a0 \u00a0 --disk=mysql-test-data --mode=rw\n```\n### Mount the new data disk in LinuxTo use the cloned data disk as the MySQL data directory, you need to stop the MySQL instance and mount the disk.- In the SSH session connected to `mysql-test` , stop the MySQL service:```\nsudo service mysql stop\n```\n- In the terminal window, display a list of disks attached to your instance:```\nlsblk\n```The output is the following:```\nNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT\nsda  8:0 0 50G 0 disk\n\u2514\u2500sda1 8:1 0 50G 0 part /\nsdb  8:16 0 100G 0 disk\n```The disk named `sdb` (100 GB) is your data disk.\n- Mount the MySQL data disk onto the MySQL data directory:```\nsudo mount -o discard,defaults /dev/sdb /var/lib/mysql\n```Mounting this disk hides any MySQL configuration files and tablespaces, replacing them with the contents of the disk.With this command, the disk is temporarily mounted and is not remounted on system boot. If you want to mount the disk on system boot, create an `fstab` entry. For more information, see [Set up the additional disk](#set_up_the_additional_disk) earlier in this tutorial. **Note:** For this tutorial, the `mysql-prod` and `mysql-test` instances use the same Unix user ID for the `mysql` user. Different user IDs might be used in a production environment. If the user IDs differ, you need to change the owner to `mysql` on all the files and directories in the data disk by running the following command: `chown -hR mysql.mysql /var/lib/mysql`\n### Start MySQL in the test instance\n- In the SSH session connected to `mysql-test` , start the MySQL service:```\nsudo service mysql start\n```\n- To verify that the cloned database is functional, run a query that counts the number of rows in the `employees` table:```\nmysql -u root -p -e \"select count(*) from employees.employees;\"\n```When you're prompted, enter the root password of the `mysql-prod` database server. The production instance root password is required because the entire MySQL data directory is a clone of the data directory of the `mysql-prod` instance, so all the databases, database users, and their passwords are copied.```\n+----------+\n| count(*) |\n+----------+\n| 300024 |\n+----------+\n```The number of rows is the same as on the `mysql-prod` instance.\nNow that you have seen how to clone a database using persistent disk snapshots, you might want to try cloning a database by using export and import. To complete the tutorial for this second approach, you must unmount the cloned disk.\n### Unmount the cloned diskTo unmount the cloned disk that you created by using disk snapshots, perform the following steps:- In the SSH session connected to your `mysql-test` instance, stop the MySQL service:```\nsudo service mysql stop\n```\n- Unmount the cloned data disk from the MySQL data directory:```\nsudo umount /var/lib/mysql\n```\n- Restart the MySQL service:```\nsudo service mysql start\n```\n## Cloning using export and importA second method of cloning a MySQL database running on Compute Engine is to use native MySQL export (using `mysqldump` ) and import. With this approach, you transfer the export file by using Cloud Storage.\nThis section of the tutorial uses resources that you created in the [Cloning the database using Compute Engine disk snapshots](#cloning_the_database_using_compute_engine_disk_snapshots) section of this tutorial. If you didn't complete that section, you must do so before continuing.\nIn this section of the tutorial, you do the following:- Create a Cloud Storage bucket.\n- Export the database on the production instance, writing it to Cloud Storage.\n- Import the export file into the test instance, reading it from Cloud Storage.\nThe following diagram shows how a database is cloned by transferring an export using Cloud Storage.Because systems outside of Google Cloud can be given access to Cloud Storage, you can use this approach to clone databases from external MySQL instances.\n### Create a Cloud Storage bucketYou need to create a Cloud Storage bucket that stores the export files while you transfer them from the `mysql-prod` instance to the `mysql-test` instance.- Open Cloud Shell. [Open Cloud Shell](https://console.cloud.google.com/welcome?cloudshell=true) \n- Create a Cloud Storage bucket in the same region as your VM instances:```\ngsutil mb -l \"${REGION}\" \"gs://$(gcloud config get-value project)-bucket\"\n```\n### Export the databaseIn your production environment, you might already make backups using `mysqldump` export files. You can use these backups as a base for cloning your database.\nIn this tutorial, you make a new export file by using `mysqldump` , which doesn't impact any existing full or incremental backup schedules.- In the SSH session connected to the `mysql-prod` instance, export the `Employees` database, streaming it into a Cloud Storage object in the bucket that you created earlier:```\nmysqldump --user=root -p --default-character-set=utf8mb4 --add-drop-database --verbose \u00a0--hex_blob \\\u00a0 \u00a0 --databases employees |\\\u00a0 \u00a0 \u00a0gsutil cp - \"gs://$(gcloud config get-value project)-bucket/employees-dump.sql\"\n```When you're prompted, enter the root password of the `mysql-prod` database server.You use the `utf8mb4` character set in the export to avoid any character encoding issues.The `--add-drop-database` option is used so that `DROP DATABASE` and `CREATE DATABASE` statements are included in the export.\n### Import the exported file\n- In the SSH session connected to the `mysql-test` instance, stream the exported file from your Cloud Storage bucket into the `mysql` command-line application:```\ngsutil cat \"gs://$(gcloud config get-value project)-bucket/employees-dump.sql\" |\\\u00a0 \u00a0 mysql --user=root -p --default-character-set=utf8mb4\n```When you're prompted, enter the root password of the `mysql-test` database server.You use the `utf8mb4` character set in the import to avoid any character encoding issues.\n- To verify that the cloned database is functional, run a query that counts the number of rows in the `employees` table:```\nmysql -u root -p -e \"select count(*) from employees.employees;\"\n```When you're prompted, enter the root password of the `mysql-test` database server.```\n+----------+\n| count(*) |\n+----------+\n| 300024 |\n+----------+\n```The number of rows is the same as on the `mysql-prod` instance.\n## Using Cloud SQL as the cloning destinationIf your destination database is hosted on Cloud SQL, and the origin database is on Compute Engine, then the only supported mechanism for cloning is by exporting the database to Cloud Storage, and then importing the database into Cloud SQL.\nAs [explained in the documentation for Cloud SQL](https://cloud.google.com/sql/docs/mysql/import-export/creating-sqldump-csv) , Cloud SQL can only import the exported file when it does not contain any triggers, stored procedures, views, or functions.\nIf your database relies on any of these elements, you must exclude them from the export by using the `--skip-triggers` and `--ignore-table [VIEW_NAME]` command-line arguments, and then manually recreate them after importing.\n### Create a Cloud SQL for MySQL instance\n- Open Cloud Shell. [Open Cloud Shell](https://console.cloud.google.com/welcome?cloudshell=true) \n- Create a Cloud SQL for MySQL instance running the same database version as your `mysql-prod` instance:```\ngcloud sql instances create mysql-cloudsql \\\u00a0 \u00a0 --tier=db-n1-standard-2 --region=${REGION} --database-version MYSQL_5_7\n```After a few minutes, your Cloud SQL database is created.\n- Reset the root user password to a known value:```\ngcloud sql users set-password root \\\u00a0 \u00a0 --host=% --instance=mysql-cloudsql \u00a0--prompt-for-password\n```When you're prompted to provide a root password, create and enter a password. Note the password or store it temporarily in a safe place.\n### Export the databaseTo export the database in a format suitable for importing into Cloud SQL, you need to exclude any views in the database.- In the SSH session connected to the `mysql-prod` instance, set an environment variable containing a set of command-line arguments for the `mysqldump` command so that it ignores the views in the `Employees` database:```\nDATABASE_NAME=employeesIGNORE_TABLES_ARGS=\"`mysql -u root -p -s -s -e \\\"\u00a0 \u00a0 SELECT CONCAT('--ignore-table ${DATABASE_NAME}.',TABLE_NAME)\u00a0 \u00a0 FROM information_schema.TABLES\u00a0 \u00a0 WHERE TABLE_TYPE LIKE 'VIEW' AND TABLE_SCHEMA = '${DATABASE_NAME}';\u00a0 \u00a0 \\\"`\"\n```When you're prompted, enter the root password of the `mysql-prod` database server.\n- View the variable contents to verify that they were set correctly:```\necho \"${IGNORE_TABLES_ARGS}\"\n``````\n--ignore-table employees.current_dept_emp\n--ignore-table employees.dept_emp_latest_date\n```\n- Export the `Employees` database, excluding triggers and views, streaming it directly into a Cloud Storage object in the bucket that you created earlier:```\nmysqldump --user=root -p --default-character-set=utf8mb4 --add-drop-database --verbose \\\u00a0 \u00a0 --hex-blob --skip-triggers --set-gtid-purged=OFF \\\u00a0 \u00a0 $IGNORE_TABLES_ARGS \\\u00a0 \u00a0 --databases employees |\\\u00a0 \u00a0 gsutil cp - \"gs://$(gcloud config get-value project)-bucket/employees-cloudsql-import.sql\"\n```When you're prompted, enter the root password of the `mysql-prod` database server.\n### Update object permissionsThe correct permissions need to be set on both the Cloud Storage bucket and the export object so that the Cloud SQL service account is able to read them. These permissions are set automatically when you use the Google Cloud console to import the object, or they can be set by using `gcloud` commands.- Open Cloud Shell. [Open Cloud Shell](https://console.cloud.google.com/welcome?cloudshell=true) \n- Set an environment variable containing the address of the service account of your Cloud SQL instance:```\nCLOUDSQL_SA=\"$(gcloud sql instances describe mysql-cloudsql --format='get(serviceAccountEmailAddress)')\"\n```\n- Add the service account to the bucket ACL as a writer, and to the export object as a reader:```\ngsutil acl ch -u \"${CLOUDSQL_SA}\":W \"gs://$(gcloud config get-value project)-bucket/\"gsutil acl ch -u \"${CLOUDSQL_SA}\":R \\\u00a0 \u00a0 \"gs://$(gcloud config get-value project)-bucket/employees-cloudsql-import.sql\"\n```\n### Import the exported database\n- Open Cloud Shell. [Open Cloud Shell](https://console.cloud.google.com/welcome?cloudshell=true) \n- Import the exported file into your Cloud SQL instance:```\ngcloud sql import sql mysql-cloudsql \\\u00a0 \u00a0 \"gs://$(gcloud config get-value project)-bucket/employees-cloudsql-import.sql\"\n```When prompted, enter `y` .\n- To verify that the cloned database is functional, run a query that counts the number of rows in the `employees` table:```\necho \"select count(*) from employees.employees;\" |\\\u00a0 \u00a0 gcloud sql connect mysql-cloudsql --user=root\n```When prompted, enter the root password of the `mysql-cloudsql` database server.The output is the following:```\nConnecting to database with SQL user [root].Enter password:\ncount(*)\n300024\n```The number of rows is the same as on the `mysql-prod` instance. **Note:** The imported database will not contain any triggers, stored procedures, views, or functions from the original database. If these are required, you must recreate them manually.\n## Additional information for production systems\n### Using disk snapshotsFor physical backups (such as disk snapshots), the MySQL documentation [recommends](https://dev.mysql.com/doc/refman/5.7/en/backup-methods.html) that you pause writes to the database before you take a snapshot. You do this by using the [FLUSH TABLES WITH READ LOCK](https://dev.mysql.com/doc/refman/5.7/en/flush.html#flush-tables-with-read-lock) command. When the snapshot is complete, you can use `UNLOCK TABLES` to restart writes.\nFor databases that use InnoDB tables, we recommend that you take the snapshot directly without first executing the `FLUSH TABLES WITH READ LOCK` command. This allows the database to stay running without any ill effects, but the snapshot might be in an inconsistent state. However, if this occurs, the InnoDB engine can rebuild the tables to a consistent state when the clone starts up.\nFor databases that use MyISAM tables, executing the `FLUSH TABLES WITH READ LOCK` command blocks all writes to the tables, making your database read-only until you run the `UNLOCK TABLES` command.\nIf you take a snapshot first flushing and locking the tables, there is a risk that the newly cloned database will contain inconsistent data, or will be corrupted.\nTherefore, to get a consistent snapshot on databases using MyISAM tables, we recommend that you run `FLUSH TABLES WITH READ LOCK` on a read replica and take a snapshot of that replica so that the performance of the primary (master) database is not affected.\n### Using the mysqldump commandIn order to create an export file that's consistent with the source database, the `mysqldump` command locks all the tables during the export operation. This means that writes to the database are blocked while the database is being exported.\nWe therefore recommend that you run the `mysqldump` command against a read replica of the primary database so that the primary is not blocked.## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this tutorial, you can delete the Google Cloud project that you created for this tutorial.- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Learn how to [monitor your slow queries in MySQL with Cloud Monitoring](/community/tutorials/stackdriver-monitor-slow-query-mysql) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Compute Engine"}