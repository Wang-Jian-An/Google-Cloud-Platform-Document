{"title": "Compute Engine - Configuring a SQL Server failover cluster instance that uses persistent disks in multi-writer mode", "url": "https://cloud.google.com/compute/docs/instances", "abstract": "# Compute Engine - Configuring a SQL Server failover cluster instance that uses persistent disks in multi-writer mode\nMicrosoft SQL Server [Always On Failover Cluster Instances](https://docs.microsoft.com/en-us/sql/sql-server/failover-clusters/windows/always-on-failover-cluster-instances-sql-server?view=sql-server-ver15) (FCI) let you run a single SQL Server instance across multiple Windows Server Failover Cluster (WSFC) nodes. At any point in time, one of the cluster nodes actively hosts the SQL instance. In the event of a failure, WSFC automatically transfers ownership of the instance's resources to another node.\nSQL Server FCI requires data to be located on shared storage so that it can be accessed across all WSFC nodes. This guide describes how you can deploy a SQL Server 2022 failover cluster instance and use persistent disks in [multi-writer mode](/compute/docs/disks#pdspecs_rw) as shared storage.\nIn an on-premises environment, you can let WSFC perform [ARP announcements](https://en.wikipedia.org/wiki/Address_Resolution_Protocol#ARP_announcements) if a failover occurs to [notify network equipment](https://learn.microsoft.com/en-us/troubleshoot/windows-server/virtualization/mac-address-changes-for-virtual-server) about an IP address change. Google Cloud, however, disregards ARP announcements. Consequently, you must implement one of the following two options:- Internal load balancer (see [Running Windows Server Failover Clustering](/compute/docs/tutorials/running-windows-server-failover-clustering#understanding_the_network_routing) )\n- Distributed network name (DNN) (see [Configure a DNN for failover cluster instance](https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/failover-cluster-instance-distributed-network-name-dnn-configure?view=azuresql) )\nThe article assumes that you have already deployed Active Directory on Google Cloud and that you have basic knowledge of SQL Server, Active Directory, and Compute Engine.", "content": "## Objectives\n- Deploy a WSFC comprising two SQL Server VM instances and a third VM instance that acts as a file share witness.\n- Deploy a SQL Server FCI on the WSFC.\n- Configure a load balancer or distributed network name (DNN) to route traffic to your availability group with SQL Server.\n- Verify that the cluster is working by simulating a failover.\n## CostsThis tutorial uses billable components of Google Cloud, including:- [Compute Engine](/compute/vm-instance-pricing) \n- [Cloud Load Balancing](/vpc/network-pricing#lb) **(not necessary with a DNN configuration)** \nUse the [pricing calculator](/products/calculator) to generate a cost estimate based on your projected usage.## Before you beginTo complete this guide, you need the following:- An Active Directory domain with at least one domain controller. You can create an Active Directory domain [by using Managed Microsoft AD](/managed-microsoft-ad/docs/create-domain) . Alternatively, you can deploy a [custom Active Directory environment on Compute Engine](/solutions/deploy-fault-tolerant-active-directory-environment) and [set up a private DNS forwarding zone](/compute/docs/instances/windows/best-practices#use_cloud_dns_private_forwarding_zones) that forwards DNS queries to your domain controllers.\n- An Active Directory user that has permission to join computers to the domain and can log in by using RDP. If you're using Managed Microsoft AD, you can use the`setupadmin`user.\n- A Google Cloud project and VPC with connectivity to your Active Directory domain controllers.\n- A subnet to use for the WSFC VM instances.\nMake sure you've reviewed the [current restrictions](/compute/docs/disks/sharing-disks-between-vms#restrictions) of persistent disks in multi-writer mode and select a zone in which persistent disks in multi-writer mode are available.\nTo complete the guide, you also need a Google Cloud project:When you finish this tutorial, you can avoid continued billing by deleting the resources you created. For more information, see [Cleaning up](#clean-up) .## Preparing the project and networkTo prepare your Google Cloud project and VPC for the deployment of SQL Server FCI, do the following:- In the Google Cloud console, open [Cloud Shell](/shell/docs/starting-cloud-shell) by clicking the **Activate Cloud Shell** button. [Go to the Google Cloud console](https://console.cloud.google.com/) \n- Initialize the following variables:```\nVPC_NAME=VPC_NAME\nSUBNET_NAME=SUBNET_NAME\n```Where:- ``: name of your VPC\n- ``: name of your subnet\n- Set your default [project ID](/resource-manager/docs/creating-managing-projects) :```\ngcloud config set project PROJECT_ID\n```Replace `` with the ID of your Google Cloud project.\n- Set your default zone:```\ngcloud config set compute/zone ZONE\n```Replace `` with the ID of the zone you want to deploy in.\n### Create firewall rulesTo allow clients to connect to SQL Server, allow communication between the WSFC nodes, and to enable the load balancer to [perform health checks](/load-balancing/docs/health-checks#fw-rule) , you need to create several firewall rules. To simplify the creation of these firewall rules, you use [network tags](/vpc/docs/add-remove-network-tags) :- The 2 WSFC nodes are annotated with the`wsfc-node`tag.\n- All servers (including the witness) are annotated with the`wsfc`tag.\nCreate firewall rules that use these network tags:- Return to your existing Cloud Shell session.\n- Create firewall rules for the WSFC nodes:```\nSUBNET_CIDR=$(gcloud compute networks subnets describe $SUBNET_NAME --format=value\\('ipCidrRange'\\))\ngcloud compute firewall-rules create allow-all-between-wsfc-nodes \\\n --direction=INGRESS \\\n --action=allow \\\n --rules=tcp,udp,icmp \\\n --enable-logging \\\n --source-tags=wsfc \\\n --target-tags=wsfc \\\n --network=$VPC_NAME \\\n --priority 10000\ngcloud compute firewall-rules create allow-sql-to-wsfc-nodes \\\n --direction=INGRESS \\\n --action=allow \\\n --rules=tcp:1433 \\\n --enable-logging \\\n --source-ranges=$SUBNET_CIDR \\\n --target-tags=wsfc-node \\\n --network=$VPC_NAME \\\n --priority 10000\n```\n- Create a firewall rule that allows health checks from the [IP ranges of the Google Cloud probers](/load-balancing/docs/health-check-concepts#ip-ranges) : **(not necessary with a DNN configuration)** ```\ngcloud compute firewall-rules create allow-health-check-to-wsfc-nodes \\\n --direction=INGRESS \\\n --action=allow \\\n --rules=tcp \\\n --source-ranges=130.211.0.0/22,35.191.0.0/16 \\\n --target-tags=wsfc-node \\\n --network=$VPC_NAME \\\n --priority 10000\n```\n **Note:** Depending on how you've deployed Active Directory, you might need to create additional firewall rules to allow servers to join the domain. See [Accessing Managed Microsoft AD from within your VPC](/managed-microsoft-ad/docs/firewalls#accessing_from_within_your_vpc) for further details.\n### Create VM instancesYou now deploy two VM instances for the failover cluster. At any point in time, only one of these VMs serves as the active FCI node while the other node serves as the failover node. The two VM instances must:- be located in the same zone so that they can access the same persistent disks.\n- have Windows Server Failover Clustering and SQL Server installed.\n- have [Compute Engine WSFC support](https://github.com/GoogleCloudPlatform/guest-agent#windows-failover-cluster-support) enabled.\nYou use a [SQL Server premium image](/compute/docs/images/os-details#sql_server) which has SQL Server 2022 preinstalled.\n **Note:** If you plan to bring your own licenses for SQL Server by using the [license mobility](/compute/docs/instances/windows/ms-licensing) program, select Windows Server base images for these nodes and install SQL Server using your own product keys.\nTo provide a tie-breaking vote and achieve a quorum for the failover scenario, you deploy a third VM that serves as a [file share witness](https://docs.microsoft.com/en-us/windows-server/failover-clustering/manage-cluster-quorum#quorum-configuration-options) .- Return to your existing Cloud Shell session.\n- Create a [spread placement policy](/compute/docs/instances/use-spread-placement-policies#create-spread-policy) that spreads the WSFC nodes across the underlying data center infrastructure so that they do not share the same host or power system:```\nZONE=$(gcloud config get-value compute/zone)\ngcloud compute resource-policies create group-placement spread-placement \\\n --availability-domain-count 2 \\\n --region ${ZONE::-2}\n```\n- Create a [specialize script](/compute/docs/startupscript#providing_a_startup_script_for_windows_instances) for the WSFC nodes. The script installs the necessary Windows feature and creates firewall rules for WSFC and SQL Server:```\ncat << \"EOF\" > specialize-node.ps1\n$ErrorActionPreference = \"stop\"\n# Install required Windows features\nInstall-WindowsFeature Failover-Clustering -IncludeManagementTools\nInstall-WindowsFeature RSAT-AD-PowerShell\n# Open firewall for WSFC\nnetsh advfirewall firewall add rule name=\"Allow SQL Server health check\" dir=in action=allow protocol=TCP localport=59997\n# Open firewall for SQL Server\nnetsh advfirewall firewall add rule name=\"Allow SQL Server\" dir=in action=allow protocol=TCP localport=1433\nEOF\n```\n- Create the VM instances and enable the Windows Server Failover Clustering agent on the WSFC nodes by setting the metadata key `enable-wsfc` to `true` :```\nMACHINE_TYPE=n2-standard-8\ngcloud compute instances create node-1 \\\n --resource-policies spread-placement \\\n --machine-type $MACHINE_TYPE \\\n --subnet $SUBNET_NAME \\\n --image-family sql-ent-2022-win-2022 \\\n --image-project windows-sql-cloud \\\n --tags wsfc,wsfc-node \\\n --boot-disk-size 50 \\\n --boot-disk-type pd-ssd \\\n --boot-disk-device-name \"node-1\" \\\n --metadata enable-wsfc=true \\\n --metadata-from-file=sysprep-specialize-script-ps1=specialize-node.ps1\ngcloud compute instances create node-2 \\\n --resource-policies spread-placement \\\n --machine-type $MACHINE_TYPE \\\n --subnet $SUBNET_NAME \\\n --image-family sql-ent-2022-win-2022 \\\n --image-project windows-sql-cloud \\\n --tags wsfc,wsfc-node \\\n --boot-disk-size 50 \\\n --boot-disk-type pd-ssd \\\n --boot-disk-device-name \"node-2\" \\\n --metadata enable-wsfc=true \\\n --metadata-from-file=sysprep-specialize-script-ps1=specialize-node.ps1\ngcloud compute instances create \"witness\" \\\n --machine-type n2-standard-2 \\\n --subnet $SUBNET_NAME \\\n --image-family=windows-2022 \\\n --image-project=windows-cloud \\\n --tags wsfc \\\n --boot-disk-size 50 \\\n --boot-disk-type pd-ssd \\\n --metadata sysprep-specialize-script-ps1=\"add-windowsfeature FS-FileServer\"\n``` **Note:** Depending on your [performance requirements](/compute/docs/disks/performance#optimize_disk_performance) , consider using a machine type larger than `n2-standard-8` for the WSFC nodes.\n- To join the 3 VM instances to Active Directory, do the following for each of the 3 VM instances:- Monitor the initialization process of the VM by viewing its serial port output:```\ngcloud compute instances tail-serial-port-output NAME\n```Replace `` with the name of the VM instance.Wait about 3 minutes until you see the output `Instance setup finished` , then press Ctrl+C. At this point, the VM instance is ready to be used.\n- [Create a username and password](/compute/docs/instances/windows/creating-passwords-for-windows-instances) for the VM instance\n- [Connect to the VM by using Remote Desktop](/compute/docs/instances/connecting-to-windows) and log in using the username and password created in the previous step.\n- Right-click the **Start** button (or press **Win+X** ) and click **Windows PowerShell (Admin)** .\n- Confirm the elevation prompt by clicking **Yes** .\n- Join the computer to your Active Directory domain and restart:```\nAdd-Computer -Domain DOMAIN -Restart\n```Replace `` with the DNS name of your Active Directory domain.Wait for approximately 1 minute for the restart to complete.\n### Create persistent disks in multi-writer modeYou now create 3 persistent disks in multi-writer mode and attach each of them to both WSFC nodes.- Return to your existing Cloud Shell session.\n- Create 3 shared persistent disks:```\nPD_SIZE=50\ngcloud beta compute disks create datadisk-1 \\\n --size $PD_SIZE \\\n --type pd-ssd \\\n --multi-writer \\\n --zone $(gcloud config get-value compute/zone)\ngcloud beta compute disks create datadisk-2 \\\n --size $PD_SIZE \\\n --type pd-ssd \\\n --multi-writer \\\n --zone $(gcloud config get-value compute/zone)\ngcloud beta compute disks create datadisk-3 \\\n --size $PD_SIZE \\\n --type pd-ssd \\\n --multi-writer \\\n --zone $(gcloud config get-value compute/zone)\n``` **Note:** For the purpose of this tutorial, and to fit within the default regional SSD persistent disk quota, the size of the disks attached to each VM is smaller than it would be in a production environment.\n- Attach the disks to `node-1` :```\ngcloud compute instances attach-disk node-1 --disk datadisk-1\ngcloud compute instances attach-disk node-1 --disk datadisk-2\ngcloud compute instances attach-disk node-1 --disk datadisk-3\n```\n- Attach the disks to `node-2` :```\ngcloud compute instances attach-disk node-2 --disk datadisk-1\ngcloud compute instances attach-disk node-2 --disk datadisk-2\ngcloud compute instances attach-disk node-2 --disk datadisk-3\n```\n### Reserve cluster IP addressesYou now reserve two static IP addresses in your VPC. One IP address is used as the WSFC cluster IP address, the other is used by the internal load balancer.- Reserve a static IP for the internal load balancer and capture the address in a new environment variable named `LOADBALANCER_ADDRESS` :```\ngcloud compute addresses create wsfc \\\n --region $(gcloud config get-value compute/zone | rev | cut -c 3- | rev) \\\n --subnet $SUBNET_NAME\nLOADBALANCER_ADDRESS=$(gcloud compute addresses describe wsfc \\\n --region $(gcloud config get-value compute/zone | rev | cut -c 3- | rev) \\\n --format=value\\(address\\)) && \\\necho \"Load Balancer IP: $LOADBALANCER_ADDRESS\"\n```Note the IP address, you need it later.\n- Reserve another static IP address that you use as cluster IP: **(not necessary with a DNN configuration)** ```\ngcloud compute addresses create wsfc-cluster \\\n --subnet $SUBNET_NAME \\\n --region $(gcloud config get-value compute/zone | rev | cut -c 3- | rev) && \\\nCLUSTER_ADDRESS=$(gcloud compute addresses describe wsfc-cluster \\\n --region $(gcloud config get-value compute/zone | rev | cut -c 3- | rev) \\\n --format=value\\(address\\)) && \\\necho \"Cluster IP: $CLUSTER_ADDRESS\"\n```Note the IP address, you need it later.\nYour project and VPC are now ready for the deployment of the WSFC and SQL Server.\n### Create a witness file shareTo prepare `witness` to serve as file share witness, create a file share and grant yourself and the two WSFC nodes access to the file share:- Connect to`witness` [by using Remote Desktop](/compute/docs/instances/connecting-to-windows) . Sign in with your domain user account.\n- Right-click the **Start** button (or press **Win+X** ) and click **Windows PowerShell (Admin)** .\n- Confirm the elevation prompt by clicking **Yes** .\n- Create the witness folder and share the folder:```\nNew-Item \"C:\\QWitness\" \u2013type directory\nicacls C:\\QWitness\\ /grant 'node-1$:(OI)(CI)(M)'\nicacls C:\\QWitness\\ /grant 'node-2$:(OI)(CI)(M)'\nNew-SmbShare `\n -Name QWitness `\n -Path \"C:\\QWitness\" `\n -Description \"SQL File Share Witness\" `\n -FullAccess $env:username,node-1$,node-2$\n```\n## Deploying the failover clusterYou now use the VM instances to deploy a WSFC and SQL Server.\n### Deploy WSFCYou are now ready to create the failover cluster:- Connect to`node-1` [by using Remote Desktop](/compute/docs/instances/connecting-to-windows) . Sign in with your domain user account.\n- Right-click the **Start** button (or press **Win+X** ) and click **Windows PowerShell (Admin)** .\n- Confirm the elevation prompt by clicking **Yes** .\n- Create a new cluster:- For load balancer configuration\n```\nNew-Cluster `\n -Name sql-cluster `\n -Node node-1,node-2 `\n -NoStorage `\n -StaticAddress CLUSTER_ADDRESS\n```Replace `` with the cluster IP address that you created earlier.- For DNN configuration\n```\nNew-Cluster `\n -Name sql-cluster `\n -Node node-1,node-2 `\n -NoStorage `\n -ManagementPointNetworkType Distributed\n```\n- Return to the PowerShell session on `witness` and grant the virtual computer object of the cluster permission to access the file share:```\nicacls C:\\QWitness\\ /grant 'sql-cluster$:(OI)(CI)(M)'\nGrant-SmbShareAccess `\n -Name QWitness `\n -AccountName 'sql-cluster$' `\n -AccessRight Full `\n -Force\n```\n- Return to the PowerShell session on `node-1` and configure the cluster to use the file share on `witness` as a cluster quorum:```\nSet-ClusterQuorum -FileShareWitness \\\\witness\\QWitness\n```\n- Verify that the cluster was created successfully:```\nTest-Cluster\n```You might see some warnings that can be safely ignored:```\nWARNING: System Configuration - Validate All Drivers Signed: The test reported some warnings..\nWARNING: Network - Validate Network Communication: The test reported some warnings..\nWARNING:\nTest Result:\nHadUnselectedTests, ClusterConditionallyApproved\nTesting has completed for the tests you selected. You should review the warnings in the Report. A cluster solution is\nsupported by Microsoft only if you run all cluster validation tests, and all tests succeed (with or without warnings).\n```You can also launch the Failover Cluster Manager MMC snap-in to review the cluster's health by running `cluadmin.msc` .\n- If you're using Managed AD, add the computer account used by WSFC to the **Cloud Service Domain Join Accounts** group so that it can join computers to the domain:```\nAdd-ADGroupMember `\n -Identity \"Cloud Service Domain Join Accounts\" `\n -Members sql-cluster$\n```\n### Create a storage poolYou now create a storage pool that combines the three persistent disks that you created earlier, and use the storage pool to create a cluster shared volume:- Return to the PowerShell session on`node-1`.\n- Create a new storage pool that uses the 3 persistent disks:```\n$NodeName = [System.Net.Dns]::GetHostName()\n$ClusterDisks = Get-PhysicalDisk -CanPool $True |\n Where-Object { ($_ |\n Get-PhysicalDiskStorageNodeView |\n Select-Object -Property StorageNodeObjectId) -like ('*' + $NodeName + '*') }\n$Pool = New-StoragePool `\n -StorageSubsystemFriendlyName 'Clustered*' `\n -FriendlyName FciPool `\n -PhysicalDisks $ClusterDisks `\n -ResiliencySettingNameDefault Simple `\n -Verbose\n```\n- On the storage pool, create a new volume that uses the cluster shared volume versions of ReFS and a 64 KB cluster size:```\n$Pool | New-Volume `\n -FriendlyName FciVolume `\n -FileSystem CSVFS_ReFS `\n -Size 100GB `\n -AllocationUnitSize 65536\n```The volume automatically shows up on `node-2` since the underlying persistent disks are attached to both VM instances.\n- Open the **Failover Cluster Manager** MMC snap-in:```\ncluadmin.msc\n```\n- In the left window pane, navigate to **Failover Cluster Manager > sql-cluster > Storage > Disks** .\n- Right-click **Cluster Virtual Disk (FciVolume)** and select **Remove From Cluster Shared Volumes** .\n- Select **Cluster Virtual Disk (FciVolume)** .\n- In the **Volumes** tab at the bottom, right-lick the volume and select **Change drive letter** .\n- Select the drive letter **D** and click **OK** .\n### Testing storage pool failoverOptionally, you can now test whether the storage pool failover works properly:- Connect to`node-2` [by using Remote Desktop](/compute/docs/instances/connecting-to-windows) . Sign in with your domain user account.\n- Right-click the **Start** button (or press **Win+X** ) and select **Run** \n- Enter`cluadmin.msc`and select **OK** .\n- In the left window pane, navigate to **Failover Cluster Manager > sql-cluster > Storage > Pools** .You should see a pool named **Cluster Pool 1** with **Owner node** set to `node-1` .\n- Return to Cloud Shell and reset `node-1` VM to simulate a failover:```\ngcloud compute instances reset node-1\n```\n- Return to the **Failover Cluster Manager** on `node-2` .\n- Observe the status of the storage pool by repeatedly pressing **F5** to refresh the view.After about 30 seconds, the **owner node** should automatically switch to `node-2` .\n### Remove the default SQL Server installationYou now remove the default SQL Server installation from the two nodes and replace it with a new FCI configuration.\nFor each of the two WSFC nodes, `node-1` and `node-2` , perform the following steps:- Right-click the **Start** button (or press **Win+X** ) and click **Windows PowerShell (Admin)** .\n- Confirm the elevation prompt by clicking **Yes** .\n- Remove the default SQL Server instance:```\nC:\\sql_server_install\\Setup.exe /Action=Uninstall /FEATURES=SQL,AS,IS,RS /INSTANCENAME=MSSQLSERVER /Q\n```\n- Remove Microsoft OLE Driver:```\nGet-Package -Name \"Microsoft OLE*\" | Uninstall-Package -Force\n```\n- Remove Microsoft ODBC Driver:```\nGet-Package -Name \"Microsoft ODBC*\" | Uninstall-Package -Force\n```\n- Restart the computer:```\nRestart-Computer\n```\n- Wait for approximately 1 minute for the restart to complete.\n### Install SQL Server FCIBefore you install the new FCI configuration, verify that the `node-1` is the active node in the cluster:- Reconnect to`node-1` [by using Remote Desktop](/compute/docs/instances/connecting-to-windows) and sign in using your domain user.\n- Right-click the **Start** button (or press **Win+X** ) and select **Run** \n- Enter`cluadmin.msc`and select **OK** .\n- In the left window pane, navigate to **Failover Cluster Manager > sql-cluster** .Verify that the **current host server** is set to `node-1` .If the **current host server** is set to `node-2` , right-click **sql-cluster** in the left window pane and select **More actions > Move core cluster resources > Select node\u2026 > node-1** and click **OK** .\n- In the left window pane, navigate to **Failover Cluster Manager > sql-cluster > Storage > Pools** .Verify that the **owner node** of **Cluster Pool 1** is set to `node-1` .If the **owner node** is set to `node-2` , right-click the pool, select **Move > Select Node > node-1** and click **OK** .\nYou now create a new SQL Server failover cluster installation on `node-1` :- Right-click the **Start** button (or press **Win+X** ) and click **Windows PowerShell (Admin)** .\n- Confirm the elevation prompt by clicking **Yes** .\n- Create a domain user account for SQL server and the SQL agent and assign a password:```\n$Credential = Get-Credential -UserName sql_server -Message 'Enter password'\nNew-ADUser `\n -Name \"sql_server\" `\n -Description \"SQL Agent and SQL Admin account.\" `\n -AccountPassword $Credential.Password `\n -Enabled $true -PasswordNeverExpires $true\n``` **Note:** If you use Managed AD, append `-Path \"OU=Cloud,DC=example,DC=org\"` to the command to create the user in the `Cloud` OU.\n- Start the SQL Server setup:```\n& c:\\sql_server_install\\setup.exe\n```\n- In the menu on the left, select **Installation** .\n- Select **New SQL Server failover cluster installation** \n- On the **Edition** page, check **I have a SQL Server license only** and select **Next** .\n- On the **License Terms** page, review the terms and, if you accept, select **Next** .\n- On the **Microsoft Update** page, select **Next** to start the installation.\n- On the **Install Failover Cluster Rules** page, you see a Warning **MSCS cluster verification warnings** and **Windows firewall** . You can ignore these warnings and select **Next** .\n- On the **Feature Selection** page, select **Database Engine Services** and select **Next** .\n- On the **Instance Configuration** page, enter `sql` as network name and select **Next** .\n- On the **Cluster Resource Group** page, keep the defaults and select **Next** .\n- On the **Cluster Disk Selection** page, select **Cluster Virtual Disk (FciVolume)** and select **Next** .\n- On the **Cluster Network Configuration** page, configure the following settings, then select **Next** :- **DHCP** : clear\n- **IP address:** enter the IP address of the internal load balancer.\n- On the **Server configuration** page, configure the following settings for both **SQL Server Agent** and **SQL Server Database Engine** :- **Account name** :`` `\\sql_server`where``is the NetBIOS name of your Active Directory domain\n- **Password** : Enter the password that you created earlier\n- Select the **Collation** tab and select the collation that you want to use. Then click **Next** .\n- On the **Database Engine Configuration** page, select **Add current user** to designate the current user as SQL Server administrator. Then select **Next** .\n- On the **Ready to Install** page, review the settings, then select **Install** .\n- After the installation completes, select **Close** .\nNow add `node-2` to the SQL Server failover cluster:- Connect to`node-2` [by using Remote Desktop](/compute/docs/instances/connecting-to-windows) and sign in using your domain user.\n- Right-click the **Start** button (or press **Win+X** ) and click **Windows PowerShell (Admin)** .\n- Confirm the elevation prompt by clicking **Yes** .\n- Start the SQL Server setup:```\n& c:\\sql_server_install\\setup.exe\n```\n- In the menu on the left, select **Installation** .\n- Select **Add node to a SQL Server failover cluster** .\n- Follow the instructions of the installation wizard and accept the default settings until you reach the page **Service Accounts** .\n- On the **Service Accounts** page, enter the password that you created earlier for both **SQL Server Agent** and **SQL Server Database Engine** . Then select **Next** .\n- On the **Ready to Install** page, review the settings, then select **Install** .\n- After the installation completes, select **Close** .\n### Configure health checks **Note:** The following section does not apply to DNN configuration.\nAs a final step, configure the cluster to expose a health check endpoint that can be used by an internal load balancer:- Return to the PowerShell session on`node-2`\n- Initialize a variable with the IP address of the load balancer.```\n$LoadBalancerIP = 'IP_ADDRESS'\n```Replace `` with the IP address of the `wsfc` address that you reserved earlier.\n- Configure the Failover Cluster to respond to the health check service:```\n$SqlGroup = Get-ClusterGroup |\n Where-Object {$_.Name.StartsWith(\"SQL Server\")}\n$SqlIpAddress = Get-ClusterResource |\n Where-Object {$_.Name.StartsWith(\"SQL IP Address\")}\n$SqlIpAddress | Set-ClusterParameter -Multiple @{\n 'Address'=$LoadBalancerIP;\n 'ProbePort'= 59997;\n 'SubnetMask'='255.255.255.255';\n 'Network'= (Get-ClusterNetwork).Name;\n 'EnableDhcp'=0; }\n```\n- Restart the cluster resource:```\n$SqlIpAddress | Stop-ClusterResource\n$SqlIpAddress | Start-ClusterResource\n```\n- Restart the cluster group:```\n$SqlGroup | Stop-ClusterGroup\n$SqlGroup | Start-ClusterGroup\n```\n## Create an internal load balancer **Note:** The following section does not apply to DNN configuration.\nTo provide a single endpoint for SQL Server clients, you now deploy an [internal load balancer](/compute/docs/load-balancing/internal#deploying_internal_load_balancing) . The load balancer uses a health check which ensures that traffic is directed to the active node of the WSFC.- Return to your existing Cloud Shell session.\n- Create an [unmanaged instance group](/compute/docs/instance-groups/creating-groups-of-unmanaged-instances) , and add the two nodes to the group:```\ngcloud compute instance-groups unmanaged create wsfc-group\ngcloud compute instance-groups unmanaged add-instances wsfc-group --instances node-1,node-2\n```\n- Create a health check that the load balancer can use to determine which is the active node.```\ngcloud compute health-checks create tcp wsfc-healthcheck \\\n --check-interval=\"2s\" \\\n --healthy-threshold=1 \\\n --unhealthy-threshold=2 \\\n --port=59997 \\\n --timeout=\"1s\"\n```The health check probes port `59997` , which is the port you previously configured as `ProbePort` for the WSFC cluster IP address.\n- Create a backend service and add the instance group:```\ngcloud compute backend-services create wsfc-backend \\\n --load-balancing-scheme internal \\\n --region $(gcloud config get-value compute/zone | rev | cut -c 3- | rev) \\\n --health-checks wsfc-healthcheck \\\n --protocol tcp\ngcloud compute backend-services add-backend wsfc-backend \\\n --instance-group wsfc-group \\\n --instance-group-zone $(gcloud config get-value compute/zone) \\\n --region $(gcloud config get-value compute/zone | rev | cut -c 3- | rev)\n```\n- Create the internal load balancer:```\ngcloud compute forwarding-rules create wsfc-sql \\\n --load-balancing-scheme internal \\\n --address $LOADBALANCER_ADDRESS \\\n --ports 1433 \\\n --network $VPC_NAME \\\n --subnet $SUBNET_NAME \\\n --region $(gcloud config get-value compute/zone | rev | cut -c 3- | rev) \\\n --backend-service wsfc-backend\n```\n### Configure a DNN resource and DNN DNS name **Note:** The following section does not apply to load balancer configuration.\nSimilar to the internal load balancer, the DNN resource acts as a single gateway for SQL Server clients. During failovers, the cluster seamlessly routes traffic to the active SQL Server FCI node. Clients connect to the SQL Server FCI with the DNS name.- Return to the PowerShell session on`node-1`.\n- Execute the script to create DNN resource```\n $DNNResourceName='fci-dnn'\n $DNN_DNSName='fcidnn'\n # create the DNN resource\n Add-ClusterResource -Name $DNNResourceName -ResourceType 'Distributed Network Name' -Group 'SQL Server (MSSQLSERVER)'\n # set the DNS name of the DNN resource\n Get-ClusterResource -Name $DNNResourceName | Set-ClusterParameter -Name DnsName -Value $DNN_DNSName\n # start the DNN resource\n Start-ClusterResource -Name $DNNResourceName\n```\n- Restart `node-1` and `node-2`\n## Testing the failover clusterYou've completed the installation of the failover cluster, but you still have to test whether the cluster works correctly.\n### Prepare a clientCreate a new VM instance which you can use to connect to the failover cluster:- Return to your existing Cloud Shell session.\n- Create a new VM instance:```\ngcloud compute instances create sqlclient \\\n --machine-type n2-standard-2 \\\n --subnet $SUBNET_NAME \\\n --image-family sql-ent-2022-win-2022 \\\n --image-project windows-sql-cloud \\\n --boot-disk-size 50 \\\n --boot-disk-type pd-ssd\n```\n- Monitor the initialization process of the VM by viewing its serial port output:```\ngcloud compute instances tail-serial-port-output sqlclient\n```Wait about 3 minutes until you see the output Instance setup finished, then press Ctrl+C. At this point, the VM instance is ready to be used.\n- [Create a username and password](/compute/docs/instances/windows/creating-passwords-for-windows-instances) for the VM instance\n- [Connect to the VM by using Remote Desktop](/compute/docs/instances/connecting-to-windows) and sign in using the username and password created in the previous step.\n- Right-click the **Start** button (or press **Win+X** ) and click **Windows PowerShell (Admin)** .\n- Confirm the elevation prompt by clicking **Yes** .\n- Join the computer to your Active Directory domain:```\nAdd-Computer -Domain DOMAIN\n```Replace `` with the DNS name of your Active Directory domain.\n- Restart the computer:```\nRestart-Computer\n```Wait for approximately 1 minute for the restart to complete.\n### Run the testUse the `sqlclient` VM to test that you can connect to the failover cluster and to verify that the failover works correctly:- Connect to`sqlclient` [by using Remote Desktop](/compute/docs/instances/connecting-to-windows) and sign in using your domain user.\n- Right-click the **Start** button (or press **Win+X** ) and click **Windows PowerShell** .\n- Connect to SQL Server cluster by using its network name `sql` and query the `dm_os_cluster_nodes` table:```\n& \"$env:ProgramFiles\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn\\SQLCMD.EXE\" `\n -S SQL_SERVER_NAME -E -Q \"SELECT * FROM sys.dm_os_cluster_nodes\"\n```Replace `` with the `sql` for **load balancer configuration** or `fcidnn` for **DNN configuration** .The output should look like this:```\nNodeName      status  status_description is_current_owner\n------------------------------ ----------- ------------------ ---------------NODE-1         0 up        1\nNODE-2         0 up        0\n(2 rows affected)\n```Notice that `node-1` is the current owner of the SQL Server failover cluster resource.\n- Return to Cloud Shell and bring down the node-1 VM to test the failover scenario.```\ngcloud compute instances stop node-1\n```\n- Repeat the query:```\n& \"$env:ProgramFiles\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn\\SQLCMD.EXE\" `\n -S SQL_SERVER_NAME -E -Q \"SELECT * FROM sys.dm_os_cluster_nodes\"\n```Replace `` with the `sql` for **load balancer configuration** or `fcidnn` for **DNN configuration** .The output should now look like this:```\nNodeName      status  status_description is_current_owner\n------------------------------ ----------- ------------------ ---------------NODE-1         1 down        0\nNODE-2         0 up        1\n(2 rows affected)\n```Notice that despite the loss of `node-1` , the query succeeds, and shows that `node-2` is now the current owner of the failover cluster.\n## Clean up\nAfter you finish the tutorial, you can clean up the resources that you created so that they stop using quota and incurring charges. The following sections describe how to delete or turn off these resources.\n### Deleting the project\nThe easiest way to eliminate billing is to delete the project that you created for the tutorial.\nTo delete the project:\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Read more about [persistent disks in multi-writer mode](/compute/docs/disks#pdspecs_rw) .\n- Learn how you can [configure a SQL Server failover cluster instance that uses Storage Spaces Direct](/compute/docs/instances/sql-server/configure-failover-cluster-instance) .", "guide": "Compute Engine"}