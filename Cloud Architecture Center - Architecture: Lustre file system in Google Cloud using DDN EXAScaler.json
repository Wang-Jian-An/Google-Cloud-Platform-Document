{"title": "Cloud Architecture Center - Architecture: Lustre file system in Google Cloud using DDN EXAScaler", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Architecture: Lustre file system in Google Cloud using DDN EXAScaler\nLast reviewed 2023-11-15 UTC\nThis document provides architectural guidance to help you design and size a [Lustre](https://www.lustre.org/) file system for high performance computing ( [HPC](/solutions/hpc) ) workloads. It also provides an overview of the process to deploy a Lustre file system in Google Cloud by using [DDN EXAScaler](https://www.ddn.com/products/lustre-file-system-exascaler/) .\nLustre is an open source, parallel file system that provides high-throughput and low-latency storage for [tightly coupled HPC workloads](/architecture/parallel-file-systems-for-hpc#examples_of_tightly_coupled_hpc_applications) . You can scale a Lustre file system to support tens of thousands of HPC clients and petabytes of storage. EXAScaler Cloud is an enterprise version of Lustre that's offered by [DDN](https://www.ddn.com/partners/google-cloud-platform/) , a Google partner. You can deploy EXAScaler Cloud in a hybrid-cloud architecture to augment your on-premises HPC capacity. EXAScaler Cloud can also serve as a repository for storing longer-term assets from an on-premises EXAScaler deployment.\nThe guidance in this document is intended for enterprise architects and technical practitioners who design, provision, and manage storage for HPC workloads in the cloud. The document assumes that you have a conceptual understanding of parallel file systems. You should also have an understanding of the HPC use cases for which parallel file systems like Lustre are ideal. For more information, see [Parallel file systems for HPC workloads](/architecture/parallel-file-systems-for-hpc#when_to_use_parallel_file_systems) .\n", "content": "## Overview of the Lustre file system\nThe following diagram shows the architecture of a Lustre file system:\nAs shown in the diagram, the architecture contains the following components:\n- **Management server (MGS) and management targets (MGT)** : The MGS stores and manages configuration information about one or more Lustre file systems. The diagram shows the MGS managing a single Lustre file system. The MGS provides configuration information to the other Lustre components in all the file systems that it manages. The MGS records file-system configuration logs in storage devices that are called MGTs.\n- **Metadata servers (MDS) and metadata targets (MDT)** : The MDS nodes manage client access to the namespace of a Lustre file system. This namespace includes all the metadata of the file system, such as the directory hierarchy, file creation time, and access permissions. The metadata is stored in storage devices that are called MDTs. A Lustre file system has at least one MDS and one associated MDT. To improve the performance for metadata\u2011intensive workloads, such as when thousands of clients create and access millions of small files, you can add more MDS nodes to the file system.\n- **Object storage servers (OSS) and object storage targets (OST)** : The OSS nodes manage client access to the file data that's stored in a Lustre file system. Each file is stored as one or more Lustre . The objects are stored either in a single storage device (called an OST) or [striped](https://wiki.lustre.org/Configuring_Lustre_File_Striping) across multiple OSS nodes and OSTs. A Lustre file system has at least one OSS and one associated OST. You can scale the storage capacity and the performance of the file system by adding more OSS nodes and OSTs. The total storage capacity of the file system is the sum of the storage capacities of the OSTs that are attached to all the OSS nodes in the file system.\n- **Clients** : A Lustre client is a compute node, such as a virtual machine (VM), that accesses a Lustre file system through a [mount point](https://wiki.lustre.org/Mounting_a_Lustre_File_System_on_Client_Nodes) . The mount point provides a unified namespace for the entire file system. You can scale a Lustre file system to support concurrent access by over 10,000 clients. Lustre clients access all the MDS and OSS nodes in a Lustre file system in parallel. This parallel access helps to maximize the performance of the file system. The parallel access also helps reduce storage , which are storage locations that are accessed much more frequently than other locations. Hotspots are common in non-parallel file systems, and can cause a performance imbalance between clients.To access a Lustre file system, a client gets the required directory and file metadata from an MDS, and then reads or writes data by communicating with one or more OSS nodes. Lustre provides close [compliance with POSIX semantics](https://wiki.lustre.org/Lustre_Clients_Overview) , and it allows all clients full and parallel access to the file system.\nFor more information about the Lustre file system and how it works, see the [Lustre documentation](https://doc.lustre.org/lustre_manual.xhtml#understandinglustre) .\n## Architecture of Lustre in Google Cloud\nThe following diagram shows an architecture for deploying a Lustre file system in Google Cloud:\nThe architecture shown in this diagram contains the following resources. All the resources are deployed in a single Google Cloud project. The compute and storage resources are provisioned within a single zone.\n- [Compute Engine](https://cloud.google.com/compute) VMs host the MGS, MDS, and OSS nodes and the Lustre clients. You can also choose to deploy the Lustre clients in a Google Kubernetes Engine cluster, and deploy the file system on Compute Engine VMs.\n- The architecture includes the following networking resources:- A single [Virtual Private Cloud (VPC) subnet](/vpc/docs/subnets) that's used for all the VMs.\n- An optional [Cloud NAT](/nat/docs/overview) gateway and an optional [Cloud Router](/network-connectivity/docs/router/concepts/overview) for outbound traffic from the private VMs to the internet.\n- Optional [firewall rules](/vpc/docs/firewalls) to allow SSH ingress connections to all the VMs in the topology.\n- An optional firewall rule to allow HTTP access from the internet to the DDN EXAScaler web console on the MGS.\n- A firewall to allow TCP connections between all the VMs.\n- [Persistent Disks](https://cloud.google.com//persistent-disk) provide storage capacity for the MGS, MDS, and OSS nodes. If you don't need persistent storage, you can build a [scratch](https://wikipedia.org/wiki/Scratch_space) file system by using local solid-state drive (SSD) disks, which are attached to the VMs.Google has submitted [IO500](https://io500.org/about) entries demonstrating the performance of both persistent and scratch Lustre file systems. Read about the [Google Cloud submission](https://cloud.google.com/blog/topics/hpc/google-cloud-ranks-on-io500-benchmark-with-lustre) that demonstrates a 10+ Tbps, Lustre-based scratch file system on the IO500 ranking of HPC storage systems.## Design guidelines\nUse the following guidelines to design a Lustre file system that meets the requirements of your HPC workloads. The guidelines in this section are not exhaustive. They provide a framework to help you assess the storage requirements of your HPC workloads, evaluate the available storage options, and size your Lustre file system.\n**Note:** If the requirements of your HPC workloads vary widely, you can deploy multiple Lustre file systems. You can then configure each of these file systems to meet the price-to-performance and I/O requirements of a specific workload.\n### Workload requirements\nIdentify the storage requirements of your HPC workloads. Define the requirements as granularly as possible, and consider your future requirements. Use the following questions as a starting point to identify the requirements of your workload:\n- What are your requirements for [throughput](https://wikipedia.org/wiki/Throughput) and I/O operations per second ( [IOPS](https://wikipedia.org/wiki/IOPS) )?\n- How much storage capacity do you need?\n- What is your most important design goal: throughput, IOPS, or storage capacity?\n- Do you need persistent storage, scratch storage, or both?\n### Storage options\nFor the most cost-effective storage options, you can choose from the following types of Persistent Disks:\n- Standard Persistent Disks (`pd-standard`) are the most cost-effective option when storage capacity is the main design goal. They provide efficient and reliable block storage by using hard-disk drives (HDD).\n- SSD Persistent Disks (`pd-ssd`) are the most cost-effective option when the goal is to maximize IOPS. They provide fast and reliable block storage by using SSDs.\n- Balanced Persistent Disks (`pd-balanced`) are the most cost-effective option for maximizing throughput. They provide cost-effective and reliable block storage by using SSDs.\nExtreme Persistent Disks ( `pd-extreme` ) can provide higher performance than the other disk types, and you can choose the required IOPS. But `pd-extreme` costs more than the other disk types.\nFor more information about the performance capabilities of Persistent Disks, see [Block storage performance](/compute/docs/disks/performance) .\nFor scratch storage, you can use ephemeral [local SSDs](/compute/docs/disks#localssds) . Local SSDs are physically attached to the server that hosts the VMs. So local SSDs provide higher throughput and lower latency than Persistent Disks. But the data that's stored on a local SSD persists only until the VM is stopped or deleted.\n### Object storage servers\nWhen you design the infrastructure for the OSS nodes, we recommend the following:\n- For storage, choose an appropriate Persistent Disk type based on your requirements for storage capacity, throughput, and IOPS.- Use`pd-standard`for workloads that have the following requirements:- The workload needs high storage capacity (for example, more than 10 TB), or it needs both high read throughput and high storage capacity.\n- I/O latency is not important.\n- Low write throughput is acceptable.\n- Use`pd-balanced`for workloads that have any of the following requirements:- High throughput at low capacity.\n- The low latency that's provided by SSD-based disks.\n- Use`pd-ssd`for workloads that require high IOPS (either small I/O requests or small files).\n- Provision enough storage capacity to achieve the required IOPS. Consider the [read and write IOPS](/compute/docs/disks/performance#performance_limits) provided by each disk type.\n- For the VMs, use a [machine type](/compute/docs/machine-types#machine_types) from the N2 or N2D machine family. These machine types provide predictable and cost-efficient performance.\n- Allocate enough [vCPUs](/compute/docs/cpu-platforms) to achieve the required Persistent Disk throughput. The maximum Persistent Disk throughput per VM is 1.2 GBps, and this throughput can be achieved with 16 vCPUs. So start with a machine type that has 16 vCPUs. [Monitor the performance](/compute/docs/disks/review-disk-metrics) , and allocate more vCPUs when you need to scale the IOPS.\n### Metadata servers\nThe MDS nodes don't need high storage capacity for serving metadata, but they need storage that supports high IOPS. When designing the infrastructure for the MDS nodes, we recommend the following:\n- For storage, use`pd-ssd`because this type of Persistent Disk provides high IOPS (30 IOPS per GB) even at low storage capacity.\n- Provision enough storage capacity to achieve the required IOPS.\n- For the VMs, use a [machine type](/compute/docs/machine-types#machine_types) from the N2 or N2D machine family. These machine types provide predictable and cost-efficient performance.\n- Allocate enough vCPUs to achieve the required IOPs:- For low-metadata workloads, use 16 vCPUs.\n- For medium-metadata workloads, use 32 vCPUs.\n- For metadata-intensive workloads, use 64 vCPUs. [Monitor the performance](/compute/docs/disks/review-disk-metrics) , and allocate more vCPUs when necessary.\n### Management server\nThe MGS needs minimal compute resources. It is not a storage-intensive service. Start with a small machine type for the VM (for example, `n2-standard-2` ) and a 128-GB `pd-ssd` disk for storage, and [monitor the performance](/compute/docs/disks/review-disk-metrics) . If the response is slow, allocate more vCPUs and increase the disk size.\n### Availability and durability\nIf you need persistent storage, the `pd-standard` and `pd-balanced` Persistent Disk types provide highly available and durable storage within a zone. For cross-zone or cross-region persistence, you can copy the data to low-cost [Cloud Storage](https://cloud.google.com/storage) by using the [Google Cloud CLI](/sdk/gcloud/reference/alpha/storage) or [Storage Transfer Service](/storage-transfer/docs/create-transfers#create_a_transfer) . To reduce the cost of storing data in Cloud Storage, you can store infrequently accessed data in a bucket of the [Nearline storage](/storage/docs/storage-classes#nearline) or [Coldline storage](/storage/docs/storage-classes#coldline) class.\nIf you need only ephemeral storage for a scratch deployment, use [localSSDs](/compute/docs/disks#localssds) as the data disks for the OSS and MDS nodes. This design delivers high performance with the fewest number of OSS VMs. This design also helps you achieve an optimal cost-to-performance ratio when compared with the other options.\n## Sizing example for OSS VMs\nThe recommended strategy for sizing and provisioning your Lustre file system is to provision enough OSS VMs to meet the overall throughput requirement. Then, increase the storage capacity of the OST disks until you reach the required storage capacity. The example workload that is used in this section shows you how to implement this strategy by using the following steps:\n- Determine the workload requirements.\n- Choose a Persistent Disk type.\n- Calculate the number of OSS VMs.\n- Calculate the disk size per VM.\n- Determine the number of vCPUs per VM.\n### Determine the workload requirements\nIn this example, the workload requires 80 TB of persistent storage capacity with a read throughput of 30 GBps.\n### Choose a Persistent Disk type\nAs discussed in the [Storage options](#storage_options) section, `pd-standard` is the most cost-effective option when storage capacity is the main design goal, and `pd-balanced` is the most cost-effective option for maximizing throughput. The maximum throughput is different for each disk type, and the throughput scales with the disk size.\nFor each Persistent Disk type that can be used for this workload, calculate the storage capacity that's necessary to scale the read throughput to the target of 30 GBps.\n| Disk type | Read throughput per TB | Storage capacity required to achieve the target throughput |\n|:------------|:-------------------------|:-------------------------------------------------------------|\n| pd-standard | 0.12 GBps    | 30 divided by 0.12 = 250 TB         |\n| pd-balanced | 0.28 GBps    | 30 divided by 0.28 = 107 TB         |\nTo achieve the target read throughput of 30 Gbps by using `pd-standard` , you would need to provision 250 TB of storage capacity. This amount is over three times the required capacity of 80 TB. So for the workload in this example, `pd-balanced` provides cost-efficient storage that meets the performance requirements.\n### Calculate the number of OSS VMs\nThe maximum read throughput per Compute Engine VM is 1.2 GBps. To achieve the target read throughput of 30 GBps, divide the target read throughput by the maximum throughput per VM as follows:\n```\n 30 GBps / 1.2 GBps = 25\n```\nYou need 25 OSS VMs to achieve the target read throughput.\n### Calculate the disk size per VM\nTo calculate the disk size per VM, divide the capacity (107 TB) that's necessary to achieve the target throughput (30 GBps) by the number of VMs as follows:\n```\n 107 TB / 25 VMs = 4.3\n```\nYou need 4.3 TB of `pd-balanced` capacity per VM.\n### Determine the number of vCPUs per VM\nThe read throughput of a VM scales with the number of vCPUs allocated to the VM. The read throughput peaks at 1.2 Gbps for 16 (or more) vCPUs. You need a [machine type](/compute/docs/disks/performance#machine-type-disk-limits) that provides at least 16 vCPUs. So choose a machine type from the N2 or N2D machine family, such as `n2-standard-32` .\n### Configuration summary\nThe example workload has the following requirements:\n- 80-TB persistent storage capacity\n- 30-Gbps read throughput\nTo meet the requirements of this workload, you need the following compute and storage resources:\n- **Number of OSS VMs** : 25\n- **VM machine family** : N2 or N2D\n- **Number of vCPUs per VM** : 16 or more\n- **Persistent Disk type** :`pd-balanced`\n- **Disk size per VM** : 4.3 TB## Configuration example for a scratch file system\nThe following configuration is based on a [Google Cloud submission to IO500](https://cloud.google.com/blog/topics/hpc/google-cloud-ranks-on-io500-benchmark-with-lustre) that demonstrates the performance of an extreme-scale scratch file system that uses Lustre and is deployed on Google Cloud:\n| Configuration parameter | MDS     | OSS     | Clients  |\n|:--------------------------|:---------------------|:---------------------|:---------------|\n| Number of VMs    | 50     | 200     | 1000   |\n| Machine type    | n2-standard-80  | n2-standard-64  | c2-standard-16 |\n| Number of vCPUs per VM | 80 vCPUs    | 64 vCPUs    | 16 vCPUs  |\n| RAM per VM    | 320 GB    | 256 GB    | 64 GB   |\n| OS      | CentOS 8    | CentOS 8    | CentOS 8  |\n| Network bandwidth   | 100 Gbps    | 75 Gbps    | 32 Gbps  |\n| Local SSD storage   | 9-TB NVMe (24 disks) | 9-TB NVMe (24 disks) | nan   |\nThe preceding configuration provided the following performance:\n| Performance metric    | Result     |\n|:-------------------------------|:------------------------|\n| Write throughput    | 700 GBps (5.6 Tbps)  |\n| Read throughput    | 1,270 GBps (10.16 Tbps) |\n| File stat() operations   | 1.9 million per second |\n| Small file reads (3,901 bytes) | 1.5 million per second |\n## Deployment options\nThis section provides an overview of the methods that you can use to deploy an EXAScaler Cloud file system in Google Cloud. This section also outlines the steps to follow when you deploy the client VMs.\n### EXAScaler Cloud file system deployment\nYou can choose from the following methods to deploy an EXAScaler Cloud file system in Google Cloud:\n- Deploy by using a [Google Cloud Marketplace solution](https://console.cloud.google.com/marketplace/product/ddnstorage/exascaler-cloud) that's provided by DDN.\n- Deploy by using the [Cloud HPC Toolkit](/hpc-toolkit/docs/deploy/deploy-cluster-overview) . The toolkit includes a [blueprint](https://github.com/GoogleCloudPlatform/hpc-toolkit/tree/main/examples#lustreyaml-) that uses a DDN-provided [Terraform module](https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/main/community/modules/file-system/DDN-EXAScaler/README.md) to deploy a high-I/O file system.\nWith either method, you can customize the file system when you deploy it. For example, you can specify the number of OSS VMs, the machine type for the VMs, the Persistent Disk types, and the storage capacity.\nWhen choosing a method, consider the following differences:\n- **Post-deployment modification** : If you use the Cloud HPC Toolkit, you can efficiently modify the file system after deployment. For example, to add storage capacity, you can increase the number of OSS nodes by updating the Cloud HPC Toolkit blueprint and applying the generated Terraform configuration again. For a list of the parameters that you can specify in the blueprint, see the [Inputs](https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/main/community/modules/file-system/DDN-EXAScaler/README.md#inputs) section in the README for the Terraform module. To modify a file system that's deployed by using the Cloud Marketplace solution, you must make the changes for each compute and storage resource individually by using the Google Cloud console, gcloud CLI, or API.\n- **Support** : The Cloud Marketplace solution uses Deployment Manager, which is not supported by [VPC Service Controls](/vpc-service-controls/docs/overview) . For more information about this limitation, see [VPC Service Controls supported products and limitations](/vpc-service-controls/docs/supported-products#deployment_manager) .\n### Client deployment\nYou can use either of the methods described in the [EXAScaler Cloud file system deployment](#exascaler_cloud_file_system_deployment) section to deploy the client VMs. However, Google recommends that you provision and manage your client VMs separately from the file system. The recommended way to deploy your clients is by using the Google-provided [HPC VM image](https://console.cloud.google.com/marketplace/product/click-to-deploy-images/hpc-vm-image) , which is optimized for HPC workloads.\nThe following is an overview of the process of using the HPC VM image to deploy Lustre clients:\n- Create a VM by using the [HPC VM image](/compute/docs/instances/create-hpc-vm) .\n- Install the Lustre client packages on the VM.\n- [Customize](/compute/docs/instances/create-hpc-vm#configure_your_hpc_vm_according_to_best_practices) the VM as necessary.\n- Create a [custom image](/compute/docs/instances/create-hpc-vm#create_a_custom_image_using_the_hpc_vm_image) from the VM.\n- Provision the Lustre client VMs by using the custom image that you created. To automate the provisioning and management of the client VMs, you can use a Compute Engine [managed instance group](/compute/docs/instance-groups#managed_instance_groups) or a third-party tool like [Slurm Workload Manager](https://slurm.schedmd.com/overview.html) . For more guidance about provisioning and managing clusters of Compute Engine VMs for HPC workloads, see [Using clusters for large-scale technical computing in the cloud](/architecture/using-clusters-for-large-scale-technical-computing) .\n- Mount the Lustre file system on the client VMs.## Data transfer options\nAfter you deploy a Lustre file system in Google Cloud, you need to move your data to the file system. The following table shows the methods that you can use to move data to your Lustre file system. Choose the method that corresponds to the volume of data that you need to move and the location of your source data (on-premises or in Cloud Storage).\n| Data source | Data size       | Recommended data-transfer method                                                                        |\n|:--------------|:------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| On\u2011premises | Small (for example, less than 1 TB) | Stage the data in Cloud Storage by using the gsutil tool. Then, download the data to the Lustre file system by using Storage Transfer Service or the gcloud CLI.                                        |\n| On-premises | Large        | Move the data to the Lustre file system by using Storage Transfer Service. Follow the instructions in Transfer data between POSIX file systems. This method involves the use of an intermediary Cloud Storage bucket. After completing the transfer job, Storage Transfer Service deletes the data in the intermediary bucket. |\n| Cloud Storage | Large or small      | Download the data from Cloud Storage to the Lustre file system by using Storage Transfer Service or the gcloud CLI.                                                   |\n## What's next\n- Learn more about [Lustre](https://www.lustre.org/getting-started-with-lustre/) .\n- Learn more about [DDN EXAScaler Cloud](/architecture/filers-on-compute-engine#DDN) and [DDN's partnership with Google](https://www.ddn.com/partners/google-cloud-platform/) .\n- Read about the [Google Cloud submission](https://cloud.google.com/blog/topics/hpc/google-cloud-ranks-on-io500-benchmark-with-lustre) that demonstrates a 10+ Tbps, Lustre-based scratch file system on the IO500 ranking of HPC storage systems.\n- Deploy DDN EXAScaler in Google Cloud as described in the [EXAScaler Cloud file system deployment](/architecture/lustre-architecture#exascaler_cloud_file_system_deployment) section of this document.## Contributors\nAuthor: [Kumar Dhanagopal](https://www.linkedin.com/in/kumardhanagopal) | Cross-Product Solution Developer\nOther contributors:\n- [Carlos Boneti](https://www.linkedin.com/in/carlosboneti) | Senior Staff Software Engineer\n- [Dean Hildebrand](https://www.linkedin.com/in/dean) | Technical Director\n- [Sean Derrington](https://www.linkedin.com/in/seanderrington) | Group Outbound Product Manager", "guide": "Cloud Architecture Center"}