{"title": "Cloud Architecture Center - Migrating Apache Spark Jobs to Dataproc", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Migrating Apache Spark Jobs to Dataproc\nThis document describes how to move Apache Spark jobs to Dataproc. The document is intended for big-data engineers and architects. It covers topics such as considerations for migration, preparation, job migration, and management.\n**Note:** The information and recommendations in this document were gathered through collaboration with a variety of customers and environments in the field. We thank our customers and partners for sharing their experiences and insights.\n", "content": "## Overview\nWhen you want to move your Apache Spark workloads from an on-premises environment to Google Cloud, we recommend using [Dataproc](/dataproc) to run Apache Spark/Apache Hadoop clusters. Dataproc is a fully managed, fully supported service offered by Google Cloud. It allows you to separate storage and compute, which helps you to manage your costs and be more flexible in scaling your workloads.\nIf a managed Hadoop environment doesn't fit your needs, you can also use a different setup, such as running Spark on [Google Kubernetes Engine (GKE)](/kubernetes-engine) , or renting virtual machines on Compute Engine and setting up a Hadoop or Spark cluster yourself. However, take into account that options other than using Dataproc are self-managed and have only community support.\n## Planning your migration\nThere are many differences between running Spark jobs on-premises and running Spark jobs on Dataproc or Hadoop clusters on Compute Engine. It's important to look closely at your workload and prepare for migration. In this section, we outline considerations to take into account, and preparations that to take before you migrate Spark jobs.\n### Identify job types and plan clusters\nThere are three types of Spark workloads, as described in this section.\nRegularly scheduled batch jobs include use cases like daily or hourly ETLs, or pipelines for training machine learning models with Spark ML. For these cases, we recommend that you create a cluster for each batch workload and then delete the cluster after your job is finished. You have the flexibility to configure your cluster, because you can adjust the configuration for each workload separately. Dataproc clusters are billed in one-second block increments after the first minute, so this approach is also cost-effective, because you can [label](/dataproc/docs/concepts/labels) your clusters. For more information, see the [Dataproc pricing](/dataproc/pricing) page.\nYou can implement batch jobs with [workflow templates](/dataproc/docs/concepts/workflows/overview) or by following these steps:\n- [Create](/dataproc/docs/guides/create-cluster) a cluster and wait until the cluster is created. (You can monitor whether the cluster has been created by using an [API call](/dataproc/docs/reference/rest/v1/projects.regions.clusters/get) or a [gcloud command](/sdk/gcloud/reference/dataproc/clusters/describe) .) If you run your job on a dedicated Dataproc cluster, it might help to turn off the [dynamic allocation](https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation) and the [external shuffle service](https://books.japila.pl/apache-spark-internals/external-shuffle-service/ExternalShuffleService/) . The following `gcloud` command shows Spark configuration properties that are provided when you create the Dataproc cluster:```\ndataproc clusters create ... \\\n --properties 'spark:spark.dynamicAllocation.enabled=false,spark:spark.shuffle.service.enabled=false,spark.executor.instances=10000'\n```\n- [Submit](/dataproc/docs/guides/submit-job) your job to the cluster. (You can monitor the status of your job by using an [API call](/dataproc/docs/reference/rest/v1/projects.regions.jobs/get) or a [gcloud command](/sdk/gcloud/reference/dataproc/jobs/describe) .) For example:```\njobId=$(gcloud --quiet dataproc jobs submit pyspark \\\n --async \\\n --format='value(reference.jobId)' \\\n --cluster $clusterName \\\n --region global \\\n gs://dataproc-examples-2f10d78d114f6aaec76462e3c310f31f/src/pyspark/hello-world/hello-world.py)\ngcloud dataproc jobs describe $jobId \\\n --region=global \\\n --format='value(status.state)'\n```\n- Delete the cluster after the job has been executed by using an [API call](/dataproc/docs/reference/rest/v1/projects.regions.clusters/delete) or a [gcloud command](/sdk/gcloud/reference/dataproc/clusters/delete) .For streaming jobs, you need to create a long-running Dataproc cluster and configure the cluster to run in [high-availability mode](/dataproc/docs/concepts/configuring-clusters/high-availability) . We don't recommend using preemptible VMs for this case.\nExamples of ad hoc workloads can include users who write queries or execute analytical jobs during the day.\nFor these cases, you must decide whether you need the cluster to run in high-availability mode, whether you'd like to use preemptible VMs, and how you'll manage access to the cluster. You can schedule cluster creation and termination (for example, if you never need the cluster during the night or weekends), and you can implement up and down [scaling](/dataproc/docs/concepts/configuring-clusters/scaling-clusters) according to the schedule.\n### Identify data sources and dependencies\nEach job has its own dependencies (for example, the data sources it needs), and other teams at your company might be dependent on the outcome of your jobs. Therefore, you must identify all dependencies and then create a migration plan that includes procedures for the following:\n- Step-by-step migration of all of your data sources to Google Cloud. In the beginning, it's helpful to mirror the data source in Google Cloud so that you have it in two places.\n- Job-by-job migration of your Spark workloads to Google Cloud as soon as corresponding data sources have been migrated. As with data, at some point you might have two workloads that are running in parallel both in your old environment and in Google Cloud.\n- Migration of other workloads that depend on the output of your Spark workloads. Or you might just replicate the output back to the initial environment.\n- Shutdown of Spark jobs in the old environment after all dependent teams have confirmed that they no longer require the jobs.\n### Choose storage options\nThere are two storage options to use with your Dataproc clusters: you can store all of the data in Cloud Storage, or you can use local disks or persistent disks with the cluster workers. The correct choice depends on the character of your jobs.\nEach node of a Dataproc cluster has a Cloud Storage [connector](/dataproc/docs/concepts/connectors/cloud-storage) installed on it. By default, the connector is installed under `/usr/lib/hadoop/lib` . The connector implements the Hadoop [FileSystem](https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html) interface and makes Cloud Storage compatible with HDFS.\nBecause Cloud Storage is a binary large object (BLOB) storage system, the connector emulates directories according to the object's name. You can access your data by using the `gs://` prefix instead of the `hdfs://` prefix.\nThe Cloud Storage connector typically doesn't require any customization. However, if you need to make changes, you can follow the instructions for [configuring the connector](https://github.com/GoogleCloudDataproc/hadoop-connectors/tree/master/gcs#configuring-the-connector) . A full list of configuration keys is also [available](https://github.com/GoogleCloudDataproc/hadoop-connectors/tree/master/gcs#configuring-the-connector) .\nCloud Storage is a good option if:\n- Your data in ORC, Parquet, Avro, or any other format will be used by different clusters or jobs, and you need data persistence if the cluster terminates.\n- You need high throughput and your data is stored in files larger than 128 MB.\n- You need cross-zone durability for your data.\n- You need data to be highly available\u2014for example, you want to eliminate HDFS [NameNode](https://cwiki.apache.org/confluence/display/HADOOP2/NameNode) as a single point of failure.\nLocal HDFS storage is a good option if:\n- Your jobs require a lot of metadata operations\u2014for example, you have thousands of partitions and directories, and each file size is relatively small.\n- You modify the HDFS data frequently or you rename directories. (Cloud Storage objects are [immutable](/storage/docs/objects#immutability) , so renaming a directory is an expensive operation because it consists of copying all objects to a new key and deleting them afterwards.)\n- You heavily use the [append](https://hadoop.apache.org/docs/r3.1.0/api/org/apache/hadoop/fs/FileSystem.html#append-org.apache.hadoop.fs.Path-) operation on HDFS files.\n- You have workloads that involve heavy I/O. For example, you have a lot of partitioned writes, such as the following:```\nspark.read().write.partitionBy(...).parquet(\"gs://\")\n```\n- You have I/O workloads that are especially sensitive to latency. For example, you require single-digit millisecond latency per storage operation.\nIn general, we recommend using Cloud Storage as the initial and final source of data in a big-data pipeline. For example, if a workflow contains five Spark jobs in series, the first job retrieves the initial data from Cloud Storage and then writes shuffle data and intermediate job output to HDFS. The final Spark job writes its results to Cloud Storage.\n### Adjust storage size\nUsing Dataproc with Cloud Storage allows you to reduce the disk requirements and save costs by putting your data there instead of in the HDFS. When you keep your data on Cloud Storage and don't store it on the local HDFS, you can use smaller disks for your cluster. By making your cluster truly on-demand, you're also able to separate storage and compute, as noted earlier, which helps you reduce costs significantly.\nEven if you store all of your data in Cloud Storage, your Dataproc cluster needs HDFS for certain operations such as storing control and recovery files, or aggregating logs. It also needs non-HDFS local disk space for shuffling. You can reduce the disk size per worker if you are not heavily using the local HDFS.\nHere are some options to adjust the size of the local HDFS:\n- Decrease the total size of the local HDFS by decreasing the size of primary persistent disks for the master and workers. The primary persistent disk also contains the boot volume and system libraries, so allocate at least 100 GB.\n- Increase the total size of the local HDFS by increasing the size of primary persistent disk for workers. Consider this option carefully\u2014 it's rare to have workloads that get better performance by using HDFS with standard persistent disks in comparison to using Cloud Storage or local HDFS with SSD.\n- Attach up to eight SSDs (375 GB each) to each worker and use these disks for the HDFS. This is a good option if you need to use the HDFS for I/O-intensive workloads and you need single-digit millisecond latency. Make sure that you use a machine type that has enough CPUs and memory on the worker to support these disks.\n- Use [persistent-disk SSDs (PD-SSDs)](/dataproc/docs/concepts/compute/dataproc-pd-ssd) for your master or workers as a primary disk.\n### Access Dataproc\nAccessing Dataproc or Hadoop on Compute Engine is different than accessing an on-premises cluster. You need to determine security settings and network access options.\nAll VM instances of a Dataproc cluster require internal networking to each other, and they require open UDP, TCP, and ICMP ports. You can allow access to your Dataproc cluster from external IP addresses by using the default network configuration or by using a VPC network. Your Dataproc cluster will have networking access to all Google Cloud services (Cloud Storage buckets, APIs, and so on) in any network option you use. To allow networking access to or from on-premises resources, choose a VPC network configuration and set up the appropriate [firewall rules](/vpc/docs/firewalls) . For details, see the [Dataproc Cluster Network Configuration](/dataproc/docs/concepts/configuring-clusters/network) guide and the [Access YARN](#access_yarn) section below.\nIn addition to network access, your Dataproc cluster needs permissions to access resources. For example, to write data to a Cloud Storage bucket, your Dataproc cluster must have write access to the bucket. You establish access by using [roles](/storage/docs/access-control/iam#roles) . Scan through your Spark code and find all the non\u2013Dataproc resources that the code needs and grant the correct roles to the cluster's [service account](/dataproc/docs/concepts/iam/iam#service_accounts) . Also, ensure that users who will create clusters, jobs, operations, and workflow templates have the [right permissions](/dataproc/docs/concepts/iam/iam) .\nFor more details and best practices, see the [IAM documentation](/iam/docs) .\n### Verify Spark and other library dependencies\nCompare your version of Spark and the versions of other libraries to the official Dataproc [version list](/dataproc/docs/concepts/versioning/dataproc-versions) and look for any libraries that are not already available. We recommend that you use Spark versions that are officially supported by Dataproc.\nIf you need to add libraries, you can do the following:\n- Create a [custom image](/dataproc/docs/guides/dataproc-images) of a Dataproc cluster.\n- Create [initialization scripts](/dataproc/docs/concepts/configuring-clusters/init-actions) in Cloud Storage for your cluster. You can use initialization scripts to install additional dependencies, to copy binaries, and so on.\n- Recompile your Java or Scala code and package all additional dependencies that are not part of the base distribution as a \"fat jar\" by using Gradle, Maven, Sbt, or another tool.\n### Adjust Dataproc cluster size\nIn any cluster configuration, whether on-premises or in the cloud, the cluster size is crucial for Spark job performance. A Spark job without enough resources will either be slow or will fail, especially if it does not have enough executor memory. For advice on what you need to consider when sizing any Hadoop cluster, see the [sizing your cluster](/architecture/hadoop/hadoop-gcp-migration-jobs#sizing_your_cluster) section of the Hadoop migration guide.\nThe following sections describe some options for how to size your cluster.\nLook at how your current Spark jobs are configured and make sure that the Dataproc cluster is large enough. If you move from a shared cluster to multiple Dataproc clusters (one for each batch workload), look at the YARN configuration for each application so that you understand how many executors you need, the number of CPUs per executor, and the total executor memory. If your on-premises cluster has YARN queues set up, see which jobs are sharing each queue's resources and identify bottlenecks. This migration is an opportunity to remove any resource restrictions you might have had in your on-premises cluster.\nChoose the number and [type of VMs](/dataproc/pricing) to match the needs of your workload. If you have decided to use local HDFS for storage, make sure the VMs have the right [disk type and size](#adjust_storage_size) . Don't forget to include the resource needs of the driver programs in your calculation.\nEach VM has a [networking egress cap](/vpc/docs/advanced-vpc#measurenetworkthroughput) of 2 Gbps per vCPU. Writing to persistent disks or to persistent SSDs [counts toward this cap](/compute/docs/disks/performance#egress_performance_cap) , so a VM with a very low number of vCPUs might be throttled by the cap when it writes to these disks. This is likely to happen in the shuffle phase, when Spark writes shuffle data to disk and moves shuffle data over the network between executors. Persistent disks require at least 2 vCPUs to reach maximum write performance and persistent SSDs require 4 vCPUs. Note that these minimums don't take into account traffic such as communication between VMs. Also, the size of each disk affects its [peak performance](/compute/docs/disks/performance#type_comparison) .\nThe configuration you choose will have an impact on the cost of your Dataproc cluster. Dataproc pricing is in addition to the Compute Engine per-instance price for each VM and [other Google Cloud resources](/dataproc/pricing#use_of_other_google_cloud_resources) . For more information, and to use the Google Cloud [pricing calculator](/products/calculator) to get an estimate of your costs, see the Dataproc [pricing page](/dataproc/pricing) .\nWhen you've finished the job migration phase, but before you stop running Spark workloads in your on-premises cluster, benchmark your Spark jobs and consider any [optimizations](#optimize_performance) . Remember that you can [resize](#scale_and_resize_dataprocnameshort_clusters_idscaleandresizeclouddataprocclusters) your cluster if your configuration is not optimal.\nUse [Dataproc Serverless](https://cloud.google.com/dataproc-serverless/docs/overview) to run Spark workloads without provisioning and managing your own cluster. Specify workload parameters, and then submit the workload to the Dataproc Serverless service. The service will run the workload on a managed compute infrastructure, autoscaling resources as needed. Dataproc Serverless charges apply only to the time when the workload is executing.\n## Performing the migration\nThis section discusses migrating the data, changing job code, and changing how jobs are run.\n### Migrate data\nBefore you run any Spark jobs in your Dataproc cluster, you need to migrate your data to Google Cloud. For more information, see the [Data Migration Guide](/architecture/hadoop/hadoop-gcp-migration-data) .\n### Migrate Spark code\nAfter you've planned your migration to Dataproc and moved any required data sources, you can migrate the job code. If there are no differences in Spark versions between the two clusters, and if you want to store data on Cloud Storage instead of local HDFS, you only need to change the prefix of all of your HDFS file paths from `hdfs://` to `gs://` .\nIf you are using different Spark versions, consult the [Spark release notes](https://spark.apache.org/downloads.html) , compare the two versions, and adapt your Spark code accordingly.\nYou can copy the jar files for your Spark applications either [to the Cloud Storage bucket](/dataproc/docs/tutorials/spark-scala#copy_jar_to_cloud_storage) that is tied to your Dataproc cluster or to an HDFS folder. The next section explains the available options for running Spark jobs.\nIf you decide to use [workflow templates](/dataproc/docs/concepts/workflows/overview) , we recommend that you separately test each Spark job that you plan to add. Then you can run a final test run of the template to make sure the template's workflow is correct (no missing upstream jobs, outputs are stored in the right locations, and so on).\n### Run jobs\nYou can run Spark jobs in the following ways:\n- By using the following `gcloud` command:```\ngcloud dataproc jobs submit [COMMAND]\n```where:`[COMMAND]` is `spark` , `pyspark` , or `spark-sql`You can set Spark properties with the `--properties` option. For more information, see the [documentation](/sdk/gcloud/reference/dataproc/jobs/submit) for this command.\n- By using the same process you used before you migrated the job to Dataproc. The Dataproc cluster must be accessible from on-premises, and you need to use the same configuration.\n- By using [Cloud Composer](/composer) . You can [create an environment](/composer/docs/how-to/managing/creating) (a managed Apache Airflow server), define multiple Spark jobs as a [DAG workflow](/composer/docs/how-to/using/writing-dags) , and then run the entire workflow.\nFor more details, see the [Submit a Job](/dataproc/docs/guides/submit-job) guide.\n## Managing jobs after migration\nAfter you move the Spark jobs to Google Cloud, it's important to manage these jobs by using the tools and mechanisms provided by Google Cloud. In this section, we discuss logging, monitoring, accessing clusters, scaling clusters, and optimizing jobs.\n### Use logging and performance monitoring\nIn Google Cloud, you can use [Cloud Logging](/logging) and [Cloud Monitoring](/dataproc/docs/guides/stackdriver-monitoring) to view and customize logs, and to monitor jobs and resources.\nThe best way to find what error caused a Spark job failure is to look at the driver output and the logs generated by the Spark executors.\n**Note:** If you submit the Spark job by connecting directly to the master node using SSH, it's not possible to get the driver output.\nYou can retrieve the driver program output by using the Google Cloud console or by using a `gcloud` command. The output is also stored in the Cloud Storage bucket of the Dataproc cluster. For more details, see the section on [job driver output](/dataproc/docs/guides/driver-output#accessing_job_driver_output) in the Dataproc documentation.\n**Note:** When you view the driver program output in the Google Cloud console, the progress bar is stored as a long line without a newline at the end. Therefore, the first thing the driver program prints will appear at the end of this line. To view this output more easily, you can click **Line wrapping** .\nAll other logs are located in different files inside the machines of the cluster. It's possible to see the logs for each container from the Spark app web UI (or from the History Server after the program ends) in the executors tab. You need to browse through each Spark container to view each log. If you write logs or print to `stdout` or `stderr` in your application code, the logs are saved in the redirection of `stdout` or `stderr` .\nIn a Dataproc cluster, YARN is configured to collect all these logs by default, and they're available in [Cloud Logging](/logging) . Cloud Logging provides a consolidated and concise view of all logs so that you don't need to spend time browsing among container logs to find errors.\nThe following figure shows the Cloud Logging page in the Google Cloud console. You can view all logs from your Dataproc cluster by selecting the cluster's name in the selector menu. Don't forget to expand the time duration in the time-range selector.\nYou can get logs from a Spark application by filtering by its ID. You can get the application ID from the driver output.\nTo find logs faster, you can create and use your own [labels](/dataproc/docs/concepts/labels) for each cluster or for each Dataproc job. For example, you can create a label with the key `env` and the value `exploration` and use it for your data exploration job. You can then get logs for all exploration job creations by filtering with `label:env:exploration` in Cloud Logging. Note that this filter will not return all logs for this job, only the resource creation logs.\nYou can [set the driver log level](/dataproc/docs/guides/driver-output#configuring_logging) using the following `gcloud` command:\n```\ngcloud dataproc jobs submit hadoop --driver-log-levels\n```\nYou set the log level for the rest of the application from the Spark context. For example:\n```\nspark.sparkContext.setLogLevel(\"DEBUG\")\n```\nCloud Monitoring can monitor the cluster's CPU, disk, network usage, and YARN resources. You can create a custom dashboard to get up- to-date charts for these and other metrics. Dataproc runs on top of Compute Engine. If you want to visualize CPU usage, disk I/O, or networking metrics in a chart, you need to select a Compute Engine VM instance as the resource type and then filter by the cluster name. The following diagram shows an example of the output.\nTo view metrics for Spark queries, jobs, stages, or tasks, connect to the Spark application's web UI. The next section section explains how to do this. For details on how to create custom metrics, see the [Custom Metrics from the Agent](/monitoring/agent/custom-metrics-agent) guide.\n### Access YARN\nYou can access the YARN resource manager web interface from outside the Dataproc cluster by setting up an [SSH tunnel](/dataproc/docs/concepts/accessing/cluster-web-interfaces) . It's preferred to use the lightweight [SOCKS proxy instead of local port forwarding](/dataproc/docs/concepts/accessing/cluster-web-interfaces#can_i_use_local_port_forwarding_instead_of_a_socks_proxy) , because browsing through the web interface is easier that way.\nThe following URLs are useful for YARN access:\n- YARN resource manager: `http://[MASTER_HOST_NAME]:8088`\n- Spark history server: `http://[MASTER_HOST_NAME]:18080`\nIf the Dataproc cluster has only [internal IP addresses](/dataproc/docs/concepts/configuring-clusters/network#create_a_cloud_dataproc_cluster_with_internal_ip_address_only) , you can connect either through a VPN connection or through a bastion host. For more information, see [Choose a connection option for internal-only VMs](/compute/docs/connect/ssh-internal-ip) .\n### Scale and resize Dataproc clusters\nThe Dataproc cluster can be [scaled](/dataproc/docs/concepts/configuring-clusters/scaling-clusters) by increasing or decreasing the number of primary or secondary (preemptible) workers. Dataproc also supports [graceful decommissioning](/dataproc/docs/concepts/configuring-clusters/scaling-clusters#graceful_decommissioning) .\nDownscaling in Spark is affected by a number of factors. Take into account the following:\n- We [do not recommend](#identify_job_types_and_plan_clusters) using [ExternalShuffleService](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/ExternalShuffleService.scala) , especially if you downscale the cluster periodically. Shuffling uses the results that have been written to the worker's local disk after the compute phase has been executed, so the node can't be removed even if compute resources are not being consumed anymore.\n- Spark [caches](https://spark.apache.org/docs/latest/tuning.html#memory-management-overview) data in memory (both RDDs and datasets), and executors that are used for caching never exit. As a result, if a worker is used for caching, it will never be gracefully decommissioned. Removing workers forcefully would influence the overall performance, because the cached data would be lost.\n- Spark Streaming has dynamic allocation [disabled](https://github.com/apache/spark/blob/branch-2.0/streaming/src/main/scala/org/apache/spark/streaming/scheduler/ExecutorAllocationManager.scala#L189) by default, and the configuration key that sets this behavior is not documented. (You can follow a discussion of dynamic allocation behavior in a [Spark issues thread](https://issues.apache.org/jira/browse/SPARK-12133) .) If you're using Spark Streaming or Spark Structured Streaming, you must also explicitly disable [dynamic allocation](https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation) as discussed earlier under [Identify job types and plan clusters](#identify_job_types_and_plan_clusters) .\nIn general, we recommend that you avoid downscaling a Dataproc cluster if you're running batch or streaming workloads.\n### Optimize performance\nThis section discusses ways to get better performance and reduce cost while running Spark jobs.\nTo get optimal performance, split your data in Cloud Storage into files with sizes from 128 MB to 1 GB. Using lots of small files can create a bottleneck. If you have many small files, consider copying files for processing to the local HDFS and then copying the results back.\nIf you perform many shuffling operations or partitioned writes, switch to SSDs to boost performance.\nTo reduce networking costs and to boost performance, use the same regional location for your Cloud Storage buckets that you use for your Dataproc clusters.\nBy default, when you use global or regional Dataproc [endpoints](/dataproc/docs/concepts/regional-endpoints) , your cluster's VMs will be placed into the same zone (or another zone in the same region that has enough capacity) when the cluster is created. You can also specify the zone when the cluster is created.\nDataproc cluster can use [preemptible VM](/compute/docs/instances/preemptible) instances as workers. This results in lower per-hour compute costs for your non-critical workloads than by using normal instances. However, there are some factors to consider when you use preemptible VMs:\n- Preemptible VMs can't be used for HDFS storage.\n- As a default, preemptible VMs are created with a smaller boot disk size, and you might want to override this configuration if you are running shuffle-heavy workloads. For details, see the [page on preemptible VMs](/dataproc/docs/concepts/compute/preemptible-vms#using_preemptibles_in_a_cluster) in the Dataproc documentation.\n- We don't recommend making more than half of your total workers preemptible.\n- When you use preemptible VMs, we recommend adjusting your cluster configuration to be more tolerant to task failures, because VMs might be less available. For example, make settings like the following in the YARN configuration:```\nyarn.resourcemanager.am.max-attempts\nmapreduce.map.maxattempts\nmapreduce.reduce.maxattempts\nspark.task.maxFailures\nspark.stage.maxConsecutiveAttempts\n```\n- You can easily add or remove preemptible VMs from your cluster. For more details, see [Preemptible VMs](/dataproc/docs/concepts/compute/preemptible-vms) .## What's next\n- See the guide on how to [migrate on-premises Hadoop infrastructure to Google Cloud](/architecture/hadoop) .\n- See our description of a [Life of a Dataproc Job](/dataproc/docs/concepts/jobs/life-of-a-job) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}