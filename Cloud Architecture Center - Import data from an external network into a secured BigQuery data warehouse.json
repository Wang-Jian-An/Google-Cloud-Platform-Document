{"title": "Cloud Architecture Center - Import data from an external network into a secured BigQuery data warehouse", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Import data from an external network into a secured BigQuery data warehouse\nLast reviewed 2023-08-15 UTC\nMany organizations deploy data warehouses that store confidential information so that they can analyze the data for various business purposes. This document is intended for data engineers and security administrators who deploy and secure data warehouses using BigQuery. It's part of a security blueprint that includes the following:\n- A [GitHub repository](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse-onprem-ingest) that contains a set of Terraform configurations and scripts. The Terraform configuration sets up an environment in Google Cloud that supports a data warehouse that stores confidential data.\n- A guide to the architecture, design, and security controls that you use this blueprint to implement (this document).\nThis document discusses the following:\n- The architecture and Google Cloud services that you can use to help secure a data warehouse in a production environment.\n- Best practices for importing data into BigQuery from an external network such as an on-premises environment.\n- Best practices for [data governance](/blog/topics/developers-practitioners/bigquery-admin-reference-guide-data-governance) when creating, deploying, and operating a data warehouse in Google Cloud, including column-level encryption, differential handling of confidential data, and column-level access controls.\nThis document assumes that you have already configured a foundational set of security controls as described in the [Google Cloud enterprise foundations blueprint](/architecture/security-foundations) . It helps you to layer additional controls onto your existing security controls to help protect confidential data in a data warehouse.\n", "content": "## Data warehouse use cases\nThe blueprint supports the following use cases:\n- Import data from an on-premises environment or another cloud into a BigQuery warehouse (this document)\n- [Import data from Google Cloud into a secured BigQuery data warehouse](/architecture/confidential-data-warehouse-blueprint) ## Overview\nData warehouses such as [BigQuery](/bigquery) let businesses analyze their business data for insights. Analysts access the business data that is stored in data warehouses to create insights. If your data warehouse includes data that you consider confidential, you must take measures to preserve the security, confidentiality, integrity, and availability of the business data while it's imported and stored, while it's in transit, or while it's being analyzed. In this blueprint, you do the following:\n- Encrypt your source data that's located outside of Google Cloud (for example, in an on-premises environment) and import it into BigQuery.\n- Configure controls that help secure access to confidential data.\n- Configure controls that help secure the data pipeline.\n- Configure an appropriate separation of duties for different personas.\n- Set up appropriate security controls and logging to help protect confidential data.\n- Use data classification, policy tags, dynamic data masking, and column-level encryption to restrict access to specific columns in the data warehouse.## Architecture\nTo create a confidential data warehouse, you need to import data securely and then store the data in a VPC Service Controls perimeter. The following image shows how data is ingested and stored.\nThe architecture uses a combination of the following Google Cloud services and features:\n- [Dedicated Interconnect](/network-connectivity/docs/interconnect/concepts/dedicated-overview) lets you move data between your network and Google Cloud. You can use another connectivity option, as described in [Choosing a Network Connectivity product](/network-connectivity/docs/how-to/choose-product) .\n- [Identity and Access Management (IAM)](/iam) and [Resource Manager](/resource-manager) restrict access and segment resources. The access controls and resource hierarchy follow the principle of least privilege.\n- [VPC Service Controls](/vpc-service-controls/docs/overview) creates [security perimeters](/vpc-service-controls/docs/service-perimeters) that isolate services and resources by setting up authorization, access controls, and [secure data exchange](/vpc-service-controls/docs/secure-data-exchange) . The perimeters are as follows:- A data ingestion perimeter that accepts incoming data (in batch or stream). A separate perimeter helps to protect the rest of your workloads from incoming data.\n- A data perimeter that isolates the encryption data from other workloads.\n- A governance perimeter that stores the encryption keys and defines what is considered confidential data.\nThese perimeters are designed to protect incoming content, isolate confidential data by setting up additional access controls and monitoring, and separate your governance from the actual data in the warehouse. Your governance includes key management, data catalog management, and logging.\n- [Cloud Storage](/storage) and [Pub/Sub](/pubsub) receive data as follows:- **Cloud Storage:** receives and stores batch data. By default, Cloud Storage uses TLS to encrypt data in transit and AES-256 to encrypt data in storage. The encryption key is a [customer-managed encryption key (CMEK)](/kms/docs/cmek) . For more information about encryption, see [Data encryption options](/storage/docs/encryption) .You can help to secure access to Cloud Storage buckets using security controls such as Identity and Access Management, access control lists (ACLs), and policy documents. For more information about supported access controls, see [Overview of access control](/storage/docs/access-control) .\n- **Pub/Sub:** receives and stores streaming data. Pub/Sub uses [authentication](/pubsub/docs/authentication) , [access controls](/pubsub/docs/access-control) , and [message-level encryption](/pubsub/docs/encryption#using-cmek) with a CMEK to protect your data.\n- Cloud Functions is triggered by Cloud Storage and writes the data that Cloud Storage uploads to the ingestion bucket into BigQuery.\n- A [Dataflow](/dataflow) pipeline writes streaming data into BigQuery. To protect data, Dataflow uses a unique service account and access controls. To help secure pipeline execution by moving it to the backend service, Dataflow uses [Streaming Engine](/dataflow/docs/guides/deploying-a-pipeline#streaming-engine) . For more information, see [Dataflow security and permissions](/dataflow/docs/concepts/security-and-permissions) .\n- [Cloud Data Loss Prevention (Cloud DLP)](/dlp) scans data that is stored in BigQuery to find any sensitive data that isn't protected. For more information, see [Using Cloud DLP to scan BigQuery data](/bigquery/docs/scan-with-dlp) .\n- [Cloud HSM](/kms/docs/hsm) hosts the key encryption key (KEK). Cloud HSM is a cloud-based Hardware Security Module (HSM) service. You use Cloud HSM to generate the encryption key that you use to encrypt the data in your network before sending it to Google Cloud.\n- [Data Catalog](/data-catalog) automatically categorizes confidential data with metadata, also known as [policy tags](/bigquery/docs/column-level-security-intro) , when it's discovered in BigQuery. Data Catalog also uses metadata to manage access to confidential data. For more information, see [Data Catalog overview](/data-catalog/docs/concepts/overview) . To control access to data within the data warehouse, you apply policy tags to columns that include confidential data.\n- [BigQuery](/bigquery) stores the encrypted data and the wrapped encryption key in separate tables.BigQuery uses various security controls to help protect content, including [access controls](/bigquery/docs/access-control) , [column-level encryption](/bigquery/docs/column-key-encrypt) , [column-level security](/bigquery/docs/column-level-security-intro) , and [data encryption](/bigquery/docs/customer-managed-encryption) .\n- Security Command Center monitors and reviews security findings from across your Google Cloud environment in a central location.\n- [Cloud Logging](/logging/docs) collects all the logs from Google Cloud services for storage and retrieval by your analysis and investigation tools.\n- [Cloud Monitoring](/monitoring/docs/monitoring-overview) collects and stores performance information and metrics about Google Cloud services.\n- [Data Profiler for BigQuery](/dlp/docs/data-profiles) automatically scans for sensitive data in all BigQuery tables and columns across the entire organization, including all folders and projects.## Organization structure\nYou group your organization's resources so that you can manage them and separate your testing environments from your production environment. [Resource Manager](/resource-manager) lets you logically group resources by project, folder, and organization.\nThe following diagram shows you a resource hierarchy with folders that represent different environments such as bootstrap, common, production, non-production (or staging), and development. This hierarchy aligns with the [organization structure used by the enterprise foundations blueprint](/architecture/security-foundations/organization-structure) . You deploy most of the projects in the blueprint into the production folder, and the Data governance project in the common folder which is used for governance.\nFor alternative resource hierarchies, see [Decide a resource hierarchy for your Google Cloud landing zone](/architecture/landing-zones/decide-resource-hierarchy) .\n### Folders\nYou use folders to isolate your production environment and governance services from your non-production and testing environments. The following table describes the folders from the enterprise foundations blueprint that are used by this blueprint.\n| Folder   | Description                                               |\n|:---------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Bootstrap  | Contains resources required to deploy the enterprise foundations blueprint.                               |\n| Common   | Contains centralized services for the organization, such as the Data governance project.                            |\n| Production  | Contains projects that have cloud resources that have been tested and are ready to use. In this blueprint, the Production folder contains the Data ingestion project and Data project.     |\n| Non-production | Contains projects that have cloud resources that are currently being tested and staged for release. In this blueprint, the Non-production folder contains the Data ingestion project and Data project. |\n| Development | Contains projects that have cloud resources that are currently being developed. In this blueprint, the Development folder contains the Data ingestion project and Data project.      |\nYou can change the names of these folders to align with your organization's folder structure, but we recommend that you maintain a similar structure. For more information, see the [Google Cloud enterprise foundations blueprint](/architecture/security-foundations/organization-structure) .\n### Projects\nYou isolate parts of your environment using projects. The following table describes the projects that are needed within the organization. You create these projects when you run the Terraform code. You can change the names of these projects, but we recommend that you maintain a similar project structure.\n| Project   | Description                    |\n|:----------------|:------------------------------------------------------------------------------------------|\n| Data ingestion | Contains services that are required to receive data and write it to BigQuery.    |\n| Data governance | Contains services that provide key management, logging, and data cataloging capabilities. |\n| Data   | Contains services that are required to store data.          |\nIn addition to these projects, your environment must also include a project that hosts a Dataflow [Flex Template](/dataflow/docs/guides/templates/using-flex-templates) job. The Flex Template job is required for the streaming data pipeline.\n## Mapping roles and groups to projects\nYou must give different user groups in your organization access to the projects that make up the confidential data warehouse. The following sections describe the blueprint recommendations for user groups and role assignments in the projects that you create. You can customize the groups to match your organization's existing structure, but we recommend that you maintain a similar segregation of duties and role assignment.\n### Data analyst group\nData analysts view and analyze the data in the warehouse. This group can view data after it has been loaded into the data warehouse and perform the same operations as the [Encrypted data viewer group](#encrypted-data-viewer-group) . This group requires roles in different projects, as described in the following table.\n| Scope of assignment | Roles                                                                      |\n|:-----------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Data ingestion project | Dataflow Developer (roles/dataflow.developer) Dataflow Viewer (roles/dataflow.viewer) Logs Viewer (roles/logging.viewer)                                          |\n| Data project   | BigQuery Data Viewer (roles/bigquery.dataViewer) BigQuery Job User (roles/bigquery.jobUser) BigQuery User (roles/bigquery.user) Dataflow Developer (roles/dataflow.developer) Dataflow Viewer (roles/dataflow.viewer) DLP Administrator (roles/dlp.admin) Logs Viewer (roles/logging.viewer) |\n| Data policy level  | Masked Reader (roles/bigquerydatapolicy.maskedReader)                                                          |\n### Encrypted data viewer group\nThe Encrypted data viewer group can view encrypted data from BigQuery reporting tables through Cloud Looker Studio and other reporting tools, such as SAP Business Objects. The encrypted data viewer group can't view cleartext data from encrypted columns.\nThis group requires the BigQuery User ( `roles/bigquery.jobUser` ) role in the Data project. This group also requires the Masked Reader ( `roles/bigquerydatapolicy.maskedReader` ) at the data policy level.\n### Plaintext reader group\nThe Plaintext reader group has the required permission to call the decryption user-defined function (UDF) to view plaintext data and the additional permission to read unmasked data. This group requires roles in Data project, as described in the following table.\nThis group requires the following roles in the Data project:\n- BigQuery User (`roles/bigquery.user`)\n- BigQuery Job User (`roles/bigquery.jobUser`)\n- [Cloud KMS Viewer (roles/cloudkms.viewer)](/kms/docs/reference/permissions-and-roles#cloudkms.viewer) \nIn addition, this group requires the [Fine-Grained Reader (roles/datacatalog.categoryFineGrainedReader)](/iam/docs/understanding-roles#datacatalog.categoryFineGrainedReader) role at the Data Catalog level.\n### Data engineer group\nData engineers set up and maintain the data pipeline and warehouse. This group requires roles in different projects, as described in the following table.\n| Scope of assignment | Roles                                                                                     |\n|:-----------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Data ingestion project | Cloud Build Editor (roles/cloudbuild.builds.editor) Cloud KMS (roles/cloudkms.viewer) Composer User (roles/composer.user) Compute Network User (roles/compute.networkUser) Dataflow Admin (roles/dataflow.admin) Logs Viewer (roles/logging.viewer)                         |\n| Data project   | BigQuery Data Editor (roles/bigquery.dataeditor) BigQuery Job User (roles/bigquery.jobUser) Cloud Build Editor (roles/cloudbuild.builds.editor) Cloud KMS Viewer (roles/cloudkms.viewer) Compute Network User (roles/compute.networkuser) Dataflow Admin (roles/dataflow.admin) DLP Administrator (roles/dlp.admin) Logs Viewer (roles/logging.viewer) |\n### Network administrator group\nNetwork administrators configure the network. Typically, they are members of the networking team.\nNetwork administrators require the following roles at the organization level:\n- Compute Admin (`roles/compute.networkAdmin`)\n- Logs Viewer (`roles/logging.viewer`)\n### Security administrator group\nSecurity administrators administer security controls such as access, keys, firewall rules, VPC Service Controls, and the [Security Command Center](/security-command-center/docs/concepts-security-command-center-overview) .\nSecurity administrators require the following roles at the organization level:\n- [Access Context Manager Admin (roles/accesscontextmanager.policyAdmin)](/iam/docs/understanding-roles#accesscontextmanager.policyAdmin) \n- [Cloud Asset Viewer (roles/cloudasset.viewer)](/iam/docs/understanding-roles#cloudasset.viewer) \n- [Cloud KMS Admin (roles/cloudkms.admin)](/kms/docs/reference/permissions-and-roles#cloudkms.admin) \n- [Compute Security Admin (roles/compute.securityAdmin)](/compute/docs/access/iam#compute.securityAdmin) \n- [Data Catalog Admin (roles/datacatalog.admin)](/iam/docs/understanding-roles#datacatalog.admin) \n- DLP Administrator (`roles/dlp.admin`)\n- [Logging Admin (roles/logging.admin)](/logging/docs/access-control#logging.admin) \n- [Organization Administrator (roles/orgpolicy.policyAdmin)](/resource-manager/docs/access-control-org#resourcemanager.organizationAdmin) \n- [Security Admin (roles/iam.securityAdmin)](/iam/docs/understanding-roles#iam.securityAdmin) \n### Security analyst group\nSecurity analysts monitor and respond to security incidents and Cloud DLP findings.\nSecurity analysts require the following roles at the organization level:\n- [Access Context Manager Reader (roles/accesscontextmanager.policyReader)](/iam/docs/understanding-roles#accesscontextmanager.policyReader) \n- [Compute Network Viewer (roles/compute.networkViewer)](/compute/docs/access/iam#compute.networkViewer) \n- [Data Catalog Viewer (roles/datacatalog.viewer)](/iam/docs/understanding-roles#datacatalog.viewer) \n- Cloud KMS Viewer (`roles/cloudkms.viewer`)\n- Logs Viewer (`roles/logging.viewer`)\n- [Organization Policy Viewer (roles/orgpolicy.policyViewer)](/iam/docs/understanding-roles#orgpolicy.policyViewer) \n- [Security Center Admin Viewer (roles/securitycenter.adminViewer)](/iam/docs/understanding-roles#securitycenter.adminViewer) \n- [Security Center Findings Editor (roles/securitycenter.findingsEditor)](/iam/docs/understanding-roles#securitycenter.findingsEditor) \n- One of the following Security Command Center roles:- [Security Center Findings Bulk Mute Editor (roles/securitycenter.findingsBulkMuteEditor)](/iam/docs/understanding-roles#securitycenter.findingsBulkMuteEditor) \n- [Security Center Finders Mute Setter (roles/securitycenter.findingsMuteSetter)](/iam/docs/understanding-roles#securitycenter.findingsMuteSetter) \n- [Security Center Findings State Setter (roles/securitycenter.findingsStateSetter)](/iam/docs/understanding-roles#securitycenter.findingsStateSetter) \n### Example group access flows\nThe following sections describe access flows for two groups within the secured data warehouse solution.\nThe following diagram shows what occurs when a user from the [Encrypted data viewer group](#encrypted-data-viewer-group) tries to access encrypted data in BigQuery.\nThe steps to access data in BigQuery are as follows:\n- The Encrypted data viewer executes the following query on BigQuery to access confidential data:```\nSELECT ssn, pan FROM cc_card_table\n```\n- BigQuery verifies access as follows:- The user is authenticated using valid, unexpired Google Cloud credentials.\n- The user identity and the IP address that the request originated from are part of the allowlist in the Access Level/Ingress rule on the VPC Service Controls perimeter.\n- IAM verifies that the user has the appropriate roles and is authorized to access selected encrypted columns on the BigQuery table.BigQuery returns the confidential data in encrypted format.\nThe following diagram shows what occurs when a user from the [Plaintext reader group](#plaintext-reader-group) tries to access encrypted data in BigQuery.\nThe steps to access data in BigQuery are as follows:\n- The Plaintext reader executes the following query on BigQuery to access confidential data in decrypted format:```\nSELECT decrypt_ssn(ssn) FROM cc_card_table\n```\n- BigQuery calls the decrypt [user-defined function (UDF)](/bigquery/docs/user-defined-functions) within the query to access protected columns.\n- Access is verified as follows:- IAM verifies that the user has appropriate roles and is authorized to access the decrypt UDF on BigQuery.\n- The UDF retrieves the wrapped data encryption key (DEK) that was used to protect sensitive data columns.\nThe decrypt UDF calls the key encryption key (KEK) in Cloud HSM to unwrap the DEK. The decrypt UDF uses the [BigQuery AEAD decrypt function](/bigquery/docs/aead-encryption-concepts#decryption) to decrypt the sensitive data columns.\n- The user is granted access to the plaintext data in the sensitive data columns.## Understanding the security controls you need\nThis section discusses the security controls within Google Cloud that you use to help to secure your data warehouse. The key security principles to consider are as follows:\n- Secure access by adopting least privilege principles.\n- Secure network connections through segmentation design and policies.\n- Secure the configuration for each of the services.\n- Classify and protect data based on its risk level.\n- Understand the security requirements for the environment that hosts the data warehouse.\n- Configure sufficient monitoring and logging for detection, investigation, and response.\n### Security controls for data ingestion\nTo create your data warehouse, you must transfer data from another source in your on-premises environment, another cloud, or another Google Cloud source. This document focuses on transferring data from your on-premises environment or another cloud; if you're transferring data from another Google Cloud source, see [Import data from Google Cloud into a secured BigQuery data warehouse](/architecture/confidential-data-warehouse-blueprint) .\nYou can use one of the following options to transfer your data into the data warehouse on BigQuery:\n- A batch job that loads data to a Cloud Storage bucket.\n- A streaming job that uses Pub/Sub.\nTo help protect data during ingestion, you can use client-side encryption, firewall rules, and access level policies. The ingestion process is sometimes referred to as an [extract, transform, load (ETL) process](/learn/what-is-etl) .\nYou can use Cloud VPN or Cloud Interconnect to protect all data that flows between Google Cloud and your environment. This blueprint recommends Dedicated Interconnect, because it provides a direct connection and high throughput, which are important if you're streaming a lot of data.\nTo permit access to Google Cloud from your environment, you must define allowlisted IP addresses in the access levels policy rules.\n[Virtual Private Cloud (VPC) firewall rules](/vpc/docs/firewalls) control the flow of data into the perimeters. You create firewall rules that deny all egress, except for specific TCP port 443 connections from the restricted.googleapis.com special domain names. The restricted.googleapis.com domain has the following benefits:\n- It helps reduce your network attack surface by using Private Google Access when workloads communicate to Google APIs and services.\n- It ensures that you only use services that support VPC Service Controls.\nFor more information, see [Configuring Private Google Access](/vpc/docs/configure-private-google-access) .\nThe data pipeline requires you to open TCP ports in the firewall, as defined in the [dataflow_firewall.tf](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse-onprem-ingest/blob/main/modules/harness-projects/dataflow_firewall.tf) file in the [harness-projects module](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse-onprem-ingest/tree/main/modules/harness-projects) repository. For more information, see [Configuring internet access and firewall rules](/dataflow/docs/guides/routes-firewall) .\nTo deny resources the ability to use external IP addresses, the [Define allowed external IPs for VM instances (compute.vmExternalIpAccess)](/resource-manager/docs/organization-policy/org-policy-constraints#:%7E:text=compute.vmExternalIpAccess) organization policy is set to deny all.\nAs shown in the [architecture diagram](#architecture) , you place the resources for the data warehouse into separate perimeters. To enable services in different perimeters to share data, you create [perimeter bridges](/vpc-service-controls/docs/share-across-perimeters) .\nPerimeter bridges let protected services make requests for resources outside of their perimeter. These bridges make the following connections:\n- They connect the Data ingestion project to the Data project so that data can be ingested into BigQuery.\n- They connect the Data project to the Data governance project so that Cloud DLP can scan BigQuery for unprotected confidential data.\n- They connect the Data ingestion project to the Data governance project for access to logging, monitoring, and encryption keys.\nIn addition to perimeter bridges, you use [egress rules](/vpc-service-controls/docs/ingress-egress-rules) to let resources that are protected by perimeters to access resources that are outside the perimeter. In this solution, you configure egress rules to obtain the external Dataflow Flex Template jobs that are located in Cloud Storage in an external project. For more information, see [Access a Google Cloud resource outside the perimeter](/vpc-service-controls/docs/secure-data-exchange#allow_access_to_a_google_cloud_resource_outside_the_perimeter) .\nTo help ensure that only specific identities (user or service) can access resources and data, you enable IAM groups and roles.\nTo help ensure that only specific sources can access your projects, you enable an [access policy](/access-context-manager/docs/overview) for your Google organization. We recommend that you create an access policy that specifies the allowed IP address range for requests originating from your on-premises environment and only allows requests from specific users or service accounts. For more information, see [Access level attributes](/access-context-manager/docs/access-level-attributes) .\nBefore you move your sensitive data into Google Cloud, encrypt your data locally to help protect it at rest and in transit. You can use the [Tink](https://developers.google.com/tink) encryption library, or you can use other encryption libraries. The Tink encryption library is compatible with [BigQuery AEAD encryption](/bigquery/docs/column-key-encrypt) , which the blueprint uses to decrypt column-level encrypted data after the data is imported.\nThe Tink encryption library uses DEKs that you can generate locally or from [Cloud HSM](/kms/docs/generate-random#generating_random_bytes) . To wrap or protect the DEK, you can use a KEK that is generated in Cloud HSM. The KEK is a [symmetric CMEK](/kms/docs/cmek) encryption keyset that is stored securely in Cloud HSM and managed using IAM roles and permissions.\nDuring ingestion, both the wrapped DEK and the data are stored in BigQuery. BigQuery includes two tables: one for the data and the other for the wrapped DEK. When analysts need to view confidential data, BigQuery can use [AEAD decryption](/bigquery/docs/reference/standard-sql/aead-encryption-concepts) to unwrap the DEK with the KEK and decrypt the protected column.\nAlso, client-side encryption using Tink further protects your data by encrypting sensitive data columns in BigQuery. The blueprint uses the following Cloud HSM encryption keys:\n- A CMEK key for the ingestion process that's also used by Pub/Sub, Dataflow pipeline for streaming, Cloud Storage batch upload, and Cloud Functions artifacts for subsequent batch uploads.\n- The cryptographic key wrapped by Cloud HSM for the data encrypted on your network using Tink.\n- CMEK key for the BigQuery warehouse in the Data project.\nYou specify the CMEK location, which determines the geographical location that the key is stored and is made available for access. You must ensure that your CMEK is in the same location as your resources. By default, the CMEK is rotated every 30 days.\nIf your organization's compliance obligations require that you manage your own keys externally from Google Cloud, you can enable [Cloud External Key Manager](/kms/docs/ekm) . If you use external keys, you're responsible for key management activities, including key rotation.\nService accounts are identities that Google Cloud can use to run API requests on your behalf. Service accounts ensure that user identities do not have direct access to services. To permit separation of duties, you create service accounts with different roles for specific purposes. These service accounts are defined in the [data-ingestion-sa module](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse-onprem-ingest/tree/main/modules/data-ingestion-sa) and the [data-governance-sa module](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse-onprem-ingest/tree/main/modules/data-governance-sa) .\nThe service accounts are as follows:\n- Cloud Storage service account runs the automated batch data upload process to the ingestion storage bucket.\n- Pub/Sub service account enables streaming of data to Pub/Sub service.\n- Dataflow controller service account is used by the Dataflow pipeline to transform and write data from Pub/Sub to BigQuery.\n- Cloud Functions service account writes subsequent batch data uploaded from Cloud Storage to BigQuery.\n- Storage Upload service account allows the ETL pipeline to create objects.\n- Pub/Sub Write service Account lets the ETL pipeline write data to Pub/Sub.\nThe following table lists the roles that are assigned to each service account:\n| Name        | Roles                                                                                             | Scope of Assignment |\n|:------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------|\n| Dataflow controller service account | BigQuery Data Editor (roles/bigquery.dataEditor) BigQuery Job User (roles/bigquery.jobUser) Dataflow Developer (roles/dataflow.developer) Dataflow Worker (roles/dataflow.worker) Pub/Sub Editor (roles/pubsub.editor) Pub/Sub Subscriber (roles/pubsub.subscriber) Service Usage Consumer (roles/serviceUsage.serverUsageConsumer) Storage Object Viewer (roles/storage.ObjectViewer) | Data ingestion project |\n| Dataflow controller service account | BigQuery Data Editor (roles/bigquery.dataEditor) BigQuery Metadata Viewer (roles/bigquery.metadataViewer)                                                                    | Data project   |\n| Dataflow controller service account | DLP Inspect Findings Reader (roles/dlp.deidentifyTemplatesReader) DLP Inspect Templates Editor (roles/dlp.inspectTemplatesReader) DLP User (roles/dlp.user)                                                       | Data governance  |\n| Cloud Functions service account  | BigQuery Data Editor (roles/bigquery.dataEditor) BigQuery Job User (roles/bigquery.JobUser) Cloud Run Invoker (roles/run.invoker) Eventarc Event Receiver (roles/eventarc.eventReceiver)                                                | Data ingestion project |\n| Cloud Functions service account  | BigQuery Data Editor (roles/bigquery.dataEditor) BigQuery Metadata Viewer (roles/bigquery.metadataViewer)                                                                    | Data project   |\n| Storage Upload service account  | Storage Object Creator (roles/storage.objectCreator) Storage Object Viewer (roles/storage.objectViewer)                                                                    | Data ingestion Project |\n| Pub/Sub Write service account  | Pub/Sub Publisher (roles/pubsub.publisher) Pub/Sub Subscriber (roles/pubsub.subscriber)                                                                        | Data ingestion Project |\n### Security controls for data storage\nYou configure the following security controls to help protect data in the BigQuery warehouse:\n- Column-level access controls\n- Service accounts with limited roles\n- Dynamic data masking of sensitive fields\n- Organization policies\n- Cloud DLP automatic scanning and data profiler\n- VPC Service Controls perimeters between the Data ingestion project and the Data project, with appropriate [perimeter bridges](/architecture/confidential-data-warehouse-blueprint#perimeter-bridges) \n- Encryption and key management, as follows:- Encryption at rest with CMEK keys that are stored in Cloud HSM\n- Column-level encryption using Tink and BigQuery AEAD Encryption\nTo help with sharing and applying data access policies at scale, you can configure [dynamic data masking](/bigquery/docs/column-data-masking-intro) . Dynamic data masking lets existing queries automatically mask column data using the following criteria:\n- The masking rules that are applied to the column at query runtime.\n- The roles that are assigned to the user who is running the query. To access unmasked column data, the data analyst must have the [Fine-Grained Reader](/iam/docs/understanding-roles#datacatalog.categoryFineGrainedReader) role.\nTo define access for columns in BigQuery, you create [policy tags](/bigquery/docs/best-practices-policy-tags) . For example, the taxonomy created in the [standalone example](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse-onprem-ingest/tree/main/examples/standalone) creates the `1_Sensitive` policy tag for columns that include data that cannot be made public, such as the credit limit. The default data masking rule is applied to these columns to hide the value of the column.\nAnything that isn't tagged is available to all users who have access to the data warehouse. These access controls ensure that, even after the data is written to BigQuery, the data in sensitive fields still cannot be read until access is explicitly granted to the user.\n[Column-level encryption](/bigquery/docs/column-key-encrypt) lets you encrypt data in BigQuery at a more granular level. Instead of encrypting an entire table, you select the columns that contain sensitive data within BigQuery, and only those columns are encrypted. BigQuery uses [AEAD encryption and decryption](/bigquery/docs/reference/standard-sql/aead_encryption_functions) functions that create the keysets that contain the keys for encryption and decryption. These keys are then used to encrypt and decrypt individual values in a table, and rotate keys within a keyset. Column-level encryption provides dual-access control on encrypted data in BigQuery, because the user must have permissions to both the table and the encryption key to read data in cleartext.\n[Data profiler](/dlp/docs/data-profiles) lets you identify the locations of sensitive and high risk data in BigQuery tables. Data profiler automatically scans and analyzes all BigQuery tables and columns across the entire organization, including all folders and projects. Data profiler then outputs metrics such as the predicted [infoTypes](/dlp/docs/concepts-infotypes) , the assessed data risk and sensitivity levels, and metadata about your tables. Using these insights, you can make informed decisions about how you protect, share, and use your data.\nYou must limit access to the Data project so that only authorized users can view the sensitive data fields. To do so, you create a service account with the [roles/iam.serviceAccountUser](/compute/docs/access/iam#the_serviceaccountuser_role) role that authorized users must impersonate. [Service account impersonation](/iam/docs/impersonating-service-accounts) helps users to use service accounts without downloading the service account keys, which improves the overall security of your project. Impersonation creates a short-term token that authorized users who have the [roles/iam.serviceAccountTokenCreator](/iam/docs/understanding-roles#iam.serviceAccountTokenCreator) role are allowed to download.\nThis blueprint includes the organization policy constraints that the enterprise foundations blueprint uses and adds additional constraints. For more information about the constraints that the enterprise foundations blueprint uses, see [Organization policy constraints](/architecture/security-foundations/preventative-controls#organization-policy) .\nThe following table describes the additional organizational policy [constraints](/resource-manager/docs/organization-policy/org-policy-constraints) that are defined in the [organization-policies](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse-onprem-ingest/tree/main/modules/organization-policies) module.\n| Policy                   | Constraint Name         | Recommended Value                                      |\n|:-------------------------------------------------------------------------------|:---------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Restrict resource deployments to specific physical locations     | gcp.resourceLocations        | One of the following: in:us-locations in:eu-locations in:asia-locations                        |\n| Require CMEK protection              | gcp.restrictNonCmekServices      | bigquery.googleapis.com                                    |\n| Disable service account creation            | iam.disableServiceAccountCreation     | true                                         |\n| Disable Service account key creation           | disableServiceAccountKeyCreation     | true                                         |\n| Enable OS Login for VMs created in the project         | compute.requireOsLogin        | true                                         |\n| Disable automatic role grants to default service account      | automaticIamGrantsForDefaultServiceAccounts  | true                                         |\n| Allowed ingress settings (Cloud Functions)          | cloudfunctions.allowedIngressSettings    | ALLOW_INTERNAL_AND_GCLB                                    |\n| Restrict new forwarding rules to be internal only, based on IP address   | compute.restrictProtocolForwardingCreationForTypes | INTERNAL                                        |\n| Disable serial port output logging to Cloud Logging       | compute.disableSerialPortLogging     | true                                         |\n| Define the set of shared VPC subnetworks that Compute Engine resources can use | compute.restrictSharedVpcSubnetworks    | projects/PROJECT_ID/regions/REGION/subnetworks/SUBNETWORK-NAME Replace SUBNETWORK-NAME with the resource ID of the private subnet that you want the blueprint to use. |\n### Operational controls\nYou can enable logging and [Security Command Center Premium tier features](/security-command-center/pricing#premium-tier) such as Security Health Analytics and Event Threat Detection. These controls help you to do the following:\n- Monitor who is accessing your data.\n- Ensure that proper auditing is put in place.\n- Generate findings for misconfigured cloud resources.\n- Support the ability of your incident management and operations teams to respond to issues that might occur.[Access Transparency](/logging/docs/audit/access-transparency-overview) provides you with real-time notification in the event [Google support personnel](/cloud-provider-access-management/access-transparency/docs/overview#google-personnel-access) require access to your data. Access Transparency logs are generated whenever a human accesses content, and only Google personnel with valid business justifications (for example, a support case) can obtain access. We recommend that you [enable Access Transparency](/cloud-provider-access-management/access-transparency/docs/enable) .\nTo help you to meet auditing requirements and get insight into your projects, you configure the [Google Cloud Observability](/products/operations) with data logs for services you want to track. The [harness-logging module](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse-onprem-ingest/tree/main/modules/harness-logging) configures the following best practices:\n- [Creating an aggregated log sink](/logging/docs/export/aggregated_sinks) across all projects.\n- Storing your logs in the appropriate region.\n- Adding CMEK keys to your logging sink.\nFor all services within the projects, your logs must include information about data reads and writes, and information about what administrators read. For additional logging best practices, see [Detective controls](/architecture/security-foundations/detective-controls) in the enterprise foundations blueprint.\nAfter you deploy the blueprint, you can set up alerts to notify your security operations center (SOC) that a security incident might be occurring. For example, you can use alerts to let your security analyst know when an IAM permission has changed. For more information about configuring Security Command Center alerts, see [Setting up finding notifications](/security-command-center/docs/how-to-notifications) . For additional alerts that aren't published by Security Command Center, you can set up [alerts](/monitoring/alerts) with Cloud Monitoring.\n### Additional security considerations\nIn addition to the security controls described in this solution, you should review and manage the security and risk in key areas that overlap and interact with your use of this solution. These security considerations include the following:\n- The security of the code that you use to configure, deploy, and run Dataflow jobs and Cloud Functions.\n- The data classification taxonomy that you use with this solution.\n- Generation and management of encryption keys.\n- The content, quality, and security of the datasets that you store and analyze in the data warehouse.\n- The overall environment in which you deploy the solution, including the following:- The design, segmentation, and security of networks that you connect to this solution.\n- The security and governance of your organization's IAM controls.\n- The authentication and authorization settings for the actors to whom you grant access to the infrastructure that's part of this solution, and who have access to the data that's stored and managed in that infrastructure.\n## Bringing it all together\nTo implement the architecture described in this document, do the following:\n- Determine whether you will deploy the blueprint with the enterprise foundations blueprint or on its own. If you choose not to deploy the enterprise foundations blueprint, ensure that your environment has a similar security baseline in place.\n- Set up a [Dedicated Interconnect connection](/network-connectivity/docs/interconnect/concepts/dedicated-overview) with your network.\n- Review the [README](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse-onprem-ingest) for the blueprint and ensure that you meet all the prerequisites.\n- Verify that your user identity has the`iam.serviceAccountUser`and`iam.serviceAccountTokenCreator`roles for your organization's development folder, as described in [Organization structure](#organization-structure) . If you do not have a folder that you use for testing, [create a folder](/resource-manager/docs/creating-managing-folders#creating-folders) and [configure access](/resource-manager/docs/creating-managing-folders#configuring_access_to_folders) .\n- Record your billing account ID, organization's display name, folder ID for your test or demo folder, and the email addresses for the following [user groups](#mapping-roles-and-groups) :- Data analysts\n- Encrypted data viewer\n- Plaintext reader\n- Data engineers\n- Network administrators\n- Security administrators\n- Security analysts\n- Create the Data, Data governance, Data ingestion, and Flex template projects. For a list of APIs that you must enable, see the README.\n- Create the service account for Terraform and assign the appropriate roles for all projects.\n- Set up the Access Control Policy.\n- In your testing environment, deploy the solution:- Clone and run the Terraform scripts to set up an environment in Google Cloud.\n- [Install the Tink encryption library](https://developers.google.com/tink/install-tinkey) on your network.\n- [Set up Application Default Credentials](/docs/authentication/provide-credentials-adc) so that you can run the Tink library on your network.\n- [Create encryption keys](/kms/docs/create-encryption-keys) with Cloud KMS.\n- [Generate encrypted keysets](https://developers.google.com/tink/generate-encrypted-keyset) with Tink.\n- Encrypt data with Tink using one of the following methods:- Using [deterministic encryption](https://developers.google.com/tink/deterministic-encryption) .\n- Using a [helper script with sample data](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse-onprem-ingest/tree/main/examples/standalone/helpers) .\n- Upload encrypted data to BigQuery using streaming or batch uploads.\n- Verify that authorized users can read unencrypted data from BigQuery using the BigQuery AEAD decrypt function. For example, run the following create decryption function:```\nCREATE OR REPLACE FUNCTION `{project_id}.{bigquery_dataset}.decrypt`(encodedText STRING) RETURNS STRING AS(AEAD.DECRYPT_STRING(KEYS.KEYSET_CHAIN('gcp-kms://projects/myProject/locations/us/keyRings/myKeyRing/cryptoKeys/myKeyName', b'\\012\\044\\000\\321\\054\\306\\036\\026\u2026..'),FROM_BASE64(encodedText), \"\"));\n```Run the create view query:```\nCREATE OR REPLACE VIEW `{project_id}.{bigquery_dataset}.decryption_view` ASSELECT\u00a0Card_Type_Code,\u00a0Issuing_Bank,\u00a0Card_Number,\u00a0`bigquery_dataset.decrypt`(Card_Number) AS Card_Number_DecryptedFROM `project_id.dataset.table_name`\n```Run the select query from view:```\nSELECT\u00a0 Card_Type_Code,\u00a0 Issuing_Bank,\u00a0 Card_Number,\u00a0 Card_Number_DecryptedFROM`{project_id}.{bigquery_dataset}.decrypted_view`\n```For additional queries and use cases, see [Column-level encryption with Cloud KMS](/bigquery/docs/column-key-encrypt#use_cases) .\n- Use Security Command Center to scan the newly created projects against your [compliance requirements](/security-command-center/docs/concepts-vulnerabilities-findings) .\n- Deploy the blueprint into your production environment.## What's next\n- Review the [Google Cloud enterprise foundations blueprint](/architecture/security-foundations) for a baseline secure environment.\n- To see the details of the blueprint, read the [Terraform configuration README](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse-onprem-ingest) .\n- To ingest data that is stored in Google Cloud into a BigQuery data warehouse, see [Import data from Google Cloud into a secured BigQuery data warehouse](/architecture/confidential-data-warehouse-blueprint) .\n- For more best practices and blueprints, see the [security best practices center](/security/best-practices) .", "guide": "Cloud Architecture Center"}