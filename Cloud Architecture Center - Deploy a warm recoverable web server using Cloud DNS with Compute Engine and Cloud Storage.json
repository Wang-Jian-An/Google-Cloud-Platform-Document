{"title": "Cloud Architecture Center - Deploy a warm recoverable web server using Cloud DNS with Compute Engine and Cloud Storage", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Deploy a warm recoverable web server using Cloud DNS with Compute Engine and Cloud Storage\nLast reviewed 2021-06-28 UTC\nThis document is intended for architects and people who work in operations and administrative teams. The document describes an example pattern that you can use for your own deployments in Google Cloud.\nIn this architecture, [Cloud DNS](/dns/docs) directs traffic to [Compute Engine](/compute/docs) instances in [managed instance groups](/compute/docs/instance-groups) that serve the content. In an outage, you update the Cloud DNS zone and fail over to a static site in [Cloud Storage](/storage/docs/introduction) .\nTo deploy this solution, you need a registered domain name that you control and want to use with this document.\nIn production deployments, your website likely includes many more files and additional application code on your managed instance group virtual machines (VMs) than is shown in this document. Cloud Storage then hosts a more limited static version that provides minimal functionality. In a warm failover scenario, users see this limited website until the managed instance groups recover and can serve traffic for the full website experience.", "content": "## ArchitectureIn this architecture, you deploy resources to create an environment as shown in the following image:When you need to fail over, you update the Cloud DNS configuration to direct traffic to Cloud Storage, as shown in the following image:This warm failover pattern balances the cost of running another managed instance group in a different region that you only use when the primary region fails. The cost of a static site using Cloud Storage is lower than running another managed instance group, but there's a short delay as you update Cloud DNS between the hosting options. The limited website experience in Cloud Storage is better than a completely unavailable website and poor customer experience.\nFor an alternative approach that uses an external Application Load Balancer instead of Cloud DNS to control the failover, see [Deploy a warm recoverable web server with Compute Engine and Cloud Storage](/architecture/warm-recoverable-static-site-failover-load-balancer) . This pattern is useful if you don't have, or don't want to use, Cloud DNS.\nTo run reliable applications in Google Cloud, we recommend that you design your application infrastructure to handle outages. Depending on your application and business needs, you might need a cold failover, warm failover, or hot failover pattern. For more information on how to determine the best approach for your own applications, see the [Disaster recovery planning guide](/architecture/dr-scenarios-planning-guide) .\nThis document uses a basic [Apache web server](https://httpd.apache.org/) , but the same approach to the infrastructure deployment applies to other application environments you need to create.## Objectives\n- Create regional managed instance groups with a custom VM image.\n- Create a Cloud Storage bucket.\n- Create and configure a Cloud DNS zone.\n- Test the warm web server failover with updated Cloud DNS records.\n- Test the recovery and failback with updated Cloud DNS records.\n## Costs\nIn this document, you use the following billable components of Google Cloud:- [Compute Engine](https://cloud.google.com/compute/vm-instance-pricing) \n- [Networking](https://cloud.google.com/vpc/pricing) \n- [Cloud DNS](https://cloud.google.com/dns#section-5) \n- [Cloud Storage](https://cloud.google.com/storage/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . ## Before you beginSecurity constraints defined by your organization might prevent you from completing the following steps. For troubleshooting information, see [Develop applications in a constrained Google Cloud environment](/resource-manager/docs/organization-policy/develop-apps-constrained-environment) .\n- You can run the Google Cloud CLI in the [Google Cloud console](https://console.cloud.google.com/) without installing the Google Cloud CLI. To run the gcloud CLI in the Google Cloud console, use the Cloud Shell.## Prepare the environmentIn this section, you define some variables for your resource names and locations. These variables are used by the Google Cloud CLI commands as you deploy the resources.\nThroughout this deployment, unless otherwise noted, you enter all commands in [Cloud Shell](https://cloud.google.com/shell) or your local development environment.- Replace `` with your own project ID. If desired, provide your own name suffix for resources to help search for and identify them, such as `` .Specify two regions, such as `` and `` , and a zone within one of those regions, such as `` . This zone defines where the initial base VM is created that's used to create an image for the managed instance group.Finally, set a domain that's used for your static website, such as `` .```\nPROJECT_ID=PROJECT_IDNAME_SUFFIX=appREGION1=us-west1REGION2=us-west2ZONE=us-west1-aDOMAIN=example.com\n```\n## Create a VPC and subnetTo provide network access to the VMs, you create Virtual Private Cloud (VPC) and subnets. As you need managed instance groups in two regions, you create one subnet in each region. For more information on the advantages of the custom subnet mode to manage IP address ranges in use in your environment, see [Use custom mode VPC networks](/solutions/best-practices-vpc-design#custom-mode) .- Create the VPC with a custom subnet mode:```\ngcloud compute networks create network-$NAME_SUFFIX --subnet-mode=custom\n```\n- Now create two subnets in the new VPC, one for each region. Define your own address ranges, such as `` and `` , that fit in your network range:```\ngcloud compute networks subnets create \\\u00a0 \u00a0 subnet-$NAME_SUFFIX-$REGION1 \\\u00a0 \u00a0 --network=network-$NAME_SUFFIX \\\u00a0 \u00a0 --range=10.1.0.0/20 \\\u00a0 \u00a0 --region=$REGION1gcloud compute networks subnets create \\\u00a0 \u00a0 subnet-$NAME_SUFFIX-$REGION2 \\\u00a0 \u00a0 --network=network-$NAME_SUFFIX \\\u00a0 \u00a0 --range=10.2.0.0/20 \\\u00a0 \u00a0 --region=$REGION2\n```\n## Create firewall rulesTo let network traffic flow correctly in the VPC, use [firewall rules](/vpc/docs/firewalls) .- Create firewall rules to allow web traffic and health checks for the load balancer and managed instance groups:```\ngcloud compute firewall-rules create allow-http-$NAME_SUFFIX \\\u00a0 \u00a0 --network=network-$NAME_SUFFIX \\\u00a0 \u00a0 --direction=INGRESS \\\u00a0 \u00a0 --priority=1000 \\\u00a0 \u00a0 --action=ALLOW \\\u00a0 \u00a0 --rules=tcp:80 \\\u00a0 \u00a0 --source-ranges=0.0.0.0/0 \\\u00a0 \u00a0 --target-tags=http-servergcloud compute firewall-rules create allow-health-check-$NAME_SUFFIX \\\u00a0 \u00a0 --network=network-$NAME_SUFFIX \\\u00a0 \u00a0 --action=allow \\\u00a0 \u00a0 --direction=ingress \\\u00a0 \u00a0 --source-ranges=130.211.0.0/22,35.191.0.0/16 \\\u00a0 \u00a0 --target-tags=allow-health-check \\\u00a0 \u00a0 --rules=tcp:80\n```The HTTP rule allows traffic to any VM where the `http-server` tag is applied, and from any source using the `0.0.0.0/0` range. For the [health check rule](/load-balancing/docs/health-checks#firewall_rules) , default ranges for Google Cloud are set to allow the platform to correctly check the health of resources.\n- To allow SSH traffic for the initial configuration of a base VM image, scope the firewall rule to your environment using the `--source-range` parameter. You might need to work with your network team to determine what source ranges your organization uses. **Caution:** We don't recommend using a broad `0.0.0.0/0` range that would allow all traffic. To scope traffic to a single IP address, use a network mask, such as `35.230.62.163/32` .Replace `` with your own IP address scopes:```\ngcloud compute firewall-rules create allow-ssh-$NAME_SUFFIX \\\u00a0 \u00a0 --network=network-$NAME_SUFFIX \\\u00a0 \u00a0 --direction=INGRESS \\\u00a0 \u00a0 --priority=1000 \\\u00a0 \u00a0 --action=ALLOW \\\u00a0 \u00a0 --rules=tcp:22 \\\u00a0 \u00a0 --source-ranges=IP_ADDRESS_SCOPE\n```\n- After you create the firewall rules, verify that the three rules have been added:```\ngcloud compute firewall-rules list \\\u00a0 \u00a0 --project=$PROJECT_ID \\\u00a0 \u00a0 --filter=\"NETWORK=network-$NAME_SUFFIX\"\n```The following example output shows the three rules have been correctly created:```\nNAME     NETWORK  DIRECTION PRIORITY ALLOW\nallow-health-check-app network-app INGRESS 1000  tcp:80\nallow-http-app   network-app INGRESS 1000  tcp:80\nallow-ssh-app   network-app INGRESS 1000  tcp:22\n```\n## Create and configure a base VM imageTo create identical VMs that you deploy without additional configuration, you use a custom VM image. This image captures the OS and Apache configuration, and is used to create each VM in the managed instance group in the next steps.\n **Note:** In this document, you manually create and configure the base VM and images. In production deployments, use your existing deployment tools and pipelines to automate as much of this process as possible. For more information, see [Managing infrastructure as code with Terraform, Cloud Build, and GitOps](/architecture/managing-infrastructure-as-code) .\nOn the VM, you create a basic `index.html` file on the persistent disk and mount it to `/var/www/example.com` . An Apache configuration file at `/etc/apache2/sites-available/example.com.conf` serves web content from the mounted persistent disk location.\nThe following diagram shows the basic HTML page served by Apache that's stored on the persistent disk:You build this environment in the following steps.- Create a base VM with an attached persistent disk:```\ngcloud compute instances create vm-base-$NAME_SUFFIX \\\u00a0 \u00a0 --zone=$ZONE \\\u00a0 \u00a0 --machine-type=n1-standard-1 \\\u00a0 \u00a0 --subnet=subnet-$NAME_SUFFIX-$REGION1 \\\u00a0 \u00a0 --tags=http-server \\\u00a0 \u00a0 --image=debian-10-buster-v20210420 \\\u00a0 \u00a0 --image-project=debian-cloud \\\u00a0 \u00a0 --boot-disk-size=10GB \\\u00a0 \u00a0 --boot-disk-type=pd-balanced \\\u00a0 \u00a0 --boot-disk-device-name=vm-base-$NAME_SUFFIX \\\u00a0 \u00a0 --create-disk=type=pd-ssd,name=disk-base-$NAME_SUFFIX,size=10GB,device-name=disk-base-$NAME_SUFFIX\n```You use parameters defined at the start of this document to name the VM and connect to the correct subnet. Names are also assigned from the parameters for the boot disk and data disk.\n- To install and configure the simple website, first connect to the base VM using SSH:```\ngcloud compute ssh vm-base-$NAME_SUFFIX --zone=$ZONE\n```\n- In your SSH session to the VM, create a script to configure the VM in an editor of your choice. The following example uses [Nano](https://www.nano-editor.org/) as the editor:```\nnano configure-vm.sh\n```Paste the following configuration script into the file:```\n#!/bin/bashNAME_SUFFIX=app# Create directory for the basic website filessudo mkdir -p /var/www/example.comsudo chmod a+w /var/www/example.comsudo chown -R www-data: /var/www/example.com# Find the disk name, then format and mount itDISK_NAME=\"google-disk-base-$NAME_SUFFIX\"DISK_PATH=\"$(find /dev/disk/by-id -name \"${DISK_NAME}\" | xargs -I '{}' readlink -f '{}')\"sudo mkfs.ext4 -m 0 -E lazy_itable_init=0,lazy_journal_init=0,discard $DISK_PATHsudo mount -o discard,defaults $DISK_PATH /var/www/example.com# Install Apachesudo apt-get update && sudo apt-get -y install apache2# Write out a basic HTML file to the mounted persistent disksudo tee -a /var/www/example.com/index.html >/dev/null <<'EOF'<!doctype html><html lang=en><head><meta charset=utf-8>\u00a0 \u00a0 <title>HA / DR example</title></head><body>\u00a0 \u00a0 <p>Welcome to a Compute Engine website with warm failover to Cloud Storage!</p></body></html>EOF# Write out an Apache configuration filesudo tee -a /etc/apache2/sites-available/example.com.conf >/dev/null <<'EOF'<VirtualHost *:80>\u00a0 \u00a0 \u00a0 \u00a0 ServerName www.example.com\u00a0 \u00a0 \u00a0 \u00a0 ServerAdmin webmaster@localhost\u00a0 \u00a0 \u00a0 \u00a0 DocumentRoot /var/www/example.com\u00a0 \u00a0 \u00a0 \u00a0 ErrorLog ${APACHE_LOG_DIR}/error.log\u00a0 \u00a0 \u00a0 \u00a0 CustomLog ${APACHE_LOG_DIR}/access.log combined</VirtualHost>EOF# Enable the Apache configuration file and reload servicesudo a2dissite 000-defaultsudo a2ensite example.com.confsudo systemctl reload apache2\n```Update the `` variable to match the value set at the start of this document, such as .\n- Write out the file and exit your editor. For example, in Nano you use `Ctrl-O` to write out the file, then exit with `Ctrl-X` .\n- Make the configuration script executable, then run it:```\nchmod +x configure-vm.sh./configure-vm.sh\n```\n- Exit the SSH session to the VM:```\nexit\n```\n- Get the IP address of the VM and use `curl` to see the basic web page:```\ncurl $(gcloud compute instances describe vm-base-$NAME_SUFFIX \\\u00a0 \u00a0 --zone $ZONE \\\u00a0 \u00a0 --format=\"value(networkInterfaces.accessConfigs.[0].natIP)\")\n```The basic website is returned, as shown in the following example output:```\n<!doctype html>\n<html lang=en>\n<head>\n<meta charset=utf-8>\n <title>HA / DR example</title>\n</head>\n<body>\n <p>Welcome to a Compute Engine website with warm failover to Cloud Storage!</p>\n</body>\n</html>\n```This step confirms that Apache is configured correctly, and the page is loaded from the attached persistent disk. In the following sections, you create an image using this base VM and configure an instance template with a startup script.\n## Deploy the Compute Engine resourcesThis warm failover pattern uses managed instance groups to run the VMs. The managed instance groups run in two regions, and each group monitors the health of the VMs. If there's an outage and one of the VMs fails, the managed instance group recreates the VM. This configuration creates a highly available application, even without the warm failover to a static site in Cloud Storage.- Before you can create an image, you must stop the VM:```\ngcloud compute instances stop vm-base-$NAME_SUFFIX --zone=$ZONE\n```\n- Run the following set of commands to create the VM images, instance templates, and managed instance groups:```\n# Create the base VM imagesgcloud compute images create image-$NAME_SUFFIX \\\u00a0 \u00a0 --source-disk=vm-base-$NAME_SUFFIX \\\u00a0 \u00a0 --source-disk-zone=$ZONEgcloud compute images create image-disk-$NAME_SUFFIX \\\u00a0 \u00a0 --source-disk=disk-base-$NAME_SUFFIX \\\u00a0 \u00a0 --source-disk-zone=$ZONE# Create instance templatesgcloud compute instance-templates create template-$NAME_SUFFIX-$REGION1 \\\u00a0 \u00a0 --machine-type=n1-standard-1 \\\u00a0 \u00a0 --subnet=projects/$PROJECT_ID/regions/$REGION1/subnetworks/subnet-$NAME_SUFFIX-$REGION1 \\\u00a0 \u00a0 --region=$REGION1 \\\u00a0 \u00a0 --tags=http-server \\\u00a0 \u00a0 --metadata=^,@^startup-script=\\!\\#\\ /bin/bash$'\\n'echo\\ UUID=\\`blkid\\ -s\\ UUID\\ -o\\ value\\ /dev/sdb\\`\\ /var/www/example.com\\ ext4\\ discard,defaults,nofail\\ 0\\ 2\\ \\|\\ tee\\ -a\\ /etc/fstab$'\\n'mount\\ -a \\\u00a0 \u00a0 --image=image-$NAME_SUFFIX \\\u00a0 \u00a0 --create-disk=image=image-disk-$NAME_SUFFIX,auto-delete=yesgcloud compute instance-templates create template-$NAME_SUFFIX-$REGION2 \\\u00a0 \u00a0 --machine-type=n1-standard-1 \\\u00a0 \u00a0 --subnet=projects/$PROJECT_ID/regions/$REGION2/subnetworks/subnet-$NAME_SUFFIX-$REGION2 \\\u00a0 \u00a0 --region=$REGION2 \\\u00a0 \u00a0 --tags=http-server \\\u00a0 \u00a0 --metadata=^,@^startup-script=\\!\\#\\ /bin/bash$'\\n'echo\\ UUID=\\`blkid\\ -s\\ UUID\\ -o\\ value\\ /dev/sdb\\`\\ /var/www/example.com\\ ext4\\ discard,defaults,nofail\\ 0\\ 2\\ \\|\\ tee\\ -a\\ /etc/fstab$'\\n'mount\\ -a \\\u00a0 \u00a0 --image=image-$NAME_SUFFIX \\\u00a0 \u00a0 --create-disk=image=image-disk-$NAME_SUFFIX,auto-delete=yes# Create a health check for VM instancesgcloud compute health-checks create http http-basic-check-$NAME_SUFFIX \\\u00a0 \u00a0 --port 80# Create the managed instance groupsgcloud compute instance-groups managed create instance-group-$NAME_SUFFIX-$REGION1 \\\u00a0 \u00a0 --template=template-$NAME_SUFFIX-$REGION1 \\\u00a0 \u00a0 --size=2 \\\u00a0 \u00a0 --region=$REGION1 \\\u00a0 \u00a0 --health-check=http-basic-check-$NAME_SUFFIXgcloud compute instance-groups managed create instance-group-$NAME_SUFFIX-$REGION2 \\\u00a0 \u00a0 --template=template-$NAME_SUFFIX-$REGION2 \\\u00a0 \u00a0 --size=2 \\\u00a0 \u00a0 --region=$REGION2 \\\u00a0 \u00a0 --health-check=http-basic-check-$NAME_SUFFIX\n```\n## Create and configure a load balancerFor users to access your website, you need to allow traffic through to the VMs that run in the managed instance groups. You also want to automatically redirect traffic to new VMs if there's a zone failure in a managed instance group.\nIn the following section, you create an [external HTTPS load balancer](/load-balancing/docs/load-balancing-overview) with a backend service for HTTP traffic on port 80, use the health check created in the previous steps, and map an external IP address through to the backend service.\nFor more information, see [How to set up a simple external HTTP load balancer](/load-balancing/docs/https/ext-http-lb-simple) .- Create and configure the load balancer for your application:```\n# Configure port rules for HTTP port 80gcloud compute instance-groups set-named-ports \\\u00a0 \u00a0 instance-group-$NAME_SUFFIX-$REGION1 \\\u00a0 \u00a0 --named-ports http:80 \\\u00a0 \u00a0 --region $REGION1gcloud compute instance-groups set-named-ports \\\u00a0 \u00a0 instance-group-$NAME_SUFFIX-$REGION2 \\\u00a0 \u00a0 --named-ports http:80 \\\u00a0 \u00a0 --region $REGION2# Create a backend service and add the managed instance groups to itgcloud compute backend-services create \\\u00a0 \u00a0 web-backend-service-$NAME_SUFFIX \\\u00a0 \u00a0 --protocol=HTTP \\\u00a0 \u00a0 --port-name=http \\\u00a0 \u00a0 --health-checks=http-basic-check-$NAME_SUFFIX \\\u00a0 \u00a0 --globalgcloud compute backend-services add-backend \\\u00a0 \u00a0 web-backend-service-$NAME_SUFFIX \\\u00a0 \u00a0 --instance-group=instance-group-$NAME_SUFFIX-$REGION1 \\\u00a0 \u00a0 --instance-group-region=$REGION1 \\\u00a0 \u00a0 --globalgcloud compute backend-services add-backend \\\u00a0 \u00a0 web-backend-service-$NAME_SUFFIX \\\u00a0 \u00a0 --instance-group=instance-group-$NAME_SUFFIX-$REGION2 \\\u00a0 \u00a0 --instance-group-region=$REGION2 \\\u00a0 \u00a0 --global# Create a URL map for the backend servicegcloud compute url-maps create web-map-http-$NAME_SUFFIX \\\u00a0 \u00a0 --default-service web-backend-service-$NAME_SUFFIX# Configure forwarding for the HTTP trafficgcloud compute target-http-proxies create \\\u00a0 \u00a0 http-lb-proxy-$NAME_SUFFIX \\\u00a0 \u00a0 --url-map web-map-http-$NAME_SUFFIXgcloud compute forwarding-rules create \\\u00a0 \u00a0 http-content-rule-$NAME_SUFFIX \\\u00a0 \u00a0 --global \\\u00a0 \u00a0 --target-http-proxy=http-lb-proxy-$NAME_SUFFIX \\\u00a0 \u00a0 --ports=80\n```\n- Get the IP address of the forwarding rule for the web traffic:```\nIP_ADDRESS=$(gcloud compute forwarding-rules describe http-content-rule-$NAME_SUFFIX \\\u00a0 \u00a0 --global \\\u00a0 \u00a0 --format=\"value(IPAddress)\")\n```\n- Use `curl` , or open your web browser, to view the website using the IP address of the load balancer from the previous step:```\ncurl $IP_ADDRESS\n```It takes a few minutes for the load balancer to finish deploying and to correctly direct traffic to your backend. An HTTP 404 error is returned if the load balancer is still deploying. If needed, wait a few minutes and try to access the website again.The basic website is returned, as shown in the following example output:```\n<!doctype html>\n<html lang=en>\n<head>\n<meta charset=utf-8>\n <title>HA / DR example</title>\n</head>\n<body>\n <p>Welcome to a Compute Engine website with warm failover to Cloud Storage!</p>\n</body>\n</html>\n```\n## Create and configure a storage bucket [Cloud Storage](/storage/docs/introduction) is used to hold static website files. In this basic example, you create a single file with slightly different text than on the VMs.\nIn production deployments, your website likely includes many more files and additional application code on your managed instance group VMs than is shown in this document. The static version hosted in Cloud Storage is often then a more limited version that provides minimal functionality. In a warm failover scenario, this limited website from Cloud Storage is displayed until the managed instance groups recover and can serve traffic for the full website experience.- [Verify the domain](/storage/docs/domain-name-verification#verification) you want to use with your Cloud Storage bucket.\n- Create a Cloud Storage bucket to match the name of the domain you own and want to use:```\ngsutil mb gs://static-web.$DOMAIN\n```The `DOMAIN` variable defined at the start of this document is used, such as `` . This example stores the static files at `static-web.example.com` .\n- Create a local file that you copy to the Cloud Storage bucket in the next step:```\ncat <<EOF > index.html<!doctype html><html lang=en><head><meta charset=utf-8>\u00a0 \u00a0 <title>HA / DR example</title></head><body>\u00a0 \u00a0 <p>Welcome to a test static web server with warm failover from Cloud Storage!</p></body></html>EOF\n```\n- Upload the basic HTML file to the Cloud Storage bucket:```\ngsutil cp index.html gs://static-web.$DOMAIN\n```\n- To allow users to view the static web content, set the appropriate permissions on the Cloud Storage bucket:```\ngsutil iam ch allUsers:objectViewer gs://static-web.$DOMAIN\n```\n- Configure the Cloud Storage bucket to serve the `index.html` file as the default web page:```\ngsutil web set -m index.html gs://static-web.$DOMAIN\n```\n## Create a DNS zone and recordTo allow traffic to be directed to the warm static site on Cloud Storage when there's an outage with the managed instance groups, create a Cloud DNS zone. Under normal conditions, this DNS zone directs traffic through the external load balancer to the managed instance groups created in the previous sections.\n **Note:** With a DNS-based approach to failover, users continue to resolve the old address of the load balancer until their cached DNS record expires. The length of time that the cached DNS record exists for a user is defined with the time-to-live (TTL) value when you create the Cloud DNS records. As this document describes a warm failover pattern, some amount of downtime should be expected as part of the balance from a hot failover pattern with additional cost and configuration complexity.- Create a Cloud DNS zone:```\ngcloud dns managed-zones create zone-$NAME_SUFFIX \\\u00a0 \u00a0 --dns-name=$DOMAIN \\\u00a0 \u00a0 --description=\"DNS zone for warm site failover\"\n```The `DOMAIN` variable defined at the start of this document is used, such as ``\n- Get the details of the Cloud DNS zone:```\ngcloud dns managed-zones describe zone-$NAME_SUFFIX\n```The following example output shows the `nameServers` for the zone, such as `ns-cloud-b1.googledomains.com` .```\n[...]\nkind: dns#managedZone\nname: zone-app\nnameServers:\n- ns-cloud-b1.googledomains.com.\n- ns-cloud-b2.googledomains.com.\n- ns-cloud-b3.googledomains.com.\n- ns-cloud-b4.googledomains.com.\n```\n- Cloud DNS must be authoritative for your domain. Create nameserver (NS) records with your domain registrar that point to your Cloud DNS zone. Use the nameserver addresses returned in the previous step.For more information and an example using Google Domains, see [How to update name servers](/dns/docs/tutorials/create-domain-tutorial#update-nameservers) .\n- In your Cloud DNS zone, add a record for `www` using the load balancer IP address obtained in a previous section:```\ngcloud dns record-sets transaction start \\\u00a0 \u00a0 --zone=zone-$NAME_SUFFIXgcloud dns record-sets transaction add $IP_ADDRESS \\\u00a0 \u00a0 --name=www.$DOMAIN \\\u00a0 \u00a0 --ttl=300 \\\u00a0 \u00a0 --type=A \\\u00a0 \u00a0 --zone=zone-$NAME_SUFFIX\n```This record directs user requests for the website through the load balancer to the managed instance groups. A TTL of 300 seconds is set to reduce the length of time the cached DNS record exists for a user.\n- Create a record to be used by the Cloud Storage bucket for the static website:```\ngcloud dns record-sets transaction add c.storage.googleapis.com. \\\u00a0 \u00a0 --name=static-web.$DOMAIN \\\u00a0 \u00a0 --ttl=300 \\\u00a0 \u00a0 --type=CNAME \\\u00a0 \u00a0 --zone=zone-$NAME_SUFFIX\n```This example uses `static-web` as the subdomain. Leave the `c.storage.googleapis.com.` Again, a TTL of 300 seconds is set to reduce the length of time the cached DNS record exists for a user\n- Finally, commit the DNS record additions to the zone:```\ngcloud dns record-sets transaction execute \\\u00a0 \u00a0 --zone=zone-$NAME_SUFFIX\n```\n## Verify and test the DNS zone and recordsLet's review the resource deployments before simulating a zone failure. All of the resources have been created to support the environment, as shown in the following image:\n- Cloud DNS zone records direct users to the load balancer for distribution across the managed instance group VMs.\n- A Cloud Storage bucket is configured to host static web pages if there's an outage with the managed instance groups.\n- The Cloud DNS zone is configured to use the static site in Cloud Storage, but doesn't currently resolve requests to the storage bucket.\nTo view the DNS records and test resolution, you must resolve addresses against the Cloud DNS servers. In production deployments, make sure you test and verify the addresses resolve correctly, then update your own DNS servers to resolve appropriately. This document doesn't detail the steps to update your own DNS servers, only how to verify traffic flows correctly under normal and failover conditions.- Get the details of the Cloud DNS zone again:```\ngcloud dns managed-zones describe zone-$NAME_SUFFIX\n```The following example output shows the `nameServers` for the zone, such as `ns-cloud-b1.googledomains.com` .```\n[...]\nkind: dns#managedZone\nname: zone-app\nnameServers:\n- ns-cloud-b1.googledomains.com.\n- ns-cloud-b2.googledomains.com.\n- ns-cloud-b3.googledomains.com.\n- ns-cloud-b4.googledomains.com.\n```\n- To resolve the `www` record for your Cloud DNS zone against one of these name servers, use the `dig` command:```\ndig @ns-cloud-b1.googledomains.com www.$DOMAIN\n```This example uses the `ns-cloud-b1.googledomains.com` nameserver address returned from the previous `describe` command. Provide your own nameserver address shown in the output of the previous commandThe following example output shows that the record resolves to the IP address of the load balancer. If you used this nameserver to access the address, such as using `curl` and the `--resolve` parameter with the Cloud DNS nameserver, the default page would be displayed from one of the managed instance groups behind the load balancer.```\n; <<>> DiG 9.11.5-P4-5.1+deb10u3-Debian <<>> @ns-cloud-b1.googledomains.com www.example.com\n; (1 server found)\n[...]\n;; QUESTION SECTION:\n;www.example.com.   IN  A\n;; ANSWER SECTION:\nwww.example.com. 300  IN  A  35.227.253.90\n```\n- Use the `dig` command again to verify the DNS record for the static website in Cloud Storage:```\ndig @ns-cloud-b1.googledomains.com static-web.$DOMAIN\n```The following example output shows that the record resolves to Cloud Storage that can serve the static content from the storage bucket:```\n; <<>> DiG 9.11.5-P4-5.1+deb10u3-Debian <<>> @ns-cloud-b1.googledomains.com static-web.example.com\n; (1 server found)\n[...]\n;; QUESTION SECTION:\n;static-web.example.com. IN  A\n;; ANSWER SECTION:\nstatic-web.example.com. 300 IN  CNAME c.storage.googleapis.com.\n```\n## Fail over to the Cloud Storage bucketIn a production environment, you might get an alert using [Cloud Monitoring](/monitoring/alerts) or other monitoring solution when there's a problem with the managed instance groups. This alert prompts a human to understand the scope of the failure before you update the Cloud DNS records to redirect traffic to the Cloud Storage-hosted static website. An alternative approach is to use your monitoring solution to automatically respond to outages with the managed instance groups.\nWhen you fail over, Cloud DNS resolves traffic to the Cloud Storage-hosted static website, as shown in the following image:When you or your monitoring solution determine the most appropriate action is to update the Cloud DNS records to direct traffic to Cloud Storage, update the existing DNS `A` record. In this document, you manually update the Cloud DNS records to redirect traffic to the Cloud Storage-hosted static website.- To fail over the Cloud DNS records, remove the existing `A` record that resolves to the load balancer:```\ngcloud dns record-sets transaction start \\\u00a0 \u00a0 --zone=zone-$NAME_SUFFIXgcloud dns record-sets transaction remove $IP_ADDRESS \\\u00a0 \u00a0 --name=www.$DOMAIN \\\u00a0 \u00a0 --ttl=300 \\\u00a0 \u00a0 --type=A \\\u00a0 \u00a0 --zone=zone-$NAME_SUFFIX\n```\n- Create a `CNAME` record for `www` that points to the Cloud Storage-hosted content:```\ngcloud dns record-sets transaction add static-web.$DOMAIN \\\u00a0 \u00a0 --name=www.$DOMAIN. \\\u00a0 \u00a0 --ttl=30 \\\u00a0 \u00a0 --type=CNAME \\\u00a0 \u00a0 --zone=zone-$NAME_SUFFIX\n```\n- Commit the updates to the Cloud DNS zone:```\ngcloud dns record-sets transaction execute \\\u00a0 \u00a0 --zone=zone-$NAME_SUFFIX\n```\n- Use the `dig` command to confirm the `www` record now resolves to the address of the Cloud Storage static website:```\ndig @ns-cloud-b1.googledomains.com www.$DOMAIN\n```The following example output shows that the `www.example.com` record resolves to the CNAME record of the Cloud Storage static website. Requests to access `www.example.com` are redirected to the Cloud Storage bucket, which displays the static website:```\n; <<>> DiG 9.11.5-P4-5.1+deb10u3-Debian <<>> @ns-cloud-b1.googledomains.com www.example.com\n; (1 server found)\n[...]\n;; QUESTION SECTION:\n;www.example.com.   IN  A\n;; ANSWER SECTION:\nwww.example.com. 30  IN  CNAME static-web.example.com.\nstatic-web.example.com. 300 IN  CNAME c.storage.googleapis.com.\n```\n## Fail back to the managed instance groupsAfter issues with the managed instance groups are resolved, you can fail back to serving content from the load-balanced managed instance groups by updating the Cloud DNS records again. Again, a human might make this decision using Cloud Monitoring insights for the health of the managed instance groups. Or, you could use automation to respond to the restored health of the managed instance group. In this document, you manually update the Cloud DNS records.\nWhen you fail back, Cloud DNS resolves traffic to the managed instance groups again, as shown in the following image:\n- Remove the `www` CNAME record that redirects traffic to the Cloud Storage-hosted content:```\ngcloud dns record-sets transaction start \\\u00a0 \u00a0 --zone=zone-$NAME_SUFFIXgcloud dns record-sets transaction remove static-web.$DOMAIN \\\u00a0 \u00a0 --name=www.$DOMAIN \\\u00a0 \u00a0 --ttl=30 \\\u00a0 \u00a0 --type=CNAME \\\u00a0 \u00a0 --zone=zone-$NAME_SUFFIX\n```\n- Add an `A` record to point to the load balancer in front of the managed instance groups again:```\ngcloud dns record-sets transaction add $IP_ADDRESS \\\u00a0 \u00a0 --name=www.$DOMAIN \\\u00a0 \u00a0 --ttl=300 \\\u00a0 \u00a0 --type=A \\\u00a0 \u00a0 --zone=zone-$NAME_SUFFIX\n```\n- Commit the updates to the Cloud DNS zone:```\ngcloud dns record-sets transaction execute \\\u00a0 \u00a0 --zone=zone-$NAME_SUFFIX\n```\n- Use the `dig` command one more time to confirm the `www` record resolves to the address of the load balancer in front of the managed instance groups again:```\ndig @ns-cloud-b1.googledomains.com www.$DOMAIN\n```The following example output shows that the record resolves to the IP address of the load balancer and traffic would be served from one of the managed instance groups:```\n; <<>> DiG 9.11.5-P4-5.1+deb10u3-Debian <<>> @ns-cloud-b1.googledomains.com www.example.com\n; (1 server found)\n[...]\n;; QUESTION SECTION:\n;www.example.com.   IN  A\n;; ANSWER SECTION:\nwww.example.com. 300  IN  A  35.227.253.90\n```\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\nTo delete the individual resources created in this document, complete the following steps:- Delete the DNS zone and records:```\ntouch empty-filegcloud dns record-sets import -z zone-$NAME_SUFFIX \\\u00a0 \u00a0 --delete-all-existing \\\u00a0 \u00a0 empty-filerm empty-filegcloud dns managed-zones delete zone-$NAME_SUFFIX\n```\n- Delete the Cloud Storage bucket:```\ngsutil rm -r gs://static-web.$DOMAIN\n```\n- Delete the load balancer configuration:```\ngcloud compute forwarding-rules delete \\\u00a0 \u00a0 http-content-rule-$NAME_SUFFIX --global --quietgcloud compute target-http-proxies delete \\\u00a0 \u00a0 http-lb-proxy-$NAME_SUFFIX --quietgcloud compute url-maps delete web-map-http-$NAME_SUFFIX --quietgcloud compute backend-services delete \\\u00a0 \u00a0 web-backend-service-$NAME_SUFFIX --global --quiet\n```\n- Delete the managed instance groups and health check:```\ngcloud compute instance-groups managed delete \\\u00a0 \u00a0 instance-group-$NAME_SUFFIX-$REGION1 \\\u00a0 \u00a0 --region=$REGION1 --quietgcloud compute instance-groups managed delete \\\u00a0 \u00a0 instance-group-$NAME_SUFFIX-$REGION2 \\\u00a0 \u00a0 --region=$REGION2 --quietgcloud compute health-checks delete http-basic-check-$NAME_SUFFIX --quiet\n```\n- Delete the instance templates, images, base VM, and persistent disks:```\ngcloud compute instance-templates delete \\\u00a0 \u00a0 template-$NAME_SUFFIX-$REGION1 --quietgcloud compute instance-templates delete \\\u00a0 \u00a0 template-$NAME_SUFFIX-$REGION2 --quietgcloud compute images delete image-$NAME_SUFFIX --quietgcloud compute images delete image-disk-$NAME_SUFFIX --quietgcloud compute instances delete vm-base-$NAME_SUFFIX \\\u00a0 \u00a0 --zone=$ZONE --quiet\n```\n- Delete the firewall rules.```\ngcloud compute firewall-rules delete \\\u00a0 \u00a0 allow-health-check-$NAME_SUFFIX --quietgcloud compute firewall-rules delete \\\u00a0 \u00a0 allow-ssh-$NAME_SUFFIX --quietgcloud compute firewall-rules delete \\\u00a0 \u00a0 allow-http-$NAME_SUFFIX --quiet\n```\n- Delete the subnet and VPC.```\ngcloud compute networks subnets delete \\\u00a0 \u00a0 subnet-$NAME_SUFFIX-$REGION1 --region=$REGION1 --quietgcloud compute networks subnets delete \\\u00a0 \u00a0 subnet-$NAME_SUFFIX-$REGION2 --region=$REGION2 --quietgcloud compute networks delete network-$NAME_SUFFIX --quiet\n```\n## What's next\n- For an alternative approach that uses an external Application Load Balancer instead of Cloud DNS to control the failover, see [Deploy a warm recoverable web server with Compute Engine and Cloud Storage](/architecture/warm-recoverable-static-site-failover-load-balancer) . This pattern is useful if you don't have, or don't want to use, Cloud DNS.\n- To learn how how to determine the best approach for your own applications and which recovery method to use, see the [Disaster recovery planning guide](/architecture/dr-scenarios-planning-guide) .\n- To see other patterns for applications, such as cold and hot failover, see [Disaster recovery scenarios for applications](/architecture/dr-scenarios-for-applications) .\n- For more ways to handle scale and availability, see the [Patterns for scalable and resilient apps](/architecture/scalable-and-resilient-apps) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}