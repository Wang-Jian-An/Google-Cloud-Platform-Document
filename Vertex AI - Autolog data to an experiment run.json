{"title": "Vertex AI - Autolog data to an experiment run", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Autolog data to an experiment run\nAutologging is a feature in the Vertex AI SDK that automatically logs parameters and metrics from model-training runs to Vertex AI Experiments. This can save time and effort by eliminating the need to manually log this data. Currently, autologging only supports parameter and metric logging.\n", "content": "## Autolog data\nThere are two options for autologging data to Vertex AI Experiments.\n- Let the Vertex AI SDK automatically create [ExperimentRun](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.ExperimentRun?_gl=1*m6thp1*_ga*NDY0ODI5NjQ5LjE2ODIxMjU5Nzc.*_ga_4LYFWVHBEB*MTY4MjM2MDE5NC40LjEuMTY4MjM2MDcxNy4wLjAuMA..#google_cloud_aiplatform_ExperimentRun) resources for you.\n- Specify the ExperimentRun resource that you'd like autologged parameters  and metrics to be written to.The Vertex AI SDK for Python handles creating ExperimentRun resources for you.   Automatically created ExperimentRun resources will have a run name in the following format: `{ml-framework-name}-{timestamp}-{uid}` ,   for example: \"tensorflow-2023-01-04-16-09-20-86a88\". \nThe following sample uses the [init](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_init) method,   from the `aiplatform` [Package   functions](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#functions) .### Python [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/experiment_tracking/autologging_with_auto_run_creation_sample.py) \n```\ndef autologging_with_auto_run_creation_sample(\u00a0 \u00a0 experiment_name: str,\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 experiment_tensorboard: Optional[Union[str, aiplatform.Tensorboard]] = None,):\u00a0 \u00a0 aiplatform.init(\u00a0 \u00a0 \u00a0 \u00a0 experiment=experiment_name,\u00a0 \u00a0 \u00a0 \u00a0 project=project,\u00a0 \u00a0 \u00a0 \u00a0 location=location,\u00a0 \u00a0 \u00a0 \u00a0 experiment_tensorboard=experiment_tensorboard,\u00a0 \u00a0 )\u00a0 \u00a0 aiplatform.autolog()\u00a0 \u00a0 # Your model training code goes here\u00a0 \u00a0 aiplatform.autolog(disable=True)\n```\n- `experiment_name`: Provide a name for your experiment. You can find your   list of experiments in the Google Cloud console by selecting **Experiments** in the section nav.\n- `experiment_tensorboard`: (Optional) Provide a name for your Vertex AI TensorBoard   instance.\n- `project`: Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) . You can find these Project IDs in the   Google Cloud console [welcome](https://console.cloud.google.com/welcome) page.\n- `location`: See [List of available locations](/vertex-ai/docs/general/locations) \nProvide your own ExperimentRun names and have metrics and parameters  from multiple model-training runs logged to the same ExperimentRun. Any metrics from model to the current run set by calling `aiplatform.start_run(\"your-run-name\")` until `aiplatform.end_run()` is called. \nThe following sample uses the [init](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_init) method,  from the `aiplatform` [Package functions](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#functions) .### Python [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/experiment_tracking/autologging_with_manual_run_creation_sample.py) \n```\ndef autologging_with_manual_run_creation_sample(\u00a0 \u00a0 experiment_name: str,\u00a0 \u00a0 run_name: str,\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 experiment_tensorboard: Optional[Union[str, aiplatform.Tensorboard]] = None,):\u00a0 \u00a0 aiplatform.init(\u00a0 \u00a0 \u00a0 \u00a0 experiment=experiment_name,\u00a0 \u00a0 \u00a0 \u00a0 project=project,\u00a0 \u00a0 \u00a0 \u00a0 location=location,\u00a0 \u00a0 \u00a0 \u00a0 experiment_tensorboard=experiment_tensorboard,\u00a0 \u00a0 )\u00a0 \u00a0 aiplatform.autolog()\u00a0 \u00a0 aiplatform.start_run(run=run_name)\u00a0 \u00a0 # Your model training code goes here\u00a0 \u00a0 aiplatform.end_run()\u00a0 \u00a0 aiplatform.autolog(disable=True)\n```\n- `experiment_name`: Provide the name of your experiment.\n- `run_name`: Provide a name for your experiment run.   You can find your list of experiments in the   Google Cloud console by selecting **Experiments** in the section nav.\n- `project`: Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) . You can find these Project IDs in the Google Cloud console [welcome](https://console.cloud.google.com/welcome) page.\n- `location`: See [List of available locations](/vertex-ai/docs/general/locations) \n- `experiment_tensorboard`: (Optional) Provide a name for your Vertex AI TensorBoard   instance.\nVertex AI SDK autologging uses MLFlow's autologging in its implementation. Evaluation metrics and parameters from the following frameworks are logged to your ExperimentRun when autologging is enabled.\n- Fastai\n- Gluon\n- Keras\n- LightGBM\n- Pytorch Lightning\n- Scikit-learn\n- Spark\n- Statsmodels\n- XGBoost## View autologged parameters and metrics\nUse the Vertex AI SDK for Python to [compare runs](/vertex-ai/docs/experiments/compare-analyze-runs#compare-runs) and get runs data. The [Google Cloud console](/vertex-ai/docs/experiments/compare-analyze-runs#console-compare-analyze-runs) provides an easy way to compare these runs.\n## Relevant notebook sample\n- [Autolog data](/vertex-ai/docs/experiments/user-journey/uj-autologging) ## Blog post\n- [How you can automate ML experiment tracking with Vertex AI Experimentsautologging](https://cloud.google.com/blog/products/ai-machine-learning/effortless-tracking-of-your-vertex-ai-model-training)", "guide": "Vertex AI"}