{"title": "Documentation - Deploy a model to an endpoint", "url": "https://cloud.google.com/distributed-cloud/hosted/docs/latest/gdch/overview", "abstract": "# Documentation - Deploy a model to an endpoint\n**Preview:** The Vertex AI online prediction is a Preview feature that is available as-is and is not recommended for production environments. Google provides no service-level agreements (SLA) or technical support commitments for Preview features. For more information, see GDCH's [feature stages](/distributed-cloud/hosted/docs/latest/gdch/resources/feature-stages) .\n**Note:** Authorization is not enforced for Vertex AI online prediction because it is a Preview feature.\nYou must deploy a model to an endpoint before that model can be used to serve online predictions in Google Distributed Cloud Hosted (GDCH). Deploying a model associates physical resources with the model so it can serve online predictions with low latency.\nThis page describes the steps you must follow to deploy a model to an endpoint for online predictions.\n", "content": "## Before you begin\nBefore deploying a model, perform the following steps:\n- Create and train a prediction model targeting one of the [supported containers](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-online-predictions#available-container-images) .\n- If you don't have a project, work with your Platform Administrator (PA) to [create one](/distributed-cloud/hosted/docs/latest/gdch/platform/pa-user/create-a-project) .\n- Work with your Infrastructure Operator (IO) to [create the Prediction user cluster](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/prediction-user-cluster) . The IO creates the cluster for you, associates it with your project, and assigns the appropriate node pools within the cluster, considering the resources you need for online predictions.\n- [Create a storage bucket for your project](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/create-storage-buckets) .\n- Create the Vertex AI Default Serving (`vai-default-serving-sa`) service identity within your project. For more information about how to create service identities, see [Manage service identities](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/iam/service-identities) .\n- Grant the Project Bucket Object Viewer (`project-bucket-object-viewer`) role to the Vertex AI Default Serving (`vai-default-serving-sa`) service identity for the storage bucket you created. For more information about how to grant bucket access to service identities, see [Grant bucket access](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/grant-obtain-storage-access#grant_bucket_access) .## Upload your model\nYou must upload your model to the storage bucket you created. For more information about how to upload objects to storage buckets, see [Upload and download storage objects in projects](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/upload-download-storage-objects) .\nIf you [use TensorFlow to train a model](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-online-predictions#available-container-images) , export your model as a [TensorFlow SavedModel directory](https://www.tensorflow.org/guide/saved_model) .\nThere are several ways to export `SavedModels` from TensorFlow training code. The following list describes a few ways that work for various TensorFlow APIs:\n- If you use Keras for training, [use tf.keras.Model.save to export a SavedModel](https://www.tensorflow.org/guide/keras/save_and_serialize#whole-model_saving_loading) \n- If you use an Estimator for training, [use tf.estimator.Estimator.export_saved_model to export a SavedModel](https://www.tensorflow.org/guide/estimator#savedmodels_from_estimators) .\n- Otherwise, [use tf.saved_model.save](https://www.tensorflow.org/guide/saved_model#saving_a_custom_model) or [use tf.compat.v1.saved_model.SavedModelBuilder](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/builder) .\nIf you are not using Keras or an Estimator, then make sure to [use the serve tag and serving_default signature when you export your SavedModel](https://www.tensorflow.org/tfx/serving/serving_basic#train_and_export_tensorflow_model) to ensure Vertex AI can use your model artifacts to serve predictions. Keras and Estimator handle this task automatically. Learn more about [specifying signatures during export](https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export) .\nTo serve predictions using these artifacts, create a `Model` with the [prebuilt container for prediction](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-online-predictions#available-container-images) matching the version of TensorFlow that you used for training.\nThe path to the storage bucket of your model must have the following structure:\n```\ns3://BUCKET_NAME/MODEL_ID/MODEL_VERSION_ID\n```\nIn the `` folders, you must have the following structure for your files:\n- A PB (protocol buffer or protobuf) file.\n- A`variables`folder with the following files:- A`variable.index`file.\n- One or more`variables.data`files, for example,`variables.data-00000-of-00001`.\n## Create a resource pool\nA `ResourcePool` custom resource (CR) lets you have fine-grained control over the behavior of your model. You can define settings such as the following:\n- Autoscaling configurations\n- Machine type, which defines CPU and memory requirements\n- Accelerator options, for example, GPU resources\nTo create a `ResourcePool` CR, perform the following steps:\n- Create a YAML file defining the `ResourcePool` CR.- **Sample YAML file without GPU accelerators (CPU-based models):** ```\napiVersion: prediction.aiplatform.gdc.goog/v1kind: ResourcePoolmetadata:\u00a0 name: RESOURCE_POOL_NAME\u00a0 namespace: PROJECT_NAMESPACEspec:\u00a0 dedicatedResources:\u00a0 \u00a0 machineSpec:\u00a0 \u00a0 \u00a0 # GDCH adds computing overhead to the nodes for mandatory system components.\u00a0 \u00a0 \u00a0 # Choose a machineType value that allocates fewer CPU and memory resources\u00a0 \u00a0 \u00a0 # than those used by the nodes in the Prediction user cluster.\u00a0 \u00a0 \u00a0 machineType: n2-highcpu-8-gdc\u00a0 \u00a0 autoscaling:\u00a0 \u00a0 \u00a0 minReplica: 2\u00a0 \u00a0 \u00a0 maxReplica: 10\n```\n- **Sample YAML file including GPU accelerators (GPU-based models):** ```\napiVersion: prediction.aiplatform.gdc.goog/v1kind: ResourcePoolmetadata:\u00a0 name: RESOURCE_POOL_NAME\u00a0 namespace: PROJECT_NAMESPACEspec:\u00a0 dedicatedResources:\u00a0 \u00a0 machineSpec:\u00a0 \u00a0 \u00a0 # GDCH adds computing overhead to the nodes for mandatory system components.\u00a0 \u00a0 \u00a0 # Choose a machineType value that allocates fewer CPU and memory resources\u00a0 \u00a0 \u00a0 # than those used by the nodes in the Prediction user cluster.\u00a0 \u00a0 \u00a0 machineType: a2-highgpu-1g-gdc\u00a0 \u00a0 \u00a0 acceleratorType: nvidia-a100-80gb\u00a0 \u00a0 \u00a0 # The accelerator count is a slice of the requested virtualized GPUs.\u00a0 \u00a0 \u00a0 # The value corresponds to one-seventh of 80 GB of GPUs for each count.\u00a0 \u00a0 \u00a0 acceleratorCount: 2\u00a0 \u00a0 autoscaling:\u00a0 \u00a0 \u00a0 minReplica: 2\u00a0 \u00a0 \u00a0 maxReplica: 10\n```\nReplace the following:- ``: the name you want to give to the`ResourcePool`definition file.\n- ``: the name of the project namespace associated with the Prediction user cluster.\nModify the values on the `dedicatedResources` fields according to your resource needs and what is available in your Prediction user cluster.\n- Apply the `ResourcePool` definition file to the Prediction user cluster:```\nkubectl --kubeconfig PREDICTION_CLUSTER_KUBECONFIG apply -f RESOURCE_POOL_NAME.yaml\n```Replace the following:- ``: the path to the kubeconfig file in the Prediction user cluster.\n- ``: the name of the`ResourcePool`definition file.When you create the `ResourcePool` CR, the Kubernetes API and the webhook service validate the YAML file and report success or failure. The Prediction operator provisions and reserves your resources from the resource pool when you [deploy your models to an endpoint](#deploy-your-model-to-an-endpoint) .\n## Deploy your model to an endpoint\nIf you have a resource pool, you can deploy more than one model to an endpoint, and you can deploy a model to more than one endpoint. Deploy a prediction model targeting [supported containers](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-online-predictions#available-container-images) . Depending on whether the endpoint already exists or not, choose between one of the following two methods:\n- [Deploy a model to a new endpoint](#new-endpoint) \n- [Deploy a model to an existing endpoint](#existing-endpoint) \n### Deploy a model to a new endpoint\nTo deploy a prediction model to a new endpoint, perform the following steps:\n- Create a YAML file defining a `DeployedModel` CR.The following YAML file shows a sample configuration for a Tensorflow model:```\napiVersion: prediction.aiplatform.gdc.goog/v1kind: DeployedModelmetadata:\u00a0 name: DEPLOYED_MODEL_NAME\u00a0 namespace: PROJECT_NAMESPACEspec:\u00a0 # The endpoint path structure is endpoints/<endpoint-id>\u00a0 endpointPath: endpoints/PREDICTION_ENDPOINT\u00a0 modelSpec:\u00a0 \u00a0 # The artifactLocation field must be the s3 path to the folder that\u00a0 \u00a0 # contains the various model versions.\u00a0 \u00a0 # For example, s3://my-prediction-bucket/tensorflow\u00a0 \u00a0 artifactLocation: s3://PATH_TO_MODEL\u00a0 \u00a0 # The value in the id field must be unique to each model.\u00a0 \u00a0 id: img-detection-model\u00a0 \u00a0 modelDisplayName: my_img_detection_model\u00a0 \u00a0 # The model resource name structure is models/<model-id>/<model-version-id>\u00a0 \u00a0 modelResourceName: models/img-detection-model/1\u00a0 \u00a0 # The model version ID must match the name of the first folder in \u00a0 \u00a0 # the artifactLocation bucket, inside the 'tensorflow' folder.\u00a0 \u00a0 # For example, if the bucket path is\u00a0 \u00a0 # s3://my-prediction-bucket/tensorflow/1/,\u00a0 \u00a0 # then the value for the model version ID is \"1\".\u00a0 \u00a0 modelVersionID: \"1\"\u00a0 \u00a0 modelContainerSpec:\u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 - --model_config_file=/models/models.config\u00a0 \u00a0 \u00a0 - --rest_api_port=8080\u00a0 \u00a0 \u00a0 - --port=8500\u00a0 \u00a0 \u00a0 - --file_system_poll_wait_seconds=30\u00a0 \u00a0 \u00a0 - --model_config_file_poll_wait_seconds=30\u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 - /bin/tensorflow_model_server\u00a0 \u00a0 \u00a0 # The image URI field must contain one of the following values:\u00a0 \u00a0 \u00a0 # For CPU-based models: gcr.io/aiml/prediction/containers/tf2-cpu.2-6:latest\u00a0 \u00a0 \u00a0 # For GPU-based models: gcr.io/aiml/prediction/containers/tf2-gpu.2-6:latest\u00a0 \u00a0 \u00a0 imageURI: gcr.io/aiml/prediction/containers/tf2-gpu.2-6:latest\u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 - 8080\u00a0 \u00a0 \u00a0 grpcPorts:\u00a0 \u00a0 \u00a0 - 8500\u00a0 resourcePoolRef:\u00a0 \u00a0 kind: ResourcePool\u00a0 \u00a0 name: RESOURCE_POOL_NAME\u00a0 \u00a0 namespace: PROJECT_NAMESPACE\n```Replace the following:- ``: the name you want to give to the`DeployedModel`definition file.\n- ``: the name of the project namespace associated with the Prediction user cluster.\n- ``: the name you want to give to the new endpoint, for example,`my-img-prediction-endpoint`.\n- ``: the path to your model in the storage bucket.\n- ``: the name you gave to the`ResourcePool`definition file when [you created a resource pool](#resource-pool) to host the model.\nModify the values on the remaining fields according to your prediction model.\n- Apply the `DeployedModel` definition file to the Prediction user cluster:```\nkubectl --kubeconfig PREDICTION_CLUSTER_KUBECONFIG apply -f DEPLOYED_MODEL_NAME.yaml\n```Replace the following:- ``: the path to the kubeconfig file in the Prediction user cluster.\n- ``with the name of the`DeployedModel`definition file.\nWhen you create the `DeployedModel` CR, the Kubernetes API and the webhook service validate the YAML file and report success or failure. The Prediction operator reconciles the `DeployedModel` CR and serves it in the Prediction user cluster. **Tip:** You can check the status of the `DeployedModel` CR and ensure it is ready to accept prediction requests using the `kubectl --kubeconfig` `` `get -f` `` `.yaml -o jsonpath='{.status.primaryCondition}'` command. The `DeployedModel` must be in a ready state.\n- Create a YAML file defining an `Endpoint` CR.The following YAML file shows a sample configuration:```\napiVersion: aiplatform.gdc.goog/v1kind: Endpointmetadata:\u00a0 name: ENDPOINT_NAME\u00a0 namespace: PROJECT_NAMESPACEspec:\u00a0 createDns: true\u00a0 id: PREDICTION_ENDPOINT\u00a0 destinations:\u00a0 \u00a0 - serviceRef:\u00a0 \u00a0 \u00a0 \u00a0 kind: DeployedModel\u00a0 \u00a0 \u00a0 \u00a0 name: DEPLOYED_MODEL_NAME\u00a0 \u00a0 \u00a0 \u00a0 namespace: PROJECT_NAMESPACE\u00a0 \u00a0 \u00a0 trafficPercentage: 50\u00a0 \u00a0 \u00a0 grpcPort: 8501\u00a0 \u00a0 \u00a0 httpPort: 8081\u00a0 \u00a0 - serviceRef:\u00a0 \u00a0 \u00a0 \u00a0 kind: DeployedModel\u00a0 \u00a0 \u00a0 \u00a0 name: DEPLOYED_MODEL_NAME_2\u00a0 \u00a0 \u00a0 \u00a0 namespace: PROJECT_NAMESPACE\u00a0 \u00a0 \u00a0 trafficPercentage: 50\u00a0 \u00a0 \u00a0 grpcPort: 8501\u00a0 \u00a0 \u00a0 httpPort: 8081\n```Replace the following:- ``: the name you want to give to the`Endpoint`definition file.\n- ``: the name of the project namespace associated with the Prediction user cluster.\n- ``: the name of the new endpoint. You defined this name on the`DeployedModel`definition file.\n- ``: the name you gave to the`DeployedModel`definition file.\nYou can have one or more `serviceRef` destinations. If you have a second `serviceRef` object, add it to the YAML file on the `destinations` field and replace `` with the name you gave to a second `DeployedModel` definition file you created. Keep adding or removing `serviceRef` objects as you need them, depending on the amount of models you are deploying.Set the `trafficPercentage` fields based on how you want to split traffic between the models on this endpoint. Modify the values on the remaining fields according to your endpoint configurations.\n- Apply the `Endpoint` definition file to the Prediction user cluster:```\nkubectl --kubeconfig PREDICTION_CLUSTER_KUBECONFIG apply -f ENDPOINT_NAME.yaml\n```Replace `` with the name of the `Endpoint` definition file.\nTo get the endpoint URL path for the prediction model, run the following command:\n```\nkubectl --kubeconfig PREDICTION_CLUSTER_KUBECONFIG get endpoint PREDICTION_ENDPOINT -n PROJECT_NAMESPACE -o jsonpath='{.status.endpointFQDN}'\n```\nReplace the following:\n- ``: the path to the kubeconfig file in the Prediction user cluster.\n- ``: the name of the new endpoint.\n- ``: the name of the prediction project namespace.\n### Deploy a model to an existing endpoint\nYou can only deploy a model to an existing endpoint if you had previously [deployed another model to that endpoint when it was new](#new-endpoint) . The system requires this previous step to create the endpoint.\nTo deploy a prediction model to an existing endpoint, perform the following steps:\n- Create a YAML file defining a `DeployedModel` CR.The following YAML file shows a sample configuration:```\napiVersion: prediction.aiplatform.gdc.goog/v1kind: DeployedModelmetadata:\u00a0 name: DEPLOYED_MODEL_NAME\u00a0 namespace: PROJECT_NAMESPACEspec:\u00a0 # The endpoint path structure is endpoints/<endpoint-id>\u00a0 endpointPath: endpoints/PREDICTION_ENDPOINT\u00a0 modelSpec:\u00a0 \u00a0 # The artifactLocation field must be the s3 path to the folder that\u00a0 \u00a0 # contains the various model versions.\u00a0 \u00a0 # For example, s3://my-prediction-bucket/tensorflow\u00a0 \u00a0 artifactLocation: s3://PATH_TO_MODEL\u00a0 \u00a0 # The value in the id field must be unique to each model.\u00a0 \u00a0 id: img-detection-model-v2\u00a0 \u00a0 modelDisplayName: my_img_detection_model\u00a0 \u00a0 # The model resource name structure is models/<model-id>/<model-version-id>\u00a0 \u00a0 modelResourceName: models/img-detection-model/2\u00a0 \u00a0 # The model version ID must match the name of the first folder in \u00a0 \u00a0 # the artifactLocation bucket,\u00a0 \u00a0 # inside the 'tensorflow' folder.\u00a0 \u00a0 # For example, if the bucket path is\u00a0 \u00a0 # s3://my-prediction-bucket/tensorflow/2/,\u00a0 \u00a0 # then the value for the model version ID is \"2\".\u00a0 \u00a0 modelVersionID: \"2\"\u00a0 \u00a0 modelContainerSpec:\u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 - --model_config_file=/models/models.config\u00a0 \u00a0 \u00a0 - --rest_api_port=8080\u00a0 \u00a0 \u00a0 - --port=8500\u00a0 \u00a0 \u00a0 - --file_system_poll_wait_seconds=30\u00a0 \u00a0 \u00a0 - --model_config_file_poll_wait_seconds=30\u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 - /bin/tensorflow_model_server\u00a0 \u00a0 \u00a0 # The image URI field must contain one of the following values:\u00a0 \u00a0 \u00a0 # For CPU-based models: gcr.io/aiml/prediction/containers/tf2-cpu.2-6:latest\u00a0 \u00a0 \u00a0 # For GPU-based models: gcr.io/aiml/prediction/containers/tf2-gpu.2-6:latest\u00a0 \u00a0 \u00a0 imageURI: gcr.io/aiml/prediction/containers/tf2-gpu.2-6:latest\u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 - 8080\u00a0 \u00a0 \u00a0 grpcPorts:\u00a0 \u00a0 \u00a0 - 8500\u00a0 resourcePoolRef:\u00a0 \u00a0 kind: ResourcePool\u00a0 \u00a0 name: RESOURCE_POOL_NAME\u00a0 \u00a0 namespace: PROJECT_NAMESPACE\n```Replace the following:- ``: the name you want to give to the`DeployedModel`definition file.\n- ``: the name of the project namespace associated with the Prediction user cluster.\n- ``: the name of the existing endpoint, for example,`my-img-prediction-endpoint`.\n- ``: the path to your model in the storage bucket.\n- ``: the name you gave to the`ResourcePool`definition file when [you created a resource pool](#resource-pool) to host the model.\nModify the values on the remaining fields according to your prediction model.\n- Apply the `DeployedModel` definition file to the Prediction user cluster:```\nkubectl --kubeconfig PREDICTION_CLUSTER_KUBECONFIG apply -f DEPLOYED_MODEL_NAME.yaml\n```Replace the following:- ``: the path to the kubeconfig file in the Prediction user cluster.\n- ``with the name of the`DeployedModel`definition file.\nWhen you create the `DeployedModel` CR, the Kubernetes API and the webhook service validate the YAML file and report success or failure. The Prediction operator reconciles the `DeployedModel` CR and serves it in the Prediction user cluster. **Tip:** You can check the status of the `DeployedModel` CR and ensure it is ready to accept prediction requests using the `kubectl --kubeconfig` `` `get -f` `` `.yaml -o jsonpath='{.status.primaryCondition}'` command. The `DeployedModel` must be in a ready state.\n- Show details of the existing `Endpoint` CR:```\nkubectl --kubeconfig PREDICTION_CLUSTER_KUBECONFIG describe -f ENDPOINT_NAME.yaml\n```Replace `` with the name of the `Endpoint` definition file.\n- Update the YAML file of the `Endpoint` CR definition by adding a new `serviceRef` object on the `destinations` field. On the new object, include the appropriate service name based on your newly created `DeployedModel` CR.The following YAML file shows a sample configuration:```\napiVersion: aiplatform.gdc.goog/v1kind: Endpointmetadata:\u00a0 name: ENDPOINT_NAME\u00a0 namespace: PROJECT_NAMESPACEspec:\u00a0 createDns: true\u00a0 id: PREDICTION_ENDPOINT\u00a0 destinations:\u00a0 \u00a0 - serviceRef:\u00a0 \u00a0 \u00a0 \u00a0 kind: DeployedModel\u00a0 \u00a0 \u00a0 \u00a0 name: DEPLOYED_MODEL_NAME\u00a0 \u00a0 \u00a0 \u00a0 namespace: PROJECT_NAMESPACE\u00a0 \u00a0 \u00a0 trafficPercentage: 40\u00a0 \u00a0 \u00a0 grpcPort: 8501\u00a0 \u00a0 \u00a0 httpPort: 8081\u00a0 \u00a0 - serviceRef:\u00a0 \u00a0 \u00a0 \u00a0 kind: DeployedModel\u00a0 \u00a0 \u00a0 \u00a0 name: DEPLOYED_MODEL_NAME_2\u00a0 \u00a0 \u00a0 \u00a0 namespace: PROJECT_NAMESPACE\u00a0 \u00a0 \u00a0 trafficPercentage: 50\u00a0 \u00a0 \u00a0 grpcPort: 8501\u00a0 \u00a0 \u00a0 httpPort: 8081\u00a0 \u00a0 - serviceRef:\u00a0 \u00a0 \u00a0 \u00a0 kind: DeployedModel\u00a0 \u00a0 \u00a0 \u00a0 name: DEPLOYED_MODEL_NAME_3\u00a0 \u00a0 \u00a0 \u00a0 namespace: PROJECT_NAMESPACE\u00a0 \u00a0 \u00a0 trafficPercentage: 10\u00a0 \u00a0 \u00a0 grpcPort: 8501\u00a0 \u00a0 \u00a0 httpPort: 8081\n```Replace the following:- ``: the name of the existing`Endpoint`definition file.\n- ``: the name of the project namespace associated with the Prediction user cluster.\n- ``: the name of the existing endpoint. You referenced this name on the`DeployedModel`definition file.\n- ``: the name of a previously created`DeployedModel`definition file.\n- ``: the name you gave to the newly created`DeployedModel`definition file.\nYou can have one or more `serviceRef` destinations. If you have a third `serviceRef` object, add it to the YAML file on the `destinations` field and replace `` with the name you gave to a third `DeployedModel` definition file you created. Keep adding or removing `serviceRef` objects as you need them, depending on the amount of models you are deploying.Set the `trafficPercentage` fields based on how you want to split traffic between the models of this endpoint. Modify the values on the remaining fields according to your endpoint configurations.\n- Apply the `Endpoint` definition file to the Prediction user cluster:```\nkubectl --kubeconfig PREDICTION_CLUSTER_KUBECONFIG apply -f ENDPOINT_NAME.yaml\n```Replace `` with the name of the `Endpoint` definition file.\nTo get the endpoint URL path for the prediction model, run the following command:\n```\nkubectl --kubeconfig PREDICTION_CLUSTER_KUBECONFIG get endpoint PREDICTION_ENDPOINT -n PROJECT_NAMESPACE -o jsonpath='{.status.endpointFQDN}'\n```\nReplace the following:\n- ``: the path to the kubeconfig file in the Prediction user cluster.\n- ``: the name of the endpoint.\n- ``: the name of the prediction project namespace.", "guide": "Documentation"}