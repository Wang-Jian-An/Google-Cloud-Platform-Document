{"title": "Cloud Architecture Center - Deploy a job to import logs from Cloud Storage to Cloud Logging", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Deploy a job to import logs from Cloud Storage to Cloud Logging\nLast reviewed 2024-01-02 UTC\nThis document describes how you deploy the reference architecture described in [Import logs from Cloud Storage to Cloud Logging](/architecture/import-logs-from-storage-to-logging) .\nThese instructions are intended for engineers and developers, including DevOps, site reliability engineers (SREs), and security investigators, who want to configure and run the log importing job. This document also assumes you are familiar with running Cloud Run import jobs, and how to use Cloud Storage and Cloud Logging.\n", "content": "## Architecture\nThe following diagram shows how Google Cloud services are used in this reference architecture:\nFor details, see [Import logs from Cloud Storage to Cloud Logging](/architecture/import-logs-from-storage-to-logging) .\n## Objectives\n- Create and configure a Cloud Run import job\n- Create a service account to run the job## Costs\nIn this document, you use the following billable components of Google Cloud:\n- [Cloud Logging](/logging) \n- [Cloud Run](/run) \n- [Cloud Storage](/storage) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) .\n## Before you begin\n- Ensure that the logs you intend to import were previously exported to Cloud Storage, which means that they're already organized in the expected [export format](/logging/docs/export/storage#gcs-organization) .\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- [Create or select a Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects) .- Create a Google Cloud project:```\ngcloud projects create PROJECT_ID\n```Replace `` with a name for the Google Cloud project you are creating.\n- Select the Google Cloud project that you created:```\ngcloud config set project PROJECT_ID\n```Replace `` with your Google Cloud project name.- Replace with the destination project ID.\n- **Note:** We recommend that you create a new designated project for the imported logs. If you use an existing project, imported logs might get routed to unwanted destinations, which can cause extra charges or accidental export. To ensure logs don't route to unwanted destinations, review the filters on all the [sinks](/logging/docs/routing/overview#sinks) , including `_Required` and `_Default` . Ensure that sinks are not inherited from [organizations or folders](/logging/docs/default-settings) .\n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#gcloud) .\n- Enable the Cloud Run and Identity and Access Management (IAM) APIs:```\ngcloud services enable run.googleapis.com iam.googleapis.com\n```\n### Required roles\nTo get the permissions that you need to deploy this solution,   ask your administrator to grant you the  following IAM roles:\n- To grant the Logs Writer role on the log bucket: [Project IAM Admin ](https://cloud.google.com/iam/docs/understanding-roles#resourcemanager.projectIamAdmin) (`roles/resourcemanager.projectIamAdmin`)    on the destination project\n- To grant the Storage Object Viewer role on the storage bucket: [Storage Admin ](https://cloud.google.com/iam/docs/understanding-roles#storage.admin) (`roles/storage.admin`)    on the project where the storage bucket is hosted\n- To create a service account: [Create Service Accounts ](https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountCreator) (`roles/iam.serviceAccountCreator`)    on the destination project\n- To enable services on the project: [Service Usage Admin ](https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin) (`roles/serviceusage.serviceUsageAdmin`)    on the destination project\n- To upgrade the log bucket and delete imported logs: [Logging Admin ](https://cloud.google.com/iam/docs/understanding-roles#logging.admin) (`roles/logging.admin`)    on the destination project\n- To create, run, and modify the import job: [Cloud Run Developer ](https://cloud.google.com/iam/docs/understanding-roles#run.developer) (`roles/run.developer`)    on the destination project\nFor more information about granting roles, see [Manage access](/iam/docs/granting-changing-revoking-access) .\nYou might also be able to get  the required permissions through [custom  roles](/iam/docs/creating-custom-roles) or other [predefined  roles](/iam/docs/understanding-roles) .\n### Upgrade the log bucket to use Log Analytics\nWe recommend that you use the default log bucket, and upgrade it to use Log Analytics. However, in a production environment, you can use your own log bucket if the default bucket doesn't meet your requirements. If you decide to use your own bucket, you must route logs that are ingested to the destination project to this log bucket. For more information, see [Configure log buckets](/logging/docs/buckets#create_bucket) and [Create a sink](/logging/docs/export/configure_export_v2#creating_sink) .\nWhen you upgrade the bucket, you can use SQL to query and analyze your logs. There's no additional cost to upgrade the bucket or use Log Analytics.\n**Note:** After you upgrade a bucket, it can't be downgraded. For details about settings and restrictions, see [Upgrade a bucket to use Log Analytics](/logging/docs/buckets#upgrade-bucket) .\nTo upgrade the default log bucket in the destination project, do the following:\n- Upgrade the default log bucket to use Log Analytics:```\ngcloud logging buckets update BUCKET_ID --location=LOCATION --enable-analytics\n```Replace the following:- : the name of the log bucket (for example,`_Default`)\n- : a supported region (for example,`global`)\n## Create the Cloud Run import job\nWhen you create the job, you can use the prebuilt container image that is provided for this reference architecture. If you need to modify the implementation to change the 30-day [retention period](/architecture/import-logs-from-storage-to-logging#retention-period) or if you have other requirements, you can [build your own custom image](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/logging/import-logs/README.md#build) .\n- In Cloud Shell, create the job with the configurations and environment variables:```\ngcloud run jobs create JOB_NAME \\--image=IMAGE_URL \\--region=REGION \\--tasks=TASKS \\--max-retries=0 \\--task-timeout=60m \\--cpu=CPU \\--memory=MEMORY \\--set-env-vars=END_DATE=END_DATE,LOG_ID=LOG_ID,\\START_DATE=START_DATE,STORAGE_BUCKET_NAME=STORAGE_BUCKET_NAME,\\PROJECT_ID=PROJECT_ID\n```Replace the following:- : the name of your job.\n- : the reference to the container image; use`us-docker.pkg.dev/cloud-devrel-public-resources/samples/import-logs-solution`or the URL of the custom image, if you built one by using the [instructions in GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/main/logging/import-logs/README.md#build) .\n- : the region where you want your job to be located; to avoid additional costs, we recommend keeping the job region the same or within the same multi-region as the Cloud Storage bucket region. For example, if your bucket is multi-region US, you can use us-central1. For details, see [Cost optimization](/architecture/import-logs-from-storage-to-logging#cost-optimization) .\n- : the number of tasks that the job must run. The default value is`1`. You can increase the number of tasks if timeouts occur.\n- : the CPU limit, which can be 1, 2, 4, 6, or 8 CPUs. The default value is`2`. You can increase the number if timeouts occur; for details, see [Configure CPU limits](/run/docs/configuring/services/cpu) .\n- : the memory limit. The default value is`2Gi`. You can increase the number if timeouts occur; for details, see [Configure memory limits](/run/docs/configuring/services/memory-limits) .\n- : the end of the date range in the format MM/DD/YYYY. Logs with timestamps earlier than or equal to this date are imported.\n- : the log identifier of the logs you want to import. Log ID is a part of the [logName field](/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.log_name) of the log entry. For example,`cloudaudit.googleapis.com`.\n- : the start of the date range in the format MM/DD/YYYY. Logs with timestamps later than or equal to this date are imported.\n- : the name of the Cloud Storage bucket where logs are stored (without the`gs://`prefix).\nThe `max-retries` option is set to zero to prevent retries for failed tasks, which can cause duplicate log entries.If the Cloud Run job fails due to a timeout, an incomplete import can result. To prevent incomplete imports due to timeouts, increase the `tasks` value, as well as the [CPU](/run/docs/configuring/services/cpu) and [memory](/run/docs/configuring/services/memory-limits) resources. Avoid setting a `task-timeout` value greater than `60m` ; for details, see [Using task timeouts greater than one hour](/run/docs/configuring/task-timeout#long-task-timeout) .\nIncreasing these values might increase costs. For details about costs, see [Cost optimization](/architecture/import-logs-from-storage-to-logging#cost-optimization) .\n## Create a service account to run your Cloud Run job\n- In Cloud Shell, create the user-managed service account:```\ngcloud iam service-accounts create SA_NAME\n```Replace with the name of the service account.\n- Grant the [Storage Object Viewer](/iam/docs/understanding-roles#storage.objectViewer) role on the storage bucket:```\ngcloud storage buckets add-iam-policy-binding gs://STORAGE_BUCKET_NAME \\--member=serviceAccount:SA_NAME@PROJECT_ID.iam.gserviceaccount.com \\--role=roles/storage.objectViewer\n```Replace the following:- : the name of the storage bucket that you used in the import job configuration. For example,`my-bucket`.\n- : the destination project ID.\n- Grant the [Logs Writer](/iam/docs/understanding-roles#logging.logWriter) role on the log bucket:```\ngcloud projects add-iam-policy-binding PROJECT_ID \\--member=serviceAccount:SA_NAME@PROJECT_ID.iam.gserviceaccount.com \\--role=roles/logging.logWriter\n```\n- Set the service account for the Cloud Run job:```\ngcloud run jobs update JOB_NAME \\--region=REGION \\--service-account SA_NAME@PROJECT_ID.iam.gserviceaccount.com\n```Replace with the same region where you deployed the Cloud Run import job.## Run the import job\n- In Cloud Shell, execute the created job:```\ngcloud run jobs execute JOB_NAME \\--region=REGION\n```\nFor more information, see [Execute jobs](/run/docs/execute/jobs) and [Manage job executions](/run/docs/managing/job-executions) .\nIf you need to rerun the job, delete the previously imported logs to avoid creating duplicates. For details, see [Delete imported logs](#delete_imported_logs) later in this document.\nWhen you query the imported logs, duplicates don't appear in the query results. Cloud Logging removes duplicates (log entries from the same project, with the same insertion ID and timestamp) from query results. For more information, see the [insert_id field](/logging/docs/reference/v2/rest/v2/LogEntry#FIELDS.insert_id) in the Logging API reference.\n## Verify results\nTo validate that the job has completed successfully, in Cloud Shell, you can query import results:\n```\n\u00a0 gcloud logging read 'log_id(\"imported_logs\") AND timestamp<=END_DATE'\n```\nThe output shows the imported logs. If this project was used to run more than one import job within the specified timeframe, the output shows imported logs from those jobs as well.\nFor more options and details about querying log entries, see [gcloud logging read](/sdk/gcloud/reference/logging/read) .\n## Delete imported logs\nIf you need to run the same job more than one time, delete the previously imported logs to avoid duplicated entries and increased costs.\n- To delete imported logs, in Cloud Shell, execute the logs delete:```\ngcloud logging logs delete imported_logs\n```\nBe aware that deleting imported logs purges log entries that were imported to the destination project and not only the results of the last import job execution.\n## What's Next\n- Review the implementation code in the [GitHub repository](https://github.com/GoogleCloudPlatform/python-docs-samples/tree/main/logging/import-logs) .\n- Learn how to analyze imported logs by using [Log Analytics and SQL](/logging/docs/analyze/query-and-view) .\n- Learn how to [export logs](/architecture/exporting-stackdriver-logging-for-compliance-requirements) to the Cloud Storage bucket.\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .## Contributors\nAuthor: [Leonid Yankulin](https://www.linkedin.com/in/minherz) | Developer Relations Engineer\nOther contributors:\n- [Summit Tuladhar](https://www.linkedin.com/in/summitraj) | Sr. Staff Software Engineer\n- [Wilton Wong](https://www.linkedin.com/in/wiltwong) | Enterprise Architect\n- [Xiang Shen](https://www.linkedin.com/in/xiangshen07) | Cloud Solutions Architect", "guide": "Cloud Architecture Center"}