{"title": "Documentation - Maintain Kubernetes clusters", "url": "https://cloud.google.com/distributed-cloud/hosted/docs/latest/gdch/overview", "abstract": "# Documentation - Maintain Kubernetes clusters\nGoogle Distributed Cloud Hosted (GDCH) lets you manage your Kubernetes clusters after creation using GKE on GDCH. This service lets you adapt to your evolving container workload requirements.\n", "content": "## Before you begin\nTo view and manage node pools in a user cluster, you must have the following roles:\n- User Cluster Admin (`user-cluster-admin`)\n- User Cluster Node Viewer (`user-cluster-node-viewer`)\nTo run commands against a user cluster, ensure you have the following resources:\n- Locate the user cluster name, or ask your Platform Administrator what the cluster name is.\n- [Sign in and generate](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/iam/sign-in#cli) the kubeconfig file for the user cluster if you don't have one.\n- Use the kubeconfig path of the user cluster to replace `` in these instructions.## Perform node maintenance\nWhen you need to repair or maintain nodes, first put the nodes into maintenance mode. Putting nodes into maintenance mode safely drains their pods and workloads, and excludes the nodes from pod scheduling. In maintenance mode, you can work on your nodes without a risk of disrupting pod traffic.\n### How it works\nMaintenance mode for GDCH is similar to running `kubectl cordon` and `kubectl drain` for a specific node. Here are a few details that are relevant to maintenance mode:\n- Specified nodes are marked as unschedulable. This action is what`kubectl cordon`does.\n- Node taints are added to specified nodes to indicate that no pods can be scheduled or executed on the nodes. This action is similar to`kubectl drain`.\n- A 20-minute timeout is enforced to ensure that the nodes don't get stuck waiting for pods to terminate. Pods might not terminate if they are configured to tolerate all taints or they have finalizers. GDCH clusters attempt to terminate all pods, but if the timeout is exceeded, the node is put into maintenance mode. This timeout prevents running pods from blocking upgrades.\n- If you have a VM-based workload running on the node, GDCH clusters apply a`NodeSelector`to the virtual machine instance (VMI) pod, then stop the pod. The`NodeSelector`ensures that the VMI pod is restarted on the same node when the node is removed from maintenance mode.\n### Put a node into maintenance mode\nChoose the nodes you want to put into maintenance mode by specifying IP address ranges for the selected nodes in the `maintenanceBlocks` section of your cluster configuration file. The nodes you choose must be in a `Ready` state, and functioning in the cluster.\nTo put nodes into maintenance mode:\n- Edit the cluster configuration file to select the nodes you want to put into maintenance mode.You can edit the configuration file with an editor of your choice, or you can edit the cluster custom resource directly by running the following command:```\nkubectl edit cluster CLUSTER_NAME \\\u00a0 \u00a0 -n CLUSTER_NAMESPACE \\\u00a0 \u00a0 --kubeconfig USER_CLUSTER_KUBECONFIG\n```Replace the following:- ``: the name of the cluster.\n- ``: the namespace of the cluster.\n- ``: The path of the kubeconfig file.\nAfter the cluster configuration is applied, the cluster puts the applicable nodes into maintenance mode.\n- Add the `maintenanceBlocks` section to the cluster configuration file to specify either a single IP address, or an address range, for nodes you want to put into maintenance mode.The following sample shows how to select multiple nodes by specifying a range of IP addresses:```\n...metadata:\u00a0 name: my-cluster\u00a0 namespace: cluster-my-clusterspec:\u00a0 maintenanceBlocks:\u00a0 \u00a0 cidrBlocks:\u00a0 \u00a0 - 172.16.128.1-172.16.128.64...\n```\n- Get the status of the nodes in your cluster:```\nkubectl get nodes -n CLUSTER_NAME \\\u00a0 \u00a0 --kubeconfig USER_CLUSTER_KUBECONFIG\n```The response is something like the following:```\nNAME    STATUS      ROLES AGE  VERSION\nuser-gdch-01  Ready      master 2d22h v1.23.5-gke.1502\nuser-gdch-04  Ready      none  2d22h v1.23.5-gke.1502\nuser-gdch-05  Ready,SchedulingDisabled none  2d22h v1.23.5-gke.1502\nuser-gdch-06  Ready      none  2d22h v1.23.5-gke.1502\n```A status of `SchedulingDisabled` indicates that a node is in maintenance mode.\n- Get the number of nodes in maintenance mode:```\nkubectl get nodepools --kubeconfig USER_CLUSTER_KUBECONFIG\n```The response looks similar to the following output:```\nNAME READY RECONCILING STALLED UNDERMAINTENANCE UNKNOWN\nnp1 3  0    0   1     0\n```The `UNDERMAINTENANCE` column in this sample shows that one node is in maintenance mode.Clusters also add the following taints to nodes when they are put into maintenance mode:\n- `baremetal.cluster.gke.io/maintenance:NoExecute`\n- `baremetal.cluster.gke.io/maintenance:NoSchedule`## Resize node pools\nAny user cluster in the GDCH environment can have its node pool resized to scale with workload changes. To manage node pools in a user cluster, you must have the **User Cluster Admin** ( `user-cluster-admin` ) role.\nTo scale a node pool in an existing cluster, complete the following steps:\n- In the dashboard, select the project in which the cluster you intend to edit exists.\n- In the navigation menu, select **Clusters** .\n- Select the cluster name that the node pool is associated with. The **Cluster\ndetails** page is displayed.\n- Click the **Node pools** tab.\n- Select the **Edit** icon for the node pool you want to resize. The **Edit node pool** prompt is displayed.\n- Update the **Number of nodes** field to reflect the new amount of nodes required in the node pool. You can increase or decrease the number of nodes to fit your workload requirements. **Note:** Downscaling your node pool can cause resource issues if the remaining nodes do not have enough memory to handle your existing workloads.\n- Click **Save** .\n- Navigate back to the **Node pools** tab for your cluster and confirm the resized node pool has the `Ready` status and has the correct number of nodes. It can take a few minutes for the node pool to scale to your specification.\n- Open the `Cluster` custom resource spec with the `kubectl` CLI using the interactive editor:```\nkubectl edit clusters.cluster.gdc.goog/USER_CLUSTER_NAME -n platform \\\u00a0 \u00a0 \u00a0 --kubeconfig ORG_ADMIN_CLUSTER_KUBECONFIG\n```\n- Update the `nodeCount` field for the node pool to resize:```\nnodePools:...- machineTypeName: n2-standard-2-gdc\u00a0 name: nodepool-1\u00a0 nodeCount: NUMBER_OF_WORKER_NODES\n```Replace `` with the updated number of worker nodes to provision in the node pool.\n- Save the file and exit the editor.\n- Verify your node scaling is complete by checking the node pool's configuration:```\nkubectl get clusters.cluster.gdc.goog/USER_CLUSTER_NAME -n platform -o json \\\u00a0 \u00a0 --kubeconfig ORG_ADMIN_CLUSTER_KUBECONFIG | jq .status.workerNodePoolStatuses\n```Confirm the `readyNodes` number reflects the amount of nodes you set for the node pool. It can take a few minutes for the node pool to scale to your specification.## View all clusters in an organization\nYou can view all available user clusters in an organization, including their statuses, Kubernetes versions, and other details.\n- In the navigation menu, select **Clusters** .All available clusters in the organization with their statuses and other information are displayed:\n- List the available user clusters in an organization:```\nkubectl get clusters.cluster.gdc.goog -n platform \\\u00a0 \u00a0 --kubeconfig ORG_ADMIN_CLUSTER_KUBECONFIG\n```The output is similar to the following:```\nNAME \u00a0 \u00a0 \u00a0 \u00a0STATE \u00a0 \u00a0 K8S VERSIONuser-vm-1 \u00a0 Running \u00a0 1.25.10-gke.2100user-test \u00a0 Running \u00a0 1.26.5-gke.2100\n```## View updatable properties\nFor each user cluster, a set of properties are available to change after it is created. You can only change the mutable properties that are in the `spec` of the `Cluster` custom resource. Not all properties in the `spec` are eligible to update after the cluster is provisioned. To view these updatable properties, complete the following steps:\n- In the navigation menu, select **Clusters** .\n- In the list of user clusters, click a cluster name to view its properties.\n- Editable properties have an **Edit** icon.\n- View the list of properties for the `Cluster` spec and the valid values corresponding to each property:```\nkubectl explain clusters.cluster.gdc.goog.spec \\\u00a0 \u00a0 --kubeconfig ORG_ADMIN_CLUSTER_KUBECONFIG\n```The output is similar to the following:```\nKIND: \u00a0 \u00a0 ClusterVERSION: \u00a0cluster.gdc.goog/v1RESOURCE: spec <Object>DESCRIPTION:\u00a0 \u00a0 <empty>FIELDS:\u00a0 clusterNetwork \u00a0 \u00a0<Object>\u00a0 \u00a0 The cluster network configuration. If unset, the default configurations\u00a0 \u00a0 with pod and service CIDR sizes are used. Optional. Mutable.\u00a0 initialVersion \u00a0 \u00a0<Object>\u00a0 \u00a0 The GDCH version information of the user cluster during cluster creation.\u00a0 \u00a0 Optional. Default to use the latest applicable version. Immutable.\u00a0 loadBalancer \u00a0<Object>\u00a0 \u00a0 The load balancer configuration. If unset, the default configuration with\u00a0 \u00a0 the ingress service IP address size is used. Optional. Mutable.\u00a0 nodePools <[]Object>\u00a0 \u00a0 The list of node pools for the cluster worker nodes. Optional. Mutable.\u00a0 releaseChannel \u00a0 \u00a0<Object>\u00a0 \u00a0 The release channel a cluster is subscribed to. When a cluster is\u00a0 \u00a0 subscribed to a release channel, GDCH maintains the cluster versions for\u00a0 \u00a0 users. Optional. Mutable.\n```Update these settings by using the GDCH console or `kubectl` CLI. For example, you can [resize a node pool](#resize-node-pools) .## Scale ingress service IP address size\nYou can scale your ingress service IP address size after you create a user cluster.\n**Important:** You are not able to scale down the ingress service IP address size after you create a user cluster.\n- Open the `Cluster` custom resource spec with the `kubectl` CLI using the  interactive editor:```\nkubectl edit clusters.cluster.gdc.goog/USER_CLUSTER_NAME -n platform \\\u00a0 \u00a0 --kubeconfig ORG_ADMIN_CLUSTER_KUBECONFIG\n```\n- Update the `ingressServiceIPSize` field to the new IP address size:```\n...spec:...\u00a0 loadBalancer:\u00a0 \u00a0 ingressServiceIPSize: INGRESS_SERVICE_IP_SIZE...\n```Replace `` with the updated ingress service IP address size.\n- Save the file and exit the editor.\nThere is no set limit on the ingress service IP address size. The amount of IP addresses you request is fulfilled based on your organization. If the request cannot be fulfilled, the cluster reports an error.\n## Upgrade a user cluster\nYou can perform an automated or manual upgrade of your user cluster. For more instructions on how to upgrade your cluster, see the [User cluster upgrade](/distributed-cloud/hosted/docs/latest/gdch/platform/pa-user/upgrade-guide/instructions#user-cluster-upgrade) section.", "guide": "Documentation"}