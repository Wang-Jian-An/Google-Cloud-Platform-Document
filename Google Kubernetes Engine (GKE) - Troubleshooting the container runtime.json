{"title": "Google Kubernetes Engine (GKE) - Troubleshooting the container runtime", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview", "abstract": "# Google Kubernetes Engine (GKE) - Troubleshooting the container runtime\nThis document provides troubleshooting steps for common issues that you might encounter with the container runtime on your Google Kubernetes Engine (GKE) nodes.\n[Cloud Customer Care](/kubernetes-engine/docs/getting-support)\n", "content": "## Mount paths with simple drive letters fail on Windows node pools with containerd\nGKE clusters running Windows Server node pools that use the containerd runtime prior to version 1.6.6 might experience errors when starting containers like the following:\n```\nfailed to create containerd task : CreateComputeSystem : The parameter is incorrect : unknown\n```\nFor more details, refer to [GitHub issue #6589](https://github.com/containerd/containerd/issues/6589) .\n### Solution\nTo resolve this issue, upgrade your node pools to the latest GKE versions that uses containerd runtime version 1.6.6 or higher.\n## Container images with non-array pre-escaped CMD or ENTRYPOINT command lines fail on Windows node pools with containerd\nGKE clusters running Windows Server node pools that use the containerd runtime 1.5.X might experience errors when starting containers like the following:\n```\nfailed to start containerd task : hcs::System::CreateProcess : The system cannot find the file specified.: unknown\n```\nFor more details, refer to [GitHub issue #5067](https://github.com/containerd/containerd/issues/5067) and [GitHub issue #6300](https://github.com/containerd/containerd/issues/6300) .\n### Solution\nTo resolve this issue, upgrade your node pools to the latest GKE versions that uses containerd runtime version 1.6.6 or higher.\n## Container image volumes with non-existing paths or Linux-like (forward slash) paths fail on Windows node pools with containerd\nGKE clusters running Windows Server node pools that use the containerd runtime 1.5.X might experience errors when starting containers like the following:\n```\nfailed to generate spec: failed to stat \"<volume_path>\": CreateFile : The system cannot find the path specified.\n```\nFor more details, refer to [GitHub issue #5671](https://github.com/containerd/containerd/issues/5671) .\n### Solution\nTo resolve this issue, upgrade your node pools to the latest GKE versions that uses containerd runtime version 1.6.x or higher.\n## /etc/mtab: No such file or directory\nThe Docker container runtime populates this symlink inside the container by default, but the containerd runtime does not.\nFor more details, refer to [GitHub issue #2419](https://github.com/containerd/containerd/issues/2419) .\n### Solution\nTo resolve this issue, manually create the symlink `/etc/mtab` during your image build.\n```\nln -sf /proc/mounts /etc/mtab\n```\n## Image pull error: not a directory\nall\nWhen you build an image with [kaniko](https://github.com/GoogleContainerTools/kaniko) , it might fail to be pulled with containerd with the error message \"not a directory\". This error happens if the image is built in a special way: when a previous command removes a directory and the next command recreates the same files in that directory.\nThe following Dockerfile example with `npm` that illustrates this problem.\n```\nRUN npm cache clean --force\nRUN npm install\n```\nFor more details, refer to [GitHub issue #4659](https://github.com/containerd/containerd/issues/4659) .\n### Solution\nTo resolve this issue, build your image using `docker build` , which is unaffected by this issue.\nIf `docker build` isn't an option for you, then combine the commands into one. The following Dockerfile example combines `RUN npm cache clean --force` and `RUN npm install` :\n```\nRUN npm cache clean --force && npm install\n```\n## Some file system metrics are missing and the metrics format is different\n**Note:** This issue doesn't affect customers that use [Cloud Monitoring](/monitoring/api/metrics_kubernetes) .\nall\nThe Kubelet `/metrics/cadvisor` endpoint provides Prometheus metrics, as documented in [Metrics for Kubernetes system components](https://kubernetes.io/docs/concepts/cluster-administration/system-metrics/) . If you install a metrics collector that depends on that endpoint, you might see the following issues:\n- The metrics format on the Docker node is`k8s_<container-name>_<pod-name>_<namespace>_<pod-uid>_<restart-count>`but the format on the containerd node is`<container-id>`.\n- Some file system metrics are missing on the containerd node, as follows:```\ncontainer_fs_inodes_free\ncontainer_fs_inodes_total\ncontainer_fs_io_current\ncontainer_fs_io_time_seconds_total\ncontainer_fs_io_time_weighted_seconds_total\ncontainer_fs_limit_bytes\ncontainer_fs_read_seconds_total\ncontainer_fs_reads_merged_total\ncontainer_fs_sector_reads_total\ncontainer_fs_sector_writes_total\ncontainer_fs_usage_bytes\ncontainer_fs_write_seconds_total\ncontainer_fs_writes_merged_total\n```\n### Solution\nYou can mitigate this issue by using [cAdvisor](https://github.com/google/cadvisor) as a standalone daemonset.\n- Find the latest [cAdvisor release](https://github.com/google/cadvisor/releases) with the name pattern`vX.Y.Z-containerd-cri`(for example,`v0.42.0-containerd-cri`).\n- Follow the steps in [cAdvisor Kubernetes Daemonset](https://github.com/google/cadvisor/tree/master/deploy/kubernetes) to create the daemonset.\n- Point the installed metrics collector to use the cAdvisor`/metrics`endpoint that provides the full set of [Prometheus container metrics](https://github.com/google/cadvisor/blob/master/docs/storage/prometheus.md) .\n### Alternatives\n- Migrate your monitoring solution to [Cloud Monitoring](/monitoring/api/metrics_kubernetes) , which provides the full set of container metrics.\n- Collect metrics from the [Kubelet summary API](https://pkg.go.dev/k8s.io/kubelet/pkg/apis/stats/v1alpha1) with an endpoint of`/stats/summary`.## Attach-based operations don't function correctly after container-runtime restarts on GKE Windows\n1.21 to 1.21.5-gke.1802, 1.22 to 1.22.3-gke.700\nGKE clusters running Windows Server node pools that use the containerd runtime (version 1.5.4 and 1.5.7-gke.0) might experience issues if the container runtime is forcibly restarted, with attach operations to existing running containers not being able to bind IO again. The issue won't cause failures in API calls, however data won't be sent or received. This includes data for attach and logs CLIs and APIs through the cluster API server.\n### Solution\nTo resolve this issue, upgrade to patched container runtime version (1.5.7-gke.1) with newer GKE releases.\n## Pods display failed to allocate for range 0: no IP addresses available in range set error message\n1.24.6-gke.1500 or earlier, 1.23.14-gke.1800 or earlier, and 1.22.16-gke.2000 or earlier\nGKE clusters running node pools that use containerd might experience IP leak issues and exhaust all the Pod IPs on a node. A Pod scheduled on an affected node displays an error message similar to the following:\n```\nfailed to allocate for range 0: no IP addresses available in range set: 10.48.131.1-10.48.131.62\n```\nFor more information about the issue, see containerd [GitHub issue #5438](https://github.com/containerd/containerd/issues/5438) and [GitHub issue #5768](https://github.com/containerd/containerd/issues/5768) .\nThere is a [known issue](/kubernetes-engine/docs/how-to/dataplane-v2#containerd-pod-ip-leak) in GKE Dataplane V2 that can trigger this issue. However, this issue can be triggered by other causes, including [runc stuck](https://github.com/opencontainers/runc/issues/2865) .\n### Solution\nTo resolve this issue, follow the workarounds mentioned in the [Workarounds for Standard GKE clusters](/kubernetes-engine/docs/how-to/dataplane-v2#workarounds_for_standard_clusters) for GKE Dataplane V2.\n## Exec probe behavior difference when probe exceeds the timeout\nall\nExec probe behavior on containerd images is different from the behavior on `dockershim` images. When exec probe, defined for the Pod, exceeds the declared [Kubernetes timeoutSeconds](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes) threshold, on `dockershim` images, it is treated as a probe failure. On containerd images, probe results returned after the declared `timeoutSeconds` threshold are ignored.\n### Solution\nIn GKE, the feature gate `ExecProbeTimeout` is set to `false` and cannot be changed. To resolve this issue, increase the `timeoutSeconds` threshold for all affected exec probes or implement the timeout functionality as part of the probe logic.\n## Insecure registry option is not configured for local network (10.0.0.0/8)\nall\nOn containerd images, the insecure registry option is **not** configured for local network `10.0.0.0/8` . If you use insecure private registries, you might notice errors similar to the following:\n```\npulling image: rpc error: code = Unknown desc = failed to pull and unpack image \"IMAGE_NAME\": failed to do request: Head \"IMAGE_NAME\": http: server gave HTTP response to HTTPS client\n```\nWe recommend that you use [Artifact Registry](/artifact-registry/docs/repositories/create-repos) or that you configure TLS on your private registries if your use case supports these options.\n### Solution\nFor Standard clusters, try the following steps. This workaround isn't available in Autopilot because privileged containers are a security risk. If your environment is exposed to the internet, consider your risk tolerance before deploying this solution.\n**Caution:** This operation requires a containerd restart, which might cause disruptions in workloads that are already running on the node.\n- Review the following manifest: [  container-insecure-registry/insecure-registry-config.yaml ](https://github.com/GoogleCloudPlatform/k8s-node-tools/blob/HEAD/container-insecure-registry/insecure-registry-config.yaml) [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/GoogleCloudPlatform/k8s-node-tools&page=editor&cloudshell_workspace=container-insecure-registry&cloudshell_open_in_editor=insecure-registry-config.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/k8s-node-tools/blob/HEAD/container-insecure-registry/insecure-registry-config.yaml) ```\napiVersion: apps/v1kind: DaemonSetmetadata:\u00a0 name: insecure-registries\u00a0 namespace: default\u00a0 labels:\u00a0 \u00a0 k8s-app: insecure-registriesspec:\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 name: insecure-registries\u00a0 updateStrategy:\u00a0 \u00a0 type: RollingUpdate\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 name: insecure-registries\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-container-runtime: \"containerd\"\u00a0 \u00a0 \u00a0 hostPID: true\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 \u00a0 - name: startup-script\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image: gcr.io/google-containers/startup-script:v1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 imagePullPolicy: Always\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: ADDRESS\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"REGISTRY_ADDRESS\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: STARTUP_SCRIPT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 set -o errexit\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 set -o pipefail\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 set -o nounset\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if [[ -z \"$ADDRESS\" || \"$ADDRESS\" == \"REGISTRY_ADDRESS\" ]]; then\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 echo \"Error: Environment variable ADDRESS is not set in containers.spec.env\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 exit 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fi\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 echo \"Allowlisting insecure registries\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 grep -qxF '[plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"'$ADDRESS'\"]' /etc/containerd/config.toml || \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 echo -e '[plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"'$ADDRESS'\"]\\n \u00a0endpoint = [\"http://'$ADDRESS'\"]' >> /etc/containerd/config.toml\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 echo \"Reloading systemd management configuration\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 systemctl daemon-reload\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 echo \"Restarting containerd...\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 systemctl restart containerd\n```In the `.spec.containers.env` field, replace the `` value of the `ADDRESS` variable with the address of your local HTTP registry in the format `DOMAIN_NAME:PORT` . For example,```\ncontainers:- name: startup-script\u00a0 ...\u00a0 env:\u00a0 - name: ADDRESS\u00a0 \u00a0 value: \"example.com:5000\"\n```\n- Deploy the DaemonSet:```\nkubectl apply -f insecure-registry-ds.yaml\n```\nThe DaemonSet adds your insecure registry to the containerd configuration on every node.\n## containerd ignores any device mappings for privileged pods\nall\nFor [privileged Kubernetes Pods](https://kubernetes.io/docs/concepts/policy/pod-security-policy/#privileged) , the container runtime ignores any device mappings that `volumeDevices.devicePath` pass to it, and instead makes every device on the host available to the container under `/dev` .\n## containerd leaks shim processes when nodes are under I/O pressure\nWhen a GKE node is under I/O pressure, containerd might fail to delete the `containerd-shim-runc-v2` processes when a Pod is deleted, resulting in process leaks. When the leak happens on a node, you'll see more `containerd-shim-runc-v2` processes on the node than the number of Pods on that node. You might also see increased memory and CPU usage along with extra PIDs. For details, see the GitHub issue [Fix leaked shim caused by high IO pressure](https://github.com/containerd/containerd/pull/8954) .\nTo resolve this issue, [upgrade your nodes](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrade_cp) to the following versions or later:\n- 1.25.15-gke.1040000\n- 1.26.10-gke.1030000\n- 1.27.6-gke.1513000\n- 1.28.3-gke.1061000## IPv6 address family is enabled on pods running containerd\n1.18, 1.19, 1.20.0 to 1.20.9\nIPv6 image family is enabled for Pods running with containerd. The `dockershim` image disables IPv6 on all Pods, while the containerd image does not. For example, `localhost` resolves to IPv6 address `::1` first. This typically isn't a problem, but this might result in unexpected behavior in certain cases.\n### Solution\nTo resolve this issue, use an IPv4 address such as `127.0.0.1` explicitly, or configure an application running in the Pod to work on both address families.\n## Node auto-provisioning only provisions Container-Optimized OS with Docker node pools\n1.18, 1.19, 1.20.0 to 1.20.6-gke.1800\n[Node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) allows autoscaling node pools with [any supported image type](/kubernetes-engine/docs/concepts/node-images#available_node_images) , but can only create node pools with the [Container-Optimized OS with Docker](/kubernetes-engine/docs/concepts/node-images#cos) image type.\n### Solution\nTo resolve this issue, upgrade your GKE clusters to version 1.20.6-gke.1800 or later. In these GKE versions, the default image type can be [set for the cluster](/kubernetes-engine/docs/how-to/node-auto-provisioning#default-image-type) .\n## Conflict with 172.17/16 IP address range\n1.18.0 to 1.18.14\nThe `172.17/16` IP address range is occupied by the `docker0` interface on the node VM with containerd enabled. Traffic sending to or originating from that range might not be routed correctly (for example, a Pod might not be able to connect to a VPN-connected host with an IP address within `172.17/16` ).\n## GPU metrics not collected\n1.18.0 to 1.18.18\n[GPU usage metrics](/kubernetes-engine/docs/how-to/gpus#monitoring) are not collected when using containerd as a runtime on GKE versions before 1.18.18.\n### Solution\nTo resolve this issue, upgrade your clusters to GKE versions 1.18.18 or later.\n## Images with config.mediaType set to application/octet-stream can't be used on containerd\nall\nImages with `config.mediaType` set to `\"application/octet-stream\"` cannot be used on containerd. For more information, see [GitHub issue #4756](https://github.com/containerd/containerd/issues/4756#issuecomment-731287262) . These images are not compatible with the Open Container Initiative specification and are considered incorrect. These images work with Docker to provide backward compatibility, while in containerd these images are not supported.\n### Symptom and diagnosis\nExample error in node logs:\n```\nError syncing pod <pod-uid> (\"<pod-name>_<namespace>(<pod-uid>)\"), skipping: failed to \"StartContainer\" for \"<container-name>\" with CreateContainerError: \"failed to create containerd container: error unpacking image: failed to extract layer sha256:<some id>: failed to get reader from content store: content digest sha256:<some id>: not found\"\n```\nThe image manifest can usually be found in the registry where it is hosted. Once you have the manifest, check `config.mediaType` to determine if you have this issue:\n```\n\"mediaType\": \"application/octet-stream\",\n```\n### Solution\nAs the containerd community decided to not support such images, all versions of containerd are affected and there is no fix. The container image must be rebuilt with Docker version 1.11 or later and you must ensure that the `config.mediaType` field is not set to `\"application/octet-stream\"` .\n## CNI config uninitialized\nall\nGKE fails to create nodes during an upgrade, resize, or other action.\n### Symptom and diagnosis\nExample error in the Google Cloud console:\n```\nError: \"runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\".\n```\nThis error might occur in the following situations:\n- During node bootstrapping in log files while GKE installs the CNI config.\n- As a node error status in the Google Cloud console if a custom webhook that intercepts the DaemonSet controller command to create a Pod has errors. This prevents GKE from creating a`netd`or`calico-node`Pod. If`netd`or`calico-node`Pods started successfully while the error persists, contact support.\n### Solution\nTo resolve this issue, try the following solutions:\n- Wait for GKE to finish installing the CNI config.\n- Remove any misconfigured webhooks.\n- Configure webhooks to [ignore system Pods](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#avoiding-operating-on-the-kube-system-namespace) .## What's next\n[Cloud Customer Care](/kubernetes-engine/docs/getting-support)", "guide": "Google Kubernetes Engine (GKE)"}