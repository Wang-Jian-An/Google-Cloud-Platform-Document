{"title": "Vertex AI - Get predictions for a forecast model", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Get predictions for a forecast model\nThis page shows you how to create a forecast using your trained forecast model.\nTo create a forecast, you make a batch prediction request directly to your forecast model, specifying an input source and an output location to store the forecast results.\nForecasting with AutoML is not compatible with endpoint deployment or online predictions. If you want to request online predictions from your forecast model, use [Tabular Workflow for Forecasting](/vertex-ai/docs/tabular-data/tabular-workflows/forecasting) .\nYou can request a prediction with explanations (also called feature attributions) to see how your model arrived at a prediction. The local feature importance values tell you how much each feature contributed to the prediction result. For a conceptual overview, see [Feature attributions for forecasting](/vertex-ai/docs/tabular-data/forecasting-explanations) .\n", "content": "## Before you begin\nBefore you can create a forecast, you must [train a forecast model](/vertex-ai/docs/tabular-data/forecasting/train-model) .\n## Input data\nThe input data for batch prediction requests is the data that your model uses to create forecasts. You can provide input data in one of two formats:\n- CSV objects in Cloud Storage\n- BigQuery tables\nWe recommend that you use the same format for your input data as you used for training the model. For example, if you trained your model using data in BigQuery, it is best to use a BigQuery table as the input for your batch prediction. Because Vertex AI treats all CSV input fields as strings, mixing training and input data formats may cause errors.\nYour data source must contain tabular data that includes all of the columns, in any order, that were used to train the model. You can include columns that were not in the training data, or that were in the training data but excluded from use for training. These extra columns are included in the output but don't affect forecast results.### Input data requirements\nInput for forecast models must adhere to the following requirements:\n- All values in the time column must be present and valid.\n- The data frequency for the input data and the training data must match. If  there are missing rows in the time series, you must manually insert them  according to the proper domain knowledge.\n- Time series with duplicate timestamps are removed from predictions. To  include them, remove any duplicate timestamps.\n- Provide historical data for each time series to forecast. For the most  accurate forecasts, the amount of data should equal the context window,  which is set during model training. For example, if the context window is 14  days, provide at least 14 days of historical data. If you provide less data,  Vertex AI pads the data with empty values.\n- The forecast starts on the first row of a time series (ordered by time)  with a null value in the target column. The null value must be continuous  within the time series. For example, if the target column is ordered by  time, you cannot have something like`1`,`2`,`null`,`3`,`4`,`null`,`null`for a single time series.  For CSV files, Vertex AI treats an empty string as null, and for BigQuery, null  values are natively supported.If you choose a BigQuery table as the input, you must ensure the following:- BigQuery data source tables must be no larger than 100 GB.\n- If the table is in a different project, you must grant the`BigQuery Data Editor`role to the Vertex AI service account in  that project.\nIf you choose a CSV object in Cloud Storage as the input, you must ensure the following:- The data source must begin with a header row with the column names.\n- Each data source object must not be larger than 10 GB. You can include  multiple files, up to a maximum amount of 100 GB.\n- If the Cloud Storage bucket is in a different project, you must grant the`Storage Object Creator`role to the Vertex AI service account in  that project.\n- You must enclose all strings in double quotation marks (\").## Output format\nThe output format of your batch prediction request doesn't need to be the same as the format that you used for the input. For example, if you used BigQuery table as the input, you can output forecasting results to a CSV object in Cloud Storage.\n## Make a batch prediction request to your model\nTo make batch prediction requests, you can use the Google Cloud console or the Vertex AI API. The input data source can be CSV objects stored in a Cloud Storage bucket or BigQuery tables. Depending on the amount of data that you submit as input, a batch prediction task can take some time to complete.\nUse the Google Cloud console to request a batch prediction.- In the Google Cloud console, in the Vertex AI section, go to  the **Batch predictions** page. [Go to the Batch predictions page](https://console.cloud.google.com/vertex-ai/batch-predictions) \n- Click **Create** to open the **New batch prediction** window.\n- For **Define your batch prediction** , complete the following steps:- Enter a name for the batch prediction.\n- For **Model name** , select the name of the model to use for this   batch prediction.\n- For **Version** , select the version of the model.\n- For **Select source** , select whether your source input data is a CSV   file on Cloud Storage or a table in BigQuery.- For CSV files, specify the Cloud Storage location where your CSV    input file is located.\n- For BigQuery tables, specify the project ID where the    table is located, the BigQuery dataset ID, and the    BigQuery table or view ID.\n- For **Batch prediction output** , select **CSV** or **BigQuery** .- For CSV, specify the Cloud Storage bucket where    Vertex AI stores your output.\n- For BigQuery, you can specify a project ID or an existing    dataset:- To specify the project ID, enter the project ID in the **Google Cloud project ID** field. Vertex AI creates a new output     dataset for you.\n- To specify an existing dataset, enter its BigQuery path     in the **Google Cloud project ID** field, such as`bq://projectid.datasetid`.\n- **Optional.** If your output destination is BigQuery    or JSONL on Cloud Storage, you can enable feature attributions    in addition to predictions. To do this, select **Enable feature attributions for this model** . Feature attributions    are not supported for CSV on Cloud Storage. [Learn more](/vertex-ai/docs/tabular-data/forecasting-explanations) .\n- Optional: [Model Monitoring](/vertex-ai/docs/model-monitoring/overview) analysis for batch predictions is available in [Preview](/products#product-launch-stages) . See the [Prerequisites](/vertex-ai/docs/model-monitoring/model-monitoring-batch-predictions#prerequisites) for adding skew detection configuration to your batch  prediction job.- Click to toggle on **Enable model monitoring for this batch prediction** .\n- Select a **Training data source** . Enter the data path or location for   the training data source that you selected.\n- Optional: Under **Alert thresholds** , specify thresholds at which to trigger alerts.\n- For **Notification emails** , enter one or more comma-separated email   addresses to receive alerts when a model exceeds an alerting threshold.\n- Optional: For **Notification channels** , add [Cloud Monitoring](/monitoring/support/notification-options) channels to receive alerts when a model exceeds an alerting threshold.   You can select existing Cloud Monitoring channels or create a new   one by clicking **Manage notification channels** . The Console   supports PagerDuty, Slack, and Pub/Sub notification   channels.- Click **Create** .\nYou use the [batchPredictionJobs.create](/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs/create) method to request a batch prediction.\nBefore using any of the request data, make the following replacements:- : Region where Model is stored and batch prediction job is executed. For example,`us-central1`.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) \n- : Display name for the batch job\n- : The ID for the model to use for making predictions\n- : Reference to the BigQuery data source. In the form:```\nbq://bqprojectId.bqDatasetId.bqTableId\n```\n- : Reference to the BigQuery destination (where the  predictions will be written). Specify the project ID and, **optionally** ,  an existing dataset ID. Use the following form:```\nbq://bqprojectId.bqDatasetId\n```If you specify just the project ID, Vertex AI creates a new output dataset for you.  Use the following form:```\nbq://bqprojectId\n```\n- : Default value is. Set toto enable  feature attributions. To learn more, see [Feature attributions for forecasting](/vertex-ai/docs/tabular-data/forecasting-explanations) .\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/batchPredictionJobs\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"BATCH_JOB_NAME\",\n \"model\": \"projects/PROJECT_ID/locations/LOCATION_ID/models/MODEL_ID\",\n \"inputConfig\": {\n \"instancesFormat\": \"bigquery\",\n \"bigquerySource\": {\n  \"inputUri\": \"INPUT_URI\"\n }\n },\n \"outputConfig\": {\n \"predictionsFormat\": \"bigquery\",\n \"bigqueryDestination\": {\n  \"outputUri\": \"OUTPUT_URI\"\n }\n },\n \"generate_explanation\": GENERATE_EXPLANATION\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/batchPredictionJobs\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/batchPredictionJobs\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_ID/locations/LOCATION_ID/batchPredictionJobs/67890\",\n \"displayName\": \"batch_job_1 202005291958\",\n \"model\": \"projects/12345/locations/us-central1/models/5678\",\n \"state\": \"JOB_STATE_PENDING\",\n \"inputConfig\": {\n \"instancesFormat\": \"bigquery\",\n \"bigquerySource\": {\n  \"inputUri\": \"INPUT_URI\"\n }\n },\n \"outputConfig\": {\n \"predictionsFormat\": \"bigquery\",\n \"bigqueryDestination\": {\n  \"outputUri\": bq://12345\n }\n },\n \"dedicatedResources\": {\n \"machineSpec\": {\n  \"machineType\": \"n1-standard-32\",\n  \"acceleratorCount\": \"0\"\n },\n \"startingReplicaCount\": 2,\n \"maxReplicaCount\": 6\n },\n \"manualBatchTuningParameters\": {\n \"batchSize\": 4\n },\n \"outputInfo\": {\n \"bigqueryOutputDataset\": \"bq://12345.reg_model_2020_10_02_06_04\n }\n \"state\": \"JOB_STATE_PENDING\",\n \"createTime\": \"2020-09-30T02:58:44.341643Z\",\n \"updateTime\": \"2020-09-30T02:58:44.341643Z\",\n}\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nIn the following sample, replace\nand\nwith `bigquery`. To learn how to replace the other placeholders, see the `REST & CMD LINE` tab of this section.\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateBatchPredictionJobBigquerySample.java) \n```\nimport com.google.cloud.aiplatform.v1.BatchPredictionJob;import com.google.cloud.aiplatform.v1.BigQueryDestination;import com.google.cloud.aiplatform.v1.BigQuerySource;import com.google.cloud.aiplatform.v1.JobServiceClient;import com.google.cloud.aiplatform.v1.JobServiceSettings;import com.google.cloud.aiplatform.v1.LocationName;import com.google.cloud.aiplatform.v1.ModelName;import com.google.gson.JsonObject;import com.google.protobuf.Value;import com.google.protobuf.util.JsonFormat;import java.io.IOException;public class CreateBatchPredictionJobBigquerySample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"PROJECT\";\u00a0 \u00a0 String displayName = \"DISPLAY_NAME\";\u00a0 \u00a0 String modelName = \"MODEL_NAME\";\u00a0 \u00a0 String instancesFormat = \"INSTANCES_FORMAT\";\u00a0 \u00a0 String bigquerySourceInputUri = \"BIGQUERY_SOURCE_INPUT_URI\";\u00a0 \u00a0 String predictionsFormat = \"PREDICTIONS_FORMAT\";\u00a0 \u00a0 String bigqueryDestinationOutputUri = \"BIGQUERY_DESTINATION_OUTPUT_URI\";\u00a0 \u00a0 createBatchPredictionJobBigquerySample(\u00a0 \u00a0 \u00a0 \u00a0 project,\u00a0 \u00a0 \u00a0 \u00a0 displayName,\u00a0 \u00a0 \u00a0 \u00a0 modelName,\u00a0 \u00a0 \u00a0 \u00a0 instancesFormat,\u00a0 \u00a0 \u00a0 \u00a0 bigquerySourceInputUri,\u00a0 \u00a0 \u00a0 \u00a0 predictionsFormat,\u00a0 \u00a0 \u00a0 \u00a0 bigqueryDestinationOutputUri);\u00a0 }\u00a0 static void createBatchPredictionJobBigquerySample(\u00a0 \u00a0 \u00a0 String project,\u00a0 \u00a0 \u00a0 String displayName,\u00a0 \u00a0 \u00a0 String model,\u00a0 \u00a0 \u00a0 String instancesFormat,\u00a0 \u00a0 \u00a0 String bigquerySourceInputUri,\u00a0 \u00a0 \u00a0 String predictionsFormat,\u00a0 \u00a0 \u00a0 String bigqueryDestinationOutputUri)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 JobServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 JobServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (JobServiceClient client = JobServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 JsonObject jsonModelParameters = new JsonObject();\u00a0 \u00a0 \u00a0 Value.Builder modelParametersBuilder = Value.newBuilder();\u00a0 \u00a0 \u00a0 JsonFormat.parser().merge(jsonModelParameters.toString(), modelParametersBuilder);\u00a0 \u00a0 \u00a0 Value modelParameters = modelParametersBuilder.build();\u00a0 \u00a0 \u00a0 BigQuerySource bigquerySource =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BigQuerySource.newBuilder().setInputUri(bigquerySourceInputUri).build();\u00a0 \u00a0 \u00a0 BatchPredictionJob.InputConfig inputConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BatchPredictionJob.InputConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInstancesFormat(instancesFormat)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setBigquerySource(bigquerySource)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 BigQueryDestination bigqueryDestination =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BigQueryDestination.newBuilder().setOutputUri(bigqueryDestinationOutputUri).build();\u00a0 \u00a0 \u00a0 BatchPredictionJob.OutputConfig outputConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BatchPredictionJob.OutputConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setPredictionsFormat(predictionsFormat)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setBigqueryDestination(bigqueryDestination)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 String modelName = ModelName.of(project, location, model).toString();\u00a0 \u00a0 \u00a0 BatchPredictionJob batchPredictionJob =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BatchPredictionJob.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(displayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModel(modelName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModelParameters(modelParameters)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputConfig(inputConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setOutputConfig(outputConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 LocationName parent = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 BatchPredictionJob response = client.createBatchPredictionJob(parent, batchPredictionJob);\u00a0 \u00a0 \u00a0 System.out.format(\"response: %s\\n\", response);\u00a0 \u00a0 \u00a0 System.out.format(\"\\tName: %s\\n\", response.getName());\u00a0 \u00a0 }\u00a0 }}\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_batch_prediction_job_bigquery_sample.py) \n```\ndef create_batch_prediction_job_bigquery_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 model_resource_name: str,\u00a0 \u00a0 job_display_name: str,\u00a0 \u00a0 bigquery_source: str,\u00a0 \u00a0 bigquery_destination_prefix: str,\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 my_model = aiplatform.Model(model_resource_name)\u00a0 \u00a0 batch_prediction_job = my_model.batch_predict(\u00a0 \u00a0 \u00a0 \u00a0 job_display_name=job_display_name,\u00a0 \u00a0 \u00a0 \u00a0 bigquery_source=bigquery_source,\u00a0 \u00a0 \u00a0 \u00a0 bigquery_destination_prefix=bigquery_destination_prefix,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 batch_prediction_job.wait()\u00a0 \u00a0 print(batch_prediction_job.display_name)\u00a0 \u00a0 print(batch_prediction_job.resource_name)\u00a0 \u00a0 print(batch_prediction_job.state)\u00a0 \u00a0 return batch_prediction_job\n```\nYou use the [batchPredictionJobs.create](/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs/create) method to request a batch prediction.\nBefore using any of the request data, make the following replacements:- : Region where Model is stored and batch prediction job is executed. For example,`us-central1`.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) \n- : Display name for the batch job\n- : The ID for the model to use for making predictions\n- : Paths (URIs) to the Cloud Storage buckets containing the training data.  There can be more than one. Each URI has the form:```\ngs://bucketName/pathToFileName\n```\n- : Path to a Cloud Storage destination where the  predictions will be written. Vertex AI writes batch predictions to a timestamped  subdirectory of this path. Set this value to a string with the following format:```\ngs://bucketName/pathToOutputDirectory\n```\n- : Default value is. Set toto enable  feature attributions. This option is available only if your output destination is JSONL.  Feature attributions are not supported for CSV on Cloud Storage. To learn more, see [Feature attributions for forecasting](/vertex-ai/docs/tabular-data/forecasting-explanations) .\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/batchPredictionJobs\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"BATCH_JOB_NAME\",\n \"model\": \"projects/PROJECT_ID/locations/LOCATION_ID/models/MODEL_ID\",\n \"inputConfig\": {\n \"instancesFormat\": \"csv\",\n \"gcsSource\": {\n  \"uris\": [  URI1,...\n  ]\n },\n },\n \"outputConfig\": {\n \"predictionsFormat\": \"csv\",\n \"gcsDestination\": {\n  \"outputUriPrefix\": \"OUTPUT_URI_PREFIX\"\n }\n },\n \"generate_explanation\": GENERATE_EXPLANATION\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/batchPredictionJobs\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/batchPredictionJobs\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_ID/locations/LOCATION_ID/batchPredictionJobs/67890\",\n \"displayName\": \"batch_job_1 202005291958\",\n \"model\": \"projects/12345/locations/us-central1/models/5678\",\n \"state\": \"JOB_STATE_PENDING\",\n \"inputConfig\": {\n \"instancesFormat\": \"csv\",\n \"gcsSource\": {\n  \"uris\": [  \"gs://bp_bucket/reg_mode_test\"\n  ]\n }\n },\n \"outputConfig\": {\n \"predictionsFormat\": \"csv\",\n \"gcsDestination\": {\n  \"outputUriPrefix\": \"OUTPUT_URI_PREFIX\"\n }\n },\n \"dedicatedResources\": {\n \"machineSpec\": {\n  \"machineType\": \"n1-standard-32\",\n  \"acceleratorCount\": \"0\"\n },\n \"startingReplicaCount\": 2,\n \"maxReplicaCount\": 6\n }\n \"outputInfo\": {\n \"gcsOutputDataset\": \"OUTPUT_URI_PREFIX/prediction-batch_job_1 202005291958-2020-09-30T02:58:44.341643Z\"\n }\n \"state\": \"JOB_STATE_PENDING\",\n \"createTime\": \"2020-09-30T02:58:44.341643Z\",\n \"updateTime\": \"2020-09-30T02:58:44.341643Z\",\n}\n```\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_batch_prediction_job_sample.py) \n```\ndef create_batch_prediction_job_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 model_resource_name: str,\u00a0 \u00a0 job_display_name: str,\u00a0 \u00a0 gcs_source: Union[str, Sequence[str]],\u00a0 \u00a0 gcs_destination: str,\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 my_model = aiplatform.Model(model_resource_name)\u00a0 \u00a0 batch_prediction_job = my_model.batch_predict(\u00a0 \u00a0 \u00a0 \u00a0 job_display_name=job_display_name,\u00a0 \u00a0 \u00a0 \u00a0 gcs_source=gcs_source,\u00a0 \u00a0 \u00a0 \u00a0 gcs_destination_prefix=gcs_destination,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 batch_prediction_job.wait()\u00a0 \u00a0 print(batch_prediction_job.display_name)\u00a0 \u00a0 print(batch_prediction_job.resource_name)\u00a0 \u00a0 print(batch_prediction_job.state)\u00a0 \u00a0 return batch_prediction_job\n```\n## Retrieve batch prediction results\nVertex AI sends the output of batch predictions to the destination that you specified, which can be either BigQuery or Cloud Storage.\nCloud Storage output for feature attributions is currently not supported.\n### Output dataset\nIf you are using BigQuery, the output of batch prediction is stored in an output dataset. If you had provided a dataset to Vertex AI, the name of the dataset ( ) is the name you had provided earlier. If you did not provide an output dataset, Vertex AI created one for you. You can find its name ( ) with the following steps:- In the Google Cloud console, go to the Vertex AI **Batch predictions** page. [Go to the Batch predictions page](https://console.cloud.google.com/vertex-ai/batch-predictions) \n- Select the prediction you created.\n- The output dataset is given in **Export location** . The dataset name is  formatted as follows:`prediction_` `` `_` ``### Output tables\nThe output dataset contains one or more of the following three output tables:- Predictions tableThis table contains a row for every row in your input data where a  prediction was requested (that is, where = null).  For example, if your input included 14 null entries for the  target column (such as sales for the next 14 days), your prediction request  returns 14 rows, the sales number for each day. If your prediction request  exceeds the model's forecast horizon, Vertex AI returns only  predictions up to the forecast horizon.\n- Errors validation tableThis table contains a row for each non-critical error encountered  during the aggregation phase that takes place prior to batch prediction.  Each non-critical error corresponds with a row in the input data that  Vertex AI could not return a forecast for.\n- Errors tableThis table contains a row for each non-critical error encountered  during batch prediction. Each non-critical error corresponds with a row  in the input data that Vertex AI could not return a forecast for.\nThe name of the table ( ) is formed by  appending `predictions_` with the timestamp of when the batch prediction  job started: `predictions_` `` \nTo retrieve the predictions table:- In the console, go to the BigQuery page. [Go to BigQuery](https://console.cloud.google.com/bigquery) \n- Run the following query:```\nSELECT * FROM BQ_DATASET_NAME.BQ_PREDICTIONS_TABLE_NAME\n  \n```\nVertex AI stores predictions in the `predicted_` `` `.value` column.\nIf you [trained a model with Temporal Fusion Transformer (TFT)](/vertex-ai/docs/tabular-data/forecasting-parameters#training-methods) , you can find TFT interpretability output in the `predicted_` `` `.tft_feature_importance` column.\nThis column is further divided into the following:- `context_columns`: Forecasting features whose [context window](/vertex-ai/docs/tabular-data/forecasting-parameters#forecast-window) values serve  as inputs to the TFT Long Short-Term Memory (LSTM) Encoder.\n- `context_weights`: The feature importance weights associated with each of the`context_columns`for the predicted instance.\n- `horizon_columns`: Forecasting features whose [forecast horizon](/vertex-ai/docs/tabular-data/forecasting-parameters#forecast-window) values serve as inputs to the TFT Long Short-Term Memory (LSTM) Decoder.\n- `horizon_weights`: The feature importance weights associated with each of the`horizon_columns`for the predicted instance.\n- `attribute_columns`: Forecasting features which are [time-invariant](/vertex-ai/docs/tabular-data/forecasting-parameters#feature-type) .\n- `attribute_weights`: The weights associated with each of the`attribute_columns`.\nIf your model is [ optimized for quantile loss](/vertex-ai/docs/tabular-data/forecasting-parameters#optimization-objectives) and your set of quantiles includes the median, `predicted_` `` `.value` is the prediction value at the  median. Otherwise, `predicted_` `` `.value` is the  prediction value at the lowest quantile in the set. For example, if your set of quantiles  is `[0.1, 0.5, 0.9]` , `value` is the prediction for quantile `0.5` .  If your set of quantiles is `[0.1, 0.9]` , `value` is the prediction for  quantile `0.1` .\nAdditionally, Vertex AI stores quantile values and predictions in the following columns:- `predicted_` `` `.quantile_values`: The values of the  quantiles, which are set during model training. For example, these can be`0.1`,`0.5`, and`0.9`.\n- `predicted_` `` `.quantile_predictions`: The  prediction values associated with the quantile values.\nIf your model uses probabilistic inference, `predicted_` `` `.value` contains the minimizer of the  optimization objective. For example, if your optimization objective is `minimize-rmse` , `predicted_` `` `.value` contains the mean value. If it  is `minimize-mae` , `predicted_` `` `.value` contains the median value.\nIf your model uses probabilistic inference with quantiles, Vertex AI stores quantile  values and predictions in the following columns:- `predicted_` `` `.quantile_values`: The values of the  quantiles, which are set during model training. For example, these can be`0.1`,`0.5`, and`0.9`.\n- `predicted_` `` `.quantile_predictions`: The prediction  values associated with the quantile values.\nIf you enabled feature attributions, you can find them in the predictions table as well. To access attributions for a feature , run the following query:\n```\nSELECT explanation.attributions[OFFSET(0)].featureAttributions.BQ_FEATURE_NAME FROM BQ_DATASET_NAME.BQ_PREDICTIONS_TABLE_NAME\n \n```\nTo learn more, see [Feature attributions for forecasting](/vertex-ai/docs/tabular-data/forecasting-explanations) .\nThe name of the table ( )  is formed by appending `errors_validation` with the timestamp of when the  batch prediction job started: `errors_validation_` `` \nTo retrieve the errors validation table:\n- In the console, go to the BigQuery page. [Go to BigQuery](https://console.cloud.google.com/bigquery) \n- Run the following query:```\nSELECT * FROM BQ_DATASET_NAME.BQ_ERRORS_VALIDATION_TABLE_NAME\n  \n```\nThe error message is stored in the following column:\n- errors_\nThe name of the table ( ) is formed by  appending `errors_` with the timestamp of when the batch prediction job  started: `errors_` `` \nTo retrieve the errors validation table:\n- In the console, go to the BigQuery page. [Go to BigQuery](https://console.cloud.google.com/bigquery) \n- Run the following query:```\nSELECT * FROM BQ_DATASET_NAME.BQ_ERRORS_TABLE_NAME\n  \n```\nThe errors are stored in the following columns:\n- errors_.code\n- errors_.messageIf you specified Cloud Storage as your output destination, the  results of your batch prediction request are returned as CSV objects in a new  folder in the bucket you specified. The name of the folder is the name of your  model, prepended with \"prediction-\" and appended with the timestamp of when  the batch prediction job started. You can find the Cloud Storage  folder name in the **Batch predictions** tab for your model.\nThe Cloud Storage folder contains two kinds of objects:\n- **Prediction objects** The prediction objects are named `predictions_1.csv`, `predictions_2.csv`,   and so on. They contain a header row with the column names, and a row for   every forecast returned. The number of prediction values depends on your prediction input and   forecast horizon. For example, if your input included 14 null entries for the   target column (such as sales for the next 14 days), your prediction request   returns 14 rows, the sales number for each day. If your prediction request   exceeds the model's forecast horizon, Vertex AI returns only   predictions up to the forecast horizon.The forecast values are returned in a column   named `predicted_ `. For quantile   forecasts, the output column contains the quantile predictions and   quantile values in the JSON format.\n- **Error objects** The error objects are named `errors_1.csv`, `errors_2.csv`, and so on.   They contain a header row, and a row for every row in your input data that   Vertex AI could not return a forecast (for example, if a   non-nullable feature was null) for.\nNote: If the results are large, it is split into multiple objects.\n## Sample feature attribution queries in BigQuery\n### Example 1: Determine attributions for a single prediction\nConsider the following question:\nThe corresponding query is as follows:\n```\nSELECT\n * EXCEPT(explanation, predicted_sales),\n ROUND(predicted_sales.value, 2) AS predicted_sales,\n ROUND(\n explanation.attributions[OFFSET(0)].featureAttributions.advertisement,\n 2\n ) AS attribution_advertisement\nFROM\n `project.dataset.predictions`\nWHERE\n product = 'product_0'\n AND store = 'store_0'\n AND date = '2019-11-24'\n```### Example 2: Determine global feature importance\nConsider the following question:\nYou can manually compute global feature importance by aggregating the local feature importance attributions. The corresponding query is as follows:\n```\nWITH\n/*\n* Aggregate from (id, date) level attributions to global feature importance.\n*/\nattributions_aggregated AS (\n SELECT\n SUM(ABS(attributions.featureAttributions.date)) AS date,\n SUM(ABS(attributions.featureAttributions.advertisement)) AS advertisement,\n SUM(ABS(attributions.featureAttributions.holiday)) AS holiday,\n SUM(ABS(attributions.featureAttributions.sales)) AS sales,\n SUM(ABS(attributions.featureAttributions.store)) AS store,\n SUM(ABS(attributions.featureAttributions.product)) AS product,\n FROM\n project.dataset.predictions,\n UNNEST(explanation.attributions) AS attributions\n),\n/*\n* Calculate the normalization constant for global feature importance.\n*/\nattributions_aggregated_with_total AS (\n SELECT\n *,\n date + advertisement + holiday + sales + store + product AS total\n FROM\n attributions_aggregated\n)\n/*\n* Calculate the normalized global feature importance.\n*/\nSELECT\n ROUND(date / total, 2) AS date,\n ROUND(advertisement / total, 2) AS advertisement,\n ROUND(holiday / total, 2) AS holiday,\n ROUND(sales / total, 2) AS sales,\n ROUND(store / total, 2) AS store,\n ROUND(product / total, 2) AS product,\nFROM\n attributions_aggregated_with_total\n```\n## Example batch prediction output in BigQuery\nIn an example dataset of liquor sales, there are four stores in the city of \"Ida Grove\": \"Ida Grove Food Pride\", \"Discount Liquors of Ida Grove\", \"Casey's General Store #3757\", and \"Brew Ida Grove\". `store_name` is the `series identifier` and three of the four stores request predictions for the target column `sale_dollars` . A validation error is generated because no forecast was requested for \"Discount Liquors of Ida Grove\".\nThe following is an extract from the input dataset used for prediction:\nThe following is an extract from the prediction results:\nThe following is an extract from the validation errors:\n### Example batch prediction output for a quantile-loss optimized model\nThe following example is batch prediction output for a quantile-loss optimized model. In this scenario, the forecast model predicted sales for the next 14 days for each store.\nThe quantile values are given in the `predicted_Sales.quantile_values` column. In this example, the model predicted values at the `0.1` , `0.5` , and `0.9` quantiles.\nThe prediction values are given in the `predicted_Sales.quantile_predictions` column. This is an array of sales values, which map to the quantile values in the `predicted_Sales.quantile_values` column. In the first row, we see that the probability of the sales value being lower than `4484.04` is 10%. The probability of the sales value being lower than `5615.64` is 50%. The probability of the sales value being lower than `6853.29` is 90%. The prediction for the first row, represented as a single value, is `5615.64` .", "guide": "Vertex AI"}