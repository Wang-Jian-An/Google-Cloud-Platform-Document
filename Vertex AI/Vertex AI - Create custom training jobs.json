{"title": "Vertex AI - Create custom training jobs", "url": "https://cloud.google.com/vertex-ai/docs/training/create-custom-job", "abstract": "# Vertex AI - Create custom training jobs\nCustom training jobs ( `CustomJob` resources in the Vertex AI API) are the basic way to run your custom machine learning (ML) training code in Vertex AI.\n", "content": "## Before you submit a job\nBefore you create a `CustomJob` in Vertex AI, you must create a Python training application or a custom container image to define the training code and dependencies you want to run on Vertex AI.\nWe recommend that you use the Google Cloud CLI's feature, described in a [later section of this guide](#create) , to create a Docker container image from code on your local machine, push this container image to Container Registry, and create a `CustomJob` , all with a single command.\nOtherwise, you must manually create a [Python training application](/vertex-ai/docs/training/create-python-pre-built-container) or a [custom container image](/vertex-ai/docs/training/create-custom-container) .\nIf you're not sure which of these options to choose, refer to the [training code requirements](/vertex-ai/docs/training/code-requirements) to learn more.\n## What a custom job includes\nWhen you create a , you specify settings that Vertex AI needs to run your training code, including:\n- One worker pool for single-node training ( [WorkerPoolSpec](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#workerpoolspec) ), or multiple worker pools for distributed training\n- Optional settings for configuring job scheduling ( [Scheduling](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#scheduling) ), [settingcertain environment variables for your trainingcode](/vertex-ai/docs/training/code-requirements#environment-variables) , [using a customservice account](/vertex-ai/docs/general/custom-service-account) , and [using VPC NetworkPeering](/vertex-ai/docs/general/vpc-peering) \nWithin the worker pool(s), you can specify the following settings:\n- [Machine types and accelerators](/vertex-ai/docs/training/configure-compute) \n- [Configuration of what type of training code the worker poolruns](/vertex-ai/docs/training/configure-container-settings) : either a Python training application ( [PythonPackageSpec](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#pythonpackagespec) ) or a custom container ( [ContainerSpec](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#containerspec) )\nYou can also configure custom jobs to run on a persistent resource instead of creating new compute resources during job startup. To learn more about persistent resource, see [Overview of persistent resource](/vertex-ai/docs/training/persistent-resource-overview) .\n## Configure distributed training\nYou can configure a `CustomJob` for distributed training by specifying multiple worker pools.\nMost examples on this page show single-replica training jobs with one worker pool. To modify them for distributed training:\n- Use your first worker pool to configure your primary replica, and set the replica count to 1.\n- [Add more worker pools](/vertex-ai/docs/training/distributed-training#additional-worker-pools) to configure worker replicas, parameter server replicas, or evaluator replicas, if your machine learning framework supports these additional cluster tasks for distributed training.\nLearn more about [using distributed training](/vertex-ai/docs/training/distributed-training) .\n## Create a CustomJob\nTo create a `CustomJob` , follow the instructions in one of the following tabs, depending on what tool you want to use. If you use the gcloud CLI, you can use a single command to autopackage training code on your local machine into a Docker container image, push the container image to Container Registry, and create a `CustomJob` . Other options assume you have [already created a Python training application or custom container image](#before) .\nThe following examples use the [gcloud ai custom-jobs createcommand](/sdk/gcloud/reference/ai/custom-jobs/create) .\nIf your training code is on your local computer, we recommend that you follow the [With autopackaging](#autopackaging) section. Alternatively, if you have [already created a Python training application or custom container image](#before) , then skip ahead to the [Without autopackaging](#without-autopackaging) section.\n### With autopackagingIf you have training code on your local computer, you can use a single command to do the following:- Build a custom Docker image based on your code.\n- Push the image to Container Registry.\n- Start a`CustomJob`based on the image.\nThe result is similar to creating a `CustomJob` using any other custom container; you can use this version of the command if it is convenient for your workflow.Since this version of the command builds and pushes a Docker image, you must perform the following configuration on your local computer:- [Install Docker Engine.](https://docs.docker.com/engine/install/) \n- If you are using Linux, [configure Docker so you can run it withoutsudo](https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user) .\n- Enable the Container Registry API. [Enable the API](https://console.cloud.google.com/flows/enableapi?apiid=containerregistry.googleapis.com) \n- [Configure authentication for Docker](/container-registry/docs/quickstart#auth) , so that you can push Docker images to Container Registry:```\ngcloud auth configure-docker\n```\nThe following command builds a Docker image based on a prebuilt training container image and your local Python code, pushes the image to Container Registry, and creates a `CustomJob` .\n```\ngcloud ai custom-jobs create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=JOB_NAME \\\u00a0 --worker-pool-spec=machine-type=MACHINE_TYPE,replica-count=REPLICA_COUNT,executor-image-uri=EXECUTOR_IMAGE_URI,local-package-path=WORKING_DIRECTORY,script=SCRIPT_PATH\n```\nReplace the following:- : The region where the container or Python package will be run.\n- : Required. A display name for the `CustomJob` .\n- : The type of the machine. Refer to [available machine types fortraining](/vertex-ai/docs/training/configure-compute) .\n- : The number of worker replicas to use. In most cases, set this to `1` for your [first worker pool](#configure_distributed_training) .\n- : The URI of the container image that runs the provided code. Refer to the [available prebuilt containers fortraining](/vertex-ai/docs/training/pre-built-containers) .This image acts as the base image for the new Docker image that you are building with this command.\n- : A directory in your local file system containing the entry point script that runs your training code (see the following list item).You can use the parent directory of the script, or a higher-level directory. You might want to use a higher-level directory in order to specify a fully-qualified Python module name (see the following list item). You might also want to use a higher-level directory if it contains a `requirements.txt` or `setup.py` file. To learn more, see [Install dependencies](#install-dependencies) .Note that even if you specify a higher-level directory, this command only copies the parent directory of your entry point script to the Docker image.\n- : The path, relative to on your local file system, to the script that is the entry point for your training code. This can be a Python script (ending in `.py` ) or a Bash script.For example, if you want to run `/hello-world/trainer/task.py` and is `/hello-world` , then use `trainer/task.py` for this value.You can optionally replace `script=` `` with `python-module=` `` to specify the name of a Python module in to run as the entry point for training. For example, instead of `script=trainer/task.py` , you might specify `python-module=trainer.task` .In this case, the resulting Docker container [loads your code as a module](https://docs.python.org/3/using/cmdline.html#cmdoption-m) rather than as a script. You likely want to use this option if your entry point script imports other Python modules in .\nWhen using autopackaging, you can install Python dependencies in your container in the same ways that are available when you use the gcloud CLI's `local-run` command. To learn about the various ways to install Python dependencies, read the [Install dependencies](/vertex-ai/docs/training/containerize-run-code-local#dependencies) section of the guide to the `local-run` command.\nThe syntax for specifying dependencies differs slightly when you use autopackaging compared to when you use the `local-run` command. Instead of using command-line flags to specify dependencies, you must use options in the value of the `--worker-pool-spec` flag. Additionally, values these options must be separated by semicolons rather than commas. Specifically, the syntax:- Instead of the `local-run` command's `--local-package-path` flag, use the `local-package-path` option in the value of the `--worker-pool-spec` flag. If the working directory that you specify with this option contains a `requirements.txt` or `setup.py` file, autopackaging installs dependencies based on this file.The preceding example demonstrates this syntax.\n- (Optional) Instead of the `--requirements` flag, use the `requirements` option in the value of the `--worker-pool-spec` flag. Instead of separating PyPI dependencies with commas, use semicolons.\n- (Optional) Instead of the `--extra-packages` flag, use the `extra-packages` option in the value of the `--worker-pool-spec` flag. Instead of separating local dependencies with commas, use semicolons.\n- (Optional) Instead of the `--extra-dirs` flag, use the `extra-dirs` option in the value of the `--worker-pool-spec` flag. Instead of separating directory paths with commas, use semicolons.\nThe following example shows how you might install dependencies using all the optional techniques. (You can specify any subset of them.) To demonstrate the semicolon syntax, the example specifies two values for each option. To reduce the length of the example, other `--worker-pool-spec` options are replaced with `[...]` .\n```\ngcloud ai custom-jobs create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=JOB_NAME \\\u00a0 --worker-pool-spec=[...],requirements=PYPI_DEP_1;PYPI_DEP_2,extra-packages=LOCAL_DEP_1;LOCAL_DEP_2,extra-dirs=EXTRA_DIR_1;EXTRA_DIR_2\n```\nTo learn about appropriate values for these placeholders, see \"Install dependencies\" in the guide to the `local-run` command.\n### Without autopackagingIf you don't use autopackaging, you can create a `CustomJob` with a command similar to one of the following. Depending on whether you have [created a Python training application or a custom container image](#before) , choose one of the following tabs:\n```\ngcloud ai custom-jobs create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=JOB_NAME \\\u00a0 --python-package-uris=PYTHON_PACKAGE_URIS \\\u00a0 --worker-pool-spec=machine-type=MACHINE_TYPE,replica-count=REPLICA_COUNT,executor-image-uri=EXECUTOR_IMAGE_URI,python-module=PYTHON_MODULE\n```\nReplace the following:- : The region where the container or Python package will be run.\n- : Required. A display name for the`CustomJob`.\n- : Comma-separated list of Cloud Storage URIs specifying the Python package files which are the training program and its dependent packages. The maximum number of package URIs is 100.\n- : The type of the machine. Refer to [available machine types fortraining](/vertex-ai/docs/training/configure-compute) .\n- : The number of worker replicas to use. In most cases, set this to`1`for your [first worker pool](#configure_distributed_training) .\n- : The URI of the container image that runs the provided code. Refer to the [available prebuilt containers fortraining](/vertex-ai/docs/training/pre-built-containers) .\n- : The Python module name to run after installing the packages.\n```\ngcloud ai custom-jobs create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=JOB_NAME \\\u00a0 --worker-pool-spec=machine-type=MACHINE_TYPE,replica-count=REPLICA_COUNT,container-image-uri=CUSTOM_CONTAINER_IMAGE_URI\n```\nReplace the following:- : The region where the container or Python package will be run.\n- : Required. A display name for the`CustomJob`.\n- : The type of the machine. Refer to [available machine types fortraining](/vertex-ai/docs/training/configure-compute) .\n- : The number of worker replicas to use. In most cases, set this to`1`for your [first worker pool](#configure_distributed_training) .\n- : The URI of a container image in Artifact Registry, Container Registry, or Docker Hub that is to be run on each worker replica.### Distributed trainingTo perform [distributed training](#configure_distributed_training) , specify the `--worker-pool-spec` flag multiple times, once for each worker pool.\nIf you are using autopackaging, then you must only specify `local-package-path` , `script` , and other options related to autopackaging in the first worker pool. Omit fields related to your training code in subsequent worker pools, which will all use the same training container built by autopackaging.\nFor example, the following command adapts an earlier autopackaging example to use a second worker pool:\n```\ngcloud ai custom-jobs create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=JOB_NAME \\\u00a0 --worker-pool-spec=machine-type=MACHINE_TYPE,replica-count=REPLICA_COUNT,executor-image-uri=EXECUTOR_IMAGE_URI,local-package-path=WORKING_DIRECTORY,script=SCRIPT_PATH \\\u00a0 --worker-pool-spec=machine-type=SECOND_POOL_MACHINE_TYPE,replica-count=SECOND_POOL_REPLICA_COUNT\n```\nIf you aren't using autopackaging, then specify each worker pool completely and independently; do not omit any fields.\nThe following commands adapt earlier examples to use a second worker pool:\n```\ngcloud ai custom-jobs create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=JOB_NAME \\\u00a0 --python-package-uris=PYTHON_PACKAGE_URIS \\\u00a0 --worker-pool-spec=machine-type=MACHINE_TYPE,replica-count=REPLICA_COUNT,executor-image-uri=EXECUTOR_IMAGE_URI,python-module=PYTHON_MODULE \\\u00a0 --worker-pool-spec=machine-type=SECOND_POOL_MACHINE_TYPE,replica-count=SECOND_POOL_REPLICA_COUNT,executor-image-uri=SECOND_POOL_EXECUTOR_IMAGE_URI,python-module=SECOND_POOL_PYTHON_MODULE\n```\n **Note:** In this case, each worker pool uses the same Python package URIs.\n```\ngcloud ai custom-jobs create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=JOB_NAME \\\u00a0 --worker-pool-spec=machine-type=MACHINE_TYPE,replica-count=REPLICA_COUNT,container-image-uri=CUSTOM_CONTAINER_IMAGE_URI \\\u00a0 --worker-pool-spec=machine-type=SECOND_POOL_MACHINE_TYPE,replica-count=SECOND_POOL_REPLICA_COUNT,container-image-uri=SECOND_POOL_CUSTOM_CONTAINER_IMAGE_URI\n```\n### Advanced configurationIf you want to specify configuration options that are not available in the preceding examples, you can use the `--config` flag to specify the path to a `config.yaml` file in your local environment that contains the fields of [CustomJobSpec](/vertex-ai/docs/reference/rest/v1/CustomJobSpec) . For example:\n```\ngcloud ai custom-jobs create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=JOB_NAME \\\u00a0 --config=config.yaml\n```\nSee an [example of a config.yamlfile](/sdk/gcloud/reference/ai/custom-jobs/create#--config) .In the Google Cloud console, you can't create a `CustomJob` resource directly. However, you can create a `TrainingPipeline` resource that creates a `CustomJob` .\nThe following instructions describe how to create a `TrainingPipeline` that creates a `CustomJob` and doesn't do anything else. If you want to use additional `TrainingPipeline` features, like training with a managed dataset or creating a `Model` resource at the end of training, read [Creating trainingpipelines](/vertex-ai/docs/training/create-training-pipeline) .- In the Google Cloud console, in the Vertex AI section, go to the **Training pipelines** page. [Go to Training pipelines](https://console.cloud.google.com/vertex-ai/training/training-pipelines) \n- Click **add_boxCreate** to open the **Train new model** pane. **Note:** You can type [model.new](https://model.new) into a browser to go directly to the model creation page.\n- On the **Training method** step, specify the following settings:- In the **Dataset** drop-down list, select **No manageddataset.** \n- Select **Custom training (advanced)** .\nClick **Continue** .\n- On the **Model details** step, choose **Train new model** or **Train new version** . If you select train new model, enter a name of your choice, , for your model. Click **Continue** .\n- On the **Training container** step, specify the following settings:- Select [whether to use a Prebuilt container or a Customcontainer](/vertex-ai/docs/training/custom-training-methods#pre-built-custom) for training.\n- Depending on your choice, do one of the following:- If you want to use a prebuilt container for training, then provide Vertex AI with information it needs to use the training package that you have uploaded to Cloud Storage:- Use the **Model framework** and **Model framework version** drop-down lists to specify the [prebuiltcontainer](/vertex-ai/docs/training/pre-built-containers) that you want to use.\n- In the **Package location** field, specify the Cloud Storage URI of the [Python training application thatyou have created anduploaded](/vertex-ai/docs/training/create-python-pre-built-container) . This file usually ends with `.tar.gz` .\n- In the **Python module** field, enter the [module name of yourtraining application's entrypoint](/vertex-ai/docs/training/create-python-pre-built-container#python-modules) .\n- If you want to use a [custom container fortraining](/vertex-ai/docs/training/create-custom-container) , then in the **Container image** field, specify the Artifact Registry or Docker Hub URI of your container image.\n- In the **Model output directory** field, you may specify the Cloud Storage URI of a directory in a bucket that you have access to. The directory does not need to exist yet.This value gets passed to Vertex AI in the [baseOutputDirectory APIfield](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#FIELDS.base_output_directory) , which sets [several environment variables that your training application can accesswhen it runs](/vertex-ai/docs/training/code-requirements#environment-variables) .\n- **Optional** : In the **Arguments** field, you can specify arguments for Vertex AI to use when it starts running your training code. The maximum length for all arguments combined is 100,000 characters. The behavior of these arguments differs depending on what type of container you are using:- If you are using a prebuilt container, then Vertex AI passes the arguments as command-line flags to your **Python module** .\n- If you are using a custom container, then Vertex AI [overrides your container's CMD instruction with thearguments](/vertex-ai/docs/training/configure-container-settings#configure) .Click **Continue** .\n- On the **Hyperparameter tuning** step, make sure that the **Enablehyperparameter tuning** checkbox is not selected. Click **Continue** .\n- On the **Compute and pricing** step, specify the following settings:- In the **Region** drop-down list, select a \" [region that supports customtraining](/vertex-ai/docs/general/locations) \"\n- In the **Worker pool 0** section, specify [computeresources](/vertex-ai/docs/training/configure-compute) to use for training.If you specify accelerators, [make sure the type of accelerator that youchoose is available in your selectedregion](/vertex-ai/docs/general/locations#region_considerations) .If you want to perform [distributedtraining](/vertex-ai/docs/training/distributed-training) , then click **Add moreworker pools** and specify an additional set of compute resources for each additional worker pool that you want.\nClick **Continue** .\n- On the **Prediction container** step, select **No predictioncontainer** .\n- Click **Start training** to start the custom training pipeline.\nBefore using any of the request data, make the following replacements:- : The region where the container or Python package will be run.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : Required. A display name for the`CustomJob`.\n- Define the custom training job:- : The type of the machine. Refer to [available machine types fortraining](/vertex-ai/docs/training/configure-compute) .\n- : (Optional.) The type of accelerator to attach to the job.\n- : (Optional.) The number of accelerators to attach to the job.\n- : (Optional.) The type of the boot disk to use for the job, either`pd-standard`(default) or`pd-ssd`. [Learn more about disk types.](/compute/docs/disks#disk-types) \n- : (Optional.) The size in GB of the boot disk to use for the job. The   default value is 100.\n- : The number of worker replicas to use. In most cases, set this to`1`for your [first worker pool](#configure_distributed_training) .\n- If your training application runs in a custom container, specify the following:- : The URI of a container image in Artifact Registry, Container Registry, or Docker Hub that is to be run on each worker replica.\n- : (Optional.) The command to be invoked when the    container is started. This command overrides the container's default entrypoint.\n- : (Optional.) The arguments to be passed when    starting the container.\n- If your training application is a Python package that runs in a prebuilt container,   specify the following:- : The URI of the container image that runs the provided code. Refer to the [available prebuilt containers fortraining](/vertex-ai/docs/training/pre-built-containers) .\n- : Comma-separated list of Cloud Storage URIs specifying the Python package files which are the training program and its dependent packages. The maximum number of package URIs is 100.\n- : The Python module name to run after installing the packages.\n- : (Optional.) Command-line arguments to be passed to    the Python module.\n- Learn about [job scheduling options](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#Scheduling) .\n- : (Optional.) The maximum running time for the job.\n- Specify theandfor any labels that you want to  apply to this custom job.\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/customJobs\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"JOB_NAME\",\n \"jobSpec\": {\n \"workerPoolSpecs\": [  {\n  \"machineSpec\": {\n   \"machineType\": MACHINE_TYPE,\n   \"acceleratorType\": ACCELERATOR_TYPE,\n   \"acceleratorCount\": ACCELERATOR_COUNT\n  },\n  \"replicaCount\": REPLICA_COUNT,\n  \"diskSpec\": {\n   \"bootDiskType\": DISK_TYPE,\n   \"bootDiskSizeGb\": DISK_SIZE\n  },\n  // Union field task can be only one of the following:\n  \"containerSpec\": {\n   \"imageUri\": CUSTOM_CONTAINER_IMAGE_URI,\n   \"command\": [   CUSTOM_CONTAINER_COMMAND\n   ],\n   \"args\": [   CUSTOM_CONTAINER_ARGS\n   ]\n  },\n  \"pythonPackageSpec\": {\n   \"executorImageUri\": EXECUTOR_IMAGE_URI,\n   \"packageUris\": [   PYTHON_PACKAGE_URIS\n   ],\n   \"pythonModule\": PYTHON_MODULE,\n   \"args\": [   PYTHON_PACKAGE_ARGS\n   ]\n  }\n  // End of list of possible types for union field task.\n  }\n  // Specify one workerPoolSpec for single replica training, or multiple workerPoolSpecs\n  // for distributed training.\n ],\n \"scheduling\": {\n  \"timeout\": TIMEOUT\n }\n },\n \"labels\": {\n LABEL_NAME_1\": LABEL_VALUE_1,\n LABEL_NAME_2\": LABEL_VALUE_2\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/customJobs\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/customJobs\" | Select-Object -Expand Content\n```\nThe response contains information about specifications as well as the .Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateCustomJobSample.java) \n```\nimport com.google.cloud.aiplatform.v1.AcceleratorType;import com.google.cloud.aiplatform.v1.ContainerSpec;import com.google.cloud.aiplatform.v1.CustomJob;import com.google.cloud.aiplatform.v1.CustomJobSpec;import com.google.cloud.aiplatform.v1.JobServiceClient;import com.google.cloud.aiplatform.v1.JobServiceSettings;import com.google.cloud.aiplatform.v1.LocationName;import com.google.cloud.aiplatform.v1.MachineSpec;import com.google.cloud.aiplatform.v1.WorkerPoolSpec;import java.io.IOException;// Create a custom job to run machine learning training code in Vertex AIpublic class CreateCustomJobSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"PROJECT\";\u00a0 \u00a0 String displayName = \"DISPLAY_NAME\";\u00a0 \u00a0 // Vertex AI runs your training application in a Docker container image. A Docker container\u00a0 \u00a0 // image is a self-contained software package that includes code and all dependencies. Learn\u00a0 \u00a0 // more about preparing your training application at\u00a0 \u00a0 // https://cloud.google.com/vertex-ai/docs/training/overview#prepare_your_training_application\u00a0 \u00a0 String containerImageUri = \"CONTAINER_IMAGE_URI\";\u00a0 \u00a0 createCustomJobSample(project, displayName, containerImageUri);\u00a0 }\u00a0 static void createCustomJobSample(String project, String displayName, String containerImageUri)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 JobServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 JobServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests.\u00a0 \u00a0 try (JobServiceClient client = JobServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 MachineSpec machineSpec =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MachineSpec.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMachineType(\"n1-standard-4\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAcceleratorType(AcceleratorType.NVIDIA_TESLA_K80)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAcceleratorCount(1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 ContainerSpec containerSpec =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ContainerSpec.newBuilder().setImageUri(containerImageUri).build();\u00a0 \u00a0 \u00a0 WorkerPoolSpec workerPoolSpec =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 WorkerPoolSpec.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMachineSpec(machineSpec)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setReplicaCount(1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setContainerSpec(containerSpec)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 CustomJobSpec customJobSpecJobSpec =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CustomJobSpec.newBuilder().addWorkerPoolSpecs(workerPoolSpec).build();\u00a0 \u00a0 \u00a0 CustomJob customJob =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CustomJob.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(displayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setJobSpec(customJobSpecJobSpec)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 LocationName parent = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 CustomJob response = client.createCustomJob(parent, customJob);\u00a0 \u00a0 \u00a0 System.out.format(\"response: %s\\n\", response);\u00a0 \u00a0 \u00a0 System.out.format(\"Name: %s\\n\", response.getName());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/create-custom-job.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const customJobDisplayName = 'YOUR_CUSTOM_JOB_DISPLAY_NAME';// const containerImageUri = 'YOUR_CONTAINER_IMAGE_URI';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Job Service Client libraryconst {JobServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst jobServiceClient = new JobServiceClient(clientOptions);async function createCustomJob() {\u00a0 // Configure the parent resource\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 const customJob = {\u00a0 \u00a0 displayName: customJobDisplayName,\u00a0 \u00a0 jobSpec: {\u00a0 \u00a0 \u00a0 workerPoolSpecs: [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 machineSpec: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 machineType: 'n1-standard-4',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 acceleratorType: 'NVIDIA_TESLA_K80',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 acceleratorCount: 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 replicaCount: 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containerSpec: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 imageUri: containerImageUri,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command: [],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 args: [],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 },\u00a0 };\u00a0 const request = {parent, customJob};\u00a0 // Create custom job request\u00a0 const [response] = await jobServiceClient.createCustomJob(request);\u00a0 console.log('Create custom job response:\\n', JSON.stringify(response));}createCustomJob();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_custom_job_with_experiment_autologging_sample.py) \n```\ndef create_custom_job_with_experiment_autologging_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 staging_bucket: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 script_path: str,\u00a0 \u00a0 container_uri: str,\u00a0 \u00a0 service_account: str,\u00a0 \u00a0 experiment: str,\u00a0 \u00a0 experiment_run: Optional[str] = None,) -> None:\u00a0 \u00a0 aiplatform.init(project=project, location=location, staging_bucket=staging_bucket, experiment=experiment)\u00a0 \u00a0 job = aiplatform.CustomJob.from_local_script(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name,\u00a0 \u00a0 \u00a0 \u00a0 script_path=script_path,\u00a0 \u00a0 \u00a0 \u00a0 container_uri=container_uri,\u00a0 \u00a0 \u00a0 \u00a0 enable_autolog=True,\u00a0 \u00a0 )\u00a0 \u00a0 job.run(\u00a0 \u00a0 \u00a0 \u00a0 service_account=service_account,\u00a0 \u00a0 \u00a0 \u00a0 experiment=experiment,\u00a0 \u00a0 \u00a0 \u00a0 experiment_run=experiment_run,\u00a0 \u00a0 )\n```\n## What's next\n- Learn how to pinpoint training performance bottlenecks to train models faster and cheaper using [TensorBoard Profiler](/vertex-ai/docs/training/tensorboard-profiler) .\n- See [Create training pipelines](/vertex-ai/docs/training/create-training-pipeline) to learn how to create training pipelines to run custom training applications on Vertex AI.", "guide": "Vertex AI"}