{"title": "Vertex AI - Create a Vertex AI tabular dataset", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Create a Vertex AI tabular dataset\n- `island`- The island where a species of penguin is found.\n- `culmen_length_mm`- The length of the ridge along the top of the bill of a penguin.\n- `culmen_depth_mm`- The height of the bill of a penguin.\n- `flipper_length_mm`- The length of the flipper-like wing of a penguin.\n- `body_mass_g`- The mass of the body of a penguin.\n- `sex`- The sex of the penguin.", "content": "## Download, preprocess, and split the data\nIn this section, you download the publicly available BigQuery dataset and prepare its data. To prepare the data, you do the following:\n- Convert categorical features (features described with a string instead of a number) to numeric data. For example, you convert the names of the three types of penguins to the numerical values `0` , `1` , and `2` .\n- Remove any columns in the dataset that aren't used.\n- Remove any rows that cannot be used.\n- Split the data into two distinct sets of data. Each set of data is stored in a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) object.- The `df_train` `DataFrame` contains data used to train your model.\n- the `df_for_prediction` `DataFrame` contains data used to generate predictions.After processing the data, the code maps the three categorical columns' numerical values to their string values, then prints them so that you can see what the data looks like.\nTo download and process your data, run the following code in your notebook:\n```\nimport numpy as npimport pandas as pdLABEL_COLUMN = \"species\"# Define the BigQuery source datasetBQ_SOURCE = \"bigquery-public-data.ml_datasets.penguins\"# Define NA valuesNA_VALUES = [\"NA\", \".\"]# Download a tabletable = bq_client.get_table(BQ_SOURCE)df = bq_client.list_rows(table).to_dataframe()# Drop unusable rowsdf = df.replace(to_replace=NA_VALUES, value=np.NaN).dropna()# Convert categorical columns to numericdf[\"island\"], island_values = pd.factorize(df[\"island\"])df[\"species\"], species_values = pd.factorize(df[\"species\"])df[\"sex\"], sex_values = pd.factorize(df[\"sex\"])# Split into a training and holdout datasetdf_train = df.sample(frac=0.8, random_state=100)df_for_prediction = df[~df.index.isin(df_train.index)]# Map numeric values to string valuesindex_to_island = dict(enumerate(island_values))index_to_species = dict(enumerate(species_values))index_to_sex = dict(enumerate(sex_values))# View the mapped island, species, and sex dataprint(index_to_island)print(index_to_species)print(index_to_sex)\n```\nThe following are the printed mapped values for characteristics that are not numeric:\n```\n{0: 'Dream', 1: 'Biscoe', 2: 'Torgersen'}\n{0: 'Adelie Penguin (Pygoscelis adeliae)', 1: 'Chinstrap penguin (Pygoscelis antarctica)', 2: 'Gentoo penguin (Pygoscelis papua)'}\n{0: 'FEMALE', 1: 'MALE'}\n```\nThe first three values are the islands a penguin might inhabit. The second three values are important because they map to the predictions you receive at the end of this tutorial. The third row shows the `FEMALE` sex characteristic maps to `0` and the `MALE` the sex characteristic maps to `1` .\n## Create a tabular dataset for training your model\nIn the previous step you downloaded and processed your data. In this step, you load the data stored in your `df_train` `DataFrame` into a BigQuery dataset. Then, you use the BigQuery dataset to create a Vertex AI tabular dataset. This tabular dataset is used to train your model. For more information, see [Use manageddatasets](/vertex-ai/docs/training/using-managed-datasets) .\n### Create a BigQuery dataset\nTo create your BigQuery dataset that's used to create a Vertex AI dataset, run the following code. The [create_dataset](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.client.Client#google_cloud_bigquery_client_Client_create_dataset) command returns a new BigQuery [DataSet](https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.dataset.Dataset) .\n```\n# Create a BigQuery datasetbq_dataset_id = f\"{project_id}.dataset_id_unique\"bq_dataset = bigquery.Dataset(bq_dataset_id)bq_client.create_dataset(bq_dataset, exists_ok=True)\n```\n### Create a Vertex AI tabular dataset\nTo convert your BigQuery dataset a Vertex AI tabular dataset, run the following code. You can ignore the warning about the required number of rows to train using tabular data. Because the purpose of this tutorial is to quickly show you how to get predictions, a relatively small set of data is used to show you how to generate predictions. In a real world scenario, you want at least 1000 rows in a tabular dataset. The [create_from_dataframe](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.TabularDataset#google_cloud_aiplatform_TabularDataset_create_from_dataframe) command returns a Vertex AI [TabularDataset](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.TabularDataset#google_cloud_aiplatform_TabularDataset) .\n```\n# Create a Vertex AI tabular datasetdataset = aiplatform.TabularDataset.create_from_dataframe(\u00a0 \u00a0 df_source=df_train,\u00a0 \u00a0 staging_path=f\"bq://{bq_dataset_id}.table-unique\",\u00a0 \u00a0 display_name=\"sample-penguins\",)\n```\nYou now have the Vertex AI tabular dataset used to train your model.\n## (Optional) View the public dataset in BigQuery\nIf you want to view the public data used in this tutorial, you can open it in BigQuery.\n- In **Search** in the Google Cloud, enter BigQuery, then press return.\n- In the search results, click on BigQuery\n- In the **Explorer** window, expand **bigquery-public-data** .\n- Under **bigquery-public-data** , expand **ml_datasets** , then click on **penguins** .\n- Click any of the names under **Field name** to view that field's data.", "guide": "Vertex AI"}