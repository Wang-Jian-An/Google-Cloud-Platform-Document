{"title": "Vertex AI - Prepare video training data for classification", "url": "https://cloud.google.com/vertex-ai/docs/video-data/classification/prepare-data", "abstract": "# Vertex AI - Prepare video training data for classification\nThe following sections provide information about data requirements, schema files, and the format of the data import files (JSONL & CSV) that are defined by the schema.\nAlternatively, you can import videos that have not been annotated and annotate them later using the Google Cloud console (see [Labeling using the Google Cloud console](/vertex-ai/docs/datasets/label-using-console) ).\n", "content": "## Data requirements\nThe following requirements apply to datasets used to train AutoML or custom-trained models.\n- Vertex AI supports the following video formats for training your model or requesting a prediction (annotating a video).- .MOV\n- .MPEG4\n- .MP4\n- .AVI\n- To view the video content in the web console or to annotate a video, the video must be in a format that your browser natively supports. Since not all browsers handle .MOV or .AVI content natively, the recommendation is to use either .MPEG4 or .MP4 video format.\n- Maximum file size is 50 GB (up to 3 hours in duration). Individual video files with malformed or empty timestamps in the container aren't supported.\n- The maximum number of labels in each dataset is limited to 1,000.\n- You may assign \"ML_USE\" labels to the videos in the import files. At training time, you may choose to use those labels to split the videos and their corresponding annotations into \"training\" or \"test\" sets. For video classification, note the following:- At least two different classes are required for model training. For example, \"news\" and \"MTV\", or \"game\" and \"others\".\n- Consider including a \"None_of_the_above\" class and video segments that do not match any of your defined classes.\n## Best practices for video data used to train AutoML models\nThe following practices apply to datasets used to train AutoML models.\n- The training data should be as close as possible to the data on which predictions are to be made. For example, if your use case involves blurry and low-resolution videos (such as from a security camera), your training data should be composed of blurry, low-resolution videos. In general, you should also consider providing multiple angles, resolutions, and backgrounds for your training videos.\n- Vertex AI models can't generally predict labels that humans can't assign. If a human can't be trained to assign labels by looking at the video for 1-2 seconds, the model likely can't be trained to do it either.\n- The model works best when there are at most 100 times more videos for the most common label than for the least common label. We recommend removing low frequency labels. For video classification, the recommended number of training videos per label is about 1,000. The minimum per label is 10, or 50 for advanced models. In general, it takes more examples per label to train models with multiple labels per video, and resulting scores are harder to interpret.## Schema files\n- Use the following publicly accessible schema file when creating the jsonl file for importing annotations. This schema file dictates the format of the data input files. The structure of the file follows the [OpenAPI Schema](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.2.md#schema) test. **Video classification schema file** : [gs://google-cloud-aiplatform/schema/dataset/ioformat/video_classification_io_format_1.0.0.yaml](https://storage.cloud.google.com/google-cloud-aiplatform/schema/dataset/ioformat/video_classification_io_format_1.0.0.yaml) ## Input files\nThe format of your training data for video classification are as follows.\n**Note:** When manually managing which videos are used for training or test purposes, be sure each instance of a video in the dataset has been assigned the designation. If there's a contradiction, the dataset reverts to random assignments.\nTo import your data, create either a JSONL or CSV file.\nJSON on each line: See [Classification schema (global)](/vertex-ai/docs/training-overview#video_data) file for details.```\n{\n\t\"videoGcsUri\": \"gs://bucket/filename.ext\",\n\t\"timeSegmentAnnotations\": [{\n\t\t\"displayName\": \"LABEL\",\n\t\t\"startTime\": \"start_time_of_segment\",\n\t\t\"endTime\": \"end_time_of_segment\"\n\t}],\n\t\"dataItemResourceLabels\": {\n\t\t\"aiplatform.googleapis.com/ml_use\": \"train|test\"\n\t}\n}\n``````\n{\"videoGcsUri\": \"gs://demo/video1.mp4\", \"timeSegmentAnnotations\": [{\"displayName\": \"cartwheel\", \"startTime\": \"1.0s\", \"endTime\": \"12.0s\"}], \"dataItemResourceLabels\": {\"aiplatform.googleapis.com/ml_use\": \"training\"}}\n{\"videoGcsUri\": \"gs://demo/video2.mp4\", \"timeSegmentAnnotations\": [{\"displayName\": \"swing\", \"startTime\": \"4.0s\", \"endTime\": \"9.0s\"}], \"dataItemResourceLabels\": {\"aiplatform.googleapis.com/ml_use\": \"test\"}}\n...\n```Format of a row in the CSV:\n```\n[ML_USE,]VIDEO_URI,LABEL,START,END\n```\n **List of columns** \n- `ML_USE`(Optional). For data split purposes when training a model. Use TRAINING or TEST.\n- `VIDEO_URI`. This field contains the Cloud Storage URI for the video. Cloud Storage URIs are case-sensitive.\n- `LABEL`. Labels must start with a letter and only contain letters, numbers, and underscores. You can specify multiple labels for a video by adding multiple rows in the CSV file that each identify the same video segment, with a different label for each row.\n- `START,END`. These two columns, START and END, respectively, identify the start and end time of the video segment to analyze, in seconds. The start time must be less than the end time. Both values must be non-negative and within the time range of the video. For example,`0.09845,1.36005`. To use the entire content of the video, specify a start time of`0`and an end time of the full-length of the video or \"inf\". For example,`0,inf`.\nSingle-label on the same video segment:\n```\nTRAINING,gs://YOUR_VIDEO_PATH/vehicle.mp4,mustang,0,5.4\n...\n```Multi-label on the same video segment:\n```\ngs://YOUR_VIDEO_PATH/vehicle.mp4,fiesta,0,8.285\ngs://YOUR_VIDEO_PATH/vehicle.mp4,ranger,0,8.285\ngs://YOUR_VIDEO_PATH/vehicle.mp4,explorer,0,8.285\n...\n```You can also provide videos in the data file specifying any labels. You must then use the Google Cloud console to apply labels to your data before you train your model. To do so, you only need to provide the Cloud Storage URI for the video followed by three commas, as shown in the following example.\n```\ngs://YOUR_VIDEO_PATH/vehicle.mp4,,,\n...\n```\n[Next  Create dataset    arrow_forward  ](/vertex-ai/docs/video-data/classification/create-dataset)", "guide": "Vertex AI"}