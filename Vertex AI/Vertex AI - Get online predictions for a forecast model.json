{"title": "Vertex AI - Get online predictions for a forecast model", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Get online predictions for a forecast model\nVertex AI provides two options for projecting future values using your trained forecast model: online predictions and batch predictions.\nAn online prediction is a synchronous request. Use online predictions when you are making requests in response to application input or in other situations where you require timely inference.\nA batch prediction request is an asynchronous request. Use batch predictions when you don't require an immediate response and want to process accumulated data by using a single request.\nThis page shows you how to project future values using online predictions. To learn how to project values using batch predictions, see [Get batch predictions for a forecast model](/vertex-ai/docs/tabular-data/tabular-workflows/forecasting-batch-predictions) .\nYou must deploy your model to an endpoint before you can use it for predictions. An endpoint is a set of physical resources.\nYou can request an explanation instead of a prediction. The explanation's local feature importance values tell you how much each feature contributed to the prediction result. For a conceptual overview, see [Feature attributions for forecasting](/vertex-ai/docs/tabular-data/forecasting-explanations) .\nTo learn about pricing for online predictions, see [Pricing for Tabular Workflows](/vertex-ai/docs/tabular-data/tabular-workflows/pricing) .\n", "content": "## Before you begin\nBefore you can make an online prediction request, you must first [train](/vertex-ai/docs/tabular-data/tabular-workflows/forecasting-train) a model.\n## Create or select an endpoint\nUse the function [aiplatform.Endpoint.create()](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Endpoint#google_cloud_aiplatform_Endpoint_create) to create an endpoint. If you already have an endpoint, use the function [aiplatform.Endpoint()](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Endpoint) to select it.\nThe following code provides an example:\n```\n# Import required modulesfrom google.cloud import aiplatformfrom google.cloud.aiplatform import modelsPROJECT_ID = \"PROJECT_ID\"REGION = \"REGION\"# Initialize the Vertex SDK for Python for your project.aiplatform.init(project=PROJECT_ID, location=REGION)endpoint = aiplatform.Endpoint.create(display_name='ENDPOINT_NAME')\n```\nReplace the following:\n- : Your project ID.\n- : The region where you are using Vertex AI.\n- : Display name for the endpoint.## Select a trained model\nUse the function [aiplatform.Model()](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model) to select a trained model:\n```\n# Create reference to the model trained ahead of time.model_obj = models.Model(\"TRAINED_MODEL_PATH\")\n```\nReplace the following:\n- : For example,`projects/` `` `/locations/` `` `/models/[TRAINED_MODEL_ID]`## Deploy the model to the endpoint\nUse the function [deploy()](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Endpoint#google_cloud_aiplatform_Endpoint_deploy) to deploy the model to the endpoint. The following code provides an example:\n```\ndeployed_model = endpoint.deploy(\u00a0 \u00a0 model_obj,\u00a0 \u00a0 machine_type='MACHINE_TYPE',\u00a0 \u00a0 traffic_percentage=100,\u00a0 \u00a0 min_replica_count='MIN_REPLICA_COUNT',\u00a0 \u00a0 max_replica_count='MAX_REPLICA_COUNT',\u00a0 \u00a0 sync=True,\u00a0 \u00a0 deployed_model_display_name='DEPLOYED_MODEL_NAME',)\n```\nReplace the following:\n- : For example,`n1-standard-8`. [Learn more about machine types](/vertex-ai/docs/predictions/configure-compute) .\n- : The minimum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to the maximum number of nodes and never fewer than this number of nodes. This value must be greater than or equal to 1. If the`min_replica_count`variable is not set, the value defaults to`1`.\n- : The maximum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to this number of nodes and never fewer than the minimum number of nodes. If you don't set the`max_replica_count`variable, then the maximum number of nodes is set to the value of`min_replica_count`.\n- : A name for the`DeployedModel`. You can use the display name of the`Model`for the`DeployedModel`as well.\nModel deployment may take approximately ten minutes.\n## Get online predictions\nTo get predictions, use the function [predict()](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Endpoint#google_cloud_aiplatform_Endpoint_predict) and provide one or more input instances. The following code shows an example:\n```\npredictions = endpoint.predict(instances=[{...}, {...}])\n```\nEach input instance is a Python dictionary with the same schema that the model was trained on. It must contain an key-value pair that corresponds to the time column and an key-value pair that contains the historical values of the targeted prediction column. Vertex AI expects each input instance to belong to a single time series. The order of the key-value pairs in the instance is not important.\nThe input instance is subject to the following constraints:\n- Thekey-value pairs must all have the same number of data points.\n- Thekey-value pairs must all have the same number of data points.\n- Thekey-value pairs must have at least as many data points as thekey-value pairs.\nTo learn more about the types of columns used in forecasting, see [Feature type and availability at forecast](/vertex-ai/docs/tabular-data/forecasting-parameters#feature-type) .\nThe following code demonstrates a set of two input instances. The `Category` column contains attribute data. The `Timestamp` column contains data that is available at forecast. Three points are data and two points are data. The `Sales` column contains data that is unavailable at forecast. All three points are data. To learn how context and horizon are used in forecasting, see [Forecast horizon, context window, and forecast window](/vertex-ai/docs/tabular-data/forecasting-parameters#forecast-window) .\n```\ninstances=[\u00a0 {\u00a0 \u00a0 # Attribute\u00a0 \u00a0 \"Category\": \"Electronics\",\u00a0 \u00a0 # Available at forecast: three days of context, two days of horizon\u00a0 \u00a0 \"Timestamp\": ['2023-08-03', '2023-08-04', '2023-08-05', '2023-08-06', '2023-08-07'],\u00a0 \u00a0 # Unavailable at forecast: three days of context\u00a0 \u00a0 \"Sales\": [490.50, 325.25, 647.00],\u00a0 },\u00a0 {\u00a0 \u00a0 # Attribute\u00a0 \u00a0 \"Category\": \"Food\",\u00a0 \u00a0 # Available at forecast: three days of context, two days of horizon\u00a0 \u00a0 \"Timestamp\": ['2023-08-03', '2023-08-04', '2023-08-05', '2023-08-06', '2023-08-07'],\u00a0 \u00a0 # Unavailable at forecast: three days of context\u00a0 \u00a0 \"Sales\": [190.50, 395.25, 47.00],\u00a0 }])\n```\nFor each instance, Vertex AI responds with two predictions for `Sales` , corresponding with the two timestamps (\"2023-08-06\" and \"2023-08-07\").\nFor optimal performance, the number of data points and the number of data points in each input instance must match the context and horizon lengths that the model was trained with. If there is a mismatch, Vertex AI pads or truncates the instance to match the model's size.\nIf the number of data points in your input instance is less than or greater than the number of data points used for model training, ensure that this number of points is consistent across all of the key-value pairs and all of the key-value pairs.\nFor example, consider a model that was trained with four days of data and two days of data. You can make a prediction request with just three days of data. In this case, the key-value pairs contain three values. The key-value pairs must contain five values.\n## Output of online prediction\nVertex AI provides online prediction output in the `value` field:\n```\n{\u00a0 'value': [...]}\n```\nThe length of the prediction response depends on the horizon used in model training and on the horizon of the input instance. The length of the prediction response is the smallest of these two values.\nConsider the following examples:\n- You train a model with`context`=`15`and`horizon`=`50`. Your input instance has`context`=`15`and`horizon`=`20`. The prediction response has a length of`20`.\n- You train a model with`context`=`15`and`horizon`=`50`. Your input instance has`context`=`15`and`horizon`=`100`. The prediction response has a length of`50`.\n### Online prediction output for TFT models\nFor models trained with [Temporal Fusion Transformer (TFT)](/vertex-ai/docs/tabular-data/forecasting-parameters#training-methods) , Vertex AI provides TFT interpretability `tft_feature_importance` in addition to predictions in the `value` field:\n```\n{\u00a0 \"tft_feature_importance\": {\u00a0 \u00a0 \"attribute_weights\": [...],\u00a0 \u00a0 \"attribute_columns\": [...],\u00a0 \u00a0 \"context_columns\": [...],\u00a0 \u00a0 \"context_weights\": [...],\u00a0 \u00a0 \"horizon_weights\": [...],\u00a0 \u00a0 \"horizon_columns\": [...]\u00a0 },\u00a0 \"value\": [...]}\n```\n- `attribute_columns`: Forecasting features which are [time-invariant](/vertex-ai/docs/tabular-data/forecasting-parameters#feature-type) .\n- `attribute_weights`: The weights associated with each of the`attribute_columns`.\n- `context_columns`: Forecasting features whose [context window](/vertex-ai/docs/tabular-data/forecasting-parameters#forecast-window) values serve  as inputs to the TFT Long Short-Term Memory (LSTM) Encoder.\n- `context_weights`: The feature importance weights associated with each of the`context_columns`for the predicted instance.\n- `horizon_columns`: Forecasting features whose [forecast horizon](/vertex-ai/docs/tabular-data/forecasting-parameters#forecast-window) values serve as inputs to the TFT Long Short-Term Memory (LSTM) Decoder.\n- `horizon_weights`: The feature importance weights associated with each of the`horizon_columns`for the predicted instance.\n### Online prediction output for models optimized for quantile loss\nFor models optimized for [quantile loss](/vertex-ai/docs/tabular-data/forecasting-parameters#optimization-objectives) , Vertex AI provides the following online prediction output:\n```\n{\u00a0 \"value\": [...],\u00a0 \"quantile_values\": [...],\u00a0 \"quantile_predictions\": [...]}\n```\n- `value`: If your set of quantiles includes the  median,`value`is the prediction value at the median. Otherwise,`value`is the prediction value at the lowest quantile in the  set. For example, if your set of quantiles is`[0.1, 0.5, 0.9]`,`value`is the prediction for quantile`0.5`. If your  set of quantiles is`[0.1, 0.9]`,`value`is the  prediction for quantile`0.1`.\n- `quantile_values`: The values of the quantiles,  which are set during model training.\n- `quantile_predictions`: The prediction values  associated with quantile_values.\nConsider, for example, a model in which the target column is the sales value. Quantile values are defined as `[0.1, 0.5, 0.9]` . Vertex AI returns the following quantile predictions: `[4484, 5615, 6853]` . Here, the set of quantiles includes the median, so `value` is the prediction for quantile `0.5` ( `5615` ). The quantile predictions can be interpreted as follows:\n- P(sales value < 4484) = 10%\n- P(sales value < 5615) = 50%\n- P(sales value < 6853) = 90%\n### Online prediction output for models with probabilistic inference\nIf your model uses probabilistic inference, the `value` field contains the minimizer of the optimization objective. For example, if your optimization objective is `minimize-rmse` , the `value` field contains the mean value. If it is `minimize-mae` , the `value` field contains the median value.\nIf your model uses probabilistic inference with quantiles, Vertex AI provides quantile values and predictions in addition to the minimizer of the optimization objective. Quantile values are set during model training. Quantile predictions are the prediction values associated with the quantile values.\n## Get online explanations\nTo get explanations, use the function [explain()](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Endpoint#google_cloud_aiplatform_Endpoint_explain) and provide one or more input instances. The following code shows an example:\n```\nexplanations = endpoint.explain(instances=[{...}, {...}])\n```\nThe format of the input instances is the same for online predictions and online explanations. To learn more, see [Get online predictions](#get-predictions) .\nFor a conceptual overview of feature attributions, see [Feature attributions for forecasting](/vertex-ai/docs/tabular-data/forecasting-explanations) .\n## Output of online explanation\nThe following code demonstrates how you can output the explanation results:\n```\n# Import required modulesimport jsonfrom google.protobuf import json_formatdef explanation_to_dict(explanation):\u00a0 \"\"\"Converts the explanation proto to a human-friendly json.\"\"\"\u00a0 return json.loads(json_format.MessageToJson(explanation._pb))for response in explanations.explanations:\u00a0 print(explanation_to_dict(response))\n```\nThe explanation results have the following format:\n```\n{\u00a0 \"attributions\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"baselineOutputValue\": 1.4194682836532593,\u00a0 \u00a0 \u00a0 \"instanceOutputValue\": 2.152980089187622,\u00a0 \u00a0 \u00a0 \"featureAttributions\": {\u00a0 \u00a0 \u00a0 \u00a0 ...\u00a0 \u00a0 \u00a0 \u00a0 \"store_id\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 0.007947325706481934\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 ...\u00a0 \u00a0 \u00a0 \u00a0 \"dept_id\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 5.960464477539062e-08\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \"item_id\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 0.1100526452064514\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \"date\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 0.8525647521018982\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 ...\u00a0 \u00a0 \u00a0 \u00a0 \"sales\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 0.0\u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \"outputIndex\": [\u00a0 \u00a0 \u00a0 \u00a0 2\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"approximationError\": 0.01433318599207033,\u00a0 \u00a0 \u00a0 \"outputName\": \"value\"\u00a0 \u00a0 },\u00a0 \u00a0 ...\u00a0 ]}\n```\nThe number of `attributions` elements depends on the horizon used in model training and on the horizon of the input instance. The number of elements is the smallest of these two values.\nThe `featureAttributions` field in an `attributions` element contains one value for each of the columns in the input dataset. Vertex AI generates explanations for all types of features: , , and . To learn more about the fields of an `attributions` element, see [Attribution](/vertex-ai/docs/reference/rest/v1/ModelExplanation#attribution) .\n## Delete the endpoint\nUse the functions [undeploy_all()](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Endpoint#google_cloud_aiplatform_Endpoint_undeploy_all) and [delete()](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Endpoint#google_cloud_aiplatform_Endpoint_delete) to delete your endpoint. The following code shows an example:\n```\nendpoint.undeploy_all()endpoint.delete()\n```\n## What's next\n- Learn about [pricing for online predictions](/vertex-ai/docs/tabular-data/tabular-workflows/pricing) .", "guide": "Vertex AI"}