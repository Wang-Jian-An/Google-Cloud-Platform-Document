{"title": "Vertex AI - Serving Predictions with NVIDIA Triton", "url": "https://cloud.google.com/vertex-ai/docs/predictions/using-nvidia-triton", "abstract": "# Vertex AI - Serving Predictions with NVIDIA Triton\nThis page describes how to serve prediction requests with [NVIDIA Triton inference server](https://developer.nvidia.com/nvidia-triton-inference-server) by using Vertex AI Prediction. NVIDIA Triton inference server (Triton) is an open-source inference serving solution from NVIDIA optimized for both CPUs and GPUs and simplifies the inference serving process.\n", "content": "## NVIDIA Triton on Vertex AI Prediction\nVertex AI Prediction supports deploying models on Triton inference server running on a custom container published by NVIDIA GPU Cloud (NGC) - [NVIDIA Triton inference server Image](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver) . Triton images from NVIDIA have all the required packages and configurations that meet the [Vertex AI requirements for custom servingcontainer images](/vertex-ai/docs/predictions/custom-container-requirements) . The image contains the Triton inference server with support for TensorFlow, PyTorch, TensorRT, ONNX, and OpenVINO models. The image also includes FIL ( [Forest Inference Library](https://github.com/rapidsai/cuml/tree/branch-21.10/python/cuml/fil) ) backend that supports running ML frameworks such as XGBoost, LightGBM, and Scikit-Learn.\nTriton loads the models and exposes inference, health, and model management REST endpoints that use [standard inferenceprotocols](https://github.com/kserve/kserve/tree/master/docs/predict-api/v2) . While deploying a model to Vertex AI, Triton recognizes Vertex AI environments and adopts the Vertex AI Prediction protocol for [health checks](/vertex-ai/docs/predictions/custom-container-requirements#health) and prediction requests.\nThe following list outlines key features and use cases of NVIDIA Triton inference server:\n- **Support for multiple Deep Learning and Machine Learning frameworks:** Triton supports deployment of multiple models and a mix of frameworks and model formats - TensorFlow (SavedModel and GraphDef), PyTorch (TorchScript), TensorRT, ONNX, OpenVINO, and FIL backends to support frameworks such as XGBoost, LightGBM, Scikit-Learn, and any custom Python or C++ model formats.\n- **Concurrent multiple model execution:** Triton allows multiple models, multiple instances of the same model, or both to execute concurrently on the same compute resource with zero or more GPUs.\n- **Model ensembling (chaining or pipelining):** Tritonsupports use cases where multiple models are composed as a pipeline (or a DAG, Directed Acyclic Graph) with inputs and output tensors that are connected between them. Additionally, with a Triton Python backend, you can include any pre-processing, post-processing, or control flow logic that is defined by [Business Logic Scripting (BLS)](https://github.com/triton-inference-server/python_backend#business-logic-scripting) .\n- **Run on CPU and GPU backends:** Triton supports inference for models deployed on nodes with CPUs and GPUs.\n- **Dynamic batching of prediction requests:** For models that support batching, Triton has built-in scheduling and batching algorithms. These algorithms dynamically combine individual inference requests into batches on the server side to improve inference throughput and increase GPU utilization.\nFor more information about NVIDIA Triton inference server, see the [Triton](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html) documentation.\n### Available NVIDIA Triton container images\nThe following table shows the [Triton Docker images available onNVIDIA NGC Catalog](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver) . Choose an image based on the model framework, backend, and the container image size you use.\n`xx` and `yy` refer to major and minor versions of Triton, respectively.\n| NVIDIA Triton Image | Supports                    |\n|:----------------------|:----------------------------------------------------------------------------------------|\n| xx.yy-py3    | Full container with support for TensorFlow, PyTorch, TensorRT, ONNX and OpenVINO models |\n| xx.yy-pyt-python-py3 | PyTorch and Python backends only              |\n| xx.yy-tf2-python-py3 | TensorFlow 2.x and Python backends only             |\n| xx.yy-py3-min   | Customize Triton container as needed             |\n## Getting started: Serving Predictions with NVIDIA Triton\nThe following figure shows the high-level architecture of Triton on Vertex AI Prediction:- An ML model to be served by Triton is registered with Vertex AI Model Registry. The model's metadata references a location of the model artifacts in Cloud Storage, the custom serving container, and its configuration.\n- The model from Vertex AI Model Registry is deployed to a Vertex AI Prediction endpoint that is running Triton inference server as a [custom container](/vertex-ai/docs/predictions/custom-container-requirements) on compute nodes with CPU and GPU.\n- Inference requests arrive at the Triton inference server through a Vertex AI Prediction endpoint and routed to the appropriate scheduler.\n- The backend performs inference by using the inputs provided in the batched requests and returns a response.\n- Triton provides readiness and liveness health endpoints, which enable the integration of Triton into deployment environments such as Vertex AI Prediction.\nThis tutorial shows you how to use a [custom container](/vertex-ai/docs/predictions/custom-container-requirements) that is running NVIDIA Triton inference server to deploy a machine learning (ML) model on Vertex AI Prediction, which serves online predictions. You deploy a container that is running Triton to serve predictions from an [object detection model from TensorFlowHub](https://www.kaggle.com/models/tensorflow/faster-rcnn-resnet-v1/frameworks/tensorFlow2/variations/faster-rcnn-resnet101-v1-640x640/versions/1) that has been pre-trained on the [COCO 2017 dataset](https://cocodataset.org/#download) . You can then use Vertex AI Prediction to detect objects in an image.\nYou can also run the tutorial on Vertex AI Workbench by following this [Jupyter Notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/nvidia-triton/nvidia-triton-custom-container-prediction.ipynb) .\n**Note:** The [Jupyter Notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/nvidia-triton/nvidia-triton-custom-container-prediction.ipynb) uses [Vertex AI SDK for Python](/vertex-ai/docs/start/client-libraries#python) . This tutorial runs [gcloud ai](https://cloud.google.com/sdk/gcloud/reference/ai) commands.\n## Before you begin\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) At the bottom of the Google Cloud console, a [Cloud Shell](/shell/docs/how-cloud-shell-works) session starts and displays a command-line prompt. Cloud Shell is a shell environment  with the Google Cloud CLI  already installed and with values already set for  your current project. It can take a few seconds for the session to initialize.\nThroughout this tutorial, we recommend that you use [Cloud Shell](/shell/docs) to interact with Google Cloud. If you want to use a different Bash shell instead of Cloud Shell, then perform the following additional configuration:\n- [Install](/sdk/docs/install) the Google Cloud CLI.\n- To [initialize](/sdk/docs/initializing) the gcloud CLI, run the following command:```\ngcloud init\n```\n- Follow the Artifact Registry documentation to [Install Docker.](/artifact-registry/docs/docker/quickstart#before-you-begin) ## Building and pushing the container image\nTo use a custom container, you must specify a Docker container image that meets the [custom container requirements](/vertex-ai/docs/predictions/custom-container-requirements) . This section describes how to create the container image and push it to Artifact Registry.\n### Download model artifacts\nModel artifacts are files created by ML training that you can use to serve predictions. They contain, at a minimum, the structure and [weights](https://developers.google.com/machine-learning/glossary#weight) of your trained ML model. The format of model artifacts depends on what ML framework you use for training.\nFor this tutorial, instead of training a model from scratch, download the object detection model from [TensorFlow Hub](https://www.kaggle.com/models/tensorflow/faster-rcnn-resnet-v1/frameworks/tensorFlow2/variations/faster-rcnn-resnet101-v1-640x640/versions/1) that has been trained on the [COCO 2017 dataset](https://cocodataset.org/#download) . Triton expects [model repository](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md) to be organized in the following structure for serving [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model) format:\n```\n\u2514\u2500\u2500 model-repository-path\n  \u2514\u2500\u2500 model_name\n    \u251c\u2500\u2500 config.pbtxt\n    \u2514\u2500\u2500 1\n     \u2514\u2500\u2500 model.savedmodel\n      \u2514\u2500\u2500 <saved-model-files>\n```\nThe `config.pbtxt` file describes the [model configuration](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md) for the model. By default, the model configuration file that contains the required settings must be provided. However, if Triton is started with the `--strict-model-config=false` option, then in some cases, the model configuration can be [generated automatically](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#auto-generated-model-configuration) by Triton and does not need to be provided explicitly. Specifically, TensorRT, TensorFlow SavedModel, and ONNX models don't require a model configuration file because Triton can derive all the required settings automatically. All other model types must provide a model configuration file.\n```\n# Download and organize model artifacts according to the Triton model repository specmkdir -p models/object_detector/1/model.savedmodel/curl -L \"https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1?tf-hub-format=compressed\" | \\\u00a0 \u00a0 tar -zxvC ./models/object_detector/1/model.savedmodel/ls -ltr ./models/object_detector/1/model.savedmodel/\n```\nAfter downloading the model locally, the model repository will be organized as following:\n```\n./models\n\u2514\u2500\u2500 object_detector\n \u2514\u2500\u2500 1\n  \u2514\u2500\u2500 model.savedmodel\n   \u251c\u2500\u2500 saved_model.pb\n   \u2514\u2500\u2500 variables\n    \u251c\u2500\u2500 variables.data-00000-of-00001\n    \u2514\u2500\u2500 variables.index\n```\n### Copy model artifacts to a Cloud Storage bucket\nThe downloaded model artifacts including model configuration file are pushed to a Cloud Storage bucket that is specified by `MODEL_ARTIFACTS_REPOSITORY` , which can be used when you create the Vertex AI model resource.\n```\ngsutil cp -r ./models/object_detector MODEL_ARTIFACTS_REPOSITORY/\n```\n### Create an Artifact Registry repository\nCreate an Artifact Registry repository to store the container image that you will create in the next section.\nEnable the Artifact Registry API service for your project.\n```\ngcloud services enable artifactregistry.googleapis.com\n```\nRun the following command in your shell to create Artifact Registry repository:\n```\ngcloud artifacts repositories create getting-started-nvidia-triton \\\u00a0--repository-format=docker \\\u00a0--location=LOCATION \\\u00a0--description=\"NVIDIA Triton Docker repository\"\n```\nReplace with the region where Artifact Registry stores your container image. Later, you must create a Vertex AI model resource on a regional endpoint that matches this region, so choose [a regionwhere Vertex AI has a regionalendpoint](/vertex-ai/docs/general/locations#feature-availability) , such as `us-central1` .\nAfter completing the operation, the command prints the following output:\n```\nCreated repository [getting-started-nvidia-triton].\n```\n### Build the container image\nNVIDIA provides [Docker images](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/tritonserver) for building a container image that is running Triton and aligns with Vertex AI [custom container requirements](/vertex-ai/docs/predictions/custom-container-requirements) for serving. You can pull the image by using `docker` and tag the Artifact Registry path that the image will be pushed to.\n```\nNGC_TRITON_IMAGE_URI=\"nvcr.io/nvidia/tritonserver:22.01-py3\"docker pull $NGC_TRITON_IMAGE_URIdocker tag $NGC_TRITON_IMAGE_URI LOCATION-docker.pkg.dev/PROJECT_ID/getting-started-nvidia-triton/vertex-triton-inference\n```\nReplace the following:\n- : the region of your Artifact Registry repository, as specified in a previous section\n- : the ID of your [Google Cloud project](/resource-manager/docs/creating-managing-projects#identifying_projects) \nThe command might run for several minutes.\n### Prepare payload file for testing prediction requests\nTo send the container's server a prediction request, prepare the payload with a sample image file that uses Python. Run the following python script to generate the payload file:\n```\nimport jsonimport requests# install required packages before running# pip install pillow numpy --upgradefrom PIL import Imageimport numpy as np# method to generate payload from image urldef generate_payload(image_url):\u00a0 \u00a0 # download image from url and resize\u00a0 \u00a0 image_inputs = Image.open(requests.get(image_url, stream=True).raw)\u00a0 \u00a0 image_inputs = image_inputs.resize((200, 200))\u00a0 \u00a0 # convert image to numpy array\u00a0 \u00a0 image_tensor = np.asarray(image_inputs)\u00a0 \u00a0 # derive image shape\u00a0 \u00a0 image_shape = [1] + list(image_tensor.shape)\u00a0 \u00a0 # create payload request\u00a0 \u00a0 payload = {\u00a0 \u00a0 \u00a0 \u00a0 \"id\": \"0\",\u00a0 \u00a0 \u00a0 \u00a0 \"inputs\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"input_tensor\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"shape\": image_shape,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"datatype\": \"UINT8\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"parameters\": {},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"data\": image_tensor.tolist(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 }\u00a0 \u00a0 # save payload as json file\u00a0 \u00a0 payload_file = \"instances.json\"\u00a0 \u00a0 with open(payload_file, \"w\") as f:\u00a0 \u00a0 \u00a0 \u00a0 json.dump(payload, f)\u00a0 \u00a0 print(f\"Payload generated at {payload_file}\")\u00a0 \u00a0 return payload_fileif __name__ == '__main__':\u00a0 image_url = \"https://github.com/tensorflow/models/raw/master/research/object_detection/test_images/image2.jpg\"\u00a0 payload_file = generate_payload(image_url)\n```\nThe Python script generates payload and prints the following response:\n```\nPayload generated at instances.json\n```\n### Run the container locally (optional)\nBefore you push your container image to Artifact Registry to use it with Vertex AI Prediction, you can run it as a container in your local environment to verify that the server works as expected:\n- To run the container image locally, run the following command in your shell:```\ndocker run -t -d -p 8000:8000 --rm \\\u00a0 --name=local_object_detector \\\u00a0 -e AIP_MODE=True \\\u00a0 LOCATION-docker.pkg.dev/PROJECT_ID/getting-started-nvidia-triton/vertex-triton-inference \\\u00a0 --model-repository MODEL_ARTIFACTS_REPOSITORY \\\u00a0 --strict-model-config=false\n```Replace the following, as you did in the previous section:- : the region of your Artifact Registry repository, as specified in a previous section\n- : the ID of your [Google Cloudproject](/resource-manager/docs/creating-managing-projects#identifying_projects) \n- : the Cloud Storage path where model artifacts are located\nThis command runs a container in [detachedmode](https://docs.docker.com/engine/reference/run/#detached-vs-foreground) , mapping port `8000` of the container to port `8000` of the local environment. The Triton image from NGC configures Triton to use port `8000` .\n- To send the container's server a [healthcheck](https://github.com/triton-inference-server/server/blob/main/docs/getting_started/quickstart.md#verify-triton-is-running-correctly) , run the following command in your shell:```\ncurl -s -o /dev/null -w \"%{http_code}\" http://localhost:8000/v2/health/ready\n```If successful, the server returns the status code as `200` .\n- Run the following command to send the container's server a prediction request by using the payload generated previously and get prediction responses:```\ncurl -X POST \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 -d @instances.json \\\u00a0 localhost:8000/v2/models/object_detector/infer |jq -c '.outputs[] | select(.name == \"detection_classes\")'\n```This request uses one of the [testimages](https://github.com/tensorflow/models/raw/master/research/object_detection/test_images/image2.jpg) that is included with the TensorFlow object detection example.If successful, the server returns the following prediction:```\n{\"name\":\"detection_classes\",\"datatype\":\"FP32\",\"shape\":[1,300],\"data\":[38,1,...,44]}\n```\n- To stop the container, run the following command in your shell:```\ndocker stop local_object_detector\n```\n### Push the container image to Artifact Registry\nConfigure Docker to access Artifact Registry. Then push your container image to your Artifact Registry repository.\n- To give your local Docker installation permission to push to Artifact Registry in your chosen region, run the following command in your shell:```\ngcloud auth configure-docker LOCATION-docker.pkg.dev\n```- Replacewith the region where you created your repository in a previous section.\n- To push the container image that you just to Artifact Registry, run the following command in your shell:```\ndocker push LOCATION-docker.pkg.dev/PROJECT_ID/getting-started-nvidia-triton/vertex-triton-inference\n```Replace the following, as you did in the previous section:- : the region of your Artifact Registry repository, as specified in a previous section\n- : the ID of your [Google Cloudproject](/resource-manager/docs/creating-managing-projects#identifying_projects) \n## Deploying the model\n### Create a model\nTo create a `Model` resource that uses a custom container running Triton, run the following [gcloud ai models uploadcommand](/sdk/gcloud/reference/ai/models/upload) :\n```\ngcloud ai models upload \\\u00a0 --region=LOCATION \\\u00a0 --display-name=DEPLOYED_MODEL_NAME \\\u00a0 --container-image-uri=LOCATION-docker.pkg.dev/PROJECT_ID/getting-started-nvidia-triton/vertex-triton-inference \\\u00a0 --artifact-uri=MODEL_ARTIFACTS_REPOSITORY \\\u00a0 --container-args='--strict-model-config=false'\n```\n- : The region where you are using Vertex AI.\n- : the ID of your [Google Cloudproject](/resource-manager/docs/creating-managing-projects#identifying_projects) \n- : A name for the`DeployedModel`. You can use the display name of the`Model`for the`DeployedModel`as well.\nThe argument `--container-args='--strict-model-config=false'` allows Triton to generate the model configuration automatically.\n### Create an endpoint\nYou must deploy the model to an endpoint before the model can be used to serve online predictions. If you are deploying a model to an existing endpoint, you can skip this step. The following example uses the [gcloud ai endpoints createcommand](/sdk/gcloud/reference/ai/endpoints/create) :\n```\ngcloud ai endpoints create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=ENDPOINT_NAME\n```\nReplace the following:\n- : The region where you are using Vertex AI.\n- : The display name for the endpoint.\nThe Google Cloud CLI tool might take a few seconds to create the endpoint.\n### Deploy the model to endpoint\nAfter the endpoint is ready, deploy the model to the endpoint. When you deploy a model to an endpoint, the service associates physical resources with the model running Triton to serve online predictions.\nThe following example uses the [gcloud ai endpoints deploy-modelcommand](/sdk/gcloud/reference/ai/endpoints/deploy-model) to deploy the `Model` to an `endpoint` running Triton on GPUs to accelerate prediction serving and without splitting traffic between multiple `DeployedModel` resources:\n```\nENDPOINT_ID=$(gcloud ai endpoints list \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--filter=display_name=ENDPOINT_NAME \\\u00a0 \u00a0--format=\"value(name)\") MODEL_ID=$(gcloud ai models list \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--filter=display_name=DEPLOYED_MODEL_NAME \\\u00a0 \u00a0--format=\"value(name)\") gcloud ai endpoints deploy-model $ENDPOINT_ID \\\u00a0 --region=LOCATION \\\u00a0 --model=$MODEL_ID \\\u00a0 --display-name=DEPLOYED_MODEL_NAME \\\u00a0 --machine-type=MACHINE_TYPE \\\u00a0 --min-replica-count=MIN_REPLICA_COUNT \\\u00a0 --max-replica-count=MAX_REPLICA_COUNT \\\u00a0 --accelerator=count=ACCELERATOR_COUNT,type=ACCELERATOR_TYPE \\\u00a0 --traffic-split=0=100\n```\nReplace the following:\n- : The region where you are using Vertex AI.\n- : The display name for the endpoint.\n- : A name for the`DeployedModel`. You can use the display name of the`Model`for the`DeployedModel`as well.\n- : Optional. The machine resources used for each node of this deployment. Its default setting is`n1-standard-2`. [Learn more about machine types.](/vertex-ai/docs/predictions/configure-compute) \n- : The minimum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to the maximum number of nodes and never fewer than this number of nodes.\n- : The maximum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to this number of nodes and never fewer than the minimum number of nodes.\n- : Manage the accelerator config for GPU serving. When deploying a model with Compute Engine Machine Types, a GPU accelerator may also be selected and type must be specified. Choices are 'nvidia-tesla-a100', 'nvidia-tesla-k80', 'nvidia-tesla-p100', 'nvidia-tesla-p4', 'nvidia-tesla-t4', 'nvidia-tesla-v100'.\n- : The number of accelerators to attach to each machine running the job. This is usually 1. If not specified, the default value is 1.\nThe Google Cloud CLI tool might take a few seconds to deploy the model to the endpoint. When the model is successfully deployed, this command prints the following output:\n```\n Deployed a model to the endpoint xxxxx. Id of the deployed model: xxxxx.\n```\n## Getting online predictions from the deployed model\nTo invoke the model through the Vertex AI Prediction endpoint, format the prediction request by using a [standard Inference Request JSON Object](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#inference) or an [Inference Request JSON Object with a binary extension](https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md) and submit a request to Vertex AI Prediction [REST rawPredictendpoint](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/rawPredict) .\n**Note:** Instead of [predict](/vertex-ai/docs/predictions/get-online-predictions#predict-request) API, you must use the [rawPredict](/vertex-ai/docs/predictions/get-online-predictions#raw-predict-request) API because the inference request formats used by Triton are not compatible with the Vertex AI Prediction [standard input format](/vertex-ai/docs/predictions/get-online-predictions#formatting-prediction-input) .\nThe following example uses the [gcloud ai endpoints raw-predictcommand](/sdk/gcloud/reference/ai/endpoints/raw-predict) :\n```\nENDPOINT_ID=$(gcloud ai endpoints list \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--filter=display_name=ENDPOINT_NAME \\\u00a0 \u00a0--format=\"value(name)\") gcloud ai endpoints raw-predict $ENDPOINT_ID \\\u00a0 --region=LOCATION \\\u00a0 --http-headers=Content-Type=application/json \\\u00a0 --request @instances.json\n```\nReplace the following:\n- : The region where you are using Vertex AI.\n- : The display name for the endpoint.\nThe endpoint returns the following response for a valid request:\n```\n{\u00a0 \u00a0 \"id\": \"0\",\u00a0 \u00a0 \"model_name\": \"object_detector\",\u00a0 \u00a0 \"model_version\": \"1\",\u00a0 \u00a0 \"outputs\": [{\u00a0 \u00a0 \u00a0 \u00a0 \"name\": \"detection_anchor_indices\",\u00a0 \u00a0 \u00a0 \u00a0 \"datatype\": \"FP32\",\u00a0 \u00a0 \u00a0 \u00a0 \"shape\": [1, 300],\u00a0 \u00a0 \u00a0 \u00a0 \"data\": [2.0, 1.0, 0.0, 3.0, 26.0, 11.0, 6.0, 92.0, 76.0, 17.0, 58.0, ...]\u00a0 \u00a0 }]}\n```\n## Cleaning up\nTo avoid incurring further [Vertex AI charges](/vertex-ai/pricing) and [Artifact Registrycharges](/artifact-registry/pricing) , delete the Google Cloud resources that you created during this tutorial:\n- To undeploy model from endpoint and delete the endpoint, run the following command in your shell:```\nENDPOINT_ID=$(gcloud ai endpoints list \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--filter=display_name=ENDPOINT_NAME \\\u00a0 \u00a0--format=\"value(name)\") DEPLOYED_MODEL_ID=$(gcloud ai endpoints describe $ENDPOINT_ID \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--format=\"value(deployedModels.id)\")gcloud ai endpoints undeploy-model $ENDPOINT_ID \\\u00a0 --region=LOCATION \\\u00a0 --deployed-model-id=$DEPLOYED_MODEL_IDgcloud ai endpoints delete $ENDPOINT_ID \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--quiet\n```Replace with the region where you created your model in a previous section.\n- To delete your model, run the following command in your shell:```\nMODEL_ID=$(gcloud ai models list \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--filter=display_name=DEPLOYED_MODEL_NAME \\\u00a0 \u00a0--format=\"value(name)\") gcloud ai models delete $MODEL_ID \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--quiet\n```Replace with the region where you created your model in a previous section.\n- To delete your Artifact Registry repository and the container image in it, run the following command in your shell:```\ngcloud artifacts repositories delete getting-started-nvidia-triton \\\u00a0 --location=LOCATION \\\u00a0 --quiet\n```Replace with the region where you created your Artifact Registry repository in a previous section.## Limitations\n- The Triton custom container is not compatible with [Vertex Explainable AI](/vertex-ai/docs/explainable-ai/overview) or [Vertex AI Model Monitoring](/vertex-ai/docs/model-monitoring/overview) .## What's next\n- Refer [Vertex AI Jupyter Notebook tutorials](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/nvidia-triton/) for deployment patterns with NVIDIA Triton inference server on Vertex AI Prediction.", "guide": "Vertex AI"}