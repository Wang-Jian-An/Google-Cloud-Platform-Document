{"title": "Vertex AI - Train custom ML models on Vertex AI Pipelines", "url": "https://cloud.google.com/vertex-ai/docs/tutorials/custom-training-pipelines/tabular", "abstract": "# Vertex AI - Train custom ML models on Vertex AI Pipelines\nThis tutorial shows you how to use Vertex AI Pipelines to run an end-to-end ML workflow, including the following tasks:\n- Import and transform data.\n- Train a model using the selected ML framework.\n- Import the trained model to Vertex AI Model Registry.\n- **Optional** : Deploy the model for online serving with Vertex AI Prediction.", "content": "## Before you begin\n- Ensure that you've completed the tasks 1-3 in [Set up a Google Cloud project and a developmentenvironment](/vertex-ai/docs/pipelines/configure-project#project) .\n- Install the [Vertex AI SDK for Python](/vertex-ai/docs/start/client-libraries#client_libraries) and the Kubeflow Pipelines SDK:```\npython3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quiet\n```## Run the ML model training pipeline\nChoose training objective and ML framework in the following tabs to get sample code that you can run in your environment. The sample code does the following:\n- Loads components from a [component repository](https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/main/community-content/pipeline_components) to be used as pipeline building blocks.\n- Composes a pipeline by creating component tasks and passing data between them using arguments.\n- Submits the pipeline for execution on Vertex AI Pipelines. See [Vertex AI Pipelines pricing](/vertex-ai/pricing#pipelines) .\nCopy the code into your development environment and run it.\n[Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/GoogleCloudPlatform/vertex-ai-samples&page=editor&cloudshell_workspace=community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_classification_model_using_TensorFlow_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py) [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_classification_model_using_TensorFlow_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")binarize_column_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Binarize_column/in_CSV_format/component.yaml\")split_rows_into_subsets_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/dataset_manipulation/Split_rows_into_subsets/in_CSV/component.yaml\")create_fully_connected_tensorflow_network_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/tensorflow/Create_fully_connected_network/component.yaml\")train_model_using_Keras_on_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/tensorflow/Train_model_using_Keras/on_CSV/component.yaml\")predict_with_TensorFlow_model_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/tensorflow/Predict/on_CSV/component.yaml\")upload_Tensorflow_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_Tensorflow_model/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_classification_model_using_TensorFlow_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 training_set_fraction = 0.8\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 classification_label_column = \"class\"\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 dataset = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 dataset = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 dataset = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 # # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # column_names=None, \u00a0# =[...]\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 classification_dataset = binarize_column_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 predicate=\" > 0\",\u00a0 \u00a0 \u00a0 \u00a0 new_column_name=classification_label_column,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 split_task = split_rows_into_subsets_op(\u00a0 \u00a0 \u00a0 \u00a0 table=classification_dataset,\u00a0 \u00a0 \u00a0 \u00a0 fraction_1=training_set_fraction,\u00a0 \u00a0 )\u00a0 \u00a0 classification_training_data = split_task.outputs[\"split_1\"]\u00a0 \u00a0 classification_testing_data = split_task.outputs[\"split_2\"]\u00a0 \u00a0 network = create_fully_connected_tensorflow_network_op(\u00a0 \u00a0 \u00a0 \u00a0 input_size=len(feature_columns),\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 hidden_layer_sizes=[10],\u00a0 \u00a0 \u00a0 \u00a0 activation_name=\"elu\",\u00a0 \u00a0 \u00a0 \u00a0 output_activation_name=\"sigmoid\",\u00a0 \u00a0 \u00a0 \u00a0 # output_size=1,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 model = train_model_using_Keras_on_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 training_data=classification_training_data,\u00a0 \u00a0 \u00a0 \u00a0 model=network,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=classification_label_column,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 loss_function_name=\"binary_crossentropy\",\u00a0 \u00a0 \u00a0 \u00a0 number_of_epochs=10,\u00a0 \u00a0 \u00a0 \u00a0 #learning_rate=0.1,\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_name=\"Adadelta\",\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_parameters={},\u00a0 \u00a0 \u00a0 \u00a0 #batch_size=32,\u00a0 \u00a0 \u00a0 \u00a0 #metric_names=[\"mean_absolute_error\"],\u00a0 \u00a0 \u00a0 \u00a0 #random_seed=0,\u00a0 \u00a0 ).outputs[\"trained_model\"]\u00a0 \u00a0 predictions = predict_with_TensorFlow_model_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 dataset=classification_testing_data,\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 \u00a0 \u00a0 # label_column_name needs to be set when doing prediction on a dataset that has labels\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=classification_label_column,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # batch_size=1000,\u00a0 \u00a0 ).outputs[\"predictions\"]\u00a0 \u00a0 vertex_model_name = upload_Tensorflow_model_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func = train_tabular_classification_model_using_TensorFlow_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n``` [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/GoogleCloudPlatform/vertex-ai-samples&page=editor&cloudshell_workspace=community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_classification_model_using_PyTorch_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py) [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_classification_model_using_PyTorch_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")binarize_column_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Binarize_column/in_CSV_format/component.yaml\")create_fully_connected_pytorch_network_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/PyTorch/Create_fully_connected_network/component.yaml\")train_pytorch_model_from_csv_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/PyTorch/Train_PyTorch_model/from_CSV/component.yaml\")create_pytorch_model_archive_with_base_handler_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/PyTorch/Create_PyTorch_Model_Archive/with_base_handler/component.yaml\")upload_PyTorch_model_archive_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_PyTorch_model_archive/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_classification_model_using_PyTorch_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 classification_label_column = \"class\"\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 training_data = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 training_data = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 # Cleaning the NaN values.\u00a0 \u00a0 training_data = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 #replacement_type_name=\"float\",\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 classification_training_data = binarize_column_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 predicate=\" > 0\",\u00a0 \u00a0 \u00a0 \u00a0 new_column_name=classification_label_column,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 network = create_fully_connected_pytorch_network_op(\u00a0 \u00a0 \u00a0 \u00a0 input_size=len(feature_columns),\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 hidden_layer_sizes=[10],\u00a0 \u00a0 \u00a0 \u00a0 activation_name=\"elu\",\u00a0 \u00a0 \u00a0 \u00a0 output_activation_name=\"sigmoid\",\u00a0 \u00a0 \u00a0 \u00a0 # output_size=1,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 model = train_pytorch_model_from_csv_op(\u00a0 \u00a0 \u00a0 \u00a0 model=network,\u00a0 \u00a0 \u00a0 \u00a0 training_data=classification_training_data,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=classification_label_column,\u00a0 \u00a0 \u00a0 \u00a0 loss_function_name=\"binary_cross_entropy\",\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 #number_of_epochs=1,\u00a0 \u00a0 \u00a0 \u00a0 #learning_rate=0.1,\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_name=\"Adadelta\",\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_parameters={},\u00a0 \u00a0 \u00a0 \u00a0 #batch_size=32,\u00a0 \u00a0 \u00a0 \u00a0 #batch_log_interval=100,\u00a0 \u00a0 \u00a0 \u00a0 #random_seed=0,\u00a0 \u00a0 ).outputs[\"trained_model\"]\u00a0 \u00a0 model_archive = create_pytorch_model_archive_with_base_handler_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # model_name=\"model\",\u00a0 \u00a0 \u00a0 \u00a0 # model_version=\"1.0\",\u00a0 \u00a0 ).outputs[\"Model archive\"]\u00a0 \u00a0 vertex_model_name = upload_PyTorch_model_archive_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model_archive=model_archive,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func=train_tabular_classification_model_using_PyTorch_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n``` [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/GoogleCloudPlatform/vertex-ai-samples&page=editor&cloudshell_workspace=community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_classification_model_using_XGBoost_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py) [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_classification_model_using_XGBoost_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")binarize_column_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Binarize_column/in_CSV_format/component.yaml\")split_rows_into_subsets_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/dataset_manipulation/Split_rows_into_subsets/in_CSV/component.yaml\")train_XGBoost_model_on_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/XGBoost/Train/component.yaml\")xgboost_predict_on_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/XGBoost/Predict/component.yaml\")upload_XGBoost_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_XGBoost_model/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_classification_model_using_XGBoost_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 training_set_fraction = 0.8\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 classification_label_column = \"class\"\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 dataset = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 dataset = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 dataset = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 # # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # column_names=None, \u00a0# =[...]\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 classification_dataset = binarize_column_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 predicate=\"> 0\",\u00a0 \u00a0 \u00a0 \u00a0 new_column_name=classification_label_column,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 split_task = split_rows_into_subsets_op(\u00a0 \u00a0 \u00a0 \u00a0 table=classification_dataset,\u00a0 \u00a0 \u00a0 \u00a0 fraction_1=training_set_fraction,\u00a0 \u00a0 )\u00a0 \u00a0 classification_training_data = split_task.outputs[\"split_1\"]\u00a0 \u00a0 classification_testing_data = split_task.outputs[\"split_2\"]\u00a0 \u00a0 model = train_XGBoost_model_on_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 training_data=classification_training_data,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=classification_label_column,\u00a0 \u00a0 \u00a0 \u00a0 objective=\"binary:logistic\",\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 #starting_model=None,\u00a0 \u00a0 \u00a0 \u00a0 #num_iterations=10,\u00a0 \u00a0 \u00a0 \u00a0 #booster_params={},\u00a0 \u00a0 \u00a0 \u00a0 #booster=\"gbtree\",\u00a0 \u00a0 \u00a0 \u00a0 #learning_rate=0.3,\u00a0 \u00a0 \u00a0 \u00a0 #min_split_loss=0,\u00a0 \u00a0 \u00a0 \u00a0 #max_depth=6,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 # Predicting on the testing data\u00a0 \u00a0 predictions = xgboost_predict_on_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 data=classification_testing_data,\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 \u00a0 \u00a0 # label_column needs to be set when doing prediction on a dataset that has labels\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=classification_label_column,\u00a0 \u00a0 ).outputs[\"predictions\"]\u00a0 \u00a0 vertex_model_name = upload_XGBoost_model_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func = train_tabular_classification_model_using_XGBoost_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n``` [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/GoogleCloudPlatform/vertex-ai-samples&page=editor&cloudshell_workspace=community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_classification_logistic_regression_model_using_Scikit_learn_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py) [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_classification_logistic_regression_model_using_Scikit_learn_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")binarize_column_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Binarize_column/in_CSV_format/component.yaml\")train_logistic_regression_model_using_scikit_learn_from_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/1f5cf6e06409b704064b2086c0a705e4e6b4fcde/community-content/pipeline_components/ML_frameworks/Scikit_learn/Train_logistic_regression_model/from_CSV/component.yaml\")upload_Scikit_learn_pickle_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_Scikit-learn_pickle_model/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_classification_logistic_regression_model_using_Scikit_learn_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 classification_label_column = \"class\"\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 training_data = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 training_data = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 # Cleaning the NaN values.\u00a0 \u00a0 training_data = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 #replacement_type_name=\"float\",\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 classification_training_data = binarize_column_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 predicate=\"> 0\",\u00a0 \u00a0 \u00a0 \u00a0 new_column_name=classification_label_column,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 model = train_logistic_regression_model_using_scikit_learn_from_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 dataset=classification_training_data,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=classification_label_column,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 #penalty=\"l2\",\u00a0 \u00a0 \u00a0 \u00a0 #solver=\"lbfgs\",\u00a0 \u00a0 \u00a0 \u00a0 #max_iterations=100,\u00a0 \u00a0 \u00a0 \u00a0 #multi_class_mode=\"auto\",\u00a0 \u00a0 \u00a0 \u00a0 #random_seed=0,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 vertex_model_name = upload_Scikit_learn_pickle_model_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 sklearn_vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func = train_tabular_classification_logistic_regression_model_using_Scikit_learn_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n```\n [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/GoogleCloudPlatform/vertex-ai-samples&page=editor&cloudshell_workspace=community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_regression_model_using_TensorFlow_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py) [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_regression_model_using_TensorFlow_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")split_rows_into_subsets_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/dataset_manipulation/Split_rows_into_subsets/in_CSV/component.yaml\")create_fully_connected_tensorflow_network_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/tensorflow/Create_fully_connected_network/component.yaml\")train_model_using_Keras_on_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/tensorflow/Train_model_using_Keras/on_CSV/component.yaml\")predict_with_TensorFlow_model_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/tensorflow/Predict/on_CSV/component.yaml\")upload_Tensorflow_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_Tensorflow_model/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_regression_model_using_Tensorflow_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 training_set_fraction = 0.8\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 dataset = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 dataset = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 dataset = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 # # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # column_names=None, \u00a0# =[...]\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 split_task = split_rows_into_subsets_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 fraction_1=training_set_fraction,\u00a0 \u00a0 )\u00a0 \u00a0 training_data = split_task.outputs[\"split_1\"]\u00a0 \u00a0 testing_data = split_task.outputs[\"split_2\"]\u00a0 \u00a0 network = create_fully_connected_tensorflow_network_op(\u00a0 \u00a0 \u00a0 \u00a0 input_size=len(feature_columns),\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 hidden_layer_sizes=[10],\u00a0 \u00a0 \u00a0 \u00a0 activation_name=\"elu\",\u00a0 \u00a0 \u00a0 \u00a0 # output_activation_name=None,\u00a0 \u00a0 \u00a0 \u00a0 # output_size=1,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 model = train_model_using_Keras_on_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 training_data=training_data,\u00a0 \u00a0 \u00a0 \u00a0 model=network,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 #loss_function_name=\"mean_squared_error\",\u00a0 \u00a0 \u00a0 \u00a0 number_of_epochs=10,\u00a0 \u00a0 \u00a0 \u00a0 #learning_rate=0.1,\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_name=\"Adadelta\",\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_parameters={},\u00a0 \u00a0 \u00a0 \u00a0 #batch_size=32,\u00a0 \u00a0 \u00a0 \u00a0 metric_names=[\"mean_absolute_error\"],\u00a0 \u00a0 \u00a0 \u00a0 #random_seed=0,\u00a0 \u00a0 ).outputs[\"trained_model\"]\u00a0 \u00a0 predictions = predict_with_TensorFlow_model_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 dataset=testing_data,\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 \u00a0 \u00a0 # label_column_name needs to be set when doing prediction on a dataset that has labels\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # batch_size=1000,\u00a0 \u00a0 ).outputs[\"predictions\"]\u00a0 \u00a0 vertex_model_name = upload_Tensorflow_model_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func=train_tabular_regression_model_using_Tensorflow_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n``` [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/GoogleCloudPlatform/vertex-ai-samples&page=editor&cloudshell_workspace=community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_regression_model_using_PyTorch_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py) [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_regression_model_using_PyTorch_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")create_fully_connected_pytorch_network_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/PyTorch/Create_fully_connected_network/component.yaml\")train_pytorch_model_from_csv_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/PyTorch/Train_PyTorch_model/from_CSV/component.yaml\")create_pytorch_model_archive_with_base_handler_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/PyTorch/Create_PyTorch_Model_Archive/with_base_handler/component.yaml\")upload_PyTorch_model_archive_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_PyTorch_model_archive/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_regression_model_using_PyTorch_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 training_data = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 training_data = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 # Cleaning the NaN values.\u00a0 \u00a0 training_data = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 #replacement_type_name=\"float\",\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 network = create_fully_connected_pytorch_network_op(\u00a0 \u00a0 \u00a0 \u00a0 input_size=len(feature_columns),\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 hidden_layer_sizes=[10],\u00a0 \u00a0 \u00a0 \u00a0 activation_name=\"elu\",\u00a0 \u00a0 \u00a0 \u00a0 # output_activation_name=None,\u00a0 \u00a0 \u00a0 \u00a0 # output_size=1,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 model = train_pytorch_model_from_csv_op(\u00a0 \u00a0 \u00a0 \u00a0 model=network,\u00a0 \u00a0 \u00a0 \u00a0 training_data=training_data,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 #loss_function_name=\"mse_loss\",\u00a0 \u00a0 \u00a0 \u00a0 #number_of_epochs=1,\u00a0 \u00a0 \u00a0 \u00a0 #learning_rate=0.1,\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_name=\"Adadelta\",\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_parameters={},\u00a0 \u00a0 \u00a0 \u00a0 #batch_size=32,\u00a0 \u00a0 \u00a0 \u00a0 #batch_log_interval=100,\u00a0 \u00a0 \u00a0 \u00a0 #random_seed=0,\u00a0 \u00a0 ).outputs[\"trained_model\"]\u00a0 \u00a0 model_archive = create_pytorch_model_archive_with_base_handler_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # model_name=\"model\",\u00a0 \u00a0 \u00a0 \u00a0 # model_version=\"1.0\",\u00a0 \u00a0 ).outputs[\"Model archive\"]\u00a0 \u00a0 vertex_model_name = upload_PyTorch_model_archive_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model_archive=model_archive,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func=train_tabular_regression_model_using_PyTorch_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n``` [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/GoogleCloudPlatform/vertex-ai-samples&page=editor&cloudshell_workspace=community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py) [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")split_rows_into_subsets_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/dataset_manipulation/Split_rows_into_subsets/in_CSV/component.yaml\")train_XGBoost_model_on_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/XGBoost/Train/component.yaml\")xgboost_predict_on_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/XGBoost/Predict/component.yaml\")upload_XGBoost_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_XGBoost_model/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_regression_model_using_XGBoost_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 training_set_fraction = 0.8\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 dataset = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 dataset = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 dataset = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 # # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # column_names=None, \u00a0# =[...]\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 split_task = split_rows_into_subsets_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 fraction_1=training_set_fraction,\u00a0 \u00a0 )\u00a0 \u00a0 training_data = split_task.outputs[\"split_1\"]\u00a0 \u00a0 testing_data = split_task.outputs[\"split_2\"]\u00a0 \u00a0 model = train_XGBoost_model_on_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 training_data=training_data,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 #starting_model=None,\u00a0 \u00a0 \u00a0 \u00a0 #num_iterations=10,\u00a0 \u00a0 \u00a0 \u00a0 #booster_params={},\u00a0 \u00a0 \u00a0 \u00a0 #objective=\"reg:squarederror\",\u00a0 \u00a0 \u00a0 \u00a0 #booster=\"gbtree\",\u00a0 \u00a0 \u00a0 \u00a0 #learning_rate=0.3,\u00a0 \u00a0 \u00a0 \u00a0 #min_split_loss=0,\u00a0 \u00a0 \u00a0 \u00a0 #max_depth=6,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 # Predicting on the testing data\u00a0 \u00a0 predictions = xgboost_predict_on_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 data=testing_data,\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 \u00a0 \u00a0 # label_column needs to be set when doing prediction on a dataset that has labels\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=label_column,\u00a0 \u00a0 ).outputs[\"predictions\"]\u00a0 \u00a0 vertex_model_name = upload_XGBoost_model_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func = train_tabular_regression_model_using_XGBoost_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n``` [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/GoogleCloudPlatform/vertex-ai-samples&page=editor&cloudshell_workspace=community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_regression_linear_model_using_Scikit_learn_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py) [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_regression_linear_model_using_Scikit_learn_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")train_linear_regression_model_using_scikit_learn_from_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/1f5cf6e06409b704064b2086c0a705e4e6b4fcde/community-content/pipeline_components/ML_frameworks/Scikit_learn/Train_linear_regression_model/from_CSV/component.yaml\")upload_Scikit_learn_pickle_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_Scikit-learn_pickle_model/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_regression_linear_model_using_Scikit_learn_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 training_data = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 training_data = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 # Cleaning the NaN values.\u00a0 \u00a0 training_data = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 #replacement_type_name=\"float\",\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 model = train_linear_regression_model_using_scikit_learn_from_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 dataset=training_data,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=label_column,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 vertex_model_name = upload_Scikit_learn_pickle_model_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 sklearn_vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func = train_tabular_regression_linear_model_using_Scikit_learn_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n```\nNote the following about code samples provided:\n- A Kubeflow pipeline is defined as a Python function.\n- The pipeline's workflow steps are created using Kubeflow pipeline components. By using the outputs of a component as an input of another component, you define the pipeline's workflow as a graph. For example, the`fill_all_missing_values_using_Pandas_on_CSV_data_op`component task depends on the`transformed_table`output from the`select_columns_using_Pandas_on_CSV_data_op`component task.\n- You create a pipeline run on Vertex AI Pipelines using the Vertex AI SDK for Python.## Monitor the pipeline\nIn the Google Cloud console, in the Vertex AI section, go to the **Pipelines** page and open the **Runs** tab.\n[Go to Pipeline runs](https://console.cloud.google.com/vertex-ai/pipelines/runs)\n## What's next\n- To learn more about Vertex AI Pipelines, see [Introduction to Vertex AI Pipelines](/vertex-ai/docs/pipelines/introduction) .", "guide": "Vertex AI"}