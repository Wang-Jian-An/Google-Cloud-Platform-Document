{"title": "Vertex AI - Custom prediction routines", "url": "https://cloud.google.com/vertex-ai/docs/predictions/custom-prediction-routines", "abstract": "# Vertex AI - Custom prediction routines\nCustom prediction routines (CPR) lets you build [custom containers](/vertex-ai/docs/predictions/use-custom-container) with pre/post processing code easily, without dealing with the details of setting up an HTTP server or building a container from scratch. You can use preprocessing to normalize/transform the inputs or make calls to external services to get additional data, and use post processing to format the model prediction or run business logic.\nThe following diagram depicts the user workflow both with and without custom prediction routines.\nThe main differences are:\n- You don't need to write a model server or a Dockerfile. The model server, which is the HTTP server that hosts the model, is provided for you.\n- You can deploy and debug the model locally, speeding up the iteration cycle during development.", "content": "## Build and deploy a custom container\nThis section describes how to use CPR to build a custom container with pre/post processing logic and deploy to both a local and online endpoint.\n### Setup\nYou must have [Vertex AI SDK](https://github.com/googleapis/python-aiplatform) and [Docker](https://www.docker.com/) installed in your environment.\n### Write custom Predictor\nImplement the [Predictor](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/prediction/predictor.py) interface.\n```\nclass Predictor(ABC):\u00a0 \u00a0 \"\"\"Interface of the Predictor class for Custom Prediction Routines.\u00a0 \u00a0 The Predictor is responsible for the ML logic for processing a prediction request.\u00a0 \u00a0 Specifically, the Predictor must define:\u00a0 \u00a0 (1) How to load all model artifacts used during prediction into memory.\u00a0 \u00a0 (2) The logic that should be executed at predict time.\u00a0 \u00a0 When using the default PredictionHandler, the Predictor will be invoked as follows:\u00a0 \u00a0 \u00a0 predictor.postprocess(predictor.predict(predictor.preprocess(prediction_input)))\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 @abstractmethod\u00a0 \u00a0 def load(self, artifacts_uri: str) -> None:\u00a0 \u00a0 \u00a0 \u00a0 \"\"\"Loads the model artifact.\u00a0 \u00a0 \u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 artifacts_uri (str):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Required. The value of the environment variable AIP_STORAGE_URI.\u00a0 \u00a0 \u00a0 \u00a0 \"\"\"\u00a0 \u00a0 \u00a0 \u00a0 pass\u00a0 \u00a0 def preprocess(self, prediction_input: Any) -> Any:\u00a0 \u00a0 \u00a0 \u00a0 \"\"\"Preprocesses the prediction input before doing the prediction.\u00a0 \u00a0 \u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 prediction_input (Any):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Required. The prediction input that needs to be preprocessed.\u00a0 \u00a0 \u00a0 \u00a0 Returns:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 The preprocessed prediction input.\u00a0 \u00a0 \u00a0 \u00a0 \"\"\"\u00a0 \u00a0 \u00a0 \u00a0 return prediction_input\u00a0 \u00a0 @abstractmethod\u00a0 \u00a0 def predict(self, instances: Any) -> Any:\u00a0 \u00a0 \u00a0 \u00a0 \"\"\"Performs prediction.\u00a0 \u00a0 \u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 instances (Any):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Required. The instance(s) used for performing prediction.\u00a0 \u00a0 \u00a0 \u00a0 Returns:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Prediction results.\u00a0 \u00a0 \u00a0 \u00a0 \"\"\"\u00a0 \u00a0 \u00a0 \u00a0 pass\u00a0 \u00a0 def postprocess(self, prediction_results: Any) -> Any:\u00a0 \u00a0 \u00a0 \u00a0 \"\"\"Postprocesses the prediction results.\u00a0 \u00a0 \u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 prediction_results (Any):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Required. The prediction results.\u00a0 \u00a0 \u00a0 \u00a0 Returns:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 The postprocessed prediction results.\u00a0 \u00a0 \u00a0 \u00a0 \"\"\"\u00a0 \u00a0 \u00a0 \u00a0 return prediction_results\n```\nFor example, see [Sklearn's Predictor implementation](https://github.com/googleapis/python-aiplatform/blob/main/google/cloud/aiplatform/prediction/sklearn/predictor.py) .\n### Write custom Handler (optional)\nCustom handlers have access to the raw request object, and thus, are useful in rare cases where you need to customize web server related logic, such as supporting additional request/response headers or deserializing non-JSON formatted prediction requests.\nHere is a [sample notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/prediction/custom_prediction_routines/SDK_Custom_Predict_and_Handler_SDK_Integration.ipynb) that implements both Predictor and Handler.\nAlthough it is not required, for better code organization and reusability, we recommend that you implement the web server logic in the Handler and the ML logic in the Predictor as shown in the default handler.\n### Build custom container\nPut your custom code and an additional `requirements.txt` file, if you need to install any packages in your images, in a directory.\nUse Vertex SDK to build custom containers as shown below:\n```\nfrom google.cloud.aiplatform.prediction import LocalModel# {import your predictor and handler}local_model = LocalModel.build_cpr_model(\u00a0 \u00a0 {PATH_TO_THE_SOURCE_DIR},\u00a0 \u00a0 f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\",\u00a0 \u00a0 predictor={PREDICTOR_CLASS},\u00a0 \u00a0 handler={HANDLER_CLASS},\u00a0 \u00a0 requirements_path={PATH_TO_REQUIREMENTS_TXT},)\n```\nYou can inspect the container's spec to get useful information such as image URI and environment variables.\n```\nlocal_model.get_serving_container_spec()\n```\n### Run the container locally (optional)\nThis step is required only if you want to run and test the container locally which is useful for faster iteration. In the following example, we deploy to local endpoint and send a prediction request (format for [request body](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/predict#request-body) ).\n```\nwith local_model.deploy_to_local_endpoint(\u00a0 \u00a0 artifact_uri={GCS_PATH_TO_MODEL_ARTIFACTS},\u00a0 \u00a0 credential_path={PATH_TO_CREDENTIALS},) as local_endpoint:\u00a0 \u00a0 health_check_response = local_endpoint.run_health_check()\u00a0 \u00a0 predict_response = local_endpoint.predict(\u00a0 \u00a0 \u00a0 \u00a0 request_file={PATH_TO_INPUT_FILE},\u00a0 \u00a0 \u00a0 \u00a0 headers={ANY_NEEDED_HEADERS},\u00a0 \u00a0 )\n```\nPrint out the health check and prediction response.\n```\nprint(health_check_response, health_check_response.content)print(predict_response, predict_response.content)\n```\nPrint out all the container logs.\n```\nlocal_endpoint.print_container_logs(show_all=True)\n```\n### Upload to Vertex AI Model Registry\nYour model will need to access your model artifacts (the files from training), so make sure you've uploaded them to Google Cloud Storage.\nPush the image to the [Artifact Registry](/artifact-registry/docs) .\n```\nlocal_model.push_image()\n```\nThen, upload to Model Registry.\n```\nfrom google.cloud import aiplatformmodel = aiplatform.Model.upload(\u00a0 \u00a0 local_model=local_model,\u00a0 \u00a0 display_name={MODEL_DISPLAY_NAME},\u00a0 \u00a0 artifact_uri={GCS_PATH_TO_MODEL_ARTIFACTS},)\n```\nOnce your model is uploaded to the Vertex AI Model Registry, it may be used to [get batch predictions](/vertex-ai/docs/predictions/batch-predictions) or deployed to a Vertex AI Endpoint to get online predictions.\n### Deploy to Vertex AI endpoint\n```\nendpoint = model.deploy(machine_type=\"n1-standard-4\")\n```\nOnce deployed, you can [get online predictions](/vertex-ai/docs/predictions/get-predictions#get_online_predictions) .\n## Notebook Samples\nThe samples showcase the different ways you can deploy a model with custom pre/post-processing on Vertex AI Prediction.\n- [Custom Predictor with custom pre/post-processing for Sklearn, build your own container with Vertex SDK.](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/prediction/custom_prediction_routines/SDK_Custom_Preprocess.ipynb) - Implement only loading of serialized preprocessor, preprocess, and postprocess methods in the Predictor. Inherit default model loading and predict behavior from Vertex AI distributed SklearnPredictor.\n- [Custom Predictor, build your own container with Vertex SDK.](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/prediction/custom_prediction_routines/SDK_Custom_Predict_SDK_Integration.ipynb) - Custom implementation of the entire Predictor.\n- [Custom Predictor and Handler, build your own container with Vertex SDK.](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/prediction/custom_prediction_routines/SDK_Custom_Predict_and_Handler_SDK_Integration.ipynb) - Custom implementation of Predictor and Handler.\n- Customizing the Handler allows the model server to handle csv inputs.\n- [Custom Predictor, build your own container with Vertex SDK for PyTorch.](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/prediction/custom_prediction_routines/SDK_Pytorch_Custom_Predict.ipynb) - Custom implementation of the Predictor.\n- [Existing image, test prediction locally and deploy models with Vertex SDK.](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/prediction/custom_prediction_routines/SDK_Triton_PyTorch_Local_Prediction.ipynb) - Use NVIDIA Triton inference server for PyTorch models.", "guide": "Vertex AI"}