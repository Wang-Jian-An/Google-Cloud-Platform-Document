{"title": "Vertex AI - Get online predictions from a custom trained model", "url": "https://cloud.google.com/vertex-ai/docs/predictions/get-online-predictions", "abstract": "# Vertex AI - Get online predictions from a custom trained model\nThis page shows you how to get online (real-time) predictions from your custom trained models using the Google Cloud console or the Vertex AI API.\n", "content": "## Format your input for online prediction\nThis section shows how to format and encode your prediction input instances as JSON, which is required if you are using the [predict](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/predict) or [explain](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/explain) method. This is not required if you are [rawPredict](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/rawPredict) method. For information on which method to choose, see [Send request toendpoint](#send-request) .\nIf you're using the Vertex AI SDK for Python to send prediction requests, specify the list of instances without the `instances` field. For example, specify `[ [\"the\",\"quick\",\"brown\"], ... ]` instead of `{ \"instances\": [ [\"the\",\"quick\",\"brown\"], ... ] }` .\nIf your model uses a [custom container](/vertex-ai/docs/predictions/use-custom-container) , your input must be formatted as JSON, and there is an additional `parameters` field that can be used for your container. Learn more about [format prediction inputwith custom containers](/vertex-ai/docs/predictions/custom-container-requirements#prediction) .\n### Format instances as JSON strings\nThe basic format for online prediction is a list of data instances. These can be either plain lists of values or members of a JSON object, depending on how you configured your inputs in your training application. TensorFlow models can accept more complex inputs, while most scikit-learn and XGBoost models expect a list of numbers as input.\nThis example shows an input tensor and an instance key to a TensorFlow model:\n```\n\u00a0{\"values\": [1, 2, 3, 4], \"key\": 1}\n```\nThe makeup of the JSON string can be complex as long as it follows these rules:\n- The top level of instance data must be a JSON object: a dictionary of key-value pairs.\n- Individual values in an instance object can be strings, numbers, or lists. You can't embed JSON objects.\n- Lists must contain only items of the same type (including other lists). You may not mix string and numerical values.\nYou pass input instances for online prediction as the message body for the `projects.locations.endpoints.predict` call. Learn more about the [requestbody's formatting requirements](#request-response-examples) .\nMake each instance an item in a JSON array, and provide the array as the `instances` field of a JSON object. For example:\n```\n{\"instances\": [\u00a0 {\"values\": [1, 2, 3, 4], \"key\": 1},\u00a0 {\"values\": [5, 6, 7, 8], \"key\": 2}]}\n```\n### Encode binary data for prediction input\n**Note:** This following section only applies to prediction with TensorFlow.\nBinary data can't be formatted as the UTF-8 encoded strings that JSON supports. If you have binary data in your inputs, you must use base64 encoding to represent it. The following special formatting is required:\n- Your encoded string must be formatted as a JSON object with a single key named `b64` . In Python 3, base64 encoding outputs a byte sequence. You must convert this to a string to make it JSON serializable:```\n{'image_bytes': {'b64': base64.b64encode(jpeg_data).decode()}}\n```\n- In your TensorFlow model code, you must name the aliases for your binary input and output tensors so that they end with '_bytes'.## Request and response examples\nThis section describes the format of the prediction request body and of the response body, with examples for TensorFlow, scikit-learn, and XGBoost.\n### Request body details\nThe request body contains data with the following structure (JSON representation):\n```\n{\u00a0 \"instances\": [\u00a0 \u00a0 <value>|<simple/nested list>|<object>,\u00a0 \u00a0 ...\u00a0 ]}\n```\nThe `instances[]` object is required, and must contain the list of instances to get predictions for.\nThe structure of each element of the instances list is determined by your model's input definition. Instances can include named inputs (as objects) or can contain only unlabeled values.\nNot all data includes named inputs. Some instances are simple JSON values  (boolean, number, or string). However, instances are often lists of simple  values, or complex nested lists.\nBelow are some examples of request bodies.\nCSV data with each row encoded as a string value:\n```\n{\"instances\": [\"1.0,true,\\\\\"x\\\\\"\", \"-2.0,false,\\\\\"y\\\\\"\"]}\n```\nPlain text:\n```\n{\"instances\": [\"the quick brown fox\", \"the lazy dog\"]}\n```\nSentences encoded as lists of words (vectors of strings):\n```\n{\n \"instances\": [ [\"the\",\"quick\",\"brown\"],\n [\"the\",\"lazy\",\"dog\"],\n ...\n ]\n}\n```\nFloating point scalar values:\n```\n{\"instances\": [0.0, 1.1, 2.2]}\n```\nVectors of integers:\n```\n{\n \"instances\": [ [0, 1, 2],\n [3, 4, 5],\n ...\n ]\n}\n```\nTensors (in this case, two-dimensional tensors):\n```\n{\n \"instances\": [ [  [0, 1, 2],\n  [3, 4, 5]\n ],\n ...\n ]\n}\n```\nImages, which can be represented different ways. In this encoding scheme  the first two dimensions represent the rows and columns of the image, and  the third dimension contains lists (vectors) of the R, G, and B values for  each pixel:\n```\n{\n \"instances\": [ [  [  [138, 30, 66],\n  [130, 20, 56],\n  ...\n  ],\n  [  [126, 38, 61],\n  [122, 24, 57],\n  ...\n  ],\n  ...\n ],\n ...\n ]\n}\n```\n### Data encodingJSON strings must be encoded as UTF-8. To send binary data, you must  base64-encode the data and mark it as binary. To mark a JSON string as  binary, replace it with a JSON object with a single attribute named `b64` :\n```\n{\"b64\": \"...\"} \n```\nThe following example shows two serialized `tf.Examples` instances, requiring base64 encoding (fake data, for illustrative  purposes only):\n```\n{\"instances\": [{\"b64\": \"X5ad6u\"}, {\"b64\": \"IA9j4nx\"}]}\n```\nThe following example shows two JPEG image byte strings, requiring base64  encoding (fake data, for illustrative purposes only):\n```\n{\"instances\": [{\"b64\": \"ASa8asdf\"}, {\"b64\": \"JLK7ljk3\"}]}\n```\n### Multiple input tensorsSome models have an underlying TensorFlow graph that accepts multiple  input tensors. In this case, use the names of JSON name/value  pairs to identify the input tensors.\nFor a graph with input tensor aliases \"tag\" (string) and \"image\"  (base64-encoded string):\n```\n{\n \"instances\": [ {\n  \"tag\": \"beach\",\n  \"image\": {\"b64\": \"ASa8asdf\"}\n },\n {\n  \"tag\": \"car\",\n  \"image\": {\"b64\": \"JLK7ljk3\"}\n }\n ]\n}\n```\nFor a graph with input tensor aliases \"tag\" (string) and \"image\"  (3-dimensional array of 8-bit ints):\n```\n{\n \"instances\": [ {\n  \"tag\": \"beach\",\n  \"image\": [  [   [138, 30, 66],\n   [130, 20, 56],\n   ...\n  ],\n  [   [126, 38, 61],\n   [122, 24, 57],\n   ...\n  ],\n  ...\n  ]\n },\n {\n  \"tag\": \"car\",\n  \"image\": [  [   [255, 0, 102],\n   [255, 0, 97],\n   ...\n  ],\n  [   [254, 1, 101],\n   [254, 2, 93],\n   ...\n  ],\n  ...\n  ]\n },\n ...\n ]\n}\n```\nThe request body contains data with the following structure (JSON representation):\n```\n{\u00a0 \"instances\": [\u00a0 \u00a0 <simple list>,\u00a0 \u00a0 ...\u00a0 ]}\n```\nThe `instances[]` object is required, and must contain the list of instances to get predictions for. In the following example, each input instance is a list of floats:\n```\n{\u00a0 \"instances\": [\u00a0 \u00a0 [0.0, 1.1, 2.2],\u00a0 \u00a0 [3.3, 4.4, 5.5],\u00a0 \u00a0 ...\u00a0 ]}\n```\nThe dimension of input instances must match what your model expects. For example, if your model requires three features, then the length of each input instance must be 3.\nThe request body contains data with the following structure (JSON representation):\n```\n{\u00a0 \"instances\": [\u00a0 \u00a0 <simple list>,\u00a0 \u00a0 ...\u00a0 ]}\n```\nThe `instances[]` object is required, and must contain the list of instances to get predictions for. In the following example, each input instance is a list of floats:\n```\n{\u00a0 \"instances\": [\u00a0 \u00a0 [0.0, 1.1, 2.2],\u00a0 \u00a0 [3.3, 4.4, 5.5],\u00a0 \u00a0 ...\u00a0 ]}\n```\nThe dimension of input instances must match what your model expects. For example, if your model requires three features, then the length of each input instance must be 3.\nVertex AI does not support sparse representation of input instances for XGBoost.\nThe online prediction service interprets zeros and `NaN` s differently. If the value of a feature is zero, use `0.0` in the corresponding input. If the value of a feature is missing, use `\"NaN\"` in the corresponding input.\nThe following example represents a prediction request with a single input instance, where the value of the first feature is 0.0, the value of the second feature is 1.1, and the value of the third feature is missing:\n```\n{\"instances\": [[0.0, 1.1, \"NaN\"]]}\n```\nIf your model uses a [PyTorch prebuiltcontainer](/vertex-ai/docs/predictions/pre-built-containers#pytorch) , TorchServe's default handlers expect each instance to be wrapped in a `data` field. For example:\n```\n{\u00a0 \"instances\": [\u00a0 \u00a0 { \"data\": , <value> },\u00a0 \u00a0 { \"data\": , <value> }\u00a0 ]}\n```\n### Response body details\nIf the call is successful, the response body contains one prediction entry per instance in the request body, given in the same order:\n```\n{\u00a0 \"predictions\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 object\u00a0 \u00a0 }\u00a0 ],\u00a0 \"deployedModelId\": string}\n```\nIf prediction fails for any instance, the response body contains no predictions. Instead, it contains a single error entry:\n```\n{\u00a0 \"error\": string}\n```\nThe `predictions[]` object contains the list of predictions, one for each instance in the request.\nOn error, the `error` string contains a message describing the problem. The error is returned instead of a prediction list if an error occurred while processing any instance.\nEven though there is one prediction per instance, the format of a prediction is not directly related to the format of an instance. Predictions take whatever format is specified in the outputs collection defined in the model. The collection of predictions is returned in a JSON list. Each member of the list can be a simple value, a list, or a JSON object of any complexity. If your model has more than one output tensor, each prediction will be a JSON object containing a name/value pair for each output. The names identify the output aliases in the graph.\n### Response body examples\nThe following examples show some possible responses:- A simple set of predictions for three input instances, where each  prediction is an integer value:```\n{\"predictions\":\n [5, 4, 3],\n \"deployedModelId\": 123456789012345678\n}\n```\n- A more complex set of predictions, each containing two named values that   correspond to output tensors, named `label` and `scores` respectively. The value of `label` is the predicted category (\"car\" or \"beach\") and `scores` contains a list of probabilities for that instance across the possible   categories.```\n{\n \"predictions\": [ {\n  \"label\": \"beach\",\n  \"scores\": [0.1, 0.9]\n },\n {\n  \"label\": \"car\",\n  \"scores\": [0.75, 0.25]\n }\n ],\n \"deployedModelId\": 123456789012345678\n}\n```\n- A response when there is an error processing an input instance:```\n{\"error\": \"Divide by zero\"}\n```\nThe following examples show some possible responses:- A simple set of predictions for three input instances, where each  prediction is an integer value:```\n{\"predictions\":\n [5, 4, 3],\n \"deployedModelId\": 123456789012345678\n}\n```\n- A response when there is an error processing an input instance:```\n{\"error\": \"Divide by zero\"}\n```\nThe following examples show some possible responses:- A simple set of predictions for three input instances, where each  prediction is an integer value:```\n{\"predictions\":\n [5, 4, 3],\n \"deployedModelId\": 123456789012345678\n}\n```\n- A response when there is an error processing an input instance:```\n{\"error\": \"Divide by zero\"}\n```## Send a request to an endpoint\nThere are three ways to send a request:\n- [Prediction request](#predict-request) : send a request to [predict](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/predict) to get an online prediction.\n- [Raw prediction request](#raw-predict-request) : sends a request to [rawPredict](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/rawPredict) , which lets you use an arbitrary HTTP payload rather than following the guidelines described in the [Format your input](#formatting-prediction-input) sections of this page. You might want to get raw predictions if:- You are [using a custom container](/vertex-ai/docs/predictions/use-custom-container) which receives requests and sends responses that differ from the guidelines.\n- You require lower latency.`rawPredict`skips the serialization steps and directly forwards the request to the prediction container.\n- You are serving predictions with [NVIDIATriton](/vertex-ai/docs/predictions/using-nvidia-triton) .\n- [Explanation request](#explain-request) : sends a request to [explain](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/explain) . If you have [configuredyour Model forVertex Explainable AI](/vertex-ai/docs/explainable-ai/configuring-explanations-feature-based) , then you can [get online explanations](/vertex-ai/docs/explainable-ai/getting-explanations) . Online explanation requests have the same format as online prediction requests, and they return similar responses; the only difference is that online explanation responses include feature attributions as well as predictions.\n**Note:** Requests timeout after 60 seconds for both public and private endpoints. To request a timeout longer than 60 seconds, file a support ticket or contact your Google Cloud representative.\n**Note:** If the model is deployed to a public endpoint, each prediction request must be 1.5 MB or smaller. If your requests are larger, you can try the following:- Use a [private endpoint](/vertex-ai/docs/predictions/using-private-endpoints) ,   which does not impose the 1.5 MB limitation and provides lower latency.\n- Encode, resize, and/or compress your data into a smaller format on the   client before sending the request to the prediction server. Then, decode   the data on the prediction server using a [preprocessing function](/blog/topics/developers-practitioners/add-preprocessing-functions-tensorflow-models-and-deploy-vertex-ai) or [Custom Prediction Routine](/vertex-ai/docs/predictions/custom-prediction-routines#write_custom_predictor) .   For example, you can:- Convert floating point numbers to base64-encoded byte arrays, instead of sending them as strings.\n- Compress images to JPEG or PNG, instead of sending uncompressed images.\n- Resize large images before sending them to the prediction server.\n### Send an online prediction request\nThe following example uses the [gcloud ai endpoints predictcommand](/sdk/gcloud/reference/ai/endpoints/predict) :- Write the following JSON object to file in your local environment. The filename does not matter, but for this example name the file `request.json` .```\n{\u00a0\"instances\": INSTANCES}\n```Replace the following:- : A JSON array of instances that you want to get predictions for. The format of each instance depends on which inputs your trained ML model expects. For more information, see [Formatting your input for onlineprediction](#formatting-prediction-input) .\n- Run the following command:```\ngcloud ai endpoints predict ENDPOINT_ID \\\u00a0 --region=LOCATION_ID \\\u00a0 --json-request=request.json\n```Replace the following:- : The ID for the endpoint.\n- : The region where you are using Vertex AI.Before using any of the request data, make the following replacements:- : The region where you are using Vertex AI.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) \n- : The ID for the endpoint.\n- : A JSON array of instances that you want to get predictions for. The format of each instance depends on which inputs your trained ML model expects. For more information, see [Formatting your input for onlineprediction](#formatting-prediction-input) .\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:predict\n```\nRequest JSON body:\n```\n{\n \"instances\": INSTANCES\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:predict\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:predict\" | Select-Object -Expand Content\n```If successful, you receive a JSON response similar to the following. In the response, expect the following replacements:\n- : A JSON array of predictions, one for each instance that you included in the request body.\n- : The ID of the [DeployedModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints#DeployedModel) that served the predictions.```\n{\n \"predictions\": PREDICTIONS,\n \"deployedModelId\": \"DEPLOYED_MODEL_ID\"\n}\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/PredictCustomTrainedModelSample.java) \n```\nimport com.google.cloud.aiplatform.v1.EndpointName;import com.google.cloud.aiplatform.v1.PredictRequest;import com.google.cloud.aiplatform.v1.PredictResponse;import com.google.cloud.aiplatform.v1.PredictionServiceClient;import com.google.cloud.aiplatform.v1.PredictionServiceSettings;import com.google.protobuf.ListValue;import com.google.protobuf.Value;import com.google.protobuf.util.JsonFormat;import java.io.IOException;import java.util.List;public class PredictCustomTrainedModelSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String instance = \"[{ \u201cfeature_column_a\u201d: \u201cvalue\u201d, \u201cfeature_column_b\u201d: \u201cvalue\u201d}]\";\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String endpointId = \"YOUR_ENDPOINT_ID\";\u00a0 \u00a0 predictCustomTrainedModel(project, endpointId, instance);\u00a0 }\u00a0 static void predictCustomTrainedModel(String project, String endpointId, String instance)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 PredictionServiceSettings predictionServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (PredictionServiceClient predictionServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceClient.create(predictionServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 EndpointName endpointName = EndpointName.of(project, location, endpointId);\u00a0 \u00a0 \u00a0 ListValue.Builder listValue = ListValue.newBuilder();\u00a0 \u00a0 \u00a0 JsonFormat.parser().merge(instance, listValue);\u00a0 \u00a0 \u00a0 List<Value> instanceList = listValue.getValuesList();\u00a0 \u00a0 \u00a0 PredictRequest predictRequest =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 PredictRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(endpointName.toString())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addAllInstances(instanceList)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 PredictResponse predictResponse = predictionServiceClient.predict(predictRequest);\u00a0 \u00a0 \u00a0 System.out.println(\"Predict Custom Trained model Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\tDeployed Model Id: %s\\n\", predictResponse.getDeployedModelId());\u00a0 \u00a0 \u00a0 System.out.println(\"Predictions\");\u00a0 \u00a0 \u00a0 for (Value prediction : predictResponse.getPredictionsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\tPrediction: %s\\n\", prediction);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/predict-custom-trained-model.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const filename = \"YOUR_PREDICTION_FILE_NAME\";// const endpointId = \"YOUR_ENDPOINT_ID\";// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';const util = require('util');const {readFile} = require('fs');const readFileAsync = util.promisify(readFile);// Imports the Google Cloud Prediction Service Client libraryconst {PredictionServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst predictionServiceClient = new PredictionServiceClient(clientOptions);async function predictCustomTrainedModel() {\u00a0 // Configure the parent resource\u00a0 const endpoint = `projects/${project}/locations/${location}/endpoints/${endpointId}`;\u00a0 const parameters = {\u00a0 \u00a0 structValue: {\u00a0 \u00a0 \u00a0 fields: {},\u00a0 \u00a0 },\u00a0 };\u00a0 const instanceDict = await readFileAsync(filename, 'utf8');\u00a0 const instanceValue = JSON.parse(instanceDict);\u00a0 const instance = {\u00a0 \u00a0 structValue: {\u00a0 \u00a0 \u00a0 fields: {\u00a0 \u00a0 \u00a0 \u00a0 Age: {stringValue: instanceValue['Age']},\u00a0 \u00a0 \u00a0 \u00a0 Balance: {stringValue: instanceValue['Balance']},\u00a0 \u00a0 \u00a0 \u00a0 Campaign: {stringValue: instanceValue['Campaign']},\u00a0 \u00a0 \u00a0 \u00a0 Contact: {stringValue: instanceValue['Contact']},\u00a0 \u00a0 \u00a0 \u00a0 Day: {stringValue: instanceValue['Day']},\u00a0 \u00a0 \u00a0 \u00a0 Default: {stringValue: instanceValue['Default']},\u00a0 \u00a0 \u00a0 \u00a0 Deposit: {stringValue: instanceValue['Deposit']},\u00a0 \u00a0 \u00a0 \u00a0 Duration: {stringValue: instanceValue['Duration']},\u00a0 \u00a0 \u00a0 \u00a0 Housing: {stringValue: instanceValue['Housing']},\u00a0 \u00a0 \u00a0 \u00a0 Job: {stringValue: instanceValue['Job']},\u00a0 \u00a0 \u00a0 \u00a0 Loan: {stringValue: instanceValue['Loan']},\u00a0 \u00a0 \u00a0 \u00a0 MaritalStatus: {stringValue: instanceValue['MaritalStatus']},\u00a0 \u00a0 \u00a0 \u00a0 Month: {stringValue: instanceValue['Month']},\u00a0 \u00a0 \u00a0 \u00a0 PDays: {stringValue: instanceValue['PDays']},\u00a0 \u00a0 \u00a0 \u00a0 POutcome: {stringValue: instanceValue['POutcome']},\u00a0 \u00a0 \u00a0 \u00a0 Previous: {stringValue: instanceValue['Previous']},\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 };\u00a0 const instances = [instance];\u00a0 const request = {\u00a0 \u00a0 endpoint,\u00a0 \u00a0 instances,\u00a0 \u00a0 parameters,\u00a0 };\u00a0 // Predict request\u00a0 const [response] = await predictionServiceClient.predict(request);\u00a0 console.log('Predict custom trained model response');\u00a0 console.log(`\\tDeployed model id : ${response.deployedModelId}`);\u00a0 const predictions = response.predictions;\u00a0 console.log('\\tPredictions :');\u00a0 for (const prediction of predictions) {\u00a0 \u00a0 console.log(`\\t\\tPrediction : ${JSON.stringify(prediction)}`);\u00a0 }}predictCustomTrainedModel();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/endpoint_predict_sample.py) \n```\ndef endpoint_predict_sample(\u00a0 \u00a0 project: str, location: str, instances: list, endpoint: str):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 endpoint = aiplatform.Endpoint(endpoint)\u00a0 \u00a0 prediction = endpoint.predict(instances=instances)\u00a0 \u00a0 print(prediction)\u00a0 \u00a0 return prediction\n```\n### Send an online raw prediction request\nThe following example uses the [gcloud ai endpoints raw-predictcommand](/sdk/gcloud/reference/ai/endpoints/raw-predict) :- To request predictions with the JSON object inspecified on the command line:```\n\u00a0gcloud ai endpoints raw-predict ENDPOINT_ID \u00a0--region=LOCATION \u00a0--request REQUEST\u00a0\n```\n- To request predictions with an image stored in the file and the appropriate `Content-Type` header:```\n\u00a0gcloud ai endpoints raw-predict ENDPOINT_ID \u00a0--region=LOCATION \u00a0--http-headers=Content-Type=image/jpeg \u00a0--request @image.jpeg\u00a0\n```Replace the following:- : The ID for the endpoint.\n- : The region where you are using Vertex AI.\n- : The contents of the request that you want to get predictions for. The format of the request depends on what your custom container expects, which may not necessarily be a JSON object.\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/generated_samples/aiplatform_generated_aiplatform_v1_prediction_service_raw_predict_sync.py) \n```\n# -*- coding: utf-8 -*-# Copyright 2020 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.\n## Generated code. DO NOT EDIT!\n## Snippet for RawPredict# NOTE: This snippet has been automatically generated for illustrative purposes only.# It may require modifications to work in your environment.# To install the latest published package dependency, execute the following:# \u00a0 python3 -m pip install google-cloud-aiplatformfrom google.cloud import aiplatform_v1def sample_raw_predict():\u00a0 \u00a0 # Create a client\u00a0 \u00a0 client = aiplatform_v1.PredictionServiceClient()\u00a0 \u00a0 # Initialize request argument(s)\u00a0 \u00a0 request = aiplatform_v1.RawPredictRequest(\u00a0 \u00a0 \u00a0 \u00a0 endpoint=\"endpoint_value\",\u00a0 \u00a0 )\u00a0 \u00a0 # Make the request\u00a0 \u00a0 response = client.raw_predict(request=request)\u00a0 \u00a0 # Handle the response\u00a0 \u00a0 print(response)\n```\nThe response includes the following HTTP headers:\n- `X-Vertex-AI-Endpoint-Id` : ID of the `Endpoint` that served this prediction.\n- `X-Vertex-AI-Deployed-Model-Id` : ID of the Endpoint's `DeployedModel` that served this prediction.\n**Note:** If your container receives requests or sends responses that differ from the guidelines, then the container is not compatible with [Vertex Explainable AI](/vertex-ai/docs/explainable-ai/overview) or [Vertex AI Model Monitoring](/vertex-ai/docs/model-monitoring/overview) .\n### Send an online explanation request\nThe following example uses the [gcloud ai endpoints explaincommand](/sdk/gcloud/reference/ai/endpoints/explain) :- Write the following JSON object to file in your local environment. The filename does not matter, but for this example name the file `request.json` .```\n{\u00a0\"instances\": INSTANCES}\n```Replace the following:- : A JSON array of instances that you want to get predictions for. The format of each instance depends on which inputs your trained ML model expects. For more information, see [Formatting your input for onlineprediction](#formatting-prediction-input) .\n- Run the following command:```\ngcloud ai endpoints explain ENDPOINT_ID \\\u00a0 --region=LOCATION_ID \\\u00a0 --json-request=request.json\n```Replace the following:- : The ID for the endpoint.\n- : The region where you are using Vertex AI.\nOptionally, if you want to send an explanation request to a specific `DeployedModel` on the `Endpoint` , you can specify the `--deployed-model-id` flag:```\ngcloud ai endpoints explain ENDPOINT_ID \\\u00a0 --region=LOCATION \\\u00a0 --deployed-model-id=DEPLOYED_MODEL_ID \\\u00a0 --json-request=request.json\n```In addition to the placeholders described previously, replace the following:- Optional: The ID of the deployed model for which you want to get explanations. The ID is included in the`predict`method's response. If you need to request explanations for a particular model and you have more than one model deployed to the same endpoint, you can use this ID to ensure that the explanations are returned for that particular model.Before using any of the request data, make the following replacements:- : The region where you are using Vertex AI.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) \n- : The ID for the endpoint.\n- : A JSON array of instances that you want to get predictions for. The format of each instance depends on which inputs your trained ML model expects. For more information, see [Formatting your input for onlineprediction](#formatting-prediction-input) .\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:explain\n```\nRequest JSON body:\n```\n{\n \"instances\": INSTANCES\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:explain\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:explain\" | Select-Object -Expand Content\n```If successful, you receive a JSON response similar to the following. In the response, expect the following replacements:\n- : A JSON array of predictions, one for each instance that you included in the request body.\n- : A JSON array of [explanations](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/explain#Explanation) , one for  each prediction.\n- : The ID of the [DeployedModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints#DeployedModel) that served the predictions.```\n{\n \"predictions\": PREDICTIONS,\n \"explanations\": EXPLANATIONS,\n \"deployedModelId\": \"DEPLOYED_MODEL_ID\"\n}\n```\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/explain_tabular_sample.py) \n```\ndef explain_tabular_sample(\u00a0 \u00a0 project: str, location: str, endpoint_id: str, instance_dict: Dict):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 endpoint = aiplatform.Endpoint(endpoint_id)\u00a0 \u00a0 response = endpoint.explain(instances=[instance_dict], parameters={})\u00a0 \u00a0 for explanation in response.explanations:\u00a0 \u00a0 \u00a0 \u00a0 print(\" explanation\")\u00a0 \u00a0 \u00a0 \u00a0 # Feature attributions.\u00a0 \u00a0 \u00a0 \u00a0 attributions = explanation.attributions\u00a0 \u00a0 \u00a0 \u00a0 for attribution in attributions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\" \u00a0attribution\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\" \u00a0 baseline_output_value:\", attribution.baseline_output_value)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\" \u00a0 instance_output_value:\", attribution.instance_output_value)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\" \u00a0 output_display_name:\", attribution.output_display_name)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\" \u00a0 approximation_error:\", attribution.approximation_error)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\" \u00a0 output_name:\", attribution.output_name)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output_index = attribution.output_index\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for output_index in output_index:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\" \u00a0 output_index:\", output_index)\u00a0 \u00a0 for prediction in response.predictions:\u00a0 \u00a0 \u00a0 \u00a0 print(prediction)\n```\n## What's next\n- Learn about [Online prediction logging](/vertex-ai/docs/predictions/online-prediction-logging) .", "guide": "Vertex AI"}