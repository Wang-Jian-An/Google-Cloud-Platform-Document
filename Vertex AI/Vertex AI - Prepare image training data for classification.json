{"title": "Vertex AI - Prepare image training data for classification", "url": "https://cloud.google.com/vertex-ai/docs/image-data/classification/prepare-data", "abstract": "# Vertex AI - Prepare image training data for classification\nThe following objective sections include information about data requirements, the input/output schema file, and the format of the data import files ( [JSON Lines](https://jsonlines.org/) & CSV) that are defined by the schema.\n#", "content": "## Data requirements\n- **Training data** : The following image formats are supported when training your model.  After Vertex AI API preprocesses these imported images they serve as the data used to train  a model. Maximum file size is 30MB.\n- JPEG\n- GIF\n- PNG\n- BMP\n- ICO\n- **Prediction data** : The following image formats are supported when requesting a  prediction from (querying) your model. Maximum file size is 1.5MB.\n- JPEG\n- GIF\n- PNG\n- WEBP\n- BMP\n- TIFF\n- ICO\n- **Note** : The Vertex AI API currently only  supports sending base64-encoded image content to the predict  method.## Best practices for image data used to train AutoML models\n- The following best practices apply to datasets that train models using  AutoML.\n- AutoML models are optimized for photographs of objects  in the real world.\n- The training data should be as close as possible to the data on which  predictions are to be made. For example, if your use case involves blurry  and low-resolution images (such as from a security camera), your training  data should be composed of blurry, low-resolution images. In general, you  should also consider providing multiple angles, resolutions, and  backgrounds for your training images.\n- Vertex AI models can't generally predict labels that humans  can't assign. So, if a human can't be trained to assign labels by looking  at the image for 1-2 seconds, the model likely can't be trained to do it  either.\n- We recommend about 1000 training images per label. The minimum per  label is 10. In general it takes more examples  per label to train models with multiple labels per image, and resulting  scores are harder to interpret.\n- The model works best when there are at most 100x more images for the  most common label than for the least common label. We recommend removing  very low frequency labels.\n- Consider including a **None_of_the_above** label and images that  don't match any of your defined labels. For example, for a flower  dataset, include images of flowers outside of your labeled varieties,  and label them as **None_of_the_above** .\n### YAML schema fileUse the following publicly accessible schema file to import single-label image classification annotations. This schema file dictates the format of the data input files. This file's structure follows the [OpenAPI schema](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.2.md#schema) .\n [gs://google-cloud-aiplatform/schema/dataset/ioformat/image_classification_single_label_io_format_1.0.0.yaml](https://storage.cloud.google.com/google-cloud-aiplatform/schema/dataset/ioformat/image_classification_single_label_io_format_1.0.0.yaml) \n### Input files\nJSON on each line:\n```\n{\n \"imageGcsUri\": \"gs://bucket/filename.ext\",\n \"classificationAnnotation\": {\n \"displayName\": \"LABEL\",\n \"annotationResourceLabels\": {\n  \"aiplatform.googleapis.com/annotation_set_name\": \"displayName\",\n  \"env\": \"prod\"\n  }\n },\n \"dataItemResourceLabels\": {\n \"aiplatform.googleapis.com/ml_use\": \"training/test/validation\"\n }\n}\n```\n **Field notes** :- `imageGcsUri`- The only required field.\n- `annotationResourceLabels`- Can contain any number of   key-value string pairs. The only system-reserved key-value pair is   the following:- \"aiplatform.googleapis.com/annotation_set_name\" : \"\"\nWhere is one of the display names of the   existing annotation sets in the dataset.\n- `dataItemResourceLabels`- Can contain any number of   key-value string pairs. The only system-reserved key-value pair is   the following which specifies the machine learning use set of the   data item:- \"aiplatform.googleapis.com/ml_use\" : \"\"```\n{\"imageGcsUri\": \"gs://bucket/filename1.jpeg\", \"classificationAnnotation\": {\"displayName\": \"daisy\"}, \"dataItemResourceLabels\": {\"aiplatform.googleapis.com/ml_use\": \"test\"}}\n{\"imageGcsUri\": \"gs://bucket/filename2.gif\", \"classificationAnnotation\": {\"displayName\": \"dandelion\"}, \"dataItemResourceLabels\": {\"aiplatform.googleapis.com/ml_use\": \"training\"}}\n{\"imageGcsUri\": \"gs://bucket/filename3.png\", \"classificationAnnotation\": {\"displayName\": \"roses\"}, \"dataItemResourceLabels\": {\"aiplatform.googleapis.com/ml_use\": \"training\"}}\n{\"imageGcsUri\": \"gs://bucket/filename4.bmp\", \"classificationAnnotation\": {\"displayName\": \"sunflowers\"}, \"dataItemResourceLabels\": {\"aiplatform.googleapis.com/ml_use\": \"training\"}}\n{\"imageGcsUri\": \"gs://bucket/filename5.tiff\", \"classificationAnnotation\": {\"displayName\": \"tulips\"}, \"dataItemResourceLabels\": {\"aiplatform.googleapis.com/ml_use\": \"validation\"}}\n...\n```\nCSV format:\n```\n[ML_USE],GCS_FILE_PATH,[LABEL]\n```\n **List of columns** - `ML_USE`(Optional) - For data split purposes when training a   model. Use TRAINING, TEST, or VALIDATION. For more information about manual data   splitting, see [About data splits for AutoML models](/vertex-ai/docs/general/ml-use) .\n- `GCS_FILE_PATH`- This field contains the Cloud Storage URI for   the image. Cloud Storage URIs are case-sensitive.\n- `LABEL`(Optional) - Labels must start with a letter and only contain   letters, numbers, and underscores.\n```\ntest,gs://bucket/filename1.jpeg,daisy\ntraining,gs://bucket/filename2.gif,dandelion\ngs://bucket/filename3.png\ngs://bucket/filename4.bmp,sunflowers\nvalidation,gs://bucket/filename5.tiff,tulips\n...\n \n```### Data requirements\n- **Training data** : The following image formats are supported when training your model.  After Vertex AI API preprocesses these imported images they serve as the data used to train  a model. Maximum file size is 30MB.\n- JPEG\n- GIF\n- PNG\n- BMP\n- ICO\n- **Prediction data** : The following image formats are supported when requesting a  prediction from (querying) your model. Maximum file size is 1.5MB.\n- JPEG\n- GIF\n- PNG\n- WEBP\n- BMP\n- TIFF\n- ICO\n- **Note** : The Vertex AI API currently only  supports sending base64-encoded image content to the predict  method.## Best practices for image data used to train AutoML models\n- The following best practices apply to datasets that train models using  AutoML.\n- AutoML models are optimized for photographs of objects  in the real world.\n- The training data should be as close as possible to the data on which  predictions are to be made. For example, if your use case involves blurry  and low-resolution images (such as from a security camera), your training  data should be composed of blurry, low-resolution images. In general, you  should also consider providing multiple angles, resolutions, and  backgrounds for your training images.\n- Vertex AI models can't generally predict labels that humans  can't assign. So, if a human can't be trained to assign labels by looking  at the image for 1-2 seconds, the model likely can't be trained to do it  either.\n- We recommend about 1000 training images per label. The minimum per  label is 10. In general it takes more examples  per label to train models with multiple labels per image, and resulting  scores are harder to interpret.\n- The model works best when there are at most 100x more images for the  most common label than for the least common label. We recommend removing  very low frequency labels.\n- Consider including a **None_of_the_above** label and images that  don't match any of your defined labels. For example, for a flower  dataset, include images of flowers outside of your labeled varieties,  and label them as **None_of_the_above** .\n### YAML schema fileUse the following publicly accessible schema file to import multi-label image classification annotations. This schema file dictates the format of the data input files. This file's structure follows the [OpenAPI schema](https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.2.md#schema) .\n [gs://google-cloud-aiplatform/schema/dataset/ioformat/image_classification_multi_label_io_format_1.0.0.yaml ](https://storage.cloud.google.com/google-cloud-aiplatform/schema/dataset/ioformat/image_classification_multi_label_io_format_1.0.0.yaml) \n### Input files\nJSON on each line:\n```\n{\n \"imageGcsUri\": \"gs://bucket/filename.ext\",\n \"classificationAnnotations\": [ {\n  \"displayName\": \"LABEL1\",\n  \"annotationResourceLabels\": {\n  \"aiplatform.googleapis.com/annotation_set_name\":\"displayName\",\n  \"label_type\": \"flower_type\"\n  }\n },\n {\n  \"displayName\": \"LABEL2\",\n  \"annotationResourceLabels\": {\n  \"aiplatform.googleapis.com/annotation_set_name\":\"displayName\",\n  \"label_type\": \"image_shot_type\"\n  }\n }\n ],\n \"dataItemResourceLabels\": {\n \"aiplatform.googleapis.com/ml_use\": \"training/test/validation\"\n }\n}\n```\n **Field notes** :- `imageGcsUri`- The only required field.\n- `annotationResourceLabels`- Can contain any number of   key-value string pairs. The only system-reserved key-value pair is   the following:- \"aiplatform.googleapis.com/annotation_set_name\" : \"\"\nWhere is one of the display names of the   existing annotation sets in the dataset.\n- `dataItemResourceLabels`- Can contain any number of   key-value string pairs. The only system-reserved key-value pair is   the following which specifies the machine learning use set of the   data item:- \"aiplatform.googleapis.com/ml_use\" : \"\"```\n{\"imageGcsUri\": \"gs://bucket/filename1.jpeg\", \"classificationAnnotations\": [{\"displayName\": \"daisy\"}, {\"displayName\": \"full_shot\"}], \"dataItemResourceLabels\": {\"aiplatform.googleapis.com/ml_use\": \"test\"}}\n{\"imageGcsUri\": \"gs://bucket/filename2.gif\", \"classificationAnnotations\": [{\"displayName\": \"dandelion\"}, {\"displayName\": \"medium_shot\"}], \"dataItemResourceLabels\": {\"aiplatform.googleapis.com/ml_use\": \"training\"}}\n{\"imageGcsUri\": \"gs://bucket/filename3.png\", \"classificationAnnotations\": [{\"displayName\": \"roses\"}, {\"displayName\": \"extreme_closeup\"}], \"dataItemResourceLabels\": {\"aiplatform.googleapis.com/ml_use\": \"training\"}}\n{\"imageGcsUri\": \"gs://bucket/filename4.bmp\", \"classificationAnnotations\": [{\"displayName\": \"sunflowers\"}, {\"displayName\": \"closeup\"}], \"dataItemResourceLabels\": {\"aiplatform.googleapis.com/ml_use\": \"training\"}}\n{\"imageGcsUri\": \"gs://bucket/filename5.tiff\", \"classificationAnnotations\": [{\"displayName\": \"tulips\"}, {\"displayName\": \"extreme_closeup\"}], \"dataItemResourceLabels\": {\"aiplatform.googleapis.com/ml_use\": \"validation\"}}\n...\n```\nCSV format:\n```\n[ML_USE],GCS_FILE_PATH,[LABEL1,LABEL2,...LABELn]\n```\n **List of columns** - `ML_USE`(Optional) - For data split purposes when training a   model. Use TRAINING, TEST, or VALIDATION. For more information about manual data   splitting, see [About data splits for AutoML models](/vertex-ai/docs/general/ml-use) .\n- `GCS_FILE_PATH`- This field contains the Cloud Storage URI for   the image. Cloud Storage URIs are case-sensitive.\n- `LABEL`(Optional) - Labels must start with a letter and only contain   letters, numbers, and underscores.\n```\ntest,gs://bucket/filename1.jpeg,daisy,full_shot\ntraining,gs://bucket/filename2.gif,dandelion,medium_shot\ngs://bucket/filename3.png\ngs://bucket/filename4.bmp,sunflowers,closeup\nvalidation,gs://bucket/filename5.tiff,tulips,extreme_closeup\n...\n \n```", "guide": "Vertex AI"}