{"title": "Vertex AI - Export AutoML tabular models", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Export AutoML tabular models\nThis page describes how to use Vertex AI to export your AutoML tabular model to Cloud Storage, download the model to an on-premises server or a server hosted by another cloud provider, and then use Docker to make the model available for predictions.\nFor information about exporting image and video Edge models, see [Export AutoML Edge models](/vertex-ai/docs/export/export-edge-model) .\nAfter exporting your tabular model, if you want to import it back into Vertex AI, see [Import models to Vertex AI](/vertex-ai/docs/model-registry/import-model) .\n", "content": "## Limitations\nExporting AutoML tabular models has the following limitations:\n- You can export AutoML tabular classification and regression models only. Exporting AutoML tabular forecasting models is not supported.\n- Vertex Explainable AI is not available using exported tabular models. If you need to use Vertex Explainable AI, you must serve predictions from a model hosted by Vertex AI.\n- The exported tabular model can run only on x86 architecture CPUs that support Advanced Vector Extensions (AVX) instruction sets.## Export process\nThe steps for exporting your model are:\n- [Set up your environment](#before-you-begin) .\n- [Export the model](#export) .\n- [Pull and run the model server](#pull-and-run-server) .\n- [Request predictions](#get-predictions) .## Before you begin\nBefore you can complete this task, you must have completed the following tasks:\n- Set up your project as described in [Setting up the cloud environment](/vertex-ai/docs/start/cloud-environment) .\n- [Train the model](/vertex-ai/docs/training/automl-console) that you want to download.\n- Install and initialize the [Google Cloud CLI](/sdk/docs) on the server you will use to run the exported model.\n- Install [Docker](https://hub.docker.com/search/?type=edition&offering=community) on your server.## Export the model\n- In the Google Cloud console, in the Vertex AI section, go to the **Models** page. [Go to the Models page](https://console.cloud.google.com/vertex-ai/models) \n- Click the tabular model you want to export to open its details page.\n- Click **Export** in the button bar to export your model.\n- Select or create a Cloud Storage folder in the desired location.The bucket must meet the [bucket requirements](/vertex-ai/docs/general/locations#buckets) .You cannot export a model to a top-level bucket. You must use at least one level of folder.For best results, create a new, empty folder. You will copy the entire contents of the folder in a later step.\n- Click **Export** .You will download the exported model to your server in the next section.You use the\n [models.export](/vertex-ai/docs/reference/rest/v1/projects.locations.models/export) \nmethod to export a model to Cloud Storage.\nBefore using any of the request data, make the following replacements:- : Your region.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : the ID of the model you want to export.\n- : your destination folder in  Cloud Storage. For example,`gs://export-bucket/exports`.You cannot export a model to a top-level bucket. You must use at least  one level of folder.The folder must conform to the [bucket requirements](/vertex-ai/docs/general/locations#buckets) .For best results, create a new folder. You will copy the entire contents of the folder  in a later step.\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID:export\n```\nRequest JSON body:\n```\n{\n \"outputConfig\": {\n \"exportFormatId\": \"tf-saved-model\",\n \"artifactDestination\": {\n  \"outputUriPrefix\": \"GCS_DESTINATION\"\n }\n }\n}\n```\nTo send your request, choose one of these options:\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID:export\"\n```Save the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID:export\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION/models/MODEL_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.ExportModelOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2020-10-12T20:53:40.130785Z\",\n  \"updateTime\": \"2020-10-12T20:53:40.130785Z\"\n },\n \"outputInfo\": {\n  \"artifactOutputUri\": \"gs://OUTPUT_BUCKET/model-MODEL_ID/EXPORT_FORMAT/YYYY-MM-DDThh:mm:ss.sssZ\"\n }\n }\n}\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/ExportModelTabularClassificationSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.ExportModelOperationMetadata;import com.google.cloud.aiplatform.v1.ExportModelRequest;import com.google.cloud.aiplatform.v1.ExportModelResponse;import com.google.cloud.aiplatform.v1.GcsDestination;import com.google.cloud.aiplatform.v1.ModelName;import com.google.cloud.aiplatform.v1.ModelServiceClient;import com.google.cloud.aiplatform.v1.ModelServiceSettings;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class ExportModelTabularClassificationSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws InterruptedException, ExecutionException, TimeoutException, IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String gcsDestinationOutputUriPrefix = \"gs://your-gcs-bucket/destination_path\";\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String modelId = \"YOUR_MODEL_ID\";\u00a0 \u00a0 exportModelTableClassification(gcsDestinationOutputUriPrefix, project, modelId);\u00a0 }\u00a0 static void exportModelTableClassification(\u00a0 \u00a0 \u00a0 String gcsDestinationOutputUriPrefix, String project, String modelId)\u00a0 \u00a0 \u00a0 throws IOException, ExecutionException, InterruptedException, TimeoutException {\u00a0 \u00a0 ModelServiceSettings modelServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 ModelServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (ModelServiceClient modelServiceClient = ModelServiceClient.create(modelServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 ModelName modelName = ModelName.of(project, location, modelId);\u00a0 \u00a0 \u00a0 GcsDestination.Builder gcsDestination = GcsDestination.newBuilder();\u00a0 \u00a0 \u00a0 gcsDestination.setOutputUriPrefix(gcsDestinationOutputUriPrefix);\u00a0 \u00a0 \u00a0 ExportModelRequest.OutputConfig outputConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ExportModelRequest.OutputConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setExportFormatId(\"tf-saved-model\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setArtifactDestination(gcsDestination)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 OperationFuture<ExportModelResponse, ExportModelOperationMetadata> exportModelResponseFuture =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelServiceClient.exportModelAsync(modelName, outputConfig);\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Operation name: %s\\n\", exportModelResponseFuture.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to finish...\");\u00a0 \u00a0 \u00a0 ExportModelResponse exportModelResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 exportModelResponseFuture.get(300, TimeUnit.SECONDS);\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Export Model Tabular Classification Response: %s\", exportModelResponse.toString());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/export-model-tabular-classification.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const gcsDestinationOutputUriPrefix ='YOUR_GCS_DESTINATION_\\// OUTPUT_URI_PREFIX'; eg. \"gs://<your-gcs-bucket>/destination_path\"// const modelId = 'YOUR_MODEL_ID';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Model Service Client libraryconst {ModelServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst modelServiceClient = new ModelServiceClient(clientOptions);async function exportModelTabularClassification() {\u00a0 // Configure the name resources\u00a0 const name = `projects/${project}/locations/${location}/models/${modelId}`;\u00a0 // Configure the outputConfig resources\u00a0 const outputConfig = {\u00a0 \u00a0 exportFormatId: 'tf-saved-model',\u00a0 \u00a0 artifactDestination: {\u00a0 \u00a0 \u00a0 outputUriPrefix: gcsDestinationOutputUriPrefix,\u00a0 \u00a0 },\u00a0 };\u00a0 const request = {\u00a0 \u00a0 name,\u00a0 \u00a0 outputConfig,\u00a0 };\u00a0 // Export Model request\u00a0 const [response] = await modelServiceClient.exportModel(request);\u00a0 console.log(`Long running operation : ${response.name}`);\u00a0 // Wait for operation to complete\u00a0 await response.promise();\u00a0 console.log(`Export model response : ${JSON.stringify(response.result)}`);}exportModelTabularClassification();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/snippets/model_service/export_model_tabular_classification_sample.py) \n```\nfrom google.cloud import aiplatform_v1beta1def export_model_tabular_classification_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 model_id: str,\u00a0 \u00a0 gcs_destination_output_uri_prefix: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\u00a0 \u00a0 timeout: int = 300,):\u00a0 \u00a0 # The AI Platform services require regional API endpoints.\u00a0 \u00a0 client_options = {\"api_endpoint\": api_endpoint}\u00a0 \u00a0 # Initialize client that will be used to create and send requests.\u00a0 \u00a0 # This client only needs to be created once, and can be reused for multiple requests.\u00a0 \u00a0 client = aiplatform_v1beta1.ModelServiceClient(client_options=client_options)\u00a0 \u00a0 gcs_destination = {\"output_uri_prefix\": gcs_destination_output_uri_prefix}\u00a0 \u00a0 output_config = {\u00a0 \u00a0 \u00a0 \u00a0 \"artifact_destination\": gcs_destination,\u00a0 \u00a0 \u00a0 \u00a0 \"export_format_id\": \"tf-saved-model\",\u00a0 \u00a0 }\u00a0 \u00a0 name = client.model_path(project=project, location=location, model=model_id)\u00a0 \u00a0 response = client.export_model(name=name, output_config=output_config)\u00a0 \u00a0 print(\"Long running operation:\", response.operation.name)\u00a0 \u00a0 print(\"output_info:\", response.metadata.output_info)\u00a0 \u00a0 export_model_response = response.result(timeout=timeout)\u00a0 \u00a0 print(\"export_model_response:\", export_model_response)\n```### Get the status of an export operation\nSome requests start long-running operations that require time to complete. These requests return an operation name, which you can use to view the operation's status or cancel the operation. Vertex AI provides helper methods to make calls against long-running operations. For more information, see [Working with long-runningoperations](/vertex-ai/docs/general/long-running-operations) .\n## Pull and run the model server\nIn this task, you will download your exported model from Cloud Storage and start the Docker container, so your model is ready to receive prediction requests.\nTo pull and run the model server:\n- On the machine where you will run the model, change to the directory where you want to save the exported model.\n- Download the exported model:```\ngsutil cp -r <var>gcs-destination</var> .\n```Where is the path to the location of the exported model in Cloud Storage.The model is copied to your current directory, under the following path:`./model-<model-id>/tf-saved-model/<export-timestamp>`The path may contain either `tf-saved-model` or `custom-trained` .\n- Rename the directory so the timestamp is removed.```\nmv model-<model-id>/tf-saved-model/<export-timestamp> model-<model-id>/tf-saved-model/<new-dir-name>\n```The timestamp makes the directory invalid for Docker.\n- Pull the model server Docker image.```\nsudo docker pull MODEL_SERVER_IMAGE\n```The model server image to pull is located in the `environment.json` file in exported model directory. It should have the following path:`./model-<model-id>/tf-saved-model/<new-dir-name>/environment.json`If no environment.json file is present, use:`` `-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server-v1`Replace `` with `us` , `europe` , or `asia` to select which Docker repository you want to pull the Docker image from. Each repository provides the same Docker image, but choosing the [Artifact Registrymulti-region](/artifact-registry/docs/repo-locations#location-mr) closest to the machine where you are running Docker might reduce latency.\n- Start the Docker container, using the directory name you just created:```\ndocker run -v `pwd`/model-<model-id>/tf-saved-model/<new-dir-name>:/models/default -p 8080:8080 -it MODEL_SERVER_IMAGE\n```\nYou can stop the model server at any time by using `Ctrl-C` .\n### Update the model server docker container\nBecause you download the model server Docker container when you export the model, you must explicitly update the model server to get updates and bug fixes. You should update the model server periodically, using the following command:\n```\ndocker pull MODEL_SERVER_IMAGE\n```\nMake sure the Docker image URI matches the URI of the Docker image that you pulled previously.\n## Get predictions from the exported model\nThe model server in the Vertex AI image container handles prediction requests and returns prediction results.\nBatch prediction is not available for exported models.\n### Prediction data format\nYou provide the data ( `payload` field) for your prediction request the following JSON format:\n```\n{ \"instances\": [ { \"column_name_1\": value, \"column_name_2\": value, \u2026 } , \u2026 ] }\n```\n**Note:** Exported Vertex AI models do not validate the data format of prediction requests. This means that an otherwise well-formed prediction request with missing columns, errors in the column names, or even a request for a completely different model returns a (flawed or potentially meaningless) prediction. Make sure your prediction requests to exported models are correct.\nThe following example shows a request with three columns: a categorical column, a numeric array, and a struct. The request includes two rows.\n```\n{\n \"instances\": [ {\n  \"categorical_col\": \"mouse\",\n  \"num_array_col\": [  1,\n  2,\n  3\n  ],\n  \"struct_col\": {\n  \"foo\": \"piano\",\n  \"bar\": \"2019-05-17T23:56:09.05Z\"\n  }\n },\n {\n  \"categorical_col\": \"dog\",\n  \"num_array_col\": [  5,\n  6,\n  7\n  ],\n  \"struct_col\": {\n  \"foo\": \"guitar\",\n  \"bar\": \"2019-06-17T23:56:09.05Z\"\n  }\n }\n ]\n}\n```\n### Make the prediction request\n- Put your request data into a text file, for example, `tmp/request.json` .The number of rows of data in the prediction request, called the , affects the prediction latency and throughput. The larger the mini- batch size, the higher the latency and throughput. For reduced latency, use a smaller mini-batch size. For increased throughput, increase the mini-batch size. The most commonly used mini-batch sizes are 1, 32, 64, 128, 256, 512, and 1024.\n- Request the prediction:```\ncurl -X POST --data @/tmp/request.json http://localhost:8080/predict\n```\n### Prediction results format\nThe format of your results depends on your model objective.\nPrediction results for classification models (binary and multi-class) return a probability score for each potential value of the target column. You must determine how you want to use the scores. For example, to get a binary classification from the provided scores, you would identify a threshold value. If there are two classes, \"A\" and \"B\", you should classify the example as \"A\" if the score for \"A\" is greater than the chosen threshold, and \"B\" otherwise. For imbalanced datasets, the threshold might approach 100% or 0%.\nThe results payload for a classification model look similar to this example:\n```\n{\n \"predictions\": [ {\n  \"scores\": [  0.539999994635582,\n  0.2599999845027924,\n  0.2000000208627896\n  ],\n  \"classes\": [  \"apple\",\n  \"orange\",\n  \"grape\"\n  ]\n },\n {\n  \"scores\": [  0.23999999463558197,\n  0.35999998450279236,\n  0.40000002086278963\n  ],\n  \"classes\": [  \"apple\",\n  \"orange\",\n  \"grape\"\n  ]\n }\n ]\n}\n```\nA predicted value is returned for each valid row of the prediction request. Prediction intervals are not returned for exported models.\nThe results payload for a regression model look similar to this example:\n```\n{\n \"predictions\": [ {\n  \"value\": -304.3663330078125,\n  \"lower_bound\": -56.32196807861328,\n  \"upper_bound\": 126.51904296875\n },\n {\n  \"value\": -112.3663330078125,\n  \"lower_bound\": 16.32196807861328,\n  \"upper_bound\": 255.51904296875\n }\n ]\n}\n```\nWhat's next\n- Learn how to [import your exported tabular model back into Vertex AI](/vertex-ai/docs/model-registry/import-model) .", "guide": "Vertex AI"}