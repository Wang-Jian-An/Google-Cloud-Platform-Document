{"title": "Vertex AI - Trigger a pipeline run with Cloud Pub/Sub", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Trigger a pipeline run with Cloud Pub/Sub\nThe following code samples show you how to write, deploy, and trigger a pipeline using an [Event-Driven Cloud Function](/functions/docs/writing#event-driven_functions) with a [Cloud Pub/Sub trigger](/functions/docs/calling/pubsub) .\n#", "content": "## Build and compile a simple Pipeline\nUsing Kubeflow Pipelines SDK, build a scheduled pipeline and compile it into a YAML file.\nSample `hello-world-scheduled-pipeline` :\n```\nfrom kfp import compilerfrom kfp import dsl# A simple component that prints and returns a greeting string@dsl.componentdef hello_world(message: str) -> str:\u00a0 \u00a0 greeting_str = f'Hello, {message}'\u00a0 \u00a0 print(greeting_str)\u00a0 \u00a0 return greeting_str# A simple pipeline that contains a single hello_world task@dsl.pipeline(\u00a0 \u00a0 name='hello-world-scheduled-pipeline')def hello_world_scheduled_pipeline(greet_name: str):\u00a0 \u00a0 hello_world_task = hello_world(greet_name)# Compile the pipeline and generate a YAML filecompiler.Compiler().compile(pipeline_func=hello_world_scheduled_pipeline,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 package_path='hello_world_scheduled_pipeline.yaml')\n```\n### Upload compiled pipeline YAML to Cloud Storage bucket\n- Open the Cloud Storage browser in the Google Cloud console. [Cloud Storage Browser](https://console.cloud.google.com/storage/browser/) \n- Click the Cloud Storage bucket you created when you [configured your project](/vertex-ai/docs/pipelines/configure-project) .\n- Using either an existing folder or a new folder, upload your compiled pipeline YAML (in this example `hello_world_scheduled_pipeline.yaml` ) to the selected folder.\n- Click the uploaded YAML file to access the details. Copy the **gsutil URI** for later use.\n### Create a Cloud Function with Pub/Sub Trigger\n- Visit the Cloud Functions page in the console. [Go to the Cloud Functions page](https://console.cloud.google.com/functions) \n- Click the **Create function** button.\n- In the **Basics** section, give your function a name (for example `my-scheduled-pipeline-function` ).\n- In the **Trigger** section, select **Cloud Pub/Sub** as the Trigger type.\n- In the **Select a Cloud Pub/Sub topic** dropdown, click **Create a topic** .\n- In the **Create a topic** box, give your new topic a name (for example `my-scheduled-pipeline-topic` ), and select **Create topic** .\n- Leave all other fields as default and click **Save** to save the Trigger section configuration.\n- Leave all other fields as default and click **Next** to proceed to the Code section.\n- Under **Runtime** , select **Python 3.7** .\n- In **Entry** point, input \"subscribe\" (the example code entry point function name).\n- Under **Source code** , select **Inline Editor** if it's not already selected.\n- In the `main.py` file, add in the following code:```\n\u00a0 import base64\u00a0 import json\u00a0 from google.cloud import aiplatform\u00a0 PROJECT_ID = 'your-project-id' \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # <---CHANGE THIS\u00a0 REGION = 'your-region' \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # <---CHANGE THIS\u00a0 PIPELINE_ROOT = 'your-cloud-storage-pipeline-root' # <---CHANGE THIS\u00a0 def subscribe(event, context):\u00a0 \u00a0 \"\"\"Triggered from a message on a Cloud Pub/Sub topic.\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 event (dict): Event payload.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 context (google.cloud.functions.Context): Metadata for the event.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 # decode the event payload string\u00a0 \u00a0 payload_message = base64.b64decode(event['data']).decode('utf-8')\u00a0 \u00a0 # parse payload string into JSON object\u00a0 \u00a0 payload_json = json.loads(payload_message)\u00a0 \u00a0 # trigger pipeline run with payload\u00a0 \u00a0 trigger_pipeline_run(payload_json)\u00a0 def trigger_pipeline_run(payload_json):\u00a0 \u00a0 \"\"\"Triggers a pipeline run\u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 payload_json: expected in the following format:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"pipeline_spec_uri\": \"<path-to-your-compiled-pipeline>\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"parameter_values\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"greet_name\": \"<any-greet-string>\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 pipeline_spec_uri = payload_json['pipeline_spec_uri']\u00a0 \u00a0 parameter_values = payload_json['parameter_values']\u00a0 \u00a0 # Create a PipelineJob using the compiled pipeline from pipeline_spec_uri\u00a0 \u00a0 aiplatform.init(\u00a0 \u00a0 \u00a0 \u00a0 project=PROJECT_ID,\u00a0 \u00a0 \u00a0 \u00a0 location=REGION,\u00a0 \u00a0 )\u00a0 \u00a0 job = aiplatform.PipelineJob(\u00a0 \u00a0 \u00a0 \u00a0 display_name='hello-world-pipeline-cloud-function-invocation',\u00a0 \u00a0 \u00a0 \u00a0 template_path=pipeline_spec_uri,\u00a0 \u00a0 \u00a0 \u00a0 pipeline_root=PIPELINE_ROOT,\u00a0 \u00a0 \u00a0 \u00a0 enable_caching=False,\u00a0 \u00a0 \u00a0 \u00a0 parameter_values=parameter_values\u00a0 \u00a0 )\u00a0 \u00a0 # Submit the PipelineJob\u00a0 \u00a0 job.submit()\n```Replace the following:- : The Google Cloud project that this pipeline runs in.\n- : The region that this pipeline runs in.\n- : Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored in the pipeline root.\n- In the `requirements.txt` file, replace the contents with the following package requirements:```\ngoogle-api-python-client>=1.7.8,<2google-cloud-aiplatform\n```\n- Click **deploy** to deploy the Function.", "guide": "Vertex AI"}