{"title": "Vertex AI - Train a model with Wide & Deep", "url": "https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/wide-and-deep-train", "abstract": "# Vertex AI - Train a model with Wide & Deep\nTo see an example of how to train a model with Wide & Deep,  run the \"Tabular Workflows: Wide & Deep Pipeline\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tabular_workflows/wide_and_deep_on_vertex_pipelines.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Ftabular_workflows%2Fwide_and_deep_on_vertex_pipelines.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/tabular_workflows/wide_and_deep_on_vertex_pipelines.ipynb)\nThis page shows you how to train a classification or regression model from a tabular dataset with the Tabular Workflow for Wide & Deep.\nTwo versions of the Tabular Workflow for Wide & Deep are available:\n- [HyperparameterTuningJob](#hyperparametertuningjob) searches for the best set of hyperparameter values to use for model training.\n- [CustomJob](#customjob) lets you specify the hyperparameter values to use for model training. If you know exactly which hyperparameter values you need, you can specify them instead of searching for them and save on training resources.\nTo learn about the service accounts used by this workflow, see [Service accounts for Tabular Workflows](/vertex-ai/docs/tabular-data/tabular-workflows/service-accounts#fte-workflow) .\n", "content": "## Workflow APIs\nThis workflow uses the following APIs:\n- Vertex AI\n- Dataflow\n- Compute Engine\n- Cloud Storage## Train a model with HyperparameterTuningJob\nThe following sample code demonstrates how you can run a HyperparameterTuningJob pipeline:\n```\npipeline_job = aiplatform.PipelineJob(\u00a0 \u00a0 ...\u00a0 \u00a0 template_path=template_path,\u00a0 \u00a0 parameter_values=parameter_values,\u00a0 \u00a0 ...)pipeline_job.run(service_account=SERVICE_ACCOUNT)\n```\nThe optional `service_account` parameter in `pipeline_job.run()` lets you set the Vertex AI Pipelines service account to an account of your choice.\nThe pipeline and the parameter values are defined by the following function. The training data can be either a CSV file in Cloud Storage or a table in BigQuery.\n```\ntemplate_path, parameter_values = \u00a0automl_tabular_utils.get_wide_and_deep_hyperparameter_tuning_job_pipeline_and_parameters(...)\n```\nThe following is a subset of `get_wide_and_deep_hyperparameter_tuning_job_pipeline_and_parameters` parameters:\n| Parameter name     | Type     | Definition                                                         |\n|:--------------------------------|:------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| data_source_csv_filenames  | String     | A URI for a CSV stored in Cloud Storage.                                                  |\n| data_source_bigquery_table_path | String     | A URI for a BigQuery table.                                                     |\n| dataflow_service_account  | String     | (Optional) Custom service account to run Dataflow jobs. The Dataflow job can be configured to use private IPs and a specific VPC subnet. This parameter acts as an override for the default Dataflow worker service account.     |\n| study_spec_parameters_override | List[Dict[String, Any]] | (Optional) An override for tuning hyperparameters. This parameter can be empty or contain one or more of the possible hyperparameters. If a hyperparameter value is not set, Vertex AI uses the default tuning range for the hyperparameter. |\nIf you want to configure the hyperparameters using the `study_spec_parameters_override` parameter, you can use Vertex AI's helper function `get_wide_and_deep_study_spec_parameters_override` . This function returns a list of hyperparameters and ranges.\nThe following is an example of how the `get_wide_and_deep_study_spec_parameters_override` function can be used:\n```\nstudy_spec_parameters_override = automl_tabular_utils.get_wide_and_deep_study_spec_parameters_override()\n```\n## Train a model with CustomJob\nThe following sample code demonstrates how you can run a CustomJob pipeline:\n```\npipeline_job = aiplatform.PipelineJob(\u00a0 \u00a0 ...\u00a0 \u00a0 template_path=template_path,\u00a0 \u00a0 parameter_values=parameter_values,\u00a0 \u00a0 ...)pipeline_job.run(service_account=SERVICE_ACCOUNT)\n```\nThe optional `service_account` parameter in `pipeline_job.run()` lets you set the Vertex AI Pipelines service account to an account of your choice.\nThe pipeline and the parameter values are defined by the following function. The training data can be either a CSV file in Cloud Storage or a table in BigQuery.\n```\ntemplate_path, parameter_values = automl_tabular_utils.get_wide_and_deep_trainer_pipeline_and_parameters(...)\n```\nThe following is a subset of `get_wide_and_deep_trainer_pipeline_and_parameters` parameters:\n| Parameter name     | Type | Definition                                                     |\n|:--------------------------------|:-------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| data_source_csv_filenames  | String | A URI for a CSV stored in Cloud Storage.                                              |\n| data_source_bigquery_table_path | String | A URI for a BigQuery table.                                                 |\n| dataflow_service_account  | String | (Optional) Custom service account to run Dataflow jobs. The Dataflow job can be configured to use private IPs and a specific VPC subnet. This parameter acts as an override for the default Dataflow worker service account. |\n## What's next\nOnce you're ready to make predictions with your classification or regression model, you have two options:\n- [Make online (real-time) predictions using your model](/vertex-ai/docs/tabular-data/tabular-workflows/wide-and-deep-online-predictions) \n- [Get batch predictions directly from your model](/vertex-ai/docs/tabular-data/tabular-workflows/wide-and-deep-batch-predictions) .\n- Learn about [pricing for model training](/vertex-ai/docs/tabular-data/tabular-workflows/pricing) .", "guide": "Vertex AI"}