{"title": "Vertex AI - Deploy a model to Cloud TPU VMs", "url": "https://cloud.google.com/vertex-ai/docs/predictions/use-tpu", "abstract": "# Vertex AI - Deploy a model to Cloud TPU VMs\nGoogle Cloud provides access to custom-designed machine learning accelerators called [Tensor Processing Units (TPUs)](/tpu/docs/intro-to-tpu) . TPUs are optimized to accelerate the training and inference of machine learning models, making them ideal for a variety of applications, including natural language processing, computer vision, and speech recognition.\nThis page describes how to deploy your models to a [single host](/tpu/docs/tpus-in-gke#single-host) Cloud TPU v5e for online prediction in Vertex AI.\nOnly [Cloud TPU version v5e](/tpu/docs/v5e-inference) is supported. Other Cloud TPU generations are not supported.\n**Note:** Cloud TPUs are supported only in `us-west1` . For more information, see [locations](/vertex-ai/docs/general/locations#region_considerations) .\n", "content": "## Import your model\nFor deployment on Cloud TPUs, you must [import your model to Vertex AI](/vertex-ai/docs/model-registry/import-model) and configure it to use one of the following containers:\n- [prebuilt optimized TensorFlow runtime container](/vertex-ai/docs/predictions/optimized-tensorflow-runtime) either the`nightly`version, or version`2.15`or later\n- [prebuilt PyTorch TPU container](/vertex-ai/docs/predictions/pre-built-containers#pytorch) version`2.1`or later\n- your own custom container that supports TPUs\n### Prebuilt optimized TensorFlow runtime container\nTo import and run a `SavedModel` on a Cloud TPU, the model must be TPU optimized. If your TensorFlow `SavedModel` is not already TPU optimized, there are three ways to optimize your model:\n- **Manual model optimization** - You use [Inference Converter](/tpu/docs/v5e-inference-converter) to optimize your model and save it. Then, you must pass `--saved_model_tags='serve,tpu'` and `--disable_optimizer=true` flags when you [upload](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) your model. For example:```\nmodel = aiplatform.Model.upload(\u00a0 \u00a0 display_name='Manually optimized model',\u00a0 \u00a0 artifact_uri=\"gs://model-artifact-uri\",\u00a0 \u00a0 serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-tpu.2-15:latest\",\u00a0 \u00a0 serving_container_args=[\u00a0 \u00a0 \u00a0 \u00a0 \"--saved_model_tags='serve,tpu'\",\u00a0 \u00a0 \u00a0 \u00a0 \"--disable_optimizer=true\",\u00a0 \u00a0 ])\n```\n- **Automatic model optimization with automatic partitioning** - When you import a model, Vertex AI will attempt to optimize your unoptimized model using an automatic partitioning algorithm. This optimization does not work on all models. If optimization fails, you must either manually optimize your model or choose automatic model optimization with manual partitioning. For example:```\nmodel = aiplatform.Model.upload(\u00a0 \u00a0 display_name='TPU optimized model with automatic partitioning',\u00a0 \u00a0 artifact_uri=\"gs://model-artifact-uri\",\u00a0 \u00a0 serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-tpu.2-15:latest\",\u00a0 \u00a0 serving_container_args=[\u00a0 \u00a0 ])\n```\n- **Automatic model optimization with manual partitioning** . Specify the `--converter_options_string` flag and adjust the [ConverterOptions.TpuFunction](/tpu/docs/v5e-inference-converter#converter-options) to fit your needs. For an example, see [Converter Image](/tpu/docs/v5e-inference#converter-image) . Note that only `ConverterOptions.TpuFunction` , which is all that is needed for manual partitioning, is supported. For example:```\nmodel = aiplatform.Model.upload(display_name='TPU optimized model with manual partitioning',\u00a0 artifact_uri=\"gs://model-artifact-uri\",\u00a0 serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai-restricted/prediction/tf_opt-tpu.2-15:latest\",\u00a0 serving_container_args=[\u00a0 \u00a0 \u00a0 \"--converter_options_string='tpu_functions { function_alias: \\\"partitioning function name\\\" }'\"\u00a0 ])\n```\nFor more information on importing models, see [importing models to Vertex AI](/vertex-ai/docs/model-registry/import-model) .\n### Prebuilt PyTorch container\nThe instructions to import and run a PyTorch model on Cloud TPU are the same as the instructions to import and run a PyTorch model.\nFor example, [TorchServe for Cloud TPU v5e Inference](/tpu/docs/v5e-inference#torchserve) demonstrates how to package the Densenet 161 model into model artifacts using [Torch Model Archiver](https://github.com/pytorch/serve/blob/master/model-archiver/README.md) .\nThen, upload the model artifacts to your Cloud Storage folder and upload your model as shown:\n```\nmodel = aiplatform.Model.upload(\u00a0 \u00a0 display_name='DenseNet TPU model from SDK PyTorch 2.1',\u00a0 \u00a0 artifact_uri=\"gs://model-artifact-uri\",\u00a0 \u00a0 serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/pytorch-tpu.2-1:latest\",\u00a0 \u00a0 serving_container_args=[],\u00a0 \u00a0 serving_container_predict_route=\"/predictions/model\",\u00a0 \u00a0 serving_container_health_route=\"/ping\",\u00a0 \u00a0 serving_container_ports=[8080])\n```\nFor more information, see [export model artifacts for PyTorch](/vertex-ai/docs/training/exporting-model-artifacts#pytorch) and the Jupyter Notebook for [Serve a PyTorch model using a prebuilt container](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/pytorch_image_classification_with_prebuilt_serving_containers.ipynb) .\n### Custom container\nFor custom containers, your model does not need to be a TensorFlow model, but it must be TPU optimized. For information on producing a TPU optimized model, see the following guides for common ML frameworks:\n- [TensorFlow](https://www.tensorflow.org/guide/tpu) \n- [PyTorch](https://github.com/pytorch/xla) \n- [JAX/SAX](/vertex-ai/docs/predictions/serve-gemma-with-saxml-tpu) \nFor information on serving models trained with JAX, TensorFlow, or PyTorch on Cloud TPU v5e, see [Cloud TPU v5e Inference](/tpu/docs/v5e-inference#inference-on-lite-pods) .\nMake sure your custom container meets the [custom container requirements](/vertex-ai/docs/predictions/custom-container-requirements) .\nYou must [raise the locked memory limit](/kubernetes-engine/docs/how-to/tpus#privileged-mode) so the driver can communicate with the TPU chips over direct memory access (DMA). For example:\n```\nulimit -l 68719476736\n```\n```\nimport resourceresource.setrlimit(\u00a0 \u00a0 resource.RLIMIT_MEMLOCK,\u00a0 \u00a0 (\u00a0 \u00a0 \u00a0 \u00a0 68_719_476_736_000, \u00a0# soft limit\u00a0 \u00a0 \u00a0 \u00a0 68_719_476_736_000, \u00a0# hard limit\u00a0 \u00a0 ),\u00a0 )\n```\nThen, see [Use a custom container for prediction](/vertex-ai/docs/predictions/use-custom-container) for information on importing a model with a custom container. If you have want to implement pre or post processing logic, consider using [Custom prediction routines](/vertex-ai/docs/predictions/custom-prediction-routines) .\n## Create an endpoint\nThe instructions for creating an endpoint for Cloud TPUs are the same as the instructions for creating any endpoint.\nFor example, the following command creates an [endpoint](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints) resource:\n```\nendpoint = aiplatform.Endpoint.create(display_name='My endpoint')\n```\nThe response contains the new endpoint's ID, which you use in subsequent steps.\nFor more information on creating an endpoint, see [deploy a model to an endpoint](/vertex-ai/docs/general/deployment#deploy_a_model_to_an_endpoint) .\n## Deploy a model\nThe instructions for deploying a model to Cloud TPUs are the same as the instructions for deploying any model, except you specify one of the following supported Cloud TPU machine types:\n| Machine Type  | Number of TPU chips |\n|:-----------------|----------------------:|\n| ct5lp-hightpu-1t |      1 |\n| ct5lp-hightpu-4t |      4 |\n| ct5lp-hightpu-8t |      8 |\nTPU accelerators are built-in to the machine type. You don't have to specify accelerator type or accelerator count.\nFor example, the following command deploys a model by calling [deployModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/deployModel) :\n```\nmachine_type = 'ct5lp-hightpu-1t'deployed_model = model.deploy(\u00a0 \u00a0 endpoint=endpoint,\u00a0 \u00a0 deployed_model_display_name='My deployed model',\u00a0 \u00a0 machine_type=machine_type,\u00a0 \u00a0 traffic_percentage=100,\u00a0 \u00a0 min_replica_count=1\u00a0 \u00a0 sync=True,)\n```\n**Note:** [Autoscaling](/vertex-ai/docs/general/deployment#scaling) for deployments using TPUs is not supported at this time. The system always uses `minReplicaCount` and ignores `maxReplicaCount` for the deployed model.\nFor more information, see [deploy a model to an endpoint](/vertex-ai/docs/general/deployment#deploy_a_model_to_an_endpoint) .\n## Get online predictions\nThe instruction for getting online predictions from a Cloud TPU is the same as the instruction for [getting online predictions](/vertex-ai/docs/predictions/get-online-predictions) .\nFor example, the following command sends an online prediction request by calling [predict](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/predict) :\n```\ndeployed_model.predict(...)\n```\nFor custom containers, see the [prediction request and response requirements](/vertex-ai/docs/predictions/custom-container-requirements#prediction) for custom containers.\n## Securing capacity\nBy default, the quota for `Custom model serving TPU v5e cores per region` is 0.\nTo request an increase, see [Request a higher quota limit](/docs/quota/view-manage#requesting_higher_quota) .\n## Pricing\nTPU machine types are billed per hour, just like all other machine type in Vertex Prediction. For more information, see [Prediction pricing](/vertex-ai/pricing#prediction-prices) .\n## What's next\n- Learn how to [get an online prediction](/vertex-ai/docs/predictions/get-online-predictions)", "guide": "Vertex AI"}