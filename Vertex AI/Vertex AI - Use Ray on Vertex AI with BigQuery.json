{"title": "Vertex AI - Use Ray on Vertex AI with BigQuery", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Use Ray on Vertex AI with BigQuery\n**    Preview     ** This product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA products and features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nWhen running a Ray application on Vertex AI, you can use [BigQuery](/bigquery/docs/introduction) as your cloud database. This section covers how to read from and write to a BigQuery database from your Ray cluster on Vertex AI. The steps in this section assume that you're using the version of the Vertex AI SDK for Python that includes the functionality of the [Ray Client](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html) in an interactive Python environment.\nIf you want to read from a BigQuery dataset, you should [create a newBigQuery dataset](/bigquery/docs/datasets) or use an existing dataset.\n", "content": "## Import and initialize Ray on Vertex AI client\nIf you're already connected to your Ray cluster on Vertex AI, restart your kernel and run the following code. The `runtime_env` variable is necessary at connection time to run BigQuery commands.\n```\nimport rayfrom google.cloud import aiplatform# The CLUSTER_RESOURCE_NAME is the one returned from vertex_ray.create_ray_cluster.address = 'vertex_ray://{}'.format(CLUSTER_RESOURCE_NAME)runtime_env = {\u00a0 \u00a0 \"pip\":\u00a0 \u00a0 \u00a0 \u00a0[\"google-cloud-aiplatform[ray]\"]\u00a0 }ray.init(address=address, runtime_env=runtime_env)\n```\n## Read data from BigQuery\nRead data from your BigQuery dataset.\n**Note:** The maximum query response size is 10\u00a0GB.\n```\nfrom vertex_ray import BigQueryDatasourceaiplatform.init()dataset = \"DATASET\"parallelism = NUM_BLOCKSquery = \"SQL_QUERY\"ds = ray.data.read_datasource(\u00a0 \u00a0 BigQueryDatasource(),\u00a0 \u00a0 dataset=dataset,\u00a0 \u00a0 parallelism=parallelism,\u00a0 \u00a0 query=query)ds.fully_executed()\n```\nWhere:\n- : Google Cloud project ID\n- : BigQuery dataset. Must be in the format `dataset.table` . Set to `None` if providing a query.\n- : An integer that influences how many read tasks are created in parallel. There may be fewer read streams created than you request.\n- : A string containing a SQL query to read from BigQuery database. Set to `None` if no query is required.## Transform data\nUpdate and delete rows and columns from your BigQuery tables using pyarrow or pandas. If you want to use pandas transformations, we recommend that you keep the input type as pyarrow and convert to pandas within the user-defined function (UDF) so you can catch any pandas conversion type errors within the UDF.\n```\nimport pandas as pdimport pyarrow as padef filter_batch(table: pa.Table) -> pa.Table:\u00a0 \u00a0 df = table.to_pandas(types_mapper={pa.int64(): pd.Int64Dtype()}.get)\u00a0 \u00a0 # PANDAS_TRANSFORMATIONS_HERE\u00a0 \u00a0 return pa.Table.from_pandas(df)ds = ds.map_batches(filter_batch, batch_format=\"pyarrow\").random_shuffle()ds.fully_executed()# You can repartition before writing to determine the number of write blocksds = ds.repartition(4)ds.fully_executed()\n```\n## Write data to BigQuery\nInsert data to your BigQuery dataset.\n```\ndataset = \"DATASET\"ds.write_datasource(\u00a0 \u00a0 BigQueryDatasource(),\u00a0 \u00a0 dataset=dataset)\n```\nWhere:\n- : Google Cloud project ID\n- : BigQuery dataset. Must be in the format `dataset.table` .## What's next\n- [Deploy a model on Vertex AIand get predictions](/vertex-ai/docs/open-source/ray-on-vertex-ai/deploy-predict) \n- [View logs for your Ray cluster on Vertex AI](/vertex-ai/docs/open-source/ray-on-vertex-ai/view-logs) \n- [Delete a Ray cluster](/vertex-ai/docs/open-source/ray-on-vertex-ai/delete-cluster)", "guide": "Vertex AI"}