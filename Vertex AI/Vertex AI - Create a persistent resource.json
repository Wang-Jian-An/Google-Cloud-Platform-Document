{"title": "Vertex AI - Create a persistent resource", "url": "https://cloud.google.com/vertex-ai/docs/training/persistent-resource-create", "abstract": "# Vertex AI - Create a persistent resource\n**    Preview     ** This product or feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA products and features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nWhen you create a persistent resource, the training service first finds resources from the Compute Engine resource pool based on the specifications you provided, and then provisions a long-running cluster for you. This page shows you how to create a persistent resource for running your custom training jobs by using the Vertex AI API or the Google Cloud CLI.\n", "content": "## Required roles\nTo get the permission that you need to create a persistent resource,   ask your administrator to grant you the [Vertex AI Administrator ](https://cloud.google.com/iam/docs/understanding-roles#aiplatform.admin) ( `roles/aiplatform.admin` ) IAM role on your project.    For more information about granting roles, see [Manage access](/iam/docs/granting-changing-revoking-access) .\nThis predefined role contains the `aiplatform.persistentResources.create` permission, which is   required to create a persistent resource.\nYou might also be able to get   this permission  with [custom roles](/iam/docs/creating-custom-roles) or  other [predefined roles](/iam/docs/understanding-roles) .\n## Create a persistent resource\nSelect one of the following tabs for instructions on how to create a persistent resource.\nA persistent resource can have one or more resource pools. To create multiple resource pools in a persistent resource, specify multiple `--resource-pool-spec` flags.\nEach resource pool can have autoscaling either enabled or disabled. To enable autoscaling, specify `min_replica_count` and `max_replica_count` .\nYou can specify all resource pool configurations as part of the command-line or use the `--config` flag to specify the path to a YAML file that contains the configurations.\nBefore using any of the command data below, make the following replacements:- : The Project ID of the Google Cloud project  where you want to create the persistent resource.\n- : The region where you want to create the  persistent resource. For a list of supported regions, see [Feature availability](/vertex-ai/docs/general/locations#feature-availability) .\n- : The ID of the  persistent resource.\n- : (Optional) The display name of the  persistent resource.\n- : The type of VM to use. For a list  of supported VMs, see [Machine types](/vertex-ai/docs/training/configure-compute#machine-types) .  This field corresponds to the`machineSpec.machineType`field in the`ResourcePool`API message.\n- : (Optional) The type of GPU to attach  to each VM in the resource pool. For a list of supported GPUs, see [GPUs](/vertex-ai/docs/training/configure-compute#specifying_gpus) . This field corresponds to  the`machineSpec.acceleratorType`field in the`ResourcePool`API message.\n- : (Optional) The number of GPUs to  attach to each VM in the resource pool. The default the value is`1`. This field  corresponds to the`machineSpec.acceleratorCount`field in`ResourcePool`API message.\n- : The number of replicas to create  when creating this resource pool. This field corresponds to the`replicaCount`field  in the`ResourcePool`API message. This field is required if you're not specifyingand.\n- : (Optional) The minimum  number of replicas that autoscaling can scale down to for this resource pool. Bothandare required to enable  autoscaling on this resource pool.\n- : (Optional) The maximum  number of replicas that autoscaling can scale up to for this resource pool. Bothandare required to enable  autoscaling on this resource pool.\n- : (Optional) The type of disk to use for  as the boot disk of each VM in the resource pool. This field corresponds to the`diskSpec.bootDiskType`field in the`ResourcePool`API message.  Acceptable values include the following:- `pd-standard`(default)\n- `pd-ssd`\n- : (Optional) The disk size in GiB for  the boot disk of each VM in the resource pool. Acceptable values are`100`(default)  to`64000`. This field corresponds to the`diskSpec.bootDiskSizeGb`field  in the`ResourcePool`API message.\n- : Path to the persistent resource YAML  configuration file. This file should contain a list of ResourcePool. If an option is specified  in both the configuration file and the command-line arguments, the command-line arguments  override the configuration file. Note that keys with underscores are invalid.Example YAML configuration file:```\nresourcePoolSpecs:\n machineSpec:\n machineType: n1-standard-4\n replicaCount: 1\n \n```\nExecute the  following  command:\nYou should receive a response similar to the following:\n```\nUsing endpoint [https://us-central1-aiplatform.googleapis.com/]\nOperation to create PersistentResource [projects/123456789012/locations/us-central1/persistentResources/mypersistentresource/operations/1234567890123456789] is submitted successfully.\nYou may view the status of your PersistentResource create operation with the command\n $ gcloud beta ai operations describe projects/sample-project/locations/us-central1/operations/1234567890123456789\n```\nExample `gcloud` command:\n```\ngcloud beta ai persistent-resources create \\\n --persistent-resource-id=my-persistent-resource \\\n --region=us-central1 \\\n --resource-pool-spec=\"min-replica-count=4,max-replica-count=12,machine-type=n1-highmem-2,accelerator-type=NVIDIA_TESLA_K80,accelerator-count=1,disk-type=pd-standard,disk-size=200\" \\\n --resource-pool-spec=\"replica-count=4,machine-type=n1-standard-4\"\n```If you want to specify configuration options that are not available in the preceding examples, you can use the `--config` flag to specify the path to a `config.yaml` file in your local environment that contains the fields of [persistentResources](/vertex-ai/docs/reference/rest/v1beta1/projects.locations.persistentResources) . For example:\n```\ngcloud beta ai persistent-resources create \\\n --persistent-resource-id=PERSISTENT_RESOURCE_ID \\\n --project=PROJECT_ID \\\n --region=LOCATION \\\n --config=CONFIG\n```\nA persistent resource can have one or more resource pools ( `machine_spec` ), and each resource pool can have autoscaling either enabled or disabled.\nBefore using any of the request data, make the following replacements:- : The Project ID of the Google Cloud project  where you want to create the persistent resource.\n- : The region where you want to create the  persistent resource. For a list of supported regions, see [Feature availability](/vertex-ai/docs/general/locations#feature-availability) .\n- : The ID of the  persistent resource.\n- : (Optional) The display name of the  persistent resource.\n- : The type of VM to use. For a list  of supported VMs, see [Machine types](/vertex-ai/docs/training/configure-compute#machine-types) .  This field corresponds to the`machineSpec.machineType`field in the`ResourcePool`API message.\n- : (Optional) The type of GPU to attach  to each VM in the resource pool. For a list of supported GPUs, see [GPUs](/vertex-ai/docs/training/configure-compute#specifying_gpus) . This field corresponds to  the`machineSpec.acceleratorType`field in the`ResourcePool`API message.\n- : (Optional) The number of GPUs to  attach to each VM in the resource pool. The default the value is`1`. This field  corresponds to the`machineSpec.acceleratorCount`field in`ResourcePool`API message.\n- : The number of replicas to create  when creating this resource pool. This field corresponds to the`replicaCount`field  in the`ResourcePool`API message. This field is required if you're not specifyingand.\n- : (Optional) The minimum  number of replicas that autoscaling can scale down to for this resource pool. Bothandare required to enable  autoscaling on this resource pool.\n- : (Optional) The maximum  number of replicas that autoscaling can scale up to for this resource pool. Bothandare required to enable  autoscaling on this resource pool.\n- : (Optional) The type of disk to use for  as the boot disk of each VM in the resource pool. This field corresponds to the`diskSpec.bootDiskType`field in the`ResourcePool`API message.  Acceptable values include the following:- `pd-standard`(default)\n- `pd-ssd`\n- : (Optional) The disk size in GiB for  the boot disk of each VM in the resource pool. Acceptable values are`100`(default)  to`64000`. This field corresponds to the`diskSpec.bootDiskSizeGb`field  in the`ResourcePool`API message.\nHTTP method and URL:\n```\nPOST https://us-central1-aiplatform.googleapis.com/v1beta1/projects/PROJECT_ID/locations/LOCATION/persistentResources?persistent_resource_id=PERSISTENT_RESOURCE_ID\n```\nRequest JSON body:\n```\n{\n \"display_name\": \"DISPLAY_NAME\",\n \"resource_pools\": [ {\n  \"machine_spec\": {\n  \"machine_type\": \"MACHINE_TYPE\",\n  \"accelerator_type\": \"ACCELERATOR_TYPE\",\n  \"accelerator_count\": ACCELERATOR_COUNT\n  },\n  \"replica_count\": REPLICA_COUNT,\n  \"autoscaling_spec\": {\n  \"min_replica_count\": MIN_REPLICA_COUNT,\n  \"max_replica_count\": MAX_REPLICA_COUNT\n  },\n  \"disk_spec\": {\n  \"boot_disk_type\": \"BOOT_DISK_TYPE\",\n  \"boot_disk_size_gb\": BOOT_DISK_SIZE_GB\n  }\n }\n ]\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/123456789012/locations/us-central1/persistentResources/mypersistentresource/operations/1234567890123456789\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1beta1.CreatePersistentResourceOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2023-02-08T21:17:15.009668Z\",\n  \"updateTime\": \"2023-02-08T21:17:15.009668Z\"\n }\n }\n}\n```### Resource stockout\nThere could be stockout for scarce resources like A100 GPUs, which can lead to persistent resource creation failure when no resource is available in the region you specified. In this case, you can try to reduce the number of replicas, change to different accelerator type, or try again during non-peak hours.\n## What's next\n- [Run training jobs on a persistent resource](/vertex-ai/docs/training/persistent-resource-train) .\n- [Learn about persistent resource](/vertex-ai/docs/training/persistent-resource-overview) .\n- [Get information about a persistent resource](/vertex-ai/docs/training/persistent-resource-get) .\n- [Delete a persistent resource](/vertex-ai/docs/training/persistent-resource-delete) .", "guide": "Vertex AI"}