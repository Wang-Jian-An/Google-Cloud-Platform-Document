{"title": "Vertex AI - Set up your environment", "url": "https://cloud.google.com/vertex-ai/docs/training/neural-architecture-search/environment-setup", "abstract": "# Vertex AI - Set up your environment\nSetup an environment before you launch a Vertex AI Neural Architecture Search experiment by following the sections below.\n", "content": "## Before you begin\n- To grant all Neural Architecture Search users the [Vertex AI User](/iam/docs/understanding-roles#vertex-ai-roles) role (`roles/aiplatform.user`), contact your project administrator.\n- [Install Docker](https://docs.docker.com/install/linux/docker-ce/ubuntu/) .If you're using a Linux-based operating system, such as Ubuntu or Debian,  add your username to the `docker` group so that you can run Docker without using `sudo` :```\nsudo usermod -a -G docker ${USER}\n``` **Caution** : The `docker` group is equivalent to the `root` user. For information about how this affects the security of your system, see [Docker's documentation](https://docs.docker.com/engine/security/security/#docker-daemon-attack-surface) .You might need to restart your system after adding yourself to the `docker` group.\n- Open Docker. To ensure that Docker is running, run the following Docker command, which returns the current time and date:```\ndocker run busybox date\n```\n- Use`gcloud`as the credential helper for Docker:```\ngcloud auth configure-docker\n```\n- (Optional) If you want to run the container using GPU locally,  install [nvidia-docker](https://github.com/NVIDIA/nvidia-docker#quickstart) .\n### Set up your Cloud Storage bucket\nThis section demonstrates how to create a new bucket. You can use an existing  bucket, but it must be in the same region where you are running  AI Platform jobs. Additionally, if it isn't part of the project you are using  to run Neural Architecture Search, you must explicitly  grant access to the Neural Architecture Search service accounts.\n- Specify a name for your new bucket. The name must be unique across all  buckets in Cloud Storage.```\nBUCKET_NAME=\"YOUR_BUCKET_NAME\"\n```For example, use your project name with `-vertexai-nas` appended:```\nPROJECT_ID=\"YOUR_PROJECT_ID\"BUCKET_NAME=${PROJECT_ID}-vertexai-nas\n```\n- Check the bucket name that you created.```\necho $BUCKET_NAME\n```\n- Select a region for your bucket and set a `REGION` environment variable.Use the same region where you plan on running Neural Architecture Search  jobs.For example, the following code creates `REGION` and sets it to `us-central1` :```\nREGION=us-central1\n```\n- Create the new bucket:```\ngsutil mb -l $REGION gs://$BUCKET_NAME\n```## Request additional device quota for the project\nThe [tutorials](/vertex-ai/docs/training/neural-architecture-search/nas-tutorials) use approximately five CPU machines and don't require any additional quota. After running the tutorials, run your Neural Architecture Search job.\nThe Neural Architecture Search job trains a batch of models in parallel. Each trained model corresponds to a . Read the section on [setting the number-of-parallel-trials](/vertex-ai/docs/training/neural-architecture-search/suggested-workflow#parallel_trials) to estimate the amount of CPUs and GPUs needed for a search job. For example, if each trial uses 2 T4 GPUs and you set `number-of-parallel-trials` to 20, then you need a total quota of 40 T4 GPUs for a search job. In addition, if each trial is using a `highmem-16` CPU, then you need 16 CPU units per trial, which is 320 CPU units for 20 parallel trials. The default initial quota for GPUs and is usually found to be 0, 6, or 12 for Tesla_T4 and 0 or 6 for Tesla_V100. The default initial quota for CPUs varies by region and is usually found to be 20, 450, or 2,200.\n**Note:** Neural Architecture Search currently doesn't support TPUs.\nOptional: if you plan to run multiple search jobs in parallel, then scale the quota requirement. Requesting a quota doesn't charge you immediately. You are charged once you run a job.\n**Note:** The quota is requested per region. For example, you can't use `us-central1` quota for running jobs in `europe-west4` region.\nIf you don't have enough quota and try to launch a job which needs more resources than your quota, then the job won't launch giving an error which looks like:\n```\nException: Starting job failed: {'code': 429, 'message': 'The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_cpus,aiplatform.googleapis.com/custom_model_training_nvidia_v100_gpus,aiplatform.googleapis.com/custom_model_training_pd_ssd', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.DebugInfo', 'detail': '[ORIGINAL ERROR] generic::resource_exhausted: com.google.cloud.ai.platform.common.errors.AiPlatformException: code=RESOURCE_EXHAUSTED, message=The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_cpus,aiplatform.googleapis.com/custom_model_training_nvidia_v100_gpus,aiplatform.googleapis.com/custom_model_training_pd_ssd, cause=null [google.rpc.error_details_ext] { code: 8 message: \"The following quota metrics exceed quota limits: aiplatform.googleapis.com/custom_model_training_cpus,aiplatform.googleapis.com/custom_model_training_nvidia_v100_gpus,aiplatform.googleapis.com/custom_model_training_pd_ssd\" }'}]}\n```\nIn some cases, if multiple jobs for the same project were started at the same time and the quota isn't sufficient for all of them, then one of the jobs remain in queued state and won't start training. In this case, cancel the queued job and either request more quota or wait until the previous job finishes.\nYou can request the additional device quota from the **Quotas** page. You can apply filters to find the desired quota to edit:\n- For **Service** , select **Vertex AI API** .\n- For **region** , select the region you want to filter on.\n- For **Quota** , select an accelerator name whose prefix is **Custom model training** .- For V100 GPUs, the value is **Custom model training Nvidia V100 GPUs per\nregion** .\n- For CPUs, the value can be **Custom model training CPUs for N1/E2 machine\ntypes per region** . The number for the CPU represents the unit of CPUs. If you want 8`highmem-16`CPUs, then make your quota request for 8\u00a0* 16\u00a0= 128\u00a0CPU units. Also enter the desired value for **region** .Once you create a quota request, you receive a `Case number` and follow up emails about the status of your request. A GPU quota approval might take approximately two to five business days to get approved. In general, getting a quota of approximately 20-30 GPUs should be faster to get approved in approximately two to three days and getting approval for approximately 100 GPUs might take five business days. A CPU quota approval might take up to two business days to get approved. However, if a region is experiencing a big shortage of a GPU type, then there is no guarantee even with a small quota ask. In this case, you might be asked to go for a different region or a different GPU type. In general, T4 GPUs are easier to get than V100s. T4 GPUs take more wall-clock time but are more cost effective.\nFor more information, see [Requesting a higher quotalimit](/docs/quotas/view-manage#requesting_higher_quota) .\n## Set up artifact registry for your project\nYou must set up an [artifact registry](https://cloud.google.com/artifact-registry/docs/overview#introduction) for your project and region where you push your docker images.\nGo to the **Artifact Registry** page for your project. If not already enabled, enable the artifact registry API for your project first:\nOnce enabled, start creating a new repository by clicking **CREATE REPOSITORY** :\nChoose **Name** as **nas** , **Format** as **Docker** , and **Location type** as **Region** . For **Region** , select the location where you run your jobs and then click **CREATE** .\nThis should create your desired docker repository as shown below:\nYou also need to set up authentication to push dockers to this repository. The [local environment set up section](#local-environment-setup) below contains this step.\n## Set up your local environment\nYou can run these steps using the Bash shell in your local environment, or run them in a Vertex AI Workbench user-managed notebooks instance.\n- Set up basic environment variables:```\ngcloud config set project PROJECT_IDgcloud auth logingcloud auth application-default login\n```\n- Set up [docker authentication](https://cloud.google.com/artifact-registry/docs/docker/authentication#gcloud-helper) for your artifact registry:```\n# example: REGION=europe-west4gcloud auth configure-docker REGION-docker.pkg.dev\n```\n- (Optional) Configure a Python 3 virtual environment. Python 3 usage is recommended but not required:```\nsudo apt install python3-pip && \\pip3 install virtualenv && \\python3 -m venv --system-site-packages ~/./nas_venv && \\source ~/./nas_venv/bin/activate\n```\n- Install additional libraries:```\npip install google-cloud-storage==2.6.0pip install pyglove==0.1.0\n```## Set up a service account\nA [service account](https://cloud.google.com/iam/docs/service-accounts) is required to be set up before running NAS jobs. You can run these steps using the Bash shell in your local environment, or run them in a Vertex AI Workbench user-managed notebooks instance.\n- Create a service account:```\ngcloud iam service-accounts create NAME \\\u00a0 \u00a0 --description=DESCRIPTION \\\u00a0 \u00a0 --display-name=DISPLAY_NAME\n```\n- Grant the `aiplatform.user` and `storage.objectAdmin` role to the service account:```\ngcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member=serviceAccount:NAME@PROJECT_ID.iam.gserviceaccount.com \\\u00a0 \u00a0 --role=roles/aiplatform.usergcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member=serviceAccount:NAME@PROJECT_ID.iam.gserviceaccount.com \\\u00a0 \u00a0 --role=roles/storage.objectAdmin\n```\nFor example, the following commands create a service account named `my-nas-sa` under the project `my-nas-project` with the `aiplatform.user` and `storage.objectAdmin` role:\n```\n\u00a0 gcloud iam service-accounts create my-nas-sa \\\u00a0 \u00a0 \u00a0 --description=\"Service account for NAS\" \\\u00a0 \u00a0 \u00a0 --display-name=\"NAS service account\"\u00a0 gcloud projects add-iam-policy-binding my-nas-project \\\u00a0 \u00a0 \u00a0 --member=serviceAccount:my-nas-sa@my-nas-project.iam.gserviceaccount.com \\\u00a0 \u00a0 \u00a0 --role=roles/aiplatform.user\u00a0 gcloud projects add-iam-policy-binding my-nas-project \\\u00a0 \u00a0 \u00a0 --member=serviceAccount:my-nas-sa@my-nas-project.iam.gserviceaccount.com \\\u00a0 \u00a0 \u00a0 --role=roles/storage.objectAdmin\n```\n## Download code\nTo start a Neural Architecture Search experiment, you need to download the sample Python code, which includes prebuilt trainers, search space definitions, and associated client libraries.\nRun the following steps to download the source code.\n- Open a new Shell Terminal.\n- Run the Git clone command:```\ngit clone https://github.com/google/vertex-ai-nas.git\n```", "guide": "Vertex AI"}