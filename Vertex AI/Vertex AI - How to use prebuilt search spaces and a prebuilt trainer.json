{"title": "Vertex AI - How to use prebuilt search spaces and a prebuilt trainer", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - How to use prebuilt search spaces and a prebuilt trainer\nThis guide shows how to run a Vertex AI Neural Architecture Search job by using Google's prebuilt search spaces and prebuilt trainer code based on [TF-vision](https://github.com/tensorflow/models/tree/master/official/vision) for MnasNet and SpineNet. Refer to the [MnasNet classification notebook](https://github.com/google/vertex-ai-nas/blob/main/notebooks/vertex_nas_classification_tfvision.ipynb) and [SpineNet object detection notebook](https://github.com/google/vertex-ai-nas/blob/main/notebooks/vertex_nas_detection_tfvision.ipynb) for end-to-end examples.\n", "content": "## Data preparation for prebuilt trainer\nNeural Architecture Search prebuilt trainer requires your data to be in `TFRecord` format, containing `tf.train.Example` s. The `tf.train.Example` s must include the following fields:\n```\n'image/encoded': tf.FixedLenFeature(tf.string)\n'image/height': tf.FixedLenFeature(tf.int64)\n'image/width': tf.FixedLenFeature(tf.int64)\n# For image classification only.\n'image/class/label': tf.FixedLenFeature(tf.int64)\n# For object detection only.\n'image/object/bbox/xmin': tf.VarLenFeature(tf.float32)\n'image/object/bbox/xmax': tf.VarLenFeature(tf.float32)\n'image/object/bbox/ymin': tf.VarLenFeature(tf.float32)\n'image/object/bbox/ymax': tf.VarLenFeature(tf.float32)\n'image/object/class/label': tf.VarLenFeature(tf.int64)\n```\nYou can follow [instructions for ImageNet data preparation here](https://cloud.google.com/tpu/docs/imagenet-setup) .\nTo convert your custom data, use the parsing script that is included with the [sample code and utilities you downloaded](/vertex-ai/docs/training/neural-architecture-search/environment-setup#download-samples) . To customize the data parsing, modify the [tf_vision/dataloaders/*_input.py](https://github.com/google/vertex-ai-nas/blob/main/tf_vision/dataloaders/retinanet_input.py) files.\n**Note:** For image classification and object detection, the total number of classes includes one background class with an index value of 0.\nLearn more about [TFRecord and tf.train.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord) .\n## Define experiment environment variables\nPrior to running your experiments, you will need to define several environment variables including:\n- TRAINER_DOCKER_ID:`${USER}_nas_experiment`(recommended format)\n- Cloud Storage locations of your training and validation datasets the experiment will use. For example (CoCo for detection):- `gs://cloud-samples-data/ai-platform/built-in/image/coco/train*`\n- `gs://cloud-samples-data/ai-platform/built-in/image/coco/val*`\n- Cloud Storage location for the experiment output. Recommended format:- `gs://${USER}_nas_experiment`\n- REGION: A region which should be the same as your experiment output bucket region. For example: `us-central1` .\n- PARAM_OVERRIDE: a .yaml file overriding parameters of the prebuilt trainer. Neural Architecture Search provides some default configurations that you can use:\n```\nPROJECT_ID=PROJECT_IDTRAINER_DOCKER_ID=TRAINER_DOCKER_IDLATENCY_CALCULATOR_DOCKER_ID=LATENCY_CALCULATOR_DOCKER_IDGCS_ROOT_DIR=OUTPUT_DIRREGION=REGIONPARAM_OVERRIDE=tf_vision/configs/experiments/spinenet_search_gpu.yamlTRAINING_DATA_PATH=gs://PATH_TO_TRAINING_DATAVALIDATION_DATA_PATH=gs://PATH_TO_VALIDATION_DATA\n```\nYou might want to select and/or modify the override file that matches your training requirements. Consider the following:\n- You can set`--accelerator_type`to choose from GPU or CPU. To run only a few epoches for fast testing using CPU, you may set the Flag`--accelerator_type=\"\"`and use the configuration file`tf_vision/test_files/fast_nas_detection_spinenet_search_for_testing.yaml`.\n- Number of epochs\n- Training runtime\n- Hyperparameters such as learning rate\nFor a list of all parameters to control the training jobs, see `tf_vision/configs/` . The following are the key parameters:\n```\ntask:\n train_data:\n global_batch_size: 80\n validation_data:\n global_batch_size: 16\n init_checkpoint: null\ntrainer:\n train_steps: 16634\n steps_per_loop: 1386\n optimizer_config:\n learning_rate:\n  cosine:\n  initial_learning_rate: 0.16\n  decay_steps: 16634\n  type: 'cosine'\n warmup:\n  type: 'linear'\n  linear:\n  warmup_learning_rate: 0.0067\n  warmup_steps: 1386\n```\nCreate a Cloud Storage bucket for Neural Architecture Search to store your job outputs (i.e. checkpoints):\n```\ngsutil mkdir $GCS_ROOT_DIR\n```\n## Build a trainer container and latency calculator container\nThe following command will build a trainer image in Google Cloud with the following URI: `gcr.io/` `` `/` `` which will be used in the Neural Architecture Search job in the next step.\n```\npython3 vertex_nas_cli.py build \\--project_id=PROJECT_ID \\--trainer_docker_id=TRAINER_DOCKER_ID \\--latency_calculator_docker_id=LATENCY_CALCULATOR_DOCKER_ID \\--trainer_docker_file=tf_vision/nas_multi_trial.Dockerfile \\--latency_calculator_docker_file=tf_vision/latency_computation_using_saved_model.Dockerfile\n```\nTo change the search space and reward, update them in your Python file and then rebuild the docker image.\n## Test the trainer locally\nSince launching a job in Google Cloud service takes several minutes, it may be more convenient to test the trainer docker locally, for example, validating the TFRecord format. Use `spinenet` search space as an example, you can run the search job locally (the model will be randomly sampled):\n```\n# Define the local job output dir.JOB_DIR=\"/tmp/iod_${search_space}\"python3 vertex_nas_cli.py search_in_local \\--project_id=PROJECT_ID \\--trainer_docker_id=TRAINER_DOCKER_ID \\--prebuilt_search_space=spinenet \\--use_prebuilt_trainer=True \\--local_output_dir=${JOB_DIR} \\--search_docker_flags \\params_override=\"tf_vision/test_files/fast_nas_detection_spinenet_search_for_testing.yaml\" \\training_data_path=TEST_COCO_TF_RECORD \\validation_data_path=TEST_COCO_TF_RECORD \\model=retinanet\n```\nThe `training_data_path` and `validation_data_path` are the paths to your TFRecords.\n## Launch a stage-1 search followed by a stage-2 training job on Google Cloud\nYou should refer to the [MnasNet classification notebook](https://github.com/google/vertex-ai-nas/blob/main/notebooks/vertex_nas_classification_tfvision.ipynb) and [SpineNet object detection notebook](https://github.com/google/vertex-ai-nas/blob/main/notebooks/vertex_nas_detection_tfvision.ipynb) for end-to-end examples.\n- You can set the flag `--max_parallel_nas_trial` and `--max_nas_trial` to customize. Neural Architecture Search will start `max_parallel_nas_trial` trials in parallel and finish after `max_nas_trial` trials.\n- If the flag `--target_device_latency_ms` is set, a separate `latency calculator` job will be launched with accelerator specified by flag `--target_device_type` .\n- The Neural Architecture Search Controller will provide each trial with a suggestion for a new architecture candidate through the FLAG `--nas_params_str` .\n- Each trial will build a graph based on the value of the FLAG `nas_params_str` and start a training job. Each trial also saves its value to a json file (at `os.path.join(nas_job_dir, str(trial_id), \"nas_params_str.json\")` ).\n### Reward with a latency constraint\nThe [MnasNet classification notebook](https://github.com/google/vertex-ai-nas/blob/main/notebooks/vertex_nas_classification_tfvision.ipynb) shows an example of a cloud-cpu device-based latency-constrained search.\nTo search models with latency constraint, the trainer can report reward as a function of both accuracy and latency.\nIn the shared source code, the reward is calculated as follows:\n```\ndef compute_reward(target_latency, accuracy, inference_latency, weight=0.07):\u00a0 \"\"\"Compute reward from accuracy and latency.\"\"\"\u00a0 speed_ratio = target_latency / inference_latency\u00a0 return accuracy * (speed_ratio**weight)\n```\nYou can use other variants of the `reward` calculation on page 3 of [the mnasnet paper](https://arxiv.org/pdf/1807.11626.pdf) .\n- `target_device_type`specifies the target device type that is [supported in Google Cloud](https://cloud.google.com/ml-engine/reference/rest/v1/AcceleratorType) , such as,`NVIDIA_TESLA_P100`.\n- `use_prebuilt_latency_calculator`uses our prebuilt latency-calculator [tf_vision/latency_computation_using_saved_model.py](https://github.com/google/vertex-ai-nas/blob/main/tf_vision/latency_computation_using_saved_model.py) .\n- `target_device_latency_ms`specifies the target device latency.\nFor information about how to customize the latency calculation function, see [tf_vision/latency_computation_using_saved_model.py](https://github.com/google/vertex-ai-nas/blob/main/tf_vision/latency_computation_using_saved_model.py) .\n## Monitor your Neural Architecture Search job progress\nIn the Google Cloud console, on the job page, the **chart** shows the `reward vs. trial number` while the **table** shows the rewards for each trial. You can find the top trials with the highest reward.\n## Plot a stage-2 training curve\nAfter stage-2 training, you use either Cloud Shell or Google Cloud `TensorBoard` to plot the training curve by pointing it to the job directory:\n## Deploy a selected model\nTo create a SavedModel, you can use the [export_saved_model.py](https://github.com/tensorflow/models/blob/master/official/vision/serving/export_saved_model.py) script with `params_override=${GCS_ROOT_DIR}/${TRIAL_ID}/params.yaml` .", "guide": "Vertex AI"}