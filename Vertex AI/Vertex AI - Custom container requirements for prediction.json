{"title": "Vertex AI - Custom container requirements for prediction", "url": "https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements", "abstract": "# Vertex AI - Custom container requirements for prediction\n**Note:** This is version 1.0.0 of this document. The document is updated according to [semantic versioning](https://semver.org/) .\nTo use a custom container to serve predictions from a custom-trained model, you must provide Vertex AI with a Docker container image that runs an HTTP server. This document describes the requirements that a container image must meet to be compatible with Vertex AI. The document also describes how Vertex AI interacts with your custom container once it starts running. In other words, this document describes what you need to consider when designing a container image to use with Vertex AI.\nTo walk through using a custom container image to serve predictions, read [Usinga custom container](/vertex-ai/docs/predictions/use-custom-container) .\n", "content": "## Container image requirements\nWhen your Docker container image runs as a container, the container must run an HTTP server. Specifically, the container must listen and respond to liveness checks, health checks, and prediction requests. The following subsections describe these requirements in detail.\nYou can implement the HTTP server in any way, using any programming language, as long as it meets the requirements in this section. For example, you can write a custom HTTP server using a web framework like [Flask](https://flask.palletsprojects.com/) or use machine learning (ML) serving software that runs an HTTP server, like [TensorFlowServing](https://www.tensorflow.org/tfx/guide/serving) , [TorchServe](https://pytorch.org/serve/) , or [KServe Python Server](https://github.com/kserve/kserve/tree/master/python/kserve#kserve-python-server) .\n### Run the HTTP server\nYou can run an HTTP server by using an [ENTRYPOINTinstruction](https://docs.docker.com/engine/reference/builder/#entrypoint) , a [CMDinstruction](https://docs.docker.com/engine/reference/builder/#cmd) , or both in the Dockerfile that you use to build your container image. Read about the [interaction between CMD andENTRYPOINT](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact) .\nAlternatively, you can specify the [containerSpec.command](/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model.ModelContainerSpec.FIELDS.command) and [containerSpec.args](/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model.ModelContainerSpec.FIELDS.args) fields when you create your `Model` resource in order to override your container image's `ENTRYPOINT` and `CMD` respectively. Specifying one of these fields lets you use a container image that would otherwise not meet the requirements due to an incompatible (or nonexistent) `ENTRYPOINT` or `CMD` .\nHowever you determine which command your container runs when it starts, ensure that this entrypoint command runs indefinitely. For example, don't run a command that starts an HTTP server in the background and then exits; if you do, then the container will exit immediately after it starts running.\nYour HTTP server must listen for requests on `0.0.0.0` , on a port of your choice. When you create a `Model` , specify this port in the [containerSpec.portsfield](/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model.ModelContainerSpec.FIELDS.ports) . To learn how the container can access this value, read [the section of thisdocument about the AIP_HTTP_PORT environment variable](#aip-variables) .\n### Liveness checks\nVertex AI performs a liveness check when your container starts to ensure that your server is running. When you [deploy a custom-trained model toan Endpoint resource](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/deployModel) , Vertex AI uses a [TCP livenessprobe](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-tcp-liveness-probe) to attempt to establish a TCP connection to your container on the configured port. The probe makes up to 4 attempts to establish a connection, waiting 10 seconds after each failure. If the probe still hasn't established a connection at this point, Vertex AI restarts your container.\nYour HTTP server doesn't need to perform any special behavior to handle these checks. As long as it is listening for requests on the configured port, the liveness probe is able to make a connection.\n### Health checks\nYou can optionally specify [startup_probe](/vertex-ai/docs/reference/rest/v1/ModelContainerSpec#FIELDS.startup_probe) or [health_probe](/vertex-ai/docs/reference/rest/v1/ModelContainerSpec#FIELDS.health_probe) .\nThe **startup probe** checks whether the container application has started. If startup probe isn't provided, there is no startup probe and health checks begin immediately. If startup probe is provided, health checks aren't performed until startup probe succeeds.\nLegacy applications that might require additional startup time on their first initialization should configure a startup probe. For example, if the application needs to copy the model artifacts from an external source, a startup probe should be configured to return success when that initialization is completed.\nThe **health probe** checks whether a container is ready to accept traffic. If health probe isn't provided, Vertex AI uses the default health checks as described in [Default health checks](#default-health) .\nLegacy applications that don't return `200 OK` to indicate the model is loaded and ready to accept traffic should configure a health probe. For example, an application might return `200 OK` to indicate success even though the actual model load status that's in the response body indicates that the model might not be loaded and might therefore not ready to accept traffic. In this case, a health probe should be configured to return success only when the model is loaded and ready to serve traffic.\nTo perform a probe, Vertex AI executes the specified `exec` command in the target container. If the command succeeds, it returns 0, and the container is considered to be alive and healthy.\nBy default, Vertex AI intermittently performs health checks on your HTTP server while it is running to ensure that it is ready to handle prediction requests. The service uses a health probe to send HTTP `GET` requests to a configurable health check path on your server. Specify this path in the [containerSpec.healthRoutefield](/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model.ModelContainerSpec.FIELDS.health_route) when you create a `Model` . To learn how the container can access this value, read [the section of this document about the AIP_HEALTH_ROUTE environmentvariable](#aip-variables) .\nConfigure the HTTP server to respond to each health check request as follows:\n- **If the server is ready to handle prediction requests,** respond to the health check request within 10 seconds with status code `200 OK` . The contents of the response body don't matter; Vertex AI ignores them.This response signifies that the server is healthy.\n- **If the server isn't ready to handle prediction requests,** don't respond to the health check request within 10 seconds, or respond with any status code except for `200 OK` . For example, respond with status code `503 Service Unavailable` .This response (or lack of a response) signifies that the server is unhealthy.\nIf the health probe ever receives an unhealthy response from your server (including no response within 10 seconds), then it sends up to 3 additional health checks at 10 second intervals. During this period, Vertex AI still considers your server healthy. If the probe receives a healthy response to any of these checks, then the probe immediately returns to its intermittent schedule of health checks. However, if the probe receives 4 consecutive unhealthy responses, then Vertex AI stops routing prediction traffic to the container. (If the `DeployedModel` resource is scaled to use multiple prediction nodes, then Vertex AI routes prediction requests to other, healthy containers.)\nVertex AI doesn't restart the container; instead the health probe continues sending intermittent health check requests to the unhealthy server. If it receives a healthy response, then it marks that container as healthy and starts to route prediction traffic to it again.\nIn some cases, it is sufficient for the HTTP server in your container to always respond with status code `200 OK` to health checks. If your container loads resources before starting the server, then the container is unhealthy during the startup period and during any periods when the HTTP server fails. At all other times, it responds as healthy.\nFor a more sophisticated configuration, you might want to purposefully design the HTTP server to respond to health checks with an unhealthy status at certain times. For example, you might want to block prediction traffic to a node for a period so that the container can perform maintenance.\n### Prediction requests\nWhen a client sends a [projects.locations.endpoints.predictrequest](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/predict) to the Vertex AI API, Vertex AI forwards this request as an HTTP `POST` request to a configurable prediction path on your server. Specify this path in the [containerSpec.predictRoutefield](/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model.ModelContainerSpec.FIELDS.predict_route) when you create a `Model` . To learn how the container can access this value, read [the section of this document about the AIP_PREDICT_ROUTEenvironment variable](#aip-variables) .\nIf the model is deployed to a public endpoint, each prediction request must be 1.5 MB or smaller. The HTTP server must accept prediction requests that have the `Content-Type: application/json` HTTP header and JSON bodies with the following format:\n```\n{\u00a0 \"instances\": INSTANCES,\u00a0 \"parameters\": PARAMETERS}\n```\nIn these requests:\n- is an array of one or more JSON values of any type. Each values represents an instance that you are providing a prediction for.\n- is a JSON object containing any parameters that your container requires to help serve predictions on the instances. Vertex AI considers the `parameters` field optional, so you can design your container to require it, only use it when provided, or ignore it.\nLearn more about the [request body requirements](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/predict#request-body) .\nIf the model is deployed to a public endpoint, each prediction response must be 1.5 MB or smaller. The HTTP server must send responses with JSON bodies that meet the following format:\n```\n{\u00a0 \"predictions\": PREDICTIONS}\n```\nIn these responses, replace with an array of JSON values representing the predictions that your container has generated for each of the in the corresponding request.\nAfter your HTTP server sends this response, Vertex AI adds a [deployedModelIdfield](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/predict#body.PredictResponse.FIELDS.deployed_model_id) to the response before returning it to the client. This field specifies which [DeployedModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints#deployedmodel) on an `Endpoint` is sending the response. Learn more about the [response bodyformat](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/predict#response-body) .\n**Note:** If your container doesn't meet these requirements for receiving requests and sending responses, you can still use the container for [raw predictions](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/rawPredict) .\n## Container image publishing requirements\nYou must push your container image to [Artifact Registry](/artifact-registry/docs/overview) in order to use it with Vertex AI. Learn how to [push a container image toArtifact Registry](/artifact-registry/docs/docker/pushing-and-pulling) .\nIn particular, you must push the container image to a repository that meets the following location and permissions requirements.\n### Location\nWhen you use Artifact Registry, the repository must use [aregion](/artifact-registry/docs/repo-locations) that matches the [regional endpoint](/vertex-ai/docs/general/locations) where you plan to create a `Model` . For example, if you plan to create a `Model` on the `us-central1-aiplatform.googleapis.com` endpoint, then the full name of your container image must start with `us-central1-docker.pkg.dev/` . Don't use a multi-regional repository for your container image.\n### Permissions\nVertex AI must have permission to pull the container image when you create a `Model` . Specifically, the Vertex AI Service Agent for your project must have the permissions of the [Artifact Registry Reader role(roles/artifactregistry.reader)](/artifact-registry/docs/access-control#roles) for the container image's repository.\nThe Vertex AI Service Agent for your project is the [Google-managed serviceaccount](/iam/docs/service-account-types#google-managed) that Vertex AI uses to interact with other Google Cloud services. This service account has the email address `service-` `` `@gcp-sa-aiplatform.iam.gserviceaccount.com` , where is replaced with the [projectnumber](/resource-manager/docs/creating-managing-projects#identifying_projects) of your Vertex AI project.\nIf you have pushed your container image to the same Google Cloud project where you are using Vertex AI, then you don't have to configure any permissions. The default permissions granted to the Vertex AI Service Agent are sufficient.\nOn the other hand, if you have pushed your container image to a different Google Cloud project from the one where you are using Vertex AI, then you must [grant the Artifact Registry Reader role for the Artifact Registryrepository](/artifact-registry/docs/access-control#grant-repo) to the Vertex AI Service Agent.\n**Note:** Even if you plan to [use a custom service account in your customcontainer](/vertex-ai/docs/general/custom-service-account) , there is no need to grant your custom service account Artifact Registry permissions; only the Google-managed service account needs these permissions. Vertex AI doesn't use your custom service account to pull container images. It only uses the custom service account to run your model serving code.\n## Access model artifacts\nWhen you create a custom-trained `Model` without a custom container, you must specify the URI of a Cloud Storage directory with [modelartifacts](/vertex-ai/docs/training/exporting-model-artifacts) as the [artifactUrifield](/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model.FIELDS.artifact_uri) . When you create a `Model` with a custom container, providing model artifacts in Cloud Storage is optional.\nIf the container image includes the model artifacts that you need to serve predictions, then there is no need to load files from Cloud Storage. However, if you do provide model artifacts by specifying the `artifactUri` field, then the container must load these artifacts when it starts running. When Vertex AI starts your container, it sets the `AIP_STORAGE_URI` environment variable to a Cloud Storage URI that begins with `gs://` . Your container's entrypoint command can download the directory specified by this URI in order to access the model artifacts.\nNote that the value of the `AIP_STORAGE_URI` environment variable isn't identical to the Cloud Storage URI that you specify in the `artifactUri` field when you create the `Model` . Rather, `AIP_STORAGE_URI` points to a copy of your model artifact directory in a different Cloud Storage bucket, which Vertex AI manages. Vertex AI populates this directory when you create a `Model` . You can't update the contents of the directory. If you want to use new model artifacts, then you must create a new `Model` .\nThe service account that your container uses by default has permission to read from this URI.\nOn the other hand, if you [specify a custom serviceaccount](/vertex-ai/docs/general/custom-service-account) when you deploy the `Model` to an `Endpoint` , Vertex AI automatically grants your specified service account the [Storage Object Viewer (roles/storage.objectViewer)role](/storage/docs/access-control/iam-roles) for the URI's Cloud Storage bucket.\nUse any library that supports [Application DefaultCredentials (ADC)](/docs/authentication#adc) to load the model artifacts; you don't need to explicitly configure authentication.\n## Environment variables available in the container\nWhen running, the container's entrypoint command can reference environment variables that you have configured manually, as well as environment variables set automatically by Vertex AI. This section describes each way that you can set environment variables, and it provides details about the variables set automatically by Vertex AI.\n### Variables set in the container image\nTo set environment variables in the container image when you build it, use Docker's [ENVinstruction](https://docs.docker.com/engine/reference/builder/#env) . Don't set any environment variables that begin with the prefix `AIP_` .\nThe container's entrypoint command can use these environment variables, but you can't reference them in any of your [Model's APIfields](/vertex-ai/docs/reference/rest/v1/projects.locations.models) .\n### Variables set by Vertex AI\nWhen Vertex AI starts running the container, it sets the following environment variables in the container environment. Each variable begins with the prefix `AIP_` . Don't manually set any environment variables that use this prefix.\nThe container's entrypoint command can access these variables. To learn which Vertex AI API fields can also reference these variables, read the [API reference forModelContainerSpec](/vertex-ai/docs/reference/rest/v1/projects.locations.models#modelcontainerspec) .\n**Note:** Docker instructions used to build the container image, like [RUN](https://docs.docker.com/engine/reference/builder/#run) , can't access these variables, because you must build the container image before you provide it to Vertex AI.\n**Note:** Vertex AI sets the values of some of the following variables when you deploy your `Model` as a `DeployedModel` to an `Endpoint` . You can reference in other API fields when you create the `Model` , and the values get expanded each time you deploy the `Model` .\n| Variable name   | Default value                                                          | How to configure value                            | Details                                                                       |\n|:----------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| AIP_ACCELERATOR_TYPE | Unset                                                            | When you deploy a Model as a DeployedModel to an Endpoint resource, set the dedicatedResources.machineSpec.acceleratorType field. | If applicable, this variable specifies the type of accelerator used by the virtual machine (VM) instance that the container is running on.                                      |\n| AIP_DEPLOYED_MODEL_ID | A string of digits identifying the DeployedModel to which this container's Model has been deployed.                                    | Not configurable                             | This value is the DeployedModel's id field.                                                              |\n| AIP_ENDPOINT_ID  | A string of digits identifying the Endpoint on which the container's Model has been deployed.                                      | Not configurable                             | This value is the last segment of the Endpoint's name field (following endpoints/).                                                    |\n| AIP_FRAMEWORK   | CUSTOM_CONTAINER                                                         | Not configurable                             | nan                                                                        |\n| AIP_HEALTH_ROUTE  | /v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODEL In this string, replace ENDPOINT with the value of the AIP_ENDPOINT_ID variable and replace DEPLOYED_MODEL with the value of the AIP_DEPLOYED_MODEL_ID variable.       | When you create a Model, set the containerSpec.healthRoute field.                 | This variables specifies the HTTP path on the container that Vertex AI sends health checks to.                                                 |\n| AIP_HTTP_PORT   | 8080                                                            | When you create a Model, set the containerSpec.ports field. The first entry in this field becomes the value of AIP_HTTP_PORT.  | Vertex AI sends liveness checks, health checks, and prediction requests to this port on the container. Your container's HTTP server must listen for requests on this port.                              |\n| AIP_MACHINE_TYPE  | No default, must be configured                                                      | When you deploy a Model as a DeployedModel to an Endpoint resource, set the dedicatedResources.machineSpec.machineType field.  | This variable specifies the type of VM that the container is running on.                                                      |\n| AIP_MODE    | PREDICTION                                                           | Not configurable                             | This variable signifies that the container is running on Vertex AI to serve online predictions. You can use this environment variable to add custom logic to your container, so that it can run in multiple computing environments but only use certain code paths on when run on Vertex AI. |\n| AIP_MODE_VERSION  | 1.0.0                                                            | Not configurable                             | This variable signifies the version of the custom container requirements (this document) that Vertex AI expects the container to meet. This document updates according to semantic versioning.                         |\n| AIP_MODEL_NAME  | The value of the AIP_ENDPOINT_ID variable                                                   | Not configurable                             | See the AIP_ENDPOINT_ID row. This variable exists for compatibility reasons.                                                     |\n| AIP_PREDICT_ROUTE  | /v1/endpoints/ENDPOINT/deployedModels/DEPLOYED_MODEL:predict In this string, replace ENDPOINT with the value of the AIP_ENDPOINT_ID variable and replace DEPLOYED_MODEL with the value of the AIP_DEPLOYED_MODEL_ID variable.     | When you create a Model, set the containerSpec.predictRoute field.                 | This variable specifies the HTTP path on the container that Vertex AI forwards prediction requests to.                                               |\n| AIP_PROJECT_NUMBER | The project number of the Google Cloud project where you are using Vertex AI                                          | Not configurable                             | nan                                                                        |\n| AIP_STORAGE_URI  | If you don't set the artifactUri field when you create a Model: an empty string If you do set the artifactUri field when you create a Model: a Cloud Storage URI (starting with gs://) specifying a directory in a bucket managed by Vertex AI | Not configurable                             | This variable specifies the directory that contains a copy of your model artifacts, if applicable.                                                |\n| AIP_VERSION_NAME  | The value of the AIP_DEPLOYED_MODEL_ID variable                                                  | Not configurable                             | See the AIP_DEPLOYED_MODEL_ID row. This variable exists for compatibility reasons.                                                    |\n### Variables set in the Model resource\nWhen you create a `Model` , you can set additional environment variables in the [containerSpec.envfield](/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model.ModelContainerSpec.FIELDS.env) .\nThe container's entrypoint command can access these variables. To learn which Vertex AI API fields can also reference these variables, read the [API reference forModelContainerSpec](/vertex-ai/docs/reference/rest/v1/projects.locations.models#modelcontainerspec) .\n**Note:** Docker instructions used to build the container image, like [RUN](https://docs.docker.com/engine/reference/builder/#run) , can't access these variables, because you must build the container image before you provide it to Vertex AI.\n## What's next\n- [Learn more about serving predictions using a customcontainer](/vertex-ai/docs/predictions/use-custom-container) , including how to specify container-related API fields when you import a model.", "guide": "Vertex AI"}