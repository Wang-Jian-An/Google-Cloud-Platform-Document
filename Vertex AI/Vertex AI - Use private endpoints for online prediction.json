{"title": "Vertex AI - Use private endpoints for online prediction", "url": "https://cloud.google.com/vertex-ai/docs/predictions/using-private-endpoints", "abstract": "# Vertex AI - Use private endpoints for online prediction\nUsing private endpoints to serve online predictions with Vertex AI provides a low-latency, secure connection to the Vertex AI online prediction service. This guide shows how to configure private endpoints on Vertex AI by using VPC Network Peering to peer your network with the Vertex AI online prediction service.\n", "content": "## Overview\nBefore you serve online predictions with private endpoints, you must [configureprivate services access to create peering connections between your networkand Vertex AI](/vertex-ai/docs/general/vpc-peering) . If you have already set this up, you can use your existing peering connections.\nThis guide covers the following tasks:\n- Verifying the status of your existing peering connections.\n- Verifying the necessary APIs are enabled.\n- Creating a private endpoint.\n- Deploying a model to a private endpoint.- Only support one model per private endpoint. This is different from a public Vertex AI endpoint where you can split traffic across multiple models deployed to one endpoint.\n- Private endpoint supports AutoML tabular and custom trained models.\n- Sending a prediction to a private endpoint.\n- Cleaning up resources## Check the status of existing peering connections\nIf you have existing peering connections you use with Vertex AI, you can list them to check status:\n```\ngcloud compute networks peerings list --network NETWORK_NAME\n```\nYou should see that the state of your peering connections are `ACTIVE` . Learn more about [active peering connections](/vpc/docs/using-vpc-peering#step_3_vpc_network_peering_becomes_active) .\n## Enable the necessary APIs\n```\ngcloud services enable aiplatform.googleapis.comgcloud services enable dns.googleapis.com\n```\n## Create a private endpoint\nTo create a private endpoint, add the `--network` flag when you [create anendpoint using the Google Cloud CLI](/vertex-ai/docs/predictions/deploy-model-api#create-endpoint) :\n```\ngcloud beta ai endpoints create \\\u00a0 --display-name=ENDPOINT_DISPLAY_NAME \\\u00a0 --network=FULLY_QUALIFIED_NETWORK_NAME \\\u00a0 --region=REGION\n```\nReplace with the fully qualified network name:\n```\nprojects/PROJECT_NUMBER/global/networks/NETWORK_NAME\n```\nIf you create the endpoint without specifying a network, then you create a public endpoint.\n### Limitations of private endpoints\nNote the following limitations for private endpoints:\n- Private endpoints currently do not support traffic splitting. As a workaround, you can create traffic splitting manually by deploying your model to multiple private endpoints, and splitting traffic among the resulting prediction URLs for each private endpoint.\n- Private endpoints don't support SSL/TLS.\n- To enable access logging on a private endpoint, contact vertex-ai-feedback@google.com.\n- You can use only one network for all private endpoints in a Google Cloud project. If you want to change to another network, contact vertex-ai-feedback@google.com.\n- Client side retry on recoverable errors are highly recommended. These can include the following errors:- Empty response (HTTP error code`0`), possibly due to a transient broken connection.\n- HTTP error codes`5xx`that indicate the service might be temporarily unavailable.\n- For the HTTP error code`429`that indicates the system is currently overloaded, consider slowing down traffic to mitigate this issue instead of retrying.\n- Prediction requests from [PredictionServiceClient](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1.services.prediction_service.PredictionServiceClient) in the [Vertex AI Python client library](https://cloud.google.com/vertex-ai/docs/start/client-libraries) are not supported.\n### Monitor private endpoints\nYou can use the metrics dashboard to inspect the availability and latency of the traffic sent to a private endpoint.\nTo customize monitoring, query the following two metrics in Cloud Monitoring:\n- `aiplatform.googleapis.com/prediction/online/private/response_count`The number of prediction responses. You can filter this metric by `deployed_model_id` or HTTP response code.\n- `aiplatform.googleapis.com/prediction/online/private/prediction_latencies`The latency of the prediction request in milliseconds. You can filter this metric by `deployed_model_id` , only for successful requests.\nLearn [how to select, query, and display these metrics in Metrics Explorer](/monitoring/charts/metrics-selector) .\n## Deploy a model\nYou can import a new model, or deploy an existing model that you have already uploaded. To upload a new model, use [gcloud ai models upload](/sdk/gcloud/reference/ai/models/upload) . For more information, see [Import models to Vertex AI](/vertex-ai/docs/model-registry/import-model) .\n- To deploy a model to a private endpoint, see the guide to [deploy models](/vertex-ai/docs/predictions/deploy-model-api#deploying_the_model) . Besides traffic splitting and manually enabling access logging, you can use any of the other options available for deploying custom-trained models. Refer to the [limitations of private endpoints](#limitations) to learn more about how they are different from public endpoints.\n- After you deploy the endpoint, you can get the prediction URI from the metadata of your private endpoint.- If you have the display name of your private endpoint, run this command to get the endpoint ID:```\nENDPOINT_ID=$(gcloud ai endpoints list \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --region=REGION \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --filter=displayName:ENDPOINT_DISPLAY_NAME \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 --format=\"value(ENDPOINT_ID.scope())\")\n```Otherwise, to view the endpoint ID and display name for all of your endpoints, run the following command:```\ngcloud ai endpoints list --region=REGION\n```\n- Finally, to get the prediction URI, run the following command:```\ngcloud beta ai endpoints describe ENDPOINT_ID \\\u00a0 --region=REGION \\\u00a0 --format=\"value(deployedModels.privateEndpoints.predictHttpUri)\"\n```\n### Private prediction URI format\n**Note:** You might see `http://aiplatform` `` `.googleapis.com/v1/models/` `` `:predict` format for older private endpoints where: is a hash ID in your prediction URL that contains six alphanumeric characters and is a hash ID for your deployed model that contains 21 characters (two sets of 10 alphanumeric characters connected by a hyphen).\nThe prediction URI looks different for private endpoints compared to Vertex AI public endpoints:\n`http://` `` `.aiplatform.googleapis.com/v1/models/` `` `:predict`\nIf you choose to undeploy the current model and redeploy with a new one, the domain name is reused but the path includes a different deployed model ID.\n## Send a prediction to a private endpoint\n- [Create a Compute Engine instance](/compute/docs/instances/create-start-instance) in your VPC network. Make sure to [create theinstance in the same VPC network that you have peered withVertex AI](/compute/docs/instances/create-start-instance#creating_an_instance_in_a_specific_subnet) .\n- SSH into your Compute Engine instance, and install your prediction client, if applicable. Otherwise, you can use curl.\n- When predicting, use the prediction URL obtained from model deployment. In this example, you're sending the request from your prediction client in your Compute Engine instance in the same VPC network:```\ncurl -X POST -d@PATH_TO_JSON_FILE \u00a0http://ENDPOINT_ID.aiplatform.googleapis.com/v1/models/DEPLOYED_MODEL_ID:predict\n```In this sample request, is the path to your prediction request, saved as a JSON file. For example, `example-request.json` .## Clean up resources\nYou can undeploy models and delete private endpoints the same way as with public models and endpoints. You can only create private endpoints on one network per Google Cloud project, even if you delete these resources on your own afterward. If you need to switch to a different network, please contact vertex-ai-feedback@google.com.\n## Example: Test private endpoints in Shared VPC\nThis example uses two Google Cloud projects with a Shared VPC network:\n- Thehosts the Shared VPC network.\n- Thehosts a Compute Engine instance where you run a prediction client, such as curl, or your own REST client in the Compute Engine instance, to send prediction requests.\nWhen you create the Compute Engine instance in the client project, it must be within the custom subnet in the host project's Shared VPC network, and in the same region where the model gets deployed.\n- Create the peering connections for private services access in the host project. Run [gcloud services vpc-peerings connect](/sdk/gcloud/reference/services/vpc-peerings/connect) :```\ngcloud services vpc-peerings connect \\\u00a0 --service=servicenetworking.googleapis.com \\\u00a0 --network=HOST_SHARED_VPC_NAME \\\u00a0 --ranges=PREDICTION_RESERVED_RANGE_NAME \\\u00a0 --project=HOST_PROJECT_ID\n```\n- Create the endpoint in the client project, using the host project's network name. Run [gcloud beta ai endpoints create](/sdk/gcloud/reference/beta/ai/endpoints/create) :```\ngcloud beta ai endpoints create \\\u00a0 --display-name=ENDPOINT_DISPLAY_NAME \\\u00a0 --network=HOST_SHARED_VPC_NAME \\\u00a0 --region=REGION \\\u00a0 --project=CLIENT_PROJECT_ID\n```\n- [Send prediction requests](#sending-prediction-to-private-endpoint) , using the prediction client within the client project.## Example: Private endpoints with non-RFC 1918 subnets\nThis example uses non-RFC 1918 addresses to create private endpoints.\n- Reserve IP ranges for nodes, pods and services. IP ranges for nodes must be from RFC 1918 range.\n- Create the peering connection for private service access using the reserved ranges.```\ngcloud services vpc-peerings connect \\\u00a0 --service=servicenetworking.googleapis.com \\\u00a0 --network=VPC_NAME \\\u00a0 --ranges=NODES_RANGE_NAME,PODS_RANGE_NAME,SERVICES_RANGE_NAME \\\u00a0 --project=PROJECT_ID\n```\n- Create the endpoint, specifying the reserved ranges for nodes, pods and services. RequestedIpRangeConfig can only be specified from REST API.```\n# Sample json request for endpoint creation.{\u00a0 displayName: \"my_endpoint\",\u00a0 network: \"projects/<project_num>/global/networks/<network>\",\u00a0 requestedIpRangeConfig: {\u00a0 \u00a0 nodesIpRange: {\u00a0 \u00a0 \u00a0 ipAddress: \"xxx.xx.x.x\",\u00a0 \u00a0 \u00a0 ipPrefixLength: 22\u00a0 \u00a0 },\u00a0 \u00a0 podsIpRange: {\u00a0 \u00a0 \u00a0 ipAddress: \"yyy.yy.y.y\",\u00a0 \u00a0 \u00a0 ipPrefixLength: 17\u00a0 \u00a0 },\u00a0 \u00a0 servicesIpRange: {\u00a0 \u00a0 \u00a0 ipAddress: \"zzz.zz.z.z\",\u00a0 \u00a0 \u00a0 ipPrefixLength: 22\u00a0 \u00a0 }\u00a0 }}\n```\n- [Send prediction requests](#sending-prediction-to-private-endpoint) , using the prediction client within the client project.", "guide": "Vertex AI"}