{"title": "Vertex AI - Introduction to Vertex Explainable AI", "url": "https://cloud.google.com/vertex-ai/docs/explainable-ai/overview", "abstract": "# Vertex AI - Introduction to Vertex Explainable AI\nMachine learning models are often seen as \"black boxes\", where even its designers can't explain how or why a model produced a specific prediction. Vertex Explainable AI offers and explanations to provide better understanding of model decision making.\nKnowing how a model behaves, and how it is influenced by its training dataset, gives anyone who builds or uses ML new abilities to improve models, build confidence in their predictions, and understand when and why things go awry.\n", "content": "## Example-based explanations\nWith example-based explanations, Vertex AI uses to return a list of examples (typically from the training set) that are most similar to the input. Because we generally expect similar inputs to yield similar predictions, we can use these explanations to explore and explain our model's behavior.\nExample-based explanations can be useful in several scenarios:\n- **Improve your data or model** : One of the core use cases for example-based explanations is helping you understand why your model made certain mistakes in its predictions, and using those insights to improve your data or model. To do so, first select test data that is of interest to you. This could be either driven by business needs or heuristics like data where the model made the most egregious mistakes.For example, suppose we have a model that classifies images as either a bird or a plane, and that it is misclassifying the following bird as a plane with high confidence. You can use Example-based explanations to retrieve similar images from the training set to figure out what is happening.Since all of its explanations are dark silhouettes from the plane class, it's a signal to get more bird silhouettes.However, if the explanations were mainly from the bird class, it's a signal that our model is unable to learn relationships even when the data is rich, and we should consider increasing model complexity (for example, adding more layers).\n- **Interpret novel data** : Let's say your model was trained to classify birds and planes, but in the real world, the model also encounters images of kites, drones, and helicopters. If your nearest neighbor dataset includes some labeled images of kites, drones, and helicopters, you can use example-based explanations to classify novel images by applying the most frequently occurring label of its nearest neighbors. This is possible because we expect the latent representation of kites to be different from that of birds or planes and more similar to the labeled kites in the nearest neighbor dataset.\n- **Detect anomalies** : Intuitively, if an instance is far away from all of the data in the training set, then it is likely an outlier. Neural networks are known to be overconfident in their mistakes, thus masking their errors. Monitoring your models using example-based explanations helps identify the most serious outliers.\n- **Active learning** : Example-based explanations can help you identify the instances that might benefit from human labeling. This is particularly useful if labeling is slow or expensive, ensuring that you get the richest possible dataset from limited labeling resources.For example, suppose we have a model that classifies a medical patient as either having a cold or a flu. If a patient is classified as having the flu, and all of her example-based explanations are from the flu class, then the doctor can be more confident in the model's prediction without having to take a closer look. However, if some of the explanations are from the flu class, and some others from cold class, it would be worthwhile to get a doctor's opinion. This will lead to a dataset where difficult instances have more labels, making it easier for downstream models to learn complex relationships.\nTo create a Model that supports example-based explanations, see [Configuringexample-basedexplanations](/vertex-ai/docs/explainable-ai/configuring-explanations-example-based) .\n### Supported model types\nAny TensorFlow model that can provide an embedding (latent representation) for inputs is supported. Tree-based models, such as decision trees, are not supported. Models from other frameworks, such as PyTorch or XGBoost, are not supported yet.\nFor deep neural networks, we generally assume that the higher layers (closer to the output layer) have learned something \"meaningful\", and thus, the penultimate layer is often chosen for embeddings. You can experiment with a few different layers, investigate the examples you are getting, and choose one based on some quantitative (class match) or qualitative (looks sensible) measures.\nFor a demonstration on how to extract embeddings from a TensorFlow model and perform nearest neighbor search, see the [example-based explanationnotebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/gapic/custom/showcase_custom_image_classification_online_explain_example_based_api.ipynb) .\n## Feature-based explanations\nVertex Explainable AI integrates into Vertex AI. This section provides a brief conceptual overview of the feature attribution methods available with Vertex AI.\nFeature attributions indicate how much each feature in your model contributed to the predictions for each given instance. When you request predictions, you get predicted values as appropriate for your model. When you request , you get the predictions along with feature attribution information.\nFeature attributions work on tabular data, and include built-in visualization capabilities for image data. Consider the following examples:\n- A deep neural network is trained to predict the duration of a bike ride, based on weather data and previous ride sharing data. If you request only predictions from this model, you get predicted durations of bike rides in number of minutes. If you request , you get the predicted bike trip duration, along with an attribution score for each feature in your explanations request. The attribution scores show how much the feature affected the change in prediction value, relative to the baseline value that you specify. Choose a meaningful baseline that makes sense for your model - in this case, the median bike ride duration. You can plot the feature attribution scores to see which features contributed most strongly to the resulting prediction:\n- An image classification model is trained to predict whether a given image contains a dog or a cat. If you request predictions from this model on a new set of images, then you receive a prediction for each image (\"dog\" or \"cat\"). If you request , you get the predicted class along with an overlay for the image, showing which pixels in the image contributed most strongly to the resulting prediction:\n- An image classification model is trained to predict the species of a flower in the image. If you request predictions from this model on a new set of images, then you receive a prediction for each image (\"daisy\" or \"dandelion\"). If you request , you get the predicted class along with an overlay for the image, showing which areas in the image contributed most strongly to the resulting prediction:\n### Supported model types\nFeature attribution is supported for all types of models (both AutoML and custom-trained), frameworks (TensorFlow, scikit, XGBoost), BigQuery ML models, and modalities (images, text, tabular, video).\nTo use feature attribution, [configure your model for featureattribution](/vertex-ai/docs/explainable-ai/configuring-explanations-feature-based) when you upload or register the model to the Vertex AI Model Registry.\nAdditionally, for the following types of AutoML models, feature attribution is integrated into the Google Cloud console:\n- AutoML image models (classification models only)\n- AutoML tabular models (classification and regression models only)\nFor AutoML model types that are integrated, you can enable feature attribution in the Google Cloud console during training and see for the model overall, and for both [online](/vertex-ai/docs/predictions/online-predictions-automl) and [batch](/vertex-ai/docs/predictions/batch-predictions) predictions.\nFor AutoML model types that are not integrated, you can still enable feature attribution by exporting the model artifacts and configuring feature attribution when you upload the model artifacts to the Vertex AI Model Registry.\n### Advantages\nIf you inspect specific instances, and also aggregate feature attributions across your training dataset, you can get deeper insight into how your model works. Consider the following advantages:\n- **Debugging models** : Feature attributions can help detect issues in the data that standard model evaluation techniques would usually miss.For example, an image pathology model achieved suspiciously good results on a test dataset of chest X-Ray images. Feature attributions revealed that the model's high accuracy depended on the radiologist's pen marks in the image. For more details about this example, see the [AI Explanations Whitepaper](https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf) .\n- **Optimizing models** : You can identify and remove features that are less important, which can result in more efficient models.\n### Feature attribution methods\nEach feature attribution method is based on - a cooperative game theory algorithm that assigns credit to each player in a game for a particular outcome. Applied to machine learning models, this means that each model feature is treated as a \"player\" in the game. Vertex Explainable AI assigns proportional credit to each feature for the outcome of a particular prediction.\nThe method provides a sampling approximation of exact Shapley values. AutoML tabular models use the sampled Shapley method for feature importance. Sampled Shapley works well for these models, which are meta-ensembles of trees and neural networks.\nFor in-depth information about how the sampled Shapley method works, read the paper [Bounding the Estimation Error of Sampling-based Shapley ValueApproximation](https://arxiv.org/abs/1306.4265) .\n[](None)\nIn the method, the gradient of the prediction output is calculated with respect to the features of the input, along an integral path.\n- The gradients are calculated at different intervals of a scaling parameter. The size of each interval is determined by using the [Gaussianquadrature](https://en.wikipedia.org/wiki/Gaussian_quadrature) rule. (For image data, imagine this scaling parameter as a \"slider\" that is scaling all pixels of the image to black.)\n- The gradients are integrated as follows:- The integral is approximated by using a weighted average.\n- The element-wise product of the averaged gradients and the original input is calculated.For an intuitive explanation of this process as applied to images, refer to the blog post, [\"Attributing a deep network's prediction to its input features\"](http://www.unofficialgoogledatascience.com/2017/03/attributing-deep-networks-prediction-to.html) . The authors of the original paper about integrated gradients ( [AxiomaticAttribution for Deep Networks](https://arxiv.org/abs/1703.01365) ) show in the preceding blog post what the images look like at each step of the process.\nThe method combines the integrated gradients method with additional steps to determine which of the image contribute the most to a given class prediction.\n- Pixel-level attribution: XRAI performs pixel-level attribution for the input image. In this step, XRAI uses the integrated gradients method with a black baseline and a white baseline.\n- Oversegmentation: Independently of pixel-level attribution, XRAI oversegments the image to create a patchwork of small regions. XRAI uses [Felzenswalb's graph-based method](http://people.cs.uchicago.edu/%7Epff/papers/seg-ijcv.pdf) to create the image segments.\n- Region selection: XRAI aggregates the pixel-level attribution within each segment to determine its attribution density. Using these values, XRAI ranks each segment and then orders the segments from most to least positive. This determines which areas of the image are most salient, or contribute most strongly to a given class prediction.### Compare feature attribution methods\nVertex Explainable AI offers three methods to use for feature attributions: , , and .\n| Method          | Basic explanation                                          | Recommended model types                                    | Example use cases               | Compatible Vertex AI Model resources                     |\n|:----------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|\n| Sampled Shapley        | Assigns credit for the outcome to each feature, and considers different permutations of the features. This method provides a sampling approximation of exact Shapley values.   | Non-differentiable models, such as ensembles of trees and neural networks                       | Classification and regression on tabular data        | Any custom-trained model (running in any prediction container) AutoML tabular models         |\n| Integrated gradients       | A gradients-based method to efficiently compute feature attributions with the same axiomatic properties as the Shapley value.               | Differentiable models, such as neural networks. Recommended especially for models with large feature spaces. Recommended for low-contrast images, such as X-rays. | Classification and regression on tabular data Classification on image data | Custom-trained TensorFlow models that use a TensorFlow prebuilt container to serve predictions AutoML image models |\n| XRAI (eXplanation with Ranked Area Integrals) | Based on the integrated gradients method, XRAI assesses overlapping regions of the image to create a saliency map, which highlights relevant regions of the image rather than pixels. | Models that accept image inputs. Recommended especially for natural images, which are any real-world scenes that contain multiple objects.      | Classification on image data            | Custom-trained TensorFlow models that use a TensorFlow prebuilt container to serve predictions AutoML image models |\nFor a more thorough comparison of attribution methods, see the [AI ExplanationsWhitepaper](https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf) .\n**Note:** This section only applies to custom-trained TensorFlow models that use a [TensorFlow prebuilt container](/vertex-ai/docs/predictions/pre-built-containers#tensorflow) to serve predictions.\nIn models, you can calculate the derivative of all the operations in your TensorFlow graph. This property helps to make backpropagation possible in such models. For example, neural networks are differentiable. To get feature attributions for differentiable models, use the integrated gradients method.\nThe integrated gradients method does **not** work for non-differentiable models. Learn more about [encoding non-differentiableinputs](/vertex-ai/docs/explainable-ai/configuring-explanations#ig-metadata-considerations) to work with the integrated gradients method.\nmodels include non-differentiable operations in the TensorFlow graph, such as operations that perform decoding and rounding tasks. For example, a model built as an ensemble of trees and neural networks is non-differentiable. To get feature attributions for non-differentiable models, use the sampled Shapley method. Sampled Shapley also works on differentiable models, but in that case, it is more computationally expensive than necessary.\n### Conceptual limitations\nConsider the following limitations of feature attributions:\n- Feature attributions, including local feature importance for AutoML, are specific to individual predictions. Inspecting the feature attributions for an individual prediction may provide good insight, but the insight may not be generalizable to the entire class for that individual instance, or the entire model.To get more generalizable insight for AutoML models, refer to the model feature importance. To get more generalizable insight for other models, aggregate attributions over subsets over your dataset, or the entire dataset.\n- Although feature attributions can help with model debugging, they do not always indicate clearly whether an issue arises from the model or from the data that the model is trained on. Use your best judgment, and diagnose common data issues to narrow the space of potential causes.\n- Feature attributions are subject to similar adversarial attacks as predictions in complex models.\nFor more information about limitations, refer to the [high-level limitationslist](/vertex-ai/docs/explainable-ai/limitations) and the [AI Explanations Whitepaper](https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf) .\n## References\nFor feature attribution, the implementations of sampled Shapley, integrated gradients, and XRAI are based on the following references, respectively:\n- [Bounding the Estimation Error of Sampling-based Shapley ValueApproximation](https://arxiv.org/abs/1306.4265) \n- [Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365) \n- [XRAI: Better Attributions Through Regions](https://arxiv.org/abs/1906.02825) \nLearn more about the implementation of Vertex Explainable AI by reading the [AIExplanations Whitepaper](https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf) .\n## Notebooks\nTo get started using Vertex Explainable AI, use these notebooks:\n| Notebook | Explainability method    | ML framework | Modality | Task                                |\n|:------------|:-------------------------------------|:---------------|:-----------|:----------------------------------------------------------------------------------------------------------------------------------|\n| GitHub link | example-based explanations   | TensorFlow  | image  | Train a classification model that predicts the class of the provided input image and get online explanations      |\n| GitHub link | feature-based      | AutoML   | tabular | Train a binary classification model that predicts whether a bank custom purchased a term deposit and get batch explanations  |\n| GitHub link | feature-based      | AutoML   | tabular | Train a classification model that predicts the type of Iris flower species and get online explanations       |\n| GitHub link | feature-based (sampled Shapley)  | scikit-learn | tabular | Train a linear regression model that predicts taxi fares and get online explanations            |\n| GitHub link | feature-based (integrated gradients) | TensorFlow  | image  | Train a classification model that predicts the class of the provided input image and get batch explanations      |\n| GitHub link | feature-based (integrated gradients) | TensorFlow  | image  | Train a classification model that predicts the class of the provided input image and get online explanations      |\n| GitHub link | feature-based (integrated gradients) | TensorFlow  | tabular | Train a regression model that predicts the median price of a house and get batch explanations          |\n| GitHub link | feature-based (integrated gradients) | TensorFlow  | tabular | Train a regression model that predicts the median price of a house and get online explanations         |\n| GitHub link | feature-based (integrated gradients) | TensorFlow  | image  | Use Inception_v3, a pretrained classification model, to get batch and online explanations           |\n| GitHub link | feature-based (sampled Shapley)  | TensorFlow  | text  | Train a LSTM model that classifies movie reviews as positive or negative using the text of the review and get online explanations |\n## Educational resources\nThe following resources provide further useful educational material:\n- [Explainable AI for Practitioners](https://www.oreilly.com/library/view/explainable-ai-for/9781098119126/) \n- [Interpretable Machine Learning: Shapley values](https://christophm.github.io/interpretable-ml-book/shapley.html) \n- Ankur Taly's [Integrated Gradients GitHub repository](https://github.com/ankurtaly/Integrated-Gradients) .\n- [Introduction to Shapley values](https://www.kaggle.com/dansbecker/shap-values#Introduction) ## What's next\n- [Configure your model for feature-based explanations](/vertex-ai/docs/explainable-ai/configuring-explanations-feature-based) \n- [Configure your model for example-based explanations](/vertex-ai/docs/explainable-ai/configuring-explanations-example-based) \n- View [feature importance for AutoML tabularmodels](/vertex-ai/docs/predictions/interpreting-results-automl) .", "guide": "Vertex AI"}