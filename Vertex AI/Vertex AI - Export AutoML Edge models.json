{"title": "Vertex AI - Export AutoML Edge models", "url": "https://cloud.google.com/vertex-ai/docs/export/export-edge-model", "abstract": "# Vertex AI - Export AutoML Edge models\nThis page describes how to use Vertex AI to export your image and video AutoML Edge models to Cloud Storage.\nFor information about exporting tabular models, see [Exporting an AutoML tabular model](/vertex-ai/docs/export/export-model-tabular) .\n", "content": "## Introduction\nAfter you have [trained](/vertex-ai/docs/training/automl-edge-console) an AutoML Edge model you can, in some cases, export the model in different formats, depending on how you want to use it. The exported model files are saved in a Cloud Storage bucket, and can they be used for prediction in the environment of your choosing.\nYou cannot use an Edge model in Vertex AI to serve predictions; you must deploy Edge model to an external device to get predictions.\n## Export a model\nUse the following code samples to identify an AutoML Edge model, specify an output file storage location, and then send the export model request.\nSelect the tab below for your objective:\nTrained AutoML Edge image classification models can be exported in the following   formats:- **TF Lite** - Export your model as a TF Lite package to run your model on edge or   mobile devices.\n- **Edge TPU TF Lite** - Export your model as a TF Lite package to run your model on Edge   TPU devices.\n- **Container** - Export your model as a TF Saved Model to run on a Docker container.\n- **Core ML** - Export an .mlmodel file to run your model on iOS and macOS devices.\n- **Tensorflow.js** - Export your model as a TensorFlow.js package to run your model in the   browser and in Node.js.\nSelect the tab below for your language or environment:\n- In the Google Cloud console, in the Vertex AI section, go to the **Models** page. [Go to the Models page](https://console.cloud.google.com/vertex-ai/models) \n- Click the version number of the AutoML Edge model you want to export to open its details page.\n- Click **Export** .\n- In the **Export model** side window, specify the location in Cloud Storage to store Edge model export output.\n- Click **Export** .\n- Click **Done** to close the **Export model** side window.\nBefore using any of the request data, make the following replacements:- : Your project's location.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The ID number of the trained AutoML Edge model you are exporting.\n- : The type of Edge model you are exporting. For this objective the  options are:- `tflite`(TF Lite) - Export your model as a TF Lite package to run your model   on edge or mobile devices.\n- `edgetpu-tflite`(Edge TPU TF Lite) - Export your model as a TF Lite package   to run your model on Edge TPU devices.\n- `tf-saved-model`(Container) - Export your model as a TF Saved Model to run on   a Docker container.\n- `core-ml`(Core ML) - Export an .mlmodel file to run your model on iOS and   macOS devices.\n- `tf-js`(Tensorflow.js) - Export your model as a TensorFlow.js package to run   your model in the browser and in Node.js.\n- : The path to the Cloud Storage bucket directory where you want  to store your Edge model files.\n- HTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID:export\n```- Request JSON body:\n```\n{\n \"outputConfig\": {\n \"exportFormatId\": \"EXPORT_FORMAT\",\n \"artifactDestination\": {\n  \"outputUriPrefix\": \"gs://OUTPUT_BUCKET/\"\n }\n }\n}\n```- To send your request, choose one of these options: **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID:export\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID:export\" | Select-Object -Expand Content\n```\n- The response contains information about specifications as well as the .\n- You can [get the status](#get-oper) of the export operation to see   when it finishes.Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/GoogleCloudPlatform/java-docs-samples&page=editor&cloudshell_workspace=aiplatform/src/main/java/aiplatform&cloudshell_open_in_editor=ExportModelSample.java) [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/ExportModelSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.ExportModelOperationMetadata;import com.google.cloud.aiplatform.v1.ExportModelRequest;import com.google.cloud.aiplatform.v1.ExportModelResponse;import com.google.cloud.aiplatform.v1.GcsDestination;import com.google.cloud.aiplatform.v1.ModelName;import com.google.cloud.aiplatform.v1.ModelServiceClient;import com.google.cloud.aiplatform.v1.ModelServiceSettings;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class ExportModelSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String modelId = \"YOUR_MODEL_ID\";\u00a0 \u00a0 String gcsDestinationOutputUriPrefix = \"gs://YOUR_GCS_SOURCE_BUCKET/path_to_your_destination/\";\u00a0 \u00a0 String exportFormat = \"YOUR_EXPORT_FORMAT\";\u00a0 \u00a0 exportModelSample(project, modelId, gcsDestinationOutputUriPrefix, exportFormat);\u00a0 }\u00a0 static void exportModelSample(\u00a0 \u00a0 \u00a0 String project, String modelId, String gcsDestinationOutputUriPrefix, String exportFormat)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 ModelServiceSettings modelServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 ModelServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (ModelServiceClient modelServiceClient = ModelServiceClient.create(modelServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 GcsDestination.Builder gcsDestination = GcsDestination.newBuilder();\u00a0 \u00a0 \u00a0 gcsDestination.setOutputUriPrefix(gcsDestinationOutputUriPrefix);\u00a0 \u00a0 \u00a0 ModelName modelName = ModelName.of(project, location, modelId);\u00a0 \u00a0 \u00a0 ExportModelRequest.OutputConfig outputConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ExportModelRequest.OutputConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setExportFormatId(exportFormat)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setArtifactDestination(gcsDestination)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 OperationFuture<ExportModelResponse, ExportModelOperationMetadata> exportModelResponseFuture =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelServiceClient.exportModelAsync(modelName, outputConfig);\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Operation name: %s\\n\", exportModelResponseFuture.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to finish...\");\u00a0 \u00a0 \u00a0 ExportModelResponse exportModelResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 exportModelResponseFuture.get(300, TimeUnit.SECONDS);\u00a0 \u00a0 \u00a0 System.out.format(\"Export Model Response: %s\\n\", exportModelResponse);\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/export-model.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0 \u00a0(Not necessary if passing values as arguments)\u00a0*/// const modelId = 'YOUR_MODEL_ID';// const gcsDestinationOutputUriPrefix ='YOUR_GCS_DEST_OUTPUT_URI_PREFIX';// \u00a0 \u00a0eg. \"gs://<your-gcs-bucket>/destination_path\"// const exportFormat = 'YOUR_EXPORT_FORMAT';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Model Service Client libraryconst {ModelServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst modelServiceClient = new ModelServiceClient(clientOptions);async function exportModel() {\u00a0 // Configure the name resources\u00a0 const name = `projects/${project}/locations/${location}/models/${modelId}`;\u00a0 // Configure the outputConfig resources\u00a0 const outputConfig = {\u00a0 \u00a0 exportFormatId: exportFormat,\u00a0 \u00a0 gcsDestination: {\u00a0 \u00a0 \u00a0 outputUriPrefix: gcsDestinationOutputUriPrefix,\u00a0 \u00a0 },\u00a0 };\u00a0 const request = {\u00a0 \u00a0 name,\u00a0 \u00a0 outputConfig,\u00a0 };\u00a0 // Export Model request\u00a0 const [response] = await modelServiceClient.exportModel(request);\u00a0 console.log(`Long running operation : ${response.name}`);\u00a0 // Wait for operation to complete\u00a0 await response.promise();\u00a0 const result = response.result;\u00a0 console.log(`Export model response : ${JSON.stringify(result)}`);}exportModel();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/googleapis/python-aiplatform&page=editor&cloudshell_workspace=samples/snippets/model_service&cloudshell_open_in_editor=export_model_sample.py) [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/snippets/model_service/export_model_sample.py) \n```\nfrom google.cloud import aiplatformdef export_model_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 model_id: str,\u00a0 \u00a0 gcs_destination_output_uri_prefix: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\u00a0 \u00a0 timeout: int = 300,):\u00a0 \u00a0 # The AI Platform services require regional API endpoints.\u00a0 \u00a0 client_options = {\"api_endpoint\": api_endpoint}\u00a0 \u00a0 # Initialize client that will be used to create and send requests.\u00a0 \u00a0 # This client only needs to be created once, and can be reused for multiple requests.\u00a0 \u00a0 client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\u00a0 \u00a0 output_config = {\u00a0 \u00a0 \u00a0 \u00a0 \"artifact_destination\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"output_uri_prefix\": gcs_destination_output_uri_prefix\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 # For information about export formats: https://cloud.google.com/ai-platform-unified/docs/export/export-edge-model#aiplatform_export_model_sample-drest\u00a0 \u00a0 \u00a0 \u00a0 \"export_format_id\": \"tf-saved-model\",\u00a0 \u00a0 }\u00a0 \u00a0 name = client.model_path(project=project, location=location, model=model_id)\u00a0 \u00a0 response = client.export_model(name=name, output_config=output_config)\u00a0 \u00a0 print(\"Long running operation:\", response.operation.name)\u00a0 \u00a0 print(\"output_info:\", response.metadata.output_info)\u00a0 \u00a0 export_model_response = response.result(timeout=timeout)\u00a0 \u00a0 print(\"export_model_response:\", export_model_response)\n```Trained AutoML Edge image classification models can be exported in the following   formats:- **TF Lite** - Export your model as a TF Lite package to run your model on edge or   mobile devices.\n- **Edge TPU TF Lite** - Export your model as a TF Lite package to run your model on Edge   TPU devices.\n- **Container** - Export your model as a TF Saved Model to run on a Docker container.\n- **Core ML** - Export an .mlmodel file to run your model on iOS and macOS devices.\n- **Tensorflow.js** - Export your model as a TensorFlow.js package to run your model in the   browser and in Node.js.\nSelect the tab below for your language or environment:\n- In the Google Cloud console, in the Vertex AI section, go to the **Models** page. [Go to the Models page](https://console.cloud.google.com/vertex-ai/models) \n- Click the version number of the AutoML Edge model you want to export to open its details page.\n- Click **Export** .\n- In the **Export model** side window, specify the location in Cloud Storage to store Edge model export output.\n- Click **Export** .\n- Click **Done** to close the **Export model** side window.\nBefore using any of the request data, make the following replacements:- : Your project's location.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The ID number of the trained AutoML Edge model you are exporting.\n- : The type of Edge model you are exporting. For this objective the  options are:- `tflite`(TF Lite) - Export your model as a TF Lite package to run your model   on edge or mobile devices.\n- `edgetpu-tflite`(Edge TPU TF Lite) - Export your model as a TF Lite package   to run your model on Edge TPU devices.\n- `tf-saved-model`(Container) - Export your model as a TF Saved Model to run on   a Docker container.\n- `core-ml`(Core ML) - Export an .mlmodel file to run your model on iOS and   macOS devices.\n- `tf-js`(Tensorflow.js) - Export your model as a TensorFlow.js package to run   your model in the browser and in Node.js.\n- : The path to the Cloud Storage bucket directory where you want  to store your Edge model files.\n- HTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID:export\n```- Request JSON body:\n```\n{\n \"outputConfig\": {\n \"exportFormatId\": \"EXPORT_FORMAT\",\n \"artifactDestination\": {\n  \"outputUriPrefix\": \"gs://OUTPUT_BUCKET/\"\n }\n }\n}\n```- To send your request, choose one of these options: **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID:export\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID:export\" | Select-Object -Expand Content\n```\n- The response contains information about specifications as well as the .\n- You can [get the status](#get-oper) of the export operation to see   when it finishes.Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/GoogleCloudPlatform/java-docs-samples&page=editor&cloudshell_workspace=aiplatform/src/main/java/aiplatform&cloudshell_open_in_editor=ExportModelSample.java) [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/ExportModelSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.ExportModelOperationMetadata;import com.google.cloud.aiplatform.v1.ExportModelRequest;import com.google.cloud.aiplatform.v1.ExportModelResponse;import com.google.cloud.aiplatform.v1.GcsDestination;import com.google.cloud.aiplatform.v1.ModelName;import com.google.cloud.aiplatform.v1.ModelServiceClient;import com.google.cloud.aiplatform.v1.ModelServiceSettings;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class ExportModelSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String modelId = \"YOUR_MODEL_ID\";\u00a0 \u00a0 String gcsDestinationOutputUriPrefix = \"gs://YOUR_GCS_SOURCE_BUCKET/path_to_your_destination/\";\u00a0 \u00a0 String exportFormat = \"YOUR_EXPORT_FORMAT\";\u00a0 \u00a0 exportModelSample(project, modelId, gcsDestinationOutputUriPrefix, exportFormat);\u00a0 }\u00a0 static void exportModelSample(\u00a0 \u00a0 \u00a0 String project, String modelId, String gcsDestinationOutputUriPrefix, String exportFormat)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 ModelServiceSettings modelServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 ModelServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (ModelServiceClient modelServiceClient = ModelServiceClient.create(modelServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 GcsDestination.Builder gcsDestination = GcsDestination.newBuilder();\u00a0 \u00a0 \u00a0 gcsDestination.setOutputUriPrefix(gcsDestinationOutputUriPrefix);\u00a0 \u00a0 \u00a0 ModelName modelName = ModelName.of(project, location, modelId);\u00a0 \u00a0 \u00a0 ExportModelRequest.OutputConfig outputConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ExportModelRequest.OutputConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setExportFormatId(exportFormat)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setArtifactDestination(gcsDestination)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 OperationFuture<ExportModelResponse, ExportModelOperationMetadata> exportModelResponseFuture =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelServiceClient.exportModelAsync(modelName, outputConfig);\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Operation name: %s\\n\", exportModelResponseFuture.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to finish...\");\u00a0 \u00a0 \u00a0 ExportModelResponse exportModelResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 exportModelResponseFuture.get(300, TimeUnit.SECONDS);\u00a0 \u00a0 \u00a0 System.out.format(\"Export Model Response: %s\\n\", exportModelResponse);\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/export-model.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0 \u00a0(Not necessary if passing values as arguments)\u00a0*/// const modelId = 'YOUR_MODEL_ID';// const gcsDestinationOutputUriPrefix ='YOUR_GCS_DEST_OUTPUT_URI_PREFIX';// \u00a0 \u00a0eg. \"gs://<your-gcs-bucket>/destination_path\"// const exportFormat = 'YOUR_EXPORT_FORMAT';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Model Service Client libraryconst {ModelServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst modelServiceClient = new ModelServiceClient(clientOptions);async function exportModel() {\u00a0 // Configure the name resources\u00a0 const name = `projects/${project}/locations/${location}/models/${modelId}`;\u00a0 // Configure the outputConfig resources\u00a0 const outputConfig = {\u00a0 \u00a0 exportFormatId: exportFormat,\u00a0 \u00a0 gcsDestination: {\u00a0 \u00a0 \u00a0 outputUriPrefix: gcsDestinationOutputUriPrefix,\u00a0 \u00a0 },\u00a0 };\u00a0 const request = {\u00a0 \u00a0 name,\u00a0 \u00a0 outputConfig,\u00a0 };\u00a0 // Export Model request\u00a0 const [response] = await modelServiceClient.exportModel(request);\u00a0 console.log(`Long running operation : ${response.name}`);\u00a0 // Wait for operation to complete\u00a0 await response.promise();\u00a0 const result = response.result;\u00a0 console.log(`Export model response : ${JSON.stringify(result)}`);}exportModel();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/googleapis/python-aiplatform&page=editor&cloudshell_workspace=samples/snippets/model_service&cloudshell_open_in_editor=export_model_sample.py) [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/snippets/model_service/export_model_sample.py) \n```\nfrom google.cloud import aiplatformdef export_model_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 model_id: str,\u00a0 \u00a0 gcs_destination_output_uri_prefix: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\u00a0 \u00a0 timeout: int = 300,):\u00a0 \u00a0 # The AI Platform services require regional API endpoints.\u00a0 \u00a0 client_options = {\"api_endpoint\": api_endpoint}\u00a0 \u00a0 # Initialize client that will be used to create and send requests.\u00a0 \u00a0 # This client only needs to be created once, and can be reused for multiple requests.\u00a0 \u00a0 client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\u00a0 \u00a0 output_config = {\u00a0 \u00a0 \u00a0 \u00a0 \"artifact_destination\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"output_uri_prefix\": gcs_destination_output_uri_prefix\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 # For information about export formats: https://cloud.google.com/ai-platform-unified/docs/export/export-edge-model#aiplatform_export_model_sample-drest\u00a0 \u00a0 \u00a0 \u00a0 \"export_format_id\": \"tf-saved-model\",\u00a0 \u00a0 }\u00a0 \u00a0 name = client.model_path(project=project, location=location, model=model_id)\u00a0 \u00a0 response = client.export_model(name=name, output_config=output_config)\u00a0 \u00a0 print(\"Long running operation:\", response.operation.name)\u00a0 \u00a0 print(\"output_info:\", response.metadata.output_info)\u00a0 \u00a0 export_model_response = response.result(timeout=timeout)\u00a0 \u00a0 print(\"export_model_response:\", export_model_response)\n```Trained AutoML Edge image object detection models can be exported in the following   formats:- **TF Lite** - Export your model as a TF Lite package to run your model on edge or   mobile devices.\n- **Container** - Export your model as a TF Saved Model to run on a Docker container.\n- **Tensorflow.js** - Export your model as a TensorFlow.js package to run your model in the   browser and in Node.js.\nSelect the tab below for your language or environment:\n- In the Google Cloud console, in the Vertex AI section, go to the **Models** page. [Go to the Models page](https://console.cloud.google.com/vertex-ai/models) \n- Click the version number of the AutoML Edge model you want to export to open its details page.\n- Select the **Deploy & Test** tab to view the available export formats.\n- Select your desired export model format from the **Use your edge-optimized model** section.\n- In the **Export model** side window, specify the location in Cloud Storage to store Edge model export output.\n- Click **Export** .\n- Click **Done** to close the **Export model** side window.\nBefore using any of the request data, make the following replacements:- : Your project's location.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The ID number of the trained AutoML Edge model you are exporting.\n- : The type of Edge model you are exporting. For this objective the  options are:- `tflite`(TF Lite) - Export your model as a TF Lite package to run your model   on edge or mobile devices.\n- `tf-saved-model`(Container) - Export your model as a TF Saved Model to run on   a Docker container.\n- `tf-js`(Tensorflow.js) - Export your model as a TensorFlow.js package to run   your model in the browser and in Node.js.\n- : The path to the Cloud Storage bucket directory where you want  to store your Edge model files.\n- HTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID:export\n```- Request JSON body:\n```\n{\n \"outputConfig\": {\n \"exportFormatId\": \"EXPORT_FORMAT\",\n \"artifactDestination\": {\n  \"outputUriPrefix\": \"gs://OUTPUT_BUCKET/\"\n }\n }\n}\n```- To send your request, choose one of these options: **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID:export\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID:export\" | Select-Object -Expand Content\n```\n- The response contains information about specifications as well as the .\n- You can [get the status](#get-oper) of the export operation to see   when it finishes.Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/GoogleCloudPlatform/java-docs-samples&page=editor&cloudshell_workspace=aiplatform/src/main/java/aiplatform&cloudshell_open_in_editor=ExportModelSample.java) [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/ExportModelSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.ExportModelOperationMetadata;import com.google.cloud.aiplatform.v1.ExportModelRequest;import com.google.cloud.aiplatform.v1.ExportModelResponse;import com.google.cloud.aiplatform.v1.GcsDestination;import com.google.cloud.aiplatform.v1.ModelName;import com.google.cloud.aiplatform.v1.ModelServiceClient;import com.google.cloud.aiplatform.v1.ModelServiceSettings;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class ExportModelSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String modelId = \"YOUR_MODEL_ID\";\u00a0 \u00a0 String gcsDestinationOutputUriPrefix = \"gs://YOUR_GCS_SOURCE_BUCKET/path_to_your_destination/\";\u00a0 \u00a0 String exportFormat = \"YOUR_EXPORT_FORMAT\";\u00a0 \u00a0 exportModelSample(project, modelId, gcsDestinationOutputUriPrefix, exportFormat);\u00a0 }\u00a0 static void exportModelSample(\u00a0 \u00a0 \u00a0 String project, String modelId, String gcsDestinationOutputUriPrefix, String exportFormat)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 ModelServiceSettings modelServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 ModelServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (ModelServiceClient modelServiceClient = ModelServiceClient.create(modelServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 GcsDestination.Builder gcsDestination = GcsDestination.newBuilder();\u00a0 \u00a0 \u00a0 gcsDestination.setOutputUriPrefix(gcsDestinationOutputUriPrefix);\u00a0 \u00a0 \u00a0 ModelName modelName = ModelName.of(project, location, modelId);\u00a0 \u00a0 \u00a0 ExportModelRequest.OutputConfig outputConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ExportModelRequest.OutputConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setExportFormatId(exportFormat)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setArtifactDestination(gcsDestination)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 OperationFuture<ExportModelResponse, ExportModelOperationMetadata> exportModelResponseFuture =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelServiceClient.exportModelAsync(modelName, outputConfig);\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Operation name: %s\\n\", exportModelResponseFuture.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to finish...\");\u00a0 \u00a0 \u00a0 ExportModelResponse exportModelResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 exportModelResponseFuture.get(300, TimeUnit.SECONDS);\u00a0 \u00a0 \u00a0 System.out.format(\"Export Model Response: %s\\n\", exportModelResponse);\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/export-model.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0 \u00a0(Not necessary if passing values as arguments)\u00a0*/// const modelId = 'YOUR_MODEL_ID';// const gcsDestinationOutputUriPrefix ='YOUR_GCS_DEST_OUTPUT_URI_PREFIX';// \u00a0 \u00a0eg. \"gs://<your-gcs-bucket>/destination_path\"// const exportFormat = 'YOUR_EXPORT_FORMAT';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Model Service Client libraryconst {ModelServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst modelServiceClient = new ModelServiceClient(clientOptions);async function exportModel() {\u00a0 // Configure the name resources\u00a0 const name = `projects/${project}/locations/${location}/models/${modelId}`;\u00a0 // Configure the outputConfig resources\u00a0 const outputConfig = {\u00a0 \u00a0 exportFormatId: exportFormat,\u00a0 \u00a0 gcsDestination: {\u00a0 \u00a0 \u00a0 outputUriPrefix: gcsDestinationOutputUriPrefix,\u00a0 \u00a0 },\u00a0 };\u00a0 const request = {\u00a0 \u00a0 name,\u00a0 \u00a0 outputConfig,\u00a0 };\u00a0 // Export Model request\u00a0 const [response] = await modelServiceClient.exportModel(request);\u00a0 console.log(`Long running operation : ${response.name}`);\u00a0 // Wait for operation to complete\u00a0 await response.promise();\u00a0 const result = response.result;\u00a0 console.log(`Export model response : ${JSON.stringify(result)}`);}exportModel();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/googleapis/python-aiplatform&page=editor&cloudshell_workspace=samples/snippets/model_service&cloudshell_open_in_editor=export_model_sample.py) [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/snippets/model_service/export_model_sample.py) \n```\nfrom google.cloud import aiplatformdef export_model_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 model_id: str,\u00a0 \u00a0 gcs_destination_output_uri_prefix: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\u00a0 \u00a0 timeout: int = 300,):\u00a0 \u00a0 # The AI Platform services require regional API endpoints.\u00a0 \u00a0 client_options = {\"api_endpoint\": api_endpoint}\u00a0 \u00a0 # Initialize client that will be used to create and send requests.\u00a0 \u00a0 # This client only needs to be created once, and can be reused for multiple requests.\u00a0 \u00a0 client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\u00a0 \u00a0 output_config = {\u00a0 \u00a0 \u00a0 \u00a0 \"artifact_destination\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"output_uri_prefix\": gcs_destination_output_uri_prefix\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 # For information about export formats: https://cloud.google.com/ai-platform-unified/docs/export/export-edge-model#aiplatform_export_model_sample-drest\u00a0 \u00a0 \u00a0 \u00a0 \"export_format_id\": \"tf-saved-model\",\u00a0 \u00a0 }\u00a0 \u00a0 name = client.model_path(project=project, location=location, model=model_id)\u00a0 \u00a0 response = client.export_model(name=name, output_config=output_config)\u00a0 \u00a0 print(\"Long running operation:\", response.operation.name)\u00a0 \u00a0 print(\"output_info:\", response.metadata.output_info)\u00a0 \u00a0 export_model_response = response.result(timeout=timeout)\u00a0 \u00a0 print(\"export_model_response:\", export_model_response)\n```Select the tab below for your objective:Trained AutoML Edge video action recognition models can be exported in the saved   model format.\nSelect the tab below for your language or environment:\n- In the Google Cloud console, in the Vertex AI section, go to the **Models** page. [Go to the Models page](https://console.cloud.google.com/vertex-ai/models) \n- Click the version number of the AutoML Edge model you want to export to open its details page.\n- Click **Export** .\n- In the **Export model** side window, specify the location in Cloud Storage to store Edge model export output.\n- Click **Export** .\n- Click **Done** to close the **Export model** side window.\nBefore using any of the request data, make the following replacements:- : Region where the Model is stored. For example,`us-central1`.\n- : The ID number of the trained AutoML Edge model you are exporting.\n- : The type of Edge model you are exporting. For video action  recognition, the model option is:- `tf-saved-model`(Container) - Export your model as a TF Saved Model to run on   a Docker container.\n- : The path to the Cloud Storage bucket directory where you want  to store your Edge model files.\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) .\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_NUMBER/locations/LOCATION/models/MODEL_ID:export\n```\nRequest JSON body:\n```\n{\n \"outputConfig\": {\n \"exportFormatId\": \"EXPORT_FORMAT\",\n \"artifactDestination\": {\n \"outputUriPrefix\": \"gs://OUTPUT_BUCKET/\"\n }\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_NUMBER/locations/LOCATION/models/MODEL_ID:export\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_NUMBER/locations/LOCATION/models/MODEL_ID:export\" | Select-Object -Expand Content\n```\nThe response contains information about specifications as well as the .You can [get the status](#get-oper) of the export operation to see   when it finishes.Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/ExportModelVideoActionRecognitionSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.ExportModelOperationMetadata;import com.google.cloud.aiplatform.v1.ExportModelRequest;import com.google.cloud.aiplatform.v1.ExportModelResponse;import com.google.cloud.aiplatform.v1.GcsDestination;import com.google.cloud.aiplatform.v1.ModelName;import com.google.cloud.aiplatform.v1.ModelServiceClient;import com.google.cloud.aiplatform.v1.ModelServiceSettings;import java.io.IOException;import java.util.concurrent.ExecutionException;public class ExportModelVideoActionRecognitionSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, ExecutionException, InterruptedException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"PROJECT\";\u00a0 \u00a0 String modelId = \"MODEL_ID\";\u00a0 \u00a0 String gcsDestinationOutputUriPrefix = \"GCS_DESTINATION_OUTPUT_URI_PREFIX\";\u00a0 \u00a0 String exportFormat = \"EXPORT_FORMAT\";\u00a0 \u00a0 exportModelVideoActionRecognitionSample(\u00a0 \u00a0 \u00a0 \u00a0 project, modelId, gcsDestinationOutputUriPrefix, exportFormat);\u00a0 }\u00a0 static void exportModelVideoActionRecognitionSample(\u00a0 \u00a0 \u00a0 String project, String modelId, String gcsDestinationOutputUriPrefix, String exportFormat)\u00a0 \u00a0 \u00a0 throws IOException, ExecutionException, InterruptedException {\u00a0 \u00a0 ModelServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 ModelServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (ModelServiceClient client = ModelServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 GcsDestination gcsDestination =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 GcsDestination.newBuilder().setOutputUriPrefix(gcsDestinationOutputUriPrefix).build();\u00a0 \u00a0 \u00a0 ExportModelRequest.OutputConfig outputConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ExportModelRequest.OutputConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setArtifactDestination(gcsDestination)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setExportFormatId(exportFormat)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 ModelName name = ModelName.of(project, location, modelId);\u00a0 \u00a0 \u00a0 OperationFuture<ExportModelResponse, ExportModelOperationMetadata> response =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 client.exportModelAsync(name, outputConfig);\u00a0 \u00a0 \u00a0 // You can use OperationFuture.getInitialFuture to get a future representing the initial\u00a0 \u00a0 \u00a0 // response to the request, which contains information while the operation is in progress.\u00a0 \u00a0 \u00a0 System.out.format(\"Operation name: %s\\n\", response.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 // OperationFuture.get() will block until the operation is finished.\u00a0 \u00a0 \u00a0 ExportModelResponse exportModelResponse = response.get();\u00a0 \u00a0 \u00a0 System.out.format(\"exportModelResponse: %s\\n\", exportModelResponse);\u00a0 \u00a0 }\u00a0 }}\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/snippets/model_service/export_model_video_action_recognition_sample.py) \n```\nfrom google.cloud import aiplatformdef export_model_video_action_recognition_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 model_id: str,\u00a0 \u00a0 gcs_destination_output_uri_prefix: str,\u00a0 \u00a0 export_format: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\u00a0 \u00a0 timeout: int = 300,):\u00a0 \u00a0 # The AI Platform services require regional API endpoints.\u00a0 \u00a0 client_options = {\"api_endpoint\": api_endpoint}\u00a0 \u00a0 # Initialize client that will be used to create and send requests.\u00a0 \u00a0 # This client only needs to be created once, and can be reused for multiple requests.\u00a0 \u00a0 client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\u00a0 \u00a0 gcs_destination = {\"output_uri_prefix\": gcs_destination_output_uri_prefix}\u00a0 \u00a0 output_config = {\u00a0 \u00a0 \u00a0 \u00a0 \"artifact_destination\": gcs_destination,\u00a0 \u00a0 \u00a0 \u00a0 \"export_format_id\": export_format,\u00a0 \u00a0 }\u00a0 \u00a0 name = client.model_path(project=project, location=location, model=model_id)\u00a0 \u00a0 response = client.export_model(name=name, output_config=output_config)\u00a0 \u00a0 print(\"Long running operation:\", response.operation.name)\u00a0 \u00a0 print(\"output_info:\", response.metadata.output_info)\u00a0 \u00a0 export_model_response = response.result(timeout=timeout)\u00a0 \u00a0 print(\"export_model_response:\", export_model_response)\n```\nTrained AutoML Edge video classification models can only be exported in the saved model format.\nSelect the tab below for your language or environment:\n- In the Google Cloud console, in the Vertex AI section, go to the **Models** page. [Go to the Models page](https://console.cloud.google.com/vertex-ai/models) \n- Click the version number of the AutoML Edge model you want to export to open its details page.\n- Click **Export** .\n- In the **Export model** side window, specify the location in Cloud Storage to store Edge model export output.\n- Click **Export** .\n- Click **Done** to close the **Export model** side window.\nBefore using any of the request data, make the following replacements:- : Region where the Model is stored. For example,`us-central1`.\n- : The ID number of the trained AutoML Edge model you are exporting.\n- : The type of Edge model you are exporting. For video classification,  the model option is:- `tf-saved-model`(Container) - Export your model as a TF Saved Model to run on   a Docker container.\n- : The path to the Cloud Storage bucket directory where you want  to store your Edge model files.\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) .\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_NUMBER/locations/LOCATION/models/MODEL_ID:export\n```\nRequest JSON body:\n```\n{\n \"outputConfig\": {\n \"exportFormatId\": \"EXPORT_FORMAT\",\n \"artifactDestination\": {\n \"outputUriPrefix\": \"gs://OUTPUT_BUCKET/\"\n }\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_NUMBER/locations/LOCATION/models/MODEL_ID:export\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_NUMBER/locations/LOCATION/models/MODEL_ID:export\" | Select-Object -Expand Content\n```\nThe response contains information about specifications as well as the .\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION/models/MODEL_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.ExportModelOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2020-10-12T20:53:40.130785Z\",\n  \"updateTime\": \"2020-10-12T20:53:40.130785Z\"\n },\n \"outputInfo\": {\n  \"artifactOutputUri\": \"gs://OUTPUT_BUCKET/model-MODEL_ID/EXPORT_FORMAT/YYYY-MM-DDThh:mm:ss.sssZ\"\n }\n }\n}\n```\nYou can [get the status](#get-oper) of the export operation to see   when it finishes.\nTrained AutoML Edge video object tracking models can be exported in the following   formats:- **TF Lite** - Export your model as a TensorFlow Lite package to run your model on   edge or mobile devices.\n- **Container** - Export your model as a TensorFlow Saved Model to run on a Docker   container.\nSelect the tab below for your language or environment:\n- In the Google Cloud console, in the Vertex AI section, go to the **Models** page. [Go to the Models page](https://console.cloud.google.com/vertex-ai/models) \n- Click the version number of the AutoML Edge model you want to export to open its details page.\n- Click **Export** .\n- In the **Export model** side window, specify the location in Cloud Storage to store Edge model export output.\n- Click **Export** .\n- Click **Done** to close the **Export model** side window.\nBefore using any of the request data, make the following replacements:- : Region where the Model is stored. For example,`us-central1`.\n- : The ID number of the trained AutoML Edge model you are exporting.\n- : The type of Edge model you are exporting. For video object tracking  models, the options are:- `tflite`(TF Lite) - Export your model as a TF Lite package to run your model   on edge or mobile devices.\n- `edgetpu-tflite`(Edge TPU TF Lite) - Export your model as a TF Lite package   to run your model on Edge TPU devices.\n- `tf-saved-model`(Container) - Export your model as a TF Saved Model to run on   a Docker container.\n- : The path to the Cloud Storage bucket directory where you want  to store your Edge model files.\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) .\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_NUMBER/locations/LOCATION/models/MODEL_ID:export\n```\nRequest JSON body:\n```\n{\n \"outputConfig\": {\n \"exportFormatId\": \"EXPORT_FORMAT\",\n \"artifactDestination\": {\n \"outputUriPrefix\": \"gs://OUTPUT_BUCKET/\"\n }\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_NUMBER/locations/LOCATION/models/MODEL_ID:export\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_NUMBER/locations/LOCATION/models/MODEL_ID:export\" | Select-Object -Expand Content\n```\nThe response contains information about specifications as well as the .\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION/models/MODEL_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.ExportModelOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2020-10-12T20:53:40.130785Z\",\n  \"updateTime\": \"2020-10-12T20:53:40.130785Z\"\n },\n \"outputInfo\": {\n  \"artifactOutputUri\": \"gs://OUTPUT_BUCKET/model-MODEL_ID/EXPORT_FORMAT/YYYY-MM-DDThh:mm:ss.sssZ\"\n }\n }\n}\n```\nYou can [get the status](#get-oper) of the export operation to see   when it finishes.\n## Get status of the operation\nUse the following code to get the status of the export operation. This   code is the same for all objectives:Before using any of the request data, make the following replacements:- : Your project's location.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- :The ID of the target operation. This ID is typically contained in the  response to the original request.\nHTTP method and URL:\n```\nGET https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/operations/OPERATION_ID\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nExecute the following command:\n```\ncurl -X GET \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/operations/OPERATION_ID\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nExecute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method GET ` -Headers $headers ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/operations/OPERATION_ID\" | Select-Object -Expand Content\n```You should see output similar to the following for a completed operation:\n```\n{\n \"name\": \"projects/PROJECT/locations/LOCATION/models/MODEL_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.ExportModelOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2020-10-12T20:53:40.130785Z\",\n  \"updateTime\": \"2020-10-12T20:53:40.793983Z\"\n },\n \"outputInfo\": {\n  \"artifactOutputUri\": \"gs://OUTPUT_BUCKET/model-MODEL_ID/EXPORT_FORMAT/YYYY-MM-DDThh:mm:ss.sssZ\"\n }\n },\n \"done\": true,\n \"response\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.ExportModelResponse\"\n }\n}\n```Before using any of the request data, make the following replacements:- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : Region where the Model is stored. For example,`us-central1`.\n- : ID of your operations.\nHTTP method and URL:\n```\nGET https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/operations/OPERATION_ID\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nExecute the following command:\n```\ncurl -X GET \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/operations/OPERATION_ID\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nExecute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method GET ` -Headers $headers ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/operations/OPERATION_ID\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following:\n## Output files\nSelect the tab below for your model format:\nThe `OUTPUT_BUCKET` you specified in the request determines where the output   files are stored. The directory format where the output files are stored follows the format:- gs:///model-/ **tflite** //\n **Files:** - `model.tflite`: A file containing a version of the model that is   ready to be used with TensorFlow Lite.\n **Key point** : Unlike previous export behavior in AutoML Vision, exporting this format in Vertex AI does not produce a label file (`dict.txt`) as one of the output files. This information is now included in the`.tflite`file itself. For information about extracting this information, see the following TensorFlow documentation:- [TensorFlow Lite inference with metadata](https://www.tensorflow.org/lite/inference_with_metadata/overview) \n- [Generate model interfaces with TensorFlow Lite code generator](https://www.tensorflow.org/lite/inference_with_metadata/codegen) \n- [Adding metadata to TensorFlow Lite models](https://www.tensorflow.org/lite/convert/metadata) The `OUTPUT_BUCKET` you specified in the request determines where the output   files are stored. The directory format where the output files are stored follows the format:- gs:///model-/ **edgetpu-tflite** //\n **Files:** - `edgetpu_model.tflite`: A file containing a version of the model for   TensorFlow Lite, passed through the Edge TPU compiler to be compatible with the Edge TPU.\n **Key point** : Unlike previous export behavior in AutoML Vision, exporting this format in Vertex AI does not produce a label file (`dict.txt`) as one of the output files. This information is now included in the`.tflite`file itself. For information about extracting this information, see the following TensorFlow documentation:- [TensorFlow Lite inference with metadata](https://www.tensorflow.org/lite/inference_with_metadata/overview) \n- [Generate model interfaces with TensorFlow Lite code generator](https://www.tensorflow.org/lite/inference_with_metadata/codegen) \n- [Adding metadata to TensorFlow Lite models](https://www.tensorflow.org/lite/convert/metadata) The `OUTPUT_BUCKET` you specified in the request determines where the output   files are stored. The directory format where the output files are stored follows the format:- gs:///model-/ **tf-saved-model** //\n **Files:** - `saved_model.pb`: A protocol buffer file containing the graph definition   and the weights of the model.The `OUTPUT_BUCKET` you specified in the request determines where the output   files are stored. The directory format where the output files are stored follows the format:- gs:///model-/ **core-ml** //\n **Files:** - `dict.txt`: A label file. Each line in the label file`dict.txt`represents a label of the predictions returned by the model, in the same order they were   requested. **Sample dict.txt** ```\nroses\ndaisy\ntulips\ndandelion\nsunflowers\n```\n- `model.mlmodel`: A file specifying a Core ML model.The `OUTPUT_BUCKET` you specified in the request determines where the output   files are stored. The directory format where the output files are stored follows the format:- gs:///model-/ **tf-js** //\n **Files:** - `dict.txt`: A label file. Each line in the label file`dict.txt`represents a label of the predictions returned by the model, in the same order they were   requested. **Sample dict.txt** ```\nroses\ndaisy\ntulips\ndandelion\nsunflowers\n```\n- `group1-shard1of3.bin`: A binary file.\n- `group1-shard2of3.bin`: A binary file.\n- `group1-shard3of3.bin`: A binary file.\n- `model.json`: A JSON file representation of a model. **Sample** `model.json` (shortened for clarity)```\n{\n \"format\": \"graph-model\",\n \"generatedBy\": \"2.4.0\",\n \"convertedBy\": \"TensorFlow.js Converter v1.7.0\",\n \"userDefinedMetadata\": {\n \"signature\": {\n  \"inputs\": {\n  \"image:0\": {\n   \"name\": \"image:0\",\n   \"dtype\": \"DT_FLOAT\",\n   \"tensorShape\": {\n   \"dim\": [    {\n    \"size\": \"1\"\n    },\n    {\n    \"size\": \"224\"\n    },\n    {\n    \"size\": \"224\"\n    },\n    {\n    \"size\": \"3\"\n    }\n   ]\n   }\n  }\n  },\n  \"outputs\": {\n  \"scores:0\": {\n   \"name\": \"scores:0\",\n   \"dtype\": \"DT_FLOAT\",\n   \"tensorShape\": {\n   \"dim\": [    {\n    \"size\": \"1\"\n    },\n    {\n    \"size\": \"5\"\n    }\n   ]\n   }\n  }\n  }\n }\n },\n \"modelTopology\": {\n \"node\": [  {\n  \"name\": \"image\",\n  \"op\": \"Placeholder\",\n  \"attr\": {\n   \"dtype\": {\n   \"type\": \"DT_FLOAT\"\n   },\n   \"shape\": {\n   \"shape\": {\n    \"dim\": [    {\n     \"size\": \"1\"\n    },\n    {\n     \"size\": \"224\"\n    },\n    {\n     \"size\": \"224\"\n    },\n    {\n     \"size\": \"3\"\n    }\n    ]\n   }\n   }\n  }\n  },\n  {\n  \"name\": \"mnas_v4_a_1/feature_network/feature_extractor/Mean/reduction_indices\",\n  \"op\": \"Const\",\n  \"attr\": {\n   \"value\": {\n   \"tensor\": {\n    \"dtype\": \"DT_INT32\",\n    \"tensorShape\": {\n    \"dim\": [     {\n     \"size\": \"2\"\n     }\n    ]\n    }\n   }\n   },\n   \"dtype\": {\n   \"type\": \"DT_INT32\"\n   }\n  }\n  },\n  ...\n  {\n  \"name\": \"scores\",\n  \"op\": \"Identity\",\n  \"input\": [   \"Softmax\"\n  ],\n  \"attr\": {\n   \"T\": {\n   \"type\": \"DT_FLOAT\"\n   }\n  }\n  }\n ],\n \"library\": {},\n \"versions\": {}\n },\n \"weightsManifest\": [ {\n  \"paths\": [  \"group1-shard1of3.bin\",\n  \"group1-shard2of3.bin\",\n  \"group1-shard3of3.bin\"\n  ],\n  \"weights\": [  {\n   \"name\": \"mnas_v4_a_1/feature_network/feature_extractor/Mean/reduction_indices\",\n   \"shape\": [   2\n   ],\n   \"dtype\": \"int32\"\n  },\n  {\n   \"name\": \"mnas_v4_a/output/fc/tf_layer/kernel\",\n   \"shape\": [   1280,\n   5\n   ],\n   \"dtype\": \"float32\"\n  },\n  ...\n  {\n   \"name\": \"mnas_v4_a_1/feature_network/lead_cell_17/op_0/conv2d_0/Conv2D_weights\",\n   \"shape\": [   1,\n   1,\n   320,\n   1280\n   ],\n   \"dtype\": \"float32\"\n  },\n  {\n   \"name\": \"mnas_v4_a_1/feature_network/cell_14/op_0/expand_0/Conv2D_bn_offset\",\n   \"shape\": [   1152\n   ],\n   \"dtype\": \"float32\"\n  }\n  ]\n }\n ]\n}\n```Select the tab below for your model format:\nThe `OUTPUT_BUCKET` you specified in the request determines where the output   files are stored. The directory format where the output files are stored follows the format:- gs:///model-/ **tflite** //\n **Files:** - `model.tflite`: A file containing a version of the model that is   ready to be used with TensorFlow Lite.\n- `frozen_inference_graph.pb`: A serialized protocol buffer file containing   the graph definition and the weights of the model.\n- `label_map.pbtxt`: A label map file that maps each of the used labels to   an integer value.The `OUTPUT_BUCKET` you specified in the request determines where the output   files are stored. The directory format where the output files are stored follows the format:- gs:///model-/ **edgetpu-tflite** //\n **Files:** - `edgetpu_model.tflite`: A file containing a version of the model for   TensorFlow Lite, passed through the Edge TPU compiler to be compatible with the Edge TPU.\n- `label_map.pbtxt`: A label map file that maps each of the used labels to an   integer value.The `OUTPUT_BUCKET` you specified in the request determines where the output   files are stored. The directory format where the output files are stored follows the format:- gs:///model-/ **tf-saved-model** //\n **Files:** - `frozen_inference_graph.pb`: A serialized protocol buffer file containing   the graph definition and the weights of the model.\n- `label_map.pbtxt`: A label map file that maps each of the used labels to an   integer value.\n- `saved_model/saved_model.pb`: The file stores the actual TensorFlow program,   or model, and a set of named signatures, each identifying a function that accepts tensor   inputs and produces tensor outputs.\n- `saved_model/variables/`: The variables directory contains a   standard training checkpoint.", "guide": "Vertex AI"}