{"title": "Vertex AI - Training with TPU accelerators", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Training with TPU accelerators\nVertex AI supports training with various frameworks and libraries using a TPU VM. When configuring compute resources, you can specify TPU V2 or TPU V3 VMs. For details, see [Configure compute resources for custom training](/vertex-ai/docs/training/configure-compute#tpu) .\n", "content": "## TensorFlow training\n### Prebuilt container\nUse a [prebuilt training container](/vertex-ai/docs/training/pre-built-containers) that supports TPUs, and create a [Python training application](/vertex-ai/docs/training/create-python-pre-built-container) .\n### Custom container\nUse a [custom container](/vertex-ai/docs/training/create-custom-container) in which you have installed versions of the `tensorflow` and `libtpu` specially built for TPU VMs. These libraries are maintained by the Cloud TPU service and are listed in the [Supported TPU configurations](/tpu/docs/supported-tpu-versions#tensorflow) documentation.\n**Note:** Vertex AI supports only TensorFlow 2.11 and newer versions for custom containers and TensorFlow 2.12 and newer versions for prebuilt training containers.\nSelect the `tensorflow` version of your choice and its corresponding `libtpu` library. Next, install these in your Docker container image when you build the container.\nFor example, if you want to use TensorFlow 2.12, include the following instructions in your Dockerfile:\n```\n# Download and install `tensorflow`.RUN pip install https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.12.0/tensorflow-2.12.0-cp38-cp38-linux_x86_64.whl# Download and install `libtpu`.# You must save `libtpu.so` in the '/lib' directory of the container image.RUN curl -L https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.6.0/libtpu.so -o /lib/libtpu.so\n```\n**Note:** This is not a complete Dockerfile. If you want to use these instructions, add them to a [Dockerfile for a custom training container](/vertex-ai/docs/training/create-custom-container#create_a_dockerfile) . You can also find the corresponding libtpu version for the TensorFlow version from the [Cloud TPU configurations page](/tpu/docs/supported-tpu-configurations#tpu_vm_with_tpu_v4) .\n`tensorflow` training on a `TPU Pod` requires additional setup in the training container. Vertex AI maintains a base docker image that handles the initial setup.\n| Image URIs                                                | Python Version |\n|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------:|\n| us-docker.pkg.dev/vertex-ai/training/tf-tpu-pod-base-cp38:latest europe-docker.pkg.dev/vertex-ai/training/tf-tpu-pod-base-cp38:latest asia-docker.pkg.dev/vertex-ai/training/tf-tpu-pod-base-cp38:latest |    3.8 |\nHere are the steps to build your custom container:\n- Choose the base image for the python version of your choice. TPU TensorFlow wheels support Python 3.8 for versions 2.12 and lower. Starting 2.13, Python 3.10 will be supported. For the specific TensorFlow wheels, refer to the [Cloud TPU configurations page](/tpu/docs/supported-tpu-configurations#tpu_vm_with_tpu_v4) .\n- Extend the image with your trainer code and the startup command.\n```\n# Specifies base image and tagFROM us-docker.pkg.dev/vertex-ai/training/tf-tpu-pod-base-cp38:latestWORKDIR /root# Download and install `tensorflow`.RUN pip install https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/tensorflow/tf-2.12.0/tensorflow-2.12.0-cp38-cp38-linux_x86_64.whl# Download and install `libtpu`.# You must save `libtpu.so` in the '/lib' directory of the container image.RUN curl -L https://storage.googleapis.com/cloud-tpu-tpuvm-artifacts/libtpu/1.6.0/libtpu.so -o /lib/libtpu.so# Copies the trainer code to the docker image.COPY your-path-to/model.py /root/model.pyCOPY your-path-to/trainer.py /root/trainer.py# The base image is setup so that it runs the CMD that you provide.# You can provide CMD inside the Dockerfile like as follows.# Alternatively, you can pass it as an `args` value in ContainerSpec:# (https://cloud.google.com/vertex-ai/docs/reference/rest/v1/CustomJobSpec#containerspec)CMD [\"python3\", \"trainer.py\"]\n```\nTo see an example of how to train a custom TensorFlow model by using TPUs in Vertex AI Pipelines,  run the \"Vertex AI Pipelines: TPU model train, upload, and deploy using google-cloud-pipeline-components\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/google_cloud_pipeline_components_TPU_model_train_upload_deploy.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fpipelines%2Fgoogle_cloud_pipeline_components_TPU_model_train_upload_deploy.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/google_cloud_pipeline_components_TPU_model_train_upload_deploy.ipynb)\n## PyTorch training\nYou can use prebuilt or custom containers for PyTorch when training with TPUs.\n### Prebuilt container\nUse a [prebuilt training container](/vertex-ai/docs/training/pre-built-containers) that supports TPUs, and create a [Python training application](/vertex-ai/docs/training/create-python-pre-built-container) .\n### Custom container\nUse a [custom container](/vertex-ai/docs/training/create-custom-container) in which you have installed the `PyTorch` library.\nFor example, your Dockerfile might look like the following:\n```\nFROM python:3.8# install pytorch and torch_xlaRUN pip3 install torch~=2.0.0 https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-2.0-cp38-cp38-linux_x86_64.whl# Add your artifacts hereCOPY trainer.py .# Run the trainer codeCMD [\"python3\", \"trainer.py\"]\n```\nThe training runs on all hosts of the TPU Pod (see [Run PyTorch code on TPU Pod slices](/tpu/docs/pytorch-pods) ).\nVertex AI waits for a response from all the hosts to decide completion of the job.\n## JAX training\n### Prebuilt container\nThere are no prebuilt containers for JAX.\n### Custom container\nUse a [custom container](/vertex-ai/docs/training/create-custom-container) in which you have installed the `JAX` library.\nFor example, your Dockerfile might look like the following:\n```\n# Install JAX.RUN pip install 'jax[tpu]>=0.2.16' -f https://storage.googleapis.com/jax-releases/libtpu_releases.html# Add your artifacts hereCOPY trainer.py trainer.py# Set an entrypoint.ENTRYPOINT [\"python3\", \"trainer.py\"]\n```\nThe training runs on all hosts of the TPU Pod (see [Run JAX code on TPU Pod slices](/tpu/docs/jax-pods) ).\nVertex AI watches the first host of the TPU Pod to decide completion of the job. You can use the following code snippet to make sure that all hosts exit at the same time:\n```\n# Your training logic...if jax.process_count() > 1:\u00a0 # Make sure all hosts stay up until the end of main.\u00a0 x = jnp.ones([jax.local_device_count()])\u00a0 x = jax.device_get(jax.pmap(lambda x: jax.lax.psum(x, 'i'), 'i')(x))\u00a0 assert x[0] == jax.device_count()\n```\n## Environment variables\nThe following table details the environment variables that you can use within the container:\n| Name   | Value                       |\n|:--------------|:-------------------------------------------------------------------------------------------------|\n| TPU_NODE_NAME | my-first-tpu-node                    |\n| TPU_CONFIG | {\"project\": \"tenant-project-xyz\", \"zone\": \"us-central1-b\", \"tpu_node_name\": \"my-first-tpu-node\"} |\n## Custom Service Account\nA custom service account can be used for TPU training. On how to use a custom service account, refer to the [page on how to use a custom service account](/vertex-ai/docs/general/custom-service-account) .\n## Private IP (VPC network peering) for training\nA private IP can be used for TPU training. Refer to the page on [how to use a private IP for custom training](/vertex-ai/docs/training/using-private-ip) .\n## VPC Service Controls\nVPC Service Controls enabled projects can submit TPU training jobs.\n## Limitations\nThe following limitations apply when you train using a TPU VM:\n- [TPUs are only available in certain Vertex AI regions](/vertex-ai/docs/general/locations#accelerators) .\n- If you use a TPU VM, then you can't encrypt any materials with a [customer-managed encryption key (CMEK)](/vertex-ai/docs/general/cmek) .## TPU types\nRefer to [TPU types](/tpu/docs/types-zones) for more information about TPU accelerators such as memory limit.", "guide": "Vertex AI"}