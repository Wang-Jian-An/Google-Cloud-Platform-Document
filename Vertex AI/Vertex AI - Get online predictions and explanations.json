{"title": "Vertex AI - Get online predictions and explanations", "url": "https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-online-predictions", "abstract": "# Vertex AI - Get online predictions and explanations\nThis page shows you how to get online (real-time) predictions and explanations from your tabular classification or regression models using the Google Cloud console or the Vertex AI API.\nAn online prediction is a synchronous request as opposed to a [batch prediction](/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions) , which is an asynchronous request. Use online predictions when you are making requests in response to application input or in other situations where you require timely inference.\nYou must [deploy a model](#deploy-model) to an endpoint before that model can be used to serve online predictions. Deploying a model associates physical resources with the model so it can serve online predictions with low latency.\nThe topics covered are:\n- [Deploy a model](#deploy-model) to an endpoint\n- Get an [online prediction](#online-prediction) using your deployed model\n- Get an [online explanation](#online-explanation) using your deployed model", "content": "## Before you begin\nBefore you can get online predictions, you must first [train](/vertex-ai/docs/tabular-data/classification-regression/train-model) a classification or regression model and [evaluate](/vertex-ai/docs/tabular-data/classification-regression/evaluate-model) it for accuracy.\n## Deploy a model to an endpoint\nYou can deploy more than one model to an endpoint, and you can deploy a model to more than one endpoint. For more information about options and use cases for deploying models, see [About deploying models](/vertex-ai/docs/general/deployment) .\nUse one of the following methods to deploy a model:\n- In the Google Cloud console, in the Vertex AI section, go to the **Models** page. [Go to the Models page](https://console.cloud.google.com/vertex-ai/models) \n- Click the name of the model you want to deploy to open its details page.\n- Select the **Deploy & Test** tab.If your model is already deployed to any endpoints, they are listed in the **Deploy your model** section.\n- Click **Deploy to endpoint** .\n- In the **Define your endpoint** page, configure as follows:- You can choose to deploy your model to a new endpoint or an existing endpoint.- To deploy your model to a new endpoint, selectradio_button_checked **Create new endpoint** and provide a name for the new endpoint.\n- To deploy your model to an existing endpoint, selectradio_button_checked **Add to existing endpoint** and select the endpoint from the drop-down list.\n- You can add more than one model to an endpoint, and you can add a model to more than one endpoint. [Learn more](/vertex-ai/docs/general/deployment) .\n- Click **Continue** .\n- In the **Model settings** page, configure as follows:- If you're deploying your model to a new endpoint, accept 100 for the **Traffic split** . If you're deploying your model to an existing endpoint that has one or   more models deployed to it, you must update the **Traffic split** percentage for the model you are deploying and the already deployed models   so that all of the percentages add up to 100%.\n- Enter the **Minimum number of compute nodes** you want to provide for   your model.This is the number of nodes available to this model at all times.     You are charged for the nodes used, whether to handle prediction load   or for standby (minimum) nodes, even without prediction traffic.   See the [pricing page](/vertex-ai/pricing) .\n- Select your **Machine type** .Larger machine resources will increase your prediction performance   and increase costs.\n- Learn how to [change thedefault settings for prediction logging](/vertex-ai/docs/predictions/online-prediction-logging#enabling-and-disabling) .\n- Click **Continue** \n- In the **Model monitoring** page, click **Continue** .\n- In the **Monitoring objectives** page, configure as follows:- Enter the location of your training data.\n- Enter the name of the target column.\n- Click **Deploy** to deploy your model to the endpoint.\nWhen you deploy a model using the Vertex AI API, you complete the following steps:- Create an endpoint if needed.\n- Get the endpoint ID.\n- Deploy the model to the endpoint.\n### Create an endpoint\nIf you are deploying a model to an existing endpoint, you can skip this step.\nThe following example uses the [gcloud ai endpoints create command](/sdk/gcloud/reference/ai/endpoints/create) :\n```\n\u00a0 gcloud ai endpoints create \\\u00a0 \u00a0 --region=LOCATION \\\u00a0 \u00a0 --display-name=ENDPOINT_NAME\n```\nReplace the following:- : The region where you are using Vertex AI.\n- : The display name for the endpoint.The Google Cloud CLI tool might take a few seconds to create the endpoint.Before using any of the request data, make the following replacements:- : Your region.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The display name for the endpoint.\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints\n```\nRequest JSON body:\n```\n{\n \"display_name\": \"ENDPOINT_NAME\"\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/endpoints/ENDPOINT_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.CreateEndpointOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2020-11-05T17:45:42.812656Z\",\n  \"updateTime\": \"2020-11-05T17:45:42.812656Z\"\n }\n }\n}\n```\nYou can poll for the status of the operation until the response includes\n`\"done\": true`\n.\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateEndpointSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.CreateEndpointOperationMetadata;import com.google.cloud.aiplatform.v1.Endpoint;import com.google.cloud.aiplatform.v1.EndpointServiceClient;import com.google.cloud.aiplatform.v1.EndpointServiceSettings;import com.google.cloud.aiplatform.v1.LocationName;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class CreateEndpointSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String endpointDisplayName = \"YOUR_ENDPOINT_DISPLAY_NAME\";\u00a0 \u00a0 createEndpointSample(project, endpointDisplayName);\u00a0 }\u00a0 static void createEndpointSample(String project, String endpointDisplayName)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 EndpointServiceSettings endpointServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 EndpointServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (EndpointServiceClient endpointServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 EndpointServiceClient.create(endpointServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 LocationName locationName = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 Endpoint endpoint = Endpoint.newBuilder().setDisplayName(endpointDisplayName).build();\u00a0 \u00a0 \u00a0 OperationFuture<Endpoint, CreateEndpointOperationMetadata> endpointFuture =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 endpointServiceClient.createEndpointAsync(locationName, endpoint);\u00a0 \u00a0 \u00a0 System.out.format(\"Operation name: %s\\n\", endpointFuture.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to finish...\");\u00a0 \u00a0 \u00a0 Endpoint endpointResponse = endpointFuture.get(300, TimeUnit.SECONDS);\u00a0 \u00a0 \u00a0 System.out.println(\"Create Endpoint Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"Name: %s\\n\", endpointResponse.getName());\u00a0 \u00a0 \u00a0 System.out.format(\"Display Name: %s\\n\", endpointResponse.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\"Description: %s\\n\", endpointResponse.getDescription());\u00a0 \u00a0 \u00a0 System.out.format(\"Labels: %s\\n\", endpointResponse.getLabelsMap());\u00a0 \u00a0 \u00a0 System.out.format(\"Create Time: %s\\n\", endpointResponse.getCreateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"Update Time: %s\\n\", endpointResponse.getUpdateTime());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/create-endpoint.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const endpointDisplayName = 'YOUR_ENDPOINT_DISPLAY_NAME';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Endpoint Service Client libraryconst {EndpointServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst endpointServiceClient = new EndpointServiceClient(clientOptions);async function createEndpoint() {\u00a0 // Configure the parent resource\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 const endpoint = {\u00a0 \u00a0 displayName: endpointDisplayName,\u00a0 };\u00a0 const request = {\u00a0 \u00a0 parent,\u00a0 \u00a0 endpoint,\u00a0 };\u00a0 // Get and print out a list of all the endpoints for this resource\u00a0 const [response] = await endpointServiceClient.createEndpoint(request);\u00a0 console.log(`Long running operation : ${response.name}`);\u00a0 // Wait for operation to complete\u00a0 await response.promise();\u00a0 const result = response.result;\u00a0 console.log('Create endpoint response');\u00a0 console.log(`\\tName : ${result.name}`);\u00a0 console.log(`\\tDisplay name : ${result.displayName}`);\u00a0 console.log(`\\tDescription : ${result.description}`);\u00a0 console.log(`\\tLabels : ${JSON.stringify(result.labels)}`);\u00a0 console.log(`\\tCreate time : ${JSON.stringify(result.createTime)}`);\u00a0 console.log(`\\tUpdate time : ${JSON.stringify(result.updateTime)}`);}createEndpoint();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_endpoint_sample.py) \n```\ndef create_endpoint_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 location: str,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 endpoint = aiplatform.Endpoint.create(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name,\u00a0 \u00a0 \u00a0 \u00a0 project=project,\u00a0 \u00a0 \u00a0 \u00a0 location=location,\u00a0 \u00a0 )\u00a0 \u00a0 print(endpoint.display_name)\u00a0 \u00a0 print(endpoint.resource_name)\u00a0 \u00a0 return endpoint\n```\n### Get the endpoint ID\nYou need the endpoint ID to deploy the model.\nThe following example uses the [gcloud ai endpoints list command](/sdk/gcloud/reference/ai/endpoints/list) :\n```\n\u00a0 gcloud ai endpoints list \\\u00a0 \u00a0 --region=LOCATION \\\u00a0 \u00a0 --filter=display_name=ENDPOINT_NAME\n```\nReplace the following:- : The region where you are using Vertex AI.\n- : The display name for the endpoint.Note the number that appears in the `ENDPOINT_ID` column. Use this ID in the following step.Before using any of the request data, make the following replacements:- : The region where you are using Vertex AI.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The display name for the endpoint.\nHTTP method and URL:\n```\nGET https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints?filter=display_name=ENDPOINT_NAME\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"endpoints\": [ {\n  \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/endpoints/ENDPOINT_ID\",\n  \"displayName\": \"ENDPOINT_NAME\",\n  \"etag\": \"AMEw9yPz5pf4PwBHbRWOGh0PcAxUdjbdX2Jm3QO_amguy3DbZGP5Oi_YUKRywIE-BtLx\",\n  \"createTime\": \"2020-04-17T18:31:11.585169Z\",\n  \"updateTime\": \"2020-04-17T18:35:08.568959Z\"\n }\n ]\n}\n```\nNote the\n.### Deploy the modelSelect the tab below for your language or environment:The following examples use the [gcloud ai endpoints deploy-model command](/sdk/gcloud/reference/ai/endpoints/deploy-model) .\nThe following example deploys a `Model` to an `Endpoint` without using GPUs to accelerate prediction serving and without splitting traffic between multiple `DeployedModel` resources:\nBefore using any of the command data below, make the following replacements:- : The ID for the endpoint.\n- : The region where you are using Vertex AI.\n- : The ID for the model to be deployed.\n- : A name for the`DeployedModel`. You can use the display name of the`Model`for the`DeployedModel`as well.\n- : Optional. The machine resources used for each node of this deployment. Its default setting is`n1-standard-2`. [Learn more about machine types.](/vertex-ai/docs/predictions/configure-compute) \n- : The minimum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to the maximum number of nodes and never fewer than this number of nodes.    This value must be greater than or equal to 1. If the`--min-replica-count`flag is  omitted, the value defaults to 1.\n- : The maximum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to this number of nodes and never fewer than the minimum number of nodes. If you omit the`--max-replica-count`flag, then  maximum number of nodes is set to the value of`--min-replica-count`.\nExecute the [gcloud ai endpoints deploy-model](/sdk/gcloud/reference/ai/endpoints/deploy-model) command:The `--traffic-split=0=100` flag in the preceding examples sends 100% of prediction traffic that the `Endpoint` receives to the new `DeployedModel` , which is represented by the temporary ID `0` . If your `Endpoint` already has other `DeployedModel` resources, then you can split traffic between the new `DeployedModel` and the old ones. For example, to send 20% of traffic to the new `DeployedModel` and 80% to an older one, run the following command.\nBefore using any of the command data below, make the following replacements:- : the ID of the existing`DeployedModel`.\nExecute the [gcloud ai endpoints deploy-model](/sdk/gcloud/reference/ai/endpoints/deploy-model) command:You use the [endpoints.predict](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/predict) method to request an online prediction.\nDeploy the model.\nBefore using any of the request data, make the following replacements:- : The region where you are using Vertex AI.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The ID for the endpoint.\n- : The ID for the model to be deployed.\n- : A name for the`DeployedModel`. You can use the display name of the`Model`for the`DeployedModel`as well.\n- : Optional. The machine resources used for each node of this deployment. Its default setting is`n1-standard-2`. [Learn more about machine types.](/vertex-ai/docs/predictions/configure-compute) \n- : The type of accelerator to be attached to the machine. Optional  ifis not specified or is zero. Not recommended for  AutoML models or custom-trained models that are using non-GPU images. [Learn more](/vertex-ai/docs/predictions/configure-compute#gpus) .\n- : The number of accelerators for each replica to use.  Optional. Should be zero or unspecified for AutoML models or custom-trained models  that are using non-GPU images.\n- : The minimum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to the maximum number of nodes and never fewer than this number of nodes. This value must be greater than or equal to 1.\n- : The maximum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to this number of nodes and never fewer than the minimum number of nodes.\n- : The percentage of the prediction traffic to this endpoint  to be routed to the model being deployed with this operation. Defaults to 100. All traffic  percentages must add up to 100. [Learn more about traffic splits](/vertex-ai/docs/general/deployment#models-endpoint) .\n- : Optional. If other models are deployed to this endpoint, you  must update their traffic split percentages so that all percentages add up to 100.\n- : The traffic split percentage value for the deployed model id  key.\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) \nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:deployModel\n```\nRequest JSON body:\n```\n{\n \"deployedModel\": {\n \"model\": \"projects/PROJECT/locations/us-central1/models/MODEL_ID\",\n \"displayName\": \"DEPLOYED_MODEL_NAME\",\n \"dedicatedResources\": {\n  \"machineSpec\": {\n   \"machineType\": \"MACHINE_TYPE\",\n   \"acceleratorType\": \"ACCELERATOR_TYPE\",\n   \"acceleratorCount\": \"ACCELERATOR_COUNT\"\n  },\n  \"minReplicaCount\": MIN_REPLICA_COUNT,\n  \"maxReplicaCount\": MAX_REPLICA_COUNT\n  },\n },\n \"trafficSplit\": {\n \"0\": TRAFFIC_SPLIT_THIS_MODEL,\n \"DEPLOYED_MODEL_ID_1\": TRAFFIC_SPLIT_MODEL_1,\n \"DEPLOYED_MODEL_ID_2\": TRAFFIC_SPLIT_MODEL_2\n },\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_ID/locations/LOCATION/endpoints/ENDPOINT_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.DeployModelOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2020-10-19T17:53:16.502088Z\",\n  \"updateTime\": \"2020-10-19T17:53:16.502088Z\"\n }\n }\n}\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/DeployModelCustomTrainedModelSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.DedicatedResources;import com.google.cloud.aiplatform.v1.DeployModelOperationMetadata;import com.google.cloud.aiplatform.v1.DeployModelResponse;import com.google.cloud.aiplatform.v1.DeployedModel;import com.google.cloud.aiplatform.v1.EndpointName;import com.google.cloud.aiplatform.v1.EndpointServiceClient;import com.google.cloud.aiplatform.v1.EndpointServiceSettings;import com.google.cloud.aiplatform.v1.MachineSpec;import com.google.cloud.aiplatform.v1.ModelName;import java.io.IOException;import java.util.HashMap;import java.util.Map;import java.util.concurrent.ExecutionException;public class DeployModelCustomTrainedModelSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, ExecutionException, InterruptedException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"PROJECT\";\u00a0 \u00a0 String endpointId = \"ENDPOINT_ID\";\u00a0 \u00a0 String modelName = \"MODEL_NAME\";\u00a0 \u00a0 String deployedModelDisplayName = \"DEPLOYED_MODEL_DISPLAY_NAME\";\u00a0 \u00a0 deployModelCustomTrainedModelSample(project, endpointId, modelName, deployedModelDisplayName);\u00a0 }\u00a0 static void deployModelCustomTrainedModelSample(\u00a0 \u00a0 \u00a0 String project, String endpointId, String model, String deployedModelDisplayName)\u00a0 \u00a0 \u00a0 throws IOException, ExecutionException, InterruptedException {\u00a0 \u00a0 EndpointServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 EndpointServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (EndpointServiceClient client = EndpointServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 MachineSpec machineSpec = MachineSpec.newBuilder().setMachineType(\"n1-standard-2\").build();\u00a0 \u00a0 \u00a0 DedicatedResources dedicatedResources =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 DedicatedResources.newBuilder().setMinReplicaCount(1).setMachineSpec(machineSpec).build();\u00a0 \u00a0 \u00a0 String modelName = ModelName.of(project, location, model).toString();\u00a0 \u00a0 \u00a0 DeployedModel deployedModel =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 DeployedModel.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModel(modelName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(deployedModelDisplayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // `dedicated_resources` must be used for non-AutoML models\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDedicatedResources(dedicatedResources)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // key '0' assigns traffic for the newly deployed model\u00a0 \u00a0 \u00a0 // Traffic percentage values must add up to 100\u00a0 \u00a0 \u00a0 // Leave dictionary empty if endpoint should not accept any traffic\u00a0 \u00a0 \u00a0 Map<String, Integer> trafficSplit = new HashMap<>();\u00a0 \u00a0 \u00a0 trafficSplit.put(\"0\", 100);\u00a0 \u00a0 \u00a0 EndpointName endpoint = EndpointName.of(project, location, endpointId);\u00a0 \u00a0 \u00a0 OperationFuture<DeployModelResponse, DeployModelOperationMetadata> response =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 client.deployModelAsync(endpoint, deployedModel, trafficSplit);\u00a0 \u00a0 \u00a0 // You can use OperationFuture.getInitialFuture to get a future representing the initial\u00a0 \u00a0 \u00a0 // response to the request, which contains information while the operation is in progress.\u00a0 \u00a0 \u00a0 System.out.format(\"Operation name: %s\\n\", response.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 // OperationFuture.get() will block until the operation is finished.\u00a0 \u00a0 \u00a0 DeployModelResponse deployModelResponse = response.get();\u00a0 \u00a0 \u00a0 System.out.format(\"deployModelResponse: %s\\n\", deployModelResponse);\u00a0 \u00a0 }\u00a0 }}\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/deploy_model_with_dedicated_resources_sample.py) \n```\ndef deploy_model_with_dedicated_resources_sample(\u00a0 \u00a0 project,\u00a0 \u00a0 location,\u00a0 \u00a0 model_name: str,\u00a0 \u00a0 machine_type: str,\u00a0 \u00a0 endpoint: Optional[aiplatform.Endpoint] = None,\u00a0 \u00a0 deployed_model_display_name: Optional[str] = None,\u00a0 \u00a0 traffic_percentage: Optional[int] = 0,\u00a0 \u00a0 traffic_split: Optional[Dict[str, int]] = None,\u00a0 \u00a0 min_replica_count: int = 1,\u00a0 \u00a0 max_replica_count: int = 1,\u00a0 \u00a0 accelerator_type: Optional[str] = None,\u00a0 \u00a0 accelerator_count: Optional[int] = None,\u00a0 \u00a0 explanation_metadata: Optional[explain.ExplanationMetadata] = None,\u00a0 \u00a0 explanation_parameters: Optional[explain.ExplanationParameters] = None,\u00a0 \u00a0 metadata: Optional[Sequence[Tuple[str, str]]] = (),\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 model_name: A fully-qualified model resource name or model ID.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Example: \"projects/123/locations/us-central1/models/456\" or\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"456\" when project and location are initialized or passed.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 model = aiplatform.Model(model_name=model_name)\u00a0 \u00a0 # The explanation_metadata and explanation_parameters should only be\u00a0 \u00a0 # provided for a custom trained model and not an AutoML model.\u00a0 \u00a0 model.deploy(\u00a0 \u00a0 \u00a0 \u00a0 endpoint=endpoint,\u00a0 \u00a0 \u00a0 \u00a0 deployed_model_display_name=deployed_model_display_name,\u00a0 \u00a0 \u00a0 \u00a0 traffic_percentage=traffic_percentage,\u00a0 \u00a0 \u00a0 \u00a0 traffic_split=traffic_split,\u00a0 \u00a0 \u00a0 \u00a0 machine_type=machine_type,\u00a0 \u00a0 \u00a0 \u00a0 min_replica_count=min_replica_count,\u00a0 \u00a0 \u00a0 \u00a0 max_replica_count=max_replica_count,\u00a0 \u00a0 \u00a0 \u00a0 accelerator_type=accelerator_type,\u00a0 \u00a0 \u00a0 \u00a0 accelerator_count=accelerator_count,\u00a0 \u00a0 \u00a0 \u00a0 explanation_metadata=explanation_metadata,\u00a0 \u00a0 \u00a0 \u00a0 explanation_parameters=explanation_parameters,\u00a0 \u00a0 \u00a0 \u00a0 metadata=metadata,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 return model\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/automl/tables/create-model.v1beta1.js) \n```\nconst automl = require('@google-cloud/automl');const client = new automl.v1beta1.AutoMlClient();/**\u00a0* Demonstrates using the AutoML client to create a model.\u00a0* TODO(developer): Uncomment the following lines before running the sample.\u00a0*/// const projectId = '[PROJECT_ID]' e.g., \"my-gcloud-project\";// const computeRegion = '[REGION_NAME]' e.g., \"us-central1\";// const datasetId = '[DATASET_ID]' e.g., \"TBL2246891593778855936\";// const tableId = '[TABLE_ID]' e.g., \"1991013247762825216\";// const columnId = '[COLUMN_ID]' e.g., \"773141392279994368\";// const modelName = '[MODEL_NAME]' e.g., \"testModel\";// const trainBudget = '[TRAIN_BUDGET]' e.g., \"1000\",// `Train budget in milli node hours`;// A resource that represents Google Cloud Platform location.const projectLocation = client.locationPath(projectId, computeRegion);// Get the full path of the column.const columnSpecId = client.columnSpecPath(\u00a0 projectId,\u00a0 computeRegion,\u00a0 datasetId,\u00a0 tableId,\u00a0 columnId);// Set target column to train the model.const targetColumnSpec = {name: columnSpecId};// Set tables model metadata.const tablesModelMetadata = {\u00a0 targetColumnSpec: targetColumnSpec,\u00a0 trainBudgetMilliNodeHours: trainBudget,};// Set datasetId, model name and model metadata for the dataset.const myModel = {\u00a0 datasetId: datasetId,\u00a0 displayName: modelName,\u00a0 tablesModelMetadata: tablesModelMetadata,};// Create a model with the model metadata in the region.client\u00a0 .createModel({parent: projectLocation, model: myModel})\u00a0 .then(responses => {\u00a0 \u00a0 const initialApiResponse = responses[1];\u00a0 \u00a0 console.log(`Training operation name: ${initialApiResponse.name}`);\u00a0 \u00a0 console.log('Training started...');\u00a0 })\u00a0 .catch(err => {\u00a0 \u00a0 console.error(err);\u00a0 });\n```\nLearn how to [change thedefault settings for prediction logging](/vertex-ai/docs/predictions/online-prediction-logging#enabling-and-disabling) .\n### Get operation status\nSome requests start long-running operations that require time to complete. These requests return an operation name, which you can use to view the operation's status or cancel the operation. Vertex AI provides helper methods to make calls against long-running operations. For more information, see [Working with long-runningoperations](/vertex-ai/docs/general/long-running-operations) .\n## Get an online prediction using your deployed model\nTo make an online prediction, submit one or more test items to a model for analysis, and the model returns results that are based on your model's objective. Use the Google Cloud console or the Vertex AI API to request an online prediction.\n- In the Google Cloud console, in the Vertex AI section, go to the **Models** page. [Go to the Models page](https://console.cloud.google.com/vertex-ai/models) \n- From the list of models, click the name of the model to request predictions from.\n- Select the **Deploy & test** tab.\n- Under the **Test your model** section, add test items to request a prediction. The baseline prediction data is filled in for you, or you can enter your own prediction data and click **Predict** .After the prediction is complete, Vertex AI returns the results in the console.\n- Create a file named `request.json` with the following contents:```\n  {\n \"instances\": [ {\n  PREDICTION_DATA_ROW\n }\n ]\n}\n \n```Replace the following:- : A JSON object with keys as the feature names and values as the corresponding feature values. For example, for a dataset with a number, an array of strings, and a category, the row of data might look like the following example request:```\n\"length\":3.6,\n\"material\":\"cotton\",\n\"tag_array\": [\"abc\",\"def\"]\n```A value must be provided for every feature included in training. The format of the data used for prediction must match the format used for training. Refer to [Data format for predictions](/vertex-ai/docs/datasets/data-types-tabular#format-for-prediction) for details.\n- Run the following command:```\ngcloud ai endpoints predict ENDPOINT_ID \\\u00a0 --region=LOCATION_ID \\\u00a0 --json-request=request.json\n```Replace the following:- : The ID for the endpoint.\n- : The region where you are using Vertex AI.You use the [endpoints.predict](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/predict) method to request an online prediction.\nBefore using any of the request data, make the following replacements:- : Region where Endpoint is located. For example,`us-central1`.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The ID for the endpoint.\n- : A JSON object with keys as the feature names and values as the corresponding feature values. For example, for a dataset with a number, an array of strings, and a category, the row of data might look like the following example request:```\n\"length\":3.6,\n\"material\":\"cotton\",\n\"tag_array\": [\"abc\",\"def\"]\n```A value must be provided for every feature included in training. The format of the data used for prediction must match the format used for training. Refer to [Data format for predictions](/vertex-ai/docs/datasets/data-types-tabular#format-for-prediction) for details.\n- : Output by the`predict`method, and accepted as input   by the`explain`method. The ID of the model used to generate   the prediction. If you need to request explanations for a previously requested prediction,   and you have more than one model deployed, you can use this ID to ensure that the explanations   are returned for the same model that provided the previous prediction.\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:predict\n```\nRequest JSON body:\n```\n{\n \"instances\": [ {\n  PREDICTION_DATA_ROW\n }\n ]\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:predict\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:predict\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following:\n```\n {\n  \"predictions\": [  {\n   \"scores\": [   0.96771615743637085,\n   0.032283786684274673\n   ],\n   \"classes\": [   \"0\",\n   \"1\"\n   ]\n  }\n  ]\n  \"deployedModelId\": \"2429510197\"\n }\n \n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/PredictTabularClassificationSample.java) \n```\nimport com.google.cloud.aiplatform.util.ValueConverter;import com.google.cloud.aiplatform.v1.EndpointName;import com.google.cloud.aiplatform.v1.PredictResponse;import com.google.cloud.aiplatform.v1.PredictionServiceClient;import com.google.cloud.aiplatform.v1.PredictionServiceSettings;import com.google.cloud.aiplatform.v1.schema.predict.prediction.TabularClassificationPredictionResult;import com.google.protobuf.ListValue;import com.google.protobuf.Value;import com.google.protobuf.util.JsonFormat;import java.io.IOException;import java.util.List;public class PredictTabularClassificationSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String instance = \"[{ \u201cfeature_column_a\u201d: \u201cvalue\u201d, \u201cfeature_column_b\u201d: \u201cvalue\u201d}]\";\u00a0 \u00a0 String endpointId = \"YOUR_ENDPOINT_ID\";\u00a0 \u00a0 predictTabularClassification(instance, project, endpointId);\u00a0 }\u00a0 static void predictTabularClassification(String instance, String project, String endpointId)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 PredictionServiceSettings predictionServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (PredictionServiceClient predictionServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceClient.create(predictionServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 EndpointName endpointName = EndpointName.of(project, location, endpointId);\u00a0 \u00a0 \u00a0 ListValue.Builder listValue = ListValue.newBuilder();\u00a0 \u00a0 \u00a0 JsonFormat.parser().merge(instance, listValue);\u00a0 \u00a0 \u00a0 List<Value> instanceList = listValue.getValuesList();\u00a0 \u00a0 \u00a0 Value parameters = Value.newBuilder().setListValue(listValue).build();\u00a0 \u00a0 \u00a0 PredictResponse predictResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 predictionServiceClient.predict(endpointName, instanceList, parameters);\u00a0 \u00a0 \u00a0 System.out.println(\"Predict Tabular Classification Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\tDeployed Model Id: %s\\n\", predictResponse.getDeployedModelId());\u00a0 \u00a0 \u00a0 System.out.println(\"Predictions\");\u00a0 \u00a0 \u00a0 for (Value prediction : predictResponse.getPredictionsList()) {\u00a0 \u00a0 \u00a0 \u00a0 TabularClassificationPredictionResult.Builder resultBuilder =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TabularClassificationPredictionResult.newBuilder();\u00a0 \u00a0 \u00a0 \u00a0 TabularClassificationPredictionResult result =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (TabularClassificationPredictionResult)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ValueConverter.fromValue(resultBuilder, prediction);\u00a0 \u00a0 \u00a0 \u00a0 for (int i = 0; i < result.getClassesCount(); i++) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\tClass: %s\", result.getClasses(i));\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\tScore: %f\", result.getScores(i));\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/predict-tabular-classification.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const endpointId = 'YOUR_ENDPOINT_ID';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';const aiplatform = require('@google-cloud/aiplatform');const {prediction} =\u00a0 aiplatform.protos.google.cloud.aiplatform.v1.schema.predict;// Imports the Google Cloud Prediction service clientconst {PredictionServiceClient} = aiplatform.v1;// Import the helper module for converting arbitrary protobuf.Value objects.const {helpers} = aiplatform;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst predictionServiceClient = new PredictionServiceClient(clientOptions);async function predictTablesClassification() {\u00a0 // Configure the endpoint resource\u00a0 const endpoint = `projects/${project}/locations/${location}/endpoints/${endpointId}`;\u00a0 const parameters = helpers.toValue({});\u00a0 const instance = helpers.toValue({\u00a0 \u00a0 petal_length: '1.4',\u00a0 \u00a0 petal_width: '1.3',\u00a0 \u00a0 sepal_length: '5.1',\u00a0 \u00a0 sepal_width: '2.8',\u00a0 });\u00a0 const instances = [instance];\u00a0 const request = {\u00a0 \u00a0 endpoint,\u00a0 \u00a0 instances,\u00a0 \u00a0 parameters,\u00a0 };\u00a0 // Predict request\u00a0 const [response] = await predictionServiceClient.predict(request);\u00a0 console.log('Predict tabular classification response');\u00a0 console.log(`\\tDeployed model id : ${response.deployedModelId}\\n`);\u00a0 const predictions = response.predictions;\u00a0 console.log('Predictions :');\u00a0 for (const predictionResultVal of predictions) {\u00a0 \u00a0 const predictionResultObj =\u00a0 \u00a0 \u00a0 prediction.TabularClassificationPredictionResult.fromValue(\u00a0 \u00a0 \u00a0 \u00a0 predictionResultVal\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 for (const [i, class_] of predictionResultObj.classes.entries()) {\u00a0 \u00a0 \u00a0 console.log(`\\tClass: ${class_}`);\u00a0 \u00a0 \u00a0 console.log(`\\tScore: ${predictionResultObj.scores[i]}\\n\\n`);\u00a0 \u00a0 }\u00a0 }}predictTablesClassification();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/predict_tabular_classification_sample.py) \n```\ndef predict_tabular_classification_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 endpoint_name: str,\u00a0 \u00a0 instances: List[Dict],):\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 Args\u00a0 \u00a0 \u00a0 \u00a0 project: Your project ID or project number.\u00a0 \u00a0 \u00a0 \u00a0 location: Region where Endpoint is located. For example, 'us-central1'.\u00a0 \u00a0 \u00a0 \u00a0 endpoint_name: A fully qualified endpoint name or endpoint ID. Example: \"projects/123/locations/us-central1/endpoints/456\" or\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"456\" when project and location are initialized or passed.\u00a0 \u00a0 \u00a0 \u00a0 instances: A list of one or more instances (examples) to return a prediction for.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 endpoint = aiplatform.Endpoint(endpoint_name)\u00a0 \u00a0 response = endpoint.predict(instances=instances)\u00a0 \u00a0 for prediction_ in response.predictions:\u00a0 \u00a0 \u00a0 \u00a0 print(prediction_)\n```\n- Create a file named `request.json` with the following contents:```\n  {\n \"instances\": [ {\n  PREDICTION_DATA_ROW\n }\n ]\n}\n \n```Replace the following:- : A JSON object with keys as the feature names and values as the corresponding feature values. For example, for a dataset with a number, an array of numbers, and a category, the row of data might look like the following example request:```\n\"age\":3.6,\n\"sq_ft\":5392,\n\"code\": \"90331\"\n```A value must be provided for every feature included in training. The format of the data used for prediction must match the format used for training. Refer to [Data format for predictions](/vertex-ai/docs/datasets/data-types-tabular#format-for-prediction) for details.\n- Run the following command:```\ngcloud ai endpoints predict ENDPOINT_ID \\\u00a0 --region=LOCATION_ID \\\u00a0 --json-request=request.json\n```Replace the following:- : The ID for the endpoint.\n- : The region where you are using Vertex AI.You use the [endpoints.predict](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/predict) method to request an online prediction.\nBefore using any of the request data, make the following replacements:- : Region where Endpoint is located. For example,`us-central1`.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The ID for the endpoint.\n- : A JSON object with keys as the feature names and values as the corresponding feature values. For example, for a dataset with a number, an array of numbers, and a category, the row of data might look like the following example request:```\n\"age\":3.6,\n\"sq_ft\":5392,\n\"code\": \"90331\"\n```A value must be provided for every feature included in training. The format of the data used for prediction must match the format used for training. Refer to [Data format for predictions](/vertex-ai/docs/datasets/data-types-tabular#format-for-prediction) for details.\n- : Output by the`predict`method, and accepted as input   by the`explain`method. The ID of the model used to generate   the prediction. If you need to request explanations for a previously requested prediction,   and you have more than one model deployed, you can use this ID to ensure that the explanations   are returned for the same model that provided the previous prediction.\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:predict\n```\nRequest JSON body:\n```\n{\n \"instances\": [ {\n  PREDICTION_DATA_ROW\n }\n ]\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:predict\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:predict\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following:\n```\n{\n \"predictions\": [ [  {\n  \"value\": 65.14233,\n  \"lower_bound\": 4.6572,\n  \"upper_bound\": 164.0279\n  }\n ]\n ],\n \"deployedModelId\": \"DEPLOYED_MODEL_ID\"\n}\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/PredictTabularRegressionSample.java) \n```\nimport com.google.cloud.aiplatform.util.ValueConverter;import com.google.cloud.aiplatform.v1.EndpointName;import com.google.cloud.aiplatform.v1.PredictResponse;import com.google.cloud.aiplatform.v1.PredictionServiceClient;import com.google.cloud.aiplatform.v1.PredictionServiceSettings;import com.google.cloud.aiplatform.v1.schema.predict.prediction.TabularRegressionPredictionResult;import com.google.protobuf.ListValue;import com.google.protobuf.Value;import com.google.protobuf.util.JsonFormat;import java.io.IOException;import java.util.List;public class PredictTabularRegressionSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String instance = \"[{ \u201cfeature_column_a\u201d: \u201cvalue\u201d, \u201cfeature_column_b\u201d: \u201cvalue\u201d}]\";\u00a0 \u00a0 String endpointId = \"YOUR_ENDPOINT_ID\";\u00a0 \u00a0 predictTabularRegression(instance, project, endpointId);\u00a0 }\u00a0 static void predictTabularRegression(String instance, String project, String endpointId)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 PredictionServiceSettings predictionServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (PredictionServiceClient predictionServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceClient.create(predictionServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 EndpointName endpointName = EndpointName.of(project, location, endpointId);\u00a0 \u00a0 \u00a0 ListValue.Builder listValue = ListValue.newBuilder();\u00a0 \u00a0 \u00a0 JsonFormat.parser().merge(instance, listValue);\u00a0 \u00a0 \u00a0 List<Value> instanceList = listValue.getValuesList();\u00a0 \u00a0 \u00a0 Value parameters = Value.newBuilder().setListValue(listValue).build();\u00a0 \u00a0 \u00a0 PredictResponse predictResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 predictionServiceClient.predict(endpointName, instanceList, parameters);\u00a0 \u00a0 \u00a0 System.out.println(\"Predict Tabular Regression Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\tDisplay Model Id: %s\\n\", predictResponse.getDeployedModelId());\u00a0 \u00a0 \u00a0 System.out.println(\"Predictions\");\u00a0 \u00a0 \u00a0 for (Value prediction : predictResponse.getPredictionsList()) {\u00a0 \u00a0 \u00a0 \u00a0 TabularRegressionPredictionResult.Builder resultBuilder =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TabularRegressionPredictionResult.newBuilder();\u00a0 \u00a0 \u00a0 \u00a0 TabularRegressionPredictionResult result =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (TabularRegressionPredictionResult) ValueConverter.fromValue(resultBuilder, prediction);\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\tUpper bound: %f\\n\", result.getUpperBound());\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\tLower bound: %f\\n\", result.getLowerBound());\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\tValue: %f\\n\", result.getValue());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/predict-tabular-regression.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const endpointId = 'YOUR_ENDPOINT_ID';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';const aiplatform = require('@google-cloud/aiplatform');const {prediction} =\u00a0 aiplatform.protos.google.cloud.aiplatform.v1.schema.predict;// Imports the Google Cloud Prediction service clientconst {PredictionServiceClient} = aiplatform.v1;// Import the helper module for converting arbitrary protobuf.Value objects.const {helpers} = aiplatform;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst predictionServiceClient = new PredictionServiceClient(clientOptions);async function predictTablesRegression() {\u00a0 // Configure the endpoint resource\u00a0 const endpoint = `projects/${project}/locations/${location}/endpoints/${endpointId}`;\u00a0 const parameters = helpers.toValue({});\u00a0 // TODO (erschmid): Make this less painful\u00a0 const instance = helpers.toValue({\u00a0 \u00a0 BOOLEAN_2unique_NULLABLE: false,\u00a0 \u00a0 DATETIME_1unique_NULLABLE: '2019-01-01 00:00:00',\u00a0 \u00a0 DATE_1unique_NULLABLE: '2019-01-01',\u00a0 \u00a0 FLOAT_5000unique_NULLABLE: 1611,\u00a0 \u00a0 FLOAT_5000unique_REPEATED: [2320, 1192],\u00a0 \u00a0 INTEGER_5000unique_NULLABLE: '8',\u00a0 \u00a0 NUMERIC_5000unique_NULLABLE: 16,\u00a0 \u00a0 STRING_5000unique_NULLABLE: 'str-2',\u00a0 \u00a0 STRUCT_NULLABLE: {\u00a0 \u00a0 \u00a0 BOOLEAN_2unique_NULLABLE: false,\u00a0 \u00a0 \u00a0 DATE_1unique_NULLABLE: '2019-01-01',\u00a0 \u00a0 \u00a0 DATETIME_1unique_NULLABLE: '2019-01-01 00:00:00',\u00a0 \u00a0 \u00a0 FLOAT_5000unique_NULLABLE: 1308,\u00a0 \u00a0 \u00a0 FLOAT_5000unique_REPEATED: [2323, 1178],\u00a0 \u00a0 \u00a0 FLOAT_5000unique_REQUIRED: 3089,\u00a0 \u00a0 \u00a0 INTEGER_5000unique_NULLABLE: '1777',\u00a0 \u00a0 \u00a0 NUMERIC_5000unique_NULLABLE: 3323,\u00a0 \u00a0 \u00a0 TIME_1unique_NULLABLE: '23:59:59.999999',\u00a0 \u00a0 \u00a0 STRING_5000unique_NULLABLE: 'str-49',\u00a0 \u00a0 \u00a0 TIMESTAMP_1unique_NULLABLE: '1546387199999999',\u00a0 \u00a0 },\u00a0 \u00a0 TIMESTAMP_1unique_NULLABLE: '1546387199999999',\u00a0 \u00a0 TIME_1unique_NULLABLE: '23:59:59.999999',\u00a0 });\u00a0 const instances = [instance];\u00a0 const request = {\u00a0 \u00a0 endpoint,\u00a0 \u00a0 instances,\u00a0 \u00a0 parameters,\u00a0 };\u00a0 // Predict request\u00a0 const [response] = await predictionServiceClient.predict(request);\u00a0 console.log('Predict tabular regression response');\u00a0 console.log(`\\tDeployed model id : ${response.deployedModelId}`);\u00a0 const predictions = response.predictions;\u00a0 console.log('\\tPredictions :');\u00a0 for (const predictionResultVal of predictions) {\u00a0 \u00a0 const predictionResultObj =\u00a0 \u00a0 \u00a0 prediction.TabularRegressionPredictionResult.fromValue(\u00a0 \u00a0 \u00a0 \u00a0 predictionResultVal\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 console.log(`\\tUpper bound: ${predictionResultObj.upper_bound}`);\u00a0 \u00a0 console.log(`\\tLower bound: ${predictionResultObj.lower_bound}`);\u00a0 \u00a0 console.log(`\\tLower bound: ${predictionResultObj.value}`);\u00a0 }}predictTablesRegression();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/predict_tabular_regression_sample.py) \n```\ndef predict_tabular_regression_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 endpoint_name: str,\u00a0 \u00a0 instances: List[Dict],):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 endpoint = aiplatform.Endpoint(endpoint_name)\u00a0 \u00a0 response = endpoint.predict(instances=instances)\u00a0 \u00a0 for prediction_ in response.predictions:\u00a0 \u00a0 \u00a0 \u00a0 print(prediction_)\n```\n## Interpret prediction results\nClassification models return a confidence score.\nThe confidence score communicates how strongly your model associates each class or label with a test item. The higher the number, the higher the model's confidence that the label should be applied to that item. You decide how high the confidence score must be for you to accept the model's results.Regression models return a prediction value. For BigQuery destinations, they also return a prediction interval. The prediction interval provides a range of values that the model has 95% confidence contain the actual result.\n## Get an online explanation using your deployed model\nYou can request a prediction with explanations (also called feature attributions) to see how your model arrived at a prediction. The local feature importance values tell you how much each feature contributed to the prediction result. Feature attributions are included in Vertex AI predictions through [Vertex Explainable AI](/vertex-ai/docs/tabular-data/classification-explanations) .\nWhen you use the Google Cloud console to request an online prediction, the local feature importance values are automatically returned.\nIf you used the pre-filled prediction values, the local feature importance values are all zero. This is because the pre-filled values are the baseline prediction data, so the prediction returned is the baseline prediction value.\n **Note:** You might need to scroll the parameter window to the right to see the local feature importance results.\n- Create a file named `request.json` with the following contents:```\n{\u00a0 \"instances\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 PREDICTION_DATA_ROW\u00a0 \u00a0 }\u00a0 ]}\n```Replace the following:- : A JSON object with keys as the feature names and values as the corresponding feature values. For example, for a dataset with a number, an array of strings, and a category, the row of data might look like the following example request:```\n\"length\":3.6,\n\"material\":\"cotton\",\n\"tag_array\": [\"abc\",\"def\"]\n```A value must be provided for every feature included in training. The format of the data used for prediction must match the format used for training. Refer to [Data format for predictions](/vertex-ai/docs/datasets/data-types-tabular#format-for-prediction) for details.\n- Run the following command:```\ngcloud ai endpoints explain ENDPOINT_ID \\\u00a0 --region=LOCATION_ID \\\u00a0 --json-request=request.json\n```Replace the following:- : The ID for the endpoint.\n- : The region where you are using Vertex AI.\nOptionally, if you want to send an explanation request to a specific `DeployedModel` on the `Endpoint` , you can specify the `--deployed-model-id` flag:```\ngcloud ai endpoints explain ENDPOINT_ID \\\u00a0 --region=LOCATION \\\u00a0 --deployed-model-id=DEPLOYED_MODEL_ID \\\u00a0 --json-request=request.json\n```In addition to the placeholders described previously, replace the following:- Optional: The ID of the deployed model for which you want to get explanations. The ID is included in the`predict`method's response. If you need to request explanations for a particular model and you have more than one model deployed to the same endpoint, you can use this ID to ensure that the explanations are returned for that particular model.The following example shows an online prediction request for a tabular classification model with local feature attributions. The request format is the same for regression models.\nBefore using any of the request data, make the following replacements:- : Region where Endpoint is located. For example,`us-central1`.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The ID for the endpoint.\n- : A JSON object with keys as the feature names and values as the corresponding feature values. For example, for a dataset with a number, an array of strings, and a category, the row of data might look like the following example request:```\n\"length\":3.6,\n\"material\":\"cotton\",\n\"tag_array\": [\"abc\",\"def\"]\n```A value must be provided for every feature included in training. The format of the data used for prediction must match the format used for training. Refer to [Data format for predictions](/vertex-ai/docs/datasets/data-types-tabular#format-for-prediction) for details.\n- (optional): The ID of the deployed model for which you want to get  explanations. The ID is included in the`predict`method's response. If you need to  request explanations for a particular model and you have more than one model deployed to the same  endpoint, you can use this ID to ensure that the explanations are returned for that particular  model.\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/endpoints/ENDPOINT_ID:explain\n```\nRequest JSON body:\n```\n{\n \"instances\": [ {\n  PREDICTION_DATA_ROW\n }\n ],\n \"deployedModelId\": \"DEPLOYED_MODEL_ID\"\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/endpoints/ENDPOINT_ID:explain\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/endpoints/ENDPOINT_ID:explain\" | Select-Object -Expand Content\n```\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/explain_sample.py) \n```\ndef explain_sample(project: str, location: str, endpoint_id: str, instance_dict: Dict):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 endpoint = aiplatform.Endpoint(endpoint_id)\u00a0 \u00a0 response = endpoint.explain(instances=[instance_dict], parameters={})\u00a0 \u00a0 for explanation in response.explanations:\u00a0 \u00a0 \u00a0 \u00a0 print(\" explanation\")\u00a0 \u00a0 \u00a0 \u00a0 # Feature attributions.\u00a0 \u00a0 \u00a0 \u00a0 attributions = explanation.attributions\u00a0 \u00a0 \u00a0 \u00a0 for attribution in attributions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\" \u00a0attribution\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\" \u00a0 baseline_output_value:\", attribution.baseline_output_value)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\" \u00a0 instance_output_value:\", attribution.instance_output_value)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\" \u00a0 output_display_name:\", attribution.output_display_name)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\" \u00a0 approximation_error:\", attribution.approximation_error)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\" \u00a0 output_name:\", attribution.output_name)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output_index = attribution.output_index\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for output_index in output_index:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\" \u00a0 output_index:\", output_index)\u00a0 \u00a0 for prediction in response.predictions:\u00a0 \u00a0 \u00a0 \u00a0 print(prediction)\n```\nBecause explanations increase resource usage, you might want to reserve requesting explanations for situations when you specifically need them. Sometimes, it can be helpful to request explanations for a prediction result you've already received, perhaps because the prediction was an outlier or did not make sense.\nIf all of your predictions are coming from the same model, you can simply resend the request data, with explanations requested this time. However, if you have multiple models returning predictions, you must make sure you send the explanation request to the correct model. You can view explanations for a particular model by including the deployed model's ID `deployedModelID` in your request, which is included in the response of the original prediction request. Note that the deployed model ID is different from the model ID.\n## Interpret explanation results\nTo calculate local feature importance, first the is calculated. Baseline values are computed from the training data, using the median value for numeric features and the mode for categorical features. The prediction generated from the baseline values is the . Baseline values are calculated once for a model and do not change.\nFor a specific prediction, the local feature importance for each feature tells you how much that feature added to or subtracted from the result as compared with the baseline prediction score. The sum of all of the feature importance values equals the difference between the baseline prediction score and the prediction result.\nFor classification models, the score is always between 0.0 and 1.0, inclusive. Therefore, local feature importance values for classification models are always between -1.0 and 1.0 (inclusive).\n[Feature Attributions for Classification and Regression](/vertex-ai/docs/tabular-data/classification-explanations)\n## Example output for predictions and explanations\nThe return payload for an online prediction from a tabular classification model with feature importance looks similar to the following example.\nThe `instanceOutputValue` of `0.928652400970459` is the confidence score of the highest-scoring class, in this case `class_a` . The `baselineOutputValue` field contains the baseline prediction score, `0.808652400970459` . The feature that contributed most strongly to this result was `feature_3` .\n```\n{\"predictions\": [\u00a0 {\u00a0 \u00a0 \"scores\": [\u00a0 \u00a0 \u00a0 0.928652400970459,\u00a0 \u00a0 \u00a0 0.071347599029541\u00a0 \u00a0 ],\u00a0 \u00a0 \"classes\": [\u00a0 \u00a0 \u00a0 \"class_a\",\u00a0 \u00a0 \u00a0 \"class_b\"\u00a0 \u00a0 ]\u00a0 }]\"explanations\": [\u00a0 {\u00a0 \u00a0 \"attributions\": [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \"baselineOutputValue\": 0.808652400970459,\u00a0 \u00a0 \u00a0 \u00a0 \"instanceOutputValue\": 0.928652400970459,\u00a0 \u00a0 \u00a0 \u00a0 \"approximationError\": \u00a00.0058915703929231,\u00a0 \u00a0 \u00a0 \u00a0 \"featureAttributions\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"feature_1\": 0.012394922231235,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"feature_2\": 0.050212341234556,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"feature_3\": 0.057392736534209,\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \"outputIndex\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 0\u00a0 \u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \u00a0 \"outputName\": \"scores\"\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 ],\u00a0 }]\"deployedModelId\": \"234567\"}\n```\nThe return payload for an online prediction with feature importance from a tabular regression model looks similar to the following example.\nThe `instanceOutputValue` of `1795.1246466281819` is the predicted value, with the `lower_bound` and `upper_bound` fields providing the 95% confidence interval. The `baselineOutputValue` field contains the baseline prediction score, `1788.7423095703125` . The feature that contributed most strongly to this result was `feature_3` .\n```\n{\"predictions\": [\u00a0 {\u00a0 \u00a0 \"value\": 1795.1246466281819,\u00a0 \u00a0 \"lower_bound\": 246.32196807861328,\u00a0 \u00a0 \"upper_bound\": 8677.51904296875\u00a0 }]\"explanations\": [\u00a0 {\u00a0 \u00a0 \"attributions\": [\u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \"baselineOutputValue\": 1788.7423095703125,\u00a0 \u00a0 \u00a0 \u00a0 \"instanceOutputValue\": 1795.1246466281819,\u00a0 \u00a0 \u00a0 \u00a0 \"approximationError\": 0.0038215703911553,\u00a0 \u00a0 \u00a0 \u00a0 \"featureAttributions\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"feature_1\": 0.123949222312359,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"feature_2\": 0.802123412345569,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"feature_3\": 5.456264423211472,\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \"outputIndex\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 -1\u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 ]\u00a0 }],\"deployedModelId\": \"345678\"}\n```\n## What's next\n- Learn how to [export your model](/vertex-ai/docs/export/export-model-tabular) .", "guide": "Vertex AI"}