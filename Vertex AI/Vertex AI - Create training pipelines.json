{"title": "Vertex AI - Create training pipelines", "url": "https://cloud.google.com/vertex-ai/docs/training/create-training-pipeline", "abstract": "# Vertex AI - Create training pipelines\nTraining pipelines let you perform custom machine learning (ML) training and automatically create a `Model` resource based on your training output.\n", "content": "## Before you create a pipeline\nBefore you create a training pipeline on Vertex AI, you need to create a [Python training application](/vertex-ai/docs/training/create-python-pre-built-container) or a [custom container](/vertex-ai/docs/training/create-custom-container) to define the training code and dependencies you want to run on Vertex AI. If you create a Python training application using TensorFlow, scikit-learn, or XGBoost, you can use our prebuilt containers to run your code. If you're not sure which of these options to choose, refer to the [training code requirements](/vertex-ai/docs/training/code-requirements) to learn more.\n## Training pipeline options\nA encapsulates training jobs with additional steps. This guide explains two different training pipelines:\n- Launch a`CustomJob`and upload the resulting model to Vertex AI\n- Launch a hyperparameter tuning job and upload the resulting model to Vertex AI\nAdditionally, you can use in your training pipeline. [Learn more about configuring your training pipelineto use a managed dataset](#managed-dataset) .\n### What a CustomJob includes\nWhen you create a , you specify settings that Vertex AI needs to run your training code, including:\n- One worker pool for single-node training ( [WorkerPoolSpec](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#workerpoolspec) ), or multiple worker pools for distributed training\n- Optional settings for configuring job scheduling ( [Scheduling](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#scheduling) ), [settingcertain environment variables for your trainingcode](/vertex-ai/docs/training/code-requirements#environment-variables) , [using a customservice account](/vertex-ai/docs/general/custom-service-account) , and [using VPC NetworkPeering](/vertex-ai/docs/general/vpc-peering) \nWithin the worker pool(s), you can specify the following settings:\n- [Machine types and accelerators](/vertex-ai/docs/training/configure-compute) \n- [Configuration of what type of training code the worker poolruns](/vertex-ai/docs/training/configure-container-settings) : either a Python training application ( [PythonPackageSpec](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#pythonpackagespec) ) or a custom container ( [ContainerSpec](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#containerspec) )\nIf you want to create a standalone custom job outside of a Vertex AI training pipeline, refer to the [guide on custom jobs](/vertex-ai/docs/training/create-custom-job) .\n### Configure your pipeline to use a managed dataset\nWithin your training pipeline, you can configure your custom training job or hyperparameter tuning job to use a managed dataset. Managed datasets let you manage your datasets with your training applications and models.\nTo use a managed dataset in your training pipeline:\n- [Create your dataset](/vertex-ai/docs/datasets/overview) .\n- Update your training application to use a managed dataset. For more information, see [how Vertex AI passes your dataset to yourtraining application](/vertex-ai/docs/training/using-managed-datasets) .\n- Specify a managed dataset when you create your training pipeline. For example, if you create your training pipeline using the REST API, specify the dataset settings in the `inputDataConfig` section.You must create the training pipeline in the same region where you created the dataset.\nTo learn more, refer to the API reference on [TrainingPipeline](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines) .\n### Configure distributed training\nWithin your training pipeline, you can configure your custom training job or hyperparameter tuning job for distributed training by specifying multiple worker pools.\nAll the examples on this page show single-replica training jobs with one worker pool. To modify them for distributed training:\n- Use your first worker pool to configure your primary replica, and set the replica count to 1.\n- Add more worker pools to configure worker replicas, parameter server replicas, or evaluator replicas, if your machine learning framework supports these additional cluster tasks for distributed training.\nLearn more about [using distributed training](/vertex-ai/docs/training/distributed-training) .\n## CustomJob and model upload\nThis training pipeline encapsulates a custom job with an added convenience step that makes it easier to deploy your model to Vertex AI after training. This training pipeline does two main things:\n- The training pipeline creates a `CustomJob` resource. The custom job runs the training application using the computing resources that you specify.\n- After the custom job completes, the training pipeline finds the model artifacts that your training application creates in the output directory you specified for your Cloud Storage bucket. It uses these artifacts to create a resource, which sets you up for [model deployment](/vertex-ai/docs/predictions/deploy-model-api) .\nThere are two different ways to set the location for your model artifacts:\n- If you set a [baseOutputDirectory](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#FIELDS.base_output_directory) for your training job, make sure your training code saves your model artifacts to that location, using the [$AIP_MODEL_DIR environment variable](/vertex-ai/docs/training/code-requirements#environment-variables) set by Vertex AI. After the training job is completed, Vertex AI searches for the resulting model artifacts in `gs://` `` `/model` . **Note** : If you're using the Vertex AI SDK for Python, you can omit the`base_output_dir`attribute. In this case, Vertex AI outputs your model artifacts to a timestamped directory in the staging directory. For details, see:- [CustomJob](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob) \n- [CustomContainerTrainingJob](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomContainerTrainingJob#google_cloud_aiplatform_CustomContainerTrainingJob_run) \n- [CustomPythonPackageTrainingJob](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomPythonPackageTrainingJob#google_cloud_aiplatform_CustomPythonPackageTrainingJob_run) - If you set the [modelToUpload.artifactUrifield](/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model.FIELDS.artifact_uri) , the training pipeline uploads the model artifacts from that URI. You must set this field if you didn't set [baseOutputDirectory](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#FIELDS.base_output_directory) .\nIf you specify both `baseOutputDirectory` and `modelToUpload.artifactUri` , Vertex AI uses `modelToUpload.artifactUri` .\nTo create this type of training pipeline:\n- In the Google Cloud console, in the Vertex AI section, go to the **Training pipelines** page. [Go to Training pipelines](https://console.cloud.google.com/vertex-ai/training/training-pipelines) \n- Click **add_boxCreate** to open the **Train new model** pane. **Note:** You can type [model.new](https://model.new) into a browser to go directly to the model creation page.\n- On the **Training method** step, specify the following settings:- If you want to [use a managed dataset fortraining](#managed-dataset) , then specify a **Dataset** and an **Annotation set** .Otherwise, in the **Dataset** drop-down list, select **No manageddataset** .\n- Select **Custom training (advanced)** .\nClick **Continue** .\n- On the **Model details** step, choose **Train new model** or **Train new version** . If you select train new model, enter a name of your choice, , for your model. Click **Continue** .\n- On the **Training container** step, specify the following settings:- Select [whether to use a Prebuilt container or a Customcontainer](/vertex-ai/docs/training/custom-training-methods#pre-built-custom) for training.\n- Depending on your choice, do one of the following:- If you want to use a prebuilt container for training, then provide Vertex AI with information it needs to use the training package that you have uploaded to Cloud Storage:- Use the **Model framework** and **Model framework version** drop-down lists to specify the [prebuiltcontainer](/vertex-ai/docs/training/pre-built-containers) that you want to use.\n- In the **Package location** field, specify the Cloud Storage URI of the [Python training application thatyou have created anduploaded](/vertex-ai/docs/training/create-python-pre-built-container) . This file usually ends with `.tar.gz` .\n- In the **Python module** field, enter the [module name of yourtraining application's entrypoint](/vertex-ai/docs/training/create-python-pre-built-container#python-modules) .\n- If you want to use a [custom container fortraining](/vertex-ai/docs/training/create-custom-container) , then in the **Container image** field, specify the Artifact Registry or Docker Hub URI of your container image.\n- In the **Model output directory** field, specify the Cloud Storage URI of a directory in a bucket that you have access to. The directory does not need to exist yet.This value gets passed to Vertex AI in the [baseOutputDirectory APIfield](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#FIELDS.base_output_directory) , which sets [several environment variables that your training application can accesswhen it runs](/vertex-ai/docs/training/code-requirements#environment-variables) .At the end of training, Vertex AI looks for [modelartifacts](/vertex-ai/docs/training/exporting-model-artifacts) in a subdirectory of this URI in order to create a `Model` . (This subdirectory is available to your training code as the `AIP_MODEL_DIR` environment variable.)When you don't use hyperparameter tuning, Vertex AI expects to find model artifacts in `` `/model/` .\n- **Optional** : In the **Arguments** field, you can specify arguments for Vertex AI to use when it starts running your training code. The maximum length for all arguments combined is 100,000 characters. The behavior of these arguments differs depending on what type of container you are using:- If you are using a prebuilt container, then Vertex AI passes the arguments as command-line flags to your **Python module** .\n- If you are using a custom container, then Vertex AI [overrides your container's CMD instruction with thearguments](/vertex-ai/docs/training/configure-container-settings#configure) .Click **Continue** .\n- On the **Hyperparameter tuning** step, make sure that the **Enablehyperparameter tuning** checkbox is not selected. Click **Continue** .\n- On the **Compute and pricing** step, specify the following settings:- In the **Region** drop-down list, select a \" [region that supports customtraining](/vertex-ai/docs/general/locations) \"\n- In the **Worker pool 0** section, specify [computeresources](/vertex-ai/docs/training/configure-compute) to use for training.If you specify accelerators, [make sure the type of accelerator that youchoose is available in your selectedregion](/vertex-ai/docs/general/locations#region_considerations) .If you want to perform [distributedtraining](/vertex-ai/docs/training/distributed-training) , then click **Add moreworker pools** and specify an additional set of compute resources for each additional worker pool that you want.\nClick **Continue** .\n- On the **Prediction container** step, specify the following settings:- Select [whether to use a Prebuilt container or a Customcontainer](/vertex-ai/docs/model-registry/import-model#container-type) to serve predictions from your trained model.\n- Depending on your choice, do one of the following:- If you want to use a prebuilt container to serve predictions, then use the **Model framework** , **Model framework version** , and **Accelerator type** fields to choose [which prebuilt predictioncontainer](/vertex-ai/docs/predictions/pre-built-containers) to use for prediction.Match **Model framework** and **Model framework version** to the machine learning framework you used for training. Only specify an **Accelerator type** if you want to later [use GPUs for online orbatch predictions](/vertex-ai/docs/predictions/configure-compute) .\n- If you want to use a custom container to serve predictions, then do the following:- In the **Container image** field, specify the [Artifact Registry URI of your containerimage](/vertex-ai/docs/predictions/custom-container-requirements#publishing) .\n- Optionally, you may specify a **Command** to [override thecontainer's ENTRYPOINTinstruction](/vertex-ai/docs/predictions/use-custom-container#create-model) .\n- The **Model directory** field contains the value that you previously set in the **Model output directory** field of the **Training container** step. Changing either of these fields has the same effect. See the [previous instruction](#model-output-directory-False) for more information about this field.\n- Leave the fields in the **Predict schemata** section blank.\n- Click **Start training** to start the custom training pipeline.\nUse the following code sample to create a training pipeline using the [create method of the trainingPipeline resource](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines/create) .\n **Note:** If you want to set this pipeline to create a new model , you can optionally add the `PARENT_MODEL` in the `trainingPipeline` field.\nTo learn more, see [Model versioning with Vertex AI Model Registry](/vertex-ai/docs/model-registry/versioning) .\nBefore using any of the request data, make the following replacements:- : The region where the training code is run and the`Model`is stored.\n- : Your project ID.\n- : Required. A display name for the trainingPipeline.\n- If your training application uses a Vertex AI dataset, specify the following:- : The ID of the dataset.\n- : Filters the dataset by the annotations that you   specify.\n- : Filters the dataset by the specified annotation   schema URI.\n- Use one of the following options to specify how data items are split into training,   validation, and test sets.- To split the dataset based on fractions defining the size of each set, specify    the following:- : The fraction of the dataset to use to train your     model.\n- : The fraction of the dataset to use to validate     your model.\n- : The fraction of the dataset to use to evaluate     your model.\n- To split the dataset based on filters, specify the following:- : Filters the dataset to data items to use for training     your model.\n- : Filters the dataset to data items to use for     validating your model.\n- : Filters the dataset to data items to use for evaluating     your model.\n- To use a predefined split, specify the following:- : The name of the column to use to split     the dataset. Acceptable values in this column include `training`, `validation`,     and `test`.\n- To split the dataset based on the timestamp on the dataitems, specify the following:- : The fraction of the dataset to use to     train your model.\n- : The fraction of the dataset to use to     validate your model.\n- : The fraction of the dataset to use to evaluate     your model.\n- : The name of the timestamp column to use to split     the dataset.\n- : The Cloud Storage location where Vertex AI   exports your training dataset, once it has been split into training, validation, and test   sets.\n- Define the custom training job:- : The type of the machine. Refer  to [available machine types for training](/vertex-ai/docs/training/configure-compute) .\n- : (Optional.) The type of accelerator to attach to each   trial.\n- : (Optional.) The number of accelerators to attach to   each trial.\n- : The number of worker replicas to use for each trial.\n- If your training application runs in a custom container, specify the following:- : The URI of a container image in    Artifact Registry, Container Registry, or Docker Hub that is to be run on each worker    replica.\n- : (Optional.) The command to be invoked when the    container is started. This command overrides the container's default entrypoint.\n- : (Optional.) The arguments to be passed when    starting the container. The maximum length for all arguments combined is 100,000    characters.\n- If your training application is a Python package that runs in a prebuilt container,   specify the following:- : The URI of the container image that runs the    provided Python package. Refer to the [available prebuilt containers for    training](/vertex-ai/docs/training/pre-built-containers) .\n- : The Cloud Storage location of the Python    package files which are the training program and its dependent packages. The maximum    number of package URIs is 100.\n- : The Python module name to run after installing the packages.\n- : (Optional.) Command-line arguments to be passed to    the Python module. The maximum length for all arguments combined is 100,000    characters.\n- : (Optional.) The maximum running time for the job.\n- : A display name for the model uploaded (created)  by the TrainingPipeline.\n- : A description for the model.\n- : The URI of the container image to use for running predictions. For example,`us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-1:latest`. Use [prebuilt containers](/vertex-ai/docs/predictions/pre-built-containers) or [custom containers](/vertex-ai/docs/predictions/use-custom-container) .\n- : Any set of key-value pairs to organize your  models. For example:- \"env\": \"prod\"\n- \"tier\": \"backend\"\n- Specify theandfor any labels that you want to  apply to this training pipeline.\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/trainingPipelines\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"TRAINING_PIPELINE_NAME\",\n \"inputDataConfig\": {\n \"datasetId\": DATASET_ID,\n \"annotationsFilter\": ANNOTATIONS_FILTER,\n \"annotationSchemaUri\": ANNOTATION_SCHEMA_URI,\n // Union field split can be only one of the following:\n \"fractionSplit\": {\n  \"trainingFraction\": TRAINING_FRACTION,\n  \"validationFraction\": VALIDATION_FRACTION,\n  \"testFraction\": TEST_FRACTION\n },\n \"filterSplit\": {\n  \"trainingFilter\": TRAINING_FILTER,\n  \"validationFilter\": VALIDATION_FILTER,\n  \"testFilter\": TEST_FILTER\n },\n \"predefinedSplit\": {\n  \"key\": PREDEFINED_SPLIT_KEY\n },\n \"timestampSplit\": {\n  \"trainingFraction\": TIMESTAMP_TRAINING_FRACTION,\n  \"validationFraction\": TIMESTAMP_VALIDATION_FRACTION,\n  \"testFraction\": TIMESTAMP_TEST_FRACTION,\n  \"key\": TIMESTAMP_SPLIT_KEY\n }\n // End of list of possible types for union field split.\n \"gcsDestination\": {\n  \"outputUriPrefix\": OUTPUT_URI_PREFIX\n }\n },\n \"trainingTaskDefinition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/custom_task_1.0.0.yaml\",\n \"trainingTaskInputs\": {\n \"workerPoolSpecs\": [  {\n   \"machineSpec\": {\n   \"machineType\": MACHINE_TYPE,\n   \"acceleratorType\": ACCELERATOR_TYPE,\n   \"acceleratorCount\": ACCELERATOR_COUNT\n   },\n   \"replicaCount\": REPLICA_COUNT,\n   // Union field task can be only one of the following:\n   \"containerSpec\": {\n   \"imageUri\": CUSTOM_CONTAINER_IMAGE_URI,\n   \"command\": [    CUSTOM_CONTAINER_COMMAND\n   ],\n   \"args\": [    CUSTOM_CONTAINER_ARGS\n   ]\n   },\n   \"pythonPackageSpec\": {\n   \"executorImageUri\": PYTHON_PACKAGE_EXECUTOR_IMAGE_URI,\n   \"packageUris\": [    PYTHON_PACKAGE_URIS\n   ],\n   \"pythonModule\": PYTHON_MODULE,\n   \"args\": [    PYTHON_PACKAGE_ARGS\n   ]\n   }\n   // End of list of possible types for union field task.\n  }\n  ],\n  \"scheduling\": {\n  \"TIMEOUT\": TIMEOUT\n  }\n }\n },\n \"modelToUpload\": {\n \"displayName\": \"MODEL_NAME\",\n \"predictSchemata\": {},\n \"containerSpec\": {\n  \"imageUri\": \"IMAGE_URI\"\n }\n },\n \"labels\": {\n LABEL_NAME_1\": LABEL_VALUE_1,\n LABEL_NAME_2\": LABEL_VALUE_2\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/trainingPipelines\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/trainingPipelines\" | Select-Object -Expand Content\n```\nThe response contains information about specifications as well as the .Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateTrainingPipelineCustomJobSample.java) \n```\nimport com.google.cloud.aiplatform.v1.LocationName;import com.google.cloud.aiplatform.v1.Model;import com.google.cloud.aiplatform.v1.ModelContainerSpec;import com.google.cloud.aiplatform.v1.PipelineServiceClient;import com.google.cloud.aiplatform.v1.PipelineServiceSettings;import com.google.cloud.aiplatform.v1.TrainingPipeline;import com.google.gson.JsonArray;import com.google.gson.JsonObject;import com.google.protobuf.Value;import com.google.protobuf.util.JsonFormat;import java.io.IOException;public class CreateTrainingPipelineCustomJobSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"PROJECT\";\u00a0 \u00a0 String displayName = \"DISPLAY_NAME\";\u00a0 \u00a0 String modelDisplayName = \"MODEL_DISPLAY_NAME\";\u00a0 \u00a0 String containerImageUri = \"CONTAINER_IMAGE_URI\";\u00a0 \u00a0 String baseOutputDirectoryPrefix = \"BASE_OUTPUT_DIRECTORY_PREFIX\";\u00a0 \u00a0 createTrainingPipelineCustomJobSample(\u00a0 \u00a0 \u00a0 \u00a0 project, displayName, modelDisplayName, containerImageUri, baseOutputDirectoryPrefix);\u00a0 }\u00a0 static void createTrainingPipelineCustomJobSample(\u00a0 \u00a0 \u00a0 String project,\u00a0 \u00a0 \u00a0 String displayName,\u00a0 \u00a0 \u00a0 String modelDisplayName,\u00a0 \u00a0 \u00a0 String containerImageUri,\u00a0 \u00a0 \u00a0 String baseOutputDirectoryPrefix)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 PipelineServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 PipelineServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (PipelineServiceClient client = PipelineServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 JsonObject jsonMachineSpec = new JsonObject();\u00a0 \u00a0 \u00a0 jsonMachineSpec.addProperty(\"machineType\", \"n1-standard-4\");\u00a0 \u00a0 \u00a0 // A working docker image can be found at\u00a0 \u00a0 \u00a0 // gs://cloud-samples-data/ai-platform/mnist_tfrecord/custom_job\u00a0 \u00a0 \u00a0 // This sample image accepts a set of arguments including model_dir.\u00a0 \u00a0 \u00a0 JsonObject jsonContainerSpec = new JsonObject();\u00a0 \u00a0 \u00a0 jsonContainerSpec.addProperty(\"imageUri\", containerImageUri);\u00a0 \u00a0 \u00a0 JsonArray jsonArgs = new JsonArray();\u00a0 \u00a0 \u00a0 jsonArgs.add(\"--model_dir=$(AIP_MODEL_DIR)\");\u00a0 \u00a0 \u00a0 jsonContainerSpec.add(\"args\", jsonArgs);\u00a0 \u00a0 \u00a0 JsonObject jsonJsonWorkerPoolSpec0 = new JsonObject();\u00a0 \u00a0 \u00a0 jsonJsonWorkerPoolSpec0.addProperty(\"replicaCount\", 1);\u00a0 \u00a0 \u00a0 jsonJsonWorkerPoolSpec0.add(\"machineSpec\", jsonMachineSpec);\u00a0 \u00a0 \u00a0 jsonJsonWorkerPoolSpec0.add(\"containerSpec\", jsonContainerSpec);\u00a0 \u00a0 \u00a0 JsonArray jsonWorkerPoolSpecs = new JsonArray();\u00a0 \u00a0 \u00a0 jsonWorkerPoolSpecs.add(jsonJsonWorkerPoolSpec0);\u00a0 \u00a0 \u00a0 JsonObject jsonBaseOutputDirectory = new JsonObject();\u00a0 \u00a0 \u00a0 // The GCS location for outputs must be accessible by the project's AI Platform\u00a0 \u00a0 \u00a0 // service account.\u00a0 \u00a0 \u00a0 jsonBaseOutputDirectory.addProperty(\"output_uri_prefix\", baseOutputDirectoryPrefix);\u00a0 \u00a0 \u00a0 JsonObject jsonTrainingTaskInputs = new JsonObject();\u00a0 \u00a0 \u00a0 jsonTrainingTaskInputs.add(\"workerPoolSpecs\", jsonWorkerPoolSpecs);\u00a0 \u00a0 \u00a0 jsonTrainingTaskInputs.add(\"baseOutputDirectory\", jsonBaseOutputDirectory);\u00a0 \u00a0 \u00a0 Value.Builder trainingTaskInputsBuilder = Value.newBuilder();\u00a0 \u00a0 \u00a0 JsonFormat.parser().merge(jsonTrainingTaskInputs.toString(), trainingTaskInputsBuilder);\u00a0 \u00a0 \u00a0 Value trainingTaskInputs = trainingTaskInputsBuilder.build();\u00a0 \u00a0 \u00a0 String trainingTaskDefinition =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gs://google-cloud-aiplatform/schema/trainingjob/definition/custom_task_1.0.0.yaml\";\u00a0 \u00a0 \u00a0 String imageUri = \"gcr.io/cloud-aiplatform/prediction/tf-cpu.1-15:latest\";\u00a0 \u00a0 \u00a0 ModelContainerSpec containerSpec =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ModelContainerSpec.newBuilder().setImageUri(imageUri).build();\u00a0 \u00a0 \u00a0 Model modelToUpload =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Model.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(modelDisplayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setContainerSpec(containerSpec)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 TrainingPipeline trainingPipeline =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TrainingPipeline.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(displayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingTaskDefinition(trainingTaskDefinition)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingTaskInputs(trainingTaskInputs)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModelToUpload(modelToUpload)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 LocationName parent = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 TrainingPipeline response = client.createTrainingPipeline(parent, trainingPipeline);\u00a0 \u00a0 \u00a0 System.out.format(\"response: %s\\n\", response);\u00a0 \u00a0 \u00a0 System.out.format(\"Name: %s\\n\", response.getName());\u00a0 \u00a0 }\u00a0 }}\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\nThe following examples show how to use the [Vertex AI SDK for Python](/vertex-ai/docs/start/client-libraries) to create a custom training pipeline. Choose whether you plan to use a [customcontainer](/vertex-ai/docs/training/create-custom-container) or a [prebuiltcontainer](/vertex-ai/docs/training/pre-built-containers) for training:\nWhen you use the Vertex AI SDK for Python to create a training pipeline that runs your Python code in a prebuilt container, you can provide your training code in one of the following ways:- Specify the [URI of a Python source distribution package inCloud Storage.](/vertex-ai/docs/training/create-python-pre-built-container) (This option is also available when you create a training pipeline without using the Vertex AI SDK for Python.)\n- Specify the path to a Python script on your local machine. Before it creates a training pipeline, the Vertex AI SDK for Python packages your script as a source distribution and uploads it to the Cloud Storage bucket of your choice.(This option is only available when you use the Vertex AI SDK for Python.)\nTo see a code sample for each of these options, select the corresponding tab:\nThe following sample uses the [CustomPythonPackageTrainingJobclass](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomPythonPackageTrainingJob) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_training_pipeline_custom_package_job_sample.py) \n```\ndef create_training_pipeline_custom_package_job_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 staging_bucket: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 python_package_gcs_uri: str,\u00a0 \u00a0 python_module_name: str,\u00a0 \u00a0 container_uri: str,\u00a0 \u00a0 model_serving_container_image_uri: str,\u00a0 \u00a0 dataset_id: Optional[str] = None,\u00a0 \u00a0 model_display_name: Optional[str] = None,\u00a0 \u00a0 args: Optional[List[Union[str, float, int]]] = None,\u00a0 \u00a0 replica_count: int = 1,\u00a0 \u00a0 machine_type: str = \"n1-standard-4\",\u00a0 \u00a0 accelerator_type: str = \"ACCELERATOR_TYPE_UNSPECIFIED\",\u00a0 \u00a0 accelerator_count: int = 0,\u00a0 \u00a0 training_fraction_split: float = 0.8,\u00a0 \u00a0 validation_fraction_split: float = 0.1,\u00a0 \u00a0 test_fraction_split: float = 0.1,\u00a0 \u00a0 sync: bool = True,\u00a0 \u00a0 tensorboard_resource_name: Optional[str] = None,\u00a0 \u00a0 service_account: Optional[str] = None,):\u00a0 \u00a0 aiplatform.init(project=project, location=location, staging_bucket=staging_bucket)\u00a0 \u00a0 job = aiplatform.CustomPythonPackageTrainingJob(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name,\u00a0 \u00a0 \u00a0 \u00a0 python_package_gcs_uri=python_package_gcs_uri,\u00a0 \u00a0 \u00a0 \u00a0 python_module_name=python_module_name,\u00a0 \u00a0 \u00a0 \u00a0 container_uri=container_uri,\u00a0 \u00a0 \u00a0 \u00a0 model_serving_container_image_uri=model_serving_container_image_uri,\u00a0 \u00a0 )\u00a0 \u00a0 # This example uses an ImageDataset, but you can use another type\u00a0 \u00a0 dataset = aiplatform.ImageDataset(dataset_id) if dataset_id else None\u00a0 \u00a0 model = job.run(\u00a0 \u00a0 \u00a0 \u00a0 dataset=dataset,\u00a0 \u00a0 \u00a0 \u00a0 model_display_name=model_display_name,\u00a0 \u00a0 \u00a0 \u00a0 args=args,\u00a0 \u00a0 \u00a0 \u00a0 replica_count=replica_count,\u00a0 \u00a0 \u00a0 \u00a0 machine_type=machine_type,\u00a0 \u00a0 \u00a0 \u00a0 accelerator_type=accelerator_type,\u00a0 \u00a0 \u00a0 \u00a0 accelerator_count=accelerator_count,\u00a0 \u00a0 \u00a0 \u00a0 training_fraction_split=training_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 validation_fraction_split=validation_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 test_fraction_split=test_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 \u00a0 \u00a0 tensorboard=tensorboard_resource_name,\u00a0 \u00a0 \u00a0 \u00a0 service_account=service_account,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 print(model.uri)\u00a0 \u00a0 return model\n```\nThe following sample uses the [CustomTrainingJobclass](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomTrainingJob) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_training_pipeline_custom_job_sample.py) \n```\ndef create_training_pipeline_custom_job_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 staging_bucket: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 script_path: str,\u00a0 \u00a0 container_uri: str,\u00a0 \u00a0 model_serving_container_image_uri: str,\u00a0 \u00a0 dataset_id: Optional[str] = None,\u00a0 \u00a0 model_display_name: Optional[str] = None,\u00a0 \u00a0 args: Optional[List[Union[str, float, int]]] = None,\u00a0 \u00a0 replica_count: int = 0,\u00a0 \u00a0 machine_type: str = \"n1-standard-4\",\u00a0 \u00a0 accelerator_type: str = \"ACCELERATOR_TYPE_UNSPECIFIED\",\u00a0 \u00a0 accelerator_count: int = 0,\u00a0 \u00a0 training_fraction_split: float = 0.8,\u00a0 \u00a0 validation_fraction_split: float = 0.1,\u00a0 \u00a0 test_fraction_split: float = 0.1,\u00a0 \u00a0 sync: bool = True,\u00a0 \u00a0 tensorboard_resource_name: Optional[str] = None,\u00a0 \u00a0 service_account: Optional[str] = None,):\u00a0 \u00a0 aiplatform.init(project=project, location=location, staging_bucket=staging_bucket)\u00a0 \u00a0 job = aiplatform.CustomTrainingJob(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name,\u00a0 \u00a0 \u00a0 \u00a0 script_path=script_path,\u00a0 \u00a0 \u00a0 \u00a0 container_uri=container_uri,\u00a0 \u00a0 \u00a0 \u00a0 model_serving_container_image_uri=model_serving_container_image_uri,\u00a0 \u00a0 )\u00a0 \u00a0 # This example uses an ImageDataset, but you can use another type\u00a0 \u00a0 dataset = aiplatform.ImageDataset(dataset_id) if dataset_id else None\u00a0 \u00a0 model = job.run(\u00a0 \u00a0 \u00a0 \u00a0 dataset=dataset,\u00a0 \u00a0 \u00a0 \u00a0 model_display_name=model_display_name,\u00a0 \u00a0 \u00a0 \u00a0 args=args,\u00a0 \u00a0 \u00a0 \u00a0 replica_count=replica_count,\u00a0 \u00a0 \u00a0 \u00a0 machine_type=machine_type,\u00a0 \u00a0 \u00a0 \u00a0 accelerator_type=accelerator_type,\u00a0 \u00a0 \u00a0 \u00a0 accelerator_count=accelerator_count,\u00a0 \u00a0 \u00a0 \u00a0 training_fraction_split=training_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 validation_fraction_split=validation_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 test_fraction_split=test_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 \u00a0 \u00a0 tensorboard=tensorboard_resource_name,\u00a0 \u00a0 \u00a0 \u00a0 service_account=service_account,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 print(model.uri)\u00a0 \u00a0 return model\n```\nThe following sample uses the [CustomContainerTrainingJobclass](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomContainerTrainingJob) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_training_pipeline_custom_container_job_sample.py) \n```\ndef create_training_pipeline_custom_container_job_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 staging_bucket: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 container_uri: str,\u00a0 \u00a0 model_serving_container_image_uri: str,\u00a0 \u00a0 dataset_id: Optional[str] = None,\u00a0 \u00a0 model_display_name: Optional[str] = None,\u00a0 \u00a0 args: Optional[List[Union[str, float, int]]] = None,\u00a0 \u00a0 replica_count: int = 1,\u00a0 \u00a0 machine_type: str = \"n1-standard-4\",\u00a0 \u00a0 accelerator_type: str = \"ACCELERATOR_TYPE_UNSPECIFIED\",\u00a0 \u00a0 accelerator_count: int = 0,\u00a0 \u00a0 training_fraction_split: float = 0.8,\u00a0 \u00a0 validation_fraction_split: float = 0.1,\u00a0 \u00a0 test_fraction_split: float = 0.1,\u00a0 \u00a0 sync: bool = True,\u00a0 \u00a0 tensorboard_resource_name: Optional[str] = None,\u00a0 \u00a0 service_account: Optional[str] = None,):\u00a0 \u00a0 aiplatform.init(project=project, location=location, staging_bucket=staging_bucket)\u00a0 \u00a0 job = aiplatform.CustomContainerTrainingJob(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name,\u00a0 \u00a0 \u00a0 \u00a0 container_uri=container_uri,\u00a0 \u00a0 \u00a0 \u00a0 model_serving_container_image_uri=model_serving_container_image_uri,\u00a0 \u00a0 )\u00a0 \u00a0 # This example uses an ImageDataset, but you can use another type\u00a0 \u00a0 dataset = aiplatform.ImageDataset(dataset_id) if dataset_id else None\u00a0 \u00a0 model = job.run(\u00a0 \u00a0 \u00a0 \u00a0 dataset=dataset,\u00a0 \u00a0 \u00a0 \u00a0 model_display_name=model_display_name,\u00a0 \u00a0 \u00a0 \u00a0 args=args,\u00a0 \u00a0 \u00a0 \u00a0 replica_count=replica_count,\u00a0 \u00a0 \u00a0 \u00a0 machine_type=machine_type,\u00a0 \u00a0 \u00a0 \u00a0 accelerator_type=accelerator_type,\u00a0 \u00a0 \u00a0 \u00a0 accelerator_count=accelerator_count,\u00a0 \u00a0 \u00a0 \u00a0 training_fraction_split=training_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 validation_fraction_split=validation_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 test_fraction_split=test_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 \u00a0 \u00a0 tensorboard=tensorboard_resource_name,\u00a0 \u00a0 \u00a0 \u00a0 service_account=service_account,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 print(model.uri)\u00a0 \u00a0 return model\n```\n## Hyperparameter tuning job and model upload\nThis training pipeline encapsulates a hyperparameter tuning job with an added convenience step that makes it easier to deploy your model to Vertex AI after training. This training pipeline does two main things:\n- The training pipeline creates a resource. The hyperparameter tuning job creates multiple trials. For each trial, a custom job runs your training application using the computing resources and hyperparameters that you specify.\n- After the hyperparameter tuning job completes, the training pipeline finds the model artifacts from the best trial, within the output directory ( [baseOutputDirectory](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#FIELDS.base_output_directory) ) you specified for your Cloud Storage bucket. The training pipeline uses these artifacts to create a resource, which sets you up for [model deployment](/vertex-ai/docs/predictions/deploy-model-api) .\nFor this training pipeline, you must specify a [baseOutputDirectory](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#FIELDS.base_output_directory) where Vertex AI searches for the model artifacts from the best trial.\n[Hyperparameter tuning jobs](/vertex-ai/docs/training/using-hyperparameter-tuning) have additional settings to configure. Learn more about the settings for a [HyperparameterTuningJob](/vertex-ai/docs/reference/rest/v1/projects.locations.hyperparameterTuningJobs) .\nUse the following code sample to create a training pipeline using the [create method of the trainingPipeline resource](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines/create) .\nBefore using any of the request data, make the following replacements:- : Your project's region.\n- : Your project ID.\n- : Required. A display name for the trainingPipeline.\n- If your training application uses a Vertex AI dataset, specify the following:- : The ID of the dataset.\n- : Filters the dataset by the annotations that you   specify.\n- : Filters the dataset by the specified annotation   schema URI.\n- Use one of the following options to specify how data items are split into training,   validation, and test sets.- To split the dataset based on fractions defining the size of each set, specify    the following:- : The fraction of the dataset to use to train your     model.\n- : The fraction of the dataset to use to validate     your model.\n- : The fraction of the dataset to use to evaluate     your model.\n- To split the dataset based on filters, specify the following:- : Filters the dataset to data items to use for training     your model.\n- : Filters the dataset to data items to use for     validating your model.\n- : Filters the dataset to data items to use for evaluating     your model.\n- To use a predefined split, specify the following:- : The name of the column to use to split     the dataset. Acceptable values in this column include `training`, `validation`,     and `test`.\n- To split the dataset based on the timestamp on the dataitems, specify the following:- : The fraction of the dataset to use to     train your model.\n- : The fraction of the dataset to use to     validate your model.\n- : The fraction of the dataset to use to evaluate     your model.\n- : The name of the timestamp column to use to split     the dataset.\n- : The Cloud Storage location where Vertex AI   exports your training dataset, after it has been split into training, validation, and test   sets.\n- Specify your hyperparameter tuning job:- Specify your metrics:- : The name of this metric.\n- : The goal of this metric. Can be`MAXIMIZE`or`MINIMIZE`.- End of metrics list item\n- Specify your hyperparameters:- : The name of this hyperparameter.\n- : (Optional.) How the parameter should be scaled. Leave unset    for CATEGORICAL parameters. Can be`UNIT_LINEAR_SCALE`,`UNIT_LOG_SCALE`,`UNIT_REVERSE_LOG_SCALE`, or`SCALE_TYPE_UNSPECIFIED`\n- If this hyperparameter's type is DOUBLE, specify the minimum ()    and maximum () values for this hyperparameter.\n- If this hyperparameter's type is INTEGER, specify the minimum    () and maximum () values for    this hyperparameter.\n- If this hyperparameter's type is CATEGORICAL, specify the acceptable values    () as an array of strings.\n- If this hyperparameter's type is DISCRETE, specify the acceptable values    () as an array of numbers.- End of hyperparameters list item\n- : (Optional.) The search algorithm to use in this hyperparameter tuning   job. Can be`ALGORITHM_UNSPECIFIED`,`GRID_SEARCH`, or`RANDOM_SEARCH`.\n- : The maximum number of trials to run in this job.\n- : The maximum number of trials that can run in parallel.\n- : The number of jobs that can fail before the hyperparameter  tuning job fails.\n- Define the trial custom training job:- : The type of the machine. Refer to the [available machine types for training](/vertex-ai/docs/training/configure-compute) .\n- : (Optional.) The type of accelerator to attach to each    trial.\n- : (Optional.) The number of accelerators to attach to    each trial.\n- : The number of worker replicas to use for each trial.\n- If your training application runs in a custom container, specify the following:- : The URI of a container image in     Artifact Registry, Container Registry, or Docker Hub that is to be run on each worker     replica.\n- : (Optional.) The command to be invoked when the     container is started. This command overrides the container's default entrypoint.\n- : (Optional.) The arguments to be passed when     starting the container.- End of custom containers list item\n- If your training application is a Python package that runs in a prebuilt container,    specify the following:- : The URI of the container image that runs the    provided Python package. Refer to the [available prebuilt containers for    training](/vertex-ai/docs/training/pre-built-containers) .\n- : The Cloud Storage location of the Python     package files which are the training program and its dependent packages. The maximum     number of package URIs is 100.\n- : The Python module name to run after installing the     packages.\n- : (Optional.) Command-line arguments to be passed to     the Python module.- End of Python package list item- End of trial custom training job list item\n- Learn about [job scheduling options](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#Scheduling) .\n- : (Optional.) The maximum running time for each trial.\n- Specify theandfor any labels that you want   to apply to this hyperparameter tuning job.- End of hyperparameter tuning job list item\n- : A display name for the model uploaded (created)  by the TrainingPipeline.\n- : Optional. A description for the model.\n- : Required. Specify one of the two following options:- The image URI of the [prebuilt container to use  for prediction](/vertex-ai/docs/predictions/pre-built-containers) , such as \"tf2-cpu.2-1:latest\".\n- The image URI of your own [custom container   to use for prediction](/vertex-ai/docs/predictions/use-custom-container) .\n- : Optional. Any set of key-value pairs to  organize your models. For example:- \"env\": \"prod\"\n- \"tier\": \"backend\"\n- Specify theandfor any labels that you want to  apply to this training pipeline.\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/trainingPipelines\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"TRAINING_PIPELINE_NAME\",\n \"inputDataConfig\": {\n \"datasetId\": DATASET_ID,\n \"annotationsFilter\": ANNOTATIONS_FILTER,\n \"annotationSchemaUri\": ANNOTATION_SCHEMA_URI,\n // Union field split can be only one of the following:\n \"fractionSplit\": {\n  \"trainingFraction\": TRAINING_FRACTION,\n  \"validationFraction\": VALIDATION_FRACTION,\n  \"testFraction\": TEST_FRACTION\n },\n \"filterSplit\": {\n  \"trainingFilter\": TRAINING_FILTER,\n  \"validationFilter\": VALIDATION_FILTER,\n  \"testFilter\": TEST_FILTER\n },\n \"predefinedSplit\": {\n  \"key\": PREDEFINED_SPLIT_KEY\n },\n \"timestampSplit\": {\n  \"trainingFraction\": TIMESTAMP_TRAINING_FRACTION,\n  \"validationFraction\": TIMESTAMP_VALIDATION_FRACTION,\n  \"testFraction\": TIMESTAMP_TEST_FRACTION,\n  \"key\": TIMESTAMP_SPLIT_KEY\n }\n // End of list of possible types for union field split.\n \"gcsDestination\": {\n  \"outputUriPrefix\": OUTPUT_URI_PREFIX\n }\n },\n \"trainingTaskDefinition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/hyperparameter_tuning_task_1.0.0.yaml\",\n \"trainingTaskInputs\": {\n \"studySpec\": {\n \"metrics\": [  {\n  \"metricId\": METRIC_ID,\n  \"goal\": METRIC_GOAL\n  }\n ],\n \"parameters\": [  {\n  \"parameterId\": PARAMETER_ID,\n  \"scaleType\": PARAMETER_SCALE,\n  // Union field parameter_value_spec can be only one of the following:\n  \"doubleValueSpec\": {\n   \"minValue\": DOUBLE_MIN_VALUE,\n   \"maxValue\": DOUBLE_MAX_VALUE\n  },\n  \"integerValueSpec\": {\n   \"minValue\": INTEGER_MIN_VALUE,\n   \"maxValue\": INTEGER_MAX_VALUE\n  },\n  \"categoricalValueSpec\": {\n   \"values\": [    CATEGORICAL_VALUES\n   ]\n  },\n  \"discreteValueSpec\": {\n   \"values\": [    DISCRETE_VALUES\n   ]\n  }\n  // End of list of possible types for union field parameter_value_spec.\n  }\n ],\n \"ALGORITHM\": ALGORITHM\n },\n \"maxTrialCount\": MAX_TRIAL_COUNT,\n \"parallelTrialCount\": PARALLEL_TRIAL_COUNT,\n \"maxFailedTrialCount\": MAX_FAILED_TRIAL_COUNT,\n \"trialJobSpec\": {\n  \"workerPoolSpecs\": [  {\n   \"machineSpec\": {\n   \"machineType\": MACHINE_TYPE,\n   \"acceleratorType\": ACCELERATOR_TYPE,\n   \"acceleratorCount\": ACCELERATOR_COUNT\n   },\n   \"replicaCount\": REPLICA_COUNT,\n   // Union field task can be only one of the following:\n   \"containerSpec\": {\n   \"imageUri\": CUSTOM_CONTAINER_IMAGE_URI,\n   \"command\": [    CUSTOM_CONTAINER_COMMAND\n   ],\n   \"args\": [    CUSTOM_CONTAINER_ARGS\n   ]\n   },\n   \"pythonPackageSpec\": {\n   \"executorImageUri\": PYTHON_PACKAGE_EXECUTOR_IMAGE_URI,\n   \"packageUris\": [    PYTHON_PACKAGE_URIS\n   ],\n   \"pythonModule\": PYTHON_MODULE,\n   \"args\": [    PYTHON_PACKAGE_ARGS\n   ]\n   }\n   // End of list of possible types for union field task.\n  }\n  ],\n  \"scheduling\": {\n  \"TIMEOUT\": TIMEOUT\n  }\n },\n \"labels\": {\n  LABEL_NAME_1\": LABEL_VALUE_1,\n  LABEL_NAME_2\": LABEL_VALUE_2\n }\n },\n \"modelToUpload\": {\n \"displayName\": \"MODEL_NAME\",\n \"description\": \"MODEL_DESCRIPTION\",\n \"predictSchemata\": {},\n \"containerSpec\": {\n  \"imageUri\": \"PREDICTION_IMAGE_URI\"\n }\n },\n \"labels\": {\n LABEL_NAME_1\": LABEL_VALUE_1,\n LABEL_NAME_2\": LABEL_VALUE_2\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/trainingPipelines\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/trainingPipelines\" | Select-Object -Expand Content\n```\nThe response contains information about specifications as well as the .\n## Monitor training\nTo view training logs, do the following:\n- In the Google Cloud console, in the Vertex AI section, go to the **Training** page. [Go to the Training page](https://console.cloud.google.com/vertex-ai/training-pipelines) \n- Click the name of your job to go to the custom job page.\n- Click **View logs** .\nYou can also [use an interactiveshell](/vertex-ai/docs/training/monitor-debug-interactive-shell) to inspect your training containers while the training pipeline is running.\n## View your trained model\nWhen the custom training pipeline completes, you can find the trained model in the Google Cloud console, in the Vertex AI section, on the **Models** page.\n[Go to the Models page](https://console.cloud.google.com/vertex-ai/models)\n## What's next\n- Learn how to pinpoint training performance bottlenecks to train models faster and cheaper using [TensorBoard Profiler](/vertex-ai/docs/training/tensorboard-profiler) .\n- [Deploy your model to an endpoint](/vertex-ai/docs/predictions/deploy-model-api) .\n- Create a [hyperparameter tuning job](/vertex-ai/docs/training/using-hyperparameter-tuning) .", "guide": "Vertex AI"}