{"title": "Vertex AI - Online prediction logging", "url": "https://cloud.google.com/vertex-ai/docs/predictions/online-prediction-logging", "abstract": "# Vertex AI - Online prediction logging\nFor AutoML tabular models, AutoML image models, and custom-trained models, you can enable or disable prediction logs during [model deployment](/vertex-ai/docs/predictions/deploy-model-api) or [endpointcreation](/vertex-ai/docs/predictions/deploy-model-api#create-endpoint) . This page explains the different types of prediction logs available, and how to enable or disable these logs.\n", "content": "## Types of prediction logs\nThere are several types of prediction logs that you can use to get information from your prediction nodes:\n- **Container logging** , which logs the `stdout` and `stderr` streams from your prediction nodes to [Cloud Logging](/logging/docs/overview) . These logs are required for debugging.- On the `v1` service endpoint, container logging is enabled by default. You can disable it when you deploy a model. You can also disable or enable logging when you [mutate](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/mutateDeployedModel) the deployed model.\n- On the `v1beta1` service endpoint, container logging is disabled by default. You can enable it when you deploy a model. You can also disable or enable logging when you [mutate](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/mutateDeployedModel) the deployed model.\n **Note:** The default logging behavior in Python sends outputs to `stderr` , which will appear at the `ERROR` level in Cloud Logging. If you'd like for container logs to appear at the `INFO` level, configure your container logging to send outputs to `stdout` .\n- **Access logging** , which logs information like timestamp and latency for each request to Cloud Logging.On both the `v1` and `v1beta1` service endpoints, access logging is disabled by default. You can enable access logging when you deploy a model to an endpoint. **Note:** If you have [VPC Service Controls](/vpc-service-controls/docs/overview) enabled for your project, access logging is not supported.\n- **Request-response logging** , which logs a sample of online prediction requests and responses to a BigQuery table.You can enable request-response logging by creating or patching the prediction endpoint.\nYou can enable or disable each type of log independently.\n## Prediction log settings\nYou can enable or disable online prediction logs when you create an endpoint, deploy a model to the endpoint, or mutate a deployed model.\nTo update the settings for access logs, you must [undeploy your model](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/undeployModel) , and then redeploy the model with your new settings. You can update the settings for container logs without re-deploying your model.\nOnline prediction at a high rate of queries per second (QPS) can produce a substantial number of logs, which are subject to [Cloud Logging pricing](/stackdriver/pricing#logging-costs) . To estimate the pricing for your online prediction logs, see [Estimating your bills](/stackdriver/estimating-bills) for logging. To reduce this cost, you can disable prediction logging.\n### Enable and disable prediction logs\nThe following examples highlight where to modify the default log settings:\nWhen you deploy a model to an endpoint or create a new endpoint in the Google Cloud console, you can specify which types of prediction logs to enable in the **Logging** step. Select the checkboxes to enable **Access logging** or **Container logging** , or clear the checkboxes to disable these logs.\nUse the REST API to update the settings for container logs.\nUse the REST API to enable request-response logging. The Google Cloud console and gcloud CLI don't support request-response logging configuration.\nTo see more context about how to deploy models, read [Deploy a model using the Google Cloud console](/vertex-ai/docs/predictions/deploy-model-console) .To change the [default behavior](#defaults) for which logs are enabled in deployed models, add flags to your `gcloud` command:\nRun [gcloud ai endpoints deploy-model](/sdk/gcloud/reference/ai/endpoints/deploy-model) :\n```\ngcloud ai endpoints deploy-model ENDPOINT_ID\\\u00a0 --region=LOCATION \\\u00a0 --model=MODEL_ID \\\u00a0 --display-name=DEPLOYED_MODEL_NAME \\\u00a0 --machine-type=MACHINE_TYPE \\\u00a0 --accelerator=count=2,type=nvidia-tesla-t4 \\\u00a0 --disable-container-logging \\\u00a0 --enable-access-logging\n```\nRun [gcloud beta ai endpoints deploy-model](/sdk/gcloud/reference/beta/ai/endpoints/deploy-model) :\n```\ngcloud beta ai endpoints deploy-model ENDPOINT_ID\\\u00a0 --region=LOCATION \\\u00a0 --model=MODEL_ID \\\u00a0 --display-name=DEPLOYED_MODEL_NAME \\\u00a0 --machine-type=MACHINE_TYPE \\\u00a0 --accelerator=count=2,type=nvidia-tesla-t4 \\\u00a0 --enable-access-logging \\\u00a0 --enable-container-logging\n```Use the REST API to update the settings for container logs.\nUse the REST API to enable request-response logging. The Google Cloud console and gcloud CLI don't support request-response logging configuration.\nTo see more context about how to deploy models, read [Deploy a model using the Vertex AI API](/vertex-ai/docs/predictions/deploy-model-api) .To change the [default behavior](#defaults) for which logs are enabled in deployed models, set the relevant fields to `True` :\nTo disable **container logging** , set the `disableContainerLogging` field to `True` when you call either [projects.locations.endpoints.deployModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/deployModel) or [projects.locations.endpoints.mutateDeployedModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/mutateDeployedModel) .\nTo enable **access logging** , set `enableAccessLogging` to `True` when deploying your model with [projects.locations.endpoints.deployModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/deployModel) .\nTo enable **container logging** , set the `enableContainerLogging` field to `True` when you call either [projects.locations.endpoints.deployModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/deployModel) or [projects.locations.endpoints.mutateDeployedModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/mutateDeployedModel) .\nTo enable **access logging** , set `enableAccessLogging` to `True` when deploying your model with [projects.locations.endpoints.deployModel](/vertex-ai/docs/reference/rest/v1beta1/projects.locations.endpoints/deployModel) .To see more context about how to deploy models, read [Deploy a model using the Vertex AI API](/vertex-ai/docs/predictions/deploy-model-api) .\n **Request-response logging** \nYou can only enable request-response logging when you send a [create anendpoint](/vertex-ai/docs/predictions/deploy-model-api#create-endpoint) using [projects.locations.endpoints.create](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/create) or patch an existing endpoint using [projects.locations.endpoints.patch](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/patch) .\nRequest-response logging is done at the endpoint level, so requests sent to any deployed models under the same endpoint are logged.\nWhen you create or patch an endpoint, populate the `predictRequestResponseLoggingConfig` field of the [Endpoint resource](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints) with the following entries:- `enabled` : set as `True` to enable request-response logging.\n- `samplingPercentage` : a number between 0 or 1 defining the fraction of requests to log. For example, set this value to `1` in order to log all requests or to `0.1` to log 10% of requests.\n- [BigQueryDestination](/vertex-ai/docs/reference/rest/v1/BigQueryDestination) : the BigQuery table to be used for logging. If you only specify a project name, a new dataset is created with the name `logging_` `` `_` `` , where `` follows the [BigQuery naming rules](/bigquery/docs/datasets#dataset-naming) . If you don't specify a table name, a new table is created with the name `request_response_logging` .The schema for the BigQuery table should look like the following:| Field name  | Type  | Mode  |\n|:------------------|:----------|:---------|\n| endpoint   | STRING | NULLABLE |\n| deployed_model_id | STRING | NULLABLE |\n| logging_time  | TIMESTAMP | NULLABLE |\n| request_id  | NUMERIC | NULLABLE |\n| request_payload | STRING | REPEATED |\n| response_payload | STRING | REPEATED |\nThe following is an example configuration:\n```\n{\n \"predict_request_response_logging_config\": {\n  \"enabled\": true,\n  \"sampling_rate\": 0.5,\n  \"bigquery_destination\": {\n  \"output_uri\": \"bq://PROJECT_ID.DATASET_NAME.TABLE_NAME\"\n  }\n}\n```\n## Request-response logging and Model Monitoring\nRequest-response logging and [Model Monitoring](/vertex-ai/docs/model-monitoring/overview) use the same BigQuery table on the backend to log incoming requests. To prevent unexpected changes to this BigQuery table, the following limitations are enforced when using both features at the same time:\n- If an endpoint has Model Monitoring enabled, you can't enable request-response logging for the same endpoint.\n- If you enable request-response logging and then Model Monitoring on the same endpoint, you won't be able to change the request-response logging configuration.## What's next\n- [Estimate pricing](/stackdriver/estimating-bills) for online prediction logging.\n- Deploy a model [using the Google Cloud console](/vertex-ai/docs/predictions/deploy-model-console) or [using the Vertex AI API](/vertex-ai/docs/predictions/deploy-model-api) .\n- Learn [how to create a BigQuery table](/bigquery/docs/tables) .", "guide": "Vertex AI"}