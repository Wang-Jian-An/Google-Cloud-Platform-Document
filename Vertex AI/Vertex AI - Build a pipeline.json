{"title": "Vertex AI - Build a pipeline", "url": "https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline", "abstract": "# Vertex AI - Build a pipeline\nTo learn more,  run the \"Learn how to use control structures in a Kubeflow pipeline\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/control_flow_kfp.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fpipelines%2Fcontrol_flow_kfp.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/control_flow_kfp.ipynb)\nVertex AI Pipelines lets you orchestrate your machine learning (ML) workflows in a serverless manner. Before Vertex AI Pipelines can orchestrate your ML workflow, you must describe your workflow as a pipeline. ML pipelines are portable and scalable ML workflows that are based on containers and Google Cloud services.\nThis guide describes how to get started building ML pipelines.\n", "content": "## Which pipelines SDK should I use?\nVertex AI Pipelines can run pipelines built using any of the following SDKs:\n- Kubeflow Pipelines SDK v1.8 or later (v2 is recommended) **Note:** Install Kubeflow Pipelines SDK v2 to use the code samples provided in the Vertex AI Pipelines documentation. These code samples might not run successfully if you install Kubeflow Pipelines SDK v1.8.\n- TensorFlow Extended v0.30.0 or later\nIf you use TensorFlow in an ML workflow that processes terabytes of structured data or text data, we recommend that you build your pipeline using TFX.\n- To learn more about building a TFX pipeline, [follow theTFX getting started tutorials](https://www.tensorflow.org/tfx/tutorials#getting-started-tutorials) .\n- To learn more about using Vertex AI Pipelines to run a TFX pipeline, [follow the TFX onGoogle Cloud tutorials](https://www.tensorflow.org/tfx/tutorials#tfx-on-google-cloud) .\nFor other use cases, we recommend that you build your pipeline using the Kubeflow Pipelines SDK. By building a pipeline with the Kubeflow Pipelines SDK, you can implement your workflow by building custom components or reusing prebuilt components, such as the [Google Cloud Pipeline Components](/vertex-ai/docs/pipelines/components-introduction) . Google Cloud Pipeline Components make it easier to use Vertex AI services like AutoML in your pipeline.\nThis guide describes how to build pipelines using the Kubeflow Pipelines SDK.\n## Before you begin\nBefore you build and run your pipelines, use the following instructions to set up your Google Cloud project and development environment.\n- To get your Google Cloud project ready to run ML pipelines, follow the instructions in the guide to [configuring yourGoogle Cloud project](/vertex-ai/docs/pipelines/configure-project) .\n- To build your pipeline using the Kubeflow Pipelines SDK, [install theKubeflow Pipelines SDK v1.8 or later](https://www.kubeflow.org/docs/components/pipelines/sdk/install-sdk/) .\n- To use Vertex AI Python client in your pipelines, [install theVertex AI client libraries v1.7 or later](https://github.com/googleapis/python-aiplatform) .\n- To use Vertex AI services in your pipelines, [install theGoogle Cloud Pipeline Components SDK](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud#installation) .## Getting started building a pipeline\nTo orchestrate your ML workflow on Vertex AI Pipelines, you must first describe your workflow as a pipeline. The following sample demonstrates how to use the [Google Cloud Pipeline Components](/vertex-ai/docs/pipelines/components-introduction) with Vertex AI to create a dataset, train a model using AutoML, and deploy the trained model for predictions.\nBefore you run the following code sample, you must set up authentication.\n**How to set up authentication**\nTo set up authentication, you must create a service account key, and set an environment variable for the path to the service account key.\n- Create a service account:- In the Google Cloud console, go to the **Create service account** page. [Go to Create service account](https://console.cloud.google.com/projectselector/iam-admin/serviceaccounts/create?supportedpurview=project) \n- In the **Service account name** field, enter a name.\n- Optional: In the **Service account description** field, enter a description.\n- Click **Create** .\n- Click the **Select a role** field. Under **All roles** , select **Vertex AI** > **Vertex AI User** .\n- **Note** : The roles you select allow your service account to access resources. You can   view and change these roles later by using the [Google Cloud console](https://console.cloud.google.com/) . For more information, see [access control for   Vertex AI](/vertex-ai/docs/general/access-control#predefined_roles) .\n- Click **Done** to create the service account.Do not close your browser window. You will use it in the next step.\n- Create a service account key for authentication:- In the Google Cloud console, click the email address for the service account that you   created.\n- Click **Keys** .\n- Click **Add key** , then **Create new key** .\n- Click **Create** . A JSON key file is downloaded to your computer.\n- Click **Close** .\n- Grant your new service account access to the service account that you use to run pipelines.- Clickarrow_backto return to the   list of service accounts.\n- Click the name of the service account that you use to run pipelines. The **Service account details** page appears.If you followed the instructions in the guide to configuring your project for   Vertex AI Pipelines, this is the same service account that you created in the [Configure a service account   with granular permissions](/vertex-ai/docs/pipelines/configure-project#service-account) section. Otherwise, Vertex AI uses the   Compute Engine default service account to run pipelines. The Compute Engine default   service account is named like the following: `` `-compute@developer.gserviceaccount.com`\n- Click the **Permissions** tab.\n- Click **Grant access** . The **Add principals** panel   appears.\n- In the **New principals** box, enter the email address for the service   account you created in a previous step.\n- In the **Role** drop-down list, select **Service accounts** > **Service account user** .\n- Click **Save** \n- Set the environment variable to the path of the JSON file that contains your service account key.   This variable only applies to your current shell session, so if you open   a new session, set the variable again. **Example:** Linux or macOSReplace with the path of the JSON file that    contains your service account key.```\nexport GOOGLE_APPLICATION_CREDENTIALS=\"[PATH]\"\n```For example:```\nexport GOOGLE_APPLICATION_CREDENTIALS=\"/home/user/Downloads/service-account-file.json\"\n``` **Example:** WindowsReplace with the path of the JSON file that    contains your service account key, and with the    filename.With PowerShell:```\n$env:GOOGLE_APPLICATION_CREDENTIALS=\"[PATH]\"\n```For example:```\n$env:GOOGLE_APPLICATION_CREDENTIALS=\"C:\\Users\\username\\Downloads\\[FILE_NAME].json\"\n```With command prompt:```\nset GOOGLE_APPLICATION_CREDENTIALS=[PATH]\n```\n### Define your workflow using Kubeflow Pipelines DSL package\nThe `kfp.dsl` package contains the domain-specific language (DSL) that you can use to define and interact with pipelines and components.\nKubeflow pipeline components are factory functions that create pipeline steps. Each component describes the inputs, outputs, and implementation of the component. For example, in the code sample below, `ds_op` is a component.\nComponents are used to create pipeline steps. When a pipeline runs, steps are executed as the data they depend on becomes available. For example, a training component could take a CSV file as an input and use it to train a model.\n```\nimport kfpfrom google.cloud import aiplatformfrom google_cloud_pipeline_components.v1.dataset import ImageDatasetCreateOpfrom google_cloud_pipeline_components.v1.automl.training_job import AutoMLImageTrainingJobRunOpfrom google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOpproject_id = PROJECT_IDpipeline_root_path = PIPELINE_ROOT# Define the workflow of the pipeline.@kfp.dsl.pipeline(\u00a0 \u00a0 name=\"automl-image-training-v2\",\u00a0 \u00a0 pipeline_root=pipeline_root_path)def pipeline(project_id: str):\u00a0 \u00a0 # The first step of your workflow is a dataset generator.\u00a0 \u00a0 # This step takes a Google Cloud Pipeline Component, providing the necessary\u00a0 \u00a0 # input arguments, and uses the Python variable `ds_op` to define its\u00a0 \u00a0 # output. Note that here the `ds_op` only stores the definition of the\u00a0 \u00a0 # output but not the actual returned object from the execution. The value\u00a0 \u00a0 # of the object is not accessible at the dsl.pipeline level, and can only be\u00a0 \u00a0 # retrieved by providing it as the input to a downstream component.\u00a0 \u00a0 ds_op = ImageDatasetCreateOp(\u00a0 \u00a0 \u00a0 \u00a0 project=project_id,\u00a0 \u00a0 \u00a0 \u00a0 display_name=\"flowers\",\u00a0 \u00a0 \u00a0 \u00a0 gcs_source=\"gs://cloud-samples-data/vision/automl_classification/flowers/all_data_v2.csv\",\u00a0 \u00a0 \u00a0 \u00a0 import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\u00a0 \u00a0 )\u00a0 \u00a0 # The second step is a model training component. It takes the dataset\u00a0 \u00a0 # outputted from the first step, supplies it as an input argument to the\u00a0 \u00a0 # component (see `dataset=ds_op.outputs[\"dataset\"]`), and will put its\u00a0 \u00a0 # outputs into `training_job_run_op`.\u00a0 \u00a0 training_job_run_op = AutoMLImageTrainingJobRunOp(\u00a0 \u00a0 \u00a0 \u00a0 project=project_id,\u00a0 \u00a0 \u00a0 \u00a0 display_name=\"train-iris-automl-mbsdk-1\",\u00a0 \u00a0 \u00a0 \u00a0 prediction_type=\"classification\",\u00a0 \u00a0 \u00a0 \u00a0 model_type=\"CLOUD\",\u00a0 \u00a0 \u00a0 \u00a0 dataset=ds_op.outputs[\"dataset\"],\u00a0 \u00a0 \u00a0 \u00a0 model_display_name=\"iris-classification-model-mbsdk\",\u00a0 \u00a0 \u00a0 \u00a0 training_fraction_split=0.6,\u00a0 \u00a0 \u00a0 \u00a0 validation_fraction_split=0.2,\u00a0 \u00a0 \u00a0 \u00a0 test_fraction_split=0.2,\u00a0 \u00a0 \u00a0 \u00a0 budget_milli_node_hours=8000,\u00a0 \u00a0 )\u00a0 \u00a0 # The third and fourth step are for deploying the model.\u00a0 \u00a0 create_endpoint_op = EndpointCreateOp(\u00a0 \u00a0 \u00a0 \u00a0 project=project_id,\u00a0 \u00a0 \u00a0 \u00a0 display_name = \"create-endpoint\",\u00a0 \u00a0 )\u00a0 \u00a0 model_deploy_op = ModelDeployOp(\u00a0 \u00a0 \u00a0 \u00a0 model=training_job_run_op.outputs[\"model\"],\u00a0 \u00a0 \u00a0 \u00a0 endpoint=create_endpoint_op.outputs['endpoint'],\u00a0 \u00a0 \u00a0 \u00a0 automatic_resources_min_replica_count=1,\u00a0 \u00a0 \u00a0 \u00a0 automatic_resources_max_replica_count=1,\u00a0 \u00a0 )\n```\nReplace the following:\n- : The Google Cloud project that this pipeline runs in.\n- : Specify a Cloud Storage URI that your [pipelines service account can access](/vertex-ai/docs/pipelines/configure-project#service-account) . The artifacts of your pipeline runs are stored within the pipeline root.The pipeline root can be set as an argument of the `@kfp.dsl.pipeline` annotation on the pipeline function, or it can be set when you call `create_run_from_job_spec` to create a pipeline run.\n### Compile your pipeline into a YAML file\nAfter the workflow of your pipeline is defined, you can proceed to compile the pipeline into YAML format. The YAML file includes all the information for executing your pipeline on Vertex AI Pipelines.\n```\nfrom kfp import compilercompiler.Compiler().compile(\u00a0 \u00a0 pipeline_func=pipeline,\u00a0 \u00a0 package_path='image_classif_pipeline.yaml')\n```\n### Submit your pipeline run\nAfter the workflow of your pipeline is compiled into the YAML format, you can use the Vertex AI Python client to submit and run your pipeline.\n```\nimport google.cloud.aiplatform as aip# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS# environment variable to the path of your service account.aip.init(\u00a0 \u00a0 project=project_id,\u00a0 \u00a0 location=PROJECT_REGION,)# Prepare the pipeline jobjob = aip.PipelineJob(\u00a0 \u00a0 display_name=\"automl-image-training-v2\",\u00a0 \u00a0 template_path=\"image_classif_pipeline.yaml\",\u00a0 \u00a0 pipeline_root=pipeline_root_path,\u00a0 \u00a0 parameter_values={\u00a0 \u00a0 \u00a0 \u00a0 'project_id': project_id\u00a0 \u00a0 })job.submit()\n```\nReplace the following:\n- : The region that this pipeline runs in.\nIn the preceding example:\n- A Kubeflow pipeline is defined as a Python function. The function is annotated with the`@kfp.dsl.pipeline`decorator, which specifies the pipeline's name and root path. The pipeline root path is the location where the pipeline's artifacts are stored.\n- The pipeline's workflow steps are created using the [Google Cloud Pipeline Components](/vertex-ai/docs/pipelines/components-introduction) . By using the outputs of a component as an input of another component, you define the pipeline's workflow as a graph. For example:`training_job_run_op`depends on the`dataset`output of`ds_op`.\n- You compile the pipeline using`kfp.compiler.Compiler`.\n- You create a pipeline run on Vertex AI Pipelines using the Vertex AI Python client. When you run a pipeline, you can override the pipeline name and the pipeline root path. Pipeline runs can be grouped using the pipeline name. Overriding the pipeline name can help you distinguish between production and experimental pipeline runs.\nTo learn more about building pipelines, read the [building Kubeflowpipelines](#build-pipeline) section, and [follow the samples andtutorials](/vertex-ai/docs/pipelines/notebooks#general-tutorials) .\n## Building Kubeflow pipelines\nUse the following process to build a pipeline.\n- Design your pipeline as a series of components. To promote reusability, each component should have a single responsibility. Whenever possible, design your pipeline to reuse proven components such as the [Google Cloud Pipeline Components](/vertex-ai/docs/pipelines/components-introduction) . [Learn more about designing pipelines](https://www.kubeflow.org/docs/components/pipelines/v2/hello-world/) .\n- Build any custom components that are required to implement your ML workflow using Kubeflow Pipelines SDK. Components are self-contained sets of code that perform a step in your ML workflow. Use the following options to create your pipeline components.- Package your component's code as a container image. This option lets you include code in your pipeline that was written in any language that can be packaged as a container image. [Learn how to build a Kubeflow pipelinecomponent](https://www.kubeflow.org/docs/components/pipelines/v2/components/) .\n- Implement your component's code as a standalone Python function and use the Kubeflow Pipelines SDK to package your function as a component. This option makes it easier to build Python-based components. [Learn how to build Python function-basedcomponents](https://www.kubeflow.org/docs/components/pipelines/v2/components/lightweight-python-components/) .\n- Build your pipeline as a Python function. [Learn more about defining your pipeline as a Pythonfunction](https://www.kubeflow.org/docs/components/pipelines/v2/pipelines/pipeline-basics/) .\n- Use the Kubeflow Pipelines SDK compiler to compile your pipeline.```\nfrom kfp import compilercompiler.Compiler().compile(\u00a0 \u00a0 pipeline_func=PIPELINE_FUNCTION,\u00a0 \u00a0 package_path=PIPELINE_PACKAGE_PATH)\n```Replace the following:- : The name of your pipeline's function.\n- : The path to where to store your compiled pipeline.\n- [Run your pipeline using Google Cloud console or Python](/vertex-ai/docs/pipelines/run-pipeline) .## Accessing Google Cloud resources in a pipeline\nIf you do not specify a service account when you run a pipeline, Vertex AI Pipelines uses the Compute Engine default service account to run your pipeline. Vertex AI Pipelines also uses a pipeline run's service account to authorize your pipeline to access Google Cloud resources. The Compute Engine default service account has the [Project Editor role](/iam/docs/understanding-roles#basic) by default. This may grant your pipelines excessive access to Google Cloud resources in your Google Cloud project.\nWe recommend that you [create a service account to run your pipelines andthen grant this account granular permissions to the Google Cloud resourcesthat are needed to run your pipeline](/vertex-ai/docs/pipelines/configure-project#service-account) .\nLearn more about using Identity and Access Management to [create a service account](/iam/docs/creating-managing-service-accounts) and [manage the access granted to a service account](/iam/docs/granting-changing-revoking-access) .\n## Keep your pipelines up-to-date\nThe SDK clients and container images that you use to build and run pipelines are periodically updated to new versions to patch security vulnerabilities and add new functionality. To keep your pipelines up to date with the latest version, we recommend that you do the following:\n- Review the [Vertex AI framework support policy](/vertex-ai/docs/framework-support-policy) and [Supported frameworks list](/vertex-ai/docs/supported-frameworks-list#pipelines) .\n- Subscribe to the [Vertex AI release notes](/vertex-ai/docs/release-notes) and the PyPi.org RSS feeds for SDKs you use ( [Kubeflow Pipelines SDK](https://www.kubeflow.org/docs/components/pipelines/sdk/install-sdk/) , [Google Cloud Pipeline Components SDK](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.11.0/api/v1/index.html) , or [TensorFlow Extended SDK](https://github.com/tensorflow/tfx) ) to stay aware of new releases.\n- If you have a pipeline template or definition that references a container with security vulnerabilities, you should do the following:- Install the latest patched version of the SDK.\n- Rebuild and recompile your pipeline template or definition.\n- Re-upload the template or definition to Artifact Registry or Cloud Storage.\n## What's next\n- Read the [introduction to Vertex AI Pipelines](/vertex-ai/docs/pipelines/introduction) to learn more about orchestrating ML workflows.\n- Learn how to [run a pipeline](/vertex-ai/docs/pipelines/run-pipeline) .\n- [Visualize and analyze the results of your pipelineruns](/vertex-ai/docs/pipelines/visualize-pipeline) .", "guide": "Vertex AI"}