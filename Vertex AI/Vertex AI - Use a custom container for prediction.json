{"title": "Vertex AI - Use a custom container for prediction", "url": "https://cloud.google.com/vertex-ai/docs/predictions/use-custom-container", "abstract": "# Vertex AI - Use a custom container for prediction\nTo customize how Vertex AI serves online predictions from your custom-trained model, you can specify a custom container instead of a [prebuiltcontainer](/vertex-ai/docs/predictions/pre-built-containers) when you create a `Model` resource. When you use a custom container, Vertex AI runs a Docker container of your choice on each prediction node.\nYou might want to use a custom container for any of the following reasons:\n- to serve predictions from an ML model trained using a framework that isn't available as a [prebuilt container](/vertex-ai/docs/predictions/pre-built-containers) \n- to preprocess prediction requests or postprocess the predictions generated by your model\n- to run a prediction server written in a programming language of your choice\n- to install dependencies that you want to use to customize prediction\nThis guide describes how to create a `Model` that uses a custom container. It doesn't provide detailed instructions about designing and creating a Docker container image.\n", "content": "## Prepare a container image\nTo create a `Model` that uses a custom container, you must provide a Docker container image as the basis of that container. This container image must meet the requirements described in [Custom containerrequirements](/vertex-ai/docs/predictions/custom-container-requirements) .\nIf you plan to use an existing container image created by a third party that you trust, then you might be able to skip one or both of the following sections.\n### Create a container image\nDesign and build a Docker container image that meets the [container imagerequirements](/vertex-ai/docs/predictions/custom-container-requirements#image) .\nTo learn the basics of designing and building a Docker container image, read the [Docker documentation'squickstart](https://docs.docker.com/get-started/)\n### Push the container image to Artifact Registry\nPush your container image to an [Artifact Registry](/artifact-registry/docs/overview) repository.\nLearn how to [push a container image toArtifact Registry](/artifact-registry/docs/docker/pushing-and-pulling) .\n## Create a Model\nTo create a `Model` that uses a custom container, do one of the following:\n- [Import a custom-trained model.](/vertex-ai/docs/model-registry/import-model) \n- [Create a TrainingPipeline that runs a CustomJob and imports the resultingartifacts as aModel.](/vertex-ai/docs/training/create-training-pipeline#custom-job-model-upload) \nThe follow sections show how to configure the API fields related to custom containers when you create a `Model` in one of these ways.\n### Container-related API fields\nWhen you create the `Model` , make sure to configure the [containerSpecfield](/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model.FIELDS.container_spec) with your custom container details, rather than with a [prebuiltcontainer](/vertex-ai/docs/predictions/pre-built-containers) .\nYou must specify a [ModelContainerSpecmessage](/vertex-ai/docs/reference/rest/v1/projects.locations.models#modelcontainerspec) in the `Model.containerSpec` field. Within this message, you can specify the following subfields:\nIn addition to the variables that you set in the `Model.containerSpec.env` field, Vertex AI sets several other variables based on your configuration. Learn more about [using these environment variables in thesefields and in the container's entrypointcommand](/vertex-ai/docs/predictions/custom-container-requirements#variables) .\n### Model import examples\nThe following examples show how to specify container-related API fields when you import a model.\n**Note:** If you use a `TrainingPipeline` to create a `Model` , then you must specify these fields in a different context. You can't use the Google Cloud CLI to create a `TrainingPipeline` . [Read more about creating custom TrainingPipelineresources.](/vertex-ai/docs/training/create-training-pipeline)\nThe following example uses the [gcloud ai models uploadcommand](/sdk/gcloud/reference/ai/models/upload) :\n```\ngcloud ai models upload \\\u00a0 --region=LOCATION \\\u00a0 --display-name=MODEL_NAME \\\u00a0 --container-image-uri=IMAGE_URI \\\u00a0 --container-command=COMMAND \\\u00a0 --container-args=ARGS \\\u00a0 --container-ports=PORTS \\\u00a0 --container-env-vars=ENV \\\u00a0 --container-health-route=HEALTH_ROUTE \\\u00a0 --container-predict-route=PREDICT_ROUTE \\\u00a0 --container-shared-memory-size-mb=SHARED_MEMORY_SIZE \\\u00a0 --container-startup-probe-exec=STARTUP_PROBE_EXEC \\\u00a0 --container-startup-probe-period-seconds=STARTUP_PROBE_PERIOD \\\u00a0 --container-startup-probe-timeout-seconds=STARTUP_PROBE_TIMEOUT \\\u00a0 --container-health-probe-exec=HEALTH_PROBE_EXEC \\\u00a0 --container-health-probe-period-seconds=HEALTH_PROBE_PERIOD \\\u00a0 --container-health-probe-timeout-seconds=HEALTH_PROBE_TIMEOUT \\\u00a0 --artifact-uri=PATH_TO_MODEL_ARTIFACT_DIRECTORY\n```\nThe `--container-image-uri` flag is required; all other flags that begin with `--container-` are optional. To learn about the values for these fields, see the [preceding section of this guide](#fields) .Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/UploadModelSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.LocationName;import com.google.cloud.aiplatform.v1.Model;import com.google.cloud.aiplatform.v1.ModelContainerSpec;import com.google.cloud.aiplatform.v1.ModelServiceClient;import com.google.cloud.aiplatform.v1.ModelServiceSettings;import com.google.cloud.aiplatform.v1.UploadModelOperationMetadata;import com.google.cloud.aiplatform.v1.UploadModelResponse;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class UploadModelSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws InterruptedException, ExecutionException, TimeoutException, IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String modelDisplayName = \"YOUR_MODEL_DISPLAY_NAME\";\u00a0 \u00a0 String metadataSchemaUri =\u00a0 \u00a0 \u00a0 \u00a0 \"gs://google-cloud-aiplatform/schema/trainingjob/definition/custom_task_1.0.0.yaml\";\u00a0 \u00a0 String imageUri = \"YOUR_IMAGE_URI\";\u00a0 \u00a0 String artifactUri = \"gs://your-gcs-bucket/artifact_path\";\u00a0 \u00a0 uploadModel(project, modelDisplayName, metadataSchemaUri, imageUri, artifactUri);\u00a0 }\u00a0 static void uploadModel(\u00a0 \u00a0 \u00a0 String project,\u00a0 \u00a0 \u00a0 String modelDisplayName,\u00a0 \u00a0 \u00a0 String metadataSchemaUri,\u00a0 \u00a0 \u00a0 String imageUri,\u00a0 \u00a0 \u00a0 String artifactUri)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 ModelServiceSettings modelServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 ModelServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (ModelServiceClient modelServiceClient = ModelServiceClient.create(modelServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 LocationName locationName = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 ModelContainerSpec modelContainerSpec =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ModelContainerSpec.newBuilder().setImageUri(imageUri).build();\u00a0 \u00a0 \u00a0 Model model =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Model.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(modelDisplayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMetadataSchemaUri(metadataSchemaUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setArtifactUri(artifactUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setContainerSpec(modelContainerSpec)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 OperationFuture<UploadModelResponse, UploadModelOperationMetadata> uploadModelResponseFuture =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelServiceClient.uploadModelAsync(locationName, model);\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Operation name: %s\\n\", uploadModelResponseFuture.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to finish...\");\u00a0 \u00a0 \u00a0 UploadModelResponse uploadModelResponse = uploadModelResponseFuture.get(5, TimeUnit.MINUTES);\u00a0 \u00a0 \u00a0 System.out.println(\"Upload Model Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"Model: %s\\n\", uploadModelResponse.getModel());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/upload-model.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0*/// const modelDisplayName = 'YOUR_MODEL_DISPLAY_NAME';// const metadataSchemaUri = 'YOUR_METADATA_SCHEMA_URI';// const imageUri = 'YOUR_IMAGE_URI';// const artifactUri = 'YOUR_ARTIFACT_URI';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Model Service Client libraryconst {ModelServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst modelServiceClient = new ModelServiceClient(clientOptions);async function uploadModel() {\u00a0 // Configure the parent resources\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 // Configure the model resources\u00a0 const model = {\u00a0 \u00a0 displayName: modelDisplayName,\u00a0 \u00a0 metadataSchemaUri: '',\u00a0 \u00a0 artifactUri: artifactUri,\u00a0 \u00a0 containerSpec: {\u00a0 \u00a0 \u00a0 imageUri: imageUri,\u00a0 \u00a0 \u00a0 command: [],\u00a0 \u00a0 \u00a0 args: [],\u00a0 \u00a0 \u00a0 env: [],\u00a0 \u00a0 \u00a0 ports: [],\u00a0 \u00a0 \u00a0 predictRoute: '',\u00a0 \u00a0 \u00a0 healthRoute: '',\u00a0 \u00a0 },\u00a0 };\u00a0 const request = {\u00a0 \u00a0 parent,\u00a0 \u00a0 model,\u00a0 };\u00a0 console.log('PARENT AND MODEL');\u00a0 console.log(parent, model);\u00a0 // Upload Model request\u00a0 const [response] = await modelServiceClient.uploadModel(request);\u00a0 console.log(`Long running operation : ${response.name}`);\u00a0 // Wait for operation to complete\u00a0 await response.promise();\u00a0 const result = response.result;\u00a0 console.log('Upload model response ');\u00a0 console.log(`\\tModel : ${result.model}`);}uploadModel();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/upload_model_sample.py) \n```\ndef upload_model_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 serving_container_image_uri: str,\u00a0 \u00a0 artifact_uri: Optional[str] = None,\u00a0 \u00a0 serving_container_predict_route: Optional[str] = None,\u00a0 \u00a0 serving_container_health_route: Optional[str] = None,\u00a0 \u00a0 description: Optional[str] = None,\u00a0 \u00a0 serving_container_command: Optional[Sequence[str]] = None,\u00a0 \u00a0 serving_container_args: Optional[Sequence[str]] = None,\u00a0 \u00a0 serving_container_environment_variables: Optional[Dict[str, str]] = None,\u00a0 \u00a0 serving_container_ports: Optional[Sequence[int]] = None,\u00a0 \u00a0 instance_schema_uri: Optional[str] = None,\u00a0 \u00a0 parameters_schema_uri: Optional[str] = None,\u00a0 \u00a0 prediction_schema_uri: Optional[str] = None,\u00a0 \u00a0 explanation_metadata: Optional[explain.ExplanationMetadata] = None,\u00a0 \u00a0 explanation_parameters: Optional[explain.ExplanationParameters] = None,\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 model = aiplatform.Model.upload(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name,\u00a0 \u00a0 \u00a0 \u00a0 artifact_uri=artifact_uri,\u00a0 \u00a0 \u00a0 \u00a0 serving_container_image_uri=serving_container_image_uri,\u00a0 \u00a0 \u00a0 \u00a0 serving_container_predict_route=serving_container_predict_route,\u00a0 \u00a0 \u00a0 \u00a0 serving_container_health_route=serving_container_health_route,\u00a0 \u00a0 \u00a0 \u00a0 instance_schema_uri=instance_schema_uri,\u00a0 \u00a0 \u00a0 \u00a0 parameters_schema_uri=parameters_schema_uri,\u00a0 \u00a0 \u00a0 \u00a0 prediction_schema_uri=prediction_schema_uri,\u00a0 \u00a0 \u00a0 \u00a0 description=description,\u00a0 \u00a0 \u00a0 \u00a0 serving_container_command=serving_container_command,\u00a0 \u00a0 \u00a0 \u00a0 serving_container_args=serving_container_args,\u00a0 \u00a0 \u00a0 \u00a0 serving_container_environment_variables=serving_container_environment_variables,\u00a0 \u00a0 \u00a0 \u00a0 serving_container_ports=serving_container_ports,\u00a0 \u00a0 \u00a0 \u00a0 explanation_metadata=explanation_metadata,\u00a0 \u00a0 \u00a0 \u00a0 explanation_parameters=explanation_parameters,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 return model\n```\nFor more context, read the [Model import guide](/vertex-ai/docs/model-registry/import-model) .\n## Send prediction requests\nTo send an online prediction request to your `Model` , follow the instructions at [Get predictions from a custom trained model](/vertex-ai/docs/predictions/get-online-predictions) : this process works the same regardless of whether you use a custom container.\nRead about [predict request and response requirements for customcontainers](/vertex-ai/docs/predictions/custom-container-requirements#prediction) .\n## What's next\n- To learn about everything to consider when you design a custom container to use with Vertex AI, read [Custom containerrequirements](/vertex-ai/docs/predictions/custom-container-requirements) .", "guide": "Vertex AI"}