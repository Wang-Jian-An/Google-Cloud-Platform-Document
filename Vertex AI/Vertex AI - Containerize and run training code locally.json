{"title": "Vertex AI - Containerize and run training code locally", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Containerize and run training code locally\nYou can use the [gcloud ai custom-jobs local-run command](/sdk/gcloud/reference/ai/custom-jobs/local-run) to build a Docker container image based on your training code and run the image as a container on your local computer. This feature offers several benefits:\n- You can build a container image with minimal knowledge of Docker. You don't need to write your own Dockerfile. You can later push this image to [Artifact Registry](/artifact-registry/docs/docker) and use it for [custom containertraining](/vertex-ai/docs/training/containers-overview) .For advanced use cases, you might still want to [write your ownDockerfile](/vertex-ai/docs/training/create-custom-container) .\n- Your container image can run a Python training application or a Bash script.You can use a Bash script to run training code written in another programming language (as long as you also specify a base container image that supports the other language).\n- Running a container locally executes your training code in a similar way to how it runs on Vertex AI.Running your code locally can help you debug problems with your code before you perform custom training on Vertex AI.", "content": "## Before you begin\n- [Set up your Vertex AI developmentenvironment.](/vertex-ai/docs/start/cloud-environment) \n- [Install Docker Engine.](https://docs.docker.com/engine/install/) \n- If you are using Linux, [configure Docker so you can run it withoutsudo](https://docs.docker.com/engine/install/linux-postinstall/#manage-docker-as-a-non-root-user) .The `local-run` command requires this configuration in order to use Docker.## Use the local-run command\nRun the following command to build a container image based on your training code and run a container locally:\n```\ngcloud ai custom-jobs local-run \\\u00a0 --executor-image-uri=BASE_IMAGE_URI \\\u00a0 --local-package-path=WORKING_DIRECTORY \\\u00a0 --script=SCRIPT_PATH \\\u00a0 --output-image-uri=OUTPUT_IMAGE_NAME\n```\nReplace the following:\n- : The URI of a Docker image to use as the base of the container. Choose a base image that includes dependencies required for your training code.You can use the URI to a [prebuilt trainingcontainer](/vertex-ai/docs/training/pre-built-containers) image or any other value that would be valid for a [Dockerfile FROMinstruction](https://docs.docker.com/engine/reference/builder/#from) ; for example, a publicly available Docker image or a Docker image in [Artifact Registry](/artifact-registry/docs/docker) that you have access to.\n- : The lowest-level directory in your file system that contains all your training code and local dependencies that you need to use for training.By default, the command only copies the parent directory of the file specified by the `--script` flag (see the following list item) into the resulting Docker image. The Docker image does necessarily include all the files within ; to customize which files get included, see the section in this document about [including dependencies](#dependencies) .If you omit the `--local-package-path` flag (and this placeholder), then the `local-run` command uses the current working directory for this value.\n- : The path, relative to on your local file system, to the script that is the entry point for your training code. This can be a Python script (ending in `.py` ) or a Bash script.For example, if you want to run `/hello-world/trainer/task.py` and is `/hello-world` , then use `trainer/task.py` for this value.If you specify a Python script, then your base image must have Python installed, and if you specify a bash script, then your base image must have Bash installed. (All prebuilt training containers and many other publicly available Docker images include both of these dependencies.)If you omit the `--script` flag (and ), then you must instead use the `--python-module` flag to specify the name of a Python module in to run as the entry point for training. For example, instead of `--script=trainer/task.py` , you might specify `--python-module=trainer.task` .In this case, the resulting Docker container [loads your code as a module](https://docs.python.org/3/using/cmdline.html#cmdoption-m) rather than as a script. You likely want to use this option if your entry point script imports other Python modules in .\n- : A name for the resulting Docker image built by the command. You can use any value that is accepted by [docker build's -tflag](https://docs.docker.com/engine/reference/commandline/build/#tag-an-image--t) .If you plan to later push the image to Artifact Registry, then you might want to use [an image name that meets the Artifact Registryrequirements](/artifact-registry/docs/docker/pushing-and-pulling#tag) . If you plan to later push the image to Container Registry, then you might want to use [an image name that meets the Container Registryrequirements](/container-registry/docs/pushing-and-pulling#tag) . (Alternatively, you can tag the image with additional names later).If you omit the `--output-image-uri` flag (and this placeholder), then the `local-run` command tags the image with a name based on the current time and the filename of your entry point script.\nThe command builds a Docker container image based on your configuration. After building the image, the command prints the following output:\n```\nA training image is built.\nStarting to run ...\n```\nThe command then immediately uses this container image to run a container on your local computer. When the container exits, the command prints the following output:\n```\nA local run is finished successfully using custom image: OUTPUT_IMAGE_NAME\n```\n## Additional options\nThe following sections describe additional options that you can use to customize the behavior of the `local-run` command.\n### Install dependencies\nYour training code can rely on any dependencies installed on your base image (for example, [prebuilt training container](/vertex-ai/docs/training/pre-built-containers) images include many Python libraries for machine learning), as well as any files that you include in the Docker image created by the `local-run` command.\nWhen you specify a script with the `--script` flag or the `--python-module` flag, the command copies the script's parent directory (and its subdirectories) into the Docker image. For example, if you specify `--local-package-path=/hello-world` and `--script=trainer/task.py` , then the command copies `/hello-world/trainer/` into the Docker image.\nYou can also include additional Python dependencies or arbitrary files from your file system by completing the extra steps described in one of the following sections:\n- [Using a requirements.txt file for Python dependencies](#requirements) \n- [Using a setup.py file for Python dependencies](#setup) \n- [Specifying individual PyPI dependencies](#pypi) \n- [Specifying local Python dependencies](#local-dependencies) \n- [Including other files](#other-files) You can include additional Python dependencies in the Docker image in several ways:\nIf there is a file named `requirements.txt` in the working directory, then the `local-run` command treats this as a [pip requirementsfile](https://pip.pypa.io/en/stable/user_guide/#requirements-files) and uses it to install Python dependencies in the Docker image.\nIf there is a file named `setup.py` in the working directory, then the `local-run` command treats this as a [Python setup.pyfile](https://packaging.python.org/guides/distributing-packages-using-setuptools/#setup-py) , copies the file to the Docker image, and runs `pip install` on the directory in the Docker image that contains this file.\nYou can, for example, add an [install_requiresargument](https://packaging.python.org/guides/distributing-packages-using-setuptools/#install-requires) to `setup.py` in order to install Python dependencies in the Docker image.\nYou can use the `--requirements` flag to install specific dependencies from [PyPI](https://pypi.org/) in the Docker image. For example:\n```\ngcloud ai custom-jobs local-run \\\u00a0 --executor-image-uri=BASE_IMAGE_URI \\\u00a0 --local-package-path=WORKING_DIRECTORY \\\u00a0 --script=SCRIPT_PATH \\\u00a0 --output-image-uri=OUTPUT_IMAGE_NAME \\\u00a0 --requirements=REQUIREMENTS\n```\nReplace with a comma-separated list of [Pythonrequirementspecifiers](https://pip.pypa.io/en/stable/cli/pip_install/#requirement-specifiers) .\nYou can use the `--extra-packages` flag to install specific local Python dependencies. These Python dependencies must be under the working directory, and each dependency must be in a format that [pip install](https://pip.pypa.io/en/stable/cli/pip_install) supports; for example, a [wheelfile](https://pip.pypa.io/en/stable/user_guide/#installing-from-wheels) or a [Python sourcedistribution](https://packaging.python.org/en/latest/overview/#python-source-distributions) .\nFor example:\n```\ngcloud ai custom-jobs local-run \\\u00a0 --executor-image-uri=BASE_IMAGE_URI \\\u00a0 --local-package-path=WORKING_DIRECTORY \\\u00a0 --script=SCRIPT_PATH \\\u00a0 --output-image-uri=OUTPUT_IMAGE_NAME \\\u00a0 --extra-packages=LOCAL_DEPENDENCIES\n```\nReplace with a comma-separated list of local file paths, expressed relative to the working directory.\nTo copy additional directories to the Docker image (without installing them as Python dependencies), you can use the `--extra-dirs` flag. You may only specify directories under the working directory. For example:\n```\ngcloud ai custom-jobs local-run \\\u00a0 --executor-image-uri=BASE_IMAGE_URI \\\u00a0 --local-package-path=WORKING_DIRECTORY \\\u00a0 --script=SCRIPT_PATH \\\u00a0 --output-image-uri=OUTPUT_IMAGE_NAME \\\u00a0 --extra-dirs=EXTRA_DIRECTORIES\n```\nReplace with a comma-separated list of local directories, expressed relative to the working directory.\n### Training application arguments\nIf the entry point script for your training application expects command-line arguments, you can specify these when you run the `local-run` command. These arguments do not get saved in the Docker image; rathern they get passed as arguments when the image runs as a container.\nTo pass arguments to your entry point script, pass the `--` argument followed by your script's arguments to the `local-run` command after all the command's other flags.\nFor example, imagine a script that you run locally with the following command:\n```\npython /hello-world/trainer/task.py \\\u00a0 --learning_rate=0.1 \\\u00a0 --input_data=gs://BUCKET/small-dataset/\n```\nWhen you use the `local-run` command, you can use the following flags to run the script in the container with the same arguments:\n```\ngcloud ai custom-jobs local-run \\\\\u00a0 --executor-image-uri=BASE_IMAGE_URI \\\u00a0 --local-package-path=/hello-world \\\u00a0 --script=/trainer/task.py \\\u00a0 --output-image-uri=OUTPUT_IMAGE_NAME \\\u00a0 -- \\\u00a0 --learning_rate=0.1 \\\u00a0 --input_data=gs://BUCKET/small-dataset/\n```\n### Accelerate model training with GPUs\nIf you want to eventually deploy the Docker image created by the `local-run` command to Vertex AI and [use GPUs fortraining](/vertex-ai/docs/training/configure-compute#specifying_gpus) , then make sure to [write training code that takes advantage ofGPUs](/vertex-ai/docs/training/code-requirements#gpus) and use a GPU-enabled Docker image for the value of the `--executor-image-uri` flag. For example, you can use one of the [prebuilt trainingcontainer](/vertex-ai/docs/training/pre-built-containers) images that supports GPUs.\nIf your local computer runs Linux and has GPUs, you can also configure the `local-run` command to use your GPUs when it runs a container locally. This is optional, but it can be useful if you want to test how your training code works with GPUs. Do the following:\n- [Install the NVIDIA Container Toolkit(nvidia-docker)](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#setting-up-nvidia-container-toolkit) on your local computer, if you haven't already.\n- Specify the `--gpu` flag when you run the `local-run` command. For example:```\ngcloud ai custom-jobs local-run \\\u00a0 --executor-image-uri=BASE_IMAGE_URI \\\u00a0 --local-package-path=WORKING_DIRECTORY \\\u00a0 --script=SCRIPT_PATH \\\u00a0 --output-image-uri=OUTPUT_IMAGE_NAME \\\u00a0 --gpu\n```\n### Specify a custom service account\nBy default, when the `local-run` command runs your training code in a local container, it mounts the Google Cloud credentials available in your local environment through [Application Default Credentials(ADC)](/docs/authentication#adc) into the container, so that your training code can use ADC for authentication with the same credentials. In other words, the credentials available by ADC in your local shell are also available by ADC to your code when you run the `local-run` command.\nYou can use the [gcloud auth application-default logincommand](/sdk/gcloud/reference/auth/application-default/login) to use your user account for ADC, or you can [set an environment variable in your shell to use aservice account for ADC](/docs/authentication/production#passing_variable) .\nIf you want the container to run with Google Cloud credentials other than those available by ADC in your local shell, do the following:\n- [Create or select a serviceaccount](/iam/docs/creating-managing-service-accounts) with the permissions you want your training code to have access to.\n- [Download a service accountkey](/iam/docs/creating-managing-service-account-keys) for this service account to your local computer.\n- When you run the `local-run` command, specify the `--service-account-key-file` flag. For example:```\ngcloud ai custom-jobs local-run \\\u00a0 --executor-image-uri=BASE_IMAGE_URI \\\u00a0 --local-package-path=WORKING_DIRECTORY \\\u00a0 --script=SCRIPT_PATH \\\u00a0 --output-image-uri=OUTPUT_IMAGE_NAME \\\u00a0 --service-account-key-file=KEY_PATH\n```Replace with the path to the service account key in your local file system. This must be absolute or relative to the current working directory of your shell, relative to the directory specified by the `--local-package-path` flag.\nIn the resulting container, your training code can use ADC to authenticate with the specified Service Account Credentials.\nWhen you perform custom training on Vertex AI, Vertex AI uses the [Vertex AI Custom Code Service Agent for yourproject](/vertex-ai/docs/general/access-control#service-agents) by default to run your code. You can also [attach a different serviceaccount](/vertex-ai/docs/general/custom-service-account) for custom training.\nWhen you use the `local-run` command, you can't authenticate as the Vertex AI Custom Code Service Agent, but you can create a service account with similar permissions and use it locally.\n## What's next\n- Learn about [requirements for your trainingcode](/vertex-ai/docs/training/code-requirements) .\n- Learn how to [push your Docker image toArtifact Registry](/artifact-registry/docs/docker/pushing-and-pulling) and [use it as a custom container for training on Vertex AI](/vertex-ai/docs/training/configure-container-settings) .", "guide": "Vertex AI"}