{"title": "Vertex AI - Use Google Cloud Pipeline Components", "url": "https://cloud.google.com/vertex-ai/docs/pipelines/use-components", "abstract": "# Vertex AI - Use Google Cloud Pipeline Components\nWhen you use Google Cloud Pipeline Components (GCPC), you can use the following Vertex AI and Google Cloud features to secure your components and artifacts.\n", "content": "## Specify a service account for a component\nWhen you use a component, you can optionally specify a service account. Your component launches and acts with the permissions of this service account. For example, you can use the following code to specify the service account of a `ModelDeploy` component:\n```\nmodel_deploy_op = ModelDeployOp(model=training_job_run_op.outputs[\"model\"],\u00a0 \u00a0 endpoint=endpoint_op.outputs[\"endpoint\"],\u00a0 \u00a0 automatic_resources_min_replica_count=1,\u00a0 \u00a0 automatic_resources_max_replica_count=1,\u00a0 \u00a0 service_account=\"SERVICE_ACCOUNT_ID@PROJECT_ID.iam.gserviceaccount.com\")\n```\nReplace the following:\n- : The ID for the service account.\n- : The ID of the project.\nLearn more about [using a custom service account](/vertex-ai/docs/general/custom-service-account) and [configuring a service account](/vertex-ai/docs/pipelines/configure-project#service-account) for use with Vertex AI Pipelines.\n## Use VPC Service Controls to prevent data exfiltration\n[VPC Service Controls](/vpc-service-controls/docs/overview) can help you mitigate the risk of data exfiltration from Vertex AI Pipelines. When you use VPC Service Controls to create a service perimeter, resources and data that are created by Vertex AI Pipelines and the Google Cloud Pipeline Components are automatically protected. For example, when you use VPC Service Controls to protect your pipeline, the following artifacts can't leave your service perimeter:\n- Training data for an AutoML model\n- Models that you created\n- Results from a batch prediction request\nLearn more about [VPC Service Controls with Vertex AI](/vertex-ai/docs/general/vpc-service-controls) .\n## Set up VPC Network Peering\nYou can configure Google Cloud Pipeline Components to peer with a Virtual Private Cloud by providing extra parameters. For example, you can use the following code to specify a VPC network for an `EndpointCreate` component:\n```\nendpoint_create_op = EndpointCreateOp(\u00a0 \u00a0 project=\"PROJECT_ID\",\u00a0 \u00a0 location=\"REGION\",\u00a0 \u00a0 display_name=\"endpoint-display-name\",\u00a0 \u00a0 network=\"NETWORK\")\n```\nReplace the following:\n- : The ID of the project.\n- : The region where you are using Vertex AI.\n- : The VPC network, for example,`\"projects/12345/global/networks/myVPC\"`.\nLearn more about [VPC Network Peering in Vertex AI](/vertex-ai/docs/general/vpc-peering) .\n## Use customer-managed encryption keys (CMEK)\nBy default, Google Cloud automatically [encrypts data when atrest](/security/encryption/default-encryption) using encryption keys managed by Google. If you have specific compliance or regulatory requirements related to the keys that protect your data, you can use customer-managed encryption keys (CMEK) for your resources. Before you start to use customer-managed encryption keys, learn about the [benefits of CMEK on Vertex AI](/vertex-ai/docs/general/cmek#benefits) and [current CMEK supported resources](/vertex-ai/docs/general/cmek#resource-list) .\n### Configuring your component with CMEK\nAfter you create a key ring and key in [Cloud Key Management Service](/vertex-ai/docs/general/cmek#configure-cmek) , and grant Vertex AI encrypter and decrypter permissions for your key, you can create a new CMEK-supported component by specifying your key as one of the create parameters. For example, you can use the following code to specify a key for a `ModelBatchPredict` component:\n```\nmodel_batch_predict_op = ModelBatchPredictOp(project=\"PROJECT_ID\",\u00a0 \u00a0 model=model_upload_op.outputs[\"model\"],\u00a0 \u00a0 encryption_spec_key_name=\"projects/PROJECT_ID/locations/LOCATION_ID/keyRings/KEY_RING_NAME/cryptoKeys/KEY_NAME\")\n```\nReplace the following:\n- : Your Google Cloud project ID.\n- : A valid location or region identifier, for example,`us-central1`.\n- : The name of the key ring for your CMEK. For more information about key rings, see [Cloud KMS resources](/kms/docs/resource-hierarchy#key_rings) .\n- : The CMEK key name.\n**Note:** Google Cloud components that aren't Vertex AI components might require additional permissions. For example, a BigQuery component might require [encryption and decryption permission](/bigquery/docs/customer-managed-encryption#grant_permission) . In addition, the location of the CMEK key must be the same as the location of the component. For example, if a BigQuery component loads data from a dataset that's based in the [multi-region US location](/bigquery/docs/locations#multi-regional_locations) , the CMEK key must also be based in the multi-region US location.\n## Consume or produce artifacts in your component\nThe Google Cloud Pipeline Components SDK defines a set of [ML metadata artifact types](/vertex-ai/docs/pipelines/artifact-types) that serve as component input and output. Some Google Cloud Pipeline Components consume these artifacts as input or produce them as output.\nThis page shows how to consume and produce these artifacts.\n### Consume an ML artifact\nThe artifact's metadata can serve as input to a component. To prepare an artifact to be consumed as input, you must extract it and put it in a component YAML file.\nFor example, the [ModelUploadOp](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.8.0/api/v1/model.html#v1.model.ModelUploadOp) component generates a `google.VertexModel` artifact which can be consumed by a [ModelDeployOp](https://github.com/kubeflow/pipelines/blob/google-cloud-pipeline-components-0.1.9/components/google-cloud/google_cloud_pipeline_components/aiplatform/endpoint/deploy_model/component.yaml#L137) component. Use the following code in a component YAML file to retrieve the a Vertex AI `Model` resource from the inputs (reference):\n```\n\"model\": \"',\"{{$.inputs.artifacts['model'].metadata['resourceName']}}\", '\"'\n```\nFor the complete schema of the artifact's metadata, see the [artifact_types.py file](https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/types/artifact_types.py) in the Kubeflow GitHub repo.\n```\nfrom kfp.dsl import Artifact, Input@dsl.componentdef classification_model_eval_metrics(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str, \u00a0# \"us-central1\",\u00a0 \u00a0 model: Input[Artifact],) :\u00a0 \u00a0# Consumes the `resourceName` metadata\u00a0 \u00a0model_resource_path = model.metadata[\"resourceName\"]\n```\nFor an example of how to consume the Vertex ML Metadata artifacts types, see [Train a classification model using tabular data and Vertex AI AutoML](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/automl_tabular_classification_beans.ipynb) .\n### Create an ML artifact\nThe following code examples show how to create a Vertex ML Metadata artifact that a GCPC component can accept as input.\nThe following example creates an Importer node that registers a new artifact entry to Vertex ML Metadata. The importer node takes the artifact's URI and metadata as primitives and packages them into an artifact.\n```\nfrom google_cloud_pipeline_components import v1from google_cloud_pipeline_components.types import artifact_typesfrom kfp.components import importer_nodefrom kfp import dsl@dsl.pipeline(name=_PIPELINE_NAME)def pipeline():\u00a0 # Using importer and UnmanagedContainerModel artifact for model upload\u00a0 # component.\u00a0 importer_spec = importer_node.importer(\u00a0 \u00a0 \u00a0 artifact_uri='gs://managed-pipeline-gcpc-e2e-test/automl-tabular/model',\u00a0 \u00a0 \u00a0 artifact_class=artifact_types.UnmanagedContainerModel,\u00a0 \u00a0 \u00a0 metadata={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'containerSpec': {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'imageUri':\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:prod'\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 })\u00a0 # Consuming the UnmanagedContainerModel artifact for the previous step\u00a0 model_upload_with_artifact_op = v1.model.ModelUploadOp(\u00a0 \u00a0 \u00a0 project=_GCP_PROJECT_ID,\u00a0 \u00a0 \u00a0 location=_GCP_REGION,\u00a0 \u00a0 \u00a0 display_name=_MODEL_DISPLAY_NAME,\u00a0 \u00a0 \u00a0 unmanaged_container_model=importer_spec.outputs['artifact'])\n```\nThe following example shows how to output a Vertex ML Metadata artifact directly from a Python component.\n```\nfrom google_cloud_pipeline_components import v1from kfp.components import importer_nodefrom kfp import dsl@dsl.component(\u00a0 \u00a0 base_image='python:3.9',\u00a0 \u00a0 packages_to_install=['google-cloud-aiplatform'],)# Note currently KFP SDK doesn't support outputting artifacts in `google` namespace.# Use the base type dsl.Artifact instead.def return_unmanaged_model(model: dsl.Output[dsl.Artifact]):\u00a0 model.metadata['containerSpec'] = {\u00a0 \u00a0 \u00a0 'imageUri':\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:prod'\u00a0 }\u00a0 model.uri = f'gs://automl-tabular-pipeline/automl-tabular/model'@dsl.pipeline(name=_PIPELINE_NAME)def pipeline():\u00a0 unmanaged_model_op = return_unmanaged_model()\u00a0 # Consuming the UnmanagedContainerModel artifact for the previous step\u00a0 model_upload_with_artifact_op = v1.model.ModelUploadOp(\u00a0 \u00a0 \u00a0 project=_GCP_PROJECT_ID,\u00a0 \u00a0 \u00a0 location=_GCP_REGION,\u00a0 \u00a0 \u00a0 display_name=_MODEL_DISPLAY_NAME,\u00a0 \u00a0 \u00a0 unmanaged_container_model=unmanaged_model_op.outputs['model'])\n```\nThe following example shows how to generate a [VertexBatchPredictionJob](https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.8.0/api/artifact_types.html#google_cloud_pipeline_components.types.artifact_types.VertexBatchPredictionJob) artifact as output from a container-based component using the [artifact_types.py](https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/types/artifact_types.py) utility class.\n```\nbp_job_artifact = VertexBatchPredictionJob(\u00a0 \u00a0 'batchpredictionjob', vertex_uri_prefix + get_job_response.name,\u00a0 \u00a0 get_job_response.name, get_job_response.output_info.bigquery_output_table,\u00a0 \u00a0 get_job_response.output_info.bigquery_output_dataset,\u00a0 \u00a0 get_job_response.output_info.gcs_output_directory)\u00a0 \u00a0 output_artifacts = executor_input_json.get('outputs', {}).get('artifacts', {})\u00a0 \u00a0 executor_output['artifacts'] = bp_job_artifact.to_executor_output_artifact(output_artifacts[bp_job_artifact.name])\n```", "guide": "Vertex AI"}