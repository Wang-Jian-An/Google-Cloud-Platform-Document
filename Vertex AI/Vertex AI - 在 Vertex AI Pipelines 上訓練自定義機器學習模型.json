{"title": "Vertex AI - \u5728 Vertex AI Pipelines \u4e0a\u8a13\u7df4\u81ea\u5b9a\u7fa9\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b", "url": "https://cloud.google.com/vertex-ai/docs/tutorials/custom-training-pipelines/tabular?hl=zh-cn", "abstract": "# Vertex AI - \u5728 Vertex AI Pipelines \u4e0a\u8a13\u7df4\u81ea\u5b9a\u7fa9\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\n\u672c\u6559\u7a0b\u4ecb\u7d39\u5982\u4f55\u4f7f\u7528 Vertex AI Pipelines \u904b\u884c\u7aef\u5230\u7aef\u6a5f\u5668\u5b78\u7fd2\u5de5\u4f5c\u6d41\uff0c\u5305\u62ec\u4ee5\u4e0b\u4efb\u52d9\uff1a\n- \u5c0e\u5165\u548c\u8f49\u63db\u6578\u64da\u3002\n- \u4f7f\u7528\u6240\u9078\u7684\u6a5f\u5668\u5b78\u7fd2\u6846\u67b6\u8a13\u7df4\u6a21\u578b\u3002\n- \u5c07\u7d93\u904e\u8a13\u7df4\u7684\u6a21\u578b\u5c0e\u5165 Vertex AI Model Registry\u3002\n- **\u53ef\u9078** \uff1a\u4f7f\u7528 Vertex AI Prediction \u90e8\u7f72\u7528\u65bc\u5728\u7dda\u670d\u52d9\u7684\u6a21\u578b\u3002", "content": "## \u6e96\u5099\u5de5\u4f5c\n- \u78ba\u4fdd\u60a8\u5df2\u5b8c\u6210 [\u8a2d\u7f6e Google Cloud \u9805\u76ee\u548c\u958b\u767c\u74b0\u5883](https://cloud.google.com/vertex-ai/docs/pipelines/configure-project?hl=zh-cn#project) \u4e2d\u7684\u4efb\u52d9 1-3\u3002\n- \u5b89\u88dd [Python \u7248 Vertex AI SDK](https://cloud.google.com/vertex-ai/docs/start/client-libraries?hl=zh-cn#client_libraries) \u548c Kubeflow Pipelines SDK\uff1a```\npython3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quiet\n```## \u904b\u884c\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u8a13\u7df4\u6d41\u6c34\u7dda\n\u5728\u4ee5\u4e0b\u6a19\u7c64\u9801\u4e2d\u9078\u64c7\u8a13\u7df4\u76ee\u6a19\u548c\u6a5f\u5668\u5b78\u7fd2\u6846\u67b6\uff0c\u4ee5\u7372\u53d6\u53ef\u5728\u60a8\u7684\u74b0\u5883\u4e2d\u904b\u884c\u7684\u793a\u4f8b\u4ee3\u78bc\u3002\u793a\u4f8b\u4ee3\u78bc\u6703\u57f7\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a\n- \u5f9e [\u7d44\u4ef6\u4ee3\u78bc\u5eab](https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/main/community-content/pipeline_components) \u52a0\u8f09\u7d44\u4ef6\uff0c\u4ee5\u7528\u4f5c\u6d41\u6c34\u7dda\u57fa\u672c\u7d44\u4ef6\u3002\n- \u5275\u5efa\u7d44\u4ef6\u4efb\u52d9\u4e26\u4f7f\u7528\u53c3\u6578\u5728\u4efb\u52d9\u4e4b\u9593\u50b3\u905e\u6578\u64da\uff0c\u5f9e\u800c\u69cb\u5efa\u6d41\u6c34\u7dda\u3002\n- \u63d0\u4ea4\u6d41\u6c34\u7dda\u4ee5\u5728 Vertex AI Pipelines \u4e0a\u57f7\u884c\u3002\u8acb\u53c3\u95b1 [Vertex AI Pipelines \u50f9\u683c](https://cloud.google.com/vertex-ai/pricing?hl=zh-cn#pipelines) \u3002\n\u5c07\u4ee3\u78bc\u8907\u88fd\u5230\u958b\u767c\u74b0\u5883\u4e2d\u4e26\u904b\u884c\u4ee3\u78bc\u3002\n[\u5728\u7de8\u8f2f\u5668\u4e2d\u6253\u958b](https://ide.cloud.google.com/?git_repo=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fvertex-ai-samples&page=editor&cloudshell_workspace=community-content%2FTrain_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines%2FTrain_tabular_classification_model_using_TensorFlow_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py&hl=zh-cn) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_classification_model_using_TensorFlow_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")binarize_column_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Binarize_column/in_CSV_format/component.yaml\")split_rows_into_subsets_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/dataset_manipulation/Split_rows_into_subsets/in_CSV/component.yaml\")create_fully_connected_tensorflow_network_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/tensorflow/Create_fully_connected_network/component.yaml\")train_model_using_Keras_on_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/tensorflow/Train_model_using_Keras/on_CSV/component.yaml\")predict_with_TensorFlow_model_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/tensorflow/Predict/on_CSV/component.yaml\")upload_Tensorflow_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_Tensorflow_model/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_classification_model_using_TensorFlow_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 training_set_fraction = 0.8\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 classification_label_column = \"class\"\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 dataset = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 dataset = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 dataset = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 # # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # column_names=None, \u00a0# =[...]\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 classification_dataset = binarize_column_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 predicate=\" > 0\",\u00a0 \u00a0 \u00a0 \u00a0 new_column_name=classification_label_column,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 split_task = split_rows_into_subsets_op(\u00a0 \u00a0 \u00a0 \u00a0 table=classification_dataset,\u00a0 \u00a0 \u00a0 \u00a0 fraction_1=training_set_fraction,\u00a0 \u00a0 )\u00a0 \u00a0 classification_training_data = split_task.outputs[\"split_1\"]\u00a0 \u00a0 classification_testing_data = split_task.outputs[\"split_2\"]\u00a0 \u00a0 network = create_fully_connected_tensorflow_network_op(\u00a0 \u00a0 \u00a0 \u00a0 input_size=len(feature_columns),\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 hidden_layer_sizes=[10],\u00a0 \u00a0 \u00a0 \u00a0 activation_name=\"elu\",\u00a0 \u00a0 \u00a0 \u00a0 output_activation_name=\"sigmoid\",\u00a0 \u00a0 \u00a0 \u00a0 # output_size=1,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 model = train_model_using_Keras_on_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 training_data=classification_training_data,\u00a0 \u00a0 \u00a0 \u00a0 model=network,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=classification_label_column,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 loss_function_name=\"binary_crossentropy\",\u00a0 \u00a0 \u00a0 \u00a0 number_of_epochs=10,\u00a0 \u00a0 \u00a0 \u00a0 #learning_rate=0.1,\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_name=\"Adadelta\",\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_parameters={},\u00a0 \u00a0 \u00a0 \u00a0 #batch_size=32,\u00a0 \u00a0 \u00a0 \u00a0 #metric_names=[\"mean_absolute_error\"],\u00a0 \u00a0 \u00a0 \u00a0 #random_seed=0,\u00a0 \u00a0 ).outputs[\"trained_model\"]\u00a0 \u00a0 predictions = predict_with_TensorFlow_model_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 dataset=classification_testing_data,\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 \u00a0 \u00a0 # label_column_name needs to be set when doing prediction on a dataset that has labels\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=classification_label_column,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # batch_size=1000,\u00a0 \u00a0 ).outputs[\"predictions\"]\u00a0 \u00a0 vertex_model_name = upload_Tensorflow_model_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func = train_tabular_classification_model_using_TensorFlow_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n``` [\u5728\u7de8\u8f2f\u5668\u4e2d\u6253\u958b](https://ide.cloud.google.com/?git_repo=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fvertex-ai-samples&page=editor&cloudshell_workspace=community-content%2FTrain_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines%2FTrain_tabular_classification_model_using_PyTorch_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py&hl=zh-cn) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_classification_model_using_PyTorch_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")binarize_column_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Binarize_column/in_CSV_format/component.yaml\")create_fully_connected_pytorch_network_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/PyTorch/Create_fully_connected_network/component.yaml\")train_pytorch_model_from_csv_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/PyTorch/Train_PyTorch_model/from_CSV/component.yaml\")create_pytorch_model_archive_with_base_handler_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/PyTorch/Create_PyTorch_Model_Archive/with_base_handler/component.yaml\")upload_PyTorch_model_archive_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_PyTorch_model_archive/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_classification_model_using_PyTorch_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 classification_label_column = \"class\"\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 training_data = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 training_data = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 # Cleaning the NaN values.\u00a0 \u00a0 training_data = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 #replacement_type_name=\"float\",\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 classification_training_data = binarize_column_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 predicate=\" > 0\",\u00a0 \u00a0 \u00a0 \u00a0 new_column_name=classification_label_column,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 network = create_fully_connected_pytorch_network_op(\u00a0 \u00a0 \u00a0 \u00a0 input_size=len(feature_columns),\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 hidden_layer_sizes=[10],\u00a0 \u00a0 \u00a0 \u00a0 activation_name=\"elu\",\u00a0 \u00a0 \u00a0 \u00a0 output_activation_name=\"sigmoid\",\u00a0 \u00a0 \u00a0 \u00a0 # output_size=1,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 model = train_pytorch_model_from_csv_op(\u00a0 \u00a0 \u00a0 \u00a0 model=network,\u00a0 \u00a0 \u00a0 \u00a0 training_data=classification_training_data,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=classification_label_column,\u00a0 \u00a0 \u00a0 \u00a0 loss_function_name=\"binary_cross_entropy\",\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 #number_of_epochs=1,\u00a0 \u00a0 \u00a0 \u00a0 #learning_rate=0.1,\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_name=\"Adadelta\",\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_parameters={},\u00a0 \u00a0 \u00a0 \u00a0 #batch_size=32,\u00a0 \u00a0 \u00a0 \u00a0 #batch_log_interval=100,\u00a0 \u00a0 \u00a0 \u00a0 #random_seed=0,\u00a0 \u00a0 ).outputs[\"trained_model\"]\u00a0 \u00a0 model_archive = create_pytorch_model_archive_with_base_handler_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # model_name=\"model\",\u00a0 \u00a0 \u00a0 \u00a0 # model_version=\"1.0\",\u00a0 \u00a0 ).outputs[\"Model archive\"]\u00a0 \u00a0 vertex_model_name = upload_PyTorch_model_archive_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model_archive=model_archive,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func=train_tabular_classification_model_using_PyTorch_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n``` [\u5728\u7de8\u8f2f\u5668\u4e2d\u6253\u958b](https://ide.cloud.google.com/?git_repo=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fvertex-ai-samples&page=editor&cloudshell_workspace=community-content%2FTrain_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines%2FTrain_tabular_classification_model_using_XGBoost_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py&hl=zh-cn) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_classification_model_using_XGBoost_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")binarize_column_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Binarize_column/in_CSV_format/component.yaml\")split_rows_into_subsets_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/dataset_manipulation/Split_rows_into_subsets/in_CSV/component.yaml\")train_XGBoost_model_on_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/XGBoost/Train/component.yaml\")xgboost_predict_on_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/XGBoost/Predict/component.yaml\")upload_XGBoost_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_XGBoost_model/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_classification_model_using_XGBoost_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 training_set_fraction = 0.8\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 classification_label_column = \"class\"\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 dataset = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 dataset = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 dataset = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 # # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # column_names=None, \u00a0# =[...]\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 classification_dataset = binarize_column_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 predicate=\"> 0\",\u00a0 \u00a0 \u00a0 \u00a0 new_column_name=classification_label_column,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 split_task = split_rows_into_subsets_op(\u00a0 \u00a0 \u00a0 \u00a0 table=classification_dataset,\u00a0 \u00a0 \u00a0 \u00a0 fraction_1=training_set_fraction,\u00a0 \u00a0 )\u00a0 \u00a0 classification_training_data = split_task.outputs[\"split_1\"]\u00a0 \u00a0 classification_testing_data = split_task.outputs[\"split_2\"]\u00a0 \u00a0 model = train_XGBoost_model_on_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 training_data=classification_training_data,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=classification_label_column,\u00a0 \u00a0 \u00a0 \u00a0 objective=\"binary:logistic\",\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 #starting_model=None,\u00a0 \u00a0 \u00a0 \u00a0 #num_iterations=10,\u00a0 \u00a0 \u00a0 \u00a0 #booster_params={},\u00a0 \u00a0 \u00a0 \u00a0 #booster=\"gbtree\",\u00a0 \u00a0 \u00a0 \u00a0 #learning_rate=0.3,\u00a0 \u00a0 \u00a0 \u00a0 #min_split_loss=0,\u00a0 \u00a0 \u00a0 \u00a0 #max_depth=6,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 # Predicting on the testing data\u00a0 \u00a0 predictions = xgboost_predict_on_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 data=classification_testing_data,\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 \u00a0 \u00a0 # label_column needs to be set when doing prediction on a dataset that has labels\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=classification_label_column,\u00a0 \u00a0 ).outputs[\"predictions\"]\u00a0 \u00a0 vertex_model_name = upload_XGBoost_model_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func = train_tabular_classification_model_using_XGBoost_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n``` [\u5728\u7de8\u8f2f\u5668\u4e2d\u6253\u958b](https://ide.cloud.google.com/?git_repo=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fvertex-ai-samples&page=editor&cloudshell_workspace=community-content%2FTrain_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines%2FTrain_tabular_classification_logistic_regression_model_using_Scikit_learn_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py&hl=zh-cn) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_classification_logistic_regression_model_using_Scikit_learn_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")binarize_column_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Binarize_column/in_CSV_format/component.yaml\")train_logistic_regression_model_using_scikit_learn_from_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/1f5cf6e06409b704064b2086c0a705e4e6b4fcde/community-content/pipeline_components/ML_frameworks/Scikit_learn/Train_logistic_regression_model/from_CSV/component.yaml\")upload_Scikit_learn_pickle_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_Scikit-learn_pickle_model/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_classification_logistic_regression_model_using_Scikit_learn_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 classification_label_column = \"class\"\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 training_data = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 training_data = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 # Cleaning the NaN values.\u00a0 \u00a0 training_data = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 #replacement_type_name=\"float\",\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 classification_training_data = binarize_column_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 predicate=\"> 0\",\u00a0 \u00a0 \u00a0 \u00a0 new_column_name=classification_label_column,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 model = train_logistic_regression_model_using_scikit_learn_from_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 dataset=classification_training_data,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=classification_label_column,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 #penalty=\"l2\",\u00a0 \u00a0 \u00a0 \u00a0 #solver=\"lbfgs\",\u00a0 \u00a0 \u00a0 \u00a0 #max_iterations=100,\u00a0 \u00a0 \u00a0 \u00a0 #multi_class_mode=\"auto\",\u00a0 \u00a0 \u00a0 \u00a0 #random_seed=0,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 vertex_model_name = upload_Scikit_learn_pickle_model_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 sklearn_vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func = train_tabular_classification_logistic_regression_model_using_Scikit_learn_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n```\n [\u5728\u7de8\u8f2f\u5668\u4e2d\u6253\u958b](https://ide.cloud.google.com/?git_repo=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fvertex-ai-samples&page=editor&cloudshell_workspace=community-content%2FTrain_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines%2FTrain_tabular_regression_model_using_TensorFlow_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py&hl=zh-cn) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_regression_model_using_TensorFlow_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")split_rows_into_subsets_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/dataset_manipulation/Split_rows_into_subsets/in_CSV/component.yaml\")create_fully_connected_tensorflow_network_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/tensorflow/Create_fully_connected_network/component.yaml\")train_model_using_Keras_on_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/tensorflow/Train_model_using_Keras/on_CSV/component.yaml\")predict_with_TensorFlow_model_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/tensorflow/Predict/on_CSV/component.yaml\")upload_Tensorflow_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_Tensorflow_model/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_regression_model_using_Tensorflow_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 training_set_fraction = 0.8\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 dataset = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 dataset = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 dataset = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 # # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # column_names=None, \u00a0# =[...]\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 split_task = split_rows_into_subsets_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 fraction_1=training_set_fraction,\u00a0 \u00a0 )\u00a0 \u00a0 training_data = split_task.outputs[\"split_1\"]\u00a0 \u00a0 testing_data = split_task.outputs[\"split_2\"]\u00a0 \u00a0 network = create_fully_connected_tensorflow_network_op(\u00a0 \u00a0 \u00a0 \u00a0 input_size=len(feature_columns),\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 hidden_layer_sizes=[10],\u00a0 \u00a0 \u00a0 \u00a0 activation_name=\"elu\",\u00a0 \u00a0 \u00a0 \u00a0 # output_activation_name=None,\u00a0 \u00a0 \u00a0 \u00a0 # output_size=1,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 model = train_model_using_Keras_on_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 training_data=training_data,\u00a0 \u00a0 \u00a0 \u00a0 model=network,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 #loss_function_name=\"mean_squared_error\",\u00a0 \u00a0 \u00a0 \u00a0 number_of_epochs=10,\u00a0 \u00a0 \u00a0 \u00a0 #learning_rate=0.1,\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_name=\"Adadelta\",\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_parameters={},\u00a0 \u00a0 \u00a0 \u00a0 #batch_size=32,\u00a0 \u00a0 \u00a0 \u00a0 metric_names=[\"mean_absolute_error\"],\u00a0 \u00a0 \u00a0 \u00a0 #random_seed=0,\u00a0 \u00a0 ).outputs[\"trained_model\"]\u00a0 \u00a0 predictions = predict_with_TensorFlow_model_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 dataset=testing_data,\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 \u00a0 \u00a0 # label_column_name needs to be set when doing prediction on a dataset that has labels\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # batch_size=1000,\u00a0 \u00a0 ).outputs[\"predictions\"]\u00a0 \u00a0 vertex_model_name = upload_Tensorflow_model_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func=train_tabular_regression_model_using_Tensorflow_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n``` [\u5728\u7de8\u8f2f\u5668\u4e2d\u6253\u958b](https://ide.cloud.google.com/?git_repo=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fvertex-ai-samples&page=editor&cloudshell_workspace=community-content%2FTrain_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines%2FTrain_tabular_regression_model_using_PyTorch_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py&hl=zh-cn) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_regression_model_using_PyTorch_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")create_fully_connected_pytorch_network_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/PyTorch/Create_fully_connected_network/component.yaml\")train_pytorch_model_from_csv_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/PyTorch/Train_PyTorch_model/from_CSV/component.yaml\")create_pytorch_model_archive_with_base_handler_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/PyTorch/Create_PyTorch_Model_Archive/with_base_handler/component.yaml\")upload_PyTorch_model_archive_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_PyTorch_model_archive/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_regression_model_using_PyTorch_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 training_data = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 training_data = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 # Cleaning the NaN values.\u00a0 \u00a0 training_data = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 #replacement_type_name=\"float\",\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 network = create_fully_connected_pytorch_network_op(\u00a0 \u00a0 \u00a0 \u00a0 input_size=len(feature_columns),\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 hidden_layer_sizes=[10],\u00a0 \u00a0 \u00a0 \u00a0 activation_name=\"elu\",\u00a0 \u00a0 \u00a0 \u00a0 # output_activation_name=None,\u00a0 \u00a0 \u00a0 \u00a0 # output_size=1,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 model = train_pytorch_model_from_csv_op(\u00a0 \u00a0 \u00a0 \u00a0 model=network,\u00a0 \u00a0 \u00a0 \u00a0 training_data=training_data,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 #loss_function_name=\"mse_loss\",\u00a0 \u00a0 \u00a0 \u00a0 #number_of_epochs=1,\u00a0 \u00a0 \u00a0 \u00a0 #learning_rate=0.1,\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_name=\"Adadelta\",\u00a0 \u00a0 \u00a0 \u00a0 #optimizer_parameters={},\u00a0 \u00a0 \u00a0 \u00a0 #batch_size=32,\u00a0 \u00a0 \u00a0 \u00a0 #batch_log_interval=100,\u00a0 \u00a0 \u00a0 \u00a0 #random_seed=0,\u00a0 \u00a0 ).outputs[\"trained_model\"]\u00a0 \u00a0 model_archive = create_pytorch_model_archive_with_base_handler_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # model_name=\"model\",\u00a0 \u00a0 \u00a0 \u00a0 # model_version=\"1.0\",\u00a0 \u00a0 ).outputs[\"Model archive\"]\u00a0 \u00a0 vertex_model_name = upload_PyTorch_model_archive_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model_archive=model_archive,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func=train_tabular_regression_model_using_PyTorch_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n``` [\u5728\u7de8\u8f2f\u5668\u4e2d\u6253\u958b](https://ide.cloud.google.com/?git_repo=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fvertex-ai-samples&page=editor&cloudshell_workspace=community-content%2FTrain_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines%2FTrain_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py&hl=zh-cn) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")split_rows_into_subsets_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/dataset_manipulation/Split_rows_into_subsets/in_CSV/component.yaml\")train_XGBoost_model_on_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/XGBoost/Train/component.yaml\")xgboost_predict_on_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/XGBoost/Predict/component.yaml\")upload_XGBoost_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_XGBoost_model/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_regression_model_using_XGBoost_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 training_set_fraction = 0.8\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 dataset = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 dataset = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 dataset = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 # # Optional:\u00a0 \u00a0 \u00a0 \u00a0 # column_names=None, \u00a0# =[...]\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 split_task = split_rows_into_subsets_op(\u00a0 \u00a0 \u00a0 \u00a0 table=dataset,\u00a0 \u00a0 \u00a0 \u00a0 fraction_1=training_set_fraction,\u00a0 \u00a0 )\u00a0 \u00a0 training_data = split_task.outputs[\"split_1\"]\u00a0 \u00a0 testing_data = split_task.outputs[\"split_2\"]\u00a0 \u00a0 model = train_XGBoost_model_on_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 training_data=training_data,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=label_column,\u00a0 \u00a0 \u00a0 \u00a0 # Optional:\u00a0 \u00a0 \u00a0 \u00a0 #starting_model=None,\u00a0 \u00a0 \u00a0 \u00a0 #num_iterations=10,\u00a0 \u00a0 \u00a0 \u00a0 #booster_params={},\u00a0 \u00a0 \u00a0 \u00a0 #objective=\"reg:squarederror\",\u00a0 \u00a0 \u00a0 \u00a0 #booster=\"gbtree\",\u00a0 \u00a0 \u00a0 \u00a0 #learning_rate=0.3,\u00a0 \u00a0 \u00a0 \u00a0 #min_split_loss=0,\u00a0 \u00a0 \u00a0 \u00a0 #max_depth=6,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 # Predicting on the testing data\u00a0 \u00a0 predictions = xgboost_predict_on_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 data=testing_data,\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 \u00a0 \u00a0 # label_column needs to be set when doing prediction on a dataset that has labels\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=label_column,\u00a0 \u00a0 ).outputs[\"predictions\"]\u00a0 \u00a0 vertex_model_name = upload_XGBoost_model_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func = train_tabular_regression_model_using_XGBoost_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n``` [\u5728\u7de8\u8f2f\u5668\u4e2d\u6253\u958b](https://ide.cloud.google.com/?git_repo=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fvertex-ai-samples&page=editor&cloudshell_workspace=community-content%2FTrain_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines%2FTrain_tabular_regression_linear_model_using_Scikit_learn_and_import_to_Vertex_AI&cloudshell_open_in_editor=pipeline.py&hl=zh-cn) [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/67d8d949e38f32666d9a115209f2f2187d71e886/community-content/Train_tabular_models_with_many_frameworks_and_import_to_Vertex_AI_using_Pipelines/Train_tabular_regression_linear_model_using_Scikit_learn_and_import_to_Vertex_AI/pipeline.py) \n```\n# python3 -m pip install \"kfp<2.0.0\" \"google-cloud-aiplatform>=1.16.0\" --upgrade --quietfrom kfp import components# %% Loading componentsdownload_from_gcs_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/storage/download/component.yaml\")select_columns_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Select_columns/in_CSV_format/component.yaml\")fill_all_missing_values_using_Pandas_on_CSV_data_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml\")train_linear_regression_model_using_scikit_learn_from_CSV_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/1f5cf6e06409b704064b2086c0a705e4e6b4fcde/community-content/pipeline_components/ML_frameworks/Scikit_learn/Train_linear_regression_model/from_CSV/component.yaml\")upload_Scikit_learn_pickle_model_to_Google_Cloud_Vertex_AI_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Upload_Scikit-learn_pickle_model/component.yaml\")deploy_model_to_endpoint_op = components.load_component_from_url(\"https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/399405402d95f4a011e2d2e967c96f8508ba5688/community-content/pipeline_components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml\")# %% Pipeline definitiondef train_tabular_regression_linear_model_using_Scikit_learn_pipeline():\u00a0 \u00a0 dataset_gcs_uri = \"gs://ml-pipeline-dataset/Chicago_taxi_trips/chicago_taxi_trips_2019-01-01_-_2019-02-01_limit=10000.csv\"\u00a0 \u00a0 feature_columns = [\"trip_seconds\", \"trip_miles\", \"pickup_community_area\", \"dropoff_community_area\", \"fare\", \"tolls\", \"extras\"] \u00a0# Excluded \"trip_total\"\u00a0 \u00a0 label_column = \"tips\"\u00a0 \u00a0 all_columns = [label_column] + feature_columns\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 deploy_model = False\u00a0 \u00a0 training_data = download_from_gcs_op(\u00a0 \u00a0 \u00a0 \u00a0 gcs_path=dataset_gcs_uri\u00a0 \u00a0 ).outputs[\"Data\"]\u00a0 \u00a0 training_data = select_columns_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 column_names=all_columns,\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 # Cleaning the NaN values.\u00a0 \u00a0 training_data = fill_all_missing_values_using_Pandas_on_CSV_data_op(\u00a0 \u00a0 \u00a0 \u00a0 table=training_data,\u00a0 \u00a0 \u00a0 \u00a0 replacement_value=\"0\",\u00a0 \u00a0 \u00a0 \u00a0 #replacement_type_name=\"float\",\u00a0 \u00a0 ).outputs[\"transformed_table\"]\u00a0 \u00a0 model = train_linear_regression_model_using_scikit_learn_from_CSV_op(\u00a0 \u00a0 \u00a0 \u00a0 dataset=training_data,\u00a0 \u00a0 \u00a0 \u00a0 label_column_name=label_column,\u00a0 \u00a0 ).outputs[\"model\"]\u00a0 \u00a0 vertex_model_name = upload_Scikit_learn_pickle_model_to_Google_Cloud_Vertex_AI_op(\u00a0 \u00a0 \u00a0 \u00a0 model=model,\u00a0 \u00a0 ).outputs[\"model_name\"]\u00a0 \u00a0 # Deploying the model might incur additional costs over time\u00a0 \u00a0 if deploy_model:\u00a0 \u00a0 \u00a0 \u00a0 sklearn_vertex_endpoint_name = deploy_model_to_endpoint_op(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 model_name=vertex_model_name,\u00a0 \u00a0 \u00a0 \u00a0 ).outputs[\"endpoint_name\"]pipeline_func = train_tabular_regression_linear_model_using_Scikit_learn_pipeline# %% Pipeline submissionif __name__ == '__main__':\u00a0 \u00a0 from google.cloud import aiplatform\u00a0 \u00a0 aiplatform.PipelineJob.from_pipeline_func(pipeline_func=pipeline_func).submit()\n```\n\u5c0d\u65bc\u63d0\u4f9b\u7684\u4ee3\u78bc\u793a\u4f8b\uff0c\u8acb\u6ce8\u610f\u4ee5\u4e0b\u4e8b\u9805\uff1a\n- Kubeflow \u6d41\u6c34\u7dda\u5b9a\u7fa9\u7232 Python \u51fd\u6578\u3002\n- \u6d41\u6c34\u7dda\u7684\u5de5\u4f5c\u6d41\u6b65\u9a5f\u662f\u4f7f\u7528 Kubeflow \u6d41\u6c34\u7dda\u7d44\u4ef6\u5275\u5efa\u7684\u3002\u901a\u904e\u4f7f\u7528\u7d44\u4ef6\u7684\u8f38\u51fa\u4f5c\u7232\u53e6\u4e00\u500b\u7d44\u4ef6\u7684\u8f38\u5165\uff0c\u60a8\u53ef\u4ee5\u5c07\u6d41\u6c34\u7dda\u7684\u5de5\u4f5c\u6d41\u5b9a\u7fa9\u7232\u5716\u3002\u4f8b\u5982\uff0c`fill_all_missing_values_using_Pandas_on_CSV_data_op`\u7d44\u4ef6\u4efb\u52d9\u4f9d\u8cf4\u65bc`select_columns_using_Pandas_on_CSV_data_op`\u7d44\u4ef6\u4efb\u52d9\u4e2d\u7684`transformed_table`\u8f38\u51fa\u3002\n- \u60a8\u53ef\u4ee5\u4f7f\u7528 Python \u7248 Vertex AI SDK \u5275\u5efa\u5728 Vertex AI Pipelines \u4e0a\u904b\u884c\u7684\u6d41\u6c34\u7dda\u3002## \u76e3\u63a7\u6d41\u6c34\u7dda\n\u5728 Google Cloud \u63a7\u5236\u6aaf\u7684 Vertex AI \u90e8\u5206\u4e2d\uff0c\u8f49\u5230 **\u6d41\u6c34\u7dda** \u9801\u9762\u4e26\u6253\u958b **\u904b\u884c** \u6a19\u7c64\u9801\u3002\n[\u8f49\u5230\u201c\u6d41\u6c34\u7dda\u904b\u884c\u201d](https://console.cloud.google.com/vertex-ai/pipelines/runs?hl=zh-cn)\n## \u5f8c\u7e8c\u6b65\u9a5f\n- \u5982\u9700\u8a73\u7d30\u77ad\u89e3 Vertex AI Pipelines\uff0c\u8acb\u53c3\u95b1 [Vertex AI Pipelines \u7c21\u4ecb](https://cloud.google.com/vertex-ai/docs/pipelines/introduction?hl=zh-cn) \u3002", "guide": "Vertex AI"}