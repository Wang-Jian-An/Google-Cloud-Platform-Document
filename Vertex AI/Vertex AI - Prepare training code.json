{"title": "Vertex AI - Prepare training code", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Prepare training code\nPerform on Vertex AI to run your own machine learning (ML) training code in the cloud, instead of using AutoML. This document describes best practices to consider as you write training code.\n**Note:** This document describes training code best practices specific to Vertex AI, but it doesn't comprehensively explain how to design an ML model or write ML training code. These details vary depending on the purpose of your model and the ML framework that you use for training. If you are new to creating custom ML models, we recommend working through Google's [MachineLearning Crash Course with TensorFlowAPIs.](https://developers.google.com/machine-learning/crash-course)\n", "content": "## Choose a training code structure\nFirst, determine what structure you want your ML training code to take. You can provide training code to Vertex AI in one of the following forms:\n- **A Python script to use with a prebuilt container.** Use the [Vertex AI SDK](/vertex-ai/docs/python-sdk/use-vertex-ai-python-sdk) to [create a custom job](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob#google_cloud_aiplatform_CustomJob_from_local_script) . This method lets you provide your training application as a single Python script.\n- **A Python training application to use with a prebuilt container.** Create a [Python sourcedistribution](https://packaging.python.org/en/latest/overview/#python-source-distributions) with code that trains an ML model and exports it to Cloud Storage. This training application can use any of the dependencies included in the prebuilt container that you plan to use it with. **Note:** If you use the [Vertex AI SDK for Python](/vertex-ai/docs/start/client-libraries) to [create a TrainingPipelineresource](/vertex-ai/docs/training/create-training-pipeline#custom-job-model-upload) , then you can provide your training application as a single Python script, rather than as a Python source distribution.Use this option if one of the Vertex AI [prebuilt containers fortraining](/vertex-ai/docs/training/pre-built-containers) includes all the dependencies that you need for training. For example, if you want to train with PyTorch, scikit-learn, TensorFlow, or XGBoost, then this is likely the better option.To learn about best practices specific to this option, read the guide to [creating a Python trainingapplication](/vertex-ai/docs/training/create-python-pre-built-container) .\n- **A custom container image.** Create a [Docker containerimage](https://docs.docker.com/get-started/overview/#docker-objects) with code that trains an ML model and exports it to Cloud Storage. Include any dependencies required by your code in the container image.Use this option if you want to use dependencies that are not included in one of the Vertex AI [prebuilt containers fortraining](/vertex-ai/docs/training/pre-built-containers) . For example, if you want to train using a Python ML framework that is not available in a prebuilt container, or if you want to train using a programming language other than Python, then this is the better option.To learn about best practices specific to this option, read the guide to [creating a custom container image](/vertex-ai/docs/training/create-custom-container) .\nThe rest of this document describes best practices relevant to both training code structures.\n## Best practices for all custom training code\nWhen you write custom training code for Vertex AI, keep in mind that the code will run on one or more virtual machine (VM) instances managed by Google Cloud. This section describes best practices applicable to all custom training code.\n### Access Google Cloud services in your code\nSeveral of the following sections describe accessing other Google Cloud services from your code. To access Google Cloud services, write your training code to use [Application Default Credentials (ADC)](/docs/authentication#adc) . Many Google Cloud client libraries authenticate with ADC by default. You don't need to configure any environment variables; Vertex AI automatically configures ADC to authenticate as either the [Vertex AI Custom Code Service Agent](/vertex-ai/docs/general/access-control#service-agents) for your project (by default) or a [custom service account](/vertex-ai/docs/general/custom-service-account) (if you have configured one).\n**Note:** If you want to run your training code in your local environment before you run it on Vertex AI, you might want to configure your local environment for ADC. You can do this by [downloading a service accountkey](/docs/authentication/production#manually) or by using the [gcloud auth application-default logincommand](/sdk/gcloud/reference/auth/application-default/login) .\nHowever, when you use a Google Cloud client library in your code, Vertex AI might not always connect to the correct Google Cloud project by default. If you encounter permission errors, connecting to the wrong project might be the problem.\nThis problem occurs because Vertex AI does not run your code directly in your Google Cloud project. Instead, Vertex AI runs your code in one of several separate projects managed by Google. Vertex AI uses these projects exclusively for operations related to your project. Therefore, don't try to infer a project ID from the environment in your training or prediction code; specify project IDs explicitly.\nIf you don't want to hardcode a project ID in your training code, you can reference the `CLOUD_ML_PROJECT_ID` environment variable: Vertex AI sets this environment variable in every custom training container to contain the [project number](/docs/overview#projects) of the project where you initiated custom training. Many Google Cloud tools can accept a project number wherever they take a project ID.\nFor example, if you want to use the [Python Client for GoogleBigQuery](https://github.com/googleapis/python-bigquery) to access a BigQuery table in the same project, then don't try to infer the project in your training code:\nImplicit project selection\n```\nfrom google.cloud import bigqueryclient = bigquery.Client()\n```\nInstead use code that explicitly selects a project:\nExplicit project selection\n```\nimport osfrom google.cloud import bigqueryproject_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]client = bigquery.Client(project=project_number)\n```\nIf you encounter permission errors after configuring your code in this way, then read the following section about [which resources your code can access](#which-resources) to adjust the permissions available to your training code.\nBy default, your training application can access any Google Cloud resources that are available to the [Vertex AI Custom Code Service Agent (CCSA)](/vertex-ai/docs/general/access-control#service-agents) of your project. You can grant the CCSA, and thereby your training application, access to a limited number of other resources by following the instructions in [Grant Vertex AI service agents access to other resources](/vertex-ai/docs/general/access-control#grant_service_agents_access_to_other_resources) . If your training application needs more than read-level access to Google Cloud resources that are not listed in that page, it needs to acquire an OAuth 2.0 access token with the [https://www.googleapis.com/auth/cloud-platform](https://www.googleapis.com/auth/cloud-platform) scope, which can only be done by using a [custom service account](/vertex-ai/docs/general/custom-service-account) .\nFor example, consider your training code's access to Cloud Storage resources:\nBy default, Vertex AI can access any Cloud Storage bucket in the Google Cloud project where you are performing custom training. You can also [grant Vertex AI access to Cloud Storage bucketsin other projects](/vertex-ai/docs/general/access-control#different-project) , or you can precisely customize what buckets a specific job can access by [using a custom service account](/vertex-ai/docs/general/custom-service-account) .\nIn all custom training jobs, Vertex AI mounts Cloud Storage buckets that you have access to in the `/gcs/` directory of each training node's file system. As a convenient alternative to using the Python Client for Cloud Storage or another library to access Cloud Storage, you can read and write directly to the local file system in order to read data from Cloud Storage or write data to Cloud Storage. For example, to load data from `gs://` `` `/data.csv` , you can use the following Python code:\n```\nfile = open('/gcs/BUCKET/data.csv', 'r')\n```\nVertex AI uses [Cloud Storage FUSE](/storage/docs/gcs-fuse) to mount the storage buckets. Note that [directories mounted by Cloud Storage FUSE are not POSIX compliant](/storage/docs/gcs-fuse#notes) .\nThe credentials that you are using for custom training determine which buckets you can access in this way. The preceding section about [which resources your code can access](#which-resources) describes exactly which buckets you can access by default and how to customize this access.\n### Load input data\nML code usually operates on training data in order to train a model. Don't store training data together with your code, whether you create a Python training application or a custom container image. Storing data with code can lead to a poorly organized project, make it difficult to reuse code on different datasets, and cause errors for large datasets.\nYou can load data from a [Vertex AI managed dataset](/vertex-ai/docs/training/using-managed-datasets) or write your own code to load data from a source outside of Vertex AI, such as BigQuery or Cloud Storage.\nFor best performance when you load data from Cloud Storage, use a bucket in the [region where you areperforming custom training](/vertex-ai/docs/general/locations) . To learn how to store data in Cloud Storage, read [Creating storagebuckets](/storage/docs/creating-buckets) and [Uploadingobjects](/storage/docs/uploading-objects) .\nTo learn about which Cloud Storage buckets you can load data from, read the previous section about [which resources your code can access](#which-resources) .\nTo load data from Cloud Storage in your training code, use the [Cloud Storage FUSE feature described in the preceding section](#fuse) , or use any library that supports ADC. You don't need to explicitly provide any authentication credentials in your code.\nFor example, you can use one of the client libraries demonstrated in the Cloud Storage guide to [Downloadingobjects](/storage/docs/downloading-objects#storage-download-object-python) . The [Python Client forCloud Storage](/python/docs/reference/storage/latest) , in particular, is included in prebuilt containers. TensorFlow's [tf.io.gfile.GFileclass](https://www.tensorflow.org/api_docs/python/tf/io/gfile/GFile) also supports ADC.\nDepending on which [machinetypes](/vertex-ai/docs/training/configure-compute#specifying_machine_types) you plan to use during custom training, your VMs might not be able to load the entirety of a large dataset into memory.\nIf you need to read data that is too large to fit in memory, stream the data or read it incrementally. Different ML frameworks have different best practices for doing this. For example, TensorFlow's [tf.data.Datasetclass](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) can stream TFRecord or text data from Cloud Storage.\nPerforming custom training on multiple VMs with data parallelism is another way to reduce the amount of data each VM loads into memory. See the [Writing codefor distributed training](#distributed) section of this document.\n### Export a trained ML model\nML code usually exports a trained model at the end of training in the form of one or more model artifacts. You can then use the model artifacts to get predictions.\nAfter custom training completes, you can no longer access the VMs that ran your training code. Therefore, your training code must export model artifacts to a location outside of Vertex AI.\nWe recommend that you export model artifacts to a Cloud Storage bucket. As described in the previous section about [which resources your code can access](#which-resources) , Vertex AI can access any Cloud Storage bucket in the Google Cloud project where you are performing custom training. Use a library that supports ADC to export your model artifacts. For example, the [TensorFlow APIs for saving Keras models](https://www.tensorflow.org/guide/keras/save_and_serialize) can export artifacts directly to a Cloud Storage path.\n**Note:** You can't export model artifacts from your training code in a way that directly creates a [Model resource](/vertex-ai/docs/reference/rest/v1/projects.locations.models) . However, if you perform custom training by creating a [TrainingPipelineresource](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines) , the `TrainingPipeline` can export model artifacts to Cloud Storage. The `TrainingPipeline` can then immediately import those same model artifacts back into Vertex AI as a `Model` . Learn more in the [guide to creatingcustom training pipelines](/vertex-ai/docs/training/create-training-pipeline) .\nIf you want to use your trained model to serve predictions on Vertex AI, then your code must export model artifacts in a format compatible with one of the [prebuilt containers forprediction](/vertex-ai/docs/predictions/pre-built-containers) . Learn more in the [guide toexporting model artifacts forprediction](/vertex-ai/docs/training/exporting-model-artifacts) .\n### Environment variables for special Cloud Storage directories\nTo see an example of using `AIP_MODEL_DIR` to export model artifacts as part of a more comprehensive workflow,  run the \"Custom training and online prediction\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/custom/sdk-custom-image-classification-online.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fcustom%2Fsdk-custom-image-classification-online.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/custom/sdk-custom-image-classification-online.ipynb)\nIf you specify the [baseOutputDirectory APIfield](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#FIELDS.base_output_directory) , Vertex AI sets the following environment variables when it runs your training code:\n- `AIP_MODEL_DIR`: a Cloud Storage URI of a directory intended for [saving model artifacts](#export) .\n- `AIP_CHECKPOINT_DIR`: a Cloud Storage URI of a directory intended for [saving checkpoints](#resilience) .\n- `AIP_TENSORBOARD_LOG_DIR`: a Cloud Storage URI of a directory intended for saving [TensorBoard](https://www.tensorflow.org/tensorboard) logs. See [Using Vertex AI TensorBoard with customtraining](/vertex-ai/docs/experiments/tensorboard-training) .\nThe values of these environment variables differ slightly depending on whether you are using hyperparameter tuning. To learn more, see the [API reference forbaseOutputDirectory](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#FIELDS.base_output_directory) .\nUsing these environment variables makes it easier to reuse the same training code multiple times\u2014for example with different data or configuration options\u2014and save model artifacts and checkpoints to different locations, just by changing the `baseOutputDirectory` API field. However, you are not required to use the environment variables in your code if you don't want to. For example, you can alternatively hardcode locations for saving checkpoints and exporting model artifacts.\nAdditionally, if you [use a TrainingPipeline for customtraining](/vertex-ai/docs/training/create-training-pipeline) and don't specify the [modelToUpload.artifactUrifield](/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model.FIELDS.artifact_uri) , then Vertex AI uses the value of the `AIP_MODEL_DIR` environment variable for `modelToUpload.artifactUri` . (For hyperparameter tuning, Vertex AI uses the value of the `AIP_MODEL_DIR` environment variable from the best trial.)\n### Ensure resilience to restarts\nThe VMs that run your training code restart occasionally. For example, Google Cloud might need to restart a VM for maintenance reasons. When a VM restarts, Vertex AI starts running your code again from its start.\nIf you expect your training code to run for more than four hours, add several behaviors to your code to make it resilient to restarts:\n- Frequently export your training progress to Cloud Storage, at least once every four hours, so that you don't lose progress if your VMs restart.\n- At the start of your training code, check whether any training progress already exists in your export location. If so, load the saved training state instead of starting training from scratch.\nFour hours is a guideline, not a hard limit. If ensuring resilience is a priority, consider adding these behaviors to your code even if you don't expect it to run for that long.\nHow to accomplish these behaviors depends on which ML framework you use. For example, if you use TensorFlow Keras, [learn how to use the ModelCheckpointcallback for thispurpose](https://www.tensorflow.org/guide/keras/writing_your_own_callbacks) .\nTo learn more about how Vertex AI manages VMs, see [Understand the custom training service](/vertex-ai/docs/training/understanding-training-service) .\n## Best practices for optional custom training features\nIf you want to use certain optional custom training features, you might need to make additional changes to your training code. This section describes code best practices for hyperparameter tuning, GPUs, distributed training, and Vertex AI TensorBoard.\n### Write code to enable autologging\nYou can enable auotologging using the Vertex AI SDK for Python to automatically capture parameters and performance metrics when submitting the custom job. For details, see [Run training job with experiment tracking](/vertex-ai/docs/experiments/run-training-job-experiments) .\nTo see an example of how to create a custom job with autologging enabled,  run the \"Vertex AI Experiments: Autologging\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/get_started_with_vertex_experiments_autologging.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fexperiments%2Fget_started_with_vertex_experiments_autologging.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/get_started_with_vertex_experiments_autologging.ipynb)\n### Write code to return container logs\nWhen you write logs from your service or job, they will be picked up automatically by Cloud Logging so long as the logs are written to any of these locations:\n- [Standard output (stdout) or standard error (stderr) streams](https://en.wikipedia.org/wiki/Standard_streams) \n- Any files under the`/var/log`directory\n- syslog (`/dev/log`)\n- Logs written using [Cloud Logging client libraries](/logging/docs/reference/libraries) , which are available for many popular languages\nMost developers are expected to write logs using standard output and standard error.\nThe container logs written to these supported locations are automatically associated with the Vertex AI custom training service, revision, and location, or with the custom training job. Exceptions contained in these logs are captured by and reported in [Error Reporting](/run/docs/error-reporting) .\nWhen you write logs, you can send a simple text string or send a single line of serialized JSON, also called \"structured\" data. This is picked up and parsed by Cloud Logging and is placed into `jsonPayload` . In contrast, the simple text message is placed in `textPayload` .\nYou can pass structured JSON logs in multiple ways. The most common ways are by using the [Python Logging library](https://docs.python.org/3/library/logging.html) or by passing raw JSON using `print` .\n```\nimport jsonimport loggingfrom pythonjsonlogger import jsonloggerclass CustomJsonFormatter(jsonlogger.JsonFormatter):\u00a0\"\"\"Formats log lines in JSON.\"\"\"\u00a0 def process_log_record(self, log_record):\u00a0 \u00a0 \"\"\"Modifies fields in the log_record to match Cloud Logging's expectations.\"\"\"\u00a0 \u00a0 log_record['severity'] = log_record['levelname']\u00a0 \u00a0 log_record['timestampSeconds'] = int(log_record['created'])\u00a0 \u00a0 log_record['timestampNanos'] = int(\u00a0 \u00a0 \u00a0 \u00a0 (log_record['created'] % 1) * 1000 * 1000 * 1000)\u00a0 \u00a0 return log_recorddef configure_logger():\u00a0 \"\"\"Configures python logger to format logs as JSON.\"\"\"\u00a0 formatter = CustomJsonFormatter(\u00a0 \u00a0 \u00a0 \u00a0 '%(name)s|%(levelname)s|%(message)s|%(created)f'\u00a0 \u00a0 \u00a0 \u00a0 '|%(lineno)d|%(pathname)s', '%Y-%m-%dT%H:%M:%S')\u00a0 root_logger = logging.getLogger()\u00a0 handler = logging.StreamHandler()\u00a0 handler.setFormatter(formatter)\u00a0 root_logger.addHandler(handler)\u00a0 root_logger.setLevel(logging.WARNING)logging.warning(\"This is a warning log\")\n``````\nimport jsondef log(severity, message):\u00a0 global_extras = {\"debug_key\": \"debug_value\"}\u00a0 structured_log = {\"severity\": severity, \"message\": message, **global_extras}\u00a0 print(json.dumps(structured_log))def main(args):\u00a0 log(\"DEBUG\", \"Debugging the application.\")\u00a0 log(\"INFO\", \"Info.\")\u00a0 log(\"WARNING\", \"Warning.\")\u00a0 log(\"ERROR\", \"Error.\")\u00a0 log(\"CRITICAL\", \"Critical.\")\n```\nWhen you provide a structured log as a JSON dictionary, some special fields are stripped from the `jsonPayload` and are written to the corresponding field in the generated [LogEntry](/logging/docs/reference/v2/rest/v2/LogEntry) as described in the documentation for [special fields](/logging/docs/agent/configuration#special-fields) .\nFor example, if your JSON includes a `severity` property, it is removed from the `jsonPayload` and appears instead as the log entry's `severity` . The `message` property is used as the main display text of the log entry if present.\nIn the Logs Explorer, logs correlated by the same `trace` are viewable in \"parent-child\" format: when you click the triangle icon at the left of the request log entry, the container logs related to that request show up nested under the request log.\nContainer logs are not automatically correlated to request logs unless you use a [Cloud Logging client library](/logging/docs/reference/libraries) . To correlate container logs with request logs without using a client library, you can use a structured JSON log line that contains a `logging.googleapis.com/trace` field with the trace identifier extracted from the `X-Cloud-Trace-Context` header.\nTo view your container logs in the Google Cloud console, do the following:\n- In the Google Cloud console, go to the **Vertex AI custom jobs** page. [Go to Custom jobs](https://console.cloud.google.com/vertex-ai/training/custom-jobs) \n- Click the name of the custom job that you want to see logs for.\n- Click **View logs** .\n### Write code for hyperparameter tuning\nVertex AI can perform hyperparameter tuning on your ML training code. Learn more about [how hyperparameter tuning on Vertex AIworks](/vertex-ai/docs/training/hyperparameter-tuning-overview) and [how to configure aHyperparameterTuningJobresource](/vertex-ai/docs/training/using-hyperparameter-tuning) .\nIf you want to use hyperparameter tuning, your training code must do the following:\n- Parse command-line arguments representing the hyperparameters that you want to tune, and use the parsed values to set the hyperparameters for training.\n- Intermittently report the hyperparameter tuning metric to Vertex AI.For hyperparameter tuning, Vertex AI runs your training code multiple times, with different command-line arguments each time. Your training code must parse these command-line arguments and use them as hyperparameters for training. For example, to tune your optimizer's [learningrate](https://developers.google.com/machine-learning/glossary#learning-rate) , you might want to parse a command-line argument named `--learning_rate` . Learn [how to configure which command-line arguments Vertex AIprovides](/vertex-ai/docs/training/using-hyperparameter-tuning#command-line-arguments) .\nWe recommend that you use Python's [argparselibrary](https://docs.python.org/3/library/argparse.html) to parse command-line arguments.\nYour training code must intermittently report the hyperparameter metric that you are trying to optimize to Vertex AI. For example, if you want to maximize your model's accuracy, you might want to report this metric at the end of every training epoch. Vertex AI uses this information to decide what hyperparameters to use for the next training trial. Learn more about [selecting and specifying a hyperparameter tuningmetric](/vertex-ai/docs/training/hyperparameter-tuning-overview#what_hyperparameter_tuning_optimizes) .\nUse the `cloudml-hypertune` Python library to report the hyperparameter tuning metric. This library is included in all [prebuilt containers fortraining](/vertex-ai/docs/training/pre-built-containers) , and you can use `pip` to install it in a custom container.\nTo learn how to install and use this library, see [the cloudml-hypertuneGitHubrepository](https://github.com/GoogleCloudPlatform/cloudml-hypertune) , or refer to the [Vertex AI: Hyperparameter Tuning codelab](https://codelabs.developers.google.com/vertex_hyperparameter_tuning#0) .\n### Write code for GPUs\nYou can select VMs with graphics processing units (GPUs) to run your custom training code. Learn more about [configuring custom training to use GPU-enabledVMs](/vertex-ai/docs/training/configure-compute#specifying_gpus) .\nIf you want to train with GPUs, make sure your training code can take advantage of them. Depending on which ML framework you use, this might require changes to your code. For example, if you use TensorFlow Keras, [you only need to adjustyour code if you want to use more than oneGPU](https://www.tensorflow.org/guide/gpu) . Some ML frameworks can't use GPUs at all.\nIn addition, make sure that your container supports GPUs: Select a [prebuiltcontainer for training](/vertex-ai/docs/training/pre-built-containers) that supports GPUs, or install the [NVIDIA CUDAToolkit](https://developer.nvidia.com/cuda-toolkit) and [NVIDIAcuDNN](https://developer.nvidia.com/cudnn) on your custom container. One way to do this is to use base image from the [nvidia/cuda Dockerrepository](https://hub.docker.com/r/nvidia/cuda/) ; another way is to [use a Deep Learning Containers instance as your baseimage](/deep-learning-containers/docs/derivative-container) .\n### Write code for distributed training\nTo train on large datasets, you can run your code on multiple VMs in a distributed cluster managed by Vertex AI. Learn how to [configuremultiple VMs for training](/vertex-ai/docs/training/distributed-training) .\nSome ML frameworks, like [TensorFlow](https://www.tensorflow.org/guide/distributed_training#multiworkermirroredstrategy) and [PyTorch](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) , let you run identical training code on multiple machines which automatically coordinate how to divide the work based on environment variables set on each machine. Find out if Vertex AI [setsenvironment variables to make this possible for your MLframework](/vertex-ai/docs/training/distributed-training#cluster-variables) .\nAlternatively, you can run a different container on each of several . A worker pool is a group of VMs that you configure to use the same compute options and container. In this case, you still probably want to rely on the environment variables set by Vertex AI to coordinate communication between the VMs. You can customize the training code of each worker pool to perform whatever arbitrary tasks you want; how you do this depends on your goal and which ML framework you use.\n### Track and visualize custom training experiments using Vertex AI TensorBoard\n[Vertex AI TensorBoard](/vertex-ai/docs/experiments/tensorboard-introduction) is a managed version of [TensorBoard](https://www.tensorflow.org/tensorboard/get_started) , a Google open source project for visualizing machine learning experiments. With Vertex AI TensorBoard you can track, visualize, and compare ML experiments and then share them with your team. You can also use [TensorBoard Profiler](/vertex-ai/docs/training/tensorboard-profiler) to pinpoint and fix performance bottlenecks to train models faster and cheaper.\nTo use Vertex AI TensorBoard with custom training, you must do the following:\n- Create a Vertex AI TensorBoard instance in your project to store your experiments (see [Create a TensorBoard instance](/vertex-ai/docs/experiments/tensorboard-setup#create-tensorboard-instance) ).\n- Configure a service account to run the custom training job with appropriate permissions.\n- Adjust your custom training code to write out TensorBoard compatible logs to Cloud Storage (see [Changes to your training script](/vertex-ai/docs/experiments/tensorboard-training#script_changes) )\nFor a step-by-step guide, see [Using Vertex AI TensorBoard with custom training](/vertex-ai/docs/experiments/tensorboard-training) .\n## What's next\n- Learn the details of [creating a Python training application to use with aprebuilt container](/vertex-ai/docs/training/create-python-pre-built-container) or [creating a custom container image](/vertex-ai/docs/training/create-custom-container) .\n- If you aren't sure that you want to perform custom training, read a [comparison of custom training andAutoML](/vertex-ai/docs/start/training-methods) .", "guide": "Vertex AI"}