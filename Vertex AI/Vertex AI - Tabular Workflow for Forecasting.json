{"title": "Vertex AI - Tabular Workflow for Forecasting", "url": "https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/forecasting", "abstract": "# Vertex AI - Tabular Workflow for Forecasting\n**    Preview     ** This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nThis document provides an overview of the Tabular Workflow for Forecasting [pipeline and components](#components) . To learn how to train a model, see [Train a model with Tabular Workflow for Forecasting](/vertex-ai/docs/tabular-data/tabular-workflows/forecasting-train) .\nTabular Workflow for Forecasting is the complete pipeline for forecasting tasks. It is similar to the [AutoML API](/vertex-ai/docs/tabular-data/forecasting/overview) , but allows you to choose what to control and what to automate. Instead of having controls for the pipeline, you have controls for in the pipeline. These pipeline controls include:\n- Data splitting\n- Feature engineering\n- Architecture search\n- Model training\n- Model ensembling\n", "content": "## Benefits\nThe following are some of the benefits of Tabular Workflow for Forecasting :- Supports **large datasets** that are up to 1TB in size and have up to 200 columns.\n- Allows you to **improve stability and lower training time** by limiting the search space of architecture types or skipping architecture search.\n- Allows you to **improve training speed** by manually selecting the hardware used for training and architecture search.\n- For some model training methods, allows you to **reduce model size and improve latency** by changing the ensemble size.\n- Each component can be inspected in a powerful pipelines graph interface that lets you see the transformed data tables, evaluated model architectures and many more details.\n- Each component gets extended flexibility and transparency, such as being able to customize parameters, hardware, view process status, logs and more.\n## Forecasting on Vertex AI Pipelines\nTabular Workflow for Forecasting is a managed instance of Vertex AI Pipelines.\n[Vertex AI Pipelines](/vertex-ai/docs/pipelines/introduction) is a serverless service that runs Kubeflow pipelines. You can use pipelines to automate and monitor your machine learning and data preparation tasks. Each step in a pipeline performs part of the pipeline's workflow. For example, a pipeline can include steps to split data, transform data types, and train a model. Since steps are instances of pipeline components, steps have inputs, outputs, and a container image. Step inputs can be set from the pipeline's inputs or they can depend on the output of other steps within this pipeline. These dependencies define the pipeline's workflow as a directed acyclic graph.\n## Overview of pipeline and components\nThe following diagram shows the modeling pipeline for Tabular Workflow for Forecasting :\nThe pipeline components are:\n- **feature-transform-engine** : Perform feature engineering. See [Feature Transform Engine](/vertex-ai/docs/tabular-data/tabular-workflows/feature-engineering) for details.\n- **training-configurator-and-validator** : Validate the training configuration and generate the training metadata.Input:- `instance_schema`: Instance schema in OpenAPI specification, which describes the data types of the prediction data.\n- `dataset_stats`: Statistics that describe the raw dataset. For example,`dataset_stats`gives the number of rows in the dataset.\n- `training_schema`: Training data schema in OpenAPI specification, which describes the data types of the training data.\n- **split-materialized-data** : Split the materialized data into a training set, an evaluation set, and a test set.Input:- `materialized_data`: Materialized data.\nOutput:- `materialized_train_split`: Materialized training split.\n- `materialized_eval_split`: Materialized evaluation split.\n- `materialized_test_split`: Materialized test set.\n- **calculate-training-parameters-2** : Calculate the expected runtime duration for **automl-forecasting-stage-1-tuner** .\n- **get-hyperparameter-tuning-results** - **Optional** : If you configured the pipeline to skip the architecture search, load the hyperparameter tuning results from a previous pipeline run.\n- Perform model architecture search and tune hyperparameters ( **automl-forecasting-stage-1-tuner** ) or use the hyperparameter tuning results from a previous pipeline run ( **automl-forecasting-stage-2-tuner** ).- An architecture is defined by a set of hyperparameters.\n- Hyperparameters include the model type and the model parameters.\n- Model types considered are neural networks and boosted trees.\n- A model is trained for each architecture considered.\nInput:- `materialized_train_split`: Materialized training split.\n- `materialized_eval_split`: Materialized evaluation split.\n- `artifact`- Hyperparameter tuning results from a previous pipeline run. This artifact is an input only if you configured the pipeline to skip the architecture search.\nOutput:- `tuning_result_output`: Tuning output.\n- **get-prediction-image-uri-2** : Produce the correct prediction image URI based on the [model type](/vertex-ai/docs/tabular-data/forecasting/train-model#training-methods) .\n- **automl-forecasting-ensemble-2** : Ensemble the best architectures to produce a final model.Input:- `tuning_result_output`: Tuning output.\nOutput:- `unmanaged_container_model`: Output model.\n- **model-upload-2** - Upload the model.Input:- `unmanaged_container_model`: Output model.\nOutput:- `model`: Vertex AI model.\n- **should_run_model_evaluation** - **Optional** : Use the test set to calculate evaluation metrics.## What's next\n- [Train a model using Tabular Workflow for Forecasting](/vertex-ai/docs/tabular-data/tabular-workflows/forecasting-train) .", "guide": "Vertex AI"}