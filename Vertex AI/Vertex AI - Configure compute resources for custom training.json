{"title": "Vertex AI - Configure compute resources for custom training", "url": "https://cloud.google.com/vertex-ai/docs/training/configure-compute", "abstract": "# Vertex AI - Configure compute resources for custom training\nWhen you perform custom training, your training code runs on one or more virtual machine (VM) instances. You can configure what types of VM to use for training: using VMs with more compute resources can speed up training and let you work with larger datasets, but they can also incur greater [trainingcosts](/vertex-ai/pricing) .\nIn some cases, you can additionally use GPUs to accelerate training. GPUs incur additional costs.\nYou can also optionally customize the type and size of your training VMs' boot disks.\nThis document describes the different compute resources that you can use for custom training and how to configure them.\n", "content": "## Where to specify compute resources\nSpecify configuration details within a [WorkerPoolSpec](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#workerpoolspec) . Depending on how you perform custom training, put this `WorkerPoolSpec` in one of the following API fields:\n- **If you are creating a CustomJobresource,** specify the `WorkerPoolSpec` in `CustomJob.jobSpec.workerPoolSpecs` .If you are using the Google Cloud CLI, then you can use the `--worker-pool-spec` flag or the `--config` flag on the [gcloud ai custom-jobs createcommand](/sdk/gcloud/reference/ai/custom-jobs/create) to specify worker pool options.Learn more about [creating a CustomJob](/vertex-ai/docs/training/create-custom-job) .\n- **If you are creating a HyperparameterTuningJobresource,** specify the `WorkerPoolSpec` in `HyperparameterTuningJob.trialJobSpec.workerPoolSpecs` .If you are using the gcloud CLI, then you can use the `--config` flag on the [gcloud ai hpt-tuning-jobs createcommand](/sdk/gcloud/reference/ai/hp-tuning-jobs/create) to specify worker pool options.Learn more about [creating aHyperparameterTuningJob](/vertex-ai/docs/training/using-hyperparameter-tuning) .\n- **If you are creating a TrainingPipelineresource withouthyperparameter tuning,** specify the `WorkerPoolSpec` in `TrainingPipeline.trainingTaskInputs.workerPoolSpecs` .Learn more about [creating a customTrainingPipeline](/vertex-ai/docs/training/create-training-pipeline) .\n- **If you are creating a TrainingPipeline with hyperparameter tuning** , specify the `WorkerPoolSpec` in `TrainingPipeline.trainingTaskInputs.trialJobSpec.workerPoolSpecs` .\nIf you are performing [distributedtraining](/vertex-ai/docs/training/code-requirements#distributed) , you can use different settings for each worker pool.\n## Machine types\nIn your `WorkerPoolSpec` , you must specify one of the following machine types in the [machineSpec.machineType field](/vertex-ai/docs/reference/rest/v1/MachineSpec) . Each replica in the worker pool runs on a separate VM that has the specified machine type.\n* Machine types marked with asterisks in the preceding list must be used with certain GPUs or TPUs. See the following sections of this guide.\nTo learn about the technical specifications of each machine type, read the [Compute Engine documentation about machinetypes](/compute/docs/machine-types) . To learn about the cost of using each machine type for custom training, read [Pricing](/vertex-ai/pricing) .\nThe following examples highlight where you specify a machine type when you create a `CustomJob` :\nIn the Google Cloud console, you can't create a `CustomJob` directly. However, you can [create a TrainingPipeline that creates aCustomJob](/vertex-ai/docs/training/create-custom-job#create) . When you create a `TrainingPipeline` in the Google Cloud console, specify a machine type for each worker pool on the **Compute and pricing** step, in the **Machine type** field.```\ngcloud ai custom-jobs create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=JOB_NAME \\\u00a0 --worker-pool-spec=machine-type=MACHINE_TYPE,replica-count=REPLICA_COUNT,container-image-uri=CUSTOM_CONTAINER_IMAGE_URI\n```Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateCustomJobSample.java) \n```\nimport com.google.cloud.aiplatform.v1.AcceleratorType;import com.google.cloud.aiplatform.v1.ContainerSpec;import com.google.cloud.aiplatform.v1.CustomJob;import com.google.cloud.aiplatform.v1.CustomJobSpec;import com.google.cloud.aiplatform.v1.JobServiceClient;import com.google.cloud.aiplatform.v1.JobServiceSettings;import com.google.cloud.aiplatform.v1.LocationName;import com.google.cloud.aiplatform.v1.MachineSpec;import com.google.cloud.aiplatform.v1.WorkerPoolSpec;import java.io.IOException;// Create a custom job to run machine learning training code in Vertex AIpublic class CreateCustomJobSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"PROJECT\";\u00a0 \u00a0 String displayName = \"DISPLAY_NAME\";\u00a0 \u00a0 // Vertex AI runs your training application in a Docker container image. A Docker container\u00a0 \u00a0 // image is a self-contained software package that includes code and all dependencies. Learn\u00a0 \u00a0 // more about preparing your training application at\u00a0 \u00a0 // https://cloud.google.com/vertex-ai/docs/training/overview#prepare_your_training_application\u00a0 \u00a0 String containerImageUri = \"CONTAINER_IMAGE_URI\";\u00a0 \u00a0 createCustomJobSample(project, displayName, containerImageUri);\u00a0 }\u00a0 static void createCustomJobSample(String project, String displayName, String containerImageUri)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 JobServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 JobServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests.\u00a0 \u00a0 try (JobServiceClient client = JobServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 MachineSpec machineSpec =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MachineSpec.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMachineType(\"n1-standard-4\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAcceleratorType(AcceleratorType.NVIDIA_TESLA_K80)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAcceleratorCount(1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 ContainerSpec containerSpec =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ContainerSpec.newBuilder().setImageUri(containerImageUri).build();\u00a0 \u00a0 \u00a0 WorkerPoolSpec workerPoolSpec =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 WorkerPoolSpec.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMachineSpec(machineSpec)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setReplicaCount(1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setContainerSpec(containerSpec)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 CustomJobSpec customJobSpecJobSpec =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CustomJobSpec.newBuilder().addWorkerPoolSpecs(workerPoolSpec).build();\u00a0 \u00a0 \u00a0 CustomJob customJob =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CustomJob.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(displayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setJobSpec(customJobSpecJobSpec)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 LocationName parent = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 CustomJob response = client.createCustomJob(parent, customJob);\u00a0 \u00a0 \u00a0 System.out.format(\"response: %s\\n\", response);\u00a0 \u00a0 \u00a0 System.out.format(\"Name: %s\\n\", response.getName());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/create-custom-job.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const customJobDisplayName = 'YOUR_CUSTOM_JOB_DISPLAY_NAME';// const containerImageUri = 'YOUR_CONTAINER_IMAGE_URI';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Job Service Client libraryconst {JobServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst jobServiceClient = new JobServiceClient(clientOptions);async function createCustomJob() {\u00a0 // Configure the parent resource\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 const customJob = {\u00a0 \u00a0 displayName: customJobDisplayName,\u00a0 \u00a0 jobSpec: {\u00a0 \u00a0 \u00a0 workerPoolSpecs: [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 machineSpec: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 machineType: 'n1-standard-4',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 acceleratorType: 'NVIDIA_TESLA_K80',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 acceleratorCount: 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 replicaCount: 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containerSpec: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 imageUri: containerImageUri,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command: [],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 args: [],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 },\u00a0 };\u00a0 const request = {parent, customJob};\u00a0 // Create custom job request\u00a0 const [response] = await jobServiceClient.createCustomJob(request);\u00a0 console.log('Create custom job response:\\n', JSON.stringify(response));}createCustomJob();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/snippets/job_service/create_custom_job_sample.py) \n```\nfrom google.cloud import aiplatformdef create_custom_job_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 container_image_uri: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",):\u00a0 \u00a0 # The AI Platform services require regional API endpoints.\u00a0 \u00a0 client_options = {\"api_endpoint\": api_endpoint}\u00a0 \u00a0 # Initialize client that will be used to create and send requests.\u00a0 \u00a0 # This client only needs to be created once, and can be reused for multiple requests.\u00a0 \u00a0 client = aiplatform.gapic.JobServiceClient(client_options=client_options)\u00a0 \u00a0 custom_job = {\u00a0 \u00a0 \u00a0 \u00a0 \"display_name\": display_name,\u00a0 \u00a0 \u00a0 \u00a0 \"job_spec\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"worker_pool_specs\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"machine_spec\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"machine_type\": \"n1-standard-4\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"accelerator_type\": aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"accelerator_count\": 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"replica_count\": 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"container_spec\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"image_uri\": container_image_uri,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"command\": [],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"args\": [],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 }\u00a0 \u00a0 parent = f\"projects/{project}/locations/{location}\"\u00a0 \u00a0 response = client.create_custom_job(parent=parent, custom_job=custom_job)\u00a0 \u00a0 print(\"response:\", response)\n```\nFor more context, read the [guide to creating aCustomJob](/vertex-ai/docs/training/create-custom-job) .\n## GPUs\nIf you have [written your training code to useGPUs](/vertex-ai/docs/training/code-requirements#gpus) , then you may configure your worker pool to use one or more GPUs on each VM. To use GPUs, you must use an A2, N1, or G2 machine type. Additionally, using smaller machines types like `n1-highmem-2` with GPUs might cause logging to fail for some workloads because of CPU constraints. If your training job stops returning logs, consider selecting a larger machine type.\nVertex AI supports the following types of GPU for custom training:\n**Note:** In June 2022, NVIDIA announced the upcoming end of support (EOS) of the NVIDIA K80 GPU model. For Google Cloud, NVIDIA K80 GPUs will reach EOS on May 1, 2024.\n- `NVIDIA_A100_80GB`\n- `NVIDIA_TESLA_A100`(NVIDIA A100 40GB)\n- `NVIDIA_TESLA_K80`\n- `NVIDIA_TESLA_P4`\n- `NVIDIA_TESLA_P100`\n- `NVIDIA_TESLA_T4`\n- `NVIDIA_TESLA_V100`\n- `NVIDIA_L4`\nTo learn more about the technical specification for each type of GPU, read the [Compute Engine short documentation about GPUs for computeworkloads](/compute/docs/gpus#gpus-list) . To learn about the cost of using each machine type for custom training, read [Pricing](/vertex-ai/pricing) .\nIn your `WorkerPoolSpec` , specify the type of GPU that you want to use in the [machineSpec.acceleratorType field](/vertex-ai/docs/reference/rest/v1/MachineSpec) and number of GPUs that you want each VM in the worker pool to use in the [machineSpec.acceleratorCount field](/vertex-ai/docs/reference/rest/v1/MachineSpec) . However, your choices for these fields must meet the following restrictions:\n- The type of GPU that you choose must be available in the location where you are performing custom training. Not all types of GPU are available in all regions. [Learn about regional availability](/vertex-ai/docs/general/locations#accelerators) .\n- You can only use certain numbers of GPUs in your configuration. For example, you can use 2 or 4 `NVIDIA_TESLA_T4` GPUs on a VM, but not 3. To see what `acceleratorCount` values are valid for each type of GPU, see the [following compatibility table](#gpu-compatibility-table) .\n- You must make sure that your GPU configuration provides sufficient virtual CPUs and memory to the machine type that you use it with. For example, if you use the `n1-standard-32` machine type in your worker pool, then each VM has 32 virtual CPUs and 120 GB of memory. Since each `NVIDIA_TESLA_V100` GPU can provide up to 12 virtual CPUs and 76 GB of memory, you must use at least 4 GPUs for each `n1-standard-32` VM to support its requirements. (2 GPUs provide insufficient resources, and you can't specify 3 GPUs.)The [following compatibility table](#gpu-compatibility-table) accounts for this requirement.Note the following additional limitations on using GPUs for custom training that differ from using GPUs with Compute Engine:- A configuration with 8`NVIDIA_TESLA_K80`GPUs only provides up to 208 GB of memory inregions and zones.\n- A configuration with 4`NVIDIA_TESLA_P100`GPUs only provides up to 64 virtual CPUS and up to 208 GB of memory inregions and zones.The following compatibility table lists the valid values for `machineSpec.acceleratorCount` depending on your choices for `machineSpec.machineType` and `machineSpec.acceleratorType` :\n| ('Valid numbers of GPUs for each machine type', 'Machine type') | ('Valid numbers of GPUs for each machine type', 'NVIDIA_A100_80GB') | ('Valid numbers of GPUs for each machine type', 'NVIDIA_TESLA_A100') | ('Valid numbers of GPUs for each machine type', 'NVIDIA_TESLA_K80') | ('Valid numbers of GPUs for each machine type', 'NVIDIA_TESLA_P4') | ('Valid numbers of GPUs for each machine type', 'NVIDIA_TESLA_P100') | ('Valid numbers of GPUs for each machine type', 'NVIDIA_TESLA_T4') | ('Valid numbers of GPUs for each machine type', 'NVIDIA_TESLA_V100') | ('Valid numbers of GPUs for each machine type', 'NVIDIA_L4') |\n|:------------------------------------------------------------------|----------------------------------------------------------------------:|-----------------------------------------------------------------------:|:----------------------------------------------------------------------|:---------------------------------------------------------------------|:-----------------------------------------------------------------------|:---------------------------------------------------------------------|:-----------------------------------------------------------------------|---------------------------------------------------------------:|\n| a2-ultragpu-1g             |                  1 |                 nan | nan                 | nan                 | nan                 | nan                 | nan                 |               nan |\n| a2-ultragpu-2g             |                  2 |                 nan | nan                 | nan                 | nan                 | nan                 | nan                 |               nan |\n| a2-ultragpu-4g             |                  4 |                 nan | nan                 | nan                 | nan                 | nan                 | nan                 |               nan |\n| a2-ultragpu-8g             |                  8 |                 nan | nan                 | nan                 | nan                 | nan                 | nan                 |               nan |\n| a2-highgpu-1g              |                 nan |                  1 | nan                 | nan                 | nan                 | nan                 | nan                 |               nan |\n| a2-highgpu-2g              |                 nan |                  2 | nan                 | nan                 | nan                 | nan                 | nan                 |               nan |\n| a2-highgpu-4g              |                 nan |                  4 | nan                 | nan                 | nan                 | nan                 | nan                 |               nan |\n| a2-highgpu-8g              |                 nan |                  8 | nan                 | nan                 | nan                 | nan                 | nan                 |               nan |\n| a2-megagpu-16g             |                 nan |                  16 | nan                 | nan                 | nan                 | nan                 | nan                 |               nan |\n| n1-standard-4              |                 nan |                 nan | 1, 2, 4, 8               | 1, 2, 4                | 1, 2, 4                | 1, 2, 4                | 1, 2, 4, 8                |               nan |\n| n1-standard-8              |                 nan |                 nan | 1, 2, 4, 8               | 1, 2, 4                | 1, 2, 4                | 1, 2, 4                | 1, 2, 4, 8                |               nan |\n| n1-standard-16             |                 nan |                 nan | 2, 4, 8                | 1, 2, 4                | 1, 2, 4                | 1, 2, 4                | 2, 4, 8                |               nan |\n| n1-standard-32             |                 nan |                 nan | 4, 8                 | 2, 4                 | 2, 4                 | 2, 4                 | 4, 8                 |               nan |\n| n1-standard-64             |                 nan |                 nan | nan                 | 4                 | nan                 | 4                 | 8                  |               nan |\n| n1-standard-96             |                 nan |                 nan | nan                 | 4                 | nan                 | 4                 | 8                  |               nan |\n| n1-highmem-2              |                 nan |                 nan | 1, 2, 4, 8               | 1, 2, 4                | 1, 2, 4                | 1, 2, 4                | 1, 2, 4, 8                |               nan |\n| n1-highmem-4              |                 nan |                 nan | 1, 2, 4, 8               | 1, 2, 4                | 1, 2, 4                | 1, 2, 4                | 1, 2, 4, 8                |               nan |\n| n1-highmem-8              |                 nan |                 nan | 1, 2, 4, 8               | 1, 2, 4                | 1, 2, 4                | 1, 2, 4                | 1, 2, 4, 8                |               nan |\n| n1-highmem-16              |                 nan |                 nan | 2, 4, 8                | 1, 2, 4                | 1, 2, 4                | 1, 2, 4                | 2, 4, 8                |               nan |\n| n1-highmem-32              |                 nan |                 nan | 4, 8                 | 2, 4                 | 2, 4                 | 2, 4                 | 4, 8                 |               nan |\n| n1-highmem-64              |                 nan |                 nan | nan                 | 4                 | nan                 | 4                 | 8                  |               nan |\n| n1-highmem-96              |                 nan |                 nan | nan                 | 4                 | nan                 | 4                 | 8                  |               nan |\n| n1-highcpu-16              |                 nan |                 nan | 2, 4, 8                | 1, 2, 4                | 1, 2, 4                | 1, 2, 4                | 2, 4, 8                |               nan |\n| n1-highcpu-32              |                 nan |                 nan | 4, 8                 | 2, 4                 | 2, 4                 | 2, 4                 | 4, 8                 |               nan |\n| n1-highcpu-64              |                 nan |                 nan | 8                  | 4                 | 4                  | 4                 | 8                  |               nan |\n| n1-highcpu-96              |                 nan |                 nan | nan                 | 4                 | nan                 | 4                 | 8                  |               nan |\n| g2-standard-4              |                 nan |                 nan | nan                 | nan                 | nan                 | nan                 | nan                 |                1 |\n| g2-standard-8              |                 nan |                 nan | nan                 | nan                 | nan                 | nan                 | nan                 |                1 |\n| g2-standard-12             |                 nan |                 nan | nan                 | nan                 | nan                 | nan                 | nan                 |                1 |\n| g2-standard-16             |                 nan |                 nan | nan                 | nan                 | nan                 | nan                 | nan                 |                1 |\n| g2-standard-24             |                 nan |                 nan | nan                 | nan                 | nan                 | nan                 | nan                 |                2 |\n| g2-standard-32             |                 nan |                 nan | nan                 | nan                 | nan                 | nan                 | nan                 |                1 |\n| g2-standard-48             |                 nan |                 nan | nan                 | nan                 | nan                 | nan                 | nan                 |                4 |\n| g2-standard-96             |                 nan |                 nan | nan                 | nan                 | nan                 | nan                 | nan                 |                8 |\nThe following examples highlight where you can specify GPUs when you create a `CustomJob` :\nIn the Google Cloud console, you can't create a `CustomJob` directly. However, you can [create a TrainingPipeline that creates aCustomJob](/vertex-ai/docs/training/create-custom-job#create) . When you create a `TrainingPipeline` in the Google Cloud console, you can specify GPUs for each worker pool on the **Compute and pricing** step. First specify a **Machinetype** . Then, you can specify GPU details in the **Accelerator type** and **Accelerator count** fields.To specify GPUs using the Google Cloud CLI tool, you must use a [config.yamlfile](/sdk/gcloud/reference/ai/custom-jobs/create#--config) . For example:```\nworkerPoolSpecs:\u00a0 machineSpec:\u00a0 \u00a0 machineType: MACHINE_TYPE\u00a0 \u00a0 acceleratorType: ACCELERATOR_TYPE\u00a0 \u00a0 acceleratorCount: ACCELERATOR_COUNT\u00a0 replicaCount: REPLICA_COUNT\u00a0 containerSpec:\u00a0 \u00a0 imageUri: CUSTOM_CONTAINER_IMAGE_URI\n```\nThen run a command like the following:\n```\ngcloud ai custom-jobs create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=JOB_NAME \\\u00a0 --config=config.yaml\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/create-custom-job.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const customJobDisplayName = 'YOUR_CUSTOM_JOB_DISPLAY_NAME';// const containerImageUri = 'YOUR_CONTAINER_IMAGE_URI';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Job Service Client libraryconst {JobServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst jobServiceClient = new JobServiceClient(clientOptions);async function createCustomJob() {\u00a0 // Configure the parent resource\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 const customJob = {\u00a0 \u00a0 displayName: customJobDisplayName,\u00a0 \u00a0 jobSpec: {\u00a0 \u00a0 \u00a0 workerPoolSpecs: [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 machineSpec: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 machineType: 'n1-standard-4',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 acceleratorType: 'NVIDIA_TESLA_K80',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 acceleratorCount: 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 replicaCount: 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 containerSpec: {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 imageUri: containerImageUri,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command: [],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 args: [],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 },\u00a0 };\u00a0 const request = {parent, customJob};\u00a0 // Create custom job request\u00a0 const [response] = await jobServiceClient.createCustomJob(request);\u00a0 console.log('Create custom job response:\\n', JSON.stringify(response));}createCustomJob();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/snippets/job_service/create_custom_job_sample.py) \n```\nfrom google.cloud import aiplatformdef create_custom_job_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 container_image_uri: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",):\u00a0 \u00a0 # The AI Platform services require regional API endpoints.\u00a0 \u00a0 client_options = {\"api_endpoint\": api_endpoint}\u00a0 \u00a0 # Initialize client that will be used to create and send requests.\u00a0 \u00a0 # This client only needs to be created once, and can be reused for multiple requests.\u00a0 \u00a0 client = aiplatform.gapic.JobServiceClient(client_options=client_options)\u00a0 \u00a0 custom_job = {\u00a0 \u00a0 \u00a0 \u00a0 \"display_name\": display_name,\u00a0 \u00a0 \u00a0 \u00a0 \"job_spec\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"worker_pool_specs\": [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"machine_spec\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"machine_type\": \"n1-standard-4\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"accelerator_type\": aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"accelerator_count\": 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"replica_count\": 1,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"container_spec\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"image_uri\": container_image_uri,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"command\": [],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"args\": [],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 }\u00a0 \u00a0 parent = f\"projects/{project}/locations/{location}\"\u00a0 \u00a0 response = client.create_custom_job(parent=parent, custom_job=custom_job)\u00a0 \u00a0 print(\"response:\", response)\n```\nFor more context, read the [guide to creating aCustomJob](/vertex-ai/docs/training/create-custom-job) .\n## TPUs\nTo use [Tensor Processing Units (TPUs)](/tpu/docs/tpus) for custom training on Vertex AI, you can configure a worker pool to use a [TPU VM](/tpu/docs/system-architecture-tpu-vm#tpu-vm) .\nWhen you use a TPU VM in Vertex AI, you must only use a single worker pool for custom training, and you must configure this worker pool to use only one replica.\nTo use TPU VMs in your worker pool, you must use one of the following configurations:\n- To configure a TPU VM with [TPU V2](/tpu/docs/system-architecture-tpu-vm#tpu_v2) , specify the following fields in the `WorkerPoolSpec` :- Set`machineSpec.machineType`to`cloud-tpu`.\n- Set`machineSpec.acceleratorType`to`TPU_V2`.\n- Set`machineSpec.acceleratorCount`to`8`for single TPU or`32 or multiple of 32`for TPU Pods.\n- Set`replicaCount`to`1`.\n- To configure a TPU VM with [TPU V3](/tpu/docs/system-architecture-tpu-vm#tpu_v3) , specify the following fields in the `WorkerPoolSpec` :- Set`machineSpec.machineType`to`cloud-tpu`.\n- Set`machineSpec.acceleratorType`to`TPU_V3`.\n- Set`machineSpec.acceleratorCount`to`8`for single TPU or`32+`for TPU Pods.\n- Set`replicaCount`to`1`.The following example highlights how to specify a TPU VM when you create a `CustomJob` :\nTo specify a TPU VM using the gcloud CLI tool, you must use a [config.yaml file](/sdk/gcloud/reference/ai/custom-jobs/create#--config) . For example:```\nworkerPoolSpecs:\u00a0 machineSpec:\u00a0 \u00a0 machineType: cloud-tpu\u00a0 \u00a0 acceleratorType: TPU_V2\u00a0 \u00a0 acceleratorCount: 8\u00a0 replicaCount: 1\u00a0 containerSpec:\u00a0 \u00a0 imageUri: CUSTOM_CONTAINER_IMAGE_URI\n```\nThen run a command like the following:\n```\ngcloud ai custom-jobs create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=JOB_NAME \\\u00a0 --config=config.yaml\n```\nFor more context, read the [guide to creating a CustomJob](/vertex-ai/docs/training/create-custom-job) .\n## Boot disk options\nYou can optionally customize the boot disks for your training VMs. All VMs in a worker pool use the same type and size of boot disk.\n- **To customize the type of boot disk that each training VM uses,** specify the [diskSpec.bootDiskType field](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#diskspec) in your `WorkerPoolSpec` .You can set this field to `pd-standard` in order to use a standard persistent disk backed by a standard hard drive, or you can set it to `pd-ssd` to use an SSD persistent disk backed by a solid-state drive. The default value is `pd-ssd` .Using `pd-ssd` might improve performance if your training code reads and writes to disk. Learn about [disk types](/compute/docs/disks#disk-types) .\n- **To customize the size (in GB) of the boot disk that each training VMuses,** specify the [diskSpec.bootDiskSizeGbfield](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#diskspec) in your `WorkerPoolSpec` .You can set this field to an integer between 100 and 64,000, inclusive. The default value is `100` .You might want to increase the boot disk size if your training code writes a lot of temporary data to disk. Note that any data you write to the boot disk is temporary, and you can't retrieve it after training completes.\nChanging the type and size of your boot disks affects [custom trainingpricing](/vertex-ai/pricing#training) .\nThe following examples highlight where you can specify boot disk options when you create a `CustomJob` :\nIn the Google Cloud console, you can't create a `CustomJob` directly. However, you can [create a TrainingPipeline that creates aCustomJob](/vertex-ai/docs/training/create-custom-job#create) . When you create a `TrainingPipeline` in the Google Cloud console, you can specify boot disk options for each worker pool on the **Compute and pricing** step, in the **Disktype** drop-down list and the **Disk size (GB)** field.To specify boot disk options using the Google Cloud CLI tool, you must use a [config.yamlfile](/sdk/gcloud/reference/ai/custom-jobs/create#--config) . For example:```\nworkerPoolSpecs:\u00a0 machineSpec:\u00a0 \u00a0 machineType: MACHINE_TYPE\u00a0 diskSpec:\u00a0 \u00a0 bootDiskType: DISK_TYPE\u00a0 \u00a0 bootDiskSizeGb: DISK_SIZE\u00a0 replicaCount: REPLICA_COUNT\u00a0 containerSpec:\u00a0 \u00a0 imageUri: CUSTOM_CONTAINER_IMAGE_URI\n```\nThen run a command like the following:\n```\ngcloud ai custom-jobs create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=JOB_NAME \\\u00a0 --config=config.yaml\n```\nFor more context, read the [guide to creating aCustomJob](/vertex-ai/docs/training/create-custom-job) .\n## What's next\n- Learn how to [create a persistent resource](/vertex-ai/docs/training/persistent-resource-overview) to run custom training jobs.\n- Learn how to perform custom training by [creating aCustomJob](/vertex-ai/docs/training/create-custom-job) .", "guide": "Vertex AI"}