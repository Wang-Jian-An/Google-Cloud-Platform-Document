{"title": "Vertex AI - Introduction to Vertex AI Experiments", "url": "https://cloud.google.com/vertex-ai/docs/experiments/intro-vertex-ai-experiments", "abstract": "# Vertex AI - Introduction to Vertex AI Experiments\nTo see an example of getting started with Vertex AI Experiments,  run the \"Get started with Vertex AI Experiments\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/get_started_with_vertex_experiments.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fexperiments%2Fget_started_with_vertex_experiments.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/get_started_with_vertex_experiments.ipynb)\nVertex AI Experiments is a tool that helps you track and analyze different model architectures, hyperparameters, and training environments, letting you track the steps, inputs, and outputs of an experiment run. Vertex AI Experiments can also evaluate how your model performed in aggregate, against test datasets, and during the training run. You can then use this information to select the best model for your particular use case.\nExperiment runs don't incur additional charges. You're only charged for resources that you use during your experiment as described in [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) .\n| What do you want to do?  | Check out notebook sample |\n|:-----------------------------|:----------------------------|\n| track metrics and parameters | Compare models    |\n| track experiment lineage  | Model training    |\n| track pipeline runs   | Compare pipeline runs  |\n", "content": "## Track steps, inputs, and outputs\nVertex AI Experiments lets you track:\n- steps of an, for example, preprocessing, training,\n- inputs, for example, algorithm, parameters, datasets,\n- outputs of those steps, for example, models, checkpoints, metrics.\nYou can then figure out what worked and what didn't, and identify further avenues for experimentation.\nFor user journey examples, check out:\n- [Model training](/vertex-ai/docs/experiments/user-journey/uj-model-training) \n- [Compare models](/vertex-ai/docs/experiments/user-journey/uj-compare-models) ## Analyze model performance\nVertex AI Experiments lets you track and evaluate how the model performed in aggregate, against test datasets, and during the training run. This ability helps to understand the performance characteristics of the models -- how well a particular model works overall, where it fails, and where the model excels.\nFor user journey examples, check out:\n- [Compare pipeline runs](/vertex-ai/docs/experiments/user-journey/uj-compare-pipeline-runs) \n- [Compare models](/vertex-ai/docs/experiments/user-journey/uj-compare-models) ## Compare model performance\nVertex AI Experiments lets you group and compare multiple models across . Each model has its own specified parameters, modeling techniques, architectures, and input. This approach helps select the best model.\nFor user journey examples, check out:\n- [Compare pipeline runs](/vertex-ai/docs/experiments/user-journey/uj-compare-pipeline-runs) \n- [Compare models](/vertex-ai/docs/experiments/user-journey/uj-compare-models) ## Search experiments\nThe Google Cloud console provides a centralized view of experiments, a cross-sectional view of the experiment runs, and the details for each run. The Vertex AI SDK for Python provides APIs to consume experiments, experiment runs, experiment run parameters, metrics, and artifacts.\nVertex AI Experiments, along with [Vertex ML Metadata](/vertex-ai/docs/ml-metadata/introduction) , provides a way to find the artifacts tracked in an experiment. This lets you quickly view the artifact's lineage and the artifacts consumed and produced by steps in a run.\n## Scope of support\nVertex AI Experiments supports development of models using Vertex AI custom training, Vertex AI Workbench notebooks, Notebooks, and all Python ML Frameworks across most ML Frameworks. For some ML frameworks, such as TensorFlow, Vertex AI Experiments provides deep integrations into the framework that makes the user experience automagical. For other ML frameworks, Vertex AI Experiments provides a framework neutral Vertex AI SDK for Python that you can use. (see: [Prebuilt containers](/vertex-ai/docs/training/pre-built-containers) for TensorFlow, scikit-learn, PyTorch, XGBoost).\n## Data models and concepts\nVertex AI Experiments is a in [Vertex ML Metadata](/vertex-ai/docs/ml-metadata/introduction) where an experiment can contain experiment runs in addition to pipeline runs. An experiment run consists of parameters, summary metrics, time series metrics, and [PipelineJob](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.PipelineJob) , [Artifact](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Artifact) , and [Execution](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Execution) Vertex AI resources. [Vertex AI TensorBoard](/vertex-ai/docs/experiments/tensorboard-introduction) , a managed version of open source TensorBoard, is used for time-series metrics storage. Executions and of a pipeline run are viewable in the [Google Cloud console](/vertex-ai/docs/pipelines/visualize-pipeline#visualize_pipeline_runs_using) .\n## Vertex AI Experiments terms\n### Experiment, experiment run, and pipeline run\n- An experiment is a context that can contain a set of n experiment runs in addition to pipeline runs where a user can investigate, as a group, different configurations such as input artifacts or hyperparameters.\n[Create an experiment](/vertex-ai/docs/experiments/create-experiment)- An experiment run can contain user-defined metrics, parameters, executions, artifacts, and Vertex resources (for example, PipelineJob).\n[Create and manage experiment runs](/vertex-ai/docs/experiments/create-manage-exp-run)- One or more Vertex PipelineJobs can be associated with an experiment where each PipelineJob is represented as a single run. In this context, the parameters of the run are inferred by the parameters of the PipelineJob. The metrics are inferred from the system.Metric artifacts produced by that PipelineJob. The artifacts of the run are inferred from artifacts produced by that PipelineJob.\n[PipelineJob](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.PipelineJob)\n[ExperimentRun](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.ExperimentRun)\nSee [Associate a pipeline with an experiment](/vertex-ai/docs/experiments/add-pipelinerun-experiment) .\n### Parameters and metrics\n- Parameters are keyed input values that configure a run, regulate the behavior of the run, and affect the results of the run. Examples include learning rate, dropout rate, and number of training steps.See [Log parameters](/vertex-ai/docs/experiments/log-data#parameters) .\n- Summary metrics are a single value for each metric key in an experiment run. For example, the test accuracy of an experiment is the accuracy calculated against a test dataset at the end of training that can be captured as a single value summary metric.See [Log summary metrics](/vertex-ai/docs/experiments/log-data#summary-metrics) .\n- Time series metrics are longitudinal metric values where each value represents a step in the training routine portion of a run. Time series metrics are stored in Vertex AI TensorBoard. Vertex AI Experiments stores a reference to the Vertex TensorBoard resource.See [Log time series metrics](/vertex-ai/docs/experiments/log-data#time-series-metrics) .\n### Resource types\n- A resource in the Vertex AI API corresponding to Vertex Pipeline Jobs. Users create a PipelineJob when they want to run an ML Pipeline on Vertex AI.\n- An artifact is a discrete entity or piece of data produced and consumed by a machine learning workflow. Examples of artifacts include datasets, models, input files, and training logs.Vertex AI Experiments lets you use a schema to define the type of artifact. For example, supported schema types include `system.Dataset` , `system.Model` , and `system.Artifact` . For more information, see [System schemas](/vertex-ai/docs/ml-metadata/system-schemas) .\n## Notebook tutorial\n- [Get started with Vertex AI Experiments](/vertex-ai/docs/experiments/user-journey/uj-get-started-vertex-ai-experiments) ## What's next\n- [Set up to get started with Vertex AI Experiments](/vertex-ai/docs/experiments/setup)", "guide": "Vertex AI"}