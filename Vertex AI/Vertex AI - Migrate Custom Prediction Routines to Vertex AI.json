{"title": "Vertex AI - Migrate Custom Prediction Routines to Vertex AI", "url": "https://cloud.google.com/vertex-ai/docs/predictions/migrate-cpr", "abstract": "# Vertex AI - Migrate Custom Prediction Routines to Vertex AI\nThis page describes how to migrate your [Custom Prediction Routine (CPR)](/ai-platform/prediction/docs/custom-prediction-routines) deployments from AI Platform to Vertex AI.\nSpecifically, given a CPR deployment on AI Platform, this page shows you how to:\n- Create a corresponding [custom container](/vertex-ai/docs/predictions/use-custom-container) for deployment on Vertex AI. This custom container works like any custom container created with [CPR on Vertex AI](/vertex-ai/docs/predictions/custom-prediction-routines) .\n- Run and test the custom container locally.\n- Upload it to the [Vertex AI Model Registry](/vertex-ai/docs/model-registry/introduction) .\n- Deploy the model to a Vertex AI Endpoint to serve online predictions.", "content": "## Before you begin\n- Make sure that the following software is installed:- [Docker](https://www.docker.com/) \n- [Cloud SDK (gcloud)](/sdk/docs/install) \n- [Python 3](https://www.python.org) \n- Have the model artifacts and custom code from your CPR on AI Platform deployment that you want to migrate to Vertex AI.\n- Have a Cloud Storage bucket to store the model artifacts.\n- Make sure you have the Vertex AI API enabled in your project. [Enable Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com) ## Prepare the source folder for Vertex AI deployment\n- Create a local folder called `model_artifacts` and copy the model artifacts from your CPR on AI Platform deployment. These should be the same model artifacts that you specified in `deployment_uri` (or `--origin` if you used gcloud) when you [deployed your CPR on AI Platform model](/ai-platform/prediction/docs/custom-prediction-routines#deploy_your_custom_prediction_routine) .\n- Create a local folder called `cpr_src_dir` . This folder will hold your source distribution packages, `adapter.py` , and `requirements.txt` (described below) which are used to build your custom container for deployment on Vertex AI .\n- Copy all of the packages you supplied in [package_uris](/ai-platform/prediction/docs/custom-prediction-routines#deploy_your_custom_prediction_routine) when you deployed your CPR on AI Platform, including the one that contains your `Predictor` class.\n- Create a `adapter.py` file that contains the `AdapterPredictor` (shown below) and set the `PREDICTION_CLASS` to the fully qualified name of your `Predictor` . This value is the same as `prediction_class` when you deployed your CPR on AI Platform.The adapter wraps the CPR on AI Platform `Predictor` interface so that it is compatible with the CPR on Vertex AI interface.```\nimport pydoc\u200bfrom google.cloud.aiplatform.utils import prediction_utilsfrom google.cloud.aiplatform.prediction.predictor import Predictor\u200b# Fully qualified name of your CPR on CAIP Predictor class.PREDICTION_CLASS = \"predictor.MyPredictor\"\u200bclass AdapterPredictor(Predictor):\u00a0 \"\"\"Predictor implementation for adapting CPR on CAIP predictors.\"\"\"\u200b\u00a0 def __init__(self):\u00a0 \u00a0 \u00a0 return\u200b\u00a0 def load(self, artifacts_uri: str):\u00a0 \u00a0 \u00a0 \"\"\"Loads the model artifact.\u200b\u00a0 \u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 artifacts_uri (str):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Required. The model artifacts path (may be local or on Cloud Storage).\u00a0 \u00a0 \u00a0 \"\"\"\u00a0 \u00a0 \u00a0 prediction_utils.download_model_artifacts(artifacts_uri)\u00a0 \u00a0 \u00a0 custom_class = pydoc.locate(PREDICTION_CLASS)\u00a0 \u00a0 \u00a0 self._predictor = custom_class.from_path(\".\")\u200b\u200b\u00a0 def predict(self, instances):\u00a0 \u00a0 \u00a0 \"\"\"Performs prediction.\u200b\u00a0 \u00a0 \u00a0 Args:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 instances (Any):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Required. The instance(s) used for performing prediction.\u200b\u00a0 \u00a0 \u00a0 Returns:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Prediction results.\u00a0 \u00a0 \u00a0 \"\"\"\u00a0 \u00a0 \u00a0 return self._predictor.predict(**instances)\n```\n- Create a `requirements.txt` file that contains your model's dependencies, for example:```\n# Required for model servinggoogle-cloud-storage>=1.26.0,<2.0.0devgoogle-cloud-aiplatform[prediction]>=1.16.0# ML dependenciesnumpy>=1.16.0scikit-learn==0.20.2\n```The first section lists the dependencies required for model serving.The second section lists the Machine Learning packages required for model serving (for example, scikit-learn, xgboost, tensorflow, etc.). Be sure to install the same version of these libraries as [listed](https://cloud.google.com/ai-platform/prediction/docs/runtime-version-list#supported_runtime_versions) under the runtime version you chose when previously deploying your model version.\n- Install the dependencies in your local environment```\npip install -U --user -r cpr_src_dir/requirements.txt \n```## Upload your model artifacts to Cloud Storage\nUpload the model artifacts to Cloud Storage:\n```\ngsutil cp model_artifacts/* gs://BUCKET_NAME/MODEL_ARTIFACT_DIR\n```\n## Set up Artifact Registry\n[Artifact Registry](/artifact-registry/docs) is used to to store and manage your Docker container images.\n- Make sure you have the Artifacts Registry API enabled in your project. [Enable the Artifacts Registry API](https://console.cloud.google.com/flows/enableapi?apiid=artifactregistry.googleapis.com) \n- Create your repository if you don't already have one.```\ngcloud artifacts repositories create {REPOSITORY} \\\u00a0 \u00a0 --repository-format=docker \\\u00a0 \u00a0 --location={REGION}\n```\n- Before you can push or pull images, configure Docker to use the Google Cloud CLI to authenticate requests to Artifact Registry.```\ngcloud auth configure-docker {REGION}-docker.pkg.dev\n```## Build, test, and deploy your custom container\nThe following Python script demonstrates how to build, test, and deploy your custom container by using the APIs in the [Vertex AI SDK](https://github.com/googleapis/python-aiplatform) . Be sure to set the variables at the top of the script.\n```\nimport jsonimport loggingimport osfrom google.cloud import aiplatformfrom google.cloud.aiplatform.prediction import LocalModelfrom cpr_src_dir.adapter import AdapterPredictor\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n### CONFIGURE THE FOLLOWING\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n### We recommend that you choose the region closest to you.REGION = \u2026# Your project ID.PROJECT_ID = \u2026# Name of the Artifact Repository to create or use.REPOSITORY = \u2026# Name of the container image that will be pushed.IMAGE = \u2026# Cloud Storage bucket where your model artifacts will be stored.BUKCET_NAME = \u2026# Directory within the bucket where your model artifacts are stored.MODEL_ARTIFACT_DIR = \u2026# Your model's input instances.INSTANCES = \u2026\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n### Build the CPR custom container\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##local_model = LocalModel.build_cpr_model(\u00a0 \u00a0 \"cpr_src_dir\",\u00a0 \u00a0 f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\",\u00a0 \u00a0 predictor=AdapterPredictor,\u00a0 \u00a0 requirements_path=\"cpr_src_dir/requirements.txt\",\u00a0 \u00a0 extra_packages=[\"cpr_src_dir/my_custom_code-0.1.tar.gz\"],)\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n### Run and test the custom container locally\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##logging.basicConfig(level=logging.INFO)local_endpoint =\u00a0 \u00a0 \u00a0 \u00a0local_model.deploy_to_local_endpoint(artifact_uri=\"model_artifacts\")local_endpoint.serve()health_check_response = local_endpoint.run_health_check()predict_response = local_endpoint.predict(\u00a0 \u00a0 \u00a0 \u00a0 request=json.dumps({\"instances\": INSTANCES}),\u00a0 \u00a0 \u00a0 \u00a0 headers={\"Content-Type\": \"application/json\"},\u00a0 \u00a0 )local_endpoint.stop()print(predict_response, predict_response.content)print(health_check_response, health_check_response.content)local_endpoint.print_container_logs(show_all=True)\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n### Upload and deploy to Vertex\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##\n##local_model.push_image()model = aiplatform.Model.upload(\\\u00a0 \u00a0 local_model=local_model,\u00a0 \u00a0 display_name=MODEL_DISPLAY_NAME,\u00a0 \u00a0 artifact_uri=f\"gs://{BUKCET_NAME}/{MODEL_ARTIFACT_DIR}\",)endpoint = model.deploy(machine_type=\"n1-standard-4\")endpoint.predict(instances=INSTANCES)\n```\nLearn more about [Vertex AI Prediction](/vertex-ai/docs/predictions/getting-predictions) .", "guide": "Vertex AI"}