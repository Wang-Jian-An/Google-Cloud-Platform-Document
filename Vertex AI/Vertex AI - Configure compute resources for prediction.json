{"title": "Vertex AI - Configure compute resources for prediction", "url": "https://cloud.google.com/vertex-ai/docs/predictions/configure-compute", "abstract": "# Vertex AI - Configure compute resources for prediction\nVertex AI allocates to handle online and batch predictions. When you [deploy a custom-trained model or AutoML model to an Endpointresource to serve online predictions](/vertex-ai/docs/predictions/deploy-model-console) or when you [request batch predictions](/vertex-ai/docs/predictions/batch-predictions) , you can customize the type of virtual machine that the prediction service uses for these nodes. You can optionally configure prediction nodes to use GPUs.\ndiffer in a few ways:\n- Number of virtual CPUs (vCPUs) per node\n- Amount of memory per node\n- [Pricing](/vertex-ai/pricing) \nBy selecting a machine type with more computing resources, you can serve predictions with lower latency or handle more prediction requests at the same time.\n", "content": "## Where to specify compute resources\n### Online prediction\nIf you want to use a custom-trained model or an AutoML tabular model to serve online predictions, you must specify a machine type when you deploy the `Model` resource as a `DeployedModel` to an `Endpoint` . For other types of AutoML models, Vertex AI configures the machine types automatically.\nSpecify the machine type (and, optionally, GPU configuration) in the [dedicatedResources.machineSpec field of yourDeployedModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints#deployedmodel) .\nLearn how to deploy each model type:\n- [Deploy an AutoML tabular model in Google Cloud console](/vertex-ai/docs/predictions/deploy-model-console) \n- [Deploy a custom-trained model in Google Cloud console](/vertex-ai/docs/tutorials/image-classification-custom/serving#1_create_an_endpoint) \n- [Deploy a custom-trained model using client libraries](/vertex-ai/docs/predictions/deploy-model-api) \n### Batch prediction\nIf you want to get batch predictions from a custom-trained model or an AutoML tabular model, you must specify a machine type when you [create aBatchPredictionJob resource](/vertex-ai/docs/predictions/batch-predictions) . Specify the machine type (and, optionally, GPU configuration) in the [dedicatedResources.machineSpec field of yourBatchPredictionJob](/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs) .\n[](None)\n## Machine types\nThe following table compares the available machine types for serving predictions from custom-trained models and AutoML tabular models:\n| Name   | vCPUs | Memory (GB) |\n|:---------------|--------:|--------------:|\n| e2-standard-2 |  2 |    8 |\n| e2-standard-4 |  4 |   16 |\n| e2-standard-8 |  8 |   32 |\n| e2-standard-16 |  16 |   64 |\n| e2-standard-32 |  32 |   128 |\n| e2-highmem-2 |  2 |   16 |\n| e2-highmem-4 |  4 |   32 |\n| e2-highmem-8 |  8 |   64 |\n| e2-highmem-16 |  16 |   128 |\n| e2-highcpu-2 |  2 |    2 |\n| e2-highcpu-4 |  4 |    4 |\n| e2-highcpu-8 |  8 |    8 |\n| e2-highcpu-16 |  16 |   16 |\n| e2-highcpu-32 |  32 |   32 |\n| Name   | vCPUs | Memory (GB) |\n|:---------------|--------:|--------------:|\n| n1-standard-2 |  2 |   7.5 |\n| n1-standard-4 |  4 |   15 |\n| n1-standard-8 |  8 |   30 |\n| n1-standard-16 |  16 |   60 |\n| n1-standard-32 |  32 |   120 |\n| n1-highmem-2 |  2 |   13 |\n| n1-highmem-4 |  4 |   26 |\n| n1-highmem-8 |  8 |   52 |\n| n1-highmem-16 |  16 |   104 |\n| n1-highmem-32 |  32 |   208 |\n| n1-highcpu-4 |  4 |   3.6 |\n| n1-highcpu-8 |  8 |   7.2 |\n| n1-highcpu-16 |  16 |   14.4 |\n| n1-highcpu-32 |  32 |   28.8 |\n| Name   | vCPUs | Memory (GB) |\n|:----------------|--------:|--------------:|\n| n2-standard-2 |  2 |    8 |\n| n2-standard-4 |  4 |   16 |\n| n2-standard-8 |  8 |   32 |\n| n2-standard-16 |  16 |   64 |\n| n2-standard-32 |  32 |   128 |\n| n2-standard-48 |  48 |   192 |\n| n2-standard-64 |  64 |   256 |\n| n2-standard-80 |  80 |   320 |\n| n2-standard-96 |  96 |   384 |\n| n2-standard-128 |  128 |   512 |\n| n2-highmem-2 |  2 |   16 |\n| n2-highmem-4 |  4 |   32 |\n| n2-highmem-8 |  8 |   64 |\n| n2-highmem-16 |  16 |   128 |\n| n2-highmem-32 |  32 |   256 |\n| n2-highmem-48 |  48 |   384 |\n| n2-highmem-64 |  64 |   512 |\n| n2-highmem-80 |  80 |   640 |\n| n2-highmem-96 |  96 |   768 |\n| n2-highmem-128 |  128 |   864 |\n| n2-highcpu-2 |  2 |    2 |\n| n2-highcpu-4 |  4 |    4 |\n| n2-highcpu-8 |  8 |    8 |\n| n2-highcpu-16 |  16 |   16 |\n| n2-highcpu-32 |  32 |   32 |\n| n2-highcpu-48 |  48 |   48 |\n| n2-highcpu-64 |  64 |   64 |\n| n2-highcpu-80 |  80 |   80 |\n| n2-highcpu-96 |  96 |   96 |\n| Name    | vCPUs | Memory (GB) |\n|:-----------------|--------:|--------------:|\n| n2d-standard-2 |  2 |    8 |\n| n2d-standard-4 |  4 |   16 |\n| n2d-standard-8 |  8 |   32 |\n| n2d-standard-16 |  16 |   64 |\n| n2d-standard-32 |  32 |   128 |\n| n2d-standard-48 |  48 |   192 |\n| n2d-standard-64 |  64 |   256 |\n| n2d-standard-80 |  80 |   320 |\n| n2d-standard-96 |  96 |   384 |\n| n2d-standard-128 |  128 |   512 |\n| n2d-standard-224 |  224 |   896 |\n| n2d-highmem-2 |  2 |   16 |\n| n2d-highmem-4 |  4 |   32 |\n| n2d-highmem-8 |  8 |   64 |\n| n2d-highmem-16 |  16 |   128 |\n| n2d-highmem-32 |  32 |   256 |\n| n2d-highmem-48 |  48 |   384 |\n| n2d-highmem-64 |  64 |   512 |\n| n2d-highmem-80 |  80 |   640 |\n| n2d-highmem-96 |  96 |   768 |\n| n2d-highcpu-2 |  2 |    2 |\n| n2d-highcpu-4 |  4 |    4 |\n| n2d-highcpu-8 |  8 |    8 |\n| n2d-highcpu-16 |  16 |   16 |\n| n2d-highcpu-32 |  32 |   32 |\n| n2d-highcpu-48 |  48 |   48 |\n| n2d-highcpu-64 |  64 |   64 |\n| n2d-highcpu-80 |  80 |   80 |\n| n2d-highcpu-96 |  96 |   96 |\n| n2d-highcpu-128 |  128 |   128 |\n| n2d-highcpu-224 |  224 |   224 |\n| Name   | vCPUs | Memory (GB) |\n|:---------------|--------:|--------------:|\n| c2-standard-4 |  4 |   16 |\n| c2-standard-8 |  8 |   32 |\n| c2-standard-16 |  16 |   64 |\n| c2-standard-30 |  30 |   120 |\n| c2-standard-60 |  60 |   240 |\n| Name    | vCPUs | Memory (GB) |\n|:-----------------|--------:|--------------:|\n| c2d-standard-2 |  2 |    8 |\n| c2d-standard-4 |  4 |   16 |\n| c2d-standard-8 |  8 |   32 |\n| c2d-standard-16 |  16 |   64 |\n| c2d-standard-32 |  32 |   128 |\n| c2d-standard-56 |  56 |   224 |\n| c2d-standard-112 |  112 |   448 |\n| c2d-highcpu-2 |  2 |    4 |\n| c2d-highcpu-4 |  4 |    8 |\n| c2d-highcpu-8 |  8 |   16 |\n| c2d-highcpu-16 |  16 |   32 |\n| c2d-highcpu-32 |  32 |   64 |\n| c2d-highcpu-56 |  56 |   112 |\n| c2d-highcpu-112 |  112 |   224 |\n| c2d-highmem-2 |  2 |   16 |\n| c2d-highmem-4 |  4 |   32 |\n| c2d-highmem-8 |  8 |   64 |\n| c2d-highmem-16 |  16 |   128 |\n| c2d-highmem-32 |  32 |   256 |\n| c2d-highmem-56 |  56 |   448 |\n| c2d-highmem-112 |  112 |   896 |\n| Name   | vCPUs | Memory (GB) |\n|:---------------|--------:|--------------:|\n| c3-highcpu-4 |  4 |    8 |\n| c3-highcpu-8 |  8 |   16 |\n| c3-highcpu-22 |  22 |   44 |\n| c3-highcpu-44 |  44 |   88 |\n| c3-highcpu-88 |  88 |   176 |\n| c3-highcpu-176 |  176 |   352 |\n| Name   | vCPUs | Memory (GB) | GPUs (NVIDIA A100) |\n|:---------------|--------:|--------------:|:---------------------|\n| a2-highgpu-1g |  12 |   85 | 1 (A100 40GB)  |\n| a2-highgpu-2g |  24 |   170 | 2 (A100 40GB)  |\n| a2-highgpu-4g |  48 |   340 | 4 (A100 40GB)  |\n| a2-highgpu-8g |  96 |   680 | 8 (A100 40GB)  |\n| a2-megagpu-16g |  96 |   1360 | 16 (A100 40GB)  |\n| a2-ultragpu-1g |  12 |   170 | 1 (A100 80GB)  |\n| a2-ultragpu-2g |  24 |   340 | 2 (A100 80GB)  |\n| a2-ultragpu-4g |  48 |   680 | 4 (A100 80GB)  |\n| a2-ultragpu-8g |  96 |   1360 | 8 (A100 80GB)  |\n| Name   | vCPUs | Memory (GB) | GPUs (NVIDIA H100) |\n|:--------------|--------:|--------------:|:---------------------|\n| a3-highgpu-8g |  208 |   1872 | 8 (H100 80GB)  |\n| Name   | vCPUs | Memory (GB) | GPUs (NVIDIA L4) |\n|:---------------|--------:|--------------:|-------------------:|\n| g2-standard-4 |  4 |   16 |     1 |\n| g2-standard-8 |  8 |   32 |     1 |\n| g2-standard-12 |  12 |   48 |     1 |\n| g2-standard-16 |  16 |   64 |     1 |\n| g2-standard-24 |  24 |   96 |     2 |\n| g2-standard-32 |  32 |   128 |     1 |\n| g2-standard-48 |  48 |   192 |     4 |\n| g2-standard-96 |  96 |   384 |     8 |\nLearn about [pricing for each machinetype](/vertex-ai/pricing) . Read more about the detailed specifications of these machine types in the [Compute Engine documentation about machinetypes](/compute/docs/machine-types#n1_machine_types) .\n### Find the ideal machine type\nTo find the ideal machine type for your use case, we recommend loading your model on multiple machine types and measuring characteristics such as the latency, cost, concurrency, and throughput.\nOne way to do this is to run [this notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/find_ideal_machine_type/find_ideal_machine_type.ipynb) on multiple machine types and compare the results to find the one that works best for you.\nVertex AI reserves approximately 1 vCPU on each replica for running system processes. This means that running the notebook on a single core machine type would be comparable to using a 2-core machine type for serving predictions.\nWhen considering prediction costs, remember that although larger machines cost more, they can lower overall cost because fewer replicas are required to serve the same workload. This is particularly evident for GPUs, which tend to cost more per hour, but can both provide lower latency and cost less overall.\nFor more information, see [Choose machine type and replica count](/vertex-ai/docs/predictions/get-batch-predictions#choose_machine_type_and_replica_count) .\n## Optional GPU accelerators\nSome configurations, such as the [A2 series](/compute/docs/accelerator-optimized-machines#a2-vms) and [G2 series](/compute/docs/accelerator-optimized-machines#g2-vms) , have a fixed number of GPUs built-in.\nOther configurations, such as the N1 series, let you optionally add GPUs to accelerate each prediction node.\n**Note:** GPUs are **not** recommended for use with AutoML tabular models. For this type of model, GPUs do not provide a worthwhile performance benefit. Specifying GPUs during AutoML model deployment is not supported in Google Cloud console.\nTo add optional GPU accelerators, you must account for several requirements:\n- You can only use GPUs when your`Model`resource is based on a [TensorFlowSavedModel](/vertex-ai/docs/training/exporting-model-artifacts) , or when you [use a custom container](/vertex-ai/docs/predictions/use-custom-container) that has been designed to take advantage of GPUs. You can't use GPUs for scikit-learn or XGBoost models.\n- The availability of each type of GPU varies depending on which region you use for your model. Learn [which types of GPUs are available in whichregions](/vertex-ai/docs/general/locations#accelerators) .\n- You can only use one type of GPU for your`DeployedModel`resource or`BatchPredictionJob`, and there are limitations on the number of GPUs you can add depending on which machine type you are using. The following table describes these limitations.\nThe following table shows the optional GPUs that are available for online prediction and how many of each type of GPU you can use with each Compute Engine machine type:\n| ('Valid numbers of GPUs for each machine type', 'Machine type') | ('Valid numbers of GPUs for each machine type', 'NVIDIA Tesla K80') | ('Valid numbers of GPUs for each machine type', 'NVIDIA Tesla P100') | ('Valid numbers of GPUs for each machine type', 'NVIDIA Tesla V100') | ('Valid numbers of GPUs for each machine type', 'NVIDIA Tesla P4') | ('Valid numbers of GPUs for each machine type', 'NVIDIA Tesla T4') |\n|:------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------------------|:-----------------------------------------------------------------------|:---------------------------------------------------------------------|:---------------------------------------------------------------------|\n| n1-standard-2              | 1, 2, 4, 8               | 1, 2, 4                | 1, 2, 4, 8                | 1, 2, 4                | 1, 2, 4                |\n| n1-standard-4              | 1, 2, 4, 8               | 1, 2, 4                | 1, 2, 4, 8                | 1, 2, 4                | 1, 2, 4                |\n| n1-standard-8              | 1, 2, 4, 8               | 1, 2, 4                | 1, 2, 4, 8                | 1, 2, 4                | 1, 2, 4                |\n| n1-standard-16             | 2, 4, 8                | 1, 2, 4                | 2, 4, 8                | 1, 2, 4                | 1, 2, 4                |\n| n1-standard-32             | 4, 8                 | 2, 4                 | 4, 8                 | 2, 4                 | 2, 4                 |\n| n1-highmem-2              | 1, 2, 4, 8               | 1, 2, 4                | 1, 2, 4, 8                | 1, 2, 4                | 1, 2, 4                |\n| n1-highmem-4              | 1, 2, 4, 8               | 1, 2, 4                | 1, 2, 4, 8                | 1, 2, 4                | 1, 2, 4                |\n| n1-highmem-8              | 1, 2, 4, 8               | 1, 2, 4                | 1, 2, 4, 8                | 1, 2, 4                | 1, 2, 4                |\n| n1-highmem-16              | 2, 4, 8                | 1, 2, 4                | 2, 4, 8                | 1, 2, 4                | 1, 2, 4                |\n| n1-highmem-32              | 4, 8                 | 2, 4                 | 4, 8                 | 2, 4                 | 2, 4                 |\n| n1-highcpu-2              | 1, 2, 4, 8               | 1, 2, 4                | 1, 2, 4, 8                | 1, 2, 4                | 1, 2, 4                |\n| n1-highcpu-4              | 1, 2, 4, 8               | 1, 2, 4                | 1, 2, 4, 8                | 1, 2, 4                | 1, 2, 4                |\n| n1-highcpu-8              | 1, 2, 4, 8               | 1, 2, 4                | 1, 2, 4, 8                | 1, 2, 4                | 1, 2, 4                |\n| n1-highcpu-16              | 2, 4, 8                | 1, 2, 4                | 2, 4, 8                | 1, 2, 4                | 1, 2, 4                |\n| n1-highcpu-32              | 4, 8                 | 2, 4                 | 4, 8                 | 2, 4                 | 2, 4                 |\nOptional GPUs incur [additional costs](/vertex-ai/pricing) .\n## What's next\n- [Deploy an AutoML tabular model in Google Cloud console](/vertex-ai/docs/predictions/deploy-model-console) \n- [Deploy a custom-trained model in Google Cloud console](/vertex-ai/docs/tutorials/image-recognition-custom/serving#1_create_an_endpoint) \n- [Deploy a custom-trained model using client libraries](/vertex-ai/docs/predictions/deploy-model-api) \n- [Get batch predictions](/vertex-ai/docs/predictions/batch-predictions)", "guide": "Vertex AI"}