{"title": "Vertex AI - Monitor and debug training with an interactive shell", "url": "https://cloud.google.com/vertex-ai/docs/training/monitor-debug-interactive-shell", "abstract": "# Vertex AI - Monitor and debug training with an interactive shell\nThis page shows you how to use an interactive shell to inspect the container where your training code is running. You can browse the file system and run debugging utilities in each [prebuiltcontainer](/vertex-ai/docs/training/pre-built-containers) or [customcontainer](/vertex-ai/docs/training/containers-overview) running on Vertex AI.\nUsing an interactive shell to inspect your training container can help you debug problems with your training code or your Vertex AI configuration. For example, you can use an interactive shell to do the following:\n- Run tracing and profiling tools.\n- Analyze GPU usage.\n- Check Google Cloud permissions available to the container.\nVertex AI is also integrated with TensorFlow Profiler, which you can use to debug model training performance for your custom training jobs. For details, see [Profile model training performance using Profiler](/vertex-ai/docs/training/tensorboard-profiler) .\n", "content": "## Before you begin\nYou can use an interactive shell when you perform custom training with a `CustomJob` resource, a `HyperparameterTuningJob` resource, or a custom `TrainingPipeline` resource. As you [prepare your trainingcode](/vertex-ai/docs/training/code-requirements) and [configure the custom trainingresource of your choice](/vertex-ai/docs/training/custom-training-methods) , make sure to meet the following requirements:\n- Ensure that your training container has [bash](https://www.gnu.org/software/bash/) installed.All [prebuilt training containers](/vertex-ai/docs/training/pre-built-containers) have `bash` installed. If you [create a custom container fortraining](/vertex-ai/docs/training/create-custom-container) , use a base container that includes `bash` or install `bash` in your Dockerfile.\n- Perform custom training in a [region that supports interactiveshells](/vertex-ai/docs/general/locations#feature-availability) .\n- Ensure that anyone who wants to access an interactive shell has the following permissions for the Google Cloud project where custom training is running:- `aiplatform.customJobs.create`\n- `aiplatform.customJobs.get`\n- `aiplatform.customJobs.cancel`\nIf you initiate custom training yourself, then you most likely already have these permissions and can access an interactive shell. However, if you want to use an interactive shell to inspect a custom training resource created by someone else in your organization, then you might need to obtain these permissions.One way to obtain these permissions is to ask an administrator of your organization to grant you the [Vertex AI Userrole](/vertex-ai/docs/general/access-control#predefined-roles) ( `roles/aiplatform.user` ).\n### Requirements for advanced cases\nIf you are using certain advanced features, meet the following additional requirements:\n- If you [attach a custom service account](/vertex-ai/docs/general/custom-service-account#attach) to your custom training resource, then make sure that any user who wants to access an interactive shell has the `iam.serviceAccounts.actAs` permission for the attached service account.The guide to custom service accounts notes that you must have this permission to attach a service account. You also need this permission to view an interactive shell during custom training.For example, to create a `CustomJob` with a service account attached, you must have the `iam.serviceAccounts.actAs` permission for the service account. If one of your colleagues then wants to view an interactive shell for this `CustomJob` , they must also have the same `iam.serviceAccounts.actAs` permission.\n- If you have configured your project to [use VPC Service Controls withVertex AI](/vertex-ai/docs/general/vpc-service-controls) , then account for the following additional limitations:- You can't use [private IP for custom training](/vertex-ai/docs/training/using-private-ip) .\n- From within an interactive shell, you can't access the public internet or Google Cloud resources outside your service perimeter.\n- To secure access to interactive shells, you must add `notebooks.googleapis.com` as a restricted service in your service perimeter, in addition to `aiplatform.googleapis.com` . If you only restrict `aiplatform.googleapis.com` and not `notebooks.googleapis.com` , then users can access interactive shells from machines outside the service perimeter, which reduces the security benefit of using VPC Service Controls. **Note:** More generally, we recommend that you restrict all services when you create a service perimeter. See the [VPC Service Controls guide to creating a service perimeter](/vpc-service-controls/docs/create-service-perimeters) .\n## Enable interactive shells\nTo enable interactive shells for a custom training resource, set the [enableWebAccess APIfield](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#FIELDS.enable_web_access) to `true` when you create a `CustomJob` , `HyperparameterTuningJob` , or custom `TrainingPipeline` .\nThe following examples show how to do this using several different tools:\nFollow the guide to [creating a custom TrainingPipeline in the Google Cloud console](/vertex-ai/docs/training/create-training-pipeline#custom-job-model-upload) . In the **Train new model** pane, when you reach the **Model details** step, do the following:- Click **Advanced options** .\n- Select the **Enable training debugging** checkbox.\nThen, complete the rest of the **Train new model** workflow.\n- If you want to create a `CustomJob` , run the [gcloud ai custom-jobs create command](/sdk/gcloud/reference/ai/custom-jobs/create) , and specify the `--enable-web-access` flag on this command.\n- If you want to create a `HyperparameterTuningJob` , run the [gcloud ai hp-tuning-jobs create command](/sdk/gcloud/reference/ai/hp-tuning-jobs/create) , and specify the `--enable-web-access` flag on this command.\nTo learn how to use these commands, see the guide to [creating a CustomJob](/vertex-ai/docs/training/create-custom-job#create) and the guide to [creating a HyperparameterTuningJob](/vertex-ai/docs/training/using-hyperparameter-tuning#create) .The following partial REST request bodies show where to specify the `enableWebAccess` field for each type of custom training resource:\nThe following example is a partial request body for the [projects.locations.customJobs.create APImethod](/vertex-ai/docs/reference/rest/v1/projects.locations.customJobs/create) :\n```\n{\u00a0 ...\u00a0 \"jobSpec\": {\u00a0 \u00a0 ...\u00a0 \u00a0 \"enableWebAccess\": true\u00a0 }\u00a0 ...}\n```\nFor an example of sending an API request to create a `CustomJob` , see [Creating custom training jobs](/vertex-ai/docs/training/create-custom-job) .\nThe following example is a partial request body for the [projects.locations.hyperparameterTuningJobs.create APImethod](/vertex-ai/docs/reference/rest/v1/projects.locations.hyperparameterTuningJobs/create) :\n```\n{\u00a0 ...\u00a0 \"trialJobSpec\": {\u00a0 \u00a0 ...\u00a0 \u00a0 \"enableWebAccess\": true\u00a0 }\u00a0 ...}\n```\nFor an example of sending an API request to create a `HyperparameterTuningJob` , see [Using hyperparametertuning](/vertex-ai/docs/training/using-hyperparameter-tuning) .\nThe following examples show partial request bodies for the [projects.locations.trainingPipelines.create APImethod](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines/create) . Select one of the following tabs, depending on whether you are using hyperparameter tuning:\n```\n{\u00a0 ...\u00a0 \"trainingTaskInputs\": {\u00a0 \u00a0 ...\u00a0 \u00a0 \"enableWebAccess\": true\u00a0 }\u00a0 ...}\n```\n```\n{\u00a0 ...\u00a0 \"trainingTaskInputs\": {\u00a0 \u00a0 ...\u00a0 \u00a0 \"trialJobSpec\": {\u00a0 \u00a0 \u00a0 ...\u00a0 \u00a0 \u00a0 \"enableWebAccess\": true\u00a0 \u00a0 }\u00a0 }\u00a0 ...}\n```For an example of sending an API request to create a custom `TrainingPipeline` , see [Creating trainingpipelines](/vertex-ai/docs/training/create-training-pipeline) .To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\nSet the `enable_web_access` parameter to `true` when you run one of the following methods:- If you want to create a `CustomJob` , use the [CustomJob.run method](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob#google_cloud_aiplatform_CustomJob_run) .\n- If you want to create a `HyperparameterTuningJob` use the [HyperparameterTuningJob.run method](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.HyperparameterTuningJob#google_cloud_aiplatform_HyperparameterTuningJob_run) .\n- If you want to create a custom `TrainingPipeline` , use one of the following methods:- [CustomTrainingJob.run](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomTrainingJob#google_cloud_aiplatform_CustomTrainingJob_run) \n- [CustomContainerTrainingJob.run](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomContainerTrainingJob#google_cloud_aiplatform_CustomContainerTrainingJob_run) \n- [CustomPythonPackageTrainingJob.run](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomPythonPackageTrainingJob#google_cloud_aiplatform_CustomPythonPackageTrainingJob_run) \n## Navigate to an interactive shell\nAfter you have initiated custom training according to the guidance in the preceding section, Vertex AI generates one or more URIs that you can use to access interactive shells. Vertex AI generates a unique URI for each [training node](/vertex-ai/docs/training/distributed-training) in your job.\nYou can navigate to an interactive shell in one of the following ways:\n- Click a link in the Google Cloud console\n- Use the Vertex AI API to get the shell's web access URI\n### Navigate from the Google Cloud console\n- In the Google Cloud console, in the **Vertex AI** section, go to one of the following pages:- If you aren't using hyperparameter tuning, go to the **Custom jobs** page: [Go to Custom jobs](https://console.cloud.google.com/vertex-ai/training/custom-jobs) \n- If you are using hyperparameter tuning, go to the **Hyperparameter tuning jobs** page: [Go to Hyperparameter tuning jobs](https://console.cloud.google.com/vertex-ai/training/hyperparameter-tuning-jobs) \n- Click the name of your custom training resource.If you created a `TrainingPipeline` for custom training, click the name of the `CustomJob` or `HyperparameterTuningJob` that was created by your `TrainingPipeline` . For example, if your pipeline has the name `` , this might be called `` `-custom-job` or `` `-hyperparameter-tuning-job` .\n- On the page for your job, click **Launch web terminal** . If your job uses multiple nodes, click **Launch web terminal** next to the node for which you want an interactive shell.Note that you can only access an interactive shell while the job is running. If you don't see **Launch web terminal** , this might be because Vertex AI hasn't started running your job yet, or because the job has already finished or failed. If the job's **Status** is `Queued` or `Pending` , wait a minute; then try refreshing the page.If you are using hyperparameter tuning, there are separate **Launch webterminal** links for each trial.\n### Get the web access URI from the API\nUse the [projects.locations.customJobs.get APImethod](/vertex-ai/docs/reference/rest/v1/projects.locations.customJobs/get) or the [projects.locations.hyperparameterTuningJobs.get APImethod](/vertex-ai/docs/reference/rest/v1/projects.locations.hyperparameterTuningJobs/get) to see the URIs that you can use to access interactive shells.\n**Note:** If you created a `TrainingPipeline` for custom training, run the appropriate `get` method on the `CustomJob` identified by the `TrainingPipeline` 's `trainingTaskMetadata.backingCustomJob` field or the `HyperparameterTuningJob` identified by the `TrainingPipeline` 's `trainingTaskMetadata.backingHyperparameterTuningJob` field.\nDepending on which type of custom training resource you are using, select one of the following tabs to see examples of how to find the `webAccessUris` API field, which contains an interactive shell URI for each node in your job:\nThe following tabs show different ways to send a `projects.locations.customJobs.get` request:\nRun the [gcloud ai custom-jobs describecommand](/sdk/gcloud/reference/ai/custom-jobs/describe) :\n```\ngcloud ai custom-jobs describe JOB_ID \\\u00a0 --region=LOCATION \\\u00a0 --format=json\n```\nReplace the following:- : The numerical ID of your job. This ID is the last last part of the job's `name` field. You might have seen the ID when you created the job. (If you don't know your job's ID, you can run the [gcloud ai custom-jobs listcommand](/sdk/gcloud/reference/ai/custom-jobs/list) and look for the appropriate job.)\n- : The region where you created the job.\nBefore using any of the request data, make the following replacements:- : The region where you created the job.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The numerical ID of your job. This ID is the last last part of the job's `name` field. You might have seen the ID when you created the job.\nHTTP method and URL:\n```\nGET https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/customJobs/JOB_ID\n```\nTo send your request, expand one of these options:In the output, look for the following:\n```\n{\u00a0 ...\u00a0 \"state\": \"JOB_STATE_RUNNING\",\u00a0 ...\u00a0 \"webAccessUris\": {\u00a0 \u00a0 \"workerpool0-0\": \"INTERACTIVE_SHELL_URI\"\u00a0 }}\n```\nIf you don't see the `webAccessUris` field, this might be because Vertex AI hasn't started running your job yet. Verify that you see `JOB_STATE_RUNNING` in the `state` field. If the state is `JOB_STATE_QUEUED` or `JOB_STATE_PENDING` , wait a minute; then try getting the project info again.\nThe following tabs show different ways to send a `projects.locations.hyperparameterTuningJobs.get` request:\nRun the [gcloud ai hp-tuning-jobs describecommand](/sdk/gcloud/reference/ai/hp-tuning-jobs/describe) :\n```\ngcloud ai hp-tuning-jobs describe JOB_ID \\\u00a0 --region=LOCATION \\\u00a0 --format=json\n```\nReplace the following:- : The numerical ID of your job. This ID is the last last part of the job's `name` field. You might have seen the ID when you created the job. (If you don't know your job's ID, you can run the [gcloud ai hp-tuning-jobs listcommand](/sdk/gcloud/reference/ai/hp-tuning-jobs/list) and look for the appropriate job.)\n- : The region where you created the job.\nBefore using any of the request data, make the following replacements:- : The region where you created the job.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The numerical ID of your job. This ID is the last last part of the job's `name` field. You might have seen the ID when you created the job.\nHTTP method and URL:\n```\nGET https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/hyperparameterTuningJobs/JOB_ID\n```\nTo send your request, expand one of these options:In the output, look for the following:\n```\n{\u00a0 ...\u00a0 \"state\": \"JOB_STATE_RUNNING\",\u00a0 ...\u00a0 \"trials\": [\u00a0 \u00a0 ...\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 ...\u00a0 \u00a0 \u00a0 \"state\": \"ACTIVE\",\u00a0 \u00a0 \u00a0 ...\u00a0 \u00a0 \u00a0 \"webAccessUris\": {\u00a0 \u00a0 \u00a0 \u00a0 \"workerpool0-0\": \"INTERACTIVE_SHELL_URI\"\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ],}\n```\nIf you don't see the `webAccessUris` field, this might be because Vertex AI hasn't started running your job yet. Verify that you see `JOB_STATE_RUNNING` in the `state` field. If the state is `JOB_STATE_QUEUED` or `JOB_STATE_PENDING` , wait a minute; then try getting the project info again.\nVertex AI provides a set of interactive shell URIs for each [hyperparameter tuningtrial](/vertex-ai/docs/training/hyperparameter-tuning-overview#understanding-trials) as the trial enters the `ACTIVE` state. If you want to get interactive shell URIs for later trials, get the job info again after those trials start.\nThe preceding example shows the expected output for single-replica training: one URI for the primary training node. If you are performing distributed training, the output contains one URI for each training node, identified by worker pool.\nFor example, if your job has a primary worker pool with one replica and a secondary worker pool with two replicas, then the `webAccessUris` field looks similar to the following:\n```\n{\u00a0 \"workerpool0-0\": \"URI_FOR_PRIMARY\",\u00a0 \"workerpool1-0\": \"URI_FOR_FIRST_SECONDARY\",\u00a0 \"workerpool1-1\": \"URI_FOR_SECOND_SECONDARY\"}\n```\n## Use an interactive shell\nTo use the interactive shell for a training node, navigate to one of the URIs that you found in the preceding section. A Bash shell appears in your browser, giving you access to the file system of the container where Vertex AI is running your training code.\nThe following sections describe some things to consider as you use the shell and provide some examples of monitoring tools you might use in the shell.\n### Prevent the job from ending\nWhen Vertex AI finishes running your job or trial, you will immediately lose access to your interactive shell. If this happens, you might see the message `command terminated with exit code 137` or the shell might stop responding. If you created any files in the container's file system, they will not persist after the job ends.\nIn some cases, you might want to purposefully make your job run longer in order to debug with an interactive shell. For example, you can add code like the following to your training code in order to make the job keep running for at least an hour after an exception occurs:\n```\nimport timeimport tracebacktry:\u00a0 \u00a0 # Replace with a function that runs your training code\u00a0 \u00a0 train_model()except Exception as e:\u00a0 \u00a0 traceback.print_exc()\u00a0 \u00a0 time.sleep(60 * 60) \u00a0# 1 hour\n```\nHowever, note that you incur [Vertex AI trainingcharges](/vertex-ai/pricing) as long as the job keeps running.\n### Check permissions issues\nThe interactive shell environment is authenticated using [application defaultcredentials (ADC)](/docs/authentication#adc) for the service account that Vertex AI uses to run your training code. You can run `gcloud auth list` in the shell for more details.\nIn the shell, you can use [gsutil](/storage/docs/gsutil) , [bq](/bigquery/docs/bq-command-line-tool) , and other tools that support ADC. This can help you verify that the job is able to access a particular Cloud Storage bucket, BigQuery table, or other Google Cloud resource that your training code needs.\n### Visualize Python execution with py-spy\n[py-spy](https://github.com/benfred/py-spy) lets you profile an executing Python program, without modifying it. To use `py-spy` in an interactive shell, do the following:\n- Install `py-spy` :```\npip3 install py-spy\n```\n- Run `ps aux` in the shell, and look for the PID of the Python training program.\n- Run any of the subcommands described in the [py-spy documentation](https://github.com/benfred/py-spy) , using the PID that you found in the preceding step.\n- If you use `py-spy record` to create an SVG file, copy this file to a Cloud Storage bucket so you can view it later on your local computer. For example:```\ngsutil cp profile.svg gs://BUCKET\n```Replace with the name of a bucket you have access to.\n### Analyze performance with perf\n[perf](https://perf.wiki.kernel.org/) lets you analyze the performance of your training node. To install the version of `perf` appropriate for your node's Linux kernel, run the following commands:\n```\napt-get updateapt-get install -y linux-tools-genericrm /usr/bin/perfLINUX_TOOLS_VERSION=$(ls /usr/lib/linux-tools | tail -n 1)ln -s \"/usr/lib/linux-tools/${LINUX_TOOLS_VERSION}/perf\" /usr/bin/perf\n```\nAfter this, you can run any of the subcommands described in the [perfdocumentation](https://perf.wiki.kernel.org/) .\n### Retrieve information about GPU usage\nGPU-enabled containers running on nodes with GPUs typically have several command-line tools preinstalled that can help you monitor GPU usage. For example:\n- Use [nvidia-smi](https://developer.nvidia.com/nvidia-system-management-interface) to monitor GPU utilization of various processes.\n- Use [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html) to collect a variety of GPU profiling information. Since `nvprof` can't attach to an existing process, you might want to use the tool to start an additional process running your training code. (This means your training code will run twice on the node.) For example:```\nnvprof -o prof.nvvp python3 -m MODULE_NAME\n```Replace with the fully-qualified name of your [trainingapplication's entry pointmodule](/vertex-ai/docs/training/create-python-pre-built-container#python-modules) ; for example, `trainer.task` .Then transfer the output file to a Cloud Storage bucket so you can analyze it later on your local computer. For example:```\ngsutil cp prof.nvvp gs://BUCKET\n```Replace with the name of a bucket you have access to.\n- If you encounter a GPU error (not a problem with your configuration or with Vertex AI), use [nvidia-bug-report.sh](https://forums.developer.nvidia.com/t/if-you-have-a-problem-please-read-this-first/27131) to create a bug report.Then transfer the report to a Cloud Storage bucket so you can analyze it later on your local computer or send it to NVIDIA. For example:```\ngsutil cp nvidia-bug-report.log.gz gs://BUCKET\n```Replace with the name of a bucket you have access to.\nIf `bash` can't find any of these NVIDIA commands, try adding `/usr/local/nvidia/bin` and `/usr/local/cuda/bin` to the shell's `PATH` :\n```\nexport PATH=\"/usr/local/nvidia/bin:/usr/local/cuda/bin:${PATH}\"\n```\n## What's next\n- Learn how to optimize the performance of your custom training jobs using [Vertex AI TensorBoard Profiler](/vertex-ai/docs/training/tensorboard-profiler) .\n- Learn more about [how Vertex AI orchestrates customtraining](/vertex-ai/docs/training/understanding-training-service) .\n- Read about [Training code requirements](/vertex-ai/docs/training/code-requirements) .", "guide": "Vertex AI"}