{"title": "Vertex AI - Prediction classes", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Prediction classes\nThe Vertex AI SDK includes the following prediction classes. One class is for batch predictions. The others are related to online predictions or Vector Search predictions. For more information, see [Overview of getting predictions on Vertex AI](/vertex-ai/docs/predictions/overview) .\n", "content": "## Batch prediction class\nA batch prediction is a group of asynchronous prediction requests. You request batch predictions from the model resource without needing to deploy the model to an endpoint. Batch predictions are suitable for when you don't need an immediate response and want to process data with a single request. [BatchPredictionJob](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.BatchPredictionJob) is the one class in the Vertex AI SDK that is specific to batch predictions.\n### BatchPredictionJob\nThe [BatchPredictionJob](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.BatchPredictionJob) class represents a group of asynchronous prediction requests. There are two ways to create a batch prediction job:\n- The preferred way to create a batch prediction job is to use the [batch_predict](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_batch_predict) method on your trained [Model](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model) . This method requires the following parameters:- `instances_format`: The format of the batch prediction request file:`jsonl`,`csv`,`bigquery`,`tf-record`,`tf-record-gzip`, or`file-list`.\n- `prediction_format`: The format of the batch prediction response file:`jsonl`,`csv`,`bigquery`,`tf-record`,`tf-record-gzip`, or`file-list`.\n- `gcs_source:`A list of one or more Cloud Storage paths to your batch prediction requests.\n- `gcs_destination_prefix`: The Cloud Storage path to which Vertex AI writes the predictions.\nThe following code is an example of how you might call [Model.batch_predict](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_batch_predict) :```\nbatch_prediction_job = model.batch_predict(\u00a0 \u00a0 instances_format=\"jsonl\",\u00a0 \u00a0 predictions_format=\"jsonl\",\u00a0 \u00a0 job_display_name=\"your_job_display_name_string\",\u00a0 \u00a0 gcs_source=['gs://path/to/my/dataset.csv'],\u00a0 \u00a0 gcs_destination_prefix='gs://path/to/my/destination',\u00a0 \u00a0 model_parameters=None,\u00a0 \u00a0 starting_replica_count=1,\u00a0 \u00a0 max_replica_count=5,\u00a0 \u00a0 machine_type=\"n1-standard-4\",\u00a0 \u00a0 sync=True)\n```\n- The second way to create a batch prediction job is to call the [BatchPredictionJob.create](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.BatchPredictionJob#google_cloud_aiplatform_BatchPredictionJob_create) method. The [BatchPredictionJob.create](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.BatchPredictionJob#google_cloud_aiplatform_BatchPredictionJob_create) method requires four parameters:- `job_display_name`: A name you that you assign to the batch prediction job. Note that while`job_display_name`is required for [BatchPredictionJob.create](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.BatchPredictionJob#google_cloud_aiplatform_BatchPredictionJob_create) , it is optional for [Model.batch_predict](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_batch_predict) .\n- `model_name`: The fully-qualified name or ID of the trained [Model](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model) you use for the batch prediction job.\n- `instances_format`: The format of the batch prediction request file:`jsonl`,`csv`,`bigquery`,`tf-record`,`tf-record-gzip`, or`file-list`.\n- `predictions_format`: The format of the batch prediction response file:`jsonl`,`csv`,`bigquery`,`tf-record`,`tf-record-gzip`, or`file-list`.\n## Online prediction classes\nOnline predictions are synchronous requests made to a model endpoint. You must deploy your model to an endpoint before you can make an online prediction request. Use online predictions when you want predictions that are generated based on application input or when you need a fast prediction response.\n### Endpoint\nBefore you can get online predictions from your model, you must deploy your model to an endpoint. When you deploy a model to an endpoint, you associate the physical machine resources with the model so it can serve online predictions.\nYou can deploy more than one model to one endpoint. You can also deploy one model to more than one endpoint. For more information, see [Considerations for deploying models](/vertex-ai/docs/general/deployment) .\nTo create an [Endpoint](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Endpoint) resource, you deploy your model. When you call the [Model.deploy](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.ModelModel#google_cloud_aiplatform_Model_deploy) method, it creates and returns an [Endpoint](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Endpoint) .\nThe following is a sample code snippet that shows how to create a custom training job, create and train a model, and then deploy the model to an endpoint.\n```\n# Create your custom training jobjob = aiplatform.CustomTrainingJob(\u00a0 \u00a0 display_name=\"my_custom_training_job\",\u00a0 \u00a0 script_path=\"task.py\",\u00a0 \u00a0 container_uri=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\",\u00a0 \u00a0 requirements=[\"google-cloud-bigquery>=2.20.0\", \"db-dtypes\"],\u00a0 \u00a0 model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\")# Start the training and create your modelmodel = job.run(\u00a0 \u00a0 dataset=dataset,\u00a0 \u00a0 model_display_name=\"my_model_name\",\u00a0 \u00a0 bigquery_destination=f\"bq://{project_id}\")# Create an endpoint and deploy your model to that endpointendpoint = model.deploy(deployed_model_display_name=\"my_deployed_model\")# Get predictions using test data in a DataFrame named 'df_my_test_data'predictions = endpoint.predict(instances=df_my_test_data)\n```\n### PrivateEndpoint\nA private endpoint is like an [Endpoint](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Endpoint) resource, except predictions are sent across a secure network to the Vertex AI online prediction service. Use a private endpoint if your organization wants to keep all traffic private.\nTo use a private endpoint, you must configure Vertex AI to peer with a Virtual Private Cloud (VPC). A VPC is required for the private prediction endpoint to connect directly with Vertex AI. For more information, see [Set up VPC network peering](/vertex-ai/docs/general/vpc-peering) and [Use private endpoints for online prediction](/vertex-ai/docs/predictions/using-private-endpoints) .\n### ModelDeploymentMonitoringJob\nUse the [ModelDeploymentMonitoringJob](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.ModelDeploymentMonitoringJob) resource to monitor your model and receive alerts if it deviates in a way that might impact the quality of your model's predictions.\nWhen the input data deviates from the data used to train your model, the model's performance can deteriorate, even if the model hasn't changed. Model monitoring analyzes input date for feature and :\n- occurs when the production feature data distribution deviates from the feature data used to train the model.\n- occurs when the production feature data changes significantly over time.\nFor more information, see [Introduction to Vertex AI modelmonitoring](/vertex-ai/docs/model-monitoring/overview) . For an example of how to implement Vertex AI monitoring with the Vertex AI SDK, see the [Vertex AI model monitoring with explainable AI featureattributions](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_monitoring/model_monitoring.ipynb) notebook on GitHub.\n## Vector Search prediction classes\nVector Search is a managed service that builds similarity indexes, or vectors, to perform similarity matching. There are two high-level steps to perform similarity matching:\n- Create a vector representation of your data. Data can be text, images, video, audio, or tabular data.\n- Vector Search uses the endpoints of the vectors you create to perform a high scale, low latency search for similar vectors.\nFor more information, see [Vector Search overview](/vertex-ai/docs/vector-search/overview) and the [Create a Vector Search index](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/matching_engine/sdk_matching_engine_for_indexing.ipynb) notebook on GitHub.\n### MatchingEngineIndex\nThe [MatchingEngineIndex](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.MatchingEngineIndex) class represents the indexes, or vectors, you create that Vector Search uses to perform its similarity search.\nThere are two search algorithms you can use for your index:\n- `TreeAhConfig`uses a shallow the tree-AH algorithm (shallow tree using asymmetric hashing). Use [MatchingEngineIndex.create_tree_ah_index](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.MatchingEngineIndex#google_cloud_aiplatform_MatchingEngineIndex_create_tree_ah_index) to create an index that uses the tree-AH algorithm algorithm.\n- `BruteForceConfig`uses a standard linear search) Use [MatchingEngineIndex.create_brute_force_index](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.MatchingEngineIndex#google_cloud_aiplatform_MatchingEngineIndex_create_brute_force_index) to create an index that uses a standard linear search.\nFor more information about how you can configure your indices, see [Configure indices](/vertex-ai/docs/vector-search/configuring-indexes) .\nThe following code is an example of creating an index that uses the tree-AH algorithm:\n```\nmy_tree_ah_index = aiplatform.Index.create_tree_ah_index(\u00a0 \u00a0 display_name=\"my_display_name\",\u00a0 \u00a0 contents_delta_uri=\"gs://my_bucket/embeddings\",\u00a0 \u00a0 dimensions=1,\u00a0 \u00a0 approximate_neighbors_count=150,\u00a0 \u00a0 distance_measure_type=\"SQUARED_L2_DISTANCE\",\u00a0 \u00a0 leaf_node_embedding_count=100,\u00a0 \u00a0 leaf_nodes_to_search_percent=50,\u00a0 \u00a0 description=\"my description\",\u00a0 \u00a0 labels={ \"label_name\": \"label_value\" })\n```\nThe following code is an example of creating an index that uses the brute force algorithm:\n```\nmy_brute_force_index = aiplatform.Index.create_brute_force_index(\u00a0 \u00a0 display_name=\"my_display_name\",\u00a0 \u00a0 contents_delta_uri=\"gs://my_bucket/embeddings\",\u00a0 \u00a0 dimensions=1,\u00a0 \u00a0 approximate_neighbors_count=150,\u00a0 \u00a0 distance_measure_type=\"SQUARED_L2_DISTANCE\",\u00a0 \u00a0 description=\"my description\",\u00a0 \u00a0 labels={ \"label_name\": \"label_value\" })\n```\n### MatchingEngineIndexEndpoint\nUse the [MatchingEngineIndexEndpoint](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.MatchingEngineIndexEndpoint) class to create and retrieve an endpoint. After you deploy a model to your endpoint, you get an IP address that you use to run your queries.\nThe following code is an example of creating a matching engine index endpoint and then deploying a matching engine index to it:\n```\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\u00a0 \u00a0 display_name=\"sample_index_endpoint\",\u00a0 \u00a0 description=\"index endpoint description\",\u00a0 \u00a0 network=\"projects/123456789123/global/networks/my_vpc\")my_index_endpoint = my_index_endpoint.deploy_index(\u00a0 \u00a0 index=my_tree_ah_index, deployed_index_id=\"my_matching_engine_index_id\")\n```\n## What's next\n- Learn about the [Vertex AI SDK](/vertex-ai/docs/python-sdk/use-vertex-ai-python-sdk) .", "guide": "Vertex AI"}