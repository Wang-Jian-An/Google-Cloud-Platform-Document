{"title": "Vertex AI - Export model artifacts for prediction and explanation", "url": "https://cloud.google.com/vertex-ai/docs/training/exporting-model-artifacts", "abstract": "# Vertex AI - Export model artifacts for prediction and explanation\nVertex AI offers [prebuilt containers](/vertex-ai/docs/predictions/pre-built-containers) to serve predictions and explanations from models trained using the following machine learning (ML) frameworks:\n- TensorFlow\n- PyTorch\n- XGBoost\n- scikit-learn\nTo use one of these prebuilt containers, you must save your model as one or more that comply with the requirements of the prebuilt container. These requirements apply whether or not your model artifacts are created on Vertex AI.\nIf you want to use a [custom container to serve predictions](/vertex-ai/docs/predictions/use-custom-container) , you don't need to comply with the requirements in this page, but you can still use them as guidelines.\n", "content": "## Framework-specific requirements for exporting to prebuilt containers\nDepending on [which ML framework you plan to use forprediction](/vertex-ai/docs/predictions/pre-built-containers) , you must export model artifacts in different formats. The following sections describe the acceptable model formats for each ML framework.\n### TensorFlow\nIf you [use TensorFlow to train amodel](/vertex-ai/docs/training/pre-built-containers#tensorflow) , export your model as a [TensorFlow SavedModeldirectory](https://www.tensorflow.org/guide/saved_model) .\nThere are several ways to export SavedModels from TensorFlow training code. The following list describes a few ways that work for various TensorFlow APIs:\n- If you have used Keras for training, [use tf.keras.Model.save to export aSavedModel](https://www.tensorflow.org/guide/keras/save_and_serialize#whole-model_saving_loading) \n- If you use an Estimator for training, [usetf.estimator.Estimator.export_saved_model to export aSavedModel](https://www.tensorflow.org/guide/estimator#savedmodels_from_estimators) .\n- Otherwise, [usetf.saved_model.save](https://www.tensorflow.org/guide/saved_model#saving_a_custom_model) or [usetf.compat.v1.saved_model.SavedModelBuilder](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/builder) .If you aren't using Keras or an Estimator, then make sure to [use the servetag and serving_default signature when you export yourSavedModel](https://www.tensorflow.org/tfx/serving/serving_basic#train_and_export_tensorflow_model) in order to make ensure Vertex AI can use your model artifacts to serve predictions. Keras and Estimator handle this automatically. Learn more about [specifying signatures duringexport](https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export) .\nTo serve predictions using these artifacts, create a `Model` with the [prebuiltcontainer for prediction](/vertex-ai/docs/predictions/pre-built-containers#tensorflow) matching the version of TensorFlow that you used for training.\nIf you want to [get explanations](/vertex-ai/docs/explainable-ai/overview) from a `Model` that uses a TensorFlow prebuilt container to serve predictions, then read the [additionalrequirements for exporting a TensorFlow model forVertex Explainable AI](/vertex-ai/docs/explainable-ai/tensorflow#exporting) .\nIf you want to enable request batching for a `Model` that uses a TensorFlow prebuilt container to serve predictions, then include config/batching_parameters_config in the same gcs directory as `saved_model.pb` file. To configure the batching config file, please refer to the [Tensorflow'sofficial documentation](https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration) .\n### PyTorch\nYou must package the model artifacts including either a [default](https://pytorch.org/serve/#default-handlers) or [custom](https://pytorch.org/serve/custom_service.html) handler by creating an archive file using [Torch model archiver](https://github.com/pytorch/serve/tree/master/model-archiver) . The prebuilt PyTorch images expect the archive to be named `model.mar` , so make sure you set the model-name to 'model'.\nFor information about optimizing the memory usage, latency or throughput of a PyTorch model served with TorchServe, see the [PyTorch performance guide](https://github.com/pytorch/serve/blob/master/docs/performance_guide.md) .\n### XGBoost\nIf you [use XGBoost to train amodel](/vertex-ai/docs/training/pre-built-containers#xgboost) , you may export the trained model in one of three ways:\n- Use`xgboost.Booster`'s`save_model`method to export a file named`model.bst`.\n- Use the`joblib`library to export a file named`model.joblib`.\n- Use Python's`pickle`module to export a file named`model.pkl`.\nYour model artifact's filename must exactly match one of these options.\nThe following tabbed examples show how to train and export a model in each of the three ways:\n**Note:** The following examples assume that you are training on Vertex AI and use the [AIP_MODEL_DIR environment variable set byVertex AI](/vertex-ai/docs/training/code-requirements#environment-variables) .\n```\nimport osfrom google.cloud import storagefrom sklearn import datasetsimport xgboost as xgbdigits = datasets.load_digits()dtrain = xgb.DMatrix(digits.data, label=digits.target)bst = xgb.train({}, dtrain, 20)artifact_filename = 'model.bst'# Save model artifact to local filesystem (doesn't persist)local_path = artifact_filenamebst.save_model(local_path)# Upload model artifact to Cloud Storagemodel_directory = os.environ['AIP_MODEL_DIR']storage_path = os.path.join(model_directory, artifact_filename)blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())blob.upload_from_filename(local_path)\n```\n```\nimport osfrom google.cloud import storagefrom sklearn import datasetsimport joblibimport xgboost as xgbdigits = datasets.load_digits()dtrain = xgb.DMatrix(digits.data, label=digits.target)bst = xgb.train({}, dtrain, 20)artifact_filename = 'model.joblib'# Save model artifact to local filesystem (doesn't persist)local_path = artifact_filenamejoblib.dump(bst, local_path)# Upload model artifact to Cloud Storagemodel_directory = os.environ['AIP_MODEL_DIR']storage_path = os.path.join(model_directory, artifact_filename)blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())blob.upload_from_filename(local_path)\n```\n```\nimport osimport picklefrom google.cloud import storagefrom sklearn import datasetsimport xgboost as xgbdigits = datasets.load_digits()dtrain = xgb.DMatrix(digits.data, label=digits.target)bst = xgb.train({}, dtrain, 20)artifact_filename = 'model.pkl'# Save model artifact to local filesystem (doesn't persist)local_path = artifact_filenamewith open(local_path, 'wb') as model_file:\u00a0 pickle.dump(bst, model_file)# Upload model artifact to Cloud Storagemodel_directory = os.environ['AIP_MODEL_DIR']storage_path = os.path.join(model_directory, artifact_filename)blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())blob.upload_from_filename(local_path)\n```\nTo serve predictions using this artifact, create a `Model` with the [prebuiltcontainer for prediction](/vertex-ai/docs/predictions/pre-built-containers#xgboost) matching the version of XGBoost that you used for training.\n### scikit-learn\nIf you [use scikit-learn to train amodel](/vertex-ai/docs/training/pre-built-containers#scikit-learn) , you may export it in one of two ways:\n- Use the`joblib`library to export a file named`model.joblib`.\n- Use Python's`pickle`module to export a file named`model.pkl`.\nYour model artifact's filename must exactly match one of these options. You can export standard scikit-learn estimators or [scikit-learnpipelines](https://scikit-learn.org/stable/modules/compose.html) .\nThe following tabbed examples show how to train and export a model in each of the two ways:\n**Note:** The following examples assume that you are training on Vertex AI and use the [AIP_MODEL_DIR environment variable set byVertex AI](/vertex-ai/docs/training/code-requirements#environment-variables) .\n```\nimport osfrom google.cloud import storagefrom sklearn import datasetsfrom sklearn.ensemble import RandomForestClassifierimport joblibdigits = datasets.load_digits()classifier = RandomForestClassifier()classifier.fit(digits.data, digits.target)artifact_filename = 'model.joblib'# Save model artifact to local filesystem (doesn't persist)local_path = artifact_filenamejoblib.dump(classifier, local_path)# Upload model artifact to Cloud Storagemodel_directory = os.environ['AIP_MODEL_DIR']storage_path = os.path.join(model_directory, artifact_filename)blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())blob.upload_from_filename(local_path)\n```\n```\nimport osimport picklefrom google.cloud import storagefrom sklearn import datasetsfrom sklearn.ensemble import RandomForestClassifierdigits = datasets.load_digits()classifier = RandomForestClassifier()classifier.fit(digits.data, digits.target)artifact_filename = 'model.pkl'# Save model artifact to local filesystem (doesn't persist)local_path = artifact_filenamewith open(local_path, 'wb') as model_file:\u00a0 pickle.dump(classifier, model_file)# Upload model artifact to Cloud Storagemodel_directory = os.environ['AIP_MODEL_DIR']storage_path = os.path.join(model_directory, artifact_filename)blob = storage.blob.Blob.from_string(storage_path, client=storage.Client())blob.upload_from_filename(local_path)\n```\nTo serve predictions using this artifact, create a `Model` with the [prebuiltcontainer for prediction](/vertex-ai/docs/predictions/pre-built-containers#scikit-learn) matching the version of scikit-learn that you used for training.\n## What's next\n- Read about [additional requirements for your trainingcode](/vertex-ai/docs/training/code-requirements) that you must consider when performing custom training on Vertex AI.\n- Learn how to [create a custom TrainingPipelineresource](/vertex-ai/docs/training/create-training-pipeline) in order to run your custom training code and create a `Model` from the resulting model artifacts.\n- Learn how to [import a Model](/vertex-ai/docs/model-registry/import-model) from model artifacts in Cloud Storage. This applies to model artifacts that you have [created using a CustomJobresource](/vertex-ai/docs/training/create-custom-job) or a [HyperparameterTuningJobresource](/vertex-ai/docs/training/using-hyperparameter-tuning) , as well as model artifacts that you train outside of Vertex AI.", "guide": "Vertex AI"}