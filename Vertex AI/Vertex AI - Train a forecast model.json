{"title": "Vertex AI - Train a forecast model", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Train a forecast model\nThis page shows you how to train a forecast model from a tabular dataset using either the Google Cloud console or the Vertex AI API.\n", "content": "## Before you begin\nBefore you can train a forecast model, you must complete the following:\n- [Prepare your training data](/vertex-ai/docs/tabular-data/forecasting/prepare-data) \n- [Create a Vertex AI dataset](/vertex-ai/docs/tabular-data/forecasting/create-dataset) ## Train a model\n- In the Google Cloud console, in the Vertex AI section, go to the **Datasets** page. [Go to the Datasets page](https://console.cloud.google.com/vertex-ai/datasets) \n- Click the name of the dataset you want to use to train your model to open its details page.\n- If your data type uses annotation sets, select the annotation set you want to use for this model.\n- Click **Train new model** .\n- Select **Other** .\n- In the **Training method** page, configure as follows:- Select the model training method. To learn more, see [Model training methods](/vertex-ai/docs/tabular-data/forecasting-parameters#training-methods) .\n- Click **Continue** .\n- In the **Model details** page, configure as follows:- Enter the display name for your new model.\n- Select your target column.The target column is the value that the model will forecast. Learn more about [target column requirements](/vertex-ai/docs/tabular-data/forecasting/prepare-data#data-structure) .\n- If you did not set your [Series identifier and Timestamp columns](/vertex-ai/docs/tabular-data/forecasting/prepare-data#data-structure) on your dataset, select them now.\n- Select your **Data granularity** . Select `Daily` if you would like to use holiday effect modeling. [Learn how to choose the data granularity](/vertex-ai/docs/tabular-data/bp-tabular#granularity) .\n- **Optional** : In the **Holiday regions** dropdown, choose one or more geographical regions to enable holiday effect modeling. During training, Vertex AI creates holiday categorical features within the model based on the date from the **Timestamp** column and the specified geographical regions. You can select this option only when **Data granularity** is set to `Daily` . By default, holiday effect modeling is disabled. To learn about the geographical regions used for holiday effect modeling, see [Holiday regions](/vertex-ai/docs/tabular-data/forecasting-parameters#holiday-regions) .\n- Enter your **Context window** and **Forecast horizon** .The forecast horizon determines how far into the future the model forecasts the target value for each row of prediction data. The **Forecast horizon** is specified in units of **Data granularity** .The context window sets how far back the model looks during training (and for forecasts). In other words, for each training datapoint, the context window determines how far back the model looks for predictive patterns. The **Context window** is specified in units of **Data granularity** . [Learn more](/vertex-ai/docs/tabular-data/forecasting-parameters#forecast-window) .\n- If you would like to export your test dataset to BigQuery, check **Export test dataset to BigQuery** and provide the name of the table.\n- If you want to manually control your data split or configure the forecasting window, open the **Advanced options** .\n- The default data split is chronological, with the standard 80/10/10 percentages. If you would like to manually specify which rows are assigned to which split, select **Manual** and specify your Data split column.Learn more about [data splits](/vertex-ai/docs/tabular-data/data-splits) .\n- Select a rolling window strategy for forecast window generation. The default strategy is **Count** .- **Count** : Set the value for the maximum number of windows in the textbox provided.\n- **Stride** : Set the value of the stride length in the textbox provided.\n- **Column** : Select the appropriate column name from the dropdown provided.\nTo learn more, see [Rolling window strategies](/vertex-ai/docs/tabular-data/forecasting-parameters#rolling_window_strategies) .\n- Click **Continue** .\n- In the **Training options** page, configure as follows:- If you haven't already, click **Generate statistics** .Generating statistics populates the **Transformation** dropdown menus.\n- Review your column list and exclude any columns from training that should not be used to train the model.If you are using a data split column, it should be included.\n- Review the transformations selected for your included features and make any required updates.Rows containing data that is invalid for the selected transformation are excluded from training. Learn more about [transformations](/vertex-ai/docs/datasets/data-types-tabular#transformations) .\n- For each column you included for training, specify the **Feature type** for how that feature relates to its time series, and whether it is available at forecast time. Learn more about [feature type and availability](/vertex-ai/docs/tabular-data/forecasting-parameters#feature-type) .\n- If you want to specify a weight column, change your optimization objective from the default, or enable hierarchical forecasting, open **Advanced options** .\n- **Optional** . If you want to specify a weight column, select it from the dropdown list. Learn more about [weight columns](/vertex-ai/docs/tabular-data/forecasting/prepare-data#weight) .\n- **Optional** . If you want to select the optimization objective, select it from the list. Learn more [optimization objectives](/vertex-ai/docs/tabular-data/forecasting-parameters#optimization-objectives) .\n- **Optional** . If you want to use hierarchical forecasting, select **Enablehierarchical forecasting** . You can choose between three grouping options:- `No grouping`\n- `Group by columns`\n- `Group all`\nYou can also choose to set the following aggregate loss weights:- `Group total weight`. This field can be set only if you select the`Group by columns`or the`Group all`option.\n- `Temporal total weight`.\n- `Group temporal total weight`. This field can be set only if you select the`Group by columns`or the`Group all`option.\nLearn more about [hierarchical forecasting](/vertex-ai/docs/tabular-data/forecasting/hierarchical) .\n- Click **Continue** .\n- In the **Compute and pricing** page, configure as follows:- Enter the maximum number of hours you want your model to train for. This setting helps you put a cap on the training costs. The actual time elapsed can be longer than this value, because there are other operations involved in creating a new model.Suggested training time is related to the size of your forecast horizon and your training data. The table below provides some sample forecasting training runs, and the range of training time that was needed to train a high-quality model.| Rows  | Features | Forecast horizon | Training time |\n|:-----------|-----------:|-------------------:|:----------------|\n| 12 million |   10 |     6 | 3-6 hours  |\n| 20 million |   50 |     13 | 6-12 hours  |\n| 16 million |   30 |    365 | 24-48 hours  |For information about training pricing, see the [pricing page](/vertex-ai/pricing#tabular-data) .\n- Click **Start Training** .Model training can take many hours, depending on the size and complexity of your data and your training budget, if you specified one. You can close this tab and return to it later. You will receive an email when your model has completed training.Tabular training data in Cloud Storage or BigQuery  is not imported into Vertex AI. (When you import from local files, they are  imported into Cloud Storage.) When you create a dataset  with tabular data, the data is associated with the dataset. Changes you make to  your data source in Cloud Storage or BigQuery after dataset creation  are incorporated into models subsequently trained with that dataset. A snapshot of the  dataset is taken when model training begins.\nTabular training data in Cloud Storage or BigQuery  is not imported into Vertex AI. (When you import from local files, they are  imported into Cloud Storage.) When you create a dataset  with tabular data, the data is associated with the dataset. Changes you make to  your data source in Cloud Storage or BigQuery after dataset creation  are incorporated into models subsequently trained with that dataset. A snapshot of the  dataset is taken when model training begins.\nSelect a tab for your language or environment:You use the [trainingPipelines.create](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines/create) command  to train a model.\nBefore using any of the request data, make the following replacements:- : Your region.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : Display name for the training pipeline created for this operation.\n- : The model training method.- Time series Dense Encoder (TiDE)`gs://google-cloud-aiplatform/schema/trainingjob/definition/time_series_dense_encoder_forecasting_1.0.0.yaml`\n- Temporal Fusion Transformer (TFT)`gs://google-cloud-aiplatform/schema/trainingjob/definition/temporal_fusion_transformer_time_series_forecasting_1.0.0.yaml`\n- AutoML (L2L)`gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_forecasting_1.0.0.yaml`\n- Seq2Seq+`gs://google-cloud-aiplatform/schema/trainingjob/definition/seq2seq_plus_time_series_forecasting_1.0.0.yaml`\nTo learn more, see [Model training methods](/vertex-ai/docs/tabular-data/forecasting-parameters#training-methods) .\n- : The column (value) you want this model to predict.\n- : The time column. [Learn more](/vertex-ai/docs/tabular-data/forecasting/prepare-data#data-structure) .\n- : The time series identifier column. [Learn more](/vertex-ai/docs/tabular-data/forecasting/prepare-data#data-structure) .\n- : (Optional) The weight column. [Learn more](/vertex-ai/docs/tabular-data/forecasting/prepare-data#weight) .\n- : The maximum amount of time you want the model to train, in  milli node hours (1,000 milli node hours equals one node hour).\n- : The unit to use for the granularity of your training data and your  forecast horizon and context window. Can be`minute`,`hour`,`day`,`week`,`month`, or`year`. Select`day`if you would like to use holiday effect modeling. [Learn how to choose the data granularity](/vertex-ai/docs/tabular-data/bp-tabular#granularity) .\n- : The number of granularity units that make up the interval  between observations in your training data. Must be one for all units except minutes, which can  be 1, 5, 10, 15, or 30. [Learn how to choose the data granularity](/vertex-ai/docs/tabular-data/bp-tabular#granularity) .\n- : Column names in your training input table that identify the grouping  for the hierarchy level. The column(s) must be `time_series_attribute_columns`. [Learn more](/vertex-ai/docs/tabular-data/forecasting/hierarchical) .\n- : Weight of the group aggregated loss relative to the individual  loss. Disabled if set to `0.0` or is not set. If the group column is not set, all time series  will be treated as part of the same group and is aggregated over all time series. [Learn more](/vertex-ai/docs/tabular-data/forecasting/hierarchical) .\n- : Weight of the time aggregated loss relative to the individual  loss. Disabled if set to `0.0` or is not set. [Learn more](/vertex-ai/docs/tabular-data/forecasting/hierarchical) .\n- : Weight of the total (group x time) aggregated loss  relative to the individual loss. Disabled if set to `0.0` or is not set. If the group column is  not set, all time series will be treated as part of the same group and is aggregated over all  time series. [Learn more](/vertex-ai/docs/tabular-data/forecasting/hierarchical) .\n- : (Optional) You can select one or more geographical regions to enable  holiday effect modeling. During training, Vertex AI creates holiday categorical  features within the model based on the date fromand the specified  geographical regions. To enable it, setto`day`and specify one or more regions in thefield.  By default, holiday effect modeling is disabled. To learn more, see [Holiday regions](/vertex-ai/docs/tabular-data/forecasting-parameters#holiday-regions) .\n- : The forecast horizon determines how far into the future the model forecasts the target value for each row of prediction data. The forecast horizon is specified in  units of data granularity (). [Learn more](/vertex-ai/docs/tabular-data/forecasting-parameters#forecast-window) .\n- : The context window sets how far back the model looks during training (and for forecasts). In other words, for each training datapoint, the context window determines how far back the model looks for predictive patterns. The context window is specified in  units of data granularity (). [Learn more](/vertex-ai/docs/tabular-data/forecasting-parameters#forecast-window) .\n- : By default, Vertex AI minimizes the  root-mean-squared error (RMSE). If you want a different optimization objective for your  forecast model, choose one of the options in [Optimization objectives for forecast models](/vertex-ai/docs/tabular-data/forecasting-parameters#optimization-objectives) .  If you choose to minimize the quantile loss, you must also specify a value for.\n- : (Optional) If set to`true`,  Vertex AI models the probability distribution of the forecast. Probabilistic  inference can improve model quality by handling noisy data and quantifying uncertainty. Ifare specified, then Vertex AI also returns the quantiles of  the probability distribution. Probabilistic inference is compatible only with the`Time series Dense Encoder (TiDE)` ``and the` ``AutoML (L2L)`` `training  methods. It is incompatible with hierarchical forecasting and the` ``minimize-quantile-loss`` `optimization objective.``\n- ``` `: Quantiles to use for the`minimize-quantile-loss`optimization  objective and probabilistic inference. Provide a list of up to five unique numbers between`0`and`1`, exclusive.` `` `: The name or names of the columns that are time series  attributes. [Learn more](/vertex-ai/docs/tabular-data/forecasting-parameters#feature-type) .` `` `: The name or names of the covariate columns whose value  is known at forecast time. [Learn more](/vertex-ai/docs/tabular-data/forecasting-parameters#feature-type) .` `` `: The name or names of the covariate columns whose value  is unknown at forecast time. [Learn more](/vertex-ai/docs/tabular-data/forecasting-parameters#feature-type) .` `` `: The transformation type is provided for each column used to  train the model. [Learn more](/vertex-ai/docs/datasets/data-types-tabular#transformations) .` `` `: The name of the column with the specified transformation type. Every  column used to train the model must be specified.` `` `: Display name for the newly trained model.` `` `: ID for the training Dataset.` `` `You can provide a`Split`object to control your data split. For information about  controlling data split, see [Control the data split using REST](#data-split) .` `` `You can provide a`windowConfig`object to configure a rolling window strategy for  forecast window generation. For further information, see [Configure the rolling window strategy using REST](#rolling-window) .` `` `: Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) ` ```\n``` `HTTP method and URL:` `` `\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/trainingPipelines\n```\n` `` `Request JSON body:` `` `\n```\n{\n \"displayName\": \"TRAINING_PIPELINE_DISPLAY_NAME\",\n \"trainingTaskDefinition\": \"TRAINING_TASK_DEFINITION\",\n \"trainingTaskInputs\": {\n  \"targetColumn\": \"TARGET_COLUMN\",\n  \"timeColumn\": \"TIME_COLUMN\",\n  \"timeSeriesIdentifierColumn\": \"TIME_SERIES_IDENTIFIER_COLUMN\",\n  \"weightColumn\": \"WEIGHT_COLUMN\",\n  \"trainBudgetMilliNodeHours\": TRAINING_BUDGET,\n  \"dataGranularity\": {\"unit\": \"GRANULARITY_UNIT\", \"quantity\": GRANULARITY_QUANTITY},\n  \"hierarchyConfig\": {\"groupColumns\": GROUP_COLUMNS, \"groupTotalWeight\": GROUP_TOTAL_WEIGHT, \"temporalTotalWeight\": TEMPORAL_TOTAL_WEIGHT, \"groupTemporalTotalWeight\": GROUP_TEMPORAL_TOTAL_WEIGHT}\n  \"holidayRegions\" : [\"HOLIDAY_REGIONS_1\", \"HOLIDAY_REGIONS_2\", ...]\n  \"forecast_horizon\": FORECAST_HORIZON,\n  \"context_window\": CONTEXT_WINDOW,\n  \"optimizationObjective\": \"OPTIMIZATION_OBJECTIVE\",\n  \"quantiles\": \"QUANTILES\",\n  \"enableProbabilisticInference\": \"PROBABILISTIC_INFERENCE\",\n  \"time_series_attribute_columns\": [\"TIME_SERIES_ATTRIBUTE_COL_1\", \"TIME_SERIES_ATTRIBUTE_COL_2\", ...]\n  \"available_at_forecast_columns\": [\"AVAILABLE_AT_FORECAST_COL_1\", \"AVAILABLE_AT_FORECAST_COL_2\", ...]\n  \"unavailable_at_forecast_columns\": [\"UNAVAILABLE_AT_FORECAST_COL_1\", \"UNAVAILABLE_AT_FORECAST_COL_2\", ...]\n  \"transformations\": [   {\"TRANSFORMATION_TYPE_1\": {\"column_name\" : \"COLUMN_NAME_1\"} },\n   {\"TRANSFORMATION_TYPE_2\": {\"column_name\" : \"COLUMN_NAME_2\"} },\n   ...\n },\n \"modelToUpload\": {\"displayName\": \"MODEL_DISPLAY_NAME\"},\n \"inputDataConfig\": {\n  \"datasetId\": \"DATASET_ID\",\n }\n}\n```\n` `` `To send your request, expand one of these options:` `` `` `` `` `` `You should receive a JSON response similar to the following:` `` `\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION/trainingPipelines/TRAINING_PIPELINE_ID\",\n \"displayName\": \"myModelName\",\n \"trainingTaskDefinition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_tabular_1.0.0.yaml\",\n \"modelToUpload\": {\n \"displayName\": \"myModelName\"\n },\n \"state\": \"PIPELINE_STATE_PENDING\",\n \"createTime\": \"2020-08-18T01:22:57.479336Z\",\n \"updateTime\": \"2020-08-18T01:22:57.479336Z\"\n}\n```\n` ```\n``` `### PythonTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_training_pipeline_forecasting_tide_sample.py) \n```\ndef create_training_pipeline_forecasting_time_series_dense_encoder_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 dataset_id: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 model_display_name: str = \"my_model\",\u00a0 \u00a0 target_column: str = \"target_column\",\u00a0 \u00a0 time_column: str = \"date\",\u00a0 \u00a0 time_series_identifier_column: str = \"time_series_id\",\u00a0 \u00a0 unavailable_at_forecast_columns: List[str] = [],\u00a0 \u00a0 available_at_forecast_columns: List[str] = [],\u00a0 \u00a0 forecast_horizon: int = 1,\u00a0 \u00a0 data_granularity_unit: str = \"week\",\u00a0 \u00a0 data_granularity_count: int = 1,\u00a0 \u00a0 training_fraction_split: float = 0.8,\u00a0 \u00a0 validation_fraction_split: float = 0.1,\u00a0 \u00a0 test_fraction_split: float = 0.1,\u00a0 \u00a0 budget_milli_node_hours: int = 8000,\u00a0 \u00a0 timestamp_split_column_name: str = \"timestamp_split\",\u00a0 \u00a0 weight_column: str = \"weight\",\u00a0 \u00a0 time_series_attribute_columns: List[str] = [],\u00a0 \u00a0 context_window: int = 0,\u00a0 \u00a0 export_evaluated_data_items: bool = False,\u00a0 \u00a0 export_evaluated_data_items_bigquery_destination_uri: Optional[str] = None,\u00a0 \u00a0 export_evaluated_data_items_override_destination: bool = False,\u00a0 \u00a0 quantiles: Optional[List[float]] = None,\u00a0 \u00a0 enable_probabilistic_inference: bool = False,\u00a0 \u00a0 validation_options: Optional[str] = None,\u00a0 \u00a0 predefined_split_column_name: Optional[str] = None,\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 # Create training job\u00a0 \u00a0 forecasting_tide_job = aiplatform.TimeSeriesDenseEncoderForecastingTrainingJob(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name,\u00a0 \u00a0 \u00a0 \u00a0 optimization_objective=\"minimize-rmse\",\u00a0 \u00a0 )\u00a0 \u00a0 # Retrieve existing dataset\u00a0 \u00a0 dataset = aiplatform.TimeSeriesDataset(dataset_id)\u00a0 \u00a0 # Run training job\u00a0 \u00a0 model = forecasting_tide_job.run(\u00a0 \u00a0 \u00a0 \u00a0 dataset=dataset,\u00a0 \u00a0 \u00a0 \u00a0 target_column=target_column,\u00a0 \u00a0 \u00a0 \u00a0 time_column=time_column,\u00a0 \u00a0 \u00a0 \u00a0 time_series_identifier_column=time_series_identifier_column,\u00a0 \u00a0 \u00a0 \u00a0 unavailable_at_forecast_columns=unavailable_at_forecast_columns,\u00a0 \u00a0 \u00a0 \u00a0 available_at_forecast_columns=available_at_forecast_columns,\u00a0 \u00a0 \u00a0 \u00a0 forecast_horizon=forecast_horizon,\u00a0 \u00a0 \u00a0 \u00a0 data_granularity_unit=data_granularity_unit,\u00a0 \u00a0 \u00a0 \u00a0 data_granularity_count=data_granularity_count,\u00a0 \u00a0 \u00a0 \u00a0 training_fraction_split=training_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 validation_fraction_split=validation_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 test_fraction_split=test_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 predefined_split_column_name=predefined_split_column_name,\u00a0 \u00a0 \u00a0 \u00a0 timestamp_split_column_name=timestamp_split_column_name,\u00a0 \u00a0 \u00a0 \u00a0 weight_column=weight_column,\u00a0 \u00a0 \u00a0 \u00a0 time_series_attribute_columns=time_series_attribute_columns,\u00a0 \u00a0 \u00a0 \u00a0 context_window=context_window,\u00a0 \u00a0 \u00a0 \u00a0 export_evaluated_data_items=export_evaluated_data_items,\u00a0 \u00a0 \u00a0 \u00a0 export_evaluated_data_items_bigquery_destination_uri=export_evaluated_data_items_bigquery_destination_uri,\u00a0 \u00a0 \u00a0 \u00a0 export_evaluated_data_items_override_destination=export_evaluated_data_items_override_destination,\u00a0 \u00a0 \u00a0 \u00a0 quantiles=quantiles,\u00a0 \u00a0 \u00a0 \u00a0 enable_probabilistic_inference=enable_probabilistic_inference,\u00a0 \u00a0 \u00a0 \u00a0 validation_options=validation_options,\u00a0 \u00a0 \u00a0 \u00a0 budget_milli_node_hours=budget_milli_node_hours,\u00a0 \u00a0 \u00a0 \u00a0 model_display_name=model_display_name,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 print(model.uri)\u00a0 \u00a0 return model\n```\n` `` `### PythonTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_training_pipeline_forecasting_tft_sample.py) \n```\ndef create_training_pipeline_forecasting_temporal_fusion_transformer_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 dataset_id: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 model_display_name: str = \"my_model\",\u00a0 \u00a0 target_column: str = \"target_column\",\u00a0 \u00a0 time_column: str = \"date\",\u00a0 \u00a0 time_series_identifier_column: str = \"time_series_id\",\u00a0 \u00a0 unavailable_at_forecast_columns: List[str] = [],\u00a0 \u00a0 available_at_forecast_columns: List[str] = [],\u00a0 \u00a0 forecast_horizon: int = 1,\u00a0 \u00a0 data_granularity_unit: str = \"week\",\u00a0 \u00a0 data_granularity_count: int = 1,\u00a0 \u00a0 training_fraction_split: float = 0.8,\u00a0 \u00a0 validation_fraction_split: float = 0.1,\u00a0 \u00a0 test_fraction_split: float = 0.1,\u00a0 \u00a0 budget_milli_node_hours: int = 8000,\u00a0 \u00a0 timestamp_split_column_name: str = \"timestamp_split\",\u00a0 \u00a0 weight_column: str = \"weight\",\u00a0 \u00a0 time_series_attribute_columns: List[str] = [],\u00a0 \u00a0 context_window: int = 0,\u00a0 \u00a0 export_evaluated_data_items: bool = False,\u00a0 \u00a0 export_evaluated_data_items_bigquery_destination_uri: Optional[str] = None,\u00a0 \u00a0 export_evaluated_data_items_override_destination: bool = False,\u00a0 \u00a0 validation_options: Optional[str] = None,\u00a0 \u00a0 predefined_split_column_name: Optional[str] = None,\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 # Create training job\u00a0 \u00a0 forecasting_tft_job = aiplatform.TemporalFusionTransformerForecastingTrainingJob(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name,\u00a0 \u00a0 \u00a0 \u00a0 optimization_objective=\"minimize-rmse\",\u00a0 \u00a0 )\u00a0 \u00a0 # Retrieve existing dataset\u00a0 \u00a0 dataset = aiplatform.TimeSeriesDataset(dataset_id)\u00a0 \u00a0 # Run training job\u00a0 \u00a0 model = forecasting_tft_job.run(\u00a0 \u00a0 \u00a0 \u00a0 dataset=dataset,\u00a0 \u00a0 \u00a0 \u00a0 target_column=target_column,\u00a0 \u00a0 \u00a0 \u00a0 time_column=time_column,\u00a0 \u00a0 \u00a0 \u00a0 time_series_identifier_column=time_series_identifier_column,\u00a0 \u00a0 \u00a0 \u00a0 unavailable_at_forecast_columns=unavailable_at_forecast_columns,\u00a0 \u00a0 \u00a0 \u00a0 available_at_forecast_columns=available_at_forecast_columns,\u00a0 \u00a0 \u00a0 \u00a0 forecast_horizon=forecast_horizon,\u00a0 \u00a0 \u00a0 \u00a0 data_granularity_unit=data_granularity_unit,\u00a0 \u00a0 \u00a0 \u00a0 data_granularity_count=data_granularity_count,\u00a0 \u00a0 \u00a0 \u00a0 training_fraction_split=training_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 validation_fraction_split=validation_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 test_fraction_split=test_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 predefined_split_column_name=predefined_split_column_name,\u00a0 \u00a0 \u00a0 \u00a0 timestamp_split_column_name=timestamp_split_column_name,\u00a0 \u00a0 \u00a0 \u00a0 weight_column=weight_column,\u00a0 \u00a0 \u00a0 \u00a0 time_series_attribute_columns=time_series_attribute_columns,\u00a0 \u00a0 \u00a0 \u00a0 context_window=context_window,\u00a0 \u00a0 \u00a0 \u00a0 export_evaluated_data_items=export_evaluated_data_items,\u00a0 \u00a0 \u00a0 \u00a0 export_evaluated_data_items_bigquery_destination_uri=export_evaluated_data_items_bigquery_destination_uri,\u00a0 \u00a0 \u00a0 \u00a0 export_evaluated_data_items_override_destination=export_evaluated_data_items_override_destination,\u00a0 \u00a0 \u00a0 \u00a0 validation_options=validation_options,\u00a0 \u00a0 \u00a0 \u00a0 budget_milli_node_hours=budget_milli_node_hours,\u00a0 \u00a0 \u00a0 \u00a0 model_display_name=model_display_name,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 print(model.uri)\u00a0 \u00a0 return model\n```\n` `` `### PythonTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_training_pipeline_forecasting_sample.py) \n```\ndef create_training_pipeline_forecasting_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 dataset_id: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 model_display_name: str = \"my_model\",\u00a0 \u00a0 target_column: str = \"target_column\",\u00a0 \u00a0 time_column: str = \"date\",\u00a0 \u00a0 time_series_identifier_column: str = \"time_series_id\",\u00a0 \u00a0 unavailable_at_forecast_columns: List[str] = [],\u00a0 \u00a0 available_at_forecast_columns: List[str] = [],\u00a0 \u00a0 forecast_horizon: int = 1,\u00a0 \u00a0 data_granularity_unit: str = \"week\",\u00a0 \u00a0 data_granularity_count: int = 1,\u00a0 \u00a0 training_fraction_split: float = 0.8,\u00a0 \u00a0 validation_fraction_split: float = 0.1,\u00a0 \u00a0 test_fraction_split: float = 0.1,\u00a0 \u00a0 budget_milli_node_hours: int = 8000,\u00a0 \u00a0 timestamp_split_column_name: str = \"timestamp_split\",\u00a0 \u00a0 weight_column: str = \"weight\",\u00a0 \u00a0 time_series_attribute_columns: List[str] = [],\u00a0 \u00a0 context_window: int = 0,\u00a0 \u00a0 export_evaluated_data_items: bool = False,\u00a0 \u00a0 export_evaluated_data_items_bigquery_destination_uri: Optional[str] = None,\u00a0 \u00a0 export_evaluated_data_items_override_destination: bool = False,\u00a0 \u00a0 quantiles: Optional[List[float]] = None,\u00a0 \u00a0 enable_probabilistic_inference: bool = False,\u00a0 \u00a0 validation_options: Optional[str] = None,\u00a0 \u00a0 predefined_split_column_name: Optional[str] = None,\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 # Create training job\u00a0 \u00a0 forecasting_job = aiplatform.AutoMLForecastingTrainingJob(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name, optimization_objective=\"minimize-rmse\"\u00a0 \u00a0 )\u00a0 \u00a0 # Retrieve existing dataset\u00a0 \u00a0 dataset = aiplatform.TimeSeriesDataset(dataset_id)\u00a0 \u00a0 # Run training job\u00a0 \u00a0 model = forecasting_job.run(\u00a0 \u00a0 \u00a0 \u00a0 dataset=dataset,\u00a0 \u00a0 \u00a0 \u00a0 target_column=target_column,\u00a0 \u00a0 \u00a0 \u00a0 time_column=time_column,\u00a0 \u00a0 \u00a0 \u00a0 time_series_identifier_column=time_series_identifier_column,\u00a0 \u00a0 \u00a0 \u00a0 unavailable_at_forecast_columns=unavailable_at_forecast_columns,\u00a0 \u00a0 \u00a0 \u00a0 available_at_forecast_columns=available_at_forecast_columns,\u00a0 \u00a0 \u00a0 \u00a0 forecast_horizon=forecast_horizon,\u00a0 \u00a0 \u00a0 \u00a0 data_granularity_unit=data_granularity_unit,\u00a0 \u00a0 \u00a0 \u00a0 data_granularity_count=data_granularity_count,\u00a0 \u00a0 \u00a0 \u00a0 training_fraction_split=training_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 validation_fraction_split=validation_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 test_fraction_split=test_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 predefined_split_column_name=predefined_split_column_name,\u00a0 \u00a0 \u00a0 \u00a0 timestamp_split_column_name=timestamp_split_column_name,\u00a0 \u00a0 \u00a0 \u00a0 weight_column=weight_column,\u00a0 \u00a0 \u00a0 \u00a0 time_series_attribute_columns=time_series_attribute_columns,\u00a0 \u00a0 \u00a0 \u00a0 context_window=context_window,\u00a0 \u00a0 \u00a0 \u00a0 export_evaluated_data_items=export_evaluated_data_items,\u00a0 \u00a0 \u00a0 \u00a0 export_evaluated_data_items_bigquery_destination_uri=export_evaluated_data_items_bigquery_destination_uri,\u00a0 \u00a0 \u00a0 \u00a0 export_evaluated_data_items_override_destination=export_evaluated_data_items_override_destination,\u00a0 \u00a0 \u00a0 \u00a0 quantiles=quantiles,\u00a0 \u00a0 \u00a0 \u00a0 enable_probabilistic_inference=enable_probabilistic_inference,\u00a0 \u00a0 \u00a0 \u00a0 validation_options=validation_options,\u00a0 \u00a0 \u00a0 \u00a0 budget_milli_node_hours=budget_milli_node_hours,\u00a0 \u00a0 \u00a0 \u00a0 model_display_name=model_display_name,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 print(model.uri)\u00a0 \u00a0 return model\n```\n` `` `### PythonTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_training_pipeline_forecasting_seq2seq_sample.py) \n```\ndef create_training_pipeline_forecasting_seq2seq_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 dataset_id: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 model_display_name: str = \"my_model\",\u00a0 \u00a0 target_column: str = \"target_column\",\u00a0 \u00a0 time_column: str = \"date\",\u00a0 \u00a0 time_series_identifier_column: str = \"time_series_id\",\u00a0 \u00a0 unavailable_at_forecast_columns: List[str] = [],\u00a0 \u00a0 available_at_forecast_columns: List[str] = [],\u00a0 \u00a0 forecast_horizon: int = 1,\u00a0 \u00a0 data_granularity_unit: str = \"week\",\u00a0 \u00a0 data_granularity_count: int = 1,\u00a0 \u00a0 training_fraction_split: float = 0.8,\u00a0 \u00a0 validation_fraction_split: float = 0.1,\u00a0 \u00a0 test_fraction_split: float = 0.1,\u00a0 \u00a0 budget_milli_node_hours: int = 8000,\u00a0 \u00a0 timestamp_split_column_name: str = \"timestamp_split\",\u00a0 \u00a0 weight_column: str = \"weight\",\u00a0 \u00a0 time_series_attribute_columns: List[str] = [],\u00a0 \u00a0 context_window: int = 0,\u00a0 \u00a0 export_evaluated_data_items: bool = False,\u00a0 \u00a0 export_evaluated_data_items_bigquery_destination_uri: Optional[str] = None,\u00a0 \u00a0 export_evaluated_data_items_override_destination: bool = False,\u00a0 \u00a0 validation_options: Optional[str] = None,\u00a0 \u00a0 predefined_split_column_name: Optional[str] = None,\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 # Create training job\u00a0 \u00a0 forecasting_seq2seq_job = aiplatform.SequenceToSequencePlusForecastingTrainingJob(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name, optimization_objective=\"minimize-rmse\"\u00a0 \u00a0 )\u00a0 \u00a0 # Retrieve existing dataset\u00a0 \u00a0 dataset = aiplatform.TimeSeriesDataset(dataset_id)\u00a0 \u00a0 # Run training job\u00a0 \u00a0 model = forecasting_seq2seq_job.run(\u00a0 \u00a0 \u00a0 \u00a0 dataset=dataset,\u00a0 \u00a0 \u00a0 \u00a0 target_column=target_column,\u00a0 \u00a0 \u00a0 \u00a0 time_column=time_column,\u00a0 \u00a0 \u00a0 \u00a0 time_series_identifier_column=time_series_identifier_column,\u00a0 \u00a0 \u00a0 \u00a0 unavailable_at_forecast_columns=unavailable_at_forecast_columns,\u00a0 \u00a0 \u00a0 \u00a0 available_at_forecast_columns=available_at_forecast_columns,\u00a0 \u00a0 \u00a0 \u00a0 forecast_horizon=forecast_horizon,\u00a0 \u00a0 \u00a0 \u00a0 data_granularity_unit=data_granularity_unit,\u00a0 \u00a0 \u00a0 \u00a0 data_granularity_count=data_granularity_count,\u00a0 \u00a0 \u00a0 \u00a0 training_fraction_split=training_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 validation_fraction_split=validation_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 test_fraction_split=test_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 predefined_split_column_name=predefined_split_column_name,\u00a0 \u00a0 \u00a0 \u00a0 timestamp_split_column_name=timestamp_split_column_name,\u00a0 \u00a0 \u00a0 \u00a0 weight_column=weight_column,\u00a0 \u00a0 \u00a0 \u00a0 time_series_attribute_columns=time_series_attribute_columns,\u00a0 \u00a0 \u00a0 \u00a0 context_window=context_window,\u00a0 \u00a0 \u00a0 \u00a0 export_evaluated_data_items=export_evaluated_data_items,\u00a0 \u00a0 \u00a0 \u00a0 export_evaluated_data_items_bigquery_destination_uri=export_evaluated_data_items_bigquery_destination_uri,\u00a0 \u00a0 \u00a0 \u00a0 export_evaluated_data_items_override_destination=export_evaluated_data_items_override_destination,\u00a0 \u00a0 \u00a0 \u00a0 validation_options=validation_options,\u00a0 \u00a0 \u00a0 \u00a0 budget_milli_node_hours=budget_milli_node_hours,\u00a0 \u00a0 \u00a0 \u00a0 model_display_name=model_display_name,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 print(model.uri)\u00a0 \u00a0 return model\n```\n` ```\n``` `\n## Control the data split using REST\n` `` `You can control how your training data is split between the training, validation, and test sets. Use a split column to manually specify the data split for each row and provide it as part of a `PredefinedSplit` [Split object](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines#InputDataConfig) in the `inputDataConfig` of the JSON request.` `` ` is the column containing the data split values ( `TRAIN` , `VALIDATION` , `TEST` ).` `` ````\n\u00a0\"predefinedSplit\": {\u00a0 \u00a0\"key\": DATA_SPLIT_COLUMN\u00a0},\n```` `` ` [Learn more](/vertex-ai/docs/tabular-data/data-splits) about data splits.` `` `\n## Configure the rolling window strategy using REST\n` `` `You can provide a `windowConfig` object to configure a rolling window strategy for forecast window generation. The default strategy is `maxCount` .` `` `- To use the `maxCount` option, add the following to `trainingTaskInputs` of the JSON request. refers to the maximum number of windows.```\n\u00a0\"windowConfig\": {\u00a0 \u00a0\"maxCount\": MAX_COUNT_VALUE\u00a0},\u00a0```\n```\n- To use the `strideLength` option, add the following to `trainingTaskInputs` of the JSON request. refers to the value of the stride length.```\n\u00a0\"windowConfig\": {\u00a0 \u00a0\"strideLength\": STRIDE_LENGTH_VALUE\u00a0},\u00a0```\n```\n- To use the `column` option, add the following to `trainingTaskInputs` of the JSON request. refers to the name of the column with `True` or `False` values.```\n\u00a0\"windowConfig\": {\u00a0 \u00a0\"column\": \"COLUMN_NAME\"\u00a0},\u00a0```\n```\n` `` `To learn more, see [Rolling window strategies](/vertex-ai/docs/tabular-data/forecasting-parameters#rolling-window-strategies) .``\n````\n``` `\n## What's next\n` `` `- [Evaluate your model](/vertex-ai/docs/tabular-data/forecasting/evaluate-model) .\n` `` `` ```", "guide": "Vertex AI"}