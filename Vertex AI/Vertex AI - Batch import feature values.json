{"title": "Vertex AI - Batch import feature values", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Batch import feature values\nTo learn more,  run the \"Example Feature Store workflow with sample data\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/feature_store/sdk-feature-store.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Ffeature_store%2Fsdk-feature-store.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/feature_store/sdk-feature-store.ipynb)\nBatch import lets you import feature values in bulk from a valid data source. In a batch import request, you can import values for up to 100 features for an entity type. Note that you can have only one batch import job running per entity type to avoid collisions.\nIn a batch import request, specify the location of your source data and how it maps to features in your featurestore. Because each batch import request is for a single entity type, your source data must also be for a single entity type.\nAfter the import has successfully completed, feature values are available to subsequent read operations.\n- For information about source data requirements, see [Source datarequirements](/vertex-ai/docs/featurestore/source-data) .\n- For information about how long Vertex AI Feature Store (Legacy) retains your data in the offline store, see [Vertex AI Feature Store (Legacy)](/vertex-ai/docs/quotas#featurestore) in [Quotas and limits](/vertex-ai/docs/quotas) .\n- For information about the oldest feature value timestamp that you can import, see [Vertex AI Feature Store (Legacy)](/vertex-ai/docs/quotas#featurestore) in [Quotas and limits](/vertex-ai/docs/quotas) .\n- You can't import feature values for which the timestamps indicate future dates or times.", "content": "## Import job performance\nVertex AI Feature Store (Legacy) provides high throughput import, but the minimum latency can take a few minutes. Each request to Vertex AI Feature Store (Legacy) starts a job to complete the work. An import job takes a few minutes to complete even if you are importing a single record.\nIf you want to make adjustments to how a job performs, change the following two variables:\n- The number of featurestore online serving nodes.\n- The number of workers used for the import job. Workers process and write data into the featurestore.\nThe recommended number of workers is one worker for every 10 online serving nodes on the featurestore. You can go higher if the online serving load is low. You can specify a maximum of 100 workers. For more guidance, see [monitor and tune resources accordingly to optimize batchimport](/vertex-ai/docs/featurestore/best-practices) .\nIf the online serving cluster is under-provisioned, the import job might fail. In the event of a failure, retry the import request when the online serving load is low, or increase the node count of your featurestore and then retry the request.\nIf the featurestore doesn't have an online store (zero online serving nodes), the import job writes only to the offline store, and the performance of the job depends solely on the number of import workers.\n## Data consistency\nInconsistencies can be introduced if the source data is modified during import. Ensure that any source data modifications are complete before you start an import job. Also, duplicate feature values can result in different values being served between online and batch requests. Ensure that you have one feature value for each entity ID and timestamp pair.\nIf an import operation fails, the featurestore might only have partial data, which can lead to inconsistent values being returned between online and batch serving requests. To avoid this inconsistency, retry the same import request again and wait until the request successfully completes.\n## Null values and empty arrays\nDuring import, Vertex AI Feature Store (Legacy) considers null scalar values or empty arrays as empty values. These include empty values in a CSV column. Vertex AI Feature Store (Legacy) doesn't support non-scalar null values, such as a `null` value in an array.\nDuring online serving and batch serving, Vertex AI Feature Store (Legacy) returns the latest non-null or non-empty value of the feature. If a historical value of the feature isn't available, then Vertex AI Feature Store (Legacy) returns `null` .\n## NaN values\nVertex AI Feature Store (Legacy) supports NaN (Not a Number) values in `Double` and `DoubleArray` . During import, you can enter `NaN` in the serving input CSV file to represent a NaN value. During online serving and batch serving, Vertex AI Feature Store (Legacy) returns `NaN` for NaN values.\n**Note:** `NaN` is considered as valid data only if for the data type `Double` . For all other data types, `NaN` is considered as an invalid feature value. In such a scenario, Vertex AI Feature Store (Legacy) invalidates and ignores the entire row.\n## Batch import\nImport values in bulk into a featurestore for one or more features of a single entity type.\n- In the Vertex AI section of the Google Cloud console, go to  the **Features** page. [Go to the Features page](https://console.cloud.google.com/vertex-ai/features) \n- Select a region from the **Region** drop-down list.\n- In the features table, view the **Entity type** column and find the entity type that contains the features that you want to import values for.\n- Click the name of the entity type.\n- From the action bar, click **Ingest values** .\n- For **Data source** , select one of the following:- **Cloud Storage CSV file** : Select this option to import data from multiple CSV files from Cloud Storage. Specify the path and name of the CSV file. To specify additional files, click **Add another file** .\n- **Cloud Storage AVRO file** : Select this option to import data from an AVRO file from Cloud Storage. Specify the path and name of the AVRO file.\n- **BigQuery table** : Select this option to import data from a BigQuery table or BigQuery view. Browse and select a table or view to use, which is in the following format:`` `.` `` `.` ``\n- Click **Continue** .\n- For **Map column to features** , specify which columns in your source data map to entities and features in your featurestore.- Specify the column name in your source data that contains the entity  IDs.\n- For the timestamp, specify a timestamp column in your  source data or specify a single timestamp associated with all  feature values that you import.\n- In the list of features, enter the source data column name that  maps to each feature. By default, Vertex AI Feature Store (Legacy)  assumes that the feature name and column name match.\n- Click **Ingest** .\nTo import feature values for existing features, send a POST request by using the [featurestores.entityTypes.importFeatureValues](/vertex-ai/docs/reference/rest/v1/projects.locations.featurestores.entityTypes/importFeatureValues) method. Note that if the names of the source data columns and the destination feature IDs are different, include the `sourceField` parameter.\nBefore using any of the request data, make the following replacements:- : Region where the featurestore is created. For example,`us-central1`.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : ID of the featurestore.\n- : ID of the entity type.\n- : ID of source column that contains entity IDs.\n- : ID of source column that contains the feature timestamps for the feature values.\n- : ID of an existing feature in the featurestore to import values for.\n- : ID of source column that contain feature values for the entities.\n- : The source data location, which also indicates the format, such as`\"bigquerySource\": { \"inputUri\": \"bq://test.dataset.sourcetable\" }`for a BigQuery table or BigQuery view.\n- : The number of workers to use to write data to the featurestore.\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/featurestores/FEATURESTORE_ID/entityTypes/ENTITY_TYPE_ID:importFeatureValues\n```\nRequest JSON body:\n```\n{\n \"entityIdField\": \"ENTITY_SOURCE_COLUMN_ID\",\n \"featureTimeField\": \"FEATURE_TIME_ID\",\n SOURCE_DATA_DETAILS,\n \"featureSpecs\": [{\n \"id\": \"FEATURE_ID\",\n \"sourceField\": \"FEATURE_SOURCE_COLUMN_ID\"\n }],\n \"workerCount\": WORKER_COUNT\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/featurestores/FEATURESTORE_ID/entityTypes/ENTITY_TYPE_ID:importFeatureValues\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/featurestores/FEATURESTORE_ID/entityTypes/ENTITY_TYPE_ID:importFeatureValues\" | Select-Object -Expand Content\n```\nYou should see output similar to the following. You can use the in the response to [get the status](/vertex-ai/docs/general/long-running-operations) of the operation.\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/featurestores/FEATURESTORE_ID/entityTypes/ENTITY_TYPE_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.ImportFeatureValuesOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2021-03-02T00:04:13.039166Z\",\n  \"updateTime\": \"2021-03-02T00:04:13.039166Z\"\n }\n }\n}\n```\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/import_feature_values_sample.py) \n```\nimport datetimefrom typing import List, Unionfrom google.cloud import aiplatformdef import_feature_values_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 entity_type_id: str,\u00a0 \u00a0 featurestore_id: str,\u00a0 \u00a0 feature_ids: List[str],\u00a0 \u00a0 feature_time: Union[str, datetime.datetime],\u00a0 \u00a0 gcs_source_uris: Union[str, List[str]],\u00a0 \u00a0 gcs_source_type: str,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 my_entity_type = aiplatform.featurestore.EntityType(\u00a0 \u00a0 \u00a0 \u00a0 entity_type_name=entity_type_id, featurestore_id=featurestore_id\u00a0 \u00a0 )\u00a0 \u00a0 my_entity_type.ingest_from_gcs(\u00a0 \u00a0 \u00a0 \u00a0 feature_ids=feature_ids,\u00a0 \u00a0 \u00a0 \u00a0 feature_time=feature_time,\u00a0 \u00a0 \u00a0 \u00a0 gcs_source_uris=gcs_source_uris,\u00a0 \u00a0 \u00a0 \u00a0 gcs_source_type=gcs_source_type,\u00a0 \u00a0 )\n```The client library for Vertex AI is included when you install the Vertex AI SDK for Python. To learn how to install the Vertex AI SDK for Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/install-sdk) . For more information, see the [ Vertex AI SDK for Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/snippets/feature_store_service/import_feature_values_sample.py) \n```\nfrom google.cloud import aiplatformdef import_feature_values_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 featurestore_id: str,\u00a0 \u00a0 entity_type_id: str,\u00a0 \u00a0 avro_gcs_uri: str,\u00a0 \u00a0 entity_id_field: str,\u00a0 \u00a0 feature_time_field: str,\u00a0 \u00a0 worker_count: int = 2,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\u00a0 \u00a0 timeout: int = 300,):\u00a0 \u00a0 # The AI Platform services require regional API endpoints, which need to be\u00a0 \u00a0 # in the same region or multi-region overlap with the Feature Store location.\u00a0 \u00a0 client_options = {\"api_endpoint\": api_endpoint}\u00a0 \u00a0 # Initialize client that will be used to create and send requests.\u00a0 \u00a0 # This client only needs to be created once, and can be reused for multiple requests.\u00a0 \u00a0 client = aiplatform.gapic.FeaturestoreServiceClient(client_options=client_options)\u00a0 \u00a0 entity_type = f\"projects/{project}/locations/{location}/featurestores/{featurestore_id}/entityTypes/{entity_type_id}\"\u00a0 \u00a0 avro_source = aiplatform.gapic.AvroSource(\u00a0 \u00a0 \u00a0 \u00a0 gcs_source=aiplatform.gapic.GcsSource(uris=[avro_gcs_uri])\u00a0 \u00a0 )\u00a0 \u00a0 feature_specs = [\u00a0 \u00a0 \u00a0 \u00a0 aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=\"age\"),\u00a0 \u00a0 \u00a0 \u00a0 aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=\"gender\"),\u00a0 \u00a0 \u00a0 \u00a0 aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=\"liked_genres\"),\u00a0 \u00a0 ]\u00a0 \u00a0 import_feature_values_request = aiplatform.gapic.ImportFeatureValuesRequest(\u00a0 \u00a0 \u00a0 \u00a0 entity_type=entity_type,\u00a0 \u00a0 \u00a0 \u00a0 avro_source=avro_source,\u00a0 \u00a0 \u00a0 \u00a0 feature_specs=feature_specs,\u00a0 \u00a0 \u00a0 \u00a0 entity_id_field=entity_id_field,\u00a0 \u00a0 \u00a0 \u00a0 feature_time_field=feature_time_field,\u00a0 \u00a0 \u00a0 \u00a0 worker_count=worker_count,\u00a0 \u00a0 )\u00a0 \u00a0 lro_response = client.import_feature_values(request=import_feature_values_request)\u00a0 \u00a0 print(\"Long running operation:\", lro_response.operation.name)\u00a0 \u00a0 import_feature_values_response = lro_response.result(timeout=timeout)\u00a0 \u00a0 print(\"import_feature_values_response:\", import_feature_values_response)\n```Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/ImportFeatureValuesSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.AvroSource;import com.google.cloud.aiplatform.v1.EntityTypeName;import com.google.cloud.aiplatform.v1.FeaturestoreServiceClient;import com.google.cloud.aiplatform.v1.FeaturestoreServiceSettings;import com.google.cloud.aiplatform.v1.GcsSource;import com.google.cloud.aiplatform.v1.ImportFeatureValuesOperationMetadata;import com.google.cloud.aiplatform.v1.ImportFeatureValuesRequest;import com.google.cloud.aiplatform.v1.ImportFeatureValuesRequest.FeatureSpec;import com.google.cloud.aiplatform.v1.ImportFeatureValuesResponse;import java.io.IOException;import java.util.ArrayList;import java.util.List;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class ImportFeatureValuesSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String featurestoreId = \"YOUR_FEATURESTORE_ID\";\u00a0 \u00a0 String entityTypeId = \"YOUR_ENTITY_TYPE_ID\";\u00a0 \u00a0 String entityIdField = \"YOUR_ENTITY_FIELD_ID\";\u00a0 \u00a0 String featureTimeField = \"YOUR_FEATURE_TIME_FIELD\";\u00a0 \u00a0 String gcsSourceUri = \"YOUR_GCS_SOURCE_URI\";\u00a0 \u00a0 int workerCount = 2;\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 String endpoint = \"us-central1-aiplatform.googleapis.com:443\";\u00a0 \u00a0 int timeout = 300;\u00a0 \u00a0 importFeatureValuesSample(\u00a0 \u00a0 \u00a0 \u00a0 project,\u00a0 \u00a0 \u00a0 \u00a0 featurestoreId,\u00a0 \u00a0 \u00a0 \u00a0 entityTypeId,\u00a0 \u00a0 \u00a0 \u00a0 gcsSourceUri,\u00a0 \u00a0 \u00a0 \u00a0 entityIdField,\u00a0 \u00a0 \u00a0 \u00a0 featureTimeField,\u00a0 \u00a0 \u00a0 \u00a0 workerCount,\u00a0 \u00a0 \u00a0 \u00a0 location,\u00a0 \u00a0 \u00a0 \u00a0 endpoint,\u00a0 \u00a0 \u00a0 \u00a0 timeout);\u00a0 }\u00a0 static void importFeatureValuesSample(\u00a0 \u00a0 \u00a0 String project,\u00a0 \u00a0 \u00a0 String featurestoreId,\u00a0 \u00a0 \u00a0 String entityTypeId,\u00a0 \u00a0 \u00a0 String gcsSourceUri,\u00a0 \u00a0 \u00a0 String entityIdField,\u00a0 \u00a0 \u00a0 String featureTimeField,\u00a0 \u00a0 \u00a0 int workerCount,\u00a0 \u00a0 \u00a0 String location,\u00a0 \u00a0 \u00a0 String endpoint,\u00a0 \u00a0 \u00a0 int timeout)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 FeaturestoreServiceSettings featurestoreServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 FeaturestoreServiceSettings.newBuilder().setEndpoint(endpoint).build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (FeaturestoreServiceClient featurestoreServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 FeaturestoreServiceClient.create(featurestoreServiceSettings)) {\u00a0 \u00a0 \u00a0 List<FeatureSpec> featureSpecs = new ArrayList<>();\u00a0 \u00a0 \u00a0 featureSpecs.add(FeatureSpec.newBuilder().setId(\"title\").build());\u00a0 \u00a0 \u00a0 featureSpecs.add(FeatureSpec.newBuilder().setId(\"genres\").build());\u00a0 \u00a0 \u00a0 featureSpecs.add(FeatureSpec.newBuilder().setId(\"average_rating\").build());\u00a0 \u00a0 \u00a0 ImportFeatureValuesRequest importFeatureValuesRequest =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ImportFeatureValuesRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEntityType(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 EntityTypeName.of(project, location, featurestoreId, entityTypeId).toString())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEntityIdField(entityIdField)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setFeatureTimeField(featureTimeField)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addAllFeatureSpecs(featureSpecs)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setWorkerCount(workerCount)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAvroSource(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AvroSource.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setGcsSource(GcsSource.newBuilder().addUris(gcsSourceUri)))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 OperationFuture<ImportFeatureValuesResponse, ImportFeatureValuesOperationMetadata>\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 importFeatureValuesFuture =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 featurestoreServiceClient.importFeatureValuesAsync(importFeatureValuesRequest);\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Operation name: %s%n\", importFeatureValuesFuture.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to finish...\");\u00a0 \u00a0 \u00a0 ImportFeatureValuesResponse importFeatureValuesResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 importFeatureValuesFuture.get(timeout, TimeUnit.SECONDS);\u00a0 \u00a0 \u00a0 System.out.println(\"Import Feature Values Response\");\u00a0 \u00a0 \u00a0 System.out.println(importFeatureValuesResponse);\u00a0 \u00a0 \u00a0 featurestoreServiceClient.close();\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/import-feature-values-sample.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const project = 'YOUR_PROJECT_ID';// const featurestoreId = 'YOUR_FEATURESTORE_ID';// const entityTypeId = 'YOUR_ENTITY_TYPE_ID';// const avroGcsUri = 'AVRO_FILE_IN_THE_GCS_URI';// const entityIdField = 'ENTITY_ID_FIELD_IN_AVRO';// const featureTimeField = 'TIMESTAMP_FIELD_IN_AVRO';// const workerCount = <NO_OF_WORKERS_FOR_INGESTION_JOB>;// const location = 'YOUR_PROJECT_LOCATION';// const apiEndpoint = 'YOUR_API_ENDPOINT';// const timeout = <TIMEOUT_IN_MILLI_SECONDS>;// Imports the Google Cloud Featurestore Service Client libraryconst {FeaturestoreServiceClient} = require('@google-cloud/aiplatform').v1;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: apiEndpoint,};// Instantiates a clientconst featurestoreServiceClient = new FeaturestoreServiceClient(\u00a0 clientOptions);async function importFeatureValues() {\u00a0 // Configure the entityType resource\u00a0 const entityType = `projects/${project}/locations/${location}/featurestores/${featurestoreId}/entityTypes/${entityTypeId}`;\u00a0 const avroSource = {\u00a0 \u00a0 gcsSource: {\u00a0 \u00a0 \u00a0 uris: [avroGcsUri],\u00a0 \u00a0 },\u00a0 };\u00a0 const featureSpecs = [{id: 'age'}, {id: 'gender'}, {id: 'liked_genres'}];\u00a0 const request = {\u00a0 \u00a0 entityType: entityType,\u00a0 \u00a0 avroSource: avroSource,\u00a0 \u00a0 entityIdField: entityIdField,\u00a0 \u00a0 featureSpecs: featureSpecs,\u00a0 \u00a0 featureTimeField: featureTimeField,\u00a0 \u00a0 workerCount: Number(workerCount),\u00a0 };\u00a0 // Import Feature Values Request\u00a0 const [operation] = await featurestoreServiceClient.importFeatureValues(\u00a0 \u00a0 request,\u00a0 \u00a0 {timeout: Number(timeout)}\u00a0 );\u00a0 const [response] = await operation.promise();\u00a0 console.log('Import feature values response');\u00a0 console.log('Raw response:');\u00a0 console.log(JSON.stringify(response, null, 2));}importFeatureValues();\n```\n## View import jobs\nUse the Google Cloud console to view batch import jobs in a Google Cloud project.\n**Note:** For featurestores created during [Preview](https://cloud.google.com/products#product-launch-stages) , the Google Cloud console lists both batch import and batch serving jobs. Featurestores created on or after General Availability (October 5, 2021) show only batch import jobs. If you see both job types and just want to view batch import jobs, you must [create](/vertex-ai/docs/featurestore/managing-featurestores) and use a new featurestore.\n- In the Vertex AI section of the Google Cloud console, go to  the **Features** page. [Go to the Features page](https://console.cloud.google.com/vertex-ai/features) \n- Select a region from the **Region** drop-down list.\n- From the action bar, click **View ingestion jobs** to list import jobs for all featurestores.\n- Click the ID of an import job to view its details such as its data source, number of import entities, and number of feature values imported.\n## Overwrite existing data in a featurestore\nYou can re-import values to overwrite existing feature values if they both have the same timestamps. You don't need to delete existing feature values first. For example, you might rely on an underlying source data that was recently changed. To keep your featurestore consistent with that underlying data, import your feature values again. If you have mismatched timestamps, the imported values are considered unique and the old values continue to exist (they aren't overwritten).\nTo ensure consistency between online and batch serving requests, wait until the import job is complete before making any serving requests.\n## Backfill historical data\nIf you're backfilling data, where you're importing past feature values, disable online serving for your import job. Online serving is for serving the latest feature values only, which backfilling doesn't include. Disabling online serving is useful because you eliminate any load on your online serving nodes and increase throughput for your import job, which can decrease its completion time.\nYou can disable online serving for import jobs when you use the API or client libraries. For more information, see the `disableOnlineServing` field for the [importFeatureValuemethod](/vertex-ai/docs/reference/rest/v1/projects.locations.featurestores.entityTypes/importFeatureValues) .\n## What's next\n- Learn how to serve features through [onlineserving](/vertex-ai/docs/featurestore/serving-online) or [batchserving](/vertex-ai/docs/featurestore/serving-batch) .\n- Learn how to [monitor imported feature values overtime](/vertex-ai/docs/featurestore/monitoring#feature) .\n- View the Vertex AI Feature Store (Legacy) [concurrent batch jobquota](/vertex-ai/quotas#featurestore) .\n- [Troubleshoot](/vertex-ai/docs/general/troubleshooting#feature-store) common Vertex AI Feature Store (Legacy) issues.", "guide": "Vertex AI"}