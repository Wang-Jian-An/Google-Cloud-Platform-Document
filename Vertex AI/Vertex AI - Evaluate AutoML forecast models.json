{"title": "Vertex AI - Evaluate AutoML forecast models", "url": "https://cloud.google.com/vertex-ai/docs/tabular-data/forecasting/evaluate-model", "abstract": "# Vertex AI - Evaluate AutoML forecast models\nThis page shows you how to evaluate your AutoML forecast models using model evaluation metrics. These metrics provide quantitative measurements of how your model performed on the [test set](/vertex-ai/docs/tabular-data/data-splits) . How you interpret and use those metrics depends on your business need and the problem your model is trained to solve. For example, you might have a lower tolerance for false positives than for false negatives or the other way around. These kinds of questions affect which metrics you would focus on.\n", "content": "## Before you begin\nBefore you can evaluate a model, you must [train it](/vertex-ai/docs/tabular-data/forecasting/train-model) and wait for the training to complete.\nUse the console or the API to check the status of your training job.\n- In the Google Cloud console, in the Vertex AI section, go to the **Training** page. [Go to the Training page](https://console.cloud.google.com/vertex-ai/training) \n- If the status of your training job is \"Training\", you must continue to wait for the training job to finish. If the status of your training job is \"Finished\", you are ready to begin model evaluation.Select a tab that corresponds to your language or environment:Before using any of the request data, make the following replacements:- : Region where your model is stored.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : ID of the training pipeline.\nHTTP method and URL:\n```\nGET https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/trainingPipelines/TRAINING_PIPELINE_ID\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nExecute the following command:\n```\ncurl -X GET \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/trainingPipelines/TRAINING_PIPELINE_ID\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nExecute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method GET ` -Headers $headers ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/trainingPipelines/TRAINING_PIPELINE_ID\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following:\n## Getting evaluation metrics\nYou can get an aggregate set of [evaluation metrics](#metrics) for your model. The following content describes how to get these metrics using the Google Cloud console or API.\n- In the Google Cloud console, in the Vertex AI section, go to the **Models** page. [Go to the Models page](https://console.cloud.google.com/vertex-ai/models) \n- In the **Region** drop-down, select the region where your model is located.\n- From the list of models, select your model.\n- Select your model's version number.\n- In the **Evaluate** tab, you can view your model's aggregate evaluation metrics.\nTo view aggregate model evaluation metrics, use the [projects.locations.models.evaluations.get](/vertex-ai/docs/reference/rest/v1/projects.locations.models.evaluations) method.\nSelect a tab that corresponds to your language or environment:Before using any of the request data, make the following replacements:- : Region where your model is stored.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The ID of the. Theappears in the training pipeline after model training is successfully completed. Refer to the [Before you begin](/vertex-ai/docs/tabular-data/forecasting/evaluate-model#before-you-begin) section to get the.\nHTTP method and URL:\n```\nGET https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID/evaluations\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nExecute the following command:\n```\ncurl -X GET \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID/evaluations\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nExecute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method GET ` -Headers $headers ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models/MODEL_ID/evaluations\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following:\n## Model evaluation metrics\nA schema file determines which evaluation metrics Vertex AI provides for each objective.\nYou can view and download schema files from the following Cloud Storage location:  [gs://google-cloud-aiplatform/schema/modelevaluation/](https://console.cloud.google.com/storage/browser/google-cloud-aiplatform/schema/modelevaluation)\nThe evaluation metrics for forecasting models are:\n- **MAE** : The mean absolute error (MAE) is the average absolute difference between the target values and the predicted values. This metric ranges from zero to infinity; a lower value indicates a higher quality model.\n- **MAPE** : Mean absolute percentage error (MAPE) is the average absolute percentage difference between the labels and the predicted values. This metric ranges between zero and infinity; a lower value indicates a higher quality model.MAPE is not shown if the target column contains any 0 values. In this case, MAPE is undefined.\n- **RMSE** : The root-mean-squared error is the square root of the average squared difference between the target and predicted values. RMSE is more sensitive to outliers than MAE,so if you're concerned about large errors, then RMSE can be a more useful metric to evaluate. Similar to MAE, a smaller value indicates a higher quality model (0 represents a perfect predictor).\n- **RMSLE** : The root-mean-squared logarithmic error metric is similar to RMSE, except that it uses the natural logarithm of the predicted and actual values plus 1. RMSLE penalizes under-prediction more heavily than over-prediction. It can also be a good metric when you don't want to penalize differences for large prediction values more heavily than for small prediction values. This metric ranges from zero to infinity; a lower value indicates a higher quality model. The RMSLE evaluation metric is returned only if all label and predicted values are non-negative.\n- **r^2** : r squared (r^2) is the square of the Pearson correlation coefficient between the labels and predicted values. This metric ranges between zero and one. A higher value indicates a closer fit to the regression line.\n- **Quantile** : The percent quantile, which indicates the probability that an observed value will be below the predicted value. For example, at the 0.2 quantile, the observed values are expected to be lower than the predicted values 20% of the time. Vertex AI provides this metric if you specify`minimize-quantile-loss`for the optimization objective.\n- **Observed quantile** : Shows the percentage of true values that were less than the predicted value for a given quantile. Vertex AI provides this metric if you specify`minimize-quantile-loss`for the optimization objective.\n- **Scaled pinball loss** : The scaled pinball loss at a particular quantile. A lower value indicates a higher quality model at the given quantile. Vertex AI provides this metric if you specify`minimize-quantile-loss`for the optimization objective.\n- **Model feature attributions** : Vertex AI shows you how much each feature impacts a model. The values are provided as a percentage for each feature: the higher the percentage, the more impact the feature had on model training. Review this information to ensure that all of the most important features make sense for your data and business problem. To learn  more, see [Feature attributions for forecasting](/vertex-ai/docs/tabular-data/forecasting-explanations) .## What's next\n- [Get predictions from your forecast model](/vertex-ai/docs/tabular-data/forecasting/get-predictions) .", "guide": "Vertex AI"}