{"title": "Vertex AI - Train a video object tracking model", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Train a video object tracking model\nThis page shows you how to train an AutoML object tracking model from a video dataset using either the Google Cloud console or the Vertex AI API.\n", "content": "## Train an AutoML model\n- In the Google Cloud console, in the Vertex AI section, go to the **Datasets** page. [Go to the Datasets page](https://console.cloud.google.com/vertex-ai/datasets) \n- Click the name of the dataset you want to use to train your model to open its details page.\n- Click **Train new model** . **Note:** You can type [model.new](https://model.new) into a browser to go directly to the model creation page.\n- Enter the display name for your new model.\n- If you want manually set how your training data is split, expand **Advanced options** and select a data split option. [Learn more](/vertex-ai/docs/general/ml-use) .\n- Click **Continue** .\n- Select the model training method.- `AutoML`is a good choice for a wide range of use cases.\n- `Seq2seq+`is a good choice for experimentation. The algorithm is likely to converge faster than`AutoML`because its architecture is simpler and it uses a smaller  search space. Our experiments find that Seq2Seq+ performs well with a small time budget and  on datasets smaller than 1\u00a0GB in size.\nClick **Continue** .\n- Click **Start Training** .Model training can take many hours, depending on the size and complexity of your data and your training budget, if you specified one. You can close this tab and return to it later. You will receive an email when your model has completed training.Several minutes after training starts, you can check the training node hour estimation from the model's properties information. If you cancel the training, there is no charge on the current product.\nSelect the tab below for your language or environment:Before using any of the request data, make the following replacements:- : Region where Dataset is located and Model will be stored. For example,`us-central1`.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : Display name for the newly trained model.\n- : ID for the training Dataset.\n- The`filterSplit`object is optional; you use it to control your data split. For more  information about controlling data split, see [Controlling the data split using REST](#data-split) .\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) \nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/trainingPipelines\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"MODE_DISPLAY_NAME\",\n \"trainingTaskDefinition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_video_object_tracking_1.0.0.yaml\",\n \"trainingTaskInputs\": {},\n \"modelToUpload\": {\"displayName\": \"MODE_DISPLAY_NAME\"},\n \"inputDataConfig\": {\n  \"datasetId\": \"DATASET_ID\",\n  \"filterSplit\": {\n  \"trainingFilter\": \"labels.ml_use = training\",\n  \"validationFilter\": \"labels.ml_use = -\",\n  \"testFilter\": \"labels.ml_use = test\"\n  }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/us-central1/trainingPipelines/2307109646608891904\",\n \"displayName\": \"myModelName\",\n \"trainingTaskDefinition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_video_object_tracking_1.0.0.yaml\",\n \"modelToUpload\": {\n \"displayName\": \"myModelName\"\n },\n \"state\": \"PIPELINE_STATE_PENDING\",\n \"createTime\": \"2020-04-18T01:22:57.479336Z\",\n \"updateTime\": \"2020-04-18T01:22:57.479336Z\"\n}\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateTrainingPipelineVideoObjectTrackingSample.java) \n```\nimport com.google.cloud.aiplatform.util.ValueConverter;import com.google.cloud.aiplatform.v1.FilterSplit;import com.google.cloud.aiplatform.v1.FractionSplit;import com.google.cloud.aiplatform.v1.InputDataConfig;import com.google.cloud.aiplatform.v1.LocationName;import com.google.cloud.aiplatform.v1.Model;import com.google.cloud.aiplatform.v1.PipelineServiceClient;import com.google.cloud.aiplatform.v1.PipelineServiceSettings;import com.google.cloud.aiplatform.v1.PredefinedSplit;import com.google.cloud.aiplatform.v1.TimestampSplit;import com.google.cloud.aiplatform.v1.TrainingPipeline;import com.google.cloud.aiplatform.v1.schema.trainingjob.definition.AutoMlVideoObjectTrackingInputs;import com.google.cloud.aiplatform.v1.schema.trainingjob.definition.AutoMlVideoObjectTrackingInputs.ModelType;import com.google.rpc.Status;import java.io.IOException;public class CreateTrainingPipelineVideoObjectTrackingSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 String trainingPipelineVideoObjectTracking =\u00a0 \u00a0 \u00a0 \u00a0 \"YOUR_TRAINING_PIPELINE_VIDEO_OBJECT_TRACKING_DISPLAY_NAME\";\u00a0 \u00a0 String datasetId = \"YOUR_DATASET_ID\";\u00a0 \u00a0 String modelDisplayName = \"YOUR_MODEL_DISPLAY_NAME\";\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 createTrainingPipelineVideoObjectTracking(\u00a0 \u00a0 \u00a0 \u00a0 trainingPipelineVideoObjectTracking, datasetId, modelDisplayName, project);\u00a0 }\u00a0 static void createTrainingPipelineVideoObjectTracking(\u00a0 \u00a0 \u00a0 String trainingPipelineVideoObjectTracking,\u00a0 \u00a0 \u00a0 String datasetId,\u00a0 \u00a0 \u00a0 String modelDisplayName,\u00a0 \u00a0 \u00a0 String project)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 PipelineServiceSettings pipelineServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PipelineServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (PipelineServiceClient pipelineServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 PipelineServiceClient.create(pipelineServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 String trainingTaskDefinition =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gs://google-cloud-aiplatform/schema/trainingjob/definition/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"automl_video_object_tracking_1.0.0.yaml\";\u00a0 \u00a0 \u00a0 LocationName locationName = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 AutoMlVideoObjectTrackingInputs trainingTaskInputs =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutoMlVideoObjectTrackingInputs.newBuilder().setModelType(ModelType.CLOUD).build();\u00a0 \u00a0 \u00a0 InputDataConfig inputDataConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputDataConfig.newBuilder().setDatasetId(datasetId).build();\u00a0 \u00a0 \u00a0 Model modelToUpload = Model.newBuilder().setDisplayName(modelDisplayName).build();\u00a0 \u00a0 \u00a0 TrainingPipeline trainingPipeline =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TrainingPipeline.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(trainingPipelineVideoObjectTracking)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingTaskDefinition(trainingTaskDefinition)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingTaskInputs(ValueConverter.toValue(trainingTaskInputs))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputDataConfig(inputDataConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModelToUpload(modelToUpload)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 TrainingPipeline createTrainingPipelineResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pipelineServiceClient.createTrainingPipeline(locationName, trainingPipeline);\u00a0 \u00a0 \u00a0 System.out.println(\"Create Training Pipeline Video Object Tracking Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"Name: %s\\n\", createTrainingPipelineResponse.getName());\u00a0 \u00a0 \u00a0 System.out.format(\"Display Name: %s\\n\", createTrainingPipelineResponse.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Training Task Definition %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 createTrainingPipelineResponse.getTrainingTaskDefinition());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Training Task Inputs: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 createTrainingPipelineResponse.getTrainingTaskInputs().toString());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Training Task Metadata: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 createTrainingPipelineResponse.getTrainingTaskMetadata().toString());\u00a0 \u00a0 \u00a0 System.out.format(\"State: %s\\n\", createTrainingPipelineResponse.getState().toString());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Create Time: %s\\n\", createTrainingPipelineResponse.getCreateTime().toString());\u00a0 \u00a0 \u00a0 System.out.format(\"StartTime %s\\n\", createTrainingPipelineResponse.getStartTime().toString());\u00a0 \u00a0 \u00a0 System.out.format(\"End Time: %s\\n\", createTrainingPipelineResponse.getEndTime().toString());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Update Time: %s\\n\", createTrainingPipelineResponse.getUpdateTime().toString());\u00a0 \u00a0 \u00a0 System.out.format(\"Labels: %s\\n\", createTrainingPipelineResponse.getLabelsMap().toString());\u00a0 \u00a0 \u00a0 InputDataConfig inputDataConfigResponse = createTrainingPipelineResponse.getInputDataConfig();\u00a0 \u00a0 \u00a0 System.out.println(\"Input Data config\");\u00a0 \u00a0 \u00a0 System.out.format(\"Dataset Id: %s\\n\", inputDataConfigResponse.getDatasetId());\u00a0 \u00a0 \u00a0 System.out.format(\"Annotations Filter: %s\\n\", inputDataConfigResponse.getAnnotationsFilter());\u00a0 \u00a0 \u00a0 FractionSplit fractionSplit = inputDataConfigResponse.getFractionSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"Fraction split\");\u00a0 \u00a0 \u00a0 System.out.format(\"Training Fraction: %s\\n\", fractionSplit.getTrainingFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"Validation Fraction: %s\\n\", fractionSplit.getValidationFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"Test Fraction: %s\\n\", fractionSplit.getTestFraction());\u00a0 \u00a0 \u00a0 FilterSplit filterSplit = inputDataConfigResponse.getFilterSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"Filter Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"Training Filter: %s\\n\", filterSplit.getTrainingFilter());\u00a0 \u00a0 \u00a0 System.out.format(\"Validation Filter: %s\\n\", filterSplit.getValidationFilter());\u00a0 \u00a0 \u00a0 System.out.format(\"Test Filter: %s\\n\", filterSplit.getTestFilter());\u00a0 \u00a0 \u00a0 PredefinedSplit predefinedSplit = inputDataConfigResponse.getPredefinedSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"Predefined Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"Key: %s\\n\", predefinedSplit.getKey());\u00a0 \u00a0 \u00a0 TimestampSplit timestampSplit = inputDataConfigResponse.getTimestampSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"Timestamp Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"Training Fraction: %s\\n\", timestampSplit.getTrainingFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"Validation Fraction: %s\\n\", timestampSplit.getValidationFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"Test Fraction: %s\\n\", timestampSplit.getTestFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"Key: %s\\n\", timestampSplit.getKey());\u00a0 \u00a0 \u00a0 Model modelResponse = createTrainingPipelineResponse.getModelToUpload();\u00a0 \u00a0 \u00a0 System.out.println(\"Model To Upload\");\u00a0 \u00a0 \u00a0 System.out.format(\"Name: %s\\n\", modelResponse.getName());\u00a0 \u00a0 \u00a0 System.out.format(\"Display Name: %s\\n\", modelResponse.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\"Description: %s\\n\", modelResponse.getDescription());\u00a0 \u00a0 \u00a0 System.out.format(\"Metadata Schema Uri: %s\\n\", modelResponse.getMetadataSchemaUri());\u00a0 \u00a0 \u00a0 System.out.format(\"Metadata: %s\\n\", modelResponse.getMetadata());\u00a0 \u00a0 \u00a0 System.out.format(\"Training Pipeline: %s\\n\", modelResponse.getTrainingPipeline());\u00a0 \u00a0 \u00a0 System.out.format(\"Artifact Uri: %s\\n\", modelResponse.getArtifactUri());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Supported Deployment Resources Types: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedDeploymentResourcesTypesList().toString());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Supported Input Storage Formats: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedInputStorageFormatsList().toString());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Supported Output Storage Formats: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedOutputStorageFormatsList().toString());\u00a0 \u00a0 \u00a0 System.out.format(\"Create Time: %s\\n\", modelResponse.getCreateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"Update Time: %s\\n\", modelResponse.getUpdateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"Labels: %s\\n\", modelResponse.getLabelsMap());\u00a0 \u00a0 \u00a0 Status status = createTrainingPipelineResponse.getError();\u00a0 \u00a0 \u00a0 System.out.println(\"Error\");\u00a0 \u00a0 \u00a0 System.out.format(\"Code: %s\\n\", status.getCode());\u00a0 \u00a0 \u00a0 System.out.format(\"Message: %s\\n\", status.getMessage());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/create-training-pipeline-video-object-tracking.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const datasetId = 'YOUR_DATASET_ID';// const modelDisplayName = 'YOUR_MODEL_DISPLAY_NAME';// const trainingPipelineDisplayName = 'YOUR_TRAINING_PIPELINE_DISPLAY_NAME';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';const aiplatform = require('@google-cloud/aiplatform');const {definition} =\u00a0 aiplatform.protos.google.cloud.aiplatform.v1.schema.trainingjob;const ModelType = definition.AutoMlVideoObjectTrackingInputs.ModelType;// Imports the Google Cloud Pipeline Service Client libraryconst {PipelineServiceClient} = aiplatform.v1;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst pipelineServiceClient = new PipelineServiceClient(clientOptions);async function createTrainingPipelineVideoObjectTracking() {\u00a0 // Configure the parent resource\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 const trainingTaskInputsObj =\u00a0 \u00a0 new definition.AutoMlVideoObjectTrackingInputs({\u00a0 \u00a0 \u00a0 modelType: ModelType.CLOUD,\u00a0 \u00a0 });\u00a0 const trainingTaskInputs = trainingTaskInputsObj.toValue();\u00a0 const modelToUpload = {displayName: modelDisplayName};\u00a0 const inputDataConfig = {datasetId: datasetId};\u00a0 const trainingPipeline = {\u00a0 \u00a0 displayName: trainingPipelineDisplayName,\u00a0 \u00a0 trainingTaskDefinition:\u00a0 \u00a0 \u00a0 'gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_video_object_tracking_1.0.0.yaml',\u00a0 \u00a0 trainingTaskInputs,\u00a0 \u00a0 inputDataConfig,\u00a0 \u00a0 modelToUpload,\u00a0 };\u00a0 const request = {\u00a0 \u00a0 parent,\u00a0 \u00a0 trainingPipeline,\u00a0 };\u00a0 // Create training pipeline request\u00a0 const [response] =\u00a0 \u00a0 await pipelineServiceClient.createTrainingPipeline(request);\u00a0 console.log('Create training pipeline video object tracking response');\u00a0 console.log(`Name : ${response.name}`);\u00a0 console.log('Raw response:');\u00a0 console.log(JSON.stringify(response, null, 2));}createTrainingPipelineVideoObjectTracking();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/snippets/pipeline_service/create_training_pipeline_video_object_tracking_sample.py) \n```\nfrom google.cloud import aiplatformfrom google.cloud.aiplatform.gapic.schema import trainingjobdef create_training_pipeline_video_object_tracking_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 dataset_id: str,\u00a0 \u00a0 model_display_name: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",):\u00a0 \u00a0 # The AI Platform services require regional API endpoints.\u00a0 \u00a0 client_options = {\"api_endpoint\": api_endpoint}\u00a0 \u00a0 # Initialize client that will be used to create and send requests.\u00a0 \u00a0 # This client only needs to be created once, and can be reused for multiple requests.\u00a0 \u00a0 client = aiplatform.gapic.PipelineServiceClient(client_options=client_options)\u00a0 \u00a0 training_task_inputs = trainingjob.definition.AutoMlVideoObjectTrackingInputs(\u00a0 \u00a0 \u00a0 \u00a0 model_type=\"CLOUD\",\u00a0 \u00a0 ).to_value()\u00a0 \u00a0 training_pipeline = {\u00a0 \u00a0 \u00a0 \u00a0 \"display_name\": display_name,\u00a0 \u00a0 \u00a0 \u00a0 \"training_task_definition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_video_object_tracking_1.0.0.yaml\",\u00a0 \u00a0 \u00a0 \u00a0 \"training_task_inputs\": training_task_inputs,\u00a0 \u00a0 \u00a0 \u00a0 \"input_data_config\": {\"dataset_id\": dataset_id},\u00a0 \u00a0 \u00a0 \u00a0 \"model_to_upload\": {\"display_name\": model_display_name},\u00a0 \u00a0 }\u00a0 \u00a0 parent = f\"projects/{project}/locations/{location}\"\u00a0 \u00a0 response = client.create_training_pipeline(\u00a0 \u00a0 \u00a0 \u00a0 parent=parent, training_pipeline=training_pipeline\u00a0 \u00a0 )\u00a0 \u00a0 print(\"response:\", response)\n```\n## Control the data split using RESTYou can control how your training data is split between the training, validation, and test sets. When using the Vertex AI API, use the [Split object](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines#InputDataConfig) to determine your data split. The `Split` object can be included in the `InputConfig` object as one of several object types, each of which provides a different way to split the training data. You can select one method only.- `FractionSplit`:- : The fraction of the training data to   be used for the training set.\n- : The fraction of the training data   to be used for the validation set. Not used for video data.\n- : The fraction of the training data to be   used for the test set.\nIf any of the fractions are specified, all must be specified. The  fractions must add up to 1.0. The [default values for the fractions](/vertex-ai/docs/general/ml-use#default) differ depending on your data type. [Learn more](/vertex-ai/docs/general/ml-use#percentages) .```\n\"fractionSplit\": {\n \"trainingFraction\": TRAINING_FRACTION,\n \"validationFraction\": VALIDATION_FRACTION,\n \"testFraction\": TEST_FRACTION\n},\n```\n- `FilterSplit`:\n- : Data items that match this filter are used for the training set.\n- : Data items that match this filter are used for the validation set. Must be \"-\" for video data.\n- : Data items that match this filter are used for the test set.\n- These filters can be used with the `ml_use` label, or with any labels you apply to your data. Learn more about using [the ml-use label](/vertex-ai/docs/general/ml-use#ml-use) and [other labels](/vertex-ai/docs/general/ml-use#filter) to filter your data.\n- The following example shows how to use the `filterSplit` object with the `ml_use` label, with the validation set included:\n- ```\n\"filterSplit\": {\n\"trainingFilter\": \"labels.aiplatform.googleapis.com/ml_use=training\",\n\"validationFilter\": \"labels.aiplatform.googleapis.com/ml_use=validation\",\n\"testFilter\": \"labels.aiplatform.googleapis.com/ml_use=test\"\n}\n```\n[Previous  arrow_back    Create dataset  ](/vertex-ai/docs/video-data/object-tracking/create-dataset) [Next  Evaluate model    arrow_forward  ](/vertex-ai/docs/video-data/object-tracking/evaluate-model)", "guide": "Vertex AI"}