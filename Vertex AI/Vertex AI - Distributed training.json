{"title": "Vertex AI - Distributed training", "url": "https://cloud.google.com/vertex-ai/docs/training/distributed-training", "abstract": "# Vertex AI - Distributed training\nThis page describes how to run distributed training jobs on Vertex AI.\n", "content": "## Code requirements\nUse an ML framework that supports distributed training. In your training code, you can use the [CLUSTER_SPEC or TF_CONFIG environment variables](#cluster-variables) to reference specific parts of your training cluster.\n## Structure of the training cluster\nIf you run a distributed training job with Vertex AI, you specify multiple machines (nodes) in a . The training service allocates the resources for the machine types you specify. Your running job on a given node is called a . A group of replicas with the same configuration is called a .\nEach replica in the training cluster is given a single role or task in distributed training. For example:\n- **Primary replica** : Exactly one replica is designated the . This task manages the others and reports status for the job as a whole.\n- **Worker(s)** : One or more replicas may be designated as . These replicas do their portion of the work as you designate in your job configuration.\n- **Parameter server(s)** : If supported by your ML framework, one or more replicas may be designated as . These replicas store model parameters and coordinate shared model state between the workers.\n- **Evaluator(s)** : If supported by your ML framework, one or more replicas may be designated as . These replicas can be used to evaluate your model. If you are using TensorFlow, note that TensorFlow generally expects that you use no more than one evaluator.## Configure a distributed training job\nYou can configure any custom training job as a distributed training job by defining multiple worker pools. You can also run distributed training within a training pipeline or a hyperparameter tuning job.\n[](None)\nTo configure a distributed training job, define your list of worker pools ( [workerPoolSpecs[]](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#FIELDS.worker_pool_specs) ), designating one `WorkerPoolSpec` for each type of task:\n| Position in workerPoolSpecs[] | Task performed in cluster    |\n|:--------------------------------|:---------------------------------------|\n| First (workerPoolSpecs[0])  | Primary, chief, scheduler, or \"master\" |\n| Second (workerPoolSpecs[1])  | Secondary, replicas, workers   |\n| Third (workerPoolSpecs[2])  | Parameter servers, Reduction Server |\n| Fourth (workerPoolSpecs[3])  | Evaluators        |\nYou must specify a primary replica, which coordinates the work done by all the other replicas. Use the first worker pool specification only for your primary replica, and set its [replicaCount](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#WorkerPoolSpec.FIELDS.replica_count) to `1` :\n```\n{\n \"workerPoolSpecs\": [  // `WorkerPoolSpec` for worker pool 0, primary replica, required\n  {\n  \"machineSpec\": {...},\n  \"replicaCount\": 1,\n  \"diskSpec\": {...},\n  ...\n  },\n  // `WorkerPoolSpec` for worker pool 1, optional\n  {},\n  // `WorkerPoolSpec` for worker pool 2, optional\n  {},\n  // `WorkerPoolSpec` for worker pool 3, optional\n  {}\n ]\n ...\n}\n```\n### Specify additional worker pools\nDepending on your ML framework, you may specify additional worker pools for other purposes. For example, if you are using TensorFlow, you could specify worker pools to configure worker replicas, parameter server replicas, and evaluator replicas.\nThe order of the worker pools you specify in the `workerPoolSpecs[]` list determines the [type of worker pool](#worker-pool-types) . Set empty values for worker pools that you don't want to use, so that you can skip them in the `workerPoolSpecs[]` list in order to specify worker pools that you do want to use. For example:\nIf you want to specify a job that has only a primary replica and a parameter server worker pool, you must set an empty value for worker pool 1:\n```\n{\n \"workerPoolSpecs\": [  // `WorkerPoolSpec` for worker pool 0, required\n  {\n  \"machineSpec\": {...},\n  \"replicaCount\": 1,\n  \"diskSpec\": {...},\n  ...\n  },\n  // `WorkerPoolSpec` for worker pool 1, optional\n  {},\n  // `WorkerPoolSpec` for worker pool 2, optional\n  {\n  \"machineSpec\": {...},\n  \"replicaCount\": 1,\n  \"diskSpec\": {...},\n  ...\n  },\n  // `WorkerPoolSpec` for worker pool 3, optional\n  {}\n ]\n ...\n}\n```\n### Reduce training time with Reduction Server\nWhen you train a large ML model using multiple nodes, communicating gradients between nodes can contribute significant latency. is an all-reduce algorithm that can increase throughput and reduce latency for distributed training. Vertex AI makes Reduction Server available in a Docker container image that you can use for one of your worker pools during distributed training.\nTo learn about how Reduction Server works, see [Faster distributed GPU training with Reduction Server on Vertex AI](/blog/products/ai-machine-learning/faster-distributed-training-with-google-clouds-reduction-server) .\nYou can use Reduction Server if you meet the following requirements:\n- You are performing distributed training with GPU workers.\n- Your training code uses TensorFlow or PyTorch and is configured for multi-host data-parallel training with GPUs using [NCCL](https://developer.nvidia.com/nccl) all-reduce. (You might also be able to use other ML frameworks that use NCCL.)\n- The containers running on your primary node ( `workerPoolSpecs[0]` ) and workers ( `workerPoolSpecs[1]` ) support Reduction Server. Specifically, each container is one of the following:- A [prebuilt TensorFlow trainingcontainer](/vertex-ai/docs/training/pre-built-containers#tensorflow) , version 2.3 or later.\n- A [prebuilt Pytorch trainingcontainer](/vertex-ai/docs/training/pre-built-containers#pytorch) , version 1.4 or later.\n- A [custom container](/vertex-ai/docs/training/containers-overview) with NCCL 2.7 or later and the `google-reduction-server` package installed. You can install this package on a custom container image by adding the following line to your Dockerfile:```\nRUN echo \"deb https://packages.cloud.google.com/apt google-fast-socket main\" | tee /etc/apt/sources.list.d/google-fast-socket.list && \\\n curl -s -L https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - && \\\n apt update && apt install -y google-reduction-server\n```\nTo use Reduction Server, do the following when you create a custom training resource:\n- Specify one of the following URIs in the [containerSpec.imageUrifield](/vertex-ai/docs/reference/rest/v1/CustomJobSpec#ContainerSpec) of the third worker pool ( `workerPoolSpecs[2]` ):- `us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest`\n- `europe-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest`\n- `asia-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest`\nChoosing the multi-region closest to where you are performing custom training might reduce latency.\n- When selecting the [machine type and number of nodes](/vertex-ai/docs/training/configure-compute) for the third worker pool, make sure that the total network bandwidth of the third worker pool matches or exceeds the total network bandwidth of the first and second worker pools.To learn about the maximum available bandwidth of each node in the second worker pool, see [Network bandwidth andGPUs](/compute/docs/gpus/gpu-network-bandwidth) .You do not use GPUs for the Reduction Server nodes. To learn about the maximum available bandwidth of each node in the third worker pool, see the \"Maximum egress bandwidth (Gbps)\" columns in [General-purpose machinefamily](/compute/docs/general-purpose-machines) .For example, if you configure the first and second worker pools to use 5 `n1-highmem-96` nodes, each with 8 `NVIDIA_TESLA_V100` GPUs, then each node has a maximum available bandwidth of 100\u00a0Gbps, for a total bandwidth of 500\u00a0Gbps. In order to match this bandwidth in the third worker pool, you might use 16 `n1-highcpu-16` nodes, each with a maximum bandwidth of 32\u00a0Gbps, for a total bandwidth of 512\u00a0Gbps.We recommend that you use the `n1-highcpu-16` machine type for Reduction Server nodes, because this machine type offers relatively high bandwidth for its resources.\nThe following command provides an example of how to create a `CustomJob` resource that uses Reduction Server:\n```\ngcloud ai custom-jobs create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=JOB_NAME \\\u00a0 --worker-pool-spec=machine-type=n1-highmem-96,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,accelerator-count=8,container-image-uri=CUSTOM_CONTAINER_IMAGE_URI \\\u00a0 --worker-pool-spec=machine-type=n1-highmem-96,replica-count=4,accelerator-type=NVIDIA_TESLA_V100,accelerator-count=8,container-image-uri=CUSTOM_CONTAINER_IMAGE_URI \\\u00a0 --worker-pool-spec=machine-type=n1-highcpu-16,replica-count=16,container-image-uri=us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\n```\nFor more context, see the [guide to creating a CustomJob](/vertex-ai/docs/training/create-custom-job) .\nTo see an example of how to perform distributed training in PyTorch by using Reduction Server,  run the \"PyTorch distributed training with Vertex AI Reduction Server\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/reduction_server/pytorch_distributed_training_reduction_server.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Freduction_server%2Fpytorch_distributed_training_reduction_server.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/reduction_server/pytorch_distributed_training_reduction_server.ipynb)\nTo see an example of how to perform parallel training in XGBoost by using Dask,  run the \"Distributed XGBoost training with Dask\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/xgboost_data_parallel_training_on_cpu_using_dask.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Ftraining%2Fxgboost_data_parallel_training_on_cpu_using_dask.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/training/xgboost_data_parallel_training_on_cpu_using_dask.ipynb)\n### Best practices for training using Reduction Server\nIn Reduction Server training, each worker needs to connect to all of the reducer hosts. To minimize the number of connections on the worker host, use a machine type with the highest network bandwidth for your reducer host.\nA good choice for reducer hosts is a general purpose N1/N2 VM with at least 16 vCPU that provides [32\u00a0Gbps egress bandwidth](/compute/docs/networking/configure-vm-with-high-bandwidth-configuration#bandwidth-tiers) , such as `n1-highcpu-16` and `n2-highcpu-16` . Tier 1 VM bandwidth for N1/N2 VMs increases the maximum egress bandwidth ranging from 50\u00a0Gbps and 100\u00a0Gbps, making these a good choice for reducer VM nodes.\nThe total egress bandwidth of workers and reducers should be the same. For example, if you use 8 `a2-megagpu-16g` VMs as workers, you should use at least 25 `n1-highcpu-16` VMs as reducers.\n```\n`(8 worker VMs * 100&nbsp;Gbps) / 32&nbsp;Gbps egress = 25 reducer VMs`.\n```\nReduction Server works best if the messages to be aggregated are sufficiently large. Most ML frameworks already provide techniques under different terminology for batching small gradient tensors before performing all-reduce.\nHorovod supports [Tensor Fusion](https://horovod.readthedocs.io/en/stable/tensor-fusion_include.html) to batch small tensors for all-reduce. Tensors are filled in a fusion buffer until the buffer is fully filled and the all-reduce operation on the buffer executes. You can adjust the size of the fusion buffer by setting the `HOROVOD_FUSION_THRESHOLD` environment variable.\nThe recommended value for the `HOROVOD_FUSION_THRESHOLD` environment variable is at least 128\u00a0MB. In this case, set the `HOROVOD_FUSION_THRESHOLD` environment variable to 134217728 (128 * 1024 * 1024).\nPyTorch [DistributedDataParallel](https://pytorch.org/docs/stable/notes/ddp.html) supports batch messages as \"gradient bucketing\". Set the `bucket_cap_mb` parameter in the `DistributedDataParallel` constructor to control the size of your batch buckets. The default size is 25\u00a0MB.\nBEST PRACTICE: The recommended value of bucket_cap_mb is 64 (64\u00a0MB).\n## Environment variables for your cluster\nVertex AI populates an environment variable, `CLUSTER_SPEC` , on every replica to describe how the overall cluster is set up. Like [TensorFlow'sTF_CONFIG](#tf-config) , `CLUSTER_SPEC` describes every replica in the cluster, including its index and role (primary replica, worker, parameter server, or evaluator).\nWhen you run distributed training with TensorFlow, `TF_CONFIG` is parsed to build [tf.train.ClusterSpec](https://www.tensorflow.org/api_docs/python/tf/train/ClusterSpec) . Similarly, when you run distributed training with other ML frameworks, you must parse `CLUSTER_SPEC` to populate any environment variables or settings required by the framework.\n### The format of CLUSTER_SPEC\nThe `CLUSTER_SPEC` environment variable is a JSON string with the following format:\n| Key   | Description                                                                      | Unnamed: 2                                                                                 |\n|:--------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| \"cluster\"  | The cluster description for your custom container. As with TF_CONFIG, this object is formatted as a TensorFlow cluster specification, and can be passed to the constructor of tf.train.ClusterSpec. The cluster description contains a list of replica names for each worker pool you specify. | The cluster description for your custom container. As with TF_CONFIG, this object is formatted as a TensorFlow cluster specification, and can be passed to the constructor of tf.train.ClusterSpec. The cluster description contains a list of replica names for each worker pool you specify.           |\n| nan   | \"workerpool0\"                                                                      | All distributed training jobs have one primary replica in the first worker pool.                                                               |\n| nan   | \"workerpool1\"                                                                      | This worker pool contains worker replicas, if you specified them when creating your job.                                                             |\n| nan   | \"workerpool2\"                                                                      | This worker pool contains parameter servers, if you specified them when creating your job.                                                             |\n| nan   | \"workerpool3\"                                                                      | This worker pool contains evaluators, if you specified them when creating your job.                                                              |\n| \"environment\" | The string cloud.                                                                     | The string cloud.                                                                               |\n| \"task\"  | Describes the task of the particular node on which your code is running. You can use this information to write code for specific workers in a distributed job. This entry is a dictionary with the following keys:                    | Describes the task of the particular node on which your code is running. You can use this information to write code for specific workers in a distributed job. This entry is a dictionary with the following keys:                              |\n| nan   | \"type\"                                                                        | The type of worker pool this task is running in. For example, \"workerpool0\" refers to the primary replica.                                                         |\n| nan   | \"index\"                                                                       | The zero-based index of the task. For example, if your training job includes two workers, this value is set to 0 on one of them and 1 on the other.                                              |\n| nan   | \"trial\"                                                                       | The identifier of the hyperparameter tuning trial currently running. When you configure hyperparameter tuning for your job, you set a number of trials to train. This value gives you a way to differentiate in your code between trials that are running. The identifier is a string value containing the trial number, starting at 1. |\n| job   | The CustomJobSpec that you provided to create the current training job, represented as a dictionary.                                                | The CustomJobSpec that you provided to create the current training job, represented as a dictionary.                                                          |\nHere is an example value:\n```\n{\n \"cluster\":{\n  \"workerpool0\":[   \"cmle-training-workerpool0-ab-0:2222\"\n  ],\n  \"workerpool1\":[   \"cmle-training-workerpool1-ab-0:2222\",\n   \"cmle-training-workerpool1-ab-1:2222\"\n  ],\n  \"workerpool2\":[   \"cmle-training-workerpool2-ab-0:2222\",\n   \"cmle-training-workerpool2-ab-1:2222\"\n  ],\n  \"workerpool3\":[   \"cmle-training-workerpool3-ab-0:2222\",\n   \"cmle-training-workerpool3-ab-1:2222\",\n   \"cmle-training-workerpool3-ab-2:2222\"\n  ]\n },\n \"environment\":\"cloud\",\n \"task\":{\n  \"type\":\"workerpool0\",\n  \"index\":0,\n  \"trial\":\"TRIAL_ID\"\n },\n \"job\": {\n  ...\n }\n}\n```\n### The format of TF_CONFIG\nIn addition to `CLUSTER_SPEC` , Vertex AI sets the [TF_CONFIG](https://www.tensorflow.org/guide/distributed_training#setting_up_tf_config_environment_variable) environment variable on each replica of all distributed training jobs. Vertex AI does set `TF_CONFIG` for single-replica training jobs.\n`CLUSTER_SPEC` and `TF_CONFIG` share some values, but they have different formats. Both environment variables include additional fields beyond what TensorFlow requires.\nDistributed training with TensorFlow works the same way when you use custom containers as when you use a prebuilt container.\nThe `TF_CONFIG` environment variable is a JSON string with the following format:\n| TF_CONFIG fields | TF_CONFIG fields.1                                                                                                                                                                                                                                                                                                                                                |\n|:-------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| cluster   | The TensorFlow cluster description. A dictionary mapping one or more task names (chief, worker, ps, or master) to lists of network addresses where these tasks are running. For a given training job, this dictionary is the same on every VM. This is a valid first argument for the tf.train.ClusterSpec constructor. Note that this dictionary never contains evaluator as a key, since evaluators are not considered part of the training cluster even if you use them for your job.                                                                                                                                                                                                                             |\n| task    | The task description of the VM where this environment variable is set. For a given training job, this dictionary is different on every VM. You can use this information to customize what code runs on each VM in a distributed training job. You can also use it to change the behavior of your training code for different trials of a hyperparameter tuning job. This dictionary includes the following key-value pairs: task fields type The type of task that this VM is performing. This value is set to worker on workers, ps on parameter servers, and evaluator on evaluators. On your job's master worker, the value is set to either chief or master; learn more about the difference between the two in the chief versus master section of this document. index The zero-based index of the task. For example, if your training job includes two workers, this value is set to 0 on one of them and 1 on the other. trial The ID of the hyperparameter tuning trial currently running on this VM. This field is only set if the current training job is a hyperparameter tuning job. For hyperparameter tuning jobs, Vertex AI runs your training code repeatedly in many trials with different hyperparameters each time. This field contains the current trial number, starting at 1 for the first trial. cloud An ID used internally by Vertex AI. You can ignore this field. |\n| task fields  | task fields                                                                                                                                                                                                                                                                                                                                                  |\n| type    | The type of task that this VM is performing. This value is set to worker on workers, ps on parameter servers, and evaluator on evaluators. On your job's master worker, the value is set to either chief or master; learn more about the difference between the two in the chief versus master section of this document.                                                                                                                                                                                                                                                                     |\n| index    | The zero-based index of the task. For example, if your training job includes two workers, this value is set to 0 on one of them and 1 on the other.                                                                                                                                                                                                                                                                                                               |\n| trial    | The ID of the hyperparameter tuning trial currently running on this VM. This field is only set if the current training job is a hyperparameter tuning job. For hyperparameter tuning jobs, Vertex AI runs your training code repeatedly in many trials with different hyperparameters each time. This field contains the current trial number, starting at 1 for the first trial.                                                                                                                                                                                                                                                       |\n| cloud    | An ID used internally by Vertex AI. You can ignore this field.                                                                                                                                                                                                                                                                                                                                     |\n| job    | The CustomJobSpec that you provided to create the current training job, represented as a dictionary.                                                                                                                                                                                                                                                                                                                           |\n| environment  | The string cloud.                                                                                                                                                                                                                                                                                                                                                |\nThe following example code prints the `TF_CONFIG` environment variable to your training logs:\n```\nimport jsonimport ostf_config_str = os.environ.get('TF_CONFIG')tf_config_dict \u00a0= json.loads(tf_config_str)# Convert back to string just for pretty printingprint(json.dumps(tf_config_dict, indent=2))\n```\nIn a hyperparameter tuning job that runs in runtime version 2.1 or later and uses a master worker, two workers, and a parameter server, this code produces the following log for one of the workers during the first hyperparameter tuning trial. The example output hides the `job` field for conciseness and replaces some IDs with generic values.\n```\n{\n \"cluster\": {\n \"chief\": [  \"training-workerpool0-[ID_STRING_1]-0:2222\"\n ],\n \"ps\": [  \"training-workerpool2-[ID_STRING_1]-0:2222\"\n ],\n \"worker\": [  \"training-workerpool1-[ID_STRING_1]-0:2222\",\n  \"training-workerpool1-[ID_STRING_1]-1:2222\"\n ]\n },\n \"environment\": \"cloud\",\n \"job\": {\n ...\n },\n \"task\": {\n \"cloud\": \"[ID_STRING_2]\",\n \"index\": 0,\n \"trial\": \"1\",\n \"type\": \"worker\"\n }\n}\n```\n## When to use TF_CONFIG\n`TF_CONFIG` is set only for distributed training jobs.\nYou likely don't need to interact with the `TF_CONFIG` environment variable directly in your training code. Only access the the `TF_CONFIG` environment variable if TensorFlow's distribution strategies and Vertex AI's standard hyperparameter tuning workflow, both described in the next sections, do not work for your job.\n### Distributed training\nVertex AI sets the `TF_CONFIG` environment variable to extend the [specifications that TensorFlow requires for distributedtraining](https://www.tensorflow.org/guide/distributed_training#setting_up_tf_config_environment_variable) .\nTo perform distributed training with TensorFlow, use the [tf.distribute.StrategyAPI](https://www.tensorflow.org/guide/distributed_training) . In particular, we recommend that you use the Keras API together with the [MultiWorkerMirroredStrategy](https://www.tensorflow.org/guide/distributed_training#multiworkermirroredstrategy) or, if you [specify parameter servers for your job](/vertex-ai/docs/training/configure-compute) , the [ParameterServerStrategy](https://www.tensorflow.org/guide/distributed_training#parameterserverstrategy) . However, note that TensorFlow currently only provides experimental support for these strategies.\nThese distribution strategies use the `TF_CONFIG` environment variable to assign roles to each VM in your training job and to facilitate communication between the VMs. You do not need to access the `TF_CONFIG` environment variable directly in your training code, because TensorFlow handles it for you.\nOnly parse the `TF_CONFIG` environment variable directly if you want to customize how the different VMs running your training job behave.\n### Hyperparameter tuning\nWhen you run a [hyperparameter tuningjob](/vertex-ai/docs/training/hyperparameter-tuning-overview) , Vertex AI provides different arguments to your training code for each trial. Your training code does not necessarily need to be aware of what trial is currently running. In addition, you can monitor the progress of hyperparameter tuning jobs in Google Cloud console.\nIf needed, your code can read the current trial number from [the trial fieldof the task field of the TF_CONFIG environment variable](#tf-config) .\n## What's next\n- Create a [training pipeline](/vertex-ai/docs/training/create-training-pipeline) .", "guide": "Vertex AI"}