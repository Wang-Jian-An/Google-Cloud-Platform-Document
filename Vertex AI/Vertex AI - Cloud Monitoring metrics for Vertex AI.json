{"title": "Vertex AI - Cloud Monitoring metrics for Vertex AI", "url": "https://cloud.google.com/vertex-ai/docs/general/monitoring-metrics", "abstract": "# Vertex AI - Cloud Monitoring metrics for Vertex AI\nVertex AI exports metrics to [Cloud Monitoring](/monitoring/docs) . Vertex AI also shows some of these metrics in the Vertex AI Google Cloud console. You can use Cloud Monitoring to create dashboards or configure alerts based on the metrics. For example, you can receive alerts if a model's prediction latency in Vertex AI gets too high.\nThe following sections describe the metrics provided in the Vertex AI Google Cloud console, which might be direct or calculated metrics that Vertex AI sends to Cloud Monitoring.\nTo view a list of most metrics that Vertex AI exports to Cloud Monitoring, see the [\"aiplatform\" section of the Monitoring Google Cloudmetrics page](/monitoring/api/metrics_gcp#gcp-aiplatform) . For custom training metrics, see metric types that start with `training` in the [\"ml\" section of that page](/monitoring/api/metrics_gcp#gcp-ml) .\n", "content": "## Custom training monitoring metrics\nWhen you [perform custom training](/vertex-ai/docs/training/overview) , you can monitor the following types of resource usage for each training node:\n- CPU or GPU utilization of each training node\n- Memory utilization of each training node\n- Network usage (bytes sent per second and bytes received per second)\nIf you are using [hyperparameter tuning](/vertex-ai/docs/training/hyperparameter-tuning-overview) , you can see the metrics for each trial.\nTo view these metrics after you have initiated custom training, do the following:\n- In the Google Cloud console, go to one of the following pages, depending on whether you are using hyperparameter tuning:- If you aren't using hyperparameter tuning, go to the **Custom jobs** page. [Go to Custom jobs](https://console.cloud.google.com/vertex-ai/training/custom-jobs) \n- If you are using hyperparameter tuning, go to the **Hyperparameter tuningjobs** page. [Go to Hyperparameter tuning jobs](https://console.cloud.google.com/vertex-ai/training/hyperparameter-tuning-jobs) \n- Click the name of your custom training resource.If you created a [custom TrainingPipeline resource](/vertex-ai/docs/training/create-training-pipeline) , then click the name of the job created by the `TrainingPipeline` ; for example, `` `-custom-job` or `` `-hyperparameter-tuning-job` .\n- Click the **CPU** , **GPU** , or **Network** tab to view utilization charts for the metric that you are interested in.If you are using hyperparameter tuning, you can click a row in the **Hyperparamater tuning trials** table to view metrics for a specific trial.\nTo see older metrics or to customize how you view metrics, use Monitoring. Vertex AI exports custom training metrics to Monitoring as [metric types with the prefix ml.googleapis.com/training](/monitoring/api/metrics_gcp#gcp-ml) . The monitored resource type is [cloudml_job](/monitoring/api/resources#tag_cloudml_job) .\nNote that [AI Platform Training](/ai-platform/training/docs) exports metrics to Monitoring with the same metric types and resource type.\n## Endpoint monitoring metrics\nAfter you deploy a model to an [endpoint](/vertex-ai/docs/predictions/deploy-model-console) , you can monitor the endpoint to understand your model's performance and resource usage. You can track metrics such as traffic patterns, error rates, latency, and resource utilization to ensure that your model consistently and predictably responds to requests. For example, you might redeploy your model with a different machine type to optimize for cost. After you make the change, you can monitor the model to check whether your changes adversely affected its performance.\nIn Cloud Monitoring, the monitored resource type for deployed models is [aiplatform.googleapis.com/Endpoint](/monitoring/api/resources#tag_aiplatform.googleapis.com/Endpoint) .\n### Performance metrics\nPerformance metrics can help you find information about your model's traffic patterns, errors, and latency. You can view the following performance metrics in the Google Cloud console.\n- **Predictions per second** : The number of predictions per second across both online and batch predictions. If you have more than one instance per request, each instance is counted in this chart.\n- **Prediction error percentage** : The rate of errors that your model is producing. A high error rate might indicate an issue with the model or with the requests to the model. View the response codes chart to determine which errors are occurring.\n- **Model latency (for tabular and custom models only)** : The time spent performing computation.\n- **Overhead latency (for tabular and custom models only)** : The total time spent processing a request, outside of computation.\n- **Total latency duration** : The total time that a request spends in the service, which is the model latency plus the overhead latency.\n### Resource usage\nResource usage metrics can help you track your model's CPU usage, memory usage, and network usage. You can view the following usage metrics in the Google Cloud console.\n- **Replica count** : The number of active replicas used by the deployed model.\n- **Replica target** : The number of active replicas required for the deployed model.\n- **CPU usage** : Current CPU core usage rate of the deployed model replica. 100% represents one fully utilized CPU core, so a replica may achieve more than 100% utilization if its machine type has multiple cores.\n- **Memory usage** : The amount of memory allocated by the deployed model replica and currently in use.\n- **Network bytes sent** : The number of bytes sent over the network by the deployed model replica.\n- **Network bytes received** : The number of bytes received over the network by the deployed model replica.\n- **Accelerator average duty cycle** : The average fraction of time over the past sample period during which one or more accelerators were actively processing.\n- **Accelerator memory usage** : The amount of memory allocated by the deployed model replica.\n### View endpoint monitoring metric charts\n- Go to the Vertex AI **Endpoints** page in the Google Cloud console. [Go to the Endpoints page](https://console.cloud.google.com/vertex-ai/endpoints) \n- Click the name of an endpoint to view its metrics.\n- Below the chart intervals, click **Performance** or **Resource usage** to view the performance or resource usage metrics.You can select different chart intervals to see metric values over a particular time period, such as 1 hour, 12 hours, or 14 days.If you have multiple models deployed to the endpoint, you can select or deselect models to view or hide metrics for particular models. If you select multiple models, the console groups some model metrics into a single chart. For example, if a metric provides only one value per model, the console groups the model metrics into a single chart, such as CPU usage. For metrics that can have multiple values per model, the console provides a chart for each model. For example, the console provides a response code chart for each model.## Vertex AI Feature Store monitoring metrics\nAfter you build a [featurestore](/vertex-ai/docs/featurestore) , you can monitor it's performance and resource utilization, such as the online storage serving latencies or the number of online storage nodes. For example, you might update the number of online storage nodes of a featurestore and then monitor changes to the online storage serving metrics.\nIn Cloud Monitoring, the monitored resource type for a featurestore is [aiplatform.googleapis.com/Featurestore](/monitoring/api/resources?hl=en#tag_aiplatform.googleapis.com/Featurestore) .\n### Metrics\n- **Request size** : The request size by entity type in your featurestore.\n- **Offline storage write for streaming write** : The number of streaming write requests processed for the offline storage.\n- **Streaming write to offline storage delay time** : The time elapsed (in seconds) between calling the write API and writing to the offline storage.\n- **Node count** : The number of online serving nodes for your featurestore.\n- **Latency** : The total time that an online serving or streaming ingestion request spends in the service.\n- **Queries per second** : The number of online serving or streaming ingestion queries that your featurestore handles.\n- **Errors percentage** : The percentage of errors that your featurestore produces when handling online serving or streaming ingestion requests.\n- **CPU utilization** : The fraction of CPU allocated by the featurestore and currently in use by online storage. This number can exceed 100% if the online serving storage is overloaded. Consider increasing the featurestore's number of online serving nodes to reduce CPU utilization.\n- **CPU utilization - hottest node** : The CPU load for the hottest node in the featurestore's online storage.\n- **Total offline storage** : Amount of data stored in the featurestore's [offline storage](/vertex-ai/docs/featurestore/concepts#storage) .\n- **Total online storage** : Amount of data stored in the featurestore's [onlinestorage](/vertex-ai/docs/featurestore/concepts#storage) .\n- **Online serving throughput** : In MB/s, the throughput for online serving requests.\n### View featurestore monitoring metric charts\n- Go to the Vertex AI **Features** page in the Google Cloud console. [Go to the Features page](https://console.cloud.google.com/vertex-ai/features) \n- In the **Featurestore** column, click the name of a featurestore to view its metrics.You can select different chart intervals to see metric values over a particular time period, such as 1 hour, 1 day, or 1 week.For some online serving metrics, you can choose to view metrics for a particular method, which further breaks down metrics by entity type. For example, you can view the latency for the `ReadFeatureValues` method or the `StreamingReadFeatureValues` method.", "guide": "Vertex AI"}