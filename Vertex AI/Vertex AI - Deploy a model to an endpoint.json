{"title": "Vertex AI - Deploy a model to an endpoint", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Deploy a model to an endpoint\nYou must deploy a model to an endpoint before that model can be used to serve online predictions. Deploying a model associates physical resources with the model so it can serve online predictions with low latency.\nYou can deploy more than one model to an endpoint, and you can deploy a model to more than one endpoint. For more information about options and use cases for deploying models, see [Reasons to deploy more than one model to the sameendpoint](#models-endpoint) below.\n", "content": "## Deploy a model to an endpoint\nUse one of the following methods to deploy a model:\n- In the Google Cloud console, in the Vertex AI section, go to the **Models** page. [Go to the Models page](https://console.cloud.google.com/vertex-ai/models) \n- Click the name and version ID of the model you want to deploy to open its details page.\n- Select the **Deploy & Test** tab.If your model is already deployed to any endpoints, they are listed in the **Deploy your model** section.\n- Click **Deploy to endpoint** .\n- To deploy your model to a new endpoint, select radio_button_checked **Create new endpoint** , and provide a name for the new endpoint. To deploy your model to an existing endpoint, select radio_button_checked **Add to existing endpoint** , and select the endpoint from the drop-down list.You can add more than one model to an endpoint, and you can add a model to more than one endpoint.\n- If you deploy your model to an existing endpoint that has one or more models deployed to it, you must update the **Traffic split** percentage for the model you are deploying and the already deployed models so that all of the percentages add up to 100%.\n- If you're deploying your model to a new endpoint, accept 100 for the **Traffic split** . Otherwise, adjust the traffic split values for all models on the endpoint so they add up to 100.\n- Enter the **Minimum number of compute nodes** you want to provide for your model.This is the number of nodes available to this model at all times.You are charged for the nodes used, whether to handle prediction load or for standby (minimum) nodes, even without prediction traffic. See the [pricingpage](/vertex-ai/pricing) .The number of compute nodes can increase if needed to handle prediction traffic, but it will never go higher than the maximum number of nodes.\n- To use autoscaling, enter the **Maximum number of compute nodes** you want Vertex AI to scale up to.\n- Select your **Machine type** .Larger machine resources increase your prediction performance and increase costs. [Compare the available machinetypes](/vertex-ai/docs/predictions/configure-compute#machine_type_comparison) .\n- Select an **Accelerator type** and an **Accelerator count** .If you enabled accelerator use when you [imported](/vertex-ai/docs/model-registry/import-model) or created the model, this option displays.For the accelerator count, refer to the [GPUtable](/vertex-ai/docs/predictions/configure-compute#gpus) to check for valid numbers of GPUs that you can use with each CPU machine type. The accelerator count refers to the number of accelerators per node, not the total number of accelerators in your deployment.\n- If you want to use a [custom serviceaccount](/vertex-ai/docs/general/custom-service-account) for the deployment, select a service account in the **Service account** drop-down box.\n- Learn how to [change thedefault settings for prediction logging](/vertex-ai/docs/predictions/online-prediction-logging#enabling-and-disabling) .\n- Click **Done** for your model, and when all the **Traffic split** percentages are correct, click **Continue** .The region where your model deploys is displayed. This must be the region where you created your model.\n- Click **Deploy** to deploy your model to the endpoint.\nWhen you deploy a model using the Vertex AI API, you complete the following steps:- [Create](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/create) an endpoint if needed.\n- [Get](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/get) the endpoint ID.\n- [Deploy](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/deployModel) the model to the endpoint.\n### Create an endpointIf you are deploying a model to an existing endpoint, you can skip this step.The following example uses the [gcloud ai endpoints createcommand](/sdk/gcloud/reference/ai/endpoints/create) :\n```\ngcloud ai endpoints create \\\u00a0 --region=LOCATION_ID \\\u00a0 --display-name=ENDPOINT_NAME\n```\nReplace the following:- : The region where you are using Vertex AI.\n- : The display name for the endpoint.\nThe Google Cloud CLI tool might take a few seconds to create the endpoint.Before using any of the request data, make the following replacements:- : Your region.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The display name for the endpoint.\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints\n```\nRequest JSON body:\n```\n{\n \"display_name\": \"ENDPOINT_NAME\"\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/endpoints/ENDPOINT_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.CreateEndpointOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2020-11-05T17:45:42.812656Z\",\n  \"updateTime\": \"2020-11-05T17:45:42.812656Z\"\n }\n }\n}\n```\nYou can poll for the status of the operation until the response includes\n`\"done\": true`\n.\nThe following sample uses the [google_vertex_ai_endpoint](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/vertex_ai_endpoint) Terraform resource to create an endpoint.\nTo learn how to apply or remove a Terraform configuration, see [Basic Terraform commands](/docs/terraform/basic-commands) .\n [View on GitHub](https://github.com/terraform-google-modules/terraform-docs-samples/blob/HEAD/vertex_ai/endpoint/main.tf) \n```\n# Endpoint name must be unique for the projectresource \"random_id\" \"endpoint_id\" {\u00a0 byte_length = 4}resource \"google_vertex_ai_endpoint\" \"default\" {\u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 = substr(random_id.endpoint_id.dec, 0, 10)\u00a0 display_name = \"sample-endpoint\"\u00a0 description \u00a0= \"A sample Vertex AI endpoint\"\u00a0 location \u00a0 \u00a0 = \"us-central1\"\u00a0 labels = {\u00a0 \u00a0 label-one = \"value-one\"\u00a0 }}\n```Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateEndpointSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.CreateEndpointOperationMetadata;import com.google.cloud.aiplatform.v1.Endpoint;import com.google.cloud.aiplatform.v1.EndpointServiceClient;import com.google.cloud.aiplatform.v1.EndpointServiceSettings;import com.google.cloud.aiplatform.v1.LocationName;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class CreateEndpointSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String endpointDisplayName = \"YOUR_ENDPOINT_DISPLAY_NAME\";\u00a0 \u00a0 createEndpointSample(project, endpointDisplayName);\u00a0 }\u00a0 static void createEndpointSample(String project, String endpointDisplayName)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 EndpointServiceSettings endpointServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 EndpointServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (EndpointServiceClient endpointServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 EndpointServiceClient.create(endpointServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 LocationName locationName = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 Endpoint endpoint = Endpoint.newBuilder().setDisplayName(endpointDisplayName).build();\u00a0 \u00a0 \u00a0 OperationFuture<Endpoint, CreateEndpointOperationMetadata> endpointFuture =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 endpointServiceClient.createEndpointAsync(locationName, endpoint);\u00a0 \u00a0 \u00a0 System.out.format(\"Operation name: %s\\n\", endpointFuture.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to finish...\");\u00a0 \u00a0 \u00a0 Endpoint endpointResponse = endpointFuture.get(300, TimeUnit.SECONDS);\u00a0 \u00a0 \u00a0 System.out.println(\"Create Endpoint Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"Name: %s\\n\", endpointResponse.getName());\u00a0 \u00a0 \u00a0 System.out.format(\"Display Name: %s\\n\", endpointResponse.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\"Description: %s\\n\", endpointResponse.getDescription());\u00a0 \u00a0 \u00a0 System.out.format(\"Labels: %s\\n\", endpointResponse.getLabelsMap());\u00a0 \u00a0 \u00a0 System.out.format(\"Create Time: %s\\n\", endpointResponse.getCreateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"Update Time: %s\\n\", endpointResponse.getUpdateTime());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/create-endpoint.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const endpointDisplayName = 'YOUR_ENDPOINT_DISPLAY_NAME';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Endpoint Service Client libraryconst {EndpointServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst endpointServiceClient = new EndpointServiceClient(clientOptions);async function createEndpoint() {\u00a0 // Configure the parent resource\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 const endpoint = {\u00a0 \u00a0 displayName: endpointDisplayName,\u00a0 };\u00a0 const request = {\u00a0 \u00a0 parent,\u00a0 \u00a0 endpoint,\u00a0 };\u00a0 // Get and print out a list of all the endpoints for this resource\u00a0 const [response] = await endpointServiceClient.createEndpoint(request);\u00a0 console.log(`Long running operation : ${response.name}`);\u00a0 // Wait for operation to complete\u00a0 await response.promise();\u00a0 const result = response.result;\u00a0 console.log('Create endpoint response');\u00a0 console.log(`\\tName : ${result.name}`);\u00a0 console.log(`\\tDisplay name : ${result.displayName}`);\u00a0 console.log(`\\tDescription : ${result.description}`);\u00a0 console.log(`\\tLabels : ${JSON.stringify(result.labels)}`);\u00a0 console.log(`\\tCreate time : ${JSON.stringify(result.createTime)}`);\u00a0 console.log(`\\tUpdate time : ${JSON.stringify(result.updateTime)}`);}createEndpoint();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_endpoint_sample.py) \n```\ndef create_endpoint_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 location: str,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 endpoint = aiplatform.Endpoint.create(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name,\u00a0 \u00a0 \u00a0 \u00a0 project=project,\u00a0 \u00a0 \u00a0 \u00a0 location=location,\u00a0 \u00a0 )\u00a0 \u00a0 print(endpoint.display_name)\u00a0 \u00a0 print(endpoint.resource_name)\u00a0 \u00a0 return endpoint\n```\n### Retrieve the endpoint IDYou need the endpoint ID to deploy the model.\nThe following example uses the [gcloud ai endpoints listcommand](/sdk/gcloud/reference/ai/endpoints/list) :\n```\ngcloud ai endpoints list \\\u00a0 --region=LOCATION_ID \\\u00a0 --filter=display_name=ENDPOINT_NAME\n```\nReplace the following:- : The region where you are using Vertex AI.\n- : The display name for the endpoint.\nNote the number that appears in the `ENDPOINT_ID` column. Use this ID in the following step.Before using any of the request data, make the following replacements:- : The region where you are using Vertex AI.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The display name for the endpoint.\nHTTP method and URL:\n```\nGET https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints?filter=display_name=ENDPOINT_NAME\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"endpoints\": [ {\n  \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/endpoints/ENDPOINT_ID\",\n  \"displayName\": \"ENDPOINT_NAME\",\n  \"etag\": \"AMEw9yPz5pf4PwBHbRWOGh0PcAxUdjbdX2Jm3QO_amguy3DbZGP5Oi_YUKRywIE-BtLx\",\n  \"createTime\": \"2020-04-17T18:31:11.585169Z\",\n  \"updateTime\": \"2020-04-17T18:35:08.568959Z\"\n }\n ]\n}\n```\nNote the\n.### Deploy the modelSelect the tab below for your language or environment:The following examples use the [gcloud ai endpoints deploy-model command](/sdk/gcloud/reference/ai/endpoints/deploy-model) .\nThe following example deploys a `Model` to an `Endpoint` without using GPUs to accelerate prediction serving and without splitting traffic between multiple `DeployedModel` resources:\nBefore using any of the command data below, make the following replacements:- : The ID for the endpoint.\n- : The region where you are using Vertex AI.\n- : The ID for the model to be deployed.\n- : A name for the`DeployedModel`. You can use the display name of the`Model`for the`DeployedModel`as well.\n- : The minimum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to the maximum number of nodes and never fewer than this number of nodes.\n- : The maximum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to this number of nodes and never fewer than the minimum number of nodes. If you omit the`--max-replica-count`flag, then  maximum number of nodes is set to the value of`--min-replica-count`.\nExecute the [gcloud ai endpoints deploy-model](/sdk/gcloud/reference/ai/endpoints/deploy-model) command:The `--traffic-split=0=100` flag in the preceding examples sends 100% of prediction traffic that the `Endpoint` receives to the new `DeployedModel` , which is represented by the temporary ID `0` . If your `Endpoint` already has other `DeployedModel` resources, then you can split traffic between the new `DeployedModel` and the old ones. For example, to send 20% of traffic to the new `DeployedModel` and 80% to an older one, run the following command.\nBefore using any of the command data below, make the following replacements:- : the ID of the existing`DeployedModel`.\nExecute the [gcloud ai endpoints deploy-model](/sdk/gcloud/reference/ai/endpoints/deploy-model) command:Deploy the model.\nBefore using any of the request data, make the following replacements:- : The region where you are using Vertex AI.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The ID for the endpoint.\n- : The ID for the model to be deployed.\n- : A name for the`DeployedModel`. You can use the display name of the`Model`for the`DeployedModel`as well.\n- : Optional. The machine resources used for each node of this deployment. Its default setting is`n1-standard-2`. [Learn more about machine types.](/vertex-ai/docs/predictions/configure-compute) \n- : The type of accelerator to be attached to the machine. Optional  ifis not specified or is zero. Not recommended for  AutoML models or custom-trained models that are using non-GPU images. [Learn more](/vertex-ai/docs/predictions/configure-compute#gpus) .\n- : The number of accelerators for each replica to use.  Optional. Should be zero or unspecified for AutoML models or custom-trained models  that are using non-GPU images.\n- : The minimum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to the maximum number of nodes and never fewer than this number of nodes. This value must be greater than or equal to 1.\n- : The maximum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to this number of nodes and never fewer than the minimum number of nodes.\n- : The percentage of the prediction traffic to this endpoint  to be routed to the model being deployed with this operation. Defaults to 100. All traffic  percentages must add up to 100. [Learn more about traffic splits](/vertex-ai/docs/general/deployment#models-endpoint) .\n- : Optional. If other models are deployed to this endpoint, you  must update their traffic split percentages so that all percentages add up to 100.\n- : The traffic split percentage value for the deployed model id  key.\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) \nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:deployModel\n```\nRequest JSON body:\n```\n{\n \"deployedModel\": {\n \"model\": \"projects/PROJECT/locations/us-central1/models/MODEL_ID\",\n \"displayName\": \"DEPLOYED_MODEL_NAME\",\n \"dedicatedResources\": {\n  \"machineSpec\": {\n   \"machineType\": \"MACHINE_TYPE\",\n   \"acceleratorType\": \"ACCELERATOR_TYPE\",\n   \"acceleratorCount\": \"ACCELERATOR_COUNT\"\n  },\n  \"minReplicaCount\": MIN_REPLICA_COUNT,\n  \"maxReplicaCount\": MAX_REPLICA_COUNT\n  },\n },\n \"trafficSplit\": {\n \"0\": TRAFFIC_SPLIT_THIS_MODEL,\n \"DEPLOYED_MODEL_ID_1\": TRAFFIC_SPLIT_MODEL_1,\n \"DEPLOYED_MODEL_ID_2\": TRAFFIC_SPLIT_MODEL_2\n },\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_ID/locations/LOCATION/endpoints/ENDPOINT_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.DeployModelOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2020-10-19T17:53:16.502088Z\",\n  \"updateTime\": \"2020-10-19T17:53:16.502088Z\"\n }\n }\n}\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/DeployModelCustomTrainedModelSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.DedicatedResources;import com.google.cloud.aiplatform.v1.DeployModelOperationMetadata;import com.google.cloud.aiplatform.v1.DeployModelResponse;import com.google.cloud.aiplatform.v1.DeployedModel;import com.google.cloud.aiplatform.v1.EndpointName;import com.google.cloud.aiplatform.v1.EndpointServiceClient;import com.google.cloud.aiplatform.v1.EndpointServiceSettings;import com.google.cloud.aiplatform.v1.MachineSpec;import com.google.cloud.aiplatform.v1.ModelName;import java.io.IOException;import java.util.HashMap;import java.util.Map;import java.util.concurrent.ExecutionException;public class DeployModelCustomTrainedModelSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, ExecutionException, InterruptedException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"PROJECT\";\u00a0 \u00a0 String endpointId = \"ENDPOINT_ID\";\u00a0 \u00a0 String modelName = \"MODEL_NAME\";\u00a0 \u00a0 String deployedModelDisplayName = \"DEPLOYED_MODEL_DISPLAY_NAME\";\u00a0 \u00a0 deployModelCustomTrainedModelSample(project, endpointId, modelName, deployedModelDisplayName);\u00a0 }\u00a0 static void deployModelCustomTrainedModelSample(\u00a0 \u00a0 \u00a0 String project, String endpointId, String model, String deployedModelDisplayName)\u00a0 \u00a0 \u00a0 throws IOException, ExecutionException, InterruptedException {\u00a0 \u00a0 EndpointServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 EndpointServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (EndpointServiceClient client = EndpointServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 MachineSpec machineSpec = MachineSpec.newBuilder().setMachineType(\"n1-standard-2\").build();\u00a0 \u00a0 \u00a0 DedicatedResources dedicatedResources =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 DedicatedResources.newBuilder().setMinReplicaCount(1).setMachineSpec(machineSpec).build();\u00a0 \u00a0 \u00a0 String modelName = ModelName.of(project, location, model).toString();\u00a0 \u00a0 \u00a0 DeployedModel deployedModel =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 DeployedModel.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModel(modelName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(deployedModelDisplayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // `dedicated_resources` must be used for non-AutoML models\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDedicatedResources(dedicatedResources)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // key '0' assigns traffic for the newly deployed model\u00a0 \u00a0 \u00a0 // Traffic percentage values must add up to 100\u00a0 \u00a0 \u00a0 // Leave dictionary empty if endpoint should not accept any traffic\u00a0 \u00a0 \u00a0 Map<String, Integer> trafficSplit = new HashMap<>();\u00a0 \u00a0 \u00a0 trafficSplit.put(\"0\", 100);\u00a0 \u00a0 \u00a0 EndpointName endpoint = EndpointName.of(project, location, endpointId);\u00a0 \u00a0 \u00a0 OperationFuture<DeployModelResponse, DeployModelOperationMetadata> response =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 client.deployModelAsync(endpoint, deployedModel, trafficSplit);\u00a0 \u00a0 \u00a0 // You can use OperationFuture.getInitialFuture to get a future representing the initial\u00a0 \u00a0 \u00a0 // response to the request, which contains information while the operation is in progress.\u00a0 \u00a0 \u00a0 System.out.format(\"Operation name: %s\\n\", response.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 // OperationFuture.get() will block until the operation is finished.\u00a0 \u00a0 \u00a0 DeployModelResponse deployModelResponse = response.get();\u00a0 \u00a0 \u00a0 System.out.format(\"deployModelResponse: %s\\n\", deployModelResponse);\u00a0 \u00a0 }\u00a0 }}\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/deploy_model_with_dedicated_resources_sample.py) \n```\ndef deploy_model_with_dedicated_resources_sample(\u00a0 \u00a0 project,\u00a0 \u00a0 location,\u00a0 \u00a0 model_name: str,\u00a0 \u00a0 machine_type: str,\u00a0 \u00a0 endpoint: Optional[aiplatform.Endpoint] = None,\u00a0 \u00a0 deployed_model_display_name: Optional[str] = None,\u00a0 \u00a0 traffic_percentage: Optional[int] = 0,\u00a0 \u00a0 traffic_split: Optional[Dict[str, int]] = None,\u00a0 \u00a0 min_replica_count: int = 1,\u00a0 \u00a0 max_replica_count: int = 1,\u00a0 \u00a0 accelerator_type: Optional[str] = None,\u00a0 \u00a0 accelerator_count: Optional[int] = None,\u00a0 \u00a0 explanation_metadata: Optional[explain.ExplanationMetadata] = None,\u00a0 \u00a0 explanation_parameters: Optional[explain.ExplanationParameters] = None,\u00a0 \u00a0 metadata: Optional[Sequence[Tuple[str, str]]] = (),\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 model_name: A fully-qualified model resource name or model ID.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Example: \"projects/123/locations/us-central1/models/456\" or\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"456\" when project and location are initialized or passed.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 model = aiplatform.Model(model_name=model_name)\u00a0 \u00a0 # The explanation_metadata and explanation_parameters should only be\u00a0 \u00a0 # provided for a custom trained model and not an AutoML model.\u00a0 \u00a0 model.deploy(\u00a0 \u00a0 \u00a0 \u00a0 endpoint=endpoint,\u00a0 \u00a0 \u00a0 \u00a0 deployed_model_display_name=deployed_model_display_name,\u00a0 \u00a0 \u00a0 \u00a0 traffic_percentage=traffic_percentage,\u00a0 \u00a0 \u00a0 \u00a0 traffic_split=traffic_split,\u00a0 \u00a0 \u00a0 \u00a0 machine_type=machine_type,\u00a0 \u00a0 \u00a0 \u00a0 min_replica_count=min_replica_count,\u00a0 \u00a0 \u00a0 \u00a0 max_replica_count=max_replica_count,\u00a0 \u00a0 \u00a0 \u00a0 accelerator_type=accelerator_type,\u00a0 \u00a0 \u00a0 \u00a0 accelerator_count=accelerator_count,\u00a0 \u00a0 \u00a0 \u00a0 explanation_metadata=explanation_metadata,\u00a0 \u00a0 \u00a0 \u00a0 explanation_parameters=explanation_parameters,\u00a0 \u00a0 \u00a0 \u00a0 metadata=metadata,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 return model\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/automl/tables/create-model.v1beta1.js) \n```\nconst automl = require('@google-cloud/automl');const client = new automl.v1beta1.AutoMlClient();/**\u00a0* Demonstrates using the AutoML client to create a model.\u00a0* TODO(developer): Uncomment the following lines before running the sample.\u00a0*/// const projectId = '[PROJECT_ID]' e.g., \"my-gcloud-project\";// const computeRegion = '[REGION_NAME]' e.g., \"us-central1\";// const datasetId = '[DATASET_ID]' e.g., \"TBL2246891593778855936\";// const tableId = '[TABLE_ID]' e.g., \"1991013247762825216\";// const columnId = '[COLUMN_ID]' e.g., \"773141392279994368\";// const modelName = '[MODEL_NAME]' e.g., \"testModel\";// const trainBudget = '[TRAIN_BUDGET]' e.g., \"1000\",// `Train budget in milli node hours`;// A resource that represents Google Cloud Platform location.const projectLocation = client.locationPath(projectId, computeRegion);// Get the full path of the column.const columnSpecId = client.columnSpecPath(\u00a0 projectId,\u00a0 computeRegion,\u00a0 datasetId,\u00a0 tableId,\u00a0 columnId);// Set target column to train the model.const targetColumnSpec = {name: columnSpecId};// Set tables model metadata.const tablesModelMetadata = {\u00a0 targetColumnSpec: targetColumnSpec,\u00a0 trainBudgetMilliNodeHours: trainBudget,};// Set datasetId, model name and model metadata for the dataset.const myModel = {\u00a0 datasetId: datasetId,\u00a0 displayName: modelName,\u00a0 tablesModelMetadata: tablesModelMetadata,};// Create a model with the model metadata in the region.client\u00a0 .createModel({parent: projectLocation, model: myModel})\u00a0 .then(responses => {\u00a0 \u00a0 const initialApiResponse = responses[1];\u00a0 \u00a0 console.log(`Training operation name: ${initialApiResponse.name}`);\u00a0 \u00a0 console.log('Training started...');\u00a0 })\u00a0 .catch(err => {\u00a0 \u00a0 console.error(err);\u00a0 });\n```\nLearn how to [change thedefault settings for prediction logging](/vertex-ai/docs/predictions/online-prediction-logging#enabling-and-disabling) .## Get operation status\nSome requests start long-running operations that require time to complete. These requests return an operation name, which you can use to view the operation's status or cancel the operation. Vertex AI provides helper methods to make calls against long-running operations. For more information, see [Working with long-runningoperations](/vertex-ai/docs/general/long-running-operations) .- If you have VPC Service Controls enabled, your deployed model's container won't have access to the internet.During model deployment, you make the following important decisions about how to run online prediction:\n| Resource created | Setting specified at resource creation |\n|:-------------------|:-----------------------------------------|\n| Endpoint   | Location in which to run predictions  |\n| Model    | Container to use (ModelContainerSpec) |\n| DeployedModel  | Machines to use for online prediction |\nYou can't update the settings listed above after the initial creation of the model or endpoint, and you can't override them in the online prediction request. If you need to change these settings, you must redeploy your model.\n## What happens when you deploy a model\nWhen you deploy a model to an endpoint, you associate physical (machine) resources with that model so it can serve online predictions. Online predictions have low latency requirements. Providing resources to the model in advance reduces latency.\nThe model's training type (AutoML or custom) and (AutoML) data type determine the kinds of physical resources available to the model. After model deployment, you can [mutate](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/mutateDeployedModel) some of those resources without creating a new deployment.\nThe endpoint resource provides the service endpoint (URL) you use to request the prediction. For example:\n```\nhttps://us-central1-aiplatform.googleapis.com/v1/projects/{project}/locations/{location}/endpoints/{endpoint}:predict\n```\n## Reasons to deploy more than one model to the same endpoint\nDeploying two models to the same endpoint lets you gradually replace one model with the other. For example, suppose you are using a model, and find a way to increase the accuracy of that model with new training data. However, you don't want to update your application to point to a new endpoint URL, and you don't want to create sudden changes in your application. You can add the new model to the same endpoint, serving a small percentage of traffic, and gradually increase the traffic split for the new model until it is serving 100% of the traffic.\nBecause the resources are associated with the model rather than the endpoint, you could deploy models of different types to the same endpoint. However, the best practice is to deploy models of a specific type (for example, AutoML text, AutoML tabular, custom-trained) to an endpoint. This configuration is easier to manage.\n## Reasons to deploy a model to more than one endpoint\nYou might want to deploy your models with different resources for different application environments, such as testing and production. You might also want to support different SLOs for your prediction requests. Perhaps one of your applications has much higher performance needs than the others. In this case, you can deploy that model to a higher-performance endpoint with more machine resources. To optimize costs, you can also deploy the model to a lower-performance endpoint with fewer machine resources.\n## Scaling behavior\nWhen you deploy a `Model` for online prediction as a [DeployedModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints#deployedmodel) , you can configure prediction nodes to automatically scale. To do this, set [dedicatedResources.maxReplicaCount](/vertex-ai/docs/reference/rest/v1/DedicatedResources) to a greater value than `dedicatedResources.minReplicaCount` .\nWhen you configure a `DeployedModel` , you must set `dedicatedResources.minReplicaCount` to at least `1` . In other words, you cannot configure the `DeployedModel` to scale to `0` prediction nodes when it is unused.\n**Note:** Quota for is calculated based on your [deployed model's](https://cloud.google.com/vertex-ai/docs/general/deployment#scaling) real-time usage of compute resources. If the sum of `maxReplicaCount` for all the deployments in your project is more than your project's quota, some deployments may fail to autoscale due to quota being exhausted.\n**Note:** Endpoints are scaled up and down per machine, but quota is calculated per CPU and/or GPU. For example, if your model is deployed to `a2-highgpu-2g` machine type, then each active replica will count as 24 CPUs and 2 GPUs against your project's quota. For more information, see [Quota and limits](/vertex-ai/docs/quotas#serving) .\n**Note:** The prediction nodes for batch prediction do not automatically scale. Vertex AI uses [BatchDedicatedResources.startingReplicaCount](/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchDedicatedResources) and ignores `BatchDedicatedResources.maxReplicaCount` .\n### Target utilization and configuration\nBy default, if you deploy a model without dedicated GPU resources, Vertex AI automatically scales the number of replicas up or down so that CPU usage matches the default 60% target value.\nBy default, if you deploy a model with dedicated GPU resources (if [machineSpec.accelerator_count](/vertex-ai/docs/reference/rest/v1/MachineSpec#FIELDS.accelerator_count) is above 0), Vertex AI will automatically scale the number of replicas up or down so that the CPU or GPU usage, whichever is higher, matches the default 60% target value. Therefore, if your prediction throughput is causing high GPU usage, but not high CPU usage, Vertex AI will scale up, and the CPU utilization will be very low, which will be visible in monitoring. Conversely, if your custom container is underutilizing the GPU, but has an unrelated process that bring CPU utilization above 60%, Vertex AI will scale up, even if this may not have been needed to achieve QPS and latency targets.\nYou can override the default threshold metric and target by specifying [autoscalingMetricSpecs](/vertex-ai/docs/reference/rest/v1/DedicatedResources#autoscalingmetricspec) . Note that if your deployment is configured to scale based only on CPU usage, it will not scale up even if GPU usage is high.\n### Manage resource usage\nYou can [monitor yourendpoint](/vertex-ai/docs/general/monitoring-metrics#endpoint_monitoring_metrics) to track metrics like CPU and Accelerator usage, number of requests, latency, and the current and target number of replicas. This information can help you understand your endpoint's resource usage and scaling behavior.\nKeep in mind that each replica runs only a single container. This means that if a prediction container cannot fully utilize the selected compute resource, such as single threaded code for a multi-core machine, or a custom model that calls another service as part of making the prediction, your nodes may not scale up.\nFor example, if you are using [FastAPI](https://fastapi.tiangolo.com/) , or any model server that has a configurable number of workers or threads, there are many cases where having more than one worker can increase resource utilization, which improves the ability for the service to automatically scale the number of replicas.\nWe generally recommend starting with one worker or thread per core. If you notice that CPU utilization is low, especially under high load, or your model is not scaling up because CPU utilization is low, then increase the number of workers. On the other hand, if you notice that utilization is too high and your latencies increase more than expected under load, then try using fewer workers. If you are already using only a single worker, try using a smaller machine type.\n### Scaling behavior and lag\nVertex AI adjusts the number of replicas every 15 seconds using data from the previous 5 minutes window. For each 15 second cycle, the system measures the server utilization and generates a target number of replicas based on the following formula:\n`target # of replicas = Ceil(current # of replicas * (current utilization / target utilization))`\nFor example, if you currently have two replicas that are being utilized at 100%, the target is 4:\n`4 = Ceil(3.33) = Ceil(2 * (100% / 60%))`\nAnother example, if you currently have 10 replicas and utilization drops to 1%, the target is 1:\n`1 = Ceil(.167) = Ceil(10 * (1% / 60%))`\nAt the end of each 15 second cycle, the system adjusts the number of replicas to match the highest target value from the previous 5 minutes window. Notice that because the highest target value is chosen, your endpoint will not scale down if there is a spike in utilization during that 5 minute window, even if overall utilization is very low. On the other hand, if the system needs to be scaled up, it will do that within 15 seconds since the highest target value is chosen instead of the average.\nKeep in mind that even after Vertex AI adjusts the number of replicas, it takes time to start up or turn down the replicas. Thus there is an additional delay before the endpoint can adjust to the traffic. The main factors that contribute to this time include the following:\n- the time to provision and start the Compute Engine VMs\n- the time to download the container from the registry\n- the time to load the model from storage\nThe best way to understand the real world scaling behavior of your model is to run a load test and optimize the characteristics that matter for your model and your use case. If the autoscaler is not scaling up fast enough for your application, provision enough `min_replicas` to handle your expected baseline traffic.\n## Update the scaling configuration\nIf you specified either [DedicatedResources](/vertex-ai/docs/reference/rest/v1/DedicatedResources) or [AutomaticResources](/vertex-ai/docs/reference/rest/v1/AutomaticResources) when you deployed the model, you can update the scaling configuration without redeploying the model by calling [mutateDeployedModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/mutateDeployedModel) .\nFor example, the following request updates `max_replica` , `autoscaling_metric_specs` , and disables container logging.\n```\n{\n \"deployedModel\": {\n \"id\": \"2464520679043629056\",\n \"dedicatedResources\": {\n  \"maxReplicaCount\": 9,\n  \"autoscalingMetricSpecs\": [  {\n   \"metricName\": \"aiplatform.googleapis.com/prediction/online/cpu/utilization\",\n   \"target\": 50\n  }\n  ]\n },\n \"disableContainerLogging\": true\n },\n \"update_mask\": {\n \"paths\": [  \"dedicated_resources.max_replica_count\",\n  \"dedicated_resources.autoscaling_metric_specs\",\n  \"disable_container_logging\"\n ]\n }\n}\n```\nUsage notes:\n- You cannot change the machine type or switch from`DedicatedResources`to`AutomaticResources`or the other way around. The only scaling configuration fields you can change are:`min_replica`,`max_replica`, and`AutoscalingMetricSpec`(`DedicatedResources`only).\n- You must list every field you wish to update in`updateMask`. Unlisted fields are ignored.\n- The [DeployedModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints#DeployedModel) must be in a`DEPLOYED`state. There can be at most one active mutate operation per deployed model.\n- [mutateDeployedModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/mutateDeployedModel) also allows you to enable or disable container logging. For more information, see [Online predictionlogging](/vertex-ai/docs/predictions/online-prediction-logging) .## What's next\n- Learn how to [get an online prediction](/vertex-ai/docs/predictions/get-online-predictions) .\n- Learn about [private endpoints](/vertex-ai/docs/predictions/using-private-endpoints) .", "guide": "Vertex AI"}