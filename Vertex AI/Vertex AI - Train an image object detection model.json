{"title": "Vertex AI - Train an image object detection model", "url": "https://cloud.google.com/vertex-ai/docs/image-data/object-detection/train-model", "abstract": "# Vertex AI - Train an image object detection model\nThis page shows you how to train an AutoML object detection model from an image dataset using either the Google Cloud console or the Vertex AI API.\n", "content": "## Train an AutoML model\n- In the Google Cloud console, in the Vertex AI section, go to the **Datasets** page. [Go to the Datasets page](https://console.cloud.google.com/vertex-ai/datasets) \n- Click the name of the dataset you want to use to train your model to open its details page.\n- Click **Train new model** . **Note:** You can type [model.new](https://model.new) into a browser to go directly to the model creation page.\n- For the training method, select radio_button_checked **AutoML** .\n- In the **Choose where to use the model** section, choose the model host location: radio_button_checked **Cloud** , radio_button_checked **Edge** , or radio_button_checked **Vertex AI Vision** .\n- Click **Continue** .\n- Enter a name for the model.\n- If you want manually set how your training data is split, expand **Advanced options** and select a data split option. [Learn more](/vertex-ai/docs/general/ml-use) .\n- Click **Start Training** .Model training can take many hours, depending on the size and complexity of your data and your training budget, if you specified one. You can close this tab and return to it later. You will receive an email when your model has completed training.Select the tab below for your language or environment:Before using any of the request data, make the following replacements:- : Region where dataset is located and Model is created. For example,`us-central1`.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : Required. A display name for the trainingPipeline.\n- : The ID number for the dataset to use for training.\n- `fractionSplit`: Optional. One of several possible ML use [split  options](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines#inputdataconfig) for your data. For`fractionSplit`, values must sum to 1. For example:\n- `{\"trainingFraction\": \"0.7\",\"validationFraction\": \"0.15\",\"testFraction\": \"0.15\"}`- : A display name for the model uploaded (created)  by the TrainingPipeline.\n- : A description for the model.\n- : Any set of key-value pairs to organize your  models. For example:- \"env\": \"prod\"\n- \"tier\": \"backend\"\n- : The type of Cloud-hosted model to train. The options  are:- `CLOUD_1`- A model best tailored to be used within Google Cloud,   and whichbe exported. Compared to the CLOUD_HIGH_ACCURACY_1 and   CLOUD_LOW_LATENCY_1 models above, it is expected to have higher prediction quality and lower   latency.\n- `CLOUD_HIGH_ACCURACY_1`- A model best tailored to be used within Google Cloud,   and whichbe exported. This model is expected to have a higher latency, but   should also have a higher prediction quality than other cloud models.\n- `CLOUD_LOW_LATENCY_1`- A model best tailored to be used within Google Cloud,   and which cannot be exported. This model is expected to have a low latency, but may have   lower prediction quality than other cloud models.\nOther model type options can be found in the [reference documentation](https://cloud.google.com/vertex-ai/docs/reference/rpc/google.cloud.aiplatform.v1/schema/trainingjob.definition#modeltype_1) .\n- : The actual training cost will be equal or less than  this value. For Cloud models the budget must be: 20,000 - 900,000 milli node hours (inclusive).  The default value is 216,000 which represents one day in wall time, assuming 9 nodes are used.\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) \n| 0 | 1                         |\n|:----|:--------------------------------------------------------------------------------------------------|\n| * | Schema file's description you specify in trainingTaskDefinition describes the use of this field. |\n| \u2020 | Schema file you specify in trainingTaskDefinition declares and describes this field.    |\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/trainingPipelines\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"TRAININGPIPELINE_DISPLAYNAME\",\n \"inputDataConfig\": {\n \"datasetId\": \"DATASET_ID\",\n \"fractionSplit\": {\n  \"trainingFraction\": \"DECIMAL\",\n  \"validationFraction\": \"DECIMAL\",\n  \"testFraction\": \"DECIMAL\"\n }\n },\n \"modelToUpload\": {\n \"displayName\": \"MODEL_DISPLAYNAME\",\n \"description\": \"MODEL_DESCRIPTION\",\n \"labels\": {\n  \"KEY\": \"VALUE\"\n }\n },\n \"trainingTaskDefinition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_image_object_detection_1.0.0.yaml\",\n \"trainingTaskInputs\": {\n \"modelType\": [\"MODELTYPE\"],\n \"budgetMilliNodeHours\": NODE_HOUR_BUDGET\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/trainingPipelines\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/trainingPipelines\" | Select-Object -Expand Content\n```\nThe response contains information about specifications as well as the .Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/GoogleCloudPlatform/java-docs-samples&page=editor&cloudshell_workspace=aiplatform/src/main/java/aiplatform&cloudshell_open_in_editor=CreateTrainingPipelineImageObjectDetectionSample.java) [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateTrainingPipelineImageObjectDetectionSample.java) \n```\nimport com.google.cloud.aiplatform.util.ValueConverter;import com.google.cloud.aiplatform.v1.DeployedModelRef;import com.google.cloud.aiplatform.v1.EnvVar;import com.google.cloud.aiplatform.v1.FilterSplit;import com.google.cloud.aiplatform.v1.FractionSplit;import com.google.cloud.aiplatform.v1.InputDataConfig;import com.google.cloud.aiplatform.v1.LocationName;import com.google.cloud.aiplatform.v1.Model;import com.google.cloud.aiplatform.v1.Model.ExportFormat;import com.google.cloud.aiplatform.v1.ModelContainerSpec;import com.google.cloud.aiplatform.v1.PipelineServiceClient;import com.google.cloud.aiplatform.v1.PipelineServiceSettings;import com.google.cloud.aiplatform.v1.Port;import com.google.cloud.aiplatform.v1.PredefinedSplit;import com.google.cloud.aiplatform.v1.PredictSchemata;import com.google.cloud.aiplatform.v1.TimestampSplit;import com.google.cloud.aiplatform.v1.TrainingPipeline;import com.google.cloud.aiplatform.v1beta1.schema.trainingjob.definition.AutoMlImageObjectDetectionInputs;import com.google.cloud.aiplatform.v1beta1.schema.trainingjob.definition.AutoMlImageObjectDetectionInputs.ModelType;import com.google.rpc.Status;import java.io.IOException;public class CreateTrainingPipelineImageObjectDetectionSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String trainingPipelineDisplayName = \"YOUR_TRAINING_PIPELINE_DISPLAY_NAME\";\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String datasetId = \"YOUR_DATASET_ID\";\u00a0 \u00a0 String modelDisplayName = \"YOUR_MODEL_DISPLAY_NAME\";\u00a0 \u00a0 createTrainingPipelineImageObjectDetectionSample(\u00a0 \u00a0 \u00a0 \u00a0 project, trainingPipelineDisplayName, datasetId, modelDisplayName);\u00a0 }\u00a0 static void createTrainingPipelineImageObjectDetectionSample(\u00a0 \u00a0 \u00a0 String project, String trainingPipelineDisplayName, String datasetId, String modelDisplayName)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 PipelineServiceSettings pipelineServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PipelineServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (PipelineServiceClient pipelineServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 PipelineServiceClient.create(pipelineServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 String trainingTaskDefinition =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gs://google-cloud-aiplatform/schema/trainingjob/definition/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"automl_image_object_detection_1.0.0.yaml\";\u00a0 \u00a0 \u00a0 LocationName locationName = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 AutoMlImageObjectDetectionInputs autoMlImageObjectDetectionInputs =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutoMlImageObjectDetectionInputs.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModelType(ModelType.CLOUD_HIGH_ACCURACY_1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setBudgetMilliNodeHours(20000)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisableEarlyStopping(false)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 InputDataConfig trainingInputDataConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputDataConfig.newBuilder().setDatasetId(datasetId).build();\u00a0 \u00a0 \u00a0 Model model = Model.newBuilder().setDisplayName(modelDisplayName).build();\u00a0 \u00a0 \u00a0 TrainingPipeline trainingPipeline =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TrainingPipeline.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(trainingPipelineDisplayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingTaskDefinition(trainingTaskDefinition)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingTaskInputs(ValueConverter.toValue(autoMlImageObjectDetectionInputs))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputDataConfig(trainingInputDataConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModelToUpload(model)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 TrainingPipeline trainingPipelineResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pipelineServiceClient.createTrainingPipeline(locationName, trainingPipeline);\u00a0 \u00a0 \u00a0 System.out.println(\"Create Training Pipeline Image Object Detection Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"Name: %s\\n\", trainingPipelineResponse.getName());\u00a0 \u00a0 \u00a0 System.out.format(\"Display Name: %s\\n\", trainingPipelineResponse.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Training Task Definition %s\\n\", trainingPipelineResponse.getTrainingTaskDefinition());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Training Task Inputs: %s\\n\", trainingPipelineResponse.getTrainingTaskInputs());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Training Task Metadata: %s\\n\", trainingPipelineResponse.getTrainingTaskMetadata());\u00a0 \u00a0 \u00a0 System.out.format(\"State: %s\\n\", trainingPipelineResponse.getState());\u00a0 \u00a0 \u00a0 System.out.format(\"Create Time: %s\\n\", trainingPipelineResponse.getCreateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"StartTime %s\\n\", trainingPipelineResponse.getStartTime());\u00a0 \u00a0 \u00a0 System.out.format(\"End Time: %s\\n\", trainingPipelineResponse.getEndTime());\u00a0 \u00a0 \u00a0 System.out.format(\"Update Time: %s\\n\", trainingPipelineResponse.getUpdateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"Labels: %s\\n\", trainingPipelineResponse.getLabelsMap());\u00a0 \u00a0 \u00a0 InputDataConfig inputDataConfig = trainingPipelineResponse.getInputDataConfig();\u00a0 \u00a0 \u00a0 System.out.println(\"Input Data Config\");\u00a0 \u00a0 \u00a0 System.out.format(\"Dataset Id: %s\", inputDataConfig.getDatasetId());\u00a0 \u00a0 \u00a0 System.out.format(\"Annotations Filter: %s\\n\", inputDataConfig.getAnnotationsFilter());\u00a0 \u00a0 \u00a0 FractionSplit fractionSplit = inputDataConfig.getFractionSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"Fraction Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"Training Fraction: %s\\n\", fractionSplit.getTrainingFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"Validation Fraction: %s\\n\", fractionSplit.getValidationFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"Test Fraction: %s\\n\", fractionSplit.getTestFraction());\u00a0 \u00a0 \u00a0 FilterSplit filterSplit = inputDataConfig.getFilterSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"Filter Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"Training Filter: %s\\n\", filterSplit.getTrainingFilter());\u00a0 \u00a0 \u00a0 System.out.format(\"Validation Filter: %s\\n\", filterSplit.getValidationFilter());\u00a0 \u00a0 \u00a0 System.out.format(\"Test Filter: %s\\n\", filterSplit.getTestFilter());\u00a0 \u00a0 \u00a0 PredefinedSplit predefinedSplit = inputDataConfig.getPredefinedSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"Predefined Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"Key: %s\\n\", predefinedSplit.getKey());\u00a0 \u00a0 \u00a0 TimestampSplit timestampSplit = inputDataConfig.getTimestampSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"Timestamp Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"Training Fraction: %s\\n\", timestampSplit.getTrainingFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"Validation Fraction: %s\\n\", timestampSplit.getValidationFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"Test Fraction: %s\\n\", timestampSplit.getTestFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"Key: %s\\n\", timestampSplit.getKey());\u00a0 \u00a0 \u00a0 Model modelResponse = trainingPipelineResponse.getModelToUpload();\u00a0 \u00a0 \u00a0 System.out.println(\"Model To Upload\");\u00a0 \u00a0 \u00a0 System.out.format(\"Name: %s\\n\", modelResponse.getName());\u00a0 \u00a0 \u00a0 System.out.format(\"Display Name: %s\\n\", modelResponse.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\"Description: %s\\n\", modelResponse.getDescription());\u00a0 \u00a0 \u00a0 System.out.format(\"Metadata Schema Uri: %s\\n\", modelResponse.getMetadataSchemaUri());\u00a0 \u00a0 \u00a0 System.out.format(\"Metadata: %s\\n\", modelResponse.getMetadata());\u00a0 \u00a0 \u00a0 System.out.format(\"Training Pipeline: %s\\n\", modelResponse.getTrainingPipeline());\u00a0 \u00a0 \u00a0 System.out.format(\"Artifact Uri: %s\\n\", modelResponse.getArtifactUri());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Supported Deployment Resources Types: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedDeploymentResourcesTypesList());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Supported Input Storage Formats: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedInputStorageFormatsList());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Supported Output Storage Formats: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedOutputStorageFormatsList());\u00a0 \u00a0 \u00a0 System.out.format(\"Create Time: %s\\n\", modelResponse.getCreateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"Update Time: %s\\n\", modelResponse.getUpdateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"Labels: %sn\\n\", modelResponse.getLabelsMap());\u00a0 \u00a0 \u00a0 PredictSchemata predictSchemata = modelResponse.getPredictSchemata();\u00a0 \u00a0 \u00a0 System.out.println(\"Predict Schemata\");\u00a0 \u00a0 \u00a0 System.out.format(\"Instance Schema Uri: %s\\n\", predictSchemata.getInstanceSchemaUri());\u00a0 \u00a0 \u00a0 System.out.format(\"Parameters Schema Uri: %s\\n\", predictSchemata.getParametersSchemaUri());\u00a0 \u00a0 \u00a0 System.out.format(\"Prediction Schema Uri: %s\\n\", predictSchemata.getPredictionSchemaUri());\u00a0 \u00a0 \u00a0 for (ExportFormat exportFormat : modelResponse.getSupportedExportFormatsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"Supported Export Format\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"Id: %s\\n\", exportFormat.getId());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ModelContainerSpec modelContainerSpec = modelResponse.getContainerSpec();\u00a0 \u00a0 \u00a0 System.out.println(\"Container Spec\");\u00a0 \u00a0 \u00a0 System.out.format(\"Image Uri: %s\\n\", modelContainerSpec.getImageUri());\u00a0 \u00a0 \u00a0 System.out.format(\"Command: %s\\n\", modelContainerSpec.getCommandList());\u00a0 \u00a0 \u00a0 System.out.format(\"Args: %s\\n\", modelContainerSpec.getArgsList());\u00a0 \u00a0 \u00a0 System.out.format(\"Predict Route: %s\\n\", modelContainerSpec.getPredictRoute());\u00a0 \u00a0 \u00a0 System.out.format(\"Health Route: %s\\n\", modelContainerSpec.getHealthRoute());\u00a0 \u00a0 \u00a0 for (EnvVar envVar : modelContainerSpec.getEnvList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"Env\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"Name: %s\\n\", envVar.getName());\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"Value: %s\\n\", envVar.getValue());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 for (Port port : modelContainerSpec.getPortsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"Port\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"Container Port: %s\\n\", port.getContainerPort());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 for (DeployedModelRef deployedModelRef : modelResponse.getDeployedModelsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"Deployed Model\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"Endpoint: %s\\n\", deployedModelRef.getEndpoint());\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"Deployed Model Id: %s\\n\", deployedModelRef.getDeployedModelId());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 Status status = trainingPipelineResponse.getError();\u00a0 \u00a0 \u00a0 System.out.println(\"Error\");\u00a0 \u00a0 \u00a0 System.out.format(\"Code: %s\\n\", status.getCode());\u00a0 \u00a0 \u00a0 System.out.format(\"Message: %s\\n\", status.getMessage());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/create-training-pipeline-image-object-detection.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const datasetId = 'YOUR_DATASET_ID';// const modelDisplayName = 'YOUR_MODEL_DISPLAY_NAME';// const trainingPipelineDisplayName = 'YOUR_TRAINING_PIPELINE_DISPLAY_NAME';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';const aiplatform = require('@google-cloud/aiplatform');const {definition} =\u00a0 aiplatform.protos.google.cloud.aiplatform.v1.schema.trainingjob;const ModelType = definition.AutoMlImageObjectDetectionInputs.ModelType;// Imports the Google Cloud Pipeline Service Client libraryconst {PipelineServiceClient} = aiplatform.v1;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst pipelineServiceClient = new PipelineServiceClient(clientOptions);async function createTrainingPipelineImageObjectDetection() {\u00a0 // Configure the parent resource\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 const trainingTaskInputsObj =\u00a0 \u00a0 new definition.AutoMlImageObjectDetectionInputs({\u00a0 \u00a0 \u00a0 disableEarlyStopping: false,\u00a0 \u00a0 \u00a0 modelType: ModelType.CLOUD_1,\u00a0 \u00a0 \u00a0 budgetMilliNodeHours: 20000,\u00a0 \u00a0 });\u00a0 const trainingTaskInputs = trainingTaskInputsObj.toValue();\u00a0 const modelToUpload = {displayName: modelDisplayName};\u00a0 const inputDataConfig = {datasetId: datasetId};\u00a0 const trainingPipeline = {\u00a0 \u00a0 displayName: trainingPipelineDisplayName,\u00a0 \u00a0 trainingTaskDefinition:\u00a0 \u00a0 \u00a0 'gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_image_object_detection_1.0.0.yaml',\u00a0 \u00a0 trainingTaskInputs,\u00a0 \u00a0 inputDataConfig,\u00a0 \u00a0 modelToUpload,\u00a0 };\u00a0 const request = {\u00a0 \u00a0 parent,\u00a0 \u00a0 trainingPipeline,\u00a0 };\u00a0 // Create training pipeline request\u00a0 const [response] =\u00a0 \u00a0 await pipelineServiceClient.createTrainingPipeline(request);\u00a0 console.log('Create training pipeline image object detection response');\u00a0 console.log(`Name : ${response.name}`);\u00a0 console.log('Raw response:');\u00a0 console.log(JSON.stringify(response, null, 2));}createTrainingPipelineImageObjectDetection();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/googleapis/python-aiplatform&page=editor&cloudshell_workspace=samples/snippets/pipeline_service&cloudshell_open_in_editor=create_training_pipeline_image_object_detection_sample.py) [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/snippets/pipeline_service/create_training_pipeline_image_object_detection_sample.py) \n```\nfrom google.cloud import aiplatformfrom google.cloud.aiplatform.gapic.schema import trainingjobdef create_training_pipeline_image_object_detection_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 dataset_id: str,\u00a0 \u00a0 model_display_name: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",):\u00a0 \u00a0 # The AI Platform services require regional API endpoints.\u00a0 \u00a0 client_options = {\"api_endpoint\": api_endpoint}\u00a0 \u00a0 # Initialize client that will be used to create and send requests.\u00a0 \u00a0 # This client only needs to be created once, and can be reused for multiple requests.\u00a0 \u00a0 client = aiplatform.gapic.PipelineServiceClient(client_options=client_options)\u00a0 \u00a0 training_task_inputs = trainingjob.definition.AutoMlImageObjectDetectionInputs(\u00a0 \u00a0 \u00a0 \u00a0 model_type=\"CLOUD_HIGH_ACCURACY_1\",\u00a0 \u00a0 \u00a0 \u00a0 budget_milli_node_hours=20000,\u00a0 \u00a0 \u00a0 \u00a0 disable_early_stopping=False,\u00a0 \u00a0 ).to_value()\u00a0 \u00a0 training_pipeline = {\u00a0 \u00a0 \u00a0 \u00a0 \"display_name\": display_name,\u00a0 \u00a0 \u00a0 \u00a0 \"training_task_definition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_image_object_detection_1.0.0.yaml\",\u00a0 \u00a0 \u00a0 \u00a0 \"training_task_inputs\": training_task_inputs,\u00a0 \u00a0 \u00a0 \u00a0 \"input_data_config\": {\"dataset_id\": dataset_id},\u00a0 \u00a0 \u00a0 \u00a0 \"model_to_upload\": {\"display_name\": model_display_name},\u00a0 \u00a0 }\u00a0 \u00a0 parent = f\"projects/{project}/locations/{location}\"\u00a0 \u00a0 response = client.create_training_pipeline(\u00a0 \u00a0 \u00a0 \u00a0 parent=parent, training_pipeline=training_pipeline\u00a0 \u00a0 )\u00a0 \u00a0 print(\"response:\", response)\n```\n## Control the data split using RESTYou can control how your training data is split between the training, validation, and test sets. When using the Vertex AI API, use the [Split object](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines#InputDataConfig) to determine your data split. The `Split` object can be included in the `InputConfig` object as one of several object types, each of which provides a different way to split the training data. You can select one method only.- `FractionSplit`:- : The fraction of the training data to   be used for the training set.\n- : The fraction of the training data   to be used for the validation set. Not used for video data.\n- : The fraction of the training data to be   used for the test set.\nIf any of the fractions are specified, all must be specified. The  fractions must add up to 1.0. The [default values for the fractions](/vertex-ai/docs/general/ml-use#default) differ depending on your data type. [Learn more](/vertex-ai/docs/general/ml-use#percentages) .```\n\"fractionSplit\": {\n \"trainingFraction\": TRAINING_FRACTION,\n \"validationFraction\": VALIDATION_FRACTION,\n \"testFraction\": TEST_FRACTION\n},\n```\n- `FilterSplit`:\n- : Data items that match this filter are used for the training set.\n- : Data items that match this filter are used for the validation set. Must be \"-\" for video data.\n- : Data items that match this filter are used for the test set.\n- These filters can be used with the `ml_use` label, or with any labels you apply to your data. Learn more about using [the ml-use label](/vertex-ai/docs/general/ml-use#ml-use) and [other labels](/vertex-ai/docs/general/ml-use#filter) to filter your data.\n- The following example shows how to use the `filterSplit` object with the `ml_use` label, with the validation set included:\n- ```\n\"filterSplit\": {\n\"trainingFilter\": \"labels.aiplatform.googleapis.com/ml_use=training\",\n\"validationFilter\": \"labels.aiplatform.googleapis.com/ml_use=validation\",\n\"testFilter\": \"labels.aiplatform.googleapis.com/ml_use=test\"\n}\n```", "guide": "Vertex AI"}