{"title": "Vertex AI - Train a video action recognition model", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Train a video action recognition model\nThis page shows you how to train an AutoML action recognition model from a video dataset using either the Google Cloud console or the Vertex AI API.\n", "content": "## Train an AutoML model\n- In the Google Cloud console, in the Vertex AI section, go to the **Datasets** page. [Go to the Datasets page](https://console.cloud.google.com/vertex-ai/datasets) \n- Click the name of the dataset you want to use to train your model to open its details page.\n- Click **Train new model** . **Note:** You can type [model.new](https://model.new) into a browser to go directly to the model creation page.\n- Enter the display name for your new model.\n- If you want manually set how your training data is split, expand **Advanced options** and select a data split option. [Learn more](/vertex-ai/docs/general/ml-use) .\n- Click **Continue** .\n- Select the model training method.- `AutoML`is a good choice for a wide range of use cases.\n- `Seq2seq+`is a good choice for experimentation. The algorithm is likely to converge faster than`AutoML`because its architecture is simpler and it uses a smaller  search space. Our experiments find that Seq2Seq+ performs well with a small time budget and  on datasets smaller than 1\u00a0GB in size.\nClick **Continue** .\n- Click **Start Training** .Model training can take many hours, depending on the size and complexity of your data and your training budget, if you specified one. You can close this tab and return to it later. You will receive an email when your model has completed training.Several minutes after training starts, you can check the training node hour estimation from the model's properties information. If you cancel the training, there is no charge on the current product.\nBefore using any of the request data, make the following replacements:- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : Region where dataset is located and Model is created. For example,`us-central1`.\n- : Required. A display name for the TrainingPipeline.\n- : ID for the training Dataset.\n- ,:  The`fractionSplit`object is optional; you use it to control your data split. For more  information about controlling data split, see  see [About data splits for AutoML models](https://cloud.google.com/vertex-ai/docs/general/ml-use) . For example:- `{\"trainingFraction\": \"0.8\",\"validationFraction\": \"0\",\"testFraction\": \"0.2\"}`- : Display name of the trained Model.\n- : A description for the Model.\n- : Any set of key-value pairs to organize your  models. For example:- \"env\": \"prod\"\n- \"tier\": \"backend\"\n- :- `MOBILE_VERSATILE_1`: general purpose usage\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) \nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/beta1/projects/PROJECT/locations/LOCATION/trainingPipelines\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"TRAINING_PIPELINE_DISPLAY_NAME\",\n \"inputDataConfig\": {\n \"datasetId\": \"DATASET_ID\",\n \"fractionSplit\": {\n  \"trainingFraction\": \"TRAINING_FRACTION\",\n  \"validationFraction\": \"0\",\n  \"testFraction\": \"TEST_FRACTION\"\n }\n },\n \"modelToUpload\": {\n \"displayName\": \"MODEL_DISPLAY_NAME\",\n \"description\": \"MODEL_DESCRIPTION\",\n \"labels\": {\n  \"KEY\": \"VALUE\"\n }\n },\n \"trainingTaskDefinition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_video_object_tracking_1.0.0.yaml\",\n \"trainingTaskInputs\": {\n \"modelType\": [\"EDGE_MODEL_TYPE\"],\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/beta1/projects/PROJECT/locations/LOCATION/trainingPipelines\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/beta1/projects/PROJECT/locations/LOCATION/trainingPipelines\" | Select-Object -Expand Content\n```\nThe response contains information about specifications as well as the .Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateTrainingPipelineVideoActionRecognitionSample.java) \n```\nimport com.google.cloud.aiplatform.util.ValueConverter;import com.google.cloud.aiplatform.v1.InputDataConfig;import com.google.cloud.aiplatform.v1.LocationName;import com.google.cloud.aiplatform.v1.Model;import com.google.cloud.aiplatform.v1.PipelineServiceClient;import com.google.cloud.aiplatform.v1.PipelineServiceSettings;import com.google.cloud.aiplatform.v1.TrainingPipeline;import com.google.cloud.aiplatform.v1.schema.trainingjob.definition.AutoMlVideoActionRecognitionInputs;import com.google.cloud.aiplatform.v1.schema.trainingjob.definition.AutoMlVideoActionRecognitionInputs.ModelType;import java.io.IOException;public class CreateTrainingPipelineVideoActionRecognitionSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"PROJECT\";\u00a0 \u00a0 String displayName = \"DISPLAY_NAME\";\u00a0 \u00a0 String datasetId = \"DATASET_ID\";\u00a0 \u00a0 String modelDisplayName = \"MODEL_DISPLAY_NAME\";\u00a0 \u00a0 createTrainingPipelineVideoActionRecognitionSample(\u00a0 \u00a0 \u00a0 \u00a0 project, displayName, datasetId, modelDisplayName);\u00a0 }\u00a0 static void createTrainingPipelineVideoActionRecognitionSample(\u00a0 \u00a0 \u00a0 String project, String displayName, String datasetId, String modelDisplayName)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 PipelineServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 PipelineServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (PipelineServiceClient client = PipelineServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 AutoMlVideoActionRecognitionInputs trainingTaskInputs =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutoMlVideoActionRecognitionInputs.newBuilder().setModelType(ModelType.CLOUD).build();\u00a0 \u00a0 \u00a0 InputDataConfig inputDataConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputDataConfig.newBuilder().setDatasetId(datasetId).build();\u00a0 \u00a0 \u00a0 Model modelToUpload = Model.newBuilder().setDisplayName(modelDisplayName).build();\u00a0 \u00a0 \u00a0 TrainingPipeline trainingPipeline =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TrainingPipeline.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(displayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingTaskDefinition(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gs://google-cloud-aiplatform/schema/trainingjob/definition/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"automl_video_action_recognition_1.0.0.yaml\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingTaskInputs(ValueConverter.toValue(trainingTaskInputs))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputDataConfig(inputDataConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModelToUpload(modelToUpload)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 LocationName parent = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 TrainingPipeline response = client.createTrainingPipeline(parent, trainingPipeline);\u00a0 \u00a0 \u00a0 System.out.format(\"response: %s\\n\", response);\u00a0 \u00a0 \u00a0 System.out.format(\"Name: %s\\n\", response.getName());\u00a0 \u00a0 }\u00a0 }}\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/snippets/pipeline_service/create_training_pipeline_video_action_recognition_sample.py) \n```\nfrom google.cloud import aiplatformfrom google.cloud.aiplatform.gapic.schema import trainingjobdef create_training_pipeline_video_action_recognition_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 dataset_id: str,\u00a0 \u00a0 model_display_name: str,\u00a0 \u00a0 model_type: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",):\u00a0 \u00a0 # The AI Platform services require regional API endpoints.\u00a0 \u00a0 client_options = {\"api_endpoint\": api_endpoint}\u00a0 \u00a0 # Initialize client that will be used to create and send requests.\u00a0 \u00a0 # This client only needs to be created once, and can be reused for multiple requests.\u00a0 \u00a0 client = aiplatform.gapic.PipelineServiceClient(client_options=client_options)\u00a0 \u00a0 training_task_inputs = trainingjob.definition.AutoMlVideoActionRecognitionInputs(\u00a0 \u00a0 \u00a0 \u00a0 # modelType can be either 'CLOUD' or 'MOBILE_VERSATILE_1'\u00a0 \u00a0 \u00a0 \u00a0 model_type=model_type,\u00a0 \u00a0 ).to_value()\u00a0 \u00a0 training_pipeline = {\u00a0 \u00a0 \u00a0 \u00a0 \"display_name\": display_name,\u00a0 \u00a0 \u00a0 \u00a0 \"training_task_definition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_video_action_recognition_1.0.0.yaml\",\u00a0 \u00a0 \u00a0 \u00a0 \"training_task_inputs\": training_task_inputs,\u00a0 \u00a0 \u00a0 \u00a0 \"input_data_config\": {\"dataset_id\": dataset_id},\u00a0 \u00a0 \u00a0 \u00a0 \"model_to_upload\": {\"display_name\": model_display_name},\u00a0 \u00a0 }\u00a0 \u00a0 parent = f\"projects/{project}/locations/{location}\"\u00a0 \u00a0 response = client.create_training_pipeline(\u00a0 \u00a0 \u00a0 \u00a0 parent=parent, training_pipeline=training_pipeline\u00a0 \u00a0 )\u00a0 \u00a0 print(\"response:\", response)\n```\n## Control the data split using RESTYou can control how your training data is split between the training, validation, and test sets. When using the Vertex AI API, use the [Split object](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines#InputDataConfig) to determine your data split. The `Split` object can be included in the `InputConfig` object as one of several object types, each of which provides a different way to split the training data. You can select one method only.- `FractionSplit`:- : The fraction of the training data to   be used for the training set.\n- : The fraction of the training data   to be used for the validation set. Not used for video data.\n- : The fraction of the training data to be   used for the test set.\nIf any of the fractions are specified, all must be specified. The  fractions must add up to 1.0. The [default values for the fractions](/vertex-ai/docs/general/ml-use#default) differ depending on your data type. [Learn more](/vertex-ai/docs/general/ml-use#percentages) .```\n\"fractionSplit\": {\n \"trainingFraction\": TRAINING_FRACTION,\n \"validationFraction\": VALIDATION_FRACTION,\n \"testFraction\": TEST_FRACTION\n},\n```\n- `FilterSplit`:\n- : Data items that match this filter are used for the training set.\n- : Data items that match this filter are used for the validation set. Must be \"-\" for video data.\n- : Data items that match this filter are used for the test set.\n- These filters can be used with the `ml_use` label, or with any labels you apply to your data. Learn more about using [the ml-use label](/vertex-ai/docs/general/ml-use#ml-use) and [other labels](/vertex-ai/docs/general/ml-use#filter) to filter your data.\n- The following example shows how to use the `filterSplit` object with the `ml_use` label, with the validation set included:\n- ```\n\"filterSplit\": {\n\"trainingFilter\": \"labels.aiplatform.googleapis.com/ml_use=training\",\n\"validationFilter\": \"labels.aiplatform.googleapis.com/ml_use=validation\",\n\"testFilter\": \"labels.aiplatform.googleapis.com/ml_use=test\"\n}\n```", "guide": "Vertex AI"}