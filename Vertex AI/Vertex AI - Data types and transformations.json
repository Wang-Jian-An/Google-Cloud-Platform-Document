{"title": "Vertex AI - Data types and transformations", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Data types and transformations\nThis page describes how Vertex AI works with different types of tabular data for AutoML models.\n", "content": "## Introduction to transformations in Vertex AI\n### Model training\nInput tabular data must be transformed before it can be used for model training. The transformation indicates the function of a particular data feature.\nThe following transformations are supported:\n- [Categorical](#categorical-transf) \n- [Text](#text-transf) \n- [Numeric](#numeric-transf) \n- [Timestamp](#timestamp) \nIf your data source is a CSV file in Cloud Storage, commas (\",\") must be used for the delimiter. Vertex AI uses the [RFC 4180](https://tools.ietf.org/html/rfc4180) CSV format.\nIf your data source is BigQuery and you are performing classification or regression, you can include data that is made up of multiple data primitives. These compound data types must be pre-processed before transformations can be applied. The following compound data types are supported:\n- [Struct](#struct) \n- [Array](#array) \nIf a type value is missing or null, Vertex AI handles it based on your model objective and the transformation applied for that feature. For details, see [How missing or null values are handled](#null-values) .\n### Prediction\nThe format of the data used for prediction must match the format used for training. For details, see [Data format for predictions](#format-for-prediction) .\n## Vertex AI transformations\n### Categorical\nApplying a categorical transformation causes that feature to represent values in a category. That is, a nominal level. The values differ only based on their name without order. You can use numbers to represent categorical values, but the values have no numeric relationship with each other. That is, a categorical 1 is not \"greater\" than a categorical 0.\nHere are some examples of categorical values:\n- Boolean -`true`,`false`.\n- Country -`\"USA\"`,`\"Canada\"`,`\"China\"`, and so on.\n- HTTP status code -`\"200\"`,`\"404\"`,`\"500\"`, and so on.\nCategorical values are case-sensitive; spelling variations are treated as different categories (for example, \"Color\" and \"Colour\" are not combined).\nWhen you train a model with a feature with a categorical transformation, Vertex AI applies the following data transformations to the feature, and uses any that provide signal for training:\n- The categorical string as is--no change to case, punctuation,  spelling, tense, and so on.\n- Convert the category name to a dictionary lookup index and generate  an embedding for each index.\n- Categories that appear less than 5 times in the training dataset  are treated as the \"unknown\" category. The \"unknown\" category gets  its own special lookup index and resulting embedding.\nCategorical transformations can be applied to STRING data in CSV files or to the following BigQuery data types:\n- INT64\n- NUMERIC, BIGNUMERIC\n- FLOAT64\n- BOOL\n- STRING\n- DATE\n- DATETIME\n- TIME\n- TIMESTAMP\n### Text\nA text transformation causes the feature to be used as free-form text, typically comprised of text tokens.\nHere are some examples of text values:\n- `\"The quick brown fox\"`\n- `\"This restaurant is the best! The food is delicious\"`\nFor forecasting models, the text transformation is not supported for [covariate](#feature-type) features.\nWhen you train a model with a feature with a text transformation, Vertex AI applies the following data transformations to the feature, and uses any that provide signal for training:\n- The text as is--no change to case, punctuation, spelling, tense, and so on.\n- Tokenize text to words and generate 1-grams and 2-grams from  words. Convert each n-gram to a dictionary lookup index and generate  an embedding for each index. Combine the embedding of all elements  into a single embedding using the mean. [Tokenization](https://www.tensorflow.org/beta/tutorials/tensorflow_text/intro#unicodescripttokenizer) is based on unicode script boundaries.\n- Missing values get their own lookup index and resulting embedding.\n- Stop-words receive no special treatment and are not removed.\nText transformations can be applied to STRING data in CSV files or to the following BigQuery data types:\n- STRING\n- DATETIME\n### Numeric\nA numeric transformation causes a column to be used as an ordinal or quantitative number. These numbers can be compared. That is, two distinct numbers can be less than or greater than one another.\nLeading or trailing whitespace is trimmed.\nThe following table shows all compatible formats for a numeric transformation:\n| Format    | Examples      | Notes                             |\n|:--------------------|:-----------------------------|:----------------------------------------------------------------------------------------------------------------------|\n| Numeric string  | \"101\", 101.5\"    | The period character (\".\") is the only valid decimal delimiter. \"101,5\" and \"100,000\" are not valid numeric strings. |\n| Scientific notation | \"1.12345E+11\", \"1.12345e+11\" | See note for numeric strings regarding decimal delimiters.               |\n| Not a number  | \"NAN\", \"nan\", \"+NAN\"   | Case is ignored. Prepended plus (\"+\") or minus (\"-\") characters are ignored. Interpreted as NULL value.    |\n| Infinity   | \"INF\", \"+inf\"    | Case is ignored. Prepended plus (\"+\") or minus (\"-\") characters are ignored. Interpreted as NULL value.    |\nIf a value in a column with a numeric transformation does not conform to one of these formats, then either the entire row is excluded from training, or the value is treated as [null](#null-values) . You choose between these outcomes when you select the numeric transformation.\nWhen you train a model with a feature with a numeric transformation, Vertex AI applies the following data transformations to the feature, and uses any that provide signal for training:\n- The value converted to float32.\n- The z_score of the value.\n- A bucket index of the value based on quantiles. Bucket size is   100.\n- log(value+1) when the value is greater than or equal to 0.   Otherwise, this transformation is not applied and the value is   considered a missing value.\n- z_score of log(value+1) when the value is greater than or equal to   0. Otherwise, this transformation is not applied and the value is   considered a missing value.\n- A boolean value that indicates whether the value is null.\n- Rows with invalid numerical inputs (for example, a string that can   not be parsed to float32) are not included for training and   prediction.\n- Extreme/outlier values are not given any special treatment.\nNumeric transformations can be applied to STRING data in CSV files or to the following BigQuery data types:\n- INT64\n- NUMERIC, BIGNUMERIC\n- FLOAT64\n- STRING\n- TIMESTAMP\n### Timestamp\nA Timestamp transformation causes a feature to be used as a point in time, represented either as a civil time with a time zone, or a Unix timestamp. Only features with a Timestamp transformation can be used for the [Time column](/vertex-ai/docs/datasets/prepare-tabular#time) .\nIf a time zone is not specified with the civil time, it defaults to UTC.\nThe following table shows all compatible timestamp string formats:\n| Format        | Example     | Notes               |\n|:--------------------------------------|:--------------------------|:---------------------------------------------------------------|\n| %E4Y-%m-%d       | \"2017-01-30\"    | See the Abseil documentation for a description of this format. |\n| %E4Y/%m/%d       | \"2017/01/30\"    | See the Abseil documentation for a description of this format. |\n| %Y/%m/%d %H:%M:%E*S     | \"2017/01/30 23:59:58\"  | See the Abseil documentation for a description of this format. |\n| %d-%m-%E4Y       | \"30-11-2018\"    | See the Abseil documentation for a description of this format. |\n| %d/%m/%E4Y       | \"30/11/2018\"    | See the Abseil documentation for a description of this format. |\n| %d-%B-%E4Y       | \"30-November-2018\"  | See the Abseil documentation for a description of this format. |\n| %Y-%m-%dT%H:%M:%E*S%Ez    | \"2019-05-17T23:56:09.05Z\" | RFC 3339              |\n| Unix timestamp string in seconds  | \"1541194447\"    | Only for times between 01/Jan/1990 and 01/Jan/2030.   |\n| Unix timestamp string in milliseconds | \"1541194447000\"   | Only for times between 01/Jan/1990 and 01/Jan/2030.   |\n| Unix timestamp string in microseconds | \"1541194447000000\"  | Only for times between 01/Jan/1990 and 01/Jan/2030.   |\n| Unix timestamp string in nanoseconds | \"1541194447000000000\"  | Only for times between 01/Jan/1990 and 01/Jan/2030.   |\nIf a value in a column with a timestamp transformation does not conform to one of these formats, then either the entire row is excluded from training, or the value is treated as [null](#null-values) . You choose between these outcomes when you select the timestamp transformation.\nWhen you train a model with a feature with a timestamp transformation, Vertex AI applies the following data transformations to the feature, and uses any that provide signal for training:\n- Apply the transformations for **Numerical** columns.\n- Determine the year, month, day,and weekday. Treat each value from  the timestamp as a **Categorical** column.\n- Invalid numerical values (for example, values that fall outside of  a typical timestamp range, or are extreme values) receive no special  treatment and are not removed.\n- Rows with invalid timestamp inputs (for example, an invalid  timestamp string) are not included for training and prediction.\nTimestamp transformations can be applied to STRING data in CSV files or to the following BigQuery data types:\n- INT64\n- STRING\n- DATE\n- DATETIME\n- TIMESTAMP## Compound data types\nSometimes you need to include data that is made up of multiple data primitives, such as an array or a struct. Compound data types are available only by using BigQuery as a data source, and are not supported for forecasting models.\n### Struct\nA struct can be used to represent a group of labeled fields. A struct has a list of field names, each associated with a data type. The list of fields and their data types must be the same for all struct values in a column.\nHere are some examples of structs:\n- Blood pressure -`{\"timestamp\": 1535761416, \"systolic\": 110, \"diastolic\": 70}`\n- Product -`{\"name\": \"iPhone\", price: 1000}`\nYou use the BigQuery STRUCT data type to represent structs.\nStruct values are automatically flattened into fields. Vertex AI applies the data transformation to the flattened fields according to their transformation type.\n### Array\nAn array can be used to represent a list of values. The contained values must accept the same transformation type. You can include structs in arrays; all of the structs in the array must have the same structure.\nVertex AI processes arrays as representing relative weight. In other words, items that appear later in the array are weighted more heavily than items that appear towards the beginning.\nHere are some examples of arrays:\n- Product categories:`[\"Clothing\", \"Women\", \"Dress\", ...]`\n- Most recent purchases:`[\"iPhone\", \"Laptop\", \"Suitcase\", ...]`\n- User records:`[{\"name\": \"Joelle\", ID: 4093}, {\"name\": \"Chloe\", ID: 2047}, {\"name\": \"Neko\", ID: 3432}, ...]`\nYou use the BigQuery ARRAY data type to represent arrays.\nThe data transformation Vertex AI applies depends on the transformation type applied to the array:\n| Array type  | Transformation                                                                         |\n|:------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Numerical array | All transformations for Numerical types applied to the average of the last N items where N = {1, 2, 4, 8, all}. So the items most heavily emphasized are the ones towards the end of the array, not the beginning. The average of empty arrays is treated as zero.           |\n| Categorical array | For each element in the array of the last N items where N = {1, 2, 4, 8, all}, convert the category name to a dictionary lookup index and generate an embedding for each index. Combine the embedding of all elements into a single embedding using the mean. Empty arrays treated as an embedding of zeroes. |\n| Text array  | Concatenate all text values in the array into a single text value using a space (\" \") as a delimiter, and then treat the result as a single text value. Apply the transformations for Text columns. Empty arrays treated as an embedding of zeroes.               |\n| Timestamp array | Apply the transformations for Numerical columns to the average of the last N items of the array. N = {1, 2, 4, 8, all}. This means that the items most heavily emphasized are the ones towards the end of the array.                       |\n| Struct array  | The structs in the array are flattened into individual fields, and assembled into arrays by field. The array transformation (as described in this table) is applied, according to the field type for that array.                        |\n## How missing or null values are handled\nHow missing values are handled depends on your model objective and the transformation applied for that feature.\nFor classification and regression models, null values result in an embedding for categorical and text transformations; for other transformations, the null value is left as null.\nFor forecasting models, null values are imputed from the surrounding data. (There is no option to leave a null value as null.) If you would prefer to control the way null values are imputed, you can impute them explicitly. The best values to use might depend on your data and your business problem.\nMissing rows (for example, no row for a specific date, with a data granularity of daily) are allowed, but Vertex AI does not impute values for the missing data. Because missing rows can decrease model quality, you should avoid missing rows where possible. For example, if a row is missing because sales quantity for that day was zero, add a row for that day and explicitly set sales data to 0.\n### What values are treated as null values\nWhen training an AutoML tabular model, Vertex AI treats the following values as null values:\n- A BigQuery [NULL value](/bigquery/docs/reference/standard-sql/data-types) .\n- NaN or infinite numeric values.\n- An empty string. Vertex AI does not trim spaces from strings. That is, \" \" is considered a null value.\n- A string that can be [converted to NaN or an infinite numeric value](https://en.cppreference.com/w/c/string/byte/strtof) .- For \"NAN\": ignore case, with an optional plus or minus prepended.\n- For \"INF\": ignore case, with an optional plus or minus prepended.\n- Missing values.\n- Values in a column with a Numeric or Timestamp transformation that are not in a valid format for the column's transformation. In this case, if you specified that the row with the invalid value should be used in training, the invalid value is considered to be null.## Data format for predictions\nThe format of data used for prediction must match the format used for training.\nIf you trained your model on data in a CSV file in Cloud Storage, your data was of type STRING. If you are using a JSON object to send your prediction request, ensure that all the values in the key-value pairs are also of type STRING.\nIf you trained your model on data stored in BigQuery and you are using a JSON object to send your prediction request, the data types of the values in the JSON key-value pairs must follow the mapping in the table below.\n| BigQuery data type | JSON data type |\n|:---------------------|:-----------------|\n| INT64    | String   |\n| NUMERIC, BIGNUMERIC | Number   |\n| FLOAT64    | Number   |\n| BOOL     | Boolean   |\n| STRING    | String   |\n| DATE     | String   |\n| DATETIME    | String   |\n| TIME     | String   |\n| TIMESTAMP   | String   |\n| Array    | Array   |\n| STRUCT    | Object   |\nFor example, if your training data contained `length` features of type FLOAT64, the following JSON key-value pair is correct:\n```\n\"length\":3.6,\n```\nConversely, the following JSON key-value pair will throw an error:\n```\n\"length\":\"3.6\",\n```\n## What's next\n- Learn more about [BigQuery data types](/bigquery/docs/reference/standard-sql/data-types) \n- Learn how to [prepare your tabular training data](/vertex-ai/docs/datasets/prepare-tabular) \n- Learn about [best practices for creating tabular training data](/vertex-ai/docs/tabular-data/bp-tabular)", "guide": "Vertex AI"}