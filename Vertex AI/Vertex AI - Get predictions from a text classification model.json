{"title": "Vertex AI - Get predictions from a text classification model", "url": "https://cloud.google.com/vertex-ai/docs/text-data/classification/get-predictions", "abstract": "# Vertex AI - Get predictions from a text classification model\n", "content": "## Difference between online and batch predictions\nOnline predictions are synchronous requests made to a model endpoint. Use online predictions when you are making requests in response to application input or in situations that require timely inference.\nBatch predictions are asynchronous requests. You request batch predictions directly from the model resource without needing to deploy the model to an endpoint. For text data, use batch predictions when you don't require an immediate response and want to process accumulated data by using a single request.\n## Get online predictions\n### Deploy a model to an endpoint\nYou must deploy a model to an endpoint before that model can be used to serve online predictions. Deploying a model associates physical resources with the model so it can serve online predictions with low latency.\nYou can deploy more than one model to an endpoint, and you can deploy a model to more than one endpoint. For more information about options and use cases for deploying models, see [About deploying models](/vertex-ai/docs/general/deployment) .\nUse one of the following methods to deploy a model:\n- In the Google Cloud console, in the Vertex AI section, go to the **Models** page. [Go to the Models page](https://console.cloud.google.com/vertex-ai/models) \n- Click the name of the model you want to deploy to open its details page.\n- Select the **Deploy & Test** tab.If your model is already deployed to any endpoints, they are listed in the **Deploy your model** section.\n- Click **Deploy to endpoint** .\n- To deploy your model to a new endpoint, select radio_button_checked **Create new endpoint** and provide a name for the new endpoint. To deploy your model to an existing endpoint, select radio_button_checked **Add to existing endpoint** and select the endpoint from the drop-down list.You can add more than one model to an endpoint, and you can add a model to more than one endpoint. [Learn more](/vertex-ai/docs/general/deployment) .\n- If you deploy your model to an existing endpoint that has one or more models deployed to it, you must update the **Traffic split** percentage for the model you are deploying and the already deployed models so that all of the percentages add up to 100%.\n- Select **AutoML Text** and configure as follows:- If you're deploying your model to a new endpoint, accept 100 for the **Traffic split** . Otherwise, adjust the traffic split values for all models on the endpoint so they add up to 100.\n- Click **Done** for your model, and when all the **Traffic split** percentages are correct, click **Continue** .The region where your model deploys is displayed. This must be the region where you created your model.\n- Click **Deploy** to deploy your model to the endpoint.\nWhen you deploy a model using the Vertex AI API, you complete the following steps:- Create an endpoint if needed.\n- Get the endpoint ID.\n- Deploy the model to the endpoint.\n### Create an endpointIf you are deploying a model to an existing endpoint, you can skip this step.\nThe following example uses the [gcloud ai endpoints createcommand](/sdk/gcloud/reference/ai/endpoints/create) :\n```\ngcloud ai endpoints create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=ENDPOINT_NAME\n```\nReplace the following:- : The region where you are using Vertex AI.\n- : The display name for the endpoint.\nThe Google Cloud CLI tool might take a few seconds to create the endpoint.Before using any of the request data, make the following replacements:- : Your region.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The display name for the endpoint.\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints\n```\nRequest JSON body:\n```\n{\n \"display_name\": \"ENDPOINT_NAME\"\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/endpoints/ENDPOINT_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.CreateEndpointOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2020-11-05T17:45:42.812656Z\",\n  \"updateTime\": \"2020-11-05T17:45:42.812656Z\"\n }\n }\n}\n```\nYou can poll for the status of the operation until the response includes\n`\"done\": true`\n.\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateEndpointSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.CreateEndpointOperationMetadata;import com.google.cloud.aiplatform.v1.Endpoint;import com.google.cloud.aiplatform.v1.EndpointServiceClient;import com.google.cloud.aiplatform.v1.EndpointServiceSettings;import com.google.cloud.aiplatform.v1.LocationName;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class CreateEndpointSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String endpointDisplayName = \"YOUR_ENDPOINT_DISPLAY_NAME\";\u00a0 \u00a0 createEndpointSample(project, endpointDisplayName);\u00a0 }\u00a0 static void createEndpointSample(String project, String endpointDisplayName)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 EndpointServiceSettings endpointServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 EndpointServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (EndpointServiceClient endpointServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 EndpointServiceClient.create(endpointServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 LocationName locationName = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 Endpoint endpoint = Endpoint.newBuilder().setDisplayName(endpointDisplayName).build();\u00a0 \u00a0 \u00a0 OperationFuture<Endpoint, CreateEndpointOperationMetadata> endpointFuture =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 endpointServiceClient.createEndpointAsync(locationName, endpoint);\u00a0 \u00a0 \u00a0 System.out.format(\"Operation name: %s\\n\", endpointFuture.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to finish...\");\u00a0 \u00a0 \u00a0 Endpoint endpointResponse = endpointFuture.get(300, TimeUnit.SECONDS);\u00a0 \u00a0 \u00a0 System.out.println(\"Create Endpoint Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"Name: %s\\n\", endpointResponse.getName());\u00a0 \u00a0 \u00a0 System.out.format(\"Display Name: %s\\n\", endpointResponse.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\"Description: %s\\n\", endpointResponse.getDescription());\u00a0 \u00a0 \u00a0 System.out.format(\"Labels: %s\\n\", endpointResponse.getLabelsMap());\u00a0 \u00a0 \u00a0 System.out.format(\"Create Time: %s\\n\", endpointResponse.getCreateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"Update Time: %s\\n\", endpointResponse.getUpdateTime());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/create-endpoint.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const endpointDisplayName = 'YOUR_ENDPOINT_DISPLAY_NAME';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Endpoint Service Client libraryconst {EndpointServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst endpointServiceClient = new EndpointServiceClient(clientOptions);async function createEndpoint() {\u00a0 // Configure the parent resource\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 const endpoint = {\u00a0 \u00a0 displayName: endpointDisplayName,\u00a0 };\u00a0 const request = {\u00a0 \u00a0 parent,\u00a0 \u00a0 endpoint,\u00a0 };\u00a0 // Get and print out a list of all the endpoints for this resource\u00a0 const [response] = await endpointServiceClient.createEndpoint(request);\u00a0 console.log(`Long running operation : ${response.name}`);\u00a0 // Wait for operation to complete\u00a0 await response.promise();\u00a0 const result = response.result;\u00a0 console.log('Create endpoint response');\u00a0 console.log(`\\tName : ${result.name}`);\u00a0 console.log(`\\tDisplay name : ${result.displayName}`);\u00a0 console.log(`\\tDescription : ${result.description}`);\u00a0 console.log(`\\tLabels : ${JSON.stringify(result.labels)}`);\u00a0 console.log(`\\tCreate time : ${JSON.stringify(result.createTime)}`);\u00a0 console.log(`\\tUpdate time : ${JSON.stringify(result.updateTime)}`);}createEndpoint();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_endpoint_sample.py) \n```\ndef create_endpoint_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 location: str,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 endpoint = aiplatform.Endpoint.create(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name,\u00a0 \u00a0 \u00a0 \u00a0 project=project,\u00a0 \u00a0 \u00a0 \u00a0 location=location,\u00a0 \u00a0 )\u00a0 \u00a0 print(endpoint.display_name)\u00a0 \u00a0 print(endpoint.resource_name)\u00a0 \u00a0 return endpoint\n```\n### Retrieve the endpoint IDYou need the endpoint ID to deploy the model.\nThe following example uses the [gcloud ai endpoints listcommand](/sdk/gcloud/reference/ai/endpoints/list) :\n```\ngcloud ai endpoints list \\\u00a0 --region=LOCATION \\\u00a0 --filter=display_name=ENDPOINT_NAME\n```\nReplace the following:- : The region where you are using Vertex AI.\n- : The display name for the endpoint.\nNote the number that appears in the `ENDPOINT_ID` column. Use this ID in the following step.Before using any of the request data, make the following replacements:- : The region where you are using Vertex AI.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The display name for the endpoint.\nHTTP method and URL:\n```\nGET https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints?filter=display_name=ENDPOINT_NAME\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"endpoints\": [ {\n  \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/endpoints/ENDPOINT_ID\",\n  \"displayName\": \"ENDPOINT_NAME\",\n  \"etag\": \"AMEw9yPz5pf4PwBHbRWOGh0PcAxUdjbdX2Jm3QO_amguy3DbZGP5Oi_YUKRywIE-BtLx\",\n  \"createTime\": \"2020-04-17T18:31:11.585169Z\",\n  \"updateTime\": \"2020-04-17T18:35:08.568959Z\"\n }\n ]\n}\n```\nNote the\n.### Deploy the modelSelect the tab below for your language or environment:The following examples use the [gcloud ai endpoints deploy-model command](/sdk/gcloud/reference/ai/endpoints/deploy-model) .\nThe following example deploys a `Model` to an `Endpoint` without splitting traffic between multiple `DeployedModel` resources:\nBefore using any of the command data below, make the following replacements:- : The ID for the endpoint.\n- : The region where you are using Vertex AI.\n- : The ID for the model to be deployed.\n- : A name for the`DeployedModel`. You can use the display name of the`Model`for the`DeployedModel`as well.\n- : The minimum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to the maximum number of nodes and never fewer than this number of nodes.\n- : The maximum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to this number of nodes and never fewer than the minimum number of nodes. If you omit the`--max-replica-count`flag, then  maximum number of nodes is set to the value of`--min-replica-count`.\nExecute the [gcloud ai endpoints deploy-model](/sdk/gcloud/reference/ai/endpoints/deploy-model) command:The `--traffic-split=0=100` flag in the preceding examples sends 100% of prediction traffic that the `Endpoint` receives to the new `DeployedModel` , which is represented by the temporary ID `0` . If your `Endpoint` already has other `DeployedModel` resources, then you can split traffic between the new `DeployedModel` and the old ones. For example, to send 20% of traffic to the new `DeployedModel` and 80% to an older one, run the following command.\nBefore using any of the command data below, make the following replacements:- : the ID of the existing`DeployedModel`.\nExecute the [gcloud ai endpoints deploy-model](/sdk/gcloud/reference/ai/endpoints/deploy-model) command:Deploy the model.\nBefore using any of the request data, make the following replacements:- : The region where you are using Vertex AI.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The ID for the endpoint.\n- : The ID for the model to be deployed.\n- : A name for the`DeployedModel`. You can use the display name of the`Model`for the`DeployedModel`as well.\n- : The percentage of the prediction traffic to this endpoint  to be routed to the model being deployed with this operation. Defaults to 100. All traffic  percentages must add up to 100. [Learn more about traffic splits](/vertex-ai/docs/general/deployment#models-endpoint) .\n- : Optional. If other models are deployed to this endpoint, you  must update their traffic split percentages so that all percentages add up to 100.\n- : The traffic split percentage value for the deployed model id  key.\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) \nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:deployModel\n```\nRequest JSON body:\n```\n{\n \"deployedModel\": {\n \"model\": \"projects/PROJECT_ID/locations/us-central1/models/MODEL_ID\",\n \"displayName\": \"DEPLOYED_MODEL_NAME\",\n \"automaticResources\": {\n  }\n },\n \"trafficSplit\": {\n \"0\": TRAFFIC_SPLIT_THIS_MODEL,\n \"DEPLOYED_MODEL_ID_1\": TRAFFIC_SPLIT_MODEL_1,\n \"DEPLOYED_MODEL_ID_2\": TRAFFIC_SPLIT_MODEL_2\n },\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.DeployModelOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2020-10-19T17:53:16.502088Z\",\n  \"updateTime\": \"2020-10-19T17:53:16.502088Z\"\n }\n }\n}\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/GoogleCloudPlatform/java-docs-samples&page=editor&cloudshell_workspace=aiplatform/src/main/java/aiplatform&cloudshell_open_in_editor=DeployModelSample.java) [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/DeployModelSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.api.gax.longrunning.OperationTimedPollAlgorithm;import com.google.api.gax.retrying.RetrySettings;import com.google.cloud.aiplatform.v1.AutomaticResources;import com.google.cloud.aiplatform.v1.DedicatedResources;import com.google.cloud.aiplatform.v1.DeployModelOperationMetadata;import com.google.cloud.aiplatform.v1.DeployModelResponse;import com.google.cloud.aiplatform.v1.DeployedModel;import com.google.cloud.aiplatform.v1.EndpointName;import com.google.cloud.aiplatform.v1.EndpointServiceClient;import com.google.cloud.aiplatform.v1.EndpointServiceSettings;import com.google.cloud.aiplatform.v1.MachineSpec;import com.google.cloud.aiplatform.v1.ModelName;import com.google.cloud.aiplatform.v1.stub.EndpointServiceStubSettings;import java.io.IOException;import java.util.HashMap;import java.util.Map;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;import org.threeten.bp.Duration;public class DeployModelSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String deployedModelDisplayName = \"YOUR_DEPLOYED_MODEL_DISPLAY_NAME\";\u00a0 \u00a0 String endpointId = \"YOUR_ENDPOINT_NAME\";\u00a0 \u00a0 String modelId = \"YOUR_MODEL_ID\";\u00a0 \u00a0 int timeout = 900;\u00a0 \u00a0 deployModelSample(project, deployedModelDisplayName, endpointId, modelId, timeout);\u00a0 }\u00a0 static void deployModelSample(\u00a0 \u00a0 \u00a0 String project,\u00a0 \u00a0 \u00a0 String deployedModelDisplayName,\u00a0 \u00a0 \u00a0 String endpointId,\u00a0 \u00a0 \u00a0 String modelId,\u00a0 \u00a0 \u00a0 int timeout)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // Set long-running operations (LROs) timeout\u00a0 \u00a0 final OperationTimedPollAlgorithm operationTimedPollAlgorithm =\u00a0 \u00a0 \u00a0 \u00a0 OperationTimedPollAlgorithm.create(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 RetrySettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInitialRetryDelay(Duration.ofMillis(5000L))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setRetryDelayMultiplier(1.5)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMaxRetryDelay(Duration.ofMillis(45000L))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInitialRpcTimeout(Duration.ZERO)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setRpcTimeoutMultiplier(1.0)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMaxRpcTimeout(Duration.ZERO)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTotalTimeout(Duration.ofSeconds(timeout))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 EndpointServiceStubSettings.Builder endpointServiceStubSettingsBuilder =\u00a0 \u00a0 \u00a0 \u00a0 EndpointServiceStubSettings.newBuilder();\u00a0 \u00a0 endpointServiceStubSettingsBuilder\u00a0 \u00a0 \u00a0 \u00a0 .deployModelOperationSettings()\u00a0 \u00a0 \u00a0 \u00a0 .setPollingAlgorithm(operationTimedPollAlgorithm);\u00a0 \u00a0 EndpointServiceStubSettings endpointStubSettings = endpointServiceStubSettingsBuilder.build();\u00a0 \u00a0 EndpointServiceSettings endpointServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 EndpointServiceSettings.create(endpointStubSettings);\u00a0 \u00a0 endpointServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 endpointServiceSettings.toBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (EndpointServiceClient endpointServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 EndpointServiceClient.create(endpointServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 EndpointName endpointName = EndpointName.of(project, location, endpointId);\u00a0 \u00a0 \u00a0 // key '0' assigns traffic for the newly deployed model\u00a0 \u00a0 \u00a0 // Traffic percentage values must add up to 100\u00a0 \u00a0 \u00a0 // Leave dictionary empty if endpoint should not accept any traffic\u00a0 \u00a0 \u00a0 Map<String, Integer> trafficSplit = new HashMap<>();\u00a0 \u00a0 \u00a0 trafficSplit.put(\"0\", 100);\u00a0 \u00a0 \u00a0 ModelName modelName = ModelName.of(project, location, modelId);\u00a0 \u00a0 \u00a0 AutomaticResources automaticResourcesInput =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutomaticResources.newBuilder().setMinReplicaCount(1).setMaxReplicaCount(1).build();\u00a0 \u00a0 \u00a0 DeployedModel deployedModelInput =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 DeployedModel.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModel(modelName.toString())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(deployedModelDisplayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAutomaticResources(automaticResourcesInput)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 OperationFuture<DeployModelResponse, DeployModelOperationMetadata> deployModelResponseFuture =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 endpointServiceClient.deployModelAsync(endpointName, deployedModelInput, trafficSplit);\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Operation name: %s\\n\", deployModelResponseFuture.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to finish...\");\u00a0 \u00a0 \u00a0 DeployModelResponse deployModelResponse = deployModelResponseFuture.get(20, TimeUnit.MINUTES);\u00a0 \u00a0 \u00a0 System.out.println(\"Deploy Model Response\");\u00a0 \u00a0 \u00a0 DeployedModel deployedModel = deployModelResponse.getDeployedModel();\u00a0 \u00a0 \u00a0 System.out.println(\"\\tDeployed Model\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tid: %s\\n\", deployedModel.getId());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tmodel: %s\\n\", deployedModel.getModel());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tDisplay Name: %s\\n\", deployedModel.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tCreate Time: %s\\n\", deployedModel.getCreateTime());\u00a0 \u00a0 \u00a0 DedicatedResources dedicatedResources = deployedModel.getDedicatedResources();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tDedicated Resources\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tMin Replica Count: %s\\n\", dedicatedResources.getMinReplicaCount());\u00a0 \u00a0 \u00a0 MachineSpec machineSpec = dedicatedResources.getMachineSpec();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\t\\tMachine Spec\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\t\\tMachine Type: %s\\n\", machineSpec.getMachineType());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\t\\tAccelerator Type: %s\\n\", machineSpec.getAcceleratorType());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\t\\tAccelerator Count: %s\\n\", machineSpec.getAcceleratorCount());\u00a0 \u00a0 \u00a0 AutomaticResources automaticResources = deployedModel.getAutomaticResources();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tAutomatic Resources\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tMin Replica Count: %s\\n\", automaticResources.getMinReplicaCount());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tMax Replica Count: %s\\n\", automaticResources.getMaxReplicaCount());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/deploy-model.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const modelId = \"YOUR_MODEL_ID\";// const endpointId = 'YOUR_ENDPOINT_ID';// const deployedModelDisplayName = 'YOUR_DEPLOYED_MODEL_DISPLAY_NAME';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';const modelName = `projects/${project}/locations/${location}/models/${modelId}`;const endpoint = `projects/${project}/locations/${location}/endpoints/${endpointId}`;// Imports the Google Cloud Endpoint Service Client libraryconst {EndpointServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpoint:const clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst endpointServiceClient = new EndpointServiceClient(clientOptions);async function deployModel() {\u00a0 // Configure the parent resource\u00a0 // key '0' assigns traffic for the newly deployed model\u00a0 // Traffic percentage values must add up to 100\u00a0 // Leave dictionary empty if endpoint should not accept any traffic\u00a0 const trafficSplit = {0: 100};\u00a0 const deployedModel = {\u00a0 \u00a0 // format: 'projects/{project}/locations/{location}/models/{model}'\u00a0 \u00a0 model: modelName,\u00a0 \u00a0 displayName: deployedModelDisplayName,\u00a0 \u00a0 // AutoML Vision models require `automatic_resources` field\u00a0 \u00a0 // Other model types may require `dedicated_resources` field instead\u00a0 \u00a0 automaticResources: {minReplicaCount: 1, maxReplicaCount: 1},\u00a0 };\u00a0 const request = {\u00a0 \u00a0 endpoint,\u00a0 \u00a0 deployedModel,\u00a0 \u00a0 trafficSplit,\u00a0 };\u00a0 // Get and print out a list of all the endpoints for this resource\u00a0 const [response] = await endpointServiceClient.deployModel(request);\u00a0 console.log(`Long running operation : ${response.name}`);\u00a0 // Wait for operation to complete\u00a0 await response.promise();\u00a0 const result = response.result;\u00a0 console.log('Deploy model response');\u00a0 const modelDeployed = result.deployedModel;\u00a0 console.log('\\tDeployed model');\u00a0 if (!modelDeployed) {\u00a0 \u00a0 console.log('\\t\\tId : {}');\u00a0 \u00a0 console.log('\\t\\tModel : {}');\u00a0 \u00a0 console.log('\\t\\tDisplay name : {}');\u00a0 \u00a0 console.log('\\t\\tCreate time : {}');\u00a0 \u00a0 console.log('\\t\\tDedicated resources');\u00a0 \u00a0 console.log('\\t\\t\\tMin replica count : {}');\u00a0 \u00a0 console.log('\\t\\t\\tMachine spec {}');\u00a0 \u00a0 console.log('\\t\\t\\t\\tMachine type : {}');\u00a0 \u00a0 console.log('\\t\\t\\t\\tAccelerator type : {}');\u00a0 \u00a0 console.log('\\t\\t\\t\\tAccelerator count : {}');\u00a0 \u00a0 console.log('\\t\\tAutomatic resources');\u00a0 \u00a0 console.log('\\t\\t\\tMin replica count : {}');\u00a0 \u00a0 console.log('\\t\\t\\tMax replica count : {}');\u00a0 } else {\u00a0 \u00a0 console.log(`\\t\\tId : ${modelDeployed.id}`);\u00a0 \u00a0 console.log(`\\t\\tModel : ${modelDeployed.model}`);\u00a0 \u00a0 console.log(`\\t\\tDisplay name : ${modelDeployed.displayName}`);\u00a0 \u00a0 console.log(`\\t\\tCreate time : ${modelDeployed.createTime}`);\u00a0 \u00a0 const dedicatedResources = modelDeployed.dedicatedResources;\u00a0 \u00a0 console.log('\\t\\tDedicated resources');\u00a0 \u00a0 if (!dedicatedResources) {\u00a0 \u00a0 \u00a0 console.log('\\t\\t\\tMin replica count : {}');\u00a0 \u00a0 \u00a0 console.log('\\t\\t\\tMachine spec {}');\u00a0 \u00a0 \u00a0 console.log('\\t\\t\\t\\tMachine type : {}');\u00a0 \u00a0 \u00a0 console.log('\\t\\t\\t\\tAccelerator type : {}');\u00a0 \u00a0 \u00a0 console.log('\\t\\t\\t\\tAccelerator count : {}');\u00a0 \u00a0 } else {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\t\\t\\tMin replica count : \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ${dedicatedResources.minReplicaCount}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 const machineSpec = dedicatedResources.machineSpec;\u00a0 \u00a0 \u00a0 console.log('\\t\\t\\tMachine spec');\u00a0 \u00a0 \u00a0 console.log(`\\t\\t\\t\\tMachine type : ${machineSpec.machineType}`);\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\t\\t\\t\\tAccelerator type : ${machineSpec.acceleratorType}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\t\\t\\t\\tAccelerator count : ${machineSpec.acceleratorCount}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 }\u00a0 \u00a0 const automaticResources = modelDeployed.automaticResources;\u00a0 \u00a0 console.log('\\t\\tAutomatic resources');\u00a0 \u00a0 if (!automaticResources) {\u00a0 \u00a0 \u00a0 console.log('\\t\\t\\tMin replica count : {}');\u00a0 \u00a0 \u00a0 console.log('\\t\\t\\tMax replica count : {}');\u00a0 \u00a0 } else {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\t\\t\\tMin replica count : \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ${automaticResources.minReplicaCount}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\t\\t\\tMax replica count : \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ${automaticResources.maxReplicaCount}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 }\u00a0 }}deployModel();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [Open in Editor](https://ide.cloud.google.com/?git_repo=https://github.com/googleapis/python-aiplatform&page=editor&cloudshell_workspace=samples/model-builder&cloudshell_open_in_editor=deploy_model_with_automatic_resources_sample.py) [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/deploy_model_with_automatic_resources_sample.py) \n```\ndef deploy_model_with_automatic_resources_sample(\u00a0 \u00a0 project,\u00a0 \u00a0 location,\u00a0 \u00a0 model_name: str,\u00a0 \u00a0 endpoint: Optional[aiplatform.Endpoint] = None,\u00a0 \u00a0 deployed_model_display_name: Optional[str] = None,\u00a0 \u00a0 traffic_percentage: Optional[int] = 0,\u00a0 \u00a0 traffic_split: Optional[Dict[str, int]] = None,\u00a0 \u00a0 min_replica_count: int = 1,\u00a0 \u00a0 max_replica_count: int = 1,\u00a0 \u00a0 metadata: Optional[Sequence[Tuple[str, str]]] = (),\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 model_name: A fully-qualified model resource name or model ID.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Example: \"projects/123/locations/us-central1/models/456\" or\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"456\" when project and location are initialized or passed.\u00a0 \u00a0 \"\"\"\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 model = aiplatform.Model(model_name=model_name)\u00a0 \u00a0 model.deploy(\u00a0 \u00a0 \u00a0 \u00a0 endpoint=endpoint,\u00a0 \u00a0 \u00a0 \u00a0 deployed_model_display_name=deployed_model_display_name,\u00a0 \u00a0 \u00a0 \u00a0 traffic_percentage=traffic_percentage,\u00a0 \u00a0 \u00a0 \u00a0 traffic_split=traffic_split,\u00a0 \u00a0 \u00a0 \u00a0 min_replica_count=min_replica_count,\u00a0 \u00a0 \u00a0 \u00a0 max_replica_count=max_replica_count,\u00a0 \u00a0 \u00a0 \u00a0 metadata=metadata,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 return model\n```\n## Get operation status\nSome requests start long-running operations that require time to complete. These requests return an operation name, which you can use to view the operation's status or cancel the operation. Vertex AI provides helper methods to make calls against long-running operations. For more information, see [Working with long-runningoperations](/vertex-ai/docs/general/long-running-operations) .\n### Make an online prediction using your deployed model\nTo make an online prediction, submit one or more test items to a model for analysis, and the model returns results that are based on your model's objective. For more information about prediction results, see the [Interpret results](/vertex-ai/docs/predictions/interpreting-results-automl) page.\nUse the Google Cloud console to request an online prediction. Your model must be deployed to an endpoint.- In the Google Cloud console, in the Vertex AI section, go to the **Models** page. [Go to the Models page](https://console.cloud.google.com/vertex-ai/models) \n- From the list of models, click the name of the model to request predictions from.\n- Select the **Deploy & test** tab.\n- Under the **Test your model** section, add test items to request a prediction.AutoML models for text objectives require you to type content in a text field and click **Predict** .For information about local feature importance, see [Get explanations](#explanations-api) .After the prediction is complete, Vertex AI returns the results in the console.\nUse the Vertex AI API to request an online prediction. Your model must be deployed to an endpoint.- Create a file named `request.json` with the following contents:```\n{\u00a0 \"instances\": [{\u00a0 \u00a0 \"mimeType\": \"text/plain\",\u00a0 \u00a0 \"content\": \"CONTENT\"\u00a0 }]}\n```Replace the following:- : The text snippet used to make a prediction.\n- Run the following command:```\ngcloud ai endpoints predict ENDPOINT_ID \\\u00a0 --region=LOCATION_ID \\\u00a0 --json-request=request.json\n```Replace the following:- : The ID for the endpoint.\n- : The region where you are using Vertex AI.Before using any of the request data, make the following replacements:- : Region where Endpoint is located. For example,`us-central1`.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) \n- : The ID for the endpoint\n- : The text snippet used to make a prediction.\n- : The ID of the deployed model that was used  to make the prediction.\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:predict\n```\nRequest JSON body:\n```\n{\n \"instances\": [{\n \"mimeType\": \"text/plain\",\n \"content\": \"CONTENT\"\n }]\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:predict\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/endpoints/ENDPOINT_ID:predict\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following:\n```\n{\n \"predictions\": [ {\n  \"ids\": [  \"1234567890123456789\",\n  \"2234567890123456789\",\n  \"3234567890123456789\"\n  ],\n  \"displayNames\": [  \"GreatService\",\n  \"Suggestion\",\n  \"InfoRequest\"\n  ],\n  \"confidences\": [  0.8986392080783844,\n  0.81984345316886902,\n  0.7722353458404541\n  ]\n }\n ],\n \"deployedModelId\": \"0123456789012345678\"\n}\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/PredictTextClassificationSingleLabelSample.java) \n```\nimport com.google.cloud.aiplatform.util.ValueConverter;import com.google.cloud.aiplatform.v1.EndpointName;import com.google.cloud.aiplatform.v1.PredictResponse;import com.google.cloud.aiplatform.v1.PredictionServiceClient;import com.google.cloud.aiplatform.v1.PredictionServiceSettings;import com.google.cloud.aiplatform.v1.schema.predict.instance.TextClassificationPredictionInstance;import com.google.cloud.aiplatform.v1.schema.predict.prediction.ClassificationPredictionResult;import com.google.protobuf.Value;import java.io.IOException;import java.util.ArrayList;import java.util.List;public class PredictTextClassificationSingleLabelSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String content = \"YOUR_TEXT_CONTENT\";\u00a0 \u00a0 String endpointId = \"YOUR_ENDPOINT_ID\";\u00a0 \u00a0 predictTextClassificationSingleLabel(project, content, endpointId);\u00a0 }\u00a0 static void predictTextClassificationSingleLabel(\u00a0 \u00a0 \u00a0 String project, String content, String endpointId) throws IOException {\u00a0 \u00a0 PredictionServiceSettings predictionServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (PredictionServiceClient predictionServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 PredictionServiceClient.create(predictionServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 EndpointName endpointName = EndpointName.of(project, location, endpointId);\u00a0 \u00a0 \u00a0 TextClassificationPredictionInstance predictionInstance =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TextClassificationPredictionInstance.newBuilder().setContent(content).build();\u00a0 \u00a0 \u00a0 List<Value> instances = new ArrayList<>();\u00a0 \u00a0 \u00a0 instances.add(ValueConverter.toValue(predictionInstance));\u00a0 \u00a0 \u00a0 PredictResponse predictResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 predictionServiceClient.predict(endpointName, instances, ValueConverter.EMPTY_VALUE);\u00a0 \u00a0 \u00a0 System.out.println(\"Predict Text Classification Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\tDeployed Model Id: %s\\n\", predictResponse.getDeployedModelId());\u00a0 \u00a0 \u00a0 System.out.println(\"Predictions:\\n\\n\");\u00a0 \u00a0 \u00a0 for (Value prediction : predictResponse.getPredictionsList()) {\u00a0 \u00a0 \u00a0 \u00a0 ClassificationPredictionResult.Builder resultBuilder =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ClassificationPredictionResult.newBuilder();\u00a0 \u00a0 \u00a0 \u00a0 // Display names and confidences values correspond to\u00a0 \u00a0 \u00a0 \u00a0 // IDs in the ID list.\u00a0 \u00a0 \u00a0 \u00a0 ClassificationPredictionResult result =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (ClassificationPredictionResult) ValueConverter.fromValue(resultBuilder, prediction);\u00a0 \u00a0 \u00a0 \u00a0 int counter = 0;\u00a0 \u00a0 \u00a0 \u00a0 for (Long id : result.getIdsList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Label ID: %d\\n\", id);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Label: %s\\n\", result.getDisplayNames(counter));\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Confidence: %.4f\\n\", result.getConfidences(counter));\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 counter++;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/predict-text-classification.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const text = 'YOUR_PREDICTION_TEXT';// const endpointId = 'YOUR_ENDPOINT_ID';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';const aiplatform = require('@google-cloud/aiplatform');const {instance, prediction} =\u00a0 aiplatform.protos.google.cloud.aiplatform.v1.schema.predict;// Imports the Google Cloud Model Service Client libraryconst {PredictionServiceClient} = aiplatform.v1;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst predictionServiceClient = new PredictionServiceClient(clientOptions);async function predictTextClassification() {\u00a0 // Configure the resources\u00a0 const endpoint = `projects/${project}/locations/${location}/endpoints/${endpointId}`;\u00a0 const predictionInstance =\u00a0 \u00a0 new instance.TextClassificationPredictionInstance({\u00a0 \u00a0 \u00a0 content: text,\u00a0 \u00a0 });\u00a0 const instanceValue = predictionInstance.toValue();\u00a0 const instances = [instanceValue];\u00a0 const request = {\u00a0 \u00a0 endpoint,\u00a0 \u00a0 instances,\u00a0 };\u00a0 const [response] = await predictionServiceClient.predict(request);\u00a0 console.log('Predict text classification response');\u00a0 console.log(`\\tDeployed model id : ${response.deployedModelId}\\n\\n`);\u00a0 console.log('Prediction results:');\u00a0 for (const predictionResultValue of response.predictions) {\u00a0 \u00a0 const predictionResult =\u00a0 \u00a0 \u00a0 prediction.ClassificationPredictionResult.fromValue(\u00a0 \u00a0 \u00a0 \u00a0 predictionResultValue\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 for (const [i, label] of predictionResult.displayNames.entries()) {\u00a0 \u00a0 \u00a0 console.log(`\\tDisplay name: ${label}`);\u00a0 \u00a0 \u00a0 console.log(`\\tConfidences: ${predictionResult.confidences[i]}`);\u00a0 \u00a0 \u00a0 console.log(`\\tIDs: ${predictionResult.ids[i]}\\n\\n`);\u00a0 \u00a0 }\u00a0 }}predictTextClassification();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/predict_text_classification_single_label_sample.py) \n```\ndef predict_text_classification_single_label_sample(\u00a0 \u00a0 project, location, endpoint, content):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 endpoint = aiplatform.Endpoint(endpoint)\u00a0 \u00a0 response = endpoint.predict(instances=[{\"content\": content}], parameters={})\u00a0 \u00a0 for prediction_ in response.predictions:\u00a0 \u00a0 \u00a0 \u00a0 print(prediction_)\n```\n## Get batch predictions\nTo make a batch prediction request, you specify an [input source](#input_data) and an [output format](#output_format) where Vertex AI stores predictions results.\n**Note:** To minimize processing time when you use the Google Cloud console to create batch predictions, we recommend that you select input and output locations that are in the same region as your model. If you use the API to create batch predictions, send requests to a service endpoint (such as `https://us-central1-aiplatform.googleapis.com` ) that is in the same region or geographically close to your input and output locations.\n### Input data requirements\nThe input for batch requests specifies the items to send to your model for prediction. For text classification models, you can use a JSON Lines file to specify a list of documents to make predictions about and then store the JSON Lines file in a Cloud Storage bucket. The following sample shows a single line in an input JSON Lines file.\n```\n{\"content\": \"gs://sourcebucket/datasets/texts/source_text.txt\", \"mimeType\": \"text/plain\"}\n```\n### Request a batch prediction\nFor batch prediction requests, you can use the Google Cloud console or the Vertex AI API. Depending on the number of input items that you've submitted, a batch prediction task can take some time to complete.\nUse the Google Cloud console to request a batch prediction.- In the Google Cloud console, in the Vertex AI section, go to the **Batch predictions** page. [Go to the Batch predictions page](https://console.cloud.google.com/vertex-ai/batch-predictions) \n- Click **Create** to open the **New batch prediction** window and complete the following steps:- Enter a name for the batch prediction.\n- For **Model name** , select the name of the model to use for this batch prediction.\n- For **Source path** , specify the Cloud Storage location where your JSON Lines input file is located.\n- For the **Destination path** , specify a Cloud Storage location where the batch prediction results are stored. The **Output** format is determined by your model's objective. AutoML models for text objectives output JSON Lines files.\nUse the Vertex AI API to send batch prediction requests.\nBefore using any of the request data, make the following replacements:- : Region where Model is stored and batch prediction job is executed. For example,`us-central1`.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) \n- : Display name for the batch job\n- : The ID for the model to use for making predictions\n- : Cloud Storage URI where your input JSON Lines file is  located.\n- : Your Cloud Storage bucket\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) \nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/batchPredictionJobs\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"BATCH_JOB_NAME\",\n \"model\": \"projects/PROJECT_ID/locations/LOCATION_ID/models/MODEL_ID\",\n \"inputConfig\": {\n  \"instancesFormat\": \"jsonl\",\n  \"gcsSource\": {\n   \"uris\": [\"URI\"]\n  }\n },\n \"outputConfig\": {\n  \"predictionsFormat\": \"jsonl\",\n  \"gcsDestination\": {\n   \"outputUriPrefix\": \"OUTPUT_BUCKET\"\n  }\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/batchPredictionJobs\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/batchPredictionJobs\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION/batchPredictionJobs/BATCH_JOB_ID\",\n \"displayName\": \"BATCH_JOB_NAME\",\n \"model\": \"projects/PROJECT_NUMBER/locations/LOCATION/models/MODEL_ID\",\n \"inputConfig\": {\n \"instancesFormat\": \"jsonl\",\n \"gcsSource\": {\n  \"uris\": [  \"CONTENT\"\n  ]\n }\n },\n \"outputConfig\": {\n \"predictionsFormat\": \"jsonl\",\n \"gcsDestination\": {\n  \"outputUriPrefix\": \"BUCKET\"\n }\n },\n \"state\": \"JOB_STATE_PENDING\",\n \"completionStats\": {\n \"incompleteCount\": \"-1\"\n },\n \"createTime\": \"2022-12-19T20:33:48.906074Z\",\n \"updateTime\": \"2022-12-19T20:33:48.906074Z\",\n \"modelVersionId\": \"1\"\n}\n```\nYou can poll for the status of the batch job using the until the job `state` is `JOB_STATE_SUCCEEDED` .\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateBatchPredictionJobTextClassificationSample.java) \n```\nimport com.google.api.gax.rpc.ApiException;import com.google.cloud.aiplatform.v1.BatchPredictionJob;import com.google.cloud.aiplatform.v1.GcsDestination;import com.google.cloud.aiplatform.v1.GcsSource;import com.google.cloud.aiplatform.v1.JobServiceClient;import com.google.cloud.aiplatform.v1.JobServiceSettings;import com.google.cloud.aiplatform.v1.LocationName;import com.google.cloud.aiplatform.v1.ModelName;import java.io.IOException;public class CreateBatchPredictionJobTextClassificationSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"PROJECT\";\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 String displayName = \"DISPLAY_NAME\";\u00a0 \u00a0 String modelId = \"MODEL_ID\";\u00a0 \u00a0 String gcsSourceUri = \"GCS_SOURCE_URI\";\u00a0 \u00a0 String gcsDestinationOutputUriPrefix = \"GCS_DESTINATION_OUTPUT_URI_PREFIX\";\u00a0 \u00a0 createBatchPredictionJobTextClassificationSample(\u00a0 \u00a0 \u00a0 \u00a0 project, location, displayName, modelId, gcsSourceUri, gcsDestinationOutputUriPrefix);\u00a0 }\u00a0 static void createBatchPredictionJobTextClassificationSample(\u00a0 \u00a0 \u00a0 String project,\u00a0 \u00a0 \u00a0 String location,\u00a0 \u00a0 \u00a0 String displayName,\u00a0 \u00a0 \u00a0 String modelId,\u00a0 \u00a0 \u00a0 String gcsSourceUri,\u00a0 \u00a0 \u00a0 String gcsDestinationOutputUriPrefix)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 // The AI Platform services require regional API endpoints.\u00a0 \u00a0 JobServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 JobServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (JobServiceClient client = JobServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 try {\u00a0 \u00a0 \u00a0 \u00a0 String modelName = ModelName.of(project, location, modelId).toString();\u00a0 \u00a0 \u00a0 \u00a0 GcsSource gcsSource = GcsSource.newBuilder().addUris(gcsSourceUri).build();\u00a0 \u00a0 \u00a0 \u00a0 BatchPredictionJob.InputConfig inputConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BatchPredictionJob.InputConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInstancesFormat(\"jsonl\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setGcsSource(gcsSource)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 \u00a0 GcsDestination gcsDestination =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 GcsDestination.newBuilder().setOutputUriPrefix(gcsDestinationOutputUriPrefix).build();\u00a0 \u00a0 \u00a0 \u00a0 BatchPredictionJob.OutputConfig outputConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BatchPredictionJob.OutputConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setPredictionsFormat(\"jsonl\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setGcsDestination(gcsDestination)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 \u00a0 BatchPredictionJob batchPredictionJob =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BatchPredictionJob.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(displayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModel(modelName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputConfig(inputConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setOutputConfig(outputConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 \u00a0 LocationName parent = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 \u00a0 BatchPredictionJob response = client.createBatchPredictionJob(parent, batchPredictionJob);\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"response: %s\\n\", response);\u00a0 \u00a0 \u00a0 } catch (ApiException ex) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"Exception: %s\\n\", ex.getLocalizedMessage());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/create-batch-prediction-job-text-classification.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const batchPredictionDisplayName = 'YOUR_BATCH_PREDICTION_DISPLAY_NAME';// const modelId = 'YOUR_MODEL_ID';// const gcsSourceUri = 'YOUR_GCS_SOURCE_URI';// const gcsDestinationOutputUriPrefix = 'YOUR_GCS_DEST_OUTPUT_URI_PREFIX';// \u00a0 \u00a0eg. \"gs://<your-gcs-bucket>/destination_path\"// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Job Service Client libraryconst {JobServiceClient} = require('@google-cloud/aiplatform').v1;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst jobServiceClient = new JobServiceClient(clientOptions);async function createBatchPredictionJobTextClassification() {\u00a0 // Configure the parent resource\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 const modelName = `projects/${project}/locations/${location}/models/${modelId}`;\u00a0 const inputConfig = {\u00a0 \u00a0 instancesFormat: 'jsonl',\u00a0 \u00a0 gcsSource: {uris: [gcsSourceUri]},\u00a0 };\u00a0 const outputConfig = {\u00a0 \u00a0 predictionsFormat: 'jsonl',\u00a0 \u00a0 gcsDestination: {outputUriPrefix: gcsDestinationOutputUriPrefix},\u00a0 };\u00a0 const batchPredictionJob = {\u00a0 \u00a0 displayName: batchPredictionDisplayName,\u00a0 \u00a0 model: modelName,\u00a0 \u00a0 inputConfig,\u00a0 \u00a0 outputConfig,\u00a0 };\u00a0 const request = {\u00a0 \u00a0 parent,\u00a0 \u00a0 batchPredictionJob,\u00a0 };\u00a0 // Create batch prediction job request\u00a0 const [response] = await jobServiceClient.createBatchPredictionJob(request);\u00a0 console.log('Create batch prediction job text classification response');\u00a0 console.log(`Name : ${response.name}`);\u00a0 console.log('Raw response:');\u00a0 console.log(JSON.stringify(response, null, 2));}createBatchPredictionJobTextClassification();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_batch_prediction_job_sample.py) \n```\ndef create_batch_prediction_job_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 model_resource_name: str,\u00a0 \u00a0 job_display_name: str,\u00a0 \u00a0 gcs_source: Union[str, Sequence[str]],\u00a0 \u00a0 gcs_destination: str,\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 my_model = aiplatform.Model(model_resource_name)\u00a0 \u00a0 batch_prediction_job = my_model.batch_predict(\u00a0 \u00a0 \u00a0 \u00a0 job_display_name=job_display_name,\u00a0 \u00a0 \u00a0 \u00a0 gcs_source=gcs_source,\u00a0 \u00a0 \u00a0 \u00a0 gcs_destination_prefix=gcs_destination,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 batch_prediction_job.wait()\u00a0 \u00a0 print(batch_prediction_job.display_name)\u00a0 \u00a0 print(batch_prediction_job.resource_name)\u00a0 \u00a0 print(batch_prediction_job.state)\u00a0 \u00a0 return batch_prediction_job\n```\n### Retrieve batch prediction results\nWhen a batch prediction task is complete, the output of the prediction is stored in the Cloud Storage bucket that you specified in your request.\n### Example batch prediction results\nThe following an example batch prediction results from a text classification model.\n```\n{\n \"instance\": {\"content\": \"gs://bucket/text.txt\", \"mimeType\": \"text/plain\"},\n \"predictions\": [ {\n  \"ids\": [  \"1234567890123456789\",\n  \"2234567890123456789\",\n  \"3234567890123456789\"\n  ],\n  \"displayNames\": [  \"GreatService\",\n  \"Suggestion\",\n  \"InfoRequest\"\n  ],\n  \"confidences\": [  0.8986392080783844,\n  0.81984345316886902,\n  0.7722353458404541\n  ]\n }\n ]\n}\n```", "guide": "Vertex AI"}