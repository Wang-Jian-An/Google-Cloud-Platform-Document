{"title": "Vertex AI - Configure feature-based explanations", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Configure feature-based explanations\nTo use Vertex Explainable AI with a custom-trained model, you must configure certain options when you create the `Model` resource that you plan to request explanations from, when you deploy the model, or when you submit a batch explanation job. This page describes configuring these options.\nIf you want to use Vertex Explainable AI with an AutoML tabular model, then you don't need to perform any configuration; Vertex AI automatically configures the model for Vertex Explainable AI. Skip this document and read [Getting explanations](/vertex-ai/docs/explainable-ai/getting-explanations) .\n", "content": "## When and where to configure explanations\nYou configure explanations when you create or import a model. You can also configure explanations on a model that you have already created, even if you didn't configuring explanations previously.\n### Configure explanations when creating or importing models\nWhen you create or import a `Model` , you can set a default configuration for all its explanations using the `Model` 's [explanationSpecfield](/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model.FIELDS.explanation_spec) .\nYou can create a custom-trained `Model` in Vertex AI in the following ways:\n- [Import or register a Model to the Vertex AI Model Registry](/vertex-ai/docs/model-registry/import-model) \n- [Create a custom TrainingPipeline resource that imports a Model.](/vertex-ai/docs/training/create-training-pipeline) \n- [Create a BigQuery ML model](/bigquery-ml/docs/create_vertex) and specify the optional`model_registry`setting in the`CREATE MODEL`syntax. This setting automatically registers the model to Vertex AI Model Registry and configures its`explanationSpec`.\nIn either case, you can configure the `Model` to support Vertex Explainable AI. The [examples](#import-model-example) in this document assume that you are importing a `Model` . To configure Vertex Explainable AI when you create a custom-trained `Model` using a `TrainingPipeline` , use the configuration settings described in this document in the `TrainingPipeline` 's [modelToUploadfield](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines#TrainingPipeline.FIELDS.model_to_upload) .\n### Configure explanations when deploying models or getting batch predictions\nWhen you [deploy a Model to an Endpoint resource](/vertex-ai/docs/predictions/deploy-model-api) , you can either:\n- **Configure explanations** , whether or not the model was previously configured for explanations. This is useful if you didn't originally plan to get explanations (and omitted the`explanationSpec`field when you created the model), but decide later that you want explanations for the Model, or if you want to override some of the explanation settings.\n- **Disable explanations** . This is useful if your model is configured for explanations, but you do not plan to get explanations from the endpoint. To disable explanations when deploying the model to an endpoint, either uncheck the Explainability options in the Cloud Console or set [DeployedModel.disableExplanations](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints#DeployedModel) to`true`.\nSimilarly, when you [get batch predictions from aModel](/vertex-ai/docs/predictions/batch-predictions#custom-trained) , you can either configure explanations by populating the [BatchPredictionJob.explanationSpecfield](/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchPredictionJob.FIELDS.explanation_spec) or disable explanations by setting `BatchPredictionJob.generateExplanation` to `false` .\n### Override the configuration when getting online explanations\nRegardless of whether you created or imported the `Model` with explanation settings, and regardless of whether you configured explanation settings during deployment, you can override the `Model` 's initial explanation settings when you [get online explanations](/vertex-ai/docs/explainable-ai/getting-explanations#getting_online_explanations) .\nWhen you send an `explain` request to Vertex AI, you can override some of the explanation configuration that you previously set for the `Model` or the `DeployedModel` .\nIn the `explain` request, you can override the following fields:\n- [Input baselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) for any custom-trained model\n- Visualization configuration for image models\n- [ExplanationParameters](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#ExplanationParameters) for the`method`\nOverride these settings in the explanation request's [explanationSpecOverride field](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/explain#explanationspecoverride) .\n## Import a model with an explanationSpec field\nDepending on whether you serve predictions using a [prebuiltcontainer](/vertex-ai/docs/predictions/pre-built-containers) or a [customcontainer](/vertex-ai/docs/predictions/use-custom-container) , specify slightly different details for the `ExplanationSpec` . Select the tab that matches the container that you are using:\nYou can use any of the following attribution methods for Vertex Explainable AI. Read the [comparison of feature attributionmethods](/vertex-ai/docs/explainable-ai/overview#compare-methods) to select the appropriate one for your `Model` :\nThe following examples assume that your model accepts a single input feature; this is often the case for sequential Keras models and many other TensorFlow models. Since the feature is a tensor of any shape, it can in fact encode many pieces of information. When you [request explanations](/vertex-ai/docs/explainable-ai/getting-explanations) , Vertex Explainable AI provides attributions for each element of the tensor.If your model accepts multiple named input tensors, then add corresponding entries to the `inputs` metadata field.\nFor TensorFlow 2 models, [ExplanationMetadata](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#ExplanationMetadata) is optional. If omitted, Vertex AI automatically infers the values from the model and uses the tensor names as the feature names. This [notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/explainable_ai/xai_image_classification_feature_attributions.ipynb) demonstrates how to import a model without `ExplanationMetadata` .Depending on which tool you want to use to create or import the `Model` , select one of the following tabs:\nFollow the guide to [importing a model using the Google Cloud console](/vertex-ai/docs/model-registry/import-model#import_a_model_using) . When you get to the **Explainability** step, do the following:\n- For your feature attribution method, select **Sampled Shapely (for tabularmodels)** .\n- Set the path count to the [number of featurepermutations](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#sampledshapleyattribution) to use for the Sampled Shapley attribution method. This must be an integer in the range `[1, 50]` .A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `25` .\n- Configure each input feature in your model: **Note:** For Tensorflow 2 models, input feature name is optional.- Fill in your input feature name.\n- Optionally, you can add one or more [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) . Otherwise, Vertex Explainable AI chooses a default input baseline of all-zero values, which is a black image for image data.\n- If you're importing a TensorFlow model, there are additional input fields:- Fill out the **Input tensor name** .\n- If applicable, fill in the **Indices tensor name** and/or the **Dense shape tensor name** .\n- The **Modality** cannot be updated here. It is set automatically to `NUMERIC` for tabular models, or `IMAGE` for image models.\n- If applicable, set the **Encoding** field. This defaults to `IDENTITY` if not set.\n- If applicable, set the **Group name** field.\n- If you're importing a TensorFlow model, specify output fields:- Set the **Output name** of your feature.\n- Set the **Output tensor name** of your feature.\n- If applicable, set the **Index display name mapping** .\n- If applicable, set the **Display name mapping key** .\n- Click the **Import** button when you have finished configuring the explainability settings.\n- For TensorFlow 2, `ExplanationMetadata` is optional.Write the following [ExplanationMetadata](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata) to a JSON file in your local environment. The filename does not matter, but for this example call the file `explanation-metadata.json` :```\n{\u00a0 \"inputs\": {\u00a0 \u00a0 \"FEATURE_NAME\": {\u00a0 \u00a0 \u00a0 \"inputTensorName\": \"INPUT_TENSOR_NAME\",\u00a0 \u00a0 }\u00a0 },\u00a0 \"outputs\": {\u00a0 \u00a0 \"OUTPUT_NAME\": {\u00a0 \u00a0 \u00a0 \"outputTensorName\": \"OUTPUT_TENSOR_NAME\"\u00a0 \u00a0 }\u00a0 }}\n```Replace the following:- : Any memorable name for your input feature.\n- : The name of the input tensor in TensorFlow. To find the correct value for this field, read [Using TensorFlow withVertex Explainable AI](/vertex-ai/docs/explainable-ai/tensorflow) .\n- : Any memorable name for the output of your model.\n- : The name of the output tensor in TensorFlow. To find the correct value for this field, read [Using TensorFlow withVertex Explainable AI](/vertex-ai/docs/explainable-ai/tensorflow) .\nYou can optionally add [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) to the `ExplanationMetadata` . Otherwise, Vertex AI chooses input baselines for the `Model` .\n- Run the following command to create a `Model` resource that supports Vertex Explainable AI. The flags most pertinent to Vertex Explainable AI are highlighted.```\ngcloud ai models upload \\\u00a0 --region=LOCATION \\\u00a0 --display-name=MODEL_NAME \\\u00a0 --container-image-uri=IMAGE_URI \\\u00a0 --artifact-uri=PATH_TO_MODEL_ARTIFACT_DIRECTORY \\\u00a0 --explanation-method=sampled-shapley \\\u00a0 --explanation-path-count=PATH_COUNT \\\u00a0 --explanation-metadata-file=explanation-metadata.json\n```Replace the following:- : The URI of a [TensorFlow pre-built container forserving predictions](/vertex-ai/docs/predictions/pre-built-containers#tensorflow) .\n- : The [number of featurepermutations](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#sampledshapleyattribution) to use for the Sampled Shapley attribution method. Must be an integer in the range `[1, 50]` .A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `25` .\nTo learn about appropriate values for the other placeholders, see [upload](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) and [Importing models](/vertex-ai/docs/general/import-model) .\nBefore using any of the request data, make the following replacements:- : The URI of a [TensorFlow pre-built container forserving predictions](/vertex-ai/docs/predictions/pre-built-containers#tensorflow) .\n- : The [number of featurepermutations](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#sampledshapleyattribution) to use for the Sampled Shapley attribution method. Must be an integer in the range `[1, 50]` .A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `25` .\n- : Any memorable name for your input feature.\n- : The name of the input tensor in TensorFlow. To find the correct value for this field, read [Using TensorFlow withVertex Explainable AI](/vertex-ai/docs/explainable-ai/tensorflow) .\n- : Any memorable name for the output of your model.\n- : The name of the output tensor in TensorFlow. To find the correct value for this field, read [Using TensorFlow withVertex Explainable AI](/vertex-ai/docs/explainable-ai/tensorflow) .To learn about appropriate values for the other placeholders, see [upload](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) and [Importing models](/vertex-ai/docs/general/import-model) .\nYou can optionally add [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) to the `ExplanationMetadata` . Otherwise, Vertex AI chooses input baselines for the `Model` .\nFor TensorFlow 2 models, the `metadata` field is optional. If omitted, Vertex AI automatically infers the `inputs` and `outputs` from the model.\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\n```\nRequest JSON body:\n```\n{\n \"model\": {\n \"displayName\": \"MODEL_NAME\",\n \"containerSpec\": {\n  \"imageUri\": \"IMAGE_URI\"\n },\n \"artifactUri\": \"PATH_TO_MODEL_ARTIFACT_DIRECTORY\",\n \"explanationSpec\": {\n  \"parameters\": {\n  \"sampledShapleyAttribution\": {\n   \"pathCount\": PATH_COUNT\n  }\n  },\n  \"metadata\": {\n  \"inputs\": {\n   \"FEATURE_NAME\": {\n   \"inputTensorName\": \"INPUT_TENSOR_NAME\",\n   }\n  },\n  \"outputs\": {\n   \"OUTPUT_NAME\": {\n   \"outputTensorName\": \"OUTPUT_TENSOR_NAME\"\n   }\n  }\n  }\n }\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\" | Select-Object -Expand Content\n```Depending on which tool you want to use to create or import the `Model` , select one of the following tabs:\nFollow the guide to [importing a model using the Google Cloud console](/vertex-ai/docs/model-registry/import-model#import_a_model_using) . When you get to the **Explainability** step, do the following:\n- For your feature attribution method, select **Integrated gradients (for tabularmodels)** or **Integrated gradients (for image classification models)** , depending on which is more appropriate for your model.\n- If you are importing an image classification model, do the following:- Set the **Visualization type** and **Color map** .\n- You can leave the **Clip below** , **Clip above** , **Overlay type** , and **Number of integral steps** at their default settings.\nLearn more about [visualization settings](/vertex-ai/docs/explainable-ai/visualization-settings) .\n- Set the number of steps to use for approximating the path integral during feature attribution. This must be an integer in the range `[1, 100]` .A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `50` .\n- Configure each input feature in your model: **Note:** For Tensorflow 2 models, input feature name is optional.- Fill in your input feature name.\n- Optionally, you can add one or more [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) . Otherwise, Vertex Explainable AI chooses a default input baseline of all-zero values, which is a black image for image data.\n- If you're importing a TensorFlow model, there are additional input fields:- Fill out the **Input tensor name** .\n- If applicable, fill in the **Indices tensor name** and/or the **Dense shape tensor name** .\n- The **Modality** cannot be updated here. It is set automatically to `NUMERIC` for tabular models, or `IMAGE` for image models.\n- If applicable, set the **Encoding** field. This defaults to `IDENTITY` if not set.\n- If applicable, set the **Group name** field.\n- If you're importing a TensorFlow model, specify output fields:- Set the **Output name** of your feature.\n- Set the **Output tensor name** of your feature.\n- If applicable, set the **Index display name mapping** .\n- If applicable, set the **Display name mapping key** .\n- Click the **Import** button when you have finished configuring the explainability settings.\n- For TensorFlow 2, `ExplanationMetadata` is optional.Write the following [ExplanationMetadata](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata) to a JSON file in your local environment. The filename does not matter, but for this example call the file `explanation-metadata.json` :```\n{\u00a0 \"inputs\": {\u00a0 \u00a0 \"FEATURE_NAME\": {\u00a0 \u00a0 \u00a0 \"inputTensorName\": \"INPUT_TENSOR_NAME\",\u00a0 \u00a0 \u00a0 \"modality\": \"MODALITY\",\u00a0 \u00a0 \u00a0 \"visualization\": VISUALIZATION_SETTINGS\u00a0 \u00a0 }\u00a0 },\u00a0 \"outputs\": {\u00a0 \u00a0 \"OUTPUT_NAME\": {\u00a0 \u00a0 \u00a0 \"outputTensorName\": \"OUTPUT_TENSOR_NAME\"\u00a0 \u00a0 }\u00a0 }}\n```Replace the following:- : Any memorable name for your input feature.\n- : The name of the input tensor in TensorFlow. To find the correct value for this field, read [Using TensorFlow withVertex Explainable AI](/vertex-ai/docs/explainable-ai/tensorflow) .\n- :`image`if the`Model`accepts images as input or`numeric`if the`Model`accepts tabular data as input. Defaults to`numeric`.\n- : Options for visualizing explanations. To learn how to populate this field, read [Configuring visualization settings for imagedata](/vertex-ai/docs/explainable-ai/visualization-settings) .If you omit the `modality` field or set the `modality` field to `numeric` , then omit the `visualization` field entirely.\n- : Any memorable name for the output of your model.\n- : The name of the output tensor in TensorFlow. To find the correct value for this field, read [Using TensorFlow withVertex Explainable AI](/vertex-ai/docs/explainable-ai/tensorflow) .\nYou can optionally add [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) to the `ExplanationMetadata` . Otherwise, Vertex AI chooses input baselines for the `Model` .\n- Run the following command to create a `Model` resource that supports Vertex Explainable AI. The flags most pertinent to Vertex Explainable AI are highlighted.```\ngcloud ai models upload \\\u00a0 --region=LOCATION \\\u00a0 --display-name=MODEL_NAME \\\u00a0 --container-image-uri=IMAGE_URI \\\u00a0 --artifact-uri=PATH_TO_MODEL_ARTIFACT_DIRECTORY \\\u00a0 --explanation-method=integrated-gradients \\\u00a0 --explanation-step-count=STEP_COUNT \\\u00a0 --explanation-metadata-file=explanation-metadata.json\n```Replace the following:- : The URI of a [TensorFlow pre-built container forserving predictions](/vertex-ai/docs/predictions/pre-built-containers#tensorflow) .\n- : The number of steps to use for approximating the path integral during feature attribution. Must be an integer in the range`[1, 100]`.A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `50` .\nTo learn about appropriate values for the other placeholders, see [upload](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) and [Importing models](/vertex-ai/docs/general/import-model) .You can optionally add [flags to configure the SmoothGrad approximation ofgradients](/sdk/gcloud/reference/ai/models/upload#--smooth-grad-noise-sigma) .\nBefore using any of the request data, make the following replacements:- : The URI of a [TensorFlow pre-built container forserving predictions](/vertex-ai/docs/predictions/pre-built-containers#tensorflow) .\n- : The number of steps to use for approximating the path integral during feature attribution. Must be an integer in the range`[1, 100]`.A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `50` .\n- : Any memorable name for your input feature.\n- : The name of the input tensor in TensorFlow. To find the correct value for this field, read [Using TensorFlow withVertex Explainable AI](/vertex-ai/docs/explainable-ai/tensorflow) .\n- :`image`if the`Model`accepts images as input or`numeric`if the`Model`accepts tabular data as input. Defaults to`numeric`.\n- : Options for visualizing explanations. To learn how to populate this field, read [Configuring visualization settings for imagedata](/vertex-ai/docs/explainable-ai/visualization-settings) .If you omit the `modality` field or set the `modality` field to `numeric` , then omit the `visualization` field entirely.\n- : Any memorable name for the output of your model.\n- : The name of the output tensor in TensorFlow. To find the correct value for this field, read [Using TensorFlow withVertex Explainable AI](/vertex-ai/docs/explainable-ai/tensorflow) .To learn about appropriate values for the other placeholders, see [upload](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) and [Importing models](/vertex-ai/docs/general/import-model) .\nYou can optionally add [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) to the `ExplanationMetadata` . Otherwise, Vertex AI chooses input baselines for the `Model` .\nYou can optionally add [fields to configure the SmoothGrad approximation ofgradients](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#smoothgradconfig) to the `ExplanationParameters` .\nFor TensorFlow 2 models, the `metadata` field is optional. If omitted, Vertex AI automatically infers the `inputs` and `outputs` from the model.\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\n```\nRequest JSON body:\n```\n{\n \"model\": {\n \"displayName\": \"MODEL_NAME\",\n \"containerSpec\": {\n  \"imageUri\": \"IMAGE_URI\"\n },\n \"artifactUri\": \"PATH_TO_MODEL_ARTIFACT_DIRECTORY\",\n \"explanationSpec\": {\n  \"parameters\": {\n  \"integratedGradientsAttribution\": {\n   \"stepCount\": STEP_COUNT\n  }\n  },\n  \"metadata\": {\n  \"inputs\": {\n   \"FEATURE_NAME\": {\n   \"inputTensorName\": \"INPUT_TENSOR_NAME\",\n   \"modality\": \"MODALITY\",\n   \"visualization\": VISUALIZATION_SETTINGS\n   }\n  },\n  \"outputs\": {\n   \"OUTPUT_NAME\": {\n   \"outputTensorName\": \"OUTPUT_TENSOR_NAME\"\n   }\n  }\n  }\n }\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\" | Select-Object -Expand Content\n```Depending on which tool you want to use to create or import the `Model` , select one of the following tabs:\nFollow the guide to [importing a model using the Google Cloud console](/vertex-ai/docs/model-registry/import-model#import_a_model_using) . When you get to the **Explainability** step, do the following:\n- For your feature attribution method, select **XRAI (for image classificationmodels)** .\n- Set the following visualization options:- Set the **Color map** .\n- You can leave the **Clip below** , **Clip above** , **Overlay type** , and **Number of integral steps** at their default settings.Learn more about [visualization settings](/vertex-ai/docs/explainable-ai/visualization-settings) .\n- Set the number of steps to use for approximating the path integral during feature attribution. This must be an integer in the range `[1, 100]` .A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `50` .\n- Configure each input feature in your model: **Note:** For Tensorflow 2 models, input feature name is optional.- Fill in your input feature name.\n- Optionally, you can add one or more [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) . Otherwise, Vertex Explainable AI chooses a default input baseline of all-zero values, which is a black image for image data.\n- If you're importing a TensorFlow model, there are additional input fields:- Fill out the **Input tensor name** .\n- If applicable, fill in the **Indices tensor name** and/or the **Dense shape tensor name** .\n- The **Modality** cannot be updated here. It is set automatically to `NUMERIC` for tabular models, or `IMAGE` for image models.\n- If applicable, set the **Encoding** field. This defaults to `IDENTITY` if not set.\n- If applicable, set the **Group name** field.\n- If you're importing a TensorFlow model, specify output fields:- Set the **Output name** of your feature.\n- Set the **Output tensor name** of your feature.\n- If applicable, set the **Index display name mapping** .\n- If applicable, set the **Display name mapping key** .\n- Click the **Import** button when you have finished configuring the explainability settings.\n- For TensorFlow 2, `ExplanationMetadata` is optional.Write the following [ExplanationMetadata](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata) to a JSON file in your local environment. The filename does not matter, but for this example call the file `explanation-metadata.json` :```\n{\u00a0 \"inputs\": {\u00a0 \u00a0 \"FEATURE_NAME\": {\u00a0 \u00a0 \u00a0 \"inputTensorName\": \"INPUT_TENSOR_NAME\",\u00a0 \u00a0 \u00a0 \"modality\": \"image\",\u00a0 \u00a0 \u00a0 \"visualization\": VISUALIZATION_SETTINGS\u00a0 \u00a0 }\u00a0 },\u00a0 \"outputs\": {\u00a0 \u00a0 \"OUTPUT_NAME\": {\u00a0 \u00a0 \u00a0 \"outputTensorName\": \"OUTPUT_TENSOR_NAME\"\u00a0 \u00a0 }\u00a0 }}\n```Replace the following:- : Any memorable name for your input feature.\n- : The name of the input tensor in TensorFlow. To find the correct value for this field, read [Using TensorFlow withVertex Explainable AI](/vertex-ai/docs/explainable-ai/tensorflow) .\n- : Options for visualizing explanations. To learn how to populate this field, read [Configuring visualization settings for imagedata](/vertex-ai/docs/explainable-ai/visualization-settings) .\n- : Any memorable name for the output of your model.\n- : The name of the output tensor in TensorFlow. To find the correct value for this field, read [Using TensorFlow withVertex Explainable AI](/vertex-ai/docs/explainable-ai/tensorflow) .\nYou can optionally add [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) to the `ExplanationMetadata` . Otherwise, Vertex AI chooses input baselines for the `Model` .\n- Run the following command to create a `Model` resource that supports Vertex Explainable AI. The flags most pertinent to Vertex Explainable AI are highlighted.```\ngcloud ai models upload \\\u00a0 --region=LOCATION \\\u00a0 --display-name=MODEL_NAME \\\u00a0 --container-image-uri=IMAGE_URI \\\u00a0 --artifact-uri=PATH_TO_MODEL_ARTIFACT_DIRECTORY \\\u00a0 --explanation-method=xrai \\\u00a0 --explanation-step-count=STEP_COUNT \\\u00a0 --explanation-metadata-file=explanation-metadata.json\n```Replace the following:- : The URI of a [TensorFlow pre-built container forserving predictions](/vertex-ai/docs/predictions/pre-built-containers#tensorflow) .\n- : The number of steps to use for approximating the path integral during feature attribution. Must be an integer in the range`[1, 100]`.A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `50` .\nTo learn about appropriate values for the other placeholders, see [upload](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) and [Importing models](/vertex-ai/docs/general/import-model) .You can optionally add [flags to configure the SmoothGrad approximation ofgradients](/sdk/gcloud/reference/ai/models/upload#--smooth-grad-noise-sigma) .\nBefore using any of the request data, make the following replacements:- : The URI of a [TensorFlow pre-built container forserving predictions](/vertex-ai/docs/predictions/pre-built-containers#tensorflow) .\n- : The number of steps to use for approximating the path integral during feature attribution. Must be an integer in the range`[1, 100]`.A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `50` .\n- : Any memorable name for your input feature.\n- : The name of the input tensor in TensorFlow. To find the correct value for this field, read [Using TensorFlow withVertex Explainable AI](/vertex-ai/docs/explainable-ai/tensorflow) .\n- : Options for visualizing explanations. To learn how to populate this field, read [Configuring visualization settings for imagedata](/vertex-ai/docs/explainable-ai/visualization-settings) .\n- : Any memorable name for the output of your model.\n- : The name of the output tensor in TensorFlow. To find the correct value for this field, read [Using TensorFlow withVertex Explainable AI](/vertex-ai/docs/explainable-ai/tensorflow) .To learn about appropriate values for the other placeholders, see [upload](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) and [Importing models](/vertex-ai/docs/general/import-model) .\nYou can optionally add [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) to the `ExplanationMetadata` . Otherwise, Vertex AI chooses input baselines for the `Model` .\nYou can optionally add [fields to configure the SmoothGrad approximation ofgradients](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#smoothgradconfig) to the `ExplanationParameters` .\nFor TensorFlow 2 models, the `metadata` field is optional. If omitted, Vertex AI automatically infers the `inputs` and `outputs` from the model.\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\n```\nRequest JSON body:\n```\n{\n \"model\": {\n \"displayName\": \"MODEL_NAME\",\n \"containerSpec\": {\n  \"imageUri\": \"IMAGE_URI\"\n },\n \"artifactUri\": \"PATH_TO_MODEL_ARTIFACT_DIRECTORY\",\n \"explanationSpec\": {\n  \"parameters\": {\n  \"xraiAttribution\": {\n   \"stepCount\": STEP_COUNT\n  }\n  },\n  \"metadata\": {\n  \"inputs\": {\n   \"FEATURE_NAME\": {\n   \"inputTensorName\": \"INPUT_TENSOR_NAME\",\n   \"modality\": \"image\",\n   \"visualization\": VISUALIZATION_SETTINGS\n   }\n  },\n  \"outputs\": {\n   \"OUTPUT_NAME\": {\n   \"outputTensorName\": \"OUTPUT_TENSOR_NAME\"\n   }\n  }\n  }\n }\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\" | Select-Object -Expand Content\n```\nIf your `Model` accepts tabular data as input and serves predictions using a [pre-built scikit-learn or XGBoost container forprediction](/vertex-ai/docs/predictions/pre-built-containers) , then you can configure it to use the [Sampled Shapley attributionmethod](/vertex-ai/docs/explainable-ai/overview#sampled-shapley) for explanations.\nThe following examples assume that your model accepts a single 2-D matrix as input; this is often the case for scikit-learn and XGBoost models. When you [request explanations](/vertex-ai/docs/explainable-ai/getting-explanations) , Vertex Explainable AI provides attributions for each element of the input matrix.\nDepending on which tool you want to use to create or import the `Model` , select one of the following tabs:\nFollow the guide to [importing a model using the Google Cloud console](/vertex-ai/docs/model-registry/import-model#import_a_model_using) . When you get to the **Explainability** step, do the following:\n- For your feature attribution method, select **Sampled Shapely (for tabularmodels)** .\n- Set the path count to the [number of featurepermutations](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#sampledshapleyattribution) to use for the Sampled Shapley attribution method. This must be an integer in the range `[1, 50]` .A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `25` .\n- Configure each input feature in your model:- Fill in your input feature name.If your model artifacts do not include feature names, then Vertex AI is unable to map the specified input feature names to the model. In that case, you should provide only one input feature with any arbitrary, user-friendly name, such as `input_features` . In the explanation response, you will get an N dimensional list of attributions, where N is the number of features in the model and the elements in the list appear in the same order as the training dataset.\n- Optionally, you can add one or more [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) . Otherwise, Vertex Explainable AI chooses a default input baseline of all-zero values, which is a black image for image data.\n- Set the **Output name** of your feature.\n- Click the **Import** button when you have finished configuring the explainability settings.\n- Write the following [ExplanationMetadata](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata) to a JSON file in your local environment. The filename does not matter, but for this example call the file `explanation-metadata.json` :```\n{\u00a0 \"inputs\": {\u00a0 \u00a0 \"FEATURE_NAME\": {\u00a0 \u00a0 }\u00a0 },\u00a0 \"outputs\": {\u00a0 \u00a0 \"OUTPUT_NAME\": {\u00a0 \u00a0 }\u00a0 }}\n```Replace the following:- : Any memorable name for your input feature.\n- : Any memorable name for the output of your model.\nIf you specify [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) , make sure they match your model's input, usually a list of 2-D matrices. Otherwise, the default value for the input baseline is a 0-value 2-D matrix of the input shape.\n- Run the following command to create a `Model` resource that supports Vertex Explainable AI. The flags most pertinent to Vertex Explainable AI are highlighted.```\ngcloud ai models upload \\\u00a0 --region=LOCATION \\\u00a0 --display-name=MODEL_NAME \\\u00a0 --container-image-uri=IMAGE_URI \\\u00a0 --artifact-uri=PATH_TO_MODEL_ARTIFACT_DIRECTORY \\\u00a0 --explanation-method=sampled-shapley \\\u00a0 --explanation-path-count=PATH_COUNT \\\u00a0 --explanation-metadata-file=explanation-metadata.json\n```Replace the following:- : The URI of a [pre-built container forserving predictions](/vertex-ai/docs/predictions/pre-built-containers) .\n- : The [number of featurepermutations](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#sampledshapleyattribution) to use for the Sampled Shapley attribution method. Must be an integer in the range `[1, 50]` .A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `25` .\nTo learn about appropriate values for the other placeholders, see [upload](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) and [Importing models](/vertex-ai/docs/general/import-model) .\nBefore using any of the request data, make the following replacements:- : The URI of a [pre-built container forserving predictions](/vertex-ai/docs/predictions/pre-built-containers) .\n- : The [number of featurepermutations](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#sampledshapleyattribution) to use for the Sampled Shapley attribution method. Must be an integer in the range `[1, 50]` .A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `25` .\n- : Any memorable name for your input feature.\n- : Any memorable name for the output of your model.To learn about appropriate values for the other placeholders, see [upload](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) and [Importing models](/vertex-ai/docs/general/import-model) .\nIf you specify [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) , make sure they match your model's input, usually a list of 2-D matrices. Otherwise, the default value for the input baseline is a 0-value 2-D matrix of the input shape.\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\n```\nRequest JSON body:\n```\n{\n \"model\": {\n \"displayName\": \"MODEL_NAME\",\n \"containerSpec\": {\n  \"imageUri\": \"IMAGE_URI\"\n },\n \"artifactUri\": \"PATH_TO_MODEL_ARTIFACT_DIRECTORY\",\n \"explanationSpec\": {\n \"parameters\": {\n  \"sampledShapleyAttribution\": {\n  \"pathCount\": PATH_COUNT\n  }\n },\n \"metadata\": {\n  \"inputs\": {\n   \"FEATURE_NAME\": {\n   }\n  },\n  \"outputs\": {\n   \"OUTPUT_NAME\": {\n   }\n  }\n }\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\" | Select-Object -Expand Content\n```If your `Model` accepts tabular data as input and serves predictions using a [custom container](/vertex-ai/docs/predictions/use-custom-container) , then you can configure it to use the [Sampled Shapley attributionmethod](/vertex-ai/docs/explainable-ai/overview#sampled-shapley) for explanations.\n **Note:** The following instructions do not work if you are using any of Vertex AI's [pre-built containers for servingpredictions](/vertex-ai/docs/predictions/pre-built-containers) . You must use a custom container image that is not managed by Vertex AI and that meets the [custom container requirements](/vertex-ai/docs/predictions/custom-container-requirements) .\n### Determining feature and output namesIn the following steps, you must provide Vertex AI with the names of the features that your `Model` expects as input. You must also specify the key used for outputs in the `Model` 's predictions.If your `Model` expects each [inputinstance](/vertex-ai/docs/predictions/custom-container-requirements#request_requirements) to have certain top-level keys, then those keys are your feature names.\nFor example, consider a `Model` that expects each input instance to have the following format:\n```\n{\u00a0 \"length\": <value>,\u00a0 \"width\": <value>}\n```\nIn this case, the feature names are `length` and `width` . Even if the values of these fields contain nested lists or objects, `length` and `width` are the only keys you need for the following steps. When you [requestexplanations](/vertex-ai/docs/explainable-ai/getting-explanations) , Vertex Explainable AI provides attributions for each nested element of your features.\nIf your `Model` expects unkeyed input, then Vertex Explainable AI considers the `Model` to have a single feature. You can use any memorable string for the feature name.\nFor example, consider a `Model` that expects each input instance to have the following format:\n```\n[\u00a0 <value>,\u00a0 <value>]\n```\nIn this case, provide Vertex Explainable AI with a single feature name of your choosing, like `dimensions` .\n **Note:** The following examples assume that your model has a single input feature. If your model has multiple features, as described in this section, then add corresponding entries to the `inputs` field.If your `Model` returns each [online predictioninstance](/vertex-ai/docs/predictions/custom-container-requirements#response_requirements) with keyed output, then that key is your output name.\nFor example, consider a `Model` that returns each prediction in the following format:\n```\n{\u00a0 \"scores\": <value>}\n```\nIn this case, the output name is `scores` . If the value of the `scores` field is an array, then when you get explanations, Vertex Explainable AI returns feature attributions for the element with the highest value in each prediction. To configure Vertex Explainable AI to provide feature attributions for additional elements of the output field, you can specify the `topK` or `outputIndices` fields of [ExplanationParameters](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationparameters) . However, the examples in this document do not demonstrate these options.\nIf your `Model` returns unkeyed predictions, then you can use any memorable string for the output name. For example, this applies if your `Model` returns an array or a scalar for each prediction.\n### Creating the Model\nDepending on which tool you want to use to create or import the `Model` , select one of the following tabs:\nFollow the guide to [importing a model using the Google Cloud console](/vertex-ai/docs/model-registry/import-model#import_a_model_using) . When you get to the **Explainability** step, do the following:\n- For your feature attribution method, select **Sampled Shapely (for tabularmodels)** .\n- Set the path count to the [number of featurepermutations](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#sampledshapleyattribution) to use for the Sampled Shapley attribution method. This must be an integer in the range `[1, 50]` .A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `25` .\n- Configure each input feature in your model:- Fill in your input feature name.If your model artifacts do not include feature names, then Vertex AI is unable to map the specified input feature names to the model. In that case, you should provide only one input feature with any arbitrary, user-friendly name, such as `input_features` . In the explanation response, you will get an N dimensional list of attributions, where N is the number of features in the model and the elements in the list appear in the same order as the training dataset.\n- Optionally, you can add one or more [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) . Otherwise, Vertex Explainable AI chooses a default input baseline of all-zero values, which is a black image for image data.\n- Set the **Output name** of your feature.\n- Click the **Import** button when you have finished configuring the explainability settings.\n- Write the following [ExplanationMetadata](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata) to a JSON file in your local environment. The filename does not matter, but for this example call the file `explanation-metadata.json` :```\n{\u00a0 \"inputs\": {\u00a0 \u00a0 \"FEATURE_NAME\": {}\u00a0 },\u00a0 \"outputs\": {\u00a0 \u00a0 \"OUTPUT_NAME\": {}\u00a0 }}\n```Replace the following:- : The name of the feature, as described in the \"Determining feature names\" section of this document.\n- : The name of the output, as described in the \"Determining the output name\" section of this document.\nYou can optionally add [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) to the `ExplanationMetadata` . Otherwise, Vertex AI chooses input baselines for the `Model` .If you specify [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) , make sure they match your model's input, usually a list of 2-D matrices. Otherwise, the default value for the input baseline is a 0-value 2-D matrix of the input shape.\n- Run the following command to create a `Model` resource that supports Vertex Explainable AI. The flags most pertinent to Vertex Explainable AI are highlighted.```\ngcloud ai models upload \\\u00a0 --region=LOCATION \\\u00a0 --display-name=MODEL_NAME \\\u00a0 --container-image-uri=IMAGE_URI \\\u00a0 --artifact-uri=PATH_TO_MODEL_ARTIFACT_DIRECTORY \\\u00a0 --explanation-method=sampled-shapley \\\u00a0 --explanation-path-count=PATH_COUNT \\\u00a0 --explanation-metadata-file=explanation-metadata.json\n```Replace the following:- : The [number of featurepermutations](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#sampledshapleyattribution) to use for the Sampled Shapley attribution method. Must be an integer in the range `[1, 50]` .A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `25` .\nTo learn about appropriate values for the other placeholders, see [upload](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) and [Importing models](/vertex-ai/docs/general/import-model) .Before using any of the request data, make the following replacements:- : The [number of featurepermutations](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#sampledshapleyattribution) to use for the Sampled Shapley attribution method. Must be an integer in the range `[1, 50]` .A higher value might [reduceapproximation error](/vertex-ai/docs/explainable-ai/improving-explanations) but is more computationally intensive. If you don't know what value to use, try `25` .\n- : The name of the feature, as described in the \"Determining feature names\" section of this document.\n- : The name of the output, as described in the \"Determining the output name\" section of this document.To learn about appropriate values for the other placeholders, see [upload](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) and [Importing models](/vertex-ai/docs/general/import-model) .\nYou can optionally add [inputbaselines](/vertex-ai/docs/explainable-ai/improving-explanations#baselines) to the `ExplanationMetadata` . Otherwise, Vertex AI chooses input baselines for the `Model` .\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\n```\nRequest JSON body:\n```\n{\n \"model\": {\n \"displayName\": \"MODEL_NAME\",\n \"containerSpec\": {\n  \"imageUri\": \"IMAGE_URI\"\n },\n \"artifactUri\": \"PATH_TO_MODEL_ARTIFACT_DIRECTORY\",\n \"explanationSpec\": {\n \"parameters\": {\n  \"sampledShapleyAttribution\": {\n  \"pathCount\": PATH_COUNT\n  }\n },\n \"metadata\": {\n  \"inputs\": {\n   \"FEATURE_NAME\": {}\n  },\n  \"outputs\": {\n   \"OUTPUT_NAME\": {}\n  }\n }\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/models:upload\" | Select-Object -Expand Content\n```\n## What's next\n- [Deploy your Model to an Endpoint](/vertex-ai/docs/predictions/overview#model_deployment) and [get explanations](/vertex-ai/docs/explainable-ai/getting-explanations) .", "guide": "Vertex AI"}