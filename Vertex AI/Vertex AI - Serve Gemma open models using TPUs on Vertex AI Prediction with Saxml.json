{"title": "Vertex AI - Serve Gemma open models using TPUs on Vertex AI Prediction with Saxml", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Serve Gemma open models using TPUs on Vertex AI Prediction with Saxml\n**    Preview     ** This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nThis guide shows you how to serve a Gemma open models large language model (LLM) using [Tensor Processing Units (TPUs)](/tpu/docs/intro-to-tpu) on Vertex AI Prediction with Saxml. In this guide, you download the 2B and 7B parameter instruction tuned Gemma models to Cloud Storage and deploy them on Vertex AI Prediction that runs Saxml on TPUs.\n", "content": "## Background\nBy serving Gemma using TPUs on Vertex AI Prediction with Saxml. You can take advantage of a managed AI solution that takes care of low level infrastructure and offers a cost effective way for serving LLMs. This section describes the key technologies used in this tutorial.\n### Gemma\nGemma is a set of openly available, lightweight, and generative artificial intelligence (AI) models released under an open license. These AI models are available to run in your applications, hardware, mobile devices, or hosted services. You can use the Gemma models for text generation, however you can also tune these models for specialized tasks.\nTo learn more, see the [Gemma documentation](https://ai.google.dev/gemma/docs) .\n### Saxml\n[Saxml](https://github.com/google/saxml) is an experimental system that serves [Paxml](https://github.com/google/paxml) , [JAX](https://github.com/google/jax) , and [PyTorch](https://github.com/google/jax) models for inference. For the sake of this tutorial we'll cover how to serve Gemma on TPUs that are more cost efficient for Saxml. Setup for GPUs is similar. Saxml offers scripts to build containers for [Vertex AI Prediction](https://github.com/google/saxml/tree/main/saxml/vertex) that we are going to use in this tutorial.\n### TPUs\nTPUs are Google's custom-developed application-specific integrated circuits (ASICs) used to accelerate data processing frameworks such as TensorFlow, PyTorch, and JAX.\nThis tutorial serves the Gemma 2B and Gemma 7B models. Vertex AI Prediction hosts these models on the following single-host TPU v5e node pools:\n- **Gemma 2B** : Hosted in a TPU v5e node pool with`1x1`topology that represents one TPU chip. The machine type for the nodes is`ct5lp-hightpu-1t`.\n- **Gemma 7B** : Hosted in a TPU v5e node pool with`2x2`topology that represents four TPU chips. The machine type for the nodes is`ct5lp-hightpu-4t`.## Before you begin\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) At the bottom of the Google Cloud console, a [Cloud Shell](/shell/docs/how-cloud-shell-works) session starts and displays a command-line prompt. Cloud Shell is a shell environment  with the Google Cloud CLI  already installed and with values already set for  your current project. It can take a few seconds for the session to initialize.\nThis tutorial assumes that you are using [Cloud Shell](/shell/docs) to interact with Google Cloud. If you want to use a different shell instead of Cloud Shell, then perform the following additional configuration:\n- [Install](/sdk/docs/install) the Google Cloud CLI.\n- To [initialize](/sdk/docs/initializing) the gcloud CLI, run the following command:```\ngcloud init\n```\n- Follow the Artifact Registry documentation to [Install Docker.](/artifact-registry/docs/docker/quickstart#before-you-begin) \n- Ensure that you have sufficient quotas for 5 TPU v5e chips for [ Vertex AI Prediction](/vertex-ai/docs/predictions/use-tpu#ensure-quota) .\n- Create a [Kaggle account](https://www.kaggle.com/) ,  if you don't already have one.## Get access to the model\nTo get access to the Gemma models for deployment to Vertex AI Prediction, you must sign in to the [Kaggle platform](https://www.kaggle.com/) , sign the license consent agreement, and get a Kaggle API token. In this tutorial, you use a Kubernetes Secret for the Kaggle credentials.\n### Sign the license consent agreement\nYou must sign the consent agreement to use Gemma. Follow these instructions:\n- Access the [model consent page](https://www.kaggle.com/models/google/gemma) on Kaggle.com.\n- Sign in to Kaggle if you haven't done so already.\n- Click **Request Access** .\n- In the **Choose Account for Consent** section, select **Verify via Kaggle\nAccount** to use your Kaggle account for consent.\n- Accept the model **Terms and Conditions** .\n### Generate an access token\nTo access the model through Kaggle, you need a [Kaggle API token](https://github.com/Kaggle/kaggle-api) .\nFollow these steps to generate a new token if you don't have one already:\n- In your browser, go to [Kaggle settings](https://www.kaggle.com/settings) .\n- Under the **API** section, click **Create New Token** .A file named `kaggle.json` is downloaded.\n### Upload the access token to Cloud Shell\nIn Cloud Shell, you can upload the Kaggle API token to your Google Cloud project:\n- In Cloud Shell, clickmore_vert **More** > **Upload** .\n- Select File and click **Choose Files** .\n- Open the`kaggle.json`file.\n- Click **Upload** .\n### Create the Cloud Storage bucket\nCreate Cloud Storage bucket to store the model checkpoints.\nIn Cloud Shell, run the following:\n```\ngcloud storage buckets create gs://CHECKPOINTS_BUCKET_NAME\n```\nReplace the with the name of the Cloud Storage bucket that stores the model checkpoints.\n### Copy model to Cloud Storage bucket\nIn Cloud Shell, run the following:\n```\npip install kaggle --break-system-packages# For Gemma 2Bmkdir -p /data/gemma_2b-itkaggle models instances versions download google/gemma/pax/2b-it/1 --untar -p /data/gemma_2b-itgsutil -m cp -R /data/gemma_2b-it/* gs://CHECKPOINTS_BUCKET_NAME/gemma_2b-it/# For Gemma 7Bmkdir -p /data/gemma_7b-itkaggle models instances versions download google/gemma/pax/7b-it/1 --untar -p /data/gemma_7b-itgsutil -m cp -R /data/gemma_7b-it/* gs://CHECKPOINTS_BUCKET_NAME/gemma_7b-it/\n```\n### Create an Artifact Registry repository\nCreate an Artifact Registry repository to store the container image that you will create in the next section.\nEnable the Artifact Registry API service for your project.\n```\ngcloud services enable artifactregistry.googleapis.com\n```\nRun the following command in your shell to create Artifact Registry repository:\n```\ngcloud artifacts repositories create saxml \\\u00a0--repository-format=docker \\\u00a0--location=LOCATION \\\u00a0--description=\"Saxml Docker repository\"\n```\nReplace with the region where Artifact Registry stores your container image. Later, you must create a Vertex AI model resource on a regional endpoint that matches this region, so choose [a regionwhere Vertex AI has a regionalendpoint](/vertex-ai/docs/general/locations#feature-availability) , such as `us-west1` for TPUs.\n### Push the container image to Artifact Registry\nPrebuilt Saxml container is available at `us-docker.pkg.dev/vertex-ai/prediction/sax-tpu:latest` . Copy it to your Artifact Registry. Configure Docker to access Artifact Registry. Then push your container image to your Artifact Registry repository.\n- To give your local Docker installation permission to push to Artifact Registry in your chosen region, run the following command in your shell:```\ngcloud auth configure-docker LOCATION-docker.pkg.dev\n```- Replacewith the region where you created your repository.\n- To copy the container image that you just to Artifact Registry, run the following command in your shell:```\ndocker tag us-docker.pkg.dev/vertex-ai/prediction/sax-tpu:latest LOCATION-docker.pkg.dev/PROJECT_ID/saxml/saxml-tpu:latest\n```\n- To push the container image that you just to Artifact Registry, run the following command in your shell:```\ndocker push LOCATION-docker.pkg.dev/PROJECT_ID/saxml/saxml-tpu:latest\n```Replace the following, as you did in the previous section:- : the region of your Artifact Registry repository.\n- : the ID of your [Google Cloudproject](/resource-manager/docs/creating-managing-projects#identifying_projects) \n## Deploying the model\n### Upload a model\nTo upload a `Model` resource that uses your Saxml container, run the following [gcloud ai models uploadcommand](/sdk/gcloud/reference/ai/models/upload) :\n```\ngcloud ai models upload \\\u00a0 --region=LOCATION \\\u00a0 --display-name=DEPLOYED_MODEL_NAME \\\u00a0 --container-image-uri=LOCATION-docker.pkg.dev/PROJECT_ID/saxml/saxml-tpu:latest \\\u00a0 --artifact-uri='gs://CHECKPOINTS_BUCKET_NAME/gemma_2b-it/' \\\u00a0 --container-args='--model_path=saxml.server.pax.lm.params.gemma.Gemma2BFP16' \\\u00a0 --container-args='--platform_chip=tpuv5e' \\\u00a0 --container-args='--platform_topology=2x2' \\\u00a0 --container-args='--ckpt_path_suffix=checkpoint_00000000' \\\u00a0 --container-ports=8502\n```\n```\ngcloud ai models upload \\\u00a0 --region=LOCATION \\\u00a0 --display-name=DEPLOYED_MODEL_NAME \\\u00a0 --container-image-uri=LOCATION-docker.pkg.dev/PROJECT_ID/saxml/saxml-tpu:latest \\\u00a0 --artifact-uri='gs://CHECKPOINTS_BUCKET_NAME/gemma_7b-it/' \\\u00a0 --container-args='--model_path=saxml.server.pax.lm.params.gemma.Gemma7BFP16' \\\u00a0 --container-args='--platform_chip=tpuv5e' \\\u00a0 --container-args='--platform_topology=2x2' \\\u00a0 --container-args='--ckpt_path_suffix=checkpoint_00000000' \\\u00a0 --container-ports=8502\n```\nReplace the following:\n- : the ID of your [Google Cloudproject](/resource-manager/docs/creating-managing-projects#identifying_projects) \n- : The region where you are using Vertex AI. Note that TPUs are only available in us-west1.\n- : A name for the`DeployedModel`. You can use the display name of the`Model`for the`DeployedModel`as well.\n### Create an endpoint\nYou must deploy the model to an endpoint before the model can be used to serve online predictions. If you are deploying a model to an existing endpoint, you can skip this step. The following example uses the [gcloud ai endpoints createcommand](/sdk/gcloud/reference/ai/endpoints/create) :\n```\ngcloud ai endpoints create \\\u00a0 --region=LOCATION \\\u00a0 --display-name=ENDPOINT_NAME\n```\nReplace the following:\n- : The region where you are using Vertex AI.\n- : The display name for the endpoint.\nThe Google Cloud CLI tool might take a few seconds to create the endpoint.\n### Deploy the model to endpoint\nAfter the endpoint is ready, deploy the model to the endpoint.\n```\nENDPOINT_ID=$(gcloud ai endpoints list \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--filter=display_name=ENDPOINT_NAME \\\u00a0 \u00a0--format=\"value(name)\")MODEL_ID=$(gcloud ai models list \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--filter=display_name=DEPLOYED_MODEL_NAME \\\u00a0 \u00a0--format=\"value(name)\")gcloud ai endpoints deploy-model $ENDPOINT_ID \\\u00a0 --region=LOCATION \\\u00a0 --model=$MODEL_ID \\\u00a0 --display-name=DEPLOYED_MODEL_NAME \\\u00a0 --machine-type=ct5lp-hightpu-4t \\\u00a0 --traffic-split=0=100\n```\nReplace the following:\n- : The region where you are using Vertex AI.\n- : The display name for the endpoint.\n- : A name for the`DeployedModel`. You can use the display name of the`Model`for the`DeployedModel`as well.\n`Gemma 2B` can be deployed on a smaller ct5lp-hightpu-1t machine, in such case you should specify `--platform_topology=1x1` when uploading model.\nThe Google Cloud CLI tool might take a few minutes to deploy the model to the endpoint. When the model is successfully deployed, this command prints the following output:\n```\n Deployed a model to the endpoint xxxxx. Id of the deployed model: xxxxx.\n```\n## Getting online predictions from the deployed model\nTo invoke the model through the Vertex AI Prediction endpoint, format the prediction request by using a [standard Inference Request JSON Object](https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#inference) .\nThe following example uses the [gcloud ai endpoints predictcommand](/sdk/gcloud/reference/ai/endpoints/predict) :\n```\nENDPOINT_ID=$(gcloud ai endpoints list \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--filter=display_name=ENDPOINT_NAME \\\u00a0 \u00a0--format=\"value(name)\")gcloud ai endpoints predict $ENDPOINT_ID \\\u00a0 --region=LOCATION \\\u00a0 --http-headers=Content-Type=application/json \\\u00a0 --json-request instances.json\n```\nReplace the following:\n- : The region where you are using Vertex AI.\n- : The display name for the endpoint.\n- has following format:`{\"instances\": [{\"text_batch\": \"<your prompt>\"},{...}]}`## Cleaning up\nTo avoid incurring further [Vertex AIcharges](/vertex-ai/pricing) and [Artifact Registrycharges](/artifact-registry/pricing) , delete the Google Cloud resources that you created during this tutorial:\n- To undeploy model from endpoint and delete the endpoint, run the following command in your shell:```\nENDPOINT_ID=$(gcloud ai endpoints list \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--filter=display_name=ENDPOINT_NAME \\\u00a0 \u00a0--format=\"value(name)\")DEPLOYED_MODEL_ID=$(gcloud ai endpoints describe $ENDPOINT_ID \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--format=\"value(deployedModels.id)\")gcloud ai endpoints undeploy-model $ENDPOINT_ID \\\u00a0 --region=LOCATION \\\u00a0 --deployed-model-id=$DEPLOYED_MODEL_IDgcloud ai endpoints delete $ENDPOINT_ID \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--quiet\n```Replace with the region where you created your model in a previous section.\n- To delete your model, run the following command in your shell:```\nMODEL_ID=$(gcloud ai models list \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--filter=display_name=DEPLOYED_MODEL_NAME \\\u00a0 \u00a0--format=\"value(name)\")gcloud ai models delete $MODEL_ID \\\u00a0 \u00a0--region=LOCATION \\\u00a0 \u00a0--quiet\n```Replace with the region where you created your model in a previous section.\n- To delete your Artifact Registry repository and the container image in it, run the following command in your shell:```\ngcloud artifacts repositories delete saxml \\\u00a0 --location=LOCATION \\\u00a0 --quiet\n```Replace with the region where you created your Artifact Registry repository in a previous section.## Limitations\n- On Vertex AI Prediction Cloud TPUs are supported only in`us-west1`. For more information, see [locations](/vertex-ai/docs/general/locations#region_considerations) .## What's next\n- Learn how to deploy other [Saxml models](https://github.com/google/saxml/tree/main/saxml/vertex) such as Llama2 and GPT-J.", "guide": "Vertex AI"}