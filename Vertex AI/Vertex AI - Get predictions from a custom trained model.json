{"title": "Vertex AI - Get predictions from a custom trained model", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Get predictions from a custom trained model\nA prediction is the output of a trained machine learning model. This page provides an overview of the workflow for getting predictions from your models on Vertex AI.\nVertex AI offers two methods for getting prediction:\n- **Online predictions** are synchronous requests made to a model [endpoint](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints) . Before sending a request, you must first deploy the [model](/vertex-ai/docs/reference/rest/v1/projects.locations.models) resource to an `endpoint` . This associates [computeresources](/vertex-ai/docs/predictions/configure-compute) with the model so that it can serve online predictions with low latency. Use online predictions when you are making requests in response to application input or in situations that require timely inference.\n- **Batchpredictions** are asynchronous requests. You request a [batchPredictionsJob](/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs) directly from the [model](/vertex-ai/docs/reference/rest/v1/projects.locations.models) resource without needing to deploy the model to an endpoint. Use batch predictions when you don't require an immediate response and want to process accumulated data by using a single request.", "content": "## Test your model locally\nBefore getting predictions, it's useful to deploy your model to a local endpoint during the development and testing phase. This lets you both iterate more quickly and test your model without deploying it to an online endpoint or incurring prediction costs. Local deployment is intended for local development and testing, not for production deployments.\nTo deploy a model locally, use the Vertex AI SDK for Python and deploy a [LocalModel](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.prediction.LocalModel) to a [LocalEndpoint](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.prediction.LocalEndpoint) . For a demonstration, see [thisnotebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/find_ideal_machine_type/find_ideal_machine_type.ipynb) .\nEven if your client is not written in Python, you can still use the Vertex AI SDK for Python to launch the container and server so that you can test requests from your client.\n## Get predictions from custom trained models\nTo get predictions, you must first [import yourmodel](/vertex-ai/docs/model-registry/import-model) . After it's imported, it becomes a [model](/vertex-ai/docs/reference/rest/v1/projects.locations.models) resource that is visible in [Vertex AI Model Registry](/vertex-ai/docs/model-registry/introduction) .\nThen, read the following documentation to learn how to get predictions:\n- [Get batch predictions](/vertex-ai/docs/predictions/get-batch-predictions) Or\n- [Deploy model to endpoint](/vertex-ai/docs/general/deployment) and [get online predictions](/vertex-ai/docs/predictions/get-online-predictions) .## What's next\n- Learn about [Compute resources forprediction](/vertex-ai/docs/predictions/configure-compute) .", "guide": "Vertex AI"}