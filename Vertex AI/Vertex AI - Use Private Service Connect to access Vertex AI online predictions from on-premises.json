{"title": "Vertex AI - Use Private Service Connect to access Vertex AI online predictions from on-premises", "url": "https://cloud.google.com/vertex-ai/docs/general/vertex-psc-googleapis", "abstract": "# Vertex AI - Use Private Service Connect to access Vertex AI online predictions from on-premises\nOn-premises hosts can reach a Vertex AI online prediction endpoint either through the public internet or privately through a hybrid networking architecture that uses Private Service Connect (PSC) over Cloud VPN or Cloud Interconnect. Both options offer SSL/TLS encryption. However, the private option offers much better performance and is therefore recommended for critical applications.\nIn this tutorial, you use High-Availability VPN (HA VPN) to access an online prediction endpoint both publicly, through Cloud NAT; and privately, between two Virtual Private Cloud networks that can serve as a basis for multi-cloud and on-premises private connectivity.\nThis tutorial is intended for enterprise network administrators, data scientists, and researchers who are familiar with Vertex AI, Virtual Private Cloud (VPC), the Google Cloud console, and the [Cloud Shell](/shell/docs/how-cloud-shell-works) . Familiarity with [Vertex AI Workbench](/vertex-ai/docs/workbench/introduction) is helpful but not required.\n **Note:** The Vertex AI online prediction endpoint that you create is a public endpoint. In a production environment, you would [use VPC Service Controls to create secure perimeters](/vertex-ai/docs/general/vpc-service-controls) to allow or deny access to Vertex AI and other Google APIs on the online prediction endpoint over the public internet. This tutorial does not cover using VPC Service Controls with Vertex AI.", "content": "## Objectives\n- Create two Virtual Private Cloud (VPC) networks, as shown in the preceding diagram:- One (`on-prem-vpc`) represents an on-premises network.\n- The other (`aiml-vpc`) is for building and deploying a Vertex AI online prediction model.\n- Deploy HA VPN gateways, Cloud VPN tunnels, and Cloud Routers to connect`aiml-vpc`and`on-prem-vpc`.\n- Build and deploy a Vertex AI online prediction model.\n- Create a Private Service Connect (PSC) endpoint to forward private online prediction requests to the deployed model.\n- Configure a Cloud Router custom route advertisement in`aiml-vpc`to announce routes for the Private Service Connect endpoint to`on-prem-vpc`.\n- Create two Compute Engine VM instances in`on-prem-vpc`to represent client applications:- One (`nat-client`) sends online prediction requests over the public internet (through Cloud NAT). This access method is indicated by a red arrow and the number **1** in the diagram.\n- The other (`private-client`) sends prediction requests privately over HA VPN. This access method is indicated by a green arrow and the number **2** .## CostsIn this document, you use the following billable components of Google Cloud:- [Artifact Registry](/artifact-registry/pricing) \n- [Cloud NAT](/nat/pricing) \n- [Cloud Storage](/storage/pricing) \n- [Cloud VPN](/network-connectivity/pricing#vpn-pricing) \n- [Compute Engine](/compute/all-pricing) \n- [Vertex AI](/vertex-ai/pricing) \n- [Virtual Private Cloud](/vpc/pricing) To generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- In the Google Cloud console, go to the project selector page. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- Select or create a Google Cloud project.\n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- Open [Cloud Shell](/shell/docs/launching-cloud-shell-editor) to execute the commands listed  in this tutorial. Cloud Shell is an interactive shell environment  for Google Cloud that lets you manage your projects and resources from  your web browser.\n- In the Cloud Shell, set the current project to your Google Cloud project ID and store the same project ID into the`projectid`shell variable:```\n projectid=\"PROJECT_ID\"\n gcloud config set project ${projectid}\n```Replacewith your project ID. If necessary, you can locate your project ID in the Google Cloud console. For more information, see [Find your project ID](/vertex-ai/docs/tutorials/tabular-bq-prediction/prerequisites#find-project-id) .\n- Grant roles to your Google Account. Run the following command once for each of the following   IAM roles: `roles/appengine.appViewer,  roles/artifactregistry.admin,  roles/compute.instanceAdmin.v1,  roles/compute.networkAdmin,  roles/compute.securityAdmin,  roles/dns.admin,  roles/iap.admin,  roles/iap.tunnelResourceAccessor,  roles/notebooks.admin,  roles/oauthconfig.editor,  roles/resourcemanager.projectIamAdmin,  roles/servicemanagement.quotaAdmin,  roles/iam.serviceAccountAdmin,  roles/iam.serviceAccountUser,  roles/servicedirectory.editor,  roles/storage.admin,  roles/aiplatform.user` ```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS\" --role=ROLE\n```- Replace``with your project ID.\n- Replace``with your email address.\n- Replace``with each individual role.\n- Enable the DNS,  Artifact Registry,  IAM,  Compute Engine,  Notebooks,  and Vertex AI APIs:```\ngcloud services enable dns.googleapis.com artifactregistry.googleapis.com iam.googleapis.com compute.googleapis.com notebooks.googleapis.com aiplatform.googleapis.com\n```## Create the VPC networksIn this section, you create two VPC networks: one for creating an online prediction model and deploying it to an endpoint, the other for private access to that endpoint. In each of the two VPC networks, you create a Cloud Router and Cloud NAT gateway. A Cloud NAT gateway provides outgoing connectivity for Compute Engine virtual machine (VM) instances without external IP addresses.\n### Create the VPC network for the online prediction endpoint (aiml-vpc)\n- Create the VPC network:```\ngcloud compute networks create aiml-vpc --project=$projectid --subnet-mode=custom\n```\n- Create a subnet named `workbench-subnet` , with a primary IPv4 range of `172.16.10.0/28` :```\ngcloud compute networks subnets create workbench-subnet --project=$projectid --range=172.16.10.0/28 --network=aiml-vpc --region=us-central1 --enable-private-ip-google-access\n```\n- Create a regional Cloud Router named `cloud-router-us-central1-aiml-nat` :```\ngcloud compute routers create cloud-router-us-central1-aiml-nat --network aiml-vpc --region us-central1\n```\n- Add a Cloud NAT gateway to the Cloud Router:```\ngcloud compute routers nats create cloud-nat-us-central1 --router=cloud-router-us-central1-aiml-nat --auto-allocate-nat-external-ips --nat-all-subnet-ip-ranges --region us-central1\n```\n### Create the \"on-premises\" VPC network (on-prem-vpc)\n- Create the VPC network:```\ngcloud compute networks create on-prem-vpc --project=$projectid --subnet-mode=custom\n```\n- Create a subnet named `nat-subnet` , with a primary IPv4 range of `192.168.10.0/28` :```\ngcloud compute networks subnets create nat-subnet --project=$projectid --range=192.168.10.0/28 --network=on-prem-vpc --region=us-central1\n```\n- Create a subnet named `private-ip-subnet` , with a primary IPv4 range of `192.168.20.0/28` :```\ngcloud compute networks subnets create private-ip-subnet --project=$projectid --range=192.168.20.0/28 --network=on-prem-vpc --region=us-central1\n```\n- Create a regional Cloud Router named `cloud-router-us-central1-on-prem-nat` :```\ngcloud compute routers create cloud-router-us-central1-on-prem-nat --network on-prem-vpc --region us-central1\n```\n- Add a Cloud NAT gateway to the Cloud Router:```\ngcloud compute routers nats create cloud-nat-us-central1 --router=cloud-router-us-central1-on-prem-nat --auto-allocate-nat-external-ips --nat-all-subnet-ip-ranges --region us-central1\n```\n## Create the Private Service Connect (PSC) endpointIn this section, you create the Private Service Connect (PSC) endpoint that the VM instances in the `on-prem-vpc` network use to access the online prediction endpoint through the Vertex AI API. The is an internal IP address in the `on-prem-vpc` network that can be directly accessed by clients in that network. This endpoint is created by deploying a forwarding rule that directs network traffic that matches the PSC endpoint's IP address to a bundle of Google APIs. The PSC endpoint's IP address ( `100.100.10.10` ) will be advertised from the `aiml-vpc-cloud-router-vpn` as a custom router advertisement to the on-premises network in a later step.- Reserve IP addresses for the PSC endpoint:```\ngcloud compute addresses create psc-ip \\\u00a0 \u00a0--global \\\u00a0 \u00a0--purpose=PRIVATE_SERVICE_CONNECT \\\u00a0 \u00a0--addresses=100.100.10.10 \\\u00a0 \u00a0--network=aiml-vpc\n```\n- Create the PSC endpoint:```\ngcloud compute forwarding-rules create pscvertex \\\u00a0--global \\\u00a0--network=aiml-vpc \\\u00a0--address=psc-ip \\\u00a0--target-google-apis-bundle=all-apis\n```\n- List the configured PSC endpoints and verify that the `pscvertex` endpoint was created:```\ngcloud compute forwarding-rules list \u00a0\\\u00a0--filter target=\"(all-apis OR vpc-sc)\" --global\n```\n- Get the details of the configured PSC endpoint and verify that the IP address is `100.100.10.10` :```\ngcloud compute forwarding-rules describe \\\u00a0pscvertex --global\n```\n## Configure hybrid connectivityIn this section, you create two (HA VPN) gateways that are connected to each other. Each gateway contains a Cloud Router and a pair of VPN tunnels.- Create the HA VPN gateway for the `aiml-vpc` VPC network:```\ngcloud compute vpn-gateways create aiml-vpn-gw \\\u00a0 \u00a0--network=aiml-vpc\\\u00a0 \u00a0--region=us-central1\n```\n- Create the HA VPN gateway for the `on-prem-vpc` VPC network:```\ngcloud compute vpn-gateways create on-prem-vpn-gw \\\u00a0 \u00a0--network=on-prem-vpc\\\u00a0 \u00a0--region=us-central1\n```\n- In the Google Cloud console, go to the **VPN** page. [Go to VPN](https://console.cloud.google.com/hybrid/vpn) \n- On the **VPN** page, click the **Cloud VPN Gateways** tab.\n- In the list of VPN gateways, verify that there are two gateways and that each one has two IP addresses.\n- In the Cloud Shell, create a Cloud Router for the `aiml-vpc` Virtual Private Cloud network:```\ngcloud compute routers create aiml-cr-us-central1 \\\u00a0 \u00a0--region=us-central1 \\\u00a0 \u00a0--network=aiml-vpc\\\u00a0 \u00a0--asn=65001\n```\n- Create a Cloud Router for the `on-prem-vpc` Virtual Private Cloud network:```\ngcloud compute routers create on-prem-cr-us-central1 \\\u00a0 \u00a0--region=us-central1 \\\u00a0 \u00a0--network=on-prem-vpc \\\u00a0 \u00a0--asn=65002\n```\n### Create the VPN tunnels for aiml-vpc\n- Create a VPN tunnel called `aiml-vpc-tunnel0` :```\ngcloud compute vpn-tunnels create aiml-vpc-tunnel0 \\\u00a0 \u00a0--peer-gcp-gateway on-prem-vpn-gw \\\u00a0 \u00a0--region us-central1 \\\u00a0 \u00a0--ike-version 2 \\\u00a0 \u00a0--shared-secret [ZzTLxKL8fmRykwNDfCvEFIjmlYLhMucH] \\\u00a0 \u00a0--router aiml-cr-us-central1 \\\u00a0 \u00a0--vpn-gateway aiml-vpn-gw \\\u00a0 \u00a0--interface 0\n```\n- Create a VPN tunnel called `aiml-vpc-tunnel1` :```\ngcloud compute vpn-tunnels create aiml-vpc-tunnel1 \\\u00a0 \u00a0--peer-gcp-gateway on-prem-vpn-gw \\\u00a0 \u00a0--region us-central1 \\\u00a0 \u00a0--ike-version 2 \\\u00a0 \u00a0--shared-secret [bcyPaboPl8fSkXRmvONGJzWTrc6tRqY5] \\\u00a0 \u00a0--router aiml-cr-us-central1 \\\u00a0 \u00a0--vpn-gateway aiml-vpn-gw \\\u00a0 \u00a0--interface 1\n```\n### Create the VPN tunnels for on-prem-vpc\n- Create a VPN tunnel called `on-prem-vpc-tunnel0` :```\ngcloud compute vpn-tunnels create on-prem-tunnel0 \\\u00a0 \u00a0--peer-gcp-gateway aiml-vpn-gw \\\u00a0 \u00a0--region us-central1 \\\u00a0 \u00a0--ike-version 2 \\\u00a0 \u00a0--shared-secret [ZzTLxKL8fmRykwNDfCvEFIjmlYLhMucH] \\\u00a0 \u00a0--router on-prem-cr-us-central1 \\\u00a0 \u00a0--vpn-gateway on-prem-vpn-gw \\\u00a0 \u00a0--interface 0\n```\n- Create a VPN tunnel called `on-prem-vpc-tunnel1` :```\ngcloud compute vpn-tunnels create on-prem-tunnel1 \\\u00a0 \u00a0--peer-gcp-gateway aiml-vpn-gw \\\u00a0 \u00a0--region us-central1 \\\u00a0 \u00a0--ike-version 2 \\\u00a0 \u00a0--shared-secret [bcyPaboPl8fSkXRmvONGJzWTrc6tRqY5] \\\u00a0 \u00a0--router on-prem-cr-us-central1 \\\u00a0 \u00a0--vpn-gateway on-prem-vpn-gw \\\u00a0 \u00a0--interface 1\n```\n- In the Google Cloud console, go to the **VPN** page. [Go to VPN](https://console.cloud.google.com/hybrid/vpn) \n- On the **VPN** page, click the **Cloud VPN Tunnels** tab.\n- In the list of VPN tunnels, verify that four VPN tunnels have been established.\n## Establish BGP sessionsCloud Router uses Border Gateway Protocol (BGP) to exchange routes between your VPC network (in this case, `aiml-vpc` ) and your on-premises network (represented by `on-prem-vpc` ). On Cloud Router, you configure an interface and a BGP peer for your on-premises router. The interface and BGP peer configuration together form a BGP session. In this section, you create two BGP sessions for `aiml-vpc` and two for `on-prem-vpc` .\n### Establish BGP sessions for aiml-vpc\n- In the Cloud Shell, create the first BGP interface:```\ngcloud compute routers add-interface aiml-cr-us-central1 \\\u00a0 \u00a0--interface-name if-tunnel0-to-onprem \\\u00a0 \u00a0--ip-address 169.254.1.1 \\\u00a0 \u00a0--mask-length 30 \\\u00a0 \u00a0--vpn-tunnel aiml-vpc-tunnel0 \\\u00a0 \u00a0--region us-central1\n```\n- Create the first BGP peer:```\ngcloud compute routers add-bgp-peer aiml-cr-us-central1 \\\u00a0 \u00a0--peer-name bgp-on-premises-tunnel0 \\\u00a0 \u00a0--interface if-tunnel1-to-onprem \\\u00a0 \u00a0--peer-ip-address 169.254.1.2 \\\u00a0 \u00a0--peer-asn 65002 \\\u00a0 \u00a0--region us-central1\n```\n- Create the second BGP interface:```\ngcloud compute routers add-interface aiml-cr-us-central1 \\\u00a0 \u00a0--interface-name if-tunnel1-to-onprem \\\u00a0 \u00a0--ip-address 169.254.2.1 \\\u00a0 \u00a0--mask-length 30 \\\u00a0 \u00a0--vpn-tunnel aiml-vpc-tunnel1 \\\u00a0 \u00a0--region us-central1\n```\n- Create the second BGP peer:```\ngcloud compute routers add-bgp-peer aiml-cr-us-central1 \\\u00a0 \u00a0--peer-name bgp-on-premises-tunnel1 \\\u00a0 \u00a0--interface if-tunnel2-to-onprem \\\u00a0 \u00a0--peer-ip-address 169.254.2.2 \\\u00a0 \u00a0--peer-asn 65002 \\\u00a0 \u00a0--region us-central1\n```\n### Establish BGP sessions for on-prem-vpc\n- Create the first BGP interface:```\ngcloud compute routers add-interface on-prem-cr-us-central1 \\\u00a0 \u00a0--interface-name if-tunnel0-to-aiml-vpc\\\u00a0 \u00a0--ip-address 169.254.1.2 \\\u00a0 \u00a0--mask-length 30 \\\u00a0 \u00a0--vpn-tunnel on-prem-tunnel0 \\\u00a0 \u00a0--region us-central1\n```\n- Create the first BGP peer:```\ngcloud compute routers add-bgp-peer on-prem-cr-us-central1 \\\u00a0 \u00a0--peer-name bgp-aiml-vpc-tunnel0 \\\u00a0 \u00a0--interface if-tunnel1-to-aiml-vpc\\\u00a0 \u00a0--peer-ip-address 169.254.1.1 \\\u00a0 \u00a0--peer-asn 65001 \\\u00a0 \u00a0--region us-central1\n```\n- Create the second BGP interface:```\ngcloud compute routers add-interface on-prem-cr-us-central1 \\\u00a0 \u00a0--interface-name if-tunnel1-to-aiml-vpc\\\u00a0 \u00a0--ip-address 169.254.2.2 \\\u00a0 \u00a0--mask-length 30 \\\u00a0 \u00a0--vpn-tunnel on-prem-tunnel1 \\\u00a0 \u00a0--region us-central1\n```\n- Create the second BGP peer:```\ngcloud compute routers add-bgp-peer on-prem-cr-us-central1 \\\u00a0 \u00a0--peer-name bgp-aiml-vpc-tunnel1\\\u00a0 \u00a0--interface if-tunnel2-to-aiml-vpc\\\u00a0 \u00a0--peer-ip-address 169.254.2.1 \\\u00a0 \u00a0--peer-asn 65001 \\\u00a0 \u00a0--region us-central1\n```\n **Note:** In the following section, the Cloud Router's default behavior is to advertise subnet routes between the VPCs over HA VPN. Later in the tutorial, we will modify the advertised routes to only advertise the PSC endpoint IP address from the `aiml-vpc` and the `private-ip-subnet` from the `on-prem-vpc` .\n### Validate BGP session creation\n- In the Google Cloud console, go to the **VPN** page. [Go to VPN](https://console.cloud.google.com/hybrid/vpn/list) \n- On the **VPN** page, click the **Cloud VPN Tunnels** tab.\n- In the list of VPN tunnels, you should now see that the value in the **BGP session status** column for each of the four tunnels has changed from **Configure BGP session** to **BGP established** . You may need to refresh the Google Cloud console browser tab to see the new values.\n### Validate that aiml-vpc has learned subnet routes over HA VPN\n- In the Google Cloud console, go to the **VPC networks** page. [Go to VPC networks](https://console.cloud.google.com/networking/networks/list) \n- In the list of VPC networks, click `aiml-vpc` .\n- Click the **Routes** tab.\n- Select **us-central1 (Iowa)** in the **Region** list and click **View** .\n- In the **Destination IP range** column, verify that the `aiml-vpc` VPC network has learned routes from the `on-prem-vpc` VPC networks's `nat-subnet` subnet ( `192.168.10.0/28` ) and `private-ip-subnet` ( `192.168.20.0/28` ) subnet.\n### Validate that on-prem-vpc has learned subnet routes over HA VPN\n- In the Google Cloud console, go to the **VPC networks** page. [Go to VPC networks](https://console.cloud.google.com/networking/networks/list) \n- In the list of VPC networks, click `on-prem-vpc` .\n- Click the **Routes** tab.\n- Select **us-central1 (Iowa)** in the **Region** list and click **View** .\n- In the **Destination IP range** column, verify that the `on-prem-vpc` VPC network has learned routes from the `aiml-vpc` VPC networks's `workbench-subnet` subnet ( `172.16.10.0/28` ).\n## Create custom route advertisements for aiml-vpcThe Private Service Connect endpoint IP address is not automatically advertised by the `aiml-cr-us-central1` Cloud Router because the subnet is not configured in the VPC network.\nTherefore, you will need to create a custom route advertisement from the `aiml-cr-us-central` Cloud Router for the endpoint IP Address 100.100.10.10 that will be advertised to the on-premises environment over BGP to the `on-prem-vpc` .- In the Google Cloud console, go to the **Cloud Routers** page. [Go to Cloud Routers](https://console.cloud.google.com/hybrid/routers/list) \n- In the Cloud Router list, click `aiml-cr-us-central1` .\n- On the **Router details** page, click edit **Edit** .\n- In the **Advertised routes** section, for **Routes** , select **Create custom routes** .\n- Click **Add a custom route** .\n- For **Source** , select **Custom IP range** .\n- For **IP address range** , enter `100.100.10.10` .\n- For **Description** , enter `Private Service Connect Endpoint IP` .\n- Click **Done** , and then click **Save** .\n### Validate that on-prem-vpc has learned the PSC Endpoint IP Address over HA VPN\n- In the Google Cloud console, go to the **VPC networks** page. [Go to VPC networks](https://console.cloud.google.com/networking/networks/list) \n- In the list of VPC networks, click `on-prem-vpc` .\n- Click the **Routes** tab.\n- Select **us-central1 (Iowa)** in the **Region** list and click **View** .\n- In the **Destination IP range** column, verify that the `on-prem-vpc` VPC network has learned the PSC endpoint's IP address ( `100.100.10.10` ).\n## Create custom route advertisements for on-prem-vpcThe `on-prem-vpc` Cloud Router advertises all subnets by default, but only the `private-ip-subnet` subnet is needed.\nIn the following section, update the route advertisements from the `on-prem-cr-us-central1` Cloud Router.- In the Google Cloud console, go to the **Cloud Routers** page. [Go to Cloud Routers](https://console.cloud.google.com/hybrid/routers/list) \n- In the Cloud Router list, click `on-prem-cr-us-central1` .\n- On the **Router details** page, click edit **Edit** .\n- In the **Advertised routes** section, for **Routes** , select **Create custom routes** .\n- If the **Advertise all subnets visible to the Cloud Router** checkbox is selected, clear it.\n- Click **Add a custom route** .\n- For **Source** , select **Custom IP range** .\n- For **IP address range** , enter `192.168.20.0/28` .\n- For **Description** , enter `Private Service Connect Endpoint IP subnet (private-ip-subnet)` .\n- Click **Done** , and then click **Save** .\n### Validate that aiml-vpc has learned the private-ip-subnet route from the on-prem-vpc\n- In the Google Cloud console, go to the **VPC networks** page. [Go to VPC networks](https://console.cloud.google.com/networking/networks/list) \n- In the list of VPC networks, click `aiml-vpc` .\n- Click the **Routes** tab.\n- Select **us-central1 (Iowa)** in the **Region** list and click **View** .\n- In the **Destination IP range** column, verify that the `aiml-vpc` VPC network has learned the `private-ip-subnet` route ( `192.168.20.0/28` ).\n## Create the test VM instances\n### Create a user-managed service accountIf you have applications that need to call Google Cloud APIs, Google recommends that you attach a [user-managed service account](/compute/docs/access/create-enable-service-accounts-for-instances) to the VM on which the application or workload is running. Accordingly, in this section you create a user-managed service account to be applied to the VM instances that you create later in this tutorial.- In the Cloud Shell, create the service account:```\ngcloud iam service-accounts create gce-vertex-sa \\\u00a0 \u00a0--description=\"service account for vertex\" \\\u00a0 \u00a0--display-name=\"gce-vertex-sa\"\n```\n- Assign the [Compute Instance Admin (v1) (roles/compute.instanceAdmin.v1)](/iam/docs/understanding-roles#compute.instanceAdmin.v1) IAM role to the service account:```\ngcloud projects add-iam-policy-binding $projectid --member=\"serviceAccount:gce-vertex-sa@$projectid.iam.gserviceaccount.com\" --role=\"roles/compute.instanceAdmin.v1\"\n```\n- Assign the [Vertex AI User (roles/aiplatform.user)](/iam/docs/understanding-roles#aiplatform.user) IAM role to the service account:```\ngcloud projects add-iam-policy-binding $projectid --member=\"serviceAccount:gce-vertex-sa@$projectid.iam.gserviceaccount.com\" --role=\"roles/aiplatform.user\"\n```\n### Create the test VM instancesIn this step you create test VM instances to validate different methods to reach Vertex AI APIs, specifically:- The`nat-client`instance uses Cloud NAT to resolve Vertex AI to access the Online Prediction endpoint over the public internet.\n- The`private-client`instance uses the Private Service Connect IP address`100.100.10.10`to access the online prediction endpoint over HA VPN.\nTo allow Identity-Aware Proxy (IAP) to connect to your VM instances, you create a firewall rule that:- Applies to all VM instances that you want to make accessible through IAP.\n- Allows TCP traffic through port 22 from the IP range`35.235.240.0/20`. This range contains all IP addresses that IAP uses for [TCP forwarding](/iap/docs/using-tcp-forwarding#gcloud) .\n- Create the `nat-client` VM instance:```\ngcloud compute instances create nat-client \\\u00a0 \u00a0--zone=us-central1-a \\\u00a0 \u00a0--image-family=debian-11 \\\u00a0 \u00a0--image-project=debian-cloud \\\u00a0 \u00a0--subnet=nat-subnet \\\u00a0 \u00a0--service-account=gce-vertex-sa@$projectid.iam.gserviceaccount.com \\\u00a0 \u00a0--scopes=https://www.googleapis.com/auth/cloud-platform \\\u00a0 \u00a0--no-address \\\u00a0 \u00a0--metadata startup-script=\"#! /bin/bash\u00a0 \u00a0 \u00a0 sudo apt-get update\u00a0 \u00a0 \u00a0 sudo apt-get install tcpdump dnsutils -y\"\n```\n- Create the `private-client` VM instance:```\ngcloud compute instances create private-client \\\u00a0 \u00a0--zone=us-central1-a \\\u00a0 \u00a0--image-family=debian-11 \\\u00a0 \u00a0--image-project=debian-cloud \\\u00a0 \u00a0--subnet=private-ip-subnet \\\u00a0 \u00a0--service-account=gce-vertex-sa@$projectid.iam.gserviceaccount.com \\\u00a0 \u00a0--scopes=https://www.googleapis.com/auth/cloud-platform \\\u00a0 \u00a0--no-address \\\u00a0 \u00a0--metadata startup-script=\"#! /bin/bash\u00a0 \u00a0 \u00a0 sudo apt-get update\u00a0 \u00a0 \u00a0 sudo apt-get install tcpdump dnsutils -y\"\n```\n- Create the IAP firewall rule:```\ngcloud compute firewall-rules create ssh-iap-on-prem-vpc \\\u00a0 \u00a0--network on-prem-vpc \\\u00a0 \u00a0--allow tcp:22 \\\u00a0 \u00a0--source-ranges=35.235.240.0/20\n```\n## Create a user-managed notebooks instance\nVertex AI Workbench user-managed notebooks is [deprecated](/vertex-ai/docs/deprecations) . On January 30, 2025, support for  user-managed notebooks will end and the ability to create user-managed notebooks instances  will be removed. Existing instances will continue to function  but patches, updates, and upgrades won't be available. To continue using  Vertex AI Workbench, we recommend that you [migrate your user-managed notebooks instances to Vertex AI Workbench instances](/vertex-ai/docs/workbench/user-managed/migrate-to-instances) .\n### Create a user-managed service accountWhen you create a Vertex AI Workbench user-managed notebooks instance, Google strongly recommends that you specify a user-managed service account instead of using the Compute Engine default service account. The Compute Engine default service account (and thus anyone you specify as an instance user) is granted the Editor role ( `roles/editor` ) on your Google Cloud project. You can disable this behavior by [disabling automatic role grants for default serviceaccounts](/resource-manager/docs/organization-policy/restricting-service-accounts#disable_service_account_default_grants) .- In the Cloud Shell, create a service account named `user-managed-notebook-sa` :```\ngcloud iam service-accounts create user-managed-notebook-sa \\\u00a0 \u00a0--display-name=\"user-managed-notebook-sa\"\n```\n- Assign the [Storage Admin (roles/storage.admin)](https://cloud.google.com/iam/docs/understanding-roles#storage.admin) IAM role to the service account:```\ngcloud projects add-iam-policy-binding $projectid --member=\"serviceAccount:user-managed-notebook-sa@$projectid.iam.gserviceaccount.com\" --role=\"roles/storage.admin\"\n```\n- Assign the [Vertex AI User (roles/aiplatform.user)](/iam/docs/understanding-roles#aiplatform.user) IAM role to the service account:```\ngcloud projects add-iam-policy-binding $projectid --member=\"serviceAccount:user-managed-notebook-sa@$projectid.iam.gserviceaccount.com\" --role=\"roles/aiplatform.user\"\n```\n- Assign the [Artifact Registry Administrator](https://cloud.google.com/iam/docs/understanding-roles#artifactregistry.admin) IAM role to the service account:```\ngcloud projects add-iam-policy-binding $projectid --member=\"serviceAccount:user-managed-notebook-sa@$projectid.iam.gserviceaccount.com\" --role=\"roles/artifactregistry.admin\"\n```\n### Create the user-managed notebooks instanceCreate a user-managed notebooks instance, specifying the `user-managed-notebook-sa` service account.- Create the user-managed notebooks instance:```\ngcloud notebooks instances create workbench-tutorial \\\u00a0 \u00a0--vm-image-project=deeplearning-platform-release \\\u00a0 \u00a0--vm-image-family=common-cpu-notebooks \\\u00a0 \u00a0--machine-type=n1-standard-4 \\\u00a0 \u00a0--location=us-central1-a \\\u00a0 \u00a0--subnet-region=us-central1 \\\u00a0 \u00a0--subnet=workbench-subnet \\\u00a0 \u00a0--no-public-ip \\\u00a0 \u00a0--service-account=user-managed-notebook-sa@$projectid.iam.gserviceaccount.com\n```\n## Create and deploy an online prediction model\n### Prepare your environment\n- In the Google Cloud console, go to the **User-managed notebooks** tab in the **Vertex AI Workbench** page. [Go to User-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/user-managed) \n- Next to your user-managed notebooks instance's name, click **Open JupyterLab** .Your user-managed notebooks instance opens JupyterLab.In the rest of this section, up to and including model deployment, you'll be working in Jupyterlab, not the Google Cloud console or the Cloud Shell.\n- Select **File\u00a0> New\u00a0> Terminal** .\n- In the JupyterLab terminal (not the Cloud Shell), define an environment variable for your project. Replace with your project ID:```\nPROJECT_ID=PROJECT_ID\n```\n- Create a new directory called `cpr-codelab` and `cd` into it (still in the JupyterLab terminal):```\nmkdir cpr-codelabcd cpr-codelab\n```\n- In the folder **File Browser** , double-click the new `cpr-codelab` folder.If this folder doesn't appear in the file browser, refresh the Google Cloud console browser tab, and try again.\n- Select **File\u00a0> New\u00a0> Notebook** .\n- From the **Select Kernel** menu, select **Python 3** and click **Select** .\n- Rename your new notebook file as follows:In the folder **File Browser** , right-click the `Untitled.ipynb` file icon and enter `task.ipynb` .Your `cpr-codelab` directory should now look like this:```\n+ cpr-codelab/\u00a0 \u00a0+ task.ipynb\n```In the following steps, you create your model in the Jupyterlab notebook by creating new notebook cells, pasting code into them, and running the cells.\n- Install dependencies as follows.- When you open your new notebook, there is a default code cell where you can enter code. It looks like `[ ]:` followed by a text field. That text field is where you paste your code.Paste the following code into the cell and click play_arrow **Run the selected cells and advance** to create a `requirements.txt` file to be used as input to the following step:```\n%%writefile requirements.txtfastapiuvicorn==0.17.6joblib~=1.1.1numpy>=1.17.3, <1.24.0scikit-learn~=1.0.0pandasgoogle-cloud-storage>=2.2.1,<3.0.0devgoogle-cloud-aiplatform[prediction]>=1.18.2\n```\n- In this step and each of the following ones, add a code cell by clicking add **Insert a cell below** , paste the code into the cell, and then click play_arrow **Run the selected cells and advance** .Use `Pip` to install dependencies in the notebooks instance:```\n!pip install -U --user -r requirements.txt\n```\n- When installation is complete, select **Kernel\u00a0> Restart kernel** to restart the kernel and ensure that the library is available for import.\n- Paste the following code into a new notebook cell to create the directories to store the model and preprocessing artifacts:```\nUSER_SRC_DIR = \"src_dir\"!mkdir $USER_SRC_DIR!mkdir model_artifacts# copy the requirements to the source dir!cp requirements.txt $USER_SRC_DIR/requirements.txt\n```\nIn the folder **File Browser** , your `cpr-codelab` directory structure should now look like this:```\n+ cpr-codelab/\u00a0 + model_artifacts/\u00a0 + src_dir/\u00a0 \u00a0 \u00a0+ requirements.txt\u00a0 + requirements.txt\u00a0 + task.ipynb\n```\n### Train the modelContinue adding code cells to the `task.ipynb` notebook, and paste in and run the following code in each new cell:- Import the libraries:```\nimport seaborn as snsimport numpy as npimport pandas as pdfrom sklearn import preprocessingfrom sklearn.ensemble import RandomForestRegressorfrom sklearn.pipeline import make_pipelinefrom sklearn.compose import make_column_transformerimport joblibimport logging# set logging to see the docker container logslogging.basicConfig(level=logging.INFO)\n```\n- Define the following variables, replacing with your project ID:```\nREGION = \"us-central1\"MODEL_ARTIFACT_DIR = \"sklearn-model-artifacts\"REPOSITORY = \"diamonds\"IMAGE = \"sklearn-image\"MODEL_DISPLAY_NAME = \"diamonds-cpr\"PROJECT_ID = \"PROJECT_ID\"BUCKET_NAME = \"gs://PROJECT_ID-cpr-bucket\"\n```\n- Create a Cloud Storage bucket:```\ngsutil mb -l us-central1 $BUCKET_NAME\n```\n- Load the data from the seaborn library and then create two data frames, one with the features and the other with the label:```\ndata = sns.load_dataset('diamonds', cache=True, data_home=None)label = 'price'y_train = data['price']x_train = data.drop(columns=['price'])\n```\n- Look at the training data and verify that each row represents a diamond.```\nx_train.head()\n```\n- Look at the labels, which are the corresponding prices.```\ny_train.head()\n```\n- Define a sklearn column transform to [one hot encode](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html#sklearn.compose.make_column_transformer) the categorical features and scale the numerical features:```\ncolumn_transform = make_column_transformer(\u00a0 \u00a0(preprocessing.OneHotEncoder(sparse=False), [1,2,3]),\u00a0 \u00a0(preprocessing.StandardScaler(), [0,4,5,6,7,8]))\n```\n- Define the random forest model:```\nregr = RandomForestRegressor(max_depth=10, random_state=0)\n```\n- Make a [sklearn pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) . This pipeline takes input data, encodes and scales it, and passes it to the model.```\nmy_pipeline = make_pipeline(column_transform, regr)\n```\n- Train the model:```\nmy_pipeline.fit(x_train, y_train)\n```\n- Call the predict method on the model, passing in a test sample.```\nmy_pipeline.predict([[0.23, 'Ideal', 'E', 'SI2', 61.5, 55.0, 3.95, 3.98, 2.43]])\n```You may see warnings like `\"X does not have valid feature names, but\"` , but you can ignore them.\n- Save the pipeline to the `model_artifacts` directory and copy it to your Cloud Storage bucket:```\njoblib.dump(my_pipeline, 'model_artifacts/model.joblib')!gsutil cp model_artifacts/model.joblib {BUCKET_NAME}/{MODEL_ARTIFACT_DIR}/\n```\n### Save a preprocessing artifact\n- Create a preprocessing artifact. This artifact will be loaded into the custom container when the model server starts up. Your preprocessing artifact can be of almost any form (such as a pickle file), but in this case you'll write a dictionary to a JSON file:```\nclarity_dict={\"Flawless\": \"FL\",\u00a0 \u00a0\"Internally Flawless\": \"IF\",\u00a0 \u00a0\"Very Very Slightly Included\": \"VVS1\",\u00a0 \u00a0\"Very Slightly Included\": \"VS2\",\u00a0 \u00a0\"Slightly Included\": \"S12\",\u00a0 \u00a0\"Included\": \"I3\"}\n```\n### Build a custom serving container using the CPR model server\n- The `clarity` feature in our training data was always in the abbreviated form (ie \"FL\" instead of \"Flawless\"). At serving time, we want to check that the data for this feature is also abbreviated. This is because our model knows how to one hot encode \"FL\" but not \"Flawless\". You'll write this custom preprocessing logic later. But for now, just save this lookup table to a JSON file and then write it to your Cloud Storage bucket:```\nimport jsonwith open(\"model_artifacts/preprocessor.json\", \"w\") as f:\u00a0 \u00a0json.dump(clarity_dict, f)!gsutil cp model_artifacts/preprocessor.json {BUCKET_NAME}/{MODEL_ARTIFACT_DIR}/\n```In the folder **File Browser** , your directory structure should now look like this:```\n+ cpr-codelab/+ model_artifacts/\u00a0 \u00a0+ model.joblib\u00a0 \u00a0+ preprocessor.json+ src_dir/\u00a0 \u00a0+ requirements.txt+ requirements.txt+ task.ipynb\n```\n- In your notebook, paste in and run the following code below to subclass the SklearnPredictor and write it to a Python file in the `src_dir/` . Note that in this example we are only customizing the load, preprocess, and postprocess methods, and not the predict method.```\n%%writefile $USER_SRC_DIR/predictor.pyimport joblibimport numpy as npimport jsonfrom google.cloud import storagefrom google.cloud.aiplatform.prediction.sklearn.predictor import SklearnPredictorclass CprPredictor(SklearnPredictor):\u00a0def __init__(self):\u00a0 \u00a0 \u00a0return\u00a0def load(self, artifacts_uri: str) -> None:\u00a0 \u00a0 \u00a0\"\"\"Loads the sklearn pipeline and preprocessing artifact.\"\"\"\u00a0 \u00a0 \u00a0super().load(artifacts_uri)\u00a0 \u00a0 \u00a0# open preprocessing artifact\u00a0 \u00a0 \u00a0with open(\"preprocessor.json\", \"rb\") as f:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0self._preprocessor = json.load(f)\u00a0def preprocess(self, prediction_input: np.ndarray) -> np.ndarray:\u00a0 \u00a0 \u00a0\"\"\"Performs preprocessing by checking if clarity feature is in abbreviated form.\"\"\"\u00a0 \u00a0 \u00a0inputs = super().preprocess(prediction_input)\u00a0 \u00a0 \u00a0for sample in inputs:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0if sample[3] not in self._preprocessor.values():\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0sample[3] = self._preprocessor[sample[3]]\u00a0 \u00a0 \u00a0return inputs\u00a0def postprocess(self, prediction_results: np.ndarray) -> dict:\u00a0 \u00a0 \u00a0\"\"\"Performs postprocessing by rounding predictions and converting to str.\"\"\"\u00a0 \u00a0 \u00a0return {\"predictions\": [f\"${value}\" for value in np.round(prediction_results)]}\n```\n- Use the Vertex AI SDK for Python to build the image using custom prediction routines. The Dockerfile is generated and an image is built for you.```\nfrom google.cloud import aiplatformaiplatform.init(project=PROJECT_ID, location=REGION)import osfrom google.cloud.aiplatform.prediction import LocalModelfrom src_dir.predictor import CprPredictor \u00a0# Should be path of variable $USER_SRC_DIRlocal_model = LocalModel.build_cpr_model(\u00a0 \u00a0USER_SRC_DIR,\u00a0 \u00a0f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}\",\u00a0 \u00a0predictor=CprPredictor,\u00a0 \u00a0requirements_path=os.path.join(USER_SRC_DIR, \"requirements.txt\"),)\n```\n- Write a test file with two samples for prediction. One of the instances has the abbreviated clarity name, but the other needs to be converted first.```\nimport jsonsample = {\"instances\": [\u00a0 \u00a0[0.23, 'Ideal', 'E', 'VS2', 61.5, 55.0, 3.95, 3.98, 2.43],\u00a0 \u00a0[0.29, 'Premium', 'J', 'Internally Flawless', 52.5, 49.0, 4.00, 2.13, 3.11]]}with open('instances.json', 'w') as fp:\u00a0 \u00a0json.dump(sample, fp)\n```\n- Test the container locally by deploying a local model.```\nwith local_model.deploy_to_local_endpoint(\u00a0 \u00a0artifact_uri = 'model_artifacts/', # local path to artifacts) as local_endpoint:\u00a0 \u00a0predict_response = local_endpoint.predict(\u00a0 \u00a0 \u00a0 request_file='instances.json',\u00a0 \u00a0 \u00a0 headers={\"Content-Type\": \"application/json\"},\u00a0 \u00a0)\u00a0 \u00a0health_check_response = local_endpoint.run_health_check()\n```\n- You can see the prediction results with:```\npredict_response.content\n```\n### Deploy the model to the online prediction model endpointNow that you've tested the container locally, it's time to push the image to Artifact Registry and upload the model to Vertex AI Model Registry.- Configure Docker to access Artifact Registry.```\n!gcloud artifacts repositories create {REPOSITORY} --repository-format=docker \\--location=us-central1 --description=\"Docker repository\"!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet\n```\n- Push the image.```\nlocal_model.push_image()\n```\n- Upload the model.```\nmodel = aiplatform.Model.upload(local_model = local_model,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 display_name=MODEL_DISPLAY_NAME,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 artifact_uri=f\"{BUCKET_NAME}/{MODEL_ARTIFACT_DIR}\",)\n```\n- Deploy the model:```\nendpoint = model.deploy(machine_type=\"n1-standard-2\")\n```Wait until your model deploys before you continue to the next step. Expect deployment to take at least 10 to 15 minutes.\n- Test the deployed model by getting a prediction:```\nendpoint.predict(instances=[[0.23, 'Ideal', 'E', 'VS2', 61.5, 55.0, 3.95, 3.98, 2.43]])\n```\n## Validate public internet access to Vertex AI APIsIn this section, you log into the `nat-client` VM instance in one Cloud Shell session tab and use another session tab to validate connectivity to Vertex AI APIs by running the `dig` and `tcpdump` commands against the domain `us-central1-aiplatform.googleapis.com` .\n **Tip:** To keep track of the four Cloud Shell session tabs used in the remainder of this tutorial, you can [adjust your Cloud Shell terminal settings](https://cloud.google.com/shell/docs/use-cloud-shell-terminal) to customize the tab titles.- In the Cloud Shell (Tab One), run the following commands, replacing with your project ID:```\nprojectid=PROJECT_IDgcloud config set project ${projectid}\n```\n- Log into the `nat-client` VM instance using IAP:```\ngcloud compute ssh nat-client --project=$projectid --zone=us-central1-a --tunnel-through-iap\n```\n- Run the `dig` command:```\ndig us-central1-aiplatform.googleapis.com\n```\n- From the `nat-client` VM (Tab One), run the following command to validate DNS resolution when you send an online prediction request to the endpoint.```\n\u00a0sudo tcpdump -i any port 53 -n\n```\n- Open a new Cloud Shell session (Tab Two) by clicking add **open a new tab** in Cloud Shell.\n- In the new Cloud Shell session (Tab Two), run the following commands, replacing with your project ID:```\nprojectid=PROJECT_IDgcloud config set project ${projectid}\n```\n- Log into the `nat-client` VM instance:```\ngcloud compute ssh --zone \"us-central1-a\" \"nat-client\" --project \"$projectid\"\n```\n- From the `nat-client` VM (Tab Two), use a text editor such as [vim](https://www.cs.cmu.edu/%7E15131/f17/topics/vim/vim-cheatsheet.pdf) or [nano](https://www.nano-editor.org/dist/latest/cheatsheet.html) to create an `instances.json` file. You need to prepend `sudo` in order to have permission to write to the file, for example:```\nsudo vim instances.json\n```\n- Add the following data string to the file:```\n{\"instances\": [\u00a0 \u00a0[0.23, 'Ideal', 'E', 'VS2', 61.5, 55.0, 3.95, 3.98, 2.43],\u00a0 \u00a0[0.29, 'Premium', 'J', 'Internally Flawless', 52.5, 49.0, 4.00, 2.13, 3.11]]}\n```\n- Save the file as follows:- If you're using`vim`, press the`Esc`key, and then type`:wq`to save the file and exit.\n- If you're using`nano`, type`Control+O`and press`Enter`to save the file, and then type`Control+X`to exit.\n- Locate the online prediction endpoint ID for the PSC endpoint:- In the Google Cloud console, in the Vertex AI section, go to the **Endpoints** tab in the **Online prediction** page. [Go to Endpoints](https://console.cloud.google.com/vertex-ai/endpoints) \n- Find the row of the endpoint that you created, named `diamonds-cpr_endpoint` .\n- Locate the 19-digit endpoint ID in the **ID** column and copy it.\n- In the Cloud Shell, from the `nat-client` VM (Tab Two), run the following commands, replacing with your project ID and with the PSC endpoint ID:```\nprojectid=PROJECT_IDgcloud config set project ${projectid}ENDPOINT_ID=ENDPOINT_ID\n```\n- From the `nat-client` VM (Tab Two), run the following command to send an online prediction request:```\ncurl -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" https://us-central1-aiplatform.googleapis.com/v1/projects/${projectid}/locations/us-central1/endpoints/${ENDPOINT_ID}:predict -d @instances.json\n```\nNow that you've run the prediction, you'll see that the `tcpdump` results (Tab One) show the `nat-client` VM instance ( `192.168.10.2` ) performing a Cloud DNS query to the local DNS server ( `169.254.169.254` ) for the Vertex AI API domain ( `us-central1-aiplatform.googleapis.com` ). The DNS query returns public Virtual IP Addresses (VIPs) for Vertex AI APIs.## Validate private access to Vertex AI APIsIn this section, you log into the `private-client` VM instance using Identity-Aware Proxy in a new Cloud Shell session (Tab Three), and then you validate connectivity to Vertex AI APIs by running the `dig` command against the Vertex AI domain ( `us-central1-aiplatform.googleapis.com` ).- Open a new Cloud Shell session (Tab Three) by clicking add **open a new tab** in Cloud Shell. This is Tab Three.\n- In the new Cloud Shell session (Tab Three), run the following commands, replacing with your project ID:```\nprojectid=PROJECT_IDgcloud config set project ${projectid}\n```\n- Log into the `private-client` VM instance using IAP:```\ngcloud compute ssh private-client --project=$projectid --zone=us-central1-a --tunnel-through-iap\n```\n- Run the `dig` command:```\ndig us-central1-aiplatform.googleapis.com\n```\n- In the `private-client` VM instance (Tab Three), use a text editor such as `vim` or `nano` to add the following line to the `/etc/hosts` file:```\n100.100.10.10 us-central1-aiplatform.googleapis.com\n```This line assigns the PSC endpoint's IP address ( `100.100.10.10` ) to the fully qualified domain name for the Vertex AI Google API ( `us-central1-aiplatform.googleapis.com` ). The edited file should look like this:```\n127.0.0.1 \u00a0 \u00a0 \u00a0 localhost::1 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 localhost ip6-localhost ip6-loopbackff02::1 \u00a0 \u00a0 \u00a0 \u00a0 ip6-allnodesff02::2 \u00a0 \u00a0 \u00a0 \u00a0 ip6-allrouters100.100.10.10 us-central1-aiplatform.googleapis.com # Added by you192.168.20.2 private-client.c.$projectid.internal private-client \u00a0# Added by Google169.254.169.254 metadata.google.internal \u00a0# Added by Google\n```\n- From the `private-client` VM (Tab Three), ping the Vertex AI endpoint and `Control+C` to exit when you see the output:```\nping us-central1-aiplatform.googleapis.com\n```The `ping` command should return the following output containing the PSC endpoint IP address:```\nPING us-central1-aiplatform.googleapis.com (100.100.10.10) 56(84) bytes of data.\n```\n- From the `private-client` VM (Tab Three), use `tcpdump` to run the following command to validate DNS resolution and IP data path when you send an online prediction request to the endpoint:```\n\u00a0sudo tcpdump -i any port 53 -n or host 100.100.10.10\n```\n- Open a new Cloud Shell session (Tab Four) by clicking add **open a new tab** in Cloud Shell.\n- In the new Cloud Shell session (Tab Four), run the following commands, replacing with your project ID:```\nprojectid=PROJECT_IDgcloud config set project ${projectid}\n```\n- In Tab Four, log into the `private-client` instance:```\ngcloud compute ssh --zone \"us-central1-a\" \"private-client\" --project \"$projectid\"\n```\n- From the `private-client` VM (Tab Four), using a text editor such as `vim` or `nano` , create an `instances.json` file containing the following data string:```\n{\"instances\": [\u00a0 \u00a0[0.23, 'Ideal', 'E', 'VS2', 61.5, 55.0, 3.95, 3.98, 2.43],\u00a0 \u00a0[0.29, 'Premium', 'J', 'Internally Flawless', 52.5, 49.0, 4.00, 2.13, 3.11]]}\n```\n- From the `private-client` VM (Tab Four), run the following commands, replacing with your project name and with the PSC endpoint ID:```\nprojectid=PROJECT_IDecho $projectidENDPOINT_ID=ENDPOINT_ID\n```\n- From the `private-client` VM (Tab Four), run the following command to send an online prediction request:```\ncurl -v -X POST -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\" https://us-central1-aiplatform.googleapis.com/v1/projects/${projectid}/locations/us-central1/endpoints/${ENDPOINT_ID}:predict -d @instances.json\n```\n- From the `private-client` VM in Cloud Shell (Tab Three), verify that the PSC endpoint IP address ( `100.100.10.10` ) was used to access Vertex AI APIs.From the `private-client` `tcpdump` terminal in Cloud Shell Tab Three, you can see that a DNS lookup to `us-central1-aiplatform.googleapis.com` isn't needed, because the line that you added to the `/etc/hosts` file takes precedence, and the PSC IP address `100.100.10.10` is used in the data path.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this tutorial, either [delete the project](/resource-manager/docs/creating-managing-projects#shutting_down_projects) that contains the resources, or keep the project and delete the individual resources.\nYou can delete the individual resources in the project as follows:- Delete the user-managed notebooks instance as follows:- In the Google Cloud console, in the **Vertex AI** section, go to the **user-managed notebooks** tab in the **Workbench** page. . [Go to User-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/user-managed) \n- Select the `workbench-tutorial` user-managed notebooks instance and click delete **Delete** .\n- Delete the container image as follows:- In the Google Cloud console, go to the **Artifact Registry** page. [Go to Artifact Registry](https://console.cloud.google.com/artifacts) \n- Select the `diamonds` Docker container, and click delete **Delete** .\n- Delete the storage bucket as follows:- In the Google Cloud console, go to the **Cloud Storage** page. [Go to Cloud Storage](https://console.cloud.google.com/storage) \n- Select your storage bucket, and click delete **Delete** .\n- Undeploy the model from the endpoint as follows:- In the Google Cloud console, in the **Vertex AI** section, go to the **Endpoints** page. [Go to Endpoints](https://console.cloud.google.com/vertex-ai/endpoints) \n- Click `diamonds-cpr_endpoint` to go to the endpoint details page.\n- On the row for your model, `diamonds-cpr` , click **Undeploy modeldelete** .\n- In the **Undeploy model from endpoint** dialog, click **Undeploy** .\n- Delete the model as follows:- In the Google Cloud console, in the **Vertex AI** section, go to the **Model Registry** page. [Go to Model Registry](https://console.cloud.google.com/vertex-ai/models) \n- Select the `diamonds-cpr` model.\n- To delete the model, click more_vert **Actions** , and then click **Delete model** .\n- Delete the online prediction endpoint as follows:- In the Google Cloud console, in the **Vertex AI** section, go to the **Online prediction** page. [Go to Online prediction](https://console.cloud.google.com/vertex-ai/online-prediction/endpoints) \n- Select the `diamonds-cpr_endpoint` endpoint.\n- To delete the endpoint, click more_vert **Actions** , and then click **Delete endpoint** .\n- In the Cloud Shell, delete the remaining resources by executing the following commands. [Go to Cloud Shell](https://ssh.cloud.google.com/cloudshell/editor) ```\nprojectid=PROJECT_IDgcloud config set project ${projectid}\n``````\ngcloud compute forwarding-rules delete pscvertex --global --quiet\n``````\ngcloud compute addresses delete psc-ip --global --quiet\n``````\ngcloud compute networks subnets delete workbench-subnet --region=us-central1 --quiet \n``````\ngcloud compute vpn-tunnels delete aiml-vpc-tunnel0 aiml-vpc-tunnel1 on-prem-tunnel0 on-prem-tunnel1 --region=us-central1 --quiet\n``````\ngcloud compute vpn-gateways delete aiml-vpn-gw on-prem-vpn-gw --region=us-central1 --quiet\n``````\ngcloud compute routers delete aiml-cr-us-central1 cloud-router-us-central1-aiml-nat --region=us-central1 --quiet\n``````\ngcloud compute routers delete cloud-router-us-central1-on-prem-nat on-prem-cr-us-central1 --region=us-central1 --quiet\n``````\ngcloud compute instances delete nat-client private-client --zone=us-central1-a --quiet\n``````\ngcloud compute firewall-rules delete ssh-iap-on-prem-vpc --quiet\n``````\ngcloud compute networks subnets delete nat-subnet \u00a0private-ip-subnet --region=us-central1 --quiet\n``````\ngcloud compute networks delete on-prem-vpc --quiet\n``````\ngcloud compute networks delete aiml-vpc --quiet\n```\n## What's next\n- Learn about [enterprise networking options for accessing Vertex AI endpoints and services](/vertex-ai/docs/general/netsec-overview) \n- Learn [how Private Service Connect works](/vpc/docs/private-service-connect-architecture) and why it offers significant performance benefits.\n- Learn how to [use VPC Service Controls to create secure perimeters](/vertex-ai/docs/general/vpc-service-controls) to allow or deny access to Vertex AI and other Google APIs on your online prediction endpoint.\n- Learn how and why to [use a DNS forwarding zone](/dns/docs/zones/zones-overview#forwarding_zones) instead of updating the`/etc/hosts`file in large scale and [production environments](https://codelabs.developers.google.com/codelabs/vertex-psc-googleapis#17) .", "guide": "Vertex AI"}