{"title": "Vertex AI - Train a text entity extraction model", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Train a text entity extraction model\nThis page shows you how to train an AutoML entity extraction model from a text dataset using either the Google Cloud console or the Vertex AI API.\n", "content": "## Before you begin\nBefore you can train a text entity extraction model, you must complete the following:\n- [Prepare your training data](/vertex-ai/docs/text-data/entity-extraction/prepare-data) \n- [Create a Vertex AI dataset](/vertex-ai/docs/text-data/entity-extraction/create-dataset) ## Train an AutoML model\n- In the Google Cloud console, in the Vertex AI section, go to the **Datasets** page. [Go to the Datasets page](https://console.cloud.google.com/vertex-ai/datasets) \n- Click the name of the dataset you want to use to train your model to open its details page.\n- Select the annotation set you want to use for this model.\n- Click **Train new model** . **Note:** You can type [model.new](https://model.new) into a browser to go directly to the model creation page.\n- For the training method, select radio_button_checked **AutoML** .\n- Click **Continue** .\n- Enter a name for the model.\n- If you want manually set how your training data is split, expand **Advanced options** and select a data split option. [Learn more](/vertex-ai/docs/general/ml-use) .\n- Click **Start Training** .Model training can take many hours, depending on the size and complexity of your data and your training budget, if you specified one. You can close this tab and return to it later. You will receive an email when your model has completed training.Select a tab for your language or environment:Create a `TrainingPipeline` object to train a model.\nBefore using any of the request data, make the following replacements:- : The region where the model will be created, such as`us-central1`\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) \n- : Name for the model as it appears in the  user interface\n- : The ID for the dataset\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) \nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/trainingPipelines\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"MODEL_DISPLAY_NAME\",\n \"trainingTaskDefinition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_text_extraction_1.0.0.yaml\",\n \"modelToUpload\": {\n \"displayName\": \"MODEL_DISPLAY_NAME\"\n },\n \"inputDataConfig\": {\n \"datasetId\": \"DATASET_ID\"\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/us-central1/trainingPipelines/PIPELINE_ID\",\n \"displayName\": \"MODEL_DISPLAY_NAME\",\n \"inputDataConfig\": {\n \"datasetId\": \"DATASET_ID\"\n },\n \"trainingTaskDefinition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_text_extraction_1.0.0.yaml\",\n \"modelToUpload\": {\n \"displayName\": \"MODEL_DISPLAY_NAME\"\n },\n \"state\": \"PIPELINE_STATE_PENDING\",\n \"createTime\": \"2020-04-18T01:22:57.479336Z\",\n \"updateTime\": \"2020-04-18T01:22:57.479336Z\"\n}\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateTrainingPipelineTextEntityExtractionSample.java) \n```\nimport com.google.cloud.aiplatform.util.ValueConverter;import com.google.cloud.aiplatform.v1.DeployedModelRef;import com.google.cloud.aiplatform.v1.EnvVar;import com.google.cloud.aiplatform.v1.FilterSplit;import com.google.cloud.aiplatform.v1.FractionSplit;import com.google.cloud.aiplatform.v1.InputDataConfig;import com.google.cloud.aiplatform.v1.LocationName;import com.google.cloud.aiplatform.v1.Model;import com.google.cloud.aiplatform.v1.Model.ExportFormat;import com.google.cloud.aiplatform.v1.ModelContainerSpec;import com.google.cloud.aiplatform.v1.PipelineServiceClient;import com.google.cloud.aiplatform.v1.PipelineServiceSettings;import com.google.cloud.aiplatform.v1.Port;import com.google.cloud.aiplatform.v1.PredefinedSplit;import com.google.cloud.aiplatform.v1.PredictSchemata;import com.google.cloud.aiplatform.v1.TimestampSplit;import com.google.cloud.aiplatform.v1.TrainingPipeline;import com.google.rpc.Status;import java.io.IOException;public class CreateTrainingPipelineTextEntityExtractionSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String trainingPipelineDisplayName = \"YOUR_TRAINING_PIPELINE_DISPLAY_NAME\";\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String datasetId = \"YOUR_DATASET_ID\";\u00a0 \u00a0 String modelDisplayName = \"YOUR_MODEL_DISPLAY_NAME\";\u00a0 \u00a0 createTrainingPipelineTextEntityExtractionSample(\u00a0 \u00a0 \u00a0 \u00a0 project, trainingPipelineDisplayName, datasetId, modelDisplayName);\u00a0 }\u00a0 static void createTrainingPipelineTextEntityExtractionSample(\u00a0 \u00a0 \u00a0 String project, String trainingPipelineDisplayName, String datasetId, String modelDisplayName)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 PipelineServiceSettings pipelineServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PipelineServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (PipelineServiceClient pipelineServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 PipelineServiceClient.create(pipelineServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 String trainingTaskDefinition =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gs://google-cloud-aiplatform/schema/trainingjob/definition/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"automl_text_extraction_1.0.0.yaml\";\u00a0 \u00a0 \u00a0 LocationName locationName = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 InputDataConfig trainingInputDataConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputDataConfig.newBuilder().setDatasetId(datasetId).build();\u00a0 \u00a0 \u00a0 Model model = Model.newBuilder().setDisplayName(modelDisplayName).build();\u00a0 \u00a0 \u00a0 TrainingPipeline trainingPipeline =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TrainingPipeline.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(trainingPipelineDisplayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingTaskDefinition(trainingTaskDefinition)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingTaskInputs(ValueConverter.EMPTY_VALUE)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputDataConfig(trainingInputDataConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModelToUpload(model)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 TrainingPipeline trainingPipelineResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pipelineServiceClient.createTrainingPipeline(locationName, trainingPipeline);\u00a0 \u00a0 \u00a0 System.out.println(\"Create Training Pipeline Text Entity Extraction Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\tName: %s\\n\", trainingPipelineResponse.getName());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tDisplay Name: %s\\n\", trainingPipelineResponse.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tTraining Task Definition %s\\n\", trainingPipelineResponse.getTrainingTaskDefinition());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tTraining Task Inputs: %s\\n\", trainingPipelineResponse.getTrainingTaskInputs());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tTraining Task Metadata: %s\\n\", trainingPipelineResponse.getTrainingTaskMetadata());\u00a0 \u00a0 \u00a0 System.out.format(\"State: %s\\n\", trainingPipelineResponse.getState());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tCreate Time: %s\\n\", trainingPipelineResponse.getCreateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tStartTime %s\\n\", trainingPipelineResponse.getStartTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tEnd Time: %s\\n\", trainingPipelineResponse.getEndTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tUpdate Time: %s\\n\", trainingPipelineResponse.getUpdateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tLabels: %s\\n\", trainingPipelineResponse.getLabelsMap());\u00a0 \u00a0 \u00a0 InputDataConfig inputDataConfig = trainingPipelineResponse.getInputDataConfig();\u00a0 \u00a0 \u00a0 System.out.println(\"\\tInput Data Config\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tDataset Id: %s\", inputDataConfig.getDatasetId());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tAnnotations Filter: %s\\n\", inputDataConfig.getAnnotationsFilter());\u00a0 \u00a0 \u00a0 FractionSplit fractionSplit = inputDataConfig.getFractionSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tFraction Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTraining Fraction: %s\\n\", fractionSplit.getTrainingFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tValidation Fraction: %s\\n\", fractionSplit.getValidationFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTest Fraction: %s\\n\", fractionSplit.getTestFraction());\u00a0 \u00a0 \u00a0 FilterSplit filterSplit = inputDataConfig.getFilterSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tFilter Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTraining Filter: %s\\n\", filterSplit.getTrainingFilter());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tValidation Filter: %s\\n\", filterSplit.getValidationFilter());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTest Filter: %s\\n\", filterSplit.getTestFilter());\u00a0 \u00a0 \u00a0 PredefinedSplit predefinedSplit = inputDataConfig.getPredefinedSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tPredefined Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tKey: %s\\n\", predefinedSplit.getKey());\u00a0 \u00a0 \u00a0 TimestampSplit timestampSplit = inputDataConfig.getTimestampSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tTimestamp Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTraining Fraction: %s\\n\", timestampSplit.getTrainingFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tValidation Fraction: %s\\n\", timestampSplit.getValidationFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTest Fraction: %s\\n\", timestampSplit.getTestFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tKey: %s\\n\", timestampSplit.getKey());\u00a0 \u00a0 \u00a0 Model modelResponse = trainingPipelineResponse.getModelToUpload();\u00a0 \u00a0 \u00a0 System.out.println(\"\\tModel To Upload\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tName: %s\\n\", modelResponse.getName());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tDisplay Name: %s\\n\", modelResponse.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tDescription: %s\\n\", modelResponse.getDescription());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tMetadata Schema Uri: %s\\n\", modelResponse.getMetadataSchemaUri());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tMetadata: %s\\n\", modelResponse.getMetadata());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tTraining Pipeline: %s\\n\", modelResponse.getTrainingPipeline());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tArtifact Uri: %s\\n\", modelResponse.getArtifactUri());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tSupported Deployment Resources Types: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedDeploymentResourcesTypesList());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tSupported Input Storage Formats: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedInputStorageFormatsList());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tSupported Output Storage Formats: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedOutputStorageFormatsList());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tCreate Time: %s\\n\", modelResponse.getCreateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tUpdate Time: %s\\n\", modelResponse.getUpdateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tLabels: %sn\\n\", modelResponse.getLabelsMap());\u00a0 \u00a0 \u00a0 PredictSchemata predictSchemata = modelResponse.getPredictSchemata();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tPredict Schemata\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tInstance Schema Uri: %s\\n\", predictSchemata.getInstanceSchemaUri());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\t\\tParameters Schema Uri: %s\\n\", predictSchemata.getParametersSchemaUri());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\t\\tPrediction Schema Uri: %s\\n\", predictSchemata.getPredictionSchemaUri());\u00a0 \u00a0 \u00a0 for (ExportFormat exportFormat : modelResponse.getSupportedExportFormatsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tSupported Export Format\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tId: %s\\n\", exportFormat.getId());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ModelContainerSpec modelContainerSpec = modelResponse.getContainerSpec();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tContainer Spec\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tImage Uri: %s\\n\", modelContainerSpec.getImageUri());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tCommand: %s\\n\", modelContainerSpec.getCommandList());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tArgs: %s\\n\", modelContainerSpec.getArgsList());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tPredict Route: %s\\n\", modelContainerSpec.getPredictRoute());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tHealth Route: %s\\n\", modelContainerSpec.getHealthRoute());\u00a0 \u00a0 \u00a0 for (EnvVar envVar : modelContainerSpec.getEnvList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\t\\tEnv\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\t\\tName: %s\\n\", envVar.getName());\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\t\\tValue: %s\\n\", envVar.getValue());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 for (Port port : modelContainerSpec.getPortsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\t\\tPort\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\t\\tContainer Port: %s\\n\", port.getContainerPort());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 for (DeployedModelRef deployedModelRef : modelResponse.getDeployedModelsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tDeployed Model\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tEndpoint: %s\\n\", deployedModelRef.getEndpoint());\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tDeployed Model Id: %s\\n\", deployedModelRef.getDeployedModelId());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 Status status = trainingPipelineResponse.getError();\u00a0 \u00a0 \u00a0 System.out.println(\"\\tError\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tCode: %s\\n\", status.getCode());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tMessage: %s\\n\", status.getMessage());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/create-training-pipeline-text-entity-extraction.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const datasetId = 'YOUR_DATASET_ID';// const modelDisplayName = 'YOUR_MODEL_DISPLAY_NAME';// const trainingPipelineDisplayName = 'YOUR_TRAINING_PIPELINE_DISPLAY_NAME';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';const aiplatform = require('@google-cloud/aiplatform');const {definition} =\u00a0 aiplatform.protos.google.cloud.aiplatform.v1.schema.trainingjob;// Imports the Google Cloud Pipeline Service Client libraryconst {PipelineServiceClient} = aiplatform.v1;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst pipelineServiceClient = new PipelineServiceClient(clientOptions);async function createTrainingPipelineTextEntityExtraction() {\u00a0 // Configure the parent resource\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 const trainingTaskInputObj = new definition.AutoMlTextExtractionInputs({});\u00a0 const trainingTaskInputs = trainingTaskInputObj.toValue();\u00a0 const modelToUpload = {displayName: modelDisplayName};\u00a0 const inputDataConfig = {datasetId: datasetId};\u00a0 const trainingPipeline = {\u00a0 \u00a0 displayName: trainingPipelineDisplayName,\u00a0 \u00a0 trainingTaskDefinition:\u00a0 \u00a0 \u00a0 'gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_text_extraction_1.0.0.yaml',\u00a0 \u00a0 trainingTaskInputs,\u00a0 \u00a0 inputDataConfig,\u00a0 \u00a0 modelToUpload,\u00a0 };\u00a0 const request = {\u00a0 \u00a0 parent,\u00a0 \u00a0 trainingPipeline,\u00a0 };\u00a0 // Create training pipeline request\u00a0 const [response] =\u00a0 \u00a0 await pipelineServiceClient.createTrainingPipeline(request);\u00a0 console.log('Create training pipeline text entity extraction response :');\u00a0 console.log(`Name : ${response.name}`);\u00a0 console.log('Raw response:');\u00a0 console.log(JSON.stringify(response, null, 2));}createTrainingPipelineTextEntityExtraction();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_training_pipeline_text_entity_extraction_sample.py) \n```\ndef create_training_pipeline_text_entity_extraction_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 dataset_id: str,\u00a0 \u00a0 model_display_name: Optional[str] = None,\u00a0 \u00a0 training_fraction_split: float = 0.8,\u00a0 \u00a0 validation_fraction_split: float = 0.1,\u00a0 \u00a0 test_fraction_split: float = 0.1,\u00a0 \u00a0 budget_milli_node_hours: int = 8000,\u00a0 \u00a0 disable_early_stopping: bool = False,\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 job = aiplatform.AutoMLTextTrainingJob(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name, prediction_type=\"extraction\"\u00a0 \u00a0 )\u00a0 \u00a0 text_dataset = aiplatform.TextDataset(dataset_id)\u00a0 \u00a0 model = job.run(\u00a0 \u00a0 \u00a0 \u00a0 dataset=text_dataset,\u00a0 \u00a0 \u00a0 \u00a0 model_display_name=model_display_name,\u00a0 \u00a0 \u00a0 \u00a0 training_fraction_split=training_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 validation_fraction_split=validation_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 test_fraction_split=test_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 budget_milli_node_hours=budget_milli_node_hours,\u00a0 \u00a0 \u00a0 \u00a0 disable_early_stopping=disable_early_stopping,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 print(model.uri)\u00a0 \u00a0 return model\n```\n## Control the data split using RESTYou can control how your training data is split between the training, validation, and test sets. When using the Vertex AI API, use the [Split object](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines#InputDataConfig) to determine your data split. The `Split` object can be included in the `InputConfig` object as one of several object types, each of which provides a different way to split the training data. You can select one method only.- `FractionSplit`:- : The fraction of the training data to   be used for the training set.\n- : The fraction of the training data   to be used for the validation set. Not used for video data.\n- : The fraction of the training data to be   used for the test set.\nIf any of the fractions are specified, all must be specified. The  fractions must add up to 1.0. The [default values for the fractions](/vertex-ai/docs/general/ml-use#default) differ depending on your data type. [Learn more](/vertex-ai/docs/general/ml-use#percentages) .```\n\"fractionSplit\": {\n \"trainingFraction\": TRAINING_FRACTION,\n \"validationFraction\": VALIDATION_FRACTION,\n \"testFraction\": TEST_FRACTION\n},\n```\n- `FilterSplit`:\n- : Data items that match this filter are used for the training set.\n- : Data items that match this filter are used for the validation set. Must be \"-\" for video data.\n- : Data items that match this filter are used for the test set.\n- These filters can be used with the `ml_use` label, or with any labels you apply to your data. Learn more about using [the ml-use label](/vertex-ai/docs/general/ml-use#ml-use) and [other labels](/vertex-ai/docs/general/ml-use#filter) to filter your data.\n- The following example shows how to use the `filterSplit` object with the `ml_use` label, with the validation set included:\n- ```\n\"filterSplit\": {\n\"trainingFilter\": \"labels.aiplatform.googleapis.com/ml_use=training\",\n\"validationFilter\": \"labels.aiplatform.googleapis.com/ml_use=validation\",\n\"testFilter\": \"labels.aiplatform.googleapis.com/ml_use=test\"\n}\n```\n[Previous  arrow_back    Create dataset  ](/vertex-ai/docs/text-data/entity-extraction/create-dataset) [Next  Evaluate model    arrow_forward  ](/vertex-ai/docs/text-data/entity-extraction/evaluate-model)", "guide": "Vertex AI"}