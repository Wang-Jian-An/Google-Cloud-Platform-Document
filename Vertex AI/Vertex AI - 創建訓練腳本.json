{"title": "Vertex AI - \u5275\u5efa\u8a13\u7df4\u8173\u672c", "url": "https://cloud.google.com/vertex-ai/docs/tutorials/tabular-bq-prediction/create-training-script?hl=zh-cn", "abstract": "# Vertex AI - \u5275\u5efa\u8a13\u7df4\u8173\u672c\n[run](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomTrainingJob?hl=zh-cn#google_cloud_aiplatform_CustomTrainingJob_run)\n\u5728\u672c\u4e3b\u984c\u4e2d\uff0c\u60a8\u5c07\u5275\u5efa\u8a13\u7df4\u8173\u672c\uff0c\u7136\u5f8c\u7232\u8a13\u7df4\u8173\u672c\u6307\u5b9a\u547d\u4ee4\u53c3\u6578\u3002\n", "content": "## \u5275\u5efa\u8a13\u7df4\u8173\u672c\n\u5728\u672c\u90e8\u5206\u4e2d\uff0c\u60a8\u5c07\u5275\u5efa\u4e00\u500b\u8a13\u7df4\u8173\u672c\u3002\u6b64\u8173\u672c\u662f\u7b46\u8a18\u672c\u74b0\u5883\u4e2d\u540d\u7232 `task.py` \u7684\u65b0\u6587\u4ef6\u3002\u5728\u672c\u6559\u7a0b\u7684\u5f8c\u9762\u90e8\u5206\uff0c\u60a8\u9700\u8981\u5c07\u6b64\u8173\u672c\u50b3\u905e\u7d66 [aiplatform.CustomTrainingJob](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomTrainingJob?hl=zh-cn) \u69cb\u9020\u51fd\u6578\u3002\u8a72\u8173\u672c\u904b\u884c\u6642\uff0c\u5c07\u57f7\u884c\u4ee5\u4e0b\u64cd\u4f5c\uff1a\n- \u52a0\u8f09\u60a8\u5275\u5efa\u7684 BigQuery \u6578\u64da\u96c6\u4e2d\u7684\u6578\u64da\u3002\n- \u4f7f\u7528 [TensorFlow Keras API](https://www.tensorflow.org/api_docs/python/tf/keras?hl=zh-cn) \u69cb\u5efa\u3001\u7de8\u8b6f\u548c\u8a13\u7df4\u6a21\u578b\u3002\n- \u6307\u5b9a\u8abf\u7528 Keras [Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model?hl=zh-cn#fit) \u65b9\u6cd5\u6642\u8981\u4f7f\u7528\u7684\u9031\u671f\u6578\u548c\u6279\u6b21\u5927\u5c0f\u3002\n- \u4f7f\u7528 `AIP_MODEL_DIR` \u74b0\u5883\u8b8a\u91cf\u6307\u5b9a\u4fdd\u5b58\u6a21\u578b\u5de5\u4ef6\u7684\u4f4d\u7f6e\u3002 `AIP_MODEL_DIR` \u7531 Vertex AI \u8a2d\u7f6e\uff0c\u5305\u542b\u7528\u65bc\u4fdd\u5b58\u6a21\u578b\u5de5\u4ef6\u7684\u76ee\u9304\u7684 URI\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7279\u6b8a Cloud Storage \u76ee\u9304\u7684\u74b0\u5883\u8b8a\u91cf](https://cloud.google.com/vertex-ai/docs/training/code-requirements?hl=zh-cn#environment-variables) \u3002\n- \u5c07 TensorFlow [SavedModel](https://www.tensorflow.org/api_docs/python/tf/saved_model?hl=zh-cn) \u5c0e\u51fa\u5230\u6a21\u578b\u76ee\u9304\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 TensorFlow \u7db2\u7ad9\u4e0a\u7684 [\u4f7f\u7528 SavedModel \u683c\u5f0f](https://www.tensorflow.org/guide/saved_model?hl=zh-cn#the_savedmodel_format_on_disk) \u3002\n\u5982\u9700\u5275\u5efa\u8a13\u7df4\u8173\u672c\uff0c\u8acb\u5728\u7b46\u8a18\u672c\u4e2d\u904b\u884c\u4ee5\u4e0b\u4ee3\u78bc\uff1a\n```\n%%writefile task.pyimport argparseimport numpy as npimport osimport pandas as pdimport tensorflow as tffrom google.cloud import bigqueryfrom google.cloud import storage# Read environmental variablestraining_data_uri = os.getenv(\"AIP_TRAINING_DATA_URI\")validation_data_uri = os.getenv(\"AIP_VALIDATION_DATA_URI\")test_data_uri = os.getenv(\"AIP_TEST_DATA_URI\")# Read argsparser = argparse.ArgumentParser()parser.add_argument('--label_column', required=True, type=str)parser.add_argument('--epochs', default=10, type=int)parser.add_argument('--batch_size', default=10, type=int)args = parser.parse_args()# Set up training variablesLABEL_COLUMN = args.label_column# See https://cloud.google.com/vertex-ai/docs/workbench/managed/executor#explicit-project-selection for issues regarding permissions.PROJECT_NUMBER = os.environ[\"CLOUD_ML_PROJECT_ID\"]bq_client = bigquery.Client(project=PROJECT_NUMBER)# Download a tabledef download_table(bq_table_uri: str):\u00a0 \u00a0 # Remove bq:// prefix if present\u00a0 \u00a0 prefix = \"bq://\"\u00a0 \u00a0 if bq_table_uri.startswith(prefix):\u00a0 \u00a0 \u00a0 \u00a0 bq_table_uri = bq_table_uri[len(prefix) :]\u00a0 \u00a0 # Download the BigQuery table as a dataframe\u00a0 \u00a0 # This requires the \"BigQuery Read Session User\" role on the custom training service account.\u00a0 \u00a0 table = bq_client.get_table(bq_table_uri)\u00a0 \u00a0 return bq_client.list_rows(table).to_dataframe()# Download dataset splitsdf_train = download_table(training_data_uri)df_validation = download_table(validation_data_uri)df_test = download_table(test_data_uri)def convert_dataframe_to_dataset(\u00a0 \u00a0 df_train: pd.DataFrame,\u00a0 \u00a0 df_validation: pd.DataFrame,):\u00a0 \u00a0 df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\u00a0 \u00a0 df_validation_x, df_validation_y = df_validation, df_validation.pop(LABEL_COLUMN)\u00a0 \u00a0 y_train = tf.convert_to_tensor(np.asarray(df_train_y).astype(\"float32\"))\u00a0 \u00a0 y_validation = tf.convert_to_tensor(np.asarray(df_validation_y).astype(\"float32\"))\u00a0 \u00a0 # Convert to numpy representation\u00a0 \u00a0 x_train = tf.convert_to_tensor(np.asarray(df_train_x).astype(\"float32\"))\u00a0 \u00a0 x_test = tf.convert_to_tensor(np.asarray(df_validation_x).astype(\"float32\"))\u00a0 \u00a0 # Convert to one-hot representation\u00a0 \u00a0 num_species = len(df_train_y.unique())\u00a0 \u00a0 y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_species)\u00a0 \u00a0 y_validation = tf.keras.utils.to_categorical(y_validation, num_classes=num_species)\u00a0 \u00a0 dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\u00a0 \u00a0 dataset_validation = tf.data.Dataset.from_tensor_slices((x_test, y_validation))\u00a0 \u00a0 return (dataset_train, dataset_validation)# Create datasetsdataset_train, dataset_validation = convert_dataframe_to_dataset(df_train, df_validation)# Shuffle train setdataset_train = dataset_train.shuffle(len(df_train))def create_model(num_features):\u00a0 \u00a0 # Create model\u00a0 \u00a0 Dense = tf.keras.layers.Dense\u00a0 \u00a0 model = tf.keras.Sequential(\u00a0 \u00a0 \u00a0 \u00a0 [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Dense(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 100,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 activation=tf.nn.relu,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 kernel_initializer=\"uniform\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 input_dim=num_features,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Dense(75, activation=tf.nn.relu),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Dense(50, activation=tf.nn.relu),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Dense(25, activation=tf.nn.relu),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Dense(3, activation=tf.nn.softmax),\u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 )\u00a0 \u00a0 # Compile Keras model\u00a0 \u00a0 optimizer = tf.keras.optimizers.RMSprop(lr=0.001)\u00a0 \u00a0 model.compile(\u00a0 \u00a0 \u00a0 \u00a0 loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer\u00a0 \u00a0 )\u00a0 \u00a0 return model# Create the modelmodel = create_model(num_features=dataset_train._flat_shapes[0].dims[0].value)# Set up datasetsdataset_train = dataset_train.batch(args.batch_size)dataset_validation = dataset_validation.batch(args.batch_size)# Train the modelmodel.fit(dataset_train, epochs=args.epochs, validation_data=dataset_validation)tf.saved_model.save(model, os.getenv(\"AIP_MODEL_DIR\"))\n```\n\u5275\u5efa\u8173\u672c\u5f8c\uff0c\u5b83\u6703\u986f\u793a\u5728\u7b46\u8a18\u672c\u7684\u6839\u6587\u4ef6\u593e\u4e2d\uff1a\n## \u5b9a\u7fa9\u8a13\u7df4\u8173\u672c\u7684\u53c3\u6578\n\u5c07\u4ee5\u4e0b\u547d\u4ee4\u884c\u53c3\u6578\u50b3\u905e\u7d66\u8a13\u7df4\u8173\u672c\uff1a\n- `label_column` - \u9019\u7528\u65bc\u6a19\u8b58\u6578\u64da\u4e2d\u5305\u542b\u60a8\u8981\u9810\u6e2c\u7684\u5167\u5bb9\u7684\u5217\u3002\u5728\u672c\u4f8b\u4e2d\uff0c\u8a72\u5217\u7232 `species` \u3002\u8655\u7406\u6578\u64da\u6642\uff0c\u60a8\u5728\u540d\u7232 `LABEL_COLUMN` \u7684\u8b8a\u91cf\u4e2d\u5b9a\u7fa9\u6b64\u53c3\u6578\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u4e0b\u8f09\u3001\u9810\u8655\u7406\u548c\u62c6\u5206\u6578\u64da](https://cloud.google.com/vertex-ai/docs/tutorials/tabular-bq-prediction/create-dataset?hl=zh-cn#download-process-public-dataset) \u3002\n- `epochs` - \u9019\u662f\u8a13\u7df4\u6a21\u578b\u6642\u4f7f\u7528\u7684\u9031\u671f\u6578\u3002 \u9031\u671f\u662f\u5728\u8a13\u7df4\u6a21\u578b\u6642\u5c0d\u6578\u64da\u7684\u8fed\u4ee3\u3002\u672c\u6559\u7a0b\u4f7f\u7528 20 \u500b\u9031\u671f\u3002\n- `batch_size` - \u9019\u662f\u6a21\u578b\u66f4\u65b0\u4e4b\u524d\u8655\u7406\u7684\u6a23\u672c\u6578\u3002\u672c\u6559\u7a0b\u4f7f\u7528\u7684\u6279\u6b21\u5927\u5c0f\u7232 10\u3002\n\u5982\u9700\u5b9a\u7fa9\u50b3\u905e\u7d66\u8173\u672c\u7684\u53c3\u6578\uff0c\u8acb\u904b\u884c\u4ee5\u4e0b\u4ee3\u78bc\uff1a\n```\nJOB_NAME = \"custom_job_unique\"EPOCHS = 20BATCH_SIZE = 10CMDARGS = [\u00a0 \u00a0 \"--label_column=\" + LABEL_COLUMN,\u00a0 \u00a0 \"--epochs=\" + str(EPOCHS),\u00a0 \u00a0 \"--batch_size=\" + str(BATCH_SIZE),]\n```", "guide": "Vertex AI"}