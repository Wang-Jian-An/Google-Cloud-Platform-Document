{"title": "Vertex AI - Create a dataset for training text sentiment analysis models", "url": "https://cloud.google.com/vertex-ai/docs/text-data/sentiment-analysis/create-dataset", "abstract": "# Vertex AI - Create a dataset for training text sentiment analysis models\nThis page shows you how to create a Vertex AI dataset from your text data so you can start training sentiment analysis models. You can create a dataset using either the Google Cloud console or the Vertex AI API.\n", "content": "## Before you begin\nBefore you can create a Vertex AI dataset from your text data, you must [prepare your text data](/vertex-ai/docs/text-data/sentiment-analysis/prepare-data) .\n## Create an empty dataset and import or associate your data\nUse the following instructions to create an empty dataset and either import or associate your data.- In the Google Cloud console, in the Vertex AI section, go to  the **Datasets** page. [Go to the Datasets page](https://console.cloud.google.com/vertex-ai/datasets) \n- Click **Create** to open the create dataset details page.\n- Modify the **Dataset name** field to create a descriptive dataset display  name.\n- Select the **Text** tab.\n- Select **Sentiment analysis** .\n- Select a region from the **Region** drop-down list.\n- Click **Create** to create your empty dataset, and advance to the data import page.\n- Choose one of the following options from the **Select an import method** section:- In the **Select an import method** section, choose to upload data from your computer.\n- Click **Select files** and choose all the local files to upload to a Cloud Storage  bucket.\n- In the **Select a Cloud Storage path** section click **Browse** to choose a  Cloud Storage bucket location to upload your data to.\n- Clickradio_button_checked **Upload an import file from\n your computer** .\n- Click **Select files** and choose the local import file to upload to a Cloud Storage  bucket.\n- In the **Select a Cloud Storage path** section click **Browse** to choose a  Cloud Storage bucket location to upload your file to.\n- Clickradio_button_checked **Select an import file from\n Cloud Storage** .\n- In the **Select a Cloud Storage path** section click **Browse** to choose the  import file in Cloud Storage.\n- Click **Continue** .Data import can take several hours, depending on the size of your data.  You can close this tab and return to it later. You will receive an email  when your data is imported.\nIn order to create a machine learning model you must first have a representative collection of data to train with. After importing data you can make modifications and start model training.## Create a datasetUse the following samples to create a dataset for your data.Before using any of the request data, make the following replacements:- : Region where the dataset will be stored. This must be a region that supports dataset resources. For example,`us-central1`. See [List of available locations](/vertex-ai/docs/general/locations) .\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) \n- : Name for the dataset.\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/datasets\n```\nRequest JSON body:\n```\n{\n \"display_name\": \"DATASET_NAME\",\n \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/text_1.0.0.yaml\"\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/datasets\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/datasets\" | Select-Object -Expand Content\n```\nYou should see output similar to the following. You can use the in the response to [get the status](#get-operation) of the operation.\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION/datasets/DATASET_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.CreateDatasetOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2020-07-07T21:27:35.964882Z\",\n  \"updateTime\": \"2020-07-07T21:27:35.964882Z\"\n }\n }\n}\n```\nThe following sample uses the [google_vertex_ai_dataset](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/vertex_ai_dataset) Terraform resource to create a text dataset named `text-dataset` .\nTo learn how to apply or remove a Terraform configuration, see [Basic Terraform commands](/docs/terraform/basic-commands) .\n [View on GitHub](https://github.com/terraform-google-modules/terraform-docs-samples/blob/HEAD/vertex_ai/dataset/main.tf) \n```\nresource \"google_vertex_ai_dataset\" \"text_dataset\" {\u00a0 display_name \u00a0 \u00a0 \u00a0 \u00a0= \"text-dataset\"\u00a0 metadata_schema_uri = \"gs://google-cloud-aiplatform/schema/dataset/metadata/text_1.0.0.yaml\"\u00a0 region \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"us-central1\"}\n```Before trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateDatasetTextSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.CreateDatasetOperationMetadata;import com.google.cloud.aiplatform.v1.Dataset;import com.google.cloud.aiplatform.v1.DatasetServiceClient;import com.google.cloud.aiplatform.v1.DatasetServiceSettings;import com.google.cloud.aiplatform.v1.LocationName;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class CreateDatasetTextSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String datasetDisplayName = \"YOUR_DATASET_DISPLAY_NAME\";\u00a0 \u00a0 createDatasetTextSample(project, datasetDisplayName);\u00a0 }\u00a0 static void createDatasetTextSample(String project, String datasetDisplayName)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 DatasetServiceSettings datasetServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 DatasetServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (DatasetServiceClient datasetServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 DatasetServiceClient.create(datasetServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 String metadataSchemaUri =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gs://google-cloud-aiplatform/schema/dataset/metadata/text_1.0.0.yaml\";\u00a0 \u00a0 \u00a0 LocationName locationName = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 Dataset dataset =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Dataset.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(datasetDisplayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMetadataSchemaUri(metadataSchemaUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 OperationFuture<Dataset, CreateDatasetOperationMetadata> datasetFuture =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 datasetServiceClient.createDatasetAsync(locationName, dataset);\u00a0 \u00a0 \u00a0 System.out.format(\"Operation name: %s\\n\", datasetFuture.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to finish...\");\u00a0 \u00a0 \u00a0 Dataset datasetResponse = datasetFuture.get(180, TimeUnit.SECONDS);\u00a0 \u00a0 \u00a0 System.out.println(\"Create Text Dataset Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\tName: %s\\n\", datasetResponse.getName());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tDisplay Name: %s\\n\", datasetResponse.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tMetadata Schema Uri: %s\\n\", datasetResponse.getMetadataSchemaUri());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tMetadata: %s\\n\", datasetResponse.getMetadata());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tCreate Time: %s\\n\", datasetResponse.getCreateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tUpdate Time: %s\\n\", datasetResponse.getUpdateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tLabels: %s\\n\", datasetResponse.getLabelsMap());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/create-dataset-text.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const datasetDisplayName = \"YOUR_DATASTE_DISPLAY_NAME\";// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Dataset Service Client libraryconst {DatasetServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst datasetServiceClient = new DatasetServiceClient(clientOptions);async function createDatasetText() {\u00a0 // Configure the parent resource\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 // Configure the dataset resource\u00a0 const dataset = {\u00a0 \u00a0 displayName: datasetDisplayName,\u00a0 \u00a0 metadataSchemaUri:\u00a0 \u00a0 \u00a0 'gs://google-cloud-aiplatform/schema/dataset/metadata/text_1.0.0.yaml',\u00a0 };\u00a0 const request = {\u00a0 \u00a0 parent,\u00a0 \u00a0 dataset,\u00a0 };\u00a0 // Create Dataset Request\u00a0 const [response] = await datasetServiceClient.createDataset(request);\u00a0 console.log(`Long running operation: ${response.name}`);\u00a0 // Wait for operation to complete\u00a0 await response.promise();\u00a0 const result = response.result;\u00a0 console.log('Create dataset text response');\u00a0 console.log(`Name : ${result.name}`);\u00a0 console.log(`Display name : ${result.displayName}`);\u00a0 console.log(`Metadata schema uri : ${result.metadataSchemaUri}`);\u00a0 console.log(`Metadata : ${JSON.stringify(result.metadata)}`);\u00a0 console.log(`Labels : ${JSON.stringify(result.labels)}`);}createDatasetText();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\nThe following sample uses the Vertex AI SDK for Python to both create a dataset and import data. If you run this sample code, then you can skip the [Import datasection](#import-data) of this guide.\nThis particular sample imports data for single-label classification. If your model has a different objective, then you must adjust the code.\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_and_import_dataset_text_sample.py) \n```\ndef create_and_import_dataset_text_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 src_uris: Union[str, List[str]],\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 ds = aiplatform.TextDataset.create(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name,\u00a0 \u00a0 \u00a0 \u00a0 gcs_source=src_uris,\u00a0 \u00a0 \u00a0 \u00a0 import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 ds.wait()\u00a0 \u00a0 print(ds.display_name)\u00a0 \u00a0 print(ds.resource_name)\u00a0 \u00a0 return ds\n```\n## Import dataAfter you create an empty dataset you can import your data into the dataset. If you used the Vertex AI SDK for Python to create the dataset, then you might have already imported data when you created the dataset. If so, you can skip this section.Before using any of the request data, make the following replacements:- : Region where your dataset will be stored. For example,`us-central1`.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : ID of the dataset.\n- : Path to the CSV or [JSON Lines](https://jsonlines.org/) file in Cloud Storage that lists data items stored in Cloud Storage to  use for model training; for import file formats and limitations, see [Preparing text data](/vertex-ai/docs/datasets/prepare-text) .\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/datasets/DATASET_ID:import\n```\nRequest JSON body:\n```\n{\n \"import_configs\": [ {\n  \"gcs_source\": {\n  \"uris\": \"IMPORT_FILE_URI\"\n  },\n  \"import_schema_uri\" : \"gs://google-cloud-aiplatform/schema/dataset/ioformat/text_sentiment_io_format_1.0.0.yaml \"\n }\n ]\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/datasets/DATASET_ID:import\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION/datasets/DATASET_ID:import\" | Select-Object -Expand Content\n```\nYou should see output similar to the following. You can use the in the response to [get the status](#get-operation) of the operation.\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION/datasets/DATASET_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.ImportDataOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2020-07-08T20:32:02.543801Z\",\n  \"updateTime\": \"2020-07-08T20:32:02.543801Z\"\n }\n }\n}\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/ImportDataTextSentimentAnalysisSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.DatasetName;import com.google.cloud.aiplatform.v1.DatasetServiceClient;import com.google.cloud.aiplatform.v1.DatasetServiceSettings;import com.google.cloud.aiplatform.v1.GcsSource;import com.google.cloud.aiplatform.v1.ImportDataConfig;import com.google.cloud.aiplatform.v1.ImportDataOperationMetadata;import com.google.cloud.aiplatform.v1.ImportDataResponse;import java.io.IOException;import java.util.Collections;import java.util.List;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class ImportDataTextSentimentAnalysisSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String datasetId = \"YOUR_DATASET_ID\";\u00a0 \u00a0 String gcsSourceUri =\u00a0 \u00a0 \u00a0 \u00a0 \"gs://YOUR_GCS_SOURCE_BUCKET/path_to_your_text_source/[file.csv/file.jsonl]\";\u00a0 \u00a0 importDataTextSentimentAnalysisSample(project, datasetId, gcsSourceUri);\u00a0 }\u00a0 static void importDataTextSentimentAnalysisSample(\u00a0 \u00a0 \u00a0 String project, String datasetId, String gcsSourceUri)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 DatasetServiceSettings datasetServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 DatasetServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (DatasetServiceClient datasetServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 DatasetServiceClient.create(datasetServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 String importSchemaUri =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gs://google-cloud-aiplatform/schema/dataset/ioformat/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + \"text_sentiment_io_format_1.0.0.yaml\";\u00a0 \u00a0 \u00a0 GcsSource.Builder gcsSource = GcsSource.newBuilder();\u00a0 \u00a0 \u00a0 gcsSource.addUris(gcsSourceUri);\u00a0 \u00a0 \u00a0 DatasetName datasetName = DatasetName.of(project, location, datasetId);\u00a0 \u00a0 \u00a0 List<ImportDataConfig> importDataConfigList =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Collections.singletonList(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ImportDataConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setGcsSource(gcsSource)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setImportSchemaUri(importSchemaUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 OperationFuture<ImportDataResponse, ImportDataOperationMetadata> importDataResponseFuture =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 datasetServiceClient.importDataAsync(datasetName, importDataConfigList);\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Operation name: %s\\n\", importDataResponseFuture.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to finish...\");\u00a0 \u00a0 \u00a0 ImportDataResponse importDataResponse = importDataResponseFuture.get(300, TimeUnit.SECONDS);\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Import Data Text Sentiment Analysis Response: %s\\n\", importDataResponse.toString());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/import-data-text-sentiment-analysis.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0*/// const datasetId = \"YOUR_DATASET_ID\";// const gcsSourceUri = \"YOUR_GCS_SOURCE_URI\";// eg. \"gs://<your-gcs-bucket>/<import_source_path>/[file.csv/file.jsonl]\"// const project = \"YOUR_PROJECT_ID\";// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Dataset Service Client libraryconst {DatasetServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};const datasetServiceClient = new DatasetServiceClient(clientOptions);async function importDataTextSentimentAnalysis() {\u00a0 const name = datasetServiceClient.datasetPath(project, location, datasetId);\u00a0 // Here we use only one import config with one source\u00a0 const importConfigs = [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 gcsSource: {uris: [gcsSourceUri]},\u00a0 \u00a0 \u00a0 importSchemaUri:\u00a0 \u00a0 \u00a0 \u00a0 'gs://google-cloud-aiplatform/schema/dataset/ioformat/text_sentiment_io_format_1.0.0.yaml',\u00a0 \u00a0 },\u00a0 ];\u00a0 const request = {\u00a0 \u00a0 name,\u00a0 \u00a0 importConfigs,\u00a0 };\u00a0 // Import data request\u00a0 const [response] = await datasetServiceClient.importData(request);\u00a0 console.log(`Long running operation : ${response.name}`);\u00a0 // Wait for operation to complete\u00a0 const [importDataResponse] = await response.promise();\u00a0 console.log(\u00a0 \u00a0 `Import data text sentiment analysis response \u00a0: \\\u00a0 \u00a0 \u00a0 ${JSON.stringify(importDataResponse.result, null, 2)}`\u00a0 );}importDataTextSentimentAnalysis();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/import_data_text_sentiment_analysis_sample.py) \n```\ndef import_data_text_sentiment_analysis_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 dataset: str,\u00a0 \u00a0 src_uris: Union[str, List[str]],\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 ds = aiplatform.TextDataset(dataset)\u00a0 \u00a0 ds.import_data(\u00a0 \u00a0 \u00a0 \u00a0 gcs_source=src_uris,\u00a0 \u00a0 \u00a0 \u00a0 import_schema_uri=aiplatform.schema.dataset.ioformat.text.sentiment,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 ds.wait()\u00a0 \u00a0 print(ds.display_name)\u00a0 \u00a0 print(ds.resource_name)\u00a0 \u00a0 return ds\n```\n## Get operation status\nSome requests start long-running operations that require time to complete. These requests return an operation name, which you can use to view the operation's status or cancel the operation. Vertex AI provides helper methods to make calls against long-running operations. For more information, see [Working with long-runningoperations](/vertex-ai/docs/general/long-running-operations) . [Previous  arrow_back    Prepare data  ](/vertex-ai/docs/text-data/sentiment-analysis/prepare-data) [Next  Train model    arrow_forward  ](/vertex-ai/docs/text-data/sentiment-analysis/train-model)", "guide": "Vertex AI"}