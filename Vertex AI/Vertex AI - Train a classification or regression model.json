{"title": "Vertex AI - Train a classification or regression model", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Train a classification or regression model\nThis page shows you how to train a classification or regression model from a tabular dataset using either the Google Cloud console or the Vertex AI API.\n", "content": "## Before you begin\nBefore you can train a model, you must complete the following:\n- [Prepare your training data](/vertex-ai/docs/tabular-data/classification-regression/prepare-data) \n- [Create a Vertex AI dataset](/vertex-ai/docs/tabular-data/classification-regression/create-dataset) .## Train a model\n- In the Google Cloud console, in the Vertex AI section, go to the **Datasets** page. [Go to the Datasets page](https://console.cloud.google.com/vertex-ai/datasets) \n- Click the name of the dataset you want to use to train your model to open its details page.\n- If your data type uses annotation sets, select the annotation set you want to use for this model.\n- Click **Train new model** .\n- Select **Other** .\n- In the **Train new model** page, complete the following steps:- Select the model training method.- `AutoML`is a good choice for a wide range of use cases.\nClick **Continue** .\n- Enter the display name for your new model.\n- Select your target column.The target column is the value that the model will predict.Learn more about [target column requirements](/vertex-ai/docs/tabular-data/classification-regression/prepare-data#data-structure) .\n- **Optional** : To export your test dataset to BigQuery, check **Export test dataset to BigQuery** and provide the name of the table.\n- **Optional** : To choose how to split the data between training, test, and validation sets, open the **Advanced options** . You can choose between the following data split options:- **Random** (Default): Vertex AI randomly selects the rows associated with each of the data sets. By default, Vertex AI selects 80% of your data rows for the training set, 10% for the validation set, and 10% for the test set.\n- **Manual** : Vertex AI selects data rows for each of the data sets based on the values in a data split column. Provide the name of the data split column.\n- **Chronological** : Vertex AI splits data based on the timestamp in a time column. Provide the name of the time column.\nLearn more about [data splits](/vertex-ai/docs/tabular-data/data-splits) .\n- Click **Continue** .\n- **Optional:** Click **Generate statistics** . Generating statistics populates the **Transformation** dropdown menus.\n- On the Training options page, review your column list and exclude any columns from training that should not be used to train the model.\n- Review the transformations selected for your included features, along with whether invalid data is allowed, and make any required updates.Learn more about [transformations](/vertex-ai/docs/datasets/data-types-tabular#transformations) and [invalid data](/vertex-ai/docs/datasets/data-types-tabular#null-values) .\n- If you want to specify a weight column, or change your optimization objective from the default, open the **Advanced options** and make your selections.Learn more about [weight columns](/vertex-ai/docs/tabular-data/classification-regression/prepare-data#weight) and [optimization objectives](/vertex-ai/docs/tabular-data/classification-regression/train-model#optimization-objectives) .\n- Click **Continue** .\n- In the **Compute and pricing** window, configure as follows:Enter the maximum number of hours you want your model to train for.This setting helps you put a cap on the training costs. The actual time elapsed can be longer than this value, because there are other operations involved in creating a new model.Suggested training time is related to the size of your training data. The table below shows suggested training time ranges by row count; a large number of columns will also increase the required training time.| Rows     | Suggested training time |\n|:-----------------------|:--------------------------|\n| Less than 100,000  | 1-3 hours     |\n| 100,000 - 1,000,000 | 1-6 hours     |\n| 1,000,000 - 10,000,000 | 1-12 hours    |\n| More than 10,000,000 | 3 - 24 hours    |For information about training pricing, see the [pricing page](/vertex-ai/pricing#tabular-data) .\n- Click **Start Training** .Model training can take many hours, depending on the size and complexity of your data and your training budget, if you specified one. You can close this tab and return to it later. You will receive an email when your model has completed training.Tabular training data in Cloud Storage or BigQuery  is not imported into Vertex AI. (When you import from local files, they are  imported into Cloud Storage.) When you create a dataset  with tabular data, the data is associated with the dataset. Changes you make to  your data source in Cloud Storage or BigQuery after dataset creation  are incorporated into models subsequently trained with that dataset. A snapshot of the  dataset is taken when model training begins.\nTabular training data in Cloud Storage or BigQuery  is not imported into Vertex AI. (When you import from local files, they are  imported into Cloud Storage.) When you create a dataset  with tabular data, the data is associated with the dataset. Changes you make to  your data source in Cloud Storage or BigQuery after dataset creation  are incorporated into models subsequently trained with that dataset. A snapshot of the  dataset is taken when model training begins.\nSelect a tabular data type objective.Select a tab for your language or environment:You use the [trainingPipelines.create](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines/create) command  to train a model.\nTrain the model.\nBefore using any of the request data, make the following replacements:- : Your region.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : Display name for the training pipeline created  for this operation.\n- : The column (value) you want this model to predict.\n- : (Optional) The weight column. [Learn more](/vertex-ai/docs/tabular-data/classification-regression/prepare-data#weight) .\n- : The maximum amount of time you want the model to train, in  milli node hours (1,000 milli node hours equals one node hour).\n- : Required only if you do not want the  default optimization objective for your prediction type. [Learn more](/vertex-ai/docs/tabular-data/classification-regression/train-model#optimization-objectives) .\n- : The transformation type is provided for each column used to  train the model. [Learn more](/vertex-ai/docs/datasets/data-types-tabular#transformations) .\n- : The name of the column with the specified transformation type. Every  column used to train the model must be specified.\n- : Display name for the newly trained model.\n- : ID for the training Dataset.\n- You can provide a`Split`object to control your data split. For information about  controlling data split, see [Controlling the data split using REST](#data-split) .\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) \nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/trainingPipelines\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"TRAININGPIPELINE_DISPLAY_NAME\",\n \"trainingTaskDefinition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_tabular_1.0.0.yaml\",\n \"trainingTaskInputs\": {\n  \"targetColumn\": \"TARGET_COLUMN\",\n  \"weightColumn\": \"WEIGHT_COLUMN\",\n  \"predictionType\": \"classification\",\n  \"trainBudgetMilliNodeHours\": TRAINING_BUDGET,\n  \"optimizationObjective\": \"OPTIMIZATION_OBJECTIVE\",\n  \"transformations\": [   {\"TRANSFORMATION_TYPE_1\": {\"column_name\" : \"COLUMN_NAME_1\"} },\n   {\"TRANSFORMATION_TYPE_2\": {\"column_name\" : \"COLUMN_NAME_2\"} },\n   ...\n },\n \"modelToUpload\": {\"displayName\": \"MODEL_DISPLAY_NAME\"},\n \"inputDataConfig\": {\n  \"datasetId\": \"DATASET_ID\",\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/us-central1/trainingPipelines/4567\",\n \"displayName\": \"myModelName\",\n \"trainingTaskDefinition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_tabular_1.0.0.yaml\",\n \"modelToUpload\": {\n \"displayName\": \"myModelName\"\n },\n \"state\": \"PIPELINE_STATE_PENDING\",\n \"createTime\": \"2020-08-18T01:22:57.479336Z\",\n \"updateTime\": \"2020-08-18T01:22:57.479336Z\"\n}\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateTrainingPipelineTabularClassificationSample.java) \n```\nimport com.google.cloud.aiplatform.util.ValueConverter;import com.google.cloud.aiplatform.v1.DeployedModelRef;import com.google.cloud.aiplatform.v1.EnvVar;import com.google.cloud.aiplatform.v1.FilterSplit;import com.google.cloud.aiplatform.v1.FractionSplit;import com.google.cloud.aiplatform.v1.InputDataConfig;import com.google.cloud.aiplatform.v1.LocationName;import com.google.cloud.aiplatform.v1.Model;import com.google.cloud.aiplatform.v1.ModelContainerSpec;import com.google.cloud.aiplatform.v1.PipelineServiceClient;import com.google.cloud.aiplatform.v1.PipelineServiceSettings;import com.google.cloud.aiplatform.v1.Port;import com.google.cloud.aiplatform.v1.PredefinedSplit;import com.google.cloud.aiplatform.v1.PredictSchemata;import com.google.cloud.aiplatform.v1.TimestampSplit;import com.google.cloud.aiplatform.v1.TrainingPipeline;import com.google.cloud.aiplatform.v1.schema.trainingjob.definition.AutoMlTablesInputs;import com.google.cloud.aiplatform.v1.schema.trainingjob.definition.AutoMlTablesInputs.Transformation;import com.google.cloud.aiplatform.v1.schema.trainingjob.definition.AutoMlTablesInputs.Transformation.AutoTransformation;import com.google.rpc.Status;import java.io.IOException;import java.util.ArrayList;public class CreateTrainingPipelineTabularClassificationSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String modelDisplayName = \"YOUR_DATASET_DISPLAY_NAME\";\u00a0 \u00a0 String datasetId = \"YOUR_DATASET_ID\";\u00a0 \u00a0 String targetColumn = \"TARGET_COLUMN\";\u00a0 \u00a0 createTrainingPipelineTableClassification(project, modelDisplayName, datasetId, targetColumn);\u00a0 }\u00a0 static void createTrainingPipelineTableClassification(\u00a0 \u00a0 \u00a0 String project, String modelDisplayName, String datasetId, String targetColumn)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 PipelineServiceSettings pipelineServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PipelineServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (PipelineServiceClient pipelineServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 PipelineServiceClient.create(pipelineServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 LocationName locationName = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 String trainingTaskDefinition =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_tables_1.0.0.yaml\";\u00a0 \u00a0 \u00a0 // Set the columns used for training and their data types\u00a0 \u00a0 \u00a0 Transformation transformation1 =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(AutoTransformation.newBuilder().setColumnName(\"sepal_width\").build())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 Transformation transformation2 =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(AutoTransformation.newBuilder().setColumnName(\"sepal_length\").build())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 Transformation transformation3 =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(AutoTransformation.newBuilder().setColumnName(\"petal_length\").build())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 Transformation transformation4 =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(AutoTransformation.newBuilder().setColumnName(\"petal_width\").build())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 ArrayList<Transformation> transformationArrayList = new ArrayList<>();\u00a0 \u00a0 \u00a0 transformationArrayList.add(transformation1);\u00a0 \u00a0 \u00a0 transformationArrayList.add(transformation2);\u00a0 \u00a0 \u00a0 transformationArrayList.add(transformation3);\u00a0 \u00a0 \u00a0 transformationArrayList.add(transformation4);\u00a0 \u00a0 \u00a0 AutoMlTablesInputs autoMlTablesInputs =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutoMlTablesInputs.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTargetColumn(targetColumn)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setPredictionType(\"classification\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addAllTransformations(transformationArrayList)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainBudgetMilliNodeHours(8000)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 FractionSplit fractionSplit =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 FractionSplit.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingFraction(0.8)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setValidationFraction(0.1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTestFraction(0.1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 InputDataConfig inputDataConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputDataConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDatasetId(datasetId)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setFractionSplit(fractionSplit)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 Model modelToUpload = Model.newBuilder().setDisplayName(modelDisplayName).build();\u00a0 \u00a0 \u00a0 TrainingPipeline trainingPipeline =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TrainingPipeline.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(modelDisplayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingTaskDefinition(trainingTaskDefinition)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingTaskInputs(ValueConverter.toValue(autoMlTablesInputs))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputDataConfig(inputDataConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModelToUpload(modelToUpload)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 TrainingPipeline trainingPipelineResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pipelineServiceClient.createTrainingPipeline(locationName, trainingPipeline);\u00a0 \u00a0 \u00a0 System.out.println(\"Create Training Pipeline Tabular Classification Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\tName: %s\\n\", trainingPipelineResponse.getName());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tDisplay Name: %s\\n\", trainingPipelineResponse.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tTraining Task Definition: %s\\n\", trainingPipelineResponse.getTrainingTaskDefinition());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tTraining Task Inputs: %s\\n\", trainingPipelineResponse.getTrainingTaskInputs());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tTraining Task Metadata: %s\\n\", trainingPipelineResponse.getTrainingTaskMetadata());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tState: %s\\n\", trainingPipelineResponse.getState());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tCreate Time: %s\\n\", trainingPipelineResponse.getCreateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tStart Time: %s\\n\", trainingPipelineResponse.getStartTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tEnd Time: %s\\n\", trainingPipelineResponse.getEndTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tUpdate Time: %s\\n\", trainingPipelineResponse.getUpdateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tLabels: %s\\n\", trainingPipelineResponse.getLabelsMap());\u00a0 \u00a0 \u00a0 InputDataConfig inputDataConfigResponse = trainingPipelineResponse.getInputDataConfig();\u00a0 \u00a0 \u00a0 System.out.println(\"\\tInput Data Config\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tDataset Id: %s\\n\", inputDataConfigResponse.getDatasetId());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tAnnotations Filter: %s\\n\", inputDataConfigResponse.getAnnotationsFilter());\u00a0 \u00a0 \u00a0 FractionSplit fractionSplitResponse = inputDataConfigResponse.getFractionSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tFraction Split\");\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\t\\tTraining Fraction: %s\\n\", fractionSplitResponse.getTrainingFraction());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\t\\tValidation Fraction: %s\\n\", fractionSplitResponse.getValidationFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTest Fraction: %s\\n\", fractionSplitResponse.getTestFraction());\u00a0 \u00a0 \u00a0 FilterSplit filterSplit = inputDataConfigResponse.getFilterSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tFilter Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTraining Fraction: %s\\n\", filterSplit.getTrainingFilter());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tValidation Fraction: %s\\n\", filterSplit.getValidationFilter());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTest Fraction: %s\\n\", filterSplit.getTestFilter());\u00a0 \u00a0 \u00a0 PredefinedSplit predefinedSplit = inputDataConfigResponse.getPredefinedSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tPredefined Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tKey: %s\\n\", predefinedSplit.getKey());\u00a0 \u00a0 \u00a0 TimestampSplit timestampSplit = inputDataConfigResponse.getTimestampSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tTimestamp Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTraining Fraction: %s\\n\", timestampSplit.getTrainingFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tValidation Fraction: %s\\n\", timestampSplit.getValidationFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTest Fraction: %s\\n\", timestampSplit.getTestFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tKey: %s\\n\", timestampSplit.getKey());\u00a0 \u00a0 \u00a0 Model modelResponse = trainingPipelineResponse.getModelToUpload();\u00a0 \u00a0 \u00a0 System.out.println(\"\\tModel To Upload\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tName: %s\\n\", modelResponse.getName());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tDisplay Name: %s\\n\", modelResponse.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tDescription: %s\\n\", modelResponse.getDescription());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tMetadata Schema Uri: %s\\n\", modelResponse.getMetadataSchemaUri());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tMeta Data: %s\\n\", modelResponse.getMetadata());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tTraining Pipeline: %s\\n\", modelResponse.getTrainingPipeline());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tArtifact Uri: %s\\n\", modelResponse.getArtifactUri());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tSupported Deployment Resources Types: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedDeploymentResourcesTypesList().toString());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tSupported Input Storage Formats: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedInputStorageFormatsList().toString());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tSupported Output Storage Formats: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedOutputStorageFormatsList().toString());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tCreate Time: %s\\n\", modelResponse.getCreateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tUpdate Time: %s\\n\", modelResponse.getUpdateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tLables: %s\\n\", modelResponse.getLabelsMap());\u00a0 \u00a0 \u00a0 PredictSchemata predictSchemata = modelResponse.getPredictSchemata();\u00a0 \u00a0 \u00a0 System.out.println(\"\\tPredict Schemata\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tInstance Schema Uri: %s\\n\", predictSchemata.getInstanceSchemaUri());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tParameters Schema Uri: %s\\n\", predictSchemata.getParametersSchemaUri());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tPrediction Schema Uri: %s\\n\", predictSchemata.getPredictionSchemaUri());\u00a0 \u00a0 \u00a0 for (Model.ExportFormat supportedExportFormat :\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedExportFormatsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"\\tSupported Export Format\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tId: %s\\n\", supportedExportFormat.getId());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ModelContainerSpec containerSpec = modelResponse.getContainerSpec();\u00a0 \u00a0 \u00a0 System.out.println(\"\\tContainer Spec\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tImage Uri: %s\\n\", containerSpec.getImageUri());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tCommand: %s\\n\", containerSpec.getCommandList());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tArgs: %s\\n\", containerSpec.getArgsList());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tPredict Route: %s\\n\", containerSpec.getPredictRoute());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tHealth Route: %s\\n\", containerSpec.getHealthRoute());\u00a0 \u00a0 \u00a0 for (EnvVar envVar : containerSpec.getEnvList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tEnv\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tName: %s\\n\", envVar.getName());\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tValue: %s\\n\", envVar.getValue());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 for (Port port : containerSpec.getPortsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tPort\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tContainer Port: %s\\n\", port.getContainerPort());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 for (DeployedModelRef deployedModelRef : modelResponse.getDeployedModelsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"\\tDeployed Model\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tEndpoint: %s\\n\", deployedModelRef.getEndpoint());\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tDeployed Model Id: %s\\n\", deployedModelRef.getDeployedModelId());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 Status status = trainingPipelineResponse.getError();\u00a0 \u00a0 \u00a0 System.out.println(\"\\tError\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tCode: %s\\n\", status.getCode());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tMessage: %s\\n\", status.getMessage());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/create-training-pipeline-tabular-classification.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const datasetId = 'YOUR_DATASET_ID';// const modelDisplayName = 'YOUR_MODEL_DISPLAY_NAME';// const trainingPipelineDisplayName = 'YOUR_TRAINING_PIPELINE_DISPLAY_NAME';// const targetColumn = 'YOUR_TARGET_COLUMN';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';const aiplatform = require('@google-cloud/aiplatform');const {definition} =\u00a0 aiplatform.protos.google.cloud.aiplatform.v1.schema.trainingjob;// Imports the Google Cloud Pipeline Service Client libraryconst {PipelineServiceClient} = aiplatform.v1;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst pipelineServiceClient = new PipelineServiceClient(clientOptions);async function createTrainingPipelineTablesClassification() {\u00a0 // Configure the parent resource\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 const transformations = [\u00a0 \u00a0 {auto: {column_name: 'sepal_width'}},\u00a0 \u00a0 {auto: {column_name: 'sepal_length'}},\u00a0 \u00a0 {auto: {column_name: 'petal_length'}},\u00a0 \u00a0 {auto: {column_name: 'petal_width'}},\u00a0 ];\u00a0 const trainingTaskInputsObj = new definition.AutoMlTablesInputs({\u00a0 \u00a0 targetColumn: targetColumn,\u00a0 \u00a0 predictionType: 'classification',\u00a0 \u00a0 transformations: transformations,\u00a0 \u00a0 trainBudgetMilliNodeHours: 8000,\u00a0 \u00a0 disableEarlyStopping: false,\u00a0 \u00a0 optimizationObjective: 'minimize-log-loss',\u00a0 });\u00a0 const trainingTaskInputs = trainingTaskInputsObj.toValue();\u00a0 const modelToUpload = {displayName: modelDisplayName};\u00a0 const inputDataConfig = {\u00a0 \u00a0 datasetId: datasetId,\u00a0 \u00a0 fractionSplit: {\u00a0 \u00a0 \u00a0 trainingFraction: 0.8,\u00a0 \u00a0 \u00a0 validationFraction: 0.1,\u00a0 \u00a0 \u00a0 testFraction: 0.1,\u00a0 \u00a0 },\u00a0 };\u00a0 const trainingPipeline = {\u00a0 \u00a0 displayName: trainingPipelineDisplayName,\u00a0 \u00a0 trainingTaskDefinition:\u00a0 \u00a0 \u00a0 'gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_tables_1.0.0.yaml',\u00a0 \u00a0 trainingTaskInputs,\u00a0 \u00a0 inputDataConfig,\u00a0 \u00a0 modelToUpload,\u00a0 };\u00a0 const request = {\u00a0 \u00a0 parent,\u00a0 \u00a0 trainingPipeline,\u00a0 };\u00a0 // Create training pipeline request\u00a0 const [response] =\u00a0 \u00a0 await pipelineServiceClient.createTrainingPipeline(request);\u00a0 console.log('Create training pipeline tabular classification response');\u00a0 console.log(`Name : ${response.name}`);\u00a0 console.log('Raw response:');\u00a0 console.log(JSON.stringify(response, null, 2));}createTrainingPipelineTablesClassification();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_training_pipeline_tabular_classification_sample.py) \n```\ndef create_training_pipeline_tabular_classification_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 dataset_id: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 model_display_name: str = None,\u00a0 \u00a0 target_column: str = \"target_column\",\u00a0 \u00a0 training_fraction_split: float = 0.8,\u00a0 \u00a0 validation_fraction_split: float = 0.1,\u00a0 \u00a0 test_fraction_split: float = 0.1,\u00a0 \u00a0 budget_milli_node_hours: int = 8000,\u00a0 \u00a0 disable_early_stopping: bool = False,\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 tabular_classification_job = aiplatform.AutoMLTabularTrainingJob(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name, optimization_prediction_type=\"classification\"\u00a0 \u00a0 )\u00a0 \u00a0 my_tabular_dataset = aiplatform.TabularDataset(dataset_name=dataset_id)\u00a0 \u00a0 model = tabular_classification_job.run(\u00a0 \u00a0 \u00a0 \u00a0 dataset=my_tabular_dataset,\u00a0 \u00a0 \u00a0 \u00a0 target_column=target_column,\u00a0 \u00a0 \u00a0 \u00a0 training_fraction_split=training_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 validation_fraction_split=validation_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 test_fraction_split=test_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 budget_milli_node_hours=budget_milli_node_hours,\u00a0 \u00a0 \u00a0 \u00a0 model_display_name=model_display_name,\u00a0 \u00a0 \u00a0 \u00a0 disable_early_stopping=disable_early_stopping,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 print(model.uri)\u00a0 \u00a0 return model\n```\nSelect a tab for your language or environment:You use the [trainingPipelines.create](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines/create) command  to train a model.\nTrain the model.\nBefore using any of the request data, make the following replacements:- : Your region.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : Display name for the training pipeline created  for this operation.\n- : The column (value) you want this model to predict.\n- : (Optional) The weight column. [Learn more](/vertex-ai/docs/tabular-data/classification-regression/prepare-data#weight) .\n- : The maximum amount of time you want the model to train, in  milli node hours (1,000 milli node hours equals one node hour).\n- : Required only if you do not want the  default optimization objective for your prediction type. [Learn more](/vertex-ai/docs/tabular-data/classification-regression/train-model#optimization-objectives) .\n- : The transformation type is provided for each column used to  train the model. [Learn more](/vertex-ai/docs/datasets/data-types-tabular#transformations) .\n- : The name of the column with the specified transformation type. Every  column used to train the model must be specified.\n- : Display name for the newly trained model.\n- : ID for the training Dataset.\n- You can provide a`Split`object to control your data split. For information about  controlling data split, see [Controlling the data split using REST](#data-split) .\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) \nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/trainingPipelines\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"TRAININGPIPELINE_DISPLAY_NAME\",\n \"trainingTaskDefinition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_tabular_1.0.0.yaml\",\n \"trainingTaskInputs\": {\n  \"targetColumn\": \"TARGET_COLUMN\",\n  \"weightColumn\": \"WEIGHT_COLUMN\",\n  \"predictionType\": \"regression\",\n  \"trainBudgetMilliNodeHours\": TRAINING_BUDGET,\n  \"optimizationObjective\": \"OPTIMIZATION_OBJECTIVE\",\n  \"transformations\": [   {\"TRANSFORMATION_TYPE_1\": {\"column_name\" : \"COLUMN_NAME_1\"} },\n   {\"TRANSFORMATION_TYPE_2\": {\"column_name\" : \"COLUMN_NAME_2\"} },\n   ...\n },\n \"modelToUpload\": {\"displayName\": \"MODEL_DISPLAY_NAME\"},\n \"inputDataConfig\": {\n  \"datasetId\": \"DATASET_ID\",\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/us-central1/trainingPipelines/4567\",\n \"displayName\": \"myModelName\",\n \"trainingTaskDefinition\": \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_tabular_1.0.0.yaml\",\n \"modelToUpload\": {\n \"displayName\": \"myModelName\"\n },\n \"state\": \"PIPELINE_STATE_PENDING\",\n \"createTime\": \"2020-08-18T01:22:57.479336Z\",\n \"updateTime\": \"2020-08-18T01:22:57.479336Z\"\n}\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateTrainingPipelineTabularRegressionSample.java) \n```\nimport com.google.cloud.aiplatform.util.ValueConverter;import com.google.cloud.aiplatform.v1.DeployedModelRef;import com.google.cloud.aiplatform.v1.EnvVar;import com.google.cloud.aiplatform.v1.FilterSplit;import com.google.cloud.aiplatform.v1.FractionSplit;import com.google.cloud.aiplatform.v1.InputDataConfig;import com.google.cloud.aiplatform.v1.LocationName;import com.google.cloud.aiplatform.v1.Model;import com.google.cloud.aiplatform.v1.ModelContainerSpec;import com.google.cloud.aiplatform.v1.PipelineServiceClient;import com.google.cloud.aiplatform.v1.PipelineServiceSettings;import com.google.cloud.aiplatform.v1.Port;import com.google.cloud.aiplatform.v1.PredefinedSplit;import com.google.cloud.aiplatform.v1.PredictSchemata;import com.google.cloud.aiplatform.v1.TimestampSplit;import com.google.cloud.aiplatform.v1.TrainingPipeline;import com.google.cloud.aiplatform.v1beta1.schema.trainingjob.definition.AutoMlTablesInputs;import com.google.cloud.aiplatform.v1beta1.schema.trainingjob.definition.AutoMlTablesInputs.Transformation;import com.google.cloud.aiplatform.v1beta1.schema.trainingjob.definition.AutoMlTablesInputs.Transformation.AutoTransformation;import com.google.cloud.aiplatform.v1beta1.schema.trainingjob.definition.AutoMlTablesInputs.Transformation.TimestampTransformation;import com.google.rpc.Status;import java.io.IOException;import java.util.ArrayList;public class CreateTrainingPipelineTabularRegressionSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String modelDisplayName = \"YOUR_DATASET_DISPLAY_NAME\";\u00a0 \u00a0 String datasetId = \"YOUR_DATASET_ID\";\u00a0 \u00a0 String targetColumn = \"TARGET_COLUMN\";\u00a0 \u00a0 createTrainingPipelineTableRegression(project, modelDisplayName, datasetId, targetColumn);\u00a0 }\u00a0 static void createTrainingPipelineTableRegression(\u00a0 \u00a0 \u00a0 String project, String modelDisplayName, String datasetId, String targetColumn)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 PipelineServiceSettings pipelineServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 PipelineServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (PipelineServiceClient pipelineServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 PipelineServiceClient.create(pipelineServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 LocationName locationName = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 String trainingTaskDefinition =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_tables_1.0.0.yaml\";\u00a0 \u00a0 \u00a0 // Set the columns used for training and their data types\u00a0 \u00a0 \u00a0 ArrayList<Transformation> tranformations = new ArrayList<>();\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(AutoTransformation.newBuilder().setColumnName(\"STRING_5000unique_NULLABLE\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(AutoTransformation.newBuilder().setColumnName(\"INTEGER_5000unique_NULLABLE\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(AutoTransformation.newBuilder().setColumnName(\"FLOAT_5000unique_NULLABLE\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(AutoTransformation.newBuilder().setColumnName(\"FLOAT_5000unique_REPEATED\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(AutoTransformation.newBuilder().setColumnName(\"NUMERIC_5000unique_NULLABLE\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(AutoTransformation.newBuilder().setColumnName(\"BOOLEAN_2unique_NULLABLE\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTimestamp(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TimestampTransformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setColumnName(\"TIMESTAMP_1unique_NULLABLE\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInvalidValuesAllowed(true))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(AutoTransformation.newBuilder().setColumnName(\"DATE_1unique_NULLABLE\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(AutoTransformation.newBuilder().setColumnName(\"TIME_1unique_NULLABLE\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTimestamp(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TimestampTransformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setColumnName(\"DATETIME_1unique_NULLABLE\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInvalidValuesAllowed(true))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutoTransformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setColumnName(\"STRUCT_NULLABLE.STRING_5000unique_NULLABLE\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutoTransformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setColumnName(\"STRUCT_NULLABLE.INTEGER_5000unique_NULLABLE\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutoTransformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setColumnName(\"STRUCT_NULLABLE.FLOAT_5000unique_NULLABLE\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutoTransformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setColumnName(\"STRUCT_NULLABLE.FLOAT_5000unique_REQUIRED\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutoTransformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setColumnName(\"STRUCT_NULLABLE.FLOAT_5000unique_REPEATED\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutoTransformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setColumnName(\"STRUCT_NULLABLE.NUMERIC_5000unique_NULLABLE\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 tranformations.add(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Transformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAuto(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutoTransformation.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setColumnName(\"STRUCT_NULLABLE.TIMESTAMP_1unique_NULLABLE\"))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 AutoMlTablesInputs trainingTaskInputs =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AutoMlTablesInputs.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addAllTransformations(tranformations)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTargetColumn(targetColumn)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setPredictionType(\"regression\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainBudgetMilliNodeHours(8000)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisableEarlyStopping(false)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // supported regression optimisation objectives: minimize-rmse,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // minimize-mae, minimize-rmsle\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setOptimizationObjective(\"minimize-rmse\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 FractionSplit fractionSplit =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 FractionSplit.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingFraction(0.8)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setValidationFraction(0.1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTestFraction(0.1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 InputDataConfig inputDataConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputDataConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDatasetId(datasetId)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setFractionSplit(fractionSplit)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 Model modelToUpload = Model.newBuilder().setDisplayName(modelDisplayName).build();\u00a0 \u00a0 \u00a0 TrainingPipeline trainingPipeline =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TrainingPipeline.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(modelDisplayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingTaskDefinition(trainingTaskDefinition)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setTrainingTaskInputs(ValueConverter.toValue(trainingTaskInputs))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputDataConfig(inputDataConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModelToUpload(modelToUpload)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 TrainingPipeline trainingPipelineResponse =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pipelineServiceClient.createTrainingPipeline(locationName, trainingPipeline);\u00a0 \u00a0 \u00a0 System.out.println(\"Create Training Pipeline Tabular Regression Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\tName: %s\\n\", trainingPipelineResponse.getName());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tDisplay Name: %s\\n\", trainingPipelineResponse.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tTraining Task Definition: %s\\n\", trainingPipelineResponse.getTrainingTaskDefinition());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tTraining Task Inputs: %s\\n\", trainingPipelineResponse.getTrainingTaskInputs());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tTraining Task Metadata: %s\\n\", trainingPipelineResponse.getTrainingTaskMetadata());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tState: %s\\n\", trainingPipelineResponse.getState());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tCreate Time: %s\\n\", trainingPipelineResponse.getCreateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tStart Time: %s\\n\", trainingPipelineResponse.getStartTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tEnd Time: %s\\n\", trainingPipelineResponse.getEndTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tUpdate Time: %s\\n\", trainingPipelineResponse.getUpdateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\tLabels: %s\\n\", trainingPipelineResponse.getLabelsMap());\u00a0 \u00a0 \u00a0 InputDataConfig inputDataConfigResponse = trainingPipelineResponse.getInputDataConfig();\u00a0 \u00a0 \u00a0 System.out.println(\"\\tInput Data Config\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tDataset Id: %s\\n\", inputDataConfigResponse.getDatasetId());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tAnnotations Filter: %s\\n\", inputDataConfigResponse.getAnnotationsFilter());\u00a0 \u00a0 \u00a0 FractionSplit fractionSplitResponse = inputDataConfigResponse.getFractionSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tFraction Split\");\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\t\\tTraining Fraction: %s\\n\", fractionSplitResponse.getTrainingFraction());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\t\\tValidation Fraction: %s\\n\", fractionSplitResponse.getValidationFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTest Fraction: %s\\n\", fractionSplitResponse.getTestFraction());\u00a0 \u00a0 \u00a0 FilterSplit filterSplit = inputDataConfigResponse.getFilterSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tFilter Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTraining Fraction: %s\\n\", filterSplit.getTrainingFilter());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tValidation Fraction: %s\\n\", filterSplit.getValidationFilter());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTest Fraction: %s\\n\", filterSplit.getTestFilter());\u00a0 \u00a0 \u00a0 PredefinedSplit predefinedSplit = inputDataConfigResponse.getPredefinedSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tPredefined Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tKey: %s\\n\", predefinedSplit.getKey());\u00a0 \u00a0 \u00a0 TimestampSplit timestampSplit = inputDataConfigResponse.getTimestampSplit();\u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tTimestamp Split\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTraining Fraction: %s\\n\", timestampSplit.getTrainingFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tValidation Fraction: %s\\n\", timestampSplit.getValidationFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tTest Fraction: %s\\n\", timestampSplit.getTestFraction());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tKey: %s\\n\", timestampSplit.getKey());\u00a0 \u00a0 \u00a0 Model modelResponse = trainingPipelineResponse.getModelToUpload();\u00a0 \u00a0 \u00a0 System.out.println(\"\\tModel To Upload\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tName: %s\\n\", modelResponse.getName());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tDisplay Name: %s\\n\", modelResponse.getDisplayName());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tDescription: %s\\n\", modelResponse.getDescription());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tMetadata Schema Uri: %s\\n\", modelResponse.getMetadataSchemaUri());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tMeta Data: %s\\n\", modelResponse.getMetadata());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tTraining Pipeline: %s\\n\", modelResponse.getTrainingPipeline());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tArtifact Uri: %s\\n\", modelResponse.getArtifactUri());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tSupported Deployment Resources Types: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedDeploymentResourcesTypesList().toString());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tSupported Input Storage Formats: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedInputStorageFormatsList().toString());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tSupported Output Storage Formats: %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedOutputStorageFormatsList().toString());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tCreate Time: %s\\n\", modelResponse.getCreateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tUpdate Time: %s\\n\", modelResponse.getUpdateTime());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tLables: %s\\n\", modelResponse.getLabelsMap());\u00a0 \u00a0 \u00a0 PredictSchemata predictSchemata = modelResponse.getPredictSchemata();\u00a0 \u00a0 \u00a0 System.out.println(\"\\tPredict Schemata\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tInstance Schema Uri: %s\\n\", predictSchemata.getInstanceSchemaUri());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tParameters Schema Uri: %s\\n\", predictSchemata.getParametersSchemaUri());\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t\\tPrediction Schema Uri: %s\\n\", predictSchemata.getPredictionSchemaUri());\u00a0 \u00a0 \u00a0 for (Model.ExportFormat supportedExportFormat :\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelResponse.getSupportedExportFormatsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"\\tSupported Export Format\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tId: %s\\n\", supportedExportFormat.getId());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ModelContainerSpec containerSpec = modelResponse.getContainerSpec();\u00a0 \u00a0 \u00a0 System.out.println(\"\\tContainer Spec\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tImage Uri: %s\\n\", containerSpec.getImageUri());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tCommand: %s\\n\", containerSpec.getCommandList());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tArgs: %s\\n\", containerSpec.getArgsList());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tPredict Route: %s\\n\", containerSpec.getPredictRoute());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tHealth Route: %s\\n\", containerSpec.getHealthRoute());\u00a0 \u00a0 \u00a0 for (EnvVar envVar : containerSpec.getEnvList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tEnv\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tName: %s\\n\", envVar.getName());\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tValue: %s\\n\", envVar.getValue());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 for (Port port : containerSpec.getPortsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"\\t\\tPort\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\t\\tContainer Port: %s\\n\", port.getContainerPort());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 for (DeployedModelRef deployedModelRef : modelResponse.getDeployedModelsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"\\tDeployed Model\");\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tEndpoint: %s\\n\", deployedModelRef.getEndpoint());\u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tDeployed Model Id: %s\\n\", deployedModelRef.getDeployedModelId());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 Status status = trainingPipelineResponse.getError();\u00a0 \u00a0 \u00a0 System.out.println(\"\\tError\");\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tCode: %s\\n\", status.getCode());\u00a0 \u00a0 \u00a0 System.out.format(\"\\t\\tMessage: %s\\n\", status.getMessage());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/create-training-pipeline-tabular-regression.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0* (Not necessary if passing values as arguments)\u00a0*/// const datasetId = 'YOUR_DATASET_ID';// const modelDisplayName = 'YOUR_MODEL_DISPLAY_NAME';// const trainingPipelineDisplayName = 'YOUR_TRAINING_PIPELINE_DISPLAY_NAME';// const targetColumn = 'YOUR_TARGET_COLUMN';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';const aiplatform = require('@google-cloud/aiplatform');const {definition} =\u00a0 aiplatform.protos.google.cloud.aiplatform.v1.schema.trainingjob;// Imports the Google Cloud Pipeline Service Client libraryconst {PipelineServiceClient} = aiplatform.v1;// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst pipelineServiceClient = new PipelineServiceClient(clientOptions);async function createTrainingPipelineTablesRegression() {\u00a0 // Configure the parent resource\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 const transformations = [\u00a0 \u00a0 {auto: {column_name: 'STRING_5000unique_NULLABLE'}},\u00a0 \u00a0 {auto: {column_name: 'INTEGER_5000unique_NULLABLE'}},\u00a0 \u00a0 {auto: {column_name: 'FLOAT_5000unique_NULLABLE'}},\u00a0 \u00a0 {auto: {column_name: 'FLOAT_5000unique_REPEATED'}},\u00a0 \u00a0 {auto: {column_name: 'NUMERIC_5000unique_NULLABLE'}},\u00a0 \u00a0 {auto: {column_name: 'BOOLEAN_2unique_NULLABLE'}},\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 timestamp: {\u00a0 \u00a0 \u00a0 \u00a0 column_name: 'TIMESTAMP_1unique_NULLABLE',\u00a0 \u00a0 \u00a0 \u00a0 invalid_values_allowed: true,\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 \u00a0 {auto: {column_name: 'DATE_1unique_NULLABLE'}},\u00a0 \u00a0 {auto: {column_name: 'TIME_1unique_NULLABLE'}},\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 timestamp: {\u00a0 \u00a0 \u00a0 \u00a0 column_name: 'DATETIME_1unique_NULLABLE',\u00a0 \u00a0 \u00a0 \u00a0 invalid_values_allowed: true,\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 \u00a0 {auto: {column_name: 'STRUCT_NULLABLE.STRING_5000unique_NULLABLE'}},\u00a0 \u00a0 {auto: {column_name: 'STRUCT_NULLABLE.INTEGER_5000unique_NULLABLE'}},\u00a0 \u00a0 {auto: {column_name: 'STRUCT_NULLABLE.FLOAT_5000unique_NULLABLE'}},\u00a0 \u00a0 {auto: {column_name: 'STRUCT_NULLABLE.FLOAT_5000unique_REQUIRED'}},\u00a0 \u00a0 {auto: {column_name: 'STRUCT_NULLABLE.FLOAT_5000unique_REPEATED'}},\u00a0 \u00a0 {auto: {column_name: 'STRUCT_NULLABLE.NUMERIC_5000unique_NULLABLE'}},\u00a0 \u00a0 {auto: {column_name: 'STRUCT_NULLABLE.BOOLEAN_2unique_NULLABLE'}},\u00a0 \u00a0 {auto: {column_name: 'STRUCT_NULLABLE.TIMESTAMP_1unique_NULLABLE'}},\u00a0 ];\u00a0 const trainingTaskInputsObj = new definition.AutoMlTablesInputs({\u00a0 \u00a0 transformations,\u00a0 \u00a0 targetColumn,\u00a0 \u00a0 predictionType: 'regression',\u00a0 \u00a0 trainBudgetMilliNodeHours: 8000,\u00a0 \u00a0 disableEarlyStopping: false,\u00a0 \u00a0 optimizationObjective: 'minimize-rmse',\u00a0 });\u00a0 const trainingTaskInputs = trainingTaskInputsObj.toValue();\u00a0 const modelToUpload = {displayName: modelDisplayName};\u00a0 const inputDataConfig = {\u00a0 \u00a0 datasetId: datasetId,\u00a0 \u00a0 fractionSplit: {\u00a0 \u00a0 \u00a0 trainingFraction: 0.8,\u00a0 \u00a0 \u00a0 validationFraction: 0.1,\u00a0 \u00a0 \u00a0 testFraction: 0.1,\u00a0 \u00a0 },\u00a0 };\u00a0 const trainingPipeline = {\u00a0 \u00a0 displayName: trainingPipelineDisplayName,\u00a0 \u00a0 trainingTaskDefinition:\u00a0 \u00a0 \u00a0 'gs://google-cloud-aiplatform/schema/trainingjob/definition/automl_tables_1.0.0.yaml',\u00a0 \u00a0 trainingTaskInputs,\u00a0 \u00a0 inputDataConfig,\u00a0 \u00a0 modelToUpload,\u00a0 };\u00a0 const request = {\u00a0 \u00a0 parent,\u00a0 \u00a0 trainingPipeline,\u00a0 };\u00a0 // Create training pipeline request\u00a0 const [response] =\u00a0 \u00a0 await pipelineServiceClient.createTrainingPipeline(request);\u00a0 console.log('Create training pipeline tabular regression response');\u00a0 console.log(`Name : ${response.name}`);\u00a0 console.log('Raw response:');\u00a0 console.log(JSON.stringify(response, null, 2));}createTrainingPipelineTablesRegression();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_training_pipeline_tabular_regression_sample.py) \n```\ndef create_training_pipeline_tabular_regression_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 dataset_id: str,\u00a0 \u00a0 location: str = \"us-central1\",\u00a0 \u00a0 model_display_name: str = \"my_model\",\u00a0 \u00a0 target_column: str = \"target_column\",\u00a0 \u00a0 training_fraction_split: float = 0.8,\u00a0 \u00a0 validation_fraction_split: float = 0.1,\u00a0 \u00a0 test_fraction_split: float = 0.1,\u00a0 \u00a0 budget_milli_node_hours: int = 8000,\u00a0 \u00a0 disable_early_stopping: bool = False,\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 tabular_regression_job = aiplatform.AutoMLTabularTrainingJob(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name, optimization_prediction_type=\"regression\"\u00a0 \u00a0 )\u00a0 \u00a0 my_tabular_dataset = aiplatform.TabularDataset(dataset_name=dataset_id)\u00a0 \u00a0 model = tabular_regression_job.run(\u00a0 \u00a0 \u00a0 \u00a0 dataset=my_tabular_dataset,\u00a0 \u00a0 \u00a0 \u00a0 target_column=target_column,\u00a0 \u00a0 \u00a0 \u00a0 training_fraction_split=training_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 validation_fraction_split=validation_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 test_fraction_split=test_fraction_split,\u00a0 \u00a0 \u00a0 \u00a0 budget_milli_node_hours=budget_milli_node_hours,\u00a0 \u00a0 \u00a0 \u00a0 model_display_name=model_display_name,\u00a0 \u00a0 \u00a0 \u00a0 disable_early_stopping=disable_early_stopping,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 print(model.uri)\u00a0 \u00a0 return model\n```## Control the data split using RESTYou can control how your training data is split between the training, validation, and test sets. When you use the Vertex AI API, use the [Split object](/vertex-ai/docs/reference/rest/v1/projects.locations.trainingPipelines#InputDataConfig) to determine your data split. The `Split` object can be included in the `inputDataConfig` object as one of several object types, each of which provides a different way to split the training data.\nThe methods you can use to split your data depend on your data type:- `FractionSplit` :- : The fraction of the training data to be used for the training set.\n- : The fraction of the training data to be used for the validation set.\n- : The fraction of the training data to be used for the test set.\nIf any of the fractions are specified, all must be specified. The fractions must add up to 1.0. [Learn more](/vertex-ai/docs/tabular-data/data-splits) .```\n\"fractionSplit\": {\n\"trainingFraction\": TRAINING_FRACTION,\n\"validationFraction\": VALIDATION_FRACTION,\n\"testFraction\": TEST_FRACTION\n},\n```\n- `PredefinedSplit` :- : The column containing the data split values (`TRAIN`,`VALIDATION`,`TEST`).\nManually specify the data split for each row by using a split column. [Learn more](/vertex-ai/docs/tabular-data/data-splits) .```\n\"predefinedSplit\": {\n \"key\": DATA_SPLIT_COLUMN\n},\n```\n- `TimestampSplit` :- : The percentage of the training data to be used for the training set. Defaults to 0.80.\n- : The percentage of the training data to be used for the validation set. Defaults to 0.10.\n- : The percentage of the training data to be used for the test set. Defaults to 0.10.\n- : The column containing the timestamps.\nIf any of the fractions are specified, all must be specified. The fractions must add up to 1.0. [Learn more](/vertex-ai/docs/tabular-data/data-splits) .```\n\"timestampSplit\": {\n \"trainingFraction\": TRAINING_FRACTION,\n \"validationFraction\": VALIDATION_FRACTION,\n \"testFraction\": TEST_FRACTION,\n \"key\": TIME_COLUMN\n}\n```## Optimization objectives for classification or regression models\nWhen you train a model, Vertex AI selects a default optimization objective based on your model type and the data type used for your target column.\n| Optimization objective | API value     | Use this objective if you want to...                            |\n|:-------------------------|:-----------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------|\n| AUC ROC     | maximize-au-roc    | Maximize the area under the receiver operating characteristic (ROC) curve. Distinguishes between classes. Default value for binary classification. |\n| Log loss     | minimize-log-loss   | Keep prediction probabilities as accurate as possible. Only supported objective for multi-class classification.         |\n| AUC PR     | maximize-au-prc    | Maximize the area under the precision-recall curve. Optimizes results for predictions for the less common class.         |\n| Precision at Recall  | maximize-precision-at-recall | Optimize precision at a specific recall value.                          |\n| Recall at Precision  | maximize-recall-at-precision | Optimize recall at a specific precision value.                          |\n| Optimization objective | API value  | Use this objective if you want to...                                  |\n|:-------------------------|:---------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| RMSE      | minimize-rmse | Minimize root-mean-squared error (RMSE). Captures more extreme values accurately. Default value.                   |\n| MAE      | minimize-mae | Minimize mean-absolute error (MAE). Views extreme values as outliers with less impact on model.                    |\n| RMSLE     | minimize-rmsle | Minimize root-mean-squared log error (RMSLE). Penalizes error on relative size rather than absolute value. Useful when both predicted and actual values can be quite large. |\n## What's next\n- [Evaluate your model](/vertex-ai/docs/tabular-data/classification-regression/evaluate-model) .\n- Learn how to [export your model](/vertex-ai/docs/export/export-model-tabular) .", "guide": "Vertex AI"}