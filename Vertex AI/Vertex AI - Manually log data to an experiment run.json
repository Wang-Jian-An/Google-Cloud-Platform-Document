{"title": "Vertex AI - Manually log data to an experiment run", "url": "https://cloud.google.com/vertex-ai/docs/experiments/log-data", "abstract": "# Vertex AI - Manually log data to an experiment run\nFor logging purposes, use the Vertex AI SDK for Python.\nSupported metrics and parameters:Note: When the optional `resume` parameter is specified as `TRUE` , the previously started run resumes. When not specified, `resume` defaults to `FALSE` and a new run is created.\nThe following sample uses the [init](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_init) method, from the [aiplatform functions](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#functions) .Summary metrics are single value scalar metrics stored next to time series metrics and represent a final summary of an .\nOne example use case is early stopping where a patience configuration allows continued training but the candidate model is restored from an earlier step and the metrics calculated for the model at that step would be represented as a summary metric because the latest time series metric is not representative of the restored model. The [log_metrics](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_metrics) API for summary metrics is used for this purpose.#", "content": "## Python [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/experiment_tracking/log_metrics_sample.py) \n```\ndef log_metrics_sample(\u00a0 \u00a0 experiment_name: str,\u00a0 \u00a0 run_name: str,\u00a0 \u00a0 metrics: Dict[str, float],\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,):\u00a0 \u00a0 aiplatform.init(experiment=experiment_name, project=project, location=location)\u00a0 \u00a0 aiplatform.start_run(run=run_name)\u00a0 \u00a0 aiplatform.log_metrics(metrics)\n```\n- `experiment_name`: Provide a name for your experiment. You can find your   list of experiments in the Google Cloud console by selecting **Experiments** in the section nav.\n- `run_name`: Specify a run name (see [start_run](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_start_run) ).\n- `metric`: Metrics key-value pairs. For example:`{'learning_rate': 0.1}`\n- `project`: Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) . You can find these in the Google Cloud console [welcome](https://console.cloud.google.com/welcome) page.\n- `location`: See [List of available locations](/vertex-ai/docs/general/locations) \nTo log time series metrics, Vertex AI Experiments requires a backing Vertex AI TensorBoard instance.All metrics logged through [log_time_series_metrics](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_time_series_metrics) are stored as . Vertex AI TensorBoard is the backing time series metric store.\nThe `experiment_tensorboard` can be set at both the and levels. Setting the `experiment_tensorboard` at the run level overrides the setting at the experiment level. Once the `experiment_tensorboard` is set in a run, the run's `experiment_tensorboard` can't be changed.- Set`experiment_tensorboard`at experiment level:```\n\u00a0 aiplatform.init(experiment='my-experiment',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0experiment_tensorboard='projects/.../tensorboard/my-tb-resource')\n```\n- Set`experiment_tensorboard`at run level: Note: Overrides setting at experiment level.```\n\u00a0 aiplatform.start_run(run_name='my-other-run',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tensorboard='projects/.../.../other-resource')aiplatform.log_time_series_metrics(...)\n```\n### Python [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/experiment_tracking/log_time_series_metrics_sample.py) \n```\ndef log_time_series_metrics_sample(\u00a0 \u00a0 experiment_name: str,\u00a0 \u00a0 run_name: str,\u00a0 \u00a0 metrics: Dict[str, float],\u00a0 \u00a0 step: Optional[int],\u00a0 \u00a0 wall_time: Optional[timestamp_pb2.Timestamp],\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,):\u00a0 \u00a0 aiplatform.init(experiment=experiment_name, project=project, location=location)\u00a0 \u00a0 aiplatform.start_run(run=run_name, resume=True)\u00a0 \u00a0 aiplatform.log_time_series_metrics(metrics=metrics, step=step, wall_time=wall_time)\n```\n- `experiment_name`: Provide the name of your experiment.   You can find your list of experiments in the   Google Cloud console by selecting **Experiments** in the section nav.\n- `run_name`: Specify a run name (see [start_run](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_start_run) ).\n- `metrics`: Dictionary of where keys are metric names and values   are metric values.\n- `step`: Optional. Step index of this data point within the run.\n- `wall_time`: Optional. Wall clock timestamp when this data point is     generated by the end user. If not provided,`wall_time`is generated     based on the value from time.time()\n- `project`: Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) . You can find these in the Google Cloud console [welcome](https://console.cloud.google.com/welcome) page.\n- `location`: See [List of available locations](/vertex-ai/docs/general/locations) \nThe `log_time_series_metrics` API optionally accepts `step` and `walltime` .- `step`: Optional. Step index of this data point within the  run. If not provided, an increment over the latest step among all time  series metrics already logged is used. If the step exists for  any of the provided metric keys, the step is overwritten.\n- `wall_time`: Optional. The seconds after epoch of the  logged metric. If this is not provided the default is to Python's`time.time`.\nFor example:\n```\naiplatform.log_time_series_metrics({\"mse\": 2500.00, \"rmse\": 50.00})\n``````\naiplatform.log_time_series_metrics({\"mse\": 2500.00, \"rmse\": 50.00}, step=8)\n``````\naiplatform.log_time_series_metrics({\"mse\": 2500.00, \"rmse\": 50.00}, step=10)\n```Parameters are keyed input values that configure a run, regulate the behavior of the run, and affect the results of the run. Examples include learning rate, dropout rate, and number of training steps. Log parameters using the [log_params](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_params) method.### Python [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/experiment_tracking/log_params_sample.py) \n```\ndef log_params_sample(\u00a0 \u00a0 experiment_name: str,\u00a0 \u00a0 run_name: str,\u00a0 \u00a0 params: Dict[str, Union[float, int, str]],\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,):\u00a0 \u00a0 aiplatform.init(experiment=experiment_name, project=project, location=location)\u00a0 \u00a0 aiplatform.start_run(run=run_name, resume=True)\u00a0 \u00a0 aiplatform.log_params(params)\n```\n```\naiplatform.log_params({\"learning_rate\": 0.01, \"n_estimators\": 10})\n```\n- `experiment_name`: Provide a name for your experiment.   You can find your list of experiments in the   Google Cloud console by selecting **Experiments** in the section nav.\n- `run_name`: Specify a run name (see [start_run](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_start_run) ).\n- `params`: Parameter key-value pairs   For example:`{'accuracy': 0.9}`(see [log_params](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_params) ). [welcome](https://console.cloud.google.com/welcome) page.\n- `location`: See [List of available locations](/vertex-ai/docs/general/locations) \nIn addition to summary metrics and time series metrics, confusion matrices and ROC curves are commonly used metrics. They can be logged to Vertex AI Experiments using the [log_classification_metrics](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_log_classification_metrics) API.### Python [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/experiment_tracking/log_classification_metrics_sample.py) \n```\ndef log_classification_metrics_sample(\u00a0 \u00a0 experiment_name: str,\u00a0 \u00a0 run_name: str,\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 labels: Optional[List[str]] = None,\u00a0 \u00a0 matrix: Optional[List[List[int]]] = None,\u00a0 \u00a0 fpr: Optional[List[float]] = None,\u00a0 \u00a0 tpr: Optional[List[float]] = None,\u00a0 \u00a0 threshold: Optional[List[float]] = None,\u00a0 \u00a0 display_name: Optional[str] = None,) -> None:\u00a0 \u00a0 aiplatform.init(experiment=experiment_name, project=project, location=location)\u00a0 \u00a0 aiplatform.start_run(run=run_name, resume=True)\u00a0 \u00a0 aiplatform.log_classification_metrics(\u00a0 \u00a0 \u00a0 \u00a0 labels=labels,\u00a0 \u00a0 \u00a0 \u00a0 matrix=matrix,\u00a0 \u00a0 \u00a0 \u00a0 fpr=fpr,\u00a0 \u00a0 \u00a0 \u00a0 tpr=tpr,\u00a0 \u00a0 \u00a0 \u00a0 threshold=threshold,\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name,\u00a0 \u00a0 )\n```\n- `experiment_name`: Provide a name for your experiment.   You can find your list of experiments in the Google Cloud console by selecting **Experiments** in the section nav.\n- `run_name`: Specify a run name   (see [start_run](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform#google_cloud_aiplatform_start_run) ).\n- `project`: Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) . You can find these in the Google Cloud console [welcome](https://console.cloud.google.com/welcome) page.\n- `location`: See [List of available locations](/vertex-ai/docs/general/locations) .\n- `labels`: List of label names for the confusion matrix. Must be set if 'matrix' is set.\n- `matrix`: Values for the confusion matrix. Must be set if 'labels' is set.\n- `fpr`: List of false positive rates for the ROC curve. Must be set if 'tpr' or 'thresholds' is set.\n- `tpr`: List of true positive rates for the ROC curve. Must be set if 'fpr' or 'thresholds' is set.\n- `threshold`: List of thresholds for the ROC curve. Must be set if 'fpr' or 'tpr' is set.\n- `display_name`: The user-defined name for the classification metric artifact.## View experiment runs list in the Google Cloud console\n- In the Google Cloud console, go to the **Experiments** page. [Go to Experiments](https://console.cloud.google.com/vertex-ai/experiments) A list of experiments appears.\n- Select the experiment that you want to check.A list   of runs appears.\nFor more details, see [Compare and analyze runs](/vertex-ai/docs/experiments/compare-analyze-runs#console-compare-analyze-runs) .\n## What's next\n- [Compare and analyze runs](/vertex-ai/docs/experiments/compare-analyze-runs) \n### Notebook tutorial\n- [Compare models trained and evaluated locally](/vertex-ai/docs/experiments/user-journey/uj-compare-models) \n### Blog post\n- [Machine Learning Experiments in Gaming and Why it Matters](https://cloud.google.com/blog/topics/developers-practitioners/machine-learning-experiments-gaming-and-why-it-matters)", "guide": "Vertex AI"}