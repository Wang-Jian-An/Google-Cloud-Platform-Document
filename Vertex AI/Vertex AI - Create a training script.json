{"title": "Vertex AI - Create a training script", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Create a training script\n[run](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomTrainingJob#google_cloud_aiplatform_CustomTrainingJob_run)\nIn this topic, you create the training script, then specify command arguments for your training script.\n", "content": "## Create a training script\nIn this section, you create a training script. This script is a new file in your notebook environment named `task.py` . Later in this tutorial, you pass this script to the [aiplatform.CustomTrainingJob](/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomTrainingJob) constructor. When the script runs, it does the following:\n- Loads the data in the BigQuery dataset you created.\n- Uses the [TensorFlow Keras API](https://www.tensorflow.org/api_docs/python/tf/keras) to build, compile, and train your model.\n- Specifies the number of epochs and the batch size to use when the Keras [Model.fit](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) method is invoked.\n- Specifies where to save model artifacts using the `AIP_MODEL_DIR` environment variable. `AIP_MODEL_DIR` is set by Vertex AI and contains the URI of a directory for saving model artifacts. For more information, see [Environmentvariables for special Cloud Storagedirectories](/vertex-ai/docs/training/code-requirements#environment-variables) .\n- Exports a TensorFlow [SavedModel](https://www.tensorflow.org/api_docs/python/tf/saved_model) to the model directory. For more information, see [Using the SavedModelformat](https://www.tensorflow.org/guide/saved_model#the_savedmodel_format_on_disk) on the TensorFlow website.\nTo create your training script, run the following code in your notebook:\n```\n%%writefile task.pyimport argparseimport numpy as npimport osimport pandas as pdimport tensorflow as tffrom google.cloud import bigqueryfrom google.cloud import storage# Read environmental variablestraining_data_uri = os.getenv(\"AIP_TRAINING_DATA_URI\")validation_data_uri = os.getenv(\"AIP_VALIDATION_DATA_URI\")test_data_uri = os.getenv(\"AIP_TEST_DATA_URI\")# Read argsparser = argparse.ArgumentParser()parser.add_argument('--label_column', required=True, type=str)parser.add_argument('--epochs', default=10, type=int)parser.add_argument('--batch_size', default=10, type=int)args = parser.parse_args()# Set up training variablesLABEL_COLUMN = args.label_column# See https://cloud.google.com/vertex-ai/docs/workbench/managed/executor#explicit-project-selection for issues regarding permissions.PROJECT_NUMBER = os.environ[\"CLOUD_ML_PROJECT_ID\"]bq_client = bigquery.Client(project=PROJECT_NUMBER)# Download a tabledef download_table(bq_table_uri: str):\u00a0 \u00a0 # Remove bq:// prefix if present\u00a0 \u00a0 prefix = \"bq://\"\u00a0 \u00a0 if bq_table_uri.startswith(prefix):\u00a0 \u00a0 \u00a0 \u00a0 bq_table_uri = bq_table_uri[len(prefix) :]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Download the BigQuery table as a dataframe\u00a0 \u00a0 # This requires the \"BigQuery Read Session User\" role on the custom training service account.\u00a0 \u00a0 table = bq_client.get_table(bq_table_uri)\u00a0 \u00a0 return bq_client.list_rows(table).to_dataframe()# Download dataset splitsdf_train = download_table(training_data_uri)df_validation = download_table(validation_data_uri)df_test = download_table(test_data_uri)def convert_dataframe_to_dataset(\u00a0 \u00a0 df_train: pd.DataFrame,\u00a0 \u00a0 df_validation: pd.DataFrame,):\u00a0 \u00a0 df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\u00a0 \u00a0 df_validation_x, df_validation_y = df_validation, df_validation.pop(LABEL_COLUMN)\u00a0 \u00a0 y_train = tf.convert_to_tensor(np.asarray(df_train_y).astype(\"float32\"))\u00a0 \u00a0 y_validation = tf.convert_to_tensor(np.asarray(df_validation_y).astype(\"float32\"))\u00a0 \u00a0 # Convert to numpy representation\u00a0 \u00a0 x_train = tf.convert_to_tensor(np.asarray(df_train_x).astype(\"float32\"))\u00a0 \u00a0 x_test = tf.convert_to_tensor(np.asarray(df_validation_x).astype(\"float32\"))\u00a0 \u00a0 # Convert to one-hot representation\u00a0 \u00a0 num_species = len(df_train_y.unique())\u00a0 \u00a0 y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_species)\u00a0 \u00a0 y_validation = tf.keras.utils.to_categorical(y_validation, num_classes=num_species)\u00a0 \u00a0 dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\u00a0 \u00a0 dataset_validation = tf.data.Dataset.from_tensor_slices((x_test, y_validation))\u00a0 \u00a0 return (dataset_train, dataset_validation)# Create datasetsdataset_train, dataset_validation = convert_dataframe_to_dataset(df_train, df_validation)# Shuffle train setdataset_train = dataset_train.shuffle(len(df_train))def create_model(num_features):\u00a0 \u00a0 # Create model\u00a0 \u00a0 Dense = tf.keras.layers.Dense\u00a0 \u00a0 model = tf.keras.Sequential(\u00a0 \u00a0 \u00a0 \u00a0 [\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Dense(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 100,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 activation=tf.nn.relu,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 kernel_initializer=\"uniform\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 input_dim=num_features,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Dense(75, activation=tf.nn.relu),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Dense(50, activation=tf.nn.relu), \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Dense(25, activation=tf.nn.relu),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Dense(3, activation=tf.nn.softmax),\u00a0 \u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 # Compile Keras model\u00a0 \u00a0 optimizer = tf.keras.optimizers.RMSprop(lr=0.001)\u00a0 \u00a0 model.compile(\u00a0 \u00a0 \u00a0 \u00a0 loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer\u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 return model# Create the modelmodel = create_model(num_features=dataset_train._flat_shapes[0].dims[0].value)# Set up datasetsdataset_train = dataset_train.batch(args.batch_size)dataset_validation = dataset_validation.batch(args.batch_size)# Train the modelmodel.fit(dataset_train, epochs=args.epochs, validation_data=dataset_validation)tf.saved_model.save(model, os.getenv(\"AIP_MODEL_DIR\"))\n```\nAfter you create the script, it appears in the root folder of your notebook:\n## Define arguments for your training script\nYou pass the following command-line arguments to your training script:\n- `label_column` - This identifies the column in your data that contains what you want to predict. In this case, that column is `species` . You defined this in a variable named `LABEL_COLUMN` when you processed your data. For more information, see [Download, preprocess, and split the data](/vertex-ai/docs/tutorials/tabular-bq-prediction/create-dataset#download-process-public-dataset) .\n- `epochs` - This is the number of epochs used when you train your model. An is an iteration over the data when training your model. This tutorial uses 20 epochs.\n- `batch_size` - This is the number of samples that are processed before your model updates. This tutorial uses a batch size of 10.\nTo define the arguments that are passed to your script, run the following code:\n```\nJOB_NAME = \"custom_job_unique\"EPOCHS = 20BATCH_SIZE = 10CMDARGS = [\u00a0 \u00a0 \"--label_column=\" + LABEL_COLUMN,\u00a0 \u00a0 \"--epochs=\" + str(EPOCHS),\u00a0 \u00a0 \"--batch_size=\" + str(BATCH_SIZE),]\n```", "guide": "Vertex AI"}