{"title": "Vertex AI - Share resources across deployments", "url": "https://cloud.google.com/vertex-ai/docs/predictions/model-co-hosting", "abstract": "# Vertex AI - Share resources across deployments\n**    Preview     ** This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nTo learn more,  run the \"Vertex AI Samples\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage5/get_started_with_vertex_endpoint_and_shared_vm.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fml_ops%2Fstage5%2Fget_started_with_vertex_endpoint_and_shared_vm.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage5/get_started_with_vertex_endpoint_and_shared_vm.ipynb)\n", "content": "## Introduction\nA Vertex AI model is deployed to its own virtual machine (VM) instance by default. Vertex AI offers the capability to co-host models on the same VM, which enables the following benefits:\n- Resource sharing across multiple deployments.\n- Cost-effective model serving.\n- Improved utilization of memory and computational resources.\nThis guide describes how to share resources across multiple deployments on Vertex AI.\n## Overview\nModel co-hosting support introduces the concept of a [DeploymentResourcePool](/vertex-ai/docs/reference/rest/v1/projects.locations.deploymentResourcePools) , which groups model deployments that share resources within a single VM. Multiple endpoints can be deployed on the same VM within a `DeploymentResourcePool` . Each endpoint has one or more deployed models. The deployed models for a given endpoint can be grouped under the same or a different `DeploymentResourcePool` .\nIn the following example, you have four models and two endpoints:`Model_A` , `Model_B` , and `Model_C` are deployed to `Endpoint_1` with traffic routed to all of them. `Model_D` is deployed to `Endpoint_2` , which receives 100% of the traffic for that endpoint. Instead of having each model assigned to a separate VM, you can group the models in one of the following ways:\n- Group`Model_A`and`Model_B`to share a VM, which makes them a part of`DeploymentResourcePool_X`.\n- Group`Model_C`and`Model_D`(currently not in the same endpoint) to share a VM, which makes them a part of`DeploymentResourcePool_Y`.\nDifferent Deployment Resource Pools can't share a VM.\n## Considerations\nThere is no upper limit on the number of models that can be deployed to a single Deployment Resource Pool. It depends on the chosen VM shape, model sizes, and traffic patterns. Co-hosting works well when you have many deployed models with sparse traffic, such that assigning a dedicated machine to each deployed model doesn't effectively utilize resources.\nYou can deploy models to the same Deployment Resource Pool concurrently. However, there is a limit of 20 concurrent deployment requests at any given time.\nThere is an increase in CPU utilization when a model is being deployed. The increased CPU utilization can lead to an increase in latency for existing traffic, or it can trigger autoscaling. For the best experience, it is recommended to avoid high traffic to a Deployment Resource Pool while deploying a new model to it.\nExisting traffic to a Deployment Resource Pool is not affected when you undeploy a model from it. No impact is expected to CPU utilization or latency of existing traffic while undeploying a model.\nAn empty Deployment Resource Pool doesn't consume your resource quota. Resources are provisioned to a Deployment Resource Pool when the first model is deployed and released when the last model is undeployed.\nModels in a single Deployment Resource Pool are not isolated from each other in terms of resources such as CPU and memory. If one model takes up most resources, it will trigger autoscaling.\n## Limitations\nThe following limitations exist when deploying models with resource sharing enabled:\n- This feature is only supported for TensorFlow model deployments that use [Vertex AI prediction Tensorflow prebuilt containers](/vertex-ai/docs/predictions/pre-built-containers#tensorflow) . Other model frameworks and custom containers are not yet supported.\n- Only [custom trained](/vertex-ai/docs/training/overview) or [imported models](/vertex-ai/docs/general/import-model) are supported at this time. AutoML models are not yet supported.\n- Only models with the same container image (including framework version) of [Vertex AI TensorFlow prebuilt containers for prediction](/vertex-ai/docs/predictions/pre-built-containers#tensorflow) can be deployed in the same Deployment Resource Pool.\n- The following features are not yet supported: [custom service accounts](/vertex-ai/docs/general/custom-service-account) , [container logging](/vertex-ai/docs/predictions/online-prediction-logging#defaults) , [Vertex Explainable AI](/vertex-ai/docs/explainable-ai) , [VPC Service Controls](/vertex-ai/docs/general/vpc-service-controls) , and [private endpoints](/vertex-ai/docs/predictions/using-private-endpoints) .## Deploy a model\nTo deploy a model to a `DeploymentResourcePool` , complete the following steps:\n- [Create a Deployment Resource Pool](#create_a_deployment_resource_pool) if needed.\n- [Create an Endpoint](#create_endpoint) if needed.\n- [Retrieve the Endpoint ID](#retrieve_endpoint_id) .\n- [Deploy the model to the Endpoint](#deploy_model_in_a_deployment_resource_pool) in the Deployment Resource Pool.\n### Create a Deployment Resource Pool\nIf you are deploying a model to an existing `DeploymentResourcePool` , skip this step:\nUse [CreateDeploymentResourcePool](/vertex-ai/docs/reference/rest/v1/projects.locations.deploymentResourcePools/create) to create a resource pool.\n- In the Google Cloud console, go to the Vertex AI **Deployment Resource Pools** page. [Go to Deployment Resource Pools](https://console.cloud.google.com/vertex-ai/online-prediction/deployment-resource-pools) \n- Click **Create** and fill out the form (shown below).Before using any of the request data, make the following replacements:- : The region where you are using Vertex AI.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : Optional. The machine resources used for each node of this deployment. Its default setting is`n1-standard-2`. [Learn more about machine types.](/vertex-ai/docs/predictions/configure-compute) \n- : The type of accelerator to be attached to the machine. Optional  ifis not specified or is zero. Not recommended for  AutoML models or custom-trained models that are using non-GPU images. [Learn more](/vertex-ai/docs/predictions/configure-compute#gpus) .\n- : The number of accelerators for each replica to use.  Optional. Should be zero or unspecified for AutoML models or custom-trained models  that are using non-GPU images.\n- : The minimum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to the maximum number of nodes and never fewer than this number of nodes. This value must be greater than or equal to 1.\n- : The maximum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to this number of nodes and never fewer than the minimum number of nodes.\n- : A name for your`DeploymentResourcePool`. The maximum length is 63 characters, and valid characters are /^[a-z]([a-z0-9-]{0,61}[a-z0-9])?$/.\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1beta1/projects/PROJECT_ID/locations/LOCATION_ID/deploymentResourcePools\n```\nRequest JSON body:\n```\n{\n \"deploymentResourcePool\":{\n \"dedicatedResources\":{\n  \"machineSpec\":{\n  \"machineType\":\"MACHINE_TYPE\",\n  \"acceleratorType\":\"ACCELERATOR_TYPE\",\n  \"acceleratorCount\":\"ACCELERATOR_COUNT\"\n  },\n  \"minReplicaCount\":MIN_REPLICA_COUNT, \n  \"maxReplicaCount\":MAX_REPLICA_COUNT\n }\n },\n \"deploymentResourcePoolId\":\"DEPLOYMENT_RESOURCE_POOL_ID\"\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/deploymentResourcePools/DEPLOYMENT_RESOURCE_POOL_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1beta1.CreateDeploymentResourcePoolOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2022-06-15T05:48:06.383592Z\",\n  \"updateTime\": \"2022-06-15T05:48:06.383592Z\"\n }\n }\n}\n```\nYou can poll for the status of the operation until the response includes `\"done\": true` .\n### Create Endpoint\nFollow [these instructions](/vertex-ai/docs/predictions/deploy-model-api#create-endpoint) to create an Endpoint. This step is the same as a single-model deployment.\n### Retrieve Endpoint ID\nFollow [these instructions](/vertex-ai/docs/predictions/deploy-model-api#retrieve_the_endpoint_id) to retrieve the Endpoint ID. This step is the same as a single-model deployment.\n### Deploy model in a Deployment Resource Pool\nAfter you create a `DeploymentResourcePool` and an Endpoint, you are ready to deploy using the `DeployModel` API method. This process is similar to a single-model deployment. If there is a `DeploymentResourcePool` , specify `shared_resources` of `DeployModel` with the resource name of the `DeploymentResourcePool` that you are deploying.\n- In the Google Cloud console, go to the Vertex AI **Model Registry** page. [Go to Model Registry](https://console.cloud.google.com/vertex-ai/models) \n- Find your model and click **Deploy to endpoint** .\n- Under **Model settings** (shown below), select **Deploy to a shared deployment resource pool** .Before using any of the request data, make the following replacements:- : The region where you are using Vertex AI.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : The ID for the endpoint.\n- : The ID for the model to be deployed.\n- : A name for the`DeployedModel`. You can use the display name of the`Model`for the`DeployedModel`as well.\n- : A name for your`DeploymentResourcePool`. The maximum length is 63 characters, and valid characters are /^[a-z]([a-z0-9-]{0,61}[a-z0-9])?$/.\n- : The percentage of the prediction traffic to this endpoint  to be routed to the model being deployed with this operation. Defaults to 100. All traffic  percentages must add up to 100. [Learn more about traffic splits](/vertex-ai/docs/general/deployment#models-endpoint) .\n- : Optional. If other models are deployed to this endpoint, you  must update their traffic split percentages so that all percentages add up to 100.\n- : The traffic split percentage value for the deployed model id  key.\n- : Your project's automatically generated [project number](/resource-manager/docs/creating-managing-projects#identifiers) \nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1beta1/projects/PROJECT/locations/LOCATION/endpoints/ENDPOINT_ID:deployModel\n```\nRequest JSON body:\n```\n{\n \"deployedModel\": {\n \"model\": \"projects/PROJECT/locations/us-central1/models/MODEL_ID\",\n \"displayName\": \"DEPLOYED_MODEL_NAME\",\n \"sharedResources\":\"projects/PROJECT/locations/us-central1/deploymentResourcePools/DEPLOYMENT_RESOURCE_POOL_ID\"\n },\n \"trafficSplit\": {\n \"0\": TRAFFIC_SPLIT_THIS_MODEL,\n \"DEPLOYED_MODEL_ID_1\": TRAFFIC_SPLIT_MODEL_1,\n \"DEPLOYED_MODEL_ID_2\": TRAFFIC_SPLIT_MODEL_2\n },\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION/endpoints/ENDPOINT_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1beta1.DeployModelOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2022-06-19T17:53:16.502088Z\",\n  \"updateTime\": \"2022-06-19T17:53:16.502088Z\"\n }\n }\n}\n```\nRepeat the above request with different models that have the same shared resources to deploy multiple models to the same Deployment Resource Pool.\n### Get predictions\nYou can [send prediction requests](/vertex-ai/docs/predictions/get-online-predictions) to a model in a `DeploymentResourcePool` as you would to any other model deployed on Vertex AI.", "guide": "Vertex AI"}