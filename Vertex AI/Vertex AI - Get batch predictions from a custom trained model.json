{"title": "Vertex AI - Get batch predictions from a custom trained model", "url": "https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions", "abstract": "# Vertex AI - Get batch predictions from a custom trained model\nThis page shows you how to get from your custom trained models using the Google Cloud console or the Vertex AI API.\nTo make a batch prediction request, you specify an [inputsource](#input_data_requirements) and an output location, either Cloud Storage or BigQuery, where Vertex AI stores predictions results.\nTo minimize processing time, your input and output locations must be in the same region or multi-region. For example, if your input is in `us-central1` , then your output can be in `us-central1` or `US` , but not `europe-west4` . To learn more, see [Cloud Storage locations](/storage/docs/locations) and [BigQuerylocations](/bigquery/docs/locations) .\nYour input and output must also be in the same region or multi-region as your model.\n", "content": "## Input data requirements\nThe input for batch requests specifies the items to send to your model for prediction. We support the following input formats:\n**Note:** Vertex AI transforms your input to JSON instances before sending it to the prediction container. Make sure your prediction container accepts instances as described in [Custom containerrequirements](/vertex-ai/docs/predictions/custom-container-requirements#prediction) .\n**Note:** For [PyTorch prebuiltcontainer](/vertex-ai/docs/predictions/pre-built-containers#pytorch) , Vertex AI wraps each instance in a `data` field before sending it to the prediction container. This is because TorchServe's default handlers expect each instance to be wrapped in a `data` field.\nUse a JSON Lines file to specify a list of input instances to make predictions about. Store the JSON Lines file in a Cloud Storage bucket.\n **Example 1** \nThe following example shows a JSON Lines file where each line contains an array:\n```\n[1, 2, 3, 4]\n[5, 6, 7, 8]\n```\nHere is what is sent to the prediction container in the HTTP request body:```\n{\"instances\": [ [1, 2, 3, 4], [5, 6, 7, 8] ]}\n``````\n{\"instances\": [{ \"data\": [1, 2, 3, 4] },\n{ \"data\": [5, 6, 7, 8] } ]}\n```\n **Example 2** \nThe following example shows a JSON Lines file where each line contains an object.\n```\n{ \"values\": [1, 2, 3, 4], \"key\": 1 }\n{ \"values\": [5, 6, 7, 8], \"key\": 2 }\n```\nHere is what is sent to the prediction container in the HTTP request body. Note that the same request body is sent to all containers.\n```\n{\"instances\": [ { \"values\": [1, 2, 3, 4], \"key\": 1 },\n { \"values\": [5, 6, 7, 8], \"key\": 2 }\n]}\n```\n **Example 3** \nFor PyTorch prebuilt containers, make sure that you wrap each instance in a `data` field as required by TorchServe's default handler; Vertex AI will not wrap your instances for you. For example:\n```\n{ \"data\": { \"values\": [1, 2, 3, 4], \"key\": 1 } }\n{ \"data\": { \"values\": [5, 6, 7, 8], \"key\": 2 } }\n```\nHere is what is sent to the prediction container in the HTTP request body:\n```\n{\"instances\": [ { \"data\": { \"values\": [1, 2, 3, 4], \"key\": 1 } },\n { \"data\": { \"values\": [5, 6, 7, 8], \"key\": 2 } }\n]}\n```\nSave input instances in the [TFRecordformat](https://www.tensorflow.org/guide/data#consuming_tfrecord_data) . You can optionally compress the TFRecord files with Gzip. Store the TFRecord files in a Cloud Storage bucket.\nVertex AI reads each instance in your TFRecord files as binary, then base64-encodes the instance as JSON object with a single key named `b64` .\nHere is what is sent to the prediction container in the HTTP request body:```\n{\"instances\": [{ \"b64\": \"b64EncodedASCIIString\" },\n{ \"b64\": \"b64EncodedASCIIString\" } ]}\n``````\n{\"instances\": [ { \"data\": {\"b64\": \"b64EncodedASCIIString\" } }, { \"data\": {\"b64\": \"b64EncodedASCIIString\" } }\n]}\n```\nMake sure your prediction container knows how to decode the instance.\nSpecify one input instance per row in a CSV file. The first row must be a header row. You must enclose all strings in double quotation marks (\"). We do not accept cell value including newline character. Non-quoted values are read as floating point numbers.\nThe following example shows a CSV file with two input instances:\n```\n\"input1\",\"input2\",\"input3\"\n0.1,1.2,\"cat1\"\n4.0,5.0,\"cat2\"\n```\nHere is what is sent to the prediction container in the HTTP request body:```\n{\"instances\": [ [0.1,1.2,\"cat1\"], [4.0,5.0,\"cat2\"] ]}\n``````\n{\"instances\": [{ \"data\": [0.1,1.2,\"cat1\"] },\n{ \"data\": [4.0,5.0,\"cat2\"] } ]}\n```\nCreate a text file where each row is the Cloud Storage URI to a file. Vertex AI reads the contents of each file as binary, then base64- encodes the instance as JSON object with a single key named `b64` .\nIf you plan to use the Google Cloud console to get batch predictions, paste your file list directly into the Google Cloud console. Otherwise save your file list in a Cloud Storage bucket.\nThe following example shows a file list with two input instances:\n```\ngs://path/to/image/image1.jpg\ngs://path/to/image/image2.jpg\n```\nHere is what is sent to the prediction container in the HTTP request body:```\n{ \"instances\": [{ \"b64\": \"b64EncodedASCIIString\" },\n{ \"b64\": \"b64EncodedASCIIString\" } ]}\n``````\n{ \"instances\": [ { \"data\": { \"b64\": \"b64EncodedASCIIString\" } }, { \"data\": { \"b64\": \"b64EncodedASCIIString\" } }\n]}\n```\nMake sure your prediction container knows how to decode the instance.\nSpecify a BigQuery table as `projectId.datasetId.tableId` . Vertex AI transforms each row from the table to a JSON instance.\nFor example, if your table contains the following:\n| Column 1 | Column 2 | Column 3 |\n|-----------:|-----------:|:-----------|\n|   1 |   3 | \"Cat1\"  |\n|   2 |   4 | \"Cat2\"  |\nHere is what is sent to the prediction container in the HTTP request body:```\n{\"instances\": [ [1.0,3.0,\"cat1\"], [2.0,4.0,\"cat2\"] ]}\n``````\n{\"instances\": [{ \"data\": [1.0,3.0,\"cat1\"] },\n{ \"data\": [2.0,4.0,\"cat2\"] } ]}\n```\nHere is how BigQuery data types are converted to JSON:\n| BigQuery Type | JSON Type | Example value      |\n|:----------------|:------------|:-----------------------------------|\n| String   | String  | \"abc\"        |\n| Integer   | Integer  | 1         |\n| Float   | Float  | 1.2        |\n| Numeric   | Float  | 4925.000000000      |\n| Boolean   | Boolean  | true        |\n| TimeStamp  | String  | \"2019-01-01 23:59:59.999999+00:00\" |\n| Date   | String  | \"2018-12-31\"      |\n| Time   | String  | \"23:59:59.999999\"     |\n| DateTime  | String  | \"2019-01-01T00:00:00\"    |\n| Record   | Object  | { \"A\": 1,\"B\": 2}     |\n| Repeated Type | Array[Type] | [1, 2]        |\n| Nested Record | Object  | {\"A\": {\"a\": 0}, \"B\": 1}   |\n## Partition data\nBatch prediction uses MapReduce to shard the input to each replica. To make use of the MapReduce features, the input should be partitionable.\nVertex AI automatically partitions **BigQuery** , **file list** , and **JSON lines** input.\nVertex AI does not automatically partition **CSV** files because they are not naturally partition-friendly. Rows in CSV files are not self-descriptive, typed, and may contain new lines. We recommend against using CSV input for throughput sensitive applications.\nFor **TFRecord** input, make sure you manually partition the data by splitting the instances into smaller files and passing the files to the job with a wildcard (for example, `gs://my-bucket/*.tfrecord` ). The number of files should be at least the number of replicas specified.\n## Filter and transform input data\nYou can filter and/or transform your batch input by specifying [instanceConfig](/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#instanceconfig) in your [BatchPredictionJob](/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs) request.\n**Filtering** lets you either exclude certain fields that are in the input data from your prediction request, or include only a subset of fields from the input data in your prediction request, without having to do any custom pre/post-processing in the prediction container. This is useful when your input data file has extra columns that model doesn't need, such as keys or additional data.\n**Transforming** lets you send the instances to your prediction container in either a JSON `array` or `object` format. See [instanceType](/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#instanceconfig) for more information.\nFor example, if your input table contains the following:\n| customerId | col1 | col2 |\n|-------------:|-------:|-------:|\n|   1001 |  1 |  2 |\n|   1002 |  5 |  6 |\nand you specify the following `instanceConfig` :\n```\n{\u00a0 \"name\": \"batchJob1\",\u00a0 ...\u00a0 \"instanceConfig\": {\u00a0 \u00a0 \"excludedFields\":\"customerId\"\u00a0 \u00a0 \"instanceType\":\"object\"\u00a0 }}\n```\nThen, the instances in your prediction request are sent as JSON objects, and the `customerId` column is excluded:\n```\n{\"col1\":1,\"col2\":2}{\"col1\":5,\"col2\":6}\n```\nNote that specifying the following `instanceConfig` would yield the same result:\n```\n{\u00a0 \"name\": \"batchJob1\",\u00a0 ...\u00a0 \"instanceConfig\": {\u00a0 \u00a0 \"includedFields\": [\"col1\",\"col2\"]\u00a0 \u00a0 \"instanceType\":\"object\"\u00a0 }}\n```\nFor a demonstration on how to use feature filters, see the [Custom model batchprediction with featurefiltering](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/custom_batch_prediction_feature_filter.ipynb) notebook.\n## Request a batch prediction\nFor batch prediction requests, you can use the Google Cloud console or the Vertex AI API. Depending on the number of input items that you've submitted, a batch prediction task can take some time to complete.\nWhen you request a batch prediction, the prediction container runs as the user-provided [custom serviceaccount](/vertex-ai/docs/general/custom-service-account) . The read/write operations, such as reading the prediction instances from the data source or writing the prediction results, are done using the [Vertex AI serviceagent](/vertex-ai/docs/general/access-control#service-agents) , which by default, has access to BigQuery and Cloud Storage.\nUse the Google Cloud console to request a batch prediction.- In the Google Cloud console, in the Vertex AI section, go to the **Batch predictions** page.\n [Go to the Batch predictions page](https://console.cloud.google.com/vertex-ai/batch-predictions) - Click **Create** to open the **New batch prediction** window.\n- For **Define your batch prediction** , complete the following steps:- Enter a name for the batch prediction.\n- For **Model name** , select the name of the model to use for this batch prediction.\n- For **Select source** , select the source that applies to your [input data](#input_data_requirements) :- If you have formatted your input as JSON Lines, CSV, or TFRecord, select **File on Cloud Storage (JSON Lines, CSV, TFRecord,\nTFRecord Gzip)** . Then specify your input file in the **Source path** field.\n- If you are using a file list as input, select **Files on\nCloud Storage (other)** and paste your file list into the following text box.\n- For BigQuery input, select **BigQuery path** . If you select BigQuery as input, you must also select BigQuery as output and Google-managed encryption key. Customer-managed encryption key (CMEK) is not supported with BigQuery as input/output.\n- In the **Destination path** field, specify the Cloud Storage directory where you want Vertex AI to store batch prediction output.\n- Optionally, you may check **Enable feature attributions for thismodel** , in order to get [feature attributions](/vertex-ai/docs/explainable-ai/overview) as part of the batch prediction response. Then click **Edit** to [configureexplanation settings](/vertex-ai/docs/explainable-ai/configuring-explanations) . (Editing the explanation settings is optional if you previously configured explanation settings for the model, and required otherwise.)\n- Specify compute options for the batch prediction job: **Number of compute nodes** , [Machinetype](/vertex-ai/docs/predictions/configure-compute) , and (optionally) [Acceleratortype](/vertex-ai/docs/predictions/configure-compute#gpus) and **Accelerator count** \n- Optional: [Model Monitoring](/vertex-ai/docs/model-monitoring/overview) analysis for batch predictions is available in [Preview](/products#product-launch-stages) . See the [Prerequisites](/vertex-ai/docs/model-monitoring/model-monitoring-batch-predictions#prerequisites) for adding skew detection configuration to your batch prediction job.- Click to toggle on **Enable model monitoring for this batchprediction** .\n- Select a **Training data source** . Enter the data path or location for the training data source that you selected.\n- Optional: Under **Alert thresholds** , specify thresholds at which to trigger alerts.\n- For **Notification emails** , enter one or more comma-separated email addresses to receive alerts when a model exceeds an alerting threshold.\n- Optional: For **Notification channels** , add [Cloud Monitoring](/monitoring/support/notification-options) channels to receive alerts when a model exceeds an alerting threshold. You can select existing Cloud Monitoring channels or create a new one by clicking **Manage notification channels** . The Console supports PagerDuty, Slack, and Pub/Sub notification channels.\n- Click **Create** .Use the Vertex AI API to send batch prediction requests. Select a tab depending on which tool you are using to get batch predictions:\nBefore using any of the request data, make the following replacements:- : Region where Model is stored and batch prediction job is executed. For example, `us-central1` .\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : Display name for the batch prediction job.\n- : The ID for the model to use for making predictions.\n- : The [format of your inputdata](#input_data_requirements) : `jsonl` , `csv` , `tf-record` , `tf-record-gzip` , or `file-list` .\n- : Cloud Storage URI of your input data. May contain wildcards.\n- : Cloud Storage URI of a directory where you want Vertex AI to save output.\n- : The [machine resources](/vertex-ai/docs/predictions/configure-compute) to be used for this batch prediction job.You can optionally [configure the machineSpecfield](/vertex-ai/docs/reference/rest/v1/MachineSpec) to [useaccelerators](/vertex-ai/docs/predictions/configure-compute#gpus) , but the following example does not demonstrate this.\n- : The number of instances to send in each prediction request; the default is 64. Increasing the batch size can lead to higher throughput, but it can also cause request timeouts.\n- : The number of nodes for this batch prediction job.\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/batchPredictionJobs\n```\nRequest JSON body:\n```\n{\n \"displayName\": \"BATCH_JOB_NAME\",\n \"model\": \"projects/PROJECT_ID/locations/LOCATION_ID/models/MODEL_ID\",\n \"inputConfig\": {\n \"instancesFormat\": \"INPUT_FORMAT\",\n \"gcsSource\": {\n  \"uris\": [\"INPUT_URI\"],\n },\n },\n \"outputConfig\": {\n \"predictionsFormat\": \"jsonl\",\n \"gcsDestination\": {\n  \"outputUriPrefix\": \"OUTPUT_DIRECTORY\",\n },\n },\n \"dedicatedResources\" : {\n \"machineSpec\" : {\n  \"machineType\": MACHINE_TYPE\n },\n \"startingReplicaCount\": STARTING_REPLICA_COUNT\n },\n \"manualBatchTuningParameters\": {\n \"batch_size\": BATCH_SIZE,\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/batchPredictionJobs\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/batchPredictionJobs\" | Select-Object -Expand Content\n```\nYou should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/batchPredictionJobs/BATCH_JOB_ID\",\n \"displayName\": \"BATCH_JOB_NAME 202005291958\",\n \"model\": \"projects/PROJECT_ID/locations/LOCATION_ID/models/MODEL_ID\",\n \"inputConfig\": {\n \"instancesFormat\": \"jsonl\",\n \"gcsSource\": {\n  \"uris\": [  \"INPUT_URI\"\n  ]\n }\n },\n \"outputConfig\": {\n \"predictionsFormat\": \"jsonl\",\n \"gcsDestination\": {\n  \"outputUriPrefix\": \"OUTPUT_DIRECTORY\"\n }\n },\n \"state\": \"JOB_STATE_PENDING\",\n \"createTime\": \"2020-05-30T02:58:44.341643Z\",\n \"updateTime\": \"2020-05-30T02:58:44.341643Z\",\n}\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\nIn the following sample, replace with `jsonl` . To learn how to replace the other placeholders, see the `REST & CMD LINE` tab of this section.\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/CreateBatchPredictionJobSample.java) \n```\nimport com.google.cloud.aiplatform.util.ValueConverter;import com.google.cloud.aiplatform.v1.AcceleratorType;import com.google.cloud.aiplatform.v1.BatchDedicatedResources;import com.google.cloud.aiplatform.v1.BatchPredictionJob;import com.google.cloud.aiplatform.v1.GcsDestination;import com.google.cloud.aiplatform.v1.GcsSource;import com.google.cloud.aiplatform.v1.JobServiceClient;import com.google.cloud.aiplatform.v1.JobServiceSettings;import com.google.cloud.aiplatform.v1.LocationName;import com.google.cloud.aiplatform.v1.MachineSpec;import com.google.cloud.aiplatform.v1.ModelName;import com.google.protobuf.Value;import java.io.IOException;public class CreateBatchPredictionJobSample {\u00a0 public static void main(String[] args) throws IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"PROJECT\";\u00a0 \u00a0 String displayName = \"DISPLAY_NAME\";\u00a0 \u00a0 String modelName = \"MODEL_NAME\";\u00a0 \u00a0 String instancesFormat = \"INSTANCES_FORMAT\";\u00a0 \u00a0 String gcsSourceUri = \"GCS_SOURCE_URI\";\u00a0 \u00a0 String predictionsFormat = \"PREDICTIONS_FORMAT\";\u00a0 \u00a0 String gcsDestinationOutputUriPrefix = \"GCS_DESTINATION_OUTPUT_URI_PREFIX\";\u00a0 \u00a0 createBatchPredictionJobSample(\u00a0 \u00a0 \u00a0 \u00a0 project,\u00a0 \u00a0 \u00a0 \u00a0 displayName,\u00a0 \u00a0 \u00a0 \u00a0 modelName,\u00a0 \u00a0 \u00a0 \u00a0 instancesFormat,\u00a0 \u00a0 \u00a0 \u00a0 gcsSourceUri,\u00a0 \u00a0 \u00a0 \u00a0 predictionsFormat,\u00a0 \u00a0 \u00a0 \u00a0 gcsDestinationOutputUriPrefix);\u00a0 }\u00a0 static void createBatchPredictionJobSample(\u00a0 \u00a0 \u00a0 String project,\u00a0 \u00a0 \u00a0 String displayName,\u00a0 \u00a0 \u00a0 String model,\u00a0 \u00a0 \u00a0 String instancesFormat,\u00a0 \u00a0 \u00a0 String gcsSourceUri,\u00a0 \u00a0 \u00a0 String predictionsFormat,\u00a0 \u00a0 \u00a0 String gcsDestinationOutputUriPrefix)\u00a0 \u00a0 \u00a0 throws IOException {\u00a0 \u00a0 JobServiceSettings settings =\u00a0 \u00a0 \u00a0 \u00a0 JobServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (JobServiceClient client = JobServiceClient.create(settings)) {\u00a0 \u00a0 \u00a0 // Passing in an empty Value object for model parameters\u00a0 \u00a0 \u00a0 Value modelParameters = ValueConverter.EMPTY_VALUE;\u00a0 \u00a0 \u00a0 GcsSource gcsSource = GcsSource.newBuilder().addUris(gcsSourceUri).build();\u00a0 \u00a0 \u00a0 BatchPredictionJob.InputConfig inputConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BatchPredictionJob.InputConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInstancesFormat(instancesFormat)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setGcsSource(gcsSource)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 GcsDestination gcsDestination =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 GcsDestination.newBuilder().setOutputUriPrefix(gcsDestinationOutputUriPrefix).build();\u00a0 \u00a0 \u00a0 BatchPredictionJob.OutputConfig outputConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BatchPredictionJob.OutputConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setPredictionsFormat(predictionsFormat)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setGcsDestination(gcsDestination)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 MachineSpec machineSpec =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 MachineSpec.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMachineType(\"n1-standard-2\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAcceleratorType(AcceleratorType.NVIDIA_TESLA_K80)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAcceleratorCount(1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 BatchDedicatedResources dedicatedResources =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BatchDedicatedResources.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMachineSpec(machineSpec)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setStartingReplicaCount(1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMaxReplicaCount(1)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 String modelName = ModelName.of(project, location, model).toString();\u00a0 \u00a0 \u00a0 BatchPredictionJob batchPredictionJob =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 BatchPredictionJob.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(displayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModel(modelName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setModelParameters(modelParameters)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputConfig(inputConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setOutputConfig(outputConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDedicatedResources(dedicatedResources)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 LocationName parent = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 BatchPredictionJob response = client.createBatchPredictionJob(parent, batchPredictionJob);\u00a0 \u00a0 \u00a0 System.out.format(\"response: %s\\n\", response);\u00a0 \u00a0 \u00a0 System.out.format(\"\\tName: %s\\n\", response.getName());\u00a0 \u00a0 }\u00a0 }}\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_batch_prediction_job_dedicated_resources_sample.py) \n```\ndef create_batch_prediction_job_dedicated_resources_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 model_resource_name: str,\u00a0 \u00a0 job_display_name: str,\u00a0 \u00a0 gcs_source: Union[str, Sequence[str]],\u00a0 \u00a0 gcs_destination: str,\u00a0 \u00a0 instances_format: str = \"jsonl\",\u00a0 \u00a0 machine_type: str = \"n1-standard-2\",\u00a0 \u00a0 accelerator_count: int = 1,\u00a0 \u00a0 accelerator_type: Union[str, aiplatform_v1.AcceleratorType] = \"NVIDIA_TESLA_K80\",\u00a0 \u00a0 starting_replica_count: int = 1,\u00a0 \u00a0 max_replica_count: int = 1,\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 my_model = aiplatform.Model(model_resource_name)\u00a0 \u00a0 batch_prediction_job = my_model.batch_predict(\u00a0 \u00a0 \u00a0 \u00a0 job_display_name=job_display_name,\u00a0 \u00a0 \u00a0 \u00a0 gcs_source=gcs_source,\u00a0 \u00a0 \u00a0 \u00a0 gcs_destination_prefix=gcs_destination,\u00a0 \u00a0 \u00a0 \u00a0 instances_format=instances_format,\u00a0 \u00a0 \u00a0 \u00a0 machine_type=machine_type,\u00a0 \u00a0 \u00a0 \u00a0 accelerator_count=accelerator_count,\u00a0 \u00a0 \u00a0 \u00a0 accelerator_type=accelerator_type,\u00a0 \u00a0 \u00a0 \u00a0 starting_replica_count=starting_replica_count,\u00a0 \u00a0 \u00a0 \u00a0 max_replica_count=max_replica_count,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 batch_prediction_job.wait()\u00a0 \u00a0 print(batch_prediction_job.display_name)\u00a0 \u00a0 print(batch_prediction_job.resource_name)\u00a0 \u00a0 print(batch_prediction_job.state)\u00a0 \u00a0 return batch_prediction_job\n```\nIf you want feature importance values returned for your predictions, set the `generateExplanation` property to `true` . Note that models don't support feature importance, so you can't include it in your batch prediction requests.\nFeature importance, sometimes called , is part of [Vertex Explainable AI](/vertex-ai/docs/explainable-ai/overview) .You can only set `generateExplanation` to `true` if you have [configuredyour Model for explanations](/vertex-ai/docs/explainable-ai/configuring-explanations) or if you specify the `BatchPredictionJob` 's [explanationSpec field](/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs#BatchPredictionJob.FIELDS.explanation_spec) .\n### Choose machine type and replica count\nScaling horizontally by increasing the number of replicas improves throughput more linearly and predictably than by using larger machine types.\nIn general, we recommend that you specify the smallest machine type possible for your job and increase the number of replicas.\nFor cost-effectiveness, we recommend that you choose the replica count such that your batch prediction job runs for at least 10 minutes. This is because you are billed per replica node hour, which includes the approximately 5 minutes it takes for each replica to start up. It is not cost-effective to process for only a few seconds and then shut down.\nAs general guidance, for thousands of instances, we recommend a `starting_replica_count` in the tens. For millions of instances, we recommend a `starting_replica_count` in the hundreds. You can also use the following forumla to estimate the number of replicas:\n`N / (T * (60 / Tb))`\nWhere:\n- **N:** The number of batches in the job. For example, 1 million instances / 100 batch size = 10,000 batches.\n- **T** : desired time for the batch prediction job. For example, 10 minutes.\n- **Tb** : time in seconds it takes for a replica to process a single batch. For example, 1 second per batch on a 2-core machine type.\nIn our example, 10,000 batches / (10 minutes * (60 / 1s)) rounds up to 17 replicas.\nUnlike online prediction, batch prediction jobs do not [autoscale](/vertex-ai/docs/general/deployment#scaling) . Because all of the input data is known up front, the system partitions the data to each replica when the job starts. The system uses the `starting_replica_count` parameter; the `max_replica_count` parameter is ignored.\nThese recommendations are all approximate guidelines. They are not guaranteed to give optimal throughput for every model. They do not provide exact predictions of processing time and cost. And they do not necessarily capture the best cost/throughput tradeoffs for each scenario. Use them as a reasonable starting point and adjust them as necessary. To measure characteristics such as throughput for your model, run the [Finding ideal machinetype](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/vertex_endpoints/find_ideal_machine_type/find_ideal_machine_type.ipynb) notebook.\nFollow the guidelines [for CPU-only models](#for_cpu-only_models) with the following additional considerations:\n- You might need more CPUs and GPUs (e.g. for data preprocessing).\n- GPU machine types take more time to startup (10 minutes), so you may want to target longer times (for example, at least 20 minutes instead of 10 minutes) for the batch prediction job so that a reasonable proportion of the time and cost is spent on generating predictions.\n### Retrieve batch prediction results\nWhen a batch prediction task is complete, the output of the prediction is stored in the Cloud Storage bucket or BigQuery location that you specified in your request.\n### Example batch prediction result\nThe output folder contains a set of JSON Lines files.\nThe files are named `{gcs_path}/prediction.results-{file_number}-of-{number_of_files_generated}` . The number of files not deterministic due to the distributed nature of batch prediction.\nEach line in the file corresponds to an instance from the input and has the following key/value pairs:\n- `prediction`: contains the value returned by prediction container.\n- `instance`: For FileList, it contains the Cloud Storage URI. For all other input formats, it contains the value that was sent to the prediction container in the HTTP request body.If the HTTP request contains:\n```\n{\u00a0 \"instances\": [\u00a0 \u00a0 [1, 2, 3, 4],\u00a0 \u00a0 [5, 6, 7, 8]]}\n```\nAnd the prediction container returns:\n```\n{\u00a0 \"predictions\": [\u00a0 \u00a0 [0.1,0.9],\u00a0 \u00a0 [0.7,0.3]\u00a0 ],}\n```\nThen, the jsonl output file is:\n```\n{ \"instance\": [1, 2, 3, 4], \"prediction\": [0.1,0.9]}{ \"instance\": [5, 6, 7, 8], \"prediction\": [0.7,0.3]}\n```\nIf the HTTP request contains:\n```\n{\u00a0 \"instances\": [\u00a0 \u00a0 {\"values\": [1, 2, 3, 4], \"key\": 1},\u00a0 \u00a0 {\"values\": [5, 6, 7, 8], \"key\": 2}]}\n```\nAnd the prediction container returns:\n```\n{\u00a0 \"predictions\": [\u00a0 \u00a0 {\"result\":1},\u00a0 \u00a0 {\"result\":0}\u00a0 ],}\n```\nThen, the jsonl output file is:\n```\n{ \"instance\": {\"values\": [1, 2, 3, 4], \"key\": 1}, \"prediction\": {\"result\":1}}{ \"instance\": {\"values\": [5, 6, 7, 8], \"key\": 2}, \"prediction\": {\"result\":0}}\n```\n## Use Explainable AI\nWe don't recommend running [feature-based explanations](/vertex-ai/docs/explainable-ai/overview#feature-based) on a large amount of data. This is because each input can potentially fan out to thousands of requests based on the set of possible feature values which may result in massively increased processing time and cost. In general, a small dataset is enough to understand feature importance.\nBatch prediction does not support [example-based explanations](/vertex-ai/docs/explainable-ai/overview#example-based) .\n## What's next\n- Learn about [Compute resources forprediction](/vertex-ai/docs/predictions/configure-compute) .", "guide": "Vertex AI"}