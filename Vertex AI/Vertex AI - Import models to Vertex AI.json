{"title": "Vertex AI - Import models to Vertex AI", "url": "https://cloud.google.com/vertex-ai/docs/model-registry/import-model", "abstract": "# Vertex AI - Import models to Vertex AI\nThis guide describes how to import models into the Model Registry. After you import your model, it is visible in the Model Registry. From the Model Registry, you can deploy your imported model to an endpoint and run predictions.\n", "content": "## Required roles\nTo get the permissions that you need to import models,   ask your administrator to grant you the [Vertex AI User ](https://cloud.google.com/iam/docs/understanding-roles#aiplatform.user) ( `roles/aiplatform.user` ) IAM role on the project.    For more information about granting roles, see [Manage access](/iam/docs/granting-changing-revoking-access) .\nYou might also be able to get  the required permissions through [custom  roles](/iam/docs/creating-custom-roles) or other [predefined  roles](/iam/docs/understanding-roles) .\n## Prebuilt or custom containers\nWhen you import a model, you associate it with a container for Vertex AI to run prediction requests. You can use [prebuilt containers](/vertex-ai/docs/predictions/pre-built-containers) provided by Vertex AI, or use your own [custom containers](/vertex-ai/docs/predictions/use-custom-container) that you build and push to Artifact Registry.\nYou can use a prebuilt container if your model meets the following requirements:\n- Trained in Python 3.7 or later\n- Trained using TensorFlow, PyTorch, scikit-learn, or XGBoost\n- Exported to meet [framework-specific requirements for one of the prebuiltpredictioncontainers](/vertex-ai/docs/training/exporting-model-artifacts#framework-specific_requirements) \nIf you are importing a tabular AutoML model that you previously [exported](/vertex-ai/docs/export/export-model-tabular) , you must use a specific custom container provided by Vertex AI.\nOtherwise, create a new [custom container](/vertex-ai/docs/predictions/use-custom-container) , or use an existing custom container that you have in Artifact Registry.\n## Upload model artifacts to Cloud Storage\nYou must store your model artifacts in a Cloud Storage bucket, where the bucket's region matches the regional endpoint you're using.\nIf your Cloud Storage bucket is in a different Google Cloud project, you need to [grant Vertex AI access](/vertex-ai/docs/general/access-control#different-project) to read your model artifacts.\nIf you're using a prebuilt container, ensure that your model artifacts have filenames that exactly match the following examples:\n- TensorFlow SavedModel:`saved_model.pb`\n- PyTorch:`model.mar`\n- scikit-learn:`model.joblib`or`model.pkl`\n- XGBoost:`model.bst`,`model.joblib`, or`model.pkl`\nLearn more about [exporting model artifacts for prediction](/vertex-ai/docs/training/exporting-model-artifacts) .\n## Import a model using Google Cloud console\nTo import a model using Google Cloud console:\n- In the Google Cloud console, go to the Vertex AI **Models** page. [Go to the Models page](https://console.cloud.google.com/vertex-ai/models) \n- Click **Import** .\n- Select **Import as new model** to import a new model.\n- Select **Import as new version** to import a model as a version of an existing model. To learn more about model versioning, see [Model versioning](/vertex-ai/docs/model-registry/versioning) .\n- **Name and region** : Enter a name for your model. Select the region that matches both your bucket's region, and the Vertex AI regional endpoint you're using. Click **Continue** .\n- If you expand **Advanced options** , you can optionally decide to add a [customer-managed encryption key](/vertex-ai/docs/general/cmek) .\nDepending on the type of container you are using, select the appropriate tab below.\n- Select **Import model artifacts into a new prebuilt container** .\n- Select the **Model framework** and **Model framework version** you used to train your model.\n- If you want to use GPUs for serving predictions, set the **Acceleratortype** to **GPUs** .You select the type of GPU later on, when you [deploy the model to an endpoint](/vertex-ai/docs/predictions/deploy-model-console) .\n- Specify the Cloud Storage path to the directory that contains your model artifacts.For example, `gs://` `` `/models/` .\n- Leave the **Predict schemata** blank.\n- To import your model Vertex Explainable AI settings, click **Import** .After the import has completed, your model appears on the **Models** page.Otherwise, continue configuring your model by entering your explainability settings on the **Explainability** tab. Learn more about the [Explainability settings](/vertex-ai/docs/explainable-ai/configuring-explanations#import-model-example) .\n- Select **Import an existing custom container** .\n- Set the container image URI.\n- If you want to [provide model artifacts in addition to a containerimage](/vertex-ai/docs/predictions/custom-container-requirements#artifacts) , specify the Cloud Storage path to the directory that contains your model artifacts.For example, `gs://` `` `/models/` .\n- Specify values for any of the other fields.Learn more about these [optional fields](/vertex-ai/docs/predictions/use-custom-container#create-model) .\n- To import your model Vertex Explainable AI settings, click **Import** .After the import has completed, your model appears on the **Models** page.Otherwise, continue configuring your model by entering your explainability settings on the **Explainability** tab. Learn more about the [Explainability settings](/vertex-ai/docs/explainable-ai/configuring-explanations#import-model-example) .\n- Select **Import an existing custom container** .\n- In the **Container image** field, enter `` `-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server-v1:latest` .Replace `` with `us` , `europe` , or `asia` to select which Docker repository you want to pull the Docker image from. Each repository provides the same Docker image, but choosing the [Artifact Registry multi-region](/artifact-registry/docs/repo-locations#location-mr) closest to the machine where you are running Docker might reduce latency.\n- In the **Package location** field, specify the Cloud Storage path to the directory that contains your model artifacts.The path looks similar to the following example:`gs://` `` `/models-` `` `/tf-saved-model/` `` `/`\n- Leave all other fields blank.\n- Click **Import** .After the import has completed, your model appears on the **Models** page. You can use this model just like other AutoML tabular models, except imported AutoML tabular models don't support Vertex Explainable AI.## Import a model programmatically\nThe following examples show how to import a model using various tools:\n**Warning:** Do not invoke [UploadModel](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) or [DeployModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/deployModel) more than once every 2 seconds.\nThe following example uses the [gcloud ai models uploadcommand](/sdk/gcloud/reference/ai/models/upload) :\n```\ngcloud ai models upload \\\u00a0 --region=LOCATION \\\u00a0 --display-name=MODEL_NAME \\\u00a0 --container-image-uri=IMAGE_URI \\\u00a0 --artifact-uri=PATH_TO_MODEL_ARTIFACT_DIRECTORY\n```\nReplace the following:- : The region where you are using Vertex AI.\n- : A display name for the`Model`.\n- : The URI of the container image to use for serving predictions. For example,`us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-1:latest`. Use a [prebuilt container](/vertex-ai/docs/predictions/pre-built-containers) or a [custom container](/vertex-ai/docs/predictions/use-custom-container) .\n- : The Cloud Storage URI (beginning with`gs://`) of a directory in Cloud Storage that contains your [model artifacts](/vertex-ai/docs/training/exporting-model-artifacts) .\nThe preceding example demonstrates all the flags necessary to import most models. If you are not using a prebuilt container for prediction, then you likely need to specify some additional [optionalflags](/sdk/gcloud/reference/ai/models/upload#OPTIONAL-FLAGS) so that Vertex AI can use your container image. These flags, which begin with `--container-` , correspond to fields of your `Model` 's [containerSpec](/vertex-ai/docs/reference/rest/v1/projects.locations.models#modelcontainerspec) .Use the following code sample to upload a model using the [upload method of the model resource](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) .\nBefore using any of the request data, make the following replacements:- : The region where you are using Vertex AI.\n- : Your [project ID](/resource-manager/docs/creating-managing-projects#identifiers) .\n- : A display name for the`Model`.\n- : Optional. A description for the model.\n- : The URI of the container image to use for serving predictions. For example,`us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-1:latest`. Use a [prebuilt container](/vertex-ai/docs/predictions/pre-built-containers) or a [custom container](/vertex-ai/docs/predictions/use-custom-container) .\n- : The Cloud Storage URI (beginning with`gs://`) of a directory in Cloud Storage that contains your [model artifacts](/vertex-ai/docs/training/exporting-model-artifacts) . This variable and the`artifactUri`field are optional if you're using a custom container.\n- `labels`: Optional. Any set of key-value pairs to organize your  models. For example:- \"env\": \"prod\"\n- \"tier\": \"backend\"\n- Specify theandfor any labels that you want to  apply to this training pipeline.\nHTTP method and URL:\n```\nPOST https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/models:upload\n```\nRequest JSON body:\n```\n{\n \"model\": {\n \"displayName\": \"MODEL_NAME\",\n \"predictSchemata\": {},\n \"containerSpec\": {\n  \"imageUri\": \"IMAGE_URI\"\n },\n \"artifactUri\": \"PATH_TO_MODEL_ARTIFACT_DIRECTORY\",\n \"labels\": {\n  \"LABEL_NAME_1\": \"LABEL_VALUE_1\",\n  \"LABEL_NAME_2\": \"LABEL_VALUE_2\"\n }\n }\n}\n```\nTo send your request, choose one of these options:\n **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\ncurl -X POST \\ -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ -H \"Content-Type: application/json; charset=utf-8\" \\ -d @request.json \\ \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/models:upload\"\n``` **Note:** The following command assumes that you have logged in to  the`gcloud`CLI with your user account by running [gcloud init](/sdk/gcloud/reference/init) or [gcloud auth login](/sdk/gcloud/reference/auth/login) ,  or by using [Cloud Shell](/shell/docs) ,  which automatically logs you into the`gcloud`CLI.  You can check the currently active account by running [gcloud auth list](/sdk/gcloud/reference/auth/list) .\nSave the request body in a file named `request.json` ,  and execute the following command:\n```\n$cred = gcloud auth print-access-token$headers = @{ \"Authorization\" = \"Bearer $cred\" }Invoke-WebRequest ` -Method POST ` -Headers $headers ` -ContentType: \"application/json; charset=utf-8\" ` -InFile request.json ` -Uri \"https://LOCATION_ID-aiplatform.googleapis.com/v1/projects/PROJECT_ID/locations/LOCATION_ID/models:upload\" | Select-Object -Expand Content\n```\nBefore trying this sample, follow the Java setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Java API reference documentation](/java/docs/reference/google-cloud-aiplatform/latest/com.google.cloud.aiplatform.v1) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/aiplatform/src/main/java/aiplatform/UploadModelSample.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.aiplatform.v1.LocationName;import com.google.cloud.aiplatform.v1.Model;import com.google.cloud.aiplatform.v1.ModelContainerSpec;import com.google.cloud.aiplatform.v1.ModelServiceClient;import com.google.cloud.aiplatform.v1.ModelServiceSettings;import com.google.cloud.aiplatform.v1.UploadModelOperationMetadata;import com.google.cloud.aiplatform.v1.UploadModelResponse;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class UploadModelSample {\u00a0 public static void main(String[] args)\u00a0 \u00a0 \u00a0 throws InterruptedException, ExecutionException, TimeoutException, IOException {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String project = \"YOUR_PROJECT_ID\";\u00a0 \u00a0 String modelDisplayName = \"YOUR_MODEL_DISPLAY_NAME\";\u00a0 \u00a0 String metadataSchemaUri =\u00a0 \u00a0 \u00a0 \u00a0 \"gs://google-cloud-aiplatform/schema/trainingjob/definition/custom_task_1.0.0.yaml\";\u00a0 \u00a0 String imageUri = \"YOUR_IMAGE_URI\";\u00a0 \u00a0 String artifactUri = \"gs://your-gcs-bucket/artifact_path\";\u00a0 \u00a0 uploadModel(project, modelDisplayName, metadataSchemaUri, imageUri, artifactUri);\u00a0 }\u00a0 static void uploadModel(\u00a0 \u00a0 \u00a0 String project,\u00a0 \u00a0 \u00a0 String modelDisplayName,\u00a0 \u00a0 \u00a0 String metadataSchemaUri,\u00a0 \u00a0 \u00a0 String imageUri,\u00a0 \u00a0 \u00a0 String artifactUri)\u00a0 \u00a0 \u00a0 throws IOException, InterruptedException, ExecutionException, TimeoutException {\u00a0 \u00a0 ModelServiceSettings modelServiceSettings =\u00a0 \u00a0 \u00a0 \u00a0 ModelServiceSettings.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEndpoint(\"us-central1-aiplatform.googleapis.com:443\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (ModelServiceClient modelServiceClient = ModelServiceClient.create(modelServiceSettings)) {\u00a0 \u00a0 \u00a0 String location = \"us-central1\";\u00a0 \u00a0 \u00a0 LocationName locationName = LocationName.of(project, location);\u00a0 \u00a0 \u00a0 ModelContainerSpec modelContainerSpec =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ModelContainerSpec.newBuilder().setImageUri(imageUri).build();\u00a0 \u00a0 \u00a0 Model model =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Model.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setDisplayName(modelDisplayName)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setMetadataSchemaUri(metadataSchemaUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setArtifactUri(artifactUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setContainerSpec(modelContainerSpec)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 OperationFuture<UploadModelResponse, UploadModelOperationMetadata> uploadModelResponseFuture =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 modelServiceClient.uploadModelAsync(locationName, model);\u00a0 \u00a0 \u00a0 System.out.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Operation name: %s\\n\", uploadModelResponseFuture.getInitialFuture().get().getName());\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to finish...\");\u00a0 \u00a0 \u00a0 UploadModelResponse uploadModelResponse = uploadModelResponseFuture.get(5, TimeUnit.MINUTES);\u00a0 \u00a0 \u00a0 System.out.println(\"Upload Model Response\");\u00a0 \u00a0 \u00a0 System.out.format(\"Model: %s\\n\", uploadModelResponse.getModel());\u00a0 \u00a0 }\u00a0 }}\n```Before trying this sample, follow the Node.js setup instructions in the [Vertex AI quickstart using   client libraries](/vertex-ai/docs/start/client-libraries) .       For more information, see the [Vertex AI Node.js API reference documentation](/nodejs/docs/reference/aiplatform/latest) .\nTo authenticate to Vertex AI, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/ai-platform/snippets/upload-model.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\\\u00a0*/// const modelDisplayName = 'YOUR_MODEL_DISPLAY_NAME';// const metadataSchemaUri = 'YOUR_METADATA_SCHEMA_URI';// const imageUri = 'YOUR_IMAGE_URI';// const artifactUri = 'YOUR_ARTIFACT_URI';// const project = 'YOUR_PROJECT_ID';// const location = 'YOUR_PROJECT_LOCATION';// Imports the Google Cloud Model Service Client libraryconst {ModelServiceClient} = require('@google-cloud/aiplatform');// Specifies the location of the api endpointconst clientOptions = {\u00a0 apiEndpoint: 'us-central1-aiplatform.googleapis.com',};// Instantiates a clientconst modelServiceClient = new ModelServiceClient(clientOptions);async function uploadModel() {\u00a0 // Configure the parent resources\u00a0 const parent = `projects/${project}/locations/${location}`;\u00a0 // Configure the model resources\u00a0 const model = {\u00a0 \u00a0 displayName: modelDisplayName,\u00a0 \u00a0 metadataSchemaUri: '',\u00a0 \u00a0 artifactUri: artifactUri,\u00a0 \u00a0 containerSpec: {\u00a0 \u00a0 \u00a0 imageUri: imageUri,\u00a0 \u00a0 \u00a0 command: [],\u00a0 \u00a0 \u00a0 args: [],\u00a0 \u00a0 \u00a0 env: [],\u00a0 \u00a0 \u00a0 ports: [],\u00a0 \u00a0 \u00a0 predictRoute: '',\u00a0 \u00a0 \u00a0 healthRoute: '',\u00a0 \u00a0 },\u00a0 };\u00a0 const request = {\u00a0 \u00a0 parent,\u00a0 \u00a0 model,\u00a0 };\u00a0 console.log('PARENT AND MODEL');\u00a0 console.log(parent, model);\u00a0 // Upload Model request\u00a0 const [response] = await modelServiceClient.uploadModel(request);\u00a0 console.log(`Long running operation : ${response.name}`);\u00a0 // Wait for operation to complete\u00a0 await response.promise();\u00a0 const result = response.result;\u00a0 console.log('Upload model response ');\u00a0 console.log(`\\tModel : ${result.model}`);}uploadModel();\n```To learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/upload_model_sample.py) \n```\ndef upload_model_sample(\u00a0 \u00a0 project: str,\u00a0 \u00a0 location: str,\u00a0 \u00a0 display_name: str,\u00a0 \u00a0 serving_container_image_uri: str,\u00a0 \u00a0 artifact_uri: Optional[str] = None,\u00a0 \u00a0 serving_container_predict_route: Optional[str] = None,\u00a0 \u00a0 serving_container_health_route: Optional[str] = None,\u00a0 \u00a0 description: Optional[str] = None,\u00a0 \u00a0 serving_container_command: Optional[Sequence[str]] = None,\u00a0 \u00a0 serving_container_args: Optional[Sequence[str]] = None,\u00a0 \u00a0 serving_container_environment_variables: Optional[Dict[str, str]] = None,\u00a0 \u00a0 serving_container_ports: Optional[Sequence[int]] = None,\u00a0 \u00a0 instance_schema_uri: Optional[str] = None,\u00a0 \u00a0 parameters_schema_uri: Optional[str] = None,\u00a0 \u00a0 prediction_schema_uri: Optional[str] = None,\u00a0 \u00a0 explanation_metadata: Optional[explain.ExplanationMetadata] = None,\u00a0 \u00a0 explanation_parameters: Optional[explain.ExplanationParameters] = None,\u00a0 \u00a0 sync: bool = True,):\u00a0 \u00a0 aiplatform.init(project=project, location=location)\u00a0 \u00a0 model = aiplatform.Model.upload(\u00a0 \u00a0 \u00a0 \u00a0 display_name=display_name,\u00a0 \u00a0 \u00a0 \u00a0 artifact_uri=artifact_uri,\u00a0 \u00a0 \u00a0 \u00a0 serving_container_image_uri=serving_container_image_uri,\u00a0 \u00a0 \u00a0 \u00a0 serving_container_predict_route=serving_container_predict_route,\u00a0 \u00a0 \u00a0 \u00a0 serving_container_health_route=serving_container_health_route,\u00a0 \u00a0 \u00a0 \u00a0 instance_schema_uri=instance_schema_uri,\u00a0 \u00a0 \u00a0 \u00a0 parameters_schema_uri=parameters_schema_uri,\u00a0 \u00a0 \u00a0 \u00a0 prediction_schema_uri=prediction_schema_uri,\u00a0 \u00a0 \u00a0 \u00a0 description=description,\u00a0 \u00a0 \u00a0 \u00a0 serving_container_command=serving_container_command,\u00a0 \u00a0 \u00a0 \u00a0 serving_container_args=serving_container_args,\u00a0 \u00a0 \u00a0 \u00a0 serving_container_environment_variables=serving_container_environment_variables,\u00a0 \u00a0 \u00a0 \u00a0 serving_container_ports=serving_container_ports,\u00a0 \u00a0 \u00a0 \u00a0 explanation_metadata=explanation_metadata,\u00a0 \u00a0 \u00a0 \u00a0 explanation_parameters=explanation_parameters,\u00a0 \u00a0 \u00a0 \u00a0 sync=sync,\u00a0 \u00a0 )\u00a0 \u00a0 model.wait()\u00a0 \u00a0 print(model.display_name)\u00a0 \u00a0 print(model.resource_name)\u00a0 \u00a0 return model\n```\nTo import a model with Vertex Explainable AI settings enabled, refer to the [Vertex Explainable AI model import examples](/vertex-ai/docs/explainable-ai/configuring-explanations#import-model-example) .\n### Get operation status\nSome requests start long-running operations that require time to complete. These requests return an operation name, which you can use to view the operation's status or cancel the operation. Vertex AI provides helper methods to make calls against long-running operations. For more information, see [Working with long-runningoperations](/vertex-ai/docs/general/long-running-operations) .\n## Limitations\n- Maximum supported model size is 10 GiB.## What's next\n- Deploy your model to an endpoint, [programmatically](/vertex-ai/docs/predictions/deploy-model-api) or by [using Google Cloud console](/vertex-ai/docs/predictions/deploy-model-console) .", "guide": "Vertex AI"}