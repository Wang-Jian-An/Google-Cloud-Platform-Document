{"title": "Vertex AI - Tabular Workflows on Vertex AI", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Tabular Workflows on Vertex AI\nTabular Workflows is a set of integrated, fully managed, and scalable pipelines for end-to-end ML with tabular data. It leverages Google's technology for model development and provides you with customization options to fit your needs.\n", "content": "## Benefits- Fully managed: you don't need to worry about updates, dependencies and conflicts.\n- Easy to scale: you don't need to re-engineer infrastructure as workloads or datasets grow.\n- Optimized for performance: the right hardware is automatically configured for the workflow's requirements.\n- Deeply integrated: compatibility with products in the Vertex AI MLOps suite, like Vertex AI Pipelines and Vertex AI Experiments, lets you run many experiments in a short amount of time.\n## Technical Overview\nEach workflow is a managed instance of Vertex AI Pipelines.\n[Vertex AI Pipelines](/vertex-ai/docs/pipelines/introduction) is a serverless service that runs Kubeflow pipelines. You can use pipelines to automate and monitor your machine learning and data preparation tasks. Each step in a pipeline performs part of the pipeline's workflow. For example, a pipeline can include steps to split data, transform data types, and train a model. Since steps are instances of pipeline components, steps have inputs, outputs, and a container image. Step inputs can be set from the pipeline's inputs or they can depend on the output of other steps within this pipeline. These dependencies define the pipeline's workflow as a directed acyclic graph.\n[](/static/vertex-ai/docs/tabular-data/images/tabular-workflows.png)\n## How to get started\nIn most cases, you need to define and run the pipeline using the [Google Cloud Pipeline Components SDK](https://pypi.org/project/google-cloud-pipeline-components/) . The following sample code provides an illustration. Note that the actual implementation of the code may be different.\n```\n\u00a0 // Define the pipeline and the parameters\u00a0 template_path, parameter_values = tabular_utils.get_default_pipeline_and_parameters(\u00a0 \u00a0 \u00a0\u2026\u00a0 \u00a0 \u00a0 optimization_objective=optimization_objective,\u00a0 \u00a0 \u00a0 data_source=data_source,\u00a0 \u00a0 \u00a0 target_column_name=target_column_name\u00a0 \u00a0 \u00a0\u2026)\n```\n```\n\u00a0 // Run the pipeline\u00a0 job = pipeline_jobs.PipelineJob(..., template_path=template_path, parameter_values=parameter_values)\u00a0 job.run(...)\n```\nFor sample colabs and notebooks, contact your sales representative or fill out a [request form](https://forms.gle/PGFzdf5gtZHfWTtw9) .\n## Versioning and maintenance\nTabular Workflows have an effective versioning system that allows for continuous updates and improvements without breaking changes to your applications.\nEach workflow is released and updated as part of the [Google Cloud Pipeline Components SDK](https://pypi.org/project/google-cloud-pipeline-components/) . Updates and modifications to any workflow are released as new versions of that workflow. Previous versions of every workflow are always available through the older versions of the SDK. If the SDK version is pinned, the workflow version is also pinned.\n## Available workflows\nVertex AI provides the following Tabular Workflows:\n| Name      | Type      | Availability  |\n|:-------------------------|:----------------------------|:--------------------|\n| Feature Transform Engine | Feature Engineering   | Public Preview  |\n| End-to-End AutoML  | Classification & Regression | Generally Available |\n| TabNet     | Classification & Regression | Public Preview  |\n| Wide & Deep    | Classification & Regression | Public Preview  |\n| Forecasting    | Forecasting     | Public Preview  |\nFor additional information and sample notebooks, contact your sales representative or fill out a [request form](https://forms.gle/PGFzdf5gtZHfWTtw9) .\n## Feature Transform Engine\n**    Preview     ** This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nFeature Transform Engine performs feature selection and feature transformations. If feature selection is enabled, Feature Transform Engine creates a ranked set of important features. If feature transformations are enabled, Feature Transform Engine processes the features to ensure that the input for model training and model serving is consistent. Feature Transform Engine can be used on its own or together with any of the [tabular training workflows](/vertex-ai/docs/tabular-data/tabular-workflows/overview) . It supports both TensorFlow and non-TensorFlow frameworks.\nFor more information, see [Feature engineering](/vertex-ai/docs/tabular-data/tabular-workflows/feature-engineering) .\n## Tabular Workflows for classification and regression\n### Tabular Workflow for End-to-End AutoML\nTabular Workflow for End-to-End AutoML is a complete AutoML pipeline for classification and regression tasks. It is similar to the [AutoML API](/vertex-ai/docs/tabular-data/classification-regression/overview) , but allows you to choose what to control and what to automate. Instead of having controls for the pipeline, you have controls for in the pipeline. These pipeline controls include:\n- Data splitting\n- Feature engineering\n- Architecture search\n- Model training\n- Model ensembling\n- Model distillation\n- Supports **large datasets** that are multiple TB in size and have up to 1000 columns.\n- Allows you to **improve stability and lower training time** by limiting the search space of architecture types or skipping architecture search.\n- Allows you to **improve training speed** by manually selecting the hardware used for training and architecture search.\n- Allows you to **reduce model size and improve latency** with distillation or by changing the ensemble size.\n- Each AutoML component can be inspected in a powerful pipelines graph interface that lets you see the transformed data tables, evaluated model architectures, and many more details.\n- Each AutoML component gets extended flexibility and transparency, such as being able to customize parameters, hardware, view process status, logs, and more.\n- Takes a BigQuery table or a CSV file from Cloud Storage as input.\n- Produces a Vertex AI model as output.\n- Intermediate outputs include dataset statistics and dataset splits.\nFor more information, see [Tabular Workflow for End-to-End AutoML](/vertex-ai/docs/tabular-data/tabular-workflows/e2e-automl) .\n### Tabular Workflow for TabNet\n**    Preview     ** This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nTabular Workflow for TabNet is a pipeline that you can use to train classification or regression models. [TabNet](https://arxiv.org/abs/1908.07442) uses [sequential attention](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) to choose which features to reason from at each decision step. This promotes interpretability and more efficient learning because the learning capacity is used for the most salient features.\n- Automatically selects the appropriate hyperparameter search space based on the dataset size, prediction type, and training budget.\n- Integrated with Vertex AI. The trained model is a Vertex AI model. You can run batch predictions or deploy the model for online predictions right away.\n- Provides inherent model interpretability. You can get insight into which features TabNet used to make its decision.\n- Supports GPU training.\nTakes a BigQuery table or a CSV file from Cloud Storage as input and provides a Vertex AI model as output.\nFor more information, see [Tabular Workflow for TabNet](/vertex-ai/docs/tabular-data/tabular-workflows/tabnet) .\n### Tabular Workflow for Wide & Deep\n**    Preview     ** This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nTabular Workflow for Wide & Deep is a pipeline that you can use to train classification or regression models. [Wide & Deep](https://arxiv.org/abs/1606.07792) jointly trains wide linear models and deep neural networks. It combines the benefits of memorization and generalization. In some online experiments, the results showed that Wide & Deep significantly increased Google store application acquisitions compared with wide-only and deep-only models.\n- Integrated with Vertex AI. The trained model is a Vertex AI model. You can run batch predictions or deploy the model for online predictions right away.\nTakes a BigQuery table or a CSV file from Cloud Storage as input and provides a Vertex AI model as output.\nFor more information, see [Tabular Workflow for Wide & Deep](/vertex-ai/docs/tabular-data/tabular-workflows/wide-and-deep) .\n## Tabular Workflows for forecasting\n### Tabular Workflow for Forecasting\n**    Preview     ** This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nTabular Workflow for Forecasting is the complete pipeline for forecasting tasks. It is similar to the [AutoML API](/vertex-ai/docs/tabular-data/forecasting/overview) , but allows you to choose what to control and what to automate. Instead of having controls for the pipeline, you have controls for in the pipeline. These pipeline controls include:\n- Data splitting\n- Feature engineering\n- Architecture search\n- Model training\n- Model ensembling\n- Supports **large datasets** that are up to 1TB in size and have up to 200 columns.\n- Allows you to **improve stability and lower training time** by limiting the search space of architecture types or skipping architecture search.\n- Allows you to **improve training speed** by manually selecting the hardware used for training and architecture search.\n- For some model training methods, allows you to **reduce model size and improve latency** by changing the ensemble size.\n- Each component can be inspected in a powerful pipelines graph interface that lets you see the transformed data tables, evaluated model architectures and many more details.\n- Each component gets extended flexibility and transparency, such as being able to customize parameters, hardware, view process status, logs and more.\n- Takes a BigQuery table or a CSV file from Cloud Storage as input.\n- Produces a Vertex AI model as output.\n- Intermediate outputs include dataset statistics and dataset splits.\nFor more information, see [Tabular Workflow for Forecasting](/vertex-ai/docs/tabular-data/tabular-workflows/forecasting) .\n## What's next\n- Learn about [Tabular Workflow for End-to-End AutoML](/vertex-ai/docs/tabular-data/tabular-workflows/e2e-automl) .\n- Learn about [Tabular Workflow for TabNet](/vertex-ai/docs/tabular-data/tabular-workflows/tabnet) .\n- Learn about [Tabular Workflow for Wide & Deep](/vertex-ai/docs/tabular-data/tabular-workflows/wide-and-deep) .\n- Learn about [Tabular Workflow for Forecasting](/vertex-ai/docs/tabular-data/tabular-workflows/forecasting) .\n- Learn about [Feature engineering](/vertex-ai/docs/tabular-data/tabular-workflows/feature-engineering) .\n- Learn about [Pricing for Tabular Workflows](/vertex-ai/docs/tabular-data/tabular-workflows/pricing) .", "guide": "Vertex AI"}