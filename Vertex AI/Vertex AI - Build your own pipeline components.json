{"title": "Vertex AI - Build your own pipeline components", "url": "https://cloud.google.com/vertex-ai/docs/pipelines/build-own-components", "abstract": "# Vertex AI - Build your own pipeline components\nTo learn more,  run the \"Custom training workflow with prebuilt Pipeline Components and custom components\" Jupyter notebook in one of the following  environments: [Openin Colab](https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/google_cloud_pipeline_components_model_train_upload_deploy.ipynb) | [Openin Vertex AI Workbench user-managed notebooks](https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fpipelines%2Fgoogle_cloud_pipeline_components_model_train_upload_deploy.ipynb) | [View on GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/google_cloud_pipeline_components_model_train_upload_deploy.ipynb)\n", "content": "## Write a component to show a Google Cloud console link\nIt's common that when running a component, you want to not only see the link to the component job being launched, but also the link to the underlying cloud resources, such as the Vertex batch prediction jobs or dataflow jobs.\nThe [gcp_resource proto](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud/google_cloud_pipeline_components/proto) is a special parameter that you can use in your component to enable the Google Cloud console to provide a customized view of the resource's logs and status in the Vertex AI Pipelines console.\n### Output the gcp_resource parameter\nFirst, you'll need to define the `gcp_resource` parameter in your component as shown in the following example `component.py` file:\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n [View on GitHub](https://github.com/kubeflow/pipelines/blob/HEAD/components/google-cloud/google_cloud_pipeline_components/v1/dataflow/python_job/component.py) \n```\n# Copyright 2023 The Kubeflow Authors. All Rights Reserved.\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.from typing import Listfrom google_cloud_pipeline_components import _imagefrom google_cloud_pipeline_components import _placeholdersfrom kfp.dsl import container_componentfrom kfp.dsl import ContainerSpecfrom kfp.dsl import OutputPath@container_componentdef dataflow_python(\u00a0 \u00a0 python_module_path: str,\u00a0 \u00a0 temp_location: str,\u00a0 \u00a0 gcp_resources: OutputPath(str),\u00a0 \u00a0 location: str = 'us-central1',\u00a0 \u00a0 requirements_file_path: str = '',\u00a0 \u00a0 args: List[str] = [],\u00a0 \u00a0 project: str = _placeholders.PROJECT_ID_PLACEHOLDER,):\u00a0 # fmt: off\u00a0 \"\"\"Launch a self-executing Beam Python file on Google Cloud using the\u00a0 Dataflow Runner.\u00a0 Args:\u00a0 \u00a0 \u00a0 location: Location of the Dataflow job. If not set, defaults to `'us-central1'`.\u00a0 \u00a0 \u00a0 python_module_path: The GCS path to the Python file to run.\u00a0 \u00a0 \u00a0 temp_location: A GCS path for Dataflow to stage temporary job files created during the execution of the pipeline.\u00a0 \u00a0 \u00a0 requirements_file_path: The GCS path to the pip requirements file.\u00a0 \u00a0 \u00a0 args: The list of args to pass to the Python file. Can include additional parameters for the Dataflow Runner.\u00a0 \u00a0 \u00a0 project: Project to create the Dataflow job. Defaults to the project in which the PipelineJob is run.\u00a0 Returns:\u00a0 \u00a0 \u00a0 gcp_resources: Serialized gcp_resources proto tracking the Dataflow job. For more details, see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.\u00a0 \"\"\"\u00a0 # fmt: on\u00a0 return ContainerSpec(\u00a0 \u00a0 \u00a0 image=_image.GCPC_IMAGE_TAG,\u00a0 \u00a0 \u00a0 command=[\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'python3',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '-u',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '-m',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'google_cloud_pipeline_components.container.v1.dataflow.dataflow_launcher',\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 args=[\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '--project',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 project,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '--location',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 location,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '--python_module_path',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python_module_path,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '--temp_location',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 temp_location,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '--requirements_file_path',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requirements_file_path,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '--args',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 args,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '--gcp_resources',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 gcp_resources,\u00a0 \u00a0 \u00a0 ],\u00a0 )\n```\nNext, inside the container, install the Google Cloud Pipeline Components package:\n```\npip install --upgrade google-cloud-pipeline-components\n```\nNext, in the Python code, define the resource as a `gcp_resource` parameter:\nTo learn how to install or update the Python, see [Install the Vertex AI SDK for Python](/vertex-ai/docs/start/use-vertex-ai-python-sdk) .    For more information, see the [   Python API reference documentation](/python/docs/reference/aiplatform/latest) .\n```\nfrom google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResourcesfrom google.protobuf.json_format import MessageToJsondataflow_resources = GcpResources()dr = dataflow_resources.resources.add()dr.resource_type='DataflowJob'dr.resource_uri='https://dataflow.googleapis.com/v1b3/projects/[your-project]/locations/us-east1/jobs/[dataflow-job-id]'with open(gcp_resources, 'w') as f:\u00a0 \u00a0 f.write(MessageToJson(dataflow_resources))\n```\nAlternatively, you can return the `gcp_resources` output parameter as you would any string output parameter:\n```\n@dsl.component(\u00a0 \u00a0 base_image='python:3.9',\u00a0 \u00a0 packages_to_install=['google-cloud-pipeline-components==2.11.0'],)def launch_dataflow_component(project: str, location:str) -> NamedTuple(\"Outputs\", \u00a0[(\"gcp_resources\", str)]):\u00a0 # Launch the dataflow job\u00a0 dataflow_job_id = [dataflow-id]\u00a0 dataflow_resources = GcpResources()\u00a0 dr = dataflow_resources.resources.add()\u00a0 dr.resource_type='DataflowJob'\u00a0 dr.resource_uri=f'https://dataflow.googleapis.com/v1b3/projects/{project}/locations/{location}/jobs/{dataflow_job_id}'\u00a0 gcp_resources=MessageToJson(dataflow_resources)\u00a0 return gcp_resources\n```\nYou can set the `resource_type` to be an arbitrary string, but only the following types have links in the Google Cloud console:\n- BatchPredictionJob\n- BigQueryJob\n- CustomJob\n- DataflowJob\n- HyperparameterTuningJob## Write a component to cancel the underlying resources\nWhen a pipeline job is canceled, the default behavior is for the underlying Google Cloud resources to keep running. They are not canceled automatically. To change this behavior, you should attach a [SIGTERM](https://docs.python.org/3/library/signal.html#signal.SIGTERM) handler to the pipeline job. A good place to do this is just before a polling loop for a job that could run for a long time.\nCancellation has been implemented on several Google Cloud Pipeline Components, including:\n- Batch prediction job\n- BigQuery ML job\n- Custom job\n- Dataproc Serverless batch job\n- Hyperparameter tuning job\nFor more information, including sample code that shows how to attach a SIGTERM handler, see the following GitHub links:\n- [https://github.com/kubeflow/pipelines/blob/google-cloud-pipeline-components-2.11.0/components/google-cloud/google_cloud_pipeline_components/container/utils/execution_context.py](https://github.com/kubeflow/pipelines/blob/google-cloud-pipeline-components-2.11.0/components/google-cloud/google_cloud_pipeline_components/container/utils/execution_context.py) \n- [https://github.com/kubeflow/pipelines/blob/google-cloud-pipeline-components-2.11.0/components/google-cloud/google_cloud_pipeline_components/container/v1/gcp_launcher/job_remote_runner.py#L124](https://github.com/kubeflow/pipelines/blob/google-cloud-pipeline-components-2.11.0/components/google-cloud/google_cloud_pipeline_components/container/v1/gcp_launcher/job_remote_runner.py#L124) \nConsider the following when implementing your SIGTERM handler:\n- Cancellation propagation works only after the component has been running for a few minutes. This is typically due to background startup tasks that need to be [processed](https://docs.python.org/3/library/signal.html#execution-of-python-signal-handlers) before the Python signal handlers are called.\n- Some Google Cloud resources might not have cancellation implemented. For example, creating or deleting a Vertex AI Endpoint or Model could create a long-running operation that accepts a cancellation request through its REST API, but doesn't implement the cancellation operation itself.", "guide": "Vertex AI"}