{"title": "Cloud Video Intelligence API - Video classification", "url": "https://cloud.google.com/video-intelligence/docs/streaming/video-classification", "abstract": "# Cloud Video Intelligence API - Video classification\n**    Beta     ** This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nVideo classification identifies objects, locations, activities, animal species, products, and more.\nTo use a pre-trained model see [Analyze labels](https://cloud.google.com/video-intelligence/docs/streaming/label-analysis) .\n", "content": "## Use AutoML video\n### Before you begin\nFor background on creating an AutoML model, check out the Vertex AI [Beginner's guide](/vertex-ai/docs/beginner/beginners-guide#video) . For instructions on how to create your AutoML model, see [Video data](/vertex-ai/docs/training-overview#video_data) in \"Develop and use ML models\" in the Vertex AI documentation.\n### Use your AutoML model\nThe following code sample demonstrates how to use your [AutoML model](/vertex-ai/docs/beginner/beginners-guide#video) for video classification using the streaming client library.\nTo authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/video/src/main/java/beta/video/StreamingAutoMlClassification.java) \n```\nimport com.google.api.gax.rpc.BidiStream;import com.google.cloud.videointelligence.v1p3beta1.LabelAnnotation;import com.google.cloud.videointelligence.v1p3beta1.LabelFrame;import com.google.cloud.videointelligence.v1p3beta1.StreamingAnnotateVideoRequest;import com.google.cloud.videointelligence.v1p3beta1.StreamingAnnotateVideoResponse;import com.google.cloud.videointelligence.v1p3beta1.StreamingAutomlClassificationConfig;import com.google.cloud.videointelligence.v1p3beta1.StreamingFeature;import com.google.cloud.videointelligence.v1p3beta1.StreamingVideoAnnotationResults;import com.google.cloud.videointelligence.v1p3beta1.StreamingVideoConfig;import com.google.cloud.videointelligence.v1p3beta1.StreamingVideoIntelligenceServiceClient;import com.google.protobuf.ByteString;import io.grpc.StatusRuntimeException;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;import java.util.Arrays;import java.util.concurrent.TimeoutException;class StreamingAutoMlClassification {\u00a0 // Perform streaming video classification with an AutoML Model\u00a0 static void streamingAutoMlClassification(String filePath, String projectId, String modelId)\u00a0 \u00a0 \u00a0 throws TimeoutException, StatusRuntimeException, IOException {\u00a0 \u00a0 // String filePath = \"path_to_your_video_file\";\u00a0 \u00a0 // String projectId = \"YOUR_GCP_PROJECT_ID\";\u00a0 \u00a0 // String modelId = \"YOUR_AUTO_ML_CLASSIFICATION_MODEL_ID\";\u00a0 \u00a0 try (StreamingVideoIntelligenceServiceClient client =\u00a0 \u00a0 \u00a0 \u00a0 StreamingVideoIntelligenceServiceClient.create()) {\u00a0 \u00a0 \u00a0 Path path = Paths.get(filePath);\u00a0 \u00a0 \u00a0 byte[] data = Files.readAllBytes(path);\u00a0 \u00a0 \u00a0 // Set the chunk size to 5MB (recommended less than 10MB).\u00a0 \u00a0 \u00a0 int chunkSize = 5 * 1024 * 1024;\u00a0 \u00a0 \u00a0 int numChunks = (int) Math.ceil((double) data.length / chunkSize);\u00a0 \u00a0 \u00a0 String modelPath =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\"projects/%s/locations/us-central1/models/%s\", projectId, modelId);\u00a0 \u00a0 \u00a0 System.out.println(modelPath);\u00a0 \u00a0 \u00a0 StreamingAutomlClassificationConfig streamingAutomlClassificationConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 StreamingAutomlClassificationConfig.newBuilder().setModelName(modelPath).build();\u00a0 \u00a0 \u00a0 StreamingVideoConfig streamingVideoConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 StreamingVideoConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setFeature(StreamingFeature.STREAMING_AUTOML_CLASSIFICATION)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setAutomlClassificationConfig(streamingAutomlClassificationConfig)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 BidiStream<StreamingAnnotateVideoRequest, StreamingAnnotateVideoResponse> call =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 client.streamingAnnotateVideoCallable().call();\u00a0 \u00a0 \u00a0 // The first request must **only** contain the audio configuration:\u00a0 \u00a0 \u00a0 call.send(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 StreamingAnnotateVideoRequest.newBuilder().setVideoConfig(streamingVideoConfig).build());\u00a0 \u00a0 \u00a0 // Subsequent requests must **only** contain the audio data.\u00a0 \u00a0 \u00a0 // Send the requests in chunks\u00a0 \u00a0 \u00a0 for (int i = 0; i < numChunks; i++) {\u00a0 \u00a0 \u00a0 \u00a0 call.send(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 StreamingAnnotateVideoRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputContent(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ByteString.copyFrom(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Arrays.copyOfRange(data, i * chunkSize, i * chunkSize + chunkSize)))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 // Tell the service you are done sending data\u00a0 \u00a0 \u00a0 call.closeSend();\u00a0 \u00a0 \u00a0 for (StreamingAnnotateVideoResponse response : call) {\u00a0 \u00a0 \u00a0 \u00a0 if (response.hasError()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.println(response.getError().getMessage());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break;\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 StreamingVideoAnnotationResults annotationResults = response.getAnnotationResults();\u00a0 \u00a0 \u00a0 \u00a0 for (LabelAnnotation annotation : annotationResults.getLabelAnnotationsList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String entity = annotation.getEntity().getDescription();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // There is only one frame per annotation\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 LabelFrame labelFrame = annotation.getFrames(0);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 double offset =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 labelFrame.getTimeOffset().getSeconds() + labelFrame.getTimeOffset().getNanos() / 1e9;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 float confidence = labelFrame.getConfidence();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.format(\"At %fs segment: %s (%f)\\n\", offset, entity, confidence);\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 System.out.println(\"Video streamed successfully.\");\u00a0 \u00a0 }\u00a0 }}\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/video-intelligence/analyze-streaming-automl-classification.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*/// const path = 'Local file to analyze, e.g. ./my-file.mp4';// const modelId = 'autoMl model'// const projectId = 'Your GCP Project'const {StreamingVideoIntelligenceServiceClient} =\u00a0 require('@google-cloud/video-intelligence').v1p3beta1;const fs = require('fs');// Instantiates a clientconst client = new StreamingVideoIntelligenceServiceClient();// Streaming configurationconst modelPath = `projects/${projectId}/locations/us-central1/models/${modelId}`;const configRequest = {\u00a0 videoConfig: {\u00a0 \u00a0 feature: 'STREAMING_AUTOML_CLASSIFICATION',\u00a0 \u00a0 automlClassificationConfig: {\u00a0 \u00a0 \u00a0 modelName: modelPath,\u00a0 \u00a0 },\u00a0 },};const readStream = fs.createReadStream(path, {\u00a0 highWaterMark: 5 * 1024 * 1024, //chunk size set to 5MB (recommended less than 10MB)\u00a0 encoding: 'base64',});//Load file content// Note: Input videos must have supported video codecs. See// https://cloud.google.com/video-intelligence/docs/streaming/streaming#supported_video_codecs// for more details.const chunks = [];readStream\u00a0 .on('data', chunk => {\u00a0 \u00a0 const request = {\u00a0 \u00a0 \u00a0 inputContent: chunk.toString(),\u00a0 \u00a0 };\u00a0 \u00a0 chunks.push(request);\u00a0 })\u00a0 .on('close', () => {\u00a0 \u00a0 // configRequest should be the first in the stream of requests\u00a0 \u00a0 stream.write(configRequest);\u00a0 \u00a0 for (let i = 0; i < chunks.length; i++) {\u00a0 \u00a0 \u00a0 stream.write(chunks[i]);\u00a0 \u00a0 }\u00a0 \u00a0 stream.end();\u00a0 });const stream = client\u00a0 .streamingAnnotateVideo()\u00a0 .on('data', response => {\u00a0 \u00a0 //Gets annotations for video\u00a0 \u00a0 const annotations = response.annotationResults;\u00a0 \u00a0 const labels = annotations.labelAnnotations;\u00a0 \u00a0 labels.forEach(label => {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `Label ${label.entity.description} occurs at: ${\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 label.frames[0].timeOffset.seconds || 0\u00a0 \u00a0 \u00a0 \u00a0 }` + `.${(label.frames[0].timeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(` Confidence: ${label.frames[0].confidence}`);\u00a0 \u00a0 });\u00a0 })\u00a0 .on('error', response => {\u00a0 \u00a0 console.error(response);\u00a0 });\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/beta_snippets.py) \n```\nimport iofrom google.cloud import videointelligence_v1p3beta1 as videointelligence# path = 'path_to_file'# project_id = 'gcp_project_id'# model_id = 'automl_classification_model_id'client = videointelligence.StreamingVideoIntelligenceServiceClient()model_path = \"projects/{}/locations/us-central1/models/{}\".format(\u00a0 \u00a0 project_id, model_id)# Here we use classification as an example.automl_config = videointelligence.StreamingAutomlClassificationConfig(\u00a0 \u00a0 model_name=model_path)video_config = videointelligence.StreamingVideoConfig(\u00a0 \u00a0 feature=videointelligence.StreamingFeature.STREAMING_AUTOML_CLASSIFICATION,\u00a0 \u00a0 automl_classification_config=automl_config,)# config_request should be the first in the stream of requests.config_request = videointelligence.StreamingAnnotateVideoRequest(\u00a0 \u00a0 video_config=video_config)# Set the chunk size to 5MB (recommended less than 10MB).chunk_size = 5 * 1024 * 1024# Load file content.# Note: Input videos must have supported video codecs. See# https://cloud.google.com/video-intelligence/docs/streaming/streaming#supported_video_codecs# for more details.stream = []with io.open(path, \"rb\") as video_file:\u00a0 \u00a0 while True:\u00a0 \u00a0 \u00a0 \u00a0 data = video_file.read(chunk_size)\u00a0 \u00a0 \u00a0 \u00a0 if not data:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break\u00a0 \u00a0 \u00a0 \u00a0 stream.append(data)def stream_generator():\u00a0 \u00a0 yield config_request\u00a0 \u00a0 for chunk in stream:\u00a0 \u00a0 \u00a0 \u00a0 yield videointelligence.StreamingAnnotateVideoRequest(input_content=chunk)requests = stream_generator()# streaming_annotate_video returns a generator.# The default timeout is about 300 seconds.# To process longer videos it should be set to# larger than the length (in seconds) of the stream.responses = client.streaming_annotate_video(requests, timeout=600)for response in responses:\u00a0 \u00a0 # Check for errors.\u00a0 \u00a0 if response.error.message:\u00a0 \u00a0 \u00a0 \u00a0 print(response.error.message)\u00a0 \u00a0 \u00a0 \u00a0 break\u00a0 \u00a0 for label in response.annotation_results.label_annotations:\u00a0 \u00a0 \u00a0 \u00a0 for frame in label.frames:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"At {:3d}s segment, {:5.1%} {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frame.time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frame.confidence,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 label.entity.entity_id,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\n```", "guide": "Cloud Video Intelligence API"}