{"title": "Cloud Video Intelligence API - Recognize text", "url": "https://cloud.google.com/video-intelligence/docs/text-detection", "abstract": "# Cloud Video Intelligence API - Recognize text\nperforms Optical Character Recognition (OCR), which detects and extracts text within an input video.\nText detection is available for all the [languages](/vision/docs/languages) supported by the Cloud Vision API.\n", "content": "## Request text detection for a video on Cloud Storage\nThe following samples demonstrate text detection on a file located in Cloud Storage.\n## Send video annotation requestThe following shows how to send a POST request to the [videos:annotate](/video-intelligence/docs/reference/rest/v1/videos/annotate) method. The example uses the Google Cloud CLI to create an access token. For instructions on installing the gcloud CLI, see the [Video Intelligence API Quickstart](/video-intelligence/docs/quickstarts) .\nBefore using any of the request data, make the following replacements:- : a Cloud Storage bucket that contains  the file you want to annotate, including the file name. Must  start with`gs://`.For example:`\"inputUri\": \"gs://cloud-videointelligence-demo/assistant.mp4\",`\n- : [Optional] For example, \"en-US\"\n- : The numeric identifier for your Google Cloud project\nHTTP method and URL:\n```\nPOST https://videointelligence.googleapis.com/v1/videos:annotate\n```\nRequest JSON body:\n```\n{\n \"inputUri\": \"INPUT_URI\",\n \"features\": [\"TEXT_DETECTION\"],\n \"videoContext\": {\n \"textDetectionConfig\": {\n  \"languageHints\": [\"LANGUAGE_CODE\"]\n }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/operations/OPERATION_ID\"\n}\n```\nIf the response is successful, the Video Intelligence API returns the `name` for your operation. The above shows an example of such a response, where: `project-number` is the number of your project and `operation-id` is the ID of the long running operation created for the request.\n- : the number of your project\n- : the Cloud region where annotation should take  place. Supported cloud regions are:`us-east1`,`us-west1`,`europe-west1`,`asia-east1`. If no region is  specified, a region will be determined based on video file location.\n- : the ID of the long running operation created  for the request and provided in the response when you started the  operation, for example`12345...`\n## Get annotation resultsTo retrieve the result of the operation, make a [GET](/video-intelligence/docs/reference/rest/v1/projects.locations.operations/get) request, using the operation name returned from the call to [videos:annotate](/video-intelligence/docs/reference/rest/v1/videos/annotate) , as shown in the following example.\nBefore using any of the request data, make the following replacements:- : the name of the operation as returned by Video Intelligence API. The operation name has the format`projects/` `` `/locations/` `` `/operations/` ``\n- : The numeric identifier for your Google Cloud project\nHTTP method and URL:\n```\nGET https://videointelligence.googleapis.com/v1/OPERATION_NAME\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\nText detection annotations are returned as a\n`textAnnotations`\nlist. Note: The\n **done** \nfield is only returned when its value is\n **True** \n. It's not included in responses for which the operation has not completed.\n## Download annotation resultsCopy the annotation from the source to the destination bucket: (see [Copy files and objects](https://cloud.google.com/storage/docs/gsutil/commands/cp) )\n`gsutil cp` `` `gs://my-bucket`\nNote: If the output gcs uri is provided by the user, then the annotation is stored in that gcs uri. [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/videointelligence/annotate/text_detection_gcs.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 video \"cloud.google.com/go/videointelligence/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 videopb \"cloud.google.com/go/videointelligence/apiv1/videointelligencepb\"\u00a0 \u00a0 \u00a0 \u00a0 \"github.com/golang/protobuf/ptypes\")// textDetectionGCS analyzes a video and extracts the text from the video's audio.func textDetectionGCS(w io.Writer, gcsURI string) error {\u00a0 \u00a0 \u00a0 \u00a0 // gcsURI := \"gs://python-docs-samples-tests/video/googlework_short.mp4\"\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // Creates a client.\u00a0 \u00a0 \u00a0 \u00a0 client, err := video.NewClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"video.NewClient: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 op, err := client.AnnotateVideo(ctx, &videopb.AnnotateVideoRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputUri: gcsURI,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Features: []videopb.Feature{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 videopb.Feature_TEXT_DETECTION,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 })\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"AnnotateVideo: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 resp, err := op.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"Wait: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Only one video was processed, so get the first result.\u00a0 \u00a0 \u00a0 \u00a0 result := resp.GetAnnotationResults()[0]\u00a0 \u00a0 \u00a0 \u00a0 for _, annotation := range result.TextAnnotations {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Text: %q\\n\", annotation.GetText())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Get the first text segment.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment := annotation.GetSegments()[0]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start, _ := ptypes.Duration(segment.GetSegment().GetStartTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end, _ := ptypes.Duration(segment.GetSegment().GetEndTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tSegment: %v to %v\\n\", start, end)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tConfidence: %f\\n\", segment.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Show the result for the first frame in this segment.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frame := segment.GetFrames()[0]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 seconds := float32(frame.GetTimeOffset().GetSeconds())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nanos := float32(frame.GetTimeOffset().GetNanos())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tTime offset of the first frame: %fs\\n\", seconds+nanos/1e9)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tRotated bounding box vertices:\\n\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, vertex := range frame.GetRotatedBoundingBox().GetVertices() {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tVertex x=%f, y=%f\\n\", vertex.GetX(), vertex.GetY())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/video/src/main/java/video/TextDetection.java) \n```\n/**\u00a0* Detect Text in a video.\u00a0*\u00a0* @param gcsUri the path to the video file to analyze.\u00a0*/public static VideoAnnotationResults detectTextGcs(String gcsUri) throws Exception {\u00a0 try (VideoIntelligenceServiceClient client = VideoIntelligenceServiceClient.create()) {\u00a0 \u00a0 // Create the request\u00a0 \u00a0 AnnotateVideoRequest request =\u00a0 \u00a0 \u00a0 \u00a0 AnnotateVideoRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputUri(gcsUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addFeatures(Feature.TEXT_DETECTION)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // asynchronously perform object tracking on videos\u00a0 \u00a0 OperationFuture<AnnotateVideoResponse, AnnotateVideoProgress> future =\u00a0 \u00a0 \u00a0 \u00a0 client.annotateVideoAsync(request);\u00a0 \u00a0 System.out.println(\"Waiting for operation to complete...\");\u00a0 \u00a0 // The first result is retrieved because a single video was processed.\u00a0 \u00a0 AnnotateVideoResponse response = future.get(300, TimeUnit.SECONDS);\u00a0 \u00a0 VideoAnnotationResults results = response.getAnnotationResults(0);\u00a0 \u00a0 // Get only the first annotation for demo purposes.\u00a0 \u00a0 TextAnnotation annotation = results.getTextAnnotations(0);\u00a0 \u00a0 System.out.println(\"Text: \" + annotation.getText());\u00a0 \u00a0 // Get the first text segment.\u00a0 \u00a0 TextSegment textSegment = annotation.getSegments(0);\u00a0 \u00a0 System.out.println(\"Confidence: \" + textSegment.getConfidence());\u00a0 \u00a0 // For the text segment display it's time offset\u00a0 \u00a0 VideoSegment videoSegment = textSegment.getSegment();\u00a0 \u00a0 Duration startTimeOffset = videoSegment.getStartTimeOffset();\u00a0 \u00a0 Duration endTimeOffset = videoSegment.getEndTimeOffset();\u00a0 \u00a0 // Display the offset times in seconds, 1e9 is part of the formula to convert nanos to seconds\u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Start time: %.2f\", startTimeOffset.getSeconds() + startTimeOffset.getNanos() / 1e9));\u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"End time: %.2f\", endTimeOffset.getSeconds() + endTimeOffset.getNanos() / 1e9));\u00a0 \u00a0 // Show the first result for the first frame in the segment.\u00a0 \u00a0 TextFrame textFrame = textSegment.getFrames(0);\u00a0 \u00a0 Duration timeOffset = textFrame.getTimeOffset();\u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Time offset for the first frame: %.2f\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timeOffset.getSeconds() + timeOffset.getNanos() / 1e9));\u00a0 \u00a0 // Display the rotated bounding box for where the text is on the frame.\u00a0 \u00a0 System.out.println(\"Rotated Bounding Box Vertices:\");\u00a0 \u00a0 List<NormalizedVertex> vertices = textFrame.getRotatedBoundingBox().getVerticesList();\u00a0 \u00a0 for (NormalizedVertex normalizedVertex : vertices) {\u00a0 \u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tVertex.x: %.2f, Vertex.y: %.2f\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 normalizedVertex.getX(), normalizedVertex.getY()));\u00a0 \u00a0 }\u00a0 \u00a0 return results;\u00a0 }}\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/video-intelligence/analyze.js) \n```\n// Imports the Google Cloud Video Intelligence libraryconst Video = require('@google-cloud/video-intelligence');// Creates a clientconst video = new Video.VideoIntelligenceServiceClient();/**\u00a0* TODO(developer): Uncomment the following line before running the sample.\u00a0*/// const gcsUri = 'GCS URI of the video to analyze, e.g. gs://my-bucket/my-video.mp4';const request = {\u00a0 inputUri: gcsUri,\u00a0 features: ['TEXT_DETECTION'],};// Detects text in a videoconst [operation] = await video.annotateVideo(request);const results = await operation.promise();console.log('Waiting for operation to complete...');// Gets annotations for videoconst textAnnotations = results[0].annotationResults[0].textAnnotations;textAnnotations.forEach(textAnnotation => {\u00a0 console.log(`Text ${textAnnotation.text} occurs at:`);\u00a0 textAnnotation.segments.forEach(segment => {\u00a0 \u00a0 const time = segment.segment;\u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 ` Start: ${time.startTimeOffset.seconds || 0}.${(\u00a0 \u00a0 \u00a0 \u00a0 time.startTimeOffset.nanos / 1e6\u00a0 \u00a0 \u00a0 ).toFixed(0)}s`\u00a0 \u00a0 );\u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 ` End: ${time.endTimeOffset.seconds || 0}.${(\u00a0 \u00a0 \u00a0 \u00a0 time.endTimeOffset.nanos / 1e6\u00a0 \u00a0 \u00a0 ).toFixed(0)}s`\u00a0 \u00a0 );\u00a0 \u00a0 console.log(` Confidence: ${segment.confidence}`);\u00a0 \u00a0 segment.frames.forEach(frame => {\u00a0 \u00a0 \u00a0 const timeOffset = frame.timeOffset;\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `Time offset for the frame: ${timeOffset.seconds || 0}` +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 `.${(timeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log('Rotated Bounding Box Vertices:');\u00a0 \u00a0 \u00a0 frame.rotatedBoundingBox.vertices.forEach(vertex => {\u00a0 \u00a0 \u00a0 \u00a0 console.log(`Vertex.x:${vertex.x}, Vertex.y:${vertex.y}`);\u00a0 \u00a0 \u00a0 });\u00a0 \u00a0 });\u00a0 });});\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/analyze.py) \n```\n\"\"\"Detect text in a video stored on GCS.\"\"\"from google.cloud import videointelligencevideo_client = videointelligence.VideoIntelligenceServiceClient()features = [videointelligence.Feature.TEXT_DETECTION]operation = video_client.annotate_video(\u00a0 \u00a0 request={\"features\": features, \"input_uri\": input_uri})print(\"\\nProcessing video for text detection.\")result = operation.result(timeout=600)# The first result is retrieved because a single video was processed.annotation_result = result.annotation_results[0]for text_annotation in annotation_result.text_annotations:\u00a0 \u00a0 print(\"\\nText: {}\".format(text_annotation.text))\u00a0 \u00a0 # Get the first text segment\u00a0 \u00a0 text_segment = text_annotation.segments[0]\u00a0 \u00a0 start_time = text_segment.segment.start_time_offset\u00a0 \u00a0 end_time = text_segment.segment.end_time_offset\u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \"start_time: {}, end_time: {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start_time.seconds + start_time.microseconds * 1e-6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end_time.seconds + end_time.microseconds * 1e-6,\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 print(\"Confidence: {}\".format(text_segment.confidence))\u00a0 \u00a0 # Show the result for the first frame in this segment.\u00a0 \u00a0 frame = text_segment.frames[0]\u00a0 \u00a0 time_offset = frame.time_offset\u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \"Time offset for the first frame: {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 time_offset.seconds + time_offset.microseconds * 1e-6\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 print(\"Rotated Bounding Box Vertices:\")\u00a0 \u00a0 for vertex in frame.rotated_bounding_box.vertices:\u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tVertex.x: {}, Vertex.y: {}\".format(vertex.x, vertex.y))\n```No preface\n **C#** : Please follow the [C# setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for .NET.](https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.VideoIntelligence.V1/index.html) \n **PHP** : Please follow the [PHP setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for PHP.](/php/docs/reference/cloud-videointelligence/latest) \n **Ruby** : Please follow the [Ruby setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for Ruby.](https://googleapis.dev/ruby/google-cloud-video_intelligence/latest/Google/Cloud/VideoIntelligence/V1.html)\n## Request text detection for video from a local file\nThe following samples demonstrate text detection on a file stored locally.\n## Send video annotation requestTo perform annotation on a local video file, be sure to base64-encode the contents of the video file. Include the base64-encoded contents in the `inputContent` field of the request. For information on how to base64-encode the contents of a video file, see [Base64 Encoding](/video-intelligence/docs/base64) .\nThe following shows how to send a POST request to the `videos:annotate` method. The example uses the Google Cloud CLI to create an access token. For instructions on installing the Google Cloud CLI, see the [Video Intelligence API Quickstart](/video-intelligence/docs/quickstarts) \nBefore using any of the request data, make the following replacements:- \"inputContent\":For example:`\"UklGRg41AwBBVkkgTElTVAwBAABoZHJsYXZpaDgAAAA1ggAAxPMBAAAAAAAQCAA...\"`\n- : [Optional] For example, \"en-US\"\n- : The numeric identifier for your Google Cloud project\nHTTP method and URL:\n```\nPOST https://videointelligence.googleapis.com/v1/videos:annotate\n```\nRequest JSON body:\n```\n{\n \"inputContent\": \"BASE64_ENCODED_CONTENT\",\n \"features\": [\"TEXT_DETECTION\"],\n \"videoContext\": {\n \"textDetectionConfig\": {\n  \"languageHints\": [\"LANGUAGE_CODE\"]\n }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/operations/OPERATION_ID\"\n}\n```\nIf the response is successful, the Video Intelligence API returns the `name` of your operation. The above shows an example of such a response, where `project-number` is the name of your project and `operation-id` is the ID of the long running operation created for the request.\n- : provided in the response when you started the  operation, for example`12345...`\n## Get annotation resultsTo retrieve the result of the operation, make a [GET](/video-intelligence/docs/reference/rest/v1/projects.locations.operations/get) request, using the operation name returned from the call to [videos:annotate](/video-intelligence/docs/reference/rest/v1/videos/annotate) , as shown in the following example.\nBefore using any of the request data, make the following replacements:- : The numeric identifier for your Google Cloud project\nHTTP method and URL:\n```\nGET https://videointelligence.googleapis.com/v1/OPERATION_NAME\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:Text detection annotations are returned as a `textAnnotations` list. Note: The **done** field is only returned when its value is **True** . It's not included in responses for which the operation has not completed.\n [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/videointelligence/annotate/text_detection.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 \"io/ioutil\"\u00a0 \u00a0 \u00a0 \u00a0 video \"cloud.google.com/go/videointelligence/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 videopb \"cloud.google.com/go/videointelligence/apiv1/videointelligencepb\"\u00a0 \u00a0 \u00a0 \u00a0 \"github.com/golang/protobuf/ptypes\")// textDetection analyzes a video and extracts the text from the video's audio.func textDetection(w io.Writer, filename string) error {\u00a0 \u00a0 \u00a0 \u00a0 // filename := \"../testdata/googlework_short.mp4\"\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // Creates a client.\u00a0 \u00a0 \u00a0 \u00a0 client, err := video.NewClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"video.NewClient: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 fileBytes, err := ioutil.ReadFile(filename)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"ioutil.ReadFile: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 op, err := client.AnnotateVideo(ctx, &videopb.AnnotateVideoRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputContent: fileBytes,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Features: []videopb.Feature{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 videopb.Feature_TEXT_DETECTION,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 })\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"AnnotateVideo: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 resp, err := op.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"Wait: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Only one video was processed, so get the first result.\u00a0 \u00a0 \u00a0 \u00a0 result := resp.GetAnnotationResults()[0]\u00a0 \u00a0 \u00a0 \u00a0 for _, annotation := range result.TextAnnotations {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Text: %q\\n\", annotation.GetText())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Get the first text segment.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment := annotation.GetSegments()[0]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start, _ := ptypes.Duration(segment.GetSegment().GetStartTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end, _ := ptypes.Duration(segment.GetSegment().GetEndTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tSegment: %v to %v\\n\", start, end)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tConfidence: %f\\n\", segment.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Show the result for the first frame in this segment.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frame := segment.GetFrames()[0]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 seconds := float32(frame.GetTimeOffset().GetSeconds())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nanos := float32(frame.GetTimeOffset().GetNanos())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tTime offset of the first frame: %fs\\n\", seconds+nanos/1e9)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tRotated bounding box vertices:\\n\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, vertex := range frame.GetRotatedBoundingBox().GetVertices() {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tVertex x=%f, y=%f\\n\", vertex.GetX(), vertex.GetY())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/video/src/main/java/video/TextDetection.java) \n```\n/**\u00a0* Detect text in a video.\u00a0*\u00a0* @param filePath the path to the video file to analyze.\u00a0*/public static VideoAnnotationResults detectText(String filePath) throws Exception {\u00a0 try (VideoIntelligenceServiceClient client = VideoIntelligenceServiceClient.create()) {\u00a0 \u00a0 // Read file\u00a0 \u00a0 Path path = Paths.get(filePath);\u00a0 \u00a0 byte[] data = Files.readAllBytes(path);\u00a0 \u00a0 // Create the request\u00a0 \u00a0 AnnotateVideoRequest request =\u00a0 \u00a0 \u00a0 \u00a0 AnnotateVideoRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputContent(ByteString.copyFrom(data))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addFeatures(Feature.TEXT_DETECTION)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // asynchronously perform object tracking on videos\u00a0 \u00a0 OperationFuture<AnnotateVideoResponse, AnnotateVideoProgress> future =\u00a0 \u00a0 \u00a0 \u00a0 client.annotateVideoAsync(request);\u00a0 \u00a0 System.out.println(\"Waiting for operation to complete...\");\u00a0 \u00a0 // The first result is retrieved because a single video was processed.\u00a0 \u00a0 AnnotateVideoResponse response = future.get(300, TimeUnit.SECONDS);\u00a0 \u00a0 VideoAnnotationResults results = response.getAnnotationResults(0);\u00a0 \u00a0 // Get only the first annotation for demo purposes.\u00a0 \u00a0 TextAnnotation annotation = results.getTextAnnotations(0);\u00a0 \u00a0 System.out.println(\"Text: \" + annotation.getText());\u00a0 \u00a0 // Get the first text segment.\u00a0 \u00a0 TextSegment textSegment = annotation.getSegments(0);\u00a0 \u00a0 System.out.println(\"Confidence: \" + textSegment.getConfidence());\u00a0 \u00a0 // For the text segment display it's time offset\u00a0 \u00a0 VideoSegment videoSegment = textSegment.getSegment();\u00a0 \u00a0 Duration startTimeOffset = videoSegment.getStartTimeOffset();\u00a0 \u00a0 Duration endTimeOffset = videoSegment.getEndTimeOffset();\u00a0 \u00a0 // Display the offset times in seconds, 1e9 is part of the formula to convert nanos to seconds\u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Start time: %.2f\", startTimeOffset.getSeconds() + startTimeOffset.getNanos() / 1e9));\u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"End time: %.2f\", endTimeOffset.getSeconds() + endTimeOffset.getNanos() / 1e9));\u00a0 \u00a0 // Show the first result for the first frame in the segment.\u00a0 \u00a0 TextFrame textFrame = textSegment.getFrames(0);\u00a0 \u00a0 Duration timeOffset = textFrame.getTimeOffset();\u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Time offset for the first frame: %.2f\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timeOffset.getSeconds() + timeOffset.getNanos() / 1e9));\u00a0 \u00a0 // Display the rotated bounding box for where the text is on the frame.\u00a0 \u00a0 System.out.println(\"Rotated Bounding Box Vertices:\");\u00a0 \u00a0 List<NormalizedVertex> vertices = textFrame.getRotatedBoundingBox().getVerticesList();\u00a0 \u00a0 for (NormalizedVertex normalizedVertex : vertices) {\u00a0 \u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tVertex.x: %.2f, Vertex.y: %.2f\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 normalizedVertex.getX(), normalizedVertex.getY()));\u00a0 \u00a0 }\u00a0 \u00a0 return results;\u00a0 }}\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/video-intelligence/analyze.js) \n```\n// Imports the Google Cloud Video Intelligence library + Node's fs libraryconst Video = require('@google-cloud/video-intelligence');const fs = require('fs');const util = require('util');// Creates a clientconst video = new Video.VideoIntelligenceServiceClient();/**\u00a0* TODO(developer): Uncomment the following line before running the sample.\u00a0*/// const path = 'Local file to analyze, e.g. ./my-file.mp4';// Reads a local video file and converts it to base64const file = await util.promisify(fs.readFile)(path);const inputContent = file.toString('base64');const request = {\u00a0 inputContent: inputContent,\u00a0 features: ['TEXT_DETECTION'],};// Detects text in a videoconst [operation] = await video.annotateVideo(request);const results = await operation.promise();console.log('Waiting for operation to complete...');// Gets annotations for videoconst textAnnotations = results[0].annotationResults[0].textAnnotations;textAnnotations.forEach(textAnnotation => {\u00a0 console.log(`Text ${textAnnotation.text} occurs at:`);\u00a0 textAnnotation.segments.forEach(segment => {\u00a0 \u00a0 const time = segment.segment;\u00a0 \u00a0 if (time.startTimeOffset.seconds === undefined) {\u00a0 \u00a0 \u00a0 time.startTimeOffset.seconds = 0;\u00a0 \u00a0 }\u00a0 \u00a0 if (time.startTimeOffset.nanos === undefined) {\u00a0 \u00a0 \u00a0 time.startTimeOffset.nanos = 0;\u00a0 \u00a0 }\u00a0 \u00a0 if (time.endTimeOffset.seconds === undefined) {\u00a0 \u00a0 \u00a0 time.endTimeOffset.seconds = 0;\u00a0 \u00a0 }\u00a0 \u00a0 if (time.endTimeOffset.nanos === undefined) {\u00a0 \u00a0 \u00a0 time.endTimeOffset.nanos = 0;\u00a0 \u00a0 }\u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 `\\tStart: ${time.startTimeOffset.seconds || 0}` +\u00a0 \u00a0 \u00a0 \u00a0 `.${(time.startTimeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 );\u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 `\\tEnd: ${time.endTimeOffset.seconds || 0}.` +\u00a0 \u00a0 \u00a0 \u00a0 `${(time.endTimeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 );\u00a0 \u00a0 console.log(`\\tConfidence: ${segment.confidence}`);\u00a0 \u00a0 segment.frames.forEach(frame => {\u00a0 \u00a0 \u00a0 const timeOffset = frame.timeOffset;\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `Time offset for the frame: ${timeOffset.seconds || 0}` +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 `.${(timeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log('Rotated Bounding Box Vertices:');\u00a0 \u00a0 \u00a0 frame.rotatedBoundingBox.vertices.forEach(vertex => {\u00a0 \u00a0 \u00a0 \u00a0 console.log(`Vertex.x:${vertex.x}, Vertex.y:${vertex.y}`);\u00a0 \u00a0 \u00a0 });\u00a0 \u00a0 });\u00a0 });});\n``` [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/analyze.py) \n```\nimport iofrom google.cloud import videointelligencedef video_detect_text(path):\u00a0 \u00a0 \"\"\"Detect text in a local video.\"\"\"\u00a0 \u00a0 video_client = videointelligence.VideoIntelligenceServiceClient()\u00a0 \u00a0 features = [videointelligence.Feature.TEXT_DETECTION]\u00a0 \u00a0 video_context = videointelligence.VideoContext()\u00a0 \u00a0 with io.open(path, \"rb\") as file:\u00a0 \u00a0 \u00a0 \u00a0 input_content = file.read()\u00a0 \u00a0 operation = video_client.annotate_video(\u00a0 \u00a0 \u00a0 \u00a0 request={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"features\": features,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"input_content\": input_content,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"video_context\": video_context,\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 )\u00a0 \u00a0 print(\"\\nProcessing video for text detection.\")\u00a0 \u00a0 result = operation.result(timeout=300)\u00a0 \u00a0 # The first result is retrieved because a single video was processed.\u00a0 \u00a0 annotation_result = result.annotation_results[0]\u00a0 \u00a0 for text_annotation in annotation_result.text_annotations:\u00a0 \u00a0 \u00a0 \u00a0 print(\"\\nText: {}\".format(text_annotation.text))\u00a0 \u00a0 \u00a0 \u00a0 # Get the first text segment\u00a0 \u00a0 \u00a0 \u00a0 text_segment = text_annotation.segments[0]\u00a0 \u00a0 \u00a0 \u00a0 start_time = text_segment.segment.start_time_offset\u00a0 \u00a0 \u00a0 \u00a0 end_time = text_segment.segment.end_time_offset\u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"start_time: {}, end_time: {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start_time.seconds + start_time.microseconds * 1e-6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end_time.seconds + end_time.microseconds * 1e-6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 print(\"Confidence: {}\".format(text_segment.confidence))\u00a0 \u00a0 \u00a0 \u00a0 # Show the result for the first frame in this segment.\u00a0 \u00a0 \u00a0 \u00a0 frame = text_segment.frames[0]\u00a0 \u00a0 \u00a0 \u00a0 time_offset = frame.time_offset\u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Time offset for the first frame: {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 time_offset.seconds + time_offset.microseconds * 1e-6\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 print(\"Rotated Bounding Box Vertices:\")\u00a0 \u00a0 \u00a0 \u00a0 for vertex in frame.rotated_bounding_box.vertices:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tVertex.x: {}, Vertex.y: {}\".format(vertex.x, vertex.y))\n```No preface\n **C#** : Please follow the [C# setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for .NET.](https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.VideoIntelligence.V1/index.html) \n **PHP** : Please follow the [PHP setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for PHP.](/php/docs/reference/cloud-videointelligence/latest) \n **Ruby** : Please follow the [Ruby setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for Ruby.](https://googleapis.dev/ruby/google-cloud-video_intelligence/latest/Google/Cloud/VideoIntelligence/V1.html)", "guide": "Cloud Video Intelligence API"}