{"title": "Cloud Video Intelligence API - \u8b58\u5225\u6587\u5b57", "url": "https://cloud.google.com/video-intelligence/docs/text-detection?hl=zh-cn", "abstract": "# Cloud Video Intelligence API - \u8b58\u5225\u6587\u5b57\n\u529f\u80fd\u53ef\u57f7\u884c\u5149\u5b78\u5b57\u7b26\u8b58\u5225\uff08OCR\uff09\uff0c\u6aa2\u6e2c\u4e26\u63d0\u53d6\u8f38\u5165\u8996\u983b\u4e2d\u7684\u6587\u672c\u3002\n\u6587\u672c\u6aa2\u6e2c\u9069\u7528\u65bc Cloud Vision API \u652f\u6301\u7684\u6240\u6709 [\u8a9e\u8a00](https://cloud.google.com/vision/docs/languages?hl=zh-cn) \u3002\n", "content": "## \u8acb\u6c42\u5c0d Cloud Storage \u4e2d\u7684\u8996\u983b\u57f7\u884c\u6587\u672c\u6aa2\u6e2c\n\u4ee5\u4e0b\u793a\u4f8b\u6f14\u793a\u77ad\u5982\u4f55\u5c0d Cloud Storage \u4e2d\u7684\u6587\u4ef6\u57f7\u884c\u6587\u672c\u6aa2\u6e2c\u3002\n## \u767c\u9001\u8996\u983b\u8a3b\u89e3\u8acb\u6c42\u4ee5\u4e0b\u4ee3\u78bc\u5c55\u793a\u77ad\u5982\u4f55\u5411 [videos:annotate](https://cloud.google.com/video-intelligence/docs/reference/rest/v1/videos/annotate?hl=zh-cn) \u65b9\u6cd5\u767c\u9001 POST \u8acb\u6c42\u3002\u8a72\u793a\u4f8b\u4f7f\u7528 Google Cloud CLI \u5275\u5efa\u8a2a\u554f\u4ee4\u724c\u3002\u5982\u9700\u77ad\u89e3\u5982\u4f55\u5b89\u88dd gcloud CLI\uff0c\u8acb\u53c3\u95b1 [Video Intelligence API \u5feb\u901f\u5165\u9580](https://cloud.google.com/video-intelligence/docs/quickstarts?hl=zh-cn) \u3002\n\u5728\u4f7f\u7528\u4efb\u4f55\u8acb\u6c42\u6578\u64da\u4e4b\u524d\uff0c\u8acb\u5148\u9032\u884c\u4ee5\u4e0b\u66ff\u63db\uff1a- \uff1a\u5305\u542b\u8981\u6dfb\u52a0\u8a3b\u91cb\u7684\u6587\u4ef6\u7684 Cloud Storage \u5b58\u5132\u6876\uff08\u5305\u62ec\u6587\u4ef6\u540d\uff09\u3002\u5fc5\u9808\u4ee5`gs://`\u958b\u982d\u3002\u4f8b\u5982\uff1a`\"inputUri\": \"gs://cloud-videointelligence-demo/assistant.mp4\",`\u3002\n- \uff1a[\u53ef\u9078]\u4f8b\u5982 \u201cen-US\u201d\n- \uff1a\u60a8\u7684 Google Cloud \u9805\u76ee\u7684\u6578\u5b57\u6a19\u8b58\u7b26\nHTTP \u65b9\u6cd5\u548c\u7db2\u5740\uff1a\n```\nPOST https://videointelligence.googleapis.com/v1/videos:annotate\n```\n\u8acb\u6c42 JSON \u6b63\u6587\uff1a\n```\n{\n \"inputUri\": \"INPUT_URI\",\n \"features\": [\"TEXT_DETECTION\"],\n \"videoContext\": {\n \"textDetectionConfig\": {\n  \"languageHints\": [\"LANGUAGE_CODE\"]\n }\n }\n}\n```\n\u5982\u9700\u767c\u9001\u60a8\u7684\u8acb\u6c42\uff0c\u8acb\u5c55\u958b\u4ee5\u4e0b\u9078\u9805\u4e4b\u4e00\uff1a\u60a8\u61c9\u8a72\u6536\u5230\u985e\u4f3c\u4ee5\u4e0b\u5167\u5bb9\u7684 JSON \u97ff\u61c9\uff1a\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/operations/OPERATION_ID\"\n}\n```\n\u5982\u679c\u97ff\u61c9\u6210\u529f\uff0cVideo Intelligence API \u5c07\u8fd4\u56de\u60a8\u7684\u64cd\u4f5c\u7684 `name` \u3002\u4e0a\u9762\u7684\u793a\u4f8b\u5c55\u793a\u4e86\u6b64\u985e\u97ff\u61c9\u7684\u793a\u4f8b\uff0c\u5176\u4e2d `project-number` \u662f\u60a8\u7684\u9805\u76ee\u7de8\u865f\uff0c `operation-id` \u662f\u7232\u8acb\u6c42\u5275\u5efa\u7684\u9577\u6642\u9593\u904b\u884c\u7684\u64cd\u4f5c\u7684 ID\u3002\n- \uff1a\u60a8\u9805\u76ee\u7684\u7de8\u865f\n- \uff1a\u5728\u5176\u4e2d\u6dfb\u52a0\u8a3b\u89e3\u7684 Cloud \u5340\u57df\u3002\u652f\u6301\u7684\u96f2\u5340\u57df\u7232\uff1a`us-east1`\u3001`us-west1`\u3001`europe-west1`\u3001`asia-east1`\u3002\u5982\u679c\u672a\u6307\u5b9a\u5340\u57df\uff0c\u7cfb\u7d71\u5c07\u6839\u64da\u8996\u983b\u6587\u4ef6\u4f4d\u7f6e\u78ba\u5b9a\u5340\u57df\u3002\n- \uff1a\u662f\u7232\u8acb\u6c42\u5275\u5efa\u7684\u9577\u6642\u9593\u904b\u884c\u7684\u64cd\u4f5c\u7684 ID\uff0c\u4e26\u5728\u5553\u52d5\u64cd\u4f5c\u6642\u5728\u97ff\u61c9\u4e2d\u63d0\u4f9b\uff0c\u4f8b\u5982`12345...`\n## \u7372\u53d6\u8a3b\u89e3\u7d50\u679c\u8981\u6aa2\u7d22\u64cd\u4f5c\u7684\u7d50\u679c\uff0c\u8acb\u4f7f\u7528\u5f9e [videos\uff1aannotate](https://cloud.google.com/video-intelligence/docs/reference/rest/v1/videos/annotate?hl=zh-cn) \u8abf\u7528\u8fd4\u56de\u7684\u64cd\u4f5c\u540d\u7a31\u767c\u51fa [GET](https://cloud.google.com/video-intelligence/docs/reference/rest/v1/projects.locations.operations/get?hl=zh-cn) \u8acb\u6c42\uff0c\u5982\u4ee5\u4e0b\u793a\u4f8b\u6240\u793a\u3002\n\u5728\u4f7f\u7528\u4efb\u4f55\u8acb\u6c42\u6578\u64da\u4e4b\u524d\uff0c\u8acb\u5148\u9032\u884c\u4ee5\u4e0b\u66ff\u63db\uff1a- \uff1aVideo Intelligence API \u8fd4\u56de\u7684\u64cd\u4f5c\u540d\u7a31\u3002\u64cd\u4f5c\u540d\u7a31\u63a1\u7528`projects/` `` `/locations/` `` `/operations/` ``\u683c\u5f0f\n- \uff1a\u60a8\u7684 Google Cloud \u9805\u76ee\u7684\u6578\u5b57\u6a19\u8b58\u7b26\nHTTP \u65b9\u6cd5\u548c\u7db2\u5740\uff1a\n```\nGET https://videointelligence.googleapis.com/v1/OPERATION_NAME\n```\n\u5982\u9700\u767c\u9001\u60a8\u7684\u8acb\u6c42\uff0c\u8acb\u5c55\u958b\u4ee5\u4e0b\u9078\u9805\u4e4b\u4e00\uff1a\u60a8\u61c9\u8a72\u6536\u5230\u985e\u4f3c\u4ee5\u4e0b\u5167\u5bb9\u7684 JSON \u97ff\u61c9\uff1a\n\u6587\u672c\u6aa2\u6e2c\u8a3b\u91cb\u4ee5\n`textAnnotations`\n\u5217\u8868\u7684\u5f62\u5f0f\u8fd4\u56de\u3002\u6ce8\u610f\uff1a\u50c5\u7576\u503c\u7232\n **True** \n\u6642\uff0c\u7e94\u6703\u8fd4\u56de\n **done** \n\u5b57\u6bb5\u3002\u64cd\u4f5c\u672a\u5b8c\u6210\u7684\u97ff\u61c9\u4e2d\u4e0d\u5305\u542b\u8a72\u5b57\u6bb5\u3002\n## \u4e0b\u8f09\u8a3b\u89e3\u7d50\u679c\u5c07\u4f86\u6e90\u4e2d\u7684\u8a3b\u89e3\u8907\u88fd\u5230\u76ee\u6a19\u5b58\u5132\u6876\uff08\u8acb\u53c3\u95b1 [\u8907\u88fd\u6587\u4ef6\u548c\u5c0d\u8c61](https://cloud.google.com/storage/docs/gsutil/commands/cp?hl=zh-cn) \uff09\uff1a\n`gsutil cp` `` `gs://my-bucket`\n\u6ce8\u610f\uff1a\u5982\u679c\u8f38\u51fa gcs uri \u7531\u7528\u6236\u63d0\u4f9b\uff0c\u5247\u8a3b\u89e3\u5b58\u5132\u5728\u8a72 gcs uri \u4e2d\u3002 [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/videointelligence/annotate/text_detection_gcs.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 video \"cloud.google.com/go/videointelligence/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 videopb \"cloud.google.com/go/videointelligence/apiv1/videointelligencepb\"\u00a0 \u00a0 \u00a0 \u00a0 \"github.com/golang/protobuf/ptypes\")// textDetectionGCS analyzes a video and extracts the text from the video's audio.func textDetectionGCS(w io.Writer, gcsURI string) error {\u00a0 \u00a0 \u00a0 \u00a0 // gcsURI := \"gs://python-docs-samples-tests/video/googlework_short.mp4\"\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // Creates a client.\u00a0 \u00a0 \u00a0 \u00a0 client, err := video.NewClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"video.NewClient: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 op, err := client.AnnotateVideo(ctx, &videopb.AnnotateVideoRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputUri: gcsURI,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Features: []videopb.Feature{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 videopb.Feature_TEXT_DETECTION,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 })\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"AnnotateVideo: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 resp, err := op.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"Wait: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Only one video was processed, so get the first result.\u00a0 \u00a0 \u00a0 \u00a0 result := resp.GetAnnotationResults()[0]\u00a0 \u00a0 \u00a0 \u00a0 for _, annotation := range result.TextAnnotations {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Text: %q\\n\", annotation.GetText())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Get the first text segment.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment := annotation.GetSegments()[0]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start, _ := ptypes.Duration(segment.GetSegment().GetStartTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end, _ := ptypes.Duration(segment.GetSegment().GetEndTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tSegment: %v to %v\\n\", start, end)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tConfidence: %f\\n\", segment.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Show the result for the first frame in this segment.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frame := segment.GetFrames()[0]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 seconds := float32(frame.GetTimeOffset().GetSeconds())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nanos := float32(frame.GetTimeOffset().GetNanos())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tTime offset of the first frame: %fs\\n\", seconds+nanos/1e9)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tRotated bounding box vertices:\\n\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, vertex := range frame.GetRotatedBoundingBox().GetVertices() {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tVertex x=%f, y=%f\\n\", vertex.GetX(), vertex.GetY())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/video/src/main/java/video/TextDetection.java) \n```\n/**\u00a0* Detect Text in a video.\u00a0*\u00a0* @param gcsUri the path to the video file to analyze.\u00a0*/public static VideoAnnotationResults detectTextGcs(String gcsUri) throws Exception {\u00a0 try (VideoIntelligenceServiceClient client = VideoIntelligenceServiceClient.create()) {\u00a0 \u00a0 // Create the request\u00a0 \u00a0 AnnotateVideoRequest request =\u00a0 \u00a0 \u00a0 \u00a0 AnnotateVideoRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputUri(gcsUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addFeatures(Feature.TEXT_DETECTION)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // asynchronously perform object tracking on videos\u00a0 \u00a0 OperationFuture<AnnotateVideoResponse, AnnotateVideoProgress> future =\u00a0 \u00a0 \u00a0 \u00a0 client.annotateVideoAsync(request);\u00a0 \u00a0 System.out.println(\"Waiting for operation to complete...\");\u00a0 \u00a0 // The first result is retrieved because a single video was processed.\u00a0 \u00a0 AnnotateVideoResponse response = future.get(300, TimeUnit.SECONDS);\u00a0 \u00a0 VideoAnnotationResults results = response.getAnnotationResults(0);\u00a0 \u00a0 // Get only the first annotation for demo purposes.\u00a0 \u00a0 TextAnnotation annotation = results.getTextAnnotations(0);\u00a0 \u00a0 System.out.println(\"Text: \" + annotation.getText());\u00a0 \u00a0 // Get the first text segment.\u00a0 \u00a0 TextSegment textSegment = annotation.getSegments(0);\u00a0 \u00a0 System.out.println(\"Confidence: \" + textSegment.getConfidence());\u00a0 \u00a0 // For the text segment display it's time offset\u00a0 \u00a0 VideoSegment videoSegment = textSegment.getSegment();\u00a0 \u00a0 Duration startTimeOffset = videoSegment.getStartTimeOffset();\u00a0 \u00a0 Duration endTimeOffset = videoSegment.getEndTimeOffset();\u00a0 \u00a0 // Display the offset times in seconds, 1e9 is part of the formula to convert nanos to seconds\u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Start time: %.2f\", startTimeOffset.getSeconds() + startTimeOffset.getNanos() / 1e9));\u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"End time: %.2f\", endTimeOffset.getSeconds() + endTimeOffset.getNanos() / 1e9));\u00a0 \u00a0 // Show the first result for the first frame in the segment.\u00a0 \u00a0 TextFrame textFrame = textSegment.getFrames(0);\u00a0 \u00a0 Duration timeOffset = textFrame.getTimeOffset();\u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Time offset for the first frame: %.2f\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timeOffset.getSeconds() + timeOffset.getNanos() / 1e9));\u00a0 \u00a0 // Display the rotated bounding box for where the text is on the frame.\u00a0 \u00a0 System.out.println(\"Rotated Bounding Box Vertices:\");\u00a0 \u00a0 List<NormalizedVertex> vertices = textFrame.getRotatedBoundingBox().getVerticesList();\u00a0 \u00a0 for (NormalizedVertex normalizedVertex : vertices) {\u00a0 \u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tVertex.x: %.2f, Vertex.y: %.2f\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 normalizedVertex.getX(), normalizedVertex.getY()));\u00a0 \u00a0 }\u00a0 \u00a0 return results;\u00a0 }}\n```\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/video-intelligence/analyze.js) \n```\n// Imports the Google Cloud Video Intelligence libraryconst Video = require('@google-cloud/video-intelligence');// Creates a clientconst video = new Video.VideoIntelligenceServiceClient();/**\u00a0* TODO(developer): Uncomment the following line before running the sample.\u00a0*/// const gcsUri = 'GCS URI of the video to analyze, e.g. gs://my-bucket/my-video.mp4';const request = {\u00a0 inputUri: gcsUri,\u00a0 features: ['TEXT_DETECTION'],};// Detects text in a videoconst [operation] = await video.annotateVideo(request);const results = await operation.promise();console.log('Waiting for operation to complete...');// Gets annotations for videoconst textAnnotations = results[0].annotationResults[0].textAnnotations;textAnnotations.forEach(textAnnotation => {\u00a0 console.log(`Text ${textAnnotation.text} occurs at:`);\u00a0 textAnnotation.segments.forEach(segment => {\u00a0 \u00a0 const time = segment.segment;\u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 ` Start: ${time.startTimeOffset.seconds || 0}.${(\u00a0 \u00a0 \u00a0 \u00a0 time.startTimeOffset.nanos / 1e6\u00a0 \u00a0 \u00a0 ).toFixed(0)}s`\u00a0 \u00a0 );\u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 ` End: ${time.endTimeOffset.seconds || 0}.${(\u00a0 \u00a0 \u00a0 \u00a0 time.endTimeOffset.nanos / 1e6\u00a0 \u00a0 \u00a0 ).toFixed(0)}s`\u00a0 \u00a0 );\u00a0 \u00a0 console.log(` Confidence: ${segment.confidence}`);\u00a0 \u00a0 segment.frames.forEach(frame => {\u00a0 \u00a0 \u00a0 const timeOffset = frame.timeOffset;\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `Time offset for the frame: ${timeOffset.seconds || 0}` +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 `.${(timeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log('Rotated Bounding Box Vertices:');\u00a0 \u00a0 \u00a0 frame.rotatedBoundingBox.vertices.forEach(vertex => {\u00a0 \u00a0 \u00a0 \u00a0 console.log(`Vertex.x:${vertex.x}, Vertex.y:${vertex.y}`);\u00a0 \u00a0 \u00a0 });\u00a0 \u00a0 });\u00a0 });});\n```\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/analyze.py) \n```\n\"\"\"Detect text in a video stored on GCS.\"\"\"from google.cloud import videointelligencevideo_client = videointelligence.VideoIntelligenceServiceClient()features = [videointelligence.Feature.TEXT_DETECTION]operation = video_client.annotate_video(\u00a0 \u00a0 request={\"features\": features, \"input_uri\": input_uri})print(\"\\nProcessing video for text detection.\")result = operation.result(timeout=600)# The first result is retrieved because a single video was processed.annotation_result = result.annotation_results[0]for text_annotation in annotation_result.text_annotations:\u00a0 \u00a0 print(\"\\nText: {}\".format(text_annotation.text))\u00a0 \u00a0 # Get the first text segment\u00a0 \u00a0 text_segment = text_annotation.segments[0]\u00a0 \u00a0 start_time = text_segment.segment.start_time_offset\u00a0 \u00a0 end_time = text_segment.segment.end_time_offset\u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \"start_time: {}, end_time: {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start_time.seconds + start_time.microseconds * 1e-6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end_time.seconds + end_time.microseconds * 1e-6,\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 print(\"Confidence: {}\".format(text_segment.confidence))\u00a0 \u00a0 # Show the result for the first frame in this segment.\u00a0 \u00a0 frame = text_segment.frames[0]\u00a0 \u00a0 time_offset = frame.time_offset\u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \"Time offset for the first frame: {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 time_offset.seconds + time_offset.microseconds * 1e-6\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 )\u00a0 \u00a0 print(\"Rotated Bounding Box Vertices:\")\u00a0 \u00a0 for vertex in frame.rotated_bounding_box.vertices:\u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tVertex.x: {}, Vertex.y: {}\".format(vertex.x, vertex.y))\n```No preface\n **C#** \uff1a\u8acb\u6309\u7167\u201c\u5ba2\u6236\u7aef\u5eab\u201d\u9801\u9762\u4e0a\u7684 [C# \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u9032\u884c\u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [\u9069\u7528\u65bc .NET \u7684 Video Intelligence \u53c3\u8003\u6587\u6a94](https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.VideoIntelligence.V1/index.html) \u3002\n **PHP** \uff1a\u8acb\u6309\u7167\u5ba2\u6236\u7aef\u5eab\u9801\u9762\u4e0a\u7684 [PHP \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [PHP \u7248 Video Intelligence \u53c3\u8003\u6587\u6a94](https://cloud.google.com/php/docs/reference/cloud-videointelligence/latest?hl=zh-cn) \u3002\n **Ruby** \uff1a\u8acb\u6309\u7167\u5ba2\u6236\u7aef\u5eab\u9801\u9762\u4e0a\u7684 [Ruby \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [Ruby \u7248 Video Intelligence \u53c3\u8003\u6587\u6a94](https://googleapis.dev/ruby/google-cloud-video_intelligence/latest/Google/Cloud/VideoIntelligence/V1.html) \u3002\n## \u8acb\u6c42\u5c0d\u672c\u5730\u6587\u4ef6\u4e2d\u7684\u8996\u983b\u57f7\u884c\u6587\u672c\u6aa2\u6e2c\n\u4ee5\u4e0b\u793a\u4f8b\u5c55\u793a\u5982\u4f55\u5c0d\u672c\u5730\u5b58\u5132\u7684\u6587\u4ef6\u57f7\u884c\u6587\u672c\u6aa2\u6e2c\u3002\n## \u767c\u9001\u8996\u983b\u8a3b\u89e3\u8acb\u6c42\u8981\u5c0d\u672c\u5730\u8996\u983b\u6587\u4ef6\u57f7\u884c\u8a3b\u89e3\uff0c\u8acb\u52d9\u5fc5\u5c0d\u8996\u983b\u6587\u4ef6\u7684\u5167\u5bb9\u9032\u884c base64 \u7de8\u78bc\u3002\u5728\u8acb\u6c42\u7684 `inputContent` \u5b57\u6bb5\u4e2d\u6dfb\u52a0 base64 \u7de8\u78bc\u7684\u5167\u5bb9\u3002\u5982\u9700\u77ad\u89e3\u5982\u4f55\u5c0d\u8996\u983b\u6587\u4ef6\u7684\u5167\u5bb9\u9032\u884c base64 \u7de8\u78bc\uff0c\u8acb\u53c3\u95b1 [Base64 \u7de8\u78bc](https://cloud.google.com/video-intelligence/docs/base64?hl=zh-cn) \u3002\n\u4ee5\u4e0b\u4ee3\u78bc\u5c55\u793a\u77ad\u5982\u4f55\u5411 `videos:annotate` \u65b9\u6cd5\u767c\u9001 POST \u8acb\u6c42\u3002\u8a72\u793a\u4f8b\u4f7f\u7528 Google Cloud CLI \u5275\u5efa\u8a2a\u554f\u4ee4\u724c\u3002\u5982\u9700\u77ad\u89e3\u5982\u4f55\u5b89\u88dd Google Cloud CLI\uff0c\u8acb\u53c3\u95b1 [Video Intelligence API \u5feb\u901f\u5165\u9580](https://cloud.google.com/video-intelligence/docs/quickstarts?hl=zh-cn) \n\u5728\u4f7f\u7528\u4efb\u4f55\u8acb\u6c42\u6578\u64da\u4e4b\u524d\uff0c\u8acb\u5148\u9032\u884c\u4ee5\u4e0b\u66ff\u63db\uff1a- \"inputContent\":\u4f8b\u5982\uff1a`\"UklGRg41AwBBVkkgTElTVAwBAABoZHJsYXZpaDgAAAA1ggAAxPMBAAAAAAAQCAA...\"`\n- \uff1a[\u53ef\u9078]\u4f8b\u5982 \u201cen-US\u201d\n- \uff1a\u60a8\u7684 Google Cloud \u9805\u76ee\u7684\u6578\u5b57\u6a19\u8b58\u7b26\nHTTP \u65b9\u6cd5\u548c\u7db2\u5740\uff1a\n```\nPOST https://videointelligence.googleapis.com/v1/videos:annotate\n```\n\u8acb\u6c42 JSON \u6b63\u6587\uff1a\n```\n{\n \"inputContent\": \"BASE64_ENCODED_CONTENT\",\n \"features\": [\"TEXT_DETECTION\"],\n \"videoContext\": {\n \"textDetectionConfig\": {\n  \"languageHints\": [\"LANGUAGE_CODE\"]\n }\n }\n}\n```\n\u5982\u9700\u767c\u9001\u60a8\u7684\u8acb\u6c42\uff0c\u8acb\u5c55\u958b\u4ee5\u4e0b\u9078\u9805\u4e4b\u4e00\uff1a\u60a8\u61c9\u8a72\u6536\u5230\u985e\u4f3c\u4ee5\u4e0b\u5167\u5bb9\u7684 JSON \u97ff\u61c9\uff1a\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/operations/OPERATION_ID\"\n}\n```\n\u5982\u679c\u97ff\u61c9\u6210\u529f\uff0cVideo Intelligence API \u5c07\u8fd4\u56de\u60a8\u7684\u64cd\u4f5c\u7684 `name` \u3002\u4e0a\u9762\u7684\u793a\u4f8b\u5c55\u793a\u4e86\u6b64\u985e\u97ff\u61c9\u7684\u793a\u4f8b\uff0c\u5176\u4e2d `project-number` \u662f\u60a8\u7684\u9805\u76ee\u540d\u7a31\uff0c `operation-id` \u662f\u7232\u8acb\u6c42\u5275\u5efa\u7684\u9577\u6642\u9593\u904b\u884c\u7684\u64cd\u4f5c\u7684 ID\u3002\n- \uff1a\u4e26\u5728\u5553\u52d5\u64cd\u4f5c\u6642\u5728\u97ff\u61c9\u4e2d\u63d0\u4f9b\uff0c\u4f8b\u5982`12345...`\n## \u7372\u53d6\u8a3b\u89e3\u7d50\u679c\u8981\u6aa2\u7d22\u64cd\u4f5c\u7684\u7d50\u679c\uff0c\u8acb\u4f7f\u7528\u5f9e [videos\uff1aannotate](https://cloud.google.com/video-intelligence/docs/reference/rest/v1/videos/annotate?hl=zh-cn) \u8abf\u7528\u8fd4\u56de\u7684\u64cd\u4f5c\u540d\u7a31\u767c\u51fa [GET](https://cloud.google.com/video-intelligence/docs/reference/rest/v1/projects.locations.operations/get?hl=zh-cn) \u8acb\u6c42\uff0c\u5982\u4ee5\u4e0b\u793a\u4f8b\u6240\u793a\u3002\n\u5728\u4f7f\u7528\u4efb\u4f55\u8acb\u6c42\u6578\u64da\u4e4b\u524d\uff0c\u8acb\u5148\u9032\u884c\u4ee5\u4e0b\u66ff\u63db\uff1a- \uff1a\u60a8\u7684 Google Cloud \u9805\u76ee\u7684\u6578\u5b57\u6a19\u8b58\u7b26\nHTTP \u65b9\u6cd5\u548c\u7db2\u5740\uff1a\n```\nGET https://videointelligence.googleapis.com/v1/OPERATION_NAME\n```\n\u5982\u9700\u767c\u9001\u60a8\u7684\u8acb\u6c42\uff0c\u8acb\u5c55\u958b\u4ee5\u4e0b\u9078\u9805\u4e4b\u4e00\uff1a\u60a8\u61c9\u8a72\u6536\u5230\u985e\u4f3c\u4ee5\u4e0b\u5167\u5bb9\u7684 JSON \u97ff\u61c9\uff1a\u6587\u672c\u6aa2\u6e2c\u8a3b\u91cb\u4ee5 `textAnnotations` \u5217\u8868\u7684\u5f62\u5f0f\u8fd4\u56de\u3002\u6ce8\u610f\uff1a\u50c5\u7576\u503c\u7232 **True** \u6642\uff0c\u7e94\u6703\u8fd4\u56de **done** \u5b57\u6bb5\u3002\u5b83\u4e0d\u6703\u5305\u542b\u5728\u64cd\u4f5c\u5c1a\u672a\u5b8c\u6210\u7684\u97ff\u61c9\u4e2d\u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/videointelligence/annotate/text_detection.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 \"io/ioutil\"\u00a0 \u00a0 \u00a0 \u00a0 video \"cloud.google.com/go/videointelligence/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 videopb \"cloud.google.com/go/videointelligence/apiv1/videointelligencepb\"\u00a0 \u00a0 \u00a0 \u00a0 \"github.com/golang/protobuf/ptypes\")// textDetection analyzes a video and extracts the text from the video's audio.func textDetection(w io.Writer, filename string) error {\u00a0 \u00a0 \u00a0 \u00a0 // filename := \"../testdata/googlework_short.mp4\"\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // Creates a client.\u00a0 \u00a0 \u00a0 \u00a0 client, err := video.NewClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"video.NewClient: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 fileBytes, err := ioutil.ReadFile(filename)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"ioutil.ReadFile: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 op, err := client.AnnotateVideo(ctx, &videopb.AnnotateVideoRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputContent: fileBytes,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Features: []videopb.Feature{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 videopb.Feature_TEXT_DETECTION,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 })\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"AnnotateVideo: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 resp, err := op.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"Wait: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Only one video was processed, so get the first result.\u00a0 \u00a0 \u00a0 \u00a0 result := resp.GetAnnotationResults()[0]\u00a0 \u00a0 \u00a0 \u00a0 for _, annotation := range result.TextAnnotations {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Text: %q\\n\", annotation.GetText())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Get the first text segment.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment := annotation.GetSegments()[0]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start, _ := ptypes.Duration(segment.GetSegment().GetStartTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end, _ := ptypes.Duration(segment.GetSegment().GetEndTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tSegment: %v to %v\\n\", start, end)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tConfidence: %f\\n\", segment.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Show the result for the first frame in this segment.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frame := segment.GetFrames()[0]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 seconds := float32(frame.GetTimeOffset().GetSeconds())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nanos := float32(frame.GetTimeOffset().GetNanos())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tTime offset of the first frame: %fs\\n\", seconds+nanos/1e9)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tRotated bounding box vertices:\\n\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, vertex := range frame.GetRotatedBoundingBox().GetVertices() {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tVertex x=%f, y=%f\\n\", vertex.GetX(), vertex.GetY())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n``` [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/video/src/main/java/video/TextDetection.java) \n```\n/**\u00a0* Detect text in a video.\u00a0*\u00a0* @param filePath the path to the video file to analyze.\u00a0*/public static VideoAnnotationResults detectText(String filePath) throws Exception {\u00a0 try (VideoIntelligenceServiceClient client = VideoIntelligenceServiceClient.create()) {\u00a0 \u00a0 // Read file\u00a0 \u00a0 Path path = Paths.get(filePath);\u00a0 \u00a0 byte[] data = Files.readAllBytes(path);\u00a0 \u00a0 // Create the request\u00a0 \u00a0 AnnotateVideoRequest request =\u00a0 \u00a0 \u00a0 \u00a0 AnnotateVideoRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputContent(ByteString.copyFrom(data))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addFeatures(Feature.TEXT_DETECTION)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 // asynchronously perform object tracking on videos\u00a0 \u00a0 OperationFuture<AnnotateVideoResponse, AnnotateVideoProgress> future =\u00a0 \u00a0 \u00a0 \u00a0 client.annotateVideoAsync(request);\u00a0 \u00a0 System.out.println(\"Waiting for operation to complete...\");\u00a0 \u00a0 // The first result is retrieved because a single video was processed.\u00a0 \u00a0 AnnotateVideoResponse response = future.get(300, TimeUnit.SECONDS);\u00a0 \u00a0 VideoAnnotationResults results = response.getAnnotationResults(0);\u00a0 \u00a0 // Get only the first annotation for demo purposes.\u00a0 \u00a0 TextAnnotation annotation = results.getTextAnnotations(0);\u00a0 \u00a0 System.out.println(\"Text: \" + annotation.getText());\u00a0 \u00a0 // Get the first text segment.\u00a0 \u00a0 TextSegment textSegment = annotation.getSegments(0);\u00a0 \u00a0 System.out.println(\"Confidence: \" + textSegment.getConfidence());\u00a0 \u00a0 // For the text segment display it's time offset\u00a0 \u00a0 VideoSegment videoSegment = textSegment.getSegment();\u00a0 \u00a0 Duration startTimeOffset = videoSegment.getStartTimeOffset();\u00a0 \u00a0 Duration endTimeOffset = videoSegment.getEndTimeOffset();\u00a0 \u00a0 // Display the offset times in seconds, 1e9 is part of the formula to convert nanos to seconds\u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Start time: %.2f\", startTimeOffset.getSeconds() + startTimeOffset.getNanos() / 1e9));\u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"End time: %.2f\", endTimeOffset.getSeconds() + endTimeOffset.getNanos() / 1e9));\u00a0 \u00a0 // Show the first result for the first frame in the segment.\u00a0 \u00a0 TextFrame textFrame = textSegment.getFrames(0);\u00a0 \u00a0 Duration timeOffset = textFrame.getTimeOffset();\u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Time offset for the first frame: %.2f\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timeOffset.getSeconds() + timeOffset.getNanos() / 1e9));\u00a0 \u00a0 // Display the rotated bounding box for where the text is on the frame.\u00a0 \u00a0 System.out.println(\"Rotated Bounding Box Vertices:\");\u00a0 \u00a0 List<NormalizedVertex> vertices = textFrame.getRotatedBoundingBox().getVerticesList();\u00a0 \u00a0 for (NormalizedVertex normalizedVertex : vertices) {\u00a0 \u00a0 \u00a0 System.out.println(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 String.format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tVertex.x: %.2f, Vertex.y: %.2f\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 normalizedVertex.getX(), normalizedVertex.getY()));\u00a0 \u00a0 }\u00a0 \u00a0 return results;\u00a0 }}\n```\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/video-intelligence/analyze.js) \n```\n// Imports the Google Cloud Video Intelligence library + Node's fs libraryconst Video = require('@google-cloud/video-intelligence');const fs = require('fs');const util = require('util');// Creates a clientconst video = new Video.VideoIntelligenceServiceClient();/**\u00a0* TODO(developer): Uncomment the following line before running the sample.\u00a0*/// const path = 'Local file to analyze, e.g. ./my-file.mp4';// Reads a local video file and converts it to base64const file = await util.promisify(fs.readFile)(path);const inputContent = file.toString('base64');const request = {\u00a0 inputContent: inputContent,\u00a0 features: ['TEXT_DETECTION'],};// Detects text in a videoconst [operation] = await video.annotateVideo(request);const results = await operation.promise();console.log('Waiting for operation to complete...');// Gets annotations for videoconst textAnnotations = results[0].annotationResults[0].textAnnotations;textAnnotations.forEach(textAnnotation => {\u00a0 console.log(`Text ${textAnnotation.text} occurs at:`);\u00a0 textAnnotation.segments.forEach(segment => {\u00a0 \u00a0 const time = segment.segment;\u00a0 \u00a0 if (time.startTimeOffset.seconds === undefined) {\u00a0 \u00a0 \u00a0 time.startTimeOffset.seconds = 0;\u00a0 \u00a0 }\u00a0 \u00a0 if (time.startTimeOffset.nanos === undefined) {\u00a0 \u00a0 \u00a0 time.startTimeOffset.nanos = 0;\u00a0 \u00a0 }\u00a0 \u00a0 if (time.endTimeOffset.seconds === undefined) {\u00a0 \u00a0 \u00a0 time.endTimeOffset.seconds = 0;\u00a0 \u00a0 }\u00a0 \u00a0 if (time.endTimeOffset.nanos === undefined) {\u00a0 \u00a0 \u00a0 time.endTimeOffset.nanos = 0;\u00a0 \u00a0 }\u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 `\\tStart: ${time.startTimeOffset.seconds || 0}` +\u00a0 \u00a0 \u00a0 \u00a0 `.${(time.startTimeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 );\u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 `\\tEnd: ${time.endTimeOffset.seconds || 0}.` +\u00a0 \u00a0 \u00a0 \u00a0 `${(time.endTimeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 );\u00a0 \u00a0 console.log(`\\tConfidence: ${segment.confidence}`);\u00a0 \u00a0 segment.frames.forEach(frame => {\u00a0 \u00a0 \u00a0 const timeOffset = frame.timeOffset;\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `Time offset for the frame: ${timeOffset.seconds || 0}` +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 `.${(timeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log('Rotated Bounding Box Vertices:');\u00a0 \u00a0 \u00a0 frame.rotatedBoundingBox.vertices.forEach(vertex => {\u00a0 \u00a0 \u00a0 \u00a0 console.log(`Vertex.x:${vertex.x}, Vertex.y:${vertex.y}`);\u00a0 \u00a0 \u00a0 });\u00a0 \u00a0 });\u00a0 });});\n``` [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/analyze.py) \n```\nimport iofrom google.cloud import videointelligencedef video_detect_text(path):\u00a0 \u00a0 \"\"\"Detect text in a local video.\"\"\"\u00a0 \u00a0 video_client = videointelligence.VideoIntelligenceServiceClient()\u00a0 \u00a0 features = [videointelligence.Feature.TEXT_DETECTION]\u00a0 \u00a0 video_context = videointelligence.VideoContext()\u00a0 \u00a0 with io.open(path, \"rb\") as file:\u00a0 \u00a0 \u00a0 \u00a0 input_content = file.read()\u00a0 \u00a0 operation = video_client.annotate_video(\u00a0 \u00a0 \u00a0 \u00a0 request={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"features\": features,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"input_content\": input_content,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"video_context\": video_context,\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 )\u00a0 \u00a0 print(\"\\nProcessing video for text detection.\")\u00a0 \u00a0 result = operation.result(timeout=300)\u00a0 \u00a0 # The first result is retrieved because a single video was processed.\u00a0 \u00a0 annotation_result = result.annotation_results[0]\u00a0 \u00a0 for text_annotation in annotation_result.text_annotations:\u00a0 \u00a0 \u00a0 \u00a0 print(\"\\nText: {}\".format(text_annotation.text))\u00a0 \u00a0 \u00a0 \u00a0 # Get the first text segment\u00a0 \u00a0 \u00a0 \u00a0 text_segment = text_annotation.segments[0]\u00a0 \u00a0 \u00a0 \u00a0 start_time = text_segment.segment.start_time_offset\u00a0 \u00a0 \u00a0 \u00a0 end_time = text_segment.segment.end_time_offset\u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"start_time: {}, end_time: {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start_time.seconds + start_time.microseconds * 1e-6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end_time.seconds + end_time.microseconds * 1e-6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 print(\"Confidence: {}\".format(text_segment.confidence))\u00a0 \u00a0 \u00a0 \u00a0 # Show the result for the first frame in this segment.\u00a0 \u00a0 \u00a0 \u00a0 frame = text_segment.frames[0]\u00a0 \u00a0 \u00a0 \u00a0 time_offset = frame.time_offset\u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Time offset for the first frame: {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 time_offset.seconds + time_offset.microseconds * 1e-6\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 print(\"Rotated Bounding Box Vertices:\")\u00a0 \u00a0 \u00a0 \u00a0 for vertex in frame.rotated_bounding_box.vertices:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tVertex.x: {}, Vertex.y: {}\".format(vertex.x, vertex.y))\n```No preface\n **C#** \uff1a\u8acb\u6309\u7167\u201c\u5ba2\u6236\u7aef\u5eab\u201d\u9801\u9762\u4e0a\u7684 [C# \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u9032\u884c\u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [\u9069\u7528\u65bc .NET \u7684 Video Intelligence \u53c3\u8003\u6587\u6a94](https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.VideoIntelligence.V1/index.html) \u3002\n **PHP** \uff1a\u8acb\u6309\u7167\u5ba2\u6236\u7aef\u5eab\u9801\u9762\u4e0a\u7684 [PHP \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [PHP \u7248 Video Intelligence \u53c3\u8003\u6587\u6a94](https://cloud.google.com/php/docs/reference/cloud-videointelligence/latest?hl=zh-cn) \u3002\n **Ruby** \uff1a\u8acb\u6309\u7167\u5ba2\u6236\u7aef\u5eab\u9801\u9762\u4e0a\u7684 [Ruby \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [Ruby \u7248 Video Intelligence \u53c3\u8003\u6587\u6a94](https://googleapis.dev/ruby/google-cloud-video_intelligence/latest/Google/Cloud/VideoIntelligence/V1.html) \u3002", "guide": "Cloud Video Intelligence API"}