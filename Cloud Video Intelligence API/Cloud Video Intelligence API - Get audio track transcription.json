{"title": "Cloud Video Intelligence API - Get audio track transcription", "url": "https://cloud.google.com/video-intelligence/docs/transcription", "abstract": "# Cloud Video Intelligence API - Get audio track transcription\nThe Video Intelligence API transcribes speech to text from [supported videofiles](/video-intelligence/docs/supported-formats) . There are two supported models, \"default\" and \"video.\"\n", "content": "## Request Speech Transcription for a Video\n### Send the process requestThe following shows how to send a `POST` request to the [videos:annotate](/video-intelligence/docs/reference/rest/v1/videos/annotate) method. The example uses the access token for a service account set up for the project using the Google Cloud CLI. For instructions on installing the Google Cloud CLI, setting up a project with a service account, and obtaining an access token, see the [Video Intelligence quickstart](/video-intelligence/docs/quickstarts) .\nBefore using any of the request data, make the following replacements:- : a Cloud Storage bucket that contains  the file you want to annotate, including the file name. Must  start with`gs://`.For example:`\"inputUri\": \"gs://cloud-videointelligence-demo/assistant.mp4\",`\n- : [Optional] See [supported languages](/speech-to-text/docs/speech-to-text-supported-languages) \n- : The numeric identifier for your Google Cloud project\nHTTP method and URL:\n```\nPOST https://videointelligence.googleapis.com/v1/videos:annotate\n```\nRequest JSON body:\n```\n{\n\"inputUri\": \"INPUT_URI\",\n \"features\": [\"SPEECH_TRANSCRIPTION\"],\n \"videoContext\": {\n \"speechTranscriptionConfig\": {\n  \"languageCode\": \"LANGUAGE_CODE\",\n  \"enableAutomaticPunctuation\": true,\n  \"filterProfanity\": true\n }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/operations/OPERATION_ID\"\n}\n```\nIf the request is successful, Video Intelligence returns the `name` for your operation. The above shows an example of such a response, where `project-number` is the number of your project and `operation-id` is the ID of the long-running operation created for the request.\n### Get the resultsTo get the results of your request, you must send a `GET` , using the operation name returned from the call to `videos:annotate` , as shown in the following example.\nBefore using any of the request data, make the following replacements:- : the name of the operation as returned by Video Intelligence API. The operation name has the format`projects/` `` `/locations/` `` `/operations/` ``\n- Note: The **done** field is only returned when its value is **True** . It's not included in responses for which the operation has not completed.\n- : The numeric identifier for your Google Cloud project\nHTTP method and URL:\n```\nGET https://videointelligence.googleapis.com/v1/OPERATION_NAME\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:## Download annotation resultsCopy the annotation from the source to the destination bucket: (see [Copy files and objects](https://cloud.google.com/storage/docs/gsutil/commands/cp) )\n`gsutil cp` `` `gs://my-bucket`\nNote: If the output gcs uri is provided by the user, then the annotation is stored in that gcs uri.To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/videointelligence/video_analyze/video_analyze_gcs.go) \n```\nfunc speechTranscriptionURI(w io.Writer, file string) error {\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 client, err := video.NewClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 op, err := client.AnnotateVideo(ctx, &videopb.AnnotateVideoRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Features: []videopb.Feature{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 videopb.Feature_SPEECH_TRANSCRIPTION,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 VideoContext: &videopb.VideoContext{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 SpeechTranscriptionConfig: &videopb.SpeechTranscriptionConfig{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 LanguageCode: \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"en-US\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 EnableAutomaticPunctuation: true,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputUri: file,\u00a0 \u00a0 \u00a0 \u00a0 })\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 resp, err := op.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return err\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // A single video was processed. Get the first result.\u00a0 \u00a0 \u00a0 \u00a0 result := resp.AnnotationResults[0]\u00a0 \u00a0 \u00a0 \u00a0 for _, transcription := range result.SpeechTranscriptions {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // The number of alternatives for each transcription is limited by\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // SpeechTranscriptionConfig.MaxAlternatives.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Each alternative is a different possible transcription\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // and has its own confidence score.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, alternative := range transcription.GetAlternatives() {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Alternative level information:\\n\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tTranscript: %v\\n\", alternative.GetTranscript())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tConfidence: %v\\n\", alternative.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Word level information:\\n\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, wordInfo := range alternative.GetWords() {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 startTime := wordInfo.GetStartTime()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 endTime := wordInfo.GetEndTime()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t%4.1f - %4.1f: %v (speaker %v)\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 float64(startTime.GetSeconds())+float64(startTime.GetNanos())*1e-9, // start as seconds\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 float64(endTime.GetSeconds())+float64(endTime.GetNanos())*1e-9, \u00a0 \u00a0 // end as seconds\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 wordInfo.GetWord(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 wordInfo.GetSpeakerTag())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/video/src/main/java/video/Detect.java) \n```\n// Instantiate a com.google.cloud.videointelligence.v1.VideoIntelligenceServiceClienttry (VideoIntelligenceServiceClient client = VideoIntelligenceServiceClient.create()) {\u00a0 // Set the language code\u00a0 SpeechTranscriptionConfig config =\u00a0 \u00a0 \u00a0 SpeechTranscriptionConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setLanguageCode(\"en-US\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setEnableAutomaticPunctuation(true)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 // Set the video context with the above configuration\u00a0 VideoContext context = VideoContext.newBuilder().setSpeechTranscriptionConfig(config).build();\u00a0 // Create the request\u00a0 AnnotateVideoRequest request =\u00a0 \u00a0 \u00a0 AnnotateVideoRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputUri(gcsUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addFeatures(Feature.SPEECH_TRANSCRIPTION)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setVideoContext(context)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 // asynchronously perform speech transcription on videos\u00a0 OperationFuture<AnnotateVideoResponse, AnnotateVideoProgress> response =\u00a0 \u00a0 \u00a0 client.annotateVideoAsync(request);\u00a0 System.out.println(\"Waiting for operation to complete...\");\u00a0 // Display the results\u00a0 for (VideoAnnotationResults results :\u00a0 \u00a0 \u00a0 response.get(600, TimeUnit.SECONDS).getAnnotationResultsList()) {\u00a0 \u00a0 for (SpeechTranscription speechTranscription : results.getSpeechTranscriptionsList()) {\u00a0 \u00a0 \u00a0 try {\u00a0 \u00a0 \u00a0 \u00a0 // Print the transcription\u00a0 \u00a0 \u00a0 \u00a0 if (speechTranscription.getAlternativesCount() > 0) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 SpeechRecognitionAlternative alternative = speechTranscription.getAlternatives(0);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Transcript: %s\\n\", alternative.getTranscript());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Confidence: %.2f\\n\", alternative.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"Word level information:\");\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (WordInfo wordInfo : alternative.getWordsList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 double startTime =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 wordInfo.getStartTime().getSeconds() + wordInfo.getStartTime().getNanos() / 1e9;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 double endTime =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 wordInfo.getEndTime().getSeconds() + wordInfo.getEndTime().getNanos() / 1e9;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t%4.2fs - %4.2fs: %s\\n\", startTime, endTime, wordInfo.getWord());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 } else {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"No transcription found\");\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 } catch (IndexOutOfBoundsException ioe) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.println(\"Could not retrieve frame: \" + ioe.getMessage());\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/video-intelligence/analyze.js) \n```\n// Imports the Google Cloud Video Intelligence libraryconst videoIntelligence = require('@google-cloud/video-intelligence');// Creates a clientconst client = new videoIntelligence.VideoIntelligenceServiceClient();/**\u00a0* TODO(developer): Uncomment the following line before running the sample.\u00a0*/// const gcsUri = 'GCS URI of video to analyze, e.g. gs://my-bucket/my-video.mp4';async function analyzeVideoTranscript() {\u00a0 const videoContext = {\u00a0 \u00a0 speechTranscriptionConfig: {\u00a0 \u00a0 \u00a0 languageCode: 'en-US',\u00a0 \u00a0 \u00a0 enableAutomaticPunctuation: true,\u00a0 \u00a0 },\u00a0 };\u00a0 const request = {\u00a0 \u00a0 inputUri: gcsUri,\u00a0 \u00a0 features: ['SPEECH_TRANSCRIPTION'],\u00a0 \u00a0 videoContext: videoContext,\u00a0 };\u00a0 const [operation] = await client.annotateVideo(request);\u00a0 console.log('Waiting for operation to complete...');\u00a0 const [operationResult] = await operation.promise();\u00a0 // There is only one annotation_result since only\u00a0 // one video is processed.\u00a0 const annotationResults = operationResult.annotationResults[0];\u00a0 for (const speechTranscription of annotationResults.speechTranscriptions) {\u00a0 \u00a0 // The number of alternatives for each transcription is limited by\u00a0 \u00a0 // SpeechTranscriptionConfig.max_alternatives.\u00a0 \u00a0 // Each alternative is a different possible transcription\u00a0 \u00a0 // and has its own confidence score.\u00a0 \u00a0 for (const alternative of speechTranscription.alternatives) {\u00a0 \u00a0 \u00a0 console.log('Alternative level information:');\u00a0 \u00a0 \u00a0 console.log(`Transcript: ${alternative.transcript}`);\u00a0 \u00a0 \u00a0 console.log(`Confidence: ${alternative.confidence}`);\u00a0 \u00a0 \u00a0 console.log('Word level information:');\u00a0 \u00a0 \u00a0 for (const wordInfo of alternative.words) {\u00a0 \u00a0 \u00a0 \u00a0 const word = wordInfo.word;\u00a0 \u00a0 \u00a0 \u00a0 const start_time =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 wordInfo.startTime.seconds + wordInfo.startTime.nanos * 1e-9;\u00a0 \u00a0 \u00a0 \u00a0 const end_time =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 wordInfo.endTime.seconds + wordInfo.endTime.nanos * 1e-9;\u00a0 \u00a0 \u00a0 \u00a0 console.log('\\t' + start_time + 's - ' + end_time + 's: ' + word);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}analyzeVideoTranscript();\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/analyze.py) \n```\n\"\"\"Transcribe speech from a video stored on GCS.\"\"\"from google.cloud import videointelligencevideo_client = videointelligence.VideoIntelligenceServiceClient()features = [videointelligence.Feature.SPEECH_TRANSCRIPTION]config = videointelligence.SpeechTranscriptionConfig(\u00a0 \u00a0 language_code=\"en-US\", enable_automatic_punctuation=True)video_context = videointelligence.VideoContext(speech_transcription_config=config)operation = video_client.annotate_video(\u00a0 \u00a0 request={\u00a0 \u00a0 \u00a0 \u00a0 \"features\": features,\u00a0 \u00a0 \u00a0 \u00a0 \"input_uri\": path,\u00a0 \u00a0 \u00a0 \u00a0 \"video_context\": video_context,\u00a0 \u00a0 })print(\"\\nProcessing video for speech transcription.\")result = operation.result(timeout=600)# There is only one annotation_result since only# one video is processed.annotation_results = result.annotation_results[0]for speech_transcription in annotation_results.speech_transcriptions:\u00a0 \u00a0 # The number of alternatives for each transcription is limited by\u00a0 \u00a0 # SpeechTranscriptionConfig.max_alternatives.\u00a0 \u00a0 # Each alternative is a different possible transcription\u00a0 \u00a0 # and has its own confidence score.\u00a0 \u00a0 for alternative in speech_transcription.alternatives:\u00a0 \u00a0 \u00a0 \u00a0 print(\"Alternative level information:\")\u00a0 \u00a0 \u00a0 \u00a0 print(\"Transcript: {}\".format(alternative.transcript))\u00a0 \u00a0 \u00a0 \u00a0 print(\"Confidence: {}\\n\".format(alternative.confidence))\u00a0 \u00a0 \u00a0 \u00a0 print(\"Word level information:\")\u00a0 \u00a0 \u00a0 \u00a0 for word_info in alternative.words:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 word = word_info.word\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start_time = word_info.start_time\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end_time = word_info.end_time\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t{}s - {}s: {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start_time.seconds + start_time.microseconds * 1e-6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end_time.seconds + end_time.microseconds * 1e-6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 word,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\n```No preface\n **C#** : Please follow the [C# setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for .NET.](https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.VideoIntelligence.V1/index.html) \n **PHP** : Please follow the [PHP setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for PHP.](/php/docs/reference/cloud-videointelligence/latest) \n **Ruby** : Please follow the [Ruby setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for Ruby.](https://googleapis.dev/ruby/google-cloud-video_intelligence/latest/Google/Cloud/VideoIntelligence/V1.html)", "guide": "Cloud Video Intelligence API"}