{"title": "Cloud Video Intelligence API - Recognize logos", "url": "https://cloud.google.com/video-intelligence/docs/logo-recognition", "abstract": "# Cloud Video Intelligence API - Recognize logos\nThe Video Intelligence API can detect, track, and recognize the presence of over 100,000 brands and logos in video content.\nThis page describes how to recognize a logo in a video using the Video Intelligence API.\n", "content": "## Annotate a video in Cloud Storage\nThe following code sample demonstrates how to detect logos in a video in Cloud Storage.\n### Send the process requestTo perform annotation on a local video file, base64-encode the contents of the video file. Include the base64-encoded contents in the `inputContent` field of the request. For information on how to base64-encode the contents of a video file, see [Base64 Encoding](/video-intelligence/docs/base64) .\nThe following shows how to send a `POST` request to the [videos:annotate](/video-intelligence/docs/reference/rest/v1/videos/annotate) method. The example uses the access token for a service account set up for the project using the Google Cloud CLI. For instructions on installing the Google Cloud CLI, setting up a project with a service account, and obtaining an access token, see the [Video Intelligence quickstart](/video-intelligence/docs/quickstarts) .\nBefore using any of the request data, make the following replacements:- : a Cloud Storage bucket that contains  the file you want to annotate, including the file name. Must  start with`gs://`.For example:`\"inputUri\": \"gs://cloud-videointelligence-demo/assistant.mp4\",`\n- : The numeric identifier for your Google Cloud project\nHTTP method and URL:\n```\nPOST https://videointelligence.googleapis.com/v1/videos:annotate\n```\nRequest JSON body:\n```\n{\n \"inputUri\":\"INPUT_URI\",\n \"features\": [\"LOGO_RECOGNITION\"]\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/operations/OPERATION_ID\"\n}\n```\nIf the response is successful, the Video Intelligence API returns the `name` for your operation. The above shows an example of such a response, where: `project-number` is the number of your project and `operation-id` is the ID of the long running operation created for the request.\n- : the number of your project\n- : the Cloud region where annotation should take  place. Supported cloud regions are:`us-east1`,`us-west1`,`europe-west1`,`asia-east1`. If no region is  specified, a region will be determined based on video file location.\n- : the ID of the long running operation created  for the request and provided in the response when you started the  operation, for example`12345...`\n### Get the resultsTo get the results of your request, you send a `GET` request, using the operation name returned from the call to `videos:annotate` , as shown in the following example.\nBefore using any of the request data, make the following replacements:- : the name of the operation as returned by Video Intelligence API. The operation name has the format`projects/` `` `/locations/` `` `/operations/` ``\n- Note: The **done** field is returned only when its value is **True** . It's not included in responses for which the operation has not completed.\n- : The numeric identifier for your Google Cloud project\nHTTP method and URL:\n```\nGET https://videointelligence.googleapis.com/v1/OPERATION_NAME\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:## Download annotation resultsCopy the annotation from the source to the destination bucket: (see [Copy files and objects](https://cloud.google.com/storage/docs/gsutil/commands/cp) )\n`gsutil cp` `` `gs://my-bucket`\nNote: If the output gcs uri is provided by the user, then the annotation is stored in that gcs uri.To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/videointelligence/annotate/logo_detection_gcs.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 \"time\"\u00a0 \u00a0 \u00a0 \u00a0 video \"cloud.google.com/go/videointelligence/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 videopb \"cloud.google.com/go/videointelligence/apiv1/videointelligencepb\"\u00a0 \u00a0 \u00a0 \u00a0 \"github.com/golang/protobuf/ptypes\")// logoDetectionGCS analyzes a video and extracts logos with their bounding boxes.func logoDetectionGCS(w io.Writer, gcsURI string) error {\u00a0 \u00a0 \u00a0 \u00a0 // gcsURI := \"gs://cloud-samples-data/video/googlework_tiny.mp4\"\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // Creates a client.\u00a0 \u00a0 \u00a0 \u00a0 client, err := video.NewClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"video.NewClient: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 ctx, cancel := context.WithTimeout(ctx, time.Second*180)\u00a0 \u00a0 \u00a0 \u00a0 defer cancel()\u00a0 \u00a0 \u00a0 \u00a0 op, err := client.AnnotateVideo(ctx, &videopb.AnnotateVideoRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputUri: gcsURI,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Features: []videopb.Feature{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 videopb.Feature_LOGO_RECOGNITION,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 })\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"AnnotateVideo: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 resp, err := op.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"Wait: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Only one video was processed, so get the first result.\u00a0 \u00a0 \u00a0 \u00a0 result := resp.GetAnnotationResults()[0]\u00a0 \u00a0 \u00a0 \u00a0 // Annotations for list of logos detected, tracked and recognized in video.\u00a0 \u00a0 \u00a0 \u00a0 for _, annotation := range result.LogoRecognitionAnnotations {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Description: %q\\n\", annotation.Entity.GetDescription())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Opaque entity ID. Some IDs may be available in Google Knowledge\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Graph Search API (https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if len(annotation.Entity.EntityId) > 0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tEntity ID: %q\\n\", annotation.Entity.GetEntityId())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // All logo tracks where the recognized logo appears. Each track\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // corresponds to one logo instance appearing in consecutive frames.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, track := range annotation.Tracks {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Video segment of a track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment := track.GetSegment()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start, _ := ptypes.Duration(segment.GetStartTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end, _ := ptypes.Duration(segment.GetEndTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tSegment: %v to %v\\n\", start, end)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tConfidence: %f\\n\", track.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, timestampedObject := range track.TimestampedObjects {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Normalized Bounding box in a frame, where the object is\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // located.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 box := timestampedObject.GetNormalizedBoundingBox()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tBounding box position:\\n\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tleft \u00a0: %f\\n\", box.GetLeft())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\ttop \u00a0 : %f\\n\", box.GetTop())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tright : %f\\n\", box.GetRight())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tbottom: %f\\n\", box.GetBottom())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, attribute := range timestampedObject.Attributes {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\t\\tName: %q\\n\", attribute.GetName())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\t\\tConfidence: %f\\n\", attribute.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\t\\tValue: %q\\n\", attribute.GetValue())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, trackAttribute := range track.Attributes {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tName: %q\\n\", trackAttribute.GetName())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tConfidence: %f\\n\", trackAttribute.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tValue: %q\\n\", trackAttribute.GetValue())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // All video segments where the recognized logo appears. There might be\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // multiple instances of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, segment := range annotation.Segments {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start, _ := ptypes.Duration(segment.GetStartTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end, _ := ptypes.Duration(segment.GetEndTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tSegment: %v to %v\\n\", start, end)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/video/src/main/java/video/LogoDetectionGcs.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.videointelligence.v1.AnnotateVideoProgress;import com.google.cloud.videointelligence.v1.AnnotateVideoRequest;import com.google.cloud.videointelligence.v1.AnnotateVideoResponse;import com.google.cloud.videointelligence.v1.DetectedAttribute;import com.google.cloud.videointelligence.v1.Entity;import com.google.cloud.videointelligence.v1.Feature;import com.google.cloud.videointelligence.v1.LogoRecognitionAnnotation;import com.google.cloud.videointelligence.v1.NormalizedBoundingBox;import com.google.cloud.videointelligence.v1.TimestampedObject;import com.google.cloud.videointelligence.v1.Track;import com.google.cloud.videointelligence.v1.VideoAnnotationResults;import com.google.cloud.videointelligence.v1.VideoIntelligenceServiceClient;import com.google.cloud.videointelligence.v1.VideoSegment;import com.google.protobuf.Duration;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class LogoDetectionGcs {\u00a0 public static void detectLogoGcs() throws Exception {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String gcsUri = \"gs://YOUR_BUCKET_ID/path/to/your/video.mp4\";\u00a0 \u00a0 detectLogoGcs(gcsUri);\u00a0 }\u00a0 public static void detectLogoGcs(String inputUri)\u00a0 \u00a0 \u00a0 throws IOException, ExecutionException, InterruptedException, TimeoutException {\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (VideoIntelligenceServiceClient client = VideoIntelligenceServiceClient.create()) {\u00a0 \u00a0 \u00a0 // Create the request\u00a0 \u00a0 \u00a0 AnnotateVideoRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AnnotateVideoRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputUri(inputUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addFeatures(Feature.LOGO_RECOGNITION)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // asynchronously perform object tracking on videos\u00a0 \u00a0 \u00a0 OperationFuture<AnnotateVideoResponse, AnnotateVideoProgress> future =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 client.annotateVideoAsync(request);\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to complete...\");\u00a0 \u00a0 \u00a0 // The first result is retrieved because a single video was processed.\u00a0 \u00a0 \u00a0 AnnotateVideoResponse response = future.get(600, TimeUnit.SECONDS);\u00a0 \u00a0 \u00a0 VideoAnnotationResults annotationResult = response.getAnnotationResults(0);\u00a0 \u00a0 \u00a0 // Annotations for list of logos detected, tracked and recognized in video.\u00a0 \u00a0 \u00a0 for (LogoRecognitionAnnotation logoRecognitionAnnotation :\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 annotationResult.getLogoRecognitionAnnotationsList()) {\u00a0 \u00a0 \u00a0 \u00a0 Entity entity = logoRecognitionAnnotation.getEntity();\u00a0 \u00a0 \u00a0 \u00a0 // Opaque entity ID. Some IDs may be available in\u00a0 \u00a0 \u00a0 \u00a0 // [Google Knowledge Graph Search API](https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Entity Id : %s\\n\", entity.getEntityId());\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Description : %s\\n\", entity.getDescription());\u00a0 \u00a0 \u00a0 \u00a0 // All logo tracks where the recognized logo appears. Each track corresponds to one logo\u00a0 \u00a0 \u00a0 \u00a0 // instance appearing in consecutive frames.\u00a0 \u00a0 \u00a0 \u00a0 for (Track track : logoRecognitionAnnotation.getTracksList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Video segment of a track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Duration startTimeOffset = track.getSegment().getStartTimeOffset();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset: %s.%s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 startTimeOffset.getSeconds(), startTimeOffset.getNanos());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Duration endTimeOffset = track.getSegment().getEndTimeOffset();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset: %s.%s\\n\", endTimeOffset.getSeconds(), endTimeOffset.getNanos());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\tConfidence: %s\\n\", track.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (TimestampedObject timestampedObject : track.getTimestampedObjectsList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Normalized Bounding box in a frame, where the object is located.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 NormalizedBoundingBox normalizedBoundingBox =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timestampedObject.getNormalizedBoundingBox();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\n\\t\\tLeft: %s\\n\", normalizedBoundingBox.getLeft());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tTop: %s\\n\", normalizedBoundingBox.getTop());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tRight: %s\\n\", normalizedBoundingBox.getRight());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tBottom: %s\\n\", normalizedBoundingBox.getBottom());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (DetectedAttribute attribute : timestampedObject.getAttributesList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\n\\t\\t\\tName: %s\\n\", attribute.getName());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\t\\tConfidence: %s\\n\", attribute.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\t\\tValue: %s\\n\", attribute.getValue());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (DetectedAttribute trackAttribute : track.getAttributesList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\n\\t\\tName : %s\\n\", trackAttribute.getName());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tConfidence : %s\\n\", trackAttribute.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tValue : %s\\n\", trackAttribute.getValue());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // All video segments where the recognized logo appears. There might be multiple instances\u00a0 \u00a0 \u00a0 \u00a0 // of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 \u00a0 \u00a0 for (VideoSegment segment : logoRecognitionAnnotation.getSegmentsList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset : %s.%s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getStartTimeOffset().getSeconds(), segment.getStartTimeOffset().getNanos());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset : %s.%s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getEndTimeOffset().getSeconds(), segment.getEndTimeOffset().getNanos());\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/video-intelligence/detect_logo_gcs.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*/// const inputUri = 'gs://cloud-samples-data/video/googlework_short.mp4';// Imports the Google Cloud client librariesconst Video = require('@google-cloud/video-intelligence');// Instantiates a clientconst client = new Video.VideoIntelligenceServiceClient();// Performs asynchronous video annotation for logo recognition on a file hosted in GCS.async function detectLogoGcs() {\u00a0 // Build the request with the input uri and logo recognition feature.\u00a0 const request = {\u00a0 \u00a0 inputUri: inputUri,\u00a0 \u00a0 features: ['LOGO_RECOGNITION'],\u00a0 };\u00a0 // Make the asynchronous request\u00a0 const [operation] = await client.annotateVideo(request);\u00a0 // Wait for the results\u00a0 const [response] = await operation.promise();\u00a0 // Get the first response, since we sent only one video.\u00a0 const annotationResult = response.annotationResults[0];\u00a0 for (const logoRecognitionAnnotation of annotationResult.logoRecognitionAnnotations) {\u00a0 \u00a0 const entity = logoRecognitionAnnotation.entity;\u00a0 \u00a0 // Opaque entity ID. Some IDs may be available in\u00a0 \u00a0 // [Google Knowledge Graph Search API](https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 console.log(`Entity Id: ${entity.entityId}`);\u00a0 \u00a0 console.log(`Description: ${entity.description}`);\u00a0 \u00a0 // All logo tracks where the recognized logo appears.\u00a0 \u00a0 // Each track corresponds to one logo instance appearing in consecutive frames.\u00a0 \u00a0 for (const track of logoRecognitionAnnotation.tracks) {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\n\\tStart Time Offset: ${track.segment.startTimeOffset.seconds}.${track.segment.startTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tEnd Time Offset: ${track.segment.endTimeOffset.seconds}.${track.segment.endTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(`\\tConfidence: ${track.confidence}`);\u00a0 \u00a0 \u00a0 // The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 for (const timestampedObject of track.timestampedObjects) {\u00a0 \u00a0 \u00a0 \u00a0 // Normalized Bounding box in a frame, where the object is located.\u00a0 \u00a0 \u00a0 \u00a0 const normalizedBoundingBox = timestampedObject.normalizedBoundingBox;\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\n\\t\\tLeft: ${normalizedBoundingBox.left}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tTop: ${normalizedBoundingBox.top}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tRight: ${normalizedBoundingBox.right}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tBottom: ${normalizedBoundingBox.bottom}`);\u00a0 \u00a0 \u00a0 \u00a0 // Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 for (const attribute of timestampedObject.attributes) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\n\\t\\t\\tName: ${attribute.name}`);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\t\\tConfidence: ${attribute.confidence}`);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\t\\tValue: ${attribute.value}`);\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 // Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 for (const trackAttribute of track.attributes) {\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\n\\t\\tName: ${trackAttribute.name}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tConfidence: ${trackAttribute.confidence}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tValue: ${trackAttribute.value}`);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 \u00a0 // All video segments where the recognized logo appears.\u00a0 \u00a0 // There might be multiple instances of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 for (const segment of logoRecognitionAnnotation.segments) {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\n\\tStart Time Offset: ${segment.startTimeOffset.seconds}.${segment.startTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tEnd Time Offset: ${segment.endTimeOffset.seconds}.${segment.endTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 }\u00a0 }}detectLogoGcs();\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/video_detect_logo_gcs.py) \n```\nfrom google.cloud import videointelligencedef detect_logo_gcs(input_uri=\"gs://YOUR_BUCKET_ID/path/to/your/file.mp4\"):\u00a0 \u00a0 client = videointelligence.VideoIntelligenceServiceClient()\u00a0 \u00a0 features = [videointelligence.Feature.LOGO_RECOGNITION]\u00a0 \u00a0 operation = client.annotate_video(\u00a0 \u00a0 \u00a0 \u00a0 request={\"features\": features, \"input_uri\": input_uri}\u00a0 \u00a0 )\u00a0 \u00a0 print(\"Waiting for operation to complete...\")\u00a0 \u00a0 response = operation.result()\u00a0 \u00a0 # Get the first response, since we sent only one video.\u00a0 \u00a0 annotation_result = response.annotation_results[0]\u00a0 \u00a0 # Annotations for list of logos detected, tracked and recognized in video.\u00a0 \u00a0 for logo_recognition_annotation in annotation_result.logo_recognition_annotations:\u00a0 \u00a0 \u00a0 \u00a0 entity = logo_recognition_annotation.entity\u00a0 \u00a0 \u00a0 \u00a0 # Opaque entity ID. Some IDs may be available in [Google Knowledge Graph\u00a0 \u00a0 \u00a0 \u00a0 # Search API](https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 \u00a0 \u00a0 print(\"Entity Id : {}\".format(entity.entity_id))\u00a0 \u00a0 \u00a0 \u00a0 print(\"Description : {}\".format(entity.description))\u00a0 \u00a0 \u00a0 \u00a0 # All logo tracks where the recognized logo appears. Each track corresponds\u00a0 \u00a0 \u00a0 \u00a0 # to one logo instance appearing in consecutive frames.\u00a0 \u00a0 \u00a0 \u00a0 for track in logo_recognition_annotation.tracks:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Video segment of a track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.start_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.start_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.end_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.end_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tConfidence : {}\".format(track.confidence))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for timestamped_object in track.timestamped_objects:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Normalized Bounding box in a frame, where the object is located.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 normalized_bounding_box = timestamped_object.normalized_bounding_box\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\n\\t\\tLeft : {}\".format(normalized_bounding_box.left))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tTop : {}\".format(normalized_bounding_box.top))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tRight : {}\".format(normalized_bounding_box.right))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tBottom : {}\".format(normalized_bounding_box.bottom))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for attribute in timestamped_object.attributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\n\\t\\t\\tName : {}\".format(attribute.name))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\t\\tConfidence : {}\".format(attribute.confidence))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\t\\tValue : {}\".format(attribute.value))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for track_attribute in track.attributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\n\\t\\tName : {}\".format(track_attribute.name))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tConfidence : {}\".format(track_attribute.confidence))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tValue : {}\".format(track_attribute.value))\u00a0 \u00a0 \u00a0 \u00a0 # All video segments where the recognized logo appears. There might be\u00a0 \u00a0 \u00a0 \u00a0 # multiple instances of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 \u00a0 \u00a0 for segment in logo_recognition_annotation.segments:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.start_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.start_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.end_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.end_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\n```No preface\n **C#** : Please follow the [C# setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for .NET.](https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.VideoIntelligence.V1/index.html) \n **PHP** : Please follow the [PHP setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for PHP.](/php/docs/reference/cloud-videointelligence/latest) \n **Ruby** : Please follow the [Ruby setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for Ruby.](https://googleapis.dev/ruby/google-cloud-video_intelligence/latest/Google/Cloud/VideoIntelligence/V1.html)\n## Annotate a local video\nThe following code sample demonstrates how to detect logos in a local video file.\n### Send video annotation requestTo perform annotation on a local video file, be sure to base64-encode the contents of the video file. Include the base64-encoded contents in the `inputContent` field of the request. For information on how to base64-encode the contents of a video file, see [Base64 Encoding](/video-intelligence/docs/base64) .\nThe following shows how to send a POST request to the `videos:annotate` method. The example uses the access token for a service account set up for the project using the Google Cloud CLI. For instructions on installing the Google Cloud CLI, setting up a project with a service account, and obtaining an access token, see the [Video Intelligence API Quickstart](/video-intelligence/docs/quickstarts) \nBefore using any of the request data, make the following replacements:- \"inputContent\":For example:`\"UklGRg41AwBBVkkgTElTVAwBAABoZHJsYXZpaDgAAAA1ggAAxPMBAAAAAAAQCAA...\"`\n- : [Optional] See [supported languages](/speech-to-text/docs/speech-to-text-supported-languages) \n- : The numeric identifier for your Google Cloud project\nHTTP method and URL:\n```\nPOST https://videointelligence.googleapis.com/v1/videos:annotate\n```\nRequest JSON body:\n```\n{\n \"inputContent\": \"BASE64_ENCODED_CONTENT\",\n \"features\": [\"LOGO_RECOGNITION\"],\n \"videoContext\": {\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/operations/OPERATION_ID\"\n}\n```\nIf the response is successful, the Video Intelligence API returns the `name` for your operation. The above shows an example of such a response, where `project-number` is the name of your project and `operation-id` is the ID of the long running operation created for the request.\n- : provided in the response when you started the  operation, for example`12345...`\n### Get annotation resultsTo retrieve the result of the operation, make a [GET](/video-intelligence/docs/reference/rest/v1/projects.locations.operations/get) request, using the operation name returned from the call to [videos:annotate](/video-intelligence/docs/reference/rest/v1/videos/annotate) , as shown in the following example.\nBefore using any of the request data, make the following replacements:- : The numeric identifier for your Google Cloud project\nHTTP method and URL:\n```\nGET https://videointelligence.googleapis.com/v1/OPERATION_NAME\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:Text detection annotations are returned as a `textAnnotations` list. Note: The **done** field is returned only when its value is **True** . It's not included in responses for which the operation has not completed.\nTo authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/videointelligence/annotate/logo_detection.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 \"io/ioutil\"\u00a0 \u00a0 \u00a0 \u00a0 \"time\"\u00a0 \u00a0 \u00a0 \u00a0 video \"cloud.google.com/go/videointelligence/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 videopb \"cloud.google.com/go/videointelligence/apiv1/videointelligencepb\"\u00a0 \u00a0 \u00a0 \u00a0 \"github.com/golang/protobuf/ptypes\")// logoDetection analyzes a video and extracts logos with their bounding boxes.func logoDetection(w io.Writer, filename string) error {\u00a0 \u00a0 \u00a0 \u00a0 // filename := \"../testdata/googlework_short.mp4\"\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // Creates a client.\u00a0 \u00a0 \u00a0 \u00a0 client, err := video.NewClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"video.NewClient: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 ctx, cancel := context.WithTimeout(ctx, time.Second*180)\u00a0 \u00a0 \u00a0 \u00a0 defer cancel()\u00a0 \u00a0 \u00a0 \u00a0 fileBytes, err := ioutil.ReadFile(filename)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"ioutil.ReadFile: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 op, err := client.AnnotateVideo(ctx, &videopb.AnnotateVideoRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputContent: fileBytes,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Features: []videopb.Feature{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 videopb.Feature_LOGO_RECOGNITION,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 })\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"AnnotateVideo: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 resp, err := op.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"Wait: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Only one video was processed, so get the first result.\u00a0 \u00a0 \u00a0 \u00a0 result := resp.GetAnnotationResults()[0]\u00a0 \u00a0 \u00a0 \u00a0 // Annotations for list of logos detected, tracked and recognized in video.\u00a0 \u00a0 \u00a0 \u00a0 for _, annotation := range result.LogoRecognitionAnnotations {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Description: %q\\n\", annotation.Entity.GetDescription())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Opaque entity ID. Some IDs may be available in Google Knowledge\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Graph Search API (https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if len(annotation.Entity.EntityId) > 0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tEntity ID: %q\\n\", annotation.Entity.GetEntityId())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // All logo tracks where the recognized logo appears. Each track\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // corresponds to one logo instance appearing in consecutive frames.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, track := range annotation.Tracks {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Video segment of a track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment := track.GetSegment()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start, _ := ptypes.Duration(segment.GetStartTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end, _ := ptypes.Duration(segment.GetEndTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tSegment: %v to %v\\n\", start, end)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tConfidence: %f\\n\", track.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, timestampedObject := range track.TimestampedObjects {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Normalized Bounding box in a frame, where the object is\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // located.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 box := timestampedObject.GetNormalizedBoundingBox()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tBounding box position:\\n\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tleft \u00a0: %f\\n\", box.GetLeft())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\ttop \u00a0 : %f\\n\", box.GetTop())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tright : %f\\n\", box.GetRight())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tbottom: %f\\n\", box.GetBottom())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, attribute := range timestampedObject.Attributes {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\t\\tName: %q\\n\", attribute.GetName())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\t\\tConfidence: %f\\n\", attribute.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\t\\tValue: %q\\n\", attribute.GetValue())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, trackAttribute := range track.Attributes {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tName: %q\\n\", trackAttribute.GetName())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tConfidence: %f\\n\", trackAttribute.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tValue: %q\\n\", trackAttribute.GetValue())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // All video segments where the recognized logo appears. There might be\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // multiple instances of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, segment := range annotation.Segments {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start, _ := ptypes.Duration(segment.GetStartTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end, _ := ptypes.Duration(segment.GetEndTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tSegment: %v to %v\\n\", start, end)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/video/src/main/java/video/LogoDetection.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.videointelligence.v1.AnnotateVideoProgress;import com.google.cloud.videointelligence.v1.AnnotateVideoRequest;import com.google.cloud.videointelligence.v1.AnnotateVideoResponse;import com.google.cloud.videointelligence.v1.DetectedAttribute;import com.google.cloud.videointelligence.v1.Entity;import com.google.cloud.videointelligence.v1.Feature;import com.google.cloud.videointelligence.v1.LogoRecognitionAnnotation;import com.google.cloud.videointelligence.v1.NormalizedBoundingBox;import com.google.cloud.videointelligence.v1.TimestampedObject;import com.google.cloud.videointelligence.v1.Track;import com.google.cloud.videointelligence.v1.VideoAnnotationResults;import com.google.cloud.videointelligence.v1.VideoIntelligenceServiceClient;import com.google.cloud.videointelligence.v1.VideoSegment;import com.google.protobuf.ByteString;import com.google.protobuf.Duration;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class LogoDetection {\u00a0 public static void detectLogo() throws Exception {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String localFilePath = \"path/to/your/video.mp4\";\u00a0 \u00a0 detectLogo(localFilePath);\u00a0 }\u00a0 public static void detectLogo(String filePath)\u00a0 \u00a0 \u00a0 throws IOException, ExecutionException, InterruptedException, TimeoutException {\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (VideoIntelligenceServiceClient client = VideoIntelligenceServiceClient.create()) {\u00a0 \u00a0 \u00a0 // Read file\u00a0 \u00a0 \u00a0 Path path = Paths.get(filePath);\u00a0 \u00a0 \u00a0 byte[] data = Files.readAllBytes(path);\u00a0 \u00a0 \u00a0 // Create the request\u00a0 \u00a0 \u00a0 AnnotateVideoRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AnnotateVideoRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputContent(ByteString.copyFrom(data))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addFeatures(Feature.LOGO_RECOGNITION)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // asynchronously perform object tracking on videos\u00a0 \u00a0 \u00a0 OperationFuture<AnnotateVideoResponse, AnnotateVideoProgress> future =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 client.annotateVideoAsync(request);\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to complete...\");\u00a0 \u00a0 \u00a0 // The first result is retrieved because a single video was processed.\u00a0 \u00a0 \u00a0 AnnotateVideoResponse response = future.get(300, TimeUnit.SECONDS);\u00a0 \u00a0 \u00a0 VideoAnnotationResults annotationResult = response.getAnnotationResults(0);\u00a0 \u00a0 \u00a0 // Annotations for list of logos detected, tracked and recognized in video.\u00a0 \u00a0 \u00a0 for (LogoRecognitionAnnotation logoRecognitionAnnotation :\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 annotationResult.getLogoRecognitionAnnotationsList()) {\u00a0 \u00a0 \u00a0 \u00a0 Entity entity = logoRecognitionAnnotation.getEntity();\u00a0 \u00a0 \u00a0 \u00a0 // Opaque entity ID. Some IDs may be available in\u00a0 \u00a0 \u00a0 \u00a0 // [Google Knowledge Graph Search API](https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Entity Id : %s\\n\", entity.getEntityId());\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Description : %s\\n\", entity.getDescription());\u00a0 \u00a0 \u00a0 \u00a0 // All logo tracks where the recognized logo appears. Each track corresponds to one logo\u00a0 \u00a0 \u00a0 \u00a0 // instance appearing in consecutive frames.\u00a0 \u00a0 \u00a0 \u00a0 for (Track track : logoRecognitionAnnotation.getTracksList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Video segment of a track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Duration startTimeOffset = track.getSegment().getStartTimeOffset();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset: %s.%s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 startTimeOffset.getSeconds(), startTimeOffset.getNanos());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Duration endTimeOffset = track.getSegment().getEndTimeOffset();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset: %s.%s\\n\", endTimeOffset.getSeconds(), endTimeOffset.getNanos());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\tConfidence: %s\\n\", track.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (TimestampedObject timestampedObject : track.getTimestampedObjectsList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Normalized Bounding box in a frame, where the object is located.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 NormalizedBoundingBox normalizedBoundingBox =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timestampedObject.getNormalizedBoundingBox();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\n\\t\\tLeft: %s\\n\", normalizedBoundingBox.getLeft());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tTop: %s\\n\", normalizedBoundingBox.getTop());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tRight: %s\\n\", normalizedBoundingBox.getRight());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tBottom: %s\\n\", normalizedBoundingBox.getBottom());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (DetectedAttribute attribute : timestampedObject.getAttributesList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\n\\t\\t\\tName: %s\\n\", attribute.getName());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\t\\tConfidence: %s\\n\", attribute.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\t\\tValue: %s\\n\", attribute.getValue());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (DetectedAttribute trackAttribute : track.getAttributesList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\n\\t\\tName : %s\\n\", trackAttribute.getName());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tConfidence : %s\\n\", trackAttribute.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tValue : %s\\n\", trackAttribute.getValue());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // All video segments where the recognized logo appears. There might be multiple instances\u00a0 \u00a0 \u00a0 \u00a0 // of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 \u00a0 \u00a0 for (VideoSegment segment : logoRecognitionAnnotation.getSegmentsList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset : %s.%s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getStartTimeOffset().getSeconds(), segment.getStartTimeOffset().getNanos());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset : %s.%s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getEndTimeOffset().getSeconds(), segment.getEndTimeOffset().getNanos());\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/video-intelligence/detect_logo.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*/// const localFilePath = 'path/to/your/video.mp4'// Imports the Google Cloud client librariesconst Video = require('@google-cloud/video-intelligence');const fs = require('fs');// Instantiates a clientconst client = new Video.VideoIntelligenceServiceClient();// Performs asynchronous video annotation for logo recognition on a file.async function detectLogo() {\u00a0 const inputContent = fs.readFileSync(localFilePath).toString('base64');\u00a0 // Build the request with the input content and logo recognition feature.\u00a0 const request = {\u00a0 \u00a0 inputContent: inputContent,\u00a0 \u00a0 features: ['LOGO_RECOGNITION'],\u00a0 };\u00a0 // Make the asynchronous request\u00a0 const [operation] = await client.annotateVideo(request);\u00a0 // Wait for the results\u00a0 const [response] = await operation.promise();\u00a0 // Get the first response, since we sent only one video.\u00a0 const annotationResult = response.annotationResults[0];\u00a0 for (const logoRecognitionAnnotation of annotationResult.logoRecognitionAnnotations) {\u00a0 \u00a0 const entity = logoRecognitionAnnotation.entity;\u00a0 \u00a0 // Opaque entity ID. Some IDs may be available in\u00a0 \u00a0 // [Google Knowledge Graph Search API](https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 console.log(`Entity Id: ${entity.entityId}`);\u00a0 \u00a0 console.log(`Description: ${entity.description}`);\u00a0 \u00a0 // All logo tracks where the recognized logo appears.\u00a0 \u00a0 // Each track corresponds to one logo instance appearing in consecutive frames.\u00a0 \u00a0 for (const track of logoRecognitionAnnotation.tracks) {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\n\\tStart Time Offset: ${track.segment.startTimeOffset.seconds}.${track.segment.startTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tEnd Time Offset: ${track.segment.endTimeOffset.seconds}.${track.segment.endTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(`\\tConfidence: ${track.confidence}`);\u00a0 \u00a0 \u00a0 // The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 for (const timestampedObject of track.timestampedObjects) {\u00a0 \u00a0 \u00a0 \u00a0 // Normalized Bounding box in a frame, where the object is located.\u00a0 \u00a0 \u00a0 \u00a0 const normalizedBoundingBox = timestampedObject.normalizedBoundingBox;\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\n\\t\\tLeft: ${normalizedBoundingBox.left}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tTop: ${normalizedBoundingBox.top}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tRight: ${normalizedBoundingBox.right}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tBottom: ${normalizedBoundingBox.bottom}`);\u00a0 \u00a0 \u00a0 \u00a0 // Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 for (const attribute of timestampedObject.attributes) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\n\\t\\t\\tName: ${attribute.name}`);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\t\\tConfidence: ${attribute.confidence}`);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\t\\tValue: ${attribute.value}`);\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 // Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 for (const trackAttribute of track.attributes) {\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\n\\t\\tName: ${trackAttribute.name}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tConfidence: ${trackAttribute.confidence}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tValue: ${trackAttribute.value}`);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 \u00a0 // All video segments where the recognized logo appears.\u00a0 \u00a0 // There might be multiple instances of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 for (const segment of logoRecognitionAnnotation.segments) {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\n\\tStart Time Offset: ${segment.startTimeOffset.seconds}.${segment.startTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tEnd Time Offset: ${segment.endTimeOffset.seconds}.${segment.endTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 }\u00a0 }}detectLogo();\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/video_detect_logo.py) \n```\nimport iofrom google.cloud import videointelligencedef detect_logo(local_file_path=\"path/to/your/video.mp4\"):\u00a0 \u00a0 \"\"\"Performs asynchronous video annotation for logo recognition on a local file.\"\"\"\u00a0 \u00a0 client = videointelligence.VideoIntelligenceServiceClient()\u00a0 \u00a0 with io.open(local_file_path, \"rb\") as f:\u00a0 \u00a0 \u00a0 \u00a0 input_content = f.read()\u00a0 \u00a0 features = [videointelligence.Feature.LOGO_RECOGNITION]\u00a0 \u00a0 operation = client.annotate_video(\u00a0 \u00a0 \u00a0 \u00a0 request={\"features\": features, \"input_content\": input_content}\u00a0 \u00a0 )\u00a0 \u00a0 print(\"Waiting for operation to complete...\")\u00a0 \u00a0 response = operation.result()\u00a0 \u00a0 # Get the first response, since we sent only one video.\u00a0 \u00a0 annotation_result = response.annotation_results[0]\u00a0 \u00a0 # Annotations for list of logos detected, tracked and recognized in video.\u00a0 \u00a0 for logo_recognition_annotation in annotation_result.logo_recognition_annotations:\u00a0 \u00a0 \u00a0 \u00a0 entity = logo_recognition_annotation.entity\u00a0 \u00a0 \u00a0 \u00a0 # Opaque entity ID. Some IDs may be available in [Google Knowledge Graph\u00a0 \u00a0 \u00a0 \u00a0 # Search API](https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 \u00a0 \u00a0 print(\"Entity Id : {}\".format(entity.entity_id))\u00a0 \u00a0 \u00a0 \u00a0 print(\"Description : {}\".format(entity.description))\u00a0 \u00a0 \u00a0 \u00a0 # All logo tracks where the recognized logo appears. Each track corresponds\u00a0 \u00a0 \u00a0 \u00a0 # to one logo instance appearing in consecutive frames.\u00a0 \u00a0 \u00a0 \u00a0 for track in logo_recognition_annotation.tracks:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Video segment of a track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.start_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.start_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.end_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.end_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tConfidence : {}\".format(track.confidence))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for timestamped_object in track.timestamped_objects:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Normalized Bounding box in a frame, where the object is located.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 normalized_bounding_box = timestamped_object.normalized_bounding_box\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\n\\t\\tLeft : {}\".format(normalized_bounding_box.left))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tTop : {}\".format(normalized_bounding_box.top))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tRight : {}\".format(normalized_bounding_box.right))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tBottom : {}\".format(normalized_bounding_box.bottom))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for attribute in timestamped_object.attributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\n\\t\\t\\tName : {}\".format(attribute.name))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\t\\tConfidence : {}\".format(attribute.confidence))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\t\\tValue : {}\".format(attribute.value))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for track_attribute in track.attributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\n\\t\\tName : {}\".format(track_attribute.name))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tConfidence : {}\".format(track_attribute.confidence))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tValue : {}\".format(track_attribute.value))\u00a0 \u00a0 \u00a0 \u00a0 # All video segments where the recognized logo appears. There might be\u00a0 \u00a0 \u00a0 \u00a0 # multiple instances of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 \u00a0 \u00a0 for segment in logo_recognition_annotation.segments:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.start_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.start_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.end_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.end_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\n```No preface\n **C#** : Please follow the [C# setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for .NET.](https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.VideoIntelligence.V1/index.html) \n **PHP** : Please follow the [PHP setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for PHP.](/php/docs/reference/cloud-videointelligence/latest) \n **Ruby** : Please follow the [Ruby setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for Ruby.](https://googleapis.dev/ruby/google-cloud-video_intelligence/latest/Google/Cloud/VideoIntelligence/V1.html)", "guide": "Cloud Video Intelligence API"}