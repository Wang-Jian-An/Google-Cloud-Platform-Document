{"title": "Cloud Video Intelligence API - \u8b58\u5225\u5fbd\u6a19", "url": "https://cloud.google.com/video-intelligence/docs/logo-recognition?hl=zh-cn", "abstract": "# Cloud Video Intelligence API - \u8b58\u5225\u5fbd\u6a19\nVideo Intelligence API \u53ef\u4ee5\u6aa2\u6e2c\u3001\u8ddf\u8e64\u548c\u8b58\u5225\u8996\u983b\u5167\u5bb9\u4e2d\u8d85\u904e 10 \u842c\u500b\u54c1\u724c\u548c\u5fbd\u6a19\u3002\n\u672c\u9801\u4ecb\u7d39\u77ad\u5982\u4f55\u4f7f\u7528 Video Intelligence API \u8b58\u5225\u8996\u983b\u4e2d\u7684\u5fbd\u6a19\u3002\n", "content": "## \u5728 Cloud Storage \u4e2d\u7232\u8996\u983b\u6dfb\u52a0\u8a3b\u91cb\n\u4ee5\u4e0b\u4ee3\u78bc\u793a\u4f8b\u6f14\u793a\u77ad\u5982\u4f55\u5728 Cloud Storage \u4e2d\u6aa2\u6e2c\u8996\u983b\u4e2d\u7684\u5fbd\u6a19\u3002\n### \u767c\u9001\u8655\u7406\u8acb\u6c42\u8981\u5728\u672c\u5730\u8996\u983b\u6587\u4ef6\u4e0a\u57f7\u884c\u8a3b\u89e3\uff0c\u8acb\u5c0d\u8996\u983b\u6587\u4ef6\u7684\u5167\u5bb9\u9032\u884c base64 \u7de8\u78bc\u3002\u5728\u8acb\u6c42\u7684 `inputContent` \u5b57\u6bb5\u4e2d\u6dfb\u52a0 base64 \u7de8\u78bc\u7684\u5167\u5bb9\u3002\u5982\u9700\u77ad\u89e3\u5982\u4f55\u5c0d\u8996\u983b\u6587\u4ef6\u7684\u5167\u5bb9\u9032\u884c base64 \u7de8\u78bc\uff0c\u8acb\u53c3\u95b1 [Base64 \u7de8\u78bc](https://cloud.google.com/video-intelligence/docs/base64?hl=zh-cn) \u3002\n\u4e0b\u9762\u6f14\u793a\u77ad\u5982\u4f55\u5411 [videos:annotate](https://cloud.google.com/video-intelligence/docs/reference/rest/v1/videos/annotate?hl=zh-cn) \u65b9\u6cd5\u767c\u9001 `POST` \u8acb\u6c42\u3002\u8a72\u793a\u4f8b\u4f7f\u7528\u901a\u904e Google Cloud CLI \u7232\u9805\u76ee\u8a2d\u7f6e\u7684\u670d\u52d9\u5e33\u865f\u7684\u8a2a\u554f\u4ee4\u724c\u3002\u5982\u9700\u77ad\u89e3\u5982\u4f55\u5b89\u88dd Google Cloud CLI\u3001\u7232\u9805\u76ee\u8a2d\u7f6e\u670d\u52d9\u5e33\u865f\u4ee5\u53ca\u7372\u53d6\u8a2a\u554f\u4ee4\u724c\uff0c\u8acb\u53c3\u95b1 [Video Intelligence \u5feb\u901f\u5165\u9580](https://cloud.google.com/video-intelligence/docs/quickstarts?hl=zh-cn) \u3002\n\u5728\u4f7f\u7528\u4efb\u4f55\u8acb\u6c42\u6578\u64da\u4e4b\u524d\uff0c\u8acb\u5148\u9032\u884c\u4ee5\u4e0b\u66ff\u63db\uff1a- \uff1a\u5305\u542b\u8981\u6dfb\u52a0\u8a3b\u91cb\u7684\u6587\u4ef6\u7684 Cloud Storage \u5b58\u5132\u6876\uff08\u5305\u62ec\u6587\u4ef6\u540d\uff09\u3002\u5fc5\u9808\u4ee5`gs://`\u958b\u982d\u3002\u4f8b\u5982\uff1a`\"inputUri\": \"gs://cloud-videointelligence-demo/assistant.mp4\",`\n- \uff1a\u60a8\u7684 Google Cloud \u9805\u76ee\u7684\u6578\u5b57\u6a19\u8b58\u7b26\nHTTP \u65b9\u6cd5\u548c\u7db2\u5740\uff1a\n```\nPOST https://videointelligence.googleapis.com/v1/videos:annotate\n```\n\u8acb\u6c42 JSON \u6b63\u6587\uff1a\n```\n{\n \"inputUri\":\"INPUT_URI\",\n \"features\": [\"LOGO_RECOGNITION\"]\n}\n```\n\u5982\u9700\u767c\u9001\u60a8\u7684\u8acb\u6c42\uff0c\u8acb\u5c55\u958b\u4ee5\u4e0b\u9078\u9805\u4e4b\u4e00\uff1a\u60a8\u61c9\u8a72\u6536\u5230\u985e\u4f3c\u4ee5\u4e0b\u5167\u5bb9\u7684 JSON \u97ff\u61c9\uff1a\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/operations/OPERATION_ID\"\n}\n```\n\u5982\u679c\u97ff\u61c9\u6210\u529f\uff0cVideo Intelligence API \u5c07\u8fd4\u56de\u60a8\u7684\u64cd\u4f5c\u7684 `name` \u3002\u4e0a\u9762\u7684\u793a\u4f8b\u5c55\u793a\u4e86\u6b64\u985e\u97ff\u61c9\u7684\u793a\u4f8b\uff0c\u5176\u4e2d `project-number` \u662f\u60a8\u7684\u9805\u76ee\u7de8\u865f\uff0c `operation-id` \u662f\u7232\u8acb\u6c42\u5275\u5efa\u7684\u9577\u6642\u9593\u904b\u884c\u7684\u64cd\u4f5c\u7684 ID\u3002\n- \uff1a\u60a8\u9805\u76ee\u7684\u7de8\u865f\n- \uff1a\u5728\u5176\u4e2d\u6dfb\u52a0\u8a3b\u89e3\u7684 Cloud \u5340\u57df\u3002\u652f\u6301\u7684\u96f2\u5340\u57df\u7232\uff1a`us-east1`\u3001`us-west1`\u3001`europe-west1`\u3001`asia-east1`\u3002\u5982\u679c\u672a\u6307\u5b9a\u5340\u57df\uff0c\u7cfb\u7d71\u5c07\u6839\u64da\u8996\u983b\u6587\u4ef6\u4f4d\u7f6e\u78ba\u5b9a\u5340\u57df\u3002\n- \uff1a\u662f\u7232\u8acb\u6c42\u5275\u5efa\u7684\u9577\u6642\u9593\u904b\u884c\u7684\u64cd\u4f5c\u7684 ID\uff0c\u4e26\u5728\u5553\u52d5\u64cd\u4f5c\u6642\u5728\u97ff\u61c9\u4e2d\u63d0\u4f9b\uff0c\u4f8b\u5982`12345...`\n### \u7372\u53d6\u7d50\u679c\u8981\u7372\u53d6\u8acb\u6c42\u7684\u7d50\u679c\uff0c\u8acb\u4f7f\u7528\u5c0d `videos:annotate` \u7684\u8abf\u7528\u8fd4\u56de\u7684\u64cd\u4f5c\u540d\u7a31\u767c\u9001 `GET` \u8acb\u6c42\uff0c\u5982\u4e0b\u4f8b\u6240\u793a\u3002\n\u5728\u4f7f\u7528\u4efb\u4f55\u8acb\u6c42\u6578\u64da\u4e4b\u524d\uff0c\u8acb\u5148\u9032\u884c\u4ee5\u4e0b\u66ff\u63db\uff1a- \uff1aVideo Intelligence API \u8fd4\u56de\u7684\u64cd\u4f5c\u540d\u7a31\u3002\u64cd\u4f5c\u540d\u7a31\u63a1\u7528`projects/` `` `/locations/` `` `/operations/` ``\u683c\u5f0f\n- \u6ce8\u610f\uff1a **done** \u5b57\u6bb5\u50c5\u5728\u5176\u503c\u7232 **True** \u6642\u7e94\u6703\u8fd4\u56de\u3002\u5b83\u4e0d\u6703\u5305\u542b\u5728\u64cd\u4f5c\u5c1a\u672a\u5b8c\u6210\u7684\u97ff\u61c9\u4e2d\u3002\n- \uff1a\u60a8\u7684 Google Cloud \u9805\u76ee\u7684\u6578\u5b57\u6a19\u8b58\u7b26\nHTTP \u65b9\u6cd5\u548c\u7db2\u5740\uff1a\n```\nGET https://videointelligence.googleapis.com/v1/OPERATION_NAME\n```\n\u5982\u9700\u767c\u9001\u60a8\u7684\u8acb\u6c42\uff0c\u8acb\u5c55\u958b\u4ee5\u4e0b\u9078\u9805\u4e4b\u4e00\uff1a\u60a8\u61c9\u8a72\u6536\u5230\u985e\u4f3c\u4ee5\u4e0b\u5167\u5bb9\u7684 JSON \u97ff\u61c9\uff1a## \u4e0b\u8f09\u8a3b\u89e3\u7d50\u679c\u5c07\u4f86\u6e90\u4e2d\u7684\u8a3b\u89e3\u8907\u88fd\u5230\u76ee\u6a19\u5b58\u5132\u6876\uff08\u8acb\u53c3\u95b1 [\u8907\u88fd\u6587\u4ef6\u548c\u5c0d\u8c61](https://cloud.google.com/storage/docs/gsutil/commands/cp?hl=zh-cn) \uff09\uff1a\n`gsutil cp` `` `gs://my-bucket`\n\u6ce8\u610f\uff1a\u5982\u679c\u8f38\u51fa gcs uri \u7531\u7528\u6236\u63d0\u4f9b\uff0c\u5247\u8a3b\u89e3\u5b58\u5132\u5728\u8a72 gcs uri \u4e2d\u3002\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/videointelligence/annotate/logo_detection_gcs.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 \"time\"\u00a0 \u00a0 \u00a0 \u00a0 video \"cloud.google.com/go/videointelligence/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 videopb \"cloud.google.com/go/videointelligence/apiv1/videointelligencepb\"\u00a0 \u00a0 \u00a0 \u00a0 \"github.com/golang/protobuf/ptypes\")// logoDetectionGCS analyzes a video and extracts logos with their bounding boxes.func logoDetectionGCS(w io.Writer, gcsURI string) error {\u00a0 \u00a0 \u00a0 \u00a0 // gcsURI := \"gs://cloud-samples-data/video/googlework_tiny.mp4\"\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // Creates a client.\u00a0 \u00a0 \u00a0 \u00a0 client, err := video.NewClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"video.NewClient: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 ctx, cancel := context.WithTimeout(ctx, time.Second*180)\u00a0 \u00a0 \u00a0 \u00a0 defer cancel()\u00a0 \u00a0 \u00a0 \u00a0 op, err := client.AnnotateVideo(ctx, &videopb.AnnotateVideoRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputUri: gcsURI,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Features: []videopb.Feature{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 videopb.Feature_LOGO_RECOGNITION,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 })\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"AnnotateVideo: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 resp, err := op.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"Wait: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Only one video was processed, so get the first result.\u00a0 \u00a0 \u00a0 \u00a0 result := resp.GetAnnotationResults()[0]\u00a0 \u00a0 \u00a0 \u00a0 // Annotations for list of logos detected, tracked and recognized in video.\u00a0 \u00a0 \u00a0 \u00a0 for _, annotation := range result.LogoRecognitionAnnotations {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Description: %q\\n\", annotation.Entity.GetDescription())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Opaque entity ID. Some IDs may be available in Google Knowledge\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Graph Search API (https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if len(annotation.Entity.EntityId) > 0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tEntity ID: %q\\n\", annotation.Entity.GetEntityId())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // All logo tracks where the recognized logo appears. Each track\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // corresponds to one logo instance appearing in consecutive frames.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, track := range annotation.Tracks {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Video segment of a track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment := track.GetSegment()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start, _ := ptypes.Duration(segment.GetStartTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end, _ := ptypes.Duration(segment.GetEndTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tSegment: %v to %v\\n\", start, end)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tConfidence: %f\\n\", track.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, timestampedObject := range track.TimestampedObjects {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Normalized Bounding box in a frame, where the object is\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // located.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 box := timestampedObject.GetNormalizedBoundingBox()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tBounding box position:\\n\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tleft \u00a0: %f\\n\", box.GetLeft())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\ttop \u00a0 : %f\\n\", box.GetTop())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tright : %f\\n\", box.GetRight())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tbottom: %f\\n\", box.GetBottom())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, attribute := range timestampedObject.Attributes {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\t\\tName: %q\\n\", attribute.GetName())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\t\\tConfidence: %f\\n\", attribute.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\t\\tValue: %q\\n\", attribute.GetValue())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, trackAttribute := range track.Attributes {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tName: %q\\n\", trackAttribute.GetName())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tConfidence: %f\\n\", trackAttribute.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tValue: %q\\n\", trackAttribute.GetValue())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // All video segments where the recognized logo appears. There might be\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // multiple instances of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, segment := range annotation.Segments {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start, _ := ptypes.Duration(segment.GetStartTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end, _ := ptypes.Duration(segment.GetEndTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tSegment: %v to %v\\n\", start, end)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/video/src/main/java/video/LogoDetectionGcs.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.videointelligence.v1.AnnotateVideoProgress;import com.google.cloud.videointelligence.v1.AnnotateVideoRequest;import com.google.cloud.videointelligence.v1.AnnotateVideoResponse;import com.google.cloud.videointelligence.v1.DetectedAttribute;import com.google.cloud.videointelligence.v1.Entity;import com.google.cloud.videointelligence.v1.Feature;import com.google.cloud.videointelligence.v1.LogoRecognitionAnnotation;import com.google.cloud.videointelligence.v1.NormalizedBoundingBox;import com.google.cloud.videointelligence.v1.TimestampedObject;import com.google.cloud.videointelligence.v1.Track;import com.google.cloud.videointelligence.v1.VideoAnnotationResults;import com.google.cloud.videointelligence.v1.VideoIntelligenceServiceClient;import com.google.cloud.videointelligence.v1.VideoSegment;import com.google.protobuf.Duration;import java.io.IOException;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class LogoDetectionGcs {\u00a0 public static void detectLogoGcs() throws Exception {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String gcsUri = \"gs://YOUR_BUCKET_ID/path/to/your/video.mp4\";\u00a0 \u00a0 detectLogoGcs(gcsUri);\u00a0 }\u00a0 public static void detectLogoGcs(String inputUri)\u00a0 \u00a0 \u00a0 throws IOException, ExecutionException, InterruptedException, TimeoutException {\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (VideoIntelligenceServiceClient client = VideoIntelligenceServiceClient.create()) {\u00a0 \u00a0 \u00a0 // Create the request\u00a0 \u00a0 \u00a0 AnnotateVideoRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AnnotateVideoRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputUri(inputUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addFeatures(Feature.LOGO_RECOGNITION)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // asynchronously perform object tracking on videos\u00a0 \u00a0 \u00a0 OperationFuture<AnnotateVideoResponse, AnnotateVideoProgress> future =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 client.annotateVideoAsync(request);\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to complete...\");\u00a0 \u00a0 \u00a0 // The first result is retrieved because a single video was processed.\u00a0 \u00a0 \u00a0 AnnotateVideoResponse response = future.get(600, TimeUnit.SECONDS);\u00a0 \u00a0 \u00a0 VideoAnnotationResults annotationResult = response.getAnnotationResults(0);\u00a0 \u00a0 \u00a0 // Annotations for list of logos detected, tracked and recognized in video.\u00a0 \u00a0 \u00a0 for (LogoRecognitionAnnotation logoRecognitionAnnotation :\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 annotationResult.getLogoRecognitionAnnotationsList()) {\u00a0 \u00a0 \u00a0 \u00a0 Entity entity = logoRecognitionAnnotation.getEntity();\u00a0 \u00a0 \u00a0 \u00a0 // Opaque entity ID. Some IDs may be available in\u00a0 \u00a0 \u00a0 \u00a0 // [Google Knowledge Graph Search API](https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Entity Id : %s\\n\", entity.getEntityId());\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Description : %s\\n\", entity.getDescription());\u00a0 \u00a0 \u00a0 \u00a0 // All logo tracks where the recognized logo appears. Each track corresponds to one logo\u00a0 \u00a0 \u00a0 \u00a0 // instance appearing in consecutive frames.\u00a0 \u00a0 \u00a0 \u00a0 for (Track track : logoRecognitionAnnotation.getTracksList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Video segment of a track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Duration startTimeOffset = track.getSegment().getStartTimeOffset();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset: %s.%s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 startTimeOffset.getSeconds(), startTimeOffset.getNanos());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Duration endTimeOffset = track.getSegment().getEndTimeOffset();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset: %s.%s\\n\", endTimeOffset.getSeconds(), endTimeOffset.getNanos());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\tConfidence: %s\\n\", track.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (TimestampedObject timestampedObject : track.getTimestampedObjectsList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Normalized Bounding box in a frame, where the object is located.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 NormalizedBoundingBox normalizedBoundingBox =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timestampedObject.getNormalizedBoundingBox();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\n\\t\\tLeft: %s\\n\", normalizedBoundingBox.getLeft());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tTop: %s\\n\", normalizedBoundingBox.getTop());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tRight: %s\\n\", normalizedBoundingBox.getRight());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tBottom: %s\\n\", normalizedBoundingBox.getBottom());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (DetectedAttribute attribute : timestampedObject.getAttributesList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\n\\t\\t\\tName: %s\\n\", attribute.getName());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\t\\tConfidence: %s\\n\", attribute.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\t\\tValue: %s\\n\", attribute.getValue());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (DetectedAttribute trackAttribute : track.getAttributesList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\n\\t\\tName : %s\\n\", trackAttribute.getName());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tConfidence : %s\\n\", trackAttribute.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tValue : %s\\n\", trackAttribute.getValue());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // All video segments where the recognized logo appears. There might be multiple instances\u00a0 \u00a0 \u00a0 \u00a0 // of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 \u00a0 \u00a0 for (VideoSegment segment : logoRecognitionAnnotation.getSegmentsList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset : %s.%s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getStartTimeOffset().getSeconds(), segment.getStartTimeOffset().getNanos());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset : %s.%s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getEndTimeOffset().getSeconds(), segment.getEndTimeOffset().getNanos());\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/video-intelligence/detect_logo_gcs.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*/// const inputUri = 'gs://cloud-samples-data/video/googlework_short.mp4';// Imports the Google Cloud client librariesconst Video = require('@google-cloud/video-intelligence');// Instantiates a clientconst client = new Video.VideoIntelligenceServiceClient();// Performs asynchronous video annotation for logo recognition on a file hosted in GCS.async function detectLogoGcs() {\u00a0 // Build the request with the input uri and logo recognition feature.\u00a0 const request = {\u00a0 \u00a0 inputUri: inputUri,\u00a0 \u00a0 features: ['LOGO_RECOGNITION'],\u00a0 };\u00a0 // Make the asynchronous request\u00a0 const [operation] = await client.annotateVideo(request);\u00a0 // Wait for the results\u00a0 const [response] = await operation.promise();\u00a0 // Get the first response, since we sent only one video.\u00a0 const annotationResult = response.annotationResults[0];\u00a0 for (const logoRecognitionAnnotation of annotationResult.logoRecognitionAnnotations) {\u00a0 \u00a0 const entity = logoRecognitionAnnotation.entity;\u00a0 \u00a0 // Opaque entity ID. Some IDs may be available in\u00a0 \u00a0 // [Google Knowledge Graph Search API](https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 console.log(`Entity Id: ${entity.entityId}`);\u00a0 \u00a0 console.log(`Description: ${entity.description}`);\u00a0 \u00a0 // All logo tracks where the recognized logo appears.\u00a0 \u00a0 // Each track corresponds to one logo instance appearing in consecutive frames.\u00a0 \u00a0 for (const track of logoRecognitionAnnotation.tracks) {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\n\\tStart Time Offset: ${track.segment.startTimeOffset.seconds}.${track.segment.startTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tEnd Time Offset: ${track.segment.endTimeOffset.seconds}.${track.segment.endTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(`\\tConfidence: ${track.confidence}`);\u00a0 \u00a0 \u00a0 // The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 for (const timestampedObject of track.timestampedObjects) {\u00a0 \u00a0 \u00a0 \u00a0 // Normalized Bounding box in a frame, where the object is located.\u00a0 \u00a0 \u00a0 \u00a0 const normalizedBoundingBox = timestampedObject.normalizedBoundingBox;\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\n\\t\\tLeft: ${normalizedBoundingBox.left}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tTop: ${normalizedBoundingBox.top}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tRight: ${normalizedBoundingBox.right}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tBottom: ${normalizedBoundingBox.bottom}`);\u00a0 \u00a0 \u00a0 \u00a0 // Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 for (const attribute of timestampedObject.attributes) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\n\\t\\t\\tName: ${attribute.name}`);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\t\\tConfidence: ${attribute.confidence}`);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\t\\tValue: ${attribute.value}`);\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 // Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 for (const trackAttribute of track.attributes) {\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\n\\t\\tName: ${trackAttribute.name}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tConfidence: ${trackAttribute.confidence}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tValue: ${trackAttribute.value}`);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 \u00a0 // All video segments where the recognized logo appears.\u00a0 \u00a0 // There might be multiple instances of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 for (const segment of logoRecognitionAnnotation.segments) {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\n\\tStart Time Offset: ${segment.startTimeOffset.seconds}.${segment.startTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tEnd Time Offset: ${segment.endTimeOffset.seconds}.${segment.endTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 }\u00a0 }}detectLogoGcs();\n```\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/video_detect_logo_gcs.py) \n```\nfrom google.cloud import videointelligencedef detect_logo_gcs(input_uri=\"gs://YOUR_BUCKET_ID/path/to/your/file.mp4\"):\u00a0 \u00a0 client = videointelligence.VideoIntelligenceServiceClient()\u00a0 \u00a0 features = [videointelligence.Feature.LOGO_RECOGNITION]\u00a0 \u00a0 operation = client.annotate_video(\u00a0 \u00a0 \u00a0 \u00a0 request={\"features\": features, \"input_uri\": input_uri}\u00a0 \u00a0 )\u00a0 \u00a0 print(\"Waiting for operation to complete...\")\u00a0 \u00a0 response = operation.result()\u00a0 \u00a0 # Get the first response, since we sent only one video.\u00a0 \u00a0 annotation_result = response.annotation_results[0]\u00a0 \u00a0 # Annotations for list of logos detected, tracked and recognized in video.\u00a0 \u00a0 for logo_recognition_annotation in annotation_result.logo_recognition_annotations:\u00a0 \u00a0 \u00a0 \u00a0 entity = logo_recognition_annotation.entity\u00a0 \u00a0 \u00a0 \u00a0 # Opaque entity ID. Some IDs may be available in [Google Knowledge Graph\u00a0 \u00a0 \u00a0 \u00a0 # Search API](https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 \u00a0 \u00a0 print(\"Entity Id : {}\".format(entity.entity_id))\u00a0 \u00a0 \u00a0 \u00a0 print(\"Description : {}\".format(entity.description))\u00a0 \u00a0 \u00a0 \u00a0 # All logo tracks where the recognized logo appears. Each track corresponds\u00a0 \u00a0 \u00a0 \u00a0 # to one logo instance appearing in consecutive frames.\u00a0 \u00a0 \u00a0 \u00a0 for track in logo_recognition_annotation.tracks:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Video segment of a track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.start_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.start_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.end_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.end_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tConfidence : {}\".format(track.confidence))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for timestamped_object in track.timestamped_objects:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Normalized Bounding box in a frame, where the object is located.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 normalized_bounding_box = timestamped_object.normalized_bounding_box\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\n\\t\\tLeft : {}\".format(normalized_bounding_box.left))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tTop : {}\".format(normalized_bounding_box.top))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tRight : {}\".format(normalized_bounding_box.right))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tBottom : {}\".format(normalized_bounding_box.bottom))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for attribute in timestamped_object.attributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\n\\t\\t\\tName : {}\".format(attribute.name))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\t\\tConfidence : {}\".format(attribute.confidence))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\t\\tValue : {}\".format(attribute.value))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for track_attribute in track.attributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\n\\t\\tName : {}\".format(track_attribute.name))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tConfidence : {}\".format(track_attribute.confidence))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tValue : {}\".format(track_attribute.value))\u00a0 \u00a0 \u00a0 \u00a0 # All video segments where the recognized logo appears. There might be\u00a0 \u00a0 \u00a0 \u00a0 # multiple instances of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 \u00a0 \u00a0 for segment in logo_recognition_annotation.segments:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.start_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.start_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.end_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.end_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\n```No preface\n **C#** \uff1a\u8acb\u6309\u7167\u201c\u5ba2\u6236\u7aef\u5eab\u201d\u9801\u9762\u4e0a\u7684 [C# \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u9032\u884c\u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [\u9069\u7528\u65bc .NET \u7684 Video Intelligence \u53c3\u8003\u6587\u6a94](https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.VideoIntelligence.V1/index.html) \u3002\n **PHP** \uff1a\u8acb\u6309\u7167\u5ba2\u6236\u7aef\u5eab\u9801\u9762\u4e0a\u7684 [PHP \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [PHP \u7248 Video Intelligence \u53c3\u8003\u6587\u6a94](https://cloud.google.com/php/docs/reference/cloud-videointelligence/latest?hl=zh-cn) \u3002\n **Ruby** \uff1a\u8acb\u6309\u7167\u5ba2\u6236\u7aef\u5eab\u9801\u9762\u4e0a\u7684 [Ruby \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [Ruby \u7248 Video Intelligence \u53c3\u8003\u6587\u6a94](https://googleapis.dev/ruby/google-cloud-video_intelligence/latest/Google/Cloud/VideoIntelligence/V1.html) \u3002\n## \u7232\u672c\u5730\u8996\u983b\u6dfb\u52a0\u8a3b\u91cb\n\u4ee5\u4e0b\u4ee3\u78bc\u793a\u4f8b\u6f14\u793a\u77ad\u5982\u4f55\u5728\u672c\u5730\u8996\u983b\u6587\u4ef6\u4e2d\u6aa2\u6e2c\u5fbd\u6a19\u3002\n### \u767c\u9001\u8996\u983b\u8a3b\u89e3\u8acb\u6c42\u8981\u5c0d\u672c\u5730\u8996\u983b\u6587\u4ef6\u57f7\u884c\u8a3b\u89e3\uff0c\u8acb\u52d9\u5fc5\u5c0d\u8996\u983b\u6587\u4ef6\u7684\u5167\u5bb9\u9032\u884c base64 \u7de8\u78bc\u3002\u5728\u8acb\u6c42\u7684 `inputContent` \u5b57\u6bb5\u4e2d\u6dfb\u52a0 base64 \u7de8\u78bc\u7684\u5167\u5bb9\u3002\u5982\u9700\u77ad\u89e3\u5982\u4f55\u5c0d\u8996\u983b\u6587\u4ef6\u7684\u5167\u5bb9\u9032\u884c base64 \u7de8\u78bc\uff0c\u8acb\u53c3\u95b1 [Base64 \u7de8\u78bc](https://cloud.google.com/video-intelligence/docs/base64?hl=zh-cn) \u3002\n\u4ee5\u4e0b\u4ee3\u78bc\u5c55\u793a\u77ad\u5982\u4f55\u5411 `videos:annotate` \u65b9\u6cd5\u767c\u9001 POST \u8acb\u6c42\u3002\u8a72\u793a\u4f8b\u4f7f\u7528\u901a\u904e Google Cloud CLI \u7232\u9805\u76ee\u8a2d\u7f6e\u7684\u670d\u52d9\u5e33\u865f\u7684\u8a2a\u554f\u4ee4\u724c\u3002\u5982\u9700\u77ad\u89e3\u6709\u95dc\u5b89\u88dd Google Cloud CLI\u3001\u4f7f\u7528\u670d\u52d9\u5e33\u865f\u8a2d\u7f6e\u9805\u76ee\u4ee5\u53ca\u7372\u53d6\u8a2a\u554f\u4ee4\u724c\u7684\u8aaa\u660e\uff0c\u8acb\u53c3\u95b1 [Video Intelligence API \u5feb\u901f\u5165\u9580](https://cloud.google.com/video-intelligence/docs/quickstarts?hl=zh-cn) \n\u5728\u4f7f\u7528\u4efb\u4f55\u8acb\u6c42\u6578\u64da\u4e4b\u524d\uff0c\u8acb\u5148\u9032\u884c\u4ee5\u4e0b\u66ff\u63db\uff1a- \"inputContent\":\u4f8b\u5982\uff1a`\"UklGRg41AwBBVkkgTElTVAwBAABoZHJsYXZpaDgAAAA1ggAAxPMBAAAAAAAQCAA...\"`\n- \uff1a[\u53ef\u9078]\u8acb\u53c3\u95b1 [\u652f\u6301\u7684\u8a9e\u8a00](https://cloud.google.com/speech-to-text/docs/speech-to-text-supported-languages?hl=zh-cn) \n- \uff1a\u60a8\u7684 Google Cloud \u9805\u76ee\u7684\u6578\u5b57\u6a19\u8b58\u7b26\nHTTP \u65b9\u6cd5\u548c\u7db2\u5740\uff1a\n```\nPOST https://videointelligence.googleapis.com/v1/videos:annotate\n```\n\u8acb\u6c42 JSON \u6b63\u6587\uff1a\n```\n{\n \"inputContent\": \"BASE64_ENCODED_CONTENT\",\n \"features\": [\"LOGO_RECOGNITION\"],\n \"videoContext\": {\n }\n}\n```\n\u5982\u9700\u767c\u9001\u60a8\u7684\u8acb\u6c42\uff0c\u8acb\u5c55\u958b\u4ee5\u4e0b\u9078\u9805\u4e4b\u4e00\uff1a\u60a8\u61c9\u8a72\u6536\u5230\u985e\u4f3c\u4ee5\u4e0b\u5167\u5bb9\u7684 JSON \u97ff\u61c9\uff1a\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION_ID/operations/OPERATION_ID\"\n}\n```\n\u5982\u679c\u97ff\u61c9\u6210\u529f\uff0cVideo Intelligence API \u5c07\u8fd4\u56de\u60a8\u7684\u64cd\u4f5c\u7684 `name` \u3002\u4e0a\u9762\u7684\u793a\u4f8b\u5c55\u793a\u4e86\u6b64\u985e\u97ff\u61c9\u7684\u793a\u4f8b\uff0c\u5176\u4e2d `project-number` \u662f\u60a8\u7684\u9805\u76ee\u540d\u7a31\uff0c `operation-id` \u662f\u7232\u8acb\u6c42\u5275\u5efa\u7684\u9577\u6642\u9593\u904b\u884c\u7684\u64cd\u4f5c\u7684 ID\u3002\n- \uff1a\u4e26\u5728\u5553\u52d5\u64cd\u4f5c\u6642\u5728\u97ff\u61c9\u4e2d\u63d0\u4f9b\uff0c\u4f8b\u5982`12345...`\n### \u7372\u53d6\u8a3b\u89e3\u7d50\u679c\u8981\u6aa2\u7d22\u64cd\u4f5c\u7684\u7d50\u679c\uff0c\u8acb\u4f7f\u7528\u5f9e [videos\uff1aannotate](https://cloud.google.com/video-intelligence/docs/reference/rest/v1/videos/annotate?hl=zh-cn) \u8abf\u7528\u8fd4\u56de\u7684\u64cd\u4f5c\u540d\u7a31\u767c\u51fa [GET](https://cloud.google.com/video-intelligence/docs/reference/rest/v1/projects.locations.operations/get?hl=zh-cn) \u8acb\u6c42\uff0c\u5982\u4ee5\u4e0b\u793a\u4f8b\u6240\u793a\u3002\n\u5728\u4f7f\u7528\u4efb\u4f55\u8acb\u6c42\u6578\u64da\u4e4b\u524d\uff0c\u8acb\u5148\u9032\u884c\u4ee5\u4e0b\u66ff\u63db\uff1a- \uff1a\u60a8\u7684 Google Cloud \u9805\u76ee\u7684\u6578\u5b57\u6a19\u8b58\u7b26\nHTTP \u65b9\u6cd5\u548c\u7db2\u5740\uff1a\n```\nGET https://videointelligence.googleapis.com/v1/OPERATION_NAME\n```\n\u5982\u9700\u767c\u9001\u60a8\u7684\u8acb\u6c42\uff0c\u8acb\u5c55\u958b\u4ee5\u4e0b\u9078\u9805\u4e4b\u4e00\uff1a\u60a8\u61c9\u8a72\u6536\u5230\u985e\u4f3c\u4ee5\u4e0b\u5167\u5bb9\u7684 JSON \u97ff\u61c9\uff1a\u6587\u672c\u6aa2\u6e2c\u8a3b\u91cb\u4ee5 `textAnnotations` \u5217\u8868\u7684\u5f62\u5f0f\u8fd4\u56de\u3002\u6ce8\u610f\uff1a\u50c5\u7576 **done** \u5b57\u6bb5\u7684\u503c\u7232 **True** \u6642\u7e94\u6703\u8fd4\u56de\u8a72\u5b57\u6bb5\u3002\u5b83\u4e0d\u6703\u5305\u542b\u5728\u64cd\u4f5c\u5c1a\u672a\u5b8c\u6210\u7684\u97ff\u61c9\u4e2d\u3002\n\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/golang-samples/blob/HEAD/videointelligence/annotate/logo_detection.go) \n```\nimport (\u00a0 \u00a0 \u00a0 \u00a0 \"context\"\u00a0 \u00a0 \u00a0 \u00a0 \"fmt\"\u00a0 \u00a0 \u00a0 \u00a0 \"io\"\u00a0 \u00a0 \u00a0 \u00a0 \"io/ioutil\"\u00a0 \u00a0 \u00a0 \u00a0 \"time\"\u00a0 \u00a0 \u00a0 \u00a0 video \"cloud.google.com/go/videointelligence/apiv1\"\u00a0 \u00a0 \u00a0 \u00a0 videopb \"cloud.google.com/go/videointelligence/apiv1/videointelligencepb\"\u00a0 \u00a0 \u00a0 \u00a0 \"github.com/golang/protobuf/ptypes\")// logoDetection analyzes a video and extracts logos with their bounding boxes.func logoDetection(w io.Writer, filename string) error {\u00a0 \u00a0 \u00a0 \u00a0 // filename := \"../testdata/googlework_short.mp4\"\u00a0 \u00a0 \u00a0 \u00a0 ctx := context.Background()\u00a0 \u00a0 \u00a0 \u00a0 // Creates a client.\u00a0 \u00a0 \u00a0 \u00a0 client, err := video.NewClient(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"video.NewClient: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 defer client.Close()\u00a0 \u00a0 \u00a0 \u00a0 ctx, cancel := context.WithTimeout(ctx, time.Second*180)\u00a0 \u00a0 \u00a0 \u00a0 defer cancel()\u00a0 \u00a0 \u00a0 \u00a0 fileBytes, err := ioutil.ReadFile(filename)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"ioutil.ReadFile: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 op, err := client.AnnotateVideo(ctx, &videopb.AnnotateVideoRequest{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 InputContent: fileBytes,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Features: []videopb.Feature{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 videopb.Feature_LOGO_RECOGNITION,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 })\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"AnnotateVideo: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 resp, err := op.Wait(ctx)\u00a0 \u00a0 \u00a0 \u00a0 if err != nil {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 return fmt.Errorf(\"Wait: %w\", err)\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // Only one video was processed, so get the first result.\u00a0 \u00a0 \u00a0 \u00a0 result := resp.GetAnnotationResults()[0]\u00a0 \u00a0 \u00a0 \u00a0 // Annotations for list of logos detected, tracked and recognized in video.\u00a0 \u00a0 \u00a0 \u00a0 for _, annotation := range result.LogoRecognitionAnnotations {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"Description: %q\\n\", annotation.Entity.GetDescription())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Opaque entity ID. Some IDs may be available in Google Knowledge\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Graph Search API (https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if len(annotation.Entity.EntityId) > 0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tEntity ID: %q\\n\", annotation.Entity.GetEntityId())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // All logo tracks where the recognized logo appears. Each track\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // corresponds to one logo instance appearing in consecutive frames.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, track := range annotation.Tracks {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Video segment of a track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment := track.GetSegment()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start, _ := ptypes.Duration(segment.GetStartTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end, _ := ptypes.Duration(segment.GetEndTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tSegment: %v to %v\\n\", start, end)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tConfidence: %f\\n\", track.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, timestampedObject := range track.TimestampedObjects {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Normalized Bounding box in a frame, where the object is\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // located.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 box := timestampedObject.GetNormalizedBoundingBox()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tBounding box position:\\n\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tleft \u00a0: %f\\n\", box.GetLeft())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\ttop \u00a0 : %f\\n\", box.GetTop())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tright : %f\\n\", box.GetRight())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tbottom: %f\\n\", box.GetBottom())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, attribute := range timestampedObject.Attributes {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\t\\tName: %q\\n\", attribute.GetName())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\t\\tConfidence: %f\\n\", attribute.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\t\\tValue: %q\\n\", attribute.GetValue())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, trackAttribute := range track.Attributes {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tName: %q\\n\", trackAttribute.GetName())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tConfidence: %f\\n\", trackAttribute.GetConfidence())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\t\\tValue: %q\\n\", trackAttribute.GetValue())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // All video segments where the recognized logo appears. There might be\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // multiple instances of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for _, segment := range annotation.Segments {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start, _ := ptypes.Duration(segment.GetStartTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end, _ := ptypes.Duration(segment.GetEndTimeOffset())\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fmt.Fprintf(w, \"\\tSegment: %v to %v\\n\", start, end)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 return nil}\n```\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/video/src/main/java/video/LogoDetection.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.videointelligence.v1.AnnotateVideoProgress;import com.google.cloud.videointelligence.v1.AnnotateVideoRequest;import com.google.cloud.videointelligence.v1.AnnotateVideoResponse;import com.google.cloud.videointelligence.v1.DetectedAttribute;import com.google.cloud.videointelligence.v1.Entity;import com.google.cloud.videointelligence.v1.Feature;import com.google.cloud.videointelligence.v1.LogoRecognitionAnnotation;import com.google.cloud.videointelligence.v1.NormalizedBoundingBox;import com.google.cloud.videointelligence.v1.TimestampedObject;import com.google.cloud.videointelligence.v1.Track;import com.google.cloud.videointelligence.v1.VideoAnnotationResults;import com.google.cloud.videointelligence.v1.VideoIntelligenceServiceClient;import com.google.cloud.videointelligence.v1.VideoSegment;import com.google.protobuf.ByteString;import com.google.protobuf.Duration;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.concurrent.TimeoutException;public class LogoDetection {\u00a0 public static void detectLogo() throws Exception {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String localFilePath = \"path/to/your/video.mp4\";\u00a0 \u00a0 detectLogo(localFilePath);\u00a0 }\u00a0 public static void detectLogo(String filePath)\u00a0 \u00a0 \u00a0 throws IOException, ExecutionException, InterruptedException, TimeoutException {\u00a0 \u00a0 // Initialize client that will be used to send requests. This client only needs to be created\u00a0 \u00a0 // once, and can be reused for multiple requests. After completing all of your requests, call\u00a0 \u00a0 // the \"close\" method on the client to safely clean up any remaining background resources.\u00a0 \u00a0 try (VideoIntelligenceServiceClient client = VideoIntelligenceServiceClient.create()) {\u00a0 \u00a0 \u00a0 // Read file\u00a0 \u00a0 \u00a0 Path path = Paths.get(filePath);\u00a0 \u00a0 \u00a0 byte[] data = Files.readAllBytes(path);\u00a0 \u00a0 \u00a0 // Create the request\u00a0 \u00a0 \u00a0 AnnotateVideoRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AnnotateVideoRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputContent(ByteString.copyFrom(data))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addFeatures(Feature.LOGO_RECOGNITION)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // asynchronously perform object tracking on videos\u00a0 \u00a0 \u00a0 OperationFuture<AnnotateVideoResponse, AnnotateVideoProgress> future =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 client.annotateVideoAsync(request);\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to complete...\");\u00a0 \u00a0 \u00a0 // The first result is retrieved because a single video was processed.\u00a0 \u00a0 \u00a0 AnnotateVideoResponse response = future.get(300, TimeUnit.SECONDS);\u00a0 \u00a0 \u00a0 VideoAnnotationResults annotationResult = response.getAnnotationResults(0);\u00a0 \u00a0 \u00a0 // Annotations for list of logos detected, tracked and recognized in video.\u00a0 \u00a0 \u00a0 for (LogoRecognitionAnnotation logoRecognitionAnnotation :\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 annotationResult.getLogoRecognitionAnnotationsList()) {\u00a0 \u00a0 \u00a0 \u00a0 Entity entity = logoRecognitionAnnotation.getEntity();\u00a0 \u00a0 \u00a0 \u00a0 // Opaque entity ID. Some IDs may be available in\u00a0 \u00a0 \u00a0 \u00a0 // [Google Knowledge Graph Search API](https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Entity Id : %s\\n\", entity.getEntityId());\u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"Description : %s\\n\", entity.getDescription());\u00a0 \u00a0 \u00a0 \u00a0 // All logo tracks where the recognized logo appears. Each track corresponds to one logo\u00a0 \u00a0 \u00a0 \u00a0 // instance appearing in consecutive frames.\u00a0 \u00a0 \u00a0 \u00a0 for (Track track : logoRecognitionAnnotation.getTracksList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Video segment of a track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Duration startTimeOffset = track.getSegment().getStartTimeOffset();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset: %s.%s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 startTimeOffset.getSeconds(), startTimeOffset.getNanos());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Duration endTimeOffset = track.getSegment().getEndTimeOffset();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset: %s.%s\\n\", endTimeOffset.getSeconds(), endTimeOffset.getNanos());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\tConfidence: %s\\n\", track.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (TimestampedObject timestampedObject : track.getTimestampedObjectsList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Normalized Bounding box in a frame, where the object is located.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 NormalizedBoundingBox normalizedBoundingBox =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timestampedObject.getNormalizedBoundingBox();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\n\\t\\tLeft: %s\\n\", normalizedBoundingBox.getLeft());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tTop: %s\\n\", normalizedBoundingBox.getTop());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tRight: %s\\n\", normalizedBoundingBox.getRight());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tBottom: %s\\n\", normalizedBoundingBox.getBottom());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (DetectedAttribute attribute : timestampedObject.getAttributesList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\n\\t\\t\\tName: %s\\n\", attribute.getName());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\t\\tConfidence: %s\\n\", attribute.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\t\\tValue: %s\\n\", attribute.getValue());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (DetectedAttribute trackAttribute : track.getAttributesList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\n\\t\\tName : %s\\n\", trackAttribute.getName());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tConfidence : %s\\n\", trackAttribute.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\"\\t\\tValue : %s\\n\", trackAttribute.getValue());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 // All video segments where the recognized logo appears. There might be multiple instances\u00a0 \u00a0 \u00a0 \u00a0 // of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 \u00a0 \u00a0 for (VideoSegment segment : logoRecognitionAnnotation.getSegmentsList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset : %s.%s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getStartTimeOffset().getSeconds(), segment.getStartTimeOffset().getNanos());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset : %s.%s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getEndTimeOffset().getSeconds(), segment.getEndTimeOffset().getNanos());\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/video-intelligence/detect_logo.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*/// const localFilePath = 'path/to/your/video.mp4'// Imports the Google Cloud client librariesconst Video = require('@google-cloud/video-intelligence');const fs = require('fs');// Instantiates a clientconst client = new Video.VideoIntelligenceServiceClient();// Performs asynchronous video annotation for logo recognition on a file.async function detectLogo() {\u00a0 const inputContent = fs.readFileSync(localFilePath).toString('base64');\u00a0 // Build the request with the input content and logo recognition feature.\u00a0 const request = {\u00a0 \u00a0 inputContent: inputContent,\u00a0 \u00a0 features: ['LOGO_RECOGNITION'],\u00a0 };\u00a0 // Make the asynchronous request\u00a0 const [operation] = await client.annotateVideo(request);\u00a0 // Wait for the results\u00a0 const [response] = await operation.promise();\u00a0 // Get the first response, since we sent only one video.\u00a0 const annotationResult = response.annotationResults[0];\u00a0 for (const logoRecognitionAnnotation of annotationResult.logoRecognitionAnnotations) {\u00a0 \u00a0 const entity = logoRecognitionAnnotation.entity;\u00a0 \u00a0 // Opaque entity ID. Some IDs may be available in\u00a0 \u00a0 // [Google Knowledge Graph Search API](https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 console.log(`Entity Id: ${entity.entityId}`);\u00a0 \u00a0 console.log(`Description: ${entity.description}`);\u00a0 \u00a0 // All logo tracks where the recognized logo appears.\u00a0 \u00a0 // Each track corresponds to one logo instance appearing in consecutive frames.\u00a0 \u00a0 for (const track of logoRecognitionAnnotation.tracks) {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\n\\tStart Time Offset: ${track.segment.startTimeOffset.seconds}.${track.segment.startTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tEnd Time Offset: ${track.segment.endTimeOffset.seconds}.${track.segment.endTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(`\\tConfidence: ${track.confidence}`);\u00a0 \u00a0 \u00a0 // The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 for (const timestampedObject of track.timestampedObjects) {\u00a0 \u00a0 \u00a0 \u00a0 // Normalized Bounding box in a frame, where the object is located.\u00a0 \u00a0 \u00a0 \u00a0 const normalizedBoundingBox = timestampedObject.normalizedBoundingBox;\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\n\\t\\tLeft: ${normalizedBoundingBox.left}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tTop: ${normalizedBoundingBox.top}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tRight: ${normalizedBoundingBox.right}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tBottom: ${normalizedBoundingBox.bottom}`);\u00a0 \u00a0 \u00a0 \u00a0 // Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 for (const attribute of timestampedObject.attributes) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\n\\t\\t\\tName: ${attribute.name}`);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\t\\tConfidence: ${attribute.confidence}`);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\t\\tValue: ${attribute.value}`);\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 // Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 for (const trackAttribute of track.attributes) {\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\n\\t\\tName: ${trackAttribute.name}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tConfidence: ${trackAttribute.confidence}`);\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\t\\tValue: ${trackAttribute.value}`);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 \u00a0 // All video segments where the recognized logo appears.\u00a0 \u00a0 // There might be multiple instances of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 for (const segment of logoRecognitionAnnotation.segments) {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\n\\tStart Time Offset: ${segment.startTimeOffset.seconds}.${segment.startTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tEnd Time Offset: ${segment.endTimeOffset.seconds}.${segment.endTimeOffset.nanos}`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 }\u00a0 }}detectLogo();\n```\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/video_detect_logo.py) \n```\nimport iofrom google.cloud import videointelligencedef detect_logo(local_file_path=\"path/to/your/video.mp4\"):\u00a0 \u00a0 \"\"\"Performs asynchronous video annotation for logo recognition on a local file.\"\"\"\u00a0 \u00a0 client = videointelligence.VideoIntelligenceServiceClient()\u00a0 \u00a0 with io.open(local_file_path, \"rb\") as f:\u00a0 \u00a0 \u00a0 \u00a0 input_content = f.read()\u00a0 \u00a0 features = [videointelligence.Feature.LOGO_RECOGNITION]\u00a0 \u00a0 operation = client.annotate_video(\u00a0 \u00a0 \u00a0 \u00a0 request={\"features\": features, \"input_content\": input_content}\u00a0 \u00a0 )\u00a0 \u00a0 print(\"Waiting for operation to complete...\")\u00a0 \u00a0 response = operation.result()\u00a0 \u00a0 # Get the first response, since we sent only one video.\u00a0 \u00a0 annotation_result = response.annotation_results[0]\u00a0 \u00a0 # Annotations for list of logos detected, tracked and recognized in video.\u00a0 \u00a0 for logo_recognition_annotation in annotation_result.logo_recognition_annotations:\u00a0 \u00a0 \u00a0 \u00a0 entity = logo_recognition_annotation.entity\u00a0 \u00a0 \u00a0 \u00a0 # Opaque entity ID. Some IDs may be available in [Google Knowledge Graph\u00a0 \u00a0 \u00a0 \u00a0 # Search API](https://developers.google.com/knowledge-graph/).\u00a0 \u00a0 \u00a0 \u00a0 print(\"Entity Id : {}\".format(entity.entity_id))\u00a0 \u00a0 \u00a0 \u00a0 print(\"Description : {}\".format(entity.description))\u00a0 \u00a0 \u00a0 \u00a0 # All logo tracks where the recognized logo appears. Each track corresponds\u00a0 \u00a0 \u00a0 \u00a0 # to one logo instance appearing in consecutive frames.\u00a0 \u00a0 \u00a0 \u00a0 for track in logo_recognition_annotation.tracks:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Video segment of a track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.start_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.start_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.end_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.end_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tConfidence : {}\".format(track.confidence))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # The object with timestamp and attributes per frame in the track.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for timestamped_object in track.timestamped_objects:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Normalized Bounding box in a frame, where the object is located.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 normalized_bounding_box = timestamped_object.normalized_bounding_box\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\n\\t\\tLeft : {}\".format(normalized_bounding_box.left))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tTop : {}\".format(normalized_bounding_box.top))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tRight : {}\".format(normalized_bounding_box.right))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tBottom : {}\".format(normalized_bounding_box.bottom))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Optional. The attributes of the object in the bounding box.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for attribute in timestamped_object.attributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\n\\t\\t\\tName : {}\".format(attribute.name))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\t\\tConfidence : {}\".format(attribute.confidence))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\t\\tValue : {}\".format(attribute.value))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Optional. Attributes in the track level.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for track_attribute in track.attributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\n\\t\\tName : {}\".format(track_attribute.name))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tConfidence : {}\".format(track_attribute.confidence))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\t\\tValue : {}\".format(track_attribute.value))\u00a0 \u00a0 \u00a0 \u00a0 # All video segments where the recognized logo appears. There might be\u00a0 \u00a0 \u00a0 \u00a0 # multiple instances of the same logo class appearing in one VideoSegment.\u00a0 \u00a0 \u00a0 \u00a0 for segment in logo_recognition_annotation.segments:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\n\\tStart Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.start_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.start_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd Time Offset : {}.{}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.end_time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.end_time_offset.microseconds * 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\n```No preface\n **C#** \uff1a\u8acb\u6309\u7167\u201c\u5ba2\u6236\u7aef\u5eab\u201d\u9801\u9762\u4e0a\u7684 [C# \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u9032\u884c\u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [\u9069\u7528\u65bc .NET \u7684 Video Intelligence \u53c3\u8003\u6587\u6a94](https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.VideoIntelligence.V1/index.html) \u3002\n **PHP** \uff1a\u8acb\u6309\u7167\u5ba2\u6236\u7aef\u5eab\u9801\u9762\u4e0a\u7684 [PHP \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [PHP \u7248 Video Intelligence \u53c3\u8003\u6587\u6a94](https://cloud.google.com/php/docs/reference/cloud-videointelligence/latest?hl=zh-cn) \u3002\n **Ruby** \uff1a\u8acb\u6309\u7167\u5ba2\u6236\u7aef\u5eab\u9801\u9762\u4e0a\u7684 [Ruby \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [Ruby \u7248 Video Intelligence \u53c3\u8003\u6587\u6a94](https://googleapis.dev/ruby/google-cloud-video_intelligence/latest/Google/Cloud/VideoIntelligence/V1.html) \u3002", "guide": "Cloud Video Intelligence API"}