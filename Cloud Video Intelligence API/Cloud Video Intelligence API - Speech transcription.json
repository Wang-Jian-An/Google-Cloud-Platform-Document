{"title": "Cloud Video Intelligence API - Speech transcription", "url": "https://cloud.google.com/video-intelligence/docs/feature-speech-transcription", "abstract": "# Cloud Video Intelligence API - Speech transcription\ntranscribes spoken audio in a video or video segment into text and returns blocks of text for each portion of the transcribed audio.\n", "content": "## Supported models\nThe Video Intelligence only supports English (US). For other languages, use the Speech-to-Text API, which supports all available languages. For the list of available languages, see [Languagesupport](/speech-to-text/docs/speech-to-text-supported-languages) in the Speech-to-Text documentation.\nTo transcribe speech from a video, call the [annotate](/video-intelligence/docs/reference/rest/v1/videos/annotate) method and specify [SPEECH_TRANSCRIPTION](/video-intelligence/docs/reference/rest/v1/videos#Feature) in the `features` field.\nYou can use the following features when transcribing speech:\n- **Alternative words** : Use the `maxAlternatives` option to specify the maximum number of options for recognized text translations to include in the response. This value can be an integer from 1 to 30. The default is 1. The API returns multiple transcriptions in descending order based on the confidence value for the transcription. Alternative transcriptions do not include word-level entries.\n- **Profanity filtering** : Use the `filterProfanity` option to filter out known profanities in transcriptions. Matched words are replaced with the leading character of the word followed by asterisks. The default is false.\n- **Transcription hints** : Use the `speechContexts` option to provide common or unusual phrases in your audio. Those phrases are then used to assist the transcription service to create more accurate transcriptions. You provide a transcription hint as a [SpeechContext](/video-intelligence/docs/reference/rest/v1/videos#SpeechContext) object.\n- **Audio track selection** : Use the `audioTracks` option to specify which track to transcribe from multi-track video. Users can specify up to two tracks. Default is 0. Once the language code is set to en-US, the request is routed to the enhanced mode, which is trained on en-US audio; it does not really en-US or any other languages per se. If we feed a Spanish audio into the enhanced model, transcription will run its course but there may be outputs with low confidence scores, or no output at all \u2013 which is what is expected of a good model.\n- **Automatic punctuation** : Use the `enableAutomaticPunctuation` option to include punctuation in the transcribed text. The default is false.\n- **Multiple speakers** : Use the `enableSpeakerDiarization` option to identify different speakers in a video. In the response, each recognized word includes a `speakerTag` field that identifies which speaker the recognized word is attributed to.\nFor best results, provide audio recorded at 16,000Hz or greater sampling rate.\nCheck out the [Video Intelligence API visualizer](https://zackakil.github.io/video-intelligence-api-visualiser/#Speech%20Transcription) to see this feature in action.\nFor examples of requesting speech transcription, see [Speech Transcription](/video-intelligence/docs/transcription) .", "guide": "Cloud Video Intelligence API"}