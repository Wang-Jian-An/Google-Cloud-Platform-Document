{"title": "Cloud Video Intelligence API - Detect faces", "url": "https://cloud.google.com/video-intelligence/docs/face-detection", "abstract": "# Cloud Video Intelligence API - Detect faces\nThe Video Intelligence API **Face detection** feature looks for faces in a video.\n**Note:** See the [Face detection](/video-intelligence/docs/feature-face-detection) concept page for more background on this feature.\n", "content": "## Face detection from a file in Cloud Storage\nThe following samples demonstrate face detection on a file located in Cloud Storage.\n### Send video annotation requestThe following shows how to send a POST request to the [videos:annotate](/video-intelligence/docs/reference/rest/v1/videos/annotate) method. The example uses the Google Cloud CLI to create an access token. For instructions on installing the gcloud CLI, see the [Video Intelligence API Quickstart](https://cloud.google.com/video-intelligence/docs/quickstarts) .\nBefore using any of the request data, make the following replacements:- : a Cloud Storage bucket that contains the file you want to  annotate, including the file name. Must start with gs://.For example:  `\"inputUri\": \"gs://cloud-samples-data/video/googlework_short.mp4\"`\n- : The numeric identifier for your Google Cloud project\nHTTP method and URL:\n```\nPOST https://videointelligence.googleapis.com/v1/videos:annotate\n```\nRequest JSON body:\n```\n{\n \"inputUri\": \"INPUT_URI\",\n \"features\": [\"FACE_DETECTION\"]\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:If the response is successful, the Video Intelligence API returns the `name` for your operation. The above shows an example of such a response, where:- : the number of your project\n- : the Cloud region where annotation should take  place. Supported cloud regions are:`us-east1`,`us-west1`,`europe-west1`,`asia-east1`. If no region is  specified, a region is selected based on video file location.\n- : the ID of the long running operation created  for the request and provided in the response when you started the  operation, for example`12345...`\n### Get annotation resultsTo retrieve the result of the operation, make a [GET](/video-intelligence/docs/reference/rest/v1/projects.locations.operations/get) request, using the operation name returned from the call to [videos:annotate](/video-intelligence/docs/reference/rest/v1/videos/annotate) , as shown in the following example.\nBefore using any of the request data, make the following replacements:- : the name of the operation as returned by Video Intelligence API. The operation name has the format`projects/` `` `/locations/` `` `/operations/` ``\n- : The numeric identifier for your Google Cloud project\nHTTP method and URL:\n```\nGET https://videointelligence.googleapis.com/v1/OPERATION_NAME\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:Face detection annotations are returned as a `faceAnnotations` list. Note: The **done** field is only returned when its value is **True** . It's not included in responses for which the operation has not completed.\nTo authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/video/src/main/java/video/DetectFacesGcs.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.videointelligence.v1.AnnotateVideoProgress;import com.google.cloud.videointelligence.v1.AnnotateVideoRequest;import com.google.cloud.videointelligence.v1.AnnotateVideoResponse;import com.google.cloud.videointelligence.v1.DetectedAttribute;import com.google.cloud.videointelligence.v1.FaceDetectionAnnotation;import com.google.cloud.videointelligence.v1.FaceDetectionConfig;import com.google.cloud.videointelligence.v1.Feature;import com.google.cloud.videointelligence.v1.TimestampedObject;import com.google.cloud.videointelligence.v1.Track;import com.google.cloud.videointelligence.v1.VideoAnnotationResults;import com.google.cloud.videointelligence.v1.VideoContext;import com.google.cloud.videointelligence.v1.VideoIntelligenceServiceClient;import com.google.cloud.videointelligence.v1.VideoSegment;public class DetectFacesGcs {\u00a0 public static void detectFacesGcs() throws Exception {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String gcsUri = \"gs://cloud-samples-data/video/googlework_short.mp4\";\u00a0 \u00a0 detectFacesGcs(gcsUri);\u00a0 }\u00a0 // Detects faces in a video stored in Google Cloud Storage using the Cloud Video Intelligence API.\u00a0 public static void detectFacesGcs(String gcsUri) throws Exception {\u00a0 \u00a0 try (VideoIntelligenceServiceClient videoIntelligenceServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 VideoIntelligenceServiceClient.create()) {\u00a0 \u00a0 \u00a0 FaceDetectionConfig faceDetectionConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 FaceDetectionConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Must set includeBoundingBoxes to true to get facial attributes.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setIncludeBoundingBoxes(true)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setIncludeAttributes(true)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 VideoContext videoContext =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 VideoContext.newBuilder().setFaceDetectionConfig(faceDetectionConfig).build();\u00a0 \u00a0 \u00a0 AnnotateVideoRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AnnotateVideoRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputUri(gcsUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addFeatures(Feature.FACE_DETECTION)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setVideoContext(videoContext)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // Detects faces in a video\u00a0 \u00a0 \u00a0 OperationFuture<AnnotateVideoResponse, AnnotateVideoProgress> future =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 videoIntelligenceServiceClient.annotateVideoAsync(request);\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to complete...\");\u00a0 \u00a0 \u00a0 AnnotateVideoResponse response = future.get();\u00a0 \u00a0 \u00a0 // Gets annotations for video\u00a0 \u00a0 \u00a0 VideoAnnotationResults annotationResult = response.getAnnotationResultsList().get(0);\u00a0 \u00a0 \u00a0 // Annotations for list of people detected, tracked and recognized in video.\u00a0 \u00a0 \u00a0 for (FaceDetectionAnnotation faceDetectionAnnotation :\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 annotationResult.getFaceDetectionAnnotationsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.print(\"Face detected:\\n\");\u00a0 \u00a0 \u00a0 \u00a0 for (Track track : faceDetectionAnnotation.getTracksList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 VideoSegment segment = track.getSegment();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tStart: %d.%.0fs\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getStartTimeOffset().getSeconds(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getStartTimeOffset().getNanos() / 1e6);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd: %d.%.0fs\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getEndTimeOffset().getSeconds(), segment.getEndTimeOffset().getNanos() / 1e6);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Each segment includes timestamped objects that\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // include characteristics of the face detected.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TimestampedObject firstTimestampedObject = track.getTimestampedObjects(0);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (DetectedAttribute attribute : firstTimestampedObject.getAttributesList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Attributes include glasses, headwear, smiling, direction of gaze\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tAttribute %s: %s %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 attribute.getName(), attribute.getValue(), attribute.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/video-intelligence/analyze-face-detection-gcs.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*/// const gcsUri = 'GCS URI of the video to analyze, e.g. gs://my-bucket/my-video.mp4';// Imports the Google Cloud Video Intelligence library + Node's fs libraryconst Video = require('@google-cloud/video-intelligence').v1;// Creates a clientconst video = new Video.VideoIntelligenceServiceClient();async function detectFacesGCS() {\u00a0 const request = {\u00a0 \u00a0 inputUri: gcsUri,\u00a0 \u00a0 features: ['FACE_DETECTION'],\u00a0 \u00a0 videoContext: {\u00a0 \u00a0 \u00a0 faceDetectionConfig: {\u00a0 \u00a0 \u00a0 \u00a0 // Must set includeBoundingBoxes to true to get facial attributes.\u00a0 \u00a0 \u00a0 \u00a0 includeBoundingBoxes: true,\u00a0 \u00a0 \u00a0 \u00a0 includeAttributes: true,\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 };\u00a0 // Detects faces in a video\u00a0 // We get the first result because we only process 1 video\u00a0 const [operation] = await video.annotateVideo(request);\u00a0 const results = await operation.promise();\u00a0 console.log('Waiting for operation to complete...');\u00a0 // Gets annotations for video\u00a0 const faceAnnotations =\u00a0 \u00a0 results[0].annotationResults[0].faceDetectionAnnotations;\u00a0 for (const {tracks} of faceAnnotations) {\u00a0 \u00a0 console.log('Face detected:');\u00a0 \u00a0 for (const {segment, timestampedObjects} of tracks) {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tStart: ${segment.startTimeOffset.seconds}.` +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 `${(segment.startTimeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tEnd: ${segment.endTimeOffset.seconds}.` +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 `${(segment.endTimeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 // Each segment includes timestamped objects that\u00a0 \u00a0 \u00a0 // include characteristics of the face detected.\u00a0 \u00a0 \u00a0 const [firstTimestapedObject] = timestampedObjects;\u00a0 \u00a0 \u00a0 for (const {name} of firstTimestapedObject.attributes) {\u00a0 \u00a0 \u00a0 \u00a0 // Attributes include 'glasses', 'headwear', 'smiling'.\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\tAttribute: ${name}; `);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}detectFacesGCS();\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/video_detect_faces_gcs.py) \n```\nfrom google.cloud import videointelligence_v1 as videointelligencedef detect_faces(gcs_uri=\"gs://YOUR_BUCKET_ID/path/to/your/video.mp4\"):\u00a0 \u00a0 \"\"\"Detects faces in a video.\"\"\"\u00a0 \u00a0 client = videointelligence.VideoIntelligenceServiceClient()\u00a0 \u00a0 # Configure the request\u00a0 \u00a0 config = videointelligence.FaceDetectionConfig(\u00a0 \u00a0 \u00a0 \u00a0 include_bounding_boxes=True, include_attributes=True\u00a0 \u00a0 )\u00a0 \u00a0 context = videointelligence.VideoContext(face_detection_config=config)\u00a0 \u00a0 # Start the asynchronous request\u00a0 \u00a0 operation = client.annotate_video(\u00a0 \u00a0 \u00a0 \u00a0 request={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"features\": [videointelligence.Feature.FACE_DETECTION],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"input_uri\": gcs_uri,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"video_context\": context,\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 )\u00a0 \u00a0 print(\"\\nProcessing video for face detection annotations.\")\u00a0 \u00a0 result = operation.result(timeout=300)\u00a0 \u00a0 print(\"\\nFinished processing.\\n\")\u00a0 \u00a0 # Retrieve the first result, because a single video was processed.\u00a0 \u00a0 annotation_result = result.annotation_results[0]\u00a0 \u00a0 for annotation in annotation_result.face_detection_annotations:\u00a0 \u00a0 \u00a0 \u00a0 print(\"Face detected:\")\u00a0 \u00a0 \u00a0 \u00a0 for track in annotation.tracks:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Segment: {}s to {}s\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.start_time_offset.seconds\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + track.segment.start_time_offset.microseconds / 1e6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.end_time_offset.seconds\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + track.segment.end_time_offset.microseconds / 1e6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Each segment includes timestamped faces that include\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # characteristics of the face detected.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Grab the first timestamped face\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timestamped_object = track.timestamped_objects[0]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 box = timestamped_object.normalized_bounding_box\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"Bounding box:\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tleft \u00a0: {}\".format(box.left))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\ttop \u00a0 : {}\".format(box.top))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tright : {}\".format(box.right))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tbottom: {}\".format(box.bottom))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Attributes include glasses, headwear, smiling, direction of gaze\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"Attributes:\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for attribute in timestamped_object.attributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t{}:{} {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 attribute.name, attribute.value, attribute.confidence\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\n```No preface\n **C#** : Please follow the [C# setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for .NET.](https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.VideoIntelligence.V1/index.html) \n **PHP** : Please follow the [PHP setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for PHP.](/php/docs/reference/cloud-videointelligence/latest) \n **Ruby** : Please follow the [Ruby setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for Ruby.](https://googleapis.dev/ruby/google-cloud-video_intelligence/latest/Google/Cloud/VideoIntelligence/V1.html)\n## Face detection from a local file\nThe following example uses face detection to find entities in a video from a video file uploaded from your local machine.\n### Send the process requestTo perform face detection on a local video file, base64-encode the contents of the video file. For information on how to base64-encode the contents of a video file, see [Base64 Encoding](/video-intelligence/docs/base64) . Then, make a POST request to the [videos:annotate](/video-intelligence/docs/reference/rest/v1/videos/annotate) method. Include the base64-encoded contents in the `inputContent` field of the request and specify the `FACE_DETECTION` feature.\nThe following is an example of a POST request using curl. The example uses the Google Cloud CLI to create an access token. For instructions on installing the gcloud CLI, see the [Video Intelligence API Quickstart](/video-intelligence/docs/annotate-video-command-line) \nBefore using any of the request data, make the following replacements:- Local video file in binary formatFor example: 'AAAAGGZ0eXBtcDQyAAAAAGlzb21tcDQyAAGVYW1vb3YAAABsbXZoZAAAAADWvhlR1r4ZUQABX5ABCOxo AAEAAAEAAAAAAA4...'\n- : The numeric identifier for your Google Cloud project\nHTTP method and URL:\n```\nPOST https://videointelligence.googleapis.com/v1/videos:annotate\n```\nRequest JSON body:\n```\n{\n inputContent: \"Local video file in binary format\",\n \"features\": [\"FACE_DETECTION\"]\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:If the request is successful, Video Intelligence the `name` for your operation. The above shows an example of such a response, where `project-number` is the number of your project and `operation-id` is the ID of the long-running operation created for the request.\n`{ \"name\": \"us-west1.17122464255125931980\" }`\n### Get the resultsTo retrieve the result of the operation, make a GET request to the [operations](/video-intelligence/docs/reference/rest/v1/operations/get) endpoint and specify the name of your operation.\nBefore using any of the request data, make the following replacements:- : the name of the operation as returned by Video Intelligence API. The operation name has the format`projects/` `` `/locations/` `` `/operations/` ``\n- : The numeric identifier for your Google Cloud project\nHTTP method and URL:\n```\nGET https://videointelligence.googleapis.com/v1/OPERATION_NAME\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\nTo authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/video/src/main/java/video/DetectFaces.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.videointelligence.v1.AnnotateVideoProgress;import com.google.cloud.videointelligence.v1.AnnotateVideoRequest;import com.google.cloud.videointelligence.v1.AnnotateVideoResponse;import com.google.cloud.videointelligence.v1.DetectedAttribute;import com.google.cloud.videointelligence.v1.FaceDetectionAnnotation;import com.google.cloud.videointelligence.v1.FaceDetectionConfig;import com.google.cloud.videointelligence.v1.Feature;import com.google.cloud.videointelligence.v1.TimestampedObject;import com.google.cloud.videointelligence.v1.Track;import com.google.cloud.videointelligence.v1.VideoAnnotationResults;import com.google.cloud.videointelligence.v1.VideoContext;import com.google.cloud.videointelligence.v1.VideoIntelligenceServiceClient;import com.google.cloud.videointelligence.v1.VideoSegment;import com.google.protobuf.ByteString;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;public class DetectFaces {\u00a0 public static void detectFaces() throws Exception {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String localFilePath = \"resources/googlework_short.mp4\";\u00a0 \u00a0 detectFaces(localFilePath);\u00a0 }\u00a0 // Detects faces in a video stored in a local file using the Cloud Video Intelligence API.\u00a0 public static void detectFaces(String localFilePath) throws Exception {\u00a0 \u00a0 try (VideoIntelligenceServiceClient videoIntelligenceServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 VideoIntelligenceServiceClient.create()) {\u00a0 \u00a0 \u00a0 // Reads a local video file and converts it to base64.\u00a0 \u00a0 \u00a0 Path path = Paths.get(localFilePath);\u00a0 \u00a0 \u00a0 byte[] data = Files.readAllBytes(path);\u00a0 \u00a0 \u00a0 ByteString inputContent = ByteString.copyFrom(data);\u00a0 \u00a0 \u00a0 FaceDetectionConfig faceDetectionConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 FaceDetectionConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Must set includeBoundingBoxes to true to get facial attributes.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setIncludeBoundingBoxes(true)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setIncludeAttributes(true)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 VideoContext videoContext =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 VideoContext.newBuilder().setFaceDetectionConfig(faceDetectionConfig).build();\u00a0 \u00a0 \u00a0 AnnotateVideoRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AnnotateVideoRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputContent(inputContent)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addFeatures(Feature.FACE_DETECTION)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setVideoContext(videoContext)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // Detects faces in a video\u00a0 \u00a0 \u00a0 OperationFuture<AnnotateVideoResponse, AnnotateVideoProgress> future =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 videoIntelligenceServiceClient.annotateVideoAsync(request);\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to complete...\");\u00a0 \u00a0 \u00a0 AnnotateVideoResponse response = future.get();\u00a0 \u00a0 \u00a0 // Gets annotations for video\u00a0 \u00a0 \u00a0 VideoAnnotationResults annotationResult = response.getAnnotationResultsList().get(0);\u00a0 \u00a0 \u00a0 // Annotations for list of faces detected, tracked and recognized in video.\u00a0 \u00a0 \u00a0 for (FaceDetectionAnnotation faceDetectionAnnotation :\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 annotationResult.getFaceDetectionAnnotationsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.print(\"Face detected:\\n\");\u00a0 \u00a0 \u00a0 \u00a0 for (Track track : faceDetectionAnnotation.getTracksList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 VideoSegment segment = track.getSegment();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tStart: %d.%.0fs\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getStartTimeOffset().getSeconds(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getStartTimeOffset().getNanos() / 1e6);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd: %d.%.0fs\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getEndTimeOffset().getSeconds(), segment.getEndTimeOffset().getNanos() / 1e6);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Each segment includes timestamped objects that\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // include characteristics of the face detected.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TimestampedObject firstTimestampedObject = track.getTimestampedObjects(0);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (DetectedAttribute attribute : firstTimestampedObject.getAttributesList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Attributes include glasses, headwear, smiling, direction of gaze\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tAttribute %s: %s %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 attribute.getName(), attribute.getValue(), attribute.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/video-intelligence/analyze-face-detection.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*/// const path = 'Local file to analyze, e.g. ./my-file.mp4';// Imports the Google Cloud Video Intelligence library + Node's fs libraryconst Video = require('@google-cloud/video-intelligence').v1;const fs = require('fs');// Creates a clientconst video = new Video.VideoIntelligenceServiceClient();// Reads a local video file and converts it to base64const file = fs.readFileSync(path);const inputContent = file.toString('base64');async function detectFaces() {\u00a0 const request = {\u00a0 \u00a0 inputContent: inputContent,\u00a0 \u00a0 features: ['FACE_DETECTION'],\u00a0 \u00a0 videoContext: {\u00a0 \u00a0 \u00a0 faceDetectionConfig: {\u00a0 \u00a0 \u00a0 \u00a0 // Must set includeBoundingBoxes to true to get facial attributes.\u00a0 \u00a0 \u00a0 \u00a0 includeBoundingBoxes: true,\u00a0 \u00a0 \u00a0 \u00a0 includeAttributes: true,\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 };\u00a0 // Detects faces in a video\u00a0 // We get the first result because we only process 1 video\u00a0 const [operation] = await video.annotateVideo(request);\u00a0 const results = await operation.promise();\u00a0 console.log('Waiting for operation to complete...');\u00a0 // Gets annotations for video\u00a0 const faceAnnotations =\u00a0 \u00a0 results[0].annotationResults[0].faceDetectionAnnotations;\u00a0 for (const {tracks} of faceAnnotations) {\u00a0 \u00a0 console.log('Face detected:');\u00a0 \u00a0 for (const {segment, timestampedObjects} of tracks) {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tStart: ${segment.startTimeOffset.seconds}` +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 `.${(segment.startTimeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tEnd: ${segment.endTimeOffset.seconds}.` +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 `${(segment.endTimeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 // Each segment includes timestamped objects that\u00a0 \u00a0 \u00a0 // include characteristics of the face detected.\u00a0 \u00a0 \u00a0 const [firstTimestapedObject] = timestampedObjects;\u00a0 \u00a0 \u00a0 for (const {name} of firstTimestapedObject.attributes) {\u00a0 \u00a0 \u00a0 \u00a0 // Attributes include 'glasses', 'headwear', 'smiling'.\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\tAttribute: ${name}; `);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}detectFaces();\n```To authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/video_detect_faces.py) \n```\nimport iofrom google.cloud import videointelligence_v1 as videointelligencedef detect_faces(local_file_path=\"path/to/your/video-file.mp4\"):\u00a0 \u00a0 \"\"\"Detects faces in a video from a local file.\"\"\"\u00a0 \u00a0 client = videointelligence.VideoIntelligenceServiceClient()\u00a0 \u00a0 with io.open(local_file_path, \"rb\") as f:\u00a0 \u00a0 \u00a0 \u00a0 input_content = f.read()\u00a0 \u00a0 # Configure the request\u00a0 \u00a0 config = videointelligence.FaceDetectionConfig(\u00a0 \u00a0 \u00a0 \u00a0 include_bounding_boxes=True, include_attributes=True\u00a0 \u00a0 )\u00a0 \u00a0 context = videointelligence.VideoContext(face_detection_config=config)\u00a0 \u00a0 # Start the asynchronous request\u00a0 \u00a0 operation = client.annotate_video(\u00a0 \u00a0 \u00a0 \u00a0 request={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"features\": [videointelligence.Feature.FACE_DETECTION],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"input_content\": input_content,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"video_context\": context,\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 )\u00a0 \u00a0 print(\"\\nProcessing video for face detection annotations.\")\u00a0 \u00a0 result = operation.result(timeout=300)\u00a0 \u00a0 print(\"\\nFinished processing.\\n\")\u00a0 \u00a0 # Retrieve the first result, because a single video was processed.\u00a0 \u00a0 annotation_result = result.annotation_results[0]\u00a0 \u00a0 for annotation in annotation_result.face_detection_annotations:\u00a0 \u00a0 \u00a0 \u00a0 print(\"Face detected:\")\u00a0 \u00a0 \u00a0 \u00a0 for track in annotation.tracks:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Segment: {}s to {}s\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.start_time_offset.seconds\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + track.segment.start_time_offset.microseconds / 1e6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.end_time_offset.seconds\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + track.segment.end_time_offset.microseconds / 1e6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Each segment includes timestamped faces that include\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # characteristics of the face detected.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Grab the first timestamped face\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timestamped_object = track.timestamped_objects[0]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 box = timestamped_object.normalized_bounding_box\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"Bounding box:\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tleft \u00a0: {}\".format(box.left))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\ttop \u00a0 : {}\".format(box.top))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tright : {}\".format(box.right))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tbottom: {}\".format(box.bottom))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Attributes include glasses, headwear, smiling, direction of gaze\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"Attributes:\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for attribute in timestamped_object.attributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t{}:{} {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 attribute.name, attribute.value, attribute.confidence\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\n```No preface\n **C#** : Please follow the [C# setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for .NET.](https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.VideoIntelligence.V1/index.html) \n **PHP** : Please follow the [PHP setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for PHP.](/php/docs/reference/cloud-videointelligence/latest) \n **Ruby** : Please follow the [Ruby setup instructions](/video-intelligence/docs/libraries) on the client libraries page  and then visit the [Video Intelligence reference documentation for Ruby.](https://googleapis.dev/ruby/google-cloud-video_intelligence/latest/Google/Cloud/VideoIntelligence/V1.html)", "guide": "Cloud Video Intelligence API"}