{"title": "Cloud Video Intelligence API - Action recognition", "url": "https://cloud.google.com/video-intelligence/docs/streaming/action-recognition", "abstract": "# Cloud Video Intelligence API - Action recognition\n**    Beta     ** This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nAction recognition identifies different actions from video clips, such as walking or dancing. Each of the actions may or may not be performed throughout the entire duration of the video.\n", "content": "## Using an AutoML model\n### Before you begin\nFor background on creating an AutoML model, check out the Vertex AI [Beginner's guide](/vertex-ai/docs/beginner/beginners-guide#video) . For instructions on how to create your AutoML model, see [Video data](/vertex-ai/docs/training-overview#video_data) in \"Develop and use ML models\" in the Vertex AI documentation.\n### Use your AutoML model\nThe following code sample demonstrates how to use your [AutoML model](/vertex-ai/docs/beginner/beginners-guide#video) for action recognition using the streaming client library.\nTo authenticate to Video Intelligence, set up Application Default Credentials.  For more information, see [Set up authentication for a local development environment](/docs/authentication/provide-credentials-adc#local-dev) .\n [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/beta_snippets.py) \n```\nimport iofrom google.cloud import videointelligence_v1p3beta1 as videointelligence# path = 'path_to_file'# project_id = 'project_id'# model_id = 'automl_action_recognition_model_id'client = videointelligence.StreamingVideoIntelligenceServiceClient()model_path = \"projects/{}/locations/us-central1/models/{}\".format(\u00a0 \u00a0 project_id, model_id)automl_config = videointelligence.StreamingAutomlActionRecognitionConfig(\u00a0 \u00a0 model_name=model_path)video_config = videointelligence.StreamingVideoConfig(\u00a0 \u00a0 feature=videointelligence.StreamingFeature.STREAMING_AUTOML_ACTION_RECOGNITION,\u00a0 \u00a0 automl_action_recognition_config=automl_config,)# config_request should be the first in the stream of requests.config_request = videointelligence.StreamingAnnotateVideoRequest(\u00a0 \u00a0 video_config=video_config)# Set the chunk size to 5MB (recommended less than 10MB).chunk_size = 5 * 1024 * 1024def stream_generator():\u00a0 \u00a0 yield config_request\u00a0 \u00a0 # Load file content.\u00a0 \u00a0 # Note: Input videos must have supported video codecs. See\u00a0 \u00a0 # https://cloud.google.com/video-intelligence/docs/streaming/streaming#supported_video_codecs\u00a0 \u00a0 # for more details.\u00a0 \u00a0 with io.open(path, \"rb\") as video_file:\u00a0 \u00a0 \u00a0 \u00a0 while True:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 data = video_file.read(chunk_size)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if not data:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 yield videointelligence.StreamingAnnotateVideoRequest(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 input_content=data\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )requests = stream_generator()# streaming_annotate_video returns a generator.# The default timeout is about 300 seconds.# To process longer videos it should be set to# larger than the length (in seconds) of the video.responses = client.streaming_annotate_video(requests, timeout=900)# Each response corresponds to about 1 second of video.for response in responses:\u00a0 \u00a0 # Check for errors.\u00a0 \u00a0 if response.error.message:\u00a0 \u00a0 \u00a0 \u00a0 print(response.error.message)\u00a0 \u00a0 \u00a0 \u00a0 break\u00a0 \u00a0 for label in response.annotation_results.label_annotations:\u00a0 \u00a0 \u00a0 \u00a0 for frame in label.frames:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"At {:3d}s segment, {:5.1%} {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frame.time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frame.confidence,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 label.entity.entity_id,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\n```", "guide": "Cloud Video Intelligence API"}