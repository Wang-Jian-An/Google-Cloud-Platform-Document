{"title": "Cloud Video Intelligence API - Label Detection Tutorial", "url": "https://cloud.google.com/video-intelligence/docs/label-tutorial", "abstract": "# Cloud Video Intelligence API - Label Detection Tutorial\n", "content": "## Audience\nThis tutorial is designed to help you start exploring and developing applications with the Video Intelligence API. It's designed for people with basic familiarity with Python. You should also be able to follow along with limited programming knowledge. Having walked through this tutorial, you should be able to use the [Reference documentation](/video-intelligence/docs/reference/rest) to create your own basic applications.\nThis tutorial steps through a Video Intelligence API application using Python code. The purpose here is not to explain the Python client libraries, but to explain how to make calls to the Video Intelligence API using the video label detection feature. Applications in Java and Node.js are essentially similar.\n## Prerequisites\nThis tutorial has the following prerequisites:\n- You've set up a [Video Intelligence API project](/video-intelligence/docs/before-you-begin) in the Google Cloud console.\n- You've set up your environment using a service account and [Application Default Credentials](/video-intelligence/docs/common/auth#set_up_a_service_account) .\n- You have basic familiarity with [Python](https://www.python.org/) programming.\n- You've set up your Python development environment.It's recommended that you have the latest version of Python,`pip`, and`virtualenv`installed on your system. For instructions, refer to the [Python Development Environment Setup Guide](/python/docs/setup) for Google Cloud.\n- You've installed the [Google Cloud client library](https://github.com/googleapis/google-cloud-python/tree/main/packages/google-cloud-videointelligence) ## Annotate a video using label detection\nThis tutorial walks you through a basic Video API application, using a `LABEL_DETECTION` request. A `LABEL_DETECTION` request annotates a video with labels (or \"tags\") that are selected based on the image content. For example, a video of a train at a crossing may produce labels such as \"train\", \"transportation\", \"railroad crossing.\"\nThe following is the entire code needed for this tutorial. Most comments have been removed from this code to highlight how brief the code is. Instead, comments are provided later as we walk through the code.\n[  videointelligence/samples/labels/labels.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/labels/labels.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/labels/labels.py)\n```\nimport argparsefrom google.cloud import videointelligencedef analyze_labels(path):\u00a0 \u00a0 \"\"\"Detects labels given a GCS path.\"\"\"\u00a0 \u00a0 video_client = videointelligence.VideoIntelligenceServiceClient()\u00a0 \u00a0 features = [videointelligence.Feature.LABEL_DETECTION]\u00a0 \u00a0 operation = video_client.annotate_video(\u00a0 \u00a0 \u00a0 \u00a0 request={\"features\": features, \"input_uri\": path}\u00a0 \u00a0 )\u00a0 \u00a0 print(\"\\nProcessing video for label annotations:\")\u00a0 \u00a0 result = operation.result(timeout=90)\u00a0 \u00a0 print(\"\\nFinished processing.\")\u00a0 \u00a0 segment_labels = result.annotation_results[0].segment_label_annotations\u00a0 \u00a0 for i, segment_label in enumerate(segment_labels):\u00a0 \u00a0 \u00a0 \u00a0 print(\"Video label description: {}\".format(segment_label.entity.description))\u00a0 \u00a0 \u00a0 \u00a0 for category_entity in segment_label.category_entities:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tLabel category description: {}\".format(category_entity.description)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 for i, segment in enumerate(segment_label.segments):\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 start_time = (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.segment.start_time_offset.seconds\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + segment.segment.start_time_offset.microseconds / 1e6\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 end_time = (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.segment.end_time_offset.seconds\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + segment.segment.end_time_offset.microseconds / 1e6\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 positions = \"{}s to {}s\".format(start_time, end_time)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 confidence = segment.confidence\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tSegment {}: {}\".format(i, positions))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tConfidence: {}\".format(confidence))\u00a0 \u00a0 \u00a0 \u00a0 print(\"\\n\")if __name__ == \"__main__\":\u00a0 \u00a0 parser = argparse.ArgumentParser(\u00a0 \u00a0 \u00a0 \u00a0 description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter\u00a0 \u00a0 )\u00a0 \u00a0 parser.add_argument(\"path\", help=\"GCS file path for label detection.\")\u00a0 \u00a0 args = parser.parse_args()\u00a0 \u00a0 analyze_labels(args.path)\n```\nThis simple application performs the following tasks:\n- Imports the libraries necessary to run the application.\n- Takes a video file stored in Cloud Storage URI as an argument and passes it to the`main()`function.\n- Gets credentials to run the Video Intelligence API service.\n- Creates a video annotation request to send to the video service.\n- Sends the request and returns a long-running operation.\n- Loops over the long-running operation until the video is processed and returns available values.\n- Parses the response for the service and displays the response to the user.## Import libraries\n[  videointelligence/samples/labels/labels.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/labels/labels.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/labels/labels.py)\n```\nimport argparsefrom google.cloud import videointelligence\n```\nSome standard libraries are imported: `argparse` to allow the application to accept input filenames as arguments and `sys` for formatting output while waiting for API responses. The package `time` is imported to run some simple wait loops.\nWhen using the Video Intelligence API, you'll also need to import the `google.cloud.videointelligence_v1` and its enumeration class, which holds the directory of our API calls.\n## Run the application\n[  videointelligence/samples/labels/labels.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/labels/labels.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/labels/labels.py)\n```\nparser = argparse.ArgumentParser(\u00a0 \u00a0 description=__doc__, formatter_class=argparse.RawDescriptionHelpFormatter)parser.add_argument(\"path\", help=\"GCS file path for label detection.\")args = parser.parse_args()analyze_labels(args.path)\n```\nHere, the passed argument is parsed for the Cloud Storage URI of the video filename and is passed to the `main()` function.\n## Authenticate to the API\nBefore communicating with the Video Intelligence API service, you need to authenticate your service using previously acquired credentials. Within an application, the simplest way to obtain credentials is to use [Application Default Credentials](https://developers.google.com/identity/protocols/application-default-credentials) (ADC). By default, ADC attempts to obtain credentials from the `GOOGLE_APPLICATION_CREDENTIALS` environment file, which should be set to point to your service account's JSON key file. (You should have set up your service account and environment to use ADC in the [Quickstart](/video-intelligence/docs/getting-started) .\n## Construct the request\n[  videointelligence/samples/labels/labels.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/labels/labels.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/labels/labels.py)\n```\nvideo_client = videointelligence.VideoIntelligenceServiceClient()features = [videointelligence.Feature.LABEL_DETECTION]operation = video_client.annotate_video(\u00a0 \u00a0 request={\"features\": features, \"input_uri\": path})\n```\nNow that the Video Intelligence API service is ready, you can construct a request to that service. Requests to the Video Intelligence API are provided as JSON objects. See the [Video Intelligence API Reference](/video-intelligence/docs/reference/rest) for complete information on the specific structure of such a request.\nThis code snippet performs the following tasks:\n- Constructs the JSON for a POST request to the`annotate_video()`method.\n- Injects the Cloud Storage location of the passed video filename into the request.\n- Indicates that the`annotate`method should perform`LABEL_DETECTION`.## Check the operation\n[  videointelligence/samples/labels/labels.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/labels/labels.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/labels/labels.py)\n```\nresult = operation.result(timeout=90)print(\"\\nFinished processing.\")\n```\nUsing the existing operation request for the existing operation, a `while` loop is constructed to periodically check the state of that operation. Once the operation has indicated that the operation is `done` , the response is parsed.\n## Parse the response\n[  videointelligence/samples/labels/labels.py ](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/labels/labels.py) [View on GitHub](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/labels/labels.py)\n```\nsegment_labels = result.annotation_results[0].segment_label_annotationsfor i, segment_label in enumerate(segment_labels):\u00a0 \u00a0 print(\"Video label description: {}\".format(segment_label.entity.description))\u00a0 \u00a0 for category_entity in segment_label.category_entities:\u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tLabel category description: {}\".format(category_entity.description)\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 for i, segment in enumerate(segment_label.segments):\u00a0 \u00a0 \u00a0 \u00a0 start_time = (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.segment.start_time_offset.seconds\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + segment.segment.start_time_offset.microseconds / 1e6\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 end_time = (\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.segment.end_time_offset.seconds\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + segment.segment.end_time_offset.microseconds / 1e6\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 positions = \"{}s to {}s\".format(start_time, end_time)\u00a0 \u00a0 \u00a0 \u00a0 confidence = segment.confidence\u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tSegment {}: {}\".format(i, positions))\u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tConfidence: {}\".format(confidence))\u00a0 \u00a0 print(\"\\n\")\n```\nOnce the operation has been completed, the response will contain the result within an [AnnotateVideoResponse](/video-intelligence/docs/reference/rest/Shared.Types/AnnotateVideoResponse) , which consists of a list of `annotationResults` , one for each video sent in the request. Because only one video was sent in the request, the first `segmentLabelAnnotations` of the results is taken, all the labels in `segmentLabelAnnotations` are looped through. By using only `segmentLabelAnnotations` , this tutorial displays only video-level annotations. Each `segment_label` includes a description ( `segment_label.description` ), a list of entity categories ( `segment_label.category_entities` ), and a list of segments identifying the start/end time of the label occurrences in the video (should be one segment spanning the whole video or video segment for the case of `segment_label_annotations` ).\n```\n{\n \"name\":\"us-west1.12089999971048628582\",\n \"metadata\":{\n  \"@type\":\"type.googleapis.com/google.cloud.videointelligence.v1.AnnotateVideoProgress\",\n  \"annotationProgress\":[   {\n   \"inputUri\":\"gs://YOUR_BUCKET/YOUR_OBJECT\",\n   \"updateTime\":\"2020-01-31T01:49:52.498015Z\",\n   \"startTime\":\"2020-01-31T01:49:43.056481Z\"\n   }\n  ]\n },\n \"done\": true,\n \"response\":{\n  \"@type\":\"type.googleapis.com/google.cloud.videointelligence.v1.AnnotateVideoResponse\",\n  \"annotationResults\":[   {\n   \"inputUri\":\"gs://YOUR_BUCKET/YOUR_OBJECT\",\n   \"segmentLabelAnnotations\": [    {\n    \"entity\": {\n     \"entityId\": \"/m/01yrx\",\n     \"languageCode\": \"en-US\"\n    },\n    \"segments\": [     {\n     \"segment\": {\n      \"startTimeOffset\": \"0s\",\n      \"endTimeOffset\": \"14.833664s\"\n     },\n     \"confidence\": 0.98509187\n     }\n    ]\n    },\n    ...\n   ]\n   }\n  ]\n }\n}\n```\nBecause only one video was sent in the request, the first `description` of the first result is printed.\n## Run your application\nTo run your application, simply pass it the Cloud Storage URI of a video:\n```\n$ python labels.py gs://YOUR_BUCKET/YOUR_OBJECT\nOperation us-west1.4757250774497581229 started: 2020-01-30T01:46:30.158989Z\nOperation processing ...\nThe video has been successfully processed.\nVideo label description: urban area\n  Label category description: city\n  Segment 0: 0.0s to 38.752016s\n  Confidence: 0.946980476379\nVideo label description: traffic\n  Segment 0: 0.0s to 38.752016s\n  Confidence: 0.94105899334\nVideo label description: vehicle\n  Segment 0: 0.0s to 38.752016s\n  Confidence: 0.919958174229\n...\n \n```\n## Output\nBelow is an example of a possible output.\n```\nProcessing video for label annotations:\nFinished processing.\nVideo label description: crowd\n  Label category description: people\n  Segment 0: 0.0s to 60.24s\n  Confidence: 0.527720749378\nVideo label description: official\n  Label category description: person\n  Segment 0: 0.0s to 60.24s\n  Confidence: 0.372822880745\nVideo label description: audience\n  Label category description: people\n  Segment 0: 0.0s to 60.24s\n  Confidence: 0.501719772816\nVideo label description: news\n  Segment 0: 0.0s to 60.24s\n  Confidence: 0.867252230644\nVideo label description: people\n  Label category description: person\n  Segment 0: 0.0s to 60.24s\n  Confidence: 0.46747264266\nVideo label description: politics\n  Segment 0: 0.0s to 60.24s\n  Confidence: 0.319397002459\n```\nCongratulations! You've performed an annotation task using the Video Intelligence API!", "guide": "Cloud Video Intelligence API"}