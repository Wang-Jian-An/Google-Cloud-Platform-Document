{"title": "Cloud Video Intelligence API - \u52d5\u4f5c\u8b58\u5225", "url": "https://cloud.google.com/video-intelligence/docs/streaming/action-recognition?hl=zh-cn", "abstract": "# Cloud Video Intelligence API - \u52d5\u4f5c\u8b58\u5225\n**    Beta \u7248     ** \u6b64\u529f\u80fd\u9808\u9075\u5b88 [\u670d\u52d9\u5c08\u7528\u689d\u6b3e](https://cloud.google.com/terms/service-terms?hl=zh-cn#1) \u7684\u201c\u901a\u7528\u670d\u52d9\u689d\u6b3e\u201d\u90e8\u5206\u4e2d\u7684\u201c\u975e\u6b63\u5f0f\u7248\u7522\u54c1\u689d\u6b3e\u201d\u3002 \u975e\u6b63\u5f0f\u7248\u529f\u80fd\u201c\u6309\u539f\u6a23\u201d\u63d0\u4f9b\uff0c\u53ef\u80fd\u53ea\u80fd\u7372\u5f97\u6709\u9650\u7684\u652f\u6301\u3002 \u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u767c\u4f48\u968e\u6bb5\u8aaa\u660e](https://cloud.google.com/products?hl=zh-cn#product-launch-stages) \u3002\n\u52d5\u4f5c\u8b58\u5225\u53ef\u8b58\u5225\u8207\u8996\u983b\u526a\u8f2f\u4e0d\u540c\u7684\u52d5\u4f5c\uff0c\u4f8b\u5982\u6b65\u884c\u6216\u8df3\u821e\u3002\u6574\u500b\u8996\u983b\u6301\u7e8c\u6642\u9593\u5167\u4e0d\u4e00\u5b9a\u6bcf\u500b\u52d5\u4f5c\u90fd\u57f7\u884c\u3002\n", "content": "## \u4f7f\u7528 AutoML \u6a21\u578b\n### \u6e96\u5099\u5de5\u4f5c\n\u5982\u9700\u77ad\u89e3\u5275\u5efa AutoML \u6a21\u578b\u7684\u80cc\u666f\u4fe1\u606f\uff0c\u8acb\u67e5\u770b Vertex AI [\u65b0\u624b\u6307\u5357](https://cloud.google.com/vertex-ai/docs/beginner/beginners-guide?hl=zh-cn#video) \u3002\u5982\u9700\u77ad\u89e3\u5982\u4f55\u5275\u5efa AutoML \u6a21\u578b\uff0c\u8acb\u53c3\u95b1 Vertex AI \u6587\u6a94\u201c\u958b\u767c\u548c\u4f7f\u7528\u6a5f\u5668\u5b78\u7fd2\u6a21\u578b\u201d\u4e2d\u7684 [\u8996\u983b\u6578\u64da](https://cloud.google.com/vertex-ai/docs/training-overview?hl=zh-cn#video_data) \u3002\n### \u4f7f\u7528\u60a8\u7684 AutoML \u6a21\u578b\n\u4ee5\u4e0b\u4ee3\u78bc\u793a\u4f8b\u6f14\u793a\u77ad\u5982\u4f55\u4f7f\u7528 [AutoML \u6a21\u578b](https://cloud.google.com/vertex-ai/docs/beginner/beginners-guide?hl=zh-cn#video) \uff0c\u901a\u904e\u6d41\u5f0f\u5ba2\u6236\u7aef\u5eab\u57f7\u884c\u64cd\u4f5c\u8b58\u5225\u3002\n\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/beta_snippets.py) \n```\nimport iofrom google.cloud import videointelligence_v1p3beta1 as videointelligence# path = 'path_to_file'# project_id = 'project_id'# model_id = 'automl_action_recognition_model_id'client = videointelligence.StreamingVideoIntelligenceServiceClient()model_path = \"projects/{}/locations/us-central1/models/{}\".format(\u00a0 \u00a0 project_id, model_id)automl_config = videointelligence.StreamingAutomlActionRecognitionConfig(\u00a0 \u00a0 model_name=model_path)video_config = videointelligence.StreamingVideoConfig(\u00a0 \u00a0 feature=videointelligence.StreamingFeature.STREAMING_AUTOML_ACTION_RECOGNITION,\u00a0 \u00a0 automl_action_recognition_config=automl_config,)# config_request should be the first in the stream of requests.config_request = videointelligence.StreamingAnnotateVideoRequest(\u00a0 \u00a0 video_config=video_config)# Set the chunk size to 5MB (recommended less than 10MB).chunk_size = 5 * 1024 * 1024def stream_generator():\u00a0 \u00a0 yield config_request\u00a0 \u00a0 # Load file content.\u00a0 \u00a0 # Note: Input videos must have supported video codecs. See\u00a0 \u00a0 # https://cloud.google.com/video-intelligence/docs/streaming/streaming#supported_video_codecs\u00a0 \u00a0 # for more details.\u00a0 \u00a0 with io.open(path, \"rb\") as video_file:\u00a0 \u00a0 \u00a0 \u00a0 while True:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 data = video_file.read(chunk_size)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if not data:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 break\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 yield videointelligence.StreamingAnnotateVideoRequest(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 input_content=data\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )requests = stream_generator()# streaming_annotate_video returns a generator.# The default timeout is about 300 seconds.# To process longer videos it should be set to# larger than the length (in seconds) of the video.responses = client.streaming_annotate_video(requests, timeout=900)# Each response corresponds to about 1 second of video.for response in responses:\u00a0 \u00a0 # Check for errors.\u00a0 \u00a0 if response.error.message:\u00a0 \u00a0 \u00a0 \u00a0 print(response.error.message)\u00a0 \u00a0 \u00a0 \u00a0 break\u00a0 \u00a0 for label in response.annotation_results.label_annotations:\u00a0 \u00a0 \u00a0 \u00a0 for frame in label.frames:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"At {:3d}s segment, {:5.1%} {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frame.time_offset.seconds,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 frame.confidence,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 label.entity.entity_id,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\n```", "guide": "Cloud Video Intelligence API"}