{"title": "Cloud Video Intelligence API - \u6aa2\u6e2c\u4eba\u81c9", "url": "https://cloud.google.com/video-intelligence/docs/face-detection?hl=zh-cn", "abstract": "# Cloud Video Intelligence API - \u6aa2\u6e2c\u4eba\u81c9\nVideo Intelligence API **\u4eba\u81c9\u6aa2\u6e2c** \u529f\u80fd\u6703\u67e5\u627e\u8996\u983b\u4e2d\u7684\u4eba\u81c9\u3002\n**\u6ce8\u610f** \uff1a\u5982\u9700\u8a73\u7d30\u77ad\u89e3\u6b64\u529f\u80fd\uff0c\u8acb\u53c3\u95b1 [\u4eba\u81c9\u6aa2\u6e2c](https://cloud.google.com/video-intelligence/docs/feature-face-detection?hl=zh-cn) \u6982\u5ff5\u9801\u9762\u3002\n", "content": "## \u901a\u904e Cloud Storage \u4e2d\u7684\u6587\u4ef6\u9032\u884c\u4eba\u81c9\u6aa2\u6e2c\n\u4ee5\u4e0b\u793a\u4f8b\u6f14\u793a\u77ad\u5982\u4f55\u5c0d Cloud Storage \u4e2d\u7684\u6587\u4ef6\u57f7\u884c\u4eba\u81c9\u6aa2\u6e2c\u3002\n### \u767c\u9001\u8996\u983b\u8a3b\u89e3\u8acb\u6c42\u4e0b\u9762\u6f14\u793a\u77ad\u5982\u4f55\u5411 [videos:annotate](https://cloud.google.com/video-intelligence/docs/reference/rest/v1/videos/annotate?hl=zh-cn) \u65b9\u6cd5\u767c\u9001 POST \u8acb\u6c42\u3002\u8a72\u793a\u4f8b\u4f7f\u7528 Google Cloud CLI \u5275\u5efa\u8a2a\u554f\u4ee4\u724c\u3002\u5982\u9700\u77ad\u89e3\u5982\u4f55\u5b89\u88dd gcloud CLI\uff0c\u8acb\u53c3\u95b1 [Video Intelligence API \u5feb\u901f\u5165\u9580](https://cloud.google.com/video-intelligence/docs/quickstarts?hl=zh-cn) \u3002\n\u5728\u4f7f\u7528\u4efb\u4f55\u8acb\u6c42\u6578\u64da\u4e4b\u524d\uff0c\u8acb\u5148\u9032\u884c\u4ee5\u4e0b\u66ff\u63db\uff1a- \uff1a\u5305\u542b\u8981\u6dfb\u52a0\u8a3b\u91cb\u7684\u6587\u4ef6\u7684 Cloud Storage \u5b58\u5132\u6876\uff08\u5305\u62ec\u6587\u4ef6\u540d\uff09\u3002\u5fc5\u9808\u4ee5 gs:// \u958b\u982d\u3002\u4f8b\u5982\uff1a`\"inputUri\": \"gs://cloud-samples-data/video/googlework_short.mp4\"`\n- \uff1a\u60a8\u7684 Google Cloud \u9805\u76ee\u7684\u6578\u5b57\u6a19\u8b58\u7b26\nHTTP \u65b9\u6cd5\u548c\u7db2\u5740\uff1a\n```\nPOST https://videointelligence.googleapis.com/v1/videos:annotate\n```\n\u8acb\u6c42 JSON \u6b63\u6587\uff1a\n```\n{\n \"inputUri\": \"INPUT_URI\",\n \"features\": [\"FACE_DETECTION\"]\n}\n```\n\u5982\u9700\u767c\u9001\u60a8\u7684\u8acb\u6c42\uff0c\u8acb\u5c55\u958b\u4ee5\u4e0b\u9078\u9805\u4e4b\u4e00\uff1a\u60a8\u61c9\u8a72\u6536\u5230\u985e\u4f3c\u4ee5\u4e0b\u5167\u5bb9\u7684 JSON \u97ff\u61c9\uff1a\u5982\u679c\u97ff\u61c9\u6210\u529f\uff0cVideo Intelligence API \u5c07\u8fd4\u56de\u60a8\u7684\u64cd\u4f5c\u7684 `name` \u3002\u4e0a\u9762\u986f\u793a\u4e86\u6b64\u985e\u97ff\u61c9\u7684\u793a\u4f8b\uff0c\u5176\u4e2d\uff1a- \uff1a\u60a8\u9805\u76ee\u7684\u7de8\u865f\n- \uff1a\u5728\u5176\u4e2d\u6dfb\u52a0\u8a3b\u89e3\u7684 Cloud \u5340\u57df\u3002\u652f\u6301\u7684\u96f2\u5340\u57df\u7232\uff1a`us-east1`\u3001`us-west1`\u3001`europe-west1`\u3001`asia-east1`\u3002\u5982\u679c\u672a\u6307\u5b9a\u5730\u5340\uff0c\u5247\u6839\u64da\u8996\u983b\u6587\u4ef6\u4f4d\u7f6e\u4f86\u78ba\u5b9a\u5730\u5340\u3002\n- \uff1a\u662f\u7232\u8acb\u6c42\u5275\u5efa\u7684\u9577\u6642\u9593\u904b\u884c\u7684\u64cd\u4f5c\u7684 ID\uff0c\u4e26\u5728\u5553\u52d5\u64cd\u4f5c\u6642\u5728\u97ff\u61c9\u4e2d\u63d0\u4f9b\uff0c\u4f8b\u5982`12345...`\n### \u7372\u53d6\u8a3b\u89e3\u7d50\u679c\u8981\u6aa2\u7d22\u64cd\u4f5c\u7684\u7d50\u679c\uff0c\u8acb\u4f7f\u7528\u5f9e [videos\uff1aannotate](https://cloud.google.com/video-intelligence/docs/reference/rest/v1/videos/annotate?hl=zh-cn) \u8abf\u7528\u8fd4\u56de\u7684\u64cd\u4f5c\u540d\u7a31\u767c\u51fa [GET](https://cloud.google.com/video-intelligence/docs/reference/rest/v1/projects.locations.operations/get?hl=zh-cn) \u8acb\u6c42\uff0c\u5982\u4ee5\u4e0b\u793a\u4f8b\u6240\u793a\u3002\n\u5728\u4f7f\u7528\u4efb\u4f55\u8acb\u6c42\u6578\u64da\u4e4b\u524d\uff0c\u8acb\u5148\u9032\u884c\u4ee5\u4e0b\u66ff\u63db\uff1a- \uff1aVideo Intelligence API \u8fd4\u56de\u7684\u64cd\u4f5c\u540d\u7a31\u3002\u64cd\u4f5c\u540d\u7a31\u63a1\u7528`projects/` `` `/locations/` `` `/operations/` ``\u683c\u5f0f\n- \uff1a\u60a8\u7684 Google Cloud \u9805\u76ee\u7684\u6578\u5b57\u6a19\u8b58\u7b26\nHTTP \u65b9\u6cd5\u548c\u7db2\u5740\uff1a\n```\nGET https://videointelligence.googleapis.com/v1/OPERATION_NAME\n```\n\u5982\u9700\u767c\u9001\u60a8\u7684\u8acb\u6c42\uff0c\u8acb\u5c55\u958b\u4ee5\u4e0b\u9078\u9805\u4e4b\u4e00\uff1a\u60a8\u61c9\u8a72\u6536\u5230\u985e\u4f3c\u4ee5\u4e0b\u5167\u5bb9\u7684 JSON \u97ff\u61c9\uff1a\u4eba\u81c9\u6aa2\u6e2c\u8a3b\u91cb\u4ee5 `faceAnnotations` \u5217\u8868\u7684\u5f62\u5f0f\u8fd4\u56de\u3002\u6ce8\u610f\uff1a\u50c5\u7576\u503c\u7232 **True** \u6642\uff0c\u7e94\u6703\u8fd4\u56de **done** \u5b57\u6bb5\u3002\u5b83\u4e0d\u6703\u5305\u542b\u5728\u64cd\u4f5c\u5c1a\u672a\u5b8c\u6210\u7684\u97ff\u61c9\u4e2d\u3002\n\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/video/src/main/java/video/DetectFacesGcs.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.videointelligence.v1.AnnotateVideoProgress;import com.google.cloud.videointelligence.v1.AnnotateVideoRequest;import com.google.cloud.videointelligence.v1.AnnotateVideoResponse;import com.google.cloud.videointelligence.v1.DetectedAttribute;import com.google.cloud.videointelligence.v1.FaceDetectionAnnotation;import com.google.cloud.videointelligence.v1.FaceDetectionConfig;import com.google.cloud.videointelligence.v1.Feature;import com.google.cloud.videointelligence.v1.TimestampedObject;import com.google.cloud.videointelligence.v1.Track;import com.google.cloud.videointelligence.v1.VideoAnnotationResults;import com.google.cloud.videointelligence.v1.VideoContext;import com.google.cloud.videointelligence.v1.VideoIntelligenceServiceClient;import com.google.cloud.videointelligence.v1.VideoSegment;public class DetectFacesGcs {\u00a0 public static void detectFacesGcs() throws Exception {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String gcsUri = \"gs://cloud-samples-data/video/googlework_short.mp4\";\u00a0 \u00a0 detectFacesGcs(gcsUri);\u00a0 }\u00a0 // Detects faces in a video stored in Google Cloud Storage using the Cloud Video Intelligence API.\u00a0 public static void detectFacesGcs(String gcsUri) throws Exception {\u00a0 \u00a0 try (VideoIntelligenceServiceClient videoIntelligenceServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 VideoIntelligenceServiceClient.create()) {\u00a0 \u00a0 \u00a0 FaceDetectionConfig faceDetectionConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 FaceDetectionConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Must set includeBoundingBoxes to true to get facial attributes.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setIncludeBoundingBoxes(true)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setIncludeAttributes(true)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 VideoContext videoContext =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 VideoContext.newBuilder().setFaceDetectionConfig(faceDetectionConfig).build();\u00a0 \u00a0 \u00a0 AnnotateVideoRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AnnotateVideoRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputUri(gcsUri)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addFeatures(Feature.FACE_DETECTION)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setVideoContext(videoContext)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // Detects faces in a video\u00a0 \u00a0 \u00a0 OperationFuture<AnnotateVideoResponse, AnnotateVideoProgress> future =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 videoIntelligenceServiceClient.annotateVideoAsync(request);\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to complete...\");\u00a0 \u00a0 \u00a0 AnnotateVideoResponse response = future.get();\u00a0 \u00a0 \u00a0 // Gets annotations for video\u00a0 \u00a0 \u00a0 VideoAnnotationResults annotationResult = response.getAnnotationResultsList().get(0);\u00a0 \u00a0 \u00a0 // Annotations for list of people detected, tracked and recognized in video.\u00a0 \u00a0 \u00a0 for (FaceDetectionAnnotation faceDetectionAnnotation :\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 annotationResult.getFaceDetectionAnnotationsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.print(\"Face detected:\\n\");\u00a0 \u00a0 \u00a0 \u00a0 for (Track track : faceDetectionAnnotation.getTracksList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 VideoSegment segment = track.getSegment();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tStart: %d.%.0fs\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getStartTimeOffset().getSeconds(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getStartTimeOffset().getNanos() / 1e6);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd: %d.%.0fs\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getEndTimeOffset().getSeconds(), segment.getEndTimeOffset().getNanos() / 1e6);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Each segment includes timestamped objects that\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // include characteristics of the face detected.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TimestampedObject firstTimestampedObject = track.getTimestampedObjects(0);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (DetectedAttribute attribute : firstTimestampedObject.getAttributesList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Attributes include glasses, headwear, smiling, direction of gaze\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tAttribute %s: %s %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 attribute.getName(), attribute.getValue(), attribute.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/video-intelligence/analyze-face-detection-gcs.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*/// const gcsUri = 'GCS URI of the video to analyze, e.g. gs://my-bucket/my-video.mp4';// Imports the Google Cloud Video Intelligence library + Node's fs libraryconst Video = require('@google-cloud/video-intelligence').v1;// Creates a clientconst video = new Video.VideoIntelligenceServiceClient();async function detectFacesGCS() {\u00a0 const request = {\u00a0 \u00a0 inputUri: gcsUri,\u00a0 \u00a0 features: ['FACE_DETECTION'],\u00a0 \u00a0 videoContext: {\u00a0 \u00a0 \u00a0 faceDetectionConfig: {\u00a0 \u00a0 \u00a0 \u00a0 // Must set includeBoundingBoxes to true to get facial attributes.\u00a0 \u00a0 \u00a0 \u00a0 includeBoundingBoxes: true,\u00a0 \u00a0 \u00a0 \u00a0 includeAttributes: true,\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 };\u00a0 // Detects faces in a video\u00a0 // We get the first result because we only process 1 video\u00a0 const [operation] = await video.annotateVideo(request);\u00a0 const results = await operation.promise();\u00a0 console.log('Waiting for operation to complete...');\u00a0 // Gets annotations for video\u00a0 const faceAnnotations =\u00a0 \u00a0 results[0].annotationResults[0].faceDetectionAnnotations;\u00a0 for (const {tracks} of faceAnnotations) {\u00a0 \u00a0 console.log('Face detected:');\u00a0 \u00a0 for (const {segment, timestampedObjects} of tracks) {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tStart: ${segment.startTimeOffset.seconds}.` +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 `${(segment.startTimeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tEnd: ${segment.endTimeOffset.seconds}.` +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 `${(segment.endTimeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 // Each segment includes timestamped objects that\u00a0 \u00a0 \u00a0 // include characteristics of the face detected.\u00a0 \u00a0 \u00a0 const [firstTimestapedObject] = timestampedObjects;\u00a0 \u00a0 \u00a0 for (const {name} of firstTimestapedObject.attributes) {\u00a0 \u00a0 \u00a0 \u00a0 // Attributes include 'glasses', 'headwear', 'smiling'.\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\tAttribute: ${name}; `);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}detectFacesGCS();\n```\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/video_detect_faces_gcs.py) \n```\nfrom google.cloud import videointelligence_v1 as videointelligencedef detect_faces(gcs_uri=\"gs://YOUR_BUCKET_ID/path/to/your/video.mp4\"):\u00a0 \u00a0 \"\"\"Detects faces in a video.\"\"\"\u00a0 \u00a0 client = videointelligence.VideoIntelligenceServiceClient()\u00a0 \u00a0 # Configure the request\u00a0 \u00a0 config = videointelligence.FaceDetectionConfig(\u00a0 \u00a0 \u00a0 \u00a0 include_bounding_boxes=True, include_attributes=True\u00a0 \u00a0 )\u00a0 \u00a0 context = videointelligence.VideoContext(face_detection_config=config)\u00a0 \u00a0 # Start the asynchronous request\u00a0 \u00a0 operation = client.annotate_video(\u00a0 \u00a0 \u00a0 \u00a0 request={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"features\": [videointelligence.Feature.FACE_DETECTION],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"input_uri\": gcs_uri,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"video_context\": context,\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 )\u00a0 \u00a0 print(\"\\nProcessing video for face detection annotations.\")\u00a0 \u00a0 result = operation.result(timeout=300)\u00a0 \u00a0 print(\"\\nFinished processing.\\n\")\u00a0 \u00a0 # Retrieve the first result, because a single video was processed.\u00a0 \u00a0 annotation_result = result.annotation_results[0]\u00a0 \u00a0 for annotation in annotation_result.face_detection_annotations:\u00a0 \u00a0 \u00a0 \u00a0 print(\"Face detected:\")\u00a0 \u00a0 \u00a0 \u00a0 for track in annotation.tracks:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Segment: {}s to {}s\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.start_time_offset.seconds\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + track.segment.start_time_offset.microseconds / 1e6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.end_time_offset.seconds\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + track.segment.end_time_offset.microseconds / 1e6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Each segment includes timestamped faces that include\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # characteristics of the face detected.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Grab the first timestamped face\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timestamped_object = track.timestamped_objects[0]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 box = timestamped_object.normalized_bounding_box\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"Bounding box:\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tleft \u00a0: {}\".format(box.left))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\ttop \u00a0 : {}\".format(box.top))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tright : {}\".format(box.right))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tbottom: {}\".format(box.bottom))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Attributes include glasses, headwear, smiling, direction of gaze\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"Attributes:\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for attribute in timestamped_object.attributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t{}:{} {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 attribute.name, attribute.value, attribute.confidence\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\n```No preface\n **C#** \uff1a\u8acb\u6309\u7167\u201c\u5ba2\u6236\u7aef\u5eab\u201d\u9801\u9762\u4e0a\u7684 [C# \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u9032\u884c\u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [\u9069\u7528\u65bc .NET \u7684 Video Intelligence \u53c3\u8003\u6587\u6a94](https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.VideoIntelligence.V1/index.html) \u3002\n **PHP** \uff1a\u8acb\u6309\u7167\u5ba2\u6236\u7aef\u5eab\u9801\u9762\u4e0a\u7684 [PHP \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [PHP \u7248 Video Intelligence \u53c3\u8003\u6587\u6a94](https://cloud.google.com/php/docs/reference/cloud-videointelligence/latest?hl=zh-cn) \u3002\n **Ruby** \uff1a\u8acb\u6309\u7167\u5ba2\u6236\u7aef\u5eab\u9801\u9762\u4e0a\u7684 [Ruby \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [Ruby \u7248 Video Intelligence \u53c3\u8003\u6587\u6a94](https://googleapis.dev/ruby/google-cloud-video_intelligence/latest/Google/Cloud/VideoIntelligence/V1.html) \u3002\n## \u5f9e\u672c\u5730\u6587\u4ef6\u9032\u884c\u4eba\u81c9\u6aa2\u6e2c\n\u4ee5\u4e0b\u793a\u4f8b\u4f7f\u7528\u4eba\u81c9\u6aa2\u6e2c\u5f9e\u672c\u5730\u6a5f\u5668\u4e0a\u50b3\u7684\u8996\u983b\u6587\u4ef6\u4e2d\u67e5\u627e\u8996\u983b\u4e2d\u7684\u5be6\u9ad4\u3002\n### \u767c\u9001\u8655\u7406\u8acb\u6c42\u8981\u5c0d\u672c\u5730\u8996\u983b\u6587\u4ef6\u57f7\u884c\u4eba\u81c9\u6aa2\u6e2c\uff0c\u8acb\u5c0d\u8996\u983b\u6587\u4ef6\u7684\u5167\u5bb9\u9032\u884c base64 \u7de8\u78bc\u3002\u5982\u9700\u77ad\u89e3\u5982\u4f55\u5c0d\u8996\u983b\u6587\u4ef6\u7684\u5167\u5bb9\u9032\u884c base64 \u7de8\u78bc\uff0c\u8acb\u53c3\u95b1 [Base64 \u7de8\u78bc](https://cloud.google.com/video-intelligence/docs/base64?hl=zh-cn) \u3002\u7136\u5f8c\uff0c\u5411 [videos:annotate](https://cloud.google.com/video-intelligence/docs/reference/rest/v1/videos/annotate?hl=zh-cn) \u65b9\u6cd5\u767c\u51fa POST \u8acb\u6c42\u3002\u5728\u8acb\u6c42\u7684 `inputContent` \u5b57\u6bb5\u4e2d\u6dfb\u52a0 base64 \u7de8\u78bc\u7684\u5167\u5bb9\uff0c\u4e26\u6307\u5b9a `FACE_DETECTION` \u529f\u80fd\u3002\n\u4ee5\u4e0b\u793a\u4f8b\u5c55\u793a\u4e86\u4f7f\u7528 curl \u7684 POST \u8acb\u6c42\u3002\u8a72\u793a\u4f8b\u4f7f\u7528 Google Cloud CLI \u5275\u5efa\u8a2a\u554f\u4ee4\u724c\u3002\u5982\u9700\u77ad\u89e3\u5982\u4f55\u5b89\u88dd gcloud CLI\uff0c\u8acb\u53c3\u95b1 [Video Intelligence API \u5feb\u901f\u5165\u9580](https://cloud.google.com/video-intelligence/docs/annotate-video-command-line?hl=zh-cn) \n\u5728\u4f7f\u7528\u4efb\u4f55\u8acb\u6c42\u6578\u64da\u4e4b\u524d\uff0c\u8acb\u5148\u9032\u884c\u4ee5\u4e0b\u66ff\u63db\uff1a- \u4e8c\u9032\u5236\u6587\u4ef6\u683c\u5f0f\u7684\u672c\u5730\u8996\u983b\u6587\u4ef6\uff0c\u4f8b\u5982\uff1aAAAAGGZ0eXBtcDQyAAAAAGlzb21tcDQyAAGVYW1vb3YAAABsbXZoZAAAAADWvhlR1r4ZUQABX5ABCOxoAAEAAAEAAAAAAA4\n- \uff1a\u60a8\u7684 Google Cloud \u9805\u76ee\u7684\u6578\u5b57\u6a19\u8b58\u7b26\nHTTP \u65b9\u6cd5\u548c\u7db2\u5740\uff1a\n```\nPOST https://videointelligence.googleapis.com/v1/videos:annotate\n```\n\u8acb\u6c42 JSON \u6b63\u6587\uff1a\n```\n{\n inputContent: \"Local video file in binary format\",\n \"features\": [\"FACE_DETECTION\"]\n}\n```\n\u5982\u9700\u767c\u9001\u60a8\u7684\u8acb\u6c42\uff0c\u8acb\u5c55\u958b\u4ee5\u4e0b\u9078\u9805\u4e4b\u4e00\uff1a\u60a8\u61c9\u8a72\u6536\u5230\u985e\u4f3c\u4ee5\u4e0b\u5167\u5bb9\u7684 JSON \u97ff\u61c9\uff1a\u5982\u679c\u8acb\u6c42\u6210\u529f\uff0c\u5247 Video Intelligence \u6703\u7232\u60a8\u7684\u64cd\u4f5c\u5206\u914d `name` \u3002\u4e0a\u9762\u7684\u793a\u4f8b\u5c55\u793a\u4e86\u6b64\u985e\u97ff\u61c9\u7684\u793a\u4f8b\uff0c\u5176\u4e2d `project-number` \u662f\u60a8\u7684\u9805\u76ee\u7de8\u865f\uff0c `operation-id` \u662f\u7232\u8acb\u6c42\u5275\u5efa\u7684\u9577\u6642\u9593\u904b\u884c\u7684\u64cd\u4f5c\u7684 ID\u3002\n`{ \"name\": \"us-west1.17122464255125931980\" }`\n### \u7372\u53d6\u7d50\u679c\u8981\u6aa2\u7d22\u64cd\u4f5c\u7d50\u679c\uff0c\u8acb\u5411 [operations](https://cloud.google.com/video-intelligence/docs/reference/rest/v1/operations/get?hl=zh-cn) \u7aef\u9ede\u767c\u9001 GET \u8acb\u6c42\u4e26\u6307\u5b9a\u64cd\u4f5c\u540d\u7a31\u3002\n\u5728\u4f7f\u7528\u4efb\u4f55\u8acb\u6c42\u6578\u64da\u4e4b\u524d\uff0c\u8acb\u5148\u9032\u884c\u4ee5\u4e0b\u66ff\u63db\uff1a- \uff1aVideo Intelligence API \u8fd4\u56de\u7684\u64cd\u4f5c\u540d\u7a31\u3002\u64cd\u4f5c\u540d\u7a31\u63a1\u7528`projects/` `` `/locations/` `` `/operations/` ``\u683c\u5f0f\n- \uff1a\u60a8\u7684 Google Cloud \u9805\u76ee\u7684\u6578\u5b57\u6a19\u8b58\u7b26\nHTTP \u65b9\u6cd5\u548c\u7db2\u5740\uff1a\n```\nGET https://videointelligence.googleapis.com/v1/OPERATION_NAME\n```\n\u5982\u9700\u767c\u9001\u60a8\u7684\u8acb\u6c42\uff0c\u8acb\u5c55\u958b\u4ee5\u4e0b\u9078\u9805\u4e4b\u4e00\uff1a\u60a8\u61c9\u8a72\u6536\u5230\u985e\u4f3c\u4ee5\u4e0b\u5167\u5bb9\u7684 JSON \u97ff\u61c9\uff1a\n\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/java-docs-samples/blob/HEAD/video/src/main/java/video/DetectFaces.java) \n```\nimport com.google.api.gax.longrunning.OperationFuture;import com.google.cloud.videointelligence.v1.AnnotateVideoProgress;import com.google.cloud.videointelligence.v1.AnnotateVideoRequest;import com.google.cloud.videointelligence.v1.AnnotateVideoResponse;import com.google.cloud.videointelligence.v1.DetectedAttribute;import com.google.cloud.videointelligence.v1.FaceDetectionAnnotation;import com.google.cloud.videointelligence.v1.FaceDetectionConfig;import com.google.cloud.videointelligence.v1.Feature;import com.google.cloud.videointelligence.v1.TimestampedObject;import com.google.cloud.videointelligence.v1.Track;import com.google.cloud.videointelligence.v1.VideoAnnotationResults;import com.google.cloud.videointelligence.v1.VideoContext;import com.google.cloud.videointelligence.v1.VideoIntelligenceServiceClient;import com.google.cloud.videointelligence.v1.VideoSegment;import com.google.protobuf.ByteString;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;public class DetectFaces {\u00a0 public static void detectFaces() throws Exception {\u00a0 \u00a0 // TODO(developer): Replace these variables before running the sample.\u00a0 \u00a0 String localFilePath = \"resources/googlework_short.mp4\";\u00a0 \u00a0 detectFaces(localFilePath);\u00a0 }\u00a0 // Detects faces in a video stored in a local file using the Cloud Video Intelligence API.\u00a0 public static void detectFaces(String localFilePath) throws Exception {\u00a0 \u00a0 try (VideoIntelligenceServiceClient videoIntelligenceServiceClient =\u00a0 \u00a0 \u00a0 \u00a0 VideoIntelligenceServiceClient.create()) {\u00a0 \u00a0 \u00a0 // Reads a local video file and converts it to base64.\u00a0 \u00a0 \u00a0 Path path = Paths.get(localFilePath);\u00a0 \u00a0 \u00a0 byte[] data = Files.readAllBytes(path);\u00a0 \u00a0 \u00a0 ByteString inputContent = ByteString.copyFrom(data);\u00a0 \u00a0 \u00a0 FaceDetectionConfig faceDetectionConfig =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 FaceDetectionConfig.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Must set includeBoundingBoxes to true to get facial attributes.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setIncludeBoundingBoxes(true)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setIncludeAttributes(true)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 VideoContext videoContext =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 VideoContext.newBuilder().setFaceDetectionConfig(faceDetectionConfig).build();\u00a0 \u00a0 \u00a0 AnnotateVideoRequest request =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 AnnotateVideoRequest.newBuilder()\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setInputContent(inputContent)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .addFeatures(Feature.FACE_DETECTION)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .setVideoContext(videoContext)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 .build();\u00a0 \u00a0 \u00a0 // Detects faces in a video\u00a0 \u00a0 \u00a0 OperationFuture<AnnotateVideoResponse, AnnotateVideoProgress> future =\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 videoIntelligenceServiceClient.annotateVideoAsync(request);\u00a0 \u00a0 \u00a0 System.out.println(\"Waiting for operation to complete...\");\u00a0 \u00a0 \u00a0 AnnotateVideoResponse response = future.get();\u00a0 \u00a0 \u00a0 // Gets annotations for video\u00a0 \u00a0 \u00a0 VideoAnnotationResults annotationResult = response.getAnnotationResultsList().get(0);\u00a0 \u00a0 \u00a0 // Annotations for list of faces detected, tracked and recognized in video.\u00a0 \u00a0 \u00a0 for (FaceDetectionAnnotation faceDetectionAnnotation :\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 annotationResult.getFaceDetectionAnnotationsList()) {\u00a0 \u00a0 \u00a0 \u00a0 System.out.print(\"Face detected:\\n\");\u00a0 \u00a0 \u00a0 \u00a0 for (Track track : faceDetectionAnnotation.getTracksList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 VideoSegment segment = track.getSegment();\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tStart: %d.%.0fs\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getStartTimeOffset().getSeconds(),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getStartTimeOffset().getNanos() / 1e6);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tEnd: %d.%.0fs\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 segment.getEndTimeOffset().getSeconds(), segment.getEndTimeOffset().getNanos() / 1e6);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Each segment includes timestamped objects that\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // include characteristics of the face detected.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 TimestampedObject firstTimestampedObject = track.getTimestampedObjects(0);\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for (DetectedAttribute attribute : firstTimestampedObject.getAttributesList()) {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 // Attributes include glasses, headwear, smiling, direction of gaze\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 System.out.printf(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\tAttribute %s: %s %s\\n\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 attribute.getName(), attribute.getValue(), attribute.getConfidence());\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}\n```\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/nodejs-docs-samples/blob/HEAD/video-intelligence/analyze-face-detection.js) \n```\n/**\u00a0* TODO(developer): Uncomment these variables before running the sample.\u00a0*/// const path = 'Local file to analyze, e.g. ./my-file.mp4';// Imports the Google Cloud Video Intelligence library + Node's fs libraryconst Video = require('@google-cloud/video-intelligence').v1;const fs = require('fs');// Creates a clientconst video = new Video.VideoIntelligenceServiceClient();// Reads a local video file and converts it to base64const file = fs.readFileSync(path);const inputContent = file.toString('base64');async function detectFaces() {\u00a0 const request = {\u00a0 \u00a0 inputContent: inputContent,\u00a0 \u00a0 features: ['FACE_DETECTION'],\u00a0 \u00a0 videoContext: {\u00a0 \u00a0 \u00a0 faceDetectionConfig: {\u00a0 \u00a0 \u00a0 \u00a0 // Must set includeBoundingBoxes to true to get facial attributes.\u00a0 \u00a0 \u00a0 \u00a0 includeBoundingBoxes: true,\u00a0 \u00a0 \u00a0 \u00a0 includeAttributes: true,\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 },\u00a0 };\u00a0 // Detects faces in a video\u00a0 // We get the first result because we only process 1 video\u00a0 const [operation] = await video.annotateVideo(request);\u00a0 const results = await operation.promise();\u00a0 console.log('Waiting for operation to complete...');\u00a0 // Gets annotations for video\u00a0 const faceAnnotations =\u00a0 \u00a0 results[0].annotationResults[0].faceDetectionAnnotations;\u00a0 for (const {tracks} of faceAnnotations) {\u00a0 \u00a0 console.log('Face detected:');\u00a0 \u00a0 for (const {segment, timestampedObjects} of tracks) {\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tStart: ${segment.startTimeOffset.seconds}` +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 `.${(segment.startTimeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 console.log(\u00a0 \u00a0 \u00a0 \u00a0 `\\tEnd: ${segment.endTimeOffset.seconds}.` +\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 `${(segment.endTimeOffset.nanos / 1e6).toFixed(0)}s`\u00a0 \u00a0 \u00a0 );\u00a0 \u00a0 \u00a0 // Each segment includes timestamped objects that\u00a0 \u00a0 \u00a0 // include characteristics of the face detected.\u00a0 \u00a0 \u00a0 const [firstTimestapedObject] = timestampedObjects;\u00a0 \u00a0 \u00a0 for (const {name} of firstTimestapedObject.attributes) {\u00a0 \u00a0 \u00a0 \u00a0 // Attributes include 'glasses', 'headwear', 'smiling'.\u00a0 \u00a0 \u00a0 \u00a0 console.log(`\\tAttribute: ${name}; `);\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }}detectFaces();\n```\u8981\u5411 Video Intelligence \u9032\u884c\u8eab\u4efd\u9a57\u8b49\uff0c\u8acb\u8a2d\u7f6e\u61c9\u7528\u9ed8\u8a8d\u6191\u64da\u3002\u5982\u9700\u77ad\u89e3\u8a73\u60c5\uff0c\u8acb\u53c3\u95b1 [\u7232\u672c\u5730\u958b\u767c\u74b0\u5883\u8a2d\u7f6e\u8eab\u4efd\u9a57\u8b49](https://cloud.google.com/docs/authentication/provide-credentials-adc?hl=zh-cn#local-dev) \u3002\n [\u5728 GitHub \u4e0a\u67e5\u770b](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/videointelligence/samples/analyze/video_detect_faces.py) \n```\nimport iofrom google.cloud import videointelligence_v1 as videointelligencedef detect_faces(local_file_path=\"path/to/your/video-file.mp4\"):\u00a0 \u00a0 \"\"\"Detects faces in a video from a local file.\"\"\"\u00a0 \u00a0 client = videointelligence.VideoIntelligenceServiceClient()\u00a0 \u00a0 with io.open(local_file_path, \"rb\") as f:\u00a0 \u00a0 \u00a0 \u00a0 input_content = f.read()\u00a0 \u00a0 # Configure the request\u00a0 \u00a0 config = videointelligence.FaceDetectionConfig(\u00a0 \u00a0 \u00a0 \u00a0 include_bounding_boxes=True, include_attributes=True\u00a0 \u00a0 )\u00a0 \u00a0 context = videointelligence.VideoContext(face_detection_config=config)\u00a0 \u00a0 # Start the asynchronous request\u00a0 \u00a0 operation = client.annotate_video(\u00a0 \u00a0 \u00a0 \u00a0 request={\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"features\": [videointelligence.Feature.FACE_DETECTION],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"input_content\": input_content,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"video_context\": context,\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 )\u00a0 \u00a0 print(\"\\nProcessing video for face detection annotations.\")\u00a0 \u00a0 result = operation.result(timeout=300)\u00a0 \u00a0 print(\"\\nFinished processing.\\n\")\u00a0 \u00a0 # Retrieve the first result, because a single video was processed.\u00a0 \u00a0 annotation_result = result.annotation_results[0]\u00a0 \u00a0 for annotation in annotation_result.face_detection_annotations:\u00a0 \u00a0 \u00a0 \u00a0 print(\"Face detected:\")\u00a0 \u00a0 \u00a0 \u00a0 for track in annotation.tracks:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"Segment: {}s to {}s\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.start_time_offset.seconds\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + track.segment.start_time_offset.microseconds / 1e6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 track.segment.end_time_offset.seconds\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + track.segment.end_time_offset.microseconds / 1e6,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Each segment includes timestamped faces that include\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # characteristics of the face detected.\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Grab the first timestamped face\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 timestamped_object = track.timestamped_objects[0]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 box = timestamped_object.normalized_bounding_box\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"Bounding box:\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tleft \u00a0: {}\".format(box.left))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\ttop \u00a0 : {}\".format(box.top))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tright : {}\".format(box.right))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"\\tbottom: {}\".format(box.bottom))\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # Attributes include glasses, headwear, smiling, direction of gaze\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\"Attributes:\")\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for attribute in timestamped_object.attributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"\\t{}:{} {}\".format(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 attribute.name, attribute.value, attribute.confidence\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 )\n```No preface\n **C#** \uff1a\u8acb\u6309\u7167\u201c\u5ba2\u6236\u7aef\u5eab\u201d\u9801\u9762\u4e0a\u7684 [C# \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u9032\u884c\u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [\u9069\u7528\u65bc .NET \u7684 Video Intelligence \u53c3\u8003\u6587\u6a94](https://googleapis.github.io/google-cloud-dotnet/docs/Google.Cloud.VideoIntelligence.V1/index.html) \u3002\n **PHP** \uff1a\u8acb\u6309\u7167\u5ba2\u6236\u7aef\u5eab\u9801\u9762\u4e0a\u7684 [PHP \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [PHP \u7248 Video Intelligence \u53c3\u8003\u6587\u6a94](https://cloud.google.com/php/docs/reference/cloud-videointelligence/latest?hl=zh-cn) \u3002\n **Ruby** \uff1a\u8acb\u6309\u7167\u5ba2\u6236\u7aef\u5eab\u9801\u9762\u4e0a\u7684 [Ruby \u8a2d\u7f6e\u8aaa\u660e](https://cloud.google.com/video-intelligence/docs/libraries?hl=zh-cn) \u64cd\u4f5c\uff0c\u7136\u5f8c\u8a2a\u554f [Ruby \u7248 Video Intelligence \u53c3\u8003\u6587\u6a94](https://googleapis.dev/ruby/google-cloud-video_intelligence/latest/Google/Cloud/VideoIntelligence/V1.html) \u3002", "guide": "Cloud Video Intelligence API"}