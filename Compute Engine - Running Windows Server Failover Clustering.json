{"title": "Compute Engine - Running Windows Server Failover Clustering", "url": "https://cloud.google.com/compute/docs/instances", "abstract": "# Compute Engine - Running Windows Server Failover Clustering\nYou can create a failover cluster using Windows Server on Google Cloud Platform (GCP). A group of servers works together to provide higher availability (HA) for your Windows applications. If one fails, another node can take over running the software. You can configure the failover to happen automatically, which is the usual configuration, or you can manually trigger a failover.\nThis tutorial assumes you are familiar with failover clustering, Active Directory (AD), and administration of Windows Server.\nFor a brief overview of networking in GCP, see [GCP for Data Center Pros: Networking](/docs/compare/data-centers/networking) .", "content": "## ArchitectureThis tutorial walks you through how to create an example failover cluster on Compute Engine. The example system contains the following two servers:- A primary Compute Engine VM instance running Windows Server 2016 in zone`a`.\n- A second instance, configured to match the primary instance in zone`b`.\nAdditionally, you deploy an AD domain contoller, which, for this tutorial, serves the following purposes:- Provides a Windows domain.\n- Resolves hostnames to IP addresses.\n- Hosts thethat acts as a third \"vote\" to achieve the [required quorum for the cluster](https://technet.microsoft.com/en-us/library/jj612870(v=ws.11).aspx) .\nYou can create the domain controller in any zone. This tutorial uses zone `c` . In a production system, you can host the file share witness elsewhere, and you don't need a separate AD system only to support your failover cluster. See [What's next](#whats-next) for links to articles about using AD on GCP.\nThe two servers that you will use to deploy the failover cluster are located in different zones to ensure that each server is on a different physical machine and to protect against the unlikely possibility of a zonal failure.\nThe following diagram describes the architecture you deploy by following this tutorial.\n### Shared storage optionsThis tutorial does not cover setting up a file server for high-availability shared storage.\nGoogle Cloud supports multiple shared storage solutions that you can use with Windows Server Failover Clustering, including:- [SIOS Datakeeper Cluster Edition](https://us.sios.com/products/sios-datakeeper/) \n- [Native Microsoft Windows options](https://docs.microsoft.com/en-us/windows-server/failover-clustering/clustering-requirements) \nFor information about other possible shared storage solutions, see:- [GCP Windows Partners](https://cloud.google.com/windows/partners/) \n- [File servers on Compute Engine](/solutions/filers-on-compute-engine) .\n### Understanding the network routingWhen the cluster fails over, requests must go to the newly active node. The clustering technology normally handles routing by using [address resolution protocol](https://wikipedia.org/wiki/Address_Resolution_Protocol) (ARP), which associates IP addresses with MAC addresses. In GCP, the Virtual Private Cloud (VPC) system uses [software-defined networking](https://wikipedia.org/wiki/Software-defined_networking) , which doesn't leverage MAC addresses. This means the changes broadcast by ARP don't affect routing at all. To make routing work, the cluster requires some software-level help from the [Internal Load Balancer](/compute/docs/load-balancing/internal) .\nUsually, internal load balancing distributes incoming network traffic among multiple backend instances that are internal to your VPC, to share the load. For failover clustering, you instead use internal load balancing to route all traffic to just one instance: the currently active cluster node. Here's how internal load balancing detects the correct node:- Each VM instance runs a Compute Engine agent instance that provides support for Windows failover clustering. The agent keeps track of the IP addresses for the VM instance.\n- The load balancer's frontend provides the IP address for incoming traffic to the application.\n- The load balancer's backend provides a health check. The health check process periodically pings the agent on each cluster node by using the fixed IP address of the VM instance through a particular port. The default port is 59998.\n- The health check includes the application's IP address as a payload in the request.\n- The agent compares the IP address in the request to the list of IP addresses for the host VM. If the agent finds a match, it responds with a value of 1. Otherwise, it responds with 0.\n- The load balancer marks any VM that passes the health check as. At any moment, only one VM ever passes the health check because only one VM has the IP address for the workload.\n### What happens during a failoverWhen a failover happens in the cluster, the following changes take place:- Windows failover clustering changes the status of the active node to indicate that it has failed.\n- Failover clustering moves any cluster resources and roles from the failing node to the best node, as defined by the quorum. This action includes moving the associated IP addresses.\n- Failover clustering broadcasts ARP packets to notify hardware-based network routers that the IP addresses have moved. For this scenario, GCP networking ignores these packets.\n- After the move, the Compute Engine agent on the VM for the failing node changes its response to the health check from 1 to 0, because the VM no longer hosts the IP address specified in the request.\n- The Compute Engine agent on the VM for the newly active node likewise changes its response to the health check from 0 to 1.\n- The internal load balancer stops routing traffic to the failing node and instead routes traffic to the newly active node.\n### Putting it togetherNow that you've reviewed some of the concepts, here are some details to notice about the architecture diagram:- The Compute Engine agent for the VM named`wsfc-2`is responding to the health check with the value 1, indicating it is the active cluster node. For`wsfc-1`, the response is 0.\n- The load balancer is routing requests to`wsfc-2`, as indicated by the arrow.\n- The load balancer and`wsfc-2`both have the IP address`10.0.0.9`. For the load balancer, this is the specified frontend IP address. For the VM, it's the IP address of the application. The failover cluster sets this IP address on the currently active node.\n- The failover cluster and`wsfc-2`both have the IP address`10.0.0.8`. The VM has this IP address because it currently hosts the cluster resources.\n## Advice for following this tutorialThis tutorial has a lot of steps. Sometimes, you are asked to follow steps in external documents, such as Microsoft documentation. Don't miss the notes in this document providing specifics for following the external steps.\nThis tutorial uses Cloud Shell in the Google Cloud console. Though it's possible to use the Google Cloud console user interface or the gcloud CLI to set up failover clustering, this tutorial mainly uses Cloud Shell to make it easy for you. This approach helps you to complete the tutorial faster. When more appropriate, some steps use the Google Cloud console instead.\n \nIt's a good idea to [take snapshots of yourCompute Engine persistent disks](/compute/docs/disks/create-snapshots) along the way. If something goes wrong, you can use a snapshot to avoid starting over from the beginning. This tutorial suggests good times to take the snapshots.\nIf you find that things aren't working as you expect, there might be instructions in the section you're reading. Otherwise, refer to the [Troubleshooting](#troubleshooting) section.## Objectives\n- Create a network.\n- Install Windows Server 2016 on two Compute Engine VMs.\n- Install and configure Active Directory on a third instance of Windows Server.\n- Set up the failover cluster, including a file share witness for the quorum and a role for the workload.\n- Set up the internal load balancer.\n- Test the failover operation to verify that the cluster is working.\n## CostsThis tutorial uses Compute Engine images that include Windows Server licenses. This means the cost to run this tutorial can be significant if you leave VMs running. It's a good idea to stop the VMs when you're not using them.\nSee the [Pricing Calculator](/products/calculator#id=fbf3d50a-1f20-4ac0-9c9e-d99c50529899) for an estimate of the costs to complete this tutorial.## Before you begin- Start an instance of Cloud Shell. [Go to Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n## Creating the networkYour cluster requires a custom network. Use VPC to create a custom network and one subnetwork by running `gcloud` commands in Cloud Shell.- Create the network:```\ngcloud compute networks create wsfcnet --subnet-mode custom\n```The name of the network you created is `wsfcnet` .\n- Create a subnetwork. Replace `[YOUR_REGION]` with a nearby GCP region:```\ngcloud compute networks subnets create wsfcnetsub1 --network wsfcnet --region [YOUR_REGION] --range 10.0.0.0/16\n```The name of the subnetwork you created is `wsfcnetsub1` .\nNotice that the [CIDR](https://wikipedia.org/wiki/Classless_Inter-Domain_Routing) range for IP addresses in this subnetwork is `10.0.0.0/16` . This is an example range used for this tutorial. In production systems, work with your network administrators to allocate appropriate ranges for IP addresses for your systems.\n### Create firewall rulesBy default, your network is closed to external traffic. You must open ports in the firewall to enable remote connections to the servers. Use `gcloud` commands in Cloud Shell to create the rules.- For this tutorial, open port 3389 on the main network to enable RDP connections. In the following command, replace `[YOUR_IPv4_ADDRESS]` with the IP address of the computer you use to connect to your VM instances. In a production system, [you can provide an IP address range or a series of addresses](/sdk/gcloud/reference/compute/firewall-rules/create) .```\ngcloud compute firewall-rules create allow-rdp --network wsfcnet --allow tcp:3389 --source-ranges [YOUR_IPv4_ADDRESS]\n```\n- On the subnetwork, allow all protocols on all ports to enable the servers to communicate with each other. In production systems, consider opening only specific ports, as needed.```\ngcloud compute firewall-rules create allow-all-subnet --network wsfcnet --allow all --source-ranges 10.0.0.0/16\n```Notice that the `source-ranges` value matches the CIDR range you used to create the subnetwork.\n- View your firewall rules:```\ngcloud compute firewall-rules list\n```You should see output similar to the following:```\nNAME    NETWORK DIRECTION PRIORITY ALLOW   DENY DISABLED\nallow-all-subnet wsfcnet INGRESS 1000  all     False\nallow-rdp   wsfcnet INGRESS 1000  tcp:3389    False\n```\n## Enabling failover clustering in Compute EngineTo enable failover clustering in the Compute Engine agent, you need to add the flag `enable-wsfc=true` to your VM definitions either by specifying it as custom metadata for the VM or by creating a configuration file on each VM, as described in the [Compute Enginedocumentation](/compute/docs/instances/windows/creating-managing-windows-instances#windows_server_failover_clustering) .\nThis tutorial defines the flag as custom metadata when the VMs are created, as described in the next section. The tutorial also relies on the default behavior for `wsfc-addrs` and `wsfc-agent-port` , so you don't need to set these values.## Creating the serversNext, create the 3 servers. Use the `gcloud` command in Cloud Shell.\n### Create the first cluster-node serverCreate a new Compute Engine instance. Configure the instance as follows:- Name the instance`wsfc-1`.\n- Set the`--zone`flag to a convenient zone near you. For example,`us-central1-a`.\n- Set the`--machine-type`flag to`n1-standard-2.`\n- Set the`--image-project`flag to`windows-cloud`.\n- Set the`--image-family`flag to`windows-2016`.\n- Set the`--scopes`flag to`https://www.googleapis.com/auth/compute`.\n- Set the`--can-ip-forward`flag to enable IP forwarding.\n- Set the`--private-network-ip`flag to`10.0.0.4`.\n- Set the network to`wsfcnet`and the subnetwork to`wsfcnetsub1`.\n- Use the`--metadata`parameter to set`enable-wsfc=true`.\nRun the following command, replacing `[YOUR_ZONE_1]` with the name of your first zone:\n```\ngcloud compute instances create wsfc-1 --zone [YOUR_ZONE_1] --machine-type n1-standard-2 --image-project windows-cloud --image-family windows-2016 --scopes https://www.googleapis.com/auth/compute --can-ip-forward --private-network-ip 10.0.0.4 --network wsfcnet --subnet wsfcnetsub1 --metadata enable-wsfc=true\n```\n### Create the second cluster-node serverFor the second server, follow the same steps, except:- Set the instance name to:`wsfc-2`.\n- Set the`--zone`flag to a different zone than the zone that you used for the first server. For example,`us-central1-b`.\n- Set the`--private-network-ip`flag to`10.0.0.5`.\nReplace `[YOUR_ZONE_2]` with the name of your second zone:\n```\ngcloud compute instances create wsfc-2 --zone [YOUR_ZONE_2] --machine-type n1-standard-2 --image-project windows-cloud --image-family windows-2016 --scopes https://www.googleapis.com/auth/compute --can-ip-forward --private-network-ip 10.0.0.5 --network wsfcnet --subnet wsfcnetsub1 \u00a0--metadata enable-wsfc=true\n```\n### Create the third server for Active DirectoryFor the domain controller, follow the same steps, except:- Set the instance name to:`wsfc-dc`.\n- Set the`--zone`flag to a different zone than the zones that you used for the other servers. For example,`us-central1-c`.\n- Set the`--private-network-ip`flag to`10.0.0.6`.\n- Omit`--metadata enable-wsfc=true`.\nReplace `[YOUR_ZONE_3]` with the name of your zone:\n```\ngcloud compute instances create wsfc-dc --zone [YOUR_ZONE_3] --machine-type n1-standard-2 --image-project windows-cloud --image-family windows-2016 --scopes https://www.googleapis.com/auth/compute --can-ip-forward --private-network-ip 10.0.0.6 --network wsfcnet --subnet wsfcnetsub1\n```\n### View your instancesYou can see the details about the instances you created.\n```\ngcloud compute instances list\n```\nYou will see output similar to the following:\n```\nNAME  ZONE   MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP  STATUS\nwsfc-1 us-central1-a n1-standard-2    10.0.0.4  35.203.131.133 RUNNING\nwsfc-2 us-central1-b n1-standard-2    10.0.0.5  35.203.130.194 RUNNING\nwsfc-dc us-central1-c n1-standard-2    10.0.0.6  35.197.27.2  RUNNING\n```## Connecting to your VMsTo connect to a Windows-based VM, you must first generate a password for the VM. You can then connect to the VM using RDP.\n### Generating passwords\n- In the Google Cloud console, go to the **VM instances** page. [Go to VM instances](https://console.cloud.google.com/compute/instances) \n- Click the name of the VM instance for which you need a new password.\n- On the instance details page, click the **Set Windows Password** button. A password is generated for you. Copy the password and store it in a secure place.\n### Connecting through RDPThe Compute Engine documentation provides details about how to connect to your Windows VM instances by using RDP. You can either:- Use an existing client.\n- Add a Chrome RDP plugin to your browser and then [connect](/compute/docs/instances/connecting-to-windows) through the Google Cloud console.\nWhenever this tutorial tells you to connect to a Windows instance, use your preferred RDP connection.## Configuring Windows networkingThe internal IP addresses that you assigned when you created the VMs are static. To ensure that Windows treats the IP addresses as static, you need to add them, along with the IP addresses of the default gateway and the DNS server, to the Windows Server networking configuration.\nUse RDP to connect to `wsfc-1` , `wsfc-2` , and `wsfc-dc` , and repeat the following steps for each instance:- In Server Manager, in the left pane, select **Local Server** .\n- In the **Ethernet** entry of the **Properties** pane, click **IPv4 address assigned by DHCP, IPv6 enabled** .\n- Right-click **Ethernet** and select **Properties** .\n- Double-click **Internet Protocol Version 4 (TCP/IPv4)** .\n- Select **Use the following IP address** .\n- Enter the internal IP address that you assigned to the VM when you created it.- For`wsfc-1`, enter \"10.0.0.4\".\n- For`wsfc-2`enter \"10.0.0.5\".\n- For`wsfc-dc`enter \"10.0.0.6\".\n- For **Subnet mask** , enter \"255.255.0.0\".\n- For **Default gateway** , enter `10.0.0.1` , the IP address that was automatically reserved for the default gateway when you created the subnet `wsfcnetsub1` .The IP address for the default gateway is always the second address in the primary IP range for a subnet. See [Unusable addresses in IPv4 subnet ranges](/vpc/docs/subnets#unusable-ip-addresses-in-every-subnet) .\n- For `wsfc-1` and `wsfc-2` only:- Click **Use the following DNS server addresses** .\n- For **Preferred DNS server** , enter \"10.0.0.6\".\n- Close all the dialog boxes.You lose RDP connectivity because these changes reset the virtual network adapter for the VM instance.\n- Close the RDP session and then reconnect to the instance. If a dialog box from the previous step is still open, close it.\n- In the properties section for the local server, verify that the **Ethernet** setting reflects the local server IP address ( `10.0.0.4` , `10.0.0.5` , or `10.0.0.6` ). If it doesn't, re-open the **InternetProtocol Version 4 (TCP/IPv4)** dialog box and update the setting.\nThis is a good time to take snapshots of `wsfc-1` and `wsfc-2` .## Setting up Active DirectoryNow, set up the domain controller.- Use RDP to connect to the server named`wsfc-dc`.\n- Using the Windows Computer Management desktop app, set a password for the [local Administrator account](https://docs.microsoft.com/en-us/windows/security/identity-protection/access-control/local-accounts#sec-default-accounts) .\n- Enable the local Administrator account.\n- Follow the steps in the Microsoft instructions below to set up the domain controller, with these additional notes. You can use default values for most settings.- Select the DNS Server role check box. This step is not specified in the instructions.\n- Select the **Restart the destination server automatically if\nrequired** check box.\n- Promote the file server to a domain controller.\n- During the **Add a new forest** step, name your domain \"WSFC.TEST\".\n- Set the NetBIOS domain name to \"WSFC\" (the default).\n [Microsoft instructions](https://social.technet.microsoft.com/wiki/contents/articles/12370.windows-server-2012-set-up-your-first-domain-controller-step-by-step.aspx) \nThis is a good time to take a snapshot of `wsfc-dc` .\n### Create the domain user accountIt can take some time for `wsfc-dc` to restart. Before joining servers to the domain, use RDP to sign in to `wsfc-dc` to validate that the domain controller is running.\nYou need a domain user that has administrator privileges for the cluster servers. Follow these steps:- On the domain controller (`wsfc-dc`) click **Start** , and then type **dsa** to find and open the Active Directory Users and Computers app.\n- Right-click **WSFC.TEST** , point to **New** , and then click **User** .\n- For the **Full name** and the **User logon name** , enter`cluster-admin`.\n- Click **Next** .\n- Enter and confirm a password for the user. Select password options in the dialog box. For example, you can set the password to never expire.\n- Confirm the settings and then click **Finish** .\n- Make `cluster-admin` an administrator on `wsfc-dc` :- Right-click`cluster-admin`and select **Add to a group** .\n- Type **Administrators** , and click **OK** .\nThis tutorial uses the `WSFC.TEST\\cluster-admin` account as an administrator account wherever such an account is required. In a production system, follow your usual security practices for allocating accounts and permissions. For more information, see [Overview of Active Directory accounts needed by a failover cluster](https://docs.microsoft.com/en-us/windows-server/failover-clustering/configure-ad-accounts#overview-of-active-directory-accounts-needed-by-a-failover-cluster) .\n### Join the servers to the domainAdd the two cluster-node servers to the `WSFC.TEST` domain. Perform the following steps on each cluster-node server ( `wsfc-1` and `wsfc-2` ):- In **Server Manager** > **Local Server** , in the **Properties** pane, click **WORKGROUP** .\n- Click **Change** .\n- Select **Domain** and then enter \"WSFC.TEST\".\n- Click **OK** .\n- Provide the credentials for`WSFC.TEST\\cluster-admin`to join the domain.\n- Click **OK** .\n- Close the dialog boxes and follow the prompts to restart the server.\n- In **Server Manager** , make `cluster-admin` an administrator on `wsfc-1` and `wsfc-2` . Alternatively, you can manage administrative privileges by using a group policy.- On the **Tools** menu, select **Computer Management** > **Local Users\nand Groups** > **Groups** > **Administrators** , and then click **Add** .\n- Enter \"cluster-admin\" and the click **Check names** .\n- Click **OK** .\nThis is a good point to take snapshots of all three VMs.## Setting up failover clustering\n### Reserve an IP address for the cluster in Compute EngineWhen you create the failover cluster, you assign an IP address to create an administrative access point. In a production environment, you might use an IP address from a separate subnet. However, in this tutorial you reserve an IP address from the subnet you already created. Reserving the IP address prevents conflicts with other IP assignments.- Open a terminal on a host VM or open Cloud Shell. [Go to Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Reserve an IP address. For this tutorial, use `10.0.0.8` :```\ngcloud compute addresses create cluster-access-point --region [YOUR_REGION] --subnet wsfcnetsub1 --addresses 10.0.0.8\n```\n- To confirm the reservation of the IP address:```\ngcloud compute addresses list\n```\n### Create the clusterTo create and configure the failover cluster:- Use RDP to connect`wsfc-1`and`wsfc-2`.\n- Follow the steps in the Microsoft instructions below, with these additional notes:- Install the Failover Clustering feature on`wsfc-1`and`wsfc-2`. Don't install the Failover Clustering feature on`wsfc-dc`.\n- Run the Failover Cluster Manager app as the domain user`WSFC.TEST\\cluster-admin`. Otherwise, you might encounter permissions issues. It's a good idea to always run Failover Cluster Manager this way or to connect to a server as`cluster-admin`to ensure you have the required permissions.\n- Add`wsfc-1`and`wsfc-2`to the cluster as nodes.\n- When validating the configuration:- On the **Testing Options** \u200b page, select **Run only tests I\nselect** \u200b, \u200band then click **Next\u200b** .\n- On the **Test Selection** \u200b page, clear **Storage** because the **Storage\u200b** option will fail when running on Compute Engine (as it would for separate standalone physical servers).Common issues you might encounter during cluster validation include:- **Only one network interface between replicas** . You can ignore this one, because it doesn't apply in a cloud-based setup.\n- **Windows Updates not the same on both replicas** . If you configured your Windows instances to apply updates automatically, one of the nodes might have applied updates that the other hasn't downloaded yet. You should keep the servers in identical configurations.\n- **Pending reboot** . You've made changes to one of the servers, and it needs a reboot to apply. Don't ignore this one.\n- **The servers do not all have the same domain role** . You can ignore this one.\n- **The servers are not all in the same Organizational Unit (OU)** . This tutorial doesn't use an OU at all, but in a production system consider putting your cluster in its own OU. The Microsoft instructions describe this best practice.\n- **Unsigned drivers were found** . You can ignore this one.\n- On the **Summary** page, you can select **Create the cluster now usingthe validated nodes** to continue on to create the cluster, rather than closing the wizard and reopening it.\n- In the Create Cluster Wizard, on the **Access point** page:- Name your cluster \"testcluster\".\n- In the **Address** field, enter the IP address that you reserved earlier,`10.0.0.8`. [Microsoft instructions](https://docs.microsoft.com/en-us/windows-server/failover-clustering/create-failover-cluster) \n### Add the cluster administratorAdding a domain account as an administrator for the cluster enables you to perform actions on the cluster from tools such as Windows PowerShell. Add the `cluster-admin` domain account as a cluster admin.- On the cluster node that hosts the cluster resources, in Failover Cluster Manager, select your cluster in the left pane and then click **Properties** in the right pane.\n- Select the **Cluster Permissions** tab.\n- Click **Add** and then add`cluster-admin`.\n- With`cluster-admin`selected in the **Group or user names list** , select **Full Control** in the **Permissions** pane.\n- Click **Apply** and **OK** .\nThis is a good point to take snapshots.## Creating the file share witnessYou have a two-node failover cluster, but the cluster uses a voting mechanism to decide which node should be active. To achieve a quorum, you can add a file share witness.\nThis tutorial simply adds a shared folder to the domain controller server. If this server were to go offline at the same time one of the cluster nodes is restarting, the entire cluster could stop working because the remaining server can't vote by itself. For this tutorial, the assumption is that the GCP infrastructure features, such as [Live Migration](/compute/docs/instances/live-migration-process) and [automatic restart](/compute/docs/instances/setting-instance-scheduling-options#autorestart) , provide enough reliability to keep the shared folder alive.\nIf you want to create a more-highly-available file share witness, you have these options:- Use a cluster of Windows Servers to provide the share by using [Storage Spaces Direct](https://docs.microsoft.com/en-us/windows-server/storage/storage-spaces/storage-spaces-direct-overview) . This Windows Server 2016 feature can provide a highly available share for the quorum witness. For example, you could create a cluster for your Active Directory domain controller to provide both highly available domain services and provide the file share witness at the same time.\n- Use data replication software, such as SIOS Datakeeper, with Windows Failover Server Clustering for synchronous or asynchronous replication.\nFollow these steps to create the file share for the witness:- Connect to`wsfc-dc`. This server hosts the file share.\n- In Explorer, browse to the`C`drive.\n- In the title bar, click the **New Folder** button.\n- Name the new folder`shares`.\n- Double-click the`shares`folder to open it.\n- Add a new folder and name it`clusterwitness-testcluster`.\n### Configure sharing for the file share witnessYou must set permissions on the file share witness folder to enable the cluster to use it.- From Explorer, right-click the`clusterwitness-testcluster`folder and select **Properties** .\n- On the **Sharing** tab, click **Advanced Sharing** .\n- Select **Share this folder** .\n- Click **Permissions** and then click **Add** .\n- Click **Object Types** , select **Computers** , and then click **OK** .\n- Add the machine account`testcluster$`.\n- Give **Full Control** permissions to`testcluster$`.\n- Click **Apply** and then close all the dialog boxes.\n### Add the file share witness to the failover clusterNow, configure the failover cluster to use the file share witness as a quorum vote.- On the computer that hosts the cluster resources (`wsfc-1`), open the Failover Cluster Manager.\n- In the left pane, right-click the name of the cluster ( **testcluster.WSFC.TEST** ) then point to **More Actions** , and then click **Configure Cluster Quorum Settings** .\n- On the **Select Quorum Configuration Option** panel, choose **Select the quorum witness** .\n- On the **Select Quorum Witness panel** , choose **Configure a file share witness** .\n- For the **File Share Path** , enter the path to the shared folder, such as`\\\\wsfc-dc\\clusterwitness-testcluster`.\n- Confirm the settings and then click **Finish** .\n## Testing the failover clusterYour Windows Server failover cluster should now be working. You can test manually moving cluster resources between your instances. You're not done yet, but this is a good checkpoint to validate that everything you've done so far is working.- On`wsfc-1`, note the name of the **Current Host Server** in Failover Cluster Manager.\n- Run Windows PowerShell as`cluster-admin`.\n- In PowerShell, run the following command to change the current host server:```\nMove-ClusterGroup -Name \"Cluster Group\"\n```\nYou should see the name of the current host server change to the other VM.\nIf this didn't work, review the previous steps and see if you missed anything. The most common issue is a missing firewall rule that is blocking access on the network. Refer to the [Troubleshooting](#troubleshooting) section for more issues to check.\nOtherwise, you can now move on to setting up the internal load balancer, which is required in order to route network traffic to the current host server in the cluster.\nThis is a good time to take snapshots.## Adding a role to the failover clusterIn Windows failover clustering, host clustered workloads. You can use a role to specify in the cluster the IP address that your application uses. For this tutorial, you add a role for the test workload, which is the Internet Information Services (IIS) web server, and assign an IP address to the role.\n### Reserve an IP address for the role in Compute EngineTo prevent IP addressing conflicts within your subnet in Compute Engine, reserve the IP address for the role.- Open a terminal on a host VM or open Cloud Shell. [Go to Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Reserve an IP address. For this tutorial, use `10.0.0.9` :```\ngcloud compute addresses create load-balancer-ip --region [YOUR_REGION] --subnet wsfcnetsub1 --addresses 10.0.0.9\n```\n- To confirm the reservation of the IP address:```\ngcloud compute addresses list\n```\n### Add the roleFollow these steps:- In Failover Cluster Manager, in the **Actions** pane, select **Configure Role** .\n- In the **Select Role** page, select **Other Server** .\n- In the **Client Access Point** page, enter the name`IIS`.\n- Set the address to`10.0.0.9`.\n- Skip **Select Storage** and **Select Resource Types** .\n- Confirm the settings and then click **Finish** .\n## Creating the internal load balancerCreate and configure the internal load balancer, which is required in order to route network traffic to the active cluster host node. You will use the Google Cloud console, because the user interface gives you a good view into how internal load balancing is organized.\nYou will also create a Compute Engine instance group for each zone in the cluster, which the load balancer uses to manage the cluster nodes.\n### Create the instance groupsCreate an instance group in each zone that contains a cluster node and then add each node to the instance group in its zone. Don't add the domain controller `wsfc-dc` to an instance group.- Create an instance group for each zone in the cluster, replacing `[ZONE_1]` with the name of your first zone and `[ZONE_2]` with the name of your second:```\ngcloud compute instance-groups unmanaged create wsfc-group-1 --zone=[ZONE_1]\n``````\ngcloud compute instance-groups unmanaged create wsfc-group-2 --zone=[ZONE_2]\n```\n- Add the server in each zone to the instance group for that zone:```\ngcloud compute instance-groups unmanaged add-instances wsfc-group-1 --instances wsfc-1 --zone [ZONE_1]\n``````\ngcloud compute instance-groups unmanaged add-instances wsfc-group-2 --instances wsfc-2 --zone [ZONE_2]\n```\n- Confirm that your instance groups were created and that each group contains one instance:```\ngcloud compute instance-groups unmanaged list\n``````\n NAME   ZONE   NETWORK NETWORK_PROJECT MANAGED INSTANCES\n wsfc-group-1 us-central1-a wsfcnet exampleproject No  1\n wsfc-group-2 us-central1-b wsfcnet exampleproject No  1\n```\n### Create the load balancer\n- In the Google Cloud console, go to the **Load balancing** page. [Go to Load balancing](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list) \n- Click **Create Load Balancer** .\n- On the **Network Load Balancer (TCP/SSL)** card, click **Start configuration** .\n- Select **Only between my VMs** and then click **Continue.** \n- For **Name** , enter \"wsfc-lb\".\nDon't click **Create** yet.\n### Configure the backendRecall that the GCP internal load balancer uses a periodic health check to determine the active node. The health check pings the Compute Engine cluster host agent that is running on the active cluster node. The health check payload is the IP address of the application, which is represented by the clustered role. The agent responds with a value of 1 if the node is active or 0 if it is not.- Click **Backend configuration** .\n- Select your current region.\n- Select`wsfcnet`for **Network** .\n- Under **Backends** , add each instance group that you created by selecting its name and clicking **Done** .\n- Create a health check.- For **Name** , enter \"wsfc-hc\".\n- Accept the default **Protocol** setting of **TCP** and change the **Port** to \"59998\" for cluster host agent responses.\n- For **Request** , enter \"10.0.0.9\".\n- For **Response** , enter \"1\".\n- For **Check interval** , enter \"2\".\n- For **Timeout** enter \"1\".\n- Click **Save and continue** .\n### Configure the frontendThe frontend configuration creates a forwarding rule that defines how the load balancer handles incoming requests. For this tutorial, to keep it simple, you will test the system by making requests between the VMs in the subnetwork.\nIn your production system, you probably want to open the system up to external traffic, such as Internet traffic. To do this, you can [create a bastion host](/solutions/connecting-securely#bastion) that accepts external traffic and forwards it to your internal network. Using a bastion host is not covered in this tutorial.- In the center pane, click **Frontend configuration** .\n- For **Name** , enter \"wsfc-lb-fe\".\n- Select your subnetwork (`wsfcnetsub1`).\n- For **Internal IP** , select **load-balancer-ip (10.0.0.9)** . This is the same IP address that you set for the role.\n- For **Ports** , enter \"80\".\n- Click **Done** .\n### Review and finalize\n- To see a summary of the internal load balancer settings, in the center pane, click **Review and finalize** . The summary appears in the right pane.\n- Click **Create** . It takes a moment to create the load balancer. \n### Create firewall rules for the health checkYou might have noticed that the Google Cloud console notified you that the health-check system would require a firewall rule to enable the health checks to reach their targets. In this section, you set up the firewall rule.\n **Note:** The documentation about [setting up an internal load balancer](/compute/docs/load-balancing/internal#configure-a-firewall-rule-to-allow-internal-load-balancing) instructs you to create a firewall rule for the load balancer itself. For this tutorial, this is not needed because you already created a rule that allows all traffic between nodes on the subnet, and the load balancer is operating within this CIDR range and using only allowed ports.- In the Google Cloud console, go to the **Cloud Shell** . [Go to Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Run the following command to create the firewall rule:```\ngcloud compute firewall-rules create allow-health-check --network wsfcnet --source-ranges 130.211.0.0/22,35.191.0.0/16 --allow tcp:59998\n```\n### Open the Windows FirewallOn each cluster node, `wsfc-1` and `wsfc-2` , create a firewall rule in the Windows firewall to allow the load balancer to access each Windows system.- Open the Windows Firewall with Advanced Security app.\n- In the left navigation pane, select **Inbound Rules** .\n- In the right navigation pane, select **New Rule** .\n- On the **Rule Type** panel, select **Custom** as the rule type and click **Next** .\n- On the **Program** panel, accept the default and click **Next** .\n- On the **Protocol and Ports** panel:- In the **Protocol type:** field, select **TCP** .\n- In the **Local port:** field, select **Specific Ports** and enter`59998`.\n- On the **Scope** panel, under **Which remote IP addresses does this ruleapply to** :- Select **These IP addresses:** .\n- Add each of the following IP address to the **This IP address or subnet** field by clicking **Add** :- `130.211.0.0/22`\n- `35.191.0.0/16`\n- Click **Next** .\n- On the **Action** panel, accept **Allow the connection** and click **Next** .\n- On the **Profile** panel, accept the defaults and click **Next** .\n- Specify a name for the firewall rule and click **Finish** .\n## Validating the load balancerAfter your internal load balancer is running, you can inspect its status to validate that it can find a healthy instance, and then test failover again.- In the Google Cloud console, go to the **Load balancing** page. [Go to Load balancing](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list) \n- Click the name of the load balancer ( `wsfc-lb` ).In the **Backend** section of the summary, you should see the instance groups listed.In the following image from the details page of the `wsfc-lb` load balancer, instance group `wsfc-group-1` contains the active node, as indicated by **1 / 1** in the **Healthy** column. Instance group `wsfc-group-2` contains the inactive node, as indicated by **0 / 1** . If both instance groups show **0 / 1** , the load balancer might still be syncing with the nodes. Sometimes, you need to do at least one failover action to get the load balancer to find the IP address.\n- In Failover Cluster Manager, expand the cluster name and click on **Roles** . In the **Owner Node** column, note the server name for the **IIS** role.\n- Start a failover by right-clicking the **IIS** role and selecting **Move** > **Best Possible Node** . This action moves the role to the other node, as shown in the **Owner Node** column. \n- Wait until the **Status** shows **Running** .\n- Return to the **Load balancer details** page, click **Refresh** , and verify that the **1 / 1** and **0 / 1** values in the **Healthy** column have switched instance groups. \n **Tip:** You can use the gcloud CLI to check which instance is healthy, where `[REGION]` is your region:\n```\ngcloud compute backend-services get-health wsfc-lb --region=[REGION]\n```\nThe output looks like the following:\n```\nbackend: https://compute.googleapis.com/compute/v1/projects/exampleproject/zones/us-central1-a/instanceGroups/wsfc-group-1\nstatus:\n healthStatus:\n - healthState: HEALTHY\n instance: https://compute.googleapis.com/compute/v1/projects/exampleproject/zones/us-central1-a/instances/wsfc-1\n ipAddress: 10.0.0.4\n port: 80\n kind: compute#backendServiceGroupHealth\n--backend: https://compute.googleapis.com/compute/v1/projects/exampleproject/zones/us-central1-b/instanceGroups/wsfc-group-2\nstatus:\n healthStatus:\n - healthState: UNHEALTHY\n instance: https://compute.googleapis.com/compute/v1/projects/exampleproject/zones/us-central1-b/instances/wsfc-2\n ipAddress: 10.0.0.5\n port: 80\n kind: compute#backendServiceGroupHealth\n```## Installing your applicationNow that you have a cluster, you can set up your application on each node and configure it for running in a clustered environment.\nFor this tutorial, you need to set up something that can demonstrate that the cluster is really working with the internal load balancer. Set up IIS on each VM to serve a simple web page.\nYou're not setting up IIS for HA in the cluster. You are creating separate IIS instances that each serve a different web page. After a failover, the web server serves its own content, not shared content.\nSetting up your application or IIS for HA is beyond the scope of this tutorial.\n### Set up IIS\n- On each cluster node, [install IIS](https://docs.microsoft.com/en-us/iis/get-started/whats-new-in-iis-8/installing-iis-8-on-windows-server-2012) .- On the **Select role services** page, be sure that **Default Document** is selected under **Common HTTP Features** .\n- On the **Confirmation** page, select the checkbox that enables automatic restarting of the destination server.\n- Validate that each web server is working.- Use RDP to connect to the VM named`wsfc-dc`.\n- In Server Manager, click **Local Server** in the navigation pane on the left side of the window.\n- in the **Properties** section at the top, turn off **IE Enhanced Security Configuration** .\n- Open Internet Explorer.\n- Browse to the IP address of each server:```\nhttp://10.0.0.4/\n``````\nhttp://10.0.0.5/\n```\nIn each case, you see the **Welcome** page, which is the default IIS web page.\n### Edit the default web pagesChange each default web page so you can easily see which server is currently serving the page.- Use RDP to connect to the VM named`wsfc-1`.\n- Run Notepad as administrator.\n- Open`C:\\inetpub\\wwwroot\\iisstart.htm`in Notepad. Remember to browse for **All Files** , not just text files.\n- In the `<title>` element, change the text to the name of the current server. For example:```\n\u00a0 \u00a0 <title>wsfc-1</title>\n```\n- Save the HTML file.\n- Repeat these steps for `wsfc-2` , setting the `<title>` element to `wsfc-2` .\nNow, when you view a web page served from one of these servers, the name of the server appears as the title in the Internet Explorer tab.\n### Test the failover\n- Use RDP to connect to the VM named`wsfc-dc`.\n- Open Internet Explorer.\n- Browse to the IP address of the load balancer role:```\nhttp://10.0.0.9/\n```You see the **Welcome** page with the name of the current server displayed in the tab title.\n- Stop the current server to simulate a failure. In Cloud Shell, run the following command, replacing `[INSTANCE_NAME]` with the name of the current server you saw in the previous step, such as `wsfc-1` :```\ngcloud compute instances stop [INSTANCE_NAME] --zone=[ACTIVE_ZONE]\n```\n- Switch to your RDP connection to `wsfc-dc` .It can take a few moments for the load balancer to detect the move and reroute the traffic.\n- After 30 seconds or so, refresh the page in Internet Explorer.You should now see the name of the new active node displayed in the tab title. For example, if you started with `wsfc-1` active, you now see `wsfc-2` in the title. If you don't see the change right away or see a page-not-found error, refresh the browser again.\nCongratulations! You now have a working Windows Server 2016 failover cluster running on GCP.## TroubleshootingHere are some common issues you can check if things aren't working.\n### GCP firewall rules blocks health checkIf the health check isn't working, double-check that you have a firewall rule to enable incoming traffic from the IP addresses that the health check system uses: `130.211.0.0/22` and `35.191.0.0/16` .\n### Windows Firewall blocks health checkMake sure port 59998 is open in Windows Firewall on each cluster node. See [Open the Windows Firewall](#open_the_windows_firewall) .\n### Cluster nodes using DHCPIt's important that each VM in the cluster has a static IP address. If a VM is configured to use DHCP in Windows, change the networking settings in Windows to make the IPv4 address match the IP address of the VM as shown in the Google Cloud console. Also set the gateway IP address to match the address of the subnetwork gateway in the GCP VPC.\n### GCP network tags in firewall rulesIf you use network tags in your firewall rules, be sure the correct tags are set on every VM instance. This tutorial doesn't use tags, but if you've set them for some other reason, they must be used consistently.## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\nAfter you finish the tutorial, you can clean up the resources that you created so that they stop using quota and incurring charges. The following sections describe how to delete or turn off these resources.\n### Deleting the project\nThe easiest way to eliminate billing is to delete the project that you created for the tutorial.\nTo delete the project:\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.### Cleaning up resources without deleting the projectIf you need to keep your project, you can clean up the tutorial resources by deleting them individually.\nTo delete a Compute Engine instance:\n- In the Google Cloud console, go to the **VM instances** page. [Go to VM instances](https://console.cloud.google.com/compute/instances) \n- Select the checkbox for   the instance that you want to delete.\n- To delete the instance, clickmore_vert **More actions** , click **Delete** ,  and then follow the instructions.\n- In the Google Cloud console, go to the **Instance groups** page. [Go to Instance groups](https://console.cloud.google.com/compute/instanceGroups/list) \n- Select the checkbox for   the instance group that you want to delete.\n- To delete the instance group, clickdelete **Delete** .\nTo delete a load balancer:- In the Google Cloud console, go to the **Load balancing** page. [Go to Load balancing](https://console.cloud.google.com/net-services/loadbalancing) \n- Select the checkbox next to the name of the load balancer you want to delete.\n- Click the **Delete** button at the top of the page.\nTo delete a VPC network:- In the Google Cloud console, go to the **VPC networks** page. [Go to VPC networks](https://console.cloud.google.com/networking/networks) \n- Click the name of the network you want to delete.\n- Click the **DELETE VPC NETWORK** button at the top of the page.\nUse Cloud Shell to release the reserved IP addresses:- In the Google Cloud console, go to the **Cloud Shell** . [Go to Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Release the reserved IP addresses:```\ngcloud compute addresses delete cluster-access-point load-balancer-ip\n```\nTo delete a persistent disk:- In the Google Cloud console, go to the **Disks** page. [Go to Disks](https://console.cloud.google.com/compute/disks) \n- Select the checkbox next to the name of the disk you want to delete.\n- Click the **Delete** button at the top of the page.\n## What's next\n- [Tutorials about Active Directory](/architecture?text=Active%20Directory) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Compute Engine"}