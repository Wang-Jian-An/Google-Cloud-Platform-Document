{"title": "Documentation - Dataproc Container for Spark", "url": "https://cloud.google.com/distributed-cloud/hosted/docs/latest/gdch/overview", "abstract": "# Documentation - Dataproc Container for Spark\nGDCH provides a Dataproc Container for Spark. That is an Apache Spark environment for data processing. For more information about Apache Spark, see `https://spark.apache.org/` . Use containers of Dataproc Container for Spark to run new or existing Spark applications within a GDCH Kubernetes cluster with minimal alteration. If you are familiar with Spark tools, you can keep using them.\nDefine your Spark application in a YAML file, and GDCH allocates the resources for you. The Dataproc Container for Spark container starts in seconds. Spark executors scale up or shut down according to your needs.\nConfigure containers from Dataproc Container for Spark on GDCH to use specialized hardware, such as specialty hardware nodes or GPUs.\n", "content": "## Deploy the Dataproc Container for Spark service\nThe Platform Administrator (PA) must install Marketplace services for you before you can use the services. Contact your PA if you need Dataproc Container for Spark. See [Install a GDCH Marketplace software package](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/marketplace-lifecycle#install-sw) for more information.\n## Prerequisites for running Spark applications\nYou must have a service account in user clusters to use the Dataproc Container for Spark service. The Dataproc Container for Spark creates a Spark driver pod to run a Spark application. A Spark driver pod needs a Kubernetes service account in the namespace of the pod with permissions to do the following actions:\n- Create, get, list, and delete executor pods.\n- Create a Kubernetes headless service for the driver.\nBefore running a Spark application, complete the following steps to ensure you have a service account with the previous permissions in the `foo` namespace:\n- Create a service account for a Spark driver pod to use in the `foo` namespace:```\nkubectl create serviceaccount spark --kubeconfig AO_USER_KUBECONFIG --namespace=foo\n``` **Note:** The `foo` namespace corresponds to the Project `foo` namespace.\n- Create a role for granting permissions to create, get, list, and delete executor pods, and create a Kubernetes headless service for the driver in the `foo` namespace:```\nkubectl create role spark-driver --kubeconfig AO_USER_KUBECONFIG --verb=* \\--resource=pods,services,configmaps,persistentvolumeclaims \\--namespace=foo\n```\n- Create a role binding for granting the service account role access in the `foo` namespace:```\nkubectl create --kubeconfig AO_USER_KUBECONFIG \\rolebinding spark-spark-driver \\--role=spark-driver --serviceaccount=foo:spark \\--namespace=foo\n```\n**Caution:** Later, when creating `SparkApplication` and `ScheduledSparkApplication` custom resources in the `foo` namespace, specify the service account in the `.spec.driver.serviceAccount` field of the YAML file. For a sample YAML file of the `SparkApplication` specification, see [Write a Spark application specification](#spark-spec) .\n## Run sample Spark 3 applications\nContainerizing Spark applications simplifies running big data applications on your premises using GDCH. As an Application Operator (AO), run Spark applications specified in GKE objects of the `SparkApplication` custom resource type.\nTo run and use an Apache Spark 3 application on GDCH, complete the following steps:\n- Examine the `spark-operator` image in your project to find the `$DATAPROC_IMAGE` to reference in your Spark application:```\nexport DATAPROC_IMAGE=$(kubectl get pod --kubeconfig AO_USER_KUBECONFIG \\--selector app.kubernetes.io/name=spark-operator -n foo \\-o=jsonpath='{.items[*].spec.containers[0].image}' \\| sed 's/spark-operator/dataproc/')\n```For example:```\nexport DATAPROC_IMAGE=10.200.8.2:10443/dataproc-service/private-cloud-devel/dataproc:3.1-dataproc-17\n```\n- Write a `SparkApplication` specification and store it in a YAML file. For more information, see the [Write a Spark application specification](#spark-spec) section.\n- Submit, run, and monitor your Spark application as configured in a `SparkApplication` specification on the GKE cluster with the `kubectl` command. For more information, see the [Application examples](#app-examples) section.\n- Review the status of the application.\n- Optional: Review the application logs. For more information, see the [View the logs of a Spark 3 application](#spark-logs) section.\n- Use the Spark application to collect and surface the status of the driver and executors to the user.## Write a Spark application specification\nA `SparkApplication` specification includes the following components:\n- The`apiVersion`field.\n- The`kind`field.\n- The`metadata`field.\n- The`spec`section.\nFor more information, see the on GitHub: `https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/user-guide.md#writing-a-sparkapplication-spec` .\n## Application examples\nThis section includes the following examples with their corresponding `SparkApplication` specifications to run Spark applications:\n- [Spark Pi](#spark-pi) \n- [Spark SQL](#spark-sql) \n- [Spark MLlib](#spark-mllib) \n- [SparkR](#spark-r) \n### Spark Pi\nThis section contains an example to run a compute-intensive Spark Pi application that estimates (pi) by throwing darts in a circle.\nWork through the following steps to run Spark Pi:\n- Apply the following `SparkApplication` specification example in the user cluster:```\napiVersion: \"sparkoperator.k8s.io/v1beta2\"kind: SparkApplicationmetadata:\u00a0 name: spark-pi\u00a0 namespace: foospec:\u00a0 type: Python\u00a0 pythonVersion: \"3\"\u00a0 mode: cluster\u00a0 image: \"${DATAPROC_IMAGE?}\"\u00a0 imagePullPolicy: IfNotPresent\u00a0 mainApplicationFile: \"local:///usr/lib/spark/examples/src/main/python/pi.py\"\u00a0 sparkVersion: \"3.1.3\"\u00a0 restartPolicy:\u00a0 \u00a0 type: Never\u00a0 driver:\u00a0 \u00a0 cores: 1\u00a0 \u00a0 coreLimit: \"1000m\"\u00a0 \u00a0 memory: \"512m\"\u00a0 \u00a0 serviceAccount: spark\u00a0 executor:\u00a0 \u00a0 cores: 1\u00a0 \u00a0 instances: 1\u00a0 \u00a0 memory: \"512m\"\n```\n- Verify that the `SparkApplication` specification example runs and completes in 1-2 minutes using the following command:```\nkubectl --kubeconfig AO_USER_KUBECONFIG get SparkApplication spark-pi -n foo\n```\n- View the [Driver Logs](#driver-logs) to see the result:```\nkubectl --kubeconfig AO_USER_KUBECONFIG logs spark-pi-driver -n foo | grep \"Pi is roughly\"\n```An output is similar to the following:```\nPi is roughly 3.1407357036785184\n```\nFor more information, see the following resources:\n- For the application code, see the articlefrom the Apache Spark documentation:`https://spark.apache.org/examples.html`.\n- For a sample Spark Pi YAML file, see [Write a Spark application specification](#spark-spec) .\n### Spark SQL\nWork through the following steps to run Spark SQL:\n- To run a Spark SQL application that selects the `1` value, use the following query:```\nselect 1;\n```\n- Apply the following `SparkApplication` specification example in the user cluster:```\napiVersion: \"sparkoperator.k8s.io/v1beta2\"kind: SparkApplicationmetadata:\u00a0 name: pyspark-sql-arrow\u00a0 namespace: foospec:\u00a0 type: Python\u00a0 mode: cluster\u00a0 image: \"${DATAPROC_IMAGE?}\"\u00a0 imagePullPolicy: IfNotPresent\u00a0 mainApplicationFile: \"local:///usr/lib/spark/examples/src/main/python/sql/arrow.py\"\u00a0 sparkVersion: \"3.1.3\"\u00a0 restartPolicy:\u00a0 \u00a0 type: Never\u00a0 driver:\u00a0 \u00a0 cores: 1\u00a0 \u00a0 coreLimit: \"1000m\"\u00a0 \u00a0 memory: \"512m\"\u00a0 \u00a0 serviceAccount: spark\u00a0 executor:\u00a0 \u00a0 cores: 1\u00a0 \u00a0 instances: 1\u00a0 \u00a0 memory: \"512m\"\n```\n- Verify that the `SparkApplication` specification example runs and completes in less than one minute using the following command:```\nkubectl --kubeconfig AO_USER_KUBECONFIG get SparkApplication pyspark-sql-arrow -n foo\n```\n### Spark MLlib\nWork through the following steps to run Spark MLlib:\n- Use the following Scala example to run a Spark MLlib instance that performs statistical analysis and prints a result to the console:```\nimport org.apache.spark.ml.linalg.{Matrix, Vectors}import org.apache.spark.ml.stat.Correlationimport org.apache.spark.sql.Rowval data = Seq(\u00a0 Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))),\u00a0 Vectors.dense(4.0, 5.0, 0.0, 3.0),\u00a0 Vectors.dense(6.0, 7.0, 0.0, 8.0),\u00a0 Vectors.sparse(4, Seq((0, 9.0), (3, 1.0))))val df = data.map(Tuple1.apply).toDF(\"features\")val Row(coeff1: Matrix) = Correlation.corr(df, \"features\").headprintln(s\"Pearson correlation matrix:\\n $coeff1\")val Row(coeff2: Matrix) = Correlation.corr(df, \"features\", \"spearman\").headprintln(s\"Spearman correlation matrix:\\n $coeff2\")\n```\n- Apply the following `SparkApplication` specification example in the user cluster:```\napiVersion: \"sparkoperator.k8s.io/v1beta2\"kind: SparkApplicationmetadata:\u00a0 name: spark-ml\u00a0 namespace: foospec:\u00a0 type: Scala\u00a0 mode: cluster\u00a0 image: \"${DATAPROC_IMAGE?}\"\u00a0 imagePullPolicy: IfNotPresent\u00a0 mainClass: org.apache.spark.examples.ml.SummarizerExample\u00a0 mainApplicationFile: \"local:///usr/lib/spark/examples/jars/spark-examples_2.12-3.1.3.jar\"\u00a0 sparkVersion: \"3.1.3\"\u00a0 restartPolicy:\u00a0 \u00a0 type: Never\u00a0 driver:\u00a0 \u00a0 cores: 1\u00a0 \u00a0 coreLimit: \"1000m\"\u00a0 \u00a0 memory: \"512m\"\u00a0 \u00a0 serviceAccount: spark\u00a0 executor:\u00a0 \u00a0 cores: 1\u00a0 \u00a0 instances: 1\u00a0 \u00a0 memory: \"512m\"\n```\n- Verify that the `SparkApplication` specification example runs and completes in less than one minute using the following command:```\nkubectl --kubeconfig AO_USER_KUBECONFIG get SparkApplication spark-ml -n foo\n```\n### SparkR\nWork through the following steps to run SparkR:\n- Use the following example code to run a SparkR instance that loads a bundled dataset and prints the first line:```\nlibrary(SparkR)sparkR.session()df <- as.DataFrame(faithful)head(df)\n```\n- Apply the following `SparkApplication` specification example in the user cluster:```\napiVersion: \"sparkoperator.k8s.io/v1beta2\"kind: SparkApplicationmetadata:\u00a0 name: spark-r-dataframe\u00a0 namespace: foospec:\u00a0 type: R\u00a0 mode: cluster\u00a0 image: \"${DATAPROC_IMAGE?}\"\u00a0 imagePullPolicy: Always\u00a0 mainApplicationFile: \"local:///usr/lib/spark/examples/src/main/r/dataframe.R\"\u00a0 sparkVersion: \"3.1.3\"\u00a0 restartPolicy:\u00a0 \u00a0 type: Never\u00a0 driver:\u00a0 \u00a0 cores: 1\u00a0 \u00a0 coreLimit: \"1000m\"\u00a0 \u00a0 memory: \"512m\"\u00a0 \u00a0 serviceAccount: spark\u00a0 executor:\u00a0 \u00a0 cores: 1\u00a0 \u00a0 instances: 1\u00a0 \u00a0 memory: \"512m\"\n```\n- Verify that the `SparkApplication` specification example runs and completes in less than one minute using the following command:```\nkubectl --kubeconfig AO_USER_KUBECONFIG get SparkApplication spark-r-dataframe -n foo\n```## View the logs of a Spark 3 application\nSpark has the following two log types that you can visualize:\n- [Driver logs](#driver-logs) \n- [Event logs](#event-logs) \nUse the terminal to run commands.\n### Driver logs\nWork through the following steps to view the driver logs of your Spark application:\n- Find your Spark driver pod:```\nkubectl -n spark get pods\n```\n- Open the logs from the Spark driver pod:```\nkubectl -n spark logs DRIVER_POD\n```Replace `` with the name of the Spark driver pod that you found in the previous step.\n### Event logs\nYou can find event logs at the path specified in the YAML file of the `SparkApplication` specification.\nWork through the following steps to view the event logs of your Spark application:\n- Open the YAML file of the`SparkApplication`specification.\n- Locate the`spec`field in the file.\n- Locate the`sparkConf`field nested in the`spec`field.\n- Locate the value of the`spark.eventLog.dir`field nested in the`sparkConf`section.\n- Open the path to view event logs.\nFor a sample YAML file of the `SparkApplication` specification, see [Write a Spark application specification](#spark-spec) .\nContact your account manager for more information.", "guide": "Documentation"}