{"title": "Compute Engine - Enable faster network packet processing with DPDK", "url": "https://cloud.google.com/compute/docs/instances", "abstract": "# Compute Engine - Enable faster network packet processing with DPDK\nThis document explains how to enable the [Data Plane Development Kit (DPDK)](https://www.dpdk.org/) on a virtual machine (VM) instance for faster network packet processing.\nDPDK is a framework for performance-intensive applications that require fast packet processing, low latency, and consistent performance. DPDK provides a set of data plane libraries and a network interface controller (NIC) that bypass the kernel network and run directly in the user space. For example, enabling DPDK on your VM is useful when running the following:\n- Network function virtualization (NFV) deployments\n- Software-defined networking (SDN) applications\n- Video streaming or voice over IP applications\nYou can run DPDK on a VM using one of the following virtual NIC (vNIC) types:\n- Recommended: [gVNIC](/compute/docs/networking/using-gvnic) A high-performance, secure, and scalable virtual network interface specifically designed for Compute Engine that succeeds virtIO as the next generation vNIC.\n- [VirtIO-Net](https://ozlabs.org/%7Erusty/virtio-spec/virtio-0.9.5.pdf) An open source ethernet driver that lets VMs efficiently access physical hardwares, such as block storage and networking adapters.\nOne issue with running DPDK in a virtual environment, instead of on physical hardware, is that virtual environments lack support for SR-IOV and I/O Memory Management Unit (IOMMU) for high-performing applications. To overcome this limitation, you must run DPDK on guest physical addresses rather than host virtual addresses by using one of the following drivers:\n- [Userspace I/O (UIO)](https://www.kernel.org/doc/html/v4.18/driver-api/uio-howto.html) \n- IOMMU-less [Virtual Function I/O (VFIO)](https://docs.kernel.org/driver-api/vfio.html) ", "content": "## Before you begin\n- If you haven't already, set up authentication. [Authentication](/compute/docs/authentication) is the process by which your identity is verified for access to Google Cloud services and APIs. To run code or samples from a local development environment, you can authenticate to Compute Engine as follows.Select the tab for how you plan to use the samples on this page:\nWhen you use the Google Cloud console to access Google Cloud services and   APIs, you don't need to set up authentication.- [Install](/sdk/docs/install) the Google Cloud CLI, then [initialize](/sdk/docs/initializing) it by running the following command:```\ngcloud init\n``` **Note:** If you installed the gcloud CLI  previously, make sure you have the latest version by running`gcloud components  update`.\n- [ Set a default region and zone](/compute/docs/gcloud-compute#set_default_zone_and_region_in_your_local_client) .\nTo use the REST API samples on this page in a local development environment, you use the credentials you provide to the gcloud CLI.- [Install](/sdk/docs/install) the Google Cloud CLI, then [initialize](/sdk/docs/initializing) it by running the following command:\n- ```\ngcloud init\n```## Requirements\nWhen creating a VM to run DPDK on, make sure of the following:\n- To avoid a lack of network connectivity when running your applications, use two Virtual Private Cloud networks:- A VPC network for the control plane\n- A VPC network for the data plane\n- The two VPC networks must both specify the following:- A subnet with a unique IP address range\n- The same region for their subnets\n- The same type of VNIC\u2014either gVNIC or VirtIO-Net\n- When creating the VM:- You must specify the same region as the two VPC networks' subnets.\n- You must specify the vNIC type you plan to use with DPDK.\n- You must specify a [supported machine series](/compute/docs/machine-resource#machine_type_comparison) for gVNIC or VirtIO-Net.\n## Restrictions\nRunning DPDK on a VM has the following restrictions:\n- You can only use [single-stack subnets](/vpc/docs/subnets#subnet-types) for the two VPC networks used in the VM.\n- If you're using gVNIC as the vNIC type for the two VPC networks, make sure of the following:- You must use DPDK version 22.11 or later.\n- You can only use [supported disk images](/compute/docs/images/os-details#networking) .\n- If you want to enable [per VM Tier_1 networking performance](/compute/docs/networking/configure-vm-with-high-bandwidth-configuration) for higher network performance when creating the VM, you must specify the following:- gVNIC as the vNIC type\n- A [supported machine type](/compute/docs/networking/configure-vm-with-high-bandwidth-configuration#limitations) with 30 vCPUs or more\n## Configure a VM to run DPDK\nThis section explains how to create a VM to run DPDK on.\n### Create the VPC networks\nCreate two VPC networks, for the data plane and control plane, using the Google Cloud console, Google Cloud CLI, or Compute Engine API. You can later specify these networks when creating the VM.\n- Create a VPC network for the data plane:- In the Google Cloud console, go to **VPC networks** . [Go to VPC networks](https://console.cloud.google.com/networking/networks/list) The **VPC networks** page opens.\n- Click add_box **Create VPC network** .The **Create a VPC network** page opens.\n- In the **Name** field, enter a name for your network.\n- In the **New subnet** section, do the following:- In the **Name** field, enter a name for your subnet.\n- In the **Region** menu, select a region for your subnet.\n- Select **IPv4 (single-stack)** (default).\n- In the **IPv4 range** , enter a [valid IPv4 range](/vpc/docs/subnets#ipv4-ranges) address in CIDR notation.\n- Click **Done** .\n- Click **Create** .The **VPC networks** page opens. It can take up to a minute before the creation of the VPC network completes.\n- Create a VPC network for the control plane with a firewall rule to allow SSH connections into the VM:- Click add_box **Create VPC network** again.The **Create a VPC network** page opens.\n- In the **Name** field, enter a name for your network.\n- In the **New subnet** section, do the following:- In the **Name** field, enter a name for the subnet.\n- In the **Region** menu, select the same region you specified for the subnet of the data plane network.\n- Select **IPv4 (single-stack)** (default).\n- In the **IPv4 range** , enter a [valid IPv4 range](/vpc/docs/subnets#ipv4-ranges) address in CIDR notation. **Important:** Specify a different IPv4 range than the one you specified in the subnet for the data plane network. Otherwise, creating the network fails.\n- Click **Done** .\n- In the **IPv4 firewall rules** tab, select the **NETWORK_NAME-allow-ssh** checkbox.Where is the network name you specified in the previous steps.\n- Click **Create** .The **VPC networks** page opens. It can take up to a minute before the creation of the VPC network completes.- To create a VPC network for the data plane, follow these steps:- Create a VPC network with a manually-created subnet using the [gcloud compute networks create command](/sdk/gcloud/reference/compute/networks/create) with the `--subnet-mode` flag set to `custom` .```\ngcloud compute networks create DATA_PLANE_NETWORK_NAME \\\n --bgp-routing-mode=regional \\\n --mtu=MTU \\\n --subnet-mode=custom\n```Replace the following:- `` : the name for the VPC network for the data plane.\n- `` : the maximum transmission unit (MTU), which is the largest packet size of the network. The value must be between `1300` and `8896` . The default value is `1460` . Before setting the MTU to a value higher than `1460` , see [Maximum transmission unit](/vpc/docs/mtu) .\n- Create a subnet for the VPC data plane network you've just created using the [gcloud compute networks subnets create command](/sdk/gcloud/reference/compute/networks/subnets/create) .```\ngcloud compute networks subnets create DATA_PLANE_SUBNET_NAME \\\n --network=DATA_PLANE_NETWORK_NAME \\\n --range=DATA_PRIMARY_RANGE \\\n --region=REGION\n```Replace the following:- `` : the name of the subnet for the data plane network.\n- `` : the name of the data plane network you specified in the previous steps.\n- `` : a [valid IPv4 range](/vpc/docs/subnets#ipv4-ranges) for the subnet in CIDR notation.\n- `` : the region where to create the subnet.\n- To create a VPC network for the control plane with a firewall rule to allow SSH connections into the VM, follow these steps:- Create a VPC network with a manually-created subnet using the [gcloud compute networks create command](/sdk/gcloud/reference/compute/networks/create) with the `--subnet-mode` flag set to `custom` .```\ngcloud compute networks create CONTROL_PLANE_NETWORK_NAME \\\n --bgp-routing-mode=regional \\\n --mtu=MTU \\\n --subnet-mode=custom\n```Replace the following:- `` : the name for the VPC network for the control plane.\n- `` : the MTU, which is the largest packet size of the network. The value must be between `1300` and `8896` . The default value is `1460` . Before setting the MTU to a value higher than `1460` , see [Maximum transmission unit](/vpc/docs/mtu) .\n- Create a subnet for the VPC control plane network you've just created using the [gcloud compute networks subnets create command](/sdk/gcloud/reference/compute/networks/subnets/create) .```\ngcloud compute networks subnets create CONTROL_PLANE_SUBNET_NAME \\\n --network=CONTROL_PLANE_NETWORK_NAME \\\n --range=CONTROL_PRIMARY_RANGE \\\n --region=REGION\n```Replace the following:- `` : the name of the subnet for the control plane network.\n- `` : the name of the control plane network you specified in the previous steps.\n- `` : a [valid IPv4 range](/vpc/docs/subnets#ipv4-ranges) for the subnet in CIDR notation. **Important:** Specify a different IPv4 range than the one you specified in the data plane network's subnet. Otherwise, creating the subnet fails.\n- `` : the region where to create the subnet, which must match with the region you specified in the data plane network's subnet.\n- Create a VPC firewall rule that allows SSH into the control plane network using the [gcloud compute firewall-rules create command](/sdk/gcloud/reference/compute/firewall-rules/create) with the `--allow` flag set to `tcp:22` .```\ngcloud compute firewall-rules create FIREWALL_RULE_NAME \\\n --action=allow \\\n --network=CONTROL_PLANE_NETWORK_NAME \\\n --rules=tcp:22\n```Replace the following:- `` : the name of the firewall rule.\n- `` : the name of the control plane network you created in the previous steps.\n- To create a VPC network for the data plane, follow these steps:- Create a VPC network with a manually-created subnet by making a `POST` request to the [networks.insert method](/compute/docs/reference/rest/v1/networks/insert) with the `autoCreateSubnetworks` field set to `false` .```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks{\u00a0 \"autoCreateSubnetworks\": false,\u00a0 \"name\": \"DATA_PLANE_NETWORK_NAME\",\u00a0 \"mtu\": MTU}\n```Replace the following:- `` : the project ID of the current project.\n- `` : the name for the network for the data plane.\n- `` : the maximum transmission unit (MTU), which is the largest packet size of the network. The value must be between `1300` and `8896` . The default value is `1460` . Before setting the MTU to a value higher than `1460` , see [Maximum transmission unit](/vpc/docs/mtu) .\n- Create a subnet for the VPC data plane network by making a `POST` request to the [subnetworks.insert method](/compute/docs/reference/rest/v1/subnetworks/insert) .```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/REGION/subnetworks{\u00a0 \"ipCidrRange\": \"DATA_PRIMARY_RANGE\",\u00a0 \"name\": \"DATA_PLANE_SUBNET_NAME\",\u00a0 \"network\": \"projects/PROJECT_ID/global/networks/DATA_PLANE_NETWORK_NAME\"}\n```Replace the following:- `` : the project ID of the project where the data plane network is located.\n- `` : the region where you want to create the subnet.\n- `` : the primary [IPv4 range](/vpc/docs/subnets#manually_created_subnet_ip_ranges) for the new subnet in CIDR notation.\n- `` : the name of the subnet for the data plane network you created in the previous step.\n- `` : the name of the data plane network you created in the previous step.\n- To create a VPC network for the control plane with a firewall rule to allow SSH into the VM, follow these steps:- Create a VPC network with a manually-created subnet by making a `POST` request to the [networks.insert method](/compute/docs/reference/rest/v1/networks/insert) with the `autoCreateSubnetworks` field set to `false` .```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks{\u00a0 \"autoCreateSubnetworks\": false,\u00a0 \"name\": \"CONTROL_PLANE_NETWORK_NAME\",\u00a0 \"mtu\": MTU}\n```Replace the following:- `` : the project ID of the current project.\n- `` : the name for the network for the control plane.\n- `` : the MTU, which is the largest packet size of the network. The value must be between `1300` and `8896` . The default value is `1460` . Before setting the MTU to a value higher than `1460` , see [Maximum transmission unit](/vpc/docs/mtu) .\n- Create a subnet for the VPC data control network by making a `POST` request to the [subnetworks.insert method](/compute/docs/reference/rest/v1/subnetworks/insert) .```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/REGION/subnetworks{\u00a0 \"ipCidrRange\": \"CONTROL_PRIMARY_RANGE\",\u00a0 \"name\": \"CONTROL_PLANE_SUBNET_NAME\",\u00a0 \"network\": \"projects/PROJECT_ID/global/networks/CONTROL_PLANE_NETWORK_NAME\"}\n```Replace the following:- `` : the project ID of the project where the control plane network is located.\n- `` : the region where you want to create the subnet.\n- `` : the primary [IPv4 range](/vpc/docs/subnets#manually_created_subnet_ip_ranges) for the new subnet in CIDR notation. **Important:** Specify a different IPv4 range than the one you specified in the data plane network's subnet. Otherwise, creating the subnet fails.\n- `` : the name of the subnet for the control plane network you created in the previous step.\n- `` : the name of the control plane network you created in the previous step.\n- Create a VPC firewall rule that allows SSH into the control plane network by making a `POST` request to the [firewalls.insert method](/compute/docs/reference/rest/v1/firewalls/insert) . In the request, set the `IPProtocol` field to `tcp` and the `ports` field to `22` .```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/firewalls{\u00a0 \"allowed\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"IPProtocol\": \"tcp\",\u00a0 \u00a0 \u00a0 \"ports\": [ \"22\" ]\u00a0 \u00a0 }\u00a0 ],\u00a0 \"network\": \"projects/PROJECT_ID/global/networks/CONTROL_PLANE_NETWORK_NAME\"}\n```Replace the following:- `` : the project ID of the project where the control plane network is located.\n- `` : the name of the control plane network you created in the previous steps.\nFor more configuration options when creating a VPC network, see [Create and manage VPC networks](/vpc/docs/create-modify-vpc-networks) .\n### Create a VM that uses the VPC networks for DPDK\nCreate a VM that enables gVNIC or virtIO-Net on the two VPC networks that you created previously using the Google Cloud console, gcloud CLI, and Compute Engine API.\nRecommended: Specify [Ubuntu LTS](/compute/docs/images/os-details#ubuntu_lts) or [Ubuntu Pro](/compute/docs/images/os-details#ubuntu_pro) as the operating system image because of their package manager support for the UIO and IOMMU-less VFIO drivers. If you don't want to specify any of these operating systems, specifying [Debian 11 or later](/compute/docs/images/os-details#debian) is recommended for faster packet processing.\nCreate a VM that uses the two VPC network subnets you created in the previous steps by doing the following:- In the Google Cloud console, go to **VM instances** . [Go to VM instances](https://console.cloud.google.com/compute/instances) The **VM instances** page opens.\n- Click add_box **Create instance** .The **Create an instance** page opens.\n- In the **Name** field, enter a name for your VM.\n- In the **Region** menu, select the same region where you created your networks in the previous steps. **Important:** Attempting to create a VM in a different region from where the control and data plane networks exist causes errors.\n- In the **Zone** menu, select a zone for your VM.\n- In the **Machine configuration** section, do the following:- Select one of the following options:- For common workloads, select the **General purpose** tab (default).\n- For performance-intensive workloads, select the **Compute optimized** tab.\n- For high memory-to-vCPUs ratios workloads, select the **Memory optimized** tab.\n- For workloads that use Graphics processing units (GPUs), select the **GPUs** tab.\n- Optional. If you specified **GPUs** in the previous step and you want to change the GPU to attach to the VM, do one or more of the following:- In the **GPU type** menu, select a type of GPU.\n- In the **Number of GPUs** menu, select the number of GPUs.\n- In the **Series** menu, select a machine series. **Important:** gVNIC is supported with all machine series, but VirtIO-Net is not supported with the newest machine series (third generation and T2A). If you choose a machine series that doesn't support your vNIC type, creating the VM fails.\n- In the **Machine type** menu, select a machine type.\n- Optional: Expand **Advanced configurations** , and follow the prompts to further customize the machine for this VM.\n- Optional: In the **Boot disk** section, click **Change** , and then follow the prompts to change the disk image. **Important:** If you specify gVNIC as the vNIC type for this VM, make sure to specify a [supported disk image](/compute/docs/images/os-details#networking) . Otherwise, creating the VM fails.\n- Expand the **Advanced options** section.\n- Expand the **Networking** section.\n- In the **Network performance configuration** section, do the following:- In the **Network interface card** menu, select one of the following:- To use gVNIC, select **gVNIC** .\n- To use VirtIO-Net, select **VirtIO** .\n **Note:** The value **-** in the **Network interface card** menu indicates that the vNIC type can be either gVNIC or VirtIO-Net depending on the machine family type. If both gVNIC and VirtIO-Net are available for a VM, the default is VirtIO-Net.\n- Optional: For higher network performance and reduced latency, select the **Enable Tier_1 networking** checkbox. **Important:** You can only enable Tier_1 networking when you use gVNIC and specify a [supported machine type](/compute/docs/networking/configure-vm-with-high-bandwidth-configuration#limitations) that has 30 vCPUs or more. Otherwise, creating the VM fails.\n- In the **Network interfaces** section, do the following:- In the **default** row, click delete **Delete item \"default\"** .\n- Click **Add network interface** .The **New network interface** section appears.\n- In the **Network** menu, select the control plane network you created in the previous steps.\n- Click **Done** .\n- Click **Add network interface** again.The **New network interface** section appears.\n- In the **Network** menu, select the data plane network you created in the previous steps.\n- Click **Done** .\n- Click **Create** .The **VM instances** page opens. It can take up to a minute before the creation of the VM completes.\nCreate a VM that uses the two VPC network subnets you created in the previous steps by using the [gcloud compute instances create command](/sdk/gcloud/reference/compute/instances/create) with the following flags:\n```\ngcloud compute instances create VM_NAME \\\n --image-family=IMAGE_FAMILY \\\n --image-project=IMAGE_PROJECT \\\n --machine-type=MACHINE_TYPE \\\n --network-interface=network=CONTROL_PLANE_NETWORK_NAME,subnet=CONTROL_PLANE_SUBNET_NAME,nic-type=VNIC_TYPE \\\n --network-interface=network=DATA_PLANE_NETWORK_NAME,subnet=DATA_PLANE_SUBNET_NAME,nic-type=VNIC_TYPE \\\n --zone=ZONE\n```\nReplace the following:- `` : the name of the VM.\n- `` : the image family for the operating system that the boot disk will be initialized with. Alternatively, you can specify the `--image=` `` flag and replace `` with a specific version of an image. Learn how to [view a list of images](/compute/docs/images#list_of_public_images_available_on) available in the Compute Engine images project.\n- `` : the name of the image project that contains the disk image. **Important:** If you specify gVNIC as the vNIC type for this VM, make sure to specify a [supported image family and project](/compute/docs/images/os-details) . Otherwise, creating the VM fails.\n- `` : a machine type, [predefined](/compute/docs/machine-resource) or [custom](/compute/docs/instances/creating-instance-with-custom-machine-type) , for the VM. **Important:** gVNIC is supported with all machine series, but VirtIO-Net is not supported with the newest machine series (third generation and T2A). If you choose a machine series that doesn't support your vNIC type, creating the VM fails.\n- `` : the vNIC type to use for the control plane and data plane networks. The value must be one of the following:- To use gVNIC, specify `GVNIC` .\n- To use VirtIO-Net, specify `VIRTIO_NET` .\n- `` : the name of the control plane network you created in the previous steps.\n- `` : the name of the subnet for the control plane network you created in the previous steps.\n- `` : the name of the data plane network you created in the previous steps.\n- `` : the name of the subnet for the control plane network you automatically created in the previous steps.\n- `` : the zone where to create the VM in. Specify a zone within the same region of the subnet you created in the previous steps.\nFor example, to create a VM named `dpdk-vm` in the `us-central1-a` zone that specifies a SSD persistent disk of 512 GB, a predefined C2 machine type with 60 vCPUs, Tier_1 networking, and a data plane and a control plane network that both use gVNIC, run the following command:\n```\ngcloud compute instances create dpdk-vm \\\n --boot-disk-size=512GB \\\n --boot-disk-type=pd-ssd \\\n --image-project=ubuntu-os-cloud \\\n --image-family=ubuntu-2004-lts \\\n --machine-type=c2-standard-60 \\\n --network-performance-configs=total-egress-bandwidth-tier=TIER_1 \\\n --network-interface=network=control,subnet=control,nic-type=GVNIC \\\n --network-interface=network=data,subnet=data,nic-type=GVNIC \\\n --zone=us-central1-a\n```\nCreate a VM that uses the two VPC network subnets you created in the previous steps by making a `POST` request to the [instances.insert method](/compute/docs/reference/rest/v1/instances/insert) with the following fields:\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/ZONE/instances{\u00a0 \"name\": \"VM_NAME\",\u00a0 \"machineType\": \"MACHINE_TYPE\",\u00a0 \"disks\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"initializeParams\": {\u00a0 \u00a0 \u00a0 \u00a0 \"sourceImage\": \"projects/IMAGE_PROJECT/global/images/IMAGE_FAMILY\"\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 ],\u00a0 \"networkInterfaces\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"network\": \"global/networks/CONTROL_PLANE_NETWORK_NAME\",\u00a0 \u00a0 \u00a0 \"subnetwork\": \"regions/REGION/subnetworks/CONTROL_PLANE_SUBNET_NAME\",\u00a0 \u00a0 \u00a0 \"nicType\": \"VNIC_TYPE\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"network\": \"global/networks/DATAPLANE_NETWORK_NAME\",\u00a0 \u00a0 \u00a0 \"subnetwork\": \"regions/REGION/subnetworks/DATA_PLANE_SUBNET_NAME\",\u00a0 \u00a0 \u00a0 \"nicType\": \"VNIC_TYPE\"\u00a0 \u00a0 }\u00a0 ]}\n```\nReplace the following:- `` : the project ID of the project where the control plane VPC network and the data plane VPC network are located.\n- `` : the zone where to create the VM in. **Important** : Make sure to specify a zone that meets the following requirements:- The zone is within the same region where the subnets of the control and data plane networks are located.\n- The zone of the VM and the zone containing the machine type to use for the VM are matching.\nAttempting to create a VM outside of the region where the specified subnets exist or in a zone that doesn't match the zone of the specified machine type causes errors.\n- `` : the name of the VM.\n- `` : a machine type, [predefined](/compute/docs/machine-resource) or [custom](/compute/docs/instances/creating-instance-with-custom-machine-type) , for the VM. **Important:** gVNIC is supported with all machine series, but VirtIO-Net is not supported with the newest machine series (third generation and T2A). If you choose a machine series that doesn't support your vNIC type, creating the VM fails.\n- `` : the name of the image project that contains the disk image.\n- `` : the image family for the operating system that the boot disk will be initialized with. Alternatively, you can specify a specific version of an image. Learn how to [view a list of images](/compute/docs/images#list_of_public_images_available_on) in the Compute Engine images project. **Important:** If you specify gVNIC as the vNIC type for this VM, make sure to specify a [supported disk image](/compute/docs/images/os-details#networking) . Otherwise, creating the VM fails.\n- `` : the name of the control plane network you created in the previous steps.\n- `` : the region where the subnets of the control plane and data plane networks exist.\n- `` : the name of the subnet for the control plane network you created in the previous steps.\n- `` : the vNIC type to use for the control plane and data plane networks. The value must be one of the following:- To use gVNIC, specify `GVNIC` .\n- To use VirtIO-Net, specify `VIRTIO_NET` .\n- `` : the name of the data plane network you created in the previous steps.\n- `` : the name of the subnet for the control plane network you created in the previous steps.\nFor example, to create a VM named `dpdk-vm` in the `us-central1-a` zone that specifies a SSD persistent disk of 512 GB, a predefined C2 machine type with 60 vCPUs, Tier_1 networking, and a data plane and a control plane network that both use gVNIC, make the following `POST` request:\n```\nPOST https://compute.googleapis.com/compute/v1/projects/example-project/zones/us-central1-a/instances{\u00a0 \"name\": \"dpdk-vm\",\u00a0 \"machineType\": \"c2-standard-60\",\u00a0 \"disks\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"initializeParams\": {\u00a0 \u00a0 \u00a0 \u00a0 \"diskSizeGb\": \"512GB\",\u00a0 \u00a0 \u00a0 \u00a0 \"diskType\": \"pd-ssd\",\u00a0 \u00a0 \u00a0 \u00a0 \"sourceImage\": \"projects/ubuntu-os-cloud/global/images/ubuntu-2004-lts\"\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \"boot\": true\u00a0 \u00a0 }\u00a0 ],\u00a0 \"networkInterfaces\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"network\": \"global/networks/control\",\u00a0 \u00a0 \u00a0 \"subnetwork\": \"regions/us-central1/subnetworks/control\",\u00a0 \u00a0 \u00a0 \"nicType\": \"GVNIC\"\u00a0 \u00a0 },\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"network\": \"global/networks/data\",\u00a0 \u00a0 \u00a0 \"subnetwork\": \"regions/us-central1/subnetworks/data\",\u00a0 \u00a0 \u00a0 \"nicType\": \"GVNIC\"\u00a0 \u00a0 }\u00a0 ],\u00a0 \"networkPerformanceConfig\": {\u00a0 \u00a0 \"totalEgressBandwidthTier\": \"TIER_1\"\u00a0 }}\n```\nFor more configuration options when creating a VM, see [Create and start a VM instance](/compute/docs/instances/create-start-instance) .\n## Install DPDK on your VM\nTo install DPDK on your VM, follow these steps:\n- Connect to the VM you created in the previous section [using SSH](/compute/docs/connect/standard-ssh) .\n- Configure the dependencies for DPDK installation:```\nsudo apt-get update && sudo apt-get upgrade -yq\nsudo apt-get install -yq build-essential ninja-build python3-pip \\\n linux-headers-$(uname -r) pkg-config libnuma-dev\nsudo pip install pyelftools meson\n```\n- Install DPDK:```\nwget https://fast.dpdk.org/rel/dpdk-23.07.tar.xz\ntar xvf dpdk-23.07.tar.xz\ncd dpdk-23.07\n``` **Important:** If you specified gVNIC as the vNIC type in the previous steps, you must install DPDK version 22.11 or later. Using an earlier version of DPDK causes errors when you try to test or use DPDK on your VM.\n- To build DPDK with the examples:```\nmeson setup -Dexamples=all build\nsudo ninja -C build install; sudo ldconfig\n```## Install driver\nTo prepare DPDK to run on a driver, install the driver by selecting one of the following methods:\n- [Install a IOMMU-less VFIO](#install-iommu-less-vfio) .\n- [Install UIO](#install-uio) .\n### Install a IOMMU-less VFIO\nTo install the IOMMU-less VFIO driver, follow these steps:\n- Check if VFIO is enabled:```\ncat /boot/config-$(uname -r) | grep NOIOMMU\n```If VFIO isn't enabled, then follow the steps in [Install UIO](#install-uio) .\n- Enable the [No-IOMMU mode](https://lwn.net/Articles/660745/) in VFIO:```\nsudo bash -c 'echo 1 > /sys/module/vfio/parameters/enable_unsafe_noiommu_mode'\n```\n### Install UIO\nTo install the UIO driver on DPDK, select one of the following methods:\n- [Install UIO using git](#install-uio-git) .\n- [Install UIO using Linux packages](#install-uio-package) .To install the UIO driver on DPDK using `git` , follow these steps:\n- Clone the [igb_uio git repository](https://github.com/ceph/dpdk/blob/master/lib/librte_eal/linuxapp/igb_uio/igb_uio.c) to a disk in your VM:```\ngit clone https://dpdk.org/git/dpdk-kmods\n```\n- From the parent directory of the cloned git repository, build the module and install the UIO driver on DPDK:```\npushd dpdk-kmods/linux/igb_uio\nsudo make\nsudo depmod && sudo insmod igb_uio.ko\npopd\n```To install the UIO driver on DPDK using Linux packages, follow these steps:\n- Install the [dpdk-igb-uio-dkms package](https://packages.ubuntu.com/focal/dpdk-igb-uio-dkms) :```\nsudo apt-get install -y dpdk-igb-uio-dkms\n```\n- Install the UIO driver on DPDK:```\nsudo modprobe igb_uio\n```## Bind DPDK to a driver and test it\nTo bind DPDK to the driver you installed in the previous section, follow these steps:\n- Get the Peripheral Component Interconnect (PCI) slot number for the current network interface:```\nsudo lspci | grep -e \"gVNIC\" -e \"Virtio network device\"\n```For example, if the VM is using `ens4` as the network interface, the PCI slot number is `00:04.0` .\n- Stop the network interface connected to the network adaptor:```\nsudo ip link set NETWORK_INTERFACE_NAME down\n```Replace `` with the name of the network interface specified in the VPC networks. To see which network interface the VM is using, view the configuration of the network interface:```\nsudo ifconfig\n```\n- Bind DPDK to the driver:```\nsudo dpdk-devbind.py --bind=DRIVER PCI_SLOT_NUMBER\n```Replace the following:- `` : the driver to bind DPDK on. Specify one of the following values:- UIO driver: `igb_uio`\n- IOMMU-less VFIO driver: `vfio-pci`\n- `` : the PCI slot number of the current network interface formatted as `00:0` `` `.0` .\n- Create the `/mnt/huge` directory, and then create some hugepages for DPDK to use for buffers:```\nsudo mkdir /mnt/huge\nsudo mount -t hugetlbfs -o pagesize=1G none /mnt/huge\nsudo bash -c 'echo 4 > /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages'\nsudo bash -c 'echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages'\n```\n- Test that DPDK can use the network interface you created in the previous steps by running the `testpmd` example application that is included with the DPDK libraries:```\nsudo ./build/app/dpdk-testpmd\n```For more information about testing DPDK, see [Testpmd Command-line Options](https://doc.dpdk.org/guides/testpmd_app_ug/run_app.html#testpmd-command-line-options) .## Unbind DPDK\nAfter using DPDK, you can unbind it from the driver you've installed in the previous section. To unbind DPDK, follow these steps:\n- Unbind DPDK from the driver:```\nsudo dpdk-devbind.py -u PCI_SLOT_NUMBER\n```Replace `` with the PCI slot number you specified in the previous steps. If you want to verify the PCI slot number for the current network interface:```\nsudo lspci | grep -e \"gVNIC\" -e \"Virtio network device\"\n```For example, if the VM is using `ens4` as the network interface, the PCI slot number is `00:04.0` .\n- Reload the Compute Engine network driver:```\nsudo bash -c 'echo PCI_SLOT_NUMBER > /sys/bus/pci/drivers/VNIC_DIRECTORY/bind'\nsudo ip link set NETWORK_INTERFACE_NAME up\n```Replace the following:- `` : the PCI slot number you specified in the previous steps.\n- `` : the directory of the vNIC. Depending on the vNIC type you're using, specify one of the following values:- gVNIC: `gvnic`\n- VirtIO-Net: `virtio-pci`\n- `` : the name of the network interface you specified in the previous section.\n## What's next\n- Review the [network bandwidth rates](/compute/docs/networking/configure-vm-with-high-bandwidth-configuration#bandwidth-tiers) for your machine type.\n- Learn more about [creating and managing VPC networks](/vpc/docs/create-modify-vpc-networks) .\n- Learn more about [higher MTU settings with jumbo frames](/compute/docs/network-bandwidth#jumbo-mtu) .\n- Learn more about [TCP optimization for network performance](/compute/docs/networking/tcp-optimization-for-network-performance-in-gcp-and-hybrid) .", "guide": "Compute Engine"}