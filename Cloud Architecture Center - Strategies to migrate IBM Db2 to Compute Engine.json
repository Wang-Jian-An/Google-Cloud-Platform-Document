{"title": "Cloud Architecture Center - Strategies to migrate IBM Db2 to Compute Engine", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Strategies to migrate IBM Db2 to Compute Engine\nThis document describes best practices for a homogeneous Db2 migration to [Compute Engine](/compute) . It is intended for database admins, system admins and software, database, and ops engineers who are migrating Db2 environments to Google Cloud. Migrations from Db2 to other types of databases are outside the scope of this document.\n", "content": "## Terminology\n## Architecture\nA Db2 cluster usually consists of at least primary and principal standby nodes with HADR between them. In newer versions of Db2, you can also add auxiliary standby nodes that serve DR purposes.\nThe following diagram depicts a source environment.\nIn this environment, the primary and the principal standby are in one data center and the auxiliary standbys are in different data centers.\nA migration goal is to recreate this environment on Google Cloud as shown in the following diagram.\nThe following table compares aspects of each type of migration.\n| Unnamed: 0      | Migrate to Virtual Machines        | Q replication          | SQL Replication       | HADR          |\n|:-------------------------------|:----------------------------------------------------------|:---------------------------------------------------|:------------------------------------------|:------------------------------------------|\n| Sources      | VMware, Amazon Web Services (AWS) VMs      | Any Db2 environment, based on licensing   | Any Db2 environment      | Any Db2 environment      |\n| What is replicated?   | Block-level replication of disks       | Tables in the database        | Tables in the database     | Entire database       |\n| Cutover      | Requires a few minutes for a VM to launch in Google Cloud | Point apps and DNS to the Compute Engine instances | Point apps and DNS to the cloud instances | Point apps and DNS to the cloud instances |\n| DDL change replication   | Yes (disk writes being replicated)      | Yes            | Yes          | Yes          |\n| Synchronous data replication | nan              | No             | Yes          | Yes          |\n| Asynchronous data replication | nan              | Yes            | Yes          | Yes          |\n| Point-in-time data replication | No              | Yes            | Yes          | No          |\nThe preceding table is a guide to help you match system availability requirements and resource effort level to set up the target system, set up replication, as well as maintain and test the replication over time. The table shows that Migrate to VMs is the easiest approach to implement, but the least flexible in terms of system availability. Alternatively, HADR, Q replication, and SQL Replication have a lower impact on system availability in exchange for a higher level of effort to set up and maintain the replication in a parallel model.\n## Migration types\nThere are two ways to migrate Db2 to Compute Engine:\n- Migrations that involve modifying an existing cluster configuration or topology.\n- Migrations that replicate data into completely new clusters.\nModifying an existing cluster doesn't require launching a completely new cluster in the cloud and, therefore, can be faster. The other way to migrate requires that you deploy a new cluster to Google Cloud, but it has a smaller impact on the existing cluster because the replication is [out-of-band](https://wikipedia.org/wiki/Out-of-band_data) . This method is also handy if you want to replicate only a part of the database or perform transformations on the data before it lands in the target.\nThe following sections discuss what to consider before you move your Db2 instances to Google Cloud. Some commonly used capabilities might not work as-is on Google Cloud or might need some configuration changes.\n### Floating (virtual) IP addresses\nIn a highly available Db2 cluster, TSA/MP can assign a virtual IP address to the primary node. This address is also called a floating IP address and means that traffic is always routed to the primary node and not the standby.\nCompute Engine uses a virtualized network stack in a [Virtual Private Cloud (VPC)](/compute/docs/vpc) network, so typical implementation mechanisms might not work. For example, the VPC network handles [Address Resolution Protocol (ARP)](https://tools.ietf.org/html/rfc826) requests based on the configured routing topology, and ignores [gratuitous ARP frames](https://datatracker.ietf.org/doc/html/rfc5227#section-3) . In addition, it's impossible to directly modify the VPC network routing table with standard routing protocols such as [Open Shortest Path First Protocol (OSPF)](https://www.ietf.org/rfc/rfc2328.txt) or [Border Gateway Protocol (BGP)](https://tools.ietf.org/html/rfc1105) . Therefore, you must either implement an [alternative](/solutions/best-practices-floating-ip-addresses) to floating IP addresses or use ACR.\nIf you are moving some or all of the nodes in a Db2 cluster, make sure to disable floating IP addresses for your cluster before you move any nodes.\n### ACR\nIf your Db2 environment uses ACR , you might need to change the catalog on your clients if the DNS names change or if your clients connect by using IP addresses.\n### Tiebreakers\nTSA/MP requires that the majority of the cluster nodes are online to start automation actions. If the cluster consists of an even number of nodes, there's a chance that exactly half of the nodes of the cluster are online, and there's a chance for a [split-brain scenario](https://wikipedia.org/wiki/Split-brain_(computing)) . In this case, TSA/MP uses a tiebreaker to decide the [quorum](https://www.ibm.com/support/knowledgecenter/en/SSRM2X_4.1.0/com.ibm.samp.doc_4.1/sampugsettingup_tiebreaker.html) (the majority group) state, which determines whether automation actions can be started.\nConsider the following tiebreakers that your Db2 environment might use:\n- [Storage or disk tiebreaker](https://www.ibm.com/support/knowledgecenter/en/SSRM2X_4.1.0/com.ibm.samp.doc_4.1/sampugusingtiebreaker.html) . Ibm Db2 uses disk reservations in order to break the tie. Because reservations aren't available on Google Cloud,.\n- [Network tiebreaker](https://www.ibm.com/support/knowledgecenter/SSRM2X_4.1.0/com.ibm.samp.doc_4.1/sampugnettb.html) . Uses an external (to the cluster) IP address to resolve a tie situation. In a hybrid deployment, your network tiebreaker might not need to move to Google Cloud initially as long as it is reachable from the cluster nodes. After your cluster runs on Google Cloud, however, a good practice is to create the tiebreaker in a different zone or use the Google Cloud metadata server as the tiebreaker.\n- [NFS tiebreaker](https://www.ibm.com/support/knowledgecenter/SSRM2X_4.1.0/com.ibm.samp.doc_4.1/sampugnfstiebreaker.html) . The NFS tiebreaker resolves tie situations that are based on reserve files that are stored on an NFS v4 server. Like with the network tiebreaker, the NFS tiebreaker and the NFS v4 server can also remain in their original location in a hybrid deployment. Later, a better practice is to deploy your own NFS server or use partners like [Elastifile](https://www.elastifile.com/) as the NFS tiebreaker targets on Google Cloud.## Migrating using Migrate to VMs\nIf both of the following are true for your environment, Migrate to VMs is your recommended option:\n- You have a VMware vCenter environment or virtual machines on Amazon Elastic Compute Cloud (Amazon EC2).\n- You have a private connection from Google Cloud to your environment such as Cloud VPN or Cloud Interconnect.\n[Migrate to VMs](/velostrata) is for migrating virtual machines from on-premises and cloud environments to Google Cloud. It lets you migrate a virtual machine to Google Cloud in a few minutes, while the data is copied in the background but the virtual machines are completely operational. You have a private connection between your source environment to your Google Cloud project such as Cloud VPN, Cloud Interconnect, or Partner Interconnect.\nWith Migrate to VMs, you need to reevaluate the database configuration on the cloud VMs. Some configurations might not be optimized for Google Cloud, such as [registry variables](https://www.ibm.com/support/knowledgecenter/en/SSEPGG_11.1.0/com.ibm.db2.luw.admin.regvars.doc/doc/c0007340.html) , [buffer pools](https://www.ibm.com/support/knowledgecenter/en/SSVJJU_6.4.0/com.ibm.IBMDS.doc_6.4/c_tg_db2_buffer_pool.html) , [database manager configuration](https://www.ibm.com/support/knowledgecenter/en/SSEPGG_11.1.0/com.ibm.db2.luw.admin.cmd.doc/doc/r0001988.html) or [database configuration](https://www.ibm.com/support/knowledgecenter/en/SSEPGG_11.1.0/com.ibm.db2.luw.admin.cmd.doc/doc/r0001987.html) . You can use the [AUTOCONFIGURE](https://www.ibm.com/support/knowledgecenter/en/SSEPGG_11.1.0/com.ibm.db2.luw.admin.cmd.doc/doc/r0008960.html) utility to start with a baseline.\nThe Migrate to VMs operational methodology is detailed in [VM migration lifecycle](/velostrata/docs/concepts/planning-a-migration/vm-migration-lifecycle) .\nThe following sections outlinehow to apply this methodology for a Db2 environment.\n### Test clones\n[Test clones](/velostrata/docs/how-to/using-test-clones/creating-a-test-clone) are available only on vCenter environments.\nMigrate to VMs can take a snapshot of your VM and create a ready-to-go compute instance on Google Cloud based on that snapshot. You can recreate your Db2 environment on Google Cloud, try configuration changes, test, and benchmark the deployment without any consequences to your source environment.\n**Note:** Run test clones in an isolated environment on Google Cloud so the clones don't conflict with any production services, such as active directory, databases, or other services that the test clone shouldn't access. The test clone has the same configuration and identity as the source machine.\nThe following diagram shows your DB2 environment on Google Cloud with the side-by-side environment on Google Cloud after a Migrate to VMs test cloning.\nAfter you benchmark and test the test clones on Google Cloud, you can delete the test clones.\n### Run-in-cloud\nWhen activating [run-in-cloud](/velostrata/docs/concepts/planning-a-migration/vm-migration-lifecycle#run-in-cloud) , Migrate to VMs shuts down your source cluster and starts the VMs on Google Cloud, while only fetching data as needed and not streaming the entire storage to Google Cloud. Run-in-cloud supports write-back and is enabled by default. Migrate to VMs helps you test your environment before actively streaming the storage. You can also move the VM back to your source environment by using the [move back](/velostrata/docs/how-to/vm-operations/running-a-vm-back-on-premises) feature. In cloud-to-cloud migrations, you cannot replicate writes back to the source.\nThe following diagram shows the run-in-cloud phase, if you set all your nodes to run in the cloud. You can decide to gradually move cluster nodes instead of the entire cluster at once.### Migration\nThe [migration phase](/velostrata/docs/concepts/planning-a-migration/vm-migration-lifecycle#full_migration) is similar to the run-in-cloud phase, but Migrate to VMs also actively streams the storage to Google Cloud. During the run-in-cloud phase, Migrate to VMs only brings data on demand to save on the bandwidth because you haven't indicated that you are ready to move the VM completely.\n### Detach\nDuring this phase, Migrate to VMs syncs the data from its cache and object store to the native data disks on Google Cloud, and then attaches the disks to the VM. This phase requires that you shut down the VM on Google Cloud. For Db2, we recommend detaching one node of the cluster at a time.\n## Using replication\nFor Db2, replication is the process of capturing changes from the transaction log by using a program called the capture program, and then applying them to a different cluster using the apply program. The way the capture program captures the changes and the type of communication channel used to transmit the changes to the apply program differ between the replication types.\nThe following diagram shows the logical flow of information in Db2 replication.\nThe capture app captures changes from the database and sends the changes to the apply app. The apply app writes those changes to the target database. There are some transformations that the apps can do on the data itself. The capture and apply applications don't necessarily need to run on the database server itself.\n### SQL Replication\nA [SQL Replication](https://www.ibm.com/support/knowledgecenter/en/SSTRGZ_11.4.0/com.ibm.swg.im.iis.db.repl.asnclp.sql.doc/topics/iiyrsclpbldscriptsql.html) captures changes to source tables and views and uses staging tables to store committed transactional data. The changes are then read from the staging tables and replicated to corresponding target tables. At the time of writing this document, when you install Db2, [SQL Replication is available to you](https://www.ibm.com/support/knowledgecenter/en/SSTRGZ_11.4.0/com.ibm.swg.im.iis.db.repl.asnclp.sql.doc/topics/iiyrsclpbldscriptsql.html) .\nA migration process leveraging SQL Replication would look like this:\n- Deploy Db2 on Google Cloud.\n- Configure SQL Replication.\n- Start SQL Replication.\n- Verify that the deployments are in sync.\n- Point your apps to the Google Cloud instance. Stop the replication.\nThe following diagram is an example of SQL replication.\nYour production environment works as usual, while replicating the SQL commands to the new cluster you create on Google Cloud. In the preceding diagram, the replication process runs on the primary instance but there are different ways to deploy it that are outside of the scope of this document.\n### Q replication\n[Q replication](https://www.ibm.com/support/knowledgecenter/en/SSTRGZ_11.4.0/com.ibm.swg.im.iis.repl.qrepl.doc/topics/iiyrqcnccoview.html) is a newer and more efficient way than SQL Replication to replicate data from one Db2 instance to another. This method uses IBM MQ to deliver data changes entries, which means that you have to deploy an instance of IBM MQ in the [source environment](https://www.ibm.com/support/knowledgecenter/en/SSFKSJ_9.0.0/com.ibm.mq.pla.doc/q004690_.htm) and the target environment. This method of replication is faster than SQL replication because it is in memory. SQL replication is slower but Q replication is usually more difficult to set up, because you need to set up IBM MQ. Depending on your Db2 license, you may have to acquire a license for Q replication.\nWhen you start the Db2 Q replication, you can choose between the following two methods:\n- **Automatic loading** . The Q replication processes perform the initial load, which means restoring the target database from a backup of the source.\n- **Manual loading** . You perform the initial load, and then start the replication from the point in the log.\nA migration process looks like this:\n- Deploy IBM MQ on Google Cloud and in your source environment.\n- Deploy Db2 on Google Cloud.\n- Configure Q replication.\n- Start Q replication (either with manual loading or automatic loading).\n- Verify that the two deployments are in sync.\n- Point your applications to the Google Cloud instance. Stop the replication.\nThe following diagram shows a possible Q replication solution.\nThe source environment uses IBM Q replication to send the database changes to IBM MQ and the target environment, extending a Db2 cluster to Compute Engine\nIn this approach, you gradually move your existing Db2 cluster to Compute Engine and rely on HADR for the data transfer between nodes.\nUse this approach if you meet the following conditions:\n- You don't want to deploy an entirely new cluster on Compute Engine.\n- You cannot leverage Migrate to VMs.\n- You cannot use one of the replication options.\n- You don't or cannot use a partner product (licensing, costs, or compliance to name a few reasons).\n### If your Db2 version doesn't support auxiliary standby\nYou can do the following:\n- Deploy a Db2 instance on Compute Engine.\n- Take a backup from your primary instance.\n- Restore the Db2 instance on Compute Engine from backup.\n- Remove the standby instance from the HADR setup.\n- Attach the Compute Engine Db2 instance as a standby (you can choose your sync mode, but due to possible higher latency ,`ASYNC`or`NEARASYNC`might be preferable).\n- Failover to the Compute Engine Db2 instance and make it the HADR primary.\n- Create another Compute Engine Db2 instance, restore it from backup, and set up as HADR standby.\nThe first step in the following diagram shows the newly created Db2 instance on Google Cloud set up as the principal standby of the source Db2 primary.\nIn the preceding diagram, the Google Cloud instance becomes the HADR primary. Then you remove the source principal standby and attach another Db2 instance on Compute Engine as the principal standby instance.\n### If your Db2 version supports auxiliary standby\nOne option is to follow the same steps as when Db2 version doesn't support auxiliary standby, and at the end, move the auxiliary standby instances as well.\nAnother option is to leverage the auxiliary replicas for a more fault-tolerant move to Google Cloud, because you don't have the primary or principal standby in your source environment and the other on Google Cloud. The following list outlines the steps for this second option:\n- Deploy Db2 instances on Compute Engine (primary, principal, auxiliaries if needed) to their locations.\n- Remove the auxiliary standby nodes from the source cluster.\n- Configure the nodes that will become the primary and principal of the auxiliary standbys of the source cluster.\n- Perform a takeover of one of the Compute Engine instances. This instance becomes the primary instance.Configure one of the other Compute Engine instances as principal standby of the primary instance.\nThe first step depicted in the following diagram shows two of the newly created Db2 instances on Compute Engine.\nThe instances are set up as auxiliary standbys of the source Db2 primary instance instead of the auxiliary instances in the source environment. Then, after invoking takeover to one of the Compute Engine instances, that instance becomes the HADR primary and one other instance is configured as principal standby. In the last step, two other instances are configured as auxiliary standbys.\n## Partner products\nGoogle has several partners who have products to help with such a migration. Most of these products leverage CDC to replicate data between the source Db2 and the target. These products aren't Google Cloud products, and you need to check licensing and pricing for each product or service. Usually, this service replicates data from an existing cluster to a different cluster that you create on Google Cloud, and the overall approach is similar to the replication scenarios described in this document.\nThe following are a few partner products:\n- [Informatica Intelligent Cloud Services](https://www.informatica.com/products/cloud-integration/connectivity/connectors.html#fbid=AYL3LxAfNRU) \n- [IBM InfoSphere](https://www.ibm.com/analytics/information-server) \n- [Attunity Enterprise Data Replication](https://www.attunity.com/db2-replication/) ## What's next\n- [IBM Db2 for SAP planning guide](/solutions/sap/docs/sap-ibm-db2-planning-guide) .\n- [Google Cloud database migration center](/db-migration) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}