{"title": "Documentation - Try Online Predictions", "url": "https://cloud.google.com/distributed-cloud/hosted/docs/latest/gdch/overview", "abstract": "# Documentation - Try Online Predictions\n**Preview:** The Vertex AI online prediction is a Preview feature that is available as-is and is not recommended for production environments. Google provides no service-level agreements (SLA) or technical support commitments for Preview features. For more information, see GDCH's [feature stages](/distributed-cloud/hosted/docs/latest/gdch/resources/feature-stages) .\n**Note:** Authorization is not enforced for Vertex AI online prediction because it is a Preview feature.\nThis quickstart guides the Application Operator (AO) through the process of using the Vertex AI Online Predictions API.\n", "content": "## Before you begin\nBefore trying online predictions, perform the following steps:\n- Create and train a prediction model targeting one of the [supported containers](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-online-predictions#available-container-images) .\n- If you don't have a project, work with your Platform Administrator (PA) to [create one](/distributed-cloud/hosted/docs/latest/gdch/platform/pa-user/create-a-project) .\n- Work with your Infrastructure Operator (IO) to ensure the [Prediction user cluster](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/prediction-user-cluster) exists and your user project allows incoming external traffic.\n- [Deploy your model to an endpoint](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-deploy-model) .## Format your input for online prediction\nCreate a JSON file with the request in the format required by [the target container](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-online-predictions#available-container-images) . For more information about the JSON representation with examples for TensorFlow, see [Request body details](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-get-online-prediction#request-body-details) .\n## Send an online prediction request\nSend an online prediction request to the model's endpoint URL using [HTTP](#http) or [gRPC](#grpc) .\nThe following example uses HTTP to send an online prediction request.\nUse the `curl` tool to call the HTTP endpoint. For example:\n```\ncurl -X POST -H \"Content-Type: application/json; charset=utf-8\"https://ENDPOINT_URL_PATH.GDCH_URL:443/v1/model:predict -d @JSON_FILE_NAME.json{\u00a0 \u00a0 \"predictions\": [[-357.10849], [-171.621658]\u00a0 \u00a0 ]}\n```\nReplace the following:- ``: the endpoint URL path for the online prediction request.\n- ``: the URL of your organization in GDCH, for example,`org-1.zone1.gdch.test`.\n- ``: the name of the JSON file with the request body details for your online prediction.\nYou obtain the output following the command. The API response is in JSON format.\n **Note:** HTTPS is used in GDCH and the certificate authority (CA) issues the certificate for the machine. For more information, see [Security](/distributed-cloud/hosted/docs/latest/gdch/security) .\nThe following example uses gRPC to send an online prediction request:- Install the `google-cloud-aiplatform` Python client library by following the instructions from [Install Vertex AI client libraries](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-install-libraries) .When downloading the client library you want to install, choose one of the following library files, depending on your operating system:- **CentOS:** `centos-google-cloud-aiplatform-1.34.0.tar.gz`\n- **Ubuntu:** `ubuntu-google-cloud-aiplatform-1.34.0.tar.gz`\nUse the following URL to download the client library:```\nhttps://GDCH_URL/.well-known/static/client-libraries/LIBRARY_FILE\n```Replace the following:- ``: the URL of your organization in GDCH.\n- ``: the name of the library file depending on the operating system, for example,`ubuntu-google-cloud-aiplatform-1.34.0.tar.gz`.\n- Save the following code to a Python script: **Important:** The example explicitly sets a root Certificate Authority (CA) `path-to-ca-cert-file.cert` file, which might not be applicable to your environment. You must perform the necessary adjustments for your specific case.```\nimport jsonimport osfrom typing import Sequenceimport grpcfrom absl import appfrom absl import flagsfrom google.protobuf import json_formatfrom google.protobuf.struct_pb2 import Valuefrom google.cloud.aiplatform_v1.services import prediction_service_INPUT = flags.DEFINE_string(\"input\", None, \"input\", required=True)_HOST = flags.DEFINE_string(\"host\", None, \"Prediction endpoint\", required=True)_ENDPOINT_ID = flags.DEFINE_string(\"endpoint_id\", None, \"endpoint id\", required=True)os.environ[\"GRPC_DEFAULT_SSL_ROOTS_FILE_PATH\"] = \"path-to-ca-cert-file.cert\"# ENDPOINT_RESOURCE_NAME is a placeholder value that doesn't affect prediction behavior.ENDPOINT_RESOURCE_NAME=\"projects/000000000000/locations/us-central1/endpoints/00000000000000\"# predict_client_secure builds a client that requires TLSdef predict_client_secure(host):\u00a0 \u00a0 with open(os.environ[\"GRPC_DEFAULT_SSL_ROOTS_FILE_PATH\"], 'rb') as f:\u00a0 \u00a0 \u00a0 \u00a0 creds = grpc.ssl_channel_credentials(f.read())\u00a0 \u00a0 channel_opts = ()\u00a0 \u00a0 channel_opts += (('grpc.ssl_target_name_override', host),)\u00a0 \u00a0 client = prediction_service.PredictionServiceClient(\u00a0 \u00a0 \u00a0 \u00a0 transport=prediction_service.transports.grpc.PredictionServiceGrpcTransport(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 channel=grpc.secure_channel(target=host+\":443\", credentials=creds, options=channel_opts)))\u00a0 \u00a0 return clientdef predict_func(client, instances):\u00a0 \u00a0 resp = client.predict(\u00a0 \u00a0 \u00a0 \u00a0 endpoint=ENDPOINT_RESOURCE_NAME,\u00a0 \u00a0 \u00a0 \u00a0 instances=instances,\u00a0 \u00a0 \u00a0 \u00a0 metadata=[\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (\"x-vertex-ai-endpoint-id\", _ENDPOINT_ID.value)])\u00a0 \u00a0 print(resp)def main(argv: Sequence[str]):\u00a0 \u00a0 del argv \u00a0# Unused.\u00a0 \u00a0 with open(_INPUT.value) as json_file:\u00a0 \u00a0 \u00a0 \u00a0 data = json.load(json_file)\u00a0 \u00a0 \u00a0 \u00a0 instances = [json_format.ParseDict(s, Value()) for s in data[\"instances\"]]\u00a0 \u00a0 client = predict_client_secure(_HOST.value)\u00a0 \u00a0 predict_func(client=client, instances=instances)if __name__==\"__main__\":\u00a0 \u00a0 app.run(main)\n```\n- Make the gRPC call to the prediction server:```\npython PYTHON_FILE_NAME.py --input JSON_FILE_NAME.json \\\u00a0 \u00a0 --host ENDPOINT_URL_PATH.GDCH_URL \\\u00a0 \u00a0 --endpoint_id ENDPOINT_ID \\\n```Replace the following:- ``: the name of the Python file where you saved the script.\n- ``: the name of the JSON file with the request body details for your online prediction.\n- ``: the endpoint URL path for the online prediction request.\n- ``: the URL of your organization in GDCH, for example,`org-1.zone1.gdch.test`.\n- ``: the value of the endpoint ID.\nIf successful, you receive a JSON response similar to one of the responses on [Response body examples](/distributed-cloud/hosted/docs/latest/gdch/application/ao-user/vertex-ai-get-online-prediction#response-body-examples) .", "guide": "Documentation"}