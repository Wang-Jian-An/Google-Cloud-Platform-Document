{"title": "Cloud Architecture Center - Validating data transfers between HDFS and Cloud Storage", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Validating data transfers between HDFS and Cloud Storage\nWhen you're copying or moving data between distinct storage systems such as multiple [Apache Hadoop Distributed File System](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html) (HDFS) clusters or between HDFS and [Cloud Storage](/storage) , it's a good idea to perform some type of validation to guarantee data integrity. This validation is essential to be sure data wasn't altered during transfer.\nWhile various mechanisms already ensure [point-to-point](https://wikipedia.org/wiki/Point-to-point_(telecommunications)) data integrity in transit (such as [TLS](https://tools.ietf.org/html/rfc4346#section-1) for all communication with Cloud Storage), explicit data integrity validation adds protection for cases that may go undetected by typical in-transit mechanisms. This can help you detect potential data corruption caused, for example, by noisy network links, memory errors on server computers and routers along the path, or software bugs (such as in a library that customers use).\nFor Cloud Storage, this validation happens automatically client-side with commands like [gsutil](/storage/docs/gsutil) `cp` and `rsync` . Those commands compute local file [checksums](https://wikipedia.org/wiki/Checksum) , which are then validated against the checksums computed by Cloud Storage at the end of each operation. If the checksums do not match, `gsutil` deletes the invalid copies and prints a warning message. This mismatch rarely happens, and if it does, you can retry the operation.\nNow there's also a way to automatically perform end-to-end, client-side validation in Apache Hadoop across heterogeneous Hadoop-compatible file systems like HDFS and Cloud Storage. This article describes how the new feature lets you efficiently and accurately compare file checksums.\n", "content": "## How HDFS performs file checksums\nHDFS uses CRC32C, a 32-bit [cyclic redundancy check](https://wikipedia.org/wiki/Cyclic_redundancy_check) (CRC) based on the Castagnoli polynomial, to maintain data integrity in different contexts:\n- At rest, Hadoop DataNodes continuously verify data against stored CRCs to detect and repair [bit-rot](https://wikipedia.org/wiki/Data_degradation) .\n- In transit, the DataNodes send known CRCs along with the corresponding bulk data, and HDFS client libraries cooperatively compute per-chunk CRCs to compare against the CRCs received from the DataNodes.\n- For HDFS administrative purposes, block-level checksums are used for low-level [manual integrity checks](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#Debug_Commands) of individual block files on DataNodes.\n- For arbitrary application-layer use cases, the`FileSystem`interface defines`getFileChecksum`, and the HDFS implementation uses its stored fine-grained CRCs to define a file-level checksum.\nFor most day-to-day uses, the CRCs are used transparently with respect to the application layer. The only CRCs used are the per-chunk CRC32Cs, which are already precomputed and stored in metadata files alongside block data. The chunk size is defined by `dfs.bytes-per-checksum` and has a default value of 512 bytes.\n## Shortcomings of Hadoop's default file checksum type\nBy default when using Hadoop, all API-exposed checksums take the form of an [MD5](https://wikipedia.org/wiki/MD5) of a concatenation of chunk CRC32Cs, either at the block level through the low-level `DataTransferProtocol` , or at the file level through the top-level `FileSystem` interface. A file-level checksum is defined as the MD5 of the concatenation of all the block checksums, each of which is an MD5 of a concatenation of chunk CRCs, and is therefore referred to as an `MD5MD5CRC32FileChecksum` . This is effectively an on-demand, three-layer [Merkle tree](https://wikipedia.org/wiki/Merkle_tree) .\nThis definition of the file-level checksum is sensitive to the implementation and data-layout details of HDFS, namely the chunk size (default 512 bytes) and the block size (default 128 MB). So this default file checksum isn't suitable in any of the following situations:\n- Two different copies of the same files in HDFS, but with different per-file block sizes configured.\n- Two different instances of HDFS with different block or chunk sizes configured.\n- A combination of HDFS and non-HDFS [Hadoop-compatible file systems](http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/filesystem/introduction.html) (HCFS) such as Cloud Storage.\nThe following diagram shows how the same file can end up with different checksums depending on the file system's configuration:\nYou can display the default checksum for a file in HDFS by using the Hadoop `fs -checksum` command:\n```\nhadoop fs -checksum hdfs:///user/bob/data.bin\n```\nIn an HDFS cluster that has a block size of 64 MB ( `dfs.block.size=67108864` ), this command displays a result like the following:\n```\nhdfs:///user/bob/data.bin MD5-of-131072MD5-of-512CRC32C 000002000000000000020000e9378baa1b8599e50cca212ccec2f8b7\n```\nFor the same file in another cluster that has a block size of 128 MB ( `dfs.block.size=134217728` ), you see a different output:\n```\nhdfs:///user/bob/data.bin MD5-of-0MD5-of-512CRC32C 000002000000000000000000d3a7bae0b5200ed6707803a3911959f9\n```\nYou can see in these examples that the two checksums differ for the same file.\n## How Hadoop's new composite CRC file checksum works\nA new checksum type, tracked in [HDFS-13056](https://issues.apache.org/jira/browse/HDFS-13056) , was released in Apache Hadoop 3.1.1 to address these shortcomings. The new type, configured by `dfs.checksum.combine.mode=COMPOSITE_CRC` , defines new composite block CRCs and composite file CRCs as the mathematically composed CRC across the stored chunk CRCs. Using this type replaces using an MD5 of the component CRCs in order to calculate a single CRC that represents the entire block or file and is independent of the lower-level granularity of chunk CRCs.\nCRC composition has many benefits \u2014 it is efficient; it allows the resulting checksums to be completely chunk/block agnostic; and it allows comparison between striped and replicated files, between different HDFS instances, and between HDFS and other external storage systems. (You can learn more details about the CRC algorithm in this [PDF download](https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/crcutil/crc-doc.1.0.pdf) .)\nThe following diagram provides a look at how a file's checksum is consistent after transfer across heterogeneous file system configurations:\nThis feature is minimally invasive: it can be added in place to be compatible with existing block metadata, and doesn't need to change the normal path of chunk verification. This also means even large preexisting HDFS deployments can adopt this feature to retroactively sync data. For more details, you can [download the full design PDF document](https://issues.apache.org/jira/secure/attachment/12908695/hdfs-file-composite-crc32-v3.pdf) .\n## Using the new composite CRC checksum type\nTo use the new composite CRC checksum type within Hadoop, set the `dfs.checksum.combine.mode` property to `COMPOSITE_CRC` (instead of the default value `MD5MD5CRC` ). When a file is copied from one location to another, the chunk-level checksum type (that is, the property `dfs.checksum.type` that defaults to `CRC32C` ) must also match in both locations.\nYou can display the new checksum type for a file in HDFS by passing the `-Ddfs.checksum.combine.mode=COMPOSITE_CRC` argument to the Hadoop `fs -checksum` command:\n```\nhadoop fs -Ddfs.checksum.combine.mode=COMPOSITE_CRC -checksum hdfs:///user/bob/data.bin\n```\nRegardless of the block size configuration of the HDFS cluster, you see the same output, like the following:\n```\nhdfs:///user/bob/data.bin COMPOSITE-CRC32C c517d290\n```\nFor Cloud Storage, you must also explicitly set the [Cloud Storage connector](/dataproc/docs/concepts/connectors/cloud-storage) property `fs.gs.checksum.type` to `CRC32C` . This property otherwise defaults to `NONE` , causing file checksums to be disabled by default. This default behavior by the Cloud Storage connector is a preventive measure to avoid an issue with `distcp` , where an exception is raised if the checksum mismatch instead of failing gracefully. The command looks like the following:\n```\nhadoop fs -Ddfs.checksum.combine.mode=COMPOSITE_CRC -Dfs.gs.checksum.type=CRC32C -checksum gs://[BUCKET]/user/bob/data.bin\n```\nThe command displays the same output as in the previous example on HDFS:\n```\ngs://[BUCKET]/user/bob/data.bin COMPOSITE-CRC32C c517d290\n```\nYou can see that the composite CRC checksums returned by the previous commands all match, regardless of block size, as well as between HDFS and Cloud Storage. By using composite CRC checksums, you can now guarantee that data integrity is preserved when transferring files between all types of Hadoop cluster configurations.\nIf you are running `distcp` , as in the following example, the validation is performed automatically:\n```\nhadoop distcp -Ddfs.checksum.combine.mode=COMPOSITE_CRC -Dfs.gs.checksum.type=CRC32C hdfs:///user/bob/* gs://[BUCKET]/user/bob/\n```\nIf `distcp` detects a file checksum mismatch between the source and destination during the copy, then the operation will fail and return a warning.\n## Accessing the feature\nThe new composite CRC checksum feature is available in Apache Hadoop 3.1.1 (see [release notes](https://hadoop.apache.org/docs/r3.1.1/hadoop-project-dist/hadoop-common/release/3.1.1/CHANGES.3.1.1.html) ), and [backports to versions 2.7, 2.8 and 2.9](https://issues.apache.org/jira/browse/HDFS-14147) are in the works. It has been [included](/dataproc/docs/release-notes#November_12_2018) by default in subminor versions of [Cloud Dataproc](/dataproc) 1.3 since late 2018.", "guide": "Cloud Architecture Center"}