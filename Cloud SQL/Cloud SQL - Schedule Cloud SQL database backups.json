{"title": "Cloud SQL - Schedule Cloud SQL database backups", "url": "https://cloud.google.com/sql/docs/postgres/backup-recovery/scheduling-backups", "abstract": "# Cloud SQL - Schedule Cloud SQL database backups\nThis tutorial shows how to use [Cloud Scheduler](/scheduler) and [Cloud Functions](/functions) to schedule manual backups for a [Cloud SQL](/sql/docs/postgres) database.\nThis tutorial takes approximately 30 minutes to complete.\nFirst, you set up the environment by cloning a git repository that contains test databases and storing those databases in a Cloud Storage bucket.\nThen, you create a Cloud SQL for PostgreSQL database instance and import the test databases from the Cloud Storage bucket into the instance.\nAfter the environment is set up, you create a Cloud Scheduler job that posts a backup trigger message at a scheduled date and time on a Pub/Sub topic. The message contains information about the Cloud SQL instance name and the project ID. The message triggers a Cloud Function. The function uses the Cloud SQL Admin API to start a database backup on Cloud SQL. The following diagram illustrates this workflow:", "content": "## Google Cloud componentsIn this document, you use the following billable components of Google Cloud:\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . - [Cloud Storage](/storage/docs/introduction) : Stores the test databases that you import into Cloud SQL.\n- [Cloud SQL instance](/sql/docs) : Contains the database to backup.\n- [Cloud Scheduler](/scheduler/docs) : Posts messages to a Pub/Sub topic on a set schedule.\n- [Pub/Sub](/pubsub/docs/overview) : Contains messages sent from the Cloud Scheduler.\n- [Cloud Functions](/functions/docs/concepts/overview) : Subscribes to the Pub/Sub topic and when triggered, makes an API call to the Cloud SQL instance to initiate the backup.\nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- In the Google Cloud console, on the project selector page,   select or [create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . **Note** : If you don't plan to keep the  resources that you create in this procedure, create a project instead of  selecting an existing project. After you finish these steps, you can  delete the project, removing all resources associated with the project. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- In the Google Cloud console, go to the **APIs** page, and enable the following APIs:- Cloud SQL Admin API\n- Cloud Functions API\n- Cloud Scheduler API\n- Cloud Build API\n- App Engine Admin API\n [Go to APIs](https://console.cloud.google.com/apis) Throughout the rest of this tutorial, you run all commands from Cloud Shell.## Set up your environmentTo get started, you first clone the repository that contains the sample data. You then configure your environment and create custom roles that have the permissions needed for this tutorial.\nYou can do everything in this tutorial in Cloud Shell.- Clone the repository that contains the sample data:```\ngit clone https://github.com/GoogleCloudPlatform/training-data-analyst.git\n```You use the data from the `training-data-analyst` repository to create a database with some mock records.\n- Configure the following environment variables:```\nexport PROJECT_ID=`gcloud config get-value project`export DEMO=\"sql-backup-tutorial\"export BUCKET_NAME=${USER}-PostgreSQL-$(date +%s)export SQL_INSTANCE=\"${DEMO}-sql\"export GCF_NAME=\"${DEMO}-gcf\"export PUBSUB_TOPIC=\"${DEMO}-topic\"export SCHEDULER_JOB=\"${DEMO}-job\"export SQL_ROLE=\"sqlBackupCreator\"export STORAGE_ROLE=\"simpleStorageRole\"export REGION=\"us-west2\"\n```\n- Create two custom roles that have only the permissions needed for this tutorial:```\ngcloud iam roles create ${STORAGE_ROLE} --project ${PROJECT_ID} \\\u00a0 \u00a0 --title \"Simple Storage role\" \\\u00a0 \u00a0 --description \"Grant permissions to view and create objects in Cloud Storage\" \\\u00a0 \u00a0 --permissions \"storage.objects.create,storage.objects.get\"\n``````\ngcloud iam roles create ${SQL_ROLE} --project ${PROJECT_ID} \\\u00a0 \u00a0 --title \"SQL Backup role\" \\\u00a0 \u00a0 --description \"Grant permissions to backup data from a Cloud SQL instance\" \\\u00a0 \u00a0 --permissions \"cloudsql.backupRuns.create\"\n``` **Note:** If you're prompted to authorize Cloud Shell, then click **Authorize** . Also, if you see the error message: `(gcloud.iam.roles.create) You do not currently have an active account selected` , run the `gcloud auth login` command, and then follow the steps that appear in Cloud Shell.These roles reduce the scope of access of Cloud Functions and Cloud SQL service accounts, following the [principle of least privilege](/blog/products/identity-security/dont-get-pwned-practicing-the-principle-of-least-privilege) .\n## Create a Cloud SQL instanceIn this section, you create a Cloud Storage bucket and a Cloud SQL for PostgreSQL instance. Then you upload the test database to the Cloud Storage bucket and import the database from there into the Cloud SQL instance.\n### Create a Cloud Storage bucketYou use the `gsutil` command-line tool to create a Cloud Storage bucket.\n```\n```shgsutil mb -l ${REGION} gs://${BUCKET_NAME}```\n```\n### Create a Cloud SQL instance and grant permissions to its service accountNext, you create a Cloud SQL instance and grant its service account the permissions to create backup runs.- Create a Cloud SQL for PostgreSQL instance: `sh gcloud sql instances create ${SQL_INSTANCE} --database-version POSTGRES_13 --region ${REGION}` This operation takes a few minutes to complete.\n- Verify that the Cloud SQL instance is running:```\ngcloud sql instances list --filter name=${SQL_INSTANCE}\n```The output looks similar to the following:```\nNAME      DATABASE_VERSION LOCATION TIER    PRIMARY_ADDRESS PRIVATE_ADDRESS STATUS\nsql-backup-tutorial  POSTGRES_13  us-west2-b db-n1-standard-1 x.x.x.x  -    RUNNABLE\n``` **Note:** For this tutorial the PRIMARY_ADDRESS field is shown as \"x.x.x.x\". In your case an actual IP address is displayed.\n- Grant your Cloud SQL service account the permissions to export data to Cloud Storage with the Simple Storage role:```\nexport SQL_SA=(`gcloud sql instances describe ${SQL_INSTANCE} \\\u00a0 \u00a0 --project ${PROJECT_ID} \\\u00a0 \u00a0 --format \"value(serviceAccountEmailAddress)\"`)gsutil iam ch serviceAccount:${SQL_SA}:projects/${PROJECT_ID}/roles/${STORAGE_ROLE} gs://${BUCKET_NAME}\n```\n### Populate the Cloud SQL instance with sample dataNow you can upload files to your bucket and create and populate your sample database.- Go to the repository that you cloned:```\ncd training-data-analyst/CPB100/lab3a/cloudsql\n```IMPORTANT: Remain in this directory for the rest of the tutorial.\n- Upload the files in the directory to your new bucket:```\ngsutil cp * gs://${BUCKET_NAME}\n```\n- Create a sample database; at the \"Do you want to continue (Y/n)?\" prompt, enter Y (Yes) to continue.```\ngcloud sql import sql ${SQL_INSTANCE} gs://${BUCKET_NAME}/table_creation.sql --project ${PROJECT_ID}\n```\n- Populate the database; at the \"Do you want to continue (Y/n)?\" prompt, enter Y (Yes) to continue.```\ngcloud sql import csv ${SQL_INSTANCE} gs://${BUCKET_NAME}/accommodation.csv \\\u00a0 \u00a0 --database recommendation_spark \\\u00a0 \u00a0 --table Accommodation\n``````\ngcloud sql import csv ${SQL_INSTANCE} gs://${BUCKET_NAME}/rating.csv \\\u00a0 \u00a0 --database recommendation_spark \\\u00a0 \u00a0 --table Rating\n```\n## Create a topic, a function, and a scheduler jobIn this section, you create a custom IAM service account and bind it to the custom SQL role that you created in [Set up your environment](#setting-up) . You then create a Pub/Sub topic and a Cloud Function that subscribes to the topic, and uses the Cloud SQL Admin API to initiate a backup. Finally, you create a Cloud Scheduler job to post a message to the Pub/Sub topic periodically.\n### Create a service account for the Cloud FunctionThe first step is to create a custom service account and bind it to the custom SQL role that you created in [Set up your environment](#setting-up) .- Create an IAM service account to be used by the Cloud Function:```\ngcloud iam service-accounts create ${GCF_NAME} \\\u00a0 \u00a0 --display-name \"Service Account for GCF and SQL Admin API\"\n```\n- Grant the Cloud Function service account access to the custom SQL role:```\ngcloud projects add-iam-policy-binding ${PROJECT_ID} \\\u00a0 \u00a0 --member=\"serviceAccount:${GCF_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role=\"projects/${PROJECT_ID}/roles/${SQL_ROLE}\"\n```\n### Create a Pub/Sub topicThe next step is to create a Pub/Sub topic that's used to trigger the Cloud Function that interacts with the Cloud SQL database.\n```\n```shgcloud pubsub topics create ${PUBSUB_TOPIC}```\n```\n### Create a Cloud FunctionNext, you create the Cloud Function.- Create a `main.py` file by pasting the following into Cloud Shell:```\ncat <<EOF > main.pyimport base64import loggingimport jsonfrom datetime import datetimefrom httplib2 import Httpfrom googleapiclient import discoveryfrom googleapiclient.errors import HttpErrorfrom oauth2client.client import GoogleCredentialsdef main(event, context):\u00a0 \u00a0 pubsub_message = json.loads(base64.b64decode(event['data']).decode('utf-8'))\u00a0 \u00a0 credentials = GoogleCredentials.get_application_default()\u00a0 \u00a0 service = discovery.build('sqladmin', 'v1beta4', http=credentials.authorize(Http()), cache_discovery=False)\u00a0 \u00a0 try:\u00a0 \u00a0 \u00a0 request = service.backupRuns().insert(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 project=pubsub_message['project'],\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 instance=pubsub_message['instance']\u00a0 \u00a0 \u00a0 \u00a0 )\u00a0 \u00a0 \u00a0 response = request.execute()\u00a0 \u00a0 except HttpError as err:\u00a0 \u00a0 \u00a0 \u00a0 logging.error(\"Could NOT run backup. Reason: {}\".format(err))\u00a0 \u00a0 else:\u00a0 \u00a0 \u00a0 logging.info(\"Backup task status: {}\".format(response))EOF\n```\n- Create a `requirements.txt` file by pasting the following into Cloud Shell:```\ncat <<EOF > requirements.txtgoogle-api-python-clientOauth2clientEOF\n```\n- Deploy the code:```\ngcloud functions deploy ${GCF_NAME} \\\u00a0 \u00a0 --trigger-topic ${PUBSUB_TOPIC} \\\u00a0 \u00a0 --runtime python37 \\\u00a0 \u00a0 --entry-point main \\\u00a0 \u00a0 --service-account ${GCF_NAME}@${PROJECT_ID}.iam.gserviceaccount.com\n```\n### Create a Cloud Scheduler jobFinally, you create a Cloud Scheduler job to periodically trigger the data backup function on an hourly basis. Cloud Scheduler uses an App Engine instance for deployment.- Create an App Engine instance for the Cloud Scheduler job:```\ngcloud app create --region=${REGION}\n```\n- Create a Cloud Scheduler job:```\ngcloud scheduler jobs create pubsub ${SCHEDULER_JOB} \\--schedule \"0 * * * *\" \\--topic ${PUBSUB_TOPIC} \\--message-body '{\"instance\":'\\\"${SQL_INSTANCE}\\\"',\"project\":'\\\"${PROJECT_ID}\\\"'}' \\--time-zone 'America/Los_Angeles'\n```\n## Test your solutionThe final step is to test your solution. You start by running the Cloud Scheduler job.- Run the Cloud Scheduler job manually to trigger a PostgreSQL dump of your database.```\ngcloud scheduler jobs run ${SCHEDULER_JOB}\n```\n- List the operations performed on the PostgreSQL instance, and verify that there's an operation of type `BACKUP_VOLUME` :```\ngcloud sql operations list --instance ${SQL_INSTANCE} --limit 1\n```The output shows a completed backup job. For example:```\nNAME         TYPE   START       END       ERROR STATUS\n8b031f0b-9d66-47fc-ba21-67dc20193749 BACKUP_VOLUME 2020-02-06T21:55:22.240+00:00 2020-02-06T21:55:32.614+00:00 -  DONE\n```\n## Clean upYou can avoid incurring charges to your Google Cloud account for the resources used in this tutorial by following these steps. The easiest way to eliminate billing is to delete the project you created for the tutorial.- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.If you don't want to delete the entire project, then delete each of the resources you created. To do this, go to the appropriate pages in the Google Cloud console, selecting the resource, and deleting it.- [Go to Cloud Functions](https://console.cloud.google.com/functions) \n- [Go to Cloud Storage](https://console.cloud.google.com/storage/browser) \n- [Go to Cloud SQL Instances](https://console.cloud.google.com/sql) \n- [Go to Pub/Sub](https://console.cloud.google.com/cloudpubsub) \n- [Go to Cloud Scheduler](https://console.cloud.google.com/cloudscheduler) \n## What's next\n- Learn how to [schedule compute instances with Cloud Scheduler](/scheduler/docs/start-and-stop-compute-engine-instances-on-a-schedule) .\n- Learn more about [Cloud SQL backups](/sql/docs/postgres/backup-recovery/backups) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Cloud SQL"}