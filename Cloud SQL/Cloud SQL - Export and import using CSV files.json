{"title": "Cloud SQL - Export and import using CSV files", "url": "https://cloud.google.com/sql/docs/postgres/import-export/import-export-csv", "abstract": "# Cloud SQL - Export and import using CSV files\nThis page describes exporting and importing data into Cloud SQL instances using CSV files.\n**Note:** If you're migrating an entire database from a supported database server (on-premises, in AWS, or Cloud SQL) to a new Cloud SQL instance, you can use the [Database Migration Service](/database-migration/docs) instead of exporting and then importing files. If you're exporting because you want to create a new instance from the exported file, consider [restoring from a backup to a different instance](/sql/docs/postgres/backup-recovery/restoring#restorebackups-another-instance) or [cloning the instance](/sql/docs/postgres/clone-instance) .You can cancel the import of data into Cloud SQL instances and  the export of data from the instances.  This data is contained in CSV files. For  more information about cancelling an import or export operation, see [Cancel the import and export of data](/sql/docs/postgres/import-export/cancel-import-export) .\n", "content": "## Before you begin\nBefore you begin an export or import operation:\n- Ensure that your database has adequate free space.\n- Export and import operations use database resources, but they do not  interfere with normal database operations unless the instance is under-provisioned.\n- **Important** : Before starting a large operation, ensure  that at least 25 percent of the disk is free on the instance.  Doing so helps prevent issues with aggressive autogrowth, which can  adversely affect the availability of the instance.\n- [Verify](/sql/docs/postgres/import-export#verify) that  the CSV file has the expected data and that it's in the correct format.  CSV files must have one line for each row of data fields.\n- Follow the [best practices for exporting and importing data.](/sql/docs/postgres/import-export) ## Export data from Cloud SQL for PostgreSQL\n### Required roles and permissions for exporting from Cloud SQL for PostgreSQL\nTo export data from Cloud SQL into Cloud Storage, the user initiating the export must have one of the following roles:\n- The [Cloud SQL Editor](/sql/docs/postgres/iam-roles) role\n- A [custom role](/iam/docs/understanding-custom-roles) ,  including the following permissions:- `cloudsql.instances.get`\n- `cloudsql.instances.export`\nAdditionally, the service account for the Cloud SQL instance must have one of the following roles:- The`storage.objectAdmin`Identity and Access Management (IAM) role\n- A custom role, including the following permissions:- `storage.objects.create`\n- `storage.objects.list`(for exporting files in parallel only)\n- `storage.objects.delete`(for exporting files in parallel only)For help with IAM roles, see [Identity and Access Management](/storage/docs/access-control/iam) .\n**Note** : The changes that you make to the IAM permissions and roles might take a few minutes to take effect. For more information, see [Access change propagation](/iam/docs/access-change-propagation) .### Export data to a CSV file from Cloud SQL for PostgreSQL\nYou can export your data in CSV format, which is usable by other tools and environments. Exports happen at the database level. During a CSV export, you can specify the schemas to export. All schemas at the database level are eligible for export.\n**Note:** Cloud SQL uses double quotes (hex value \"22\") as the default escape character. This can be a problem for databases where values for`NULL`are entered as string literals. When importing a file that was exported using the default escape character, the file doesn't treat the value as`NULL`but as`\"NULL\"`. We recommend that you use`--escape=\"5C\"`to override the default when you export the file.\n**Note:** You cannot export to a CSV file from a read replica instance. The export operation creates an export user and grants that user select permissions on the database that the user wants to export. Because read replica instances run in read-only mode, these operations fail.\nTo export data from a database on a Cloud SQL instance to a CSV file in a Cloud Storage bucket:\n- In the Google Cloud console, go to the **Cloud SQL Instances** page. [Go to Cloud SQL Instances](https://console.cloud.google.com/sql) \n- To open the **Overview** page of an instance, click the instance name.\n- Click **Export** .\n- Select **Offload export** to allow other operations to occur while the export is ongoing.\n- In the **Cloud Storage export location** section add the name of the bucket,  folder, and file that you want to export, or click **Browse** to  find or create a bucket, folder, or file.If you click **Browse** :- In the **Location** section, select a Cloud Storage bucket or folder   for your export. **Note: ** If you need to create a new storage    bucket, click the bucket with the **+** sign in the **Location** section. This opens **Create a bucket** , where you can    add a name and other configurations for the bucket. Select a **standard** storage class for your bucket.\n- In the **Name** box, add a name for the `CSV` file, or select an existing file from the list    in the **Location** section.You can use a file extension of `.gz` (the complete    extension would be `.csv.gz` ) to compress your export    file.\n- Click **Select** .\n- In the **Format** section, click **CSV** .\n- In the **Database for export** section, select the name of the database from the  drop-down menu.\n- For **SQL query** , enter a SQL query to specify the table to export  data from.For example, to export the entire contents of the `entries` table in the `guestbook` database, you enter```\nSELECT * FROM guestbook.entries;\n```Your query must specify a table in the specified database. You can't  export an entire database in CSV format. **Note: ** While in-transit, the query might be  processed in intermediate locations other than the location of the target  instance\n- Click **Export** to start the export.\n- The **Export database?** box opens with a message that the export process can take an hour or more for large databases. During the export, the only operation you can perform on the instance is viewing  information. After the export starts, you can [cancel the operation](/sql/docs/postgres/import-export/cancel-import-export) . If this is a good time to start an export, click **Export** . Otherwise, click **Cancel** .\n- [Create a Cloud Storage bucket.](/storage/docs/creating-buckets) \n- Upload the file to your bucket.For help with uploading files to buckets, see [Uploading objects](/storage/docs/uploading-objects) .\n- Find the service account for the Cloud SQL instance you're exporting from. You can do this running the [gcloud sql instances describe](/sdk/gcloud/reference/sql/instances/describe) command. Look for the`serviceAccountEmailAddress`field in the output.```\ngcloud sql instances describe INSTANCE_NAME\n```\n- Use` [gsutil iam](/storage/docs/gsutil/commands/iam) `to grant the`storage.objectAdmin` [IAM role](/storage/docs/access-control/iam-roles) to the Cloud SQL instance service account. For help with setting IAM permissions, see [Using IAM permissions](/storage/docs/access-control/using-iam-permissions) .\n- Export the database: **Note: ** Use the`--offload`flag if you want to use serverless export. Otherwise, remove it from the following command.```\ngcloud sql export csv INSTANCE_NAME gs://BUCKET_NAME/FILE_NAME \\\n--database=DATABASE_NAME \\\n--offload \\\n--query=SELECT_QUERY\n```For information about using the `export csv` command, see the [sql export csv](/sdk/gcloud/reference/sql/export/csv) command reference page. **Note: ** While in-transit, the SELECT_QUERY might be processed in intermediate locations other than the location of the target instance.\n- If you do not need to retain the IAM role you set previously, [revoke](/iam/docs/granting-changing-revoking-access#revoking-console) it now.\n- Create a bucket for the export:```\ngsutil mb -p PROJECT_NAME -l LOCATION_NAME gs://BUCKET_NAME\n```This step is not required, but strongly recommended, so you do not open up access to any other data.\n- Provide your instance with the`legacyBucketWriter` [IAM role](/storage/docs/access-control/iam-roles) for your bucket. For help with setting IAM permissions, see [UsingIAM permissions](/storage/docs/access-control/using-iam-permissions) .\n- Export your database:Before using any of the request data, make the following replacements:- : The project ID\n- : The instance ID\n- : The Cloud Storage bucket name\n- : The path to the CSV file\n- : The name of a database inside the Cloud SQL instance\n- : Enables serverless export. Set to`true`to use serverless export. **Note:** Serverless export costs extra. See the [pricing page](/sql/pricing#storage-networking-prices) .\n- : SQL query for export (optional) **Note:** While in-transit, the query might be  processed in intermediate locations other than the location of the target  instance.\n- : The character that should appear before a data character that needs to be escaped. The value of this argument has to be a character in Hex ASCII Code. For example, \"22\" represents double quotes. (optional)\n- :The character that encloses values from columns that have a string data type. The value of this argument has to be a character in Hex ASCII Code. For example, \"22\" represents double quotes. (optional)\n- : The character that split column values. The value of this argument has to be a character in Hex ASCII Code. For example, \"2C\" represents a comma. (optional)\n- : The character that split line records. The value of this argument has to be a character in Hex ASCII Code. For example, \"0A\" represents a new line. (optional)\nHTTP method and URL:\n```\nPOST https://sqladmin.googleapis.com/v1/projects/project-id/instances/instance-id/export\n```\nRequest JSON body:\n```\n{\n \"exportContext\":\n {\n  \"fileType\": \"CSV\",\n  \"uri\": \"gs://bucket_name/path_to_csv_file\",\n  \"databases\": [\"database_name\"],\n  \"offload\": true | false\n  \"csvExportOptions\":\n  {\n   \"selectQuery\":\"select_query\",\n   \"escapeCharacter\":\"escape_character\",\n   \"quoteCharacter\":\"quote_character\",\n   \"fieldsTerminatedBy\":\"fields_terminated_by\",\n   \"linesTerminatedBy\":\"lines_terminated_by\"\n  }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:You must specify exactly one database with the `databases` property, and if the select query specifies a database, it must be the  same.\n- If you do not need to retain the IAM permissions you set previously, remove them now.For the complete list of parameters for the request, see the\n [instances:export](/sql/docs/postgres/admin-api/rest/v1beta4/instances/export) \npage.\n- Create a bucket for the export:```\ngsutil mb -p PROJECT_NAME -l LOCATION_NAME gs://BUCKET_NAME\n```This step is not required, but strongly recommended, so you do not open up access to any other data.\n- Provide your instance with the`storage.objectAdmin` [IAM role](/storage/docs/access-control/iam-roles) for your bucket. For help with setting IAM permissions, see [UsingIAM permissions](/storage/docs/access-control/using-iam-permissions) .\n- Export your database:Before using any of the request data, make the following replacements:- : The project ID\n- : The instance ID\n- : The Cloud Storage bucket name\n- : The path to the CSV file\n- : The name of a database inside the Cloud SQL instance\n- : Enables serverless export. Set to`true`to use serverless export. **Note:** Serverless export costs extra. See the [pricing page](/sql/pricing#storage-networking-prices) .\n- : SQL query for export (optional) **Note:** While in-transit, the query might be  processed in intermediate locations other than the location of the target  instance.\n- : The character that should appear before a data character that needs to be escaped. The value of this argument must be in [ASCII hex format](https://ascii.cl/) . For example, \"22\" represents double quotes. (optional)\n- : The character that encloses values from columns that have a string data type. The value of this argument must be in [ASCII hex format](https://ascii.cl/) . For example, \"22\" represents double quotes. (optional)\n- : The character that splits column values. The value of this argument must be in [ASCII hex format](https://ascii.cl/) . For example, \"2C\" represents a comma. (optional)\n- : The character that split line records. The value of this argument must be in [ASCII hex format](https://ascii.cl/) . For example, \"0A\" represents a new line. (optional)\nHTTP method and URL:\n```\nPOST https://sqladmin.googleapis.com/sql/v1beta4/projects/project-id/instances/instance-id/export\n```\nRequest JSON body:\n```\n{\n \"exportContext\":\n {\n  \"fileType\": \"CSV\",\n  \"uri\": \"gs://bucket_name/path_to_csv_file\",\n  \"databases\": [\"database_name\"],\n  \"offload\": true | false\n  \"csvExportOptions\":\n  {\n   \"selectQuery\": \"select_query\",\n   \"escapeCharacter\": \"escape_character\",\n   \"quoteCharacter\": \"quote_character\",\n   \"fieldsTerminatedBy\": \"fields_terminated_by\",\n   \"linesTerminatedBy\": \"lines_terminated_by\"\n  }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:You must specify exactly one database with the `databases` property, and if the select query specifies a database, it must be the  same.\n- If you do not need to retain the IAM role you set  previously, [revoke](/iam/docs/granting-changing-revoking-access#revoking-console) it now.For the complete list of parameters for the request, see the\n [instances:export](/sql/docs/postgres/admin-api/rest/v1beta4/instances/export) \npage.\n### Customize the format of a CSV export file\nYou can use `gcloud` or the REST API to customize your CSV file format. When you perform an export, you can specify the following formatting options:\n| CSV option  | Default value       | gcloud flag   | REST API property | Description                           |\n|:------------------|:-----------------------------------------|:-----------------------|:--------------------|:--------------------------------------------------------------------------------------------------------------------|\n| Escape   | \"5C\" ASCII hex code for file separator. | --escape    | escapeCharacter  | Character that appears before a data character that needs to be escaped. Available only for MySQL and PostgreSQL. |\n| Quote    | \"22\" ASCII hex code for double quotes. | --quote    | quoteCharacter  | Character that encloses values from columns that have a string data type. Available only for MySQL and PostgreSQL. |\n| Field delimiter | \"2C\" ASCII hex code for comma.   | --fields-terminated-by | fieldsTerminatedBy | Character that splits column values. Available only for MySQL and PostgreSQL.          |\n| Newline character | \"0A\" ASCII hex code for newline.  | --lines-terminated-by | linesTerminatedBy | Character that splits line records. Available only for MySQL.              |\nFor example, a `gcloud` command using all of these arguments could be like the following:\n```\ngcloud sql export csv INSTANCE_NAME gs://BUCKET_NAME/FILE_NAME \\--database=DATABASE_NAME \\--offload \\--query=SELECT_QUERY \\--quote=\"22\" \\--escape=\"5C\" \\--fields-terminated-by=\"2C\" \\--lines-terminated-by=\"0A\"\n```\nThe equivalent REST API request body would look like this:\n```\n{\u00a0\"exportContext\":\u00a0 \u00a0{\u00a0 \u00a0 \u00a0 \"fileType\": \"CSV\",\u00a0 \u00a0 \u00a0 \"uri\": \"gs://bucket_name/path_to_csv_file\",\u00a0 \u00a0 \u00a0 \"databases\": [\"DATABASE_NAME\"],\u00a0 \u00a0 \u00a0 \"offload\": true,\u00a0 \u00a0 \u00a0 \"csvExportOptions\":\u00a0 \u00a0 \u00a0 \u00a0{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"selectQuery\": \"SELECT_QUERY\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"escapeCharacter\": \u00a0\"5C\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"quoteCharacter\": \"22\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"fieldsTerminatedBy\": \"2C\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"linesTerminatedBy\": \"0A\"\u00a0 \u00a0 \u00a0 \u00a0}\u00a0 \u00a0}}\n```\nCSV export creates standard CSV output by default. If you need even more options than Cloud SQL provides, you can use the following statement in a psql client:\n```\n\u00a0 \u00a0 \u00a0 \\copy [table_name] TO '[csv_file_name].csv' WITH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 (FORMAT csv, ESCAPE '[escape_character]', QUOTE '[quote_character]',\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 DELIMITER '[delimiter_character]', ENCODING 'UTF8', NULL '[null_marker_string]');\n```\n## Import data to Cloud SQL for PostgreSQL\n### Required roles and permissions for importing to Cloud SQL for PostgreSQL\nTo import data from Cloud Storage into Cloud SQL, the user initiating the import must have one of the following roles:\n- The [Cloud SQL Editor](/sql/docs/postgres/iam-roles) role\n- A [custom role](/iam/docs/understanding-custom-roles) ,  including the following permissions:- `cloudsql.instances.get`\n- `cloudsql.instances.import`\nAdditionally, the service account for the Cloud SQL instance must have one of the following roles:- The`storage.objectAdmin`IAM role\n- A custom role, including the following permissions:- `storage.objects.get`\n- `storage.objects.list`(for importing files in parallel only)For help with IAM roles, see [Identity and Access Management](/storage/docs/access-control/iam) .\n**Note** : The changes that you make to the IAM permissions and roles might take a few minutes to take effect. For more information, see [Access change propagation](/iam/docs/access-change-propagation) .### Import data from a CSV file to Cloud SQL for PostgreSQL\n- The database and table you are importing into must exist on your Cloud SQL instance. For help with creating a database, see [Creating a database](/sql/docs/postgres/create-manage-databases#create) .\n- Your CSV file must conform to the [CSV file format requirements](#csv-reqs) .\n**Note:** The default behavior for Cloud SQL is to import columns in your CSV file in the same order as the table schema. If the order is different in your CSV file or if some columns are skipped, use [ importContext.csvImportOptions.columns[]](https://cloud.google.com/sdk/gcloud/reference/sql/import/csv#--columns) to import data from the CSV file.\n**Note:** You cannot import a CSV file that was created using one database engine into an instance created using another database engine.\n[](#)\n### CSV file format requirements\nCSV files must have one line for each row of data and use comma-separated fields.\nTo import data to a Cloud SQL instance using a CSV file:\n- In the Google Cloud console, go to the **Cloud SQL Instances** page. [Go to Cloud SQL Instances](https://console.cloud.google.com/sql) \n- To open the **Overview** page of an instance, click the instance name.\n- Click **Import** .\n- In the **Choose the file you'd like to import data from** section, enter the  path to the bucket and CSV file to use for the import. Or to browse to the file:- Click **Browse** .\n- In the **Location** section, double-click the name of the bucket in the list.\n- Select the file in the list.\n- Click **Select** .\nYou can import a compressed ( `.gz` ) or an uncompressed ( `.csv` ) file.\n- In the **Format** section, select **CSV** .\n- Specify the **Database** and **Table** in  your Cloud SQL instance where you want to import the CSV file.\n- You can optionally specify a user for the import operation.\n- Click the **Import** to start the import.\n- [Create a Cloud Storage bucket.](/storage/docs/creating-buckets) \n- Upload the file to your bucket.For help with uploading files to buckets, see [Uploading objects](/storage/docs/uploading-objects) .\n- Upload data from the CSV file to the bucket.\n- Identify the service account for the Cloud SQL instance you're exporting from. You can do this running the [gcloud sql instances describe](/sdk/gcloud/reference/sql/instances/describe) command with the instance name. Look for the`serviceAccountEmailAddress`field in the output.```\ngcloud sql instances describe INSTANCE_NAME\n```\n- Copy the serviceAccountEmailAddress field.\n- Use` [gsutil iam](/storage/docs/gsutil/commands/iam) `to grant the`storage.objectAdmin` [IAM role](/storage/docs/access-control/iam-roles) to the Cloud SQL instance service account for the bucket. For help with setting IAM permissions, see [Using IAM permissions](/storage/docs/access-control/using-iam-permissions) .\n- Import the file:```\ngcloud sql import csv INSTANCE_NAME gs://BUCKET_NAME/FILE_NAME \\--database=DATABASE_NAME \\--table=TABLE_NAME\n```For information about using the `import csv` command, see the [sql import csv](/sdk/gcloud/reference/sql/import/csv) command reference page.\n- If you do not need to retain the IAM permissions you set previously, remove them using` [gsutil iam](/storage/docs/gsutil/commands/iam) `.\n- [Create a Cloud Storage bucket.](/storage/docs/creating-buckets) \n- Upload the file to your bucket.For help with uploading files to buckets, see [Uploading objects](/storage/docs/uploading-objects) .\n- Provide your instance with the`legacyBucketWriter`and`objectViewer` [IAM roles](/storage/docs/access-control/iam-roles) for your bucket. For help with setting IAM permissions, see [Using IAM permissions](/storage/docs/access-control/using-iam-permissions) .\n- Import the file:Before using any of the request data, make the following replacements:- : The project ID\n- : The instance ID\n- : The Cloud Storage bucket name\n- : The path to the CSV file\n- : The name of a database inside the Cloud SQL instance\n- : The name of the database table\n- : The character that should appear before a data character that needs to be escaped. The value of this argument has to be a character in Hex ASCII Code. For example, \"22\" represents double quotes. (optional)\n- : The character that encloses values from columns that have a string data type. The value of this argument has to be a character in Hex ASCII Code. For example, \"22\" represents double quotes. (optional)\n- : The character that split column values. The value of this argument has to be a character in Hex ASCII Code. For example, \"2C\" represents a comma. (optional)\n- : The character that split line records. The value of this argument has to be a character in Hex ASCII Code. For example, \"0A\" represents a new line. (optional)\nHTTP method and URL:\n```\nPOST https://sqladmin.googleapis.com/v1/projects/project-id/instances/instance-id/import\n```\nRequest JSON body:\n```\n{\n \"importContext\":\n {\n  \"fileType\": \"CSV\",\n  \"uri\": \"gs://bucket_name/path_to_csv_file\",\n  \"database\": \"database_name\",\n  \"csvImportOptions\":\n  {\n   \"table\": \"table_name\",\n   \"escapeCharacter\": \"escape_character\",\n   \"quoteCharacter\": \"quote_character\",\n   \"fieldsTerminatedBy\": \"fields_terminated_by\",\n   \"linesTerminatedBy\": \"lines_terminated_by\"\n  }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:For the complete list of parameters for the request, see the [instances:import](/sql/docs/postgres/admin-api/rest/v1beta4/instances/import) page.\n- If you do not need to retain the IAM permissions you set previously, remove the permissions.\n- [Create a Cloud Storage bucket.](/storage/docs/creating-buckets) \n- Upload the file to your bucket.For help with uploading files to buckets, see [Uploading objects](/storage/docs/uploading-objects) .\n- Provide your instance with the`storage.objectAdmin` [IAM role](/storage/docs/access-control/iam-roles) for your bucket. For help with setting IAM permissions, see [Using IAM permissions](/storage/docs/access-control/using-iam-permissions) .\n- Import the file:Before using any of the request data, make the following replacements:- : The project ID\n- : The instance ID\n- : The Cloud Storage bucket name\n- : The path to the CSV file\n- : The name of a database inside the Cloud SQL instance\n- : The name of the database table\n- : The character that should appear before a data character that needs to be escaped. The value of this argument must be in [ASCII hex format](https://ascii.cl/) . For example,\"22\" represents double quotes. (optional)\n- : The character that encloses values from columns that have a string data type. The value of this argument must be in [ASCII hex format](https://ascii.cl/) . For example, \"22\" represents double quotes. (optional)\n- : The character that split column values. The value of this argument must be in [ASCII hex format](https://ascii.cl/) . For example, \"2C\" represents a comma. (optional)\n- : The character that split line records. The value of this argument must be in [ASCII hex format](https://ascii.cl/) . For example, \"0A\" represents a new line. (optional)\nHTTP method and URL:\n```\nPOST https://sqladmin.googleapis.com/sql/v1beta4/projects/project-id/instances/instance-id/import\n```\nRequest JSON body:\n```\n{\n \"importContext\":\n {\n  \"fileType\": \"CSV\",\n  \"uri\": \"gs://bucket_name/path_to_csv_file\",\n  \"database\": \"database_name\",\n  \"csvImportOptions\":\n  {\n   \"table\": \"table_name\",\n   \"escapeCharacter\": \"escape_character\",\n   \"quoteCharacter\": \"quote_character\",\n   \"fieldsTerminatedBy\": \"fields_terminated_by\",\n   \"linesTerminatedBy\": \"lines_terminated_by\"\n  }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:For the complete list of parameters for the request, see the [instances:import](/sql/docs/postgres/admin-api/rest/v1beta4/instances/import) page.\n- If you do not need to retain the IAM permissions you set previously, remove the permissions.\n### Customize the format of a CSV file for Cloud SQL for PostgreSQL\nYou can use `gcloud` or the REST API to customize your CSV file format.\nA sample `gcloud` command follows:\n```\ngcloud sql import csv INSTANCE_NAME gs://BUCKET_NAME/FILE_NAME \\--database=DATABASE_NAME \\--table=TABLE_NAME \\--quote=\"22\" \\--escape=\"5C\" \\--fields-terminated-by=\"2C\" \\--lines-terminated-by=\"0A\"\n```\nThe equivalent REST API request body would look like this:\n```\n{\u00a0\"importContext\":\u00a0 \u00a0{\u00a0 \u00a0 \u00a0 \"fileType\": \"CSV\",\u00a0 \u00a0 \u00a0 \"uri\": \"gs://bucket_name/path_to_csv_file\",\u00a0 \u00a0 \u00a0 \"database\": [\"DATABASE_NAME\"],\u00a0 \u00a0 \u00a0 \"csvImportOptions\":\u00a0 \u00a0 \u00a0 \u00a0{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"table\": \"TABLE_NAME\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"escapeCharacter\": \u00a0\"5C\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"quoteCharacter\": \"22\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"fieldsTerminatedBy\": \"2C\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"linesTerminatedBy\": \"0A\"\u00a0 \u00a0 \u00a0 \u00a0}\u00a0 \u00a0}}\n```\n**Note: ** If you use custom format options in your import commands, make sure the exported file was created with the same options.\nIf you get an error such as `ERROR_RDBMS` , ensure the table exists. If the table exists, confirm that you have the correct permissions on the bucket. For help configuring access control in Cloud Storage, see [Create and Manage Access Control Lists](/storage/docs/access-control/create-manage-lists) .\n[underlying REST API request](/sql/docs/postgres/admin-api/rest/v1beta4/instances/import)\n[APIs Explorer on the instances:import page](/sql/docs/postgres/admin-api/rest/v1beta4/instances/import)\n## What's next\n- Learn how to [check the status of import and export operations](/sql/docs/postgres/import-export/checking-status-import-export) .\n- Learn more about [best practices for importing and exporting data](/sql/docs/postgres/import-export) .\n- [Known issues for imports and exports](/sql/docs/postgres/known-issues#import-export) .", "guide": "Cloud SQL"}