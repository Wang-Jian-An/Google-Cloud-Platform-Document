{"title": "Cloud SQL - Use Query insights to improve query performance", "url": "https://cloud.google.com/sql/docs/postgres/using-query-insights", "abstract": "# Cloud SQL - Use Query insights to improve query performance\nThis page describes how to use the Query insights dashboard to detect and analyze performance problems.\n", "content": "## Introduction\nQuery insights helps you detect, diagnose, and prevent query performance problems for Cloud SQL databases. It supports intuitive monitoring and provides diagnostic information that helps you go beyond detection to identify the root cause of performance problems.\nWith Query insights, you can monitor performance at an application level and trace the source of a problematic query across the application stack by model, view, controller, route, user, and host. The Query insights tool can integrate with your existing application monitoring (APM) tools and Google Cloud services by using open standards and APIs. This way, you can monitor and troubleshoot query problems by using your favorite tool.\nQuery insights helps you improve Cloud SQL query performance by guiding you through the following steps:\n- [View the database load for top queries.](/sql/docs/postgres/using-query-insights#using-db-load-graph) \n- [Identify a potentially problematic query or tag.](/sql/docs/postgres/using-query-insights#filter-db-load) \n- [Examine the query or tag to identify issues.](/sql/docs/postgres/using-query-insights#specific-query) \n- [Trace the source of the problem.](/sql/docs/postgres/using-query-insights#trace-problem) \nQuery insights is supported on all Cloud SQL machine types and available in all Google Cloud regions.\n## Pricing\nThere's no additional cost for Query insights. You can access one week of data on the Query insights dashboard.\nQuery insights doesn't occupy any storage space in your Cloud SQL instance storage space. Metrics are stored in Cloud Monitoring. For API requests, see the Cloud Monitoring [Pricing](/stackdriver/pricing) . Cloud Monitoring has a tier that you can use at no additional cost.\nYou can create an account to evaluate how Cloud SQL performs in real-world scenarios. New customers also get $300 in free credits to spend on Cloud SQL to run, test, and deploy workloads. You won't be charged until you upgrade.Sign up to [ try Cloud SQL for free](https://console.cloud.google.com/freetrial?redirectPath=/sql) .\n## Before you begin\nTo view a query plan or perform end-to-end tracing, you need specific IAM permissions. [Create a custom role](/iam/docs/creating-custom-roles#creating_a_custom_role) and add the `cloudtrace.traces.get` IAM permission to it. Then, add this role to each user account that needs to use Query insights.\nTo view query plans and their end-to-end views, your Google Cloud project must have the Trace API enabled. This setting lets your Google Cloud project receive trace data from authenticated sources at no additional cost. This data can help you detect and diagnose performance issues in your instance.\nTo confirm that the Trace API is enabled, follow these steps:\n- From the Google Cloud console, go to **APIs and Services** : [Go to APIs and Services](https://console.cloud.google.com/apis) \n- Click **Enable APIs and Services** .\n- In the search bar, enter`Trace API`.\n- If **API enabled** is displayed, then this API is enabled and there's nothing for you to do. Otherwise, click **Enable** .## Enable Query insights\nQuery insights metrics are encrypted at rest. Users who have access to the Cloud SQL dashboard can access Query insights metrics on the Query insights dashboard. If you have permission to update instances, you can configure Query insights. For a list of permissions required for Cloud SQL instances, see [Cloud SQL project access control](/sql/docs/postgres/project-access-control) . If you don't have these permissions and you want to enable Query insights on your instances, then contact your administrator.\n**Note:** When you enable Query insights, all other operations are temporarily suspended. These operations include health checks, logging, monitoring, and other instance operations. However, the instance itself doesn't restart, and you still have uninterrupted access to your database.\n**Note: ** If you don't have Query insights enabled on any of your instances, then you can enable Query insights by clicking **Enable** or **Enable for multiple instances** on the [Query insights dashboard](/sql/docs/postgres/using-query-insights#view_the_query_insights_dashboard) .\n **Enable Query insights for an instance** - In the Google Cloud console, go to the **Cloud SQL Instances** page. [Go to Cloud SQL Instances](https://console.cloud.google.com/sql) \n- To open the **Overview** page of an instance, click the instance name.\n- On the **Configuration** tile, click **Edit configuration** .\n- In the **Configuration options** section, expand **Query insights** .\n- Select the **Enable Query insights** checkbox.\n- Optional: Select one or more of the following Query insights options: **Store client IP addresses** Default: `false`Stores the client IP addresses where queries are coming from and helps you group that data to run metrics against it. Queries come from more than one host. Reviewing graphs for queries from client IP addresses can help identify the source of a problem. **Store application tags** Default: `false`Stores application tags that help you determine the APIs and model-view-controller (MVC) routes that are making requests and group the data to run metrics against it. This option requires you to comment queries with a specific set of tags using the [sqlcommenter](#using-sqlcommenter) open source object-relational mapping (ORM) auto-instrumentation library. This information helps Query insights identify the source of a problem and the MVC from which the problem is coming. Application paths help you with application monitoring. **Customize query lengths** Default: `1024`Sets the query length limit to a specified value from 256 bytes to 4500 bytes. Higher query lengths are more useful for analytical queries, but they also require more memory. Changing the query length requires you to restart the instance. You can still add tags to queries that exceed the length limit. **Set the maximum sampling rate** Default: `5`Sets the maximum sampling rate. Sampling rate is the number of executed query plan samples that are captured per minute across all databases on the instance. Change this value to a number from 0 (to disable sampling) to 20. Increasing the sampling rate is likely to give you more data points but might increase performance overhead.\n- Click **Save** .\n **Enable Query insights for multiple instances** - In the Google Cloud console, go to the **Cloud SQL Instances** page. [Go to Cloud SQL Instances](https://console.cloud.google.com/sql) \n- Click the **More Actions** more_vertmenu on any row.\n- Select **Enable Query insights** .\n- In the dialog box, select the **Enable Query insights for multiple instances ** check box.\n- Click **Enable** .\n- In the subsequent dialog box, select the instances for which you want to enable Query insights.\n- Click **Enable Query insights** . **Note:** This step enables Query insights on your instances based on the [default configuration](/sql/docs/postgres/using-query-insights#configuration-options) [.](None) [](None) \n- [](None) \n [](None) \n [](None) \n [](None) \n [ To enable Query insights for a Cloud SQL instance by using gcloud, run ](None)  [gcloud sql instances patch](/sdk/gcloud/reference/sql/instances/patch) with  the `--insights-config-query-insights-enabled` flag as follows after replacing with the ID of the instance.\n```\ngcloud sql instances patch INSTANCE_ID \\--insights-config-query-insights-enabled\u00a0 \n```\nAlso, use one or more of the following optional flags:- `--insights-config-record-client-address`Stores the client IP addresses where queries are coming from and helps you group that data to run metrics against it. Queries come from more than one host. Reviewing graphs for queries from client IP addresses can help identify the source of a problem.\n- `--insights-config-record-application-tags`Stores application tags that help you determine the APIs and model-view-controller (MVC) routes that are making requests and group the data to run metrics against it. This option requires you to comment queries with a specific set of tags. You can do this by using the [sqlcommenter](#using-sqlcommenter) open source object-relational mapping (ORM) auto-instrumentation library. This information helps Query insights identify the source of a problem and the MVC the problem is coming from. Application paths help you with application monitoring.\n- `--insights-config-query-string-length`Sets the default query length limit to a specified value from 256 to 4500 bytes. The default query length is 1024 bytes. Higher query lengths are more useful for analytical queries, but they also require more memory. Changing the query length requires you to restart the instance. You can still add tags to queries that exceed the length limit.\n- `--query_plans_per_minute`By default, a maximum of 5 executed query plan samples are captured per minute across all databases on the instance. Change this value to a number from 0 (to disable sampling) to 20. Increasing the sampling rate is likely to give you more data points but might add a performance overhead.\nReplace the following:- : The query string  length to be stored, in bytes.\n- : The [custom instance configuration](/sql/docs/postgres/create-instance#machine-types) to use for the instance.\n- : The [region](/sql/docs/postgres/instance-settings#region-values) for the instance.\n```\ngcloud sql instances patch INSTANCE_ID \\--insights-config-query-insights-enabled \\--insights-config-query-string-length=INSIGHTS_CONFIG_QUERY_STRING_LENGTH \\--query_plans_per_minute=QUERY_PLANS_PER_MINUTE \\--insights-config-record-application-tags \\--insights-config-record-client-address \\--tier=API_TIER_STRING \\--region=REGION\u00a0 \n```To enable Query insights for a Cloud SQL instance by using the REST API, call the [instances.patch](/sql/docs/postgres/admin-api/rest/v1beta4/instances/patch) method with [insightsConfig](/sql/docs/postgres/admin-api/rest/v1beta4/instances#insightsconfig) settings.\nBefore using any of the request data, make the following replacements:- : The project ID.\n- : The instance ID.\nHTTP method and URL:\n```\nPATCH https://sqladmin.googleapis.com/sql/v1beta4/projects/project-id/instances/instance-id\n```\nRequest JSON body:\n```\n{\n \"settings\" : { \"insightsConfig\" : { \"queryInsightsEnabled\" : true } }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"kind\": \"sql#operation\",\n \"targetLink\": \"https://sqladmin.googleapis.com/sql/v1beta4/projects/project-id/instances/instance-id\",\n \"status\": \"PENDING\",\n \"user\": \"user@example.com\",\n \"insertTime\": \"2021-01-28T22:43:40.009Z\",\n \"operationType\": \"UPDATE\",\n \"name\": \"operation-id\",\n \"targetId\": \"instance-id\",\n \"selfLink\": \"https://sqladmin.googleapis.com/sql/v1beta4/projects/project-id/operations/operation-id\",\n \"targetProject\": \"project-id\"\n}\n```\nTo use Terraform to enable Query insights for a Cloud SQL instance, set the `query_insights_enabled` flag to `true` . Also, you can use one or more of the following optional flags:\n`query_string_length`: Default is`1024`and you can configure it to a value between`256`and`4500`in bytes.\n`record_application_tags`: Set the value to`true`if you want to record application tags from the query.\n`record_client_address`: Set the value to`true`if you want to record the client IP address.\n`query_plans_per_minute`: Default is`5`and you can configure it to a value between`5`and`20`.\nHere's an example:\n```\nresource \"google_sql_database_instance\" \"INSTANCE_NAME\" {\u00a0name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"INSTANCE_NAME\"\u00a0database_version \u00a0 \u00a0= \"POSTGRESQL_VERSION\"\u00a0region \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= \"REGION\"\u00a0root_password \u00a0 \u00a0 \u00a0 = \"PASSWORD\"\u00a0deletion_protection = false # set to true to prevent destruction of the resource\u00a0settings {\u00a0 \u00a0tier = \"DB_TIER\"\u00a0 \u00a0insights_config {\u00a0 \u00a0 \u00a0query_insights_enabled \u00a0= true\u00a0 \u00a0 \u00a0query_string_length \u00a0 \u00a0 = 2048 # Optional\u00a0 \u00a0 \u00a0record_application_tags = true # Optional\u00a0 \u00a0 \u00a0record_client_address \u00a0 = true # Optional\u00a0 \u00a0 \u00a0query_plans_per_minute \u00a0= 10 # Optional\u00a0 \u00a0}\u00a0}}\n```\nTo apply your Terraform configuration in a Google Cloud project, complete the steps in the following sections.## Prepare Cloud Shell\n- Launch [Cloud Shell](https://shell.cloud.google.com/) .\n- Set the default Google Cloud project  where you want to apply your Terraform configurations.You only need to run this command once per project, and you can run it in any directory.```\nexport GOOGLE_CLOUD_PROJECT=PROJECT_ID\n```Environment variables are overridden if you set explicit values in the Terraform  configuration file.\n## Prepare the directoryEach Terraform configuration file must have its own directory (also called a ).- In [Cloud Shell](https://shell.cloud.google.com/) , create a directory and a new  file within that directory. The filename must have the`.tf`extension\u2014for example`main.tf`. In this  tutorial, the file is referred to as`main.tf`.```\nmkdir DIRECTORY && cd DIRECTORY && touch main.tf\n```\n- If you are following a tutorial, you can copy the sample code in each section or step.Copy the sample code into the newly created `main.tf` .Optionally, copy the code from GitHub. This is recommended  when the Terraform snippet is part of an end-to-end solution.\n- Review and modify the sample parameters to apply to your environment.\n- Save your changes.\n- Initialize Terraform. You only need to do this once per directory.```\nterraform init\n```Optionally, to use the latest Google provider version, include the `-upgrade` option:```\nterraform init -upgrade\n```\n## Apply the changes\n- Review the configuration and verify that the resources that Terraform is going to create or  update match your expectations:```\nterraform plan\n```Make corrections to the configuration as necessary.\n- Apply the Terraform configuration by running the following command and entering`yes`at the prompt:```\nterraform apply\n```Wait until Terraform displays the \"Apply complete!\" message.\n- [Open your Google Cloud project](https://console.cloud.google.com/) to view  the results. In the Google Cloud console, navigate to your resources in the UI to make sure  that Terraform has created or updated them.\n **Note:** Terraform samples typically assume that the required APIs are enabled in your Google Cloud project.\nMetrics are expected to be available in Query insights within minutes of query completion. Review the [Cloud Monitoring data retention policy](/monitoring/quotas#data_retention_policy) . Query insights traces are stored in Cloud Trace. Review the [Cloud Trace data retention policy](/trace/docs/quotas#trace_retention_periods) .\n## View the Query insights dashboard\nThe Query insights dashboard shows the based on factors that you select. Query load is a measurement of the total work for all the queries in the instance in the selected time range. The dashboard provides a series of filters that help you view query load.\nTo open the Query insights dashboard, follow these steps:\n- To open the **Overview** page of an instance, click the instance name.\n- Either select the **Query insights** tab in the left navigation  panel or click the **Go to Query insights for more in-depth info on queries and\nperformance** link.\nThe Query insights dashboard opens. It shows details about the instance at the top.\nThe areas of the dashboard include:\n- **Databases** : Filters query load on a specific database or all databases.\n- **User** : Filters query load from a specific user account.\n- **Client address** : Filters query load from a specific IP address.\n- **Time range** : Filters query load by time ranges, such as hour, day, week, or a custom range.\n- **Database load graph** : Displays the query load graph, based on filtered data.\n- **CPU capacity, CPU and CPU wait, IO wait, and Lock wait** : Filters loads based on the options that you select. See [View the database load for top queries](/sql/docs/postgres/using-query-insights#using-db-load-graph) for details about each of these filters.\n- **Queries and tags.** Filters query load by either a selected query or a selected SQL query tag. See [Filter the database load](/sql/docs/postgres/using-query-insights#filter-db-load) .## View the database load for all queries\nDatabase query load is a measure of the work (in CPU seconds) that the executed queries in your selected database perform over time. Each running query is either using or waiting for CPU resources, IO resources, or lock resources. Database query load is the ratio of the amount of time taken by all the queries that are completed in a given time window to the wall-clock time.\nThe top-level Query insights dashboard shows the **Database load \u2014 all topqueries** graph. Drop-down menus on the dashboard let you filter the graph for a specific database, user, or client address.\nColored lines in the graph show the query load, split into four categories:\n- **CPU capacity** : The number of CPUs available on the instance.\n- **CPU and CPU Wait** : The ratio of the time taken by queries in an active state to wall-clock time. IO and Lock waits don't block queries that are in an active state. This metric might mean that the query is either using the CPU or waiting for the Linux scheduler to schedule the server process running the query while other processes are using the CPU. **Note:** CPU load accounts for both the runtime and the time waiting for the Linux scheduler to schedule the server process that's running. As a result, the CPU load can go beyond the maximum core line.\n- **IO Wait** : The ratio of time taken by queries that are waiting for IO to wall-clock time. IO wait includes Read IO Wait and Write IO Wait.See the PostgreSQL [event table](https://www.postgresql.org/docs/current/monitoring-stats.html#WAIT-EVENT-TABLE) .If you want a breakdown of information for IO waits, you can see it in Cloud Monitoring. See [Cloud SQL metrics](/sql/docs/postgres/admin-api/metrics) for more information.\n- **Lock Wait** : The ratio of time taken by queries that are waiting for Locks to wall-clock time. It includes Lock Waits, LwLock Waits, and Buffer pin Lock waits. To see a breakdown of information for lock waits, use Cloud Monitoring. See [Cloud SQL metrics](/sql/docs/postgres/admin-api/metrics) for more information.\nReview the graph and use the filtering options to explore these questions:\n- **Is the query load high?** Is the graph spiking or elevated over time? If you don't see a high load, then the problem isn't with your query.\n- **How long has the load been high?** Is it high only now or has it been high for a long time? Use the range selector to select various time periods to find out how long the problem has lasted. Zoom in to view a time window where query load spikes are observed. Zoom out to view up to one week of the timeline.\n- **What's causing the high load?** You can select options to examine the CPU capacity, CPU and CPU wait, Lock wait, or IO wait. The graph for each of these options is a different color so that you can easily spot the one with the highest load. The dark blue line on the graph shows the maximum CPU capacity of the system. It lets you compare the query load with the maximum CPU system capacity. This comparison helps you determine whether an instance is running out of CPU resources.\n- **Which database is experiencing the load?** Select different databases from the Databases drop-down menu to find the databases with the highest loads.\n- **Do specific users or IP addresses cause higher loads?** Select different users and addresses from the drop-down menus to identify the ones that are causing higher loads.## Filter the database load\nYou can filter the database load by queries or tags.\n### Filter by queries\nThe **Queries** table provides an overview of the queries that cause the most query load. The table shows all the normalized queries for the time window and options selected on the Query insights dashboard. It sorts queries by the total execution time during the time window that you selected.\nTo sort the table, select a column heading or a property from **Filter queries** . The table shows the following properties:\n- **Query** : The normalized query string. Query insights shows only 1024 characters in the query string by default.Queries labeled `UTILITY COMMAND` usually include `BEGIN` , `COMMIT` , and `EXPLAIN` commands or wrapper commands.\n- **Database** : The database against which the query was run.\n- **Load by total time/Load by CPU/Load by IO wait/Load by lock wait** : The options by which you can filter specific queries to find the largest load.\n- **Avg execution time (ms)** : The average time for the query to execute.\n- **Times called** : The number of times the application called the query.\n- **Avg rows returned** : The average number of rows returned for the query.\nQuery insights stores and displays only normalized queries. By default, Query insights doesn't collect IP addresses or tag information. You can enable Query insights to collect this information and, when required, disable collection. Query plan traces don't collect or store any constant values and removes any PII information that the constant might show.\nFor PostgreSQL 9.6 and 10, Query insights displays normalized queries, that is, `?` replaces the literal constant value. In the following example, the name constant is removed and `?` replaces it.\n```\nUPDATE\u00a0 \"demo_customer\"SET\u00a0 \"customer_id\" = ?::uuid,\u00a0 \"name\" = ?,\u00a0 \"address\" = ?,\u00a0 \"rating\" = ?,\u00a0 \"balance\" = ?,\u00a0 \"current_city\" = ?,\u00a0 \"current_location\" = ?WHERE\u00a0 \"demo_customer\".\"id\" = ?\n```\nFor PostgreSQL version 11 and later, `$1` , `$2` , and so on, replace literal constant values.\n```\nUPDATE\u00a0 \"demo_customer\"SET\u00a0 \"customer_id\" = $1::uuid,\u00a0 \"name\" = $2,\u00a0 \"address\" = $3,\u00a0 \"rating\" = $4,\u00a0 \"balance\" = $5,\u00a0 \"current_city\" = $6,\u00a0 \"current_location\" = $7WHERE\u00a0 \"demo_customer\".\"id\" = $8\n```\n### Filter by query tags\nTo troubleshoot an application, you must first [add tags to your SQL queries](#adding-tags-to-sql-queries) . Query load tags provide a breakdown of the query load of the selected tag over time.\nQuery insights provides application-centric monitoring to diagnose performance problems for applications built using ORMs. If you're responsible for the entire application stack, Query insights provides query monitoring from an application view. Query tagging helps you find issues at higher-level constructs, such as with the business logic or a microservice.\nYou might tag queries by the business logic, for example, the payment, inventory, business analytics, or shipping tags. You can then find the query load that the various business logics create. For example, you might observe unexpected events, such as spikes for a business analytics tag at 1 PM or abnormal growth for a payment service trending over the previous week.\nTo calculate the **Database load for tag** , Query insights uses the amount of time taken by every query that uses the tag that you select. The tool calculates the completion time at the minute boundary by using wall-clock time.\nOn the Query insights dashboard, to view the tags table, select **Tags** . The table sorts tags by their total load by total time.\nYou can sort the table by selecting a property from **Filter tags** , or by clicking a column heading. The table shows the following properties:\n- **Action, Controller, Framework, Route, Application, DB driver** : Each property that you added to your queries appears as a column. At least one of these properties must be added if you want to filter by tags.\n- **Load by total time/Load by CPU/Load by IO wait/Load by lock wait** : Options to filter specific queries to find the largest load for each option.\n- **Avg execution time (ms)** : The average time for the query to run.\n- **Avg rows returned** : The average number of rows returned for the query.\n- **Times called** : The number of times the application called the query.\n- **Database** : The database against which the query was run.## Examine a specific query or tag\nTo determine whether a query or a tag is the root cause of the problem, do the following from the **Queries** tab or **Tags** tab, respectively:\n- To sort the list in descending order, click the **Load by total time** header.\n- Click the query or tag at the top of the list. It has the highest load and is taking more time than the others.\nA dashboard opens showing the details of the selected query or tag.\n### Examine a specific query load\nThe dashboard for a selected query appears as follows:\nThe **Database load \u2014 specific query** graph shows a measure of the work (in CPU seconds) that your normalized query has performed in your selected query over time. To calculate load, it uses the amount of time taken by the normalized queries that are completed at the minute boundary to the wall-clock time. At the top of the table, the first 1024 characters of the normalized query, with literals removed for aggregation and PII reasons, are displayed.\nAs with the total queries graph, you can filter the load for a specific query by **Database** , **User** , and **Client address** . Query load is split into **CPUcapacity** , **CPU and CPU wait** , **IO wait** , and **Lock wait.**\n### Examine a specific tagged query load\nThe dashboard for a selected tag appears as follows. For example, if all queries from a microservices payment are tagged as `payment` , you can see the amount of query load that's trending by viewing the tag `payment` .\nThe **Database load \u2014 specific tags** graph shows a measure of the work (in CPU seconds) that queries matching your selected tags have performed in your selected database over time. As with the total queries graph, you can filter the load for a specific tag by **Database** , **User** , and **Client address** .\n### Examine operations in a sampled query plan\nA [query plan](https://en.wikipedia.org/wiki/Query_plan) takes a sample of your query and breaks it down into individual operations. It explains and analyzes each operation in the query.\nThe **Query plan samples** graph shows all the query plans running at particular times and the amount of time each plan took to run. You can change the rate at which query plan samples are captured per minute. See [Enable Query insights](#enable-insights) .\nBy default, the panel on the right shows the details for the sample query plan that takes the longest time, as visible on the **Query plan samples** graph. To see the details for another sample query plan, click the relevant circle on the graph. Expanded details show a model of all the operations in the query plan. Each operation shows the latency, rows returned, and the cost of the operation. When you select an operation, you can see more details, such as shared hit blocks, the type of schema, loops, and plan rows.\nTry to narrow down the problem by looking into the following questions:\n- What's the resource consumption?\n- How does it relate to other queries?\n- Does consumption change over time?\n### Examine latency\nis the time taken for the normalized query to complete, in wall-clock time. You use the **Latency** graph to examine latency on the query or tag. The latency dashboard shows the 50th, 95th, and 99th percentile latencies to find outlier behaviors.\nThe following image shows the database load graph at the 50th percentile for a specific query with filters selected for CPU capacity, CPU and CPU wait, IO wait, and Lock wait.\nThe latency of [parallel queries](https://www.postgresql.org/docs/10/parallel-query.html) is measured in wall-clock time even though the query load can be higher for the query due to multiple cores being used to run part of the query.\nTry to narrow down the problem by looking into the following questions:\n- **What's causing the high load?** Select options to look at the CPU capacity, CPU and CPU wait, I/O wait, or Lock wait.\n- **How long has the load been high?** Is it only high now? Or has it been high for a long time? Change the time range to find the date and time that the load started performing poorly.\n- **Were there spikes in latency?** Change the time window to study the historical latency for the normalized query.## Trace the source of the problem\nWhen you find the areas and times where the load was the highest, identify the source of the problem by using tracing to drill down further.\nTo help you identify the specific source of the problem, such as a model, view, controller, route, host, or user, Query insights provides an in-context end-to-end application trace view. This view helps you understand what's going on at the Database Layer for a specific request and to find the source of a problematic query by model, view, controllers, and route.\nIf you enable [OpenCensus](https://opencensus.io/) or [OpenTelemetry](https://opentelemetry.io/) , [opencensus span](https://opencensus.io/tracing/span/) information is sent to the database along with the tag information inside SQL comments. Any traces from the application to Cloud Logging are linked with database query plan traces to help identify the source of the problem.\nClick the **End to end** tab in the Sample Query screen to look at the in-context trace.\nTo determine the client and user causing the problem, use the **Top client addresses** and **Top users** tables to find the highest loads. You can add a user or IP address to the filter to further analyze a specific user or client address. The details in the tables include the percentage of the query load, the average execution time in milliseconds, and the times called.\nYou can use Cloud Trace to see end-to-end tracing for each step in the query plan. On the Query insights dashboard, click the **View in trace** link to open the Cloud Trace tool. The trace graph shows all the traces that have been run for the selected period.\nFor details, see [Finding and viewing traces](/trace/docs/finding-traces) .\n## Add tags to SQL queries\nTagging SQL queries simplifies application troubleshooting. You can use [sqlcommenter](https://google.github.io/sqlcommenter/) to add tags to your SQL queries either automatically or manually.\n### Use sqlcommenter with ORM\nWhen you use [ORM](https://en.wikipedia.org/wiki/Object-relational_mapping) instead of directly writing SQL queries, you might not find application code that's causing performance challenges. You might also have trouble analyzing how your application code affects query performance. To tackle this issue, Query insights provides an open source library called sqlcommenter. This library is useful for developers and administrators using ORM tools to detect which application code is causing performance problems.\nIf you're using ORM and sqlcommenter together, the tags are automatically created. You don't need to add or change code in your application.\nYou can install sqlcommenter on the application server. The instrumentation library allows application information related to your MVC framework to be propagated to the database along with the queries as a SQL comment. The database picks up these tags and starts recording and aggregating statistics by tags, which are orthogonal to statistics aggregated by normalized queries. Query insights shows the tags so that you know which application is causing the query load and can find the application code that's causing performance problems.\nWhen you examine results in SQL database logs, they appear as follows:\n```\nSELECT * from USERS /*action='run+this',controller='foo%3',traceparent='00-01',tracestate='rojo%2'*/\n```\nSupported tags include the controller name, route, framework, and action.\nThe set of ORM tools in sqlcommenter is supported for the following programming languages:\n| 0  | 1        |\n|:--------|:---------------------------------|\n| Python | Django psycopg2 Sqlalchemy Flask |\n| Java | Hibernate Spring     |\n| Ruby | Rails       |\n| Node.js | Knex.js Sequelize.js Express.js |\nFor more information about sqlcommenter and how to use it in your ORM framework, see the [sqlcommenter documentation](https://google.github.io/sqlcommenter/spec/) .\n### Use sqlcommenter to add tags\nIf you're not using ORM, you must manually add sqlcommenter tags or comments in the correct [SQL comment format](https://google.github.io/sqlcommenter/spec/#format) to your SQL query. You must also augment each SQL statement with a comment containing a serialized key-value pair. Use at least one of the following keys:\n- `action=''`\n- `controller=''`\n- `framework=''`\n- `route=''`\n- `application=''`\n- `db driver=''`\nQuery insights drops all other keys.\n## Disable Query insights\nTo disable Query insights for a Cloud SQL instance by using the Google Cloud console, follow these steps:- In the Google Cloud console, go to the **Cloud SQL Instances** page. [Go to Cloud SQL Instances](https://console.cloud.google.com/sql) \n- To open the **Overview** page of an instance, click the instance name.\n- On the **Configuration** tile, click **Edit configuration** .\n- In the **Configuration options** section, expand **Query insights** .\n- Clear the **Enable Query insights** checkbox.\n- Click **Save** .\nTo disable Query insights for a Cloud SQL instance by using `gcloud` ,  run [gcloud sql instances patch](/sdk/gcloud/reference/sql/instances/patch) with  the `--no-insights-config-query-insights-enabled` flag as follows, after replacing with the ID of the instance.\n```\ngcloud sql instances patch INSTANCE_ID \\--no-insights-config-query-insights-enabled\u00a0 \n```To disable Query insights for a Cloud SQL instance by using the REST API, call the [instances.patch](/sql/docs/postgres/admin-api/rest/v1beta4/instances/patch) method with `queryInsightsEnabled` set to `false` as follows.\nBefore using any of the request data, make the following replacements:- : The project ID.\n- : The instance ID.\nHTTP method and URL:\n```\nPATCH https://sqladmin.googleapis.com/sql/v1beta4/projects/project-id/instances/instance-id\n```\nRequest JSON body:\n```\n{\n \"settings\" : { \"insightsConfig\" : { \"queryInsightsEnabled\" : false } }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"kind\": \"sql#operation\",\n \"targetLink\": \"https://sqladmin.googleapis.com/sql/v1beta4/projects/project-id/instances/instance-id\",\n \"status\": \"PENDING\",\n \"user\": \"user@example.com\",\n \"insertTime\": \"2021-01-28T22:43:40.009Z\",\n \"operationType\": \"UPDATE\",\n \"name\": \"operation-id\",\n \"targetId\": \"instance-id\",\n \"selfLink\": \"https://sqladmin.googleapis.com/sql/v1beta4/projects/project-id/operations/operation-id\",\n \"targetProject\": \"project-id\"\n}\n```\n## What's next\n- Launch blog: [Database observability for developers: introducing Cloud SQL Insights](https://cloud.google.com/blog/products/databases/get-ahead-of-database-performance-issues-with-cloud-sql-insights) \n- See [Cloud SQL metrics](/sql/docs/postgres/admin-api/metrics) . The Query insights metric type strings start with`database/postgresql/insights`.\n- Blog: [Boost your query performance troubleshooting skills with Cloud SQL Insights](https://cloud.google.com/blog/topics/developers-practitioners/boost-your-query-performance-troubleshooting-skills-cloud-sql-insights) \n- Video: [Introducing Cloud SQL Insights](https://www.youtube.com/watch?v=qN7x3ngwz1o) \n- Podcast: [ Cloud SQL Insights](https://www.gcppodcast.com/post/episode-247-cloud-sql-insights-with-nimesh-bhagat/) \n- [Insights Codelab](https://codelabs.developers.google.com/codelabs/cloud-sql-insights-intro#0) \n- Blog: [Introducing Sqlcommenter: An open source ORM auto-instrumentation library](https://cloud.google.com/blog/topics/developers-practitioners/introducing-sqlcommenter-open-source-orm-auto-instrumentation-library) \n- Blog: [Enable query tagging with Sqlcommenter](https://cloud.google.com/blog/topics/developers-practitioners/enable-query-tagging-sqlcommenter-understand-application-impact-database-performance)", "guide": "Cloud SQL"}