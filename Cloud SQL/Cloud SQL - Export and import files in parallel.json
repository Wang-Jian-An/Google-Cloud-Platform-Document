{"title": "Cloud SQL - Export and import files in parallel", "url": "https://cloud.google.com/sql/docs/postgres/import-export/import-export-parallel", "abstract": "# Cloud SQL - Export and import files in parallel\nThis page describes exporting and importing files into Cloud SQL instances in parallel.\n**Note:** If you're migrating an entire database from a supported database server (on-premises, in AWS, or Cloud SQL) to a new Cloud SQL instance, then use [Database Migration Service](/database-migration/docs) instead of exporting and importing files in parallel. If you're exporting because you want to create a new instance from the exported file, consider [restoring from a backup to a different instance](/sql/docs/postgres/backup-recovery/restoring#restorebackups-another-instance) or [cloning the instance](/sql/docs/postgres/clone-instance) .You can verify that the import or export operation for multiple files in parallel completed successfully by [checking the operation's status](/sql/docs/postgres/import-export/checking-status-import-export) . You can also cancel the import of data into Cloud SQL instances and the export of data from the instances. For  more information about cancelling an import or export operation, see [Cancel the import and export of data](/sql/docs/postgres/import-export/cancel-import-export) .\n", "content": "## Before you begin\nBefore you begin an export or import operation:\n- Ensure that your database has adequate free space.\n- Export and import operations use database resources, but they don't  interfere with typical database operations unless the instance is under-provisioned.\n- **Important** : Before starting a large operation, ensure  that at least 25 percent of the disk is free on the instance.  Doing so helps prevent issues with aggressive autogrowth, which can  adversely affect the availability of the instance.\n- Follow the [best practices for exporting and importing data.](/sql/docs/postgres/import-export) \n- After completing an import operation, [verify](/sql/docs/postgres/import-export#verify) the results.## Export data from Cloud SQL for PostgreSQL to multiple files in parallel\nThe following sections contain information about exporting data from Cloud SQL for PostgreSQL to multiple files in parallel.\n### Required roles and permissions for exporting data from Cloud SQL for PostgreSQL to multiple files in parallel\nTo export data from Cloud SQL into Cloud Storage, the user initiating the export must have one of the following roles:\n- The [Cloud SQL Editor](/sql/docs/postgres/iam-roles) role\n- A [custom role](/iam/docs/understanding-custom-roles) ,  including the following permissions:- `cloudsql.instances.get`\n- `cloudsql.instances.export`\nAdditionally, the service account for the Cloud SQL instance must have one of the following roles:- The`storage.objectAdmin`Identity and Access Management (IAM) role\n- A custom role, including the following permissions:- `storage.objects.create`\n- `storage.objects.list`(for exporting files in parallel only)\n- `storage.objects.delete`(for exporting files in parallel only)For help with IAM roles, see [Identity and Access Management](/storage/docs/access-control/iam) .\n**Note** : The changes that you make to the IAM permissions and roles might take a few minutes to take effect. For more information, see [Access change propagation](/iam/docs/access-change-propagation) .### Export data to multiple files in parallel\n[pg_dump](https://www.postgresql.org/docs/current/static/app-pgdump.html)\n`--jobs`\nIf you plan to import your data into Cloud SQL, then follow the instructions provided in [Exporting data from an external database server](/sql/docs/postgres/import-export/import-export-dmp#external-server) so that your files are formatted correctly for Cloud SQL.\n**Note:** If your data contains large objects (blobs), then the export might consume a large amount of memory, impacting instance performance. For help, see [Issues with importing and exporting data](/sql/docs/postgres/known-issues#import-export) .\nTo export data from Cloud SQL to multiple files in parallel, complete the following steps:- [Create a Cloud Storage bucket](/storage/docs/creating-buckets) . **Note: ** You don't have to create a folder in the bucket. If the folder doesn't exist, then Cloud SQL creates it for you as a part of the process of exporting multiple files in parallel. However, if the folder exists, then it must be empty or the export operation fails.\n- To find the service account for the Cloud SQL instance that you're exporting files from, use the [gcloud sql instances describe](/sdk/gcloud/reference/sql/instances/describe) command.```\ngcloud sql instances describe INSTANCE_NAME\n```\n- Replace with the name of your Cloud SQL instance.\n- In the output, look for the value that's associated with the `serviceAccountEmailAddress` field.\n- To grant the`storage.objectAdmin` [IAM role](/storage/docs/access-control/iam-roles) to the service account, use the` [gsutil iam](/storage/docs/gsutil/commands/iam) `utility. For help with setting IAM permissions, see [Use IAM permissions](/storage/docs/access-control/using-iam-permissions) .\n- To export data from Cloud SQL to multiple files in parallel, use the [gcloud sql export sql](/sdk/gcloud/reference/sql/export/sql) command:```\ngcloud sql export sql INSTANCE_NAME gs://BUCKET_NAME/BUCKET_PATH/FOLDER_NAME \\\n--offload \\\n--parallel \\\n--threads=THREAD_NUMBER \\\n--database=DATABASE_NAME \\\n--table=TABLE_EXPRESSION\n```Make the following replacements:- : the name of the Cloud SQL instance from which you're exporting files in parallel.\n- : the name of the Cloud Storage bucket.\n- : the path to the bucket where the export files are stored.\n- : the folder where the export files are stored.\n- : the number of threads that Cloud SQL uses to export files in parallel. For example, if you want to export three files at a time in parallel, then specify`3`as the value for this parameter.\n- : the name of the database inside of the Cloud SQL instance from which the export is made. You must specify only one database.\n- : the tables to export from the specified database.\n **Note: ** If you want to use serverless exports for up to 2 threads, then use the`offload`parameter. If you want to export multiple files in parallel, then use the`parallel`parameter. Otherwise, remove these parameters from the command.The `export sql` command doesn't contain triggers or stored procedures, but does contain views. To export triggers or stored procedures, use a single thread for the export. This thread uses the [pg_dump](https://www.postgresql.org/docs/current/static/app-pgdump.html) tool.After the export completes, you should have files in a folder in the Cloud Storage bucket in the `pg_dump` directory format.\n- If you don't need the IAM role that you set in [Required roles and permissions for exporting from Cloud SQL for PostgreSQL](#required-roles-and-permissions-export) , then [revoke](/iam/docs/granting-changing-revoking-access#revoking-console) it.\nTo export data from Cloud SQL to multiple files in parallel, complete the following steps:- Create a Cloud Storage bucket:```\ngsutil mb -p PROJECT_NAME -l LOCATION_NAME gs://BUCKET_NAME\n```Make the following replacements:- : the name of the Google Cloud project that contains the Cloud Storage bucket you're creating.\n- : the location of the bucket where you want to store the files you're exporting. For example,`us-east1`.\n- : the name of the bucket, subject to [naming requirements](/storage/docs/buckets#naming) . For example,`my-bucket`.\n **Note: ** You don't have to create a folder in the bucket. If the folder doesn't exist, then Cloud SQL creates it for you as a part of the process of exporting multiple files in parallel. However, if the folder exists, then it must be empty or the export operation fails.\n- Provide your instance with the`legacyBucketWriter` [IAM role](/storage/docs/access-control/iam-roles) for your bucket. For help with setting IAM permissions, see [UseIAM permissions](/storage/docs/access-control/using-iam-permissions) .\n- Export data from Cloud SQL to multiple files in parallel:Before using any of the request data, make the following replacements:- : the name of the Google Cloud project that contains the Cloud Storage bucket you created.\n- : the name of the Cloud SQL instance from which you're exporting files in parallel.\n- : the name of the Cloud Storage bucket.\n- : the path to the bucket where the export files are stored.\n- : the folder where the export files are stored.\n- : the name of the database inside of the Cloud SQL instance from which the export is made. You must specify only one database.\n- : the number of threads that Cloud SQL uses to export files in parallel. For example, if you want to export three files at a time in parallel, then specify`3`as the value for this parameter.\n **Note: ** The`offload`parameter enables you to use serverless exports for up to 2 threads. The`parallel`parameter enables you to export multiple files in parallel. To use these features, set the values of these parameters to. Otherwise, set their values to.HTTP method and URL:\n```\nPOST https://sqladmin.googleapis.com/v1/projects/PROJECT_NAME/instances/INSTANCE_NAME/export\n```\nRequest JSON body:\n```\n{\n \"exportContext\":\n {\n  \"fileType\": \"SQL\",\n  \"uri\": \"gs://BUCKET_NAME/BUCKET_PATH/FOLDER_NAME\",\n  \"databases\": [\"DATABASE_NAME\"],\n  \"offload\": [TRUE|FALSE],\n  \"sqlExportOptions\": {\n  \"parallel\": [TRUE|FALSE],\n  \"threads\": [THREAD_NUMBER]\n  }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n- After the export completes, you should have files in a folder in the Cloud Storage bucket in the `pg_dump` directory format.\n- If you don't need the IAM role that you set in [Required roles and permissions for exporting from Cloud SQL for PostgreSQL](#required-roles-and-permissions-export) , then [revoke](/iam/docs/granting-changing-revoking-access#revoking-console) it.For the complete list of parameters for the request, see the\n [Cloud SQL Admin API](/sql/docs/postgres/admin-api/rest/v1beta4/instances/export) \npage.\nTo export data from Cloud SQL to multiple files in parallel, complete the following steps:- Create a Cloud Storage bucket:```\ngsutil mb -p PROJECT_NAME -l LOCATION_NAME gs://BUCKET_NAME\n```Make the following replacements:- : the name of the Google Cloud project that contains the Cloud Storage bucket you're creating.\n- : the location of the bucket where you want to store the files you're exporting. For example,`us-east1`.\n- : the name of the bucket, subject to [naming requirements](/storage/docs/buckets#naming) . For example,`my-bucket`.\n **Note: ** You don't have to create a folder in the bucket. If the folder doesn't exist, then Cloud SQL creates it for you as a part of the process of exporting multiple files in parallel. However, if the folder exists, then it must be empty or the export operation fails.\n- Provide your instance with the`storage.objectAdmin` [IAM role](/storage/docs/access-control/iam-roles) for your bucket. For help with setting IAM permissions, see [UseIAM permissions](/storage/docs/access-control/using-iam-permissions) .\n- Export data from Cloud SQL to multiple files in parallel:Before using any of the request data, make the following replacements:- : the name of the Google Cloud project that contains the Cloud Storage bucket you created.\n- : the name of the Cloud SQL instance from which you're exporting files in parallel.\n- : the name of the Cloud Storage bucket.\n- : the path to the bucket where the export files are stored.\n- : the folder where the export files are stored.\n- : the name of the database inside of the Cloud SQL instance from which the export is made. You must specify only one database.\n- : the number of threads that Cloud SQL uses to export files in parallel. For example, if you want to export three files at a time in parallel, then specify`3`as the value for this parameter.\n **Note: ** The`offload`parameter enables you to use serverless exports for up to 2 threads. The`parallel`parameter enables you to export multiple files in parallel. To use these features, set the values of these parameters to. Otherwise, set their values to.HTTP method and URL:\n```\nPOST https://sqladmin.googleapis.com/sql/v1beta4/projects/PROJECT_NAME/instances/INSTANCE_NAME/export\n```\nRequest JSON body:\n```\n{\n \"exportContext\":\n {\n  \"fileType\": \"SQL\",\n  \"uri\": \"gs://BUCKET_NAME/BUCKET_PATH/FOLDER_NAME\",\n  \"databases\": [\"DATABASE_NAME\"],\n  \"offload\": [TRUE|FALSE],\n  \"sqlExportOptions\": {\n  \"parallel\": [TRUE|FALSE],\n  \"threads\": [THREAD_NUMBER]\n  }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n- After the export completes, you should have files in a folder in the Cloud Storage bucket in the `pg_dump` directory format.\n- If you don't need the IAM role that you set in [Required roles and permissions for exporting from Cloud SQL for PostgreSQL](#required-roles-and-permissions-export) , then [revoke](/iam/docs/granting-changing-revoking-access#revoking-console) it.For the complete list of parameters for the request, see the\n [Cloud SQL Admin API](/sql/docs/postgres/admin-api/rest/v1beta4/instances/export) \npage.\n## Import data from multiple files in parallel to Cloud SQL for PostgreSQL\nThe following sections contain information about importing data from multiple files in parallel to Cloud SQL for PostgreSQL.\n### Required roles and permissions for importing data from multiple files in parallel to Cloud SQL for PostgreSQL\nTo import data from Cloud Storage into Cloud SQL, the user initiating the import must have one of the following roles:\n- The [Cloud SQL Editor](/sql/docs/postgres/iam-roles) role\n- A [custom role](/iam/docs/understanding-custom-roles) ,  including the following permissions:- `cloudsql.instances.get`\n- `cloudsql.instances.import`\nAdditionally, the service account for the Cloud SQL instance must have one of the following roles:- The`storage.objectAdmin`IAM role\n- A custom role, including the following permissions:- `storage.objects.get`\n- `storage.objects.list`(for importing files in parallel only)For help with IAM roles, see [Identity and Access Management](/storage/docs/access-control/iam) .\n**Note** : The changes that you make to the IAM permissions and roles might take a few minutes to take effect. For more information, see [Access change propagation](/iam/docs/access-change-propagation) .### Import data to Cloud SQL for PostgreSQL\nYou can import data in parallel from multiple files that reside in Cloud Storage to your database. To do this, use the [pg_restore](https://www.postgresql.org/docs/current/static/app-pgrestore.html) utility with the `--jobs` option.\n**Note:** If your data contains large objects (blobs), then the import might consume a large amount of memory, impacting instance performance. For help, see [Issues with importing and exporting data](/sql/docs/postgres/known-issues#import-export) .\nTo import data from multiple files in parallel into Cloud SQL, complete the following steps:- [Create a Cloud Storage bucket](/storage/docs/creating-buckets) .\n- Upload the files to your bucket. **Note: ** Make sure that the files that you're uploading are in the `pg_dump` directory format. For more information, see [Export data from multiple files in parallel](#export-data-multiple-files-parallel) .For help with uploading files to buckets, see [Upload objects from files](/storage/docs/uploading-objects) .\n- To find the service account for the Cloud SQL instance that you're importing files to, use the [gcloud sql instances describe](/sdk/gcloud/reference/sql/instances/describe) command.```\ngcloud sql instances describe INSTANCE_NAME\n```\n- Replace with the name of your Cloud SQL instance.\n- In the output, look for the value that's associated with the `serviceAccountEmailAddress` field.\n- To grant the`storage.objectAdmin` [IAM role](/storage/docs/access-control/iam-roles) to the service account, use the` [gsutil iam](/storage/docs/gsutil/commands/iam) `utility. For help with setting IAM permissions, see [Use IAM permissions](/storage/docs/access-control/using-iam-permissions) .\n- To import data from multiple files in parallel into Cloud SQL, use the [gcloud sql import sql](/sdk/gcloud/reference/sql/import/sql) command:```\ngcloud sql import sql INSTANCE_NAME gs://BUCKET_NAME/BUCKET_PATH/FOLDER_NAME \\\n--offload \\\n--parallel \\\n--threads=THREAD_NUMBER \\\n--database=DATABASE_NAME\n```Make the following replacements:- : the name of the Cloud SQL instance to which you're importing files in parallel.\n- : the name of the Cloud Storage bucket.\n- : the path to the bucket where the import files are stored.\n- : the folder where the import files are stored.\n- : the number of threads that Cloud SQL uses to import files in parallel. For example, if you want to import three files at a time in parallel, then specify`3`as the value for this parameter.\n- : the name of the database inside of the Cloud SQL instance from which the import is made. You must specify only one database.\n **Note: ** If you want to use serverless imports for up to 2 threads, then use the`offload`parameter. If you want to import multiple files in parallel, then use the`parallel`parameter. Otherwise, remove these parameters from the command.If the command returns an error like `ERROR_RDBMS` , then review the  permissions; this error is often due to permissions issues.\n- If you don't need the IAM permissions that you set in [Required roles and permissions for importing to Cloud SQL for PostgreSQL](#required-roles-and-permissions-import) , then use` [gsutil iam](/storage/docs/gsutil/commands/iam) `to remove them.\nTo import data from multiple files in parallel into Cloud SQL, complete the following steps:- Create a Cloud Storage bucket:```\ngsutil mb -p PROJECT_NAME -l LOCATION_NAME gs://BUCKET_NAME\n```Make the following replacements:- : the name of the Google Cloud project that contains the Cloud Storage bucket you're creating.\n- : the location of the bucket where you want to store the files you're importing. For example,`us-east1`.\n- : the name of the bucket, subject to [naming requirements](/storage/docs/buckets#naming) . For example,`my-bucket`.\n- Upload the files to your bucket. **Note: ** Make sure that the files that you're uploading are in the `pg_dump` directory format. For more information, see [Export data from multiple files in parallel](#export-data-multiple-files-parallel) .For help with uploading files to buckets, see [Upload objects from files](/storage/docs/uploading-objects) .\n- Provide your instance with the`storage.objectAdmin` [IAM role](/storage/docs/access-control/iam-roles) for your bucket. For help with setting IAM permissions, see [Use IAM permissions](/storage/docs/access-control/using-iam-permissions) .\n- Import data from multiple files in parallel into Cloud SQL:Before using any of the request data, make the following replacements:- : the name of the Google Cloud project that contains the Cloud Storage bucket you created.\n- : the name of the Cloud SQL instance to which you're importing files in parallel.\n- : the name of the Cloud Storage bucket.\n- : the path to the bucket where the import files are stored.\n- : the folder where the import files are stored.\n- : the name of the database inside of the Cloud SQL instance from which the import is made. You must specify only one database.\n- : the number of threads that Cloud SQL uses to import files in parallel. For example, if you want to import three files at a time in parallel, then specify`3`as the value for this parameter.\n **Note: ** The`offload`parameter enables you to use serverless imports for up to 2 threads. The`parallel`parameter enables you to import multiple files in parallel. To use these features, set the values of these parameters to. Otherwise, set their values to.HTTP method and URL:\n```\nPOST https://sqladmin.googleapis.com/v1/projects/PROJECT_NAME/instances/INSTANCE_NAME/import\n```\nRequest JSON body:\n```\n{\n \"importContext\":\n {\n  \"fileType\": \"SQL\",\n  \"uri\": \"gs://BUCKET_NAME/BUCKET_PATH/FOLDER_NAME\",\n  \"databases\": [\"DATABASE_NAME\"],\n  \"offload\": [TRUE|FALSE],\n  \"sqlImportOptions\": {\n  \"parallel\": [TRUE|FALSE],\n  \"threads\": [THREAD_NUMBER]\n  }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:To use a different user for the import, specify the `importContext.importUser` property.For the complete list of parameters for the request, see the [Cloud SQL Admin API](/sql/docs/postgres/admin-api/rest/v1beta4/instances/import) page.\n- If you don't need the IAM permissions that you set in [Required roles and permissions for importing to Cloud SQL for PostgreSQL](#required-roles-and-permissions-import) , then use` [gsutil iam](/storage/docs/gsutil/commands/iam) `to remove them.\nTo import data from multiple files in parallel into Cloud SQL, complete the following steps:- Create a Cloud Storage bucket:```\ngsutil mb -p PROJECT_NAME -l LOCATION_NAME gs://BUCKET_NAME\n```Make the following replacements:- : the name of the Google Cloud project that contains the Cloud Storage bucket you're creating.\n- : the location of the bucket where you want to store the files you're importing. For example,`us-east1`.\n- : the name of the bucket, subject to [naming requirements](/storage/docs/buckets#naming) . For example,`my-bucket`.\n- Upload the files to your bucket. **Note: ** Make sure that the files that you're uploading are in the `pg_dump` directory format. For more information, see [Export data from multiple files in parallel](#export-data-multiple-files-parallel) .For help with uploading files to buckets, see [Upload objects from files](/storage/docs/uploading-objects) .\n- Provide your instance with the`storage.objectAdmin` [IAM role](/storage/docs/access-control/iam-roles) for your bucket. For help with setting IAM permissions, see [Use IAM permissions](/storage/docs/access-control/using-iam-permissions) .\n- Import data from multiple files in parallel into Cloud SQL:Before using any of the request data, make the following replacements:- : the name of the Google Cloud project that contains the Cloud Storage bucket you created.\n- : the name of the Cloud SQL instance from which you're importing files in parallel.\n- : the name of the Cloud Storage bucket.\n- : the path to the bucket where the import files are stored.\n- : the folder where the import files are stored.\n- : the name of the database inside of the Cloud SQL instance from which the import is made. You must specify only one database.\n- : the number of threads that Cloud SQL uses to import files in parallel. For example, if you want to import three files at a time in parallel, then specify`3`as the value for this parameter.\n **Note: ** The`offload`parameter enables you to use serverless imports for up to 2 threads. The`parallel`parameter enables you to import multiple files in parallel. To use these features, set the values of these parameters to. Otherwise, set their values to.HTTP method and URL:\n```\nPOST https://sqladmin.googleapis.com/sql/v1beta4/projects/PROJECT_NAME/instances/INSTANCE_NAME/import\n```\nRequest JSON body:\n```\n{\n \"importContext\":\n {\n  \"fileType\": \"SQL\",\n  \"uri\": \"gs://BUCKET_NAME/BUCKET_PATH/FOLDER_NAME\",\n  \"databases\": [\"DATABASE_NAME\"],\n  \"offload\": [TRUE|FALSE],\n  \"sqlImportOptions\": {\n  \"parallel\": [TRUE|FALSE],\n  \"threads\": [THREAD_NUMBER]\n  }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:To use a different user for the import, specify the `importContext.importUser` property.For the complete list of parameters for the request, see the [Cloud SQL Admin API](/sql/docs/postgres/admin-api/rest/v1beta4/instances/import) page.\n- If you don't need the IAM permissions that you set in [Required roles and permissions for importing to Cloud SQL for PostgreSQL](#required-roles-and-permissions-import) , then use` [gsutil iam](/storage/docs/gsutil/commands/iam) `to remove them.## Limitations\n- If you specify too many threads when you import or export data from multiple files in parallel, then you might use more memory than your Cloud SQL instance has. If this occurs, then an internal error message appears. Check the memory usage of your instance and increase the instance's size, as needed. For more information, see [About instance settings](/sql/docs/postgres/instance-settings) .\n- When performing an export, commas in database names or table names in the`databases`or`tables`fields aren't supported.\n- Make sure that you have enough disk space for the initial dump file download. Otherwise, a`no space left on disk`error appears.\n- If your instance has only one virtual CPU (vCPU), then you can't import or export multiple files in parallel. The number of vCPUs for your instance can't be smaller than the number of threads that you're using for the import or export operation, and the number of threads must be at least two.\n- The`pg_dump`utility can't chunk any tables that you export. Therefore, if you have one very large table, then it can become a bottleneck for the speed of the export operation.## What's next\n- Learn how to [check the status of import and export operations](/sql/docs/postgres/import-export/checking-status-import-export) .\n- Learn how to [cancel the import and export of data](/sql/docs/postgres/import-export/cancel-import-export) .\n- Learn about [best practices for importing and exporting data](/sql/docs/postgres/import-export) .\n- Learn about [known issues for imports and exports](/sql/docs/postgres/known-issues#import-export) .", "guide": "Cloud SQL"}