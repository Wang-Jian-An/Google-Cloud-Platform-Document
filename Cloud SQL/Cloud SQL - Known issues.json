{"title": "Cloud SQL - Known issues", "url": "https://cloud.google.com/sql/docs/postgres/known-issues", "abstract": "# Cloud SQL - Known issues\nThis page lists known issues with Cloud SQL for PostgreSQL, along with ways you can avoid or recover from these issues.\n[Diagnosing Issues](/sql/docs/postgres/diagnose-issues)\n#", "content": "## Instance connection issues\n- Expired SSL/TLS certificatesIf your instance is configured to use SSL, go to the [Cloud SQL Instances page](https://console.cloud.google.com/sql/instances) in the Google Cloud console and open the instance. Open its **Connections** page, select the **Security** tab and make sure that your server certificate is valid. If it has expired, you must add a new certificate and rotate to it.\n- Cloud SQL Auth Proxy versionIf you are connecting using the Cloud SQL Auth Proxy, make sure you are using the most recent version. For more information, see [Keeping the Cloud SQL Auth Proxy up to date](/sql/docs/postgres/sql-proxy#keep-current) .\n- Not authorized to connectIf you try to connect to an instance that does not exist in that project, the error message only says that you are not authorized to access that instance.\n- Can't create a Cloud SQL instanceIf you see the `Failed to create subnetwork. Router status is temporarily unavailable. Please try again later. Help Token: [token-ID]` error message, try to create the Cloud SQL instance again.\n- The following only works with the default user ('postgres'): `gcloud sql connect --user`If you try to connect using this command with any other user, the error message says . The workaround is to connect using the default user ('postgres'), then use the `\"\\c\"` psql command to reconnect as the different user.\n- PostgreSQL connections hang when IAM db proxy authentication is enabled.When the [Cloud SQL Auth Proxy is started using TCP sockets](/sql/docs/postgres/connect-auth-proxy#start-proxy) and with the `-enable_iam_login` flag, then a PostgreSQL client hangs during TCP connection. One workaround is to use `sslmode=disable` in the PostgreSQL connection string. For example:```\npsql \"host=127.0.0.1 dbname=postgres user=me@google.com sslmode=disable\"\n```Another workaround is to [start the Cloud SQL Auth Proxy using Unix sockets](/sql/docs/postgres/connect-auth-proxy#start-proxy) . This turns off PostgreSQL SSL encryption and lets the Cloud SQL Auth Proxy do the SSL encryption instead.\n### Administrative issues\n- Only one long-running Cloud SQL import or export operation can run at a time on an instance. When you start an operation, make sure you don't need to perform other operations on the instance. Also, when you start the operation, you can [cancel it](/sql/docs/postgres/import-export/cancel-import-export) .PostgreSQL imports data in a single transaction. Therefore, if you cancel the import operation, then Cloud SQL doesn't persist data from the import.\n### Issues with importing and exporting data- For PostgreSQL versions 15 and later, if the target database is created from `template0` , then importing data might fail and you might see a `permission denied for schema public` error message. To resolve this issue, provide public schema privileges to the `cloudsqlsuperuser` user by running the `GRANT ALL ON SCHEMA public TO cloudsqlsuperuser` SQL command.\n- Exporting many large objects cause instance to become unresponsiveIf your database contains many large objects (blobs), exporting the database can consume so much memory that the instance becomes unresponsive. This can happen even if the blobs are empty.\n- Cloud SQL doesn't support customized tablespaces but it does support data migration from customized tablespaces to the default tablespace, `pg_default` , in destination instance. For example, if you own a tablespace named `dbspace` is located at `/home/data` , after migration, all the data inside `dbspace` is migrated to the `pg_default` . But Cloud SQL will not create a tablespace named \"dbspace\" on its disk.\n- If you're trying to import and export data from a large database (for example, a database that has 500 GB of data or greater), then the import and export operations might take a long time to complete. In addition, other operations (for example, the backup operation) aren't available for you to perform while the import or export is occurring. A potential option to improve the performance of the import and export process is to [restore a previous backup](/sql/docs/postgres/backup-recovery/restoring#projectid) using `gcloud` or the API.\n- Cloud Storage supports a [maximum single-object size up five terabytes](/storage-transfer/docs/known-limitations-transfer#object-limit) . If you have databases larger than 5TB, the export operation to Cloud Storage fails. In this case, you need to break down your export files into smaller segments.### Transaction logs and disk growth\nLogs are purged once daily, not continuously. When the number of days of log retention is configured to be the same as the number of backups, a day of logging might be lost, depending on when the backup occurs. For example, setting log retention to seven days and backup retention to seven backups means that between six and seven days of logs will be retained.We recommend setting the number of backups to at least one more than the days of log retention to guarantee a minimum of specified days of log retention. **Note:** Replica instances see a storage increase when replication is suspended and then resumed later. The increase is caused when the primary instance sends the replica the transaction logs for the period of time when replication was suspended. The transaction logs updates the replica to the current state of the primary instance.\n### Issues related to Cloud Monitoring or Cloud Logging\nInstances with the following region names are displayed incorrectly in certain contexts, as follows:- `us-central1`is displayed as`us-central`\n- `europe-west1`is displayed as`europe`\n- `asia-east1`is displayed as`asia`\nThis issue occurs in the following contexts:- Alerting in Cloud Monitoring\n- Metrics Explorer\n- Cloud Logging\nYou can mitigate the issue for Alerting in Cloud Monitoring, and for Metrics Explorer, by using [Resource metadata labels](https://cloud.google.com/monitoring/api/v3/metric-model#meta-labels) . Use the system metadata label `region` instead of the [cloudsql_database](https://cloud.google.com/monitoring/api/resources#tag_cloudsql_database) monitored resource label `region` .", "guide": "Cloud SQL"}