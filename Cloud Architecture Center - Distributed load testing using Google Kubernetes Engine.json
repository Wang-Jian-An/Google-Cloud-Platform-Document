{"title": "Cloud Architecture Center - Distributed load testing using Google Kubernetes Engine", "url": "https://cloud.google.com/compute/docs/instances", "abstract": "# Cloud Architecture Center - Distributed load testing using Google Kubernetes Engine\nLast reviewed 2022-04-22 UTC\nThis tutorial explains how to use [Google Kubernetes Engine (GKE) ](/kubernetes-engine/docs/concepts/kubernetes-engine-overview) to deploy a distributed load testing framework that uses multiple containers to create traffic for a simple REST-based API. This tutorial load-tests a web application deployed to App Engine that exposes REST-style endpoints to respond to incoming HTTP POST requests.\nYou can use this same pattern to create load testing frameworks for a variety of scenarios and applications, such as messaging systems, data stream management systems, and database systems.", "content": "## Objectives\n- Define environment variables to control deployment configuration.\n- Create a GKE cluster.\n- Perform load testing.\n- Optionally scale up the number of users or extend the pattern to other use cases.\n## Costs\nIn this document, you use the following billable components of Google Cloud:- App Engine\n- Artifact Registry\n- Cloud Build\n- Cloud Storage\n- Google Kubernetes Engine\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . ## Before you begin- When you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .\n- Grant roles to your Google Account. Run the following command once for each of the following   IAM roles: `roles/serviceusage.serviceUsageAdmin, roles/container.admin, roles/appengine.appAdmin, roles/appengine.appCreator, roles/artifactregistry.admin, roles/resourcemanager.projectIamAdmin, roles/compute.instanceAdmin.v1, roles/iam.serviceAccountUser, roles/cloudbuild.builds.builder, roles/iam.serviceAccountAdmin` ```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS\" --role=ROLE\n```- Replace``with your project ID.\n- Replace``with your email address.\n- Replace``with each individual role.## Example workloadThe following diagram shows an example workload where requests go from client to application.To model this interaction, you can use [Locust](https://locust.io/) , a distributed, Python-based load testing tool that can distribute requests across multiple target paths. For example, Locust can distribute requests to the `/login` and `/metrics` target paths. The workload is modeled as a set of [tasks](https://docs.locust.io/en/latest/writing-a-locustfile.html) in Locust.## ArchitectureThis architecture involves two main components:- The Locust Docker container image.\n- The container orchestration and management mechanism.\nThe Locust Docker container image contains the Locust software. The Dockerfile, which you get when you clone the [GitHub repository](https://github.com/GoogleCloudPlatform/distributed-load-testing-using-kubernetes) that accompanies this tutorial, uses a base Python image and includes scripts to start the Locust service and execute the tasks. To approximate real-world clients, each Locust task is weighted. For example, registration happens once per thousand total client requests.\nGKE provides container orchestration and management. With GKE, you can specify the number of container nodes that provide the foundation for your load testing framework. You can also organize your load testing workers into Pods, and specify how many Pods you want GKE to keep running.\nTo deploy the load testing tasks, you do the following:- Deploy a load testing master.\n- Deploy a group of load testing workers. With these load testing workers, you can create a substantial amount of traffic for testing purposes.\nThe following diagram shows the architecture that demonstrates load testing using a sample application. The master Pod serves the web interface used to operate and monitor load testing. The worker Pods generate the REST request traffic for the application undergoing test, and send metrics to the master. **Note:** Generating excessive amounts of traffic to external systems can resemble a denial-of-service attack. Be sure to review the [Google Cloud Terms of Service](/terms) and the [Google Cloud Acceptable Use Policy](/terms/aup) .\n### About the load testing masterThe Locust master is the entry point for executing the load testing tasks. The Locust master configuration specifies several elements, including the default ports used by the container:- `8089`for the web interface\n- `5557`and`5558`for communicating with workers\nThis information is later used to configure the Locust workers.\nYou deploy a Service to ensure that the necessary ports are accessible to other Pods within the cluster through `hostname:port` . These ports are also referenceable through a descriptive port name.\nThis Service allows the Locust workers to easily discover and reliably communicate with the master, even if the master fails and is replaced with a new Pod by the Deployment.\nA second Service is deployed with the necessary annotation to create an internal passthrough Network Load Balancer that makes the Locust web application Service accessible to clients outside of your cluster that use the same VPC network and are located in the same Google Cloud region as your cluster.\nAfter you deploy the Locust master, you can open the web interface using the private IP address provisioned by the internal passthrough Network Load Balancer. After you deploy the Locust workers, you can start the simulation and look at aggregate statistics through the Locust web interface.\n### About the load testing workersThe Locust workers execute the load testing tasks. You use a single Deployment to create multiple Pods. The Pods are spread out across the Kubernetes cluster. Each Pod uses environment variables to control configuration information, such as the hostname of the system under test and the hostname of the Locust master.\nThe following diagram shows the relationship between the Locust master and the Locust workers.## Initialize common variablesYou must define several variables that control where elements of the infrastructure are deployed.- Open Cloud Shell: [Open Cloud Shell](https://console.cloud.google.com/cloudshell/) You run all the terminal commands in this tutorial from Cloud Shell.\n- Set the environment variables that require customization:```\nexport GKE_CLUSTER=GKE_CLUSTER\nexport AR_REPO=AR_REPO\nexport REGION=REGION\nexport ZONE=ZONE\nexport SAMPLE_APP_LOCATION=SAMPLE_APP_LOCATION\n```Replace the following:- ``: the name of your GKE cluster.\n- ``: the name of your Artifact Registry repository\n- ``: the region where your GKE cluster and Artifact Registry repository will be created\n- ``: the zone in your region where your Compute Engine instance will be created\n- ``: the [(regional) location](/appengine/docs/locations) where your sample App Engine application will be deployed\nThe commands should look similar to the following example:```\nexport GKE_CLUSTER=gke-lt-cluster\nexport AR_REPO=dist-lt-repo\nexport REGION=us-central1\nexport ZONE=us-central1-b\nexport SAMPLE_APP_LOCATION=us-central\n```\n- Set the following additional environment variables:```\nexport GKE_NODE_TYPE=e2-standard-4\nexport GKE_SCOPE=\"https://www.googleapis.com/auth/cloud-platform\"\nexport PROJECT=$(gcloud config get-value project)\nexport SAMPLE_APP_TARGET=${PROJECT}.appspot.com\n```\n- Set the default zone so you do not have to specify these values in subsequent commands:```\ngcloud config set compute/zone ${ZONE}\n```\n## Create a GKE cluster\n- Create a service account with the minimum permissions required by the cluster:```\ngcloud iam service-accounts create dist-lt-svc-acc\ngcloud projects add-iam-policy-binding ${PROJECT} --member=serviceAccount:dist-lt-svc-acc@${PROJECT}.iam.gserviceaccount.com --role=roles/artifactregistry.reader\ngcloud projects add-iam-policy-binding ${PROJECT} --member=serviceAccount:dist-lt-svc-acc@${PROJECT}.iam.gserviceaccount.com --role=roles/container.nodeServiceAccount\n```\n- Create the GKE cluster:```\ngcloud container clusters create ${GKE_CLUSTER} \\\n--service-account=dist-lt-svc-acc@${PROJECT}.iam.gserviceaccount.com \\\n--region ${REGION} \\\n--machine-type ${GKE_NODE_TYPE} \\\n--enable-autoscaling \\\n--num-nodes 3 \\\n--min-nodes 3 \\\n--max-nodes 10 \\\n--scopes \"${GKE_SCOPE}\"\n```\n- Connect to the GKE cluster:```\ngcloud container clusters get-credentials ${GKE_CLUSTER} \\\n --region ${REGION} \\\n --project ${PROJECT}\n```\n## Set up the environment\n- Clone the sample repository from GitHub:```\ngit clone https://github.com/GoogleCloudPlatform/distributed-load-testing-using-kubernetes\n```\n- Change your working directory to the cloned repository:```\ncd distributed-load-testing-using-kubernetes\n```\n## Build the container image\n- Create an Artifact Registry repository:```\ngcloud artifacts repositories create ${AR_REPO} \\\u00a0 \u00a0 --repository-format=docker \u00a0\\\u00a0 \u00a0 --location=${REGION} \\\u00a0 \u00a0 --description=\"Distributed load testing with GKE and Locust\"\n```\n- Build the container image and store it in your Artifact Registry repository:```\nexport LOCUST_IMAGE_NAME=locust-tasks\nexport LOCUST_IMAGE_TAG=latest\ngcloud builds submit \\\n --tag ${REGION}-docker.pkg.dev/${PROJECT}/${AR_REPO}/${LOCUST_IMAGE_NAME}:${LOCUST_IMAGE_TAG} \\\n docker-image\n```The accompanying Locust Docker image embeds a test task that calls the `/login` and `/metrics` endpoints in the sample application. In this example test task set, the respective ratio of requests submitted to these two endpoints will be `1` to `999` . [  docker-image/locust-tasks/tasks.py ](https://github.com/GoogleCloudPlatform/distributed-load-testing-using-kubernetes/blob/HEAD/docker-image/locust-tasks/tasks.py) [View on GitHub](https://github.com/GoogleCloudPlatform/distributed-load-testing-using-kubernetes/blob/HEAD/docker-image/locust-tasks/tasks.py) ```\nclass MetricsTaskSet(TaskSet):\u00a0 \u00a0 _deviceid = None\u00a0 \u00a0 def on_start(self):\u00a0 \u00a0 \u00a0 \u00a0 self._deviceid = str(uuid.uuid4())\u00a0 \u00a0 @task(1)\u00a0 \u00a0 def login(self):\u00a0 \u00a0 \u00a0 \u00a0 self.client.post(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 '/login', {\"deviceid\": self._deviceid})\u00a0 \u00a0 @task(999)\u00a0 \u00a0 def post_metrics(self):\u00a0 \u00a0 \u00a0 \u00a0 self.client.post(\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"/metrics\", {\"deviceid\": self._deviceid, \"timestamp\": datetime.now()})class MetricsLocust(FastHttpUser):\u00a0 \u00a0 tasks = {MetricsTaskSet}\n```\n- Verify that the Docker image is in your Artifact Registry repository:```\ngcloud artifacts docker images list ${REGION}-docker.pkg.dev/${PROJECT}/${AR_REPO} | \\\n grep ${LOCUST_IMAGE_NAME}\n```The output is similar to the following:```\nListing items under project PROJECT, location REGION, repository AR_REPO\nREGION-docker.pkg.dev/PROJECT/AR_REPO/locust-tasks sha256:796d4be067eae7c82d41824791289045789182958913e57c0ef40e8d5ddcf283 2022-04-13T01:55:02 2022-04-13T01:55:02\n```\n## Deploy the sample application\n- Create and deploy the sample-webapp as App Engine:```\ngcloud app create --region=${SAMPLE_APP_LOCATION}\ngcloud app deploy sample-webapp/app.yaml \\\n--project=${PROJECT}\n```\n- When prompted, type `y` to proceed with deployment.The output is similar to the following:```\nFile upload done.\nUpdating service [default]...done.\nSetting traffic split for service [default]...done.\nDeployed service [default] to [https://PROJECT.appspot.com]\n```The sample App Engine application implements `/login` and `/metrics` endpoints: [  sample-webapp/main.py ](https://github.com/GoogleCloudPlatform/distributed-load-testing-using-kubernetes/blob/HEAD/sample-webapp/main.py) [View on GitHub](https://github.com/GoogleCloudPlatform/distributed-load-testing-using-kubernetes/blob/HEAD/sample-webapp/main.py) ```\n@app.route('/login', \u00a0methods=['GET', 'POST'])def login():\u00a0 \u00a0 deviceid = request.values.get('deviceid')\u00a0 \u00a0 return '/login - device: {}\\n'.format(deviceid)@app.route('/metrics', \u00a0methods=['GET', 'POST'])def metrics():\u00a0 \u00a0 deviceid = request.values.get('deviceid')\u00a0 \u00a0 timestamp = request.values.get('timestamp')\u00a0 \u00a0 return '/metrics - device: {}, timestamp: {}\\n'.format(deviceid, timestamp)\n```\n## Deploy the Locust master and worker Pods\n- Substitute the environment variable values for target host, project, and image parameters in the `locust-master-controller.yaml` and `locust-worker-controller.yaml` files, and create the Locust master and worker Deployments:```\nenvsubst < kubernetes-config/locust-master-controller.yaml.tpl | kubectl apply -f envsubst < kubernetes-config/locust-worker-controller.yaml.tpl | kubectl apply -f envsubst < kubernetes-config/locust-master-service.yaml.tpl | kubectl apply -f \n```\n- Verify the Locust Deployments:```\nkubectl get pods -o wide\n```The output looks something like the following:```\nNAME        READY STATUS RESTARTS AGE IP   NODE\nlocust-master-87f8ffd56-pxmsk 1/1  Running 0   1m 10.32.2.6 gke-gke-load-test-default-pool-96a3f394\nlocust-worker-58879b475c-279q9 1/1  Running 0   1m 10.32.1.5 gke-gke-load-test-default-pool-96a3f394\nlocust-worker-58879b475c-9frbw 1/1  Running 0   1m 10.32.2.8 gke-gke-load-test-default-pool-96a3f394\nlocust-worker-58879b475c-dppmz 1/1  Running 0   1m 10.32.2.7 gke-gke-load-test-default-pool-96a3f394\nlocust-worker-58879b475c-g8tzf 1/1  Running 0   1m 10.32.0.11 gke-gke-load-test-default-pool-96a3f394\nlocust-worker-58879b475c-qcscq 1/1  Running 0   1m 10.32.1.4 gke-gke-load-test-default-pool-96a3f394\n```\n- Verify the Services:```\nkubectl get services\n```The output looks something like the following:```\nNAME    TYPE   CLUSTER-IP  EXTERNAL-IP PORT(S)    AGE\nkubernetes   ClusterIP  10.87.240.1  <none>  443/TCP    12m\nlocust-master  ClusterIP  10.87.245.22 <none>  5557/TCP,5558/TCP 1m\nlocust-master-web LoadBalancer 10.87.246.225 <pending>  8089:31454/TCP  1m\n```\n- Run a watch loop while the internal passthrough Network Load Balancer's private IP address (GKE external IP address) is provisioned for the Locust master web application Service:```\nkubectl get svc locust-master-web --watch\n```\n- Press `Ctrl+C` to exit the watch loop once an **EXTERNAL-IP** address is provisioned.\n## Connect to Locust web front endYou use the Locust master web interface to execute the load testing tasks against the system under test.- Obtain the internal load balancer IP address of the web host service:```\nexport INTERNAL_LB_IP=$(kubectl get svc locust-master-web \\\n        -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\") && \\\n        echo $INTERNAL_LB_IP\n```\n- Depending on your network configuration, there are two ways that you can connect to the Locust web application through the provisioned IP address:- **Network routing.** If your network is configured to allow routing from your workstation to your project VPC network, you can directly access the internal passthrough Network Load Balancer IP address from your workstation.\n- **Proxy & SSH tunnel.** If there is not a network route between your workstation and your VPC network, you can route traffic to the internal passthrough Network Load Balancer's IP address by creating a Compute Engine instance with an `nginx` proxy and an SSH tunnel between your workstation and the instance.\nIf there is a route for network traffic between your workstation and your Google Cloud project VPC network, open your browser and then open the Locust master web interface. Substitute [INTERNAL_LB_IP] in the following URL with the IP address you observed in the previous step: `http://[INTERNAL_LB_IP]:8089` .- Set an environment variable with the name of the instance.```\nexport PROXY_VM=locust-nginx-proxy\n```\n- Start an instance with a `ngnix` docker container configured to proxy the Locust web application port `8089` on the internal passthrough Network Load Balancer:```\ngcloud compute instances create-with-container ${PROXY_VM} \\\n --zone ${ZONE} \\\n --container-image gcr.io/cloud-marketplace/google/nginx1:latest \\\n --container-mount-host-path=host-path=/tmp/server.conf,mount-path=/etc/nginx/conf.d/default.conf \\\n --metadata=startup-script=\"#! /bin/bash\n  cat <<EOF > /tmp/server.conf\n  server {\n   listen 8089;\n   location / {\n    proxy_pass http://${INTERNAL_LB_IP}:8089;\n   }\n  }\nEOF\"\n```\n- Open an SSH tunnel from Cloud Shell to the proxy instance:```\ngcloud compute ssh --zone ${ZONE} ${PROXY_VM} \\\n     -- -N -L 8089:localhost:8089\n```\n- Click the **Web Preview** icon ( ), and select **Change Port** from the options listed.\n- On the **Change Preview Port** dialog, enter **8089** in the **Port Number** field, and select **Change and Preview** .In a moment, a browser tab will open with the Locust web interface.\n## Run a basic load test on your sample application\n- After you open the Locust frontend in your browser, you see a dialog that can be used to start a new load test. \n- Specify the total **Number of users** as `10` and the **Spawn rate** as `5` users per second.\n- Click **Start swarming** to begin the simulation.After requests start swarming, statistics begin to aggregate for simulation metrics, such as the number of requests and requests per second, as shown in the following image:\n- View the deployed service and other metrics from the [Google Cloud console](/console/appengine) . **Note:** After the swarm test, it might take the App Engine dashboard several minutes to show the metrics.\n- When you have observed the behavior of the application under test, click **Stop** to terminate the test.\n## Scale up the number of users (optional)If you want to test increased load on the application, you can add simulated users. Before you can add simulated users, you must ensure that there are enough resources to support the increase in load. With Google Cloud, you can add Locust worker Pods to the Deployment without redeploying the existing Pods, as long as you have the underlying VM resources to support an increased number of Pods. The initial GKE cluster starts with 3 nodes and can auto-scale up to 10 nodes.- Scale the pool of Locust worker Pods to 20.```\nkubectl scale deployment/locust-worker --replicas=20\n```It takes a few minutes to deploy and start the new Pods.\nIf you see a [Pod Unschedulable](/kubernetes-engine/docs/troubleshooting#PodUnschedulable) error, you must add more nodes to the cluster. For details, see [resizing a GKE cluster](/kubernetes-engine/docs/how-to/resizing-a-cluster) .\nAfter the Pods start, return to the Locust master web interface and restart load testing.## Extend the patternTo extend this pattern, you can create new Locust tasks or even switch to a different load testing framework.\nYou can customize the metrics you collect. For example, you might want to measure the requests per second, or monitor the response latency as load increases, or check the response failure rates and types of errors.\nFor information, see the [Cloud Monitoring](/monitoring/docs) documentation.## Clean upAfter you've finished the tutorial, you can clean up the resources you created so you won't be billed for them in the future.\n### Delete the project\nThe easiest way to eliminate billing is to delete the project that you created for the tutorial.\nTo delete the project:\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.### Delete the GKE clusterIf you don't want to delete the whole project, run the following command to delete the GKE cluster:\n```\n gcloud container clusters delete ${GKE_CLUSTER} --region ${REGION}\n \n```\n## What's next\n- [Building Scalable and Resilient Web Applications](/solutions/scalable-and-resilient-apps) .\n- Review [GKE](/kubernetes-engine) documentation in more detail.\n- Read through other [tutorials on GKE](/kubernetes-engine/docs/tutorials) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}