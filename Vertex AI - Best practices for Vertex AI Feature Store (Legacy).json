{"title": "Vertex AI - Best practices for Vertex AI Feature Store (Legacy)", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Best practices for Vertex AI Feature Store (Legacy)\nThe following best practices will help you plan and use Vertex AI Feature Store (Legacy) in various scenarios. This guide is not intended to be exhaustive..\n", "content": "## Model features that jointly describe multiple entities\nSome features might apply to multiple entity types. For example, you might have a calculated value that records clicks per product by user. This feature jointly describes product-user pairs.\nThe best practice, in this case, is to create a separate entity type to group shared features. You can create an entity type, such as `product-user` , to contain shared features.\nFor the entity IDs, concatenate the IDs of the individual entities, such as the entity IDs of the individual product and user. The only requirement is that the IDs must be strings. These combined entity types are referred to as .\nFor more information, see [creating an entitytype](/vertex-ai/docs/featurestore/managing-entity-types#create) .\n## Use IAM policies to control access across multiple teams\nUse IAM roles and policies to set different levels of access to different groups of users. For example, ML researchers, data scientists, DevOps, and site reliability engineers all require access to the same featurestore, but their level of access can differ. For example, DevOps users might require permissions to manage a featurestore, but they don't require access to the contents of the featurestore.\nYou can also restrict access to a particular featurestore or entity type by using [resource-level IAMpolicies](/vertex-ai/docs/featurestore/resource-policy) .\nAs an example, imagine that your organization includes the following personas. Because each persona requires a different level of access, each persona is assigned a different [predefined IAMrole](/vertex-ai/docs/general/access-control#predefined-roles) . You can also create and use your own custom roles.\n| Persona           | Description                                                                             | Predefined role                     |\n|:---------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------|\n| ML researcher or business analyst     | Users who only view data on specific entity types                                                                    | roles/aiplatform.featurestoreDataViewer (can be granted at the project or resource level)  |\n| Data scientists or data engineers     | Users who work with specific entity type resources. For the resources they own, they can delegate access to other principals.                                                 | roles/aiplatform.entityTypeOwner (can be granted at the project or resource level)    |\n| IT or DevOps          | Users who must maintain and tune the performance of specific featurestores but don't require access to the data.                                                    | roles/aiplatform.featurestoreInstanceCreator (can be granted at the project or resource level) |\n| Automated data import pipeline      | Applications that write data to specific entity types.                                                                   | roles/aiplatform.featurestoreDataWriter (can be granted at the project or resource level)  |\n| Site reliability engineer       | Users who manage particular featurestores or all featurestores in a project                                                             | roles/aiplatform.featurestoreAdmin (can be granted at the project or resource level)   |\n| Global (any Vertex AI Feature Store (Legacy) user) | Allow users to view and search for existing features. If they find a feature they want to work with, they can request access from the feature owners. For Google Cloud console users, this role is also required to view the Vertex AI Feature Store (Legacy) landing page, import jobs page, and batch serving jobs page. | Grant the roles/aiplatform.featurestoreResourceViewer role at the project level.    |\n## Monitor and tune resources accordingly to optimize batch import\nBatch import jobs require workers to process and write data, which can increase the CPU utilization of your featurestore and affect online serving performance. If preserving online serving performance is a priority, start with one worker for every ten online serving nodes. During import, monitor the CPU usage of the online storage. If CPU usage is lower than expected, increase the number of workers for future batch import jobs to increase throughput. If CPU usage is higher than expected, increase the number of online serving nodes to increase CPU capacity or lower the batch import worker count, both of which can lower CPU usage.\nIf you do increase the number of online serving nodes, note that Vertex AI Feature Store (Legacy) takes roughly 15 minutes to reach optimal performance after you make the update.\nFor more information, see [update a featurestore](/vertex-ai/docs/featurestore/managing-featurestores#update) and [batch import feature values](/vertex-ai/docs/featurestore/ingesting-batch) .\nFor more information about featurestore monitoring, see [Cloud Monitoringmetrics](/vertex-ai/docs/general/monitoring-metrics) .\n## Use the disableOnlineServing field when backfilling historical data\nBackfilling is the process of importing historical feature values and don't impact the most recent feature values. In this case, you can disable online serving, which skips any changes to the [onlinestore](/vertex-ai/docs/featurestore/managing-featurestores#storage) . For more information, see [Backfill historical data](/vertex-ai/docs/featurestore/ingesting-batch#backfill) .\n## Use autoscaling to reduce costs during load fluctuations\nIf you use Vertex AI Feature Store (Legacy) extensively and encounter frequent load fluctuations in your traffic patterns, use autoscaling to optimize costs. Autoscaling lets Vertex AI Feature Store (Legacy) review traffic patterns and automatically adjust the number of nodes up or down depending on CPU utilization, instead of maintaining a high node count. This option works well for traffic patterns that encounter gradual growth and decline.\n**Caution:** Autoscaling is not effective for managing sudden bursts of traffic. For more information, see [Additional considerations for autoscaling](/vertex-ai/docs/featurestore/managing-featurestores#additional_considerations) . If you are expecting sudden bursts of traffic, set the `minNodeCount` flag to a threshold that is high enough to handle the bursts. For more information, see [Scaling](/vertex-ai/docs/reference/rest/v1/projects.locations.featurestores#scaling) in the [API reference](/vertex-ai/docs/reference/rest/v1/projects.locations.featurestores#scaling) .\nFor more information about autoscaling, see [Scaling options](/vertex-ai/docs/featurestore/managing-featurestores#scaling_options) .\n## Test the performance of online serving nodes for real-time serving\nYou can ensure the performance of your featurestore during real-time online serving by testing the performance of your online serving nodes. You can perform these tests based on various benchmarking parameters, such as QPS, latency, and API. Follow these guidelines to test the performance of online serving nodes:\n- **Run all test clients from the same region, preferably on Compute Engine or Google Kubernetes Engine** : This prevents discrepancies due to network latency resulting from hops across regions.\n- **Use the gRPC API in the SDK** : The gRPC API performs better than the REST API. If you need to use the REST API, enable the HTTP keep-alive option to reuse HTTP connections. Otherwise, each request results in the creation of a new HTTP connection, which increases the latency.\n- **Run longer duration tests** : Run tests with longer duration (15 minutes or more) and a minimum of 5 QPS to calculate more accurate metrics.\n- **Add a \"warm-up\" period** : If you start testing after a period of inactivity, you might observe high latency while connections are reestablished. To account for the initial period of high latency, you can designate this period as a \"warm-up period\", when the initial data reads are ignored. As an alternative, you can send a low but consistent rate of artificial traffic to the featurestore to keep the connection active.\n- **If required, enable autoscaling** : If you anticipate gradual growth and decline in your online traffic, enable autoscaling. If you choose autoscaling, Vertex AI automatically changes the number of online serving nodes based on CPU utilization.\nFor more information about online serving, see [Online serving](/vertex-ai/docs/featurestore/serving-online) . For more information about online serving nodes, see [Online serving nodes](/vertex-ai/docs/featurestore/managing-featurestores#onlineservingnodes) .\n## Specify a start time to optimize offline storage costs during batch serve and batch export\nTo optimize offline storage costs during batch serving and batch export, you can specify the a `startTime` in your `batchReadFeatureValues` or `exportFeatureValues` request. The request runs a query over a subset of the available feature data, based on the specified `startTime` . Otherwise, the request queries the entire available volume of feature data, resulting in high offline storage usage costs.\n## What's next\nLearn Vertex AI Feature Store (Legacy) [best practices for implementing custom-trained ML models on Vertex AI](/architecture/ml-on-gcp-best-practices#use-vertex-feature-store-with-structured-data) .", "guide": "Vertex AI"}