{"title": "Cloud Architecture Center - Import data from Google Cloud into a secured BigQuery data warehouse", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Import data from Google Cloud into a secured BigQuery data warehouse\nLast reviewed 2021-12-16 UTC\nMany organizations deploy data warehouses that store confidential information so that they can analyze the data for a variety of business purposes. This document is intended for data engineers and security administrators who deploy and secure data warehouses using BigQuery. It's part of a security blueprint that's made up of the following:\n- A [GitHub repository](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse) that contains a set of Terraform configurations and scripts. The Terraform configuration sets up an environment in Google Cloud that supports a data warehouse that stores confidential data.\n- A guide to the architecture, design, and security controls that you use this blueprint to implement (this document).\n- A [walkthrough](https://console.cloud.google.com/?walkthrough_id=security--secured-data-warehouse-simple-example) that deploys a sample environment.\nThis document discusses the following:\n- The architecture and Google Cloud services that you can use to help secure a data warehouse in a production environment.\n- Best practices for [data governance](/blog/topics/developers-practitioners/bigquery-admin-reference-guide-data-governance) when creating, deploying, and operating a data warehouse in Google Cloud, including data [de-identification](/dlp/docs/classification-redaction) , differential handling of confidential data, and column-level access controls.\nThis document assumes that you have already configured a foundational set of security controls as described in the [Google Cloud enterprise foundations blueprint](/architecture/security-foundations) . It helps you to layer additional controls onto your existing security controls to help protect confidential data in a data warehouse.\n", "content": "## Data warehouse use cases\nThe blueprint supports the following use cases:\n- Import data from Google Cloud into a secured BigQuery data warehouse (this document)\n- [Import data from anon-premises environment or another cloud into a BigQuery warehouse](/architecture/secured-data-warehouse-blueprint-onprem) ## Overview\nData warehouses such as [BigQuery](/bigquery) let businesses analyze their business data for insights. Analysts access the business data that is stored in data warehouses to create insights. If your data warehouse includes confidential data, you must take measures to preserve the security, confidentiality, integrity, and availability of the business data while it is stored, while it is in transit, or while it is being analyzed. In this blueprint, you do the following:\n- Configure controls that help secure access to confidential data.\n- Configure controls that help secure the data pipeline.\n- Configure an appropriate separation of duties for different personas.\n- Set up templates to find and de-identify confidential data.\n- Set up appropriate security controls and logging to help protect confidential data.\n- Use data classification and policy tags to restrict access to specific columns in the data warehouse.## Architecture\nTo create a confidential data warehouse, you need to categorize data as confidential and non-confidential, and then store the data in separate perimeters. The following image shows how ingested data is categorized, de-identified, and stored. It also shows how you can re-identify confidential data on demand for analysis.\nThe architecture uses a combination of the following Google Cloud services and features:\n- [Identity and Access Management (IAM)](/iam) and [Resource Manager](/resource-manager) restrict access and segment resources. The access controls and resource hierarchy follow the principle of least privilege.\n- [VPC Service Controls](/vpc-service-controls) creates security perimeters that isolate services and resources by setting up authorization, access controls, and [secure data exchange](/vpc-service-controls/docs/secure-data-exchange) . The perimeters are as follows:- A data ingestion perimeter that accepts incoming data (in batch or stream) and de-identifies it. A separate landing zone helps to protect the rest of your workloads from incoming data.\n- A confidential data perimeter that can re-identify the confidential data and store it in a restricted area.\n- A governance perimeter that stores the encryption keys and defines what is considered confidential data.\nThese perimeters are designed to protect incoming content, isolate confidential data by setting up additional access controls and monitoring, and separate your governance from the actual data in the warehouse. Your governance includes key management, data catalog management, and logging.\n- [Cloud Storage](/storage) and [Pub/Sub](/pubsub) receives data as follows:- **Cloud Storage:** receives and stores batch data before de-identification. Cloud Storage uses TLS to encrypt data in transit and encrypts data in storage by default. The encryption key is a [customer-managed encryption key (CMEK)](/kms/docs/cmek) . You can help to secure access to Cloud Storage buckets using security controls such as Identity and Access Management, access control lists (ACLs), and policy documents. For more information about supported access controls, see [Overview ofaccess control](/storage/docs/access-control) .\n- **Pub/Sub:** receives and stores streaming data before de-identification. Pub/Sub uses [authentication](/pubsub/docs/authentication) , [accesscontrols](/pubsub/docs/access-control) , and [message-level encryption](/pubsub/docs/encryption#using-cmek) with a CMEK to protect your data.\n- Two [Dataflow](/dataflow) pipelines de-identify and re-identify confidential data as follows:- The first pipeline de-identifies confidential data using [pseudonymization](/dlp/docs/pseudonymization) .\n- The second pipeline re-identifies confidential data when authorized users require access.\nTo protect data, Dataflow uses a unique service account and encryption key for each pipeline, and access controls. To help secure pipeline execution by moving it to the backend service, Dataflow uses [Streaming Engine](/dataflow/docs/guides/deploying-a-pipeline#streaming-engine) . For more information, see [Dataflowsecurity and permissions](/dataflow/docs/concepts/security-and-permissions) .\n- [Sensitive Data Protection](/dlp) de-identifies confidential data during ingestion.Sensitive Data Protection de-identifies structured and unstructured data based on the infoTypes or records that are detected.\n- [Cloud HSM](/kms/docs/hsm) hosts the key encryption key (KEK). Cloud HSM is a cloud-based Hardware Security Module (HSM) service.\n- [Data Catalog](/data-catalog) automatically categorizes confidential data with metadata, also known as [policy tags](/bigquery/docs/column-level-security-intro) , during ingestion. Data Catalog also uses metadata to manage access to confidential data. For more information, see [Data Catalogoverview](/data-catalog/docs/concepts/overview) . To control access to data within the data warehouse, you apply policy tags to columns that include confidential data.\n- [BigQuery](/bigquery) stores the confidential data in the confidential data perimeter.BigQuery uses various security controls to help protect content, including [accesscontrols](/bigquery/docs/access-control) , [column-levelsecurity](/bigquery/docs/column-level-security-intro) for confidential data, and [data encryption](/bigquery/docs/customer-managed-encryption) .\n- Security Command Center monitors and reviews security findings from across your Google Cloud environment in a central location.## Organization structure\nYou group your organization's resources so that you can manage them and separate your testing environments from your production environment. [Resource Manager](/resource-manager) lets you logically group resources by project, folder, and organization.\nThe following diagram shows you a resource hierarchy with folders that represent different environments such as bootstrap, common, production, non-production (or staging), and development. You deploy most of the projects in the blueprint into the production folder, and the data governance project in the common folder which is used for governance.### Folders\nYou use folders to isolate your production environment and governance services from your non-production and testing environments. The following table describes the folders from the enterprise foundations blueprint that are used by this blueprint.\n| Folder | Description                    |\n|:---------|:-----------------------------------------------------------------------------------------|\n| Prod  | Contains projects that have cloud resources that have been tested and are ready to use. |\n| Common | Contains centralized services for the organization, such as the governance project.  |\nYou can change the names of these folders to align with your organization's folder structure, but we recommend that you maintain a similar structure. For more information, see the [Google Cloud enterprise foundations blueprint](/architecture/security-foundations) .\n### Projects\nYou isolate parts of your environment using projects. The following table describes the projects that are needed within the organization. You create these projects when you run the Terraform code. You can change the names of these projects, but we recommend that you maintain a similar project structure.\n| Project    | Description                      |\n|:----------------------|:------------------------------------------------------------------------------------------------|\n| Data ingestion  | Contains services that are required in order to receive data and de-identify confidential data. |\n| Governance   | Contains services that provide key management, logging, and data cataloging capabilities.  |\n| Non-confidential data | Contains services that are required in order to store data that has been de-identified.   |\n| Confidential data  | Contains services that are required in order to store and re-identify confidential data.  |\nIn addition to these projects, your environment must also include a project that hosts a Dataflow [Flex Template](/dataflow/docs/guides/templates/using-flex-templates) job. The Flex Template job is required for the streaming data pipeline.\n## Mapping roles and groups to projects\nYou must give different user groups in your organization access to the projects that make up the confidential data warehouse. The following sections describe the blueprint recommendations for user groups and role assignments in the projects that you create. You can customize the groups to match your organization's existing structure, but we recommend that you maintain a similar segregation of duties and role assignment.\n### Data analyst group\nData analysts analyze the data in the warehouse. This group requires roles in different projects, as described in the following table.\n| Project mapping  | Roles                                               |\n|:----------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Data ingestion  | roles/dataflow.developer roles/dataflow.viewer roles/logging.viewer Additional role for data analysts that require access to confidential data: roles/datacatalog.categoryFineGrainedReader |\n| Confidential data  | roles/bigquery.dataViewer roles/bigquery.jobUser roles/bigquery.user roles/dataflow.viewer roles/dataflow.developer roles/logging.viewer              |\n| Non-confidential data | roles/bigquery.dataViewer roles/bigquery.jobUser roles/bigquery.user roles/logging.viewer                          |\n### Data engineer group\nData engineers set up and maintain the data pipeline and warehouse. This group requires roles in different projects, as described in the following table.\n| Project mapping  | Roles                                          |\n|:----------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Data ingestion  | roles/cloudbuild.builds.editor roles/cloudkms.viewer roles/composer.user roles/compute.networkUser roles/dataflow.admin roles/logging.viewer        |\n| Confidential data  | roles/bigquery.dataEditor roles/bigquery.jobUser roles/cloudbuild.builds.editor roles/cloudkms.viewer roles/compute.networkUser roles/dataflow.admin roles/logging.viewer |\n| Non-confidential data | roles/bigquery.dataEditor roles/bigquery.jobUser roles/cloudkms.viewer roles/logging.viewer                    |\n### Network administrator group\nNetwork administrators configure the network. Typically, they are members of the networking team.\nNetwork administrators require the following roles at the organization level:\n- [roles/compute.networkAdmin](/compute/docs/access/iam#compute.admin) \n- `roles/logging.viewer`\n### Security administrator group\nSecurity administrators administer security controls such as access, keys, firewall rules, VPC Service Controls, and the [Security Command Center](/security-command-center/docs/concepts-security-command-center-overview) .\nSecurity administrators require the following roles at the organization level:\n- [roles/accesscontextmanager.policyAdmin](/access-context-manager/docs/access-control) \n- [roles/cloudasset.viewer](/asset-inventory/docs/access-control#roles) \n- [roles/cloudkms.admin](/kms/docs/reference/permissions-and-roles#predefined) \n- [roles/compute.securityAdmin](/compute/docs/access/iam#compute.securityAdmin) \n- [roles/datacatalog.admin](/iam/docs/understanding-roles#data-catalog-roles) \n- [roles/dlp.admin](/dlp/docs/iam-roles) \n- [roles/iam.securityAdmin](/iam/docs/understanding-roles#iam-roles) \n- [roles/logging.admin](/logging/docs/access-control#permissions_and_roles) \n- [roles/orgpolicy.policyAdmin](/resource-manager/docs/access-control-org#using_predefined_roles) \n### Security analyst group\nSecurity analysts monitor and respond to security incidents and Sensitive Data Protection findings.\nSecurity analysts require the following roles at the organization level:\n- [roles/accesscontextmanager.policyReader](/access-context-manager/docs/access-control) \n- [roles/datacatalog.viewer](/iam/docs/understanding-roles#data-catalog-roles) \n- `roles/cloudkms.viewer`\n- `roles/logging.viewer`\n- [roles/orgpolicy.policyViewer](/iam/docs/understanding-roles#organization-policy-roles) \n- [roles/securitycenter.adminViewer](/security-command-center/docs/access-control#security-command-center) \n- `roles/securitycenter.findingsEditor`\n- One of the following Security Command Center roles:- `roles/securitycenter.findingsBulkMuteEditor`\n- `roles/securitycenter.findingsMuteSetter`\n- `roles/securitycenter.findingsStateSetter`\n## Understanding the security controls you need\nThis section discusses the security controls within Google Cloud that you use to help to secure your data warehouse. The key security principles to consider are as follows:\n- Secure access by adopting least privilege principles.\n- Secure network connections through segmentation design and policies.\n- Secure the configuration for each of the services.\n- Classify and protect data based on its risk level.\n- Understand the security requirements for the environment that hosts the data warehouse.\n- Configure sufficient monitoring and logging for detection, investigation, and response.\n### Security controls for data ingestion\nTo create your data warehouse, you must transfer data from another Google Cloud source (for example, a data lake). You can use one of the following options to transfer your data into the data warehouse on BigQuery:\n- A batch job that uses Cloud Storage.\n- A streaming job that uses Pub/Sub. To help protect data during ingestion, you can use firewall rules, access policies, and encryption.[Virtual Private Cloud (VPC) firewall rules](/vpc/docs/firewalls) control the flow of data into the perimeters. You create firewall rules that deny all egress, except for specific TCP port 443 connections from the `restricted.googleapis.com` special domain names. The `restricted.googleapis.com` domain has the following benefits:\n- It helps reduce your network attack surface by using Private Google Access when workloads communicate to Google APIs and services.\n- It ensures that you only use services that support VPC Service Controls.\nFor more information, see [ConfiguringPrivate Google Access](/vpc/docs/configure-private-google-access) .\nYou must configure separate subnets for each Dataflow job. Separate subnets ensure that data that is being de-identified is properly separated from data that is being re-identified.\nThe data pipeline requires you to open TCP ports in the firewall, as defined in the [dataflow_firewall.tf](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse/blob/main/modules/dwh-networking/dataflow_firewall.tf) file in the [dwh-networking module](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse/tree/main/modules/dwh-networking) repository. For more information, see [Configuring internet access and firewallrules](/dataflow/docs/guides/routes-firewall) .\nTo deny resources the ability to use external IP addresses, the [compute.vmExternalIpAccess](/resource-manager/docs/organization-policy/org-policy-constraints#constraints-for-specific-services) organization policy is set to deny all.\nAs shown in the [architecture diagram](#architecture) , you place the resources for the confidential data warehouse into separate perimeters. To enable services in different perimeters to share data, you create [perimeter bridges](/vpc-service-controls/docs/share-across-perimeters) . Perimeter bridges let protected services make requests for resources outside of their perimeter. These bridges make the following connections:\n- They connect the data ingestion project to the governance project so that de-identification can take place during ingestion.\n- They connect the non-confidential data project and the confidential data project so that confidential data can be re-identified when a data analyst requests it.\n- They connect the confidential project to the data governance project so that re-identification can take place when a data analyst requests it.\nIn addition to perimeter bridges, you use [egress rules](/vpc-service-controls/docs/ingress-egress-rules) to let resources protected by service perimeters access resources that are outside the perimeter. In this solution, you configure egress rules to obtain the external Dataflow Flex Template jobs that are located in Cloud Storage in an external project. For more information, see [Access a Google Cloud resourceoutside the perimeter](/vpc-service-controls/docs/secure-data-exchange#allow_access_to_a_google_cloud_resource_outside_the_perimeter) .\nTo help ensure that only specific identities (user or service) can access resources and data, you enable IAM groups and roles.\nTo help ensure that only specific sources can access your projects, you enable an [access policy](/access-context-manager/docs/overview) for your Google organization. We recommend that you create an access policy that specifies the allowed IP address range for requests and only allows requests from specific users or service accounts. For more information, see [Access levelattributes](/access-context-manager/docs/access-level-attributes) .\nBoth ingestion options use Cloud HSM to manage the CMEK. You use the CMEK keys to help protect your data during ingestion. Sensitive Data Protection further protects your data by encrypting confidential data, using the detectors that you configure.\nTo ingest data, you use the following encryption keys:\n- A CMEK key for the ingestion process that's also used by the Dataflow pipeline and the Pub/Sub service. The ingestion process is sometimes referred to as an [extract, transform, load (ETL) process](/learn/what-is-etl) .\n- The cryptographic key [wrapped](/kms/docs/key-wrapping) by Cloud HSM for the data de-identification process using Sensitive Data Protection.\n- Two CMEK keys, one for the BigQuery warehouse in the non-confidential data project, and the other for the warehouse in the confidential data project. For more information, see [Keymanagement](/bigquery/docs/customer-managed-encryption#dataset_default_key) .\nYou specify the [CMEK location](/kms/docs/locations) , which determines the geographical location that the key is stored and is made available for access. You must ensure that your CMEK is in the same location as your resources. By default, the CMEK is rotated every 30 days.\nIf your organization's compliance obligations require that you manage your own keys externally from Google Cloud, you can enable [Cloud External Key Manager](/kms/docs/ekm) . If you use external keys, you are responsible for key management activities, including key rotation.\nService accounts are identities that Google Cloud can use to run API requests on your behalf. Service accounts ensure that user identities do not have direct access to services. To permit separation of duties, you create service accounts with different roles for specific purposes. These service accounts are defined in the [data-ingestion module](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse/tree/main/modules/data-ingestion) and the [confidential-data module](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse/tree/main/modules/confidential-data) . The service accounts are as follows:\n- A Dataflow controller service account for the Dataflow pipeline that de-identifies confidential data.\n- A Dataflow controller service account for the Dataflow pipeline that re-identifies confidential data.\n- A Cloud Storage service account to ingest data from a batch file.\n- A Pub/Sub service account to ingest data from a streaming service.\n- A Cloud Scheduler service account to run the batch Dataflow job that creates the Dataflow pipeline.\nThe following table lists the roles that are assigned to each service account:\n| Service Account             | Name      | Project   | Roles                                                   |\n|:-----------------------------------------------------------------|:----------------------------|:------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Dataflow controller This account is used for de-identification. | sa-dataflow-controller  | Data ingestion | roles/pubsub.subscriber roles/bigquery.admin roles/cloudkms.admin roles/cloudkms.cryptoKeyDecrypter roles/dlp.admin roles/storage.admin roles/dataflow.serviceAgent roles/dataflow.worker roles/compute.viewer |\n| Dataflow controller This account is used for re-identification. | sa-dataflow-controller-reid | Confidential data | roles/pubsub.subscriber roles/bigquery.admin roles/cloudkms.admin roles/cloudkms.cryptoKeyDecrypter roles/dlp.admin roles/storage.admin roles/dataflow.serviceAgent roles/dataflow.worker roles/compute.viewer |\n| Cloud Storage             | sa-storage-writer   | Data ingestion | roles/storage.objectViewer roles/storage.objectCreator For descriptions of these roles, see IAM roles for Cloud Storage.                      |\n| Pub/Sub               | sa-pubsub-writer   | Data ingestion | roles/pubsub.publisher roles/pubsub.subscriber For descriptions of these roles, see IAM roles for Pub/Sub.                          |\n| Cloud Scheduler             | sa-scheduler-controller  | Data ingestion | roles/compute.viewer roles/dataflow.developer                                         |\nYou use Sensitive Data Protection to de-identify your structured and unstructured data during the ingestion phase. For structured data, you use [recordtransformations](/dlp/docs/deidentify-sensitive-data#record_transformations) based on fields to de-identify data. For an example of this approach, see the [/examples/de_identification_template/](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse/tree/main/examples/de-identification-template) folder. This example checks structured data for any credit card numbers and card PINs. For unstructured data, you use [informationtypes](/dlp/docs/infotypes-reference) to de-identify data.\nTo de-identify data that is tagged as confidential, you use Sensitive Data Protection and a Dataflow pipeline to tokenize it. This pipeline takes data from Cloud Storage, processes it, and then sends it to the BigQuery data warehouse.\nFor more information about the data de-identification process, see [datagovernance](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse/tree/main/modules/data-governance) .\n### Security controls for data storage\nYou configure the following security controls to help protect data in the BigQuery warehouse:\n- Column-level access controls\n- Service accounts with limited roles\n- Organizational policies\n- VPC Service Controls perimeters between the confidential project and the non-confidential project, with appropriate [perimeter bridges](#perimeter-bridges) \n- Encryption and key managementTo help protect confidential data, you use [access controls for specificcolumns](/bigquery/docs/column-level-security-intro) in the BigQuery warehouse. In order to access the data in these columns, a data analyst must have the [Fine-Grained Reader](/iam/docs/understanding-roles#data-catalog-roles) role.\nTo define access for columns in BigQuery, you create [policytags](/bigquery/docs/best-practices-policy-tags) . For example, the `taxonomy.tf` file in the [bigquery-confidential-data example](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse/tree/a4267570f86ff9cb417e122140b4cff6cb85facf/examples/bigquery-confidential-data) module creates the following tags:\n- A `3_Confidential` policy tag for columns that include very sensitive information, such as credit card numbers. Users who have access to this tag also have access to columns that are tagged with the `2_Private` or `1_Sensitive` policy tags.\n- A `2_Private` policy tag for columns that include sensitive personal identifiable information (PII) information, such as a person's first name. Users who have access to this tag also have access to columns that are tagged with the `1_Sensitive` policy tag. Users do not have access to columns that are tagged with the `3_Confidential` policy tag.\n- A `1_Sensitive` policy tag for columns that include data that cannot be made public, such as the credit limit. Users who have access to this tag do not have access to columns that are tagged with the `2_Private` or `3_Confidential` policy tags.\nAnything that is not tagged is available to all users who have access to the data warehouse.\nThese access controls ensure that, even after the data is re-identified, the data still cannot be read until access is explicitly granted to the user.\n**Note:** You can use the default definitions to run the examples. For more best practices, see [Best practices for using policy tags in BigQuery](/bigquery/docs/best-practices-policy-tags) .\nYou must limit access to the confidential data project so that only authorized users can view the confidential data. To do so, you create a service account with the `roles/iam.serviceAccountUser` role that authorized users must impersonate. [ServiceAccount impersonation](/iam/docs/impersonating-service-accounts) helps users to use service accounts without downloading the service account keys, which improves the overall security of your project. Impersonation creates a short-term token that authorized users who have the [roles/iam.serviceAccountTokenCreator role](/iam/docs/service-accounts) are allowed to download.\nThis blueprint includes the organization policy constraints that the enterprise foundations blueprint uses and adds additional constraints. For more information about the constraints that the enterprise foundations blueprint uses, see [Organization policy contraints](/architecture/security-foundations/preventative-controls#organization-policy) .\nThe following table describes the additional organizational policy [constraints](/resource-manager/docs/organization-policy/org-policy-constraints) that are defined in the [org_policies](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse/tree/main/modules/org-policies) module:\n| Policy                              | Constraint name         | Recommended value                                      |\n|:-----------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Restrict resource deployments to specific physical locations. For additional values, see Value groups.      | gcp.resourceLocations        | One of the following: in:us-locations in:eu-locations in:asia-locations                         |\n| Disable service account creation                        | iam.disableServiceAccountCreation     | true                                          |\n| Enable OS Login for VMs created in the project. For more information, see Managing OS Login in an organization and OS Login. | compute.requireOsLogin        | true                                          |\n| Restrict new forwarding rules to be internal only, based on IP address.              | compute.restrictProtocolForwardingCreationForTypes | INTERNAL                                         |\n| Define the set of shared VPC subnetworks that Compute Engine resources can use.            | compute.restrictSharedVpcSubnetworks    | projects/PROJECT_ID/regions/REGION/s ubnetworks/SUBNETWORK-NAME. Replace SUBNETWORK-NAME with the resource ID of the private subnet that you want the blueprint to use. |\n| Disable serial port output logging to Cloud Logging.                   | compute.disableSerialPortLogging     | true                                          |\nYou manage separate CMEK keys for your confidential data so that you can re-identity the data. You use Cloud HSM to protect your keys. To re-identify your data, use the following keys:\n- A CMEK key that the Dataflow pipeline uses for the re-identification process.\n- The original cryptographic key that Sensitive Data Protection uses to de-identify your data.\n- A CMEK key for the BigQuery warehouse in the confidential data project.\nAs mentioned earlier in [Key management and encryption for ingestion](#key-mgmt-ingestion) , you can specify the CMEK location and rotation periods. You can use Cloud EKM if it is required by your organization.\n### Operational controls\nYou can enable logging and [Security Command Center Premium tier features](/security-command-center/pricing#premium-tier) such as security health analytics and threat detection. These controls help you to do the following:\n- Monitor who is accessing your data.\n- Ensure that proper auditing is put in place.\n- Support the ability of your incident management and operations teams to respond to issues that might occur.[Access Transparency](/logging/docs/audit/access-transparency-overview) provides you with real-time notification in the event [Google support personnel](/assured-workloads/access-transparency/docs/overview#google-personnel-access) require access to your data. Access Transparency logs are generated whenever a human accesses content, and only Google personnel with valid business justifications (for example, a support case) can obtain access. We recommend that you [enable Access Transparency](/assured-workloads/access-transparency/docs/enable) .\nTo help you to meet auditing requirements and get insight into your projects, you configure the [Google Cloud Observability](/products/operations) with data logs for services you want to track. The [centralized-logging module](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse/tree/main/modules/centralized-logging) configures the following best practices:\n- [Creating an aggregated log sink](/logging/docs/export/aggregated_sinks) across all projects.\n- Storing your logs in the appropriate region.\n- Adding CMEK keys to your logging sink.\nFor all services within the projects, your logs must include information about data reads and writes, and information about what administrators read. For additional logging best practices, see [Detective controls](/architecture/security-foundations/detective-controls) .\nAfter you deploy the blueprint, you can set up alerts to notify your security operations center (SOC) that a security incident might be occurring. For example, you can use alerts to let your security analyst know when an IAM permission has changed. For more information about configuring Security Command Center alerts, see [Setting up finding notifications](/security-command-center/docs/how-to-notifications) . For additional alerts that are not published by Security Command Center, you can set up [alerts](/monitoring/alerts) with Cloud Monitoring.\n### Additional security considerations\nThe security controls in this blueprint have been reviewed by both the [Google Cybersecurity Action Team](/security/gcat) and a third-party security team. To request access under NDA to both a [STRIDE threat model](https://wikipedia.org/wiki/STRIDE_(security)) and the summary assessment report, send an email to [secured-dw-blueprint-support@google.com](mailto:secured-dw-blueprint-support@google.com) .\nIn addition to the security controls described in this solution, you should review and manage the security and risk in key areas that overlap and interact with your use of this solution. These include the following:\n- The code that you use to configure, deploy, and run Dataflow jobs.\n- The data classification taxonomy that you use with this solution.\n- The content, quality, and security of the datasets that you store and analyze in the data warehouse.\n- The overall environment in which you deploy the solution, including the following:- The design, segmentation, and security of networks that you connect to this solution.\n- The security and governance of your organization's IAM controls.\n- The authentication and authorization settings for the actors to whom you grant access to the infrastructure that's part of this solution, and who have access to the data that's stored and managed in that infrastructure.\n## Bringing it all together\nTo implement the architecture described in this document, do the following:\n- Determine whether you will deploy the blueprint with the enterprise foundations blueprint or on its own. If you choose not to deploy the enterprise foundations blueprint, ensure that your environment has a similar security baseline in place.\n- Review the [Readme](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse) for the blueprint and ensure that you meet all the prerequisites.\n- In your testing environment, deploy the [walkthrough](https://console.cloud.google.com/?walkthrough_id=security--secured-data-warehouse-simple-example) to see the solution in action. As part of your testing process, consider the following:- Use Security Command Center to scan the newly created projects against your [compliance requirements](/security-command-center/docs/concepts-vulnerabilities-findings) .\n- Add your own sample data into the BigQuery warehouse.\n- Work with a data analyst in your enterprise to test their access to the confidential data and whether they can interact with the data from BigQuery in the way that they would expect.\n- Deploy the blueprint into your production environment.## What's next\n- Review the [Google Cloud enterprise foundations blueprint](/architecture/security-foundations) for a baseline secure environment.\n- To see the details of the blueprint, read the [Terraformconfiguration readme](https://github.com/GoogleCloudPlatform/terraform-google-secured-data-warehouse) .\n- For more best practices and blueprints, see the [security best practices center](/security/best-practices) .", "guide": "Cloud Architecture Center"}