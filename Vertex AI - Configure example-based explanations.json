{"title": "Vertex AI - Configure example-based explanations", "url": "https://cloud.google.com/vertex-ai/docs/start/introduction-unified-platform", "abstract": "# Vertex AI - Configure example-based explanations\nTo use example-based explanations, you must configure explanations by specifying an [explanationSpec](/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model.FIELDS.explanation_spec) when you import or upload the [Model](/vertex-ai/docs/reference/rest/v1/projects.locations.models) resource into the [Model Registry](/vertex-ai/docs/model-registry/introduction) .\nThen, when you request online explanations, you can override some of those configuration values by specifying an [ExplanationSpecOverride](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/explain#ExplanationSpecOverride) in the request. You cannot request batch explanations; they are not supported.\nThis page describes how to configure and update these options.\n", "content": "## Configure explanations when importing or uploading the model\nBefore you begin, make sure that you have following:\n- A Cloud Storage location that contains your model artifacts. Your model either needs to be a deep neural network (DNN) model where you provide the name of a layer, or signature, whose output can be used as the latent space, or you can provide a model that directly outputs embeddings (latent space representation). This latent space captures the example representations that are used for generating explanations.\n- A Cloud Storage location that contains the instances to be indexed for approximate nearest neighbor search. For more information, see [input data requirements](#input_data_requirements) .\n**Note:** You must specify either a [Preset](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#presets) search configuration or the full [NearestNeighborSearchConfig](#nearest-neighbor-search-config) .\n**Note:** Allow 1-2 hours for model upload to complete. This step includes two time consuming operations: running a [batchPredictionJob](/vertex-ai/docs/reference/rest/v1/projects.locations.batchPredictionJobs) to generate the latent representations (embeddings) and using [Vector Search](/vertex-ai/docs/vector-search/overview) to index them for nearest neighbor search. The actual time for your model and data will vary. This is typically the most time consuming step when using example-based explanations. The other steps, such as deploying the model and getting online explanations, don't require batch prediction or indexing.\nFollow the guide to [importing amodel](/vertex-ai/docs/model-registry/import-model#import_a_model_using) using the Google Cloud console.\nIn the **Explainability** tab, select **Example-based explanation** and fill in the fields.\nFor information about each field, see the tips in Google Cloud Console (shown below) as well as the reference documentation for [Example](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#examples) and [ExplanationMetadata](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata) .\n- Write the following [ExplanationMetadata](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata) to a JSON file in your local environment. The filename does not matter, but for this example, call the file`explanation-metadata.json`:\n```\n{\u00a0 \"inputs\": {\u00a0 \u00a0 \"my_input\": {\u00a0 \u00a0 \u00a0 \"inputTensorName\": \"INPUT_TENSOR_NAME\",\u00a0 \u00a0 \u00a0 \"encoding\": \"IDENTITY\",\u00a0 \u00a0 },\u00a0 \u00a0 \"id\": {\u00a0 \u00a0 \u00a0 \"inputTensorName\": \"id\",\u00a0 \u00a0 \u00a0 \"encoding\": \"IDENTITY\"\u00a0 \u00a0 }\u00a0 },\u00a0 \"outputs\": {\u00a0 \u00a0 \"embedding\": {\u00a0 \u00a0 \u00a0 \"outputTensorName\": \"OUTPUT_TENSOR_NAME\"\u00a0 \u00a0 }\u00a0 }}\n```- (Optional) If you are specifying the full [NearestNeighborSearchConfig](#nearest-neighbor-search-config) , write the following to a JSON file in your local environment. The filename does not matter, but for this example, call the file`search_config.json`:\n```\n{\u00a0 \"contentsDeltaUri\": \"\",\u00a0 \"config\": {\u00a0 \u00a0 \u00a0 \"dimensions\": 50,\u00a0 \u00a0 \u00a0 \"approximateNeighborsCount\": 10,\u00a0 \u00a0 \u00a0 \"distanceMeasureType\": \"SQUARED_L2_DISTANCE\",\u00a0 \u00a0 \u00a0 \"featureNormType\": \"NONE\",\u00a0 \u00a0 \u00a0 \"algorithmConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"treeAhConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"leafNodeEmbeddingCount\": 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fractionLeafNodesToSearch\": 1.0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }\u00a0 }\n```- Run the following command to upload your [Model](/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model) .\nIf you are using a `Preset` search configuration, remove the `--explanation-nearest-neighbor-search-config-file` flag. If you are specifying [NearestNeighborSearchConfig](#nearest-neighbor-search-config) , remove the `--explanation-modality` and `--explanation-query` flags.\nThe flags most pertinent to example-based explanations are bolded.\n```\ngcloud ai models upload \\\u00a0 \u00a0 --region=LOCATION \\\u00a0 \u00a0 --display-name=MODEL_NAME \\\u00a0 \u00a0 --container-image-uri=IMAGE_URI \\\u00a0 \u00a0 --artifact-uri=MODEL_ARTIFACT_URI \\\u00a0 \u00a0 --explanation-method=examples \\\u00a0 \u00a0 --uris=[URI, ...] \\\u00a0 \u00a0 --explanation-neighbor-count=NEIGHBOR_COUNT \\\u00a0 \u00a0 --explanation-metadata-file=explanation-metadata.json \\\u00a0 \u00a0 --explanation-modality=IMAGE|TEXT|TABULAR \\\u00a0 \u00a0 --explanation-query=PRECISE|FAST \\\u00a0 \u00a0 --explanation-nearest-neighbor-search-config-file=search_config.json\n```\nSee [gcloud ai models upload](/sdk/gcloud/reference/ai/models/upload) for more information.- The upload action returns an`OPERATION_ID`that can be used to check when the operation is finished. You can poll for the status of the operation until the response includes`\"done\": true`. Use the [gcloud ai operations describe](/sdk/gcloud/reference/ai/operations/describe) command to poll the status, for example:```\ngcloud ai operations describe <operation-id>\n```You will not be able to request explanations until the operation is done. Depending on the size of the dataset and model architecture, this step can take several hours to build the index used to query for examples.Before using any of the request data, make the following replacements:\nTo learn about the other placeholders, see [Model](/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model) , [explanationSpec](/vertex-ai/docs/reference/rest/v1/ExplanationSpec) , and [Examples](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#examples) .\nTo learn more about uploading models, see the [upload](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload) method and [Importing models](/vertex-ai/docs/model-registry/import-model) .\nThe JSON request body below specifies a `Preset` search configuration. Alternatively, you can specify the full [NearestNeighborSearchConfig](#nearest-neighbor-search-config) .\nHTTP method and URL:\n```\nPOST https://LOCATION-aiplatform.googleapis.com/v1/projects/PROJECT/locations/LOCATION/models:upload\n```\nRequest JSON body:\n```\n{\n \"model\": {\n \"displayName\": \"my-model\",\n \"artifactUri\": \"gs://your-model-artifact-folder\",\n \"containerSpec\": {\n  \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest\",\n },\n \"explanationSpec\": {\n  \"parameters\": {\n  \"examples\": {\n   \"gcsSource\": {\n   \"uris\": [\"gs://your-examples-folder\"]\n   },\n   \"neighborCount\": 10,\n   \"presets\": {\n   \"modality\": \"image\"\n   }\n  }\n  },\n  \"metadata\": {\n  \"outputs\": {\n   \"embedding\": {\n    \"output_tensor_name\": \"embedding\"\n   }\n  },\n  \"inputs\": {\n   \"my_fancy_input\": {\n   \"input_tensor_name\": \"input_tensor_name\",\n   \"encoding\": \"identity\",\n   \"modality\": \"image\"\n   },\n   \"id\": {\n   \"input_tensor_name\": \"id\",\n   \"encoding\": \"identity\"\n   }\n  }\n  }\n }\n }\n}\n```\nTo send your request, expand one of these options:You should receive a JSON response similar to the following:\n```\n{\n \"name\": \"projects/PROJECT_NUMBER/locations/LOCATION/models/MODEL_ID/operations/OPERATION_ID\",\n \"metadata\": {\n \"@type\": \"type.googleapis.com/google.cloud.aiplatform.v1.UploadModelOperationMetadata\",\n \"genericMetadata\": {\n  \"createTime\": \"2022-01-08T01:21:10.147035Z\",\n  \"updateTime\": \"2022-01-08T01:21:10.147035Z\"\n }\n }\n}\n```\nThe upload action returns an `OPERATION_ID` that can be used to check when the operation is finished. You can poll for the status of the operation until the response includes `\"done\": true` . Use the [gcloud ai operations describe](/sdk/gcloud/reference/ai/operations/describe) command to poll the status, for example:\n```\ngcloud ai operations describe <operation-id>\n```\nYou will not be able to request explanations until the operation is done. Depending on the size of the dataset and model architecture, this step can take several hours to build the index used to query for examples.\nSee the section **Upload the model** in the [image classificationexample-based explanationsnotebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/gapic/custom/showcase_custom_image_classification_online_explain_example_based_api.ipynb) .\n### NearestNeighborSearchConfig\nThe following JSON request body demonstrates how to specify the full [NearestNeighborSearchConfig](#nearest-neighbor-search-config) (instead of presets) in an [upload](/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload#request-body) request.\n```\n{\u00a0 \"model\": {\u00a0 \u00a0 \"displayName\": displayname,\u00a0 \u00a0 \"artifactUri\": model_path_to_deploy,\u00a0 \u00a0 \"containerSpec\": {\u00a0 \u00a0 \u00a0 \"imageUri\": DEPLOY_IMAGE,\u00a0 \u00a0 },\u00a0 \u00a0 \"explanationSpec\": {\u00a0 \u00a0 \u00a0 \"parameters\": {\u00a0 \u00a0 \u00a0 \u00a0 \"examples\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"gcsSource\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"uris\": [DATASET_PATH]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"neighborCount\": 5,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"nearest_neighbor_search_config\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"contentsDeltaUri\": \"\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"config\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"dimensions\": dimensions,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"approximateNeighborsCount\": 10,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"distanceMeasureType\": \"SQUARED_L2_DISTANCE\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"featureNormType\": \"NONE\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"algorithmConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"treeAhConfig\": {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"leafNodeEmbeddingCount\": 1000,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"fractionLeafNodesToSearch\": 1.0\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 },\u00a0 \u00a0 \u00a0 \"metadata\": { ... }\u00a0 \u00a0 }\u00a0 }}\n```\nThe tables below lists the fields for `NearestNeighborSearchConfig` .\n| Fields     | Fields.1                                                                                                                                   |\n|:--------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| dimensions    | int32 Required. The number of dimensions of the input vectors.                                                                                                                     |\n| approximateNeighborsCount | int32 Required if tree-AH algorithm is used. The default number of neighbors to find through approximate search before exact reordering is performed. Exact reordering is a procedure where results returned by an approximate search algorithm are reordered via a more expensive distance computation.                                                          |\n| distanceMeasureType  | DistanceMeasureType The distance measure used in nearest neighbor search.                                                                                                                   |\n| featureNormType   | FeatureNormType Type of normalization to be carried out on each vector.                                                                                                                   |\n| algorithmConfig   | oneOf: TreeAhConfig BruteForceConfig The configuration for the algorithms that Vector Search uses for efficient search. TreeAhConfig: Configuration options for using the tree-AH algorithm. Refer to this blog for more details: Scaling deep retrieval with TensorFlow Recommenders and Vector Search BruteForceConfig: This option implements the standard linear search in the database for each query. There are no fields to configure for a brute force search. To select this algorithm, pass an empty object for BruteForceConfig. |\n| Enums    | Enums.1                                                                         |\n|:---------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| SQUARED_L2_DISTANCE | Euclidean (L2) Distance                                                                     |\n| L1_DISTANCE   | Manhattan (L1) Distance                                                                     |\n| DOT_PRODUCT_DISTANCE | Default value. Defined as a negative of the dot product.                                                             |\n| COSINE_DISTANCE  | Cosine Distance. We strongly suggest using DOT_PRODUCT_DISTANCE + UNIT_L2_NORM instead of the COSINE distance. Our algorithms have been more optimized for the DOT_PRODUCT distance, and when combined with UNIT_L2_NORM, it offers the same ranking and mathematical equivalence as the COSINE distance. |\n| Enums  | Enums.1           |\n|:-------------|:---------------------------------------------------|\n| UNIT_L2_NORM | Unit L2 normalization type.      |\n| NONE   | Default value. No normalization type is specified. |\nThese are the fields to select for the tree-AH algorithm (Shallow tree + Asymmetric Hashing).\n| Fields     | Fields.1                                                |\n|:--------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| fractionLeafNodesToSearch | double                                                |\n| nan      | The default fraction of leaf nodes that any query may be searched. Must be in range 0.0 - 1.0, exclusive. The default value is 0.05 if not set.              |\n| leafNodeEmbeddingCount | int32                                                 |\n| nan      | Number of embeddings on each leaf node. The default value is 1000 if not set.                              |\n| leafNodesToSearchPercent | int32                                                 |\n| nan      | Deprecated, use fractionLeafNodesToSearch. The default percentage of leaf nodes that any query may be searched. Must be in range 1-100, inclusive. The default value is 10 (means 10%) if not set. |\nThis option implements the standard linear search in the database for each query. There are no fields to configure for a brute force search. To select this algorithm, pass an empty object for `BruteForceConfig` to `algorithmConfig` .\n### Input data requirements\nUpload your dataset to a Cloud Storage location. Make sure the files in the JSON Lines format.\nThe files must be in JSON Lines format. The following sample is from the [imageclassification example-based explanationsnotebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/gapic/custom/showcase_custom_image_classification_online_explain_example_based_api.ipynb) :\n```\n{\"id\": \"0\", \"bytes_inputs\": {\"b64\": \"...\"}}{\"id\": \"1\", \"bytes_inputs\": {\"b64\": \"...\"}}{\"id\": \"2\", \"bytes_inputs\": {\"b64\": \"...\"}}\n```\n## Update the index or configuration\nVertex AI lets you update a model's nearest neighbor index or [Example](/vertex-ai/docs/reference/rest/v1/ExplanationSpec#examples) configuration. This is useful if you'd like to update your model without re-indexing its dataset. For example, if your model's index contains 1,000 instances, and you'd like to add 500 more instances, you can call [UpdateExplanationDataset](/vertex-ai/docs/reference/rest/v1/projects.locations.models/updateExplanationDataset) to add to the index without re-processing the original 1,000 instances.\nTo update the explanation dataset:\n```\ndef update_explanation_dataset(model_id, new_examples):\u00a0 \u00a0 response = clients[\"model\"].update_explanation_dataset(model=model_id, \u00a0examples=new_examples)\u00a0 \u00a0 update_dataset_response = response.result()\u00a0 \u00a0 return update_dataset_responsePRESET_CONFIG = {\u00a0 \u00a0 \"modality\": \"TEXT\",\u00a0 \u00a0 \"query\": \"FAST\"}NEW_DATASET_FILE_PATH = \"new_dataset_path\"NUM_NEIGHBORS_TO_RETURN = 10EXAMPLES = aiplatform.Examples(presets=PRESET_CONFIG,\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 gcs_source=aiplatform.types.io.GcsSource(uris=[NEW_DATASET_FILE_PATH]),\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 neighbor_count=NUM_NEIGHBORS_TO_RETURN)MODEL_ID = 'model_id'update_dataset_response = update_explanation_dataset(MODEL_ID, EXAMPLES)\n```\nUsage notes:\n- The `model_id` remains unchanged after the [UpdateExplanationDataset](/vertex-ai/docs/reference/rest/v1/projects.locations.models/updateExplanationDataset) operation.\n- The `UpdateExplanationDataset` operation affects only the [Model](/vertex-ai/docs/reference/rest/v1/projects.locations.models) resource; it does not update any associated [DeployedModel](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/deployModel) s. This means that a `deployedModel` 's index contains the dataset at the time it was deployed. To update a `deployedModel` 's index, you must re-deploy the updated model to an endpoint.## Override the configuration when getting online explanations\nWhen you request an explanation, you can override some of the parameters on the fly by specifying the [ExplanationSpecOverride](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/explain#ExplanationSpecOverride) field.\nDepending on the application, some constraints might be desirable on the kind of explanations that are returned. For example, to ensure diversity of explanations, a user can specify a crowding parameter which dictates that no single type of examples are over-represented in the explanations. Concretely, if a user is trying to understand why a bird was labeled as a plane by their model, they might not be interested in seeing too many bird examples as explanations to better investigate the root cause.\nThe following table summarizes the parameters that can be overridden for an example-based explanation request:\n| Property Name | Property Value | Description              |\n|:----------------|:-----------------|:----------------------------------------------------------------|\n| neighborCount | int32   | The number of examples to return as explanation     |\n| crowdingCount | int32   | Maximum number of examples to return with the same crowding tag |\n| allow   | String Array  | The tags that are allowed for explanations to have    |\n| deny   | String Array  | The tags that are not allowed for explanations to have   |\nThe [Vector Search Filtering](/vertex-ai/docs/vector-search/filtering#json) describes these parameters in more details.\nHere's an example of a JSON request body with overrides:\n```\n{\u00a0 \"instances\":[\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"id\": data[0][\"id\"],\u00a0 \u00a0 \u00a0 \"bytes_inputs\": {\"b64\": bytes},\u00a0 \u00a0 \u00a0 \"restricts\": \"\",\u00a0 \u00a0 \u00a0 \"crowding_tag\": \"\"\u00a0 \u00a0 }\u00a0 ],\u00a0 \"explanation_spec_override\": {\u00a0 \u00a0 \"examples_override\": {\u00a0 \u00a0 \u00a0 \"neighbor_count\": 5,\u00a0 \u00a0 \u00a0 \"crowding_count\": 2,\u00a0 \u00a0 \u00a0 \"restrictions\": [\u00a0 \u00a0 \u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"namespace_name\": \"label\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"allow\": [\"Papilloma\", \"Rift_Valley\", \"TBE\", \"Influenza\", \"Ebol\"]\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 }\u00a0 }}\n```\n## What's next\n- [Deploy your Model to an Endpoint](/vertex-ai/docs/predictions/deploy-model-api) and [get example-based explanations](/vertex-ai/docs/predictions/get-online-predictions#explain-request) .\nHere's an example of the response from an example-based [explain](/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/explain) request:\n```\n[\u00a0 \u00a0{\u00a0 \u00a0 \u00a0 \"neighbors\":[\u00a0 \u00a0 \u00a0 \u00a0 \u00a0{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"neighborId\":\"311\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"neighborDistance\":383.8\u00a0 \u00a0 \u00a0 \u00a0 \u00a0},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"neighborId\":\"286\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"neighborDistance\":431.4\u00a0 \u00a0 \u00a0 \u00a0 \u00a0}\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"input\":\"0\"\u00a0 \u00a0},\u00a0 \u00a0{\u00a0 \u00a0 \u00a0 \"neighbors\":[\u00a0 \u00a0 \u00a0 \u00a0 \u00a0{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"neighborId\":\"223\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"neighborDistance\":471.6\u00a0 \u00a0 \u00a0 \u00a0 \u00a0},\u00a0 \u00a0 \u00a0 \u00a0 \u00a0{\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"neighborId\":\"55\",\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \"neighborDistance\":392.7\u00a0 \u00a0 \u00a0 \u00a0 \u00a0}\u00a0 \u00a0 \u00a0 ],\u00a0 \u00a0 \u00a0 \"input\":\"1\"\u00a0 \u00a0}]\n```\n## Pricing\nSee the section on example-based explanations in the [pricingpage](/vertex-ai/pricing#explanations) .", "guide": "Vertex AI"}