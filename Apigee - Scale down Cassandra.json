{"title": "Apigee - Scale down Cassandra", "url": "https://cloud.google.com/apigee/docs/api-platform/get-started/what-apigee", "abstract": "# Apigee - Scale down Cassandra\nYou are currently viewing version 1.1 of the Apigee hybrid documentation. **This version is end of life.** You should upgrade to a newer version. For more information, see [Supported versions](/apigee/docs/hybrid/supported-platforms#supported-versions) .\n**CAUTION:** Scaling down cassandra clusters are more invasive than scaling up. Please proceed with caution when scaling down\nApigee hybrid employs a ring of Cassandra nodes as a [StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) . Cassandra provides persistent storage for certain Apigee entities on the runtime plane. For more information about Cassandra, see [About the runtime plane](/apigee/docs/hybrid/v1.1/what-is-hybrid#about-the-runtime-plane) .\nCassandra is a resource intensive service and should not be deployed on a pod with any other hybrid services. Depending on the load, you might want to scale the number of Cassandra nodes in the ring down in your cluster.\nThe general process for scaling down a Cassandra ring is:\n- Decommission one Cassandra node.\n- Update the`cassandra.replicaCount`property in`overrides.yaml`.\n- Apply the configuration update.\n- Repeat these steps for each node you want remove.\n- Delete the persistent volume claim or volume, depending on your cluster configuration.", "content": "## \n What you need to know\n- Perform this task on one node at a time before proceeding to the  next node.\n- If any node other than the node to be  decommissioned is unhealthy, do not proceed. Kubernetes will not be  able to downscale the pods  from the cluster.\n- Always scale down or up by a factor of three nodes.\n## Prerequisites\nBefore you scale down the number of Cassandra nodes in the ring, validate if the cluster is healthy and all the nodes are up and running, as the following example shows:\n```\n kubectl get pods -n yourNamespace -l app=apigee-cassandra\nNAME     READY STATUS RESTARTS AGE\napigee-cassandra-0 1/1  Running 0   2h\napigee-cassandra-1 1/1  Running 0   2h\napigee-cassandra-2 1/1  Running 0   2h\napigee-cassandra-3 1/1  Running 0   16m\napigee-cassandra-4 1/1  Running 0   14m\napigee-cassandra-5 1/1  Running 0   13m\n```\n```\nkubectl -n yourNamespace exec -it apigee-cassandra-0 nodetool status\nDatacenter: us-east1\n====================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n-- Address  Load  Tokens  Owns (effective) Host ID        Rack\nUN 10.16.2.6 690.17 KiB 256   48.8%    b02089d1-0521-42e1-bbed-900656a58b68 ra-1\nUN 10.16.4.6 700.55 KiB 256   51.6%    dc6b7faf-6866-4044-9ac9-1269ebd85dab ra-1 to\nUN 10.16.11.11 144.36 KiB 256   48.3%    c7906366-6c98-4ff6-a4fd-17c596c33cf7 ra-1\nUN 10.16.1.11 767.03 KiB 256   49.8%    ddf221aa-80aa-497d-b73f-67e576ff1a23 ra-1\nUN 10.16.5.13 193.64 KiB 256   50.9%    2f01ac42-4b6a-4f9e-a4eb-4734c24def95 ra-1\nUN 10.16.8.15 132.42 KiB 256   50.6%    a27f93af-f8a0-4c88-839f-2d653596efc2 ra-1\n```\n**CAUTION:** If the cluster is not healthy or at least one node is not up and running, do not proceed with this process.\n## \n Decommission the Cassandra nodes\n- Decommission the Cassandra nodes from the cluster using the **nodetool** command.Always decommission the nodes with higher numbers in the pod name first. For example in a 6 node cluster start with  the apigee-cassandra-5 Cassandra node.```\nkubectl -n yourNamespace exec -it nodeName nodetool decommission\n```For example, this command decommissions `apigee-cassandra-5` , the node with the highest number value in the name:```\nkubectl -n apigee exec -it apigee-cassandra-5 nodetool decommission\n```\n- Wait for the decommission to complete, and verify that the cluster now has one less node. For example:```\nkubectl -n yourNamespace exec -it nodeName nodetool status\nDatacenter: us-east1\n====================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n-- Address  Load  Tokens  Owns (effective) Host ID        Rack\nUN 10.16.2.6 710.37 KiB 256   59.0%    b02089d1-0521-42e1-bbed-900656a58b68 ra-1\nUN 10.16.4.6 720.97 KiB 256   61.3%    dc6b7faf-6866-4044-9ac9-1269ebd85dab ra-1\nUN 10.16.1.11 777.11 KiB 256   58.9%    ddf221aa-80aa-497d-b73f-67e576ff1a23 ra-1\nUN 10.16.5.13 209.23 KiB 256   62.2%    2f01ac42-4b6a-4f9e-a4eb-4734c24def95 ra-1\nUN 10.16.8.15 143.23 KiB 256   58.6%    a27f93af-f8a0-4c88-839f-2d653596efc2 ra-1\n```\n- Update or add the`cassandra.replicaCount`property in your`overrides.yaml`file. For example, if the current node count is 6, change it to 5:```\ncassandra:\n replicaCount: 5 # (n-1)\n```\n- Apply the configuration change to your cluster. For example:```\n./apigeectl apply -v beta2 -c cassandra\nnamespace/apigee unchanged\nsecret/ssl-cassandra unchanged\nstorageclass.storage.k8s.io/apigee-gcepd unchanged\nservice/apigee-cassandra unchanged\nstatefulset.apps/apigee-cassandra configured\n```\n- Verify that all of the remaining Cassandra nodes are running:```\nkubectl get pods -n yourNamespace -l app=apigee-cassandra\nNAME     READY STATUS RESTARTS AGE\napigee-cassandra-0 1/1  Running 0   3h\napigee-cassandra-1 1/1  Running 0   3h\napigee-cassandra-2 1/1  Running 0   2h\napigee-cassandra-3 1/1  Running 0   25m\napigee-cassandra-4 1/1  Running 0   24m\n```\n- Repeat Steps 1-5 for each node that you wish to decommission.\n- When you are finished decommissioning nodes, verify that the`cassandra.replicaCount`value equals the number of nodes returned by the`nodetool status`command.```\nkubectl -n yourNamespace exec -it apigee-cassandra-0 nodetool status\nDatacenter: us-east1\n====================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n-- Address  Load  Tokens  Owns (effective) Host ID        Rack\nUN 10.16.2.6 710.37 KiB 256   59.0%    b02089d1-0521-42e1-bbed-900656a58b68 ra-1\nUN 10.16.4.6 720.97 KiB 256   61.3%    dc6b7faf-6866-4044-9ac9-1269ebd85dab ra-1\nUN 10.16.1.11 777.11 KiB 256   58.9%    ddf221aa-80aa-497d-b73f-67e576ff1a23 ra-1\nUN 10.16.5.13 209.23 KiB 256   62.2%    2f01ac42-4b6a-4f9e-a4eb-4734c24def95 ra-1\nUN 10.16.8.15 143.23 KiB 256   58.6%    a27f93af-f8a0-4c88-839f-2d653596efc2 ra-1\n```\n- After the Cassandra cluster is downsized, make sure to delete the pvc  (PersistentVolumeClaim) to make sure next scale up event does not  use the same Persistent volume and the data created earlier.```\nkubectl get pvc -n yourNamespace\nNAME        STATUS VOLUME          CAPACITY ACCESS MODES STORAGECLASS AGE\ncassandra-data-apigee-cassandra-0 Bound pvc-f9c2a5b9-818c-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 7h\ncassandra-data-apigee-cassandra-1 Bound pvc-2956cb78-818d-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 7h\ncassandra-data-apigee-cassandra-2 Bound pvc-79de5407-8190-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 7h\ncassandra-data-apigee-cassandra-3 Bound pvc-d29ba265-81a2-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 5h\ncassandra-data-apigee-cassandra-4 Bound pvc-0675a0ff-81a3-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 5h\ncassandra-data-apigee-cassandra-5 Bound pvc-354afa95-81a3-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 5h\n``````\nkubectl -n yourNamespace delete pvc cassandra-data-apigee-cassandra-5\npersistentvolumeclaim \"cassandra-data-apigee-cassandra-5\" deleted\n``````\nkubectl get pvc -n yourNamespace\nNAME        STATUS VOLUME          CAPACITY ACCESS MODES STORAGECLASS AGE\ncassandra-data-apigee-cassandra-0 Bound pvc-f9c2a5b9-818c-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 7h\ncassandra-data-apigee-cassandra-1 Bound pvc-2956cb78-818d-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 7h\ncassandra-data-apigee-cassandra-2 Bound pvc-79de5407-8190-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 7h\ncassandra-data-apigee-cassandra-3 Bound pvc-d29ba265-81a2-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 5h\ncassandra-data-apigee-cassandra-4 Bound pvc-0675a0ff-81a3-11e9-8862-42010a8e014a 100Gi  RWO   apigee-gcepd 5h\n```\n- If you are using Anthos installation, delete the Persistent volume from Anthos Kubernetes cluster also.```\nkubectl get pv -n youNamespace\nNAME          CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM          STORAGECLASS REASON AGE\npvc-0675a0ff-81a3-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-4 apigee-gcepd   5h\npvc-2956cb78-818d-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-1 apigee-gcepd   7h\npvc-354afa95-81a3-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-5 apigee-gcepd   5h\npvc-79de5407-8190-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-2 apigee-gcepd   7h\npvc-d29ba265-81a2-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-3 apigee-gcepd   5h\npvc-f9c2a5b9-818c-11e9-8862-42010a8e014a 100Gi  RWO   Delete   Bound apigee/cassandra-data-apigee-cassandra-0 apigee-gcepd   7h\n``````\nkubectl -n yourNamespace delete pv pvc-354afa95-81a3-11e9-8862-42010a8e014a\n```", "guide": "Apigee"}