{"title": "Cloud Architecture Center - Scenarios for exporting Cloud Logging: Compliance requirements", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Scenarios for exporting Cloud Logging: Compliance requirements\nLast reviewed 2022-10-26 UTC\nThis scenario shows how to export logs from Cloud Logging to Cloud Storage to meet your organization's compliance requirements. Organizations face many such requirements for creating and preserving logging files. For example, where Sarbanes Oxley (SOX) compliance is important, you might want to maintain logs for users, databases, and console activity. For more information on the default and configurable log retention periods, see the Cloud Logging [quotas and limits](/stackdriver/quotas) .\nIn this scenario, the exported logs are delivered to a Cloud Storage bucket that you configure. You grant permissions to limit access to the logs as appropriate. In order to reduce long-term storage costs, you can use the [object lifecycle management](/storage/docs/lifecycle) feature in Cloud Storage to move logs to Nearline or Coldline storage classes and delete them after the required retention period has passed.\nThis scenario assumes a common n-tier web architecture running on Google Cloud with virtual machines (VMs), databases, and a supporting storage system. For this environment, the following log types are exported: all audit logs, virtual machine\u2013related logs, storage logs, and database logs. You can change the types of logs that are exported by adjusting the logging filters in the example.\n", "content": "## Set up the logging export\nThe following diagram shows the steps for enabling logging export to Cloud Storage.### Set up the logging export bucket in Cloud Storage\nFollow the instructions to [set up a Cloud Storage bucket](/storage/docs/creating-buckets) that will host your exported logs. For the **Default storage class** , select Regional, unless you require the Multi-Regional, Nearline, or Coldline storage class.\n### Configure object lifecycle management for the Cloud Storage bucket\nThis scenario assumes a retention requirement of 7 years for all logs. In order to minimize storage costs, you can add object lifecycle rules in Cloud Storage to move logs to Nearline or Coldline storage after a given number of days and then delete the logs when you are no longer required to retain them.\n**Best practice:** Moving logs to Nearline or Coldline and then deleting them helps you manage the ongoing operational cost of maintaining the logs.\nYou can follow the instructions to [create lifecycle rules](/storage/docs/managing-lifecycles#set) . The following screenshot depicts a cascading set of rules that change the storage class to Nearline after 60 days, change the storage class to Coldline after 120 days, and then delete the logs after 2555 days, which is roughly 7 years.### Turn on audit logging for all services\nData access audit logs\u2014except for BigQuery\u2014are disabled by default. In order to enable all audit logs, follow the [instructions to update the Identity and Access Management (IAM) policy](/logging/docs/audit/configure-data-access#example) with the configuration listed in the [audit policy](/logging/docs/audit/configure-data-access#enable_all_access_logs) documentation. The steps include the following:\n- Downloading the current IAM policy as a file.\n- Adding the audit log policy JSON or YAML object to the current policy file.\n- Updating the Google Cloud project with the changed policy file.\nThe following is an example JSON object that enables all audit logs for all services.\n```\n\"auditConfigs\": [ {\n  \"service\": \"allServices\",\n  \"auditLogConfigs\": [   { \"logType\": \"ADMIN_READ\" },\n   { \"logType\": \"DATA_READ\" },\n   { \"logType\": \"DATA_WRITE\" },\n  ]\n },\n]\n```\n### Configure the logging export\nAfter you set up aggregated exports or logs export, you need to refine the logging filters to export audit logs, virtual machine\u2013related logs, storage logs, and database logs. The following logging filter includes the Admin Activity and Data Access audit logs and the logs for specific resource types.\n```\nlogName:\"/logs/cloudaudit.googleapis.com\" OR\nresource.type:gce OR\nresource.type=gcs_bucket OR\nresource.type=cloudsql_database OR\nresource.type=bigquery_resource\n```\nFrom the Google Cloud CLI, use the [gcloud logging sinks create](/sdk/gcloud/reference/logging/sinks/create) command or the [organizations.sinks.create](/logging/docs/reference/v2/rest/v2/organizations.sinks/create?apix=true) API call to create a sink with the appropriate filters. The following example `gcloud` command creates a sink called `gcp_logging_sink_gcs` for the organization. The sink includes all children projects and specifies filtering to select individual audit logs.\n```\ngcloud logging sinks create gcp_logging_sink_gcs \\\n storage.googleapis.com/gcp-logging-export-000100011000 \\\n --log-filter='logName: \"/logs/cloudaudit.googleapis.com\" OR \\\n resource.type:\\\"gce\\\" OR \\\n resource.type=\\\"gcs_bucket\\\" OR \\\n resource.type=\\\"cloudsql_database\\\" OR \\\n resource.type=\\\"bigquery_resource\\\"' \\\n --include-children \\\n --organization=324989855333\n```\nThe command output is similar to the following:\n```\nCreated [https://logging.googleapis.com/v2/organizations/324989855333/sinks/gcp_logging_sink_gcs].\nPlease remember to grant `serviceAccount:gcp-logging-sink-gcs@logging-o324989855333.iam.gserviceaccount.com` full-control access to the bucket.\nMore information about sinks can be found at /logging/docs/export/configure_export\n```\nThe `serviceAccount` entry returned from the API call includes the identity `gcp-logging-sink-gcs@logging-o324989855333.iam.gserviceaccount.com` . This identity represents a Google Cloud service account that has been created for the export. Until you grant this identity write access to the destination, log entry exports from this sink will fail. For more information, see the next section or the documentation for [Granting access for a resource](/iam/docs/granting-roles-to-service-accounts#granting_access_to_a_service_account_for_a_resource) .\n### Set IAM policy permissions for the Cloud Storage bucket\nBy adding the service account `gcp-logging-sink-gcs@logging-o324989855333.iam.gserviceaccount.com` to the `gcp-logging-export-000100011000` bucket with Storage Object Creator permissions, you grant the service account permission to write to the bucket. Until you add these permissions, the sink export will fail.\nTo add the permissions to the `gcp-logging-export` bucket, follow these steps:\n- In the Google Cloud console, open Storage Browser: [GO TO STORAGE BROWSER](https://console.cloud.google.com/storage/browser) \n- Select the `gcp-logging-export` bucket.\n- Click **Show info panel** , and then select the Storage Object Creator permissions. \nAfter you create the logging export by using the preceding filter, log files will begin to populate the Cloud Storage buckets in the configured project.\n**Best practice:** You can implement a policy of least permissions based on your needs. You [configure the bucket permissions](/storage/docs/access-control/using-iam-permissions#bucket-add) based on specific Google Cloud user accounts, Google Groups, or Google Cloud service accounts. Use IAM permissions to grant access to the Cloud Storage bucket as well as bulk access to a bucket's objects.\nFor an example set of permissions, you can do the following:\n- Remove all nonessential users from the Cloud Storage bucket permissions.\n- Add full control for the Cloud Storage admin.\n- Grant the export user permissions to write the logging export files.\n- Grant other individual users viewing access to the Google Cloud logging exports.\nYou can update the IAM permissions for the bucket directly in the Google Cloud console, through the `gsutil` command-line tool, or through the IAM API. The following example shows a sample set of permissions and screenshots associated with a Cloud Storage bucket in the Google Cloud console.\n**Role:** Storage Admin\n- IAM description: Full control of Cloud Storage resources\n- Usage: Use this role to grant access to admin users for Cloud Storage resources without granting access to modify the contents stored in Cloud Storage.\n- Example account:`storage-admin@example.com` \n**Role:** Storage Object Admin\n- IAM description: Full control of Cloud Storage objects\n- Usage: Use this role to grant full access to admin users for Cloud Storage file objects without granting access to modify the Cloud Storage resource configuration.\n- Example account:`storage-object-admin@example.com: user1@example.com, user2@example.com` \n**Role:** Storage Object Viewer\n- IAM description: Read access to Cloud Storage objects\n- Usage: Use this role to grant read-only access to the Google Cloud logs for users.\n- Example account:`storage-viewer@example.com: user3@example.com` \n**Best practice:** If you use Google Workspace or consumer Google Groups, you can add a Google group such as `gcp-logging-export-viewers@example.com` with Storage Object Viewer permissions. You can then add or remove users to the `gcp-logging-export-viewers@example.com` group without having to edit the Cloud Storage bucket permissions for each change in user viewing permissions.\n## Using the exported logs\nAfter you create the logging export by using the filter above, log files begin to populate the Cloud Storage bucket in the configured project. Each log creates a separate folder in the bucket, which is broken down in a hierarchical structure based on date. You can access the logs through the Google Cloud console, the `gsutil` command-line tool, or the IAM API. The following screenshot shows an example folder structure in the Google Cloud console.\nEach log file consists of JSON data that follows the `textPayload` , `protoPayload` , and `jsonPayload` log entry formats. Over time, the log files in the Cloud Storage bucket are subject to the Cloud Storage lifecycle process, which first moves the logs to Nearline storage, then moves the logs to Coldline storage, and finally deletes the logs based on your configuration.\n### Granting external access\nYou might want to grant specific users access to exported logs\u2014for example, security analysts, your DevOps team, and auditors.\nThere are several options for granting access to the logs in Cloud Storage.\n- **Create copies of the logs to share.** Manually or programmatically create a copy of an individual log file or set of log files, and place the copies in a separate Cloud Storage bucket. Then use the separate bucket permissions to share the logs with specific users as appropriate. **Advantages** : You can limit the amount of data that is exposed to only the copied data. **Disadvantages** : You have to create, share, and manage the separate datasets and permissions, which can lead to higher costs.\n- **Grant read-only access to all logs.** Manually or programmatically set viewer permissions to the Cloud Storage logging export bucket, which grants access to all log exports. **Advantages** : Access is easy to grant. **Disadvantages** : You must grant access to all of the logs rather than specific log files.You can use Cloud Storage bucket permissions to share the Cloud Storage bucket of logging exports with specific Google Accounts or Google Groups.\n- **Use a Google Group.** Create a Google Group such as `auditors@example.com` with read-only access to the logging export Cloud Storage bucket. You then manage the list of Google Accounts by adding or removing auditors to the Google group. **Advantages** : It is easy to manage access through a group; there's a clear user-access purpose. **Disadvantages** : It's not possible to tell who has access without looking at the group's membership.\n- **Use individual Google Accounts.** Grant individual Google Account access to the logging export Cloud Storage bucket for each user who requires it. **Advantages** : It is easy to add each user manually or programmatically. **Disadvantages** : It is not possible to discern audit users from other viewers.## What's next\n- [Exporting logs to Elastic Cloud](/community/tutorials/exporting-stackdriver-elasticcloud) \n- Look at the other export scenarios:- [Scenario \u2013 Export for security and access analytics](/architecture/exporting-stackdriver-logging-for-security-and-access-analytics) \n- [Scenario \u2013 Export to Splunk](/architecture/exporting-stackdriver-logging-for-splunk) \n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}