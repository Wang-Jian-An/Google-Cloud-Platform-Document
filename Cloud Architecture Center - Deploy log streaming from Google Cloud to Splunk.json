{"title": "Cloud Architecture Center - Deploy log streaming from Google Cloud to Splunk", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Deploy log streaming from Google Cloud to Splunk\nLast reviewed 2023-11-16 UTC\nThis document describes how you deploy an export mechanism to stream logs from Google Cloud resources to Splunk. It assumes that you've already read the corresponding [referencearchitecture](/architecture/stream-logs-from-google-cloud-to-splunk) for this use case.\nThese instructions are intended for operations and security administrators who want to stream logs from Google Cloud to Splunk. You must be familiar with Splunk and the Splunk HTTP Event Collector (HEC) when using these instructions for IT operations or security use cases. Although not required, familiarity with Dataflow pipelines, Pub/Sub, Cloud Logging, Identity and Access Management, and Cloud Storage is useful for this deployment.\nTo automate the deployment steps in this reference architecture using infrastructure as code (IaC), see the [terraform-splunk-log-export](https://github.com/GoogleCloudPlatform/terraform-splunk-log-export) GitHub repository.\n", "content": "## Architecture\nThe following diagram shows the reference architecture and demonstrates how log data flows from Google Cloud to Splunk.\nAs shown in the diagram, Cloud Logging collects the logs into an organization-level log sink and sends the logs to Pub/Sub. The Pub/Sub service creates a single topic and subscription for the logs and forwards the logs to the main Dataflow pipeline. The main Dataflow pipeline is a Pub/Sub to Splunk streaming pipeline which pulls logs from the Pub/Sub subscription and delivers them to Splunk. Parallel to the primary Dataflow pipeline, the secondary Dataflow pipeline is a Pub/Sub to Pub/Sub streaming pipeline to replay messages if a delivery fails. At the end of the process, Splunk Enterprise or Splunk Cloud Platform acts as an HEC endpoint and receives the logs for further analysis. For more details, see the [Architecture](/architecture/stream-logs-from-google-cloud-to-splunk#architecture) section of the reference architecture.\nTo deploy this reference architecture, you perform the following tasks:\n- [Perform set up tasks.](#you-begin) \n- [Create an aggregated log sink in a dedicated project.](#create-log) \n- [Create a dead-letter topic.](#create-a-dead-letter-topic) \n- [Set up a Splunk HEC endpoint.](#set-splunk) \n- [Configure the Dataflow pipeline capacity.](#configure-pipeline-capacity) \n- [Export logs to Splunk.](#export-logs) \n- [Transform logs or events in-flight using user-defined functions (UDF) within the Splunk Dataflow pipeline.](#transform-events) \n- [Handle delivery failures to avoid data loss from potential misconfiguration or transient network issues.](#handle-delivery) ## Before you begin\nComplete the following steps to set up an environment for your Google Cloud to Splunk reference architecture:\n- [Bring up a project, enable billing, and activate the APIs.](#bring_up_a_project_enable_billing_and_activate_the_apis) \n- [Grant IAM roles.](#grant_roles) \n- [Set up your environment.](#set_up_your_environment) \n- [Set up secure networking.](#set_up_secure_networking) \n### Bring up a project, enable billing, and activate the APIs\n- In the Google Cloud console, on the project selector page,   select or [create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . **Note** : If you don't plan to keep the  resources that you create in this procedure, create a project instead of  selecting an existing project. After you finish these steps, you can  delete the project, removing all resources associated with the project. [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- Enable the Cloud Monitoring API, Secret Manager, Compute Engine, Pub/Sub, and Dataflow APIs. [Enable the APIs](https://console.cloud.google.com/flows/enableapi?apiid=monitoring.googleapis.com,secretmanager.googleapis.com,compute.googleapis.com,pubsub.googleapis.com,dataflow.googleapis.com) \n### Grant IAM roles\nIn the Google Cloud console, ensure that you have the following Identity and Access Management (IAM) permissions for organization and project resources. For more information, see [Granting, changing, and revoking access toresources](/iam/docs/granting-changing-revoking-access#viewing-console) .\n| Permissions                | Predefined roles                      | Resource  |\n|:---------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------|:-------------|\n| logging.sinks.create logging.sinks.get logging.sinks.update    | Logs Configuration Writer (roles/logging.configWriter)             | Organization |\n| compute.networks.* compute.routers.* compute.firewalls.* networkservices.* | Compute Network Admin (roles/compute.networkAdmin) Compute Security Admin (roles/compute.securityAdmin) | Project  |\n| secretmanager.*               | Secret Manager Admin (roles/secretmanager.admin)              | Project  |\nIf the predefined IAM roles don't include enough permissions for you to perform your duties, [create a customrole](/iam/docs/creating-custom-roles) . A custom role gives you the access that you need, while also helping you to follow the principle of least privilege.\n**Note:** To create the organization-level aggregated log sink, you must have the **Logs Configuration Writer** role assigned to you at the organization hierarchy level. If you don't have access to change permissions at the organization level, ask your Google Cloud organization administrator to [create the organization-wide log sink](#create-log) for you. For more information about Cloud Logging access control, see [Cloud Logging predefined roles](/logging/docs/access-control#permissions_and_roles) .\n### Set up your environment\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Set the project for your active Cloud Shell session:```\ngcloud config set project PROJECT_ID\n```Replace `` with your project ID.\n### Set up secure networking\nIn this step, you set up secure networking before processing and exporting logs to Splunk Enterprise.\n- Create a VPC network and a subnet:```\ngcloud compute networks create NETWORK_NAME --subnet-mode=custom\ngcloud compute networks subnets create SUBNET_NAME \\\n--network=NETWORK_NAME \\\n--region=REGION \\\n--range=192.168.1.0/24\n```Replace the following:- ``: the name for your network\n- ``: the name for your subnet\n- ``: the region that you want to use for this network\n- Create a firewall rule for Dataflow worker virtual machines (VMs) to communicate with one another:```\ngcloud compute firewall-rules create allow-internal-dataflow \\\n--network=NETWORK_NAME \\\n--action=allow \\\n--direction=ingress \\\n--target-tags=dataflow \\\n--source-tags=dataflow \\\n--priority=0 \\\n--rules=tcp:12345-12346\n```This rule allows internal traffic between Dataflow VMs which use TCP ports 12345-12346. Also, the Dataflow service sets the `dataflow` tag.\n- Create a Cloud NAT gateway:```\ngcloud compute routers create nat-router \\\n--network=NETWORK_NAME \\\n--region=REGION\ngcloud compute routers nats create nat-config \\\n--router=nat-router \\\n--nat-custom-subnet-ip-ranges=SUBNET_NAME \\\n--auto-allocate-nat-external-ips \\\n--region=REGION\n```\n- Enable Private Google Access on the subnet:```\ngcloud compute networks subnets update SUBNET_NAME \\\n--enable-private-ip-google-access \\\n--region=REGION\n```## Create a log sink\nIn this section, you create the organization-wide log sink and its Pub/Sub destination, along with the necessary permissions.\n- In Cloud Shell, create a Pub/Sub topic and associated subscription as your new log sink destination:```\ngcloud pubsub topics create INPUT_TOPIC_NAME\ngcloud pubsub subscriptions create \\\n--topic INPUT_TOPIC_NAME INPUT_SUBSCRIPTION_NAME\n```Replace the following:- ``: the name for the Pub/Sub topic to be used as the log sink destination\n- ``: the name for the Pub/Sub subscription to the log sink destination\n- Create the organization log sink:```\ngcloud logging sinks create ORGANIZATION_SINK_NAME \\\npubsub.googleapis.com/projects/PROJECT_ID/topics/INPUT_TOPIC_NAME \\\n--organization=ORGANIZATION_ID \\\n--include-children \\\n--log-filter='NOT logName:projects/PROJECT_ID/logs/dataflow.googleapis.com'\n```Replace the following:- ``: the name of your organization\n- ``: your organization ID\nThe command consists of the following flags:- The`--organization`flag specifies that this is an organization-level log sink.\n- The`--include-children`flag is required and ensures that the organization-level log sink includes all logs across all subfolders and projects.\n- The`--log-filter`flag specifies the logs to be routed. In this example, you exclude Dataflow operations logs specifically for the project``, because the log export Dataflow pipeline generates more logs itself as it processes logs. The filter prevents the pipeline from exporting its own logs, avoiding a potentially exponential cycle. The output includes a service account in the form of`o\n##\n###-\n##\n##@gcp-sa-logging.iam.gserviceaccount.com`.\n- Grant the Pub/Sub Publisher IAM role to the log sink service account on the Pub/Sub topic `` . This role allows the log sink service account to publish messages on the topic.```\ngcloud pubsub topics add-iam-policy-binding INPUT_TOPIC_NAME \\\n--member=serviceAccount:LOG_SINK_SERVICE_ACCOUNT@PROJECT_ID.iam.gserviceaccount.com \\\n--role=roles/pubsub.publisher\n```Replace `` with the name of the service account for your log sink.## Create a dead-letter topic\nTo prevent potential data loss that results when a message fails to be delivered, you should create a Pub/Sub [dead-letter topic](https://wikipedia.org/wiki/Dead_letter_queue) and corresponding subscription. The failed message is stored in the dead-letter topic until an operator or site reliability engineer can investigate and correct the failure. For more information, see [Replay failed messages](/architecture/stream-logs-from-google-cloud-to-splunk#replay_unprocessed_messages) section of the reference architecture.\n- In Cloud Shell, create a Pub/Sub dead-letter topic and subscription to prevent data loss by storing any undeliverable messages:```\ngcloud pubsub topics create DEAD_LETTER_TOPIC_NAME\ngcloud pubsub subscriptions create --topic DEAD_LETTER_TOPIC_NAME DEAD_LETTER_SUBSCRIPTION_NAME\n```Replace the following:- ``: the name for the Pub/Sub topic that will be the dead-letter topic\n- ``: the name for the Pub/Sub subscription for the dead-letter topic\n## Set up a Splunk HEC endpoint\nIn the following procedures, you set up a Splunk HEC endpoint and store the newly created HEC token as a secret in Secret Manager. When you deploy the Splunk Dataflow pipeline, you need to supply both the endpoint URL and the token.\n### Configure the Splunk HEC\n- If you don't already have a Splunk HEC endpoint, see the Splunk documentation to learn [how to configure a SplunkHEC](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector) . Splunk HEC runs on the [Splunk Cloud Platformservice](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector#Configure_HTTP_Event_Collector_on_Splunk_Cloud_Platform) or on your own [Splunk Enterpriseinstance](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector#Configure_HTTP_Event_Collector_on_Splunk_Enterprise) .\n- In Splunk, after you create a Splunk HEC token, copy the token value.\n- In Cloud Shell, save the Splunk HEC token value in a temporary file named`splunk-hec-token-plaintext.txt`.\n**Note:** For high availability and to learn how to scale with high-volume traffic, see the Splunk docs on [how to scaleHEC](https://docs.splunk.com/Documentation/Splunk/latest/Data/ScaleHTTPEventCollector) by distributing load to multiple nodes using an HTTP(S) load balancer.\n### Store the Splunk HEC token in Secret Manager\nIn this step, you create a secret and a single underlying secret version in which to store the Splunk HEC token value.\n**Note:** Secret Manager is supported as of [Pub/Sub to Splunk Dataflowtemplate](/dataflow/docs/guides/templates/provided/pubsub-to-splunk) version **2022-03-14-00_RC00** . This reference architecture uses the latest version of this template.\n- In Cloud Shell, create a secret to contain your Splunk HEC token:```\ngcloud secrets create hec-token \\\n --replication-policy=\"automatic\"\n```For more information on the replication policies for secrets, see [Choose areplication policy](/secret-manager/docs/choosing-replication) .\n- Add the token as a secret version using the contents of the file `splunk-hec-token-plaintext.txt` :```\ngcloud secrets versions add hec-token \\\n --data-file=\"./splunk-hec-token-plaintext.txt\"\n```\n- Delete the `splunk-hec-token-plaintext.txt` file, as it is no longer needed.## Configure the Dataflow pipeline capacity\n**Note:** Before proceeding, make sure you have enough CPU and IP quota in your chosen region to provision the maximum possible number of Dataflow VMs (set with the `--max-workers` flag). For more information about how to check and request an increase in quotas, see [Allocation quotas](/compute/quotas) .\nThe following table summarizes the recommended general best practices for configuring the Dataflow pipeline capacity settings:\n| Setting     | General best practice                        |\n|:---------------------------|:-------------------------------------------------------------------------------------------------------------------|\n| --worker-machine-type flag | Set to baseline machine size n1-standard-4 for the best performance to cost ratio         |\n| --max-workers flag   | Set to the maximum number of workers needed to handle the expected peak EPS per your calculations     |\n| parallelism parameter  | Set to 2 x vCPUs/worker x the maximum number of workers to maximize the number of parallel Splunk HEC connections |\n| batchCount parameter  | Set to 10-50 events/request for logs, provided that the max buffering delay of two seconds is acceptable   |\nRemember to use your own unique values and calculations when you deploy this reference architecture in your environment.\n- Set the values for machine type and machine count. To calculate values appropriate for your cloud environment, see [Machinetype](/architecture/stream-logs-from-google-cloud-to-splunk#machine_type) and [Machinecount](/architecture/stream-logs-from-google-cloud-to-splunk#machine_count) sections of the reference architecture.```\nDATAFLOW_MACHINE_TYPE\nDATAFLOW_MACHINE_COUNT\n```\n- Set the values for Dataflow parallelism and batch count. To calculate values appropriate for your cloud environment, see the [Parallelism](/architecture/stream-logs-from-google-cloud-to-splunk#parallelism) and [Batchcount](/architecture/stream-logs-from-google-cloud-to-splunk#batch_count) sections of the reference architecture.```\nJOB_PARALLELISM\nJOB_BATCH_COUNT\n```\nFor more information on how to calculate Dataflow pipeline capacity parameters, see the [Performance and cost optimization designconsiderations](/architecture/stream-logs-from-google-cloud-to-splunk#performance-and) section of the reference architecture.\n## Export logs by using the Dataflow pipeline\nIn this section, you deploy the Dataflow pipeline with the following steps:\n- [Create a Cloud Storage bucket and Dataflow worker service account.](#create-a-bucket-and-worker-service-account) \n- [Grant roles and access to the Dataflow worker service account.](#grant-roles-and-access-to-the-worker-service-account) \n- [Deploy the Dataflow pipeline.](#deploy-the-pipeline) \n- [View logs in Splunk.](#view-logs-in-splunk) \nThe pipeline delivers Google Cloud log messages to the Splunk HEC.\n### Create a Cloud Storage bucket and Dataflow worker service account\n- In Cloud Shell, create a new Cloud Storage bucket with a uniform bucket-level access setting:```\ngsutil mb -b on gs://PROJECT_ID-dataflow/\n```The Cloud Storage bucket that you just created is where the Dataflow job stages temporary files.\n- In Cloud Shell, create a service account for your Dataflow workers:```\ngcloud iam service-accounts create WORKER_SERVICE_ACCOUNT \\\n --description=\"Worker service account to run Splunk Dataflow jobs\" \\\n --display-name=\"Splunk Dataflow Worker SA\"\n```Replace `` with the name that you want to use for the Dataflow worker service account.\n### Grant roles and access to the Dataflow worker service account\nIn this section, grant the required roles to the Dataflow worker service account as shown in the following table.\n| Role       | Path        | Purpose                         |\n|:-------------------------------|:-----------------------------------|:----------------------------------------------------------------------------------------------------------|\n| Dataflow Admin     | roles/dataflow.worker    | Enable the service account to act as a Dataflow admin.             |\n| Dataflow Worker    | roles/dataflow.worker    | Enable the service account to act as a Dataflow worker.             |\n| Storage Object Admin   | roles/storage.objectAdmin   | Enable the service account to access the Cloud Storage bucket that is used by Dataflow for staging files. |\n| Pub/Sub Publisher    | roles/pubsub.publisher    | Enable the service account to publish failed messages to the Pub/Sub dead-letter topic.     |\n| Pub/Sub Subscriber    | roles/pubsub.subscriber   | Enable the service account to access the input subscription.            |\n| Pub/Sub Viewer     | roles/pubsub.viewer    | Enable the service account to view the subscription.              |\n| Secret Manager Secret Accessor | roles/secretmanager.secretAccessor | Enable the service account to access the secret that contains the Splunk HEC token.      |\n- In Cloud Shell, grant the Dataflow worker service account the Dataflow Admin and Dataflow Worker roles that this account needs to execute Dataflow job operations and administration tasks:```\ngcloud projects add-iam-policy-binding PROJECT_ID \\\n --member=\"serviceAccount:WORKER_SERVICE_ACCOUNT@PROJECT_ID.iam.gserviceaccount.com\" \\\n --role=\"roles/dataflow.admin\"\n``````\ngcloud projects add-iam-policy-binding PROJECT_ID \\\n --member=\"serviceAccount:WORKER_SERVICE_ACCOUNT@PROJECT_ID.iam.gserviceaccount.com\" \\\n --role=\"roles/dataflow.worker\"\n```\n- Grant the Dataflow worker service account access to view and consume messages from the Pub/Sub input subscription:```\ngcloud pubsub subscriptions add-iam-policy-binding INPUT_SUBSCRIPTION_NAME \\\n --member=\"serviceAccount:WORKER_SERVICE_ACCOUNT@PROJECT_ID.iam.gserviceaccount.com\" \\\n --role=\"roles/pubsub.subscriber\"\n``````\ngcloud pubsub subscriptions add-iam-policy-binding INPUT_SUBSCRIPTION_NAME \\\n --member=\"serviceAccount:WORKER_SERVICE_ACCOUNT@PROJECT_ID.iam.gserviceaccount.com\" \\\n --role=\"roles/pubsub.viewer\"\n```\n- Grant the Dataflow worker service account access to publish any failed messages to the Pub/Sub unprocessed topic:```\ngcloud pubsub topics add-iam-policy-binding DEAD_LETTER_TOPIC_NAME \\\n --member=\"serviceAccount:WORKER_SERVICE_ACCOUNT@PROJECT_ID.iam.gserviceaccount.com\" \\\n --role=\"roles/pubsub.publisher\"\n```\n- Grant the Dataflow worker service account access to the Splunk HEC token secret in Secret Manager:```\ngcloud secrets add-iam-policy-binding hec-token \\\n--member=\"serviceAccount:WORKER_SERVICE_ACCOUNT@PROJECT_ID.iam.gserviceaccount.com\" \\\n--role=\"roles/secretmanager.secretAccessor\"\n```\n- Grant the Dataflow worker service account read and write access to the Cloud Storage bucket to be used by the Dataflow job for staging files:```\ngcloud storage buckets add-iam-policy-binding gs://PROJECT_ID-dataflow/ \\\n--member=\"serviceAccount:WORKER_SERVICE_ACCOUNT@PROJECT_ID.iam.gserviceaccount.com\"\n--role=\u201droles/storage.objectAdmin\u201d\n```\n### Deploy the Dataflow pipeline\n- In Cloud Shell, set the following environment variable for your Splunk HEC URL:```\nexport SPLUNK_HEC_URL=SPLUNK_HEC_URL\n```Replace the `` variable using the form `protocol://host[:port` ], where:- `protocol`is either`http`or`https`.\n- `host`is the [fully qualified domain name(FQDN)](https://wikipedia.org/wiki/Fully_qualified_domain_name) or IP address of either your Splunk HEC instance, or, if you have multiple HEC instances, the associated HTTP(S) (or DNS-based) load balancer.\n- `port`is the HEC port number. It is optional, and depends on your Splunk HEC endpoint configuration.\nAn example of a valid Splunk HEC URL input is `https://splunk-hec.example.com:8088` . If you are sending data to HEC on Splunk Cloud Platform, see [Send data to HEC on SplunkCloud](https://docs.splunk.com/Documentation/Splunk/latest/Data/UsetheHTTPEventCollector#Send_data_to_HTTP_Event_Collector_on_Splunk_Cloud_Platform) to determine the above `host` and `port` portions of your specific Splunk HEC URL. **Caution:** Failure to match the Splunk HEC URL format described in this step will cause Dataflow input validation errors, and the data will not be delivered to the Splunk HEC endpoint. Instead, your streaming logs will be sent to the unprocessed topic.The Splunk HEC URL must not include the HEC endpoint path, for example, `/services/collector` . The Pub/Sub to Splunk Dataflow template currently only supports the `/services/collector` endpoint for JSON-formatted events, and it automatically appends that path to your Splunk HEC URL input. To learn more about the HEC endpoint, see the Splunk documentation for [services/collector endpoint](https://docs.splunk.com/Documentation/Splunk/latest/RESTREF/RESTinput#services.2Fcollector) .\n- Deploy the Dataflow pipeline using the Pub/Sub to Splunk Dataflow template:```\ngcloud beta dataflow jobs run JOB_NAME \\\n--gcs-location=gs://dataflow-templates/latest/Cloud_PubSub_to_Splunk \\\n--staging-location=gs://PROJECT_ID-dataflow/temp/ \\\n--worker-machine-type=DATAFLOW_MACHINE_TYPE \\\n--max-workers=DATAFLOW_MACHINE_COUNT \\\n--region=REGION \\\n--network=NETWORK_NAME \\\n--subnetwork=regions/REGION/subnetworks/SUBNET_NAME \\\n--disable-public-ips \\\n--parameters \\\ninputSubscription=projects/PROJECT_ID/subscriptions/INPUT_SUBSCRIPTION_NAME,\\\noutputDeadletterTopic=projects/PROJECT_ID/topics/DEAD_LETTER_TOPIC_NAME,\\\nurl=SPLUNK_HEC_URL,\\\ntokenSource=SECRET_MANAGER, \\\ntokenSecretId=projects/PROJECT_ID/secrets/hec-token/versions/1, \\\nbatchCount=JOB_BATCH_COUNT,\\\nparallelism=JOB_PARALLELISM,\\\njavascriptTextTransformGcsPath=gs://splk-public/js/dataflow_udf_messages_replay.js,\\\njavascriptTextTransformFunctionName=process\n```Replace `` with the name format `pubsub-to-splunk-date+\"%Y%m%d-%H%M%S\"`The optional parameters `javascriptTextTransformGcsPath` and `javascriptTextTransformFunctionName` specify a publicly available sample UDF: `gs://splk-public/js/dataflow_udf_messages_replay.js` . The sample UDF includes code examples for event transformation and decoding logic that you use to replay failed deliveries. For more information about UDF, see [Transform events in-flight with UDF](#transform-events) .\n- After the pipeline job completes, find the new job ID in the output, copy the job ID, and save it. You enter this job ID in a later step.\n### View logs in Splunk\nIt takes a few minutes for the Dataflow pipeline workers to be provisioned and ready to deliver logs to Splunk HEC. You can confirm that the logs are properly received and indexed in the Splunk Enterprise or Splunk Cloud Platform search interface. To see the number of logs per type of monitored resource:\n- In Splunk, open [Splunk Search &Reporting](https://docs.splunk.com/Documentation/Splunk/latest/Search/WhatsinSplunkSearch) .\n- Run the search `index=[MY_INDEX] | stats count by resource.type` where the `MY_INDEX` index is configured for your Splunk HEC token:\n- If you don't see any events, see [Handle delivery failures](#handle-delivery) .## Transform events in-flight with UDF\nThe Pub/Sub to Splunk Dataflow template supports a JavaScript UDF for custom event transformation, such as adding new fields or setting Splunk HEC metadata on an event basis. The pipeline you deployed uses this [sample UDF](https://storage.googleapis.com/splk-public/js/dataflow_udf_messages_replay.js) .\nIn this section, you first edit the sample UDF function to add a new event field. This new field specifies the value of the originating Pub/Sub subscription as additional contextual information. You then update the Dataflow pipeline with the modified UDF.\n### Modify the sample UDF\n- In Cloud Shell, download the JavaScript file that contains the [sample UDF function](https://storage.googleapis.com/splk-public/js/dataflow_udf_messages_replay.js) :```\n wget https://storage.googleapis.com/splk-public/js/dataflow_udf_messages_replay.js\n \n```\n- In the text editor of your choice, open the JavaScript file, locate the field `event.inputSubscription` , uncomment that line and replace `splunk-dataflow-pipeline` with `` :```\nevent.inputSubscription = \"INPUT_SUBSCRIPTION_NAME\";\n```\n- Save the file.\n- Upload the file to the Cloud Storage bucket:```\ngsutil cp ./dataflow_udf_messages_replay.js gs://PROJECT_ID-dataflow/js/\n```\n### Update the Dataflow pipeline with the new UDF\n- In Cloud Shell, stop the pipeline by using the [Drainoption](/dataflow/docs/guides/stopping-a-pipeline#drain) to ensure that the logs which were already pulled from Pub/Sub are not lost:```\ngcloud dataflow jobs drain JOB_ID --region=REGION\n```\n- Run the Dataflow pipeline job with the updated UDF.```\ngcloud beta dataflow jobs run JOB_NAME \\\n--gcs-location=gs://dataflow-templates/latest/Cloud_PubSub_to_Splunk \\\n--worker-machine-type=DATAFLOW_MACHINE_TYPE \\\n--max-workers=DATAFLOW_MACHINE_COUNT \\\n--region=REGION \\\n--network=NETWORK_NAME \\\n--subnetwork=regions/REGION/subnetworks/SUBNET_NAME \\\n--disable-public-ips \\\n--parameters \\\ninputSubscription=projects/PROJECT_ID/subscriptions/INPUT_SUBSCRIPTION_NAME,\\\noutputDeadletterTopic=projects/PROJECT_ID/topics/DEAD_LETTER_TOPIC_NAME,\\\nurl=SPLUNK_HEC_URL,\\\ntokenSource=SECRET_MANAGER, \\\ntokenSecretId=projects/PROJECT_ID/secrets/hec-token/versions/1, \\\nbatchCount=JOB_BATCH_COUNT,\\\nparallelism=JOB_PARALLELISM,\\\njavascriptTextTransformGcsPath=gs://PROJECT_ID-dataflow/js/dataflow_udf_messages_replay.js,\\\njavascriptTextTransformFunctionName=process\n```Replace `` with the name format `pubsub-to-splunk-date+\"%Y%m%d-%H%M%S\"`## Handle delivery failures\nDelivery failures can happen due to errors in processing events or connecting to the Splunk HEC. In this section, you introduce delivery failure to demonstrate the error handling workflow. You also learn how to view and trigger the re-delivery of the failed messages to Splunk.\n### Trigger delivery failures\nTo introduce a delivery failure manually in Splunk, do one of the following:\n- If you run a single instance, stop the Splunk server to cause connection errors.\n- Disable the relevant HEC token from your Splunk input configuration.\n### Troubleshoot failed messages\nTo investigate a failed message, you can use the Google Cloud console:\n- In the Google Cloud console, go to the **Pub/Sub Subscriptions** page. [  Go to Pub/Sub Subscriptions](https://console.cloud.google.com/cloudpubsub/subscription) \n- Click the unprocessed subscription that you created. If you used the previous example, the subscription name is: `projects/` `` `/subscriptions/` `` .\n- To open the messages viewer, click **View Messages** .\n- To view messages, click **Pull** , making sure to leave **Enable ackmessages** cleared.\n- Inspect the failed messages. Pay attention to the following:- The Splunk event payload under the`Message body`column.\n- The error message under the`attribute.errorMessage`column.\n- The error timestamp under the`attribute.timestamp`column.The following screenshot shows an example of a failure message that you receive if the Splunk HEC endpoint is temporarily down or unreachable. Notice that the text of the `errorMessage` attribute reads `The target server failed to respond` . The message also shows the timestamp that is associated with each failure. You can use this timestamp to troubleshoot the root cause of the failure.### Replay failed messages\nIn this section, you need to restart the Splunk server or enable the Splunk HEC endpoint to fix the delivery error. You can then replay the unprocessed messages.\n- In Splunk, use one of the following methods to restore the connection to Google Cloud:- If you stopped the Splunk server, restart the server.\n- If you disabled the Splunk HEC endpoint in the [Trigger delivery failures](#trigger_delivery_failures) section, check that the Splunk HEC endpoint is now operating.\n- In Cloud Shell, take a snapshot of the unprocessed subscription before re-processing the messages in this subscription. The snapshot prevents the loss of messages if there's an unexpected configuration error.```\ngcloud pubsub snapshots create SNAPSHOT_NAME \\\n--subscription=DEAD_LETTER_SUBSCRIPTION_NAME\n```Replace `` with a name that helps you identify the snapshot, such as `dead-letter-snapshot-date+\"%Y%m%d-%H%M%S` .\n- Use the Pub/Sub to Splunk Dataflow template to create a Pub/Sub to Pub/Sub pipeline. The pipeline uses another Dataflow job to transfer the messages from the unprocessed subscription back to the input topic.```\nDATAFLOW_INPUT_TOPIC=\"INPUT_TOPIC_NAME\"\nDATAFLOW_DEADLETTER_SUB=\"DEAD_LETTER_SUBSCRIPTION_NAME\"\nJOB_NAME=splunk-dataflow-replay-date +\"%Y%m%d-%H%M%S\"\ngcloud dataflow jobs run JOB_NAME \\\n--gcs-location= gs://dataflow-templates/latest/Cloud_PubSub_to_Cloud_PubSub \\\n--worker-machine-type=n1-standard-2 \\\n--max-workers=1 \\\n--region=REGION \\\n--parameters \\\ninputSubscription=projects/PROJECT_ID/subscriptions/DEAD_LETTER_SUBSCRIPTION_NAME,\\\noutputTopic=projects/PROJECT_ID/topics/INPUT_TOPIC_NAME\n```\n- Copy the Dataflow job ID from the command output and save it for later. You'll enter this job ID as `` when you drain your Dataflow job.\n- In the Google Cloud console, go to the **Pub/Sub Subscriptions** page. [  Go to Pub/Sub Subscriptions](https://console.cloud.google.com/cloudpubsub/subscription) \n- Select the unprocessed subscription. Confirm that the **Unacked messagecount** graph is down to 0, as shown in the following screenshot.\n- In Cloud Shell, drain the Dataflow job that you created:```\ngcloud dataflow jobs drain REPLAY_JOB_ID --region=REGION\n```Replace `` with the Dataflow job ID you saved earlier.\nWhen messages are transferred back to the original input topic, the main Dataflow pipeline automatically picks up the failed messages and re-delivers them to Splunk.\n### Confirm messages in Splunk\n- To confirm that the messages have been re-delivered, in Splunk, open [Splunk Search &Reporting](https://docs.splunk.com/Documentation/Splunk/latest/Search/WhatsinSplunkSearch) .\n- Run a search for `delivery_attempts > 1` . This is a special field that the sample UDF adds to each event to track the number of delivery attempts. Make sure to expand the search time range to include events that may have occurred in the past, because the event timestamp is the original time of creation, not the time of indexing.\nIn the following screenshot, the two messages that originally failed are now successfully delivered and indexed in Splunk with the correct timestamp.\nNotice that the `insertId` field value is the same as the value that appears in the failed messages when you view the unprocessed subscription. The `insertId` field is a unique identifier that Cloud Logging assigns to the original log entry.. The `insertId` also appears in the Pub/Sub message body.\n## Clean up\nTo avoid incurring charges to your Google Cloud account for the resources used in this reference architecture, either delete the project that contains the resources, or keep the project and delete the individual resources.\n### Delete the organization-level sink\n- Use the following command to delete the organization-level log sink:```\ngcloud logging sinks delete ORGANIZATION_SINK_NAME --organization=ORGANIZATION_ID\n```\n### Delete the project\nWith the log sink deleted, you can proceed with deleting resources created to receive and export logs. The easiest way is to delete the project you created for the reference architecture.\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- For a full list of Pub/Sub to Splunk Dataflow template parameters, see the [Pub/Sub to Splunk Dataflowdocumentation](/dataflow/docs/guides/templates/provided/pubsub-to-splunk) .\n- For the corresponding Terraform templates to help you deploy this reference architecture, see the [terraform-splunk-log-export](https://github.com/GoogleCloudPlatform/terraform-splunk-log-export) GitHub repository. It includes a pre-built Cloud Monitoring dashboard for monitoring your Splunk Dataflow pipeline.\n- For more details on Splunk Dataflow custom metrics and logging to help you monitor and troubleshoot your Splunk Dataflow pipelines, refer to this blog [New observabilityfeatures for your Splunk Dataflow streamingpipelines](/blog/products/data-analytics/simplify-your-splunk-dataflow-ops-with-improved-pipeline-observability) .\n- For more reference architectures, diagrams, and best practices, explore the [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}