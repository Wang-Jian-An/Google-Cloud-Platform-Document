{"title": "Documentation - Package google.cloud.vision.v1", "url": "https://cloud.google.com/distributed-cloud/hosted/docs/latest/gdch/overview", "abstract": "# Documentation - Package google.cloud.vision.v1\n", "content": "## Index\n- ` [ImageAnnotator](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.ImageAnnotator) `(interface)\n- ` [AnnotateFileRequest](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.AnnotateFileRequest) `(message)\n- ` [AnnotateFileResponse](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.AnnotateFileResponse) `(message)\n- ` [AnnotateImageRequest](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.AnnotateImageRequest) `(message)\n- ` [AnnotateImageResponse](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.AnnotateImageResponse) `(message)\n- ` [BatchAnnotateFilesRequest](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.BatchAnnotateFilesRequest) `(message)\n- ` [BatchAnnotateFilesResponse](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.BatchAnnotateFilesResponse) `(message)\n- ` [BatchAnnotateImagesRequest](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.BatchAnnotateImagesRequest) `(message)\n- ` [BatchAnnotateImagesResponse](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.BatchAnnotateImagesResponse) `(message)\n- ` [Block](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.Block) `(message)\n- ` [Block.BlockType](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.Block.BlockType) `(enum)\n- ` [BoundingPoly](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.BoundingPoly) `(message)\n- ` [EntityAnnotation](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.EntityAnnotation) `(message)\n- ` [Feature](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.Feature) `(message)\n- ` [Feature.Type](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.Feature.Type) `(enum)\n- ` [Image](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.Image) `(message)\n- ` [ImageAnnotationContext](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.ImageAnnotationContext) `(message)\n- ` [ImageContext](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.ImageContext) `(message)\n- ` [InputConfig](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.InputConfig) `(message)\n- ` [NormalizedVertex](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.NormalizedVertex) `(message)\n- ` [Page](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.Page) `(message)\n- ` [Paragraph](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.Paragraph) `(message)\n- ` [Property](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.Property) `(message)\n- ` [Symbol](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.Symbol) `(message)\n- ` [TextAnnotation](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.TextAnnotation) `(message)\n- ` [TextAnnotation.DetectedBreak](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.TextAnnotation.DetectedBreak) `(message)\n- ` [TextAnnotation.DetectedBreak.BreakType](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.TextAnnotation.DetectedBreak.BreakType) `(enum)\n- ` [TextAnnotation.DetectedLanguage](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.TextAnnotation.DetectedLanguage) `(message)\n- ` [TextAnnotation.TextProperty](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.TextAnnotation.TextProperty) `(message)\n- ` [TextDetectionParams](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.TextDetectionParams) `(message)\n- ` [Vertex](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.Vertex) `(message)\n- ` [Word](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.Word) `(message)\n## ImageAnnotator\nService that performs Google Cloud Vision API detection tasks over client images, such as face, landmark, logo, label, and text detection. The ImageAnnotator service returns detected entities from the images.| BatchAnnotateFiles                                                                                                                                                    |\n|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| rpc BatchAnnotateFiles(BatchAnnotateFilesRequest) returns (BatchAnnotateFilesResponse) Service that performs image detection and annotation for a batch of files. Now only \"application/pdf\", \"image/tiff\" and \"image/gif\" are supported.This service will extract at most 5 (customers can specify which 5 in AnnotateFileRequest.pages) frames (gif) or pages (pdf or tiff) from each file provided and perform detection and annotation for each image extracted. Authorization Scopes Requires one of the following OAuth scopes: https://www.googleapis.com/auth/cloud-platform https://www.googleapis.com/auth/cloud-vision || BatchAnnotateImages                                                                        |\n|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| rpc BatchAnnotateImages(BatchAnnotateImagesRequest) returns (BatchAnnotateImagesResponse) Run image detection and annotation for a batch of images. Authorization Scopes Requires one of the following OAuth scopes: https://www.googleapis.com/auth/cloud-platform https://www.googleapis.com/auth/cloud-vision |\n## AnnotateFileRequest\nA request to annotate one single file, e.g. a PDF, TIFF or GIF file.| Fields  | Fields.1                                                                                                                          |\n|:--------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| input_config | InputConfig Required. Information about the input file.                                                                                                              |\n| features[] | Feature Required. Requested features.                                                                                                                  |\n| image_context | ImageContext Additional context that may accompany the image(s) in the file.                                                                                                         |\n| pages[]  | int32 Pages of the file to perform image annotation.Pages starts from 1, we assume the first page of the file is page 1. At most 5 pages are supported per request. Pages can be negative.Page 1 means the first page. Page 2 means the second page. Page -1 means the last page. Page -2 means the second to the last page.If the file is GIF instead of PDF or TIFF, page refers to GIF frames.If this field is empty, by default the service performs image annotation for the first 5 pages of the file. |\n## AnnotateFileResponse\nResponse to a single file annotation request. A file may contain one or more images, which individually have their own responses.| Fields  | Fields.1                              |\n|:-------------|:--------------------------------------------------------------------------------------------------------------------------------|\n| input_config | InputConfig Information about the file for which this response is generated.             |\n| responses[] | AnnotateImageResponse Individual responses to images found within the file. This field will be empty if the error field is set. |\n| total_pages | int32 This field gives the total number of pages in the file.                 |\n| error  | Status If set, represents the error message for the failed request. The responses field will not be set in this case.   |\n## AnnotateImageRequest\nRequest for performing Google Cloud Vision API tasks over a user-provided image, with user-requested features, and with context information.| Fields  | Fields.1              |\n|:--------------|:--------------------------------------------------------------|\n| image   | Image The image to be processed.        |\n| features[] | Feature Requested features.         |\n| image_context | ImageContext Additional context that may accompany the image. |\n## AnnotateImageResponse\nResponse to an image annotation request.| Fields    | Fields.1                                             |\n|:---------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| text_annotations[] | EntityAnnotation If present, text (OCR) detection has completed successfully.                            |\n| full_text_annotation | TextAnnotation If present, text (OCR) detection or document (OCR) text detection has completed successfully. This annotation provides the structural hierarchy for the OCR detected text. |\n| error    | Status If set, represents the error message for the operation. Note that filled-in image annotations are guaranteed to be correct, even when error is set.        |\n| context    | ImageAnnotationContext If present, contextual information is needed to understand where this image comes from.                   |\n## BatchAnnotateFilesRequest\nA list of requests to annotate files using the BatchAnnotateFiles API.| Fields  | Fields.1                                                                                  |\n|:-----------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| requests[] | AnnotateFileRequest Required. The list of file annotation requests. Right now we support only one AnnotateFileRequest in BatchAnnotateFilesRequest.                                               |\n| parent  | string Optional. Target project and location to make a call.Format: projects/{project-id}/locations/{location-id}.If no parent is specified, a region will be chosen automatically.Supported location-ids: us: USA country only, asia: East asia areas, like Japan, Taiwan, eu: The European Union.Example: projects/project-A/locations/eu. |\n## BatchAnnotateFilesResponse\nA list of file annotation responses.| Fields  | Fields.1                                   |\n|:------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|\n| responses[] | AnnotateFileResponse The list of file annotation responses, each response corresponding to each AnnotateFileRequest in BatchAnnotateFilesRequest. |\n## BatchAnnotateImagesRequest\nMultiple image annotation requests are batched into a single service call.| Fields  | Fields.1                                                                                  |\n|:-----------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| requests[] | AnnotateImageRequest Required. Individual image annotation requests for this batch.                                                               |\n| parent  | string Optional. Target project and location to make a call.Format: projects/{project-id}/locations/{location-id}.If no parent is specified, a region will be chosen automatically.Supported location-ids: us: USA country only, asia: East asia areas, like Japan, Taiwan, eu: The European Union.Example: projects/project-A/locations/eu. |\n## BatchAnnotateImagesResponse\nResponse to a batch image annotation request.| Fields  | Fields.1                     |\n|:------------|:------------------------------------------------------------------------------------------|\n| responses[] | AnnotateImageResponse Individual responses to image annotation requests within the batch. |\n## Block\nLogical element on the page.| Fields  | Fields.1                                                                                                                               |\n|:-------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| property  | TextProperty Additional information detected for the block.                                                                                                                   |\n| bounding_box | BoundingPoly The bounding box for the block. The vertices are in the order of top-left, top-right, bottom-right, bottom-left. When a rotation of the bounding box is detected the rotation is represented as around the top-left corner as defined when the text is read in the 'natural' orientation. For example: when the text is horizontal it might look like: 0----1 | | 3----2 when it's rotated 180 degrees around the top-left corner it becomes: 2----3 | | 1----0 and the vertex order will still be (0, 1, 2, 3). |\n| paragraphs[] | Paragraph List of paragraphs in this block (if this blocks is of type text).                                                                                                              |\n| block_type | BlockType Detected block type (text, image etc) for this block.                                                                                                                  |\n| confidence | float Confidence of the OCR results on the block. Range [0, 1].                                                                                                                  |\n## BlockType\nType of a block (text, image etc) as identified by OCR.| Enums | Enums.1      |\n|:--------|:------------------------------|\n| UNKNOWN | Unknown block type.   |\n| TEXT | Regular text block.   |\n| TABLE | Table block.     |\n| PICTURE | Image block.     |\n| RULER | Horizontal/vertical line box. |\n| BARCODE | Barcode block.    |\n## BoundingPoly\nA bounding polygon for the detected image annotation.| Fields    | Fields.1             |\n|:----------------------|:-----------------------------------------------------------|\n| vertices[]   | Vertex The bounding polygon vertices.      |\n| normalized_vertices[] | NormalizedVertex The bounding polygon normalized vertices. |\n## EntityAnnotation\nSet of detected entity features.| Fields     | Fields.1                                                                                    |\n|:------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| mid      | string Opaque entity ID. Some IDs may be available in Google Knowledge Graph Search API.                                                                |\n| locale     | string The language code for the locale in which the entity textual description is expressed.                                                               |\n| description    | string Entity textual description, expressed in its locale language.                                                                     |\n| score     | float Overall score of the result. Range [0, 1].                                                                          |\n| confidence (deprecated) | float This item is deprecated! Deprecated. Use score instead. The accuracy of the entity detection in an image. For example, for an image in which the \"Eiffel Tower\" entity is detected, this field represents the confidence that there is a tower in the query image. Range [0, 1].                 |\n| topicality    | float The relevancy of the ICA (Image Content Annotation) label to the image. For example, the relevancy of \"tower\" is likely higher to an image containing the detected \"Eiffel Tower\" than to an image containing a detected distant towering building, even though the confidence that there is a tower in each image may be the same. Range [0, 1]. |\n| bounding_poly   | BoundingPoly Image region to which this entity belongs. Not produced for LABEL_DETECTION features.                                                              |\n| properties[]   | Property Some entities may have optional user-supplied Property (name/value) fields, such a score or string that qualifies the entity.                                                     |\n## Feature\nThe type of Google Cloud Vision API detection to perform, and the maximum number of results to return for that type. Multiple `Feature` objects can be specified in the `features` list.| Fields | Fields.1                                                         |\n|:---------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| type  | Type The feature type.                                                      |\n| model | string Model to use for the feature. Supported values: \"builtin/stable\" (the default if unset) and \"builtin/latest\". DOCUMENT_TEXT_DETECTION and TEXT_DETECTION also support \"builtin/weekly\" for the bleeding edge release updated weekly. |\n## Type\nType of Google Cloud Vision API feature to be extracted.| Enums     | Enums.1                                               |\n|:------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| TYPE_UNSPECIFIED  | Unspecified feature type.                                          |\n| TEXT_DETECTION   | Run text detection / optical character recognition (OCR). Text detection is optimized for areas of text within a larger image; if the image is a document, use DOCUMENT_TEXT_DETECTION instead. |\n| DOCUMENT_TEXT_DETECTION | Run dense text document OCR. Takes precedence when both DOCUMENT_TEXT_DETECTION and TEXT_DETECTION are present.                     |\n## Image\nClient image to perform Google Cloud Vision API tasks over.| Fields | Fields.1                                                                         |\n|:---------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| content | bytes Image content, represented as a stream of bytes. Note: As with all bytes fields, protobuffers use a pure binary representation, whereas JSON representations use base64.Currently, this field only works for BatchAnnotateImages requests. It does not work for AsyncBatchAnnotateImages requests. |\n## ImageAnnotationContext\nIf an image was produced from a file (e.g. a PDF), this message gives information about the source of that image.| Fields  | Fields.1                           |\n|:------------|:-----------------------------------------------------------------------------------------------------------------|\n| uri   | string The URI of the file used to produce the image.               |\n| page_number | int32 If the file was a PDF or TIFF, this field gives the page number within the file used to produce the image. |\n## ImageContext\nImage context and/or feature-specific parameters.| Fields    | Fields.1                                                                                                                                  |\n|:----------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| language_hints[]  | string List of languages to use for TEXT_DETECTION. In most cases, an empty value yields the best results since it enables automatic language detection. For languages based on the Latin alphabet, setting language_hints is not needed. In rare cases, when the language of the text in the image is known, setting a hint will help get better results (although it will be a significant hindrance if the hint is wrong). Text detection returns an error if one or more of the specified languages is not one of the supported languages. |\n| text_detection_params | TextDetectionParams Parameters for text detection and document text detection.                                                                                                                 |\n## InputConfig\nThe desired input location and metadata.| Fields | Fields.1                                                                        |\n|:----------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| content | bytes File content, represented as a stream of bytes. Note: As with all bytes fields, protobuffers use a pure binary representation, whereas JSON representations use base64.Currently, this field only works for BatchAnnotateFiles requests. It does not work for AsyncBatchAnnotateFiles requests. |\n| mime_type | string The type of the file. Currently only \"application/pdf\", \"image/tiff\" and \"image/gif\" are supported. Wildcards are not supported.                                        |\n## NormalizedVertex\nA vertex represents a 2D point in the image. NOTE: the normalized vertex coordinates are relative to the original image and range from 0 to 1.| Fields | Fields.1   |\n|:---------|:--------------------|\n| x  | float X coordinate. |\n| y  | float Y coordinate. |\n## Page\nDetected page from OCR.| Fields  | Fields.1                       |\n|:-----------|:-------------------------------------------------------------------------------------------------|\n| property | TextProperty Additional information detected on the page.          |\n| width  | int32 Page width. For PDFs the unit is points. For images (including TIFFs) the unit is pixels. |\n| height  | int32 Page height. For PDFs the unit is points. For images (including TIFFs) the unit is pixels. |\n| blocks[] | Block List of blocks of text, images etc on this page.           |\n| confidence | float Confidence of the OCR results on the page. Range [0, 1].         |\n## Paragraph\nStructural unit of text representing a number of words in certain order.| Fields  | Fields.1                                                                                                                                |\n|:-------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| property  | TextProperty Additional information detected for the paragraph.                                                                                                                  |\n| bounding_box | BoundingPoly The bounding box for the paragraph. The vertices are in the order of top-left, top-right, bottom-right, bottom-left. When a rotation of the bounding box is detected the rotation is represented as around the top-left corner as defined when the text is read in the 'natural' orientation. For example: * when the text is horizontal it might look like: 0----1 | | 3----2 * when it's rotated 180 degrees around the top-left corner it becomes: 2----3 | | 1----0 and the vertex order will still be (0, 1, 2, 3). |\n| words[]  | Word List of all words in this paragraph.                                                                                                                        |\n| confidence | float Confidence of the OCR results for the paragraph. Range [0, 1].                                                                                                                 |\n## Property\nA `Property` consists of a user-supplied name/value pair.| Fields  | Fields.1       |\n|:-------------|:------------------------------------|\n| name   | string Name of the property.  |\n| value  | string Value of the property.  |\n| uint64_value | uint64 Value of numeric properties. |\n## Symbol\nA single symbol representation.| Fields  | Fields.1                                                                                                                               |\n|:-------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| property  | TextProperty Additional information detected for the symbol.                                                                                                                  |\n| bounding_box | BoundingPoly The bounding box for the symbol. The vertices are in the order of top-left, top-right, bottom-right, bottom-left. When a rotation of the bounding box is detected the rotation is represented as around the top-left corner as defined when the text is read in the 'natural' orientation. For example: * when the text is horizontal it might look like: 0----1 | | 3----2 * when it's rotated 180 degrees around the top-left corner it becomes: 2----3 | | 1----0 and the vertex order will still be (0, 1, 2, 3). |\n| text   | string The actual UTF-8 representation of the symbol.                                                                                                                    |\n| confidence | float Confidence of the OCR results for the symbol. Range [0, 1].                                                                                                                 |\n## TextAnnotation\n`TextAnnotation` contains a structured representation of OCR-extracted text. The hierarchy of an OCR-extracted text structure is like this:\n`TextAnnotation` \nEach structural component, starting from Page, might have properties, which describe detected languages, breaks, etc. For more details, refer to the\n` [TextAnnotation.TextProperty](/distributed-cloud/hosted/docs/latest/gdch/apis/vertex-ai/ocr/rpc/google.cloud.vision.v1#google.cloud.vision.v1.TextAnnotation.TextProperty) `\nmessage definition that follows.\n| Fields | Fields.1         |\n|:---------|:-----------------------------------------|\n| pages[] | Page List of pages detected by OCR.  |\n| text  | string UTF-8 text detected on the pages. |\n## DetectedBreak\nDetected start or end of a structural component.| Fields | Fields.1         |\n|:----------|:-----------------------------------------|\n| type  | BreakType Detected break type.   |\n| is_prefix | bool True if break prepends the element. |\n## BreakType\nEnum to denote the type of break found. New line, space etc.| Enums   | Enums.1                         |\n|:---------------|:--------------------------------------------------------------------------------------------------------|\n| UNKNOWN  | Unknown break label type.                    |\n| SPACE   | Regular space.                       |\n| SURE_SPACE  | Sure space (very wide).                     |\n| EOL_SURE_SPACE | Line-wrapping break.                     |\n| HYPHEN   | End-line hyphen that is not present in text; does not co-occur with SPACE, LEADER_SPACE, or LINE_BREAK. |\n| LINE_BREAK  | Line break that ends a paragraph.                  |\n## DetectedLanguage\nDetected language for a structural component.| Fields  | Fields.1                                     |\n|:--------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| language_code | string The BCP-47 language code, such as \"en-US\" or \"sr-Latn\". For more information, see https://www.unicode.org/reports/tr35/#Unicode_locale_identifier. |\n| confidence | float Confidence of detected language. Range [0, 1].                          |\n## TextProperty\nAdditional information detected on the structural component.| Fields    | Fields.1                |\n|:---------------------|:------------------------------------------------------------------------|\n| detected_languages[] | DetectedLanguage A list of detected languages together with confidence. |\n| detected_break  | DetectedBreak Detected start or end of a text segment.     |\n## TextDetectionParams\nParameters for text detections. This is used to control TEXT_DETECTION and DOCUMENT_TEXT_DETECTION features.| Fields         | Fields.1                                           |\n|:---------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| enable_text_detection_confidence_score | bool By default, Cloud Vision API only includes confidence score for DOCUMENT_TEXT_DETECTION result. Set the flag to true to include confidence score for TEXT_DETECTION as well. |\n| advanced_ocr_options[]     | string A list of advanced OCR options to fine-tune OCR behavior.                             |\n## Vertex\nA vertex represents a 2D point in the image. NOTE: the vertex coordinates are in the same scale as the original image.| Fields | Fields.1   |\n|:---------|:--------------------|\n| x  | int32 X coordinate. |\n| y  | int32 Y coordinate. |\n## Word\nA word representation.| Fields  | Fields.1                                                                                                                               |\n|:-------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| property  | TextProperty Additional information detected for the word.                                                                                                                  |\n| bounding_box | BoundingPoly The bounding box for the word. The vertices are in the order of top-left, top-right, bottom-right, bottom-left. When a rotation of the bounding box is detected the rotation is represented as around the top-left corner as defined when the text is read in the 'natural' orientation. For example: * when the text is horizontal it might look like: 0----1 | | 3----2 * when it's rotated 180 degrees around the top-left corner it becomes: 2----3 | | 1----0 and the vertex order will still be (0, 1, 2, 3). |\n| symbols[] | Symbol List of symbols in the word. The order of the symbols follows the natural reading order.                                                                                                         |\n| confidence | float Confidence of the OCR results for the word. Range [0, 1].                                                                                                                 |", "guide": "Documentation"}