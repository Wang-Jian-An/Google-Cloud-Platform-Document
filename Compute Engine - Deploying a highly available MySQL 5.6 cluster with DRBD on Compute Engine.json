{"title": "Compute Engine - Deploying a highly available MySQL 5.6 cluster with DRBD on Compute Engine", "url": "https://cloud.google.com/compute/docs/instances", "abstract": "# Compute Engine - Deploying a highly available MySQL 5.6 cluster with DRBD on Compute Engine\nLast reviewed 2019-05-10 UTC\nThis tutorial walks you through the process of deploying a MySQL 5.6 database to Google Cloud by using Distributed Replicated Block Device (DRBD) and Compute Engine. DRBD is a distributed replicated storage system for the Linux platform.\nThis tutorial is useful if you are a sysadmin, developer, engineer, database admin, or DevOps engineer. You might want to manage your own MySQL instance instead of using the managed service for several reasons, including:- You're using cross-region instances of MySQL.\n- You need to set parameters that are not available in the managed version of MySQL.\n- You want to optimize performance in ways that are not settable in the managed version.\nDRBD provides replication at the block device level. That means you don't have to configure replication in MySQL itself, and you get immediate DRBD benefits\u2014for example, support for read load balancing and secure connections.\nThe tutorial uses the following:- [Compute Engine](/compute/docs) \n- [Corosync Cluster Engine](https://clusterlabs.org/corosync.html) \n- [Debian 9](https://www.debian.org/releases/stable/) \n- [DRBD](https://www.linbit.com/linbit-software-download-page-for-linstor-and-drbd-linux-driver/) \n- [MySQL 5.6](https://dev.mysql.com/downloads/mysql/5.6.html) \n- [Pacemaker](https://wiki.clusterlabs.org/wiki/Pacemaker) \n- [Ubuntu 16](http://releases.ubuntu.com/16.04/) \nNo advanced knowledge is required in order to use these resources, although this this document does reference advanced capabilities like MySQL clustering, DRBD configuration, and Linux resource management.", "content": "## ArchitecturePacemaker is a cluster resource manager. Corosync is a cluster communication and participation package, that's used by Pacemaker. In this tutorial, you use DRBD to replicate the MySQL disk from the primary instance to the standby instance. In order for clients to connect to the MySQL cluster, you also deploy an internal load balancer.\nYou deploy a Pacemaker-managed cluster of three compute instances. You install MySQL on two of the instances, which serve as your primary and standby instances. The third instance serves as a quorum device.\nIn a cluster, each node votes for the node that should be the active node\u2014that is, the one that runs MySQL. In a two-node cluster, it takes only one vote to determine the active node. In such a case, the cluster behavior might lead to [split-brain](https://wikipedia.org/wiki/Split-brain_(computing)) issues or downtime. Split-brain issues occur when both nodes take control because only one vote is needed in a two-node scenario. Downtime occurs when the node that shuts down is the one configured to always be the primary in case of connectivity loss. If the two nodes lose connectivity with each other, there's a risk that more than one cluster node assumes it's the active node.\nAdding a quorum device prevents this situation. A quorum device serves as an arbiter, where its only job is to cast a vote. This way, in a situation where the `database1` and `database2` instances cannot communicate, this quorum device node can communicate with one of the two instances and a majority can still be reached.\nThe following diagram shows the architecture of the system described here.## Objectives\n- Create the cluster instances.\n- Install MySQL and DRBD on two of the instances.\n- Configure DRBD replication.\n- Install Pacemaker on the instances.\n- Configure Pacemaker clustering on the instances.\n- Create an instance and configure it as a quorum device.\n- Test failover.\n## Costs\nUse the [pricing calculator](/products/calculator#id=411d8ca1-210f-4f2c-babd-34c6af2b5538) to generate a cost estimate based on your projected usage.## Before you begin\nIn this tutorial, you enter commands using Cloud Shell unless otherwise noted.\nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Getting set upIn this section, you set up a service account, create environment variables, and reserve IP addresses.\n### Set up a service account for the cluster instances\n- Open Cloud Shell: [OPEN Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Create the service account:```\ngcloud iam service-accounts create mysql-instance \\\u00a0 \u00a0 --display-name \"mysql-instance\"\n```\n- Attach the roles needed for this tutorial to the service account:```\ngcloud projects add-iam-policy-binding ${DEVSHELL_PROJECT_ID} \\\u00a0 \u00a0 --member=serviceAccount:mysql-instance@${DEVSHELL_PROJECT_ID}.iam.gserviceaccount.com \\\u00a0 \u00a0 --role=roles/compute.instanceAdmin.v1gcloud projects add-iam-policy-binding ${DEVSHELL_PROJECT_ID} \\\u00a0 \u00a0 --member=serviceAccount:mysql-instance@${DEVSHELL_PROJECT_ID}.iam.gserviceaccount.com \\\u00a0 \u00a0 --role=roles/compute.viewergcloud projects add-iam-policy-binding ${DEVSHELL_PROJECT_ID} \\\u00a0 \u00a0 --member=serviceAccount:mysql-instance@${DEVSHELL_PROJECT_ID}.iam.gserviceaccount.com \\\u00a0 \u00a0 --role=roles/iam.serviceAccountUser\n```\n### Create Cloud Shell environment variables\n- Create a file with the required environment variables for this tutorial:```\ncat <<EOF > ~/.mysqldrbdrc# Cluster instance namesDATABASE1_INSTANCE_NAME=database1DATABASE2_INSTANCE_NAME=database2QUORUM_INSTANCE_NAME=qdeviceCLIENT_INSTANCE_NAME=mysql-client# Cluster IP addressesDATABASE1_INSTANCE_IP=\"10.140.0.2\"DATABASE2_INSTANCE_IP=\"10.140.0.3\"QUORUM_INSTANCE_IP=\"10.140.0.4\"ILB_IP=\"10.140.0.6\"# Cluster zones and regionDATABASE1_INSTANCE_ZONE=\"asia-east1-a\"DATABASE2_INSTANCE_ZONE=\"asia-east1-b\"QUORUM_INSTANCE_ZONE=\"asia-east1-c\"CLIENT_INSTANCE_ZONE=\"asia-east1-c\"CLUSTER_REGION=\"asia-east1\"EOF\n```\n- Load the environment variables in the current session and set Cloud Shell to automatically load the variables on future sign-ins:```\nsource ~/.mysqldrbdrcgrep -q -F \"source ~/.mysqldrbdrc\" ~/.bashrc || echo \"source ~/.mysqldrbdrc\" >> ~/.bashrc\n```\n### Reserve IP addresses\n- In Cloud Shell, reserve an internal IP address for each of the three cluster nodes:```\ngcloud compute addresses create ${DATABASE1_INSTANCE_NAME} ${DATABASE2_INSTANCE_NAME} ${QUORUM_INSTANCE_NAME} \\\u00a0 \u00a0 --region=${CLUSTER_REGION} \\\u00a0 \u00a0 --addresses \"${DATABASE1_INSTANCE_IP},${DATABASE2_INSTANCE_IP},${QUORUM_INSTANCE_IP}\" \\\u00a0 \u00a0 --subnet=default\n```\n## Creating the Compute Engine instancesIn the following steps, the cluster instances use Debian 9 and the client instances use Ubuntu 16.- In Cloud Shell, create a MySQL instance named `database1` in zone `asia-east1-a` :```\ngcloud compute instances create ${DATABASE1_INSTANCE_NAME} \\\u00a0 \u00a0 --zone=${DATABASE1_INSTANCE_ZONE} \\\u00a0 \u00a0 --machine-type=n1-standard-2 \u00a0\\\u00a0 \u00a0 --network-tier=PREMIUM \\\u00a0 \u00a0 --maintenance-policy=MIGRATE \\\u00a0 \u00a0 --image-family=debian-9 \\\u00a0 \u00a0 --image-project=debian-cloud \\\u00a0 \u00a0 --boot-disk-size=50GB \\\u00a0 \u00a0 --boot-disk-type=pd-standard \\\u00a0 \u00a0 --boot-disk-device-name=${DATABASE1_INSTANCE_NAME} \\\u00a0 \u00a0 --create-disk=mode=rw,size=300,type=pd-standard,name=disk-1 \\\u00a0 \u00a0 --private-network-ip=${DATABASE1_INSTANCE_NAME} \\\u00a0 \u00a0 --tags=mysql --service-account=mysql-instance@${DEVSHELL_PROJECT_ID}.iam.gserviceaccount.com \\\u00a0 \u00a0 --scopes=\"https://www.googleapis.com/auth/compute,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly\" \\\u00a0 \u00a0 --metadata=\"DATABASE1_INSTANCE_IP=${DATABASE1_INSTANCE_IP},DATABASE2_INSTANCE_IP=${DATABASE2_INSTANCE_IP},DATABASE1_INSTANCE_NAME=${DATABASE1_INSTANCE_NAME},DATABASE2_INSTANCE_NAME=${DATABASE2_INSTANCE_NAME},QUORUM_INSTANCE_NAME=${QUORUM_INSTANCE_NAME},DATABASE1_INSTANCE_ZONE=${DATABASE1_INSTANCE_ZONE},DATABASE2_INSTANCE_ZONE=${DATABASE2_INSTANCE_ZONE}\"\n```\n- Create a MySQL instance named `database2` in zone `asia-east1-b` :```\ngcloud compute instances create ${DATABASE2_INSTANCE_NAME} \\\u00a0 \u00a0 --zone=${DATABASE2_INSTANCE_ZONE} \\\u00a0 \u00a0 --machine-type=n1-standard-2 \u00a0\\\u00a0 \u00a0 --network-tier=PREMIUM \\\u00a0 \u00a0 --maintenance-policy=MIGRATE \\\u00a0 \u00a0 --image-family=debian-9 \\\u00a0 \u00a0 --image-project=debian-cloud \\\u00a0 \u00a0 --boot-disk-size=50GB \\\u00a0 \u00a0 --boot-disk-type=pd-standard \\\u00a0 \u00a0 --boot-disk-device-name=${DATABASE2_INSTANCE_NAME} \\\u00a0 \u00a0 --create-disk=mode=rw,size=300,type=pd-standard,name=disk-2 \\\u00a0 \u00a0 --private-network-ip=${DATABASE2_INSTANCE_NAME} \\\u00a0 \u00a0 --tags=mysql \\\u00a0 \u00a0 --service-account=mysql-instance@${DEVSHELL_PROJECT_ID}.iam.gserviceaccount.com \\\u00a0 \u00a0 --scopes=\"https://www.googleapis.com/auth/compute,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly\" \\\u00a0 \u00a0 --metadata=\"DATABASE1_INSTANCE_IP=${DATABASE1_INSTANCE_IP},DATABASE2_INSTANCE_IP=${DATABASE2_INSTANCE_IP},DATABASE1_INSTANCE_NAME=${DATABASE1_INSTANCE_NAME},DATABASE2_INSTANCE_NAME=${DATABASE2_INSTANCE_NAME},QUORUM_INSTANCE_NAME=${QUORUM_INSTANCE_NAME},DATABASE1_INSTANCE_ZONE=${DATABASE1_INSTANCE_ZONE},DATABASE2_INSTANCE_ZONE=${DATABASE2_INSTANCE_ZONE}\"\n```\n- Create a quorum node for use by Pacemaker in zone `asia-east1-c` :```\ngcloud compute instances create ${QUORUM_INSTANCE_NAME} \\\u00a0 \u00a0 --zone=${QUORUM_INSTANCE_ZONE} \\\u00a0 \u00a0 --machine-type=n1-standard-1 \\\u00a0 \u00a0 --network-tier=PREMIUM \\\u00a0 \u00a0 --maintenance-policy=MIGRATE \\\u00a0 \u00a0 --image-family=debian-9 \u00a0\\\u00a0 \u00a0 --image-project=debian-cloud \\\u00a0 \u00a0 --boot-disk-size=10GB \\\u00a0 \u00a0 --boot-disk-type=pd-standard \\\u00a0 \u00a0 --boot-disk-device-name=${QUORUM_INSTANCE_NAME} \\\u00a0 \u00a0 --private-network-ip=${QUORUM_INSTANCE_NAME}\n```\n- Create a MySQL client instance:```\ngcloud compute instances create ${CLIENT_INSTANCE_NAME} \\\u00a0 \u00a0 --image-family=ubuntu-1604-lts \\\u00a0 \u00a0 --image-project=ubuntu-os-cloud \\\u00a0 \u00a0 --tags=mysql-client \\\u00a0 \u00a0 --zone=${CLIENT_INSTANCE_ZONE} \\\u00a0 \u00a0 --boot-disk-size=10GB \\\u00a0 \u00a0 --metadata=\"ILB_IP=${ILB_IP}\"\n```\n## Installing and configuring DRBDIn this section, you install and configure the DRBD packages on the `database1` and `database2` instances, and then initiate DRBD replication from `database1` to `database2` .\n### Configure DRBD on database1\n- In the Google Cloud console, go to the **VM instances** page: [VM INSTANCES PAGE](https://console.cloud.google.com/compute/instances) \n- In the `database1` instance row, click **SSH** to connect to the instance. **Note:** Throughout this tutorial, you connect to various instances. Repeat the preceding two steps to connect to each instance in the Google Cloud console by using SSH.\n- Create a file to retrieve and store instance metadata in environment variables:```\nsudo bash -c cat <<EOF \u00a0> ~/.varsrcDATABASE1_INSTANCE_IP=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/DATABASE1_INSTANCE_IP\" -H \"Metadata-Flavor: Google\")DATABASE2_INSTANCE_IP=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/DATABASE2_INSTANCE_IP\" -H \"Metadata-Flavor: Google\")DATABASE1_INSTANCE_NAME=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/DATABASE1_INSTANCE_NAME\" -H \"Metadata-Flavor: Google\")DATABASE2_INSTANCE_NAME=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/DATABASE2_INSTANCE_NAME\" -H \"Metadata-Flavor: Google\")DATABASE2_INSTANCE_ZONE=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/DATABASE2_INSTANCE_ZONE\" -H \"Metadata-Flavor: Google\")DATABASE1_INSTANCE_ZONE=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/DATABASE1_INSTANCE_ZONE\" -H \"Metadata-Flavor: Google\")QUORUM_INSTANCE_NAME=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/QUORUM_INSTANCE_NAME\" -H \"Metadata-Flavor: Google\")EOF\n```\n- Load the metadata variables from file:```\nsource ~/.varsrc\n```\n- Format the data disk:```\nsudo bash -c \u00a0\"mkfs.ext4 -m 0 -F -E \\lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb\"\n```For a detailed description of `mkfs.ext4` options, see [the mkfs.ext4 manpage](https://linux.die.net/man/8/mkfs.ext4) .\n- Install DRBD:```\nsudo apt -y install drbd8-utils\n```\n- Create the DRBD configuration file:```\nsudo bash -c 'cat <<EOF \u00a0> /etc/drbd.d/global_common.confglobal {\u00a0 \u00a0 usage-count no;}common {\u00a0 \u00a0 protocol C;}EOF'\n```\n- Create a DRBD resource file:```\nsudo bash -c \"cat <<EOF \u00a0> /etc/drbd.d/r0.resresource r0 {\u00a0 \u00a0 meta-disk internal;\u00a0 \u00a0 device /dev/drbd0;\u00a0 \u00a0 net {\u00a0 \u00a0 \u00a0 \u00a0 allow-two-primaries no;\u00a0 \u00a0 \u00a0 \u00a0 after-sb-0pri discard-zero-changes;\u00a0 \u00a0 \u00a0 \u00a0 after-sb-1pri discard-secondary;\u00a0 \u00a0 \u00a0 \u00a0 after-sb-2pri disconnect;\u00a0 \u00a0 \u00a0 \u00a0 rr-conflict disconnect;\u00a0 \u00a0 }\u00a0 \u00a0 on database1 {\u00a0 \u00a0 \u00a0 \u00a0 disk /dev/sdb;\u00a0 \u00a0 \u00a0 \u00a0 address 10.140.0.2:7789;\u00a0 \u00a0 }\u00a0 \u00a0 on database2 {\u00a0 \u00a0 \u00a0 \u00a0 disk /dev/sdb;\u00a0 \u00a0 \u00a0 \u00a0 address 10.140.0.3:7789;\u00a0 \u00a0 }}EOF\"\n```\n- Load the DRBD kernel module:```\nsudo modprobe drbd\n```\n- Clear the contents of the `/dev/sdb` disk:```\nsudo dd if=/dev/zero of=/dev/sdb bs=1k count=1024\n```\n- Create the DRBD resource `r0` :```\nsudo drbdadm create-md r0\n```\n- Bring up DRBD:```\nsudo drbdadm up r0\n```\n- Disable DRBD when the system starts, letting the cluster resource management software bring up all necessary services in order:```\nsudo update-rc.d drbd disable\n```\n### Configure DRBD on database2You now install and configure the DRBD packages on the `database2` instance.- Connect to the`database2`instance through SSH.\n- Create a `.varsrc` file to retrieve and store instance metadata in environment variables:```\nsudo bash -c cat <<EOF \u00a0> ~/.varsrcDATABASE1_INSTANCE_IP=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/DATABASE1_INSTANCE_IP\" -H \"Metadata-Flavor: Google\")DATABASE2_INSTANCE_IP=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/DATABASE2_INSTANCE_IP\" -H \"Metadata-Flavor: Google\")DATABASE1_INSTANCE_NAME=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/DATABASE1_INSTANCE_NAME\" -H \"Metadata-Flavor: Google\")DATABASE2_INSTANCE_NAME=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/DATABASE2_INSTANCE_NAME\" -H \"Metadata-Flavor: Google\")DATABASE2_INSTANCE_ZONE=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/DATABASE2_INSTANCE_ZONE\" -H \"Metadata-Flavor: Google\")DATABASE1_INSTANCE_ZONE=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/DATABASE1_INSTANCE_ZONE\" -H \"Metadata-Flavor: Google\")QUORUM_INSTANCE_NAME=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/QUORUM_INSTANCE_NAME\" -H \"Metadata-Flavor: Google\")EOF\n```\n- Load the metadata variables from the file:```\nsource ~/.varsrc\n```\n- Format the data disk:```\nsudo bash -c \u00a0\"mkfs.ext4 -m 0 -F -E \u00a0lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb\"\n```\n- Install the DRBD packages:```\nsudo apt -y install drbd8-utils\n```\n- Create the DRBD configuration file:```\nsudo bash -c 'cat <<EOF \u00a0> /etc/drbd.d/global_common.confglobal {\u00a0 \u00a0 usage-count no;}common {\u00a0 \u00a0 protocol C;}EOF'\n```\n- Create a DRBD resource file:```\nsudo bash -c \"cat <<EOF \u00a0> /etc/drbd.d/r0.resresource r0 {\u00a0 \u00a0 meta-disk internal;\u00a0 \u00a0 device /dev/drbd0;\u00a0 \u00a0 net {\u00a0 \u00a0 \u00a0 \u00a0 allow-two-primaries no;\u00a0 \u00a0 \u00a0 \u00a0 after-sb-0pri discard-zero-changes;\u00a0 \u00a0 \u00a0 \u00a0 after-sb-1pri discard-secondary;\u00a0 \u00a0 \u00a0 \u00a0 after-sb-2pri disconnect;\u00a0 \u00a0 \u00a0 \u00a0 rr-conflict disconnect;\u00a0 \u00a0 }\u00a0 \u00a0 on ${DATABASE1_INSTANCE_NAME} {\u00a0 \u00a0 \u00a0 \u00a0 disk /dev/sdb;\u00a0 \u00a0 \u00a0 \u00a0 address ${DATABASE1_INSTANCE_IP}:7789;\u00a0 \u00a0 }\u00a0 \u00a0 on ${DATABASE2_INSTANCE_NAME} {\u00a0 \u00a0 \u00a0 \u00a0 disk /dev/sdb;\u00a0 \u00a0 \u00a0 \u00a0 address ${DATABASE2_INSTANCE_IP}:7789;\u00a0 \u00a0 }}EOF\"\n```\n- Load the DRBD kernel module:```\nsudo modprobe drbd\n```\n- Clear out the `/dev/sdb` disk:```\nsudo dd if=/dev/zero of=/dev/sdb bs=1k count=1024\n```\n- Create the DRBD resource `r0` :```\nsudo drbdadm create-md r0\n```\n- Bring up DRBD:```\nsudo drbdadm up r0\n```\n- Disable DRBD when the system starts, letting the cluster resource management software bring up all necessary services in order:```\nsudo update-rc.d drbd disable\n```\n### Initiate DRBD replication from database1 to database2\n- Connect to the`database1`instance through SSH.\n- Overwrite all `r0` resources on the primary node:```\nsudo drbdadm -- --overwrite-data-of-peer primary r0sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/drbd0\n```\n- Verify the status of DRBD:```\nsudo cat /proc/drbd | grep ============\n```The output looks like this:```\n[===================>] sync'ed:100.0% (208/307188)M\n``` **Note:** It might take a few minutes for synchronization to reach 100.0%.\n- Mount `/dev/drbd` to `/srv` :```\nsudo mount -o discard,defaults /dev/drbd0 /srv\n```\n## Installing MySQL and PacemakerIn this section, you install MySQL and Pacemaker on each instance.\n### Install MySQL on database1\n- Connect to the`database1`instance through SSH.\n- Update the APT repository with the MySQL 5.6 package definitions:```\nsudo bash -c 'cat <<EOF \u00a0> /etc/apt/sources.list.d/mysql.listdeb http://repo.mysql.com/apt/debian/ stretch mysql-5.6\\ndeb-src http://repo.mysql.com/apt/debian/ stretch mysql-5.6EOF'\n```\n- Add the GPG keys to the APT `repository.srv` file:```\nwget -O /tmp/RPM-GPG-KEY-mysql https://repo.mysql.com/RPM-GPG-KEY-mysqlsudo apt-key add /tmp/RPM-GPG-KEY-mysql\n```\n- Update the package list:```\nsudo apt update\n```\n- Install the MySQL server:```\nsudo apt -y install mysql-server\n```When prompted for a password, enter `DRBDha2` .\n- Stop the MySQL server:```\nsudo /etc/init.d/mysql stop\n```\n- Create the MySQL configuration file:```\nsudo bash -c 'cat <<EOF \u00a0> /etc/mysql/mysql.conf.d/my.cnf[mysqld]bind-address = 0.0.0.0 \u00a0# You may want to listen at localhost at the beginningdatadir = /var/lib/mysqltmpdir = /srv/tmpuser = mysqlEOF'\n```\n- Create a temporary directory for the MySQL server (configured in `mysql.conf` ):```\nsudo mkdir /srv/tmpsudo chmod 1777 /srv/tmp\n```\n- Move all MySQL data into the DRBD directory `/srv/mysql` :```\nsudo mv /var/lib/mysql /srv/mysql\n```\n- Link `/var/lib/mysql` to `/srv/mysql` under the DRBD replicated storage volume:```\nsudo ln -s /srv/mysql /var/lib/mysql\n```\n- Change the `/srv/mysql` owner to a `mysql` process:```\nsudo chown -R mysql:mysql /srv/mysql\n```\n- Remove `InnoDB` initial data to make sure the disk is as clean as possible:```\nsudo bash -c \"cd /srv/mysql && rm ibdata1 && rm ib_logfile*\"\n``` [InnoDB](https://dev.mysql.com/doc/refman/5.6/en/innodb-introduction.html) is a storage engine for the MySQL database management system.\n- Start MySQL:```\nsudo /etc/init.d/mysql start\n```\n- Grant access to root user for remote connections in order to test the deployment later:```\nmysql -uroot -pDRBDha2 -e \"GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'DRBDha2' WITH GRANT OPTION;\"\n```\n- Disable automatic MySQL startup, which cluster resource management takes care of:```\nsudo update-rc.d -f mysql disable\n```\n### Install Pacemaker on database1\n- Load the metadata variables from the `.varsrc` file that you created earlier:```\nsource ~/.varsrc\n```\n- Stop the MySQL server:```\nsudo /etc/init.d/mysql stop\n```\n- Install Pacemaker:```\nsudo apt -y install pcs\n```\n- Enable `pcsd` , `corosync` , and `pacemaker` at system start on the primary instance:```\nsudo update-rc.d -f pcsd enablesudo update-rc.d -f corosync enablesudo update-rc.d -f pacemaker enable\n```\n- Configure `corosync` to start before `pacemaker` :```\nsudo update-rc.d corosync defaults 20 20sudo update-rc.d pacemaker defaults 30 10\n```\n- Set the cluster user password to `haCLUSTER3` for authentication:```\nsudo bash -c \"echo \u00a0hacluster:haCLUSTER3 | chpasswd\"\n```\n- Run the `corosync-keygen` script to generate a 128-bit cluster authorization key and write it into `/etc/corosync/authkey` :```\nsudo corosync-keygen -l\n```\n- Copy `authkey` to the `database2` instance. When prompted for a passphrase, press `Enter` :```\nsudo chmod 444 /etc/corosync/authkeygcloud beta compute scp /etc/corosync/authkey ${DATABASE2_INSTANCE_NAME}:~/authkey --zone=${DATABASE2_INSTANCE_ZONE} --internal-ipsudo chmod 400 /etc/corosync/authkey\n```\n- Create a Corosync cluster configuration file:```\nsudo bash -c \"cat <<EOF \u00a0> /etc/corosync/corosync.conftotem {\u00a0 \u00a0 version: 2\u00a0 \u00a0 cluster_name: mysql_cluster\u00a0 \u00a0 transport: udpu\u00a0 \u00a0 interface {\u00a0 \u00a0 \u00a0 \u00a0 ringnumber: 0\u00a0 \u00a0 \u00a0 \u00a0 Bindnetaddr: ${DATABASE1_INSTANCE_IP}\u00a0 \u00a0 \u00a0 \u00a0 broadcast: yes\u00a0 \u00a0 \u00a0 \u00a0 mcastport: 5405\u00a0 \u00a0 }}quorum {\u00a0 \u00a0 provider: corosync_votequorumtwo_node: 1}nodelist {\u00a0 \u00a0 node {\u00a0 \u00a0 \u00a0 \u00a0 ring0_addr: ${DATABASE1_INSTANCE_NAME}\u00a0 \u00a0 name: \u00a0${DATABASE1_INSTANCE_NAME}\u00a0 \u00a0 nodeid: 1\u00a0 \u00a0 }\u00a0 \u00a0 node {\u00a0 \u00a0 \u00a0 \u00a0 ring0_addr: \u00a0${DATABASE2_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 name: \u00a0${DATABASE2_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 nodeid: 2\u00a0 \u00a0 }}logging {\u00a0 \u00a0 to_logfile: yes\u00a0 \u00a0 logfile: /var/log/corosync/corosync.log\u00a0 \u00a0 timestamp: on}EOF\"\n```The `totem` section configures the Totem protocol for reliable communication. Corosync uses this communication to control cluster membership, and it specifies how the cluster members should communicate with each other.The important settings in the setup are as follows:- `transport`: Specifies unicast mode (udpu).\n- `Bindnetaddr`: Specifies the network address to which Corosync binds.\n- `nodelist`: Defines the nodes in the cluster, and how they can be reached\u2014in this case, the`database1`and`database2`nodes.\n- `quorum`/`two_node`: By default, in a two-node cluster, no node will acquire a quorum. You can override this by specifying the value \"1\" for`two_node`in the`quorum`section.\nThis setup lets you configure the cluster and prepare it for later when you add a third node that will be a quorum device.\n- Create the service directory for `corosync` :```\nsudo mkdir -p /etc/corosync/service.d\n```\n- Configure `corosync` to be aware of Pacemaker:```\nsudo bash -c 'cat <<EOF \u00a0> /etc/corosync/service.d/pcmkservice {\u00a0 \u00a0 name: pacemaker\u00a0 \u00a0 ver: 1}EOF'\n```\n- Enable the `corosync` service by default:```\nsudo bash -c 'cat <<EOF \u00a0> /etc/default/corosync# Path to corosync.confCOROSYNC_MAIN_CONFIG_FILE=/etc/corosync/corosync.conf# Path to authfileCOROSYNC_TOTEM_AUTHKEY_FILE=/etc/corosync/authkey# Enable service by defaultSTART=yesEOF'\n```\n- Restart the `corosync` and `pacemaker` services:```\nsudo service corosync restartsudo service pacemaker restart\n```\n- Install the Corosync quorum device package:```\nsudo apt -y install corosync-qdevice\n```\n- Install a shell script to handle DRBD failure events:```\nsudo bash -c 'cat << 'EOF' \u00a0> /var/lib/pacemaker/drbd_cleanup.sh#!/bin/shif [ -z \\$CRM_alert_version ]; then\u00a0 \u00a0 echo \"\\$0 must be run by Pacemaker version 1.1.15 or later\"\u00a0 \u00a0 exit 0fitstamp=\"\\$CRM_alert_timestamp: \"case \\$CRM_alert_kind in\u00a0 \u00a0 resource)\u00a0 \u00a0 \u00a0 \u00a0 if [ \\${CRM_alert_interval} = \"0\" ]; then\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CRM_alert_interval=\"\"\u00a0 \u00a0 \u00a0 \u00a0 else\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CRM_alert_interval=\" (\\${CRM_alert_interval})\"\u00a0 \u00a0 \u00a0 \u00a0 fi\u00a0 \u00a0 \u00a0 \u00a0 if [ \\${CRM_alert_target_rc} = \"0\" ]; then\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CRM_alert_target_rc=\"\"\u00a0 \u00a0 \u00a0 \u00a0 else\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CRM_alert_target_rc=\" (target: \\${CRM_alert_target_rc})\"\u00a0 \u00a0 \u00a0 \u00a0 fi\u00a0 \u00a0 \u00a0 \u00a0 case \\${CRM_alert_desc} in\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Cancelled) ;;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 *)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 echo \"\\${tstamp}Resource operation \"\\${CRM_alert_task}\\${CRM_alert_interval}\" for \"\\${CRM_alert_rsc}\" on \"\\${CRM_alert_node}\": \\${CRM_alert_desc}\\${CRM_alert_target_rc}\" >> \"\\${CRM_alert_recipient}\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if [ \"\\${CRM_alert_task}\" = \"stop\" ] && [ \"\\${CRM_alert_desc}\" = \"Timed Out\" ]; then\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 echo \"Executing recovering...\" >> \"\\${CRM_alert_recipient}\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pcs resource cleanup \\${CRM_alert_rsc}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fi\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ;;\u00a0 \u00a0 \u00a0 \u00a0 esac\u00a0 \u00a0 \u00a0 \u00a0 ;;\u00a0 \u00a0 *)\u00a0 \u00a0 \u00a0 \u00a0 echo \"\\${tstamp}Unhandled \\$CRM_alert_kind alert\" >> \"\\${CRM_alert_recipient}\"\u00a0 \u00a0 \u00a0 \u00a0 env | grep CRM_alert >> \"\\${CRM_alert_recipient}\"\u00a0 \u00a0 \u00a0 \u00a0 ;;esacEOF'sudo chmod 0755 /var/lib/pacemaker/drbd_cleanup.shsudo touch /var/log/pacemaker_drbd_file.logsudo chown hacluster:haclient /var/log/pacemaker_drbd_file.log\n```\n### Install MySQL on database2\n- Connect to the`database2`instance through SSH.\n- Update the APT repository with the MySQL 5.6 package:```\nsudo bash -c 'cat <<EOF \u00a0> /etc/apt/sources.list.d/mysql.listdeb http://repo.mysql.com/apt/debian/ stretch mysql-5.6\\ndeb-src http://repo.mysql.com/apt/debian/ stretch mysql-5.6EOF'\n```\n- Add the GPG keys to the APT repository:```\nwget -O /tmp/RPM-GPG-KEY-mysql https://repo.mysql.com/RPM-GPG-KEY-mysqlsudo apt-key add /tmp/RPM-GPG-KEY-mysql\n```\n- Update the package list:```\nsudo apt update\n```\n- Install the MySQL server:```\nsudo apt -y install mysql-server\n```When prompted for a password, enter `DRBDha2` .\n- Stop the MySQL server:```\nsudo /etc/init.d/mysql stop\n```\n- Create the MySQL configuration file:```\nsudo bash -c 'cat <<EOF \u00a0> /etc/mysql/mysql.conf.d/my.cnf[mysqld]bind-address = 0.0.0.0 \u00a0# You may want to listen at localhost at the beginningdatadir = /var/lib/mysqltmpdir = /srv/tmpuser = mysqlEOF'\n```\n- Remove data under `/var/lib/mysql` and add a symbolic link to the mount point target for the replicated DRBD volume. The DRBD volume ( `/dev/drbd0` ) will be mounted at `/srv` only when a failover occurs.```\nsudo rm -rf /var/lib/mysqlsudo ln -s /srv/mysql /var/lib/mysql\n``` **Note:** At this point, the `/srv` directory is not mounted, but the symbolic link will be valid when the cluster fails over.\n- Disable automatic MySQL startup, which cluster resource management takes care of:```\nsudo update-rc.d -f mysql disable\n```\n### Install Pacemaker on database2\n- Load the metadata variables from the `.varsrc` file:```\nsource ~/.varsrc\n```\n- Install Pacemaker:```\nsudo apt -y install pcs\n```\n- Move the Corosync `authkey` file that you copied before to `/etc/corosync/` :```\nsudo mv ~/authkey /etc/corosync/sudo chown root: /etc/corosync/authkeysudo chmod 400 /etc/corosync/authkey\n```\n- Enable `pcsd` , `corosync` , and `pacemaker` at system start on the standby instance:```\nsudo update-rc.d -f pcsd enablesudo update-rc.d -f corosync enablesudo update-rc.d -f pacemaker enable\n```\n- Configure `corosync` to start before `pacemaker` :```\nsudo update-rc.d corosync defaults 20 20sudo update-rc.d pacemaker defaults 30 10\n```\n- Set the cluster user password for authentication. The password is the same one ( `haCLUSTER3` ) you used for the `database1` instance.```\nsudo bash -c \"echo \u00a0hacluster:haCLUSTER3 | chpasswd\"\n```\n- Create the `corosync` configuration file:```\nsudo bash -c \"cat <<EOF \u00a0> /etc/corosync/corosync.conftotem {\u00a0 \u00a0 version: 2\u00a0 \u00a0 cluster_name: mysql_cluster\u00a0 \u00a0 transport: udpu\u00a0 \u00a0 interface {\u00a0 \u00a0 \u00a0 \u00a0 ringnumber: 0\u00a0 \u00a0 \u00a0 \u00a0 Bindnetaddr: ${DATABASE2_INSTANCE_IP}\u00a0 \u00a0 \u00a0 \u00a0 broadcast: yes\u00a0 \u00a0 \u00a0 \u00a0 mcastport: 5405\u00a0 \u00a0 }}quorum {\u00a0 \u00a0 provider: corosync_votequorum\u00a0 \u00a0 two_node: 1}nodelist {\u00a0 \u00a0 node {\u00a0 \u00a0 \u00a0 \u00a0 ring0_addr: ${DATABASE1_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 name: ${DATABASE1_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 nodeid: 1\u00a0 \u00a0 }\u00a0 \u00a0 node {\u00a0 \u00a0 \u00a0 \u00a0 ring0_addr: ${DATABASE2_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 name: ${DATABASE2_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 nodeid: 2\u00a0 \u00a0 }}logging {\u00a0 \u00a0 to_logfile: yes\u00a0 \u00a0 logfile: /var/log/corosync/corosync.logtimestamp: on}EOF\"\n```\n- Create the Corosync service directory:```\nsudo mkdir /etc/corosync/service.d\n```\n- Configure `corosync` to be aware of Pacemaker:```\nsudo bash -c 'cat <<EOF \u00a0> /etc/corosync/service.d/pcmkservice {name: pacemakerver: 1}EOF'\n```\n- Enable the `corosync` service by default:```\nsudo bash -c 'cat <<EOF \u00a0> /etc/default/corosync# Path to corosync.confCOROSYNC_MAIN_CONFIG_FILE=/etc/corosync/corosync.conf# Path to authfileCOROSYNC_TOTEM_AUTHKEY_FILE=/etc/corosync/authkey# Enable service by defaultSTART=yesEOF'\n```\n- Restart the `corosync` and `pacemaker` services:```\nsudo service corosync restartsudo service pacemaker restart\n```\n- Install the Corosync quorum device package:```\nsudo apt -y install corosync-qdevice\n```\n- Install a shell script to handle DRBD failure events:```\nsudo bash -c 'cat << 'EOF' \u00a0> /var/lib/pacemaker/drbd_cleanup.sh#!/bin/shif [ -z \\$CRM_alert_version ]; then\u00a0 \u00a0 echo \"\\$0 must be run by Pacemaker version 1.1.15 or later\"\u00a0 \u00a0 exit 0fitstamp=\"\\$CRM_alert_timestamp: \"case \\$CRM_alert_kind in\u00a0 \u00a0 resource)\u00a0 \u00a0 \u00a0 \u00a0 if [ \\${CRM_alert_interval} = \"0\" ]; then\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CRM_alert_interval=\"\"\u00a0 \u00a0 \u00a0 \u00a0 else\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CRM_alert_interval=\" (\\${CRM_alert_interval})\"\u00a0 \u00a0 \u00a0 \u00a0 fi\u00a0 \u00a0 \u00a0 \u00a0 if [ \\${CRM_alert_target_rc} = \"0\" ]; then\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CRM_alert_target_rc=\"\"\u00a0 \u00a0 \u00a0 \u00a0 else\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 CRM_alert_target_rc=\" (target: \\${CRM_alert_target_rc})\"\u00a0 \u00a0 \u00a0 \u00a0 fi\u00a0 \u00a0 \u00a0 \u00a0 case \\${CRM_alert_desc} in\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Cancelled) ;;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 *)\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 echo \"\\${tstamp}Resource operation \"\\${CRM_alert_task}\\${CRM_alert_interval}\" for \"\\${CRM_alert_rsc}\" on \"\\${CRM_alert_node}\": \\${CRM_alert_desc}\\${CRM_alert_target_rc}\" >> \"\\${CRM_alert_recipient}\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if [ \"\\${CRM_alert_task}\" = \"stop\" ] && [ \"\\${CRM_alert_desc}\" = \"Timed Out\" ]; then\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 echo \"Executing recovering...\" >> \"\\${CRM_alert_recipient}\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pcs resource cleanup \\${CRM_alert_rsc}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fi\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ;;\u00a0 \u00a0 \u00a0 \u00a0 esac\u00a0 \u00a0 \u00a0 \u00a0 ;;\u00a0 \u00a0 *)\u00a0 \u00a0 \u00a0 \u00a0 echo \"\\${tstamp}Unhandled \\$CRM_alert_kind alert\" >> \"\\${CRM_alert_recipient}\"\u00a0 \u00a0 \u00a0 \u00a0 env | grep CRM_alert >> \"\\${CRM_alert_recipient}\"\u00a0 \u00a0 \u00a0 \u00a0 ;;esacEOF'sudo chmod 0755 /var/lib/pacemaker/drbd_cleanup.shsudo touch /var/log/pacemaker_drbd_file.logsudo chown hacluster:haclient /var/log/pacemaker_drbd_file.log\n```\n- Check the Corosync cluster status:```\nsudo corosync-cmapctl | grep \"members...ip\"\n```The output looks like this:```\nruntime.totem.pg.mrp.srp.members.1.ip (str) = r(0) ip(10.140.0.2)\nruntime.totem.pg.mrp.srp.members.2.ip (str) = r(0) ip(10.140.0.3)\n```\n## Starting the cluster\n- Connect to the`database2`instance through SSH.\n- Load the metadata variables from the `.varsrc` file:```\nsource ~/.varsrc\n```\n- Authenticate against the cluster nodes:```\nsudo pcs cluster auth --name mysql_cluster ${DATABASE1_INSTANCE_NAME} ${DATABASE2_INSTANCE_NAME} -u hacluster -p haCLUSTER3\n```\n- Start the cluster:```\nsudo pcs cluster start --all\n```\n- Verify the cluster status:```\nsudo pcs status\n```The output looks like this:```\nCluster name: mysql_cluster\nWARNING: no stonith devices and stonith-enabled is not false\nStack: corosync\nCurrent DC: database2 (version 1.1.16-94ff4df) - partition with quorum\nLast updated: Sat Nov 3 07:24:53 2018\nLast change: Sat Nov 3 07:17:17 2018 by hacluster via crmd on database2\n2 nodes configured\n0 resources configured\nOnline: [ database1 database2 ]\nNo resources\nDaemon Status:\n corosync: active/enabled\n pacemaker: active/enabled\n pcsd: active/enabled\n```\n## Configuring Pacemaker to manage cluster resourcesNext, you configure Pacemaker with the DRBD, disk, MySQL, and quorum resources.- Connect to the`database1`instance through SSH.\n- Use the Pacemaker `pcs` utility to queue several changes into a file and later push those changes to the Cluster Information Base (CIB) atomically:```\nsudo pcs cluster cib clust_cfg\n```\n- Disable [STONITH](https://wikipedia.org/wiki/STONITH) , because you'll deploy the quorum device later:```\nsudo pcs -f clust_cfg property set stonith-enabled=false\n```\n- Disable the quorum-related settings. You'll set up the quorum device node later.```\nsudo pcs -f clust_cfg property set no-quorum-policy=stop\n```\n- Prevent Pacemaker from moving back resources after a recovery:```\nsudo pcs -f clust_cfg resource defaults resource-stickiness=200\n```\n- Create the DRBD resource in the cluster:```\nsudo pcs -f clust_cfg resource create mysql_drbd ocf:linbit:drbd \\\u00a0 \u00a0 drbd_resource=r0 \\\u00a0 \u00a0 op monitor role=Master interval=110 timeout=30 \\\u00a0 \u00a0 op monitor role=Slave interval=120 timeout=30 \\\u00a0 \u00a0 op start timeout=120 \\\u00a0 \u00a0 op stop timeout=60\n```\n- Make sure that only one primary role is assigned to the DRBD resource:```\nsudo pcs -f clust_cfg resource master primary_mysql mysql_drbd \\\u00a0 \u00a0 master-max=1 master-node-max=1 \\\u00a0 \u00a0 clone-max=2 clone-node-max=1 \\\u00a0 \u00a0 notify=true\n```\n- Create the file system resource to mount the DRBD disk:```\nsudo pcs -f clust_cfg resource create mystore_FS Filesystem \\\u00a0 \u00a0 device=\"/dev/drbd0\" \\\u00a0 \u00a0 directory=\"/srv\" \\\u00a0 \u00a0 fstype=\"ext4\"\n```\n- Configure the cluster to colocate the DRBD resource with the disk resource on the same VM:```\nsudo pcs -f clust_cfg constraint colocation add mystore_FS with primary_mysql INFINITY with-rsc-role=Master\n```\n- Configure the cluster to bring up the disk resource only after the DRBD primary is promoted:```\nsudo pcs -f clust_cfg constraint order promote primary_mysql then start mystore_FS\n```\n- Create a MySQL service:```\nsudo pcs -f clust_cfg resource create mysql_service ocf:heartbeat:mysql \\\u00a0 \u00a0 binary=\"/usr/bin/mysqld_safe\" \\\u00a0 \u00a0 config=\"/etc/mysql/my.cnf\" \\\u00a0 \u00a0 datadir=\"/var/lib/mysql\" \\\u00a0 \u00a0 pid=\"/var/run/mysqld/mysql.pid\" \\\u00a0 \u00a0 socket=\"/var/run/mysqld/mysql.sock\" \\\u00a0 \u00a0 additional_parameters=\"--bind-address=0.0.0.0\" \\\u00a0 \u00a0 op start timeout=60s \\\u00a0 \u00a0 op stop timeout=60s \\\u00a0 \u00a0 op monitor interval=20s timeout=30s\n```\n- Configure the cluster to colocate the MySQL resource with the disk resource on the same VM:```\nsudo pcs -f clust_cfg constraint colocation add mysql_service with mystore_FS INFINITY\n```\n- Ensure that the DRBD file system precedes the MySQL service in the startup order:```\nsudo pcs -f clust_cfg constraint order mystore_FS then mysql_service\n```\n- Create the alert agent, and add the patch to the log file as its recipient:```\nsudo pcs -f clust_cfg alert create id=drbd_cleanup_file description=\"Monitor DRBD events and perform post cleanup\" path=/var/lib/pacemaker/drbd_cleanup.shsudo pcs -f clust_cfg alert recipient add drbd_cleanup_file id=logfile value=/var/log/pacemaker_drbd_file.log\n```\n- Commit the changes to the cluster:```\nsudo pcs cluster cib-push clust_cfg\n```\n- Verify that all the resources are online:```\nsudo pcs status\n```The output looks like this:```\nOnline: [ database1 database2 ]\nFull list of resources:\n Master/Slave Set: primary_mysql [mysql_drbd]\n  Masters: [ database1 ]\n  Slaves: [ database2 ]\n mystore_FS  (ocf::heartbeat:Filesystem): Started database1\n mysql_service (ocf::heartbeat:mysql): Started database1\n```\n## Configuring a quorum device\n- Connect to the`qdevice`instance through SSH.\n- Install `pcs` and `corosync-qnetd` :```\nsudo apt update && sudo apt -y install pcs corosync-qnetd\n```\n- Start the Pacemaker/Corosync configuration system daemon ( `pcsd` service and enable it on system start:```\nsudo service pcsd startsudo update-rc.d pcsd enable\n```\n- Set the cluster user password ( `haCLUSTER3` ) for authentication:```\nsudo bash -c \"echo \u00a0hacluster:haCLUSTER3 | chpasswd\"\n```\n- Check the quorum device status:```\nsudo pcs qdevice status net --full\n```The output looks like this:```\nQNetd address:     *:5403\nTLS:       Supported (client certificate required)\nConnected clients:    0\nConnected clusters:    0\nMaximum send/receive size:  32768/32768 bytes\n```\n### Configure the quorum device settings on database1\n- Connect to the`database1`node through SSH.\n- Load the metadata variables from the `.varsrc` file:```\nsource ~/.varsrc\n```\n- Authenticate the quorum device node for the cluster:```\nsudo pcs cluster auth --name mysql_cluster ${QUORUM_INSTANCE_NAME} -u hacluster -p haCLUSTER3\n```\n- Add the quorum device to the cluster. Use the [ffsplit algorithm](https://manpages.debian.org/testing/corosync-qdevice/corosync-qdevice.8.en.html#MODEL_NET_ALGORITHMS) , which guarantees that the active node will be decided based on 50% of the votes or more:```\nsudo pcs quorum device add model net host=${QUORUM_INSTANCE_NAME} algorithm=ffsplit\n``` **Note:** You might see error messages indicating that `corosync-qdevice` failed to enable on `database1` and `database2` . Ignore those errors for now.\n- Add the quorum setting to `corosync.conf` :```\nsudo bash -c \"cat <<EOF \u00a0> /etc/corosync/corosync.conftotem {\u00a0 \u00a0 version: 2\u00a0 \u00a0 cluster_name: mysql_cluster\u00a0 \u00a0 transport: udpu\u00a0 \u00a0 interface {\u00a0 \u00a0 \u00a0 \u00a0 ringnumber: 0\u00a0 \u00a0 \u00a0 \u00a0 Bindnetaddr: ${DATABASE1_INSTANCE_IP}\u00a0 \u00a0 \u00a0 \u00a0 broadcast: yes\u00a0 \u00a0 \u00a0 \u00a0 mcastport: 5405\u00a0 \u00a0 }}quorum {\u00a0 \u00a0 provider: corosync_votequorum\u00a0 \u00a0 device {\u00a0 \u00a0 \u00a0 \u00a0 votes: 1\u00a0 \u00a0 \u00a0 \u00a0 model: net\u00a0 \u00a0 \u00a0 \u00a0 net {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tls: on\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 host: ${QUORUM_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 algorithm: ffsplit\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }}nodelist {\u00a0 \u00a0 node {\u00a0 \u00a0 \u00a0 \u00a0 ring0_addr: ${DATABASE1_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 name: ${DATABASE1_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 nodeid: 1\u00a0 \u00a0 }\u00a0 \u00a0 node {\u00a0 \u00a0 \u00a0 \u00a0 ring0_addr: ${DATABASE2_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 name: ${DATABASE2_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 nodeid: 2\u00a0 \u00a0 }}logging {\u00a0 \u00a0 to_logfile: yes\u00a0 \u00a0 logfile: /var/log/corosync/corosync.log\u00a0 \u00a0 timestamp: on}EOF\"\n```\n- Restart the `corosync` service to reload the new quorum device setting:```\nsudo service corosync restart\n```\n- Start the `corosync` quorum device daemon and bring it up at system start:```\nsudo service corosync-qdevice startsudo update-rc.d corosync-qdevice defaults\n```\n### Configure the quorum device settings on database2\n- Connect to the`database2`node through SSH.\n- Load the metadata variables from the `.varsrc` file:```\nsource ~/.varsrc\n```\n- Add a quorum setting to `corosync.conf` :```\nsudo bash -c \"cat <<EOF \u00a0> /etc/corosync/corosync.conftotem {\u00a0 \u00a0 version: 2\u00a0 \u00a0 cluster_name: mysql_cluster\u00a0 \u00a0 transport: udpu\u00a0 \u00a0 interface {\u00a0 \u00a0 \u00a0 \u00a0 ringnumber: 0\u00a0 \u00a0 \u00a0 \u00a0 Bindnetaddr: ${DATABASE2_INSTANCE_IP}\u00a0 \u00a0 \u00a0 \u00a0 broadcast: yes\u00a0 \u00a0 \u00a0 \u00a0 mcastport: 5405\u00a0 \u00a0 }}quorum {\u00a0 \u00a0 provider: corosync_votequorum\u00a0 \u00a0 device {\u00a0 \u00a0 \u00a0 \u00a0 votes: 1\u00a0 \u00a0 \u00a0 \u00a0 model: net\u00a0 \u00a0 \u00a0 \u00a0 net {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tls: on\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 host: ${QUORUM_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 algorithm: ffsplit\u00a0 \u00a0 \u00a0 \u00a0 }\u00a0 \u00a0 }}nodelist {\u00a0 \u00a0 node {\u00a0 \u00a0 \u00a0 \u00a0 ring0_addr: ${DATABASE1_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 name: ${DATABASE1_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 nodeid: 1\u00a0 \u00a0 }\u00a0 \u00a0 node {\u00a0 \u00a0 \u00a0 \u00a0 ring0_addr: ${DATABASE2_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 name: ${DATABASE2_INSTANCE_NAME}\u00a0 \u00a0 \u00a0 \u00a0 nodeid: 2\u00a0 \u00a0 }}logging {\u00a0 \u00a0 to_logfile: yes\u00a0 \u00a0 logfile: /var/log/corosync/corosync.log\u00a0 \u00a0 timestamp: on}EOF\"\n```\n- Restart the `corosync` service to reload the new quorum device setting:```\nsudo service corosync restart\n```\n- Start the Corosync quorum device daemon and configure it to bring it up at system start:```\nsudo service corosync-qdevice startsudo update-rc.d corosync-qdevice defaults\n```\n## Verifying the cluster statusThe next step is to verify that the cluster resources are online.- Connect to the`database1`instance through SSH.\n- Verify the cluster status:```\nsudo pcs status\n```The output looks like this:```\nCluster name: mysql_cluster\nStack: corosync\nCurrent DC: database1 (version 1.1.16-94ff4df) - partition with quorum\nLast updated: Sun Nov 4 01:49:18 2018\nLast change: Sat Nov 3 15:48:21 2018 by root via cibadmin on database1\n2 nodes configured\n4 resources configured\nOnline: [ database1 database2 ]\nFull list of resources:\n Master/Slave Set: primary_mysql [mysql_drbd]\n  Masters: [ database1 ]\n  Slaves: [ database2 ]\n mystore_FS  (ocf::heartbeat:Filesystem): Started database1\n mysql_service (ocf::heartbeat:mysql): Started database1\nDaemon Status:\n corosync: active/enabled\n pacemaker: active/enabled\n pcsd: active/enabled\n```\n- Show the quorum status:```\nsudo pcs quorum status\n```The output looks like this:```\nQuorum information\n-----------------Date:    Sun Nov 4 01:48:25 2018\nQuorum provider: corosync_votequorum\nNodes:   2\nNode ID:   1\nRing ID:   1/24\nQuorate:   Yes\nVotequorum information\n---------------------Expected votes: 3\nHighest expected: 3\nTotal votes:  3\nQuorum:   2\nFlags:   Quorate Qdevice\nMembership information\n--------------------- Nodeid  Votes Qdevice Name\n   1   1 A,V,NMW database1 (local)\n   2   1 A,V,NMW database2\n   0   1   Qdevice\n```\n- Show the quorum device status:```\nsudo pcs quorum device status\n```The output looks like this:```\nQdevice information\n------------------Model:     Net\nNode ID:    1\nConfigured node list:\n 0 Node ID = 1\n 1 Node ID = 2\nMembership node list: 1, 2\nQdevice-net information\n---------------------Cluster name:   mysql_cluster\nQNetd host:    qdevice:5403\nAlgorithm:    Fifty-Fifty split\nTie-breaker:   Node with lowest node ID\nState:     Connected\n```\n## Configuring an internal load balancer as the cluster IP\n- Open Cloud Shell: [OPEN Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Create an unmanaged instance group and add the `database1` instance to it:```\ngcloud compute instance-groups unmanaged create ${DATABASE1_INSTANCE_NAME}-instance-group \\\u00a0 \u00a0 --zone=${DATABASE1_INSTANCE_ZONE} \\\u00a0 \u00a0 --description=\"${DATABASE1_INSTANCE_NAME} unmanaged instance group\"gcloud compute instance-groups unmanaged add-instances ${DATABASE1_INSTANCE_NAME}-instance-group \\\u00a0 \u00a0 --zone=${DATABASE1_INSTANCE_ZONE} \\\u00a0 \u00a0 --instances=${DATABASE1_INSTANCE_NAME}\n```\n- Create an unmanaged instance group and add the `database2` instance to it:```\ngcloud compute instance-groups unmanaged create ${DATABASE2_INSTANCE_NAME}-instance-group \\\u00a0 \u00a0 --zone=${DATABASE2_INSTANCE_ZONE} \\\u00a0 \u00a0 --description=\"${DATABASE2_INSTANCE_NAME} unmanaged instance group\"gcloud compute instance-groups unmanaged add-instances ${DATABASE2_INSTANCE_NAME}-instance-group \\\u00a0 \u00a0 --zone=${DATABASE2_INSTANCE_ZONE} \\\u00a0 \u00a0 --instances=${DATABASE2_INSTANCE_NAME}\n```\n- Create a health check for `port 3306` :```\ngcloud compute health-checks create tcp mysql-backend-healthcheck \\\u00a0 \u00a0 --port 3306\n```\n- Create a regional internal backend service:```\ngcloud compute backend-services create mysql-ilb \\\u00a0 \u00a0 --load-balancing-scheme internal \\\u00a0 \u00a0 --region ${CLUSTER_REGION} \\\u00a0 \u00a0 --health-checks mysql-backend-healthcheck \\\u00a0 \u00a0 --protocol tcp\n```\n- Add the two instance groups as backends to the backend service:```\ngcloud compute backend-services add-backend mysql-ilb \\\u00a0 \u00a0 --instance-group ${DATABASE1_INSTANCE_NAME}-instance-group \\\u00a0 \u00a0 --instance-group-zone ${DATABASE1_INSTANCE_ZONE} \\\u00a0 \u00a0 --region ${CLUSTER_REGION}gcloud compute backend-services add-backend mysql-ilb \\\u00a0 \u00a0 --instance-group ${DATABASE2_INSTANCE_NAME}-instance-group \\\u00a0 \u00a0 --instance-group-zone ${DATABASE2_INSTANCE_ZONE} \\\u00a0 \u00a0 --region ${CLUSTER_REGION}\n```\n- Create a forwarding rule for the load balancer:```\ngcloud compute forwarding-rules create mysql-ilb-forwarding-rule \\\u00a0 \u00a0 --load-balancing-scheme internal \\\u00a0 \u00a0 --ports 3306 \\\u00a0 \u00a0 --network default \\\u00a0 \u00a0 --subnet default \\\u00a0 \u00a0 --region ${CLUSTER_REGION} \\\u00a0 \u00a0 --address ${ILB_IP} \\\u00a0 \u00a0 --backend-service mysql-ilb\n```\n- Create a firewall rule to allow an internal load balancer health check:```\ngcloud compute firewall-rules create allow-ilb-healthcheck \\\u00a0 \u00a0 --direction=INGRESS --network=default \\\u00a0 \u00a0 --action=ALLOW --rules=tcp:3306 \\\u00a0 \u00a0 --source-ranges=130.211.0.0/22,35.191.0.0/16 --target-tags=mysql\n```\n- To check the status of your load balancer, go to the **Load balancing** page in the Google Cloud console. [OPEN THE LOAD BALANCING PAGE](https://console.cloud.google.com/net-services/loadbalancing/loadBalancers/list) \n- Click `mysql-ilb` : **Note:** it might take a couple of minutes before the instance becomes healthy.Because the cluster allows only one instance to run MySQL at any given time, only one instance is healthy from the internal load balancer's perspective. \n## Connecting to the cluster from the MySQL client\n- Connect to the`mysql-client`instance through SSH.\n- Update the package definitions:```\nsudo apt-get update\n```\n- Install the MySQL client:```\nsudo apt-get install -y mysql-client\n```\n- Create a script file that creates and populates a table with sample data:```\ncat <<EOF > db_creation.sqlCREATE DATABASE source_db;use source_db;CREATE TABLE source_table(\u00a0 \u00a0 id BIGINT NOT NULL AUTO_INCREMENT,\u00a0 \u00a0 timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP,\u00a0 \u00a0 event_data float DEFAULT NULL,\u00a0 \u00a0 PRIMARY KEY (id));DELIMITER $$CREATE PROCEDURE simulate_data()BEGINDECLARE i INT DEFAULT 0;WHILE i < 100 DO\u00a0 \u00a0 INSERT INTO source_table (event_data) VALUES (ROUND(RAND()*15000,2));\u00a0 \u00a0 SET i = i + 1;END WHILE;END$$DELIMITER ;CALL simulate_data()EOF\n```\n- Create the table:```\nILB_IP=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/ILB_IP\" -H \"Metadata-Flavor: Google\")mysql -u root -pDRBDha2 \"-h${ILB_IP}\" < db_creation.sql\n```\n## Testing the clusterTo test the HA capability of the deployed cluster, you can perform the following tests:- Shut down the`database1`instance to test whether the master database can fail over to the`database2`instance.\n- Start the`database1`instance to see if`database1`can successfully rejoin the cluster.\n- Shut down the`database2`instance to test whether the master database can fail over to the`database1`instance.\n- Start the`database2`instance to see whether`database2`can successfully rejoin the cluster and whether`database1`instance still keeps the master role.\n- Create a network partition between`database1`and`database2`to simulate a split-brain issue.\n- Open Cloud Shell: [OPEN Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Stop the `database1` instance:```\ngcloud compute instances stop ${DATABASE1_INSTANCE_NAME} \\\u00a0 \u00a0 --zone=${DATABASE1_INSTANCE_ZONE}\n```\n- Check the status of the cluster:```\ngcloud compute ssh ${DATABASE2_INSTANCE_NAME} \\\u00a0 \u00a0 --zone=${DATABASE2_INSTANCE_ZONE} \\\u00a0 \u00a0 --command=\"sudo pcs status\"\n```The output looks like the following. Verify that the configuration changes you made took place:```\n2 nodes configured\n4 resources configured\nOnline: [ database2 ]\nOFFLINE: [ database1 ]\nFull list of resources:\n Master/Slave Set: primary_mysql [mysql_drbd]\n  Masters: [ database2 ]\n  Stopped: [ database1 ]\n mystore_FS  (ocf::heartbeat:Filesystem): Started database2\n mysql_service (ocf::heartbeat:mysql): Started database2\nDaemon Status:\n corosync: active/enabled\n pacemaker: active/enabled\n pcsd: active/enabled\n```\n- Start the `database1` instance:```\ngcloud compute instances start ${DATABASE1_INSTANCE_NAME} \\\u00a0 \u00a0 --zone=${DATABASE1_INSTANCE_ZONE}\n```\n- Check the status of the cluster:```\ngcloud compute ssh ${DATABASE1_INSTANCE_NAME} \\\u00a0 \u00a0 --zone=${DATABASE1_INSTANCE_ZONE} \\\u00a0 \u00a0 --command=\"sudo pcs status\"\n```The output looks like this:```\n2 nodes configured\n4 resources configured\nOnline: [ database1 database2 ]\nFull list of resources:\n Master/Slave Set: primary_mysql [mysql_drbd]\n  Masters: [ database2 ]\n  Slaves: [ database1 ]\n mystore_FS  (ocf::heartbeat:Filesystem): Started database2\n mysql_service (ocf::heartbeat:mysql): Started database2\nDaemon Status:\n corosync: active/enabled\n pacemaker: active/enabled\n pcsd: active/enabled\n```\n- Stop the `database2` instance:```\ngcloud compute instances stop ${DATABASE2_INSTANCE_NAME} \\\u00a0 \u00a0 --zone=${DATABASE2_INSTANCE_ZONE}\n```\n- Check the status of the cluster:```\ngcloud compute ssh ${DATABASE1_INSTANCE_NAME} \\\u00a0 \u00a0 --zone=${DATABASE1_INSTANCE_ZONE} \\\u00a0 \u00a0 --command=\"sudo pcs status\"\n```The output looks like this:```\n2 nodes configured\n4 resources configured\nOnline: [ database1 ]\nOFFLINE: [ database2 ]\nFull list of resources:\n Master/Slave Set: primary_mysql [mysql_drbd]\n  Masters: [ database1 ]\n  Stopped: [ database2 ]\n mystore_FS  (ocf::heartbeat:Filesystem): Started database1\n mysql_service (ocf::heartbeat:mysql): Started database1\nDaemon Status:\n corosync: active/enabled\n pacemaker: active/enabled\n pcsd: active/enabled\n```\n- Start the `database2` instance:```\ngcloud compute instances start ${DATABASE2_INSTANCE_NAME} \\\u00a0 \u00a0 --zone=${DATABASE2_INSTANCE_ZONE}\n```\n- Check the status of the cluster:```\ngcloud compute ssh ${DATABASE1_INSTANCE_NAME} \\\u00a0 \u00a0 --zone=${DATABASE1_INSTANCE_ZONE} \\\u00a0 \u00a0 --command=\"sudo pcs status\"\n```The output looks like this:```\n2 nodes configured\n4 resources configured\nOnline: [ database1 database2 ]\nFull list of resources:\n Master/Slave Set: primary_mysql [mysql_drbd]\n  Masters: [ database1 ]\n  Slaves: [ database2 ]\n mystore_FS  (ocf::heartbeat:Filesystem): Started database1\n mysql_service (ocf::heartbeat:mysql): Started database1\nDaemon Status:\n corosync: active/enabled\n pacemaker: active/enabled\n pcsd: active/enabled\n```\n- Create a network partition between `database1` and `database2` :```\ngcloud compute firewall-rules create block-comms \\\u00a0 \u00a0 --description=\"no MySQL communications\" \\\u00a0 \u00a0 --action=DENY \\\u00a0 \u00a0 --rules=all \\\u00a0 \u00a0 --source-tags=mysql \\\u00a0 \u00a0 --target-tags=mysql \\\u00a0 \u00a0 --priority=800\n```\n- After a couple of minutes, check the status of the cluster. Note how `database1` keeps its primary role, because the quorum policy is the lowest ID node first under network partition situation. In the meantime, the `database2` MySQL service is stopped. This quorum mechanism avoids the split-brain issue when the network partition occurs.```\ngcloud compute ssh ${DATABASE1_INSTANCE_NAME} \\\u00a0 \u00a0 --zone=${DATABASE1_INSTANCE_ZONE} \\\u00a0 \u00a0 --command=\"sudo pcs status\"\n```The output looks like this:```\n2 nodes configured\n4 resources configured\nOnline: [ database1 ]\nOFFLINE: [ database2 ]\nFull list of resources:\n Master/Slave Set: primary_mysql [mysql_drbd]\n  Masters: [ database1 ]\n  Stopped: [ database2 ]\n mystore_FS  (ocf::heartbeat:Filesystem): Started database1\n mysql_service (ocf::heartbeat:mysql): Started database1\n```\n- Delete the network firewall rule to remove the network partition. (Press `Y` when prompted.)```\ngcloud compute firewall-rules delete block-comms\n```\n- Verify that the cluster status is back to normal:```\ngcloud compute ssh ${DATABASE1_INSTANCE_NAME} \\\u00a0 \u00a0 --zone=${DATABASE1_INSTANCE_ZONE} \\\u00a0 \u00a0 --command=\"sudo pcs status\"\n```The output looks like this:```\n2 nodes configured\n4 resources configured\nOnline: [ database1 database2 ]\nFull list of resources:\n Master/Slave Set: primary_mysql [mysql_drbd]\n  Masters: [ database1 ]\n  Slaves: [ database2 ]\n mystore_FS  (ocf::heartbeat:Filesystem): Started database1\n mysql_service (ocf::heartbeat:mysql): Started database1\n``` **Note** : You might see timeout errors in the following output. It's just an informational log from Pacemaker that doesn't affect any DB functionality. You can clean this error log by running the following command: `sudo pcs resource cleanup` .```\nFailed Actions:\n* mysql_drbd_monitor_0 on database2 'unknown error' (1): call=66, status=Timed Out, exitreason='none',\n last-rc-change='Wed Mar 13 05:13:43 2019', queued=0ms, exec=30002ms\n```\n- Connect to the `mysql-client` instance through SSH.\n- In your shell, query the table you created previously:```\nILB_IP=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/ILB_IP\" -H \"Metadata-Flavor: Google\")mysql -uroot \"-h${ILB_IP}\" -pDRBDha2 -e \"select * from source_db.source_table LIMIT 10\"\n```The output should list 10 records of the following form, verifying the data consistency in the cluster:```\n+----+---------------------+------------+\n| id | timestamp   | event_data |\n+----+---------------------+------------+\n| 1 | 2018-11-27 21:00:09 | 1279.06 |\n| 2 | 2018-11-27 21:00:09 | 4292.64 |\n| 3 | 2018-11-27 21:00:09 | 2626.01 |\n| 4 | 2018-11-27 21:00:09 |  252.13 |\n| 5 | 2018-11-27 21:00:09 | 8382.64 |\n| 6 | 2018-11-27 21:00:09 | 11156.8 |\n| 7 | 2018-11-27 21:00:09 |  636.1 |\n| 8 | 2018-11-27 21:00:09 | 14710.1 |\n| 9 | 2018-11-27 21:00:09 | 11642.1 |\n| 10 | 2018-11-27 21:00:09 | 14080.3 |\n+----+---------------------+------------+\n```\n## Failover sequenceIf the primary node in the cluster goes down, the failover sequence looks like this:- Both the quorum device and the standby node lose connectivity with the primary node.\n- The quorum device votes for the standby node, and the standby node votes for itself.\n- Quorum is acquired by the standby node.\n- The standby node is promoted to primary.\n- The new primary node does the following:- Promotes DRBD to primary\n- Mounts the MySQL data disk from DRBD\n- Starts MySQL\n- Becomes healthy for the load balancer\n- The load balancer starts sending traffic to the new primary node.\n## Clean up\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n## What's next\n- Read more about [DRBD](https://docs.linbit.com/) .\n- Read more about [Pacemaker](https://wiki.clusterlabs.org/wiki/Pacemaker) .\n- Read more about [Corosync Cluster Engine](https://wikipedia.org/wiki/Corosync_Cluster_Engine) .\n- For more advanced MySQL server 5.6 settings, see [MySQL server administration manual](https://dev.mysql.com/doc/refman/5.6/en/mysqld-server.html) .\n- If you want to set up remote access to MySQL, see [How to set up remote access to MySQL on Compute Engine](/solutions/mysql-remote-access) .\n- For more information about MySQL, see the [official MySQL documentation](https://dev.mysql.com/doc/) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Compute Engine"}