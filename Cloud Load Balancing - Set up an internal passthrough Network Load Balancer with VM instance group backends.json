{"title": "Cloud Load Balancing - Set up an internal passthrough Network Load Balancer with VM instance group backends", "url": "https://cloud.google.com/compute/docs/instances", "abstract": "# Cloud Load Balancing - Set up an internal passthrough Network Load Balancer with VM instance group backends\nThis guide uses an example to teach the fundamentals of Google Cloud internal passthrough Network Load Balancers. Before following this guide, familiarize yourself with the following:\n- [Internal passthrough Network Load Balancer concepts](/load-balancing/docs/internal) \n- [How internal passthrough Network Load Balancers work](/load-balancing/docs/internal#how_ilb_works) \n- [Firewall rules overview](/vpc/docs/firewalls) \n- [Health check concepts](/load-balancing/docs/health-check-concepts) To follow step-by-step guidance for this task directly in the Google Cloud console, click **Guide me** :\n[Guide me](https://console.cloud.google.com/welcome?walkthrough_id=load-balancing--internal-tcp-udp-load-balancer-vm-backends)\n", "content": "## Permissions\nTo follow this guide, you need to create instances and modify a network in a project. You should be either a project [owner oreditor](/iam/docs/understanding-roles#basic) , or you should have all of the following [Compute Engine IAM roles](/compute/docs/access/iam) :\n| Task             | Required Role   |\n|:-------------------------------------------------------|:-----------------------|\n| Create networks, subnets, and load balancer components | Network Admin   |\n| Add and remove firewall rules       | Security Admin   |\n| Create instances          | Compute Instance Admin |\nFor more information, see the following guides:\n- [Access control](/load-balancing/docs/access-control) \n- [IAM Conditions](/load-balancing/docs/access-control/iam-conditions) ## Set up load balancer with single-stack subnets\nThis guide shows you how to configure and test an internal passthrough Network Load Balancer. The steps in this section describe how to configure the following:\n- An example that uses a [custom mode VPCnetwork](/vpc/docs/vpc#subnet-ranges) named`lb-network`.\n- Asubnet (`stack-type`set to`IPv4`), which is required for IPv4 traffic. When you create a single stack subnet on a custom mode VPC network, you choose an [IPv4 subnet range](/vpc/docs/subnets#ipv4-ranges) for the subnet.\n- Firewall rules that allow incoming connections to backend VMs.\n- The backend instance group, which is located in the following region and subnet for this example:- Region:`us-west1`\n- Subnet:`lb-subnet`, with primary IPv4 address range`10.1.2.0/24`.\n- Four backend VMs: two VMs in an unmanaged instance group in zone`us-west1-a`and two VMs in an unmanaged instance group in zone`us-west1-c`. To demonstrate [global access](/load-balancing/docs/internal#client-access) , this example creates a second test client VM in a different region and subnet:- Region:`europe-west1`\n- Subnet:`europe-subnet`, with primary IP address range`10.3.4.0/24`\n- One client VM to test connections.\n- The following internal passthrough Network Load Balancer components:- A health check for the backend service.\n- An internal backend service in the`us-west1`region to manage connection distribution to the two zonal instance groups.\n- An internal forwarding rule and internal IP address for the frontend of the load balancer.**Note:** You can change the name of the network, the region, and the parameters for the subnet; however, subsequent steps in this guide use the network, region, and subnet parameters as outlined in the previous steps.\nThe architecture for this example looks like this:\n**Note:** For simplicity, this example uses unmanaged instance groups. In production, consider using a managed instance group, as illustrated in [Managed instance groups](#internal_load_balancing_with_regional_instance_groups) .\n### Configure a network, region, and subnet\nTo create the example network and subnet, follow these steps.\n- In the Google Cloud console, go to the **VPC networks** page. [Go to VPC networks](https://console.cloud.google.com/networking/networks/list) \n- Click **Create VPC network** .\n- For **Name** , enter `lb-network` .\n- In the **Subnets** section, do the following:- Set the **Subnet creation mode** to **Custom** .\n- In the **New subnet** section, enter the following information:- **Name** :`lb-subnet`\n- **Region** :`us-west1`\n- **IP stack type** : **IPv4 (single-stack)** \n- **IP address range** :`10.1.2.0/24`\n- Click **Done** .\n- Click **Add subnet** and enter the following information:- **Name** :`europe-subnet`\n- **Region** :`europe-west1`\n- **IP stack type** : **IPv4 (single-stack)** \n- **IP address range** :`10.3.4.0/24`\n- Click **Done** .\n- Click **Create** .\n- Create the custom VPC network:```\ngcloud compute networks create lb-network --subnet-mode=custom\n```\n- In the `lb-network` network, create a subnet for backends in the `us-west1` region:```\ngcloud compute networks subnets create lb-subnet \\\n --network=lb-network \\\n --range=10.1.2.0/24 \\\n --region=us-west1\n```\n- In the `lb-network` network, create another subnet for testing global access in the `europe-west1` region:```\ngcloud compute networks subnets create europe-subnet \\\n --network=lb-network \\\n --range=10.3.4.0/24 \\\n --region=europe-west1\n```\nMake a `POST` request to the [networks.insert method](/compute/docs/reference/rest/v1/networks/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks\n{\n \"routingConfig\": {\n \"routingMode\": \"REGIONAL\"\n },\n \"name\": \"lb-network\",\n \"autoCreateSubnetworks\": false\n}\n```\nMake two `POST` requests to the [subnetworks.insert method](/compute/docs/reference/rest/v1/subnetworks/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks\n{\n \"name\": \"lb-subnet\",\n \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n \"ipCidrRange\": \"10.1.2.0/24\",\n \"privateIpGoogleAccess\": false\n}\n```\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/europe-west1/subnetworks\n{\n \"name\": \"europe-subnet\",\n \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n \"ipCidrRange\": \"10.3.4.0/24\",\n \"privateIpGoogleAccess\": false\n}\n```\n### Configure firewall rules\nThis example uses the following firewall rules:\n- `fw-allow-lb-access` : An ingress rule, applicable to all targets in the VPC network, allowing traffic from sources in the `10.1.2.0/24` and `10.3.4.0/24` ranges. This rule allows incoming traffic from any client located in either of the two subnets. It later lets you to [configure and test global access](#ilb-global-access) .\n- `fw-allow-ssh` : An ingress rule, applicable to the instances being load balanced, that allows incoming SSH connectivity on TCP port 22 from any address. You can choose a more restrictive source IP range for this rule; for example, you can specify just the IP ranges of the system from which you will be initiating SSH sessions. This example uses the target tag `allow-ssh` to identify the VMs to which it should apply.\n- `fw-allow-health-check` : An ingress rule, applicable to the instances being load balanced, that allows traffic from the Google Cloud health checking systems ( `130.211.0.0/22` and `35.191.0.0/16` ). This example uses the target tag `allow-health-check` to identify the instances to which it should apply.\nWithout these firewall rules, the [default denyingress](/vpc/docs/firewalls#default_firewall_rules) rule blocks incoming traffic to the backend instances.\n**Note:** You must create a firewall rule to allow health checks from the IP ranges of Google Cloud probe systems. For more information, see [probe IP ranges](/load-balancing/docs/health-check-concepts) .\n- In the Google Cloud console, go to the **Firewall policies** page. [Go to Firewall policies](https://console.cloud.google.com/net-security/firewall-manager/firewall-policies/list) \n- To allow subnet traffic, click **Create firewall rule** and enter the following information:- **Name** :`fw-allow-lb-access`\n- **Network** :`lb-network`\n- **Priority** :`1000`\n- **Direction of traffic** : **ingress** \n- **Action on match** : **allow** \n- **Targets** : **All instances in the network** \n- **Source filter** : **IPv4 ranges** \n- **Source IPv4 ranges** :`10.1.2.0/24`\n- **Protocols and ports** : **Allow all** \n- Click **Create** .\n- To allow incoming SSH connections, click **Create firewall rule** again and enter the following information:- **Name** :`fw-allow-ssh`\n- **Network** :`lb-network`\n- **Priority** :`1000`\n- **Direction of traffic** : **ingress** \n- **Action on match** : **allow** \n- **Targets** : **Specified target tags** \n- **Target tags** :`allow-ssh`\n- **Source filter** : **IPv4 ranges** \n- **Source IPv4 ranges** :`0.0.0.0/0`\n- **Protocols and ports** : Select **Specified protocols and ports** , select the **TCP** checkbox, and then enter`22`in **Ports** .\n- Click **Create** .\n- To allow Google Cloud health checks, click **Create firewall rule** a third time and enter the following information:- **Name** :`fw-allow-health-check`\n- **Network** :`lb-network`\n- **Priority** :`1000`\n- **Direction of traffic** : **ingress** \n- **Action on match** : **allow** \n- **Targets** : **Specified target tags** \n- **Target tags** :`allow-health-check`\n- **Source filter** : **IPv4 ranges** \n- **Source IPv4 ranges** :`130.211.0.0/22`and`35.191.0.0/16`\n- **Protocols and ports** : **Allow all** \n- Click **Create** .\n- Create the `fw-allow-lb-access` firewall rule to allow communication from within the subnet:```\ngcloud compute firewall-rules create fw-allow-lb-access \\\n --network=lb-network \\\n --action=allow \\\n --direction=ingress \\\n --source-ranges=10.1.2.0/24,10.3.4.0/24 \\\n --rules=tcp,udp,icmp\n```\n- Create the `fw-allow-ssh` firewall rule to allow SSH connectivity to VMs with the network tag `allow-ssh` . When you omit `source-ranges` , Google Cloud [interprets the rule to mean anysource](/vpc/docs/firewalls#sources_for_the_rule) .```\ngcloud compute firewall-rules create fw-allow-ssh \\\n --network=lb-network \\\n --action=allow \\\n --direction=ingress \\\n --target-tags=allow-ssh \\\n --rules=tcp:22\n```\n- Create the `fw-allow-health-check` rule to allow Google Cloud health checks.```\ngcloud compute firewall-rules create fw-allow-health-check \\\n --network=lb-network \\\n --action=allow \\\n --direction=ingress \\\n --target-tags=allow-health-check \\\n --source-ranges=130.211.0.0/22,35.191.0.0/16 \\\n --rules=tcp,udp,icmp\n```\nCreate the `fw-allow-lb-access` firewall rule by making a `POST` request to the [firewalls.insert method](/compute/docs/reference/rest/v1/firewalls/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/firewalls\n{\n \"name\": \"fw-allow-lb-access\",\n \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n \"priority\": 1000,\n \"sourceRanges\": [ \"10.1.2.0/24\", \"10.3.4.0/24\"\n ],\n \"allowed\": [ {\n  \"IPProtocol\": \"tcp\"\n },\n {\n  \"IPProtocol\": \"udp\"\n },\n {\n  \"IPProtocol\": \"icmp\"\n }\n ],\n \"direction\": \"INGRESS\",\n \"logConfig\": {\n \"enable\": false\n },\n \"disabled\": false\n}\n```\nCreate the `fw-allow-ssh` firewall rule by making a `POST` request to the [firewalls.insert method](/compute/docs/reference/rest/v1/firewalls/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/firewalls\n{\n \"name\": \"fw-allow-ssh\",\n  \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n \"priority\": 1000,\n \"sourceRanges\": [ \"0.0.0.0/0\"\n ],\n \"targetTags\": [ \"allow-ssh\"\n ],\n \"allowed\": [ {\n \"IPProtocol\": \"tcp\",\n \"ports\": [  \"22\"\n ]\n }\n ],\n\"direction\": \"INGRESS\",\n\"logConfig\": {\n \"enable\": false\n},\n\"disabled\": false\n}\n```\nCreate the `fw-allow-health-check` firewall rule by making a `POST` request to the [firewalls.insert method](/compute/docs/reference/rest/v1/firewalls/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/firewalls\n{\n \"name\": \"fw-allow-health-check\",\n \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n \"priority\": 1000,\n \"sourceRanges\": [ \"130.211.0.0/22\",\n \"35.191.0.0/16\"\n ],\n \"targetTags\": [ \"allow-health-check\"\n ],\n \"allowed\": [ {\n  \"IPProtocol\": \"tcp\"\n },\n {\n  \"IPProtocol\": \"udp\"\n },\n {\n  \"IPProtocol\": \"icmp\"\n }\n ],\n \"direction\": \"INGRESS\",\n \"logConfig\": {\n \"enable\": false\n },\n \"disabled\": false\n}\n```\n### Create backend VMs and instance groups\nThis example uses two instance groups each having two backend (server) VMs. To demonstrate the regional nature of internal passthrough Network Load Balancers, the two instance groups are placed in separate zones, `us-west1-a` and `us-west1-c` .\n- Instance group`ig-a`contains these two VMs:- `vm-a1`\n- `vm-a2`\n- Instance group`ig-c`contains these two VMs:- `vm-c1`\n- `vm-c2`Traffic to all four of the backend VMs is load balanced.\nTo support this example and the additional configuration options, each of the four VMs runs an Apache web server that listens on the following TCP ports: 80, 8008, 8080, 8088, 443, and 8443.\nEach VM is assigned an internal IP address in the `lb-subnet` and an ephemeral external (public) IP address. You can [remove the external IP addresseslater](#remove_external_ip) .\nExternal IP address for the backend VMs are not required; however, they are useful for this example because they permit the backend VMs to download Apache from the internet, and they can [connect usingSSH](/compute/docs/instances/connecting-to-instance#connect_to_vms) .\nBy default, Apache is configured to bind to any IP address. Internal passthrough Network Load Balancers deliver packets by preserving the destination IP. Ensure that server software running on your backend VMs is listening on the IP address of the load balancer's internal forwarding rule. If you configure [multiple internal forwarding rules](/load-balancing/docs/internal#multiple_forwarding_rule) , ensure that your software listens to the internal IP address associated with each one. The destination IP address of a packet delivered to a backend VM by an internal passthrough Network Load Balancer is the internal IP address of the forwarding rule.\nFor instructional simplicity, these backend VMs run Debian Debian GNU/Linux 10.\n**Create backend VMs** - In the Google Cloud console, go to the **VM instances** page. [Go to VM instances](https://console.cloud.google.com/compute/instances) \n- Repeat steps 3 to 8 for each VM, using the following name and zone combinations.- Name:`vm-a1`, zone:`us-west1-a`\n- Name:`vm-a2`, zone:`us-west1-a`\n- Name:`vm-c1`, zone:`us-west1-c`\n- Name:`vm-c2`, zone:`us-west1-c`\n- Click **Create instance** .\n- Set the **Name** as indicated in step 2.\n- For **Region** , select **us-west1** , and choose a **Zone** as indicated in step 2.\n- In the **Boot disk** section, ensure that the **Debian** operating system and the **10 (buster)** version are selected for the boot disk options. If necessary, click **Change** to change the image.\n- Click **Advanced options** .\n- Click **Networking** and configure the following fields:- For **Network tags** , enter`allow-ssh`and`allow-health-check`.\n- For **Network interfaces** , select the following:- **Network** :`lb-network`\n- **Subnet** :`lb-subnet`\n- **IP stack type** : **IPv4 (single-stack)** \n- **Primary internal IPv4 address** : **Ephemeral (automatic)** \n- **External IPv4 address** : **Ephemeral** \n- Click **Management** , and then in the **Startup script** field, enter the following script. The script contents are identical for all four VMs.```\n#! /bin/bash\nif [ -f /etc/startup_script_completed ]; then\nexit 0\nfi\napt-get update\napt-get install apache2 -y\na2ensite default-ssl\na2enmod ssl\nfile_ports=\"/etc/apache2/ports.conf\"\nfile_http_site=\"/etc/apache2/sites-available/000-default.conf\"\nfile_https_site=\"/etc/apache2/sites-available/default-ssl.conf\"\nhttp_listen_prts=\"Listen 80\\nListen 8008\\nListen 8080\\nListen 8088\"\nhttp_vh_prts=\"*:80 *:8008 *:8080 *:8088\"\nhttps_listen_prts=\"Listen 443\\nListen 8443\"\nhttps_vh_prts=\"*:443 *:8443\"\nvm_hostname=\"$(curl -H \"Metadata-Flavor:Google\" \\\nhttp://metadata.google.internal/computeMetadata/v1/instance/name)\"\necho \"Page served from: $vm_hostname\" | \\\ntee /var/www/html/index.html\nprt_conf=\"$(cat \"$file_ports\")\"\nprt_conf_2=\"$(echo \"$prt_conf\" | sed \"s|Listen 80|${http_listen_prts}|\")\"\nprt_conf=\"$(echo \"$prt_conf_2\" | sed \"s|Listen 443|${https_listen_prts}|\")\"\necho \"$prt_conf\" | tee \"$file_ports\"\nhttp_site_conf=\"$(cat \"$file_http_site\")\"\nhttp_site_conf_2=\"$(echo \"$http_site_conf\" | sed \"s|*:80|${http_vh_prts}|\")\"\necho \"$http_site_conf_2\" | tee \"$file_http_site\"\nhttps_site_conf=\"$(cat \"$file_https_site\")\"\nhttps_site_conf_2=\"$(echo \"$https_site_conf\" | sed \"s|_default_:443|${https_vh_prts}|\")\"\necho \"$https_site_conf_2\" | tee \"$file_https_site\"\nsystemctl restart apache2\ntouch /etc/startup_script_completed\n```\n- Click **Create** .\n **Create instance groups** - In the Google Cloud console, go to the **Instance groups** page. [Go to Instance groups](https://console.cloud.google.com/compute/instanceGroups/list) \n- Repeat the following steps to create two unmanaged instance groups each with two VMs in them, using these combinations.- Instance group name:`ig-a`, zone:`us-west1-a`, VMs:`vm-a1`and`vm-a2`\n- Instance group name:`ig-c`, zone:`us-west1-c`, VMs:`vm-c1`and`vm-c2`\n- Click **Create instance group** .\n- Click **New unmanaged instance group** .\n- Set **Name** as indicated in step 2.\n- In the **Location** section, select **us-west1** for **Region** , and then choose a **Zone** as indicated in step 2.\n- For **Network** , select **lb-network** .\n- For **Subnetwork** , select **lb-subnet** .\n- In the **VM instances** section, add the VMs as indicated in step 2.\n- Click **Create** .\n- Create the four VMs by running the following command four times, using these four combinations for `[VM-NAME]` and `[ZONE]` . The script contents are identical for all four VMs.- `VM-NAME`:`vm-a1`,`ZONE`:`us-west1-a`\n- `VM-NAME`:`vm-a2`,`ZONE`:`us-west1-a`\n- `VM-NAME`:`vm-c1`,`ZONE`:`us-west1-c`\n- `VM-NAME`:`vm-c2`,`ZONE`:`us-west1-c`\n```\ngcloud compute instances create VM-NAME \\\n --zone=ZONE \\\n --image-family=debian-10 \\\n --image-project=debian-cloud \\\n --tags=allow-ssh,allow-health-check \\\n --subnet=lb-subnet \\\n --metadata=startup-script='#! /bin/bash\nif [ -f /etc/startup_script_completed ]; then\nexit 0\nfi\napt-get update\napt-get install apache2 -y\na2ensite default-ssl\na2enmod ssl\nfile_ports=\"/etc/apache2/ports.conf\"\nfile_http_site=\"/etc/apache2/sites-available/000-default.conf\"\nfile_https_site=\"/etc/apache2/sites-available/default-ssl.conf\"\nhttp_listen_prts=\"Listen 80\\nListen 8008\\nListen 8080\\nListen 8088\"\nhttp_vh_prts=\"*:80 *:8008 *:8080 *:8088\"\nhttps_listen_prts=\"Listen 443\\nListen 8443\"\nhttps_vh_prts=\"*:443 *:8443\"\nvm_hostname=\"$(curl -H \"Metadata-Flavor:Google\" \\\nhttp://metadata.google.internal/computeMetadata/v1/instance/name)\"\necho \"Page served from: $vm_hostname\" | \\\ntee /var/www/html/index.html\nprt_conf=\"$(cat \"$file_ports\")\"\nprt_conf_2=\"$(echo \"$prt_conf\" | sed \"s|Listen 80|${http_listen_prts}|\")\"\nprt_conf=\"$(echo \"$prt_conf_2\" | sed \"s|Listen 443|${https_listen_prts}|\")\"\necho \"$prt_conf\" | tee \"$file_ports\"\nhttp_site_conf=\"$(cat \"$file_http_site\")\"\nhttp_site_conf_2=\"$(echo \"$http_site_conf\" | sed \"s|*:80|${http_vh_prts}|\")\"\necho \"$http_site_conf_2\" | tee \"$file_http_site\"\nhttps_site_conf=\"$(cat \"$file_https_site\")\"\nhttps_site_conf_2=\"$(echo \"$https_site_conf\" | sed \"s|_default_:443|${https_vh_prts}|\")\"\necho \"$https_site_conf_2\" | tee \"$file_https_site\"\nsystemctl restart apache2\ntouch /etc/startup_script_completed'\n```\n- Create the two unmanaged instance groups in each zone:```\ngcloud compute instance-groups unmanaged create ig-a \\\n --zone=us-west1-a\ngcloud compute instance-groups unmanaged create ig-c \\\n --zone=us-west1-c\n```\n- Add the VMs to the appropriate instance groups:```\ngcloud compute instance-groups unmanaged add-instances ig-a \\\n --zone=us-west1-a \\\n --instances=vm-a1,vm-a2\ngcloud compute instance-groups unmanaged add-instances ig-c \\\n --zone=us-west1-c \\\n --instances=vm-c1,vm-c2\n```\nFor the four VMs, use the following VM names and zones:- `VM-NAME`:`vm-a1`,`ZONE`:`us-west1-a`\n- `VM-NAME`:`vm-a2`,`ZONE`:`us-west1-a`\n- `VM-NAME`:`vm-c1`,`ZONE`:`us-west1-c`\n- `VM-NAME`:`vm-c2`,`ZONE`:`us-west1-c`\nYou can get the current `` by running the following `gcloud` command:\n```\ngcloud compute images list \\\n --filter=\"family=debian-10\"\n```\nCreate four backend VMs by making four `POST` requests to the [instances.insert method](/compute/docs/reference/rest/v1/instances/insert) :\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/ZONE/instances\n{\n \"name\": \"VM-NAME\",\n \"tags\": {\n \"items\": [  \"allow-health-check\",\n  \"allow-ssh\"\n ]\n },\n \"machineType\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/[ZONE]/machineTypes/e2-standard-2\",\n \"canIpForward\": false,\n \"networkInterfaces\": [ {\n  \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n  \"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\",\n  \"accessConfigs\": [  {\n   \"type\": \"ONE_TO_ONE_NAT\",\n   \"name\": \"external-nat\",\n   \"networkTier\": \"PREMIUM\"\n  }\n  ]\n }\n ],\n \"disks\": [ {\n  \"type\": \"PERSISTENT\",\n  \"boot\": true,\n  \"mode\": \"READ_WRITE\",\n  \"autoDelete\": true,\n  \"deviceName\": \"VM-NAME\",\n  \"initializeParams\": {\n  \"sourceImage\": \"projects/debian-cloud/global/images/debian-image-name\",\n  \"diskType\": \"projects/PROJECT_ID/zones/zone/diskTypes/pd-standard\",\n  \"diskSizeGb\": \"10\"\n  }\n }\n ],\n \"metadata\": {\n \"items\": [  {\n  \"key\": \"startup-script\",\n  \"value\": \"#! /bin/bash\\napt-get update\\napt-get install apache2 -y\\na2ensite default-ssl\\na2enmod ssl\\nfile_ports=\\\"/etc/apache2/ports.conf\\\"\\nfile_http_site=\\\"/etc/apache2/sites-available/000-default.conf\\\"\\nfile_https_site=\\\"/etc/apache2/sites-available/default-ssl.conf\\\"\\nhttp_listen_prts=\\\"Listen 80\\\\nListen 8008\\\\nListen 8080\\\\nListen 8088\\\"\\nhttp_vh_prts=\\\"*:80 *:8008 *:8080 *:8088\\\"\\nhttps_listen_prts=\\\"Listen 443\\\\nListen 8443\\\"\\nhttps_vh_prts=\\\"*:443 *:8443\\\"\\nvm_hostname=\\\"$(curl -H \\\"Metadata-Flavor:Google\\\" \\\\\\nhttp://metadata.google.internal/computeMetadata/v1/instance/name)\\\"\\necho \\\"Page served from: $vm_hostname\\\" | \\\\\\ntee /var/www/html/index.html\\nprt_conf=\\\"$(cat \\\"$file_ports\\\")\\\"\\nprt_conf_2=\\\"$(echo \\\"$prt_conf\\\" | sed \\\"s|Listen 80|${http_listen_prts}|\\\")\\\"\\nprt_conf=\\\"$(echo \\\"$prt_conf_2\\\" | sed \\\"s|Listen 443|${https_listen_prts}|\\\")\\\"\\necho \\\"$prt_conf\\\" | tee \\\"$file_ports\\\"\\nhttp_site_conf=\\\"$(cat \\\"$file_http_site\\\")\\\"\\nhttp_site_conf_2=\\\"$(echo \\\"$http_site_conf\\\" | sed \\\"s|*:80|${http_vh_prts}|\\\")\\\"\\necho \\\"$http_site_conf_2\\\" | tee \\\"$file_http_site\\\"\\nhttps_site_conf=\\\"$(cat \\\"$file_https_site\\\")\\\"\\nhttps_site_conf_2=\\\"$(echo \\\"$https_site_conf\\\" | sed \\\"s|_default_:443|${https_vh_prts}|\\\")\\\"\\necho \\\"$https_site_conf_2\\\" | tee \\\"$file_https_site\\\"\\nsystemctl restart apache2\"\n  }\n ]\n },\n \"scheduling\": {\n \"preemptible\": false\n },\n \"deletionProtection\": false\n}\n```\nCreate two instance groups by making a `POST` request to the [instanceGroups.insert method](/compute/docs/reference/rest/v1/instanceGroups/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/instanceGroups\n{\n \"name\": \"ig-a\",\n \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n \"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\"\n}\n```\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-c/instanceGroups\n{\n \"name\": \"ig-c\",\n \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n \"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\"\n}\n```\nAdd instances to each instance group by making a `POST` request to the [instanceGroups.addInstances method](/compute/docs/reference/rest/v1/instanceGroups/addInstances) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/instanceGroups/ig-a/addInstances\n{\n \"instances\": [ {\n  \"instance\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/instances/vm-a1\",\n  \"instance\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/instances/vm-a2\"\n }\n ]\n}\n```\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-c/instanceGroups/ig-c/addInstances\n{\n \"instances\": [ {\n  \"instance\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-c/instances/vm-c1\",\n  \"instance\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-c/instances/vm-c2\"\n }\n ]\n}\n```\n### Configure load balancer components\nThese steps configure all of the [internal passthrough Network Load Balancercomponents](/load-balancing/docs/internal#components) starting with the health check and backend service, and then the frontend components:\n- **Health check** : In this example, you use an HTTP health check that checks for an HTTP `200` (OK) response. For more information, see the [health checks section of the internal passthrough Network Load Balanceroverview](/load-balancing/docs/internal#health-checking) .\n- **Backend service** : Because you need to pass HTTP traffic through the internal load balancer, you need to use TCP, not UDP.\n- **Forwarding rule** : This example creates a single internal forwarding rule.\n- **Internal IP address** : In this example, you specify an internal IP address, `10.1.2.99` , when you create the forwarding rule.\n- In the Google Cloud console, go to the **Load balancing** page. [Go to Load balancing](https://console.cloud.google.com/networking/loadbalancing/list) \n- Click **Create load balancer** .\n- On the **Network Load Balancer (TCP/SSL)** card, click **Start configuration** .\n- For **Internet facing or internal only** , select **Only between my VMs** .\n- For **Load balancer type** , select **Pass-through** .\n- Click **Continue** .\n- On the **Create internal passthrough Network Load Balancer** page, enter the following information:- **Load balancer name** :`be-ilb`\n- **Region** : **us-west1** \n- **Network** : **lb-network** \n### Configure the backends\n- Click **Backend configuration** .\n- To handle only IPv4 traffic, in the **New Backend** section of **Backends** , select the **IP stack type** as **IPv4 (single-stack)** .\n- In **Instance group** , select the`ig-c`instance group and click **Done** .\n- Click **Add a backend** and repeat the step to add`ig-a`.\n- From the **Health check** list, select **Create a health check** , enter the following information, and click **Save** .- **Name:** `hc-http-80`\n- **Protocol:** `HTTP`\n- **Port:** `80`\n- **Proxy protocol:** `NONE`\n- **Request path:** `/`\nNote that when you use the Google Cloud console to create your load balancer, the health check is global. If you want to create a regional health check, use `gcloud` or the API.\n- Verify that there is a blue check mark next to **Backend configuration** before continuing.\n### Configure the frontend\n- Click **Frontend configuration** .\n- In the **New Frontend IP and port** section, do the following:- For **Name** , enter`fr-ilb`.\n- For **Subnetwork** , select`lb-subnet`.\n- In the **Internal IP purpose** section, in the **IP address** list, select **Create IP address** , enter the following information, and then click **Reserve** .- **Name:** `ip-ilb`\n- **IP version:** **IPv4** \n- **Static IP address:** **Let me choose** \n- **Custom IP address:** `10.1.2.99`\n- For **Ports** , select **Multiple** and then in **Port numbers** , enter`80`,`8008`,`8080`, and`8088`.\n- Verify that there is a blue check mark next to **Frontend configuration** before continuing.\n### Review the configuration\n- Click **Review and finalize** .\n- Review your load balancer configuration settings.\n- Optional: Click **Equivalent code** to view the REST API request that will be used to create the load balancer.\n- Click **Create** .\n- Create a new regional HTTP health check to test HTTP connectivity to the VMs on port 80.```\ngcloud compute health-checks create http hc-http-80 \\\n --region=us-west1 \\\n --port=80\n```\n- Create the backend service for HTTP traffic:```\ngcloud compute backend-services create be-ilb \\\n --load-balancing-scheme=internal \\\n --protocol=tcp \\\n --region=us-west1 \\\n --health-checks=hc-http-80 \\\n --health-checks-region=us-west1\n```\n- Add the two instance groups to the backend service:```\ngcloud compute backend-services add-backend be-ilb \\\n --region=us-west1 \\\n --instance-group=ig-a \\\n --instance-group-zone=us-west1-a\ngcloud compute backend-services add-backend be-ilb \\\n --region=us-west1 \\\n --instance-group=ig-c \\\n --instance-group-zone=us-west1-c\n```\n- Create a forwarding rule for the backend service. When you create the forwarding rule, specify `10.1.2.99` for the internal IP address in the subnet.```\ngcloud compute forwarding-rules create fr-ilb \\\n --region=us-west1 \\\n --load-balancing-scheme=internal \\\n --network=lb-network \\\n --subnet=lb-subnet \\\n --address=10.1.2.99 \\\n --ip-protocol=TCP \\\n --ports=80,8008,8080,8088 \\\n --backend-service=be-ilb \\\n --backend-service-region=us-west1\n```\nCreate the health check by making a `POST` request to the [regionHealthChecks.insert method](/compute/docs/reference/rest/v1/regionHealthChecks/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/regionHealthChecks\n{\n\"name\": \"hc-http-80\",\n\"type\": \"HTTP\",\n\"httpHealthCheck\": {\n \"port\": 80\n}\n}\n```\nCreate the regional backend service by making a `POST` request to the [regionBackendServices.insert method](/compute/docs/reference/rest/v1/regionBackendServices/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/backendServices\n{\n\"name\": \"be-ilb\",\n\"backends\": [ {\n \"group\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/instanceGroups/ig-a\",\n \"balancingMode\": \"CONNECTION\"\n },\n {\n \"group\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-c/instanceGroups/ig-c\",\n \"balancingMode\": \"CONNECTION\"\n }\n],\n\"healthChecks\": [ \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/healthChecks/hc-http-80\"\n],\n\"loadBalancingScheme\": \"INTERNAL\",\n\"connectionDraining\": {\n \"drainingTimeoutSec\": 0\n }\n}\n```\nCreate the forwarding rule by making a `POST` request to the [forwardingRules.insert method](/compute/docs/reference/rest/v1/forwardingRules/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/forwardingRules\n{\n\"name\": \"fr-ilb\",\n\"IPAddress\": \"10.1.2.99\",\n\"IPProtocol\": \"TCP\",\n\"ports\": [ \"80\", \"8008\", \"8080\", \"8088\"\n],\n\"loadBalancingScheme\": \"INTERNAL\",\n\"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\",\n\"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n\"backendService\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/backendServices/be-ilb\",\n\"networkTier\": \"PREMIUM\"\n}\n```\n### Test your load balancer\nThese tests show how to validate your load balancer configuration and learn about its expected behavior.\nThis example creates a client VM ( `vm-client` ) in the same region as the backend (server) VMs. The client is used to validate the load balancer's configuration and demonstrate expected behavior as described in the [testing](#test-your-load-balancer) section.\n- In the Google Cloud console, go to the **VM instances** page. [Go to VM instances](https://console.cloud.google.com/compute/instances) \n- Click **Create instance** .\n- For **Name** , enter `vm-client` .\n- For **Region** , select **us-west1** .\n- For **Zone** , select **us-west1-a** .\n- Click **Advanced options** .\n- Click **Networking** and configure the following fields:- For **Network tags** , enter`allow-ssh`.\n- For **Network interfaces** , select the following:- **Network** : **lb-network** \n- **Subnet** : **lb-subnet** \n- Click **Create** .\nThe client VM can be in any zone in the same region as the load balancer, and it can use any subnet in that region. In this example, the client is in the `us-west1-a` zone, and it uses the same subnet as the backend VMs.\n```\ngcloud compute instances create vm-client \\\n --zone=us-west1-a \\\n --image-family=debian-10 \\\n --image-project=debian-cloud \\\n --tags=allow-ssh \\\n --subnet=lb-subnet\n```\nMake a `POST` request to the [instances.insert method](/compute/docs/reference/rest/v1/instances/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/instances\n{\n \"name\": \"vm-client\",\n \"tags\": {\n \"items\": [  \"allow-ssh\"\n ]\n },\n \"machineType\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/machineTypes/e2-standard-2\",\n \"canIpForward\": false,\n \"networkInterfaces\": [ {\n  \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n  \"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\",\n  \"accessConfigs\": [  {\n   \"type\": \"ONE_TO_ONE_NAT\",\n   \"name\": \"external-nat\",\n   \"networkTier\": \"PREMIUM\"\n  }\n  ]\n }\n ],\n \"disks\": [ {\n  \"type\": \"PERSISTENT\",\n  \"boot\": true,\n  \"mode\": \"READ_WRITE\",\n  \"autoDelete\": true,\n  \"deviceName\": \"vm-client\",\n  \"initializeParams\": {\n  \"sourceImage\": \"projects/debian-cloud/global/images/debian-image-name\",\n  \"diskType\": \"projects/PROJECT_ID/zones/us-west1-a/diskTypes/pd-standard\",\n  \"diskSizeGb\": \"10\"\n  }\n }\n ],\n \"scheduling\": {\n \"preemptible\": false\n },\n \"deletionProtection\": false\n}\n```\nThis test contacts the load balancer from a separate client VM; that is, not from a backend VM of the load balancer. The expected behavior is for traffic to be distributed among the four backend VMs [because no session affinity has beenconfigured](/load-balancing/docs/internal#traffic_distribution) .\n- Connect to the client VM instance.```\ngcloud compute ssh vm-client --zone=us-west1-a\n```\n- Make a web request to the load balancer using `curl` to contact its IP address. Repeat the request so you can see that responses come from different backend VMs. The name of the VM generating the response is displayed in the text in the HTML response, by virtue of the contents of `/var/www/html/index.html` on each backend VM. For example, expected responses look like `Page served from: vm-a1` and `Page served from: vm-a2` .```\ncurl http://10.1.2.99\n```The forwarding rule is configured to serve ports `80` , `8008` , `8080` , and `8088` . To send traffic to those other ports, append a colon ( `:` ) and the port number after the IP address, like this:```\ncurl http://10.1.2.99:8008\n```If you [add a service label](/load-balancing/docs/dns-names) to the internal forwarding rule, you can use [internal DNS](/compute/docs/internal-dns) to contact the load balancer using its service name.```\n curl http://web-test.fr-ilb.il4.us-west1.lb.PROJECT_ID.internal\n \n```This test demonstrates an expected behavior: You cannot ping the IP address of the load balancer. This is because internal passthrough Network Load Balancers are implemented in virtual network programming \u2014 they are not separate devices.\n- Connect to the client VM instance.```\ngcloud compute ssh vm-client --zone=us-west1-a\n```\n- Attempt to ping the IP address of the load balancer. Notice that you don't get a response and that the `ping` command times out after 10 seconds in this example.```\ntimeout 10 ping 10.1.2.99\n```This test demonstrates that when a backend VM sends packets to the IP address of its load balancer's forwarding rule, those requests are routed back to itself. This is the case regardless of the backend VM's health check state.\nInternal passthrough Network Load Balancers are implemented by using virtual network programming and VM configuration in the guest OS. On Linux VMs, the [Guestenvironment](/compute/docs/images/guest-environment) creates a route for the load balancer's IP address in the operating system's local routing table.\nBecause this local route is within the VM itself (not a route in the VPC network), packets sent to the load balancer's IP address are not processed by the VPC network. Instead, packets sent to the load balancer's IP address remain within the operating system of the VM.\n- Connect to a backend VM, such as `vm-a1` :```\ngcloud compute ssh vm-a1 --zone=us-west1-a\n``` **Important:** You won't be able to connect to a backend VM in this way if you [have removed its external IP address](#remove_external_ip) . For connection options if your backend VMs don't have external IP addresses, see [Choose a connection option for internal-only VMs](/compute/docs/connect/ssh-internal-ip) .\n- Make a web request to the load balancer (by IP address or service name) using `curl` . The response comes from the same backend VM that makes the request. Repeated requests are answered in the same way. The expected response when testing from `vm-a1` is always `Page served from: vm-a1` .```\ncurl http://10.1.2.99\n```\n- Inspect the local routing table, looking for a destination that matches the IP address of the load balancer itself, `10.1.2.99` . This route is a necessary part of an internal passthrough Network Load Balancer, but it also demonstrates why a request from a VM behind the load balancer is always responded to by the same VM.```\nip route show table local | grep 10.1.2.99\n```\nWhen a backend VM for an internal passthrough Network Load Balancer sends packets to the load balancer's forwarding rule IP address, the packets are routed back to the VM that makes the request. This is because an internal passthrough Network Load Balancer is a pass-through load balancer and is implemented by creating a local route for the load balancer's IP address within the VM's guest OS, as indicated in this section. If you have a use case where load-balanced backends need to send TCP traffic to the load balancer's IP address, and you need the traffic to be distributed as if it originated from a non-load-balanced backend, consider using a [regional internal proxy Network Load Balancer](/load-balancing/docs/tcp/internal-proxy) instead.\nFor more information, see [Internal passthrough Network Load Balancers as nexthops](/load-balancing/docs/internal/ilb-next-hop-overview) .\n## Set up load balancer with dual-stack subnets\nThis guide shows you how to configure and test an internal passthrough Network Load Balancer. The steps in this section describe how to configure the following:\n- The example on this page uses a [custom mode VPCnetwork](/vpc/docs/vpc#subnet-ranges) named`lb-network-dual-stack`. IPv6 traffic [requires a custom mode subnet](/vpc/docs/subnets#ipv6-specifications) .\n- Asubnet (`stack-type`set to`IPv4_IPv6`), which is required for IPv6 traffic. When you create a dual stack subnet on a custom mode VPC network, you choose an [IPv6 access type](/vpc/docs/subnets#ipv6-ranges) for the subnet. For this example, we set the subnet's`ipv6-access-type`parameter to`INTERNAL`. This means new VMs on this subnet can be assigned both internal IPv4 addresses and internal IPv6 addresses. For instructions, see VPC documentation about [Adding a dual-stacksubnet](/vpc/docs/create-modify-vpc-networks#add-subnet-ipv6) .\n- Firewall rules that allow incoming connections to backend VMs.\n- The backend instance group, which is located in the following region and subnet for this example:- Region:`us-west1`\n- Subnet:`lb-subnet`, with primary IPv4 address range`10.1.2.0/24`. Although you choose which IPv4 address range to configure on the subnet, the IPv6 address range is assigned automatically. Google provides a fixed size (`/64`) IPv6 CIDR block.\n- Four backend [dual-stack](/compute/docs/ip-addresses/configure-ipv6-address#create-vm-ipv6) VMs: two VMs in an unmanaged instance group in zone`us-west1-a`and two VMs in an unmanaged instance group in zone`us-west1-c`. To demonstrate [global access](/load-balancing/docs/internal#client-access) , this example creates a second test client VM in a different region and subnet:- Region:`europe-west1`\n- Subnet:`europe-subnet`, with primary IP address range`10.3.4.0/24`\n- One client VM to test connections.\n- The following internal passthrough Network Load Balancer components:- A health check for the backend service.\n- An internal backend service in the`us-west1`region to manage connection distribution to the two zonal instance groups.\n- Two internal forwarding rules for the frontend of the load balancer.The following diagram shows the architecture for this example:\n**Note:** For simplicity, this example uses unmanaged instance groups. In production, consider using a managed instance group, as illustrated in [Managed instance groups](#internal_load_balancing_with_regional_instance_groups) .\n### Configure a network, region, and subnet\nThe example internal passthrough Network Load Balancer described on this page is created in a [custom mode VPC network](/vpc/docs/vpc#subnet-ranges) named `lb-network-dual-stack` .\nTo configure subnets with internal IPv6 ranges, enable a VPC network ULA internal IPv6 range. Internal IPv6 subnet ranges are allocated from this range.\n- In the Google Cloud console, go to the **VPC networks** page. [Go to VPC networks](https://console.cloud.google.com/networking/networks/list) \n- Click **Create VPC network** .\n- For **Name** , enter `lb-network-dual-stack` .\n- If you want to configure internal IPv6 address ranges on subnets in this network, complete these steps:- For **VPC network ULA internal IPv6 range** , select **Enabled** .\n- For **Allocate internal IPv6 range** , select **Automatically** or **Manually** . **Pro Tip:** If you select **Manually** , enter a `/48` range from within the `fd20::/20` range. If the range is in use, you are prompted to provide a different range.\n- For **Subnet creation mode** , select **Custom** .\n- In the **New subnet** section, specify the following configuration parameters for a subnet:- **Name** :`lb-subnet`\n- **Region** :`us-west1`\n- **IP stack type** : **IPv4 and IPv6 (dual-stack)** \n- **IPv4 range** :`10.1.2.0/24`.\n- **IPv6 access type** : **Internal** \n- Click **Done** .\n- Click **Add subnet** and enter the following information:- **Name** :`europe-subnet`\n- **Region** :`europe-west1`\n- **IP stack type** : **IPv4 (single-stack)** \n- **IP address range** :`10.3.4.0/24`\n- Click **Done** .\n- Click **Create** .\n- To create a new custom mode VPC network, run the [gcloud compute networks create command](/sdk/gcloud/reference/compute/networks/create) .To configure IPv6 ranges on any subnets in this network, use the `--enable-ula-internal-ipv6` flag. This option assigns a `/48` ULA prefix from within the `fd20::/20` range used by Google Cloud for internal IPv6 subnet ranges. If you want to select the `/48` IPv6 range that is assigned, use the `--internal-ipv6-range` flag to specify a range.```\ngcloud compute networks create lb-network-dual-stack \\\n --subnet-mode=custom \\\n --enable-ula-internal-ipv6 \\\n --internal-ipv6-range=ULA_IPV6_RANGE \\\n --bgp-routing-mode=regional\n```Replace `` with a `/48` prefix from within the `fd20::/20` range used by Google for internal IPv6 subnet ranges. If you don't use the `--internal-ipv6-range` flag, Google selects a `/48` prefix for the network, such as `fd20:bc7:9a1c::/48` .\n- Within the `` network, create a subnet for backends in the `us-west1` region and another subnet for testing global access in the `europe-west1` region.To create the subnets, run the [gcloud compute networks subnets create command](/sdk/gcloud/reference/compute/networks/subnets/create) .```\ngcloud compute networks subnets create lb-subnet \\\n --network=lb-network-dual-stack \\\n --range=10.1.2.0/24 \\\n --region=us-west1 \\\n --stack-type=IPV4_IPV6 \\\n --ipv6-access-type=INTERNAL\n``````\ngcloud compute networks subnets create europe-subnet \\\n --network=lb-network-dual-stack \\\n --range=10.3.4.0/24 \\\n --region=europe-west1 \\\n --stack-type=IPV4_IPV6 \\\n --ipv6-access-type=INTERNAL\n```\nCreate a new custom mode VPC network.\nTo configure IPv6 ranges on any subnets in this network, set `enableUlaInternalIpv6` to true. This option assigns a `/48` range from within the `fd20::/20` range used by Google for internal IPv6 subnet ranges. If you want to select which `/48` IPv6 range that is assigned, also use the `internalIpv6Range` field to specify a range.\n```\n POST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks\n {\n \"autoCreateSubnetworks\": false,\n \"name\": \"lb-network-dual-stack\",\n \"mtu\": MTU,\n \"enableUlaInternalIpv6\": true,\n \"internalIpv6Range\": \"ULA_IPV6_RANGE\",\n \"routingConfig\": {\n \"routingMode\": \"DYNAMIC_ROUTING_MODE\"\n }\n }\n \n```\nReplace the following:- ``: the ID of the project where the VPC network is created.\n- ``: the maximum transmission unit of the network. MTU can either be`1460`(default) or`1500`. Review the [maximum transmissionunit overview](/vpc/docs/mtu) before setting the MTU to`1500`.\n- ``: a`/48`prefix from within the`fd20::/20`range used by Google for internal IPv6 subnet ranges. If you don't provide a value for`internalIpv6Range`, Google selects a`/48`prefix for the network.\n- `` : either `global` or `regional` to control the route advertisement behavior of Cloud Routers in the network. For more information, refer to [dynamic routingmode](/vpc/docs/vpc#routing_for_hybrid_networks) .For more information, refer to the [networks.insert method](/compute/docs/reference/rest/v1/networks/insert) .\nMake two `POST` requests to the [subnetworks.insert method](/compute/docs/reference/rest/v1/subnetworks/insert) .\n```\n POST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/REGION/subnetworks\n {\n \"ipCidrRange\": \"10.1.2.0/24\",\n \"network\": \"lb-network-dual-stack\",\n \"name\": \"lb-subnet\"\n \"stackType\": IPV4_IPV6,\n \"ipv6AccessType\": Internal\n }\n \n``````\n POST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/REGION/subnetworks\n {\n \"ipCidrRange\": \"10.3.4.0/24\",\n \"network\": \"lb-network-dual-stack\",\n \"name\": \"europe-subnet\"\n \"stackType\": IPV4_IPV6,\n \"ipv6AccessType\": Internal\n }\n \n```\n### Configure firewall rules\nThis example uses the following firewall rules:\n- `fw-allow-lb-access` : An ingress rule, applicable to all targets in the VPC network, that allows traffic from sources in the `10.1.2.0/24` and `10.3.4.0/24` ranges. This rule allows incoming traffic from any client located in either of the two subnets. Later, you can [configure and test global access](#ilb-global-access) .\n- `fw-allow-lb-access-ipv6` : An ingress rule, applicable to all targets in the VPC network, that allows traffic from sources in the IPv6 range configured in the subnet. This rule allows incoming IPv6 traffic from any client located in either of the two subnets. Later, you can [configure and test global access](#ilb-global-access) .\n- `fw-allow-ssh` : An ingress rule, applicable to the instances being load balanced, that allows incoming SSH connectivity on TCP port 22 from any address. You can choose a more restrictive source IP range for this rule; for example, you can specify just the IP ranges of the system from which you initiate SSH sessions. This example uses the target tag `allow-ssh` to identify the VMs to which it should apply.\n- `fw-allow-health-check` : An ingress rule, applicable to the instances being load balanced, that allows traffic from the Google Cloud health checking systems ( `130.211.0.0/22` and `35.191.0.0/16` ). This example uses the target tag `allow-health-check` to identify the instances to which it should apply.\n- `fw-allow-health-check-ipv6` : An ingress rule, applicable to the instances being load balanced, that allows traffic from the Google Cloud health checking systems ( `2600:2d00:1:b029::/64` ). This example uses the target tag `allow-health-check-ipv6` to identify the instances to which it should apply.\nWithout these firewall rules, the [default denyingress](/vpc/docs/firewalls#default_firewall_rules) rule blocks incoming traffic to the backend instances.\n**Note:** You must create a firewall rule to allow health checks from the IP ranges of Google Cloud probe systems. See [probe IPranges](/load-balancing/docs/health-check-concepts) for more information.\n- In the Google Cloud console, go to the **Firewall policies** page. [Go to Firewall policies](https://console.cloud.google.com/net-security/firewall-manager/firewall-policies/list) \n- To create the rule to allow subnet traffic, click **Create firewall rule** and enter the following information:- **Name** :`fw-allow-lb-access`\n- **Network** :`lb-network-dual-stack`\n- **Priority** :`1000`\n- **Direction of traffic** : **ingress** \n- **Action on match** : **allow** \n- **Targets** : **All instances in the network** \n- **Source filter** : **IPv4 ranges** \n- **Source IPv4 ranges** :`10.1.2.0/24`and`10.3.4.0/24`\n- **Protocols and ports** : **Allow all** \n- Click **Create** .\n- To allow IPv6 subnet traffic, click **Create firewall rule** again and enter the following information:- **Name** :`fw-allow-lb-access-ipv6`\n- **Network** :`lb-network-dual-stack`\n- **Priority** :`1000`\n- **Direction of traffic** : **ingress** \n- **Action on match** : **allow** \n- **Targets** : **All instances in the network** \n- **Source filter** : **IPv6 ranges** \n- **Source IPv6 ranges** :assigned in the`lb-subnet`\n- **Protocols and ports** : **Allow all** \n- Click **Create** .\n- To allow incoming SSH connections, click **Create firewall rule** again and enter the following information:- **Name** :`fw-allow-ssh`\n- **Network** :`lb-network-dual-stack`\n- **Priority** :`1000`\n- **Direction of traffic** : **ingress** \n- **Action on match** : **allow** \n- **Targets** : **Specified target tags** \n- **Target tags** :`allow-ssh`\n- **Source filter** : **IPv4 ranges** \n- **Source IPv4 ranges** :`0.0.0.0/0`\n- **Protocols and ports** : Select **Specified protocols and ports** , select the **TCP** checkbox, and then enter`22`in **Ports** .\n- Click **Create** .\n- To allow Google Cloud IPv6 health checks, click **Create firewall rule** again and enter the following information:- **Name** :`fw-allow-health-check-ipv6`\n- **Network** :`lb-network-dual-stack`\n- **Priority** :`1000`\n- **Direction of traffic** : **ingress** \n- **Action on match** : **allow** \n- **Targets** : **Specified target tags** \n- **Target tags** :`allow-health-check-ipv6`\n- **Source filter** : **IPv6 ranges** \n- **Source IPv6 ranges** :`2600:2d00:1:b029::/64`\n- **Protocols and ports** : **Allow all** \n- Click **Create** .\n- To allow Google Cloud health checks, click **Create firewall rule** again and enter the following information:- **Name** :`fw-allow-health-check`\n- **Network** :`lb-network-dual-stack`\n- **Priority** :`1000`\n- **Direction of traffic** : **ingress** \n- **Action on match** : **allow** \n- **Targets** : **Specified target tags** \n- **Target tags** :`allow-health-check`\n- **Source filter** : **IPv4 ranges** \n- **Source IPv4 ranges** :`130.211.0.0/22`and`35.191.0.0/16`\n- **Protocols and ports** : **Allow all** \n- Click **Create** .\n- Create the `fw-allow-lb-access` firewall rule to allow communication with the subnet:```\ngcloud compute firewall-rules create fw-allow-lb-access \\\n --network=lb-network-dual-stack \\\n --action=allow \\\n --direction=ingress \\\n --source-ranges=10.1.2.0/24,10.3.4.0/24 \\\n --rules=all\n```\n- Create the `fw-allow-lb-access-ipv6` firewall rule to allow communication with the subnet:```\ngcloud compute firewall-rules create fw-allow-lb-access-ipv6 \\\n --network=lb-network-dual-stack \\\n --action=allow \\\n --direction=ingress \\\n --source-ranges=IPV6_ADDRESS \\\n --rules=all\n```Replace `` with the IPv6 address assigned in the `lb-subnet` .\n- Create the `fw-allow-ssh` firewall rule to allow SSH connectivity to VMs with the network tag `allow-ssh` . When you omit `source-ranges` , Google Cloud [interprets the rule to mean anysource](/vpc/docs/firewalls#sources_for_the_rule) .```\ngcloud compute firewall-rules create fw-allow-ssh \\\n --network=lb-network-dual-stack \\\n --action=allow \\\n --direction=ingress \\\n --target-tags=allow-ssh \\\n --rules=tcp:22\n```\n- Create the `fw-allow-health-check-ipv6` rule to allow Google Cloud IPv6 health checks.```\ngcloud compute firewall-rules create fw-allow-health-check-ipv6 \\\n --network=lb-network-dual-stack \\\n --action=allow \\\n --direction=ingress \\\n --target-tags=allow-health-check-ipv6 \\\n --source-ranges=2600:2d00:1:b029::/64 \\\n --rules=tcp,udp\n```\n- Create the `fw-allow-health-check` rule to allow Google Cloud health checks.```\ngcloud compute firewall-rules create fw-allow-health-check \\\n --network=lb-network-dual-stack \\\n --action=allow \\\n --direction=ingress \\\n --target-tags=allow-health-check \\\n --source-ranges=130.211.0.0/22,35.191.0.0/16 \\\n --rules=tcp,udp,icmp\n```\n- Create the `fw-allow-lb-access` firewall rule by making a `POST` request to the [firewalls.insert method](/compute/docs/reference/rest/v1/firewalls/insert) .```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/firewalls\n{\n\"name\": \"fw-allow-lb-access\",\n\"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network-dual-stack\",\n\"priority\": 1000,\n\"sourceRanges\": [ \"10.1.2.0/24\", \"10.3.4.0/24\"\n],\n\"allowed\": [ {\n \"IPProtocol\": \"tcp\"\n },\n {\n \"IPProtocol\": \"udp\"\n },\n {\n \"IPProtocol\": \"icmp\"\n }\n],\n\"direction\": \"INGRESS\",\n\"logConfig\": {\n \"enable\": false\n},\n\"disabled\": false\n}\n```\n- Create the `fw-allow-lb-access-ipv6` firewall rule by making a `POST` request to the [firewalls.insert method](/compute/docs/reference/rest/v1/firewalls/insert) .```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/firewalls\n{\n \"name\": \"fw-allow-lb-access-ipv6\",\n \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network-dual-stack\",\n \"priority\": 1000,\n \"sourceRanges\": [ \"IPV6_ADDRESS\"\n ],\n \"allowed\": [ {\n  \"IPProtocol\": \"tcp\"\n },\n {\n  \"IPProtocol\": \"udp\"\n },\n {\n  \"IPProtocol\": \"icmp\"\n }\n ],\n \"direction\": \"INGRESS\",\n \"logConfig\": {\n \"enable\": false\n },\n \"disabled\": false\n}\n```Replace `` with the IPv6 address assigned in the `lb-subnet` .\n- Create the `fw-allow-ssh` firewall rule by making a `POST` request to the [firewalls.insert method](/compute/docs/reference/rest/v1/firewalls/insert) .```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/firewalls\n{\n\"name\": \"fw-allow-ssh\",\n  \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network-dual-stack\",\n\"priority\": 1000,\n\"sourceRanges\": [ \"0.0.0.0/0\"\n],\n\"targetTags\": [ \"allow-ssh\"\n],\n\"allowed\": [ {\n \"IPProtocol\": \"tcp\",\n \"ports\": [  \"22\"\n ]\n }\n],\n\"direction\": \"INGRESS\",\n\"logConfig\": {\n \"enable\": false\n},\n\"disabled\": false\n}\n```\n- Create the `fw-allow-health-check-ipv6` firewall rule by making a `POST` request to the [firewalls.insert method](/compute/docs/reference/rest/v1/firewalls/insert) .```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/firewalls\n{\n\"name\": \"fw-allow-health-check-ipv6\",\n\"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network-dual-stack\",\n\"priority\": 1000,\n\"sourceRanges\": [ \"2600:2d00:1:b029::/64\"\n],\n\"targetTags\": [ \"allow-health-check-ipv6\"\n],\n\"allowed\": [ {\n \"IPProtocol\": \"tcp\"\n },\n {\n \"IPProtocol\": \"udp\"\n }\n],\n\"direction\": \"INGRESS\",\n\"logConfig\": {\n \"enable\": false\n},\n\"disabled\": false\n}\n```\n- Create the `fw-allow-health-check` firewall rule by making a `POST` request to the [firewalls.insert method](/compute/docs/reference/rest/v1/firewalls/insert) .```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/firewalls\n{\n\"name\": \"fw-allow-health-check\",\n\"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network-dual-stack\",\n\"priority\": 1000,\n\"sourceRanges\": [ \"130.211.0.0/22\",\n \"35.191.0.0/16\"\n],\n\"targetTags\": [ \"allow-health-check\"\n],\n\"allowed\": [ {\n \"IPProtocol\": \"tcp\"\n },\n {\n \"IPProtocol\": \"udp\"\n },\n {\n \"IPProtocol\": \"icmp\"\n }\n],\n\"direction\": \"INGRESS\",\n\"logConfig\": {\n \"enable\": false\n},\n\"disabled\": false\n}\n```\n### Create backend VMs and instance groups\nThis example uses two instance groups each having two backend (server) VMs. To demonstrate the regional nature of internal passthrough Network Load Balancers, the two instance groups are placed in separate zones, `us-west1-a` and `us-west1-c` .\n- Instance group`ig-a`contains these two VMs:- `vm-a1`\n- `vm-a2`\n- Instance group`ig-c`contains these two VMs:- `vm-c1`\n- `vm-c2`Traffic to all four of the backend VMs is load balanced.\nTo support this example and the additional configuration options, each of the four VMs runs an Apache web server that listens on the following TCP ports: `80` , `8008` , `8080` , `8088` , `443` , and `8443` .\nEach VM is assigned an internal IP address in the `lb-subnet` and an ephemeral external (public) IP address. You can [remove the external IP addresseslater](#remove_external_ip) .\nExternal IP address for the backend VMs are not required; however, they are useful for this example because they permit the backend VMs to download Apache from the internet, and they can [connect usingSSH](/compute/docs/instances/connecting-to-instance#gcetools) .\nBy default, Apache is configured to bind to any IP address. Internal passthrough Network Load Balancers deliver packets by preserving the destination IP.\nEnsure that server software running on your backend VMs is listening on the IP address of the load balancer's internal forwarding rule. If you configure [multiple internal forwarding rules](/load-balancing/docs/internal#multiple_forwarding_rule) , ensure that your software listens to the internal IP address associated with each one. The destination IP address of a packet delivered to a backend VM by an internal passthrough Network Load Balancer is the internal IP address of the forwarding rule.\nEnsure that the subnetwork stack type matches the stack type of instance templates used by the managed instance groups. The subnetwork must be dual-stack if the managed instance group is using a dual-stack instance template.\nFor instructional simplicity, these backend VMs run Debian GNU/Linux 10.\n- In the Google Cloud console, go to the **VM instances** page. [Go to VM instances](https://console.cloud.google.com/compute/instances) \n- Repeat steps 3 to 8 for each VM, using the following name and zone combinations.- Name:`vm-a1`, zone:`us-west1-a`\n- Name:`vm-a2`, zone:`us-west1-a`\n- Name:`vm-c1`, zone:`us-west1-c`\n- Name:`vm-c2`, zone:`us-west1-c`\n- Click **Create instance** .\n- Set the **Name** as indicated in step 2.\n- For **Region** , select `us-west1` , and choose a **Zone** as indicated in step 2.\n- In the **Boot disk** section, ensure that the **Debian** operating system and the **10 (buster)** version are selected for the boot disk options. If necessary, click **Change** to change the image.\n- Click **Advanced options** .\n- Click **Networking** and configure the following fields:- For **Network tags** , enter`allow-ssh`and`allow-health-check-ipv6`.\n- For **Network interfaces** , select the following:- **Network** :`lb-network-dual-stack`\n- **Subnet** :`lb-subnet`\n- **IP stack type** : **IPv4 and IPv6 (dual-stack)** \n- **Primary internal IPv4 address** : **Ephemeral (automatic)** \n- **External IPv4 address** : **Ephemeral** \n- Click **Management** , and then in the **Startup script** field, enter the following script. The script contents are identical for all four VMs.```\n#! /bin/bash\nif [ -f /etc/startup_script_completed ]; then\nexit 0\nfi\napt-get update\napt-get install apache2 -y\na2ensite default-ssl\na2enmod ssl\nfile_ports=\"/etc/apache2/ports.conf\"\nfile_http_site=\"/etc/apache2/sites-available/000-default.conf\"\nfile_https_site=\"/etc/apache2/sites-available/default-ssl.conf\"\nhttp_listen_prts=\"Listen 80\\nListen 8008\\nListen 8080\\nListen 8088\"\nhttp_vh_prts=\"*:80 *:8008 *:8080 *:8088\"\nhttps_listen_prts=\"Listen 443\\nListen 8443\"\nhttps_vh_prts=\"*:443 *:8443\"\nvm_hostname=\"$(curl -H \"Metadata-Flavor:Google\" \\\nhttp://metadata.google.internal/computeMetadata/v1/instance/name)\"\necho \"Page served from: $vm_hostname\" | \\\ntee /var/www/html/index.html\nprt_conf=\"$(cat \"$file_ports\")\"\nprt_conf_2=\"$(echo \"$prt_conf\" | sed \"s|Listen 80|${http_listen_prts}|\")\"\nprt_conf=\"$(echo \"$prt_conf_2\" | sed \"s|Listen 443|${https_listen_prts}|\")\"\necho \"$prt_conf\" | tee \"$file_ports\"\nhttp_site_conf=\"$(cat \"$file_http_site\")\"\nhttp_site_conf_2=\"$(echo \"$http_site_conf\" | sed \"s|*:80|${http_vh_prts}|\")\"\necho \"$http_site_conf_2\" | tee \"$file_http_site\"\nhttps_site_conf=\"$(cat \"$file_https_site\")\"\nhttps_site_conf_2=\"$(echo \"$https_site_conf\" | sed \"s|_default_:443|${https_vh_prts}|\")\"\necho \"$https_site_conf_2\" | tee \"$file_https_site\"\nsystemctl restart apache2\ntouch /etc/startup_script_completed\n```\n- Click **Create** .\n- In the Google Cloud console, go to the **Instance groups** page. [Go to Instance groups](https://console.cloud.google.com/compute/instanceGroups/list) \n- Repeat the following steps to create two unmanaged instance groups each with two VMs in them, using these combinations.- Instance group name:`ig-a`, zone:`us-west1-a`, VMs:`vm-a1`and`vm-a2`\n- Instance group name:`ig-c`, zone:`us-west1-c`, VMs:`vm-c1`and`vm-c2`\n- Click **Create instance group** .\n- Click **New unmanaged instance group** .\n- Set **Name** as indicated in step 2.\n- In the **Location** section, select `us-west1` for the **Region** , and then choose a **Zone** as indicated in step 2.\n- For **Network** , select `lb-network-dual-stack` .\n- For **Subnetwork** , select `lb-subnet` .\n- In the **VM instances** section, add the VMs as indicated in step 2.\n- Click **Create** .\n- To create the four VMs, run the [gcloud compute instances create command](/sdk/gcloud/reference/compute/instances/create) four times, using these four combinations for `[VM-NAME]` and `[ZONE]` . The script contents are identical for all four VMs.- `VM-NAME`:`vm-a1`,`ZONE`:`us-west1-a`\n- `VM-NAME`:`vm-a2`,`ZONE`:`us-west1-a`\n- `VM-NAME`:`vm-c1`,`ZONE`:`us-west1-c`\n- `VM-NAME` : `vm-c2` , `ZONE` : `us-west1-c````\ngcloud compute instances create VM-NAME \\\n --zone=ZONE \\\n --image-family=debian-10 \\\n --image-project=debian-cloud \\\n --tags=allow-ssh,allow-health-check-ipv6 \\\n --subnet=lb-subnet \\\n --stack-type=IPV4_IPV6 \\\n --metadata=startup-script='#! /bin/bash\nif [ -f /etc/startup_script_completed ]; then\nexit 0\nfi\napt-get update\napt-get install apache2 -y\na2ensite default-ssl\na2enmod ssl\nfile_ports=\"/etc/apache2/ports.conf\"\nfile_http_site=\"/etc/apache2/sites-available/000-default.conf\"\nfile_https_site=\"/etc/apache2/sites-available/default-ssl.conf\"\nhttp_listen_prts=\"Listen 80\\nListen 8008\\nListen 8080\\nListen 8088\"\nhttp_vh_prts=\"*:80 *:8008 *:8080 *:8088\"\nhttps_listen_prts=\"Listen 443\\nListen 8443\"\nhttps_vh_prts=\"*:443 *:8443\"\nvm_hostname=\"$(curl -H \"Metadata-Flavor:Google\" \\\nhttp://metadata.google.internal/computeMetadata/v1/instance/name)\"\necho \"Page served from: $vm_hostname\" | \\\ntee /var/www/html/index.html\nprt_conf=\"$(cat \"$file_ports\")\"\nprt_conf_2=\"$(echo \"$prt_conf\" | sed \"s|Listen 80|${http_listen_prts}|\")\"\nprt_conf=\"$(echo \"$prt_conf_2\" | sed \"s|Listen 443|${https_listen_prts}|\")\"\necho \"$prt_conf\" | tee \"$file_ports\"\nhttp_site_conf=\"$(cat \"$file_http_site\")\"\nhttp_site_conf_2=\"$(echo \"$http_site_conf\" | sed \"s|*:80|${http_vh_prts}|\")\"\necho \"$http_site_conf_2\" | tee \"$file_http_site\"\nhttps_site_conf=\"$(cat \"$file_https_site\")\"\nhttps_site_conf_2=\"$(echo \"$https_site_conf\" | sed \"s|_default_:443|${https_vh_prts}|\")\"\necho \"$https_site_conf_2\" | tee \"$file_https_site\"\nsystemctl restart apache2\ntouch /etc/startup_script_completed'\n```\n- Create the two unmanaged instance groups in each zone:```\ngcloud compute instance-groups unmanaged create ig-a \\\n --zone=us-west1-a\ngcloud compute instance-groups unmanaged create ig-c \\\n --zone=us-west1-c\n```\n- Add the VMs to the appropriate instance groups:```\ngcloud compute instance-groups unmanaged add-instances ig-a \\\n --zone=us-west1-a \\\n --instances=vm-a1,vm-a2\ngcloud compute instance-groups unmanaged add-instances ig-c \\\n --zone=us-west1-c \\\n --instances=vm-c1,vm-c2\n```\nFor the four VMs, use the following VM names and zones:- `VM-NAME`:`vm-a1`,`ZONE`:`us-west1-a`\n- `VM-NAME`:`vm-a2`,`ZONE`:`us-west1-a`\n- `VM-NAME`:`vm-c1`,`ZONE`:`us-west1-c`\n- `VM-NAME`:`vm-c2`,`ZONE`:`us-west1-c`\nYou can get the current `` by running the following `gcloud` command:\n```\ngcloud compute images list \\\n --filter=\"family=debian-10\"\n```\nCreate four backend VMs by making four `POST` requests to the [instances.insert method](/compute/docs/reference/rest/v1/instances/insert) :\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/ZONE/instances\n{\n \"name\": \"VM-NAME\",\n \"tags\": {\n \"items\": [  \"allow-health-check-ipv6\",\n  \"allow-ssh\"\n ]\n },\n \"machineType\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/[ZONE]/machineTypes/e2-standard-2\",\n \"canIpForward\": false,\n \"networkInterfaces\": [ {\n  \"stackType\": \"IPV4_IPV6\",\n  \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network-dual-stack\",\n  \"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\",\n  \"accessConfigs\": [  {\n   \"type\": \"ONE_TO_ONE_NAT\",\n   \"name\": \"external-nat\",\n   \"networkTier\": \"PREMIUM\"\n  }\n  ]\n }\n ],\n \"disks\": [ {\n  \"type\": \"PERSISTENT\",\n  \"boot\": true,\n  \"mode\": \"READ_WRITE\",\n  \"autoDelete\": true,\n  \"deviceName\": \"VM-NAME\",\n  \"initializeParams\": {\n  \"sourceImage\": \"projects/debian-cloud/global/images/debian-image-name\",\n  \"diskType\": \"projects/PROJECT_ID/zones/zone/diskTypes/pd-standard\",\n  \"diskSizeGb\": \"10\"\n  }\n }\n ],\n \"metadata\": {\n \"items\": [  {\n  \"key\": \"startup-script\",\n  \"value\": \"#! /bin/bash\\napt-get update\\napt-get install apache2 -y\\na2ensite default-ssl\\na2enmod ssl\\nfile_ports=\\\"/etc/apache2/ports.conf\\\"\\nfile_http_site=\\\"/etc/apache2/sites-available/000-default.conf\\\"\\nfile_https_site=\\\"/etc/apache2/sites-available/default-ssl.conf\\\"\\nhttp_listen_prts=\\\"Listen 80\\\\nListen 8008\\\\nListen 8080\\\\nListen 8088\\\"\\nhttp_vh_prts=\\\"*:80 *:8008 *:8080 *:8088\\\"\\nhttps_listen_prts=\\\"Listen 443\\\\nListen 8443\\\"\\nhttps_vh_prts=\\\"*:443 *:8443\\\"\\nvm_hostname=\\\"$(curl -H \\\"Metadata-Flavor:Google\\\" \\\\\\nhttp://169.254.169.254/computeMetadata/v1/instance/name)\\\"\\necho \\\"Page served from: $vm_hostname\\\" | \\\\\\ntee /var/www/html/index.html\\nprt_conf=\\\"$(cat \\\"$file_ports\\\")\\\"\\nprt_conf_2=\\\"$(echo \\\"$prt_conf\\\" | sed \\\"s|Listen 80|${http_listen_prts}|\\\")\\\"\\nprt_conf=\\\"$(echo \\\"$prt_conf_2\\\" | sed \\\"s|Listen 443|${https_listen_prts}|\\\")\\\"\\necho \\\"$prt_conf\\\" | tee \\\"$file_ports\\\"\\nhttp_site_conf=\\\"$(cat \\\"$file_http_site\\\")\\\"\\nhttp_site_conf_2=\\\"$(echo \\\"$http_site_conf\\\" | sed \\\"s|*:80|${http_vh_prts}|\\\")\\\"\\necho \\\"$http_site_conf_2\\\" | tee \\\"$file_http_site\\\"\\nhttps_site_conf=\\\"$(cat \\\"$file_https_site\\\")\\\"\\nhttps_site_conf_2=\\\"$(echo \\\"$https_site_conf\\\" | sed \\\"s|_default_:443|${https_vh_prts}|\\\")\\\"\\necho \\\"$https_site_conf_2\\\" | tee \\\"$file_https_site\\\"\\nsystemctl restart apache2\"\n  }\n ]\n },\n \"scheduling\": {\n \"preemptible\": false\n },\n \"deletionProtection\": false\n}\n```\nCreate two instance groups by making a `POST` request to the [instanceGroups.insert method](/compute/docs/reference/rest/v1/instanceGroups/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/instanceGroups\n{\n \"name\": \"ig-a\",\n \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network-dual-stack\",\n \"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\"\n}\n```\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-c/instanceGroups\n{\n \"name\": \"ig-c\",\n \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network-dual-stack\",\n \"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\"\n}\n```\nAdd instances to each instance group by making a `POST` request to the [instanceGroups.addInstances method](/compute/docs/reference/rest/v1/instanceGroups/addInstances) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/instanceGroups/ig-a/addInstances\n{\n \"instances\": [ {\n  \"instance\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/instances/vm-a1\",\n  \"instance\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/instances/vm-a2\"\n }\n ]\n}\n```\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-c/instanceGroups/ig-c/addInstances\n{\n \"instances\": [ {\n  \"instance\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-c/instances/vm-c1\",\n  \"instance\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-c/instances/vm-c2\"\n }\n ]\n}\n```\n### Configure load balancer components\nThese steps configure all of the [internal passthrough Network Load Balancercomponents](/load-balancing/docs/internal#components) starting with the health check and backend service, and then the frontend components:\n- **Health check:** In this example, you use an HTTP health check that checks for an HTTP `200` (OK) response. For more information, see [health checks section of the internal passthrough Network Load Balancer overview](/load-balancing/docs/internal#health-checking) .\n- **Backend service:** Because you need to pass HTTP traffic through the internal load balancer, you need to use TCP, not UDP.\n- **Forwarding rule:** This example creates two internal forwarding rules for IPv4 and IPv6 traffic.\n- **Internal IP address:** In this example, you specify an internal IP address, `10.1.2.99` , when you create the IPv4 forwarding rule. For more information, see [Internal IP address](/load-balancing/docs/internal#load_balancing_ip_address) . Although you choose which IPv4 address is configured, the IPv6 address is assigned automatically.\n- In the Google Cloud console, go to the **Load balancing** page. [Go to Load balancing](https://console.cloud.google.com/networking/loadbalancing/list) \n- Click **Create load balancer** .\n- On the **Network Load Balancer (TCP/SSL)** card, click **Start configuration** .\n- For **Internet facing or internal only** , select **Only between my VMs** .\n- Click **Continue** .\n- On the **Create internal passthrough Network Load Balancer** page, enter the following information:- **Load balancer name** :`be-ilb`\n- **Region** : **us-west1** \n- **Network** : **lb-network-dual-stack** \n### Backend configuration\n- Click **Backend configuration** .\n- In the **New Backend** section of **Backends** , select the **IP stack type** as **IPv4 and IPv6 (dual-stack)** .\n- In **Instance group** , select the`ig-a`instance group and click **Done** .\n- Click **Add a backend** and repeat the step to add`ig-c`.\n- From the **Health check** list, select **Create a health check** , enter the following information, and click **Save** :- **Name** :`hc-http-80`.\n- **Scope** : **Regional** .\n- **Protocol** :`HTTP`.\n- **Port** :`80`.\n- **Proxy protocol** :`NONE`.\n- **Request path** :`/`.\n- Verify that a blue check mark appears next to **Backend\n configuration** .\n### Frontend configuration\n- Click **Frontend configuration** . In the **New Frontend IP and port** section, do the following:- For **Name** , enter`fr-ilb-ipv6`.\n- To handle IPv6 traffic, do the following:- For **IP version** , select **IPv6** .\n- For **Subnetwork** , select`lb-subnet`. The IPv6 address range in the forwarding rule is always ephemeral.\n- For **Ports** , select **Multiple** , and then in the **Port number** field, enter`80`,`8008`,`8080`,`8088`.\n- Click **Done** .\n- To handle IPv4 traffic, do the following:- Click **Add frontend IP and port** .\n- For **Name** , enter`fr-ilb`.\n- For **Subnetwork** , select`lb-subnet`.\n- In the **Internal IP purpose** section, from the **IP address** list, select **Create IP address** , enter the following information, and then click **Reserve** .- **Name:** `ip-ilb`\n- **IP version:** **IPv4** \n- **Static IP address:** **Let me choose** \n- **Custom IP address:** `10.1.2.99`\n- For **Ports** , select **Multiple** , and then in **Port numbers** , enter`80`,`8008`,`8080`, and`8088`.\n- Click **Done** .\n- Verify that there is a blue check mark next to **Frontend configuration** before continuing.\n### Review the configuration\n- Click **Review and finalize** . Check all your settings.\n- If the settings are correct, click **Create** . It takes a few minutes for the internal passthrough Network Load Balancer to be created.\n- Create a new regional HTTP health check to test HTTP connectivity to the VMs on port 80.```\ngcloud compute health-checks create http hc-http-80 \\\n --region=us-west1 \\\n --port=80\n```\n- Create the backend service for HTTP traffic:```\ngcloud compute backend-services create be-ilb \\\n --load-balancing-scheme=internal \\\n --protocol=tcp \\\n --region=us-west1 \\\n --health-checks=hc-http-80 \\\n --health-checks-region=us-west1\n```\n- Add the two instance groups to the backend service:```\ngcloud compute backend-services add-backend be-ilb \\\n --region=us-west1 \\\n --instance-group=ig-a \\\n --instance-group-zone=us-west1-a\ngcloud compute backend-services add-backend be-ilb \\\n --region=us-west1 \\\n --instance-group=ig-c \\\n --instance-group-zone=us-west1-c\n```\n- Create two forwarding rules for the backend service. When you create the IPv4 forwarding rule, specify `10.1.2.99` for the internal IP address in the subnet for IPv4 addresses.```\ngcloud compute forwarding-rules create fr-ilb \\\n --region=us-west1 \\\n --load-balancing-scheme=internal \\\n --subnet=lb-subnet \\\n --address=10.1.2.99 \\\n --ip-protocol=TCP \\\n --ports=80,8008,8080,8088 \\\n --backend-service=be-ilb \\\n --backend-service-region=us-west1\n``````\ngcloud compute forwarding-rules create fr-ilb-ipv6 \\\n --region=us-west1 \\\n --load-balancing-scheme=internal \\\n --subnet=lb-subnet \\\n --ip-protocol=TCP \\\n --ports=80,8008,8080,8088 \\\n --backend-service=be-ilb \\\n --backend-service-region=us-west1 \\\n --ip-version=IPV6\n```\nCreate the health check by making a `POST` request to the [regionHealthChecks.insert method](/compute/docs/reference/rest/v1/regionHealthChecks/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/regionHealthChecks\n{\n\"name\": \"hc-http-80\",\n\"type\": \"HTTP\",\n\"httpHealthCheck\": {\n \"port\": 80\n}\n}\n```\nCreate the regional backend service by making a `POST` request to the [regionBackendServices.insert method](/compute/docs/reference/rest/v1/regionBackendServices/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/backendServices\n{\n\"name\": \"be-ilb\",\n\"backends\": [ {\n \"group\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/instanceGroups/ig-a\",\n \"balancingMode\": \"CONNECTION\"\n },\n {\n \"group\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-c/instanceGroups/ig-c\",\n \"balancingMode\": \"CONNECTION\"\n }\n],\n\"healthChecks\": [ \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/healthChecks/hc-http-80\"\n],\n\"loadBalancingScheme\": \"INTERNAL\",\n\"connectionDraining\": {\n \"drainingTimeoutSec\": 0\n }\n}\n```\nCreate the forwarding rule by making a `POST` request to the [forwardingRules.insert method](/compute/docs/reference/rest/v1/forwardingRules/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/forwardingRules\n{\n\"name\": \"fr-ilb-ipv6\",\n\"IPProtocol\": \"TCP\",\n\"ports\": [ \"80\", \"8008\", \"8080\", \"8088\"\n],\n\"loadBalancingScheme\": \"INTERNAL\",\n\"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\",\n\"backendService\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/backendServices/be-ilb\",\n\"ipVersion\": \"IPV6\",\n\"networkTier\": \"PREMIUM\"\n}\n```\nCreate the forwarding rule by making a `POST` request to the [forwardingRules.insert method](/compute/docs/reference/rest/v1/forwardingRules/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/forwardingRules\n{\n\"name\": \"fr-ilb\",\n\"IPAddress\": \"10.1.2.99\",\n\"IPProtocol\": \"TCP\",\n\"ports\": [ \"80\", \"8008\", \"8080\", \"8088\"\n],\n\"loadBalancingScheme\": \"INTERNAL\",\n\"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\",\n\"backendService\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/backendServices/be-ilb\",\n\"networkTier\": \"PREMIUM\"\n}\n```\n### Test your load balancer\nTo test the load balancer, create a client VM in the same region as the load balancer, and then send traffic from the client to the load balancer.\nThis example creates a client VM ( `vm-client` ) in the same region as the backend (server) VMs. The client is used to validate the load balancer's configuration and demonstrate expected behavior as described in the [testing](#test-your-load-balancer) section.\n- In the Google Cloud console, go to the **VM instances** page. [Go to VM instances](https://console.cloud.google.com/compute/instances) \n- Click **Create instance** .\n- For **Name** , enter `vm-client` .\n- For **Region** , select **us-west1** .\n- For **Zone** , select **us-west1-a** .\n- Click **Advanced options** .\n- Click **Networking** and configure the following fields:- For **Network tags** , enter`allow-ssh`.\n- For **Network interfaces** , select the following:- **Network** : **lb-network-dual-stack** \n- **Subnet** : **lb-subnet** \n- **IP stack type** : **IPv4 and IPv6 (dual-stack)** \n- **Primary internal IP** : **Ephemeral (automatic)** \n- **External IP** : **Ephemeral** \n- Click **Done** .\n- Click **Create** .\nThe client VM can be in any zone in the same region as the load balancer, and it can use any subnet in that region. In this example, the client is in the `us-west1-a` zone, and it uses the same subnet as the backend VMs.\n```\ngcloud compute instances create vm-client \\\n --zone=us-west1-a \\\n --image-family=debian-10 \\\n --image-project=debian-cloud \\\n --stack-type=IPV4_IPV6 \\\n --tags=allow-ssh \\\n --subnet=lb-subnet\n```\nMake a `POST` request to the [instances.insert method](/compute/docs/reference/rest/v1/instances/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/instances\n{\n \"name\": \"vm-client\",\n \"tags\": {\n \"items\": [  \"allow-ssh\"\n ]\n },\n \"machineType\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/machineTypes/e2-standard-2\",\n \"canIpForward\": false,\n \"networkInterfaces\": [ {\n  \"stackType\": \"IPV4_IPV6\",\n  \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network-dual-stack\",\n  \"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\",\n  \"accessConfigs\": [  {\n   \"type\": \"ONE_TO_ONE_NAT\",\n   \"name\": \"external-nat\",\n   \"networkTier\": \"PREMIUM\"\n  }\n  ]\n }\n ],\n \"disks\": [ {\n  \"type\": \"PERSISTENT\",\n  \"boot\": true,\n  \"mode\": \"READ_WRITE\",\n  \"autoDelete\": true,\n  \"deviceName\": \"vm-client\",\n  \"initializeParams\": {\n  \"sourceImage\": \"projects/debian-cloud/global/images/debian-image-name\",\n  \"diskType\": \"projects/PROJECT_ID/zones/us-west1-a/diskTypes/pd-standard\",\n  \"diskSizeGb\": \"10\"\n  }\n }\n ],\n \"scheduling\": {\n \"preemptible\": false\n },\n \"deletionProtection\": false\n}\n```\nThis test contacts the load balancer from a separate client VM; that is, not from a backend VM of the load balancer. The expected behavior is for traffic to be distributed among the four backend VMs.\n- Connect to the client VM instance.```\ngcloud compute ssh vm-client --zone=us-west1-a\n```\n- Describe the IPv6 forwarding rule `fr-ilb-ipv6` . Note the `` in the description.```\ngcloud compute forwarding-rules describe fr-ilb-ipv6 --region=us-west1\n```\n- Describe the IPv4 forwarding rule `fr-ilb` .```\ngcloud compute forwarding-rules describe fr-ilb --region=us-west1\n```\n- From clients with IPv6 connectivity, run the following command:```\n$ curl -m 10 -s http://IPV6_ADDRESS:80\n```For example, if the assigned IPv6 address is `[fd20:1db0:b882:802:0:46:0:0/96]:80` , the command should look like:```\n$ curl -m 10 -s http://[fd20:1db0:b882:802:0:46:0:0]:80\n```\n- From clients with IPv4 connectivity, run the following command:```\n$ curl -m 10 -s http://10.1.2.99:80\n```Replace the placeholders with valid values:- ``is the ephemeral IPv6 address in the`fr-ilb-ipv6`forwarding rule.\n## Additional configuration options\nThis section expands on the configuration example to provide alternative and additional configuration options. All of the tasks are optional. You can perform them in any order.\n### Enable global access\nYou can enable [global access](/load-balancing/docs/internal#client-access) for your example internal passthrough Network Load Balancer to make it accessible to clients in all regions. The backends of your example load balancer must still be located in one region ( `us-west1` ).\nTo configure global access, make the following configuration changes.\n**Edit the load balancer's forwarding rule** - In the Google Cloud console, go to the **Load balancing** page. [Go to Load balancing](https://console.cloud.google.com/networking/loadbalancing/list) \n- In the **Name** column, click your internal passthrough Network Load Balancer. The example load balancer is named `be-ilb` .\n- Click **Frontend configuration** .\n- Click **Edit** edit .\n- Under **Global access** , select **Enable** .\n- Click **Done** .\n- Click **Update** .\nOn the **Load balancer details** page, verify that the frontend configuration says **Regional (REGION) with global access** .- Update the example load balancer's forwarding rule, `fr-ilb` to include the `--allow-global-access` flag.```\ngcloud compute forwarding-rules update fr-ilb \\\n --region=us-west1 \\\n --allow-global-access\n```\n- You can use the `forwarding-rules describe` command to determine whether a forwarding rule has global access enabled. For example:```\ngcloud compute forwarding-rules describe fr-ilb \\\n --region=us-west1 \\\n --format=\"get(name,region,allowGlobalAccess)\"\n```The word `True` appears in the output, after the name and region of the forwarding rule, when global access is enabled.\nMake a `PATCH` request to the [forwardingRules/patch method](/compute/docs/reference/rest/v1/forwardingRules/patch) .\n```\nPATCH https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/forwardingRules/fr-ilb\n{\n\"allowGlobalAccess\": true\n}\n```- In the Google Cloud console, go to the **VM instances** page. [Go to VM instances](https://console.cloud.google.com/compute/instances) \n- Click **Create instance** .\n- Set the **Name** to `vm-client2` .\n- Set the **Region** to **europe-west1** .\n- Set the **Zone** to **europe-west1-b** .\n- Click **Advanced options** .\n- Click **Networking** and configure the following fields:- For **Network tags** , enter`allow-ssh`.\n- For **Network interfaces** , select the following:- **Network** : **lb-network** \n- **Subnet** : **europe-subnet** \n- Click **Create** .\nThe client VM can be in any zone in the same region as the load balancer, and it can use any subnet in that region. In this example, the client is in the `europe-west1-b` zone, and it uses the same subnet as the backend VMs.\n```\ngcloud compute instances create vm-client2 \\\n --zone=europe-west1-b \\\n --image-family=debian-10 \\\n --image-project=debian-cloud \\\n --tags=allow-ssh \\\n --subnet=europe-subnet\n```\nMake a `POST` request to the [instances.insert method](/compute/docs/reference/rest/v1/instances/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/europe-west1-b/instances\n{\n\"name\": \"vm-client2\",\n\"tags\": {\n \"items\": [ \"allow-ssh\"\n ]\n},\n\"machineType\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/europe-west1-b/machineTypes/e2-standard-2\",\n\"canIpForward\": false,\n\"networkInterfaces\": [ {\n \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n \"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/europe-west1/subnetworks/europe-subnet\",\n \"accessConfigs\": [  {\n  \"type\": \"ONE_TO_ONE_NAT\",\n  \"name\": \"external-nat\",\n  \"networkTier\": \"PREMIUM\"\n  }\n ]\n }\n],\n\"disks\": [ {\n \"type\": \"PERSISTENT\",\n \"boot\": true,\n \"mode\": \"READ_WRITE\",\n \"autoDelete\": true,\n \"deviceName\": \"vm-client2\",\n \"initializeParams\": {\n  \"sourceImage\": \"projects/debian-cloud/global/images/debian-image-name\",\n  \"diskType\": \"projects/PROJECT_ID/zones/europe-west1-b/diskTypes/pd-standard\",\n  \"diskSizeGb\": \"10\"\n }\n }\n],\n\"scheduling\": {\n \"preemptible\": false\n},\n\"deletionProtection\": false\n}\n```\nTo test the connectivity, run the following command:\n```\n gcloud compute ssh vm-client2 --zone=europe-west1-b\n \n```\nTest connecting to the load balancer on all configured ports, [as you didfrom the vm-client in the us-west1 region](#test-your-load-balancer) . Test HTTP connectivity on the four ports configured on the forwarding rule:\n```\n curl http://10.1.2.99\n curl http://10.1.2.99:8008\n curl http://10.1.2.99:8080\n curl http://10.1.2.99:8088\n \n```### Configure managed instance groups\nThe example configuration created two [unmanaged instancegroups](#configure_instances_and_instance_groups) . You can instead use [managedinstance groups](/compute/docs/instance-groups#types_of_managed_instance_groups) , including zonal and regional managed instance groups, as backends for internal passthrough Network Load Balancers.\nManaged instance groups require that you create an instance template. This procedure demonstrates how to replace the two zonal unmanaged instance groups from the example with a single, regional managed instance group. A regional managed instance group automatically creates VMs in multiple zones of the region, making it simpler to distribute production traffic among zones.\nManaged instance groups also support [autoscaling](/sdk/gcloud/reference/compute/instance-groups/managed/set-autoscaling) and [autohealing](/sdk/gcloud/reference/compute/instance-groups/managed/update) . If you use autoscaling with internal passthrough Network Load Balancers, you cannot scale based on load balancing.\nThis procedure shows you how to modify the backend service for the example internal passthrough Network Load Balancer so that it uses a regional managed instance group.\n**Instance template** - In the Google Cloud console, go to the **VM instance templates** page. [Go to VM instance templates](https://console.cloud.google.com/compute/instanceTemplates) \n- Click **Create instance template** .\n- Set the **Name** to `template-vm-ilb` .\n- Choose a [machine type](/compute/docs/machine-types) .\n- In the **Boot disk** section, ensure that the **Debian** operating system and the **10 (buster)** version are selected for the boot disk options. If necessary, click **Change** to change the image.\n- Click **Advanced options** .\n- Click **Networking** and configure the following fields:- For **Network tags** , enter`allow-ssh`and`allow-health-check`.\n- For **Network interfaces** , select the following:- **Network** :`lb-network`\n- **Subnet** :`lb-subnet`\n- Click **Management** , and then in the **Startup script** field, enter the following script:```\n#! /bin/bash\nif [ -f /etc/startup_script_completed ]; then\nexit 0\nfi\napt-get update\napt-get install apache2 -y\na2ensite default-ssl\na2enmod ssl\nfile_ports=\"/etc/apache2/ports.conf\"\nfile_http_site=\"/etc/apache2/sites-available/000-default.conf\"\nfile_https_site=\"/etc/apache2/sites-available/default-ssl.conf\"\nhttp_listen_prts=\"Listen 80\\nListen 8008\\nListen 8080\\nListen 8088\"\nhttp_vh_prts=\"*:80 *:8008 *:8080 *:8088\"\nhttps_listen_prts=\"Listen 443\\nListen 8443\"\nhttps_vh_prts=\"*:443 *:8443\"\nvm_hostname=\"$(curl -H \"Metadata-Flavor:Google\" \\\nhttp://metadata.google.internal/computeMetadata/v1/instance/name)\"\necho \"Page served from: $vm_hostname\" | \\\ntee /var/www/html/index.html\nprt_conf=\"$(cat \"$file_ports\")\"\nprt_conf_2=\"$(echo \"$prt_conf\" | sed \"s|Listen 80|${http_listen_prts}|\")\"\nprt_conf=\"$(echo \"$prt_conf_2\" | sed \"s|Listen 443|${https_listen_prts}|\")\"\necho \"$prt_conf\" | tee \"$file_ports\"\nhttp_site_conf=\"$(cat \"$file_http_site\")\"\nhttp_site_conf_2=\"$(echo \"$http_site_conf\" | sed \"s|*:80|${http_vh_prts}|\")\"\necho \"$http_site_conf_2\" | tee \"$file_http_site\"\nhttps_site_conf=\"$(cat \"$file_https_site\")\"\nhttps_site_conf_2=\"$(echo \"$https_site_conf\" | sed \"s|_default_:443|${https_vh_prts}|\")\"\necho \"$https_site_conf_2\" | tee \"$file_https_site\"\nsystemctl restart apache2\ntouch /etc/startup_script_completed\n```\n- Click **Create** .\n **Managed instance group** - In the Google Cloud console, go to the **Instance groups** page. [Go to Instance groups](https://console.cloud.google.com/compute/instanceGroups) \n- Click **Create instance group** .\n- Set the **Name** to `ig-ilb` .\n- For **Location** , choose **Multi-zone** , and set the **Region** to `us-west1` .\n- Set the **Instance template** to `template-vm-ilb` .\n- Optional: Configure [autoscaling](/compute/docs/autoscaler) . You cannot autoscale the instance group based on HTTP load balancing usage because the instance group is a backend for the internal passthrough Network Load Balancer.\n- Set the **Minimum number of instances** to `1` and the **Maximum numberof instances** to `6` .\n- Optional: Configure [autohealing](/compute/docs/instance-groups/autohealing-instances-in-migs) . If you configure autohealing, use the same health check used by the backend service for the internal passthrough Network Load Balancer. In this example, use `hc-http-80` .\n- Click **Create** .\n- Create the instance template. Optionally, you can [set otherparameters](/sdk/gcloud/reference/compute/instance-templates/create) , such as [machine type](/compute/docs/machine-types) , for the image template to use.```\ngcloud compute instance-templates create template-vm-ilb \\\n --image-family=debian-10 \\\n --image-project=debian-cloud \\\n --tags=allow-ssh,allow-health-check \\\n --subnet=lb-subnet \\\n --region=us-west1 \\\n --network=lb-network \\\n --metadata=startup-script='#! /bin/bash\nif [ -f /etc/startup_script_completed ]; then\nexit 0\nfi\napt-get update\napt-get install apache2 -y\na2ensite default-ssl\na2enmod ssl\nfile_ports=\"/etc/apache2/ports.conf\"\nfile_http_site=\"/etc/apache2/sites-available/000-default.conf\"\nfile_https_site=\"/etc/apache2/sites-available/default-ssl.conf\"\nhttp_listen_prts=\"Listen 80\\nListen 8008\\nListen 8080\\nListen 8088\"\nhttp_vh_prts=\"*:80 *:8008 *:8080 *:8088\"\nhttps_listen_prts=\"Listen 443\\nListen 8443\"\nhttps_vh_prts=\"*:443 *:8443\"\nvm_hostname=\"$(curl -H \"Metadata-Flavor:Google\" \\\nhttp://metadata.google.internal/computeMetadata/v1/instance/name)\"\necho \"Page served from: $vm_hostname\" | \\\ntee /var/www/html/index.html\nprt_conf=\"$(cat \"$file_ports\")\"\nprt_conf_2=\"$(echo \"$prt_conf\" | sed \"s|Listen 80|${http_listen_prts}|\")\"\nprt_conf=\"$(echo \"$prt_conf_2\" | sed \"s|Listen 443|${https_listen_prts}|\")\"\necho \"$prt_conf\" | tee \"$file_ports\"\nhttp_site_conf=\"$(cat \"$file_http_site\")\"\nhttp_site_conf_2=\"$(echo \"$http_site_conf\" | sed \"s|*:80|${http_vh_prts}|\")\"\necho \"$http_site_conf_2\" | tee \"$file_http_site\"\nhttps_site_conf=\"$(cat \"$file_https_site\")\"\nhttps_site_conf_2=\"$(echo \"$https_site_conf\" | sed \"s|_default_:443|${https_vh_prts}|\")\"\necho \"$https_site_conf_2\" | tee \"$file_https_site\"\nsystemctl restart apache2\ntouch /etc/startup_script_completed'\n```\n- Create one regional managed instance group using the template:```\ngcloud compute instance-groups managed create ig-ilb \\\n --template=template-vm-ilb \\\n --region=us-west1 \\\n --size=6\n```\n- Add the regional managed instance group as a backend to [the backendservice that you already created](#configure_the_load_balancer) :```\ngcloud compute backend-services add-backend be-ilb \\\n --region=us-west1 \\\n --instance-group=ig-ilb \\\n --instance-group-region=us-west1\n```\n- Disconnect the two unmanaged (zonal) instance groups from the backend service:```\ngcloud compute backend-services remove-backend be-ilb \\\n --region=us-west1 \\\n --instance-group=ig-a \\\n --instance-group-zone=us-west1-a\ngcloud compute backend-services remove-backend be-ilb \\\n --region=us-west1 \\\n --instance-group=ig-c \\\n --instance-group-zone=us-west1-c\n```\n### Remove external IP addresses from backend VMs\nWhen you [created the backend VMs](#configure_instances_and_instance_groups) , each was assigned an ephemeral external IP address so it could download Apache using a startup script. Because the backend VMs are only used by an internal passthrough Network Load Balancer, you can remove their external IP addresses. Removing external IP addresses prevents the backend VMs from accessing the internet directly.\n**Important:** Removing the external IP address from a VM limits how you can connect to it. For details, see [Choose a connection option for internal-only VMs](/compute/docs/connect/ssh-internal-ip) . It's useful to leave external IP addresses on backend VMs if you need to demonstrate [how requests from a backend VM to the IP address of theload balancer are handled](#test-from-backend-vms) .\n- In the Google Cloud console, go to the **VM instances** page. [Go to VM instances](https://console.cloud.google.com/compute/instances) \n- Repeat the following steps for each backend VM.\n- Click the name of the backend VM, for example, `vm-a1` .\n- Click edit **Edit** .\n- In the **Network interfaces** section, click the network.\n- From the **External IP** list, select **None** , and click **Done** .\n- Click **Save** .\n- To look up the zone for an instance \u2013 for example, if you're using a [regional managed instance group](#internal_load_balancing_with_regional_instance_groups) \u2013 run the following command for each instance to determine its zone. Replace `[SERVER-VM]` with the name of the VM to look up.```\ngcloud compute instances list --filter=\"name=[SERVER-VM]\"\n```\n- Repeat the following step for each backend VM. Replace `[SERVER-VM]` with the name of the VM, and replace and `[ZONE]` with the VM's zone.```\ngcloud compute instances delete-access-config [SERVER-VM] \\\n --zone=[ZONE] \\\n --access-config-name=external-nat\n```\nMake a `POST` request to the [instances.deleteAccessConfig method](/compute/docs/reference/rest/v1/instances/deleteAccessConfig) for each backend VM, replacing `vm-a1` with the name of the VM, and replacing and `us-west1-a` with the VM's zone.\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/us-west1-a/instances/vm-a1/deleteAccessConfig?accessConfig=external-nat&networkInterface=None\n```\n### Use a reserved internal IP address\nWhen you create backend VMs and instance groups, the VM instance uses an ephemeral internal IPv4 or IPv6 address.\nThe following steps show you how to promote an internal IPv4 or IPv6 address to a static internal IPv4 or IPv6 address and then update the VM instance to use the static internal IP address:\n- [Promote an in-use ephemeral internal IPv4 or IPv6 address to a staticaddress](/compute/docs/ip-addresses/reserve-static-internal-ip-address#promote-in-use-internal-address) .\n- [Change or assign an internal IPv6 address to an existinginstance](/compute/docs/ip-addresses/reserve-static-internal-ip-address#change_assign_intipv6) .\nAlternatively, the following steps show you how to reserve a new static internal IPv4 or IPv6 address and then update the VM instance to use the static internal IP address:\n- [Reserve a new static internal IPv4 orIPv6 address](/compute/docs/ip-addresses/reserve-static-internal-ip-address#reservenewip) .Unlike internal IPv4 reservation, internal IPv6 reservation doesn't support reserving a specific IP address from the subnetwork. Instead, a `/96` internal IPv6 address range is automatically allocated from the subnet's `/64` internal IPv6 address range.\n- [Change or assign an internal IPv6 address to an existinginstance](/compute/docs/ip-addresses/reserve-static-internal-ip-address#change_assign_intipv6) .\nFor more information, see [How to reserve a static internal IPaddress](/compute/docs/ip-addresses/reserve-static-internal-ip-address#how_to_reserve_a_static_internal_ip_address) .\n### Accept traffic on all ports\nThe load balancer's forwarding rule, not its backend service, determines the port or ports on which the load balancer accepts traffic. For information about the purpose of each component, see [Components](/load-balancing/docs/internal#components) .\nWhen you [created this example load balancer's forwardingrule](#configure_the_load_balancer) , you configured ports `80` , `8008` , `8080` , and `8088` . The startup script that installs Apache also configures it to accept HTTPS connections on ports `443` and `8443` .\nTo support these six ports, you can configure the forwarding rule to accept traffic on all ports. With this strategy, you can also configure the firewall rule or rules that allow incoming connections to backend VMs so that they only permit certain ports.\nThis procedure shows you how to replace the example load balancer's forwarding rule with one that accepts traffic on all ports.\nFor more information about when to use this setup, see [Internal passthrough Network Load Balancers and forwarding rules with a common IPaddress](/load-balancing/docs/internal/multiple-forwarding-rules-same-ip) .\n**Delete your forwarding rule and create a new one** - In the Google Cloud console, go to the **Load balancing** page. [Go to Load balancing](https://console.cloud.google.com/networking/loadbalancing/list) \n- Click the `be-ilb` load balancer and click **Edit** .\n- Click **Frontend configuration** .\n- Hold the pointer over the `10.1.2.9` forwarding rule and click delete **Delete** .\n- Click **Add frontend IP and port** .\n- In the **New Frontend IP and port** section, enter the following information and click **Done** :- **Name** :`fr-ilb`\n- **Subnetwork** : **lb-subnet** \n- **Internal IP** : **ip-ilb** \n- **Ports** : **All** .\n- Verify that there is a blue check mark next to **Frontend configuration** before continuing.\n- Click **Review and finalize** and review your load balancer configuration settings.\n- Click **Create** .\n- Delete your existing forwarding rule, `fr-ilb` .```\ngcloud compute forwarding-rules delete fr-ilb \\\n --region=us-west1\n```\n- Create a replacement forwarding rule, with the same name, whose port configuration uses the keyword `ALL` . The other parameters for the forwarding rule remain the same.```\ngcloud compute forwarding-rules create fr-ilb \\\n --region=us-west1 \\\n --load-balancing-scheme=internal \\\n --network=lb-network \\\n --subnet=lb-subnet \\\n --address=10.1.2.99 \\\n --ip-protocol=TCP \\\n --ports=ALL \\\n --backend-service=be-ilb \\\n --backend-service-region=us-west1\n```\nDelete the forwarding rule by making a `DELETE` request to the [forwardingRules.delete method](/compute/docs/reference/rest/v1/forwardingRules/delete) .\n```\nDELETE https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/forwardingRules/fr-ilb\n```\nCreate the forwarding rule by making a `POST` request to the [forwardingRules.insert method](/compute/docs/reference/rest/v1/forwardingRules/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/forwardingRules\n{\n\"name\": \"fr-ilb\",\n\"IPAddress\": \"10.1.2.99\",\n\"IPProtocol\": \"TCP\",\n\"allPorts\": true,\n\"loadBalancingScheme\": \"INTERNAL\",\n\"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\",\n\"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n\"backendService\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/backendServices/be-ilb\",\n\"networkTier\": \"PREMIUM\"\n}\n```\nConnect to the client VM instance and test HTTP and HTTPS connections.\n- Connect to the client VM:```\ngcloud compute ssh vm-client --zone=us-west1-a\n```\n- Test HTTP connectivity on all four ports:```\ncurl http://10.1.2.99\ncurl http://10.1.2.99:8008\ncurl http://10.1.2.99:8080\ncurl http://10.1.2.99:8088\n```\n- Test HTTPS connectivity on ports `443` and `8443` . The `--insecure` flag is required because each Apache server in the example setup uses a self-signed certificate. * pragma: { seclinter_this_is_fine: true } * ```\ncurl https://10.1.2.99 --insecure\ncurl https://10.1.2.99:8443 --insecure\n```* pragma: { seclinter_this_is_fine: false } *\n- Observe that HTTP requests (on all four ports) and HTTPS requests (on both ports) are distributed among all of the backend VMs.\n### Accept traffic on multiple ports using two forwarding rules\nWhen you [created this example load balancer's forwardingrule](#configure_the_load_balancer) , you configured ports `80` , `8008` , `8080` , and `8088` . The startup script that installs Apache also configures it to accept HTTPS connections on ports `443` and `8443` .\nAn alternative strategy to configuring a single forwarding rule to [accepttraffic on all ports](#all-ports) is to create multiple forwarding rules, each supporting five or fewer ports.\nThis procedure shows you how to replace the example load balancer's forwarding rule with two forwarding rules, one handling traffic on ports `80` , `8008` , `8080` , and `8088` , and the other handling traffic on ports `443` and `8443` .\nFor more information about when to use this setup, see [Internal passthrough Network Load Balancers and forwarding rules with a common IPaddress](/load-balancing/docs/internal/multiple-forwarding-rules-same-ip) .\n- In the Google Cloud console, go to the **Forwarding rules** page. [Go to Forwarding rules](https://console.cloud.google.com/net-services/loadbalancing/advanced/forwardingRules/list) \n- In the **Name** column, click **fr-ilb** , and then click **Delete** .\n- In the Google Cloud console, go to the **Load balancing** page. [Go to Load balancing](https://console.cloud.google.com/networking/loadbalancing/list) \n- In the **Name** column, click **be-ilb** .\n- Click **Edit** .\n- Click **Frontend configuration** .\n- Click **Add frontend IP and port** .\n- In the **New Frontend IP and port** section, do the following:- For **Name** , enter`fr-ilb-http`.\n- For **Subnetwork** , select **lb-subnet** .\n- For **Internal IP purpose** , select **Shared** .\n- From the **IP address** list, select **Create IP address** , enter the following information, and click **Reserve** :- **Name** :`internal-10-1-2-99`\n- **Static IP address** : **Let me choose** \n- **Custom IP address** :`10.1.2.99`\n- For **Ports** , select **Multiple** , and then in **Port numbers** , enter`80`,`8008`,`8080`, and`8088`.\n- Click **Done** .\n- Click **Add frontend IP and port** .\n- In the **New Frontend IP and port** section, do the following:- For **Name** , enter`fr-ilb-https`.\n- For **Subnetwork** , select **lb-subnet** .\n- For **Internal IP purpose** , select **Shared** .\n- From the **IP address** list, select **internal-10-1-2-99** .\n- For **Ports** , select **Multiple** , and then in **Port numbers** , enter`443`and`8443`.\n- Click **Done** .\n- Click **Review and finalize** , and review your load balancer configuration settings.\n- Click **Update** .\n- Delete your existing forwarding rule, `fr-ilb` .```\ngcloud compute forwarding-rules delete fr-ilb \\\n --region=us-west1\n```\n- Create a static (reserved) internal IP address for `10.1.2.99` and set its `--purpose` flag to `SHARED_LOADBALANCER_VIP` . The `--purpose` flag is required so that two internal forwarding rules can use the same internal IP address.```\ngcloud compute addresses create internal-10-1-2-99 \\\n --region=us-west1 \\\n --subnet=lb-subnet \\\n --addresses=10.1.2.99 \\\n --purpose=SHARED_LOADBALANCER_VIP\n```- Create two replacement forwarding rules with the following parameters:\n```\ngcloud compute forwarding-rules create fr-ilb-http \\\n --region=us-west1 \\\n --load-balancing-scheme=internal \\\n --network=lb-network \\\n --subnet=lb-subnet \\\n --address=10.1.2.99 \\\n --ip-protocol=TCP \\\n --ports=80,8008,8080,8088 \\\n --backend-service=be-ilb \\\n --backend-service-region=us-west1\n``````\ngcloud compute forwarding-rules create fr-ilb-https \\\n --region=us-west1 \\\n --load-balancing-scheme=internal \\\n --network=lb-network \\\n --subnet=lb-subnet \\\n --address=10.1.2.99 \\\n --ip-protocol=TCP \\\n --ports=443,8443 \\\n --backend-service=be-ilb \\\n --backend-service-region=us-west1\n```\nDelete the forwarding rule by making a `DELETE` request to the [forwardingRules.delete method](/compute/docs/reference/rest/v1/forwardingRules/delete) .\n```\nDELETE https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/forwardingRules/fr-ilb\n```\nCreate a static (reserved) internal IP address for `10.1.2.99` and set its purpose to `SHARED_LOADBALANCER_VIP` by making a `POST` request to the [addresses.insert method](/compute/docs/reference/rest/v1/addresses/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/addresses\n{\n\"name\": \"internal-10-1-2-99\",\n\"address\": \"10.1.2.99\",\n\"prefixLength\": 32,\n\"addressType\": INTERNAL,\n\"purpose\": SHARED_LOADBALANCER_VIP,\n\"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\"\n}\n```\nCreate two forwarding rules by making two `POST` requests to the [forwardingRules.insert method](/compute/docs/reference/rest/v1/forwardingRules/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/forwardingRules\n{\n\"name\": \"fr-ilb-http\",\n\"IPAddress\": \"10.1.2.99\",\n\"IPProtocol\": \"TCP\",\n\"ports\": [ \"80\", \"8008\", \"8080\", \"8088\"\n],\n\"loadBalancingScheme\": \"INTERNAL\",\n\"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\",\n\"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n\"backendService\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/backendServices/be-ilb\",\n\"networkTier\": \"PREMIUM\"\n}\n```\n```\n{\n\"name\": \"fr-ilb-https\",\n\"IPAddress\": \"10.1.2.99\",\n\"IPProtocol\": \"TCP\",\n\"ports\": [ \"443\", \"8443\"\n],\n\"loadBalancingScheme\": \"INTERNAL\",\n\"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\",\n\"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n\"backendService\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/backendServices/be-ilb\",\n\"networkTier\": \"PREMIUM\"\n}\n```\nConnect to the client VM instance and test HTTP and HTTPS connections.\n- Connect to the client VM:```\ngcloud compute ssh vm-client --zone=us-west1-a\n```\n- Test HTTP connectivity on all four ports:```\ncurl http://10.1.2.99\ncurl http://10.1.2.99:8008\ncurl http://10.1.2.99:8080\ncurl http://10.1.2.99:8088\n```\n- Test HTTPS connectivity on ports `443` and `8443` . The `--insecure` flag is required because each Apache server in the example setup uses a self-signed certificate.* pragma: { seclinter_this_is_fine: true } *```\ncurl https://10.1.2.99 --insecure\ncurl https://10.1.2.99:8443 --insecure\n```* pragma: { seclinter_this_is_fine: false } *\n- Observe that HTTP requests (on all four ports) and HTTPS requests (on both ports) are distributed among all of the backend VMs.\n### Use session affinity\nThe [example configuration](#configure_the_load_balancer) creates a backend service without session affinity.\nThis procedure shows you how to update the backend service for the example internal passthrough Network Load Balancer so that it uses session affinity based on a hash created from the client's IP addresses and the IP address of the load balancer's internal forwarding rule.\nFor supported session affinity types, see [Session affinityoptions](/load-balancing/docs/internal#session_affinity) .\n**Note:** For internal load balancers, setting session affinity is supported in the gcloud CLI and the API. You can't set session affinity for UDP traffic by using the Google Cloud console.\n- In the Google Cloud console, go to the **Load balancing** page. [Go to Load balancing](https://console.cloud.google.com/networking/loadbalancing/list) \n- Click **be-ilb** (the name of the backend service that you created for this example) and click **Edit** .\n- On the **Edit internal passthrough Network Load Balancer** page, click **Backend configuration** .\n- From the **Session affinity** list, select **Client IP** .\n- Click **Update** .\nUse the following `gcloud` command to update the `be-ilb` backend service, specifying client IP session affinity:\n```\ngcloud compute backend-services update be-ilb \\\n --region=us-west1 \\\n --session-affinity CLIENT_IP\n```\nMake a `PATCH` request to the [regionBackendServices/patch method](/compute/docs/reference/rest/v1/regionBackendServices/patch) .\n```\nPATCH https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/backendServices/be-ilb\n{\n\"sessionAffinity\": \"CLIENT_IP\"\n}\n```\n### Configure a connection tracking policy\nThis section shows you how to update the backend service to change the load balancer's default connection tracking policy.\nA connection tracking policy includes the following settings:\n- [Tracking mode](/load-balancing/docs/internal#tracking-mode) \n- [Connection persistence on unhealthybackends](/load-balancing/docs/internal#connection-persistence) \n- [Idle timeout](/load-balancing/docs/internal#idle-timeout) \n**Note:** You cannot use the Google Cloud console to configure a connection tracking policy. Use the Google Cloud CLI or the [REST API](/compute/docs/reference/rest/v1/regionBackendServices) instead.\nUse the following [gcloud computebackend-services command](/sdk/gcloud/reference/compute/backend-services) to update the connection tracking policy for the backend service:\n```\ngcloud compute backend-services update BACKEND_SERVICE \\\n --region=REGION \\\n --tracking-mode=TRACKING_MODE \\\n --connection-persistence-on-unhealthy-backends=CONNECTION_PERSISTENCE_BEHAVIOR \\\n --idle-timeout-sec=IDLE_TIMEOUT_VALUE\n```\nReplace the placeholders with valid values:- ``: the backend service that you're updating\n- ``: the region of the backend service that you're updating\n- ``: the connection tracking mode to be used for incoming packets; for the list of supported values, see [Tracking mode](/load-balancing/docs/internal#tracking-mode) \n- ``: the connection persistence behavior when backends are unhealthy; for the list of supported values, see [Connection persistence on unhealthybackends](/load-balancing/docs/internal#connection-persistence) \n- `` : the number of seconds that a connection tracking table entry must be maintained after the load balancer processes the last packet that matched the entryYou can only modify this property when the connection tracking is less than 5-tuple (that is, when session affinity is configured to be either `CLIENT_IP` or `CLIENT_IP_PROTO` , and the tracking mode is `PER_SESSION` ).The default value is 600 seconds (10 minutes). The maximum configurable idle timeout value is 57,600 seconds (16 hours).\n### Create a forwarding rule in another subnet\nThis procedure creates a second IP address and forwarding rule in a different subnet to demonstrate that you can [create multiple forwardingrules](/load-balancing/docs/internal#multiple_forwarding_rule) for one internal passthrough Network Load Balancer. The region for the forwarding rule must match the region of the backend service.\nSubject to firewall rules, clients in any subnet in the region can contact either internal passthrough Network Load Balancer IP address.\n**Add the second subnet** - In the Google Cloud console, go to the **VPC networks** page. [Go to VPC networks](https://console.cloud.google.com/networking/networks/list) \n- Click **Create VPC network** .\n- Click `lb-network` .\n- In the **Subnets** section, do the following:- Click **Add subnet** .\n- In the **New subnet** section, enter the following information:- **Name** :`second-subnet`\n- **Region** :`us-west1`\n- **IP address range** :`10.5.6.0/24`\n- Click **Add** .\n **Add the second forwarding rule** - In the Google Cloud console, go to the **Load balancing** page. [Go to Load balancing](https://console.cloud.google.com/networking/loadbalancing/list) \n- Click the `be-ilb` load balancer and click **Edit** .\n- Click **Frontend configuration** .\n- Click **Add frontend IP and port** .\n- In the **New Frontend IP and port** section, set the following fields and click **Done** :- **Name** :`fr-ilb-2`\n- **IP version** : **IPv4** \n- **Subnetwork** : **second-subnet** \n- **Internal IP** : **ip-ilb** \n- **Ports** :`80`and`443`\n- Verify that there is a blue check mark next to **Frontend configuration** before continuing.\n- Click **Review and finalize** , and review your load balancer configuration settings.\n- Click **Create** .\n- Create a second subnet in the `lb-network` network in the `us-west1` region:```\ngcloud compute networks subnets create second-subnet \\\n --network=lb-network \\\n --range=10.5.6.0/24 \\\n --region=us-west1\n```\n- Create a second forwarding rule for ports 80 and 443. The other parameters for this rule, including IP address and backend service, are the same as for the primary forwarding rule, `fr-ilb` .```\ngcloud compute forwarding-rules create fr-ilb-2 \\\n --region=us-west1 \\\n --load-balancing-scheme=internal \\\n --network=lb-network \\\n --subnet=second-subnet \\\n --address=10.5.6.99 \\\n --ip-protocol=TCP \\\n --ports=80,443 \\\n --backend-service=be-ilb \\\n --backend-service-region=us-west1\n```\nMake a `POST` requests to the [subnetworks.insert method](/compute/docs/reference/rest/v1/subnetworks/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks\n{\n \"name\": \"second-subnet\",\n \"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n \"ipCidrRange\": \"10.5.6.0/24\",\n \"privateIpGoogleAccess\": false\n}\n```\nCreate the forwarding rule by making a `POST` request to the [forwardingRules.insert method](/compute/docs/reference/rest/v1/forwardingRules/insert) .\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/forwardingRules\n{\n\"name\": \"fr-ilb-2\",\n\"IPAddress\": \"10.5.6.99\",\n\"IPProtocol\": \"TCP\",\n\"ports\": [ \"80\", \"443\"\n],\n\"loadBalancingScheme\": \"INTERNAL\",\n\"subnetwork\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/subnetworks/lb-subnet\",\n\"network\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/global/networks/lb-network\",\n\"backendService\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/backendServices/be-ilb\",\n\"networkTier\": \"PREMIUM\"\n}\n```\nConnect to the client VM instance and test HTTP and HTTPS connections to the IP addresses.\n- Connect to the client VM:```\ngcloud compute ssh vm-client --zone=us-west1-a\n```\n- Test HTTP connectivity to the IP addresses:```\ncurl http://10.1.2.99\ncurl http://10.5.6.99\n```\n- Test HTTPS connectivity. Use of `--insecure` is required because the Apache server configuration in the example setup uses self-signed certificates.* pragma: { seclinter_this_is_fine: true } *```\ncurl https://10.1.2.99 --insecure\ncurl https://10.5.6.99 --insecure\n```* pragma: { seclinter_this_is_fine: false } *\n- Observe that requests are handled by all of the backend VMs, regardless of the protocol (HTTP or HTTPS) or IP address used.\n### Use backend subsetting\nThe [example configuration](#configure_the_load_balancer) creates a backend service without subsetting.\nThis procedure shows you how to enable subsetting on the backend service for the example internal passthrough Network Load Balancer so that the deployment can scale to a larger number of backend instances.\n**Caution:** Enabling backend subsetting might be temporarily disruptive and might break existing TCP connections.\nYou should only enable subsetting if you need to support more than 250 backend VMs on a single load balancer.\n**Note:** This feature does not support IPv6 addresses.\nFor more information about this use case, see [backend subsetting](/load-balancing/docs/backend-service#tcp-subsetting) .\nUse the following `gcloud` command to update the `be-ilb` backend service, specifying subsetting policy:\n```\ngcloud compute backend-services update be-ilb \\\n --subsetting-policy=CONSISTENT_HASH_SUBSETTING\n```\nMake a `PATCH` request to the [regionBackendServices/patch method](/compute/docs/reference/rest/v1/regionBackendServices/patch) .\n```\nPATCH https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/us-west1/backendServices/be-ilb\n{\n\"subsetting\":\n {\n \"policy\": CONSISTENT_HASH_SUBSETTING\n }\n}\n```\n### Create a load balancer for Packet Mirroring\n[Packet Mirroring](/vpc/docs/packet-mirroring) lets you copy and collect packet data from specific instances in a VPC. The collected data can help you detect security threats and monitor application performance.\nPacket Mirroring requires an internal passthrough Network Load Balancer in order to balance traffic to an instance group of collector destinations. To create an internal passthrough Network Load Balancer for Packet Mirroring, follow these steps.\n- In the Google Cloud console, go to the **Load balancing** page. [Go to Load balancing](https://console.cloud.google.com/networking/loadbalancing/list) \n- Click **Create load balancer** .\n- On the **Network Load Balancer (TCP/SSL)** card, click **Start configuration** .\n- In the **Internet facing or internal only** section, select **Only between my VMs** , and then click **Continue** .\n- For **Load balancer name** , enter a name.\n- For **Region** , select the region of the VM instances where you want to mirror packets.\n- For **Network** , select the network where you want to mirror packets.\n- Click **Backend configuration** .\n- In the **New Backend** section, for **Instance group** , select the instance group to forward packets to.\n- From the **Health check** list, select **Create a health check** , enter the following information, and click **Save** :- For **Name** , enter a name for the health check.\n- For **Protocol** , select`HTTP`.\n- For **Port** , enter`80`.\n- Click **Frontend configuration** .\n- In the **New Frontend IP and port** section, do the following:- For **Name** , enter a name.\n- For **Subnetwork** , select a subnetwork in the same region as the instances to mirror.\n- For **Ports** , select **All** .\n- Click **Advanced configurations** and select the **Enable this load balancer for packet mirroring** checkbox.\n- Click **Done** .\n- Click **Create** .\n- Create a new regional HTTP health check to test HTTP connectivity to an instance group on port 80:```\ngcloud compute health-checks create http HEALTH_CHECK_NAME \\\n --region=REGION \\\n --port=80\n```Replace the following:- ``: the name of the health check.\n- ``: the region of the VM instances that you want to mirror packets for.\n- Create a backend service for HTTP traffic:```\ngcloud compute backend-services create COLLECTOR_BACKEND_SERVICE \\\n --region=REGION \\\n --health-checks-region=REGION \\\n --health-checks=HEALTH_CHECK_NAME \\\n --load-balancing-scheme=internal \\\n --protocol=tcp\n```Replace the following:- ``: the name of the backend service.\n- ``: the region of the VM instances where you want to mirror packets.\n- ``: the name of the health check.\n- Add an instance group to the backend service:```\ngcloud compute backend-services add-backend COLLECTOR_BACKEND_SERVICE \\\n --region=REGION \\\n --instance-group=INSTANCE_GROUP \\\n --instance-group-zone=ZONE\n```Replace the following:- ``: the name of the backend service.\n- ``: the region of the instance group.\n- ``: the name of the instance group.\n- ``: the zone of the instance group.\n- Create a forwarding rule for the backend service:```\ngcloud compute forwarding-rules create FORWARDING_RULE_NAME \\\n --region=REGION \\\n --network=NETWORK \\\n --subnet=SUBNET \\\n --backend-service=COLLECTOR_BACKEND_SERVICE \\\n --load-balancing-scheme=internal \\\n --ip-protocol=TCP \\\n --ports=all \\\n --is-mirroring-collector\n```Replace the following:- ``: the name of the forwarding rule.\n- ``: the region for the forwarding rule.\n- ``: the network for the forwarding rule.\n- ``: a subnetwork in the region of the VMs where you want to mirror packets.\n- ``: the backend service for this load balancer.**Note:** You cannot use the`L3_DEFAULT`protocol in your forwarding rule to configure packet mirroring.\n## What's next\n- See [Internal passthrough Network Load Balancer overview](/load-balancing/docs/internal) for important fundamentals.\n- See [Failover concepts for internal passthrough Network Load Balancers](/load-balancing/docs/internal/failover-overview) for important information about failover.\n- See [Internal load balancing and DNS names](/load-balancing/docs/dns-names) for available DNS name options that your load balancer can use.\n- See [Configuring failover for internal passthrough Network Load Balancers](/load-balancing/docs/internal/setting-up-failover) for configuration steps and an example internal passthrough Network Load Balancer failover configuration.\n- See [Internal passthrough Network Load Balancer logging andmonitoring](/load-balancing/docs/internal/internal-logging-monitoring) for information about configuring Logging and Monitoring for internal passthrough Network Load Balancers.\n- See [Internal passthrough Network Load Balancers and connectednetworks](/load-balancing/docs/internal/internal-tcp-udp-lb-and-other-networks) for information about accessing internal passthrough Network Load Balancers from peer networks connected to your VPC network.\n- See [Troubleshoot internal passthrough Network Load Balancers](/load-balancing/docs/internal/troubleshooting-ilb) for information about how to troubleshoot issues with your internal passthrough Network Load Balancer.\n- [Clean up the load balancer setup](/load-balancing/docs/cleaning-up-lb-setup) .", "guide": "Cloud Load Balancing"}