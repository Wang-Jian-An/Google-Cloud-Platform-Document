{"title": "Compute Engine - Use higher network bandwidth", "url": "https://cloud.google.com/compute/docs/instances", "abstract": "# Compute Engine - Use higher network bandwidth\nYou can use higher network bandwidths, of 100 Gbps or more, to improve the performance of distributed workloads running on your GPU VMs.\nHigher network bandwidths are supported on select VM configurations of N1 general-purpose VMs that have attached NVIDIA T4 or V100 GPUs, and on select accelerator-optimized machine types.\nTo review the configurations or machine types that support these higher network bandwidths rates, see [Network bandwidths and GPUs](/compute/docs/gpus/gpu-network-bandwidth) .\nFor general network bandwidth information on Compute Engine, see [Network bandwidth](/compute/docs/network-bandwidth) .\n", "content": "## Overview\nTo use the higher network bandwidths available to each GPU VM, complete the following recommended steps:\n- [Create your GPU VM](#create-high-bandwidth-vm) by using an OS image that supports Google Virtual NIC (gVNIC). For A3 VMs, it is recommended that you use a Container-Optimized OS image.\n- Optional: [Install Fast Socket](#manual-install-fast-socket) . Fast Socket improves NCCL performance on 100 Gbps or higher networks by reducing the contention between multiple TCP connections. Some Deep Learning VM Images (DLVM) have Fast Socket preinstalled.\n### Use Deep Learning VM Images\nYou can create your VMs using any GPU supported image from the Deep Learning VM Images project. All GPU supported DLVM images have the GPU driver, ML software, and gVNIC preinstalled. For a list of DLVM images, see [Choosing an image](/deep-learning-vm/docs/images) .\nIf you want to use Fast Socket, you can choose a DLVM image such as: `tf-latest-gpu-debian-10` or `tf-latest-gpu-ubuntu-1804` .\n**Caution:** You can't use [Deep Learning VM Images](/deep-learning-vm/docs/images) as boot disks for your VMs that use G2 machine types. G2 machine types are accelerator-optimized machine series that have NVIDIA L4 GPUs attached.\n## Create VMs that use higher network bandwidths\nFor higher network bandwidths, it is recommended that you enable Google Virtual NIC (gVNIC). For more information, see [Using Google Virtual NIC](/compute/docs/networking/using-gvnic) .\nFor A3 VMs, gVNIC version 1.4.0rc3 or later is required. This driver version is available on the Container-Optimized OS. For all other operating systems, you need to install [gVNIC version 1.4.0rc3 or later](https://github.com/GoogleCloudPlatform/compute-virtual-ethernet-linux) .\nTo create a VM that has attached GPUs and a higher network bandwidth complete the following:\n- Review the following:- For N1 GPU VMs with the attached T4 or V100 GPUs: review the maximum bandwidth for your CPU, GPU, and memory configuration\n- For A3, A2, G2 accelerator-optimized VMs: review the maximum bandwidth for your machine type\nFor more information, see [maximum bandwidth available](/compute/docs/gpus/gpu-network-bandwidth) .\n- Create your GPU VM. The following examples show how to create A3, A2, and N1 with attached V100 VMs.In these examples, VMs are created by using the Google Cloud CLI. However, you can also use either the [Google Cloud console](https://console.cloud.google.com/) or the [Compute Engine API](/compute/docs/reference/latest/instances) to create these VMs. For more information about creating GPU VMs, see [Create a VM with attached GPUs](/compute/docs/gpus/create-vm-with-gpus) .\nFor example, to create a VM that has a maximum bandwidth of 100 Gbps, has eight A100 GPUs attached, and uses the `tf-latest-gpu` DLVM image, run the following command:\n```\ngcloud compute instances create VM_NAME \\\n --project=PROJECT_ID \\\n --zone=ZONE \\\n --machine-type=a2-highgpu-8g \\\n --maintenance-policy=TERMINATE --restart-on-failure \\\n --image-family=tf-latest-gpu \\\n --image-project=deeplearning-platform-release \\\n --boot-disk-size=200GB \\\n --network-interface=nic-type=GVNIC \\\n --metadata=\"install-nvidia-driver=True,proxy-mode=project_editors\" \\\n --scopes=https://www.googleapis.com/auth/cloud-platform\n```\nReplace the following:- ``: the name of your VM\n- ``: your project ID\n- ``: the zone for the VM. This zone must support the specified GPU type. For more information about zones, see [GPU regions and zones availability](/compute/docs/gpus/gpu-regions-zones) .\nFor detailed instructions on how to set up A3 VMs that have increased network performance, see [Improve network performance with GPUDirect-TCPX](/compute/docs/gpus/gpudirect) .\nFor example, to create a VM that has a maximum bandwidth of 100 Gbps, has eight V100 GPUs attached, and uses the `tf-latest-gpu` DLVM image, run the following command:\n```\ngcloud compute instances create VM_NAME \\\n --project PROJECT_ID \\\n --custom-cpu 96 \\\n --custom-memory 624 \\\n --image-project=deeplearning-platform-release \\\n --image-family=tf-latest-gpu \\\n --accelerator type=nvidia-tesla-v100,count=8 \\\n --maintenance-policy TERMINATE \\\n --metadata=\"install-nvidia-driver=True\" \\\n --boot-disk-size 200GB \\\n --network-interface=nic-type=GVNIC \\\n --zone=ZONE\n```\n- If you are not using GPU supported Deep Learning VM Images or Container-Optimized OS, install GPU drivers. For more information, see [Installing GPU drivers](/compute/docs/gpus/install-drivers-gpu) .\n- Optional. On the VM, [Install Fast Socket](#manual-install-fast-socket) .\n- After you setup the VM, you can [verify the network bandwidth](#check-bandwidth) . **Note:** To achieve the higher network bandwidth rates, your applications must use multiple network streams.## Install Fast Socket\nNVIDIA Collective Communications Library (NCCL) is used by deep learning frameworks such as TensorFlow, PyTorch, Horovod for multi-GPU and multi-node training.\nFast Socket is a Google proprietary network transport for NCCL. On Compute Engine, Fast Socket improves NCCL performance on 100 Gbps networks by reducing the contention between multiple TCP connections. For more information about working with NCCL, see the [NCCL user guide](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/overview.html) .\nCurrent evaluation shows that Fast Socket improves all-reduce throughput by 30%\u201360%, depending on the message size.\nTo setup a Fast Socket environment, you can use either a Deep Learning VM Images that has Fast Socket preinstalled, or you can manually install Fast Socket on a Linux VM. To check if Fast Socket is preinstalled, see [Verifying that Fast Socket is enabled](#verify-fast-socket-install) .\n**Note:** Fast Socket is not supported on Windows VMs.\nBefore you install Fast Socket on a Linux VM, you need to install NCCL. For detailed instructions, see [NVIDIA NCCL documentation](https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html) .\nTo download and install Fast Socket on a CentOS or RHEL VM, complete the following steps:- Add the package repository and import public keys.```\nsudo tee /etc/yum.repos.d/google-fast-socket.repo << EOM\n[google-fast-socket]\nname=Fast Socket Transport for NCCL\nbaseurl=https://packages.cloud.google.com/yum/repos/google-fast-socket\nenabled=1\ngpgcheck=0\nrepo_gpgcheck=0\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg\n  https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nEOM\n```\n- Install Fast Socket.```\nsudo yum install google-fast-socket\n```\n- Verify that Fast Socket is [enabled](#verify-fast-socket-install) .\nTo download and install Fast Socket on an SLES VM, complete the following steps:- Add the package repository.```\nsudo zypper addrepo https://packages.cloud.google.com/yum/repos/google-fast-socket google-fast-socket\n```\n- Add repository keys.```\nsudo rpm --import https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\n```\n- Install Fast Socket.```\nsudo zypper install google-fast-socket\n```\n- Verify that Fast Socket is [enabled](#verify-fast-socket-install) .\nTo download and install Fast Socket on a Debian or Ubuntu VM, complete the following steps:- Add the package repository.```\necho \"deb https://packages.cloud.google.com/apt google-fast-socket main\" | sudo tee /etc/apt/sources.list.d/google-fast-socket.list\n```\n- Add repository keys.```\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add \n```\n- Install Fast Socket.```\nsudo apt update && sudo apt install google-fast-socket\n```\n- Verify that Fast Socket is [enabled](#verify-fast-socket-install) .\n### Verifying that Fast Socket is enabled\nOn your VM, complete the following steps:\n- Locate the NCCL home directory.```\nsudo ldconfig -p | grep nccl\n```For example, on a DLVM image, you get the following output:```\nlibnccl.so.2 (libc6,x86-64) => /usr/local/nccl2/lib/libnccl.so.2\nlibnccl.so (libc6,x86-64) => /usr/local/nccl2/lib/libnccl.so\nlibnccl-net.so (libc6,x86-64) => /usr/local/nccl2/lib/libnccl-net.so\n```This shows that the NCCL home directory is `/usr/local/nccl2` .\n- Check that NCCL loads the Fast Socket plugin. To check, you need to download the NCCL test package. To download the test package, run the following command:```\ngit clone https://github.com/NVIDIA/nccl-tests.git && \\\ncd nccl-tests && make NCCL_HOME=NCCL_HOME_DIRECTORY\n```Replace `` with the NCCL home directory.\n- From the `nccl-tests` directory, run the `all_reduce_perf` process:```\nNCCL_DEBUG=INFO build/all_reduce_perf\n```If Fast Socket is enabled, the `FastSocket plugin initialized` message displays in the output log.```\n# nThread 1 nGpus 1 minBytes 33554432 maxBytes 33554432 step: 1048576(bytes) warmup iters: 5 iters: 20 validation: 1\n#\n# Using devices\n# Rank 0 Pid 63324 on fast-socket-gpu device 0 [0x00] Tesla V100-SXM2-16GB\n.....\nfast-socket-gpu:63324:63324 [0] NCCL INFO NET/FastSocket : Flow placement enabled.\nfast-socket-gpu:63324:63324 [0] NCCL INFO NET/FastSocket : queue skip: 0\nfast-socket-gpu:63324:63324 [0] NCCL INFO NET/FastSocket : Using [0]ens12:10.240.0.24\nfast-socket-gpu:63324:63324 [0] NCCL INFO NET/FastSocket plugin initialized\n......\n```## Check network bandwidth\nWhen working with high bandwidth GPUs, you can use a network traffic tool, such as iperf2, to measure the networking bandwidth.\nTo check bandwidth speeds, you need at least two VMs that have attached GPUs and can both support the bandwidth speed that you are testing.\nUse iPerf to perform the benchmark on Debian-based systems.\n**Note:** Ensure that you are using iPerf version 2 and not version 3; iPerf version 3 does not support multi-threading (by design) and can have performance implications in your results when running multiple streams.\n- Create two VMs that can support the required bandwidth speeds.\n- Once both VMs are running, use SSH to connect to one of the VMs.```\ngcloud compute ssh VM_NAME \\\n --project=PROJECT_ID\n```Replace the following:- ``: the name of the first VM\n- ``: your project ID\n- On the first VM, complete the following steps:- Install `iperf` .```\nsudo apt-get update && sudo apt-get install iperf\n```\n- Get the internal IP address for this VM. Keep track of it by writing it down.```\nip a\n```\n- Start up the iPerf server.```\niperf -s\n```This starts up a server listening for connections in order to perform the benchmark. Leave this running for the duration of the test.\n- From a new client terminal, connect to the second VM using SSH.```\ngcloud compute ssh VM_NAME \\\n --project=PROJECT_ID\n```Replace the following:- ``: the name of the second VM\n- ``: your project ID\n- On the second VM, complete the following steps:- Install iPerf.```\nsudo apt-get update && sudo apt-get install iperf\n```\n- Run the iperf test and specify the first VM's IP address as the target. **Note:** The order of the arguments is important.```\niperf -t 30 -c internal_ip_of_instance_1 -P 16\n```This executes a 30-second test and produces a result that resembles the following output. If iPerf is not able to reach the other VM you, might need to adjust the network or [firewall settings](/vpc/docs/using-firewalls) on the VMs or perhaps in the Google Cloud console.When you use the maximum available bandwidth of 100 Gbps or 1000 Gbps (A3), keep the following considerations in mind:\n- Due to header overheads for protocols such as Ethernet, IP, and TCP on the virtualization stack, the throughput, as measured by `netperf` , saturates at around 90 Gbps or 800 Gbps (A3). Generally known as [goodput](https://wikipedia.org/wiki/Goodput) .TCP is able to achieve the 100 or 1000 Gbps network speed. Other protocols, such as UDP, are slower.\n- Due to factors such as protocol overhead and network congestion, end-to-end performance of data streams might be slightly lower.\n- You need to use multiple TCP streams to achieve maximum bandwidth between VM instances. Google recommends 4 to 16 streams. At 16 flows you'll frequently maximize the throughput. Depending on your application and software stack, you might need to adjust settings for your application or your code to set up multiple streams.## What's next?\n- To monitor GPU performance, see [Monitoring GPU performance](/compute/docs/gpus/monitor-gpus) .\n- To handle GPU host maintenance, see [Handling GPU host maintenance events](/compute/docs/gpus/gpu-host-maintenance) .", "guide": "Compute Engine"}