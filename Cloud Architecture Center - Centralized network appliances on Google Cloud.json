{"title": "Cloud Architecture Center - Centralized network appliances on Google Cloud", "url": "https://cloud.google.com/architecture/reference-patterns/overview", "abstract": "# Cloud Architecture Center - Centralized network appliances on Google Cloud\nThis document is intended for network administrators, solutions architects, and operations professionals who run centralized network appliances on Google Cloud. Knowledge of [Compute Engine](/compute) and [Virtual Private Cloud (VPC) networking](/vpc) in Google Cloud is required.\nCorporate environments often need to route traffic to the internet, to an on-premises network, to other clouds, or even to other parts of the same cloud environment through virtualized, VM-based appliances that monitor, transform, or block traffic.\nThis document reviews several design options that segment VPC networks and interconnect them with a centralized, virtualized network appliance. All communication between VPC networks, on-premises networks, and the internet is routed through the centralized device. You can deploy a group of redundant appliances to get a high availability configuration. If you don't require high availability, you can deploy a single network appliance.\nRouting traffic through virtualized appliances can be challenging. On Google Cloud, for example, you can replace the route pointing to the internet and redirect traffic to a set of virtualized appliances, but it's not possible to change the routing behavior between subnetworks within a VPC network. Routing between subnetworks is automatic and those routes can't be deleted or overridden.\nAlso, due to the crucial role of virtualized appliances, they must be deployed in highly available configurations. This is challenging because you need to ensure that all traffic gets routed through one of the virtual appliances and that failover between these appliances is automatic.\n", "content": "## Architecture\nThe following diagram shows a typical use case of several design options that feature a centralized, virtualized network appliance.\nThe preceding diagram shows the communication paths between segmented VPC networks, on-premises networks, and the internet, and how they are routed through the centralized, virtualized network appliance.\n## Main use cases for this architecture\nYou can use this architecture for multiple use cases that involve virtualized network appliances that traffic is routed through. Keep in mind the following considerations:\n- Many appliances from different vendors can be found in the [Google Cloud Marketplace](https://console.cloud.google.com/marketplace/) .\n- Some appliance vendors also offer customized [Compute Engine images](/compute/docs/images) for Google Cloud on their website or support pages.\n- You can create your own virtualized appliances by using open source networking software. You also can create your own images.\nVendors often provide their own reference architectures or deployment guides for running their virtualized appliances in a high availability configuration. For more information, see the vendor's website.\nReference architectures from the vendor might not encompass all the options presented in this document.\nIt's important to understand the advantages and disadvantages of each approach. Typical use cases for virtual appliances that traffic is routed through include the following:\n- [Next-generation firewalls (NGFWs)](https://wikipedia.org/wiki/Next-generation_firewall) . A centralized set of firewalls run as virtual machines that deliver features that aren't available if you use [VPC firewall rules](/vpc/docs/firewalls) . This is the most common use case. Typical features of NGFW products include [deep packet inspection (DPI)](https://en.wikipedia.org/wiki/Deep_packet_inspection) and firewalling on the application layer. Some NGFW firewalls also provide [TLS/SSL](https://wikipedia.org/wiki/Transport_Layer_Security) traffic inspection, as well as other networking functions described in the use cases in the following bulleted items.\n- [Intrusion detection system/intrusion prevention system (IDS/IPS)](https://wikipedia.org/wiki/Intrusion_detection_system) . A network-based IDS requires the visibility of all potentially malicious traffic. To prevent intrusions, traffic is usually blocked directly by an IPS in the traffic path.\n- [Transparent proxy](https://en.wikipedia.org/wiki/Proxy_server#Transparent_proxy) . A transparent proxy is often used to monitor HTTP(S) traffic and to enforce restrictions on web access.\n- [Network address translation (NAT) gateway](https://en.wikipedia.org/wiki/Network_address_translation) . A NAT gateway translates IP addresses and ports. This helps, for example, to avoid overlapping IP addresses. Google Cloud offers [Cloud NAT](/nat/docs/overview) as a managed service, but this service is available only for traffic going to the internet, not for on-premises traffic or for other VPC networks on Google Cloud.\n- [Web application firewall (WAF)](https://en.wikipedia.org/wiki/Web_application_firewall) . A WAF blocks malicious HTTP traffic going to a web application. Google Cloud offers WAF functionality through [Google Cloud Armor security policies](/armor/docs/configure-security-policies) . Exact functionality differs between WAF vendors, so it's important to determine exactly what you need.## Typical requirements\nRequirements for routing traffic through centralized virtual appliances differ based on the specific use case. However, the following requirements generally apply:\n- All traffic between different network segments\u2014for example, environments administered by different teams, with separate security requirements or between different modules of an application\u2014must pass through a centralized virtual appliance.\n- All traffic to or from the internet to secure environments must pass through a centralized virtual appliance.\n- All traffic to or from on-premises systems connected to Google Cloud through [Cloud VPN](/vpn/docs/concepts/overview) , [Dedicated Interconnect](/interconnect/docs/concepts/dedicated-overview) , or [Partner Interconnect](/interconnect/docs/concepts/partner-overview) must pass through a centralized virtual appliance.\n- Multiple centralized virtual appliances are set up in a high availability configuration in either an active/active or active/passive configuration. Failover occurs automatically if any software or hardware issues arise on one of the virtualized appliances.\n- All traffic is routed statefully, so that one traffic flow between two virtual machines is always passing through the same virtual appliance.\n- The throughput of the system of centralized virtual appliances is scalable by one of the two following options:- Scaling the virtual appliances vertically: increasing the number of cores or the amount of memory on each virtual machine.\n- Scaling the virtual appliances horizontally: increasing the number of virtual appliances used to distribute the traffic.\n## Architecture options\nThere are multiple options for achieving high availability between virtual appliances. There are also multiple options for attaching network segments to the virtual appliances.\nYou can combine any option for high availability with any option for attaching network segments. A common option described later in this document is [an architecture that uses VPC Network Peering and the internal passthrough Network Load Balancer as the next hop](#example_architecture_using_vpc_network_peering_and_internal_tcp_udp_load_balancer_as_next_hop) .\n### Choosing a high availability option\nYou can build an architecture to achieve high availability for the central appliance by using multiple instances in the following two ways:\n- **Use an internal passthrough Network Load Balancer as the next hop.** In this approach, you place multiple virtual appliances in a managed instance group behind an [internal passthrough Network Load Balancer](/load-balancing/docs/internal) . This load balancer is [used as the next hop](/load-balancing/docs/internal/ilb-next-hop-overview) for a default route that replaces the default route in the VPC networks connected to the appliances. You can use appliances either in an active/active configuration, which is the standard configuration, or in an active/passive configuration, by employing [failover for internal passthrough Network Load Balancers](/load-balancing/docs/internal/failover-overview) . Health checks direct traffic to healthy instances. If you want to re-create appliances automatically on failure, you can use [autohealing](/compute/docs/instance-groups#autohealing) . If your device uses multiple network interfaces, an internal passthrough Network Load Balancer as the next hop can have backends on any network interface.The following diagram shows the topology of using an internal passthrough Network Load Balancer as the next hop.The preceding diagram shows a managed instance group in a VPC network, including multiple virtual appliances that are behind an internal passthrough Network Load Balancer, which acts as the next hop.\n- **Use routing.** In this approach, Google Cloud [routes](/vpc/docs/routes) direct the traffic to the virtual appliances from the connected VPC networks. You can use appliances either in an active/passive configuration by using [different priority routes](/vpc/docs/routes#individualroutes) , or in an active/active configuration when routes are configured with the same priority, in which case you use [equal-cost multi-path (ECMP) routing](https://wikipedia.org/wiki/Equal-cost_multi-path_routing) to distribute traffic. You can use autohealing to help ensure that appliances restart when they're unhealthy.The following diagram shows the topology of using routing.The preceding diagram shows a managed instance group in a VPC network with Google Cloud routes directing the traffic to the virtual appliances from the connected VPC network.\nUsing an internal passthrough Network Load Balancer as the next hop has the following advantages over using Google Cloud routing for high availability:\n- Health checks decide where the traffic is forwarded, helping to ensure that traffic is forwarded only to healthy instances. You can expose health checks so that the local instance is verified or an end-to-end path is verified.\n- You can fine-tune the health check timers for faster failover. This approach provides the fastest failover times in case of unhealthy instances.\n- With [symmetric hashing](/load-balancing/docs/internal/ilb-next-hop-overview#symmetric-hashing) , you can ensure that return traffic is routed to the same virtual machine as the outgoing traffic.\nUsing Google Cloud routing has the following advantage:\n- Consistent, symmetric hashing ensures that packets for a given connection are processed by the same backend VM, for both requests and responses, depending on the protocol and session affinity selection. The protocol and session affinity selection determine whether or not connection tracking is used. Note that some use cases might benefit from having a source NAT (SNAT) configuration in the backend instances; for example, if a response connection from one network to another should match a separate request connection in the same direction between the same two networks.\nUsing Google Cloud routing has the following disadvantages:\n- Health checks delete and re-create unhealthy instances from the instance pool. However, traffic flow doesn't change immediately in response to failed health checks because it takes time to delete unhealthy instances and create new, healthy ones. This leads to slower failover times.\n- If you set health check timers to avoid unnecessary deletion and re-creation of instances, this will cause slower failover times.\nWhether you use an internal passthrough Network Load Balancer as the next hop or Google Cloud routing, all protocol traffic is supported. For detailed information about this support, see [Processing of TCP, UDP, and other protocoltraffic](/load-balancing/docs/internal/ilb-next-hop-overview#all-traffic) .\nSimiarly, both solutions support limiting routes to certain network tags. For example, VM instances can be segmented by region to use different sets of appliances.\n### Choosing an option for attaching network segments\nBecause routing between subnetworks can't be overridden, network segments are implemented by using separate VPC networks. Traffic between these VPC networks, to an on-premises environment, and to the internet, needs to be routed through the centralized appliances. Note that all of these network segments can be either standalone VPC networks or [Shared VPC networks](/vpc/docs/shared-vpc) .\nThere are several options for attaching network segments, as follows:\n- **Multiple network interfaces.** The simplest way to connect multiple VPC networks through a virtual appliance is by using [multiple network interfaces](/vpc/docs/create-use-multiple-interfaces) , with each interface connecting to one of the VPC networks. Internet and on-premises connectivity is provided over one or two separate network interfaces. With many NGFW products, internet connectivity is connected through an interface marked as untrusted in the NGFW software.The following diagram shows this topology.The preceding diagram shows multiple VPC networks connecting through a virtual appliance by using multiple network interfaces. Each interface connects to one of the VPC networks. The diagram also shows internet and on-premises connections over separate network interfaces, including an internet connection through an untrusted interface.\n- **VPC Network Peering.** This topology uses the hub-and-spoke concept in conjunction with [VPC Network Peering](/vpc/docs/vpc-peering) . The network segments are implemented as a set of spoke VPC networks that are peered by using VPC Network Peering with a hub VPC network, in which traffic is routed through the centralized pool of virtualized appliances. The default route or routes pointing to the internal passthrough Network Load Balancer as the next hop, or to the virtual appliances directly, are exported as [custom routes](/vpc/docs/vpc-peering#importing-exporting-routes) over the VPC Network Peering. Internet and on-premises connectivity is provided over one or two separate network interfaces.The following diagram shows this topology.The preceding diagram shows a centralized pool of virtualized appliances with multiple network interfaces attached to multiple hub VPC networks. Each hub VPC network is attached to multiple VPC networks through VPC Network Peering. Traffic between any two VPC networks is routed through the centralized pool of virtualized appliances.\n- **Combining multiple network interfaces and VPC Network Peering.** This approach combines the two preceding options for additional scalability. Multiple network interfaces are attached to multiple hub VPC networks, each of which is connected to multiple spoke VPC networks through VPC Network Peering. For each hub-and-spoke VPC network, you use the same approach as described in the VPC Network Peering case.The following diagram shows this topology.The preceding diagram shows a hub VPC network attached to multiple VPC networks through VPC Network Peering. Traffic is routed through the centralized pool of virtualized appliances with one network interface in the hub network.\nThe following decision tree can help you decide on the best approach for attaching network segments.\nUsing multiple network interfaces\u2014the first approach presented in the preceding cases\u2014is the easiest approach to implement.\nHowever, using multiple network interfaces has the following disadvantages:\n- You're limited to 8 network interfaces per virtual machine instance. If you use one network interface for internet connectivity and one for on-premises connectivity, you can connect only 6 network segments on Google Cloud. Some appliances require an additional management interface, limiting you to 5 network segments.\n- You can't add or remove network interfaces after an instance has been created.\nUsing VPC Network Peering has the following advantages:\n- You can scale up to the limit of [VPC Network Peering connections from a single VPC network](/vpc/docs/quota#vpc-peering) . Contact the Google Cloud sales team if you have questions about increasing this limit.\n- It's easy to add or remove VPC networks from this architecture by setting up VPC Network Peering.\nHowever, using VPC Network Peering has the following disadvantages:\n- This approach is slightly more complex than the approach that uses multiple network interfaces.\n- The number of VPCs that can be connected is still limited as compared to the combined approach.\nUsing the combined approach\u2014multiple network interfaces and VPC Network Peering\u2014has the following advantage:\n- It is the most scalable approach because the limit is a product of the limit of network interfaces and the limit of VPC peering connections. For example, you can connect 6 hub VPC networks to separate network interfaces, with each interface having 25 spoke VPC networks connected. This gives you a limit of 150 VPC networks exchanging traffic through one set of virtual appliances.\nHowever, this approach has the following disadvantage:\n- It is the most complex approach to implement.## Example architectures\nFollowing are two example architectures. The first example architecture demonstrates how to use the internal passthrough Network Load Balancer for high availability, combined with VPC Network Peering for attaching network segments. The second example architecture, a special use case, shows you how to route traffic from a single shared VPC network on Google Cloud towards an on-premises environment and to the internet.\n### Example architecture using VPC Network Peering and internal passthrough Network Load Balancer as next hop\nThis architecture is a typical use case for enterprise environments, using the internal passthrough Network Load Balancer for high availability, combined with VPC Network Peering for attaching network segments.\nThe following diagram shows the topology of this architecture.\nThe preceding diagram shows a hub VPC network and multiple spoke VPC networks that are peered with the hub VPC network by using VPC Network Peering. The hub VPC network has 2 instances of a virtual appliance in a managed instance group behind an internal passthrough Network Load Balancer. A static default route points to the internal passthrough Network Load Balancer as the next hop. This static default route is exported over the VPC Network Peering by using [custom routes](/vpc/docs/vpc-peering#importing-exporting-routes) . The default route to the internet gateway in the spoke VPC networks is deleted.\nYou can meet the requirements in the following ways:\n- Connectivity between spokes is automatically routed through the firewall because VPC Network Peering is non-transitive, and therefore, spoke VPC networks can't exchange data with each other directly. You can configure the virtual appliances to set the conditions and policies under which the spokes can exchange traffic.\n- Connectivity to the internet is possible only through the virtual appliances because the default route to the internet gateway has been removed from the spoke VPC networks. The virtual appliances have a default route through a separate network interface, which can be marked as untrusted in the case of an NGFW.\n- Connectivity to on-premises networks is achieved through a connectivity VPC network connected to the virtual appliance on a separate network interface. A Cloud VPN or Cloud Interconnect connection terminates in this connectivity VPC network.\n- High availability is achieved through customized [health checks](/load-balancing/docs/health-checks) on the internal load balancer. You can configure appliances as active/passive by using [failover for internal passthrough Network Load Balancers](/load-balancing/docs/internal/setting-up-failover) . This helps to ensure that traffic always flows through a single virtual appliance.\nYou can use this example architecture if communication between the different VPC networks is TCP/UDP [or other protocoltraffic](/load-balancing/docs/internal/ilb-next-hop-overview#all-traffic) . It's scalable up to the limit of VPC Network Peering connections per VPC network.\nFor a sample implementation of this architecture with a NAT gateway as the virtual appliance, see [Deploy a hub-and-spoke network by using a load balancer as the next hop](/load-balancing/docs/internal/deploying-ilb-next-hop-vm) . You can use the same pattern to deploy other virtual appliances, as described in the preceding [use cases](#main_use_cases_for_this_architecture) section.\n### Special use case with one shared VPC network on Google Cloud\nIn one special use, no traffic between different VPC networks needs to pass through the virtual appliances. Instead, only traffic from a single shared VPC network is routed towards an on-premises environment and to the internet. You can implement this use case by using any of the high availability options described earlier in this document, combined with one network interface each, for connectivity to the shared VPC network, on premises, and Google Cloud. If you want visibility into traffic between subnets without running it through centralized appliances, [Packet Mirroring](/vpc/docs/packet-mirroring) can help.\nThe following diagram shows this topology.\nThe preceding diagram shows traffic from a single shared VPC network routed towards an on-premises network and to the internet through a pool of virtual appliances.\n## Implementation of an architecture with virtual appliances\nFor information about using VPC Network Peering between segments and an internal passthrough Network Load Balancer as a high availability option, see [Deploy a hub-and-spoke network by using a load balancer as the next hop](/load-balancing/docs/internal/deploying-ilb-next-hop-vm) .\nIf you want to deploy an appliance from a Google Cloud partner from the [Cloud Marketplace](https://console.cloud.google.com/marketplace/) , contact your appliance vendor, or search the vendor's website for guidelines about how to deploy their appliances on Google Cloud.\n## What's next\n- [Best practices and reference architectures for VPC design](/architecture/best-practices-vpc-design) .\n- [Hybrid and multi-cloud network topologies](/architecture/hybrid-and-multi-cloud-network-topologies) .\n- [Best practices for network design in the Google Cloud Architecture Framework](/architecture/framework/system-design/networking) .\n- [VPC documentation](/vpc/docs) .\n- [Internal passthrough Network Load Balancer documentation](/load-balancing/docs/internal) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Cloud Architecture Center"}