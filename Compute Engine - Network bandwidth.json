{"title": "Compute Engine - Network bandwidth", "url": "https://cloud.google.com/compute/docs/instances", "abstract": "# Compute Engine - Network bandwidth\nGoogle Cloud accounts for bandwidth per virtual machine (VM) instance, not per virtual network interface (vNIC) or IP address. A VM's [machine type](/compute/docs/machine-resource) defines its maximum possible egress rate; however, you can only achieve that maximum possible egress rate in specific situations.\nThis page outlines expectations, which are useful when planning your deployments. It categorizes bandwidth using two dimensions:\n- **Egress or ingress** : As used on this page, egress and ingress are always from the perspective of a Google Cloud VM:- Packets senta Google Cloud VM compose its(outbound) traffic.\n- Packets senta Google Cloud VM compose its(inbound) traffic.\n- **How the packet is routed** : A packet can be routed from a sending VM or to a receiving VM using routes whose next hops area VPC network or routes outside of a VPC network.\nNeither [additional virtual network interfaces (vNICs)](/vpc/docs/multiple-interfaces-concepts) nor additional IP addresses per vNIC increase ingress or egress bandwidth for a VM. For example, a C3 VM with 22 vCPUs is limited to 23\u00a0Gbps total egress bandwidth. If you configure the C3 VM with two vNICs, the VM is still limited to 23\u00a0Gbps total egress bandwidth, not 23\u00a0Gbps bandwidth per vNIC.\nAll of the information on this page is applicable to Compute Engine VMs, as well as products that depend on Compute Engine VMs. For example, a Google Kubernetes Engine node is a Compute Engine VM.\n", "content": "## Bandwidth summary\nThe following table illustrates bandwidth expectations based on whether a packet is sent from (egress) or received by (ingress) a VM and the packet routing method.\n| Bandwidth expectations  | Bandwidth expectations.1                                                                                                                                                        |\n|:------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Routing within a VPC network | Primarily defined by a per-VM maximum egress bandwidth based on the sending VM's machine type and whether Tier_1 networking is enabled. N2, N2D, C2, and C2D VMs with Tier_1 networking support egress bandwidth limits up to 100\u00a0Gbps. H3 VMs support VM-to-VM egress bandwidth limits up to 200\u00a0Gbps. A2 and G2 VMs support egress bandwidth limits up to 100\u00a0Gbps. A3 VMs support egress bandwidth limits up to 1,000\u00a0Gbps. C3, C3D, and Z3 (Preview) VMs support up to 200\u00a0Gbps egress bandwidth limits with Tier_1 networking. For other factors, definitions, and scenarios, see Egress to destinations routable within a VPC network. |\n| Routing outside a VPC network | Primarily defined by a per-VM maximum egress bandwidth based on the sending VM's machine type and whether Tier_1 networking is enabled. Except for H3 VMs, a sending VM's maximum possible egress to a destination outside of its VPC network cannot exceed the following: 7\u00a0Gbps total when Tier_1 networking isn't enabled 25\u00a0Gbps total when Tier_1 networking is enabled 3\u00a0Gbps per flow For other factors, definitions, and caveats, see Egress to destinations outside of a VPC network.                                    |\n| Bandwidth expectations  | Bandwidth expectations.1                                                                                                                               |\n|:------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Routing within a VPC network | Generally, ingress rates are similar to the egress rates for a machine type. The size of your VM, the capacity of the server NIC, the traffic coming into other guest VMs running on the same host hardware, your guest OS network configuration, and the number of disk reads performed by your VM can all impact the ingress rate. Google Cloud doesn't impose any additional limitations on ingress rates within a VPC network. For other factors, definitions, and scenarios, see Ingress to destinations routable within a VPC network. |\n| Routing outside a VPC network | Google Cloud protects each VM by limiting ingress traffic routed outside a VPC network. The limit is the first of the following rates encountered: 1,800,000\u00a0pps (packets per second) 30\u00a0Gbps For machine series that support multiple physical NICs such as A3, the limit is the first of the following rates encountered: 1,800,000\u00a0pps (packets per second) per physical NIC 30\u00a0Gbps per physical NIC For other factors, definitions, and scenarios, see Ingress to to destinations outside of a VPC network.        |\n## Egress bandwidth\nGoogle Cloud limits outbound (egress) bandwidth using per-VM maximum egress rates based the machine type of the VM sending the packet and whether the packet's destination is accessible using routes within a VPC network or routes outside of a VPC network. Outbound bandwidth includes packets emitted by all of the VM's NICs and data transferred to all persistent disks connected to the VM.\n### Per-VM maximum egress bandwidth\nPer-VM maximum egress bandwidth is generally 2\u00a0Gbps per vCPU, but there are some differences and exceptions, depending on the machine series. The following table shows the range of maximum limits for egress bandwidth for traffic routed within a VPC network for standard networking tier only, not [per VM Tier_1 networking performance](/compute/docs/networking/configure-vm-with-high-bandwidth-configuration#bandwidth-tiers) .\n| Machine series          | Lowest per-VM maximum egress limit without Tier_1 networking | Highest per-VM maximum egress limit without Tier_1 networking       |\n|:-----------------------------------------------------|:---------------------------------------------------------------|:----------------------------------------------------------------------------------------|\n| E2             | 1\u00a0Gbps               | 16\u00a0Gbps                     |\n| C3             | 23\u00a0Gbps              | 100\u00a0Gbps                    |\n| C3D             | 20\u00a0Gbps              | 100\u00a0Gbps                    |\n| T2D             | 10\u00a0Gbps              | 32\u00a0Gbps                     |\n| N2,C2, N2D, and C2D         | 10\u00a0Gbps              | 32\u00a0Gbps                     |\n| H3             | nan               | 200\u00a0Gbps                    |\n| Z3 (Preview)           | 23\u00a0Gbps              | 100\u00a0Gbps                    |\n| N1 (excluding VMs with 1 vCPU)      | 10\u00a0Gbps              | 32\u00a0Gbps on Intel Skylake CPU platform 16\u00a0Gbps on CPU platforms older than Intel Skylake |\n| N1 machine types with 1 vCPU, f1-micro, and g1-small | 2\u00a0Gbps               | 2\u00a0Gbps                     |\n| A3, A2, and G2          | nan               | Based on GPU type                  |\nYou can find the per-VM maximum egress bandwidth for every machine type listed on its specific machine family page:\n- [General-purpose machine family](/compute/docs/general-purpose-machines) \n- [Compute-optimized machine family](/compute/docs/compute-optimized-machines) \n- [Memory-optimized machine family](/compute/docs/memory-optimized-machines) \n- [Accelerator-optimized machine family](/compute/docs/accelerator-optimized-machines) \nPer-VM maximum egress bandwidth is not a guarantee. Actual egress bandwidth can be lowered according to factors such as the following non-exhaustive list:\n- Guest Ethernet driver\u2014 [gVNIC](/compute/docs/networking/using-gvnic) offers better performance than the VirtIO network interface\n- Packet size\n- Protocol overhead\n- The number of flows\n- Ethernet driver settings of the VM's guest OS, such as checksum offload and TCP segmentation offload (TSO)\n- Network congestion\n- In a situation where persistent disks compete with other network egress traffic, 60% of the maximum network bandwidth is given to Persistent Disk writes, leaving 40% for other network egress traffic. See [Factors that affect disk performance](/compute/docs/disks/optimizing-pd-performance#performance_factors) in the Persistent Disk documentation for more details.\nTo get the largest possible per-VM maximum egress bandwidth:\n- Enable [per VM Tier_1 networking performance](/compute/docs/networking/configure-vm-with-high-bandwidth-configuration#bandwidth-tiers) with larger general-purpose and compute-optimized machine types.\n- Use the largest VPC network [maximum transmission unit (MTU)](/vpc/docs/mtu) supported by your network topology. Larger MTUs can reduce packet-header overhead and increase payload data throughput.\n### Egress to destinations routable within a VPC network\nFrom the perspective of a sending VM and for destination IP addresses accessible by means of routes , Google Cloud limits outbound traffic using these rules:\n- **Per-VM maximum egress bandwidth:** The per-VM maximum egress bandwidth described in the [Per-VM maximum egress bandwidth](#vm-out-baseline) section.\n- **Per-project inter-regional egress bandwidth:** If a sending VM and an internal destination or its next hop are in different regions, Google Cloud enforces a maximum inter-regional egress bandwidth limit. Most customers are unlikely to reach this limit. For questions about this limit, [file a support case](/support-hub) .\n- **Cloud VPN and Cloud Interconnect limits:** When sending traffic from a VM to an internal IP address destination routable by a next hop Cloud VPN tunnel or Cloud Interconnect VLAN attachment, egress bandwidth is limited by:- [Maximum packet rate and bandwidth per Cloud VPN tunnel](/network-connectivity/docs/vpn/quotas#limits) \n- [Maximum packet rate and bandwidth per VLAN attachment](/network-connectivity/docs/interconnect/quotas#limits) \n- To fully use the bandwidth of multiple next hop Cloud VPN tunnels or Cloud Interconnect VLAN attachments using ECMP routing, you must use multiple TCP connections (unique 5-tuples).Destinations routable within a VPC network include all of the following destinations, each of which is accessible from the perspective of the sending VM by a route whose next hop is the default internet gateway:\n- Regional internal IPv4 addresses in [subnet primary IPv4 and subnet secondary IPv4 address ranges](/vpc/docs/subnets#ipv4-ranges) , including private IPv4 address ranges and privately used public IPv4 address ranges, used by these destination resources:- The primary internal IPv4 address of a receiving VM's network interface (vNIC). (When a sending VM connects to another VM's vNICIPv4 address, packets are routed using a next hop default internet gateway, so [Egress to destinations outside of a VPC network](#vm-out-dest-e) applies instead.)\n- An internal IPv4 address in an alias IP range of a receiving VM's vNIC.\n- An internal IPv4 address of an internal forwarding rule for either protocol forwarding or for an internal passthrough Network Load Balancer.\n- Global internal IPv4 addresses for these destination resources:- [Private Service Connect endpoints for Google Cloud APIs](/vpc/docs/configure-private-service-connect-apis) \n- [Allocated ranges for private services access](/vpc/docs/private-services-access) \n- [Internal IPv6 subnet address ranges](/vpc/docs/subnets#internal-ipv6) used by these destination resources:- An IPv6 address from the`/96`IPv6 address range assigned to a dual-stack receiving VM's vNIC.\n- An IPv6 address from the`/96`IPv6 address range of an internal forwarding rule for either protocol forwarding or for an internal passthrough Network Load Balancer.\n- [External IPv6 subnet address ranges](/vpc/docs/subnets#external-ipv6) used by these destination resources:- An IPv6 address from the`/96`IPv6 address range assigned to a dual-stack receiving VM's vNIC.\n- An IPv6 address from the`/96`IPv6 address range of an external forwarding rule for either protocol forwarding or for an external passthrough Network Load Balancer.\n- Other destinations accessible using the following VPC network routes:- [Dynamic routes](/vpc/docs/routes#dynamic_routes) \n- [Static routes](/vpc/docs/routes#static_routes) those that use a default internet gateway next hop\n- [Peering custom routes](/vpc/docs/routes#peering-custom-routes) The following list ranks traffic from sending VMs to internal destinations, from highest possible bandwidth to lowest:\n- Between VMs in the same zone\n- Between VMs in different zones of the same region\n- Between VMs in different regions\n- From a VM to Google Cloud APIs and services using [Private Google Access](/vpc/docs/private-google-access) or [accessing Google APIs from a VM's external IP address](/vpc/docs/access-apis-external-ip) . This includes [Private Service Connect endpoints for Google APIs](/vpc/docs/about-accessing-google-apis-endpoints) .\n### Egress to destinations outside of a VPC network\nFrom the perspective of a sending VM and for destination IP addresses , Google Cloud limits outbound traffic to whichever of the following rates is reached first:\n- **Per-VM egress bandwidth:** The maximum bandwidth for all connections from a VM to destinations outside of a VPC network is the smaller of the [Per-VM maximum egress bandwidth](#vm-out-baseline) and one of these rates:- 25\u00a0Gbps, if Tier_1 networking is enabled\n- 7\u00a0Gbps, if Tier_1 networking is not enabled\n- 1\u00a0Gbps for H3 VMs\n- 7\u00a0Gbps per physical NIC for machine series that support multiple physical NICs such as A3.\nAs an example, even though an `n2-standard-16` instance has a per-VM egress bandwidth of 32\u00a0Gbps, the per-VM egress bandwidth from an `n2-standard-16` instance to external destinations is either 25\u00a0Gbps or 7\u00a0Gbps, depending on whether Tier_1 networking is enabled.\n- **Per-flow maximum egress rate:** The maximum bandwidth for unique 5-tuple connection, from a VM to a destination outside of a VPC network is 3\u00a0Gbps, except on H3, where it is 1\u00a0Gbps.\n- **Per-project internet egress bandwidth:** The maximum bandwidth for all connections from VMs in region of a project to destinations outside of a VPC network is defined by the project's [Internet egress bandwidth](https://console.cloud.google.com/iam-admin/quotas?service=compute.googleapis.com&metric=%22GCE%20VM%20to%20Internet%20egress%20bandwidth%20Mbps%22) quotas.\nDestinations outside of a VPC network include all of the following destinations, each of which is accessible by a route in the sending VM's VPC network whose next hop the default internet gateway:\n- Global external IPv4 and IPv6 addresses for external proxy Network Load Balancers and external Application Load Balancers\n- Regional external IPv4 addresses for Google Cloud resources, including VM vNIC external IPv4 addresses, external IPv4 addresses for external protocol forwarding, external passthrough Network Load Balancers, and response packets to Cloud NAT gateways.\n- Regional external IPv6 addresses in dual-stack subnets with [external IPv6 address ranges](/vpc/docs/subnets#external-ipv6) used by dual-stack VM external IPv6 addresses, external protocol forwarding, and external passthrough Network Load Balancers, provided that the subnet is located in a separate, non-peered VPC network and the destination IPv6 address range is accessible using a route in the sending VM's VPC network whose next hopthe default internet gateway. If a dual-stack subnet with an external IPv6 address range is located in the same VPC network or in a peered VPC network, see [Egress to destinations routable within a VPC network](#vm-out-dest-i) instead.\n- Other external destinations accessible using a static route in the sending VM's VPC network provided that the next hop for the routethe default internet gateway.\nFor details about which Google Cloud resources use what types of external IP addresses, see [External IP addresses](/vpc/docs/ip-addresses#external) .\n## Ingress bandwidth\nGoogle Cloud handles inbound (ingress) bandwidth depending on how the incoming packet is routed to a receiving VM.\n### Ingress to destinations routable within a VPC network\nA receiving VM can handle as many incoming packets as its machine type, operating system, and other network conditions permit. Google Cloud does not implement any purposeful bandwidth restriction on incoming packets delivered to a VM if the incoming packet is delivered using routes a VPC network:\n- Subnet routes in the receiving VM's VPC network\n- Peering subnet routes in a peered VPC network\n- Routes in another network whose next hops are Cloud VPN tunnels, Cloud Interconnect (VLAN) attachments, or Router appliance VMs located in the receiving VM's VPC network\nDestinations for packets that are routed within a VPC network include:\n- The primary internal IPv4 address of the receiving VM's network interface (vNIC). Primary internal IPv4 addresses are regional internal IPv4 addresses that come from a [subnet's primary IPv4 address range](/vpc/docs/subnets#ipv4-ranges) .\n- An internal IPv4 address from an alias IP range of the receiving VM's vNIC. Alias IP ranges can come from either a subnet's primary IPv4 address range or one of its secondary IPv4 address ranges.\n- An IPv6 address from the`/96`IPv6 address range assigned to a dual-stack receiving VM's vNIC. VM IPv6 ranges can come from these subnet IPv6 ranges:- An [internal IPv6 address range](/vpc/docs/subnets#internal-ipv6) .\n- An [external IPv6 address range](/vpc/docs/subnets#external-ipv6) .\n- An internal IPv4 address of a forwarding rule used by internal protocol forwarding to the receiving VM or internal passthrough Network Load Balancer where the receiving VM is a backend of the load balancer. Internal forwarding rule IPv4 addresses come from a subnet's primary IPv4 address range.\n- An internal IPv6 address from the`/96`IPv6 range of a forwarding rule used by internal protocol forwarding to the receiving VM or internal passthrough Network Load Balancer where the receiving VM is a backend of the load balancer. Internal forwarding rule IPv6 addresses come from a subnet's internal IPv6 address range.\n- An external IPv6 address from the`/96`IPv6 range of a forwarding rule used by external protocol forwarding to the receiving VM or external passthrough Network Load Balancer where the receiving VM is a backend of the load balancer. External forwarding rule IPv6 addresses come from a subnet's external IPv6 address range.\n- An IP address within the destination range of a custom static route that uses the receiving VM as a next hop VM (`next-hop-instance`or`next-hop-address`).\n- An IP address within the destination range of a custom static route using an internal passthrough Network Load Balancer (`next-hop-ilb`) next hop, if the receiving VM is a backend for that load balancer.\n### Ingress to destinations outside of a VPC network\nGoogle Cloud implements the following bandwidth limits for incoming packets delivered to a receiving VM using routes a VPC network. When load balancing is involved, the bandwidth limits are applied individually to each receiving VM.\nFor machine series that don't support multiple physical NICs, the applicable inbound bandwidth restriction applies collectively to all virtual NICs. The limit is the first of the following rates encountered:\n- 1,800,000\u00a0packets per second\n- 30\u00a0Gbps\nFor machine series that do support multiple physical NICs, such as A3, the applicable inbound bandwidth restriction applies individually to each physical NICs. The limit is the first of the following rates encountered:\n- 1,800,000\u00a0packets per second per physical NIC\n- 30\u00a0Gbps per physical NIC\n**Note:** Depending on the machine type of your VM and other factors, actual inbound traffic might be less than 30\u00a0Gbps or 1,800,000 packets per second. Bandwidth from the internet is not covered by any SLA and is subject to network conditions.\nDestinations for packets that are routed using routes outside of a VPC network include:\n- An external IPv4 address assigned in a one-to-one NAT access configuration on one of the receiving VM's network interfaces (NICs).\n- An external IPv6 address from the`/96`IPv6 address range assigned to a vNIC of a dual-stack receiving VM\n- An external IPv4 address of a forwarding rule used by external protocol forwarding to the receiving VM or external passthrough Network Load Balancer where the receiving VM is a backend of the load balancer.\n- An external IPv6 address from the`/96`IPv6 range of a forwarding rule used by external protocol forwarding to the receiving VM or external passthrough Network Load Balancer where the receiving VM is a backend of the load balancer\n- Established inbound responses processed by Cloud NAT## Jumbo frames\nTo receive and send [jumbo frames](https://en.wikipedia.org/wiki/Jumbo_frame) , configure the VPC network used by your VMs; set the [maximum transmission unit (MTU)](/vpc/docs/mtu) to a larger value, up to 8896.\nHigher MTU values increase the packet size and reduce the packet-header overhead, which increases payload data throughput.\nTo use jumbo frames with the gVNIC driver, you must use version 1.3 or later of the driver. Not all Google Cloud public images include this driver version. For more information about operating system support for jumbo frames, see the **Networking features** tab on the [Operating system details](/compute/docs/images/os-details#network-features) page. The image versions that indicate full support for jumbo frames include the updated gVNIC driver, even if the guest OS shows the gve driver version as `1.0.0` .\nIf you are using an OS image that doesn't have full support for jumbo frames, you can manually install gVNIC driver version v1.3.0 or later. Google recommends installing the gVNIC driver version marked `Latest` to benefit from additional features and bug fixes. You can download the gVNIC drivers from [GitHub](https://github.com/GoogleCloudPlatform/compute-virtual-ethernet-linux/releases) .\nTo manually update the gVNIC driver version in your guest OS, see [Use on non-supported operating systems](/compute/docs/networking/using-gvnic#manual-gvnic-setup) .\n## Receive and transmit queues\nEach VM vNIC is assigned a number of receive and transmit queues for processing packets from the network.\n- Receive Queue (RX): Queue to receive packets. When the vNIC receives a packet from the network, the vNIC selects the descriptor for an incoming packet from the queue, processes it and hands the packet to the guest OS over a packet queue attached to a vCPU core using an interrupt. If the RX queue is full and there is no buffer available to place a packet, then the packet is dropped. This can typically happen if an application is over-utilizing a vCPU core that is also attached to the selected packet queue.\n- Transmit Queue (TX): Queue to transmit packets. When the guest OS sends a packet, a descriptor is allocated and placed in the TX queue. The vNIC then processes the descriptor and transmits the packet.\n### Default queue allocation\nUnless you explicitly [assign queue counts for NICs](#assign-rx-tx-queues) , you can model the algorithm Google Cloud uses to assign a fixed number of RX and TX queues per vNIC in this way:\n- Use one of the following methods, depending on your network interface type:- VirtIO: Divide the number of vCPUs by the number of NICs, and discard any remainder \u2014`[number of vCPUs/number of NICs]`.\n- gVNIC: Divide the number of vCPUs by the number of NICs, and then divide the result by 2 and discard any remainder \u2014`[number of vCPUs/number of NICs/2]`.\nThis calculation always results in a whole number (not a fraction).\n- If the calculated number is less than 1, assign each vNIC one queue instead.\n- Determine if the calculated number is greater than the maximum number of queues per vNIC. The maximum number of queues per vNIC depends on the driver type:- Using [virtIO](https://ozlabs.org/%7Erusty/virtio-spec/virtio-0.9.5.pdf) or a custom driver, the maximum number of queues per vNIC is`32`. If the calculated number is greater than`32`, ignore the calculated number, and assign each vNIC 32 queues instead.\n- Using [gVNIC](/compute/docs/networking/using-gvnic) , the maximum number of queues per vNIC is`16`. If the calculated number is greater than`16`, ignore the calculated number, and assign each vNIC 16 queues instead.The following examples show how to calculate the default number of queues:\n- If a VM uses VirtIO and has 16 vCPUs and 4 NICs, the calculated number is `[16/4] = 4` . Google Cloud assigns each vNIC four queues.\n- If a VM using gVNIC and has 128 vCPUs and two NICs, the calculated number is `[128/2/2] = 32` . Google Cloud assigns each vNIC the maximum number of queues per vNIC possible. Google Cloud assigns `16` queues per vNIC.\nOn Linux systems, you can use `ethtool` to configure a vNIC with fewer queues than the number of queues Google Cloud assigns per vNIC.\n### Custom queue allocation\nInstead of the [Default queue allocation](#rx-tx-equations) , you can assign a custom queue count (total of both RX and TX) to each vNIC when you create a new VM by using the Compute Engine API.\nThe number of custom queues you specify must adhere to the following rules:\n- The minimum queue count you can assign per vNIC is one.\n- The maximum queue count you can assign to each vNIC is the lower of the vCPU count or the per vNIC maximum queue count, based on the driver type:- Using [virtIO](https://ozlabs.org/%7Erusty/virtio-spec/virtio-0.9.5.pdf) or a custom driver, the maximum queue count is`32`.\n- Using [gVNIC](/compute/docs/networking/using-gvnic) , the maximum queue count is`16`.\n- If you assign custom queue counts to NICs of the VM, the sum of your queue count assignments must be less than or equal to the number of vCPUs assigned to the VM instance.\nYou can oversubscribe the custom queue count for your NICs. In other words, you can have a sum of the queue counts assigned to all NICs for your VM that is greater than the number of vCPUs for your VM. To oversubscribe the custom queue count, you must satisfy the following conditions:\n- You use gVNIC as the vNIC type for all NICs configured for the VM.\n- Your VM uses a machine type from the N2, N2D, C2 and C2D machine series.\n- You enabled Tier_1 networking for the VM.\n- You specified a custom queue count for all NICs configured for the VM.\nWith queue oversubscription, the maximum queue count for the VM is 16 times the number of NICs. So, if you have 6 NICs configured for a VM with 30 vCPUs, you can configure a maximum of (16 * 6), or 96 custom queues for your VM.\n**Examples**\n- If a VM has 8 vCPUs and 3 NICs, the maximum queue count for the VM is the number of vCPUS, or 8. You can assign 1 queue to `nic0` , 4 queues to `nic1` , and 3 queues to `nic2` . In this example, you cannot subsequently assign 4 queues to `nic2` while keeping the other two vNIC queue assignments because the sum of assigned queues cannot exceed the number of vCPUs (8).\n- If a VM has 96 vCPUs and 2 NICs, you can assign both NICs up to 32 queues each when using the virtIO driver, or up to 16 queues each when using the gVNIC driver. In this example, the sum of assigned queues is always less than the number of vCPUs.\nIt's also possible to assign a custom queue count for , letting Google Cloud assign queues to the remaining NICs. The number of queues you can assign per vNIC is still subject to rules mentioned previously. You can model the feasibility of your configuration, and, if your configuration is possible, the number of queues that Google Cloud assigns to the remaining NICs with this process:\n- Calculate the sum of queues for the NICs using custom queue assignment. For an example VM with 20 vCPUs and 6 NICs, suppose you assign `nic0` 5 queues, `nic1` 6 queues, `nic2` 4 queues, and let Google Cloud assign queues for `nic3` , `nic4` , and `nic5` . In this example, the sum of custom-assigned queues is `5+6+4 = 15` .\n- Subtract the sum of custom-assigned queues from the number of vCPUs. If the difference is not at least equal to the number of remaining NICs for which Google Cloud must assign queues, Google Cloud returns an error. Continuing with the example VM of 20 vCPUs and a sum of `15` custom-assigned queues, Google Cloud has `20-15 = 5` queues left to assign to the remaining NICs ( `nic3` , `nic4` , `nic5` ).\n- Divide the difference from the previous step by the number of remaining NICs and discard any remainder \u2014 `\u230a(number of vCPUs - sum of assigned queues)/(number of remaining NICs)\u230b` . This calculation always results in a whole number (not a fraction) that is at least equal to one because of the constraint explained in the previous step. Google Cloud assigns each remaining NICs a queue count matching the calculated number as long as the calculated number is not greater than the maximum number of queues per vNIC. The maximum number of queues per vNIC depends on the driver type:- Using virtIO or a custom driver, if the calculated number of queues for each remaining vNIC is greater than`32`, Google Cloud assigns each remaining vNIC`32`queues.\n- Using gVNIC, if the calculated number of queues for each remaining vNIC is greater than`16`, Google Cloud assigns each remaining vNIC`16`queues.\n### Configure custom queue counts\nTo create a VM that uses a custom queue count for one or more vNICs, complete the following steps.\n- If you don't already have a [VPC network](/vpc/docs/create-modify-vpc-networks#create-custom-network) with a subnet for each vNIC interface you plan to configure, create them.\n- Use the [gcloud compute instances create command](/sdk/gcloud/reference/compute/instances/create) to create the VM. Repeat the`--network-interface`flag for each vNIC you want to configure for the VM, and include the`queue-count`option.\n```\n gcloud compute instances create VM_NAME \\\n  --zone=ZONE \\\n  --machine-type=MACHINE_TYPE \\\n  --network-performance-configs=total-egress-bandwidth-tier=TIER_1 \\\n  --network-interface=network=NETWORK_NAME_1,subnet=SUBNET_1,nic-type=GVNIC,queue-count=QUEUE_SIZE_1 \\\n  --network-interface=network=NETWORK_NAME_2,subnet=SUBNET_2,nic-type=GVNIC,queue-count=QUEUE_SIZE_2\n```\nReplace the following:- ``: a [name](/compute/docs/naming-resources#resource-name-format) for the new VM\n- ``: the zone to create the VM in\n- ``: the [machine type](/compute/docs/machine-resource#vm_terminology) of the VM. The machine type you specify must support gVNIC and Tier_1 networking.\n- ``: the name of the network created previously\n- ``: the name of one of the subnets created previously\n- ``: the number of queues for the vNIC, subject to the rules discussed in [Custom queue allocation](#assign-rx-tx-queues) .\n- If you don't already have a [VPC network](/vpc/docs/create-modify-vpc-networks#create-custom-network) with a subnet for each vNIC interface you plan to configure, create them.\n- Create a VM with specific queue counts for vNICs using the [google_compute_instance resource](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_instance) . Repeat the `--network-interface` parameter for each vNIC you want to configure for the VM, and include the `queue-count` parameter.```\n# Queue oversubscription instance\nresource \"google_compute_instance\" \"VM_NAME\" {\nproject  = \"PROJECT_ID\"\nboot_disk {\n auto_delete = true\n device_name = \"DEVICE_NAME\"\n initialize_params {\n  image=\"IMAGE_NAME\"\n  size = DISK_SIZE\n  type = \"DISK_TYPE\"\n }\n}\nmachine_type = \"MACHINE_TYPE\"\nname   = \"VM_NAME\"\nzone = \"ZONE\"\nnetwork_performance_config {\n total_egress_bandwidth_tier = \"TIER_1\"\n}\nnetwork_interface {\n nic_type = \"GVNIC\"\n queue_count = QUEUE_COUNT_1\n subnetwork_project = \"PROJECT_ID\"\n subnetwork = \"SUBNET_1\"\n }\nnetwork_interface {\n nic_type = \"GVNIC\"\n queue_count = QUEUE_COUNT_2\n subnetwork_project = \"PROJECT_ID\"\n subnetwork = \"SUBNET_2\"\n}\nnetwork_interface {\n nic_type = \"GVNIC\"\n queue_count = QUEUE_COUNT_3\n subnetwork_project = \"PROJECT_ID\"\n subnetwork = \"SUBNET_3\"\"\n}\nnetwork_interface {\n nic_type = \"GVNIC\"\n queue_count = QUEUE_COUNT_4\n subnetwork_project = \"PROJECT_ID\"\n subnetwork = \"SUBNET_4\"\"\n}\n}\n```\nReplace the following:- ``: a [name](/compute/docs/naming-resources#resource-name-format) for the new VM\n- ``: ID of the project to create the VM in. Unless you are using a Shared VPC network, the project you specify must be the same one in which all the subnets and networks were created in.\n- ``: The name to associate with the boot disk in the guest OS\n- ``: the name of an image,for example,`\"projects/debian-cloud/global/images/debian-11-bullseye-v20231010\"`.\n- ``: the size of the boot disk, in GiB\n- ``: the type of disk to use for the boot disk, for example,`pd-standard`\n- ``: the [machine type](/compute/docs/machine-resource#vm_terminology) of the VM. The machine type you specify must support gVNIC and Tier_1 networking.\n- ``: the zone to create the VM in\n- ``: the number of queues for the vNIC, subject to the rules discussed in [Custom queue allocation](#assign-rx-tx-queues) .\n- ``: the name of the subnet that the network interface connects to\n- If you don't already have a [VPC network](/vpc/docs/create-modify-vpc-networks#create-custom-network) with a subnet for each vNIC interface you plan to configure, create them.\n- Create a VM with specific queue counts for NICs using the [instances.insert method](/compute/docs/reference/rest/v1/instances/insert) . Repeat the `networkInterfaces` property to configure multiple network interfaces.```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/zones/ZONE/instances\n{\n\"name\": \"VM_NAME\",\n\"machineType\": \"machineTypes/MACHINE_TYPE\",\n\"networkPerformanceConfig\": {\n \"totalEgressBandwidthTier\": TIER_1\n},\n\"networkInterfaces\": [ {\n  \"nicType\": gVNIC,\n  \"subnetwork\":\"regions/region/subnetworks/SUBNET_1\",\n  \"queueCount\": \"QUEUE_COUNT_1\"\n } ],\n\"networkInterfaces\": [ {\n  \"nicType\": gVNIC,\n  \"subnetwork\":\"regions/region/subnetworks/SUBNET_2\",\n  \"queueCount\": \"QUEUE_COUNT_2\"\n } ],\n}\n```Replace the following:- ``: ID of the project to create the VM in\n- ``: zone to create the VM in\n- ``: [name](/compute/docs/naming-resources#resource-name-format) of the new VM\n- ``: machine type, [predefined](/compute/docs/machine-resource) or [custom](/compute/docs/instances/creating-instance-with-custom-machine-type) , for the new VM\n- ``: the name of the subnet that the network interface connects to\n- ``: Number of queues for the vNIC, subject to the rules discussed in [Custom queue allocation](#assign-rx-tx-queues) .\n### Queue allocations and changing the machine type\nVMs are created with a [default queue allocation](/compute/docs/network-bandwidth#rx-tx-equations) , or you can assign a [custom queue count](/compute/docs/network-bandwidth#assign-rx-tx-queues) to each virtual network interface card (vNIC) when you create a new VM by using the Compute Engine API. The default or custom vNIC queue assignments are only set when creating a VM. If your VM has vNICs which use default queue counts, you can [change its machine type](/compute/docs/instances/changing-machine-type-of-stopped-instance#changing_a_machine_type) . If the machine type that you are changing to has a different number of vCPUs, the default queue counts for your VM are recalculated based on the new machine type.\nIf your VM has vNICs which use custom, non-default queue counts, you can change the machine type by using the Google Cloud CLI or Compute Engine API to [update the instance properties](/compute/docs/instances/update-instance-properties) . The conversion succeeds if the resulting VM supports the same queue count per vNIC as the original VM. For VMs that use the VirtIO-Net interface and have a custom queue count that is higher than 16 per vNIC, you can't change the machine type to a third generation machine type, which uses only gVNIC. Instead, you can migrate your VM to a third generation machine type by following the instructions in [Migrate your workload from an existing VM to a new VM](/compute/docs/import/migrate-to-new-vm) .\n## What's next\n- [Machine types](/compute/docs/machine-resource) \n- [Virtual machine instances](/compute/docs/instances) \n- [Creating and starting a VM instance](/compute/docs/instances/creating-and-starting-an-instance) \n- [Configuring per VM Tier_1 networking performance](/compute/docs/networking/configure-vm-with-high-bandwidth-configuration) \n- [Quickstart using a Linux VM](/compute/docs/create-linux-vm-instance) \n- [Quickstart using a Windows VM](/compute/docs/create-windows-server-vm-instance)", "guide": "Compute Engine"}