{"title": "Google Kubernetes Engine (GKE) - Container-native load balancing through standalone zonal NEGs", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/standalone-neg", "abstract": "# Google Kubernetes Engine (GKE) - Container-native load balancing through standalone zonal NEGs\nThis page shows you how to create a Kubernetes Service that is backed by a [zonal GCE_VM_IP_PORT network endpoint group(NEG)](/load-balancing/docs/negs#zonal-neg) in a Google Kubernetes Engine (GKE) [VPC-native cluster](/kubernetes-engine/docs/concepts/alias-ips) .\nSee [Container-native loadbalancing](/kubernetes-engine/docs/concepts/container-native-load-balancing) for information on the benefits, requirements, and limitations of container-native load balancing.\n", "content": "## Overview\nA [NEG](/load-balancing/docs/negs) represents a group of endpoints. GKE supports standalone NEGs of the `GCE_VM_IP_PORT` type. `GCE_VM_IP_PORT` NEGs support endpoints using either the VM's primary internal IP address or an IP address from one of its alias IP ranges.\nIn the context of a GKE VPC-native cluster using standalone NEGs, each endpoint is a Pod IP address and target port. Pod IP addresses are sourced from the node's alias IP range for Pods, which comes from the cluster's [subnet secondary IP address range forPods](/kubernetes-engine/docs/concepts/alias-ips#cluster_sizing_secondary_range_pods) .\nGKE provides a to manage the membership of `GCE_VM_IP_PORT` NEGs. You can add the NEGs it creates as backends to the backend services for load balancers that you configure outside of the GKE API.\nThe following diagram describes how Kubernetes API objects correspond to Compute Engine objects.### Ingress with NEGs\nWhen [NEGs are used with GKE Ingress](/kubernetes-engine/docs/how-to/container-native-load-balancing) , the Ingress controller facilitates the creation of all aspects of the load balancer. This includes creating the virtual IP address, forwarding rules, health checks, firewall rules, and more.\nIngress is the recommended way to use container-native load balancing as it has many features that simplify the management of NEGs. Standalone NEGs are an option if NEGs managed by Ingress don't serve your use case.\n### Standalone NEGs\nWhen NEGs are deployed with load balancers provisioned by anything other than Ingress, they are considered standalone NEGs. Standalone NEGs are deployed and managed through the NEG controller, but the forwarding rules, health checks, and other load balancing objects are deployed manually.\nStandalone NEGs don't conflict with Ingress enabled container-native load balancing.\nThe following illustration shows the differences in how the load balancing objects are deployed in each scenario:\nWith standalone NEGs, you are responsible for managing the lifecycles of NEGs and the resources that make up the load balancer. You could leak NEGs in these ways:\n- When a GKE service is deleted, the associated NEG won't be garbage collected if the NEG is still referenced by a backend service. Dereference the NEG from the backend service to allow NEG deletion.\n- When a cluster is deleted, standalone NEGs are not deleted.## Use cases of standalone NEGs\nStandalone NEGs have several critical uses. Standalone NEGs are very flexible. This is in contrast to Ingress (used with or without NEGs) which defines a specific set of load balancing objects that were chosen in an opinionated way to make them straightforward to use.\nUse cases for standalone NEGs include:\n### Heterogeneous services of containers and VMs\nNEGs can contain both VM and container IP addresses. This means a single virtual IP address can point to a backend that consists of both Kubernetes and non-Kubernetes workloads. This can also be used to migrate existing workloads to a GKE cluster.\nStandalone NEGs can point to VM IPs which makes it possible to manually configure load balancers to point at backends that are comprised of both VMs and containers for the same service VIP.\n### Customized Ingress controllers\nYou can use a customized Ingress controller (or no Ingress controller) to configure load balancers that target standalone NEGs.\n### Use Traffic Director with GKE\nYou can use [Traffic Director](/traffic-director) with GKE. Traffic Director uses standalone NEGs to provide container-native load balancing for the managed service mesh.\n### Use external proxy Network Load Balancers with GKE\nYou can use standalone NEGs to load balance directly to containers with the [external proxy Network Load Balancer](/load-balancing/docs/tcp) which is not supported natively by Kubernetes/GKE.\n## Pod readiness\n[Readinessgates](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate) are an extensibility feature of Kubernetes that allow the injection of extra feedback or signals into the PodStatus to allow the Pod to transition to the Ready state. The NEG controller manages a custom readiness gate to ensure the full network path from Compute Engine load balancer to pod is functional. Pod readiness gates in GKE are explained in [container-native loadbalancing](/kubernetes-engine/docs/concepts/container-native-load-balancing#pod_readiness) .\nIngress with NEGs deploys and manages Compute Engine health checks on behalf of the load balancer. However, standalone NEGs make no assumptions about Compute Engine health checks because they are expected to be deployed and managed separately. Compute Engine health checks should always be configured along with the load balancer to prevent traffic from being sent to backends which are not ready to receive. If there is no health check status associated with the NEG (usually because no health check is configured), then the NEG controller will mark the Pod's readiness gate value to True when its corresponding endpoint is programmed in NEG.\n## Requirements\nYour cluster must be VPC-native. To learn more, see [Creating a VPC-native cluster](/kubernetes-engine/docs/how-to/alias-ips) .\nYour cluster must have the `HttpLoadBalancing` add-on enabled. GKE clusters have the `HttpLoadBalancing` add-on enabled by default.\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.## Using standalone NEGs\nThe following instructions show you how to use standalone NEGs with an external HTTP load balancer on GKE.\nYou must create the following objects:\n- A [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) that creates and manages Pods.\n- A [Service](/kubernetes-engine/docs/concepts/service) that creates a NEG.\n- A load balancer created with the Compute Engine API. This differs from using NEGs with Ingress, in which case Ingress creates and configures a load balancer for you. With standalone NEGs you are responsible for associating the NEG and the backend service to connect the Pods to the load balancer. The load balancer consists of several components, shown in the following diagram:### Create a VPC-native cluster\nAutopilot clusters are VPC-native by default, so you can skip to [Deploy a workload](#deploy) .\nFor Standard clusters, create a VPC-native cluster in zone `us-central1-a` :\n```\ngcloud container clusters create neg-demo-cluster \\\u00a0 \u00a0 --create-subnetwork=\"\" \\\u00a0 \u00a0 --network=default \\\u00a0 \u00a0 --zone=us-central1-a\n```\n### Create a Deployment\nThe following example manifests specify Deployments that run three instances of a containerized HTTP server. The HTTP server responds to requests with the hostname of the application server, the name of the Pod the server is running on.\nWe recommend you use workloads that use Pod readiness feedback.\n```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 labels:\u00a0 \u00a0 run: neg-demo-app # Label for the Deployment\u00a0 name: neg-demo-app # Name of Deploymentspec:\u00a0 replicas: 3\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 run: neg-demo-app\u00a0 template: # Pod template\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 run: neg-demo-app # Labels Pods from this Deployment\u00a0 \u00a0 spec: # Pod specification; each Pod created by this Deployment has this specification\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - image: registry.k8s.io/serve_hostname:v1.4 # Application to run in Deployment's Pods\u00a0 \u00a0 \u00a0 \u00a0 name: hostname\u00a0 \n``````\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 labels:\u00a0 \u00a0 run: neg-demo-app # Label for the Deployment\u00a0 name: neg-demo-app # Name of Deploymentspec:\u00a0 minReadySeconds: 60 # Number of seconds to wait after a Pod is created and its status is Ready\u00a0 replicas: 3\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 run: neg-demo-app\u00a0 template: # Pod template\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 run: neg-demo-app # Labels Pods from this Deployment\u00a0 \u00a0 spec: # Pod specification; each Pod created by this Deployment has this specification\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - image: registry.k8s.io/serve_hostname:v1.4 # Application to run in Deployment's Pods\u00a0 \u00a0 \u00a0 \u00a0 name: hostname\u00a0 \n```\nSave this manifest as `neg-demo-app.yaml` , then create the Deployment by running the following command:\n```\nkubectl apply -f neg-demo-app.yaml\n```\n### Create a Service\nThe following manifest specifies a Service where:\n- Any Pod with the label`run: neg-demo-app`is a member of this Service.\n- The Service has one [ServicePort](https://v1-25.docs.kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#serviceport-v1-core) field with port 80.\n- The`cloud.google.com/neg`annotation specifies that port 80 will be associated with a NEG. The optional`name`field specifies that the NEG will be named``. If the`name`field is omitted, a unique name will be automatically generated. See [naming NEGs](#naming_negs) for details.\n- Each member Pod must have a container that is listening on TCP port 9376.\n```\napiVersion: v1kind: Servicemetadata:\u00a0 name: neg-demo-svc\u00a0 annotations:\u00a0 \u00a0 cloud.google.com/neg: '{\"exposed_ports\": {\"80\":{\"name\": \"NEG_NAME\"}}}'spec:\u00a0 type: ClusterIP\u00a0 selector:\u00a0 \u00a0 run: neg-demo-app # Selects Pods labelled run: neg-demo-app\u00a0 ports:\u00a0 - port: 80\u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 targetPort: 9376\n```\nReplace `` with the name for the NEG. The NEG name must be unique in its region.\nSave this manifest as `neg-demo-svc.yaml` , then create the Service by running the following command:\n```\nkubectl apply -f neg-demo-svc.yaml\n```\nA NEG is created within a few minutes of Service creation.\nWhile this example uses a `ClusterIP` service, all five [types ofServices](/kubernetes-engine/docs/concepts/service#types_of_services) support standalone NEGs. We recommend the default type, `ClusterIP` .\nIn GKE versions 1.18.18-gke.1200 and later, you can specify a custom name for NEGs, or GKE can generate a name automatically. Previous versions of GKE only support automatically generated NEG names.\nGKE creates one NEG in each zone used by the cluster. The NEGs all use the same name.\nSpecifying a custom NEG name simplifies [configuring the loadbalancer](#add_backends) because you know the name and zone(s) of the NEG(s) in advance. Custom NEG names must meet the following requirements:\n- Be unique to the cluster's zone for zonal clusters, or unique to the region for regional clusters.\n- Must not match the name of any existing NEG that was not created by the GKE NEG controller.\n- Must not contain underscores.\n**Note:** When there's a name conflict, NEG creation or syncing fails. For troubleshooting steps, see [NEG is not synced withService](#neg_is_not_synced_with_service) .\nUse the `name` field in the `cloud.google.com/neg` annotation of the Service to specify a NEG name:\n```\ncloud.google.com/neg: '{\"exposed_ports\": {\"80\":{\"name\": \"NEG_NAME\"}}}'\n```\nReplace `` with the name for the NEG. The NEG name must be unique in its region.\nAutomatically generated NEG names are guaranteed to be unique. To use an automatically generated name, omit the `name` field:\n```\ncloud.google.com/neg: '{\"exposed_ports\": {\"80\":{}}}'\n```\nThe automatically generated name has the following format:\n`k8s1-` `` `-` `` `-` `` `-` `` `-` ``\nA Service can listen on more than one port. By definition, NEGs have only a single IP address and port. This means that if you specify a Service with multiple ports, it will create a NEG for each port.\nThe format of the `cloud.google.com/neg` annotation is:\n```\ncloud.google.com/neg: '{\u00a0 \u00a0\"exposed_ports\":{\u00a0 \u00a0 \u00a0 \"SERVICE_PORT_1\":{},\u00a0 \u00a0 \u00a0 \"SERVICE_PORT_2\":{},\u00a0 \u00a0 \u00a0 \"SERVICE_PORT_3\":{},\u00a0 \u00a0 \u00a0 ...\u00a0 \u00a0}\u00a0}'\n```\nIn this example, each instance of `` is a distinct port number that refers to existing service ports of the Service. For each service port listed, the NEG controller creates one NEG in each zone the cluster occupies.\nUse the following command to retrieve the statuses of the cluster's Services:\n```\nkubectl get service neg-demo-svc -o yaml\n```\nThe output is similar to the following:\n```\ncloud.google.com/neg-status: '{\n \"network-endpoint-groups\":{\n  \"SERVICE_PORT_1\": \"NEG_NAME_1\",\n  \"SERVICE_PORT_2\": \"NEG_NAME_2\",\n  ...\n },\n \"zones\":[\"ZONE_1\", \"ZONE_2\", ...]\n}\n```\nIn this output, each element in the `network-endpoint-groups` mapping is a service port (like `` ) and the name of the corresponding managed NEGs (like `` ). The `zones` list contains every zone (like `` ) that has a NEG in it.\nThe output is similar to the following:\n```\napiVersion: v1kind: Servicemetadata:\u00a0 annotations:\u00a0 \u00a0 cloud.google.com/neg: '{\"exposed_ports\": {\"80\":{}}}'\u00a0 \u00a0 cloud.google.com/neg-status: '{\"network_endpoint_groups\":{\"80\":\"k8s1-cca197ad-default-neg-demo-app-80-4db81e02\"},\"zones\":[\"ZONE_1\", \"ZONE_2\"]}'\u00a0 labels:\u00a0 \u00a0 run: neg-demo-app\u00a0 name: neg-demo-app\u00a0 namespace: default\u00a0 selfLink: /api/v1/namespaces/default/services/neg-demo-app\u00a0 ...spec:\u00a0 clusterIP: 10.0.14.252\u00a0 ports:\u00a0 - port: 80\u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 targetPort: 9376\u00a0 selector:\u00a0 \u00a0 run: neg-demo-app\u00a0 sessionAffinity: Nonestatus:\u00a0 loadBalancer: {}\n```\nIn this example, the annotation shows that service port 80 is exposed to NEGs named `k8s1-cca197ad-default-neg-demo-app-80-4db81e02` .\nA NEG is created within a few minutes of Service creation. If there are Pods that match the label specified in the Service manifest, then upon creation the NEG will contain the IPs of the Pods.\nThere are two ways to verify that the NEG is created and is correctly configured. In GKE 1.18.6-gke.6400 and later, a custom resource `ServiceNetworkEndpointGroup` stores status information about NEGs created by the Service controller. In previous versions, you must inspect the NEGs directly.\nList the NEGs in a cluster by getting all of the `ServiceNetworkEndpointGroup` resources:\n```\nkubectl get svcneg\n```\nObserve the status of a NEG by checking the status of the `ServiceNetworkEndpointGroup` resource:\n```\nkubectl get svcneg NEG_NAME -o yaml\n```\nReplace `` with the name of the individual NEG you want to inspect.\nThe output of this command includes a status section that might contain error messages. Some errors are reported as a Service event. You can find further details by querying the Service object:\n```\nkubectl describe service SERVICE_NAME\n```\nReplace `` with the name of the relevant Service.\nTo verify that the NEG controller is successfully syncing the NEG, check the status field of the `ServiceNetworkEndpointGroup` resource for a condition with `type:Synced` . The time of the most recent sync is in the `status.lastSyncTime` field.\n`ServiceNetworkEndpointGroup` resources only exist in GKE version 1.18 and later.\nVerify that the NEG exists by listing the NEGs in your Google Cloud project and checking for a NEG that matches the Service you created. The NEG's name has the following format:\n`k8s1-` `` `-` `` `-` `` `-` `` `-` ``\nUse the following command to list NEGs:\n```\ngcloud compute network-endpoint-groups list\n```\nThe output is similar to the following:\n```\nNAME           LOCATION  ENDPOINT_TYPE SIZE\nk8s1-70aa83a6-default-my-service-80-c9710a6f ZONE_NAME  GCE_VM_IP_PORT 3\n```\nThis output shows that the `SIZE` of the NEG is 3, meaning that it has three endpoints which correspond to the three Pods in the Deployment.\nIdentify the individual endpoints with the following command:\n```\ngcloud compute network-endpoint-groups list-network-endpoints NEG_NAME\n```\nReplace `` with the name of the NEG for which you want to display the individual endpoints.\nThe output shows three endpoints, each of which has a Pod's IP address and port:\n```\nINSTANCE           IP_ADDRESS PORT\ngke-cluster-3-default-pool-4cc71a15-qlpf 10.12.1.43 9376\ngke-cluster-3-default-pool-4cc71a15-qlpf 10.12.1.44 9376\ngke-cluster-3-default-pool-4cc71a15-w9nk 10.12.2.26 9376\n```\n## Attaching an external Application Load Balancer to standalone NEGs\nYou can use NEGs as a backend for an external Application Load Balancer using the Compute Engine API.\n- Create a firewall rule. Load balancers need to access cluster endpoints to perform health checks. This command creates a firewall rule to allow access:```\ngcloud compute firewall-rules create fw-allow-health-check-and-proxy \\\u00a0 \u00a0--network=NETWORK_NAME \\\u00a0 \u00a0--action=allow \\\u00a0 \u00a0--direction=ingress \\\u00a0 \u00a0--target-tags=GKE_NODE_NETWORK_TAGS \\\u00a0 \u00a0--source-ranges=130.211.0.0/22,35.191.0.0/16 \\\u00a0 \u00a0--rules=tcp:9376\n```Replace the following:- ``: the network where the cluster runs.\n- ``: the [networking tags](/vpc/docs/add-remove-network-tags) on the GKE nodes.\nIf you did not create custom network tags for your nodes, GKE automatically generates tags for you. You can look up these generated tags by running the following command:```\ngcloud compute instances describe INSTANCE_NAME\n```Replace `` with the name of the host Compute Engine VM instance running the GKE node. For example, the output in the previous [section](#inspect-neg-directly) displays the instance names in the `INSTANCE` column for the GKE nodes. For Standard clusters, you can also run `gcloud compute instances list` to list all instances in your project.\n- Create a global virtual IP address for the load balancer:```\ngcloud compute addresses create hostname-server-vip \\\u00a0 \u00a0 --ip-version=IPV4 \\\u00a0 \u00a0 --global\n```\n- Create a health check. This is used by the load balancer to detect the liveness of individual endpoints within the NEG.```\ngcloud compute health-checks create http http-basic-check \\\u00a0 \u00a0 --use-serving-port\n```\n- Create a [backend service](/load-balancing/docs/backend-service) that specifies that this is a global external Application Load Balancer:```\ngcloud compute backend-services create my-bes \\\u00a0 \u00a0 --protocol HTTP \\\u00a0 \u00a0 --health-checks http-basic-check \\\u00a0 \u00a0 --global\n```\n- Create a [URL map](/load-balancing/docs/url-map-concepts) and [targetproxy](/load-balancing/docs/target-proxies) for the load balancer. This example is very straightforward because the `serve_hostname` app used for this guide has a single endpoint and does not feature URLs.```\ngcloud compute url-maps create web-map \\\u00a0 \u00a0 --default-service my-bes\n``````\ngcloud compute target-http-proxies create http-lb-proxy \\\u00a0 \u00a0 --url-map web-map\n```\n- Create a [forwarding rule](/load-balancing/docs/forwarding-rule-concepts) . This is what creates the load balancer.```\ngcloud compute forwarding-rules create http-forwarding-rule \\\u00a0 \u00a0 --address=HOSTNAME_SERVER_VIP \\\u00a0 \u00a0 --global \\\u00a0 \u00a0 --target-http-proxy=http-lb-proxy \\\u00a0 \u00a0 --ports=80\n```Replace `` with the IP address to use for the load balancer. If you omit `--address` , GKE automatically assigns an ephemeral IP address. You can also [reserve a new static external IP address](/compute/docs/ip-addresses/reserve-static-external-ip-address) .\n### Checkpoint\nThese are the resources you have created so far:\n- An external virtual IP address\n- The forwarding rules\n- The firewall rules\n- The target HTTP proxy\n- The URL map the Compute Engine health check\n- The backend service\n- The Compute Engine health check\nThe relationship between these resources is shown in the following diagram:\nThese resources together are a load balancer. In the next step you will add backends to the load balancer.\nOne of the benefits of standalone NEGs demonstrated here is that the lifecycles of the load balancer and backend can be completely independent. The load balancer can continue running after the application, its services, or the GKE cluster is deleted. You can add and remove new NEGs or multiple NEGs from the load balancer without changing any of the frontend load balancer objects.\n### Add backends to the load balancer\nUse [gcloud compute backend-servicesadd-backend](/sdk/gcloud/reference/compute/backend-services/add-backend) to connect the NEG to the load balancer by adding it as a backend of the `my-bes` backend service:\n```\ngcloud compute backend-services add-backend my-bes \\\u00a0 \u00a0 --global \\\u00a0 \u00a0 --network-endpoint-group=NEG_NAME \\\u00a0 \u00a0 --network-endpoint-group-zone=NEG_ZONE \\\u00a0 \u00a0 --balancing-mode RATE --max-rate-per-endpoint 5\n```\nReplace the following:\n- ``: the name of your network endpoint group. The name is either the [name you specified when creating theNEG](#naming_negs) or an autogenerated name. If you did not specify a name for the NEG, see the following instructions to find the autogenerated name.\n- ``: the zone your network endpoint group is in. See the following instructions to find this value.\nUse this command to get the name and location of the NEG:\n```\ngcloud compute network-endpoint-groups list\n```\nThe output is similar to the following:\n```\nNAME           LOCATION  ENDPOINT_TYPE SIZE\nk8s1-70aa83a6-default-my-service-80-c9710a6f ZONE_NAME  GCE_VM_IP_PORT 3\n```\nIn this example output, the name of the NEG is `k8s1-70aa83a6-default-my-service-80-c9710a6f` .\nMultiple NEGs can be added to the same backend service. Global backend services like `my-bes` can have NEG backends in different regions, while regional backend services must have backends in a single region.\n### Validate that the load balancer works\n**Note:** It may take a few minutes for the load balancer to provision and stabilize.\nThere are two ways to validate that the load balancer you set up is working:\n- Verify that the health check is correctly configured and reporting healthy.\n- Access the application and verify its response.\n### Verify health checks\nCheck that the backend service is associated with the health check and network endpoint groups, and that the individual endpoints are healthy.\nUse this command to check that the backend service is associated with your health check and your network endpoint group:\n```\ngcloud compute backend-services describe my-bes --global\n```\nThe output is similar to the following:\n```\nbackends:\n- balancingMode: RATE\n capacityScaler: 1.0\n group: ... /networkEndpointGroups/k8s1-70aa83a6-default-my-service-80-c9710a6f\n...\nhealthChecks:\n- ... /healthChecks/http-basic-check\n...\nname: my-bes\n...\n```\nNext, check the health of the individual endpoints:\n```\ngcloud compute backend-services get-health my-bes --global\n```\nThe `status:` section of the output is similar to the following:\n```\nstatus:\n healthStatus:\n - healthState: HEALTHY\n instance: ... gke-cluster-3-default-pool-4cc71a15-qlpf\n ipAddress: 10.12.1.43\n port: 50000\n - healthState: HEALTHY\n instance: ... gke-cluster-3-default-pool-4cc71a15-qlpf\n ipAddress: 10.12.1.44\n port: 50000\n - healthState: HEALTHY\n instance: ... gke-cluster-3-default-pool-4cc71a15-w9nk\n ipAddress: 10.12.2.26\n port: 50000\n```\n### Access the application\nAccess the application through the load balancer's IP address to confirm that everything is working.\n**Note:** It takes a few minutes for the load balancer to be fully programmed.\nFirst, get the virtual IP address of the load balancer:\n```\ngcloud compute addresses describe hostname-server-vip --global | grep \"address:\"\n```\nThe output will include an IP address. Next, send a request to that IP address ( `34.98.102.37` in this example):\n```\ncurl 34.98.102.37\n```\nThe response from the `serve_hostname` app should be `neg-demo-app` .\n## Attaching an internal Application Load Balancer to standalone NEGs\nYou can use NEGs to configure an internal Application Load Balancer for your services running in standalone GKE Pods.\n### Configuring the proxy-only subnet\nThe [proxy-only subnet](/load-balancing/docs/proxy-only-subnets) is for all regional internal Application Load Balancers in the load balancer's region.\n**Important:** Don't try to assign addresses from the proxy-only subnet to your load balancer's forwarding rule or backends. You assign the forwarding rule's IP address and the backend instance IP addresses from a different subnet range (or ranges) from the proxy-only subnet. Google Cloud reserves the subnet range for Google Cloud-managed proxies.\nIf you're using the Google Cloud console, you can wait and create the proxy-only subnet later.\nCreate the proxy-only subnet with the [gcloud compute networks subnetscreate](/sdk/gcloud/reference/compute/networks/subnets/create) command.\n```\ngcloud compute networks subnets create proxy-only-subnet \\\u00a0 \u00a0 --purpose=REGIONAL_MANAGED_PROXY \\\u00a0 \u00a0 --role=ACTIVE \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --network=lb-network \\\u00a0 \u00a0 --range=10.129.0.0/23\n```\nReplace `` with the [Compute Engine](/compute/docs/regions-zones#available) for the subnet.\nCreate the proxy-only subnet with the [subnetworks.insert](/compute/docs/reference/rest/v1/subnetworks/insert) method.\n```\nPOST https://compute.googleapis.com/compute/projects/PROJECT_ID/regions/COMPUTE_REGION/subnetworks\n```\n```\n{\u00a0 \"name\": \"proxy-only-subnet\",\u00a0 \"ipCidrRange\": \"10.129.0.0/23\",\u00a0 \"network\": \"projects/PROJECT_ID/global/networks/lb-network\",\u00a0 \"region\": \"projects/PROJECT_ID/regions/COMPUTE_REGION\",\u00a0 \"purpose\": \"REGIONAL_MANAGED_PROXY\",\u00a0 \"role\": \"ACTIVE\"}\n```\nReplace the following:- ``: your project ID.\n- ``: the [Compute Engine](/compute/docs/regions-zones#available) for the subnet.\n### Configuring firewall rules\nThis example uses the following firewall rules:\n- `fw-allow-ssh` : An ingress rule, applicable to the instances being load balanced, that allows incoming SSH connectivity on TCP port 22 from any address. You can choose a more restrictive source IP range for this rule. For example, you can specify just the IP ranges of the system from which you initiate SSH sessions. This example uses the target tag `allow-ssh` to identify the VMs to which the firewall rule applies.\n- `fw-allow-health-check` : An ingress rule, applicable to the instances being load balanced, that allows all TCP traffic from the Google Cloud health checking systems (in `130.211.0.0/22` and `35.191.0.0/16` ). This example uses the target tag `load-balanced-backend` to identify the instances to which it should apply.\n- `fw-allow-proxies` : An ingress rule, applicable to the instances being load balanced, that allows TCP traffic on port `9376` from the internal HTTP(S) load balancer's managed proxies. This example uses the target tag `load-balanced-backend` to identify the instances to which it should apply.\nWithout these firewall rules, the [default denyingress](/vpc/docs/firewalls#default_firewall_rules) rule blocks incoming traffic to the backend instances.\n- Go to the **Firewall policies** page in the Google Cloud console. [Go to Firewall policies](https://console.cloud.google.com/net-security/firewall-manager/firewall-policies/list) \n- Click **Create firewall rule** add_box to create the rule to allow incoming SSH connections:- **Name** :`fw-allow-ssh`\n- **Network** :`lb-network`\n- **Direction of traffic** : ingress\n- **Action on match** : allow\n- **Targets** : Specified target tags\n- **Target tags** :`allow-ssh`\n- **Source filter** :`IPv4 ranges`\n- **Source IPv4 ranges** :`0.0.0.0/0`\n- **Protocols and ports** :- Select **Specified protocols and ports** .\n- Select the`tcp`checkbox and specify port`22`.\n- Click **Create** .\n- Click **Create firewall rule** add_box again to create the rule to allow Google Cloud health checks:- **Name** :`fw-allow-health-check`\n- **Network** :`lb-network`\n- **Direction of traffic** : ingress\n- **Action on match** : allow\n- **Targets** : Specified target tags\n- **Target tags** :`load-balanced-backend`\n- **Source filter** :`IPv4 ranges`\n- **Source IPv4 ranges** :`130.211.0.0/22`and`35.191.0.0/16`\n- **Protocols and ports** :- Select **Specified protocols and ports** \n- Select the`tcp`checkbox and specify port`80`. As a best practice, limit this rule to just the protocols and ports that match those used by your health check. If you use`tcp:80`for the protocol and port, Google Cloud can contact your VMs using HTTP on port 80, but it cannot contact them using HTTPS on port 443.\n- Click **Create** .\n- Click **Create firewall rule** add_box again to create the rule to allow the load balancer's proxy servers to connect the backends:- **Name** :`fw-allow-proxies`\n- **Network** :`lb-network`\n- **Direction of traffic** : ingress\n- **Action on match** : allow\n- **Targets** : Specified target tags\n- **Target tags** :`load-balanced-backend`\n- **Source filter** :`IPv4 ranges`\n- **Source IPv4 ranges** :`10.129.0.0/23`\n- **Protocols and ports** :- Select **Specified protocols and ports** .\n- Select the`tcp`checkbox and specify port`9376`.\n- Click **Create** .\n- Create the `fw-allow-ssh` firewall rule to allow SSH connectivity to VMs with the network tag `allow-ssh` . When you omit `source-ranges` , Google Cloud [interprets the rule to mean anysource](/vpc/docs/firewalls#sources_for_the_rule) .```\ngcloud compute firewall-rules create fw-allow-ssh \\\u00a0 \u00a0 --network=lb-network \\\u00a0 \u00a0 --action=allow \\\u00a0 \u00a0 --direction=ingress \\\u00a0 \u00a0 --target-tags=allow-ssh \\\u00a0 \u00a0 --rules=tcp:22\n```\n- Create the `fw-allow-health-check` rule to allow Google Cloud health checks. This example allows all TCP traffic from health check probers; however, you can configure a narrower set of ports to meet your needs.```\ngcloud compute firewall-rules create fw-allow-health-check \\\u00a0 \u00a0 --network=lb-network \\\u00a0 \u00a0 --action=allow \\\u00a0 \u00a0 --direction=ingress \\\u00a0 \u00a0 --source-ranges=130.211.0.0/22,35.191.0.0/16 \\\u00a0 \u00a0 --target-tags=load-balanced-backend \\\u00a0 \u00a0 --rules=tcp\n```\n- Create the `fw-allow-proxies` rule to allow the internal HTTP(S) load balancer's proxies to connect to your backends.```\ngcloud compute firewall-rules create fw-allow-proxies \\\u00a0 \u00a0 --network=lb-network \\\u00a0 \u00a0 --action=allow \\\u00a0 \u00a0 --direction=ingress \\\u00a0 \u00a0 --source-ranges=10.129.0.0/23 \\\u00a0 \u00a0 --target-tags=load-balanced-backend \\\u00a0 \u00a0 --rules=tcp:9376\n```\nCreate the `fw-allow-ssh` firewall rule by making a `POST` request to the [firewalls.insert](/compute/docs/reference/rest/v1/firewalls/insert) method.\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/firewalls\n```\n```\n{\u00a0 \"name\": \"fw-allow-ssh\",\u00a0 \"network\": \"projects/PROJECT_ID/global/networks/lb-network\",\u00a0 \"sourceRanges\": [\u00a0 \u00a0 \"0.0.0.0/0\"\u00a0 ],\u00a0 \"targetTags\": [\u00a0 \u00a0 \"allow-ssh\"\u00a0 ],\u00a0 \"allowed\": [\u00a0 \u00a0{\u00a0 \u00a0 \u00a0\"IPProtocol\": \"tcp\",\u00a0 \u00a0 \u00a0\"ports\": [\u00a0 \u00a0 \u00a0 \u00a0\"22\"\u00a0 \u00a0 \u00a0]\u00a0 \u00a0}\u00a0 ],\u00a0\"direction\": \"INGRESS\"}\n```\nReplace `` with your project ID.\nCreate the `fw-allow-health-check` firewall rule by making a `POST` request to the [firewalls.insert](/compute/docs/reference/rest/v1/firewalls/insert) method.\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/firewalls\n```\n```\n{\u00a0 \"name\": \"fw-allow-health-check\",\u00a0 \"network\": \"projects/PROJECT_ID/global/networks/lb-network\",\u00a0 \"sourceRanges\": [\u00a0 \u00a0 \"130.211.0.0/22\",\u00a0 \u00a0 \"35.191.0.0/16\"\u00a0 ],\u00a0 \"targetTags\": [\u00a0 \u00a0 \"load-balanced-backend\"\u00a0 ],\u00a0 \"allowed\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"IPProtocol\": \"tcp\"\u00a0 \u00a0 }\u00a0 ],\u00a0 \"direction\": \"INGRESS\"}\n```\nCreate the `fw-allow-proxies` firewall rule to allow TCP traffic within the proxy subnet the [firewalls.insert](/compute/docs/reference/rest/v1/firewalls/insert) method.\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/global/firewalls\n```\n```\n{\u00a0 \"name\": \"fw-allow-proxies\",\u00a0 \"network\": \"projects/PROJECT_ID/global/networks/lb-network\",\u00a0 \"sourceRanges\": [\u00a0 \u00a0 \"10.129.0.0/23\"\u00a0 ],\u00a0 \"targetTags\": [\u00a0 \u00a0 \"load-balanced-backend\"\u00a0 ],\u00a0 \"allowed\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"IPProtocol\": \"tcp\",\u00a0 \u00a0 \u00a0 \"ports\": [\u00a0 \u00a0 \u00a0 \u00a0 \"9376\"\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 }\u00a0 ],\u00a0 \"direction\": \"INGRESS\"}\n```\nReplace `` with your project ID.\n### Configuring the load balancer\nFor the forwarding rule's IP address, use a backend subnet. If you try to use the [proxy-only subnet](#proxy-only-subnet) , forwarding rule creation fails.\n### Select a load balancer type\n- Go to the **Create a load balancer** page in the Google Cloud console. [Go to Create a load balancer](https://console.cloud.google.com/networking/loadbalancing/add) \n- Under **HTTP(S) Load Balancing** , click **Start configuration** .\n- Select. This setting means that the load balancer is internal.\n- Click **Continue** .\n### Prepare the load balancer\n- For the **Name** of the load balancer, enter`l7-ilb-gke-map`.\n- For the **Region** , select the region where you created the subnet.\n- For the **Network** , select`lb-network`.\n### Reserve a proxy-only subnet **Note:** If you've already reserved a proxy-only subnet, [asinstructed](#proxy-only-subnet) , the **Reserve a Subnet** button isn't displayed, so you skip this section and continue with the steps in [Configure the backend service](#config-backend-service) .\nReserve a proxy-only subnet:- Click **Reserve a Subnet** .\n- For the **Name** , enter`proxy-only-subnet`.\n- For the **IP address range** , enter`10.129.0.0/23`.\n- Click **Add** .\n### Configure the backend service\n- Click **Backend configuration** .\n- From the **Create or select backend services** menu, select **Create a\nbackend service** .\n- Set the **Name** of the backend service to`l7-ilb-gke-backend-service`.\n- For **Backend type** , select **Network endpoint groups** .\n- In the **New backend** card of the **Backends** section:- Set the **Network endpoint group** to the NEG was created by GKE. To get the NEG name, see [Validate NEGcreation](#validate-neg-creation) .\n- For **Maximum RPS** , specify a maximum rate of`5`RPS per endpoint. Google Cloud will exceed this maximum if necessary.\n- Click **Done** .\n- From the **Health check** drop-down list, select **Create a health check** and then specify the following parameters:- **Name** :`l7-ilb-gke-basic-check`\n- **Protocol** : HTTP\n- **Port specification** :\n- Click **Save and Continue** .\n- Click **Create** .\n### Configure the URL map\n- Click **Routing rules** . Ensure that theis the only backend service for any unmatched host and any unmatched path.\n### Configure the frontendClick **Frontend configuration** and perform the following steps:\n **For HTTP:** - Click **Frontend configuration** .\n- Click **Add frontend IP and port** .\n- Set the **Name** to.\n- Set the **Protocol** to.\n- Set the **Subnetwork** to.\n- Under **Internal IP** , select.\n- In the panel that appears provide the following details:- **Name** :`l7-ilb-gke-ip`\n- In the **Static IP address** section, select.\n- In the **Custom IP address** section, enter`10.1.2.199`.\n- Click **Reserve** .\n- Set the **Port** to`80`.\n- Click **Done** .\n **For HTTPS:** \nIf you are using HTTPS between the client and the load balancer, you need one or more SSL certificate resources to configure the proxy. See [SSL Certificates](/load-balancing/docs/ssl-certificates) for information on how to create SSL certificate resources. Google-managed certificates aren't supported with internal HTTP(S) load balancers.- Click **Frontend configuration** .\n- Click **Add frontend IP and port** .\n- In the **Name** field, enter`l7-ilb-gke-forwarding-rule`.\n- In the **Protocol** field, select`HTTPS (includes HTTP/2)`.\n- Set the **Subnet** to.\n- Under **Internal IP** , select.\n- In the panel that appears provide the following details:- **Name** :`l7-ilb-gke-ip`\n- In the **Static IP address** section, select.\n- In the **Custom IP address** section, enter`10.1.2.199`.\n- Click **Reserve** .\n- Ensure that the **Port** is set to`443`, to allow HTTPS traffic.\n- Click the **Certificate** drop-down list.- If you already have a [self-managed SSLcertificate resource](/load-balancing/docs/ssl-certificates#certificate-types) you want to use as the primary SSL certificate, select it from the drop-down menu.\n- Otherwise, select **Create a new certificate** .- Fill in a **Name** of`l7-ilb-cert`.\n- In the appropriate fields upload your PEM-formatted files:- **Public key certificate** \n- **Certificate chain** \n- **Private key** \n- Click **Create** .\n- To add certificate resources in addition to the primary SSL certificate resource:- Click **Add certificate** .\n- Select a certificate from the **Certificates** list or click **Create a new certificate** and follow the instructions.\n- Click **Done** .\n### Complete the configurationClick **Create** .- Define the HTTP health check with the [gcloud compute health-checkscreate http](/sdk/gcloud/reference/compute/health-checks/create/http) command.```\ngcloud compute health-checks create http l7-ilb-gke-basic-check \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --use-serving-port\n```\n- Define the backend service with the [gcloud compute backend-servicescreate](/sdk/gcloud/reference/compute/backend-services/create) command.```\ngcloud compute backend-services create l7-ilb-gke-backend-service \\\u00a0 \u00a0 --load-balancing-scheme=INTERNAL_MANAGED \\\u00a0 \u00a0 --protocol=HTTP \\\u00a0 \u00a0 --health-checks=l7-ilb-gke-basic-check \\\u00a0 \u00a0 --health-checks-region=COMPUTE_REGION \\\u00a0 \u00a0 --region=COMPUTE_REGION\n```\n- Set the `DEPLOYMENT_NAME` variable:```\nexport DEPLOYMENT_NAME=NEG_NAME\n```Replace `` with the name of the NEG.\n- Add NEG backends to the backend service with the [gcloud compute backend-servicesadd-backend](/sdk/gcloud/reference/compute/backend-services/add-backend) command.```\ngcloud compute backend-services add-backend l7-ilb-gke-backend-service \\\u00a0 \u00a0 --network-endpoint-group=$DEPLOYMENT_NAME \\\u00a0 \u00a0 --network-endpoint-group-zone=COMPUTE_ZONE-b \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --balancing-mode=RATE \\\u00a0 \u00a0 --max-rate-per-endpoint=5\n```\n- Create the URL map with the [gcloud compute url-mapscreate](/sdk/gcloud/reference/compute/url-maps/create) command.```\ngcloud compute url-maps create l7-ilb-gke-map \\\u00a0 \u00a0 --default-service=l7-ilb-gke-backend-service \\\u00a0 \u00a0 --region=COMPUTE_REGION\n```\n- Create the target proxy. **For HTTP:** Use the [gcloud compute target-http-proxiescreate](/sdk/gcloud/reference/compute/target-http-proxies/create) command.```\ngcloud compute target-http-proxies create l7-ilb-gke-proxy \\\u00a0 \u00a0 --url-map=l7-ilb-gke-map \\\u00a0 \u00a0 --url-map-region=COMPUTE_REGION \\\u00a0 \u00a0 --region=COMPUTE_REGION\n``` **For HTTPS:** See [SSL Certificates](/load-balancing/docs/ssl-certificates) for information on how to create SSL certificate resources. Google-managed certificates aren't supported with internal HTTP(S) load balancers.Assign your filepaths to variable names.```\nexport LB_CERT=PATH_TO_PEM_FORMATTED_FILE\n``````\nexport LB_PRIVATE_KEY=PATH_TO_PEM_FORMATTED_FILE\n```Create a regional SSL certificate using the [gcloud computessl-certificatescreate](/sdk/gcloud/reference/compute/ssl-certificates/create) command. [gcloud computessl-certificates create](/sdk/gcloud/reference/compute/ssl-certificates/create) ```\ngcloud compute ssl-certificates create l7-ilb-cert \\\u00a0 \u00a0 --certificate=$LB_CERT \\\u00a0 \u00a0 --private-key=$LB_PRIVATE_KEY \\\u00a0 \u00a0 --region=COMPUTE_REGION\n```Use the regional SSL certificate to create a target proxy with the [gcloudcompute target-https-proxiescreate](/sdk/gcloud/reference/compute/target-https-proxies/create) command.```\ngcloud compute target-https-proxies create l7-ilb-gke-proxy \\\u00a0 \u00a0 --url-map=l7-ilb-gke-map \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --ssl-certificates=l7-ilb-cert\n```\n- Create the forwarding rule.For custom networks, you must reference the subnet in the forwarding rule. Note that this is the VM subnet, not the proxy subnet. **For HTTP:** Use the [gcloud compute forwarding-rulescreate](/sdk/gcloud/reference/compute/forwarding-rules/create) command with the correct flags.```\ngcloud compute forwarding-rules create l7-ilb-gke-forwarding-rule \\\u00a0 \u00a0 --load-balancing-scheme=INTERNAL_MANAGED \\\u00a0 \u00a0 --network=lb-network \\\u00a0 \u00a0 --subnet=backend-subnet \\\u00a0 \u00a0 --address=10.1.2.199 \\\u00a0 \u00a0 --ports=80 \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --target-http-proxy=l7-ilb-gke-proxy \\\u00a0 \u00a0 --target-http-proxy-region=COMPUTE_REGION\n``` **For HTTPS:** Use the [gcloud compute forwarding-rulescreate](/sdk/gcloud/reference/compute/forwarding-rules/create) command with the correct flags.```\ngcloud compute forwarding-rules create l7-ilb-gke-forwarding-rule \\\u00a0 \u00a0 --load-balancing-scheme=INTERNAL_MANAGED \\\u00a0 \u00a0 --network=lb-network \\\u00a0 \u00a0 --subnet=backend-subnet \\\u00a0 \u00a0 --address=10.1.2.199 \\\u00a0 \u00a0 --ports=443 \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --target-https-proxy=l7-ilb-gke-proxy \\\u00a0 \u00a0 --target-https-proxy-region=COMPUTE_REGION\n```\nCreate the health check by making a `POST` request to the [regionHealthChecks.insert](/compute/docs/reference/rest/v1/regionHealthChecks/insert) method.\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/COMPUTE_REGION/healthChecks\n```\n```\n{\u00a0 \u00a0\"name\": \"l7-ilb-gke-basic-check\",\u00a0 \u00a0\"type\": \"HTTP\",\u00a0 \u00a0\"httpHealthCheck\": {\u00a0 \u00a0 \u00a0\"portSpecification\": \"USE_SERVING_PORT\"\u00a0 \u00a0}}\n```\nReplace `` with the project ID.\nCreate the regional backend service by making a `POST` request to the [regionBackendServices.insert](/compute/docs/reference/rest/v1/regionBackendServices/insert) method.\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/COMPUTE_REGION/backendServices\n```\n```\n{\u00a0 \"name\": \"l7-ilb-gke-backend-service\",\u00a0 \"backends\": [\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"group\": \"https://www.googleapis.com/compute/v1/projects/PROJECT_ID/zones/COMPUTE_ZONE/networkEndpointGroups/NEG_NAME\",\u00a0 \u00a0 \u00a0 \"balancingMode\": \"RATE\",\u00a0 \u00a0 \u00a0 \"maxRatePerEndpoint\": 5\u00a0 \u00a0 }\u00a0 ],\u00a0 \"healthChecks\": [\u00a0 \u00a0 \"projects/PROJECT_ID/regions/COMPUTE_REGION/healthChecks/l7-ilb-gke-basic-check\"\u00a0 ],\u00a0 \"loadBalancingScheme\": \"INTERNAL_MANAGED\"}\n```\nReplace the following:- ``: the project ID.\n- ``: the name of the NEG.\nCreate the URL map by making a `POST` request to the [regionUrlMaps.insert](/compute/docs/reference/rest/v1/regionUrlMaps/insert) method.\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/COMPUTE_REGION/urlMaps\n```\n```\n{\u00a0 \"name\": \"l7-ilb-gke-map\",\u00a0 \"defaultService\": \"projects/PROJECT_ID/regions/COMPUTE_REGION/backendServices/l7-ilb-gke-backend-service\"}\n```\nReplace `` with the project ID.\nCreate the target HTTP proxy by making a `POST` request to the [regionTargetHttpProxies.insert](/compute/docs/reference/rest/v1/regionTargetHttpProxies/insert) method.\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/COMPUTE_REGION/targetHttpProxy\n```\n```\n{\u00a0 \"name\": \"l7-ilb-gke-proxy\",\u00a0 \"urlMap\": \"projects/PROJECT_ID/global/urlMaps/l7-ilb-gke-map\",\u00a0 \"region\": \"COMPUTE_REGION\"}\n```\nReplace `` with the project ID.\nCreate the forwarding rule by making a `POST` request to the [forwardingRules.insert](/compute/docs/reference/rest/v1/forwardingRules/insert) method.\n```\nPOST https://compute.googleapis.com/compute/v1/projects/PROJECT_ID/regions/COMPUTE_REGION/forwardingRules\n```\n```\n{\u00a0 \"name\": \"l7-ilb-gke-forwarding-rule\",\u00a0 \"IPAddress\": \"10.1.2.199\",\u00a0 \"IPProtocol\": \"TCP\",\u00a0 \"portRange\": \"80-80\",\u00a0 \"target\": \"projects/PROJECT_ID/regions/COMPUTE_REGION/targetHttpProxies/l7-ilb-gke-proxy\",\u00a0 \"loadBalancingScheme\": \"INTERNAL_MANAGED\",\u00a0 \"subnetwork\": \"projects/PROJECT_ID/regions/COMPUTE_REGION/subnetworks/backend-subnet\",\u00a0 \"network\": \"projects/PROJECT_ID/global/networks/lb-network\",\u00a0 \"networkTier\": \"PREMIUM\",}\n```\nReplace `` with the project ID.\n### Testing\nCreate a VM instance in the zone to test connectivity:\n```\ngcloud compute instances create l7-ilb-client \\\u00a0 \u00a0 --image-family=debian-9 \\\u00a0 \u00a0 --image-project=debian-cloud \\\u00a0 \u00a0 --zone=COMPUTE_ZONE \\\u00a0 \u00a0 --network=lb-network \\\u00a0 \u00a0 --subnet=backend-subnet \\\u00a0 \u00a0 --tags=l7-ilb-client,allow-ssh\n```\nSign in to the client instance to check that HTTP(S) services on the backends are reachable using the internal Application Load Balancer's forwarding rule IP address, and traffic is being load balanced among endpoints in the NEG.\nConnect to each client instance using SSH:\n```\ngcloud compute ssh l7-ilb-client \\\u00a0 \u00a0 --zone=COMPUTE_ZONE-b\n```\nVerify that the IP is serving its hostname:\n```\ncurl 10.1.2.199\n```\nFor HTTPS testing, run the following command:\n```\ncurl -k -s 'https://test.example.com:443' --connect-to test.example.com:443:10.1.2.199:443\n```\nThe `-k` flag causes `curl` to skip certificate validation.\nRun 100 requests and confirm that they are load balanced.\n**For HTTP:**\n```\n{RESULTS=for i in {1..100}do\u00a0 \u00a0 RESULTS=\"$RESULTS:$(curl --silent 10.1.2.199)\"doneecho \"***\"echo \"*** Results of load-balancing to 10.1.2.199: \"echo \"***\"echo \"$RESULTS\" | tr ':' '\\n' | grep -Ev \"^$\" | sort | uniq -cecho}\n```\n**For HTTPS:**\n```\n{RESULTS=for i in {1..100}do\u00a0 \u00a0 RESULTS=\"$RESULTS:$(curl -k -s 'https://test.example.com:443' --connect-to test.example.com:443:10.1.2.199:443)\"doneecho \"***\"echo \"*** Results of load-balancing to 10.1.2.199: \"echo \"***\"echo \"$RESULTS\" | tr ':' '\\n' | grep -Ev \"^$\" | sort | uniq -cecho}\n```\n## Implementing heterogeneous services (VMs and containers)\nLoad balancers can be frontends to mixed Kubernetes and non-Kubernetes workloads. This could be part of a migration from VMs to containers or a permanent architecture that benefits from a shared load balancer. This can be achieved by creating load balancers that target different kinds of backends including standalone NEGs.\n### VMs and containers in the same backend service\nThis example shows how to create a NEG that points at an existing VM running a workload, and how to add this NEG as another backend of an existing `backendService` . This way a single load balancer balances between VMs and GKE containers.\nThis examples extends the [previous example](#how_to) that uses an external HTTP load balancer.\nBecause all endpoints are grouped by the same `backendService` , the VM and container endpoints are considered the **same service** . This means the host or path matching will treat all backends identically based on the URL map rules.\nWhen you use a NEG as a backend for a backend service, all other backends in that backend service must also be NEGs. You can't use instance groups and NEGs as backends in the same backend service. Additionally, containers and VMs cannot exist as endpoints within the same NEG, so they must always be configured with separate NEGs.\n- Deploy a VM to Compute Engine with this command:```\ngcloud compute instances create vm1 \\\u00a0 \u00a0 --zone=COMPUTE_ZONE \\\u00a0 \u00a0 --network=NETWORK \\\u00a0 \u00a0 --subnet=SUBNET \\\u00a0 \u00a0 --image-project=cos-cloud \\\u00a0 \u00a0 --image-family=cos-stable --tags=vm-neg-tag\n```Replace the following:- ``: the name of the zone.\n- ``: the name of the network.\n- ``: the name of the subnet associated with the network.\n- Deploy an application to the VM:```\ngcloud compute ssh vm1 \\\u00a0 \u00a0 --zone=COMPUTE_ZONE \\\u00a0 \u00a0 --command=\"docker run -d --rm --network=host registry.k8s.io/serve_hostname:v1.4 && sudo iptables -P INPUT ACCEPT\"\n```This command deploys to the VM the same example application used in the earlier example. For simplicity, the application is run as a Docker container but this is not essential. The `iptables` command is required to allow firewall access to the running container.\n- Validate that the application is serving on port 9376 and reporting that it is running on vm1:```\ngcloud compute ssh vm1 \\\u00a0 \u00a0 --zone=COMPUTE_ZONE \\\u00a0 \u00a0 --command=\"curl -s localhost:9376\"\n```The server should respond with `vm1` .\n- Create a NEG to use with the VM endpoint. Containers and VMs can both be NEG endpoints, but a single NEG can't have both VM and container endpoints.```\ngcloud compute network-endpoint-groups create vm-neg \\\u00a0 \u00a0 --subnet=SUBNET \\\u00a0 \u00a0 --zone=COMPUTE_ZONE\n```\n- Attach the VM endpoint to the NEG:```\ngcloud compute network-endpoint-groups update vm-neg \\\u00a0 \u00a0 --zone=COMPUTE_ZONE \\\u00a0 \u00a0 --add-endpoint=\"instance=vm1,ip=VM_PRIMARY_IP,port=9376\"\n```Replace `` with the VM's primary IP address.\n- Confirm that the NEG has the VM endpoint:```\ngcloud compute network-endpoint-groups list-network-endpoints vm-neg \\\u00a0 \u00a0 --zone COMPUTE_ZONE\n```\n- Attach the NEG to the backend service using the [same command that you usedto add a container backend](#add_backends) :```\ngcloud compute backend-services add-backend my-bes\u00a0 \u00a0 --global \\\u00a0 \u00a0 --network-endpoint-group vm-neg \\\u00a0 \u00a0 --network-endpoint-group-zone COMPUTE_ZONE \\\u00a0 \u00a0 --balancing-mode RATE --max-rate-per-endpoint 10\n```\n- Open firewall to allow the VM's health check:```\ngcloud compute firewall-rules create fw-allow-health-check-to-vm1 \\\u00a0 \u00a0 --network=NETWORK \\\u00a0 \u00a0 --action=allow \\\u00a0 \u00a0 --direction=ingress \\\u00a0 \u00a0 --target-tags=vm-neg-tag \\\u00a0 \u00a0 --source-ranges=130.211.0.0/22,35.191.0.0/16 \\\u00a0 \u00a0 --rules=tcp:9376\n```\n- Validate that the load balancer is forwarding traffic to both the new vm1 backend and the existing container backend by sending test traffic:```\nfor i in `seq 1 100`; do curl ${VIP};echo; done\n```You should see responses from both the container ( `neg-demo-app` ) and VM ( `vm1` ) endpoints.\n### VMs and containers for different backend services\nThis example shows how to create a NEG that points at an existing VM running a workload, and how to add this NEG as the backend to a new `backendService` . This is useful for the case where the containers and VMs are different services but need to share the same L7 load balancer, such as if the services share the same IP address or domain name.\nThis examples extends the [previous example](#how_to) that has a VM backend in the same backend service as the container backend. This example reuses that VM.\nBecause the container and VM endpoints are grouped in separate backend services, they are considered **different services** . This means that the URL map will match backends and direct traffic to the VM or container based on the hostname.\nThe following diagram shows how a single virtual IP address corresponds to two host names, which in turn correspond to a container-based backend service and a VM-based backend service.\nThe following diagram shows the architecture described in the previous section:- Create a new backend service for the VM:```\ngcloud compute backend-services create my-vm-bes \\\u00a0 \u00a0--protocol HTTP \\\u00a0 \u00a0--health-checks http-basic-check \\\u00a0 \u00a0--global\n```\n- Attach the NEG for the VM, `vm-neg` , to the backend-service:```\ngcloud compute backend-services add-backend my-vm-bes \\\u00a0 \u00a0 --global \\\u00a0 \u00a0 --network-endpoint-group vm-neg \\\u00a0 \u00a0 --network-endpoint-group-zone COMPUTE_ZONE \\\u00a0 \u00a0 --balancing-mode RATE --max-rate-per-endpoint 10\n```\n- Add a host rule to the [URL map](/load-balancing/docs/url-map-concepts) to direct requests for `container.example.com` host to the container backend service:```\ngcloud compute url-maps add-path-matcher web-map \\\u00a0 \u00a0 --path-matcher-name=container-path --default-service=my-bes \\\u00a0 \u00a0 --new-hosts=container.example.com --global\n```\n- Add another host rule the URL map to direct requests for `vm.example.com` host to the VM backend service:```\ngcloud compute url-maps add-path-matcher web-map \\\u00a0 \u00a0 --path-matcher-name=vm-path --default-service=my-vm-bes \\\u00a0 \u00a0 --new-hosts=vm.example.com --global\n``` **Note:** It takes a few minutes for the load balancer to be fully programmed.\n- Validate that the load balancer sends traffic to the VM backend based on the requested path:```\ncurl -H \"HOST:vm.example.com\" VIRTUAL_IP\n```Replace `` with the virtual IP address.## Limitations of standalone NEGs\n- Annotation validation errors are exposed to the user through Kubernetes events.\n- The [limitations ofNEGs](/kubernetes-engine/docs/concepts/container-native-load-balancing#limitations) also apply to standalone NEGs.\n- Standalone NEGs don't work with [legacy networks](/vpc/docs/legacy) .\n- Standalone NEGs can only be used with compatible network services including [Traffic Director](/traffic-director) and the [compatible load balancer types](/load-balancing/docs/negs/zonal-neg-concepts) .## Pricing\nRefer to the [load balancing section](/vpc/network-pricing#lb-pricing) of the pricing page for details on pricing of the load balancer. There is no additional charge for NEGs.\n## Troubleshooting\nThis section provides troubleshooting steps for common issues that you might encounter with the standalone NEGs.\n### No standalone NEG configured\n**Symptom:** No NEG is created.\n**Potential Resolution:**\n- Check the events associated with the Service and look for error messages.\n- Verify the standalone NEG annotation is well-formed JSON, and that the exposed ports match existing Ports in the Service spec.\n- Verify the NEG status annotation and see if expected service ports has corresponding NEGs.\n- Verify that the NEGs have been created in the expected zones, with the command`gcloud compute network-endpoint-groups list`.\n- If using GKE version 1.18 or later, check if the [svcneg resource](#svcneg) for the Service exists. If it does, check the`Initialized`condition for any error information.\n- If you are using [custom NEG names](#specify-a-name) , ensure that each NEG name in is unique in its region.\n### Traffic does not reach the endpoints\n**Symptom:** 502 errors or rejected connections.\n**Potential Resolution:**\n- After the service is configured new endpoints will generally become reachable after attaching them to NEG, provided they respond to health checks.\n- If after this time traffic still can't reach the endpoints resulting in 502 error code for HTTP(S) or connections being rejected for TCP/SSL load balancers, check the following:- Verify that firewall rules allow incoming TCP traffic to your endpoints from following ranges:`130.211.0.0/22`and`35.191.0.0/16`.\n- Verify that your endpoints are healthy using the Google Cloud CLI or by calling`getHealth`API on the`backendService`or the listEndpoints API on the NEG with the showHealth parameter set to`SHOW`.\n### Stalled rollout\n**Symptom:** Rolling out an updated Deployment stalls, and the number of up-to-date replicas does not match the chosen number of replicas.\n**Potential Resolution:**\nThe deployment's health checks are failing. The container image might be bad or the health check might be misconfigured. The rolling replacement of Pods waits until the newly started Pod passes its Pod Readiness gate. This only occurs if the Pod is responding to load balancer health checks. If the Pod does not respond, or if the health check is misconfigured, the readiness gate conditions can't be met and the rollout can't continue.\n- If you're using kubectl 1.13 or higher, you can check the status of a Pod's readiness gates with the following command:```\nkubectl get my-Pod -o wide\n```Check the **READINESS GATES** column.This column doesn't exist in kubectl 1.12 and lower. A Pod that is marked as being in the READY state may have a failed readiness gate. To verify this, use the following command:```\nkubectl get my-pod -o yaml\n```The readiness gates and their status are listed in the output.\n- Verify that the container image in your Deployment's Pod specification is functioning correctly and is able to respond to health checks.\n- Verify that the health checks are correctly configured.\n### NEG is not garbage collected\n**Symptom:** A NEG that should have been deleted still exists.\n**Potential Resolution:**\n- The NEG is not garbage collected if the NEG is referenced by a backend service. See [Preventing leaked NEGs](#garbage_collection) for details.\n- If using 1.18 or later, you can check for events in the`ServiceNetworkEndpointGroup`resource using [the service neg procedure](#svcneg) .\n- Check to see if the NEG is still needed by a service. Check the`svcneg`resource for the service that corresponds to the NEG and check if a Service annotation exists.\n### NEG is not synced with Service\n**Symptom:** Expected (Pod IP) endpoints don't exist in the NEG, the NEG is not synchronized, or the error `Failed to sync NEG_NAME (will not retry): neg name NEG_NAME is already in use, found a custom named neg with an empty description`\n**Potential Resolution:**\nIf you are using GKE 1.18 or later, [check the svcneg resource](#svcneg) for information:\n- Check the`status.lastSyncTime`value to verify if the NEG has synced recently.\n- Check the`Synced`condition for any errors that occurred in the most recent sync.\nIf you are using GKE 1.19.9 or later, check whether there exists a NEG whose name and zone match the name and zone of the NEG that the GKE NEG controller needs to create. For example, a NEG with the name that the NEG controller needs to use might have been created using gcloud CLI or the Google Cloud console in the cluster's zone (or one of the cluster's zones). In this case, you must delete the existing NEG before the NEG controller can synchronize its endpoints. Standalone NEG creation and membership are designed to be managed by the NEG controller.\n## What's next\n- [Zonal network endpoint groups overview](/load-balancing/docs/negs/zonal-neg-concepts) \n- [Backend Services](/load-balancing/docs/backend-service) \n- [Creating Health Checks](/load-balancing/docs/health-checks) \n- [Using Target Proxies](/load-balancing/docs/target-proxies) \n- [Using Forwarding Rules](/load-balancing/docs/forwarding-rules)", "guide": "Google Kubernetes Engine (GKE)"}