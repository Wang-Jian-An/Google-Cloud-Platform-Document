{"title": "Google Kubernetes Engine (GKE) - Serve scalable LLMs on GKE using TorchServe", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/scalable-ml-models-torchserve", "abstract": "# Google Kubernetes Engine (GKE) - Serve scalable LLMs on GKE using TorchServe\nThis tutorial shows you how to serve a pre-trained PyTorch machine learning (ML) model on a GKE cluster using the [TorchServe framework](https://pytorch.org/serve/) . The ML model used in this tutorial generates predictions based on user requests. You can use the information in this tutorial to help you to deploy and serve your own models at scale on GKE.", "content": "## About the tutorial applicationThe application is a small Python web application created using the [Fast Dash framework](https://fastdash.app/) . You use the application to send prediction requests to the T5 model. This application captures user text inputs and language pairs and sends the information to the model. The model translates the text and returns the result to the application, which displays the result to the user. For more information about Fast Dash, see [the Fast Dash documentation](https://docs.fastdash.app/) .\n### How it worksThis tutorial deploys the workloads on a GKE Autopilot cluster. GKE fully manages Autopilot nodes, which reduces administrative overhead for node configuration, scaling, and upgrades. When you deploy the ML workload and application on Autopilot, GKE chooses the correct underlying machine type and size to run the workloads. For more information, see the [Autopilot overview](/kubernetes-engine/docs/concepts/autopilot-overview) .\nAfter you deploy the model, you get a prediction URL that your application can use to send prediction requests to the model. This method decouples the model from the application, allowing the model to scale independently of the web application.## Objectives\n- Prepare a pre-trained T5 model from the [Hugging Face](https://huggingface.co/) repository for serving by packaging it as a container image and pushing it to Artifact Registry\n- Deploy the model to an Autopilot cluster\n- Deploy the Fast Dash application that communicates with the model\n- Autoscale the model based on Prometheus metrics\n## CostsIn this document, you use the following billable components of Google Cloud:- [GKE Autopilot mode](/kubernetes-engine/pricing#gpu-pods) \n- [Artifact Registry](/artifact-registry/pricing) \n- [Cloud Storage](/storage/pricing) \n- [Cloud Build](/build/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n## Prepare the environmentClone the example repository and open the tutorial directory:\n```\ngit clone https://github.com/GoogleCloudPlatform/kubernetes-engine-samples.gitcd kubernetes-engine-samples/ai-ml/t5-model-serving\n```## Create the clusterRun the following command:\n```\ngcloud container clusters create-auto ml-cluster \\\u00a0 \u00a0 --release-channel=RELEASE_CHANNEL \\\u00a0 \u00a0 --cluster-version=CLUSTER_VERSION \\\u00a0 \u00a0 --location=us-central1\n```\nReplace the following:- ``: the release channel for your cluster. Must be one of`rapid`,`regular`, or`stable`. Choose a channel that has GKE version 1.28.3-gke.1203000 or later to use L4 GPUs. To see the versions available in a specific channel, see [View the default and available versions for release channels](/kubernetes-engine/docs/concepts/release-channels#viewing_the_default_and_available_versions_for_release_channels) .\n- ``: the GKE version to use. Must be`1.28.3-gke.1203000`or later.\nThis operation takes several minutes to complete.## Create an Artifact Registry repository\n- Create a new Artifact Registry standard repository with the Docker format in the same region as your cluster:```\ngcloud artifacts repositories create models \\\u00a0 \u00a0 --repository-format=docker \\\u00a0 \u00a0 --location=us-central1 \\\u00a0 \u00a0 --description=\"Repo for T5 serving image\"\n```\n- Verify the repository name:```\ngcloud artifacts repositories describe models \\\u00a0 \u00a0 --location=us-central1\n```The output is similar to the following:```\nEncryption: Google-managed key\nRepository Size: 0.000MB\ncreateTime: '2023-06-14T15:48:35.267196Z'\ndescription: Repo for T5 serving image\nformat: DOCKER\nmode: STANDARD_REPOSITORY\nname: projects/PROJECT_ID/locations/us-central1/repositories/models\nupdateTime: '2023-06-14T15:48:35.267196Z'\n```\n## Package the modelIn this section, you package the model and the serving framework in a single container image using Cloud Build and push the resulting image to the Artifact Registry repository.- Review the Dockerfile for the container image: [  ai-ml/t5-model-serving/model/Dockerfile ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/t5-model-serving/model/Dockerfile) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/t5-model-serving/model/Dockerfile) ```\n# Copyright 2023 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 https://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.ARG BASE_IMAGE=pytorch/torchserve:0.7.1-cpuFROM alpine/gitARG MODEL_NAME=t5-smallARG MODEL_REPO=https://huggingface.co/${MODEL_NAME}ENV MODEL_NAME=${MODEL_NAME}ENV MODEL_VERSION=${MODEL_VERSION}RUN git clone \"${MODEL_REPO}\" /modelFROM ${BASE_IMAGE}ARG MODEL_NAME=t5-smallARG MODEL_VERSION=1.0ENV MODEL_NAME=${MODEL_NAME}ENV MODEL_VERSION=${MODEL_VERSION}COPY --from=0 /model/. /home/model-server/COPY handler.py \\\u00a0 \u00a0 \u00a0model.py \\\u00a0 \u00a0 \u00a0requirements.txt \\\u00a0 \u00a0 \u00a0setup_config.json /home/model-server/RUN \u00a0torch-model-archiver \\\u00a0 \u00a0 \u00a0--model-name=\"${MODEL_NAME}\" \\\u00a0 \u00a0 \u00a0--version=\"${MODEL_VERSION}\" \\\u00a0 \u00a0 \u00a0--model-file=\"model.py\" \\\u00a0 \u00a0 \u00a0--serialized-file=\"pytorch_model.bin\" \\\u00a0 \u00a0 \u00a0--handler=\"handler.py\" \\\u00a0 \u00a0 \u00a0--extra-files=\"config.json,spiece.model,tokenizer.json,setup_config.json\" \\\u00a0 \u00a0 \u00a0--runtime=\"python\" \\\u00a0 \u00a0 \u00a0--export-path=\"model-store\" \\\u00a0 \u00a0 \u00a0--requirements-file=\"requirements.txt\"FROM ${BASE_IMAGE}ENV PATH /home/model-server/.local/bin:$PATHENV TS_CONFIG_FILE /home/model-server/config.properties# CPU inference will throw a warning cuda warning (not error)# Could not load dynamic library 'libnvinfer_plugin.so.7'# This is expected behaviour. see: https://stackoverflow.com/a/61137388ENV TF_CPP_MIN_LOG_LEVEL 2COPY --from=1 /home/model-server/model-store/ /home/model-server/model-storeCOPY config.properties /home/model-server/\n```This Dockerfile defines the following multiple stage build process:- Download the model artifacts from the Hugging Face repository.\n- Package the model using the [PyTorch Serving Archive](https://github.com/pytorch/serve/tree/master/model-archiver) tool. This creates a model archive (.mar) file that the inference server uses to load the model.\n- Build the final image with PyTorch Serve.\n- Build and push the image using Cloud Build:```\ngcloud builds submit model/ \\\u00a0 \u00a0 --region=us-central1 \\\u00a0 \u00a0 --config=model/cloudbuild.yaml \\\u00a0 \u00a0 --substitutions=_LOCATION=us-central1,_MACHINE=gpu,_MODEL_NAME=t5-small,_MODEL_VERSION=1.0\n```The build process takes several minutes to complete. If you use a larger model size than `t5-small` , the build process might take **significantly** more time.\n- Check that the image is in the repository:```\ngcloud artifacts docker images list us-central1-docker.pkg.dev/PROJECT_ID/models\n```Replace `` with your Google Cloud project ID.The output is similar to the following:```\nIMAGE              DIGEST   CREATE_TIME   UPDATE_TIME\nus-central1-docker.pkg.dev/PROJECT_ID/models/t5-small  sha256:0cd... 2023-06-14T12:06:38 2023-06-14T12:06:38\n```\n## Deploy the packaged model to GKETo deploy the image, modify the Kubernetes manifest in the example repository to match your environment.- Review the manifest for the inference workload: [  ai-ml/t5-model-serving/kubernetes/serving-gpu.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/t5-model-serving/kubernetes/serving-gpu.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/t5-model-serving/kubernetes/serving-gpu.yaml) ```\n# Copyright 2023 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 https://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.---apiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: t5-inference\u00a0 labels:\u00a0 \u00a0 model: t5\u00a0 \u00a0 version: v1.0\u00a0 \u00a0 machine: gpuspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 model: t5\u00a0 \u00a0 \u00a0 version: v1.0\u00a0 \u00a0 \u00a0 machine: gpu\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 model: t5\u00a0 \u00a0 \u00a0 \u00a0 version: v1.0\u00a0 \u00a0 \u00a0 \u00a0 machine: gpu\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4\u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 fsGroup: 1000\u00a0 \u00a0 \u00a0 \u00a0 runAsUser: 1000\u00a0 \u00a0 \u00a0 \u00a0 runAsGroup: 1000\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 \u00a0 - name: inference\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image: us-central1-docker.pkg.dev/PROJECT_ID/models/t5-small:1.0-gpu\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 imagePullPolicy: IfNotPresent\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 args: [\"torchserve\", \"--start\", \"--foreground\"]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: \"1\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"3000m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: 16Gi\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: 10Gi\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: \"1\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"3000m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: 16Gi\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: 10Gi\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8080\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: http\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8081\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: management\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8082\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: metrics\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readinessProbe:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 httpGet:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path: /ping\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port: http\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 initialDelaySeconds: 120\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 failureThreshold: 10\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 livenessProbe:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 httpGet:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path: /models/t5-small\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port: management\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 initialDelaySeconds: 150\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 periodSeconds: 5---apiVersion: v1kind: Servicemetadata:\u00a0 name: t5-inference\u00a0 labels:\u00a0 \u00a0 model: t5\u00a0 \u00a0 version: v1.0\u00a0 \u00a0 machine: gpuspec:\u00a0 type: ClusterIP\u00a0 selector:\u00a0 \u00a0 model: t5\u00a0 \u00a0 version: v1.0\u00a0 \u00a0 machine: gpu\u00a0 ports:\u00a0 \u00a0 - port: 8080\u00a0 \u00a0 \u00a0 name: http\u00a0 \u00a0 \u00a0 targetPort: http\u00a0 \u00a0 - port: 8081\u00a0 \u00a0 \u00a0 name: management\u00a0 \u00a0 \u00a0 targetPort: management\u00a0 \u00a0 - port: 8082\u00a0 \u00a0 \u00a0 name: metrics\u00a0 \u00a0 \u00a0 targetPort: metrics\n```\n- Replace `` with your Google Cloud project ID:```\nsed -i \"s/PROJECT_ID/PROJECT_ID/g\" \"kubernetes/serving-gpu.yaml\"\n```This ensures that the container image path in the Deployment specification matches the path to your T5 model image in Artifact Registry.\n- Create the Kubernetes resources:```\nkubectl create -f kubernetes/serving-gpu.yaml\n```\nTo verify that the model deployed successfully, do the following:- Get the status of the Deployment and the Service:```\nkubectl get -f kubernetes/serving-gpu.yaml\n```Wait until the output shows ready Pods, similar to the following. Depending on the size of the image, the first image pull might take several minutes.```\nNAME       READY UP-TO-DATE AVAILABLE AGE\ndeployment.apps/t5-inference 1/1  1    0   66s\nNAME     TYPE  CLUSTER-IP  EXTERNAL-IP PORT(S)      AGE\nservice/t5-inference ClusterIP 10.48.131.86 <none>  8080/TCP,8081/TCP,8082/TCP 66s\n```\n- Open a local port for the `t5-inference` Service:```\nkubectl port-forward svc/t5-inference 8080\n```\n- Open a new terminal window and send a test request to the Service:```\ncurl -v -X POST -H 'Content-Type: application/json' -d '{\"text\": \"this is a test sentence\", \"from\": \"en\", \"to\": \"fr\"}' \"http://localhost:8080/predictions/t5-small/1.0\"\n```If the test request fails and the Pod connection closes, check the logs:```\nkubectl logs deployments/t5-inference\n```If the output is similar to the following, TorchServe failed to install some model dependencies:```\norg.pytorch.serve.archive.model.ModelException: Custom pip package installation failed for t5-small\n```To resolve this issue, restart the Deployment:```\nkubectl rollout restart deployment t5-inference\n```The Deployment controller creates a new Pod. Repeat the previous steps to open a port on the new Pod.\n## Access the deployed model using the web application\n- Build and push the Fast Dash web application as a container image in Artifact Registry:```\ngcloud builds submit client-app/ \\\u00a0 \u00a0 --region=us-central1 \\\u00a0 \u00a0 --config=client-app/cloudbuild.yaml\n```\n- Open `kubernetes/application.yaml` in a text editor and replace `` in the `image:` field with your project ID. Alternatively, run the following command:```\nsed -i \"s/PROJECT_ID/PROJECT_ID/g\" \"kubernetes/application.yaml\"\n```\n- Create the Kubernetes resources:```\nkubectl create -f kubernetes/application.yaml\n```The Deployment and Service might take some time to fully provision.\n- To check the status, run the following command:```\nkubectl get -f kubernetes/application.yaml\n```Wait until the output shows ready Pods, similar to the following:```\nNAME      READY UP-TO-DATE AVAILABLE AGE\ndeployment.apps/fastdash 1/1  1   0   1m\nNAME    TYPE  CLUSTER-IP  EXTERNAL-IP PORT(S)   AGE\nservice/fastdash NodePort 203.0.113.12 <none>  8050/TCP   1m\n```\n- The web application is now running, although it isn't exposed on an external IP address. To access the web application, open a local port:```\nkubectl port-forward service/fastdash 8050\n```\n- In a browser, open the web interface:- If you're using a local shell, open a browser and go to http://127.0.0.1:8050.\n- If you're using Cloud Shell, click **Web preview** , and then click **Change port** . Specify port`8050`.\n- To send a request to the T5 model, specify values in the **TEXT** , **FROM LANG** , and **TO LANG** fields in the web interface and click **Submit** . For a list of available languages, see the [T5 documentation](https://huggingface.co/t5-small#model-details) .\n## Enable autoscaling for the modelThis section shows you how to enable autoscaling for the model based on metrics from [Google Cloud Managed Service for Prometheus](/stackdriver/docs/managed-prometheus) by doing the following:- Install Custom Metrics Stackdriver Adapter\n- Apply PodMonitoring and HorizontalPodAutoscaling configurations\nGoogle Cloud Managed Service for Prometheus is enabled by default in Autopilot clusters running version 1.25 and later.\n### Install Custom Metrics Stackdriver AdapterThis adapter lets your cluster use metrics from Prometheus to make Kubernetes autoscaling decisions.- Deploy the adapter:```\nkubectl create -f https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-stackdriver/master/custom-metrics-stackdriver-adapter/deploy/production/adapter_new_resource_model.yaml\n```\n- Create an IAM service account for the adapter to use:```\ngcloud iam service-accounts create monitoring-viewer\n```\n- Grant the IAM service account the `monitoring.viewer` role on the project and the `iam.workloadIdentityUser` role:```\ngcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member \"serviceAccount:monitoring-viewer@PROJECT_ID.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role roles/monitoring.viewergcloud iam service-accounts add-iam-policy-binding monitoring-viewer@PROJECT_ID.iam.gserviceaccount.com \\\u00a0 \u00a0 --role roles/iam.workloadIdentityUser \\\u00a0 \u00a0 --member \"serviceAccount:PROJECT_ID.svc.id.goog[custom-metrics/custom-metrics-stackdriver-adapter]\"\n```Replace `` with your Google Cloud project ID.\n- Annotate the Kubernetes ServiceAccount of the adapter to let it impersonate the IAM service account:```\nkubectl annotate serviceaccount custom-metrics-stackdriver-adapter \\\u00a0 \u00a0 --namespace custom-metrics \\\u00a0 \u00a0 iam.gke.io/gcp-service-account=monitoring-viewer@PROJECT_ID.iam.gserviceaccount.com\n```\n- Restart the adapter to propagate the changes:```\nkubectl rollout restart deployment custom-metrics-stackdriver-adapter \\\u00a0 \u00a0 --namespace=custom-metrics\n```\n### Apply PodMonitoring and HorizontalPodAutoscaling configurationsPodMonitoring is a Google Cloud Managed Service for Prometheus custom resource that enables metrics ingestion and target scraping in a specific namespace.- Deploy the PodMonitoring resource in the same namespace as the TorchServe Deployment:```\nkubectl apply -f kubernetes/pod-monitoring.yaml\n```\n- Review the HorizontalPodAutoscaler manifest: [  ai-ml/t5-model-serving/kubernetes/hpa.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/t5-model-serving/kubernetes/hpa.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/t5-model-serving/kubernetes/hpa.yaml) ```\n# Copyright 2023 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 https://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.apiVersion: autoscaling/v2kind: HorizontalPodAutoscalermetadata:\u00a0 name: t5-inferencespec:\u00a0 scaleTargetRef:\u00a0 \u00a0 apiVersion: apps/v1\u00a0 \u00a0 kind: Deployment\u00a0 \u00a0 name: t5-inference\u00a0 minReplicas: 1\u00a0 maxReplicas: 5\u00a0 metrics:\u00a0 - type: Pods\u00a0 \u00a0 pods:\u00a0 \u00a0 \u00a0 metric:\u00a0 \u00a0 \u00a0 \u00a0 name: prometheus.googleapis.com|ts_queue_latency_microseconds|counter\u00a0 \u00a0 \u00a0 target:\u00a0 \u00a0 \u00a0 \u00a0 type: AverageValue\u00a0 \u00a0 \u00a0 \u00a0 averageValue: \"30000\"\n```The HorizontalPodAutoscaler scales the T5 model Pod quantity based on the cumulative duration of the request queue. Autoscaling is based on the `ts_queue_latency_microseconds` metric, which shows cumulative queue duration in microseconds.\n- Create the HorizontalPodAutoscaler:```\nkubectl apply -f kubernetes/hpa.yaml\n```\n## Verify autoscaling using a load generatorTo test your autoscaling configuration, generate load for the serving application. This tutorial uses a Locust load generator to send requests to the prediction endpoint for the model.- Create the load generator:```\nkubectl apply -f kubernetes/loadgenerator.yaml\n```Wait for the load generator Pods to become ready.\n- Expose the load generator web interface locally:```\nkubectl port-forward svc/loadgenerator 8080\n```If you see an error message, try again when the Pod is running.\n- In a browser, open the load generator web interface:- If you're using a local shell, open a browser and go to http://127.0.0.1:8080.\n- If you're using Cloud Shell, click **Web preview** , and then click **Change port** . Enter port`8080`.\n- Click the **Charts** tab to observe performance over time.\n- Open a new terminal window and watch the replica count of your horizontal Pod autoscalers:```\nkubectl get hpa -w\n```The number of replicas increases as the load increases. The scaleup might take approximately ten minutes. As new replicas start, the number of successful requests in the Locust chart increases.```\nNAME   REFERENCE     TARGETS   MINPODS MAXPODS REPLICAS AGE\nt5-inference Deployment/t5-inference 71352001470m/7M 1   5  1   2m11s\n```\n **Note:** The Locust framework reuses opened connections, so you can't see changes in a chart after a scale-up event. However, you can stop a current run of the load generator by clicking **Stop** , and then start a new load generation test to observe the increase in requests per second.## Recommendations\n- Build your model with the same version of the base Docker image that you'll use for serving.\n- If your model has special package dependencies, or if the size of your dependencies is large, create a custom version of your base Docker image.\n- Watch the tree version of your model dependency packages. Ensure that your package dependencies support each others' versions. For example, Panda version 2.0.3 supports NumPy version 1.20.3 and later.\n- Run GPU-intensive models on GPU nodes and CPU-intensive models on CPU. This could improve the stability of model serving and ensures that you're efficiently consuming node resources.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- Delete a Google Cloud project:\n- ```\ngcloud projects delete PROJECT_ID\n```\n### Delete individual resources\n- Delete the Kubernetes resources:```\nkubectl delete -f kubernetes/loadgenerator.yamlkubectl delete -f kubernetes/hpa.yamlkubectl delete -f kubernetes/pod-monitoring.yamlkubectl delete -f kubernetes/application.yamlkubectl delete -f kubernetes/serving-gpu.yamlkubectl delete -f https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-stackdriver/master/custom-metrics-stackdriver-adapter/deploy/production/adapter_new_resource_model.yaml\n```\n- Delete the GKE cluster:```\ngcloud container clusters delete \"ml-cluster\" \\\u00a0 \u00a0 --location=\"us-central1\" --quiet\n```\n- Delete the IAM service account and IAM policy bindings:```\ngcloud projects remove-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member \"serviceAccount:monitoring-viewer@PROJECT_ID.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role roles/monitoring.viewergcloud iam service-accounts remove-iam-policy-binding monitoring-viewer@PROJECT_ID.iam.gserviceaccount.com \\\u00a0 \u00a0 --role roles/iam.workloadIdentityUser \\\u00a0 \u00a0 --member \"serviceAccount:PROJECT_ID.svc.id.goog[custom-metrics/custom-metrics-stackdriver-adapter]\"gcloud iam service-accounts delete monitoring-viewer\n```\n- Delete the images in Artifact Registry. Optionally, delete the entire repository. For instructions, see the Artifact Registry documentation about [Deleting images](/artifact-registry/docs/docker/manage-images) .\n## Component overviewThis section describes the components used in this tutorial, such as the model, the web application, the framework, and the cluster.\n### About the T5 modelThis tutorial uses a pre-trained multilingual . T5 is a text-to-text transformer that converts text from one language to another. In T5, inputs and outputs are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. The T5 model can also be used for tasks such as summarization, Q&A, or text classification. The model is trained on a large quantity of text from [Colossal Clean Crawled Corpus (C4)](https://huggingface.co/datasets/c4) and [Wiki-DPR](https://huggingface.co/datasets/wiki_dpr) .\nFor more information, see [the T5 model documentation](https://huggingface.co/docs/transformers/model_doc/t5) .\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu presented the T5 model in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](http://jmlr.org/papers/v21/20-074.html) , published in the .\nThe T5 model supports various , with different levels of complexity that suit specific use cases. This tutorial uses the default size, `t5-small` , but you can also choose a different size. The following T5 sizes are distributed under the Apache 2.0 license:- [t5-small](https://huggingface.co/t5-small) : 60 million parameters\n- [t5-base](https://huggingface.co/t5-base) : 220 million parameters\n- [t5-large](https://huggingface.co/t5-large) : 770 million parameters. 3GB download.\n- [t5-3b](https://huggingface.co/t5-3b) : 3 billion parameters. 11GB download.\n- [t5-11b](https://huggingface.co/t5-11b) : 11 billion parameters. 45GB download.\nFor other available T5 models, see the [Hugging Face repository](https://huggingface.co/models?license=license:apache-2.0&other=t5&sort=downloads) .\n### About TorchServeTorchServe is a flexible tool for serving PyTorch models. It provides out of the box support for all major deep learning frameworks, including PyTorch, TensorFlow, and ONNX. TorchServe can be used to deploy models in production, or for rapid prototyping and experimentation.## What's next\n- [Serve an LLM with multiple GPUs](/kubernetes-engine/docs/tutorials/serve-multiple-gpu) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Google Kubernetes Engine (GKE)"}