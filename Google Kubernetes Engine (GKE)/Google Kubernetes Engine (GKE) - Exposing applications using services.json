{"title": "Google Kubernetes Engine (GKE) - Exposing applications using services", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/exposing-apps", "abstract": "# Google Kubernetes Engine (GKE) - Exposing applications using services\nThis page shows how to create Kubernetes Services in a Google Kubernetes Engine (GKE) cluster. For an explanation of the Service concept and a discussion of the various types of Services, see [Service](/kubernetes-engine/docs/concepts/service) .\n", "content": "## Introduction\nThe idea of a [Service](https://kubernetes.io/docs/concepts/services-networking/service/) is to group a set of Pod endpoints into a single resource. You can configure various ways to access the grouping. By default, you get a stable cluster IP address that clients inside the cluster can use to contact Pods in the Service. A client sends a request to the stable IP address, and the request is routed to one of the Pods in the Service.\nThere are five types of Services:\n- ClusterIP (default)\n- NodePort\n- LoadBalancer\n- ExternalName\n- Headless\nAutopilot clusters are public by default. If you opt for a [private](/kubernetes-engine/docs/concepts/private-cluster-concept) Autopilot cluster, you must configure [Cloud NAT](/nat/docs/set-up-network-address-translation) to make outbound internet connections, for example pulling images from DockerHub.\nThis topic has several exercises. In each exercise, you create a Deployment and expose its Pods by creating a Service. Then you send an HTTP request to the Service.\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n[Create a GKE cluster](/kubernetes-engine/docs/how-to/creating-an-autopilot-cluster)\n## Creating a Service of type ClusterIP\nIn this section, you create a Service of type [ClusterIP](/kubernetes-engine/docs/concepts/service#services_of_type_clusterip) .\nHere is a manifest for a Deployment:\n```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: my-deploymentspec:\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: metrics\u00a0 \u00a0 \u00a0 department: sales\u00a0 replicas: 3\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: metrics\u00a0 \u00a0 \u00a0 \u00a0 department: sales\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: hello\u00a0 \u00a0 \u00a0 \u00a0 image: \"us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0\"\n```\nCopy the manifest to a file named `my-deployment.yaml` , and create the Deployment:\n```\nkubectl apply -f my-deployment.yaml\n```\nVerify that three Pods are running:\n```\nkubectl get pods\n```\nThe output shows three running Pods:\n```\nNAME       READY STATUS RESTARTS AGE\nmy-deployment-dbd86c8c4-h5wsf 1/1  Running 0   7s\nmy-deployment-dbd86c8c4-qfw22 1/1  Running 0   7s\nmy-deployment-dbd86c8c4-wt4s6 1/1  Running 0   7s\n```\nHere is a manifest for a Service of type `ClusterIP` :\n```\napiVersion: v1kind: Servicemetadata:\u00a0 name: my-cip-servicespec:\u00a0 type: ClusterIP\u00a0 # Uncomment the below line to create a Headless Service\u00a0 # clusterIP: None\u00a0 selector:\u00a0 \u00a0 app: metrics\u00a0 \u00a0 department: sales\u00a0 ports:\u00a0 - protocol: TCP\u00a0 \u00a0 port: 80\u00a0 \u00a0 targetPort: 8080\n```\nThe Service has a selector that specifies two labels:- `app: metrics`\n- `department: sales`\nEach Pod in the Deployment that you created previously has those two labels. So the Pods in the Deployment will become members of this Service.\nCopy the manifest to a file named `my-cip-service.yaml` , and create the Service:\n```\nkubectl apply -f my-cip-service.yaml\n```\nWait a moment for Kubernetes to assign a stable internal address to the Service, and then view the Service:\n```\nkubectl get service my-cip-service --output yaml\n```\nThe output shows a value for `clusterIP` :\n```\nspec:\n clusterIP: 10.59.241.241\n```\nMake a note of your `clusterIP` value for later.\n### Create a Deployment\n- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload) \n- Click **Deploy** .\n- Under **Specify container** , select **Existing container image** .\n- For **Image path** , enter `us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0`\n- Click **Done** , then click **Continue** .\n- Under **Configuration** , for **Application name** , enter `my-deployment` .\n- Under **Labels** , create the following labels:- **Key:** `app`and **Value:** `metrics`\n- **Key:** `department`and **Value:** `sales`\n- Under **Cluster** , choose the cluster in which you want to create the Deployment.\n- Click **Deploy** .\n- When your Deployment is ready, the **Deployment details** page opens. Under **Managed pods** , you can see that your Deployment has one or more running Pods.\n### Create a Service to expose your Deployment\n- On the **Deployment details** page, click **Actions > Expose** .\n- In the **Expose** dialog, under **Port mapping** , set the following values:- **Port:** `80`\n- **Target port:** `8080`\n- **Protocol:** `TCP`\n- From the **Service type** drop-down list, select **Cluster IP** .\n- Click **Expose** .\n- When your Service is ready, the **Service details** page opens, and you can see details about your Service. Under **Cluster IP** , make a note of the IP address that Kubernetes assigned to your Service. This is the IP address that internal clients can use to call the Service.\n **Note:** Creating a Headless Service is not currently available though the Console.\n### Accessing your Service\nList your running Pods:\n```\nkubectl get pods\n```\nIn the output, copy one of the Pod names that begins with `my-deployment` .\n```\nNAME       READY STATUS RESTARTS AGE\nmy-deployment-dbd86c8c4-h5wsf 1/1  Running 0   2m51s\n```\nGet a shell into one of your running containers:\n```\nkubectl exec -it POD_NAME -- sh\n```\nReplace `` with the name of one of the Pods in `my-deployment` .\nIn your shell, install `curl` :\n```\napk add --no-cache curl\n```\nIn the container, make a request to your Service by using your cluster IP address and port 80. Notice that 80 is the value of the `port` field of your Service. This is the port that you use as a client of the Service.\n```\ncurl CLUSTER_IP:80\n```\nReplace `` with the value of `clusterIP` in your Service.\nYour request is forwarded to one of the member Pods on TCP port 8080, which is the value of the `targetPort` field. Note that each of the Service's member Pods must have a container listening on port 8080.\nThe response shows the output of `hello-app` :\n```\nHello, world!\nVersion: 2.0.0\nHostname: my-deployment-dbd86c8c4-h5wsf\n```\nTo exit the shell to your container, enter `exit` .\n**Note:** You need to know ahead of time that each of your member Pods has a container listening on TCP port 8080. In this exercise, you did not do anything to make the containers listen on port 8080. You can see that `hello-app` listens on port 8080 by looking at the [Dockerfile and the source code](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/tree/main/quickstarts/hello-app) for the app.\n## Creating a Service of type NodePort\nIn this section, you create a Service of type [NodePort](/kubernetes-engine/docs/concepts/service#service_of_type_nodeport) .\nHere is a manifest for a Deployment:\n```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: my-deployment-50000spec:\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: metrics\u00a0 \u00a0 \u00a0 department: engineering\u00a0 replicas: 3\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: metrics\u00a0 \u00a0 \u00a0 \u00a0 department: engineering\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: hello\u00a0 \u00a0 \u00a0 \u00a0 image: \"us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0\"\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: \"PORT\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"50000\"\n```\nNotice the `env` object in the manifest. The `env` object specifies that the `PORT` environment variable for the running container will have a value of `50000` . The `hello-app` application listens on the port specified by the `PORT` environment variable. So in this exercise, you are telling the container to listen on port 50000.\nCopy the manifest to a file named `my-deployment-50000.yaml` , and create the Deployment:\n```\nkubectl apply -f my-deployment-50000.yaml\n```\nVerify that three Pods are running:\n```\nkubectl get pods\n```\nHere is a manifest for a Service of type NodePort:\n```\napiVersion: v1kind: Servicemetadata:\u00a0 name: my-np-servicespec:\u00a0 type: NodePort\u00a0 selector:\u00a0 \u00a0 app: metrics\u00a0 \u00a0 department: engineering\u00a0 ports:\u00a0 - protocol: TCP\u00a0 \u00a0 port: 80\u00a0 \u00a0 targetPort: 50000\n```\nCopy the manifest to a file named `my-np-service.yaml` , and create the Service:\n```\nkubectl apply -f my-np-service.yaml\n```\nView the Service:\n```\nkubectl get service my-np-service --output yaml\n```\nThe output shows a `nodePort` value:\n```\n...\n spec:\n ...\n ports:\n - nodePort: 30876\n  port: 80\n  protocol: TCP\n  targetPort: 50000\n selector:\n  app: metrics\n  department: engineering\n sessionAffinity: None\n type: NodePort\n...\n```\nCreate a firewall rule to allow TCP traffic on your node port:\n```\ngcloud compute firewall-rules create test-node-port \\\u00a0 \u00a0 --allow tcp:NODE_PORT\n```\nReplace `` with the value of the `nodePort` field of your Service.\n### Create a Deployment\n- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload) \n- Click **Deploy** .\n- Under **Specify container** , select **Existing container image** .\n- For **Image path** , enter `us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0` .\n- Click **Add Environment Variable** .\n- For **Key** , enter `PORT` , and for **Value** , enter `50000` .\n- Click **Done** , then click **Continue** .\n- Under **Configuration** , for **Application name** , enter `my-deployment-50000` .\n- Under **Labels** , create the following labels:- **Key:** `app`and **Value:** `metrics`\n- **Key:** `department`and **Value:** `engineering`\n- Under **Cluster** , choose the cluster in which you want to create the Deployment.\n- Click **Deploy** .\n- When your Deployment is ready, the **Deployment details** page opens. Under **Managed pods** , you can see that your Deployment has one or more running Pods.\n### Create a Service to expose your Deployment\n- On the **Deployment details** page, click **Actions > Expose** .\n- In the **Expose** dialog, under **Port mapping** , set the following values:- **Port:** `80`\n- **Target port:** `50000`\n- **Protocol:** `TCP`\n- From the **Service type** drop-down list, select **Node port** .\n- Click **Expose** .\n- When your Service is ready, the **Service details** page opens, and you can see details about your Service. Under **Ports** , make a note of the **Node Port** that Kubernetes assigned to your Service.\n### Create a firewall rule for your node port\n- Go to the **Firewall policies** page in the Google Cloud console. [Go to Firewall policies](https://console.cloud.google.com/net-security/firewall-manager/firewall-policies/list) \n- Click **Create firewall rule** .\n- For **Name** , enter `test-node-port` .\n- From the **Targets** drop-down list, select **All instances in the network** .\n- For **Source IPv4 ranges** , enter `0.0.0.0/0` .\n- Under **Protocols and ports** , select **Specified protocols and ports** .\n- Select the **tcp** checkbox, and enter the node port value you noted.\n- Click **Create** .\n### Get a node IP address\nFind the external IP address of one of your nodes:\n```\nkubectl get nodes --output wide\n```\nThe output is similar to the following:\n```\nNAME   STATUS ROLES  AGE VERSION  EXTERNAL-IP\ngke-svc-... Ready  none  1h  v1.9.7-gke.6 203.0.113.1\n```\nNot all clusters have external IP addresses for nodes. For example, the nodes in [private clusters](/kubernetes-engine/docs/how-to/private-clusters) do not have external IP addresses.\n### Access your Service\nIn your browser's address bar, enter the following:\n```\nNODE_IP_ADDRESS:NODE_PORT\n```\nReplace the following:\n- ``: the external IP address of one of your nodes, found when creating the service in the previous task.\n- ``: your node port value.\nThe output is similar to the following:\n```\nHello, world!\nVersion: 2.0.0\nHostname: my-deployment-50000-6fb75d85c9-g8c4f\n```\n## Creating a Service of type LoadBalancer\nIn this section, you create a Service of type [LoadBalancer](/kubernetes-engine/docs/concepts/service#services_of_type_loadbalancer) .\nHere is a manifest for a Deployment:\n```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: my-deployment-50001spec:\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: products\u00a0 \u00a0 \u00a0 department: sales\u00a0 replicas: 3\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: products\u00a0 \u00a0 \u00a0 \u00a0 department: sales\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: hello\u00a0 \u00a0 \u00a0 \u00a0 image: \"us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0\"\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: \"PORT\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"50001\"\n```\nNotice that the containers in this Deployment will listen on port 50001.\nCopy the manifest to a file named `my-deployment-50001.yaml` , and create the Deployment:\n```\nkubectl apply -f my-deployment-50001.yaml\n```\nVerify that three Pods are running:\n```\nkubectl get pods\n```\nHere is a manifest for a Service of type `LoadBalancer` :\n```\napiVersion: v1kind: Servicemetadata:\u00a0 name: my-lb-servicespec:\u00a0 type: LoadBalancer\u00a0 selector:\u00a0 \u00a0 app: products\u00a0 \u00a0 department: sales\u00a0 ports:\u00a0 - protocol: TCP\u00a0 \u00a0 port: 60000\u00a0 \u00a0 targetPort: 50001\n```\nCopy the manifest to a file named `my-lb-service.yaml,` and create the Service:\n```\nkubectl apply -f my-lb-service.yaml\n```\nWhen you create a Service of type `LoadBalancer` , a Google Cloud controller wakes up and configures an [external passthrough Network Load Balancer](/load-balancing/docs/network) . Wait a minute for the controller to configure the external passthrough Network Load Balancer and generate a stable IP address.\nView the Service:\n```\nkubectl get service my-lb-service --output yaml\n```\nThe output shows a stable external IP address under `loadBalancer:ingress` :\n```\n...\nspec:\n ...\n ports:\n - ...\n port: 60000\n protocol: TCP\n targetPort: 50001\n selector:\n app: products\n department: sales\n sessionAffinity: None\n type: LoadBalancer\nstatus:\n loadBalancer:\n ingress:\n - ip: 203.0.113.10\n```\n### Create a Deployment\n- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload) \n- Click **Deploy** .\n- Under **Specify container** , select **Existing container image** .\n- For **Image path** , enter `us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0` .\n- Click **Add Environment Variable** .\n- For **Key** , enter `PORT` , and for **Value** , enter `50001` .\n- Click **Done** , then click **Continue** .\n- Under **Configuration** , for **Application name** , enter `my-deployment-50001` .\n- Under **Labels** , create the following labels:- **Key:** `app`and **Value:** `products`\n- **Key:** `department`and **Value:** `sales`\n- Under **Cluster** , choose the cluster in which you want to create the Deployment.\n- Click **Deploy** .\n- When your Deployment is ready, the **Deployment details** page opens. Under **Managed pods** , you can see that your Deployment has one or more running Pods.\n### Create a Service to expose your Deployment\n- On the **Deployment details** page, click **Actions > Expose** .\n- In the **Expose** dialog, under **Port mapping** , set the following values:- **Port:** `60000`\n- **Target port:** `50001`\n- **Protocol:** `TCP`\n- From the **Service type** drop-down list, select **Load balancer** .\n- Click **Expose** .\n- When your Service is ready, the **Service details** page opens, and you can see details about your Service. Under **Load Balancer** , make a note of the load balancer's external IP address.\n### Access your Service\nWait a few minutes for GKE to configure the load balancer.\nIn your browser's address bar, enter the following:\n```\nLOAD_BALANCER_ADDRESS:60000\n```\nReplace `` with the external IP address of your load balancer.\nThe response shows the output of `hello-app` :\n```\nHello, world!\nVersion: 2.0.0\nHostname: my-deployment-50001-68bb7dfb4b-prvct\n```\nNotice that the value of `port` in a Service is arbitrary. The preceding example demonstrates this by using a `port` value of 60000.\n## Creating a Service of type ExternalName\nIn this section, you create a Service of type [ExternalName](/kubernetes-engine/docs/concepts/service#service_of_type_externalname) .\nA Service of type `ExternalName` provides an internal alias for an external DNS name. Internal clients make requests using the internal DNS name, and the requests are redirected to the external name.\nHere is a manifest for a Service of type `ExternalName` :\n```\napiVersion: v1kind: Servicemetadata:\u00a0 name: my-xn-servicespec:\u00a0 type: ExternalName\u00a0 externalName: example.com\n```\nIn the preceding example, the DNS name is my-xn-service.default.svc.cluster.local. When an internal client makes a request to my-xn-service.default.svc.cluster.local, the request gets redirected to example.com.\n## Using kubectl expose to create a Service\nAs an alternative to writing a Service manifest, you can create a Service by using `kubectl expose` to expose a Deployment.\nTo expose `my-deployment` , shown earlier in this topic, you could enter this command:\n```\nkubectl expose deployment my-deployment --name my-cip-service \\\u00a0 \u00a0 --type ClusterIP --protocol TCP --port 80 --target-port 8080\n```\nTo expose `my-deployment-50000` , show earlier in this topic, you could enter this command:\n```\nkubectl expose deployment my-deployment-50000 --name my-np-service \\\u00a0 \u00a0 --type NodePort --protocol TCP --port 80 --target-port 50000\n```\nTo expose `my-deployment-50001` , shown earlier in this topic, you could enter this command:\n```\nkubectl expose deployment my-deployment-50001 --name my-lb-service \\\u00a0 \u00a0 --type LoadBalancer --port 60000 --target-port 50001\n```\n## Cleaning up\nAfter completing the exercises on this page, follow these steps to remove resources and prevent unwanted charges incurring on your account:\n### Deleting your Services```\nkubectl delete services my-cip-service my-np-service my-lb-service\n```\n### Deleting your Deployments```\nkubectl delete deployments my-deployment my-deployment-50000 my-deployment-50001\n```\n### Deleting your firewall rule```\ngcloud compute firewall-rules delete test-node-port\n```\n### Deleting your Services\n- Go to the **Services** page in the Google Cloud console. [Go to Services](https://console.cloud.google.com/kubernetes/discovery) \n- Select the Services you created in this exercise, then click **Delete** .\n- When prompted to confirm, click **Delete** .\n### Deleting your Deployments\n- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload) \n- Select the Deployments you created in this exercise, then click **Delete** .\n- When prompted to confirm, select the **Delete Horizontal Pod Autoscalersassociated with selected Deployments** checkbox, then click **Delete** .\n### Deleting your firewall rule\n- Go to the **Firewall policies** page in the Google Cloud console. [Go to Firewall policies](https://console.cloud.google.com/net-security/firewall-manager/firewall-policies/list) \n- Select the **test-node-port** checkbox, then click **Delete** .\n- When prompted to confirm, click **Delete** .## What's next\n- [Services](/kubernetes-engine/docs/concepts/service) \n- [StatefulSets](/kubernetes-engine/docs/concepts/statefulset) \n- [Ingress](/kubernetes-engine/docs/concepts/ingress) \n- [HTTP Load Balancing with Ingress](/kubernetes-engine/docs/tutorials/http-balancer)", "guide": "Google Kubernetes Engine (GKE)"}