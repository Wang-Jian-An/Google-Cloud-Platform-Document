{"title": "Google Kubernetes Engine (GKE) - Configure workload separation in GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/workload-separation", "abstract": "# Google Kubernetes Engine (GKE) - Configure workload separation in GKE\nThis page shows you how to tell Google Kubernetes Engine (GKE) to schedule your Pods together, separately, or in specific locations.\nlets you use [taints and tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) to tell GKE to separate Pods onto different nodes, place Pods on nodes that meet specific criteria, or to schedule specific workloads together. What you need to do to configure workload separation depends on your GKE cluster configuration. The following table describes the differences:\n| Workload separation configuration    | Workload separation configuration.1                                                                              |\n|:-----------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Autopilot Standard with node auto-provisioning | Add a toleration for a specific key:value pair to your Pod specification, and select that key:value pair using a nodeSelector. GKE creates nodes, applies the corresponding node taint, and schedules the Pod on the node. For instructions, refer to Separate workloads in Autopilot clusters on this page.          |\n| Standard without node auto-provisioning  | Create a node pool with a node taint and a node label Add a toleration for that taint to the Pod specification For instructions, refer to Isolate your workloads in dedicated node pools. Caution: With this method, if existing tainted nodes don't have enough resources to support a Pod with a toleration, the Pod remains in the Pending state. |\nThis guide uses an example scenario in which you have two workloads, a batch job and a web server, that you want to separate from each other.\n", "content": "## When to use workload separation in GKE\nWorkload separation is useful when you have workloads that perform different roles and shouldn't run on the same underlying machines. Some example scenarios include the following:\n- You have a batch coordinator workload that creates Jobs that you want to keep separate.\n- You run a game server with a matchmaking workload that you want to separate from session Pods.\n- You want to separate parts of your stack from each other, such as separating a server from a database.\n- You want to separate some workloads for compliance or policy reasons.\n**Warning:** Workload separation should **never** be used as a primary security boundary. It is not a method of isolating untrusted workloads, and doesn't mitigate all escalation paths. Workload separation is not intended for use as a defense mechanism. To learn about the risks of using Kubernetes scheduling as an isolation method, see [Avoiding privilege escalation attacks](/kubernetes-engine/docs/how-to/isolate-workloads-dedicated-nodes#avoiding-privesc) .\n## Pricing\nIn Autopilot clusters, you're billed for the resources that your Pods request while running. For details, refer to [Autopilot pricing](/kubernetes-engine/pricing#autopilot_mode) . Pods that use workload separation have [higher minimum resource requests](/kubernetes-engine/docs/concepts/autopilot-resource-requests#workload-separation) enforced than regular Pods.\nIn Standard clusters, you're billed based on the hardware configuration and size of each node, regardless of whether Pods are running on the nodes. For details, refer to [Standard pricing](/kubernetes-engine/pricing#standard_mode) .\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Ensure that you have a GKE cluster. To learn how to create a cluster, use one of the following:- [Create an Autopilot cluster](/kubernetes-engine/docs/how-to/creating-an-autopilot-cluster) (recommended)\n- [Create a Standard cluster with node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning#enable) \n- [Create a Standard cluster](/kubernetes-engine/docs/how-to/creating-a-regional-cluster#create-regional-multi-zone-nodepool) \n## Separate workloads in Autopilot clusters\nTo separate workloads from each other, add a toleration and a node selector to each workload specification that defines the node on which the workload should run. This method also works on Standard clusters that have node auto-provisioning enabled.\n- Save the following manifest as `web-server.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: web-serverspec:\u00a0 replicas: 6\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 pod: nginx-pod\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 pod: nginx-pod\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 - key: group\u00a0 \u00a0 \u00a0 \u00a0 operator: Equal\u00a0 \u00a0 \u00a0 \u00a0 value: \"servers\"\u00a0 \u00a0 \u00a0 \u00a0 effect: NoSchedule\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 group: \"servers\"\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: web-server\u00a0 \u00a0 \u00a0 \u00a0 image: nginx\n```This manifest includes the following fields:- `spec.tolerations`: GKE can place the Pods on nodes that have the`group=servers:NoSchedule`taint. GKE can't schedule Pods that don't have this toleration on those nodes.\n- `spec.nodeSelector`: GKE must place the Pods on nodes that have the`group: servers`node label.\nGKE adds the corresponding labels and taints to nodes that GKE automatically provisions to run these Pods.\n- Save the following manifest as `batch-job.yaml` :```\napiVersion: batch/v1kind: Jobmetadata:\u00a0 name: batch-jobspec:\u00a0 completions: 5\u00a0 backoffLimit: 3\u00a0 ttlSecondsAfterFinished: 120\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 pod: pi-pod\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 restartPolicy: Never\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 - key: group\u00a0 \u00a0 \u00a0 \u00a0 operator: Equal\u00a0 \u00a0 \u00a0 \u00a0 value: \"jobs\"\u00a0 \u00a0 \u00a0 \u00a0 effect: NoSchedule\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 group: \"jobs\"\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: pi\u00a0 \u00a0 \u00a0 \u00a0 image: perl\u00a0 \u00a0 \u00a0 \u00a0 command: [\"perl\", \u00a0\"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n```This manifest includes the following fields:- `spec.tolerations`: GKE can place the Pods on nodes that have the`group=jobs:NoSchedule`taint. GKE can't schedule Pods that don't have this toleration on those nodes.\n- `spec.nodeSelector`: GKE must place the Pods on nodes that have the`group: jobs`node label.\nGKE adds the corresponding labels and taints to nodes that GKE automatically provisions to run these Pods.\n- Deploy the workloads:```\nkubectl apply -f batch-job.yaml web-server.yaml\n```\nWhen you deploy the workloads, GKE does the following for each workload:\n- GKE looks for existing nodes that have the corresponding node taint and node label specified in the manifest. If nodes exist and have available resources, GKE schedules the workload on the node.\n- If GKE doesn't find an eligible existing node to schedule the workload, GKE creates a new node and applies the corresponding node taint and node label based on the manifest. GKE places the Pod on the new node.\nThe presence of the `NoSchedule` effect in the node taint ensures that workloads without a toleration don't get placed on the node.\n## Verify the workload separation\nList your Pods to find the names of the nodes:\n```\nkubectl get pods --output=wide\n```\nThe output is similar to the following:\n```\nNAME       READY ... NODE\nbatch-job-28j9h    0/1  ... gk3-sandbox-autopilot-nap-1hzelof0-ed737889-2m59\nbatch-job-78rcn    0/1  ... gk3-sandbox-autopilot-nap-1hzelof0-ed737889-2m59\nbatch-job-gg4x2    0/1  ... gk3-sandbox-autopilot-nap-1hzelof0-ed737889-2m59\nbatch-job-qgsxh    0/1  ... gk3-sandbox-autopilot-nap-1hzelof0-ed737889-2m59\nbatch-job-v4ksf    0/1  ... gk3-sandbox-autopilot-nap-1hzelof0-ed737889-2m59\nweb-server-6bb8cd79b5-dw4ds 1/1  ... gk3-sandbox-autopilot-nap-1eurxgsq-f2f3c272-n6xm\nweb-server-6bb8cd79b5-g5ld6 1/1  ... gk3-sandbox-autopilot-nap-1eurxgsq-9f447e18-275z\nweb-server-6bb8cd79b5-jcdx5 1/1  ... gk3-sandbox-autopilot-nap-1eurxgsq-9f447e18-275z\nweb-server-6bb8cd79b5-pxdzw 1/1  ... gk3-sandbox-autopilot-nap-1eurxgsq-ccd22fd9-qtfq\nweb-server-6bb8cd79b5-s66rw 1/1  ... gk3-sandbox-autopilot-nap-1eurxgsq-ccd22fd9-qtfq\nweb-server-6bb8cd79b5-zq8hh 1/1  ... gk3-sandbox-autopilot-nap-1eurxgsq-f2f3c272-n6xm\n```\nThis output shows that the `batch-job` Pods and the `web-server` Pods always run on different nodes.\n## Limitations of workload separation with taints and tolerations\nYou can't use the following key prefixes for workload separation:\n- GKE and Kubernetes-specific keys\n- *cloud.google.com/\n- *kubelet.kubernetes.io/\n- *node.kubernetes.io/\nYou should use your own, unique keys for workload separation.\n## Separate workloads in Standard clusters without node auto-provisioning\nSeparating workloads in Standard clusters without node auto-provisioning requires that you manually create node pools with the appropriate node taints and node labels to accommodate your workloads. For instructions, refer to [Isolate your workloads in dedicated node pools](/kubernetes-engine/docs/how-to/isolate-workloads-dedicated-nodes) . Only use this approach if you have specific requirements that require you to manually manage your node pools.\n## What's next\n- [Enforce pre-defined security policies using the PodSecurity admission controller](/kubernetes-engine/docs/how-to/podsecurityadmission) \n- [Run a full-stack workload at scale](/kubernetes-engine/docs/tutorials/full-stack-scale)", "guide": "Google Kubernetes Engine (GKE)"}