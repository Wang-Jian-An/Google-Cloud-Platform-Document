{"title": "Google Kubernetes Engine (GKE) - Using Envoy Proxy to load-balance gRPC services on GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/exposing-grpc-services-on-gke-using-envoy-proxy", "abstract": "# Google Kubernetes Engine (GKE) - Using Envoy Proxy to load-balance gRPC services on GKE\nLast reviewed 2019-05-30 UTC\nThis tutorial demonstrates how to expose multiple gRPC services deployed on Google Kubernetes Engine (GKE) on a single external IP address by using an [external passthrough Network Load Balancer](/load-balancing/docs/network) and [Envoy Proxy](https://www.envoyproxy.io/) . The tutorial highlights some of the advanced features that Envoy provides for gRPC.", "content": "## Introduction [gRPC](https://grpc.io/) is an open source, language-independent RPC framework based on HTTP/2 that uses protocol buffers for efficient on-the-wire representation and fast serialization. Inspired by [Stubby](https://grpc.io/blog/principles) , the internal Google RPC framework, gRPC enables low-latency communication between microservices and between mobile clients and APIs.\ngRPC runs over HTTP/2 and offers several advantages over HTTP/1.1, such as efficient binary encoding, multiplexing of requests and responses over a single connection, and automatic flow control. gRPC also offers several options for load balancing. This tutorial focuses on situations where clients are untrusted, such as mobile clients and clients running outside the trust boundary of the service provider. Of the load-balancing options that gRPC provides, you use proxy-based load balancing in this tutorial.\nIn the tutorial, you deploy a Kubernetes Service of `TYPE=LoadBalancer` , which is exposed as a transport layer (layer 4) external passthrough Network Load Balancer on Google Cloud. This service provides a single public IP address and passes TCP connections directly to the configured backends. In the tutorial, the backend is a Kubernetes Deployment of Envoy instances.\nEnvoy is an open source application layer (layer 7) proxy that offers many advanced features. In this tutorial, you use it to terminate TLS connections and route gRPC traffic to the appropriate Kubernetes Service. Compared to other application layer solutions such as Kubernetes Ingress, using Envoy directly provides multiple customization options, like the following:- Service discovery\n- Load-balancing algorithms\n- Transforming requests and responses\u2014for instance, to JSON or gRPC-Web\n- Authenticating requests by validating JWT tokens\n- gRPC health checks\nBy combining an external passthrough Network Load Balancer with Envoy, you can set up an endpoint (external IP address) that forwards traffic to a set of Envoy instances running in a Google Kubernetes Engine cluster. These instances then use application layer information to proxy requests to different gRPC services running in the cluster. The Envoy instances use cluster DNS to identify and load-balance incoming gRPC requests to the healthy and running pods for each service. This means traffic is load-balanced to the pods per RPC request rather than per TCP connection from the client.## ArchitectureIn this tutorial, you deploy two gRPC services, `echo-grpc` and `reverse-grpc` , in a Google Kubernetes Engine (GKE) cluster and expose them to the internet on a public IP address. The following diagram shows the architecture for exposing these two services through a single endpoint:An external passthrough Network Load Balancer accepts incoming requests from the internet (for example, from mobile clients or service consumers outside your company). The external passthrough Network Load Balancer performs the following tasks:- Load-balances incoming connections to the nodes in the pool. Traffic is forwarded to the`envoy`Kubernetes Service, which is exposed on all nodes in the cluster. The Kubernetes network proxy forwards these connections to pods that are running Envoy.\n- Performs HTTP health checks against the nodes in the cluster.\nEnvoy performs the following tasks:- Terminates TLS connections.\n- Discovers pods running the gRPC services by querying the internal cluster DNS service.\n- Routes and load-balances traffic to the gRPC service pods.\n- Performs health checks of the gRPC services according to the [gRPC Health Checking Protocol](https://github.com/grpc/grpc/blob/master/doc/health-checking.md) .\n- Exposes an endpoint for health checking of Envoy instances by the external passthrough Network Load Balancer.\nThe gRPC services ( `echo-grpc` and `reverse-grpc` ) are exposed as [Kubernetes headless Services](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services) . This means that no `clusterIP` address is assigned, and the Kubernetes network proxy doesn't load-balance traffic to the pods. Instead, a DNS A record that contains the pod IP addresses is created in the cluster DNS service. Envoy discovers the pod IP addresses from this DNS entry and load-balances across them according to the policy configured in Envoy.\nThe following diagram shows the Kubernetes objects involved in this tutorial:## CostsIn this document, you use the following billable components of Google Cloud:- [Artifact Registry](/artifact-registry/pricing) \n- [Google Kubernetes Engine (GKE)](/kubernetes-engine/pricing) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n- In the Google Cloud console, on the project selector page,   select or [create a Google Cloud project](/resource-manager/docs/creating-managing-projects) . [Go to project selector](https://console.cloud.google.com/projectselector2/home/dashboard) \n- [Make sure that billing is enabled for your Google Cloud project](/billing/docs/how-to/verify-billing-enabled#console) .\n- In the Google Cloud console, activate Cloud Shell. [Activate Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n## Prepare the environment\n- In Cloud Shell, set the Google Cloud project that you want to use for this tutorial:```\ngcloud config set project PROJECT_ID\n```Replace `` with your Google Cloud project ID.\n- Enable the Artifact Registry and GKE APIs:```\ngcloud services enable artifactregistry.googleapis.com \\\u00a0 \u00a0 container.googleapis.com\n```\n## Create the GKE cluster\n- In Cloud Shell, create a GKE cluster for running your gRPC services:```\ngcloud container clusters create envoy-grpc-tutorial \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --release-channel rapid \\\u00a0 \u00a0 --scopes cloud-platform \\\u00a0 \u00a0 --workload-pool PROJECT_ID.svc.id.goog \\\u00a0 \u00a0 --zone us-central1-f\n```This tutorial uses the `us-central1-f` zone. You can use a [different zone or region](/compute/docs/regions-zones) .\n- Verify that the `kubectl` context has been set up by listing the nodes in your cluster:```\nkubectl get nodes --output name\n```The output looks similar to this:```\nnode/gke-envoy-grpc-tutorial-default-pool-c9a3c791-1kpt\nnode/gke-envoy-grpc-tutorial-default-pool-c9a3c791-qn92\nnode/gke-envoy-grpc-tutorial-default-pool-c9a3c791-wf2h\n```\n## Create the Artifact Registry repository\n- In Cloud Shell, create a new repository to store container images:```\ngcloud artifacts repositories create envoy-grpc-tutorial-images \\\u00a0 \u00a0 --repository-format docker \\\u00a0 \u00a0 --location us-central1\n```You create the repository in the same region as the GKE cluster to help optimize latency and network bandwidth when nodes pull container images.\n- Grant the [Artifact Registry Reader](/artifact-registry/docs/access-control#roles) role on the repository to the Google service account used by the node VMs of the GKE cluster:```\nPROJECT_NUMBER=$(gcloud projects describe PROJECT_ID --format 'value(projectNumber)')gcloud artifacts repositories add-iam-policy-binding envoy-grpc-tutorial-images \\\u00a0 \u00a0 --location us-central1 \\\u00a0 \u00a0 --member serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com \\\u00a0 \u00a0 --role roles/artifactregistry.reader\n```\n- Add a credential helper entry for the repository hostname to the Docker configuration file in your Cloud Shell home directory:```\ngcloud auth configure-docker us-central1-docker.pkg.dev\n```The credential helper entry enables container image tools running in Cloud Shell to authenticate to the Artifact Registry repository location for pulling and pushing images.\n## Deploy the gRPC servicesTo route traffic to multiple gRPC services behind one load balancer, you deploy two sample gRPC services: `echo-grpc` and `reverse-grpc` . Both services expose a unary method that takes a string in the `content` request field. `echo-grpc` responds with the content unaltered, while `reverse-grpc` responds with the content string reversed.- In Cloud Shell, clone the repository containing the gRPC services and switch to the repository directory:```\ngit clone https://github.com/GoogleCloudPlatform/grpc-gke-nlb-tutorial.git ~/grpc-gke-nlb-tutorialcd ~/grpc-gke-nlb-tutorial\n```\n- Create a self-signed TLS certificate and private key:```\nopenssl req -x509 -newkey rsa:4096 -nodes -sha256 -days 365 \\\u00a0 \u00a0 -keyout privkey.pem -out cert.pem -extensions san \\\u00a0 \u00a0 -config \\\u00a0 \u00a0 <(echo \"[req]\";\u00a0 \u00a0 \u00a0 echo distinguished_name=req;\u00a0 \u00a0 \u00a0 echo \"[san]\";\u00a0 \u00a0 \u00a0 echo subjectAltName=DNS:grpc.example.com\u00a0 \u00a0 \u00a0) \\\u00a0 \u00a0 -subj '/CN=grpc.example.com'\n```\n- Create a Kubernetes Secret called `envoy-certs` that contains the self-signed TLS certificate and private key:```\nkubectl create secret tls envoy-certs \\\u00a0 \u00a0 --key privkey.pem --cert cert.pem \\\u00a0 \u00a0 --dry-run=client --output yaml | kubectl apply --filename ```Envoy uses this TLS certificate and private key when it terminates TLS connections.\n- Build the container images for the sample apps `echo-grpc` and `reverse-grpc` , push the images to Artifact Registry, and deploy the apps to the GKE cluster, using Skaffold:```\nskaffold run \\\u00a0 \u00a0 --default-repo=us-central1-docker.pkg.dev/PROJECT_ID/envoy-grpc-tutorial-images \\\u00a0 \u00a0 --module=echo-grpc,reverse-grpc \\\u00a0 \u00a0 --skip-tests\n``` [Skaffold](https://skaffold.dev) is an open source tool from Google that automates workflows for developing, building, pushing, and deploying applications as containers.\n- Deploy Envoy to the GKE cluster using Skaffold:```\nskaffold run \\\u00a0 \u00a0 --digest-source=none \\\u00a0 \u00a0 --module=envoy \\\u00a0 \u00a0 --skip-tests\n```\n- Verify that two pods are ready for each deployment:```\nkubectl get deployments\n```The output looks similar to the following. The values for `READY` should be `2/2` for all deployments.```\nNAME   READY UP-TO-DATE AVAILABLE AGE\necho-grpc  2/2  2   2   1m\nenvoy   2/2  2   2   1m\nreverse-grpc 2/2  2   2   1m\n```\n- Verify that `echo-grpc` , `envoy` , and `reverse-grpc` exist as Kubernetes Services:```\nkubectl get services --selector skaffold.dev/run-id\n```The output looks similar to the following. Both `echo-grpc` and `reverse-grpc` should have `TYPE=ClusterIP` and `CLUSTER-IP=None` .```\nNAME   TYPE   CLUSTER-IP EXTERNAL-IP  PORT(S)   AGE\necho-grpc  ClusterIP  None   <none>   8081/TCP  2m\nenvoy   LoadBalancer 10.40.2.203 203.0.113.1  443:31516/TCP 2m\nreverse-grpc ClusterIP  None   <none>   8082/TCP  2m\n```\n **Note** : In this tutorial, the public IP address of the external passthrough Network Load Balancer forwarding rule is used to communicate with the services. For a production-quality deployment, we recommend that you create a [DNS A record](/dns/docs/overview#supported_dns_record_types) that points to the public IP address. You can use [Cloud DNS](/dns/docs) or third-party DNS services to create this record.With the DNS record in place, you call your APIs using the domain name instead of the IP address. This approach allows you to change the IP address in the future without making changes to your API clients. Also, a domain name is required in order to obtain a TLS certificate from public certificate authorities.## Test the gRPC servicesTo test the services, you use the [grpcurl](https://github.com/fullstorydev/grpcurl/blob/master/README.md) command-line tool.- In Cloud Shell, install `grpcurl` :```\ngo install github.com/fullstorydev/grpcurl/cmd/grpcurl@latest\n```\n- Get the external IP address of the `envoy` Kubernetes Service and store it in an environment variable:```\nEXTERNAL_IP=$(kubectl get service envoy \\\u00a0 \u00a0 --output=jsonpath='{.status.loadBalancer.ingress[0].ip}')\n```\n- Send a request to the `echo-grpc` sample app:```\ngrpcurl -v -d '{\"content\": \"echo\"}' \\\u00a0 \u00a0 -proto echo-grpc/api/echo.proto \\\u00a0 \u00a0 -authority grpc.example.com -cacert cert.pem \\\u00a0 \u00a0 $EXTERNAL_IP:443 api.Echo/Echo\n```The output looks similar to this:```\nResolved method descriptor:\nrpc Echo ( .api.EchoRequest ) returns ( .api.EchoResponse );\nRequest metadata to send:\n(empty)\nResponse headers received:\ncontent-type: application/grpc\ndate: Wed, 02 Jun 2021 07:18:22 GMT\nhostname: echo-grpc-75947768c9-jkdcw\nserver: envoy\nx-envoy-upstream-service-time: 3\nResponse contents:\n{\n \"content\": \"echo\"\n}\nResponse trailers received:\n(empty)\nSent 1 request and received 1 response\n```The `hostname` response header shows the name of the `echo-grpc` pod that handled the request. If you repeat the command a few times, you should see two different values for the `hostname` response header, corresponding to the names of the `echo-grpc` pods.\n- Verify the same behavior with the Reverse gRPC service:```\ngrpcurl -v -d '{\"content\": \"reverse\"}' \\\u00a0 \u00a0 -proto reverse-grpc/api/reverse.proto \\\u00a0 \u00a0 -authority grpc.example.com -cacert cert.pem \\\u00a0 \u00a0 $EXTERNAL_IP:443 api.Reverse/Reverse\n```The output looks similar to this:```\nResolved method descriptor:\nrpc Reverse ( .api.ReverseRequest ) returns ( .api.ReverseResponse );\nRequest metadata to send:\n(empty)\nResponse headers received:\ncontent-type: application/grpc\ndate: Wed, 02 Jun 2021 07:20:15 GMT\nhostname: reverse-grpc-5c9b974f54-wlfwt\nserver: envoy\nx-envoy-upstream-service-time: 1\nResponse contents:\n{\n \"content\": \"esrever\"\n}\nResponse trailers received:\n(empty)\nSent 1 request and received 1 response\n```\n## Envoy configurationTo understand the Envoy configuration better, you can look at the configuration file `envoy/k8s/envoy.yaml` in the Git repository.\nThe `route_config` section specifies how incoming requests are routed to the `echo-grpc` and `reverse-grpc` sample apps.\n [  envoy/k8s/envoy.yaml ](https://github.com/GoogleCloudPlatform/grpc-gke-nlb-tutorial/blob/HEAD/envoy/k8s/envoy.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/grpc-gke-nlb-tutorial/blob/HEAD/envoy/k8s/envoy.yaml) \n```\nroute_config:\u00a0 name: local_route\u00a0 virtual_hosts:\u00a0 - name: local_service\u00a0 \u00a0 domains:\u00a0 \u00a0 - \"*\"\u00a0 \u00a0 routes:\u00a0 \u00a0 - match:\u00a0 \u00a0 \u00a0 \u00a0 prefix: \"/api.Echo/\"\u00a0 \u00a0 \u00a0 route:\u00a0 \u00a0 \u00a0 \u00a0 cluster: echo-grpc\u00a0 \u00a0 - match:\u00a0 \u00a0 \u00a0 \u00a0 prefix: \"/api.Reverse/\"\u00a0 \u00a0 \u00a0 route:\u00a0 \u00a0 \u00a0 \u00a0 cluster: reverse-grpc\n```\nThe sample apps are defined as [Envoy clusters](https://www.envoyproxy.io/docs/envoy/latest/api-v3/config/cluster/v3/cluster.proto) .\n [  envoy/k8s/envoy.yaml ](https://github.com/GoogleCloudPlatform/grpc-gke-nlb-tutorial/blob/HEAD/envoy/k8s/envoy.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/grpc-gke-nlb-tutorial/blob/HEAD/envoy/k8s/envoy.yaml) \n```\nclusters:- name: echo-grpc\u00a0 connect_timeout: 0.5s\u00a0 type: STRICT_DNS\u00a0 dns_lookup_family: V4_ONLY\u00a0 lb_policy: ROUND_ROBIN\u00a0 http2_protocol_options: {}\u00a0 load_assignment:\u00a0 \u00a0 cluster_name: echo-grpc\u00a0 \u00a0 endpoints:\u00a0 \u00a0 - lb_endpoints:\u00a0 \u00a0 \u00a0 - endpoint:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 address:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 socket_address:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 address: echo-grpc.default.svc.cluster.local\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port_value: 8081\u00a0 health_checks:\u00a0 \u00a0 timeout: 1s\u00a0 \u00a0 interval: 10s\u00a0 \u00a0 unhealthy_threshold: 2\u00a0 \u00a0 healthy_threshold: 2\u00a0 \u00a0 grpc_health_check: {}\n```\nThe `type: STRICT_DNS` and `lb_policy: ROUND_ROBIN` fields in the cluster definition specify that Envoy performs DNS lookups of the hostname specified in the `address` field and load-balances across the IP addresses in the response to the DNS lookup. The response contains multiple IP addresses because the Kubernetes Service objects that define the sample apps specify headless services.\nThe `http2_protocol_options` field specifies that Envoy uses the HTTP/2 protocol to the sample apps.\nThe `grpc_health_check` field in the `health_checks` section specifies that Envoy uses the gRPC health checking protocol to determine the health of the sample apps.## TroubleshootIf you run into problems with this tutorial, we recommend that you review these documents:- [GKE troubleshooting](/kubernetes-engine/docs/troubleshooting) \n- [Troubleshooting Kubernetes clusters](https://kubernetes.io/docs/tasks/debug/debug-cluster/) \n- [Troubleshooting applications deployed on Kubernetes](https://kubernetes.io/docs/tasks/debug/debug-application/) \nYou can also explore the Envoy administration interface to diagnose problems with the Envoy configuration.- To open the administration interface, set up port forwarding from Cloud Shell to the `admin` port of one of the Envoy pods:```\nkubectl port-forward \\\u00a0 \u00a0 $(kubectl get pods -o name | grep envoy | head -n1) 8080:8090\n```\n- Wait until you see this output in the console:```\nForwarding from 127.0.0.1:8080 -> 8090\n```\n- Click the **Web preview** button in Cloud Shell and select **Preview on port 8080** . This opens a new browser window showing the administration interface. \n- When you are done, switch back to Cloud Shell and press `Control+C` to end port forwarding.\n## Alternative ways to route gRPC trafficYou can modify this solution in a number of ways to suit your environment.\n### Alternative application layer load balancersSome of the application layer functionality that Envoy provides can also be provided by other load-balancing solutions:- You can use a [global external Application Load Balancer or regional external Application Load Balancer](/load-balancing/docs/https) instead of an external passthrough Network Load Balancer and self-managed Envoy. Using an external Application Load Balancer provides several benefits compared to an external passthrough Network Load Balancer, such as advanced traffic management capability, managed TLS certificates, and integration with other Google Cloud products such as Cloud CDN, Google Cloud Armor, and IAP.We recommend that you use a global external Application Load Balancer or regional external Application Load Balancer if the traffic management capabilities they offer satisfy your use cases and if you do not need support for client certificate-based authentication, also known as mutual TLS (mTLS) authentication. For more information, see the following documents:- [Traffic management overview for global external Application Load Balancers]() \n- [Traffic management overview for regional external Application Load Balancers]() \n- If you use Anthos Service Mesh or Istio, you can use their features to route and load-balance gRPC traffic. Both Anthos Service Mesh and Istio provide an [ingress gateway](https://istio.io/docs/tasks/traffic-management/ingress/) that is deployed as an external passthrough Network Load Balancer with an Envoy backend, similar to the architecture in this tutorial. The main difference is that Envoy is configured through Istio's [traffic routing](https://istio.io/docs/reference/config/) objects.To make the example services in this tutorial routable in the Anthos Service Mesh or Istio service mesh, you must remove the line `clusterIP: None` from the Kubernetes Service manifests ( `echo-service.yaml` and `reverse-service.yaml` ). This means using the service discovery and load balancing functionality of Anthos Service Mesh or Istio instead of the similar functionality in Envoy.If you already use Anthos Service Mesh or Istio, we recommend using the ingress gateway to route to your gRPC services.\n- You can use NGINX in place of Envoy, either as a Deployment or using the [NGINX Ingress Controller for Kubernetes](https://github.com/kubernetes/ingress-nginx) . Envoy is used in this tutorial because it provides more advanced gRPC functionality, such as support for the [gRPC health checking protocol](https://github.com/grpc/grpc/blob/master/doc/health-checking.md) .\n### Internal VPC network connectivityIf you want to expose the services outside your GKE cluster but only inside your VPC network, you can use either an [internal passthrough Network Load Balancer](/kubernetes-engine/docs/how-to/internal-load-balancing) or an [internal Application Load Balancer](/load-balancing/docs/l7-internal) .\nTo use an internal passthrough Network Load Balancer in place of an external passthrough Network Load Balancer, add the annotation `cloud.google.com/load-balancer-type: \"Internal\"` to the `envoy-service.yaml` manifest.\nTo use an internal Application Load Balancer, consult the documentation on [configuring Ingress for internal Application Load Balancers](/kubernetes-engine/docs/how-to/internal-load-balance-ingress) .## Clean up\nAfter you finish the tutorial, you can clean up the resources that you created so that they stop using quota and incurring charges. The following sections describe how to delete or turn off these resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Delete the resourcesIf you want to keep the Google Cloud project you used in this tutorial, delete the individual resources:- In Cloud Shell, delete the local Git repository clone:```\ncd ; rm -rf ~/grpc-gke-nlb-tutorial\n```\n- Delete the GKE cluster:```\ngcloud container clusters delete envoy-grpc-tutorial \\\u00a0 \u00a0 --zone us-central1-f --async --quiet\n```\n- Delete the repository in Artifact Registry:```\ngcloud artifacts repositories delete envoy-grpc-tutorial-images \\\u00a0 \u00a0 --location us-central1 --async --quiet\n```\n## What's next\n- Read about [GKE networking](/kubernetes-engine/docs/concepts/network-overview) .\n- Browse examples on how to [expose gRPC services to clients inside your Kubernetes cluster](https://github.com/jtattermusch/grpc-loadbalancing-kubernetes-examples) .\n- Explore options for [gRPC load-balancing](https://grpc.io/blog/loadbalancing) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Google Kubernetes Engine (GKE)"}