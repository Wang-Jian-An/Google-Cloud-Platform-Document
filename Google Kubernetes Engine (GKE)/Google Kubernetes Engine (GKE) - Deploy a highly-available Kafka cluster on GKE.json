{"title": "Google Kubernetes Engine (GKE) - Deploy a highly-available Kafka cluster on GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/stateful-workloads/kafka", "abstract": "# Google Kubernetes Engine (GKE) - Deploy a highly-available Kafka cluster on GKE\n[Kafka](https://kafka.apache.org/) is an open source, distributed publish-subscribe messaging system for handling high-volume, high-throughput, and real-time streaming data. You can use Kafka to build streaming data pipelines that move data reliably across different systems and applications for processing and analysis.\nThis tutorial is intended for platform administrators, cloud architects, and operations professionals interested in deploying highly-available Kafka clusters on Google Kubernetes Engine (GKE).", "content": "## Objectives\nIn this tutorial, you will learn how to:\n- Use Terraform to create a regional GKE cluster.\n- Deploy a highly-available Kafka cluster.\n- Upgrade Kafka binaries.\n- Backup and restore the Kafka cluster.\n- Simulate GKE node disruption and Kafka broker failover.\n## ArchitectureThis section describes the architecture of the solution you'll build in this tutorial.\nA Kafka cluster is a group of one or more servers (called ) that work together to handle incoming data streams and publish-subscribe messaging for Kafka clients (called ).\nEach data partition in a Kafka cluster has one broker and can have one or more follower brokers. The leader broker handles all reads and writes to the partition. Each follower broker passively replicates the leader broker.\nIn a typical Kafka setup, you also use an open source service called [ZooKeeper](https://zookeeper.apache.org/) to coordinate your Kafka clusters. This service helps in electing a leader among the brokers and triggering failover in case of failures.\nIn this tutorial, you deploy the Kafka clusters on GKE by configuring the Kafka brokers and Zookeeper service as individual [StatefulSets](/kubernetes-engine/docs/concepts/statefulset) . To provision highly-available Kafka clusters and prepare for disaster recovery, you'll configure your Kafka and Zookeeper StatefulSets to use separate [node pools](/kubernetes-engine/docs/concepts/node-pools) and [zones](/compute/docs/regions-zones) .\nThe following diagram shows how your Kafka StatefulSet runs on multiple nodes and zones in your GKE cluster.The following diagram shows how your Zookeeper StatefulSet runs on multiple nodes and zones in your GKE cluster.\n### Node provisioning and Pod schedulingIf you are using Autopilot clusters, Autopilot handles provisioning nodes and scheduling the Pods for your workloads. You'll use Pod [anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) to ensure that no two Pods of the same StatefulSet are scheduled on the same node and same zone.\nIf you are using Standard clusters, you'll need to configure the Pod [toleration](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) and [node affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) . To learn more, see [Isolate your workloads in dedicated node pools](/kubernetes-engine/docs/how-to/isolate-workloads-dedicated-nodes) .## CostsIn this document, you use the following billable components of Google Cloud:- [Artifact Registry](/artifact-registry/pricing) \n- [Backup for GKE](/kubernetes-engine/pricing#backup-for-gke) \n- [Compute Engine](/compute/disks-image-pricing) \n- [GKE](/kubernetes-engine/pricing) \n- [Google Cloud Managed Service for Prometheus](/stackdriver/docs/managed-prometheus) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n### Set up your project### Set up roles\n- Grant roles to your Google Account. Run the following command once for each of the following   IAM roles: `role/storage.objectViewer, role/logging.logWriter, role/artifactregistry.Admin, roles/container.clusterAdmin, role/container.serviceAgent, roles/iam.serviceAccountAdmin, roles/serviceusage.serviceUsageAdmin, roles/iam.serviceAccountAdmin` ```\n$ gcloud projects add-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS\" --role=ROLE\n```- Replace``with your project ID.\n- Replace``with your email address.\n- Replace``with each individual role.### Set up your environmentIn this tutorial, you use [Cloud Shell](/shell) to manage resources hosted on Google Cloud. Cloud Shell comes preinstalled with the software you'll need for this tutorial, including [Docker](https://www.docker.com/) , [kubectl](https://kubernetes.io/docs/reference/kubectl/) , the [gcloud CLI](/sdk/gcloud) , [Helm](https://helm.sh/) , and [Terraform](/docs/terraform) .\nTo set up your environment with Cloud Shell, follow these steps:- Launch a Cloud Shell session from the Google Cloud console, by clicking **Activate Cloud Shell** in the [Google Cloud console](http://console.cloud.google.com) . This launches a session in the bottom pane of Google Cloud console.\n- Set environment variables.```\nexport PROJECT_ID=PROJECT_IDexport REGION=us-central1\n```Replace the following values:- : your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- Set the default environment variables.```\ngcloud config set project PROJECT_ID\n```\n- Clone the code repository.```\ngit clone https://github.com/GoogleCloudPlatform/kubernetes-engine-samples\n```\n- Change to the working directory.```\ncd kubernetes-engine-samples/streaming/gke-stateful-kafka\n```\n## Create your cluster infrastructureIn this section, you'll run a Terraform script to create two [regional GKE clusters](/kubernetes-engine/docs/concepts/regional-clusters) . The primary cluster will be deployed in `us-central1` .\n **Note:** Because you'll use [Google Cloud Managed Service for Prometheus](/stackdriver/docs/managed-prometheus) and [Backup for GKE](/kubernetes-engine/docs/add-on/backup-for-gke/concepts/backup-for-gke) for this tutorial, it might take up to eight minutes to create and reconcile the cluster.\nTo create the cluster, follow these steps:\nIn Cloud Shell, run the following commands:\n```\nterraform -chdir=terraform/gke-autopilot initterraform -chdir=terraform/gke-autopilot apply -var project_id=$PROJECT_ID\n```\nWhen prompted, type `yes` .\nIn Cloud Shell, run the following commands:\n```\nterraform -chdir=terraform/gke-standard initterraform -chdir=terraform/gke-standard apply -var project_id=$PROJECT_ID \n```\nWhen prompted, type `yes` .The [Terraform configuration files](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/tree/main/streaming/gke-stateful-kafka/terraform) create the following resources to deploy your infrastructure:- Create a Artifact Registry repository to store the Docker images.\n- Create the VPC network and subnet for the VM's network interface.\n- Create a two GKE clusters.\nTerraform creates a private cluster in the two region, and enables Backup for GKE for disaster recovery.\n **Tip:** To debug issues when running Terraform, you can capture debug output by setting the Terraform log level environment variable `TF_LOG` . For example: `export TF_LOG=\"DEBUG\"` . Valid log levels are (in order of decreasing verbosity): `TRACE` , `DEBUG` , `INFO` , `WARN` , or `ERROR` .## Deploy Kafka on your clusterIn this section, you'll deploy Kafka on GKE using a Helm chart. The operation creates the following resources:- The Kafka and Zookeeper StatefulSets.\n- A [Kafka exporter](https://github.com/danielqsj/kafka_exporter) deployment. The exporter gathers Kafka metrics for Prometheus consumption.\n- A [Pod Disruption Budget (PDB)](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/) that limits the number of offline Pods during a voluntary disruption.\nTo use the Helm chart to deploy Kafka, follow these steps:- Configure Docker access.```\ngcloud auth configure-docker us-docker.pkg.dev\n```\n- Populate Artifact Registry with the Kafka and Zookeeper images.```\n./scripts/gcr.sh bitnami/kafka 3.3.2-debian-11-r0./scripts/gcr.sh bitnami/kafka-exporter 1.6.0-debian-11-r52./scripts/gcr.sh bitnami/jmx-exporter 0.17.2-debian-11-r41./scripts/gcr.sh bitnami/zookeeper 3.8.0-debian-11-r74\n```\n- Configure `kubectl` command line access to the primary cluster.```\ngcloud container clusters get-credentials gke-kafka-us-central1 \\\u00a0 \u00a0 --region=${REGION} \\\u00a0 \u00a0 --project=${PROJECT_ID}\n```\n- Create a namespace.```\nexport NAMESPACE=kafkakubectl create namespace $NAMESPACE\n```\n- Install Kafka using the Helm chart version 20.0.6.```\ncd helm../scripts/chart.sh kafka 20.0.6 && \\rm -rf Chart.lock charts && \\helm dependency update && \\helm -n kafka upgrade --install kafka . \\--set global.imageRegistry=\"us-docker.pkg.dev/$PROJECT_ID/main\"\n```The output is similar to the following:```\nNAME: kafka\nLAST DEPLOYED: Thu Feb 16 03:29:39 2023\nNAMESPACE: kafka\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n```\n- Verify that your Kafka replicas are running (this might take a few minutes).```\nkubectl get all -n kafka\n```The output is similar to the following:```\n--NAME     READY STATUS RESTARTS  AGE\npod/kafka-0    1/1  Running 2 (3m51s ago) 4m28s\npod/kafka-1    1/1  Running 3 (3m41s ago) 4m28s\npod/kafka-2    1/1  Running 2 (3m57s ago) 4m28s\npod/kafka-zookeeper-0 1/1  Running 0    4m28s\npod/kafka-zookeeper-1 1/1  Running 0    4m28s\npod/kafka-zookeeper-2 1/1  Running 0    4m28s\nNAME         TYPE  CLUSTER-IP  EXTERNAL-IP PORT(S)      AGE\nservice/kafka       ClusterIP 192.168.112.124 <none>  9092/TCP      4m29s\nservice/kafka-app      ClusterIP 192.168.75.57  <none>  9092/TCP      35m\nservice/kafka-app-headless    ClusterIP None    <none>  9092/TCP,9093/TCP   35m\nservice/kafka-app-zookeeper   ClusterIP 192.168.117.102 <none>  2181/TCP,2888/TCP,3888/TCP 35m\nservice/kafka-app-zookeeper-headless ClusterIP None    <none>  2181/TCP,2888/TCP,3888/TCP 35m\nservice/kafka-headless     ClusterIP None    <none>  9092/TCP,9093/TCP   4m29s\nservice/kafka-zookeeper    ClusterIP 192.168.89.249 <none>  2181/TCP,2888/TCP,3888/TCP 4m29s\nservice/kafka-zookeeper-headless  ClusterIP None    <none>  2181/TCP,2888/TCP,3888/TCP 4m29s\nNAME        READY AGE\nstatefulset.apps/kafka    3/3  4m29s\nstatefulset.apps/kafka-zookeeper 3/3  4m29s\n```\n## Create test dataIn this section, you will test the Kafka application and generate messages.- Create a consumer client Pod for interacting with the Kafka application.```\nkubectl run kafka-client -n kafka --rm -ti \\\u00a0 \u00a0 --image us-docker.pkg.dev/$PROJECT_ID/main/bitnami/kafka:3.3.2-debian-11-r0 -- bash\n```\n- Create a topic named `topic1` with three partitions and a replication factor of three.```\nkafka-topics.sh \\\u00a0 \u00a0 --create \\\u00a0 \u00a0 --topic topic1 \\\u00a0 \u00a0 --partitions 3 \u00a0\\\u00a0 \u00a0 --replication-factor 3 \\\u00a0 \u00a0 --bootstrap-server kafka-headless.kafka.svc.cluster.local:9092\n```\n- Verify that the topic partitions are replicated across all three brokers.```\nkafka-topics.sh \\\u00a0 \u00a0 --describe \\\u00a0 \u00a0 --topic topic1 \\\u00a0 \u00a0 --bootstrap-server kafka-headless.kafka.svc.cluster.local:9092\n```The output is similar to the following:```\nTopic: topic1  TopicId: 1ntc4WiFS4-AUNlpr9hCmg PartitionCount: 3  ReplicationFactor: 3 Configs: flush.ms=1000,segment.bytes=1073741824,flush.messages=10000,max.message.bytes=1000012,retention.bytes=1073741824\n  Topic: topic1 Partition: 0 Leader: 2  Replicas: 2,0,1 Isr: 2,0,1\n  Topic: topic1 Partition: 1 Leader: 1  Replicas: 1,2,0 Isr: 1,2,0\n  Topic: topic1 Partition: 2 Leader: 0  Replicas: 0,1,2 Isr: 0,1,2\n```In the example output, notice that `topic1` has three partitions, each with a different leader and set of replicas. This is because Kafka uses partitioning to distribute the data across multiple brokers, allowing for greater scalability and fault tolerance. The replication factor of three ensures that each partition has three replicas, so that data is still available even if one or two brokers fail.\n- Run the following command to generate message numbers in bulk into `topic1` .```\nALLOW_PLAINTEXT_LISTENER=yesfor x in $(seq 0 200); do\u00a0 echo \"$x: Message number $x\"done | kafka-console-producer.sh \\\u00a0 \u00a0 --topic topic1 \\\u00a0 \u00a0 --bootstrap-server kafka-headless.kafka.svc.cluster.local:9092 \\\u00a0 \u00a0 --property parse.key=true \\\u00a0 \u00a0 --property key.separator=\":\"\n```\n- Run the following command to consume `topic1` from all partitions.```\nkafka-console-consumer.sh \\\u00a0 \u00a0 --bootstrap-server kafka.kafka.svc.cluster.local:9092 \\\u00a0 \u00a0 --topic topic1 \\\u00a0 \u00a0 --property print.key=true \\\u00a0 \u00a0 --property key.separator=\" : \" \\\u00a0 \u00a0 --from-beginning;\n```Type `CTRL+C` to stop the consumer process.\n## Benchmark KafkaTo accurately model a use case, you can run a simulation of the expected load on the cluster. To test performance, you will use the tools included in the Kafka package, namely the `kafka-producer-perf-test.sh` and `kafka-consumer-perf-test.sh` scripts in the `bin` folder.- Create a topic for benchmarking.```\nkafka-topics.sh \\\u00a0 --create \\\u00a0 --topic topic-benchmark \\\u00a0 --partitions 3 \u00a0\\\u00a0 --replication-factor 3 \\\u00a0 --bootstrap-server kafka-headless.kafka.svc.cluster.local:9092\n```\n- Create load on the Kafka cluster.```\nKAFKA_HEAP_OPTS=\"-Xms4g -Xmx4g\" kafka-producer-perf-test.sh \\\u00a0 \u00a0 --topic topic-benchmark \\\u00a0 \u00a0 --num-records 10000000 \\\u00a0 \u00a0 --throughput -1 \\\u00a0 \u00a0 --producer-props bootstrap.servers=kafka.kafka.svc.cluster.local:9092 \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 batch.size=16384 \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 acks=all \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 linger.ms=500 \\\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 compression.type=none \\\u00a0 \u00a0 --record-size 100 \\\u00a0 \u00a0 --print-metrics\n```The producer will generate 10,000,000 records on `topic-benchmark` . The output is similar to the following:```\n623821 records sent, 124316.7 records/sec (11.86 MB/sec), 1232.7 ms avg latency, 1787.0 ms max latency.\n1235948 records sent, 247140.2 records/sec (23.57 MB/sec), 1253.0 ms avg latency, 1587.0 ms max latency.\n1838898 records sent, 367779.6 records/sec (35.07 MB/sec), 793.6 ms avg latency, 1185.0 ms max latency.\n2319456 records sent, 463242.7 records/sec (44.18 MB/sec), 54.0 ms avg latency, 321.0 ms max latency.\n```Once all records have been sent, you should see additional metrics in the output, similar to the following:```\nproducer-topic-metrics:record-send-rate:{client-id=perf-producer-client, topic=topic-benchmark}  : 173316.233\nproducer-topic-metrics:record-send-total:{client-id=perf-producer-client, topic=topic-benchmark} : 10000000.000\n```To exit the watch, type `CTRL + C` .\n- Exit the Pod shell.```\nexit\n```\n## Manage upgradesVersion updates for Kafka and Kubernetes are released on a regular schedule. Follow [operational best practices](/kubernetes-engine/docs/best-practices/upgrading-clusters) to upgrade your software environment regularly.\n **Note:** By default, GKE manages cluster and node pool upgrades for you. Autopilot clusters are [automatically upgraded](/kubernetes-engine/docs/concepts/cluster-upgrades-autopilot) , based on the release channel you selected. If you are using Standard clusters and you have disabled automatic upgrades, refer to [Manually upgrading a cluster or node pool](/kubernetes-engine/docs/how-to/upgrading-a-cluster) .\n### Plan for Kafka binary upgradesIn this section, you will update the Kafka image using Helm and verify that your topics are still available.\nTo upgrade from the earlier Kafka version from the Helm chart you used in [Deploy Kafka on your cluster](#deploy-kafka) , follow these steps:- Populate Artifact Registry with the following image:```\n../scripts/gcr.sh bitnami/kafka 3.4.0-debian-11-r2../scripts/gcr.sh bitnami/kafka-exporter 1.6.0-debian-11-r61../scripts/gcr.sh bitnami/jmx-exporter 0.17.2-debian-11-r49../scripts/gcr.sh bitnami/zookeeper 3.8.1-debian-11-r0\n```\n- Perform these steps to deploy a Helm chart with the upgraded Kafka and Zookeeper images. For version-specific guidance, refer to the [Kafka instructions for version upgrades](https://kafka.apache.org/documentation/#upgrade) .- Update the`Chart.yaml`dependency version:\n```\n../scripts/chart.sh kafka 20.1.0\n```- Deploy the Helm chart with the new Kafka and Zookeeper images, as shown in the following example:```\nrm -rf Chart.lock charts && \\helm dependency update && \\helm -n kafka upgrade --install kafka ./ \\\u00a0 \u00a0 \u00a0 --set global.imageRegistry=\"$REGION-docker.pkg.dev/$PROJECT_ID/main\"\n```\nWatch the Kafka Pods get upgraded:```\nkubectl get pod -l app.kubernetes.io/component=kafka -n kafka --watch\n```To exit the watch, type `CTRL + C` .\n- Connect to the Kafka cluster using a client Pod.```\nkubectl run kafka-client -n kafka --rm -ti \\\u00a0 --image us-docker.pkg.dev/$PROJECT_ID/main/bitnami/kafka:3.4.0-debian-11-r2 -- bash\n```\n- Verify you can access messages from `topic1` .```\nkafka-console-consumer.sh \\\u00a0 --topic topic1 \\\u00a0 --from-beginning \\\u00a0 --bootstrap-server kafka-headless.kafka.svc.cluster.local:9092\n```The output should show the generated messages from the previous step. Type `CTRL+C` to exit the process.\n- Exit the client Pod.```\nexit\n```\n## Prepare for disaster recoveryTo ensure that your production workloads remain available in the event of a service-interrupting event, you should prepare a disaster recovery (DR) plan. To learn more about DR planning, see the [Disaster recovery planning guide](/architecture/dr-scenarios-planning-guide) .\nTo backup and restore your workloads on GKE clusters, you can use [Backup for GKE](/kubernetes-engine/docs/add-on/backup-for-gke/concepts/backup-for-gke) .\n **Tip:** If you are using Autopilot clusters, check that your infrastructure works with these [Backup for GKE restrictions](/kubernetes-engine/docs/add-on/backup-for-gke/concepts/about-autopilot) .\n### Example Kafka backup and restore scenarioIn this section, you will take a backup of your cluster from `gke-kafka-us-central1` and restore the backup into `gke-kafka-us-west1` . You will perform the backup and restore operation at the application scope, using the `ProtectedApplication` Custom Resource.\nThe following diagram illustrates the components of the disaster recovery solution, and how they relate to each other.To prepare to backup and restore your Kafka cluster, follow these steps:- Set up the environment variables.```\nexport BACKUP_PLAN_NAME=kafka-protected-appexport BACKUP_NAME=protected-app-backup-1export RESTORE_PLAN_NAME=kafka-protected-appexport RESTORE_NAME=protected-app-restore-1export REGION=us-central1export DR_REGION=us-west1export CLUSTER_NAME=gke-kafka-$REGIONexport DR_CLUSTER_NAME=gke-kafka-$DR_REGION\n```\n- Verify the cluster is in a `RUNNING` state.```\ngcloud container clusters describe $CLUSTER_NAME --region us-central1 --format='value(status)'\n``` **Note:** The cluster must be in a `RUNNING` state before a backup can be created.\n- Create a Backup Plan.```\ngcloud beta container backup-restore backup-plans create $BACKUP_PLAN_NAME \\\u00a0 \u00a0 --project=$PROJECT_ID \\\u00a0 \u00a0 --location=$DR_REGION \\\u00a0 \u00a0 --cluster=projects/$PROJECT_ID/locations/$REGION/clusters/$CLUSTER_NAME \\\u00a0 \u00a0 --selected-applications=kafka/kafka,kafka/zookeeper \\\u00a0 \u00a0 --include-secrets \\\u00a0 \u00a0 --include-volume-data \\\u00a0 \u00a0 --cron-schedule=\"0 3 * * *\" \\\u00a0 \u00a0 --backup-retain-days=7 \\\u00a0 \u00a0 --backup-delete-lock-days=0\n```\n- Manually create a Backup. While scheduled backups are typically governed by the cron-schedule in the backup plan, the following example shows how you can initiate a one-time backup operation.```\ngcloud beta container backup-restore backups create $BACKUP_NAME \\\u00a0 \u00a0 --project=$PROJECT_ID \\\u00a0 \u00a0 --location=$DR_REGION \\\u00a0 \u00a0 --backup-plan=$BACKUP_PLAN_NAME \\\u00a0 \u00a0 --wait-for-completion\n```\n- Create a Restore Plan.```\ngcloud beta container backup-restore restore-plans create $RESTORE_PLAN_NAME \\\u00a0 \u00a0 --project=$PROJECT_ID \\\u00a0 \u00a0 --location=$DR_REGION \\\u00a0 \u00a0 --backup-plan=projects/$PROJECT_ID/locations/$DR_REGION/backupPlans/$BACKUP_PLAN_NAME \\\u00a0 \u00a0 --cluster=projects/$PROJECT_ID/locations/$DR_REGION/clusters/$DR_CLUSTER_NAME \\\u00a0 \u00a0 --cluster-resource-conflict-policy=use-existing-version \\\u00a0 \u00a0 --namespaced-resource-restore-mode=delete-and-restore \\\u00a0 \u00a0 --volume-data-restore-policy=restore-volume-data-from-backup \\\u00a0 \u00a0 --selected-applications=kafka/kafka,kafka/zookeeper \\\u00a0 \u00a0 --cluster-resource-scope-selected-group-kinds=\"storage.k8s.io/StorageClass\"\n```\n- Manually restore from a Backup.```\ngcloud beta container backup-restore restores create $RESTORE_NAME \\\u00a0 \u00a0 --project=$PROJECT_ID \\\u00a0 \u00a0 --location=$DR_REGION \\\u00a0 \u00a0 --restore-plan=$RESTORE_PLAN_NAME \\\u00a0 \u00a0 --backup=projects/$PROJECT_ID/locations/$DR_REGION/backupPlans/$BACKUP_PLAN_NAME/backups/$BACKUP_NAME\n```\n- Watch the restored application come up in the backup cluster. It may take a few minutes for all the Pods to be running and ready.```\ngcloud container clusters get-credentials gke-kafka-us-west1 \\\u00a0 \u00a0 --region us-west1kubectl get pod -n kafka --watch\n```Type `CTRL+C` to exit the watch when all Pods are up and running.\n- Validate that previous topics can be fetched by a consumer.```\nkubectl run kafka-client -n kafka --rm -ti \\\u00a0 \u00a0 --image us-docker.pkg.dev/$PROJECT_ID/main/bitnami/kafka:3.4.0 -- bash\n``````\nkafka-console-consumer.sh \\\u00a0 \u00a0 --bootstrap-server kafka.kafka.svc.cluster.local:9092 \\\u00a0 \u00a0 --topic topic1 \\\u00a0 \u00a0 --property print.key=true \\\u00a0 \u00a0 --property key.separator=\" : \" \\\u00a0 \u00a0 --from-beginning;\n```The output is similar to the following:```\n192 : Message number 192\n193 : Message number 193\n197 : Message number 197\n200 : Message number 200\nProcessed a total of 201 messages\n```Type `CTRL+C` to exit the process.\n- Exit the Pod.```\nexit\n```\n## Simulate a Kafka service disruptionIn this section, you will simulate a node failure by replacing a Kubernetes node hosting the broker. This section applies to Standard only. Autopilot manages your nodes for you, so node failure cannot be simulated.- Create a client pod to connect to the Kafka application.```\nkubectl run kafka-client -n kafka --restart='Never' -it \\--image us-docker.pkg.dev/$PROJECT_ID/main/bitnami/kafka:3.4.0 -- bash\n```\n- Create topic `topic-failover-test` and produce test traffic.```\nkafka-topics.sh \\\u00a0 --create \\\u00a0 --topic topic-failover-test \\\u00a0 --partitions 1 \u00a0\\\u00a0 --replication-factor 3 \u00a0\\\u00a0 --bootstrap-server kafka-headless.kafka.svc.cluster.local:9092\n```\n- Determine which broker is the leader for the `topic-failover-test` topic.```\nkafka-topics.sh --describe \\\u00a0 --topic topic-failover-test \\\u00a0 --bootstrap-server kafka-headless.kafka.svc.cluster.local:9092\n```The output is similar to the following:```\nTopic: topic-failover-test  Partition: 0 Leader: 1  Replicas: 1,0,2 Isr: 1,0,2\n```In the output above, `Leader: 1` means that the leader for `topic-failover-test` is broker 1. This corresponds to Pod `kafka-1` .\n- Open a new terminal and connect to the same cluster.```\ngcloud container clusters get-credentials gke-kafka-us-west1 --region us-west1 --project PROJECT_ID\n```\n- Find which node Pod `kafka-1` is running on.```\nkubectl get pod -n kafka kafka-1 -o wide\n```The output is similar to the following:```\nNAME  READY STATUS RESTARTS  AGE IP    NODE            NOMINATED NODE READINESS GATES\nkafka-1 2/2  Running 1 (35m ago) 36m 192.168.132.4 gke-gke-kafka-us-west1-pool-system-a0d0d395-nx72 <none>   <none>\n```In the output above, you see Pod `kafka-1` is running on node `gke-gke-kafka-us-west1-pool-system-a0d0d395-nx72` .\n- Drain the node to evict the Pods.```\nkubectl drain NODE \\\u00a0 --delete-emptydir-data \\\u00a0 --force \\\u00a0 --ignore-daemonsets\n```Replace with the node pod kafka-1 is running on. In this example, the node is `gke-gke-kafka-us-west1-pool-system-a0d0d395-nx72` .The output is similar to the following:```\nnode/gke-gke-kafka-us-west1-pool-system-a0d0d395-nx72 cordoned\nWarning: ignoring DaemonSet-managed Pods: gmp-system/collector-gjzsd, kube-system/calico-node-t28bj, kube-system/fluentbit-gke-lxpft, kube-system/gke-metadata-server-kxw78, kube-system/ip-masq-agent-kv2sq, kube-system/netd-h446k, kube-system/pdcsi-node-ql578\nevicting pod kafka/kafka-1\nevicting pod kube-system/kube-dns-7d48cb57b-j4d8f\nevicting pod kube-system/calico-typha-56995c8d85-5clph\npod/calico-typha-56995c8d85-5clph evicted\npod/kafka-1 evicted\npod/kube-dns-7d48cb57b-j4d8f evicted\nnode/gke-gke-kafka-us-west1-pool-system-a0d0d395-nx72 drained\n```\n- Find which node Pod `kafka-1` is running on.```\nkubectl get pod -n kafka kafka-1 -o wide\n```The output should look similar to the following:```\nNAME  READY STATUS RESTARTS AGE  IP    NODE            NOMINATED NODE READINESS GATES\nkafka-1 2/2  Running 0   2m49s 192.168.128.8 gke-gke-kafka-us-west1-pool-kafka-700d8e8d-05f7 <none>   <none>\n```From the output above, you see the application is running on a new node.\n- In the terminal connected to the `kafka-client` Pod, determine which broker is the leader for `topic-failover-test` .```\nkafka-topics.sh --describe \\\u00a0 --topic topic-failover-test \\\u00a0 --bootstrap-server kafka-headless.kafka.svc.cluster.local:9092\n```The output should look similar to the following:```\nTopic: topic-failover-test  TopicId: bemKyqmERAuKZC5ymFwsWg PartitionCount: 1  ReplicationFactor: 3 Configs: flush.ms=1000,segment.bytes=1073741824,flush.messages=10000,max.message.bytes=1000012,retention.bytes=1073741824\n Topic: topic-failover-test  Partition: 0 Leader: 1  Replicas: 1,0,2 Isr: 0,2,1\n```In the example output, the leader is still 1 . However, it's now running on a new node.\n### Testing Kafka leader failure\n- In Cloud Shell, connect to the Kafka client, and use `describe` to view the elected leader for each partition in `topic1` .```\nkafka-topics.sh --describe \\\u00a0 --topic topic1 \\\u00a0 --bootstrap-server kafka-headless.kafka.svc.cluster.local:9092\n```The output is similar to the following:```\nTopic: topic1 TopicId: B3Jr_5t2SPq7F1jVHu4r0g PartitionCount: 3  ReplicationFactor: 3 Configs: flush.ms=1000,segment.bytes=1073741824,flush.messages=10000,max.message.bytes=1000012,retention.bytes=1073741824\n Topic: topic1 Partition: 0 Leader: 0  Replicas: 0,2,1 Isr: 0,2,1\n Topic: topic1 Partition: 1 Leader: 0  Replicas: 2,1,0 Isr: 0,2,1\n Topic: topic1 Partition: 2 Leader: 0  Replicas: 1,0,2 Isr: 0,2,1\n```\n- In the Cloud Shell not connected to the Kafka client, delete the `kafka-0` leader broker to force a new leader election. You should delete the index that maps to one of the leaders in the previous output.```\nkubectl delete pod -n kafka kafka-0 --force\n```The output is similar to the following:```\npod \"kafka-0\" force deleted\n```\n- In the Cloud Shell connected to the Kafka client, and use `describe` to view the elected leader.```\nkafka-topics.sh --describe \\\u00a0 --topic topic1 \\\u00a0 --bootstrap-server kafka-headless.kafka.svc.cluster.local:9092\n```The output is similar to the following:```\nTopic: topic1 TopicId: B3Jr_5t2SPq7F1jVHu4r0g PartitionCount: 3  ReplicationFactor: 3 Configs: flush.ms=1000,segment.bytes=1073741824,flush.messages=10000,max.message.bytes=1000012,retention.bytes=1073741824\n Topic: topic1 Partition: 0 Leader: 2  Replicas: 0,1,2 Isr: 2,0,1\n Topic: topic1 Partition: 1 Leader: 2  Replicas: 2,0,1 Isr: 2,0,1\n Topic: topic1 Partition: 2 Leader: 2  Replicas: 1,2,0 Isr: 2,0,1\n```In the output, the new leader for each partition changes, if it was assigned to the leader that was interrupted ( `kafka-0` ). This indicates that the original leader was replaced when the Pod was deleted and re-created.\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the projectThe easiest way to avoid billing is to delete the project you created for the tutorial.\n **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\nDelete a Google Cloud project:\n```\ngcloud projects delete PROJECT_ID\n```## What's next\n- For a fully-managed and scalable messaging service, see [Migrate from Kafka to Pub/Sub](/pubsub/docs/migrating-from-kafka-to-pubsub) .", "guide": "Google Kubernetes Engine (GKE)"}