{"title": "Google Kubernetes Engine (GKE) - Consuming reserved zonal resources", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/consuming-reservations", "abstract": "# Google Kubernetes Engine (GKE) - Consuming reserved zonal resources\nYou can reserve Compute Engine instances in a specific zone to ensure resources are available for their workloads when needed. For more details on how to manage reservations, see [Reserving Compute Engine zonal resources](/compute/docs/instances/reserving-zonal-resources) .\nAfter creating reservations, you can consume the reserved resources in GKE. GKE supports the same consumption modes as Compute Engine:\n- Consuming resources fromreservations: Standard only\n- Consuming resources from areservation: Standard and Autopilot\n- Creating nodes without consuming any reservations: Standard and Autopilot", "content": "## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.## Consume capacity reservations in Autopilot clusters\nAutopilot clusters support consuming resources from specific Compute Engine capacity reservations in the same project or in a shared project. Unless explicitly specified, Autopilot clusters don't consume reservations. These reservations qualify for Autopilot [Committed Use Discounts](/kubernetes-engine/cud) . You must use the `Accelerator` compute class or the `Performance` compute class to consume capacity reservations.\n- Before you begin, [create an Autopilot cluster](/kubernetes-engine/docs/how-to/creating-an-autopilot-cluster) running the following versions:- To consume reserved accelerators using the Accelerator compute class: 1.28.6-gke.1095000 or later\n- To use the Performance compute class: 1.28.6-gke.1369000 and later or version 1.29.1-gke.1575000 and later.\n### Create capacity reservations for Autopilot\nAutopilot Pods can consume reservations in the same project as the cluster or in a shared reservation from a different project. You can consume the reserved hardware by explicitly referencing that reservation in your manifest. You can consume reservations in Autopilot for the following types of hardware:\n- Any of the following types of GPUs:- `nvidia-h100-80gb`: NVIDIA H100 (80GB) (only available with Accelerator compute class)\n- `nvidia-a100-80gb`: NVIDIA A100 (80GB)\n- `nvidia-tesla-a100`: NVIDIA A100 (40GB)\n- `nvidia-l4`: NVIDIA L4\n- `nvidia-tesla-t4`: NVIDIA T4To create a capacity reservation, see the following resources. Ensure that the machine types, accelerator types, and accelerator quantities match what your workloads will consume.\n- [Create a reservation for a single project](/compute/docs/instances/reservations-single-project) \n- [Create a shared reservation](/compute/docs/instances/reservations-shared) \n### Consume a specific reservation in the same project in Autopilot\nThis section shows you how to consume a specific capacity reservation that's in the same project as your cluster.\n- Save the following manifest as `specific-autopilot.yaml` . This manifest has node selectors that consume a specific reservation.\n```\napiVersion: v1kind: Podmetadata:\u00a0 name: specific-same-project-podspec:\u00a0 nodeSelector:\u00a0 \u00a0 cloud.google.com/compute-class: Performance\u00a0 \u00a0 cloud.google.com/machine-family: MACHINE_SERIES\u00a0 \u00a0 cloud.google.com/reservation-name: RESERVATION_NAME\u00a0 \u00a0 cloud.google.com/reservation-affinity: \"specific\"\u00a0 containers:\u00a0 - name: my-container\u00a0 \u00a0 image: \"k8s.gcr.io/pause\"\u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 cpu: 12\u00a0 \u00a0 \u00a0 \u00a0 memory: \"50Gi\"\u00a0 \u00a0 \u00a0 \u00a0 ephemeral: \"200Gi\"\n```\nReplace the following:- ``: a machine series that contains the machine type of the VMs in your specific capacity reservation. For example, if your reservation is for`c3-standard-4`machine types, specify`C3`in the``field.\n- ``: the name of the Compute Engine capacity reservation.\n```\napiVersion: v1kind: Podmetadata:\u00a0 name: specific-same-project-podspec:\u00a0 nodeSelector:\u00a0 \u00a0 cloud.google.com/compute-class: \"Accelerator\"\u00a0 \u00a0 cloud.google.com/gke-accelerator: ACCELERATOR\u00a0 \u00a0 cloud.google.com/reservation-name: RESERVATION_NAME\u00a0 \u00a0 cloud.google.com/reservation-affinity: \"specific\"\u00a0 containers:\u00a0 - name: my-container\u00a0 \u00a0 image: \"k8s.gcr.io/pause\"\u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 cpu: 12\u00a0 \u00a0 \u00a0 \u00a0 memory: \"50Gi\"\u00a0 \u00a0 \u00a0 \u00a0 ephemeral: \"200Gi\"\u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: QUANTITY\n```\nReplace the following:- ``: the accelerator that you reserved in the Compute Engine capacity reservation. Must be one of the following values:- `nvidia-h100-80gb`: NVIDIA H100 (80GB) (only available with Accelerator compute class)\n- `nvidia-a100-80gb`: NVIDIA A100 (80GB)\n- `nvidia-tesla-a100`: NVIDIA A100 (40GB)\n- `nvidia-l4`: NVIDIA L4\n- `nvidia-tesla-t4`: NVIDIA T4\n- ``: the name of the Compute Engine capacity reservation.\n- ``: the number of GPUs to attach to the container. Must be a supported quantity for the specified GPU, as described in [Supported GPU quantities](/kubernetes-engine/docs/how-to/autopilot-gpus#supported-quantities) .- Deploy the Pod:```\nkubectl apply -f specific-autopilot.yaml\n```\nAutopilot uses the reserved capacity in the specified reservation to provision a new node to place the Pod.\n### Consume a specific shared reservation in Autopilot\nThis section uses the following terms:\n- Owner project: the project that owns the reservation and shares it with other projects.\n- Consumer project: the project that runs the workloads that consume the shared reservation.\nTo consume a shared reservation, you must grant the GKE service agent access to the reservation in the project that owns the reservation. Do the following:\n- Create a custom IAM role that contains the `compute.reservations.list` permission in the owner project:```\ngcloud iam roles create ROLE_NAME \\\u00a0 \u00a0 --project=OWNER_PROJECT_ID \\\u00a0 \u00a0 --permissions='compute.reservations.list'\n```Replace the following:- ``: a name for your new role.\n- ``: the project ID of the project that owns the capacity reservation.\n- Give the GKE service agent in the consumer project access to the shared reservation in the owner project:```\ngcloud compute reservations add-iam-policy-binding RESERVATION_NAME \\\u00a0 \u00a0 --project=OWNER_PROJECT_ID \\\u00a0 \u00a0 --zone=ZONE \\\u00a0 \u00a0 --member=service-CONSUMER_PROJECT_NUMBER@container-engine-robot.iam.gserviceaccount.com \\\u00a0 \u00a0 --role='roles/ROLE_NAME'\n```Replace `` with the numerical project number of your consumer project. To find this number, see [Identifying projects](/resource-manager/docs/creating-managing-projects#identifying_projects) in the Resource Manager documentation.\n- Save the following manifest as `shared-autopilot.yaml` . This manifest has nodeSelectors that tell GKE to consume a specific shared reservation.\n```\napiVersion: v1kind: Podmetadata:\u00a0 name: performance-podspec:\u00a0 nodeSelector:\u00a0 \u00a0 cloud.google.com/compute-class: Performance\u00a0 \u00a0 cloud.google.com/machine-family: MACHINE_SERIES\u00a0 \u00a0 cloud.google.com/reservation-name: RESERVATION_NAME\u00a0 \u00a0 cloud.google.com/reservation-project: OWNER_PROJECT_ID\u00a0 \u00a0 cloud.google.com/reservation-affinity: \"specific\"\u00a0 containers:\u00a0 - name: my-container\u00a0 \u00a0 image: \"k8s.gcr.io/pause\"\u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 cpu: 12\u00a0 \u00a0 \u00a0 \u00a0 memory: \"50Gi\"\u00a0 \u00a0 \u00a0 \u00a0 ephemeral: \"200Gi\"\n```\nReplace the following:- ``: a machine series that contains the machine type of the VMs in your specific capacity reservation. For example, if your reservation is for`c3-standard-4`machine types, specify`C3`in the``field.\n- ``: the name of the Compute Engine capacity reservation.\n- ``: the project ID of the project that owns the capacity reservation.\n```\napiVersion: v1kind: Podmetadata:\u00a0 name: specific-same-project-podspec:\u00a0 nodeSelector:\u00a0 \u00a0 cloud.google.com/compute-class: \"Accelerator\"\u00a0 \u00a0 cloud.google.com/gke-accelerator: ACCELERATOR\u00a0 \u00a0 cloud.google.com/reservation-name: RESERVATION_NAME\u00a0 \u00a0 cloud.google.com/reservation-project: OWNER_PROJECT_ID\u00a0 \u00a0 cloud.google.com/reservation-affinity: \"specific\"\u00a0 containers:\u00a0 - name: my-container\u00a0 \u00a0 image: \"k8s.gcr.io/pause\"\u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 cpu: 12\u00a0 \u00a0 \u00a0 \u00a0 memory: \"50Gi\"\u00a0 \u00a0 \u00a0 \u00a0 ephemeral: \"200Gi\"\u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: QUANTITY\n```\nReplace the following:- ``: the accelerator that you reserved in the Compute Engine capacity reservation. Must be one of the following values:- `nvidia-h100-80gb`: NVIDIA H100 (80GB) (only available with Accelerator compute class)\n- `nvidia-a100-80gb`: NVIDIA A100 (80GB)\n- `nvidia-tesla-a100`: NVIDIA A100 (40GB)\n- `nvidia-l4`: NVIDIA L4\n- `nvidia-tesla-t4`: NVIDIA T4\n- ``: the name of the Compute Engine capacity reservation.\n- ``: the project ID of the project that owns the capacity reservation.\n- ``: the number of GPUs to attach to the container. Must be a supported quantity for the specified GPU, as described in [Supported GPU quantities](/kubernetes-engine/docs/how-to/autopilot-gpus#supported-quantities) .- Deploy the Pod:```\nkubectl apply -f shared-autopilot.yaml\n```\nAutopilot uses the reserved capacity in the specified reservation to provision a new node to place the Pod.\n## Consuming reserved instances in GKE Standard\nWhen you create a cluster or node pool, you can indicate the reservation consumption mode by specifying the `--reservation-affinity` flag.\n### Consuming any matching reservations\nTo consume from any matching reservations automatically, set the reservation affinity flag to `--reservation-affinity=any` .\n**Note:** Since `any` is the default value defined in Compute Engine, you can omit the reservation affinity flag entirely.\nIn the `any` reservation consumption mode, nodes first take capacity from all single-project reservations before any shared reservations, because the shared reservations are more available to other projects. For more information about how instances are automatically consumed see [Consumption order](/compute/docs/instances/reservations-overview#consumption-order) .\nTo create a reservation and instances to consume any reservation, perform the following steps:\n- Create a reservation of three VM instances:```\ngcloud compute reservations create RESERVATION_NAME \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE --vm-count=3\n```Replace the following:- ``: the name of the reservation to create.\n- ``: the [type of machine](/compute/docs/machine-types#machine_types) (name only) to use for the reservation. For example,`n1-standard-2`.\n- Verify the reservation was created successfully:```\ngcloud compute reservations describe RESERVATION_NAME\n```Replace `` with the name of the reservation you just created.\n- Create a cluster having one node to consume matching reservation:```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE --num-nodes=1 \\\u00a0 \u00a0 --reservation-affinity=any\n```Replace the following:- ``: the name of the cluster to create.\n- ``: the [type of machine](/compute/docs/machine-types#machine_types) (name only) to use for the cluster. For example`n1-standard-2`.\n- Create a node pool with three nodes to consume any matching reservation:```\ngcloud container node-pools create NODEPOOL_NAME \\\u00a0 \u00a0 --cluster CLUSTER_NAME --num-nodes=3 \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE --reservation-affinity=any\n```Replace the following:- ``: the name of the node pool to create.\n- ``: the name of the cluster you created earlier.\n- ``: the [type of machine](/compute/docs/machine-types#machine_types) (name only) to use for the node pool. For example`n1-standard-2`.The total number of nodes is four, which exceeds the capacity of the reservation. Three of the nodes consume the reservation while the last node takes capacity from the general Compute Engine resource pool.\n### Consuming a specific single-project reservation\nTo consume a reservation, set the reservation affinity flag to `--reservation-affinity=specific` and provide the specific reservation name. In this mode, instances must take capacity from the specified reservation in the zone. The request fails if the reservation does not have sufficient capacity.\nTo create a reservation and instances to consume a specific reservation, perform the following steps:\n- Create a specific reservation of three VM instances:```\ngcloud compute reservations create RESERVATION_NAME \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE --vm-count=3 \\\u00a0 \u00a0 --require-specific-reservation\n```Replace the following:- ``: the name of the reservation to create.\n- ``: the [type of machine](/compute/docs/machine-types#machine_types) (name only) to use for the reservation. For example,`n1-standard-2`.\n- Create a node pool with a single node to consume a specific single-project reservation:```\ngcloud container node-pools create NODEPOOL_NAME \\\u00a0 \u00a0 --cluster CLUSTER_NAME \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE --num-nodes=1 \\\u00a0 \u00a0 --reservation-affinity=specific --reservation=RESERVATION_NAME\n```Replace the following:- ``: the name of the node pool to create.\n- ``: the name of the cluster that you created.\n- ``: the [type of machine](/compute/docs/machine-types#machine_types) (name only) to use for the cluster. For example`n1-standard-2`.\n- ``: the name of the reservation to consume.\n### Consuming a specific shared reservation\nTo create a specific shared reservation and consume the shared reservation, perform the following steps:\n- Follow the steps in [Allowing and restricting projects from creating and modifying shared reservations](/compute/docs/instances/reservations-shared#shared_reservation_constraint) .\n- Create a specific shared reservation:```\ngcloud compute reservations create RESERVATION_NAME \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE --vm-count=3 \\\u00a0 \u00a0 --zone=ZONE \\\u00a0 \u00a0 --require-specific-reservation \\\u00a0 \u00a0 --project=OWNER_PROJECT_ID \\\u00a0 \u00a0 --share-setting=projects \\\u00a0 \u00a0 --share-with=CONSUMER_PROJECT_IDS\n```Replace the following:- ``: the name of reservation to create.\n- ``: the name of the [type of machine](/compute/docs/machine-types#machine_types) to use for the reservation. For example,`n1-standard-2`.\n- ``: the project ID of the project that you want to create this shared reservation. If you omit the`--project`flag, GKE uses the current project as the owner project by default.\n- ``: a comma-separated list of the [project IDs](/resource-manager/docs/creating-managing-projects#identifying_projects) of projects that you want to share this reservation with. For example,`project-1,project-2`. You can include 1 to 100 consumer projects. These projects must be in the same organization as the owner project. Don't include the``, because it can consume this reservation by default.\n- Consume the shared reservation:```\n\u00a0gcloud container node-pools create NODEPOOL_NAME \\\u00a0 \u00a0 \u00a0--cluster CLUSTER_NAME \\\u00a0 \u00a0 \u00a0--machine-type=MACHINE_TYPE --num-nodes=1 \\\u00a0 \u00a0 --reservation-affinity=specific \\\u00a0 \u00a0 --reservation=projects/OWNER_PROJECT_ID/reservations/RESERVATION_NAME\n```Replace the following:- ``: the name of the node pool to create.\n- ``: the name of the cluster that you created.\n- ``: the name of the type of machine to use for the cluster. For example`n1-standard-2`.\n- ``: the project ID where the shared reservation is created.\n- ``: the name of the specific shared reservation to consume.\nWhen a node pool is created with specific reservation affinity, including default node pools during cluster creation, its size is limited to the capacity of the specific reservation over the node pool's entire lifetime. This affects the following GKE features:\n- **Cluster with multiple zones** : In regional or multi-zonal clusters, nodes of a node pool can span across multiple zones. Since reservations are single-zonal, multiple reservations are needed. To create a node pool consuming specific reservation in these clusters, you must create a specific reservation with exactly the same name and machine properties in each zone of the node pool.\n- **Cluster autoscaling and node pool upgrades** : If you don't have extra capacity in the specific reservation, node pool upgrades or autoscaling of the node pool might fail because both operations require creating extra instances. To resolve this, you can [change the size of the reservation](/compute/docs/instances/reserving-zonal-resources#resizing_a_reservation) , or free up some of its bounded resources.\n### Creating nodes without consuming reservations\nTo explicitly avoid consuming resources from any reservations, set the affinity to `--reservation-affinity=none` .\n- Create a cluster that won't consume any reservation:```\ngcloud container clusters create CLUSTER_NAME --reservation-affinity=none\n```Replace `` with the name of the cluster to create.\n- Create a node pool that won't consume any reservation:```\ngcloud container node-pools create NODEPOOL_NAME \\\u00a0 \u00a0 --cluster CLUSTER_NAME \\\u00a0 \u00a0 --reservation-affinity=none\n```Replace the following:- ``: the name of the node pool to create.\n- ``: the name of the cluster you created earlier.\n### Following available reservations between zones\nWhen using node pools running in multiple zones with reservations that are not equal between zones, you can use the flag `--location_policy=ANY` . This ensures that when new nodes are added to the cluster they are created in the zone that still has unused reservations.\n## TPU reservation\nTPU reservations differ from other machine types. The following are TPU-specific aspects you should consider when creating TPU reservations:\n- When using [TPUs in GKE](/kubernetes-engine/docs/concepts/tpus) ,`SPECIFIC`is the only supported value for the`--reservation-affinity`flag of`gcloud container node-pools create`.\n- TPU reservations cannot be shared across projects.\nFor more information, see [TPU reservations](/kubernetes-engine/docs/concepts/tpus#tpu_reservation) .\n## Cleaning up\nTo avoid incurring charges to your Cloud Billing account for the resources used in this page:\n- Delete the clusters you created by running the following command for each of the clusters:```\ngcloud container clusters delete CLUSTER_NAME\n```\n- Delete the reservations you created by running the following command for each of the reservations:```\ngcloud compute reservations delete RESERVATION_NAME\n```## What's next\n- [Learn more about reserving Compute Engine zonal resources](/compute/docs/instances/reserving-zonal-resources) .\n- [Learn more about node pools](/kubernetes-engine/docs/concepts/node-pools) .\n- [Learn more about cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) .\n- [Learn more about node upgrade strategies](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies) .", "guide": "Google Kubernetes Engine (GKE)"}