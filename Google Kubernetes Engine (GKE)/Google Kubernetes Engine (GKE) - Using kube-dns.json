{"title": "Google Kubernetes Engine (GKE) - Using kube-dns", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/kube-dns", "abstract": "# Google Kubernetes Engine (GKE) - Using kube-dns\nThis page describes how Google Kubernetes Engine (GKE) implements service discovery using kube-dns, the default DNS provider for GKE clusters.\nFor Autopilot clusters, you cannot modify the default kube-dns configuration.\n", "content": "## Architecture\nWhen you create a cluster, GKE automatically deploys kube-dns pods in the `kube-system` namespace. Pods access the kube-dns deployment through a corresponding [Service](/kubernetes-engine/docs/concepts/service) that groups the kube-dns pods and gives them a single IP address (ClusterIP). By default, all pods in a cluster use this Service to resolve DNS queries. The following diagram shows the relationship between pods and the kube-dns Service.\nkube-dns scales to meet the DNS demands of the cluster. This scaling is controlled by the `kube-dns-autoscaler` , a Pod that is deployed by default in all GKE clusters. The `kube-dns-autoscaler` adjusts the number of replicas in the kube-dns Deployment based on the number of nodes and cores in the cluster.\nkube-dns supports up to 1000 endpoints per [headless service](/kubernetes-engine/docs/concepts/service#types-of-services) .\n## How Pod DNS is configured\nThe kubelet running on each Node configures the Pod's `etc/resolv.conf` to use the kube-dns service's ClusterIP. The following example configuration shows that the IP address of the kube-dns service is `10.0.0.10` . This IP address is different in other clusters.\n```\nnameserver 10.0.0.10\nsearch default.svc.cluster.local svc.cluster.local cluster.local c.my-project-id.internal google.internal\noptions ndots:5\n```\nkube-dns is the authoritative name server for the cluster domain (cluster.local) and it resolves external names recursively. Short names that are not fully qualified, such as `myservice` , are completed first with local search paths.\n**Note:** In GKE Autopilot clusters, in version 1.27 and later, [the default Service IP range is a Google-managed range:34.118.224.0/20](/kubernetes-engine/docs/release-notes#July_24_2023) and the `kube-dns` service will be addressed `34.118.x.10` by default (x being in in the range 224-239).\n## Adding custom resolvers for stub domains\nYou can modify the [ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/) for kube-dns to set stub domains as part of DNS infrastructure within your clusters.\nStub domains let you configure custom per-domain resolvers so that kube-dns forwards DNS requests to specific upstream DNS servers when resolving these domains.\n**Note:** When you set a custom resolver for a stub domain, such as **example.com** , kube-dns forwards all name resolution requests to the defined server(s) including **example.com and * **.example.com** .\nThe following example ConfigMap manifest for kube-dns includes a `stubDomains` configuration that sets custom resolvers for the domain example.com.\n```\napiVersion: v1kind: ConfigMapmetadata:\u00a0 labels:\u00a0 \u00a0 addonmanager.kubernetes.io/mode: EnsureExists\u00a0 name: kube-dns\u00a0 namespace: kube-systemdata:\u00a0 stubDomains: |\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"example.com\": [\u00a0 \u00a0 \u00a0 \u00a0 \"8.8.8.8\",\u00a0 \u00a0 \u00a0 \u00a0 \"8.8.4.4\",\u00a0 \u00a0 \u00a0 \u00a0 \"1.1.1.1\",\u00a0 \u00a0 \u00a0 \u00a0 \"1.0.0.1\"\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 }\n```\nRun the following command to open a text editor:\n```\nkubectl edit configmap kube-dns -n kube-system\n```\nReplace the contents of the file with the manifest and then exit the text editor to apply the manifest to the cluster.\n## Upstream nameservers\nIf you modify the [ConfigMap](https://kubernetes.io/docs/concepts/configuration/configmap/) for kube-dns to include `upstreamNameservers` , kube-dns forwards all DNS requests except `*.cluster.local` to those servers. This includes `metadata.internal` and `*.google.internal` , which are not resolvable by the upstream server.\nIf you enable [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) or any workloads that rely on `metadata.internal` resolution, to retain `*.internal` name resolution, add a `stubDomain` to the ConfigMap.\n```\ndata:\u00a0 stubDomains: |\u00a0 \u00a0 {\u00a0 \u00a0 \u00a0 \"internal\": [\u00a0 \u00a0 \u00a0 \u00a0 \"169.254.169.254\"\u00a0 \u00a0 \u00a0 ]\u00a0 \u00a0 }\u00a0 upstreamNameservers: |\u00a0 \u00a0 [\"8.8.8.8\"]\n```\n## Known issues\n### Search domain limit\nThere is a limit of 6 DNS search domains for `/etc/resolv.conf` . If you define more than 6 search domains, the following warning appears when you run the command `kubectl describe pod` :\n```\nSearch Line limits were exceeded, some search paths have been omitted, the applied search line is: default.svc.cluster.local svc.cluster.local cluster.local c.<project ID>.internal google.internal\n```\nThis warning is logged in Cloud Logging in the container logs section.\nTo resolve this issue, remove the extra search paths from the configuration.\n### Consider the upstreamNameservers limit\nKubernetes imposes a limit of up to three `upstreamNameservers` values. If you define more than three `upstreamNameservers` , you see the following error in Cloud Logging in the `kube-dns` deployment logs:\n```\nInvalid configuration: upstreamNameserver cannot have more than three entries (value was &TypeMeta{Kind:,APIVersion:,}), ignoring update\n```\nWhen this happens, kube-dns behaves as if it has no `upstreamNameservers` configured. To resolve this issue, remove the extra `upstreamNameservers` from the configuration.\n### Performance limitations with kube-dns\nIf you are experiencing high latency with DNS lookups or DNS resolution failures with the default kube-dns provider, this might be caused by:\n- Performing frequent DNS lookups within your workload\n- Deploying a [higher Pod density per node](/kubernetes-engine/docs/best-practices/networking#pod-density-per-node) .\n- Running kube-dns on Spot or preemptible VMs, which can lead to unexpected node deletions and subsequent DNS resolution issues.\nTo improve DNS lookup times, you can choose one of the following options:\n- Avoid running critical system components like kube-dns on Spot or preemptible VMs. Using Spot or preemptible VMs for DNS can cause failures and disrupt your cluster.\n- As best practices, create at least one node pool comprised of standard (non-Spot or preemptible) VMs to host critical system components like kube-dns. To ensure that critical workloads are only scheduled on the reliable node pool preventing them from running on Spot or preemptible VMs, you can [use taints and tolerations for Spot VMs](/kubernetes-engine/docs/how-to/spot-vms#taints-tolerations) .\n- Enable [NodeLocal DNSCache](/kubernetes-engine/docs/how-to/nodelocal-dns-cache) .\n- [Scale up kube-dns](/kubernetes-engine/docs/how-to/nodelocal-dns-cache#scaling_up_kube-dns) .\n- Ensure that your application uses [dns.resolve*](https://nodejs.org/api/dns.html#dnsresolve-dnsresolve-and-dnsreverse) based functions rather than [dns.lookup](https://nodejs.org/api/dns.html#dnslookup) based function as dns.lookup is synchronous. dns.resolve* functions always perform an asynchronous DNS query on the network.\n**Note:** Even with NodeLocal DNSCache enabled, you might encounter DNS resolution failures if both `kube-dns` and NodeLocal DNSCache are upgraded as part of the cluster upgrade. However, only some nodes will be affected as only 10% nodes will be unavailable at any given time. We recommend that you validate your upgrades in test environments before upgrading in the production environment.\n### Service DNS records\nkube-dns only creates DNS records for Services that have [Endpoints](https://kubernetes.io/docs/concepts/services-networking/service/#endpoints) .\n### Large TTL from DNS upstream servers\nIf kube-dns receives a DNS response from an upstream DNS resolver with a large or \"infinite\" TTL, it keeps this TTL value for the DNS entry in the cache. The entry never expires and could create a discrepancy between the entry and the actual IP address resolved for the TTL name.\nGKE resolves this issue in the following control plane versions by setting a max TTL value to 30 seconds for any DNS response that has a TTL higher than 30 seconds:\n- 1.21.14-gke.9100\n- 1.22.15-gke.2100\n- 1.23.13-gke.500\n- 1.24.7-gke.500\n- 1.25.2-gke.500 or later\nThis behavior is similar to [NodeLocal DNSCache](/kubernetes-engine/docs/how-to/nodelocal-dns-cache) .\n## What's next\n- Read an [overview](/kubernetes-engine/docs/concepts/service-discovery) of cluster DNS in GKE.\n- Read [DNS for Services and Pods](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service) for a general overview of how DNS is used in Kubernetes clusters.", "guide": "Google Kubernetes Engine (GKE)"}