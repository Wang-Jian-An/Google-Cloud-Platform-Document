{"title": "Google Kubernetes Engine (GKE) - About TPUs in GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/tpus", "abstract": "# Google Kubernetes Engine (GKE) - About TPUs in GKE\nThis page introduces Cloud TPU and shows you where to find information on using Cloud TPU with Google Kubernetes Engine (GKE). Tensor Processing Units (TPUs) are Google's custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads that use frameworks such as [TensorFlow](https://www.tensorflow.org/) , [PyTorch](https://pytorch.org/) , and [JAX](https://github.com/google/jax) .\nBefore you use TPUs in GKE, we recommend that you complete the following learning path:\n- Learn how machine learning accelerators work with the [Introduction to Cloud TPU](/tpu/docs/intro-to-tpu) .\n- Learn about current TPU version availability with the [Cloud TPU system architecture](/tpu/docs/system-architecture-tpu-vm#versions) .\nTo learn how to set up Cloud TPU in GKE, see [Deploy TPU workloads in GKE](/kubernetes-engine/docs/how-to/tpus) .\n", "content": "## Benefits of using TPUs in GKE\nGKE provides full support for TPU VM lifecycle management, including creating, configuring, and deleting TPU VMs. GKE also supports [Spot VMs](/spot-vms) and using [reservedCloud TPU](/tpu/docs/tpus-in-gke#migrate_your_tpu_reservations) . The benefits of using TPUs in GKE include:\n- **Consistent operational environment** : A single platform for all machine learning and other workloads.\n- **Automatic upgrades** : GKE automates version updates which reduces operational overhead.\n- **Load balancing** : GKE distributes the load reducing latency and improving reliability.\n- **Responsive scaling** : GKE automatically scales TPU resources to meet the needs of your workloads.\n- **Resource management** : With [Kueue](https://kueue.sigs.k8s.io/docs/overview/#why-use-kueue) , a Kubernetes-native job queuing system, you can manage resources across multiple tenants within your organization using queuing, preemption, prioritization, and fair sharing.## Terminology related to TPU in GKE\nTPU and Kubernetes use some similar terms that you need to consider and differentiate as you read this document:\n- **TPU VM** : A Compute Engine VM running on a physical machine with TPU hardware.\n- **TPU Pod** : A collection of interconnected TPU chips. The number of TPU chips in a TPU Pod varies by TPU version.\n- **TPU slice:** A subset of a full TPU Pod.\n- **TPU Topology** : The number and physical arrangement of the TPU chips in a TPU slice.\n- **Atomicity** : The property of a multi-host TPU node pool where the node pool is treated as a single unit. You cannot resize a multi-host node pool. GKE scales a multi-host node pool by creating or removing all nodes in the node pool in a single step.## How TPUs in GKE work\nKubernetes resource management and priority treats TPU VMs the same as other VM types. You request TPU chips using the resource name `google.com/tpu` :\n```\n\u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4\u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4\n```\nWhen you use TPUs in GKE, you must consider the following TPU characteristics:\n- A TPU VM can access up to 8 TPU chips.\n- A TPU slice contains a fixed number of TPU chips that depends on the TPU machine type you choose.\n- The number of requested`google.com/tpu`must be equal to the total number of available chips on the TPU node. Any container in a GKE Pod that requests TPUs must consumethe TPU chips in the node. Otherwise, your Deployment fails, because GKE can't partially consume TPU resources. For example, see the following scenarios:- The machine type`ct5l-hightpu-8t`has a single TPU node with 8 TPU chips. One GKE Pod that requires 8 TPU chips can be deployed on the node, but 2 GKE Pods that require 4 TPU chips eachbe deployed on a node.\n- The machine type`ct5lp-hightpu-4t`with a`2x4`topology contains two TPU nodes with 4 chips each, a total of 8 TPU chips. A GKE Pod that requires 8 TPU chipsbe deployed in any of the nodes in this node pool, but 2 Pods that require 4 TPU chips each can be deployed on the 2 nodes in the node pool.\n- Multiple Kubernetes Pods can be scheduled on a TPU VM, but only one container in each Pod can access the TPU chips.\n- Each cluster must have at least one non-TPU node pool to create kube-system Pods, such as kube-dns.\n- By default, TPU nodes have the`google.com/tpu` [taint](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) that prevents non-TPU Pods from being scheduled on them. The taint does not guarantee TPU resources are fully utilized. It allows you to run workloads that doesn't use TPUs on non-TPU nodes, freeing up compute on TPU nodes for code that uses TPUs.\n- GKE collects the logs emitted by containers running on TPU VMs. To learn more, see [Logging](/kubernetes-engine/docs/how-to/tpus#logging) .\n- TPU utilization metrics, such as runtime performance, are available in Cloud Monitoring. To learn more, see [Observability and metrics](/kubernetes-engine/docs/how-to/tpus#metrics) .\n### Machine type\nMachine types that supports TPU resources follow a naming convention that includes the TPU version and the number of chips per node such as `ct<version>-hightpu-<node-chip-count>t` . For example, the machine type `ct5lp-hightpu-1t` supports TPU v5e and contains one TPU chip in total.\n### Types of TPU node pool\nBased on the topology you define, GKE places the TPU workloads in one of the following node pool types:\n- **Single-host TPU slice node pool** : A node pool that contains one or moreTPU VMs. In single-host TPU slice node pools, the TPUs attached to the VMs aren't interconnected by high-speed interconnects.\n- **Multi-host TPU slice node pool** : A node pool that contains two or moreTPU VMs. This type of node pool is atomic and immutable, which means that you can't manually add nodes to the node pool. In case of machine failure or shutdown, GKE recreates the entire node pool as a new atomic unit.\n### Topology\nThe topology defines the physical arrangement of TPUs within a TPU slice. GKE provisions a TPU slice in [two- or three-dimensional topologies](/tpu/docs/types-topologies) depending on the TPU version. You specify a topology as the number of TPU chips in each dimension:\n- For TPU v4 and v5p scheduled in multi-host TPU slice node pools, you define the topology in 3-tuples ( `{A}x{B}x{C}` ), for example `4x4x4` . The product of `{A}x{B}x{C}` defines the number of chips in the node pool. For example, you can define small topologies smaller than 64 chips with topology forms such as `2x2x2` , `2x2x4` , or `2x4x4` . If you use topologies larger than 64 chips, the values you assign to {A},{B}, and {C} must meet the following conditions:- {A},{B}, and {C} are multiples of four.\n- The largest topology supported for v4 is`12x16x16`and v5p is`16x16x24`.\n- The assigned values keep the A \u2264 B \u2264 C pattern. For example,`4x4x8`or`8x8x8`.\n- For TPU v5e, topologies follow a 2-tuple ( `{A}x{B}` ) format, for example `2x2` . To define your TPU configuration, refer to the table in [Mapping of TPU configuration](/kubernetes-engine/docs/concepts/tpus#configuration) .## TPU availability in GKE\nThe following table lists the TPU availability depending on the machine type and version:\n| TPU version | Machine type beginning with | Minimum GKE version | Availability  | Zone   |\n|:--------------|:------------------------------|:----------------------|:--------------------|:----------------|\n| TPU v4  | ct4p-       | 1.26.1-gke.1500  | Generally Available | us-central2-b |\n| TPU v5e  | ct5l-       | 1.27.2-gke.2100  | Generally Available | europe-west4-b |\n| TPU v5e  | ct5l-       | 1.27.2-gke.2100  | Generally Available | us-central1-a |\n| TPU v5e  | ct5lp-      | 1.27.2-gke.2100  | Generally Available | europe-west4-a1 |\n| TPU v5e  | ct5lp-      | 1.27.2-gke.2100  | Generally Available | us-central1-a1 |\n| TPU v5e  | ct5lp-      | 1.27.2-gke.2100  | Generally Available | us-east1-c  |\n| TPU v5e  | ct5lp-      | 1.27.2-gke.2100  | Generally Available | us-east5-b1  |\n| TPU v5e  | ct5lp-      | 1.27.2-gke.2100  | Generally Available | us-west1-c  |\n| TPU v5e  | ct5lp-      | 1.27.2-gke.2100  | Generally Available | us-west4-a  |\n| TPU v5e  | ct5lp-      | 1.27.2-gke.2100  | Generally Available | us-west4-b1  |\n| TPU v5p  | ct5p-       | 1.28.3-gke.1024000 | Preview    | us-east1-d  |\n| TPU v5p  | ct5p-       | 1.28.3-gke.1024000 | Preview    | us-east5-a  |\n| TPU v5p  | ct5p-       | 1.28.3-gke.1024000 | Preview    | us-east5-c  |\n- When creating a TPU v5e with a machine type beginning with `ct5lp-` in any of the zones `europe-west4-a` , `us-central1-a` , `us-east5-b` , or `us-west4-b` , single-host TPU v5e node pools are not supported.  In other words, when creating a TPU v5e node pool in any of these zones,  only the machine type `ct5lp-hightpu-4t` with a topology of at  least `2x4` or larger is supported. To create a single-host TPU  v5e in `us-central1` or `europe-west4` , choose the  zones `us-central1-a` or `europe-west4-b` ,  respectively, and use machine types beginning with `ct5l-` such  as `ct5l-hightpu-1t` , `ct5l-hightpu-4t` , or `ct5l-hightpu-8t` . To create a single-host TPU v5e in the `us-west4` region, choose the zone `us-west4-a` and  use machine types beginning with `ct5lp-` such as `ct5lp-hightpu-1t` . Note machine types beginning with `ct5l-` require [different quota](/kubernetes-engine/docs/how-to/tpus#ensure-quota) than machine types beginning with `ct5lp-` . [\u21a9](#fnref1) ## Mapping of TPU configuration\nUse the following table to define the TPU machine type and topology to use based on your use case:\n- For small-scale model training or inference, use TPU v4 or TPU v5e with single-host TPU slice node pools.\n- For large-scale model training or inference, use TPU v4 or TPU v5e with multi-host TPU slice node pools.\n| TPU version | Machine type  | Topology | Number of TPU chips | Number of VMs | Node pool type |\n|:--------------|:-----------------|:------------|:----------------------|:----------------|:-----------------|\n| TPU v4  | ct4p-hightpu-4t | 2x2x1  | 4      | 1    | Single-host  |\n| TPU v4  | ct4p-hightpu-4t | 2x2x2  | 8      | 2    | Multi-host  |\n| TPU v4  | ct4p-hightpu-4t | 2x2x4  | 16     | 4    | Multi-host  |\n| TPU v4  | ct4p-hightpu-4t | 2x4x4  | 32     | 8    | Multi-host  |\n| TPU v4  | ct4p-hightpu-4t | {A}x{B}x{C} | A*B*C     | (A*B*C/4)1  | Multi-host  |\n| TPU v5p  | ct5p-hightpu-4t | 2x2x1  | 4      | 1    | Single-host  |\n| TPU v5p  | ct5p-hightpu-4t | 2x2x2  | 8      | 2    | Multi-host  |\n| TPU v5p  | ct5p-hightpu-4t | 2x2x4  | 16     | 4    | Multi-host  |\n| TPU v5p  | ct5p-hightpu-4t | 2x4x4  | 32     | 8    | Multi-host  |\n| TPU v5p  | ct5p-hightpu-4t | {A}x{B}x{C} | A*B*C     | (A*B*C/4)1  | Multi-host  |\n| TPU v5e  | ct5l-hightpu-1t | 1x1   | 1      | 1    | Single-host  |\n| TPU v5e  | ct5l-hightpu-4t | 2x2   | 4      | 1    | Single-host  |\n| TPU v5e  | ct5l-hightpu-8t | 2x4   | 8      | 1    | Single-host  |\n| TPU v5e  | ct5lp-hightpu-1t | 1x1   | 1      | 1    | Single-host  |\n| TPU v5e  | ct5lp-hightpu-4t | 2x2   | 4      | 1    | Single-host  |\n| TPU v5e  | ct5lp-hightpu-8t | 2x4   | 8      | 1    | Single-host  |\n| TPU v5e  | ct5lp-hightpu-4t | 2x4   | 8      | 2    | Multi-host  |\n| TPU v5e  | ct5lp-hightpu-4t | 4x4   | 16     | 4    | Multi-host  |\n| TPU v5e  | ct5lp-hightpu-4t | 4x8   | 32     | 8    | Multi-host  |\n| TPU v5e  | ct5lp-hightpu-4t | 8x8   | 64     | 16    | Multi-host  |\n| TPU v5e  | ct5lp-hightpu-4t | 8x16  | 128     | 32    | Multi-host  |\n| TPU v5e  | ct5lp-hightpu-4t | 16x16  | 256     | 64    | Multi-host  |\n| TPU v5e  | nan    | nan   | nan     | nan    | nan    |\n- Calculated by the topology product divided by four. [\u21a9](#fnref2) \n### TPU v5e characteristics\nTPU v5e machines have the following technical characteristics:\n| Machine type  | Number of vCPUs | Memory (GB) | Number of NUMA nodes | Likelihood of being preempted |\n|:-----------------|------------------:|--------------:|-----------------------:|:--------------------------------|\n| ct5l-hightpu-1t |    24 |   48 |      1 | Higher       |\n| ct5l-hightpu-4t |    112 |   192 |      1 | Medium       |\n| ct5l-hightpu-8t |    224 |   384 |      2 | Lower       |\n| ct5lp-hightpu-1t |    24 |   48 |      1 | Higher       |\n| ct5lp-hightpu-4t |    112 |   192 |      1 | Medium       |\n| ct5lp-hightpu-8t |    224 |   384 |      1 | Low        |\n**Note:** The machine types that start with `ct5lp-` and `ct5l-` are identical. However, the `ct5l-` machine types are [single-host](#node_pool) and therefore do not have any high-speed interconnect links between multiple hosts. `ct5l-` machine types are suitable for serving small-to-medium size models, and less suitable for large models. The `ct5lp-` machine types can be provisioned as single-host or [multi-host](#node_pool) . Multi-host `ct5lp-` machines are interconnected with high-speed links. These machine types are more suitable for serving large models or training.\n### TPU v4 and v5p characteristics\nTPU v4p and v5p machines have the following technical characteristics:\n| Machine type | Number of vCPUs | Memory (GB) | Number of NUMA nodes |\n|:----------------|------------------:|--------------:|-----------------------:|\n| ct4p-hightpu-4t |    240 |   407 |      2 |\n| ct5p-hightpu-4t |    208 |   448 |      2 |\n## TPU reservation\nTo make sure that TPU resources are available when you need them, you can use [TPU reservations](/tpu/docs/quota#quota_types) in the following scenarios:\n- If you have existing TPU reservations, you must work with your Google Cloud account team to migrate your TPU reservation to a new Compute Engine-based reservation system.\n- If you don't have an existing TPU reservation, you can [create a TPU reservation](/tpu/docs/quota#quota_types) and no migration is needed.\n**Note:** You can't use TPU capacity that is migrated to the new Compute Engine-based reservation system with the Cloud TPU [Queued Resource API](/tpu/docs/queued-resources) . If you intend to use TPU queued resources with your reservation, then migrate only a portion of your TPU reservation to the new Compute Engine-based reservation system.\n## Autoscaling TPUs in GKE\nGKE supports Tensor Processing Units (TPUs) to accelerate machine learning workloads. Both [single-host TPU slice node pool](/kubernetes-engine/docs/concepts/tpus#node_pool) and [multi-host TPU slice node pool](/kubernetes-engine/docs/concepts/tpus#node_pool) support autoscaling and auto-provisioning.\nWith the [--enable-autoprovisioning](/sdk/gcloud/reference/container/clusters/update#--enable-autoprovisioning) flag on a GKE cluster, GKE creates or deletes single-host or multi-host TPU slice node pools with a TPU version and topology that meets the requirements of pending workloads.\nWhen you use [--enable-autoscaling](/sdk/gcloud/reference/container/node-pools/create#--enable-autoscaling) , GKE scales the node pool based on its type, as follows:\n- TPU slice node pool: GKE adds or removes TPU nodes in the existing node pool. The node pool may contain any number of TPU nodes between zero and the maximum size of the node pool as determined by the [--max-nodes](/sdk/gcloud/reference/container/node-pools/create#--max-nodes) and the [--total-max-nodes](/sdk/gcloud/reference/container/node-pools/create#--total-max-nodes) flags. When the node pool scales, all the TPU nodes in the node pool have the same machine type and topology. To learn more how to create a single-host TPU slice node pool, see [Create a nodepool](/kubernetes-engine/docs/how-to/tpus#single-host) .\n- TPU slice node pool: GKE atomically scales up the node pool from zero to the number of nodes required to satisfy the TPU topology. For example, with a TPU node pool with a machine type `ct5lp-hightpu-4t` and a topology of `16x16` , the node pool contains 64 nodes. The GKE autoscaler ensures that this node pool has exactly 0 or 64 nodes. When scaling back down, GKE evicts all scheduled pods, and drains the entire node pool to zero. To learn more how to create a multi-host TPU slice node pool, see [Create a node pool](/kubernetes-engine/docs/how-to/tpus#multi-host) .## Limitations\n- When using TPUs in GKE,`SPECIFIC`is the only supported value for the`--reservation-affinity`flag of`gcloud container node-pools create`.\n- TPUs aren't available in GKE [Autopilot clusters](/kubernetes-engine/docs/concepts/autopilot-overview) .\n- GKE cost allocation and usage metering doesn't include any data about the usage or costs of reserved TPU v4.\n- TPU v5p and v5e don't support [riptide/image streaming](/kubernetes-engine/docs/how-to/image-streaming) in us-east5.## Workload scheduling considerations\nTPUs have unique characteristics that require special workload scheduling and management in Kubernetes. The following sections describe scheduling best practices.\n### CPU\nTo schedule a workload on the onboard CPU on a TPU VM, ensure that your GKE Pod can tolerate the `google.com/tpu` taint. If you want the workload to be deployed to specific nodes, use [node selectors](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector) .\nKubernetes resource management and priority treats TPU VMs the same as other VM types. To give Pods that require TPUs scheduling priority over other Pods on the same nodes, request the maximum CPU or memory for those TPU Pods. Low-priority Pods should do the following:\n- Set low CPU and memory requests to ensure that the node has enough allocatable resources for the TPU workloads. To learn more, see [How Kubernetes applies resource requests and limits](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run) .\n- Set no CPU limit (unlimited) to ensure that Pods can burst to use all unused cycles.\n- Set a high memory limit to ensure that Pods can use most unused memory while maintaining node stability.\nIf a Kubernetes Pod doesn't request CPU and memory (even if it is requesting TPUs), then Kubernetes considers it a best-effort Pod, and there is no guarantee that it needed any CPU and memory. Only Pods that explicitly request CPU and memory have such guarantees. For more information, see [Resource Management for Pods and Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) .\nTo learn more, see the [Kubernetes best practices: Resource requests and limits](/blog/products/containers-kubernetes/kubernetes-best-practices-resource-requests-and-limits) .\n### Maximize uninterrupted workload runtime\nIf you are using TPUs to train a machine learning model and your workload is interrupted, all work performed since the last checkpoint is lost. To decrease the probability that your workload is interrupted, do the following:\n- Set a higher priority for this Job than for all other Jobs: If resources are scarce, the GKE scheduler preempts lower priority Jobs to schedule a higher priority Job. This also ensures that your higher priority workload receives all the resources that it needs (up to the total resources available in the cluster). To learn more, see [Pod priority and preemption](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/) .\n- Configure maintenance exclusion: A maintenance exclusion is a non-repeating window of time during which automatic maintenance is forbidden. To learn more, see [Maintenance exclusions](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions#exclusions) .\n**Note:** These recommendations help to minimize interruptions, but don't prevent them. For example, a preemption due to a hardware failure or preemption for defragmentation can still occur. Similarly, setting a GKE maintenance exclusion doesn't prevent Compute Engine maintenance events. Therefore we recommend saving checkpoints frequently and adding code to your training script to start from the last checkpoint when resumed.\n### Maximize TPU utilization\nTo maximize your investment in TPUs, schedule a mix of Job priorities and queue them to maximize the amount of time your TPUs are operating. If you want Job level scheduling and preemption, then you need to use an add-on to Kubernetes that orchestrates Jobs into queues. We recommend using [Kueue](https://kueue.sigs.k8s.io/) for that use case.\n## What's next\n- Follow the [Deploy TPU workloads in GKE](/kubernetes-engine/docs/how-to/tpus) to set up Cloud TPU with GKE.\n- Learn about [best practices](/tpu/docs/tpus) for using Cloud TPU for your machine learning tasks.\n- [Build large-scale machine learning on Cloud TPUs withGKE](https://www.youtube.com/watch?v=wtKhG1aTgtY) \n- [Serve Large Language Models with KubeRay onTPUs](https://www.youtube.com/watch?v=RK_u6cfPnnw)", "guide": "Google Kubernetes Engine (GKE)"}