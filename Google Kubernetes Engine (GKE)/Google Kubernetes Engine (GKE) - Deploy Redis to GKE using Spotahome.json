{"title": "Google Kubernetes Engine (GKE) - Deploy Redis to GKE using Spotahome", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/stateful-workloads/spotahome-redis", "abstract": "# Google Kubernetes Engine (GKE) - Deploy Redis to GKE using Spotahome\n[Redis](https://redis.io/) is an open source in-memory NoSQL database primarily used for caching. It has built-in replication, Lua scripting, LRU eviction, transactions, on-disk persistence, and high availability.\nThis guide is intended for platform administrators, cloud architects, and operations professionals interested in deploying Redis clusters on Google Kubernetes Engine (GKE).\nThe guide shows you how to use the [Spotahome Redis operator](https://github.com/spotahome/redis-operator) to deploy Redis clusters.\nThe operator is [licensed](https://github.com/spotahome/redis-operator/blob/master/LICENSE) under Apache License 2.0.\nSpotahome offers the following benefits:- A Kubernetes-native Redis clusters management\n- High availability provided by [Redis Sentinel](https://redis.io/docs/management/sentinel/) \n- Seamless Prometheus integration for database observability\n- Support for setting custom Redis configurations\n", "content": "## Objectives\n- Plan and deploy GKE infrastructure for Redis\n- Deploy and configure the Spotahome Redis operator\n- Configure Redis using the operator to ensure availability, security, observability, and performance\n### Deployment architectureIn this tutorial, you use the Spotahome Redis operator to deploy and configure a highly-available Redis cluster to GKE with a leader node and two read replicas, along with the Redis Sentinel cluster consisting of three replicas.\n [Redis Sentinel](https://redis.io/docs/management/sentinel/) is a high-availability and monitoring system for open source Redis. It continuously monitors Redis instances, including the leader and its associated replicas. If the leader node fails, Sentinel can automatically promote one of the replicas to become the new leader, ensuring that there is always a functioning leader node available for data reads and writes. When significant events occur in a Redis cluster, such as a leader failure or a failover event, Sentinel can notify administrators or other systems through email or other notification mechanisms.\nYou also deploy a highly-available regional GKE cluster for Redis, with multiple Kubernetes nodes spread across several availability zones. This setup helps ensure fault tolerance, scalability, and geographic redundancy. It allows for rolling updates and maintenance while providing SLAs for uptime and availability. For more information, see [Regional clusters](/kubernetes-engine/docs/concepts/regional-clusters) .\nThe following diagram shows how a Redis cluster runs on multiple nodes and zones in a GKE cluster:In the diagram, the Redis StatefulSet is deployed across three nodes in three different zones. You control how GKE deploys the StatefulSet to nodes and zones by setting Pod [affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/) and [topology spread](https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/) rules on the `RedisFailover` custom resource specification.\nIf one zone fails, using the recommended configuration, GKE reschedules Pods on new nodes.\nThe following diagram shows a Sentinel Deployment scheduled across three nodes in three different zones:## CostsIn this document, you use the following billable components of Google Cloud:- [Compute Engine](/compute/disks-image-pricing) \n- [GKE](/kubernetes-engine/pricing) \n- [Google Cloud Managed Service for Prometheus](/stackdriver/docs/managed-prometheus) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin- Grant roles to your Google Account. Run the following command once for each of the following   IAM roles: `roles/storage.objectViewer, roles/container.admin, roles/iam.serviceAccountAdmin, roles/compute.admin, roles/gkebackup.admin, roles/monitoring.viewer` ```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS\" --role=ROLE\n```- Replace``with your project ID.\n- Replace``with your email address.\n- Replace``with each individual role.\n### Set up your environmentIn this tutorial, you use [Cloud Shell](/shell) to manage resources hosted on Google Cloud. Cloud Shell comes preinstalled with the software you'll need for this tutorial, including [kubectl](https://kubernetes.io/docs/reference/kubectl/) , the [gcloud CLI](/sdk/gcloud) , [Helm](https://helm.sh/) , and [Terraform](/docs/terraform) .\nTo set up your environment with Cloud Shell, follow these steps:- Launch a Cloud Shell session from the Google Cloud console, by clicking **Activate Cloud Shell** in the [Google Cloud console](https://console.cloud.google.com/) . This launches a session in the bottom pane of the Google Cloud console.\n- Set environment variables:```\nexport PROJECT_ID=PROJECT_ID\nexport KUBERNETES_CLUSTER_PREFIX=redis\nexport REGION=us-central1\n```Replace `` : your Google Cloud with your [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- Clone the GitHub repository:```\ngit clone https://github.com/GoogleCloudPlatform/kubernetes-engine-samples\n```\n- Change to the working directory:```\ncd kubernetes-engine-samples/databases/redis-spotahome\n```\n## Create your cluster infrastructureIn this section, you run a Terraform script to create a private, highly-available, regional GKE cluster. The following steps allow public access to the control plane.\nYou can install the operator using a [Standard or Autopilot](/kubernetes-engine/docs/concepts/choose-cluster-mode) cluster.\nThe following diagram shows a private regional Standard GKE cluster deployed across three different zones:To deploy this infrastructure, run the following commands from the Cloud Shell:\n```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=terraform/gke-standard initterraform -chdir=terraform/gke-standard apply -var project_id=${PROJECT_ID} \\\u00a0 -var region=${REGION} \\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```\nWhen prompted, type `yes` . It might take several minutes for this command to complete and for the cluster to show a ready status.\nTerraform creates the following resources:- A VPC network and private subnet for the Kubernetes nodes\n- A router to access the internet through NAT\n- A private GKE cluster in the`us-central1`region\n- Two node pools with autoscaling enabled (One to two nodes per zone, one node per zone minimum)\n- A`ServiceAccount`with logging and monitoring permissions\n- Backup for GKE for disaster recovery\n- Google Cloud Managed Service for Prometheus for cluster monitoring\nThe output is similar to the following:\n```\n...\nApply complete! Resources: 14 added, 0 changed, 0 destroyed.\n...\n```\nThe following diagram shows a private regional Autopilot GKE cluster:To deploy the infrastructure, run the following commands from the Cloud Shell:\n```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=terraform/gke-autopilot initterraform -chdir=terraform/gke-autopilot apply -var project_id=${PROJECT_ID} \\\u00a0 -var region=${REGION} \\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```\nWhen prompted, type `yes` . It might take several minutes for this command to complete and for the cluster to show a ready status.\nTerraform creates the following resources:- VPC network and private subnet for the Kubernetes nodes\n- A router to access the internet through NAT\n- A private GKE cluster in the`us-central1`region\n- A`ServiceAccount`with logging and monitoring permission\n- Google Cloud Managed Service for Prometheus for cluster monitoring\nThe output is similar to the following:\n```\n...\nApply complete! Resources: 12 added, 0 changed, 0 destroyed.\n...\n```\n### Connect to the clusterUsing Cloud Shell, configure `kubectl` to communicate with the cluster:\n```\ngcloud container clusters get-credentials ${KUBERNETES_CLUSTER_PREFIX}-cluster --region ${REGION}\n```## Deploy the Spotahome operator to your clusterIn this section, you deploy the Spotahome operator to your Kubernetes cluster using a Helm chart and then deploy a Redis cluster.- Add the Spotahome Redis operator Helm Chart repository:```\nhelm repo add redis-operator https://spotahome.github.io/redis-operator\n```\n- Add a namespace for the Spotahome operator and the Redis cluster:```\nkubectl create ns redis\n```\n- Deploy the Spotahome operator using the Helm command-line tool:```\nhelm install redis-operator redis-operator/redis-operator --version 3.2.9 -n redis\n```\n- Check the deployment status of the Spotahome operator using Helm:```\nhelm ls -n redis\n```The output is similar to the following:```\nNAME    NAMESPACE REVISION UPDATED        STATUS  CHART     APP VERSION\nredis-operator redis  1   2023-09-12 13:21:48.179503 +0200 CEST deployed redis-operator-3.2.9 1.2.4\n```\n## Deploy RedisThe basic configuration for the Redis cluster instance includes the following components:- Three replicas of Redis nodes: one leader and two read replicas.\n- Three replicas of Sentinel nodes, forming a quorum.\n- CPU resource allocation of one CPU request and two CPU limits, with 4\u00a0GB memory requests and limits for Redis, and 100\u00a0m/500\u00a0m CPU and 500\u00a0MB for Sentinel.\n- Tolerations,`nodeAffinities`, and`topologySpreadConstraints`configured for each workload, ensuring proper distribution across Kubernetes nodes, utilizing their respective node pools and different availability zones.\nThis configuration represents the minimal setup required to create a production-ready Redis cluster.\n### Create a basic Redis cluster\n- Create a Secret with user credentials:```\nexport PASSWORD=$(openssl rand -base64 12)kubectl create secret generic my-user -n redis \\\u00a0 \u00a0 --from-literal=password=\"$PASSWORD\"\n```The operator doesn't have a feature to generate credentials, and the database password should be pre-generated.\n- Create a new Redis cluster using the basic configuration:```\nkubectl apply -n redis -f manifests/01-basic-cluster/my-cluster.yaml\n```This command creates a `RedisFailover` custom resource of the Spotahome operator that specifies CPU, memory requests, and limits; and taints and affinities to distribute the provisioned Pod replicas across Kubernetes nodes.\n- Wait a few minutes while Kubernetes starts the required workloads:```\nkubectl wait pods -l redisfailovers.databases.spotahome.com/name=my-cluster --for condition=Ready --timeout=300s -n redis\n```\n- Verify that the Redis workloads were created:```\nkubectl get pod,svc,statefulset,deploy,pdb -n redis\n```The output is similar to the following:```\nNAME        READY STATUS RESTARTS AGE\npod/redis-operator-5dc65cb7cc-krlcs 1/1 Running 0   49m\npod/rfr-my-cluster-0    2/2  Running 0   60s\npod/rfr-my-cluster-1    2/2  Running 0   60s\npod/rfr-my-cluster-2    2/2  Running 0   60s\npod/rfs-my-cluster-8475dfd96c-h5zvw 1/1 Running 0   60s\npod/rfs-my-cluster-8475dfd96c-rmh6f 1/1 Running 0   60s\npod/rfs-my-cluster-8475dfd96c-shzxh 1/1 Running 0   60s\nNAME     TYPE  CLUSTER-IP EXTERNAL-IP PORT(S) AGE\nservice/redis-my-cluster ClusterIP 10.52.14.87 <none>  6389/TCP 55s\nservice/redis-operator ClusterIP 10.52.13.217 <none>  9710/TCP 49m\nservice/rfr-my-cluster ClusterIP None   <none>  9121/TCP 61s\nservice/rfs-my-cluster ClusterIP 10.52.15.197 <none>  26379/TCP 61s\nNAME       READY AGE\nstatefulset.apps/rfr-my-cluster 3/3 61s\nNAME       READY UP-TO-DATE AVAILABLE AGE\ndeployment.apps/redis-operator 1/1 1   1   50m\ndeployment.apps/rfs-my-cluster 3/3 3   3   62s\nNAME          MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE\npoddisruptionbudget.policy/rfr-my-cluster 2    N/A    1     64s\npoddisruptionbudget.policy/rfs-my-cluster 2    N/A    1     63s\n```\nThe operator creates the following resources:- A Redis StatefulSet and Sentinel Deployment\n- Three Pod replicas for Redis\n- Three Pod replicas for Sentinel\n- Two`PodDisruptionBudgets`, ensuring a minimum of two available replicas for cluster consistency\n- The`rfr-my-cluster`Service, which exposes Redis metrics\n- The`redis-my-cluster`Service, which targets the Redis cluster leader node\n- The`rfs-my-cluster`Service, which allows clients to connect to the cluster through the Sentinels. Sentinel support is required for client libraries.\n## Share Redis credentialsYou can share Redis credentials with clients using the Spotahome operator legacy [authentication method](https://github.com/spotahome/redis-operator/tree/master#enabling-redis-auth) \nYou must use a database password using the `requirepass` setting. This password is then used by all clients. To manage additional users, use [Redis CLI commands](https://redis.io/docs/management/security/acl/) .\n```\napiVersion: databases.spotahome.com/v1kind: RedisFailovermetadata:\u00a0 name: my-clusterspec:\u00a0 ...\u00a0 auth:\u00a0 \u00a0 secretPath: my-user\n```\nSharing Redis credentials with clients using this method has the following limitations:- Spotahome does not provide Custom Resources for user management. You can store credentials in Secrets and referred to them in`auth`specs.\n- There is no method to secure the connections with the TLS encryption using the custom resource.\n- Live updating of credentials is [not supported](https://github.com/spotahome/redis-operator/issues/658) .\n## Connect to RedisYou can deploy a Redis client and authenticate using a password stored in a Kubernetes Secret.- Run the client Pod to interact with your Redis cluster:```\nkubectl apply -n redis -f manifests/02-auth/client-pod.yaml\n```The `PASS` environment variable takes the `my-user` Secret from the vault.\n- Wait for the Pod to be ready, then connect to it:```\nkubectl wait pod redis-client --for=condition=Ready --timeout=300s -n rediskubectl exec -it redis-client -n redis -- /bin/bash\n```\n- Verify that the connection works:```\nredis-cli -h redis-my-cluster -a $PASS --no-auth-warning SET my-key \"testvalue\"\n```The output is similar to the following:```\nOK\n```\n- Get the `my-key` value:```\nredis-cli -h redis-my-cluster -a $PASS --no-auth-warning GET my-key\n```The output is similar to the following:```\n\"testvalue\"\n```\n- Exit the Pod shell```\nexit\n```\n## Understand how Prometheus collects metrics for your Redis clusterThe following diagram shows how Prometheus metrics collecting works:In the diagram, a GKE private cluster contains:- A Redis Pod that gathers metrics on path`/`and port`9121`\n- Prometheus-based collectors that process the metrics from the Redis Pod\n- A PodMonitoring resource that sends metrics to Cloud Monitoring\nGoogle Cloud Managed Service for Prometheus supports metrics collection in the Prometheus format. Cloud Monitoring uses an [integrated dashboard](/stackdriver/docs/managed-prometheus/exporters/redis) for Redis metrics.\nThe Spotahome operator exposes cluster metrics in Prometheus format using the [redis_exporter](https://github.com/oliver006/redis_exporter) as a sidecar.- Create the [PodMonitoring](/stackdriver/docs/managed-prometheus/setup-managed#gmp-pod-monitoring) resource to scrape metrics by labelSelector:```\nkubectl apply -n redis -f manifests/03-prometheus-metrics/pod-monitoring.yaml\n```\n- In the Google Cloud console, go to the **GKE Clusters Dashboard** page. [Go to GKE Clusters Dashboard](https://console.cloud.google.com/monitoring/dashboards/resourceList/gmp_gke_cluster) The dashboard shows non-zero metrics ingestion rate.\n- In the Google Cloud console, go to the **Dashboards** page. [Go to Dashboards](https://console.cloud.google.com/monitoring/dashboards) \n- Open the **Redis Prometheus Overview dashboard** . The dashboard shows the amount of connections and keys. It might take several minutes for the dashboard to auto-provision.\n- Connect to the client Pod and prepare variables:```\nkubectl exec -it redis-client -n redis -- /bin/bash\n```\n- Use the `redis-cli` tool to create new keys:```\nfor i in {1..50}; do \\\u00a0 redis-cli -h redis-my-cluster -a $PASS \\\u00a0 --no-auth-warning SET mykey-$i \"myvalue-$i\"; \\done\n```\n- Refresh the page and observe that the **Commands Per Second** and **Keys** graphs have been updated to show the actual database state.\n- Exit the Pod shell```\nexit\n```\n## Clean up\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- Delete a Google Cloud project:\n- ```\ngcloud projects delete PROJECT_ID\n```\n### Delete individual resources\n- Set environment variables.```\nexport PROJECT_ID=${PROJECT_ID}export KUBERNETES_CLUSTER_PREFIX=redisexport REGION=us-central1\n```\n- Run the `terraform destroy` command:```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=terraform/FOLDER destroy -var project_id=${PROJECT_ID} \\\u00a0 -var region=${REGION} \\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```Replace `` with either `gke-autopilot` or `gke-standard` .When prompted, type `yes` .\n- Find all unattached disks:```\nexport disk_list=$(gcloud compute disks list --filter=\"-users:* AND labels.name=${KUBERNETES_CLUSTER_PREFIX}-cluster\" --format \"value[separator=|](name,zone)\")\n```\n- Delete the disks:```\nfor i in $disk_list; do\u00a0 disk_name=$(echo $i| cut -d'|' -f1)\u00a0 disk_zone=$(echo $i| cut -d'|' -f2|sed 's|.*/||')\u00a0 echo \"Deleting $disk_name\"\u00a0 gcloud compute disks delete $disk_name --zone $disk_zone --quietdone\n```\n## What's next\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Google Kubernetes Engine (GKE)"}