{"title": "Google Kubernetes Engine (GKE) - Using GKE Dataplane V2", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/dataplane-v2", "abstract": "# Google Kubernetes Engine (GKE) - Using GKE Dataplane V2\nThis page explains how to enable [GKE Dataplane V2](/kubernetes-engine/docs/concepts/dataplane-v2) for Google Kubernetes Engine (GKE) clusters.\nNew Autopilot clusters have GKE Dataplane V2 enabled in versions 1.22.7-gke.1500 and later and versions 1.23.4-gke.1500 and later. If you're experiencing issues with using GKE Dataplane V2, skip to [Troubleshooting](/kubernetes-engine/docs/how-to/dataplane-v2#troubleshooting) .\n", "content": "## Creating a GKE cluster with GKE Dataplane V2\nYou can enable GKE Dataplane V2 when you create new clusters with GKE version 1.20.6-gke.700 and later by using the gcloud CLI or the Kubernetes Engine API. You can also enable GKE Dataplane V2 in [Preview](/products#product-launch-stages) when you create new clusters with GKE version 1.17.9 and later\n**Warning:** GKE Dataplane V2 comes with [Kubernetes networkpolicy](https://kubernetes.io/docs/concepts/services-networking/network-policies/) enforcement built-in. This means that you don't need to [enable networkpolicy](/kubernetes-engine/docs/how-to/network-policy#enabling_network_policy_enforcement) in clusters that use GKE Dataplane V2. If you try to explicitly enable or disable network policy enforcement in a cluster that uses GKE Dataplane V2, the request will fail with the error message `Enabling NetworkPolicy for clusters with DatapathProvider=ADVANCED_DATAPATH is not allowed.` .\nTo create a new cluster with GKE Dataplane V2, perform the following tasks:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- Click **Configure** to configure a Standard cluster.\n- In the Networking section, select the **Enable Dataplane V2** checkbox. The Enable Kubernetes Network Policy option is disabled when you select Enable Dataplane V2 because network policy enforcement is built into GKE Dataplane V2.\n- Click **Create** .\nTo create a new cluster with GKE Dataplane V2, use the following command:\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --enable-dataplane-v2 \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --release-channel CHANNEL_NAME \\\u00a0 \u00a0 --location COMPUTE_LOCATION\n```\nReplace the following:- ``: the name of your new cluster.\n- ``: a [release channel](/kubernetes-engine/docs/concepts/release-channels) that includes GKE version 1.20.6-gke.700 or later. If you prefer not to use a release channel, you can also use the`--cluster-version`flag instead of`--release-channel`, specifying version 1.20.6-gke.700 or later.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the new cluster.\nTo create a new cluster with GKE Dataplane V2, specify the [datapathProvider field](/kubernetes-engine/docs/reference/rest/v1/projects.locations.clusters#datapathprovider) in the [networkConfig object](/kubernetes-engine/docs/reference/rest/v1/projects.locations.clusters#networkconfig) in your cluster [create request](/kubernetes-engine/docs/reference/rest/v1/projects.zones.clusters/create) .\nThe following JSON snippet shows the configuration needed to enable GKE Dataplane V2:\n```\n\"cluster\":{\u00a0 \u00a0\"initialClusterVersion\":\"VERSION\",\u00a0 \u00a0\"ipAllocationPolicy\":{\u00a0 \u00a0 \u00a0 \"useIpAliases\":true\u00a0 \u00a0},\u00a0 \u00a0\"networkConfig\":{\u00a0 \u00a0 \u00a0 \"datapathProvider\":\"ADVANCED_DATAPATH\"\u00a0 \u00a0},\u00a0 \u00a0\"releaseChannel\":{\u00a0 \u00a0 \u00a0 \"channel\":\"CHANNEL_NAME\"\u00a0 \u00a0}}\n```\nReplace the following:- : your cluster version, which must be GKE 1.20.6-gke.700 or later.\n- : a [release channel](/kubernetes-engine/docs/concepts/release-channels) that includes GKE version 1.20.6-gke.700 or later.## Troubleshooting issues with GKE Dataplane V2\nThis section shows you how to investigate and resolve issues with GKE Dataplane V2.\n- Confirm that GKE Dataplane V2 is enabled:```\nkubectl -n kube-system get pods -l k8s-app=cilium -o wide\n```If GKE Dataplane V2 is running, the output includes Pods with the prefix `anetd-` . anetd is the networking controller for GKE Dataplane V2.\n- If the issue is with services or network policy enforcement, check the `anetd` Pod logs. Use the following log selectors in Cloud Logging:```\nresource.type=\"k8s_container\"labels.\"k8s-pod/k8s-app\"=\"cilium\"resource.labels.cluster_name=\"CLUSTER_NAME\"\n```\n- If Pod creation is failing, check the kubelet logs for clues. Use the following log selectors in Cloud Logging:```\nresource.type=\"k8s_node\"log_name=~\".*/logs/kubelet\"resource.labels.cluster_name=\"CLUSTER_NAME\"\n```Replace `` with the name of the cluster, or remove it entirely to see logs for all clusters.## Known issues\n### Network Policy port ranges do not take effect\nIf you specify an `endPort` field in a Network Policy on a cluster that has GKE Dataplane V2 enabled, it will not take effect.\nStarting in GKE 1.22, the [Kubernetes Network Policy API](https://kubernetes.io/docs/concepts/services-networking/network-policies/) lets you specify a range of ports where the Network Policy is enforced. This API is supported in clusters with Calico Network Policy but is not supported in clusters with GKE Dataplane V2.\nYou can verify the behavior of your `NetworkPolicy` objects by reading them back after writing them to the API server. If the object still contains the `endPort` field, the feature is enforced. If the `endPort` field is missing, the feature is not enforced. In all cases, the object stored in the API server is the source of truth for the Network Policy.\nFor more information see [KEP-2079: Network Policy to support Port Ranges](https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/2079-network-policy-port-range) .\n### Pods display failed to allocate for range 0: no IP addresses available in range set error message\n1.22 to 1.25\nGKE clusters running node pools that use containerd and have GKE Dataplane V2 enabled might experience IP address leak issues and exhaust all the Pod IP addresses on a node. A Pod scheduled on an affected node displays an error message similar to the following:\n```\nfailed to allocate for range 0: no IP addresses available in range set: 10.48.131.1-10.48.131.62\n```\nFor more information about the issue, see containerd [issue #5768](https://github.com/containerd/containerd/issues/5768) .\nTo fix this issue, [upgrade your cluster](/kubernetes-engine/docs/how-to/upgrading-a-cluster) to one of the following GKE versions:\n- 1.22.17-gke.3100 or later.\n- 1.23.16-gke.200 or later.\n- 1.24.9-gke.3200 or later.\n- 1.25.6-gke.200 or later.You can mitigate this issue by deleting the leaked Pod IP addresses for the node.\nTo delete the leaked Pod IP addresses, [get authentication credentials for the cluster](/kubernetes-engine/docs/deploy-app-cluster#get_authentication_credentials_for_the_cluster) and run the following steps to clean up a single node, if you know its name.\n- Save the following shell script to a file named `cleanup.sh` :```\nfor hash in $(sudo find /var/lib/cni/networks/gke-pod-network -iregex '/var/lib/cni/networks/gke-pod-network/[0-9].*' -exec head -n1 {} \\;); do hash=\"${hash%%[[:space:]]}\"; if [ -z $(sudo ctr -n k8s.io c ls | grep $hash | awk '{print $1}') ]; then sudo grep -ilr $hash /var/lib/cni/networks/gke-pod-network; fi; done | sudo xargs -r rm\n```\n- Run the script on a cluster node:```\ngcloud compute ssh --zone \"ZONE\" --project \"PROJECT\" NODE_NAME --command \"$(cat cleanup.sh)\"\n```Replace `` with the name of the node.\nYou can also run a DaemonSet version of this script to run in parallel on all nodes at once:\n- Save the following manifest to a file named `cleanup-ips.yaml` :```\napiVersion: apps/v1kind: DaemonSetmetadata:\u00a0 name: cleanup-ipam-dir\u00a0 namespace: kube-systemspec:\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 name: cleanup-ipam\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 name: cleanup-ipam\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 hostNetwork: true\u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 runAsUser: 0\u00a0 \u00a0 \u00a0 \u00a0 runAsGroup: 0\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: cleanup-ipam\u00a0 \u00a0 \u00a0 \u00a0 image: gcr.io/gke-networking-test-images/ubuntu-test:2022\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - /bin/bash\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - -c\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 while true; do\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 for hash in $(find /hostipam -iregex '/hostipam/[0-9].*' -mmin +10 -exec head -n1 {} \\; ); do\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hash=\"${hash%%[[:space:]]}\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if [ -z $(ctr -n k8s.io c ls | grep $hash | awk '{print $1}') ]; then\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 grep -ilr $hash /hostipam\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fi\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 done | xargs -r rm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 echo \"Done cleaning up /var/lib/cni/networks/gke-pod-network at $(date)\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sleep 120s\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 done\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - name: host-ipam\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mountPath: /hostipam\u00a0 \u00a0 \u00a0 \u00a0 - name: host-ctr\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mountPath: /run/containerd\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: host-ipam\u00a0 \u00a0 \u00a0 \u00a0 hostPath:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path: /var/lib/cni/networks/gke-pod-network\u00a0 \u00a0 \u00a0 - name: host-ctr\u00a0 \u00a0 \u00a0 \u00a0 hostPath:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path: /run/containerd\n```\n- Run the daemonset on the cluster:```\nkubectl apply -f cleanup-ips.yaml\n```You must have kubectl access as an administrator of the cluster to run this command.\n- Check the logs of the running DaemonSet:```\nkubectl -n kube-system logs -l name=cleanup-ipam\n```\n### Network Policy drops a connection due to incorrect connection tracking lookup\nWhen a client Pod connects to itself via a Service or the virtual IP address of an internal passthrough Network Load Balancer, the reply packet is not identified as a part of an existing connection due to incorrect conntrack lookup in the dataplane. This means that a Network Policy that restricts ingress traffic for the pod is incorrectly enforced on the packet.\nThe impact of this issue depends on the number of configured Pods for the Service. For example, if the Service has 1 backend Pod, the connection always fails. If the Service has 2 backend Pods, the connection fails 50% of the time.\nTo fix this issue, [upgrade yourcluster](/kubernetes-engine/docs/how-to/upgrading-a-cluster) to one of the following GKE versions:\n- 1.28.3-gke.1090000 or later.You can mitigate this issue by configuring the `port` and `containerPort` in the Service manifest to be the same value.\n### Packet drops for hairpin connection flows\nWhen a Pod creates a TCP connection to itself using a Service, such that the Pod is both the source and destination of the connection, GKE Dataplane V2 eBPF connection tracking incorrectly tracks the connection states, leading to leaked conntrack entries.\nWhen a connection tuple (protocol, source/destination IP, and source/destination port) has been leaked, new connections using the same connection tuple might result in return packets being dropped.\nTo fix this issue, [upgrade yourcluster](/kubernetes-engine/docs/how-to/upgrading-a-cluster) to one of the following GKE versions:\n- 1.28.3-gke.1090000 or later\n- 1.27.11-gke.1097000 or laterUse one of the following workarounds:\n- Enable TCP reuse (keep-alives) for applications running in Pods that might communicate with itself using a Service. This prevents the TCP FIN flag from being issued and avoid leaking the conntrack entry.\n- When using short-lived connections, expose the Pod using a proxy load balancer, such as [Gateway](/kubernetes-engine/docs/concepts/gateway-api) , to expose the Service. This results in the destination of the connection request being set to the load balancer IP address, preventing GKE Dataplane V2 from performing SNAT to the loopback IP address.## What's next\n- Use [network policylogging](/kubernetes-engine/docs/how-to/network-policy-logging) to record when connections to Pods are allowed or denied by your cluster's [networkpolicies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) .\n- Learn how [GKE Dataplane V2](/kubernetes-engine/docs/concepts/dataplane-v2) works.", "guide": "Google Kubernetes Engine (GKE)"}