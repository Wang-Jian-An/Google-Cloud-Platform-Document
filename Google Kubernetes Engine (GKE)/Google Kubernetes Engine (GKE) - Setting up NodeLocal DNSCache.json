{"title": "Google Kubernetes Engine (GKE) - Setting up NodeLocal DNSCache", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/nodelocal-dns-cache", "abstract": "# Google Kubernetes Engine (GKE) - Setting up NodeLocal DNSCache\nThis page explains how to improve DNS lookup latency in a Google Kubernetes Engine (GKE) cluster by using NodeLocal DNSCache.\nFor GKE Autopilot clusters, NodeLocal DNSCache is enabled by default and cannot be overridden.\n", "content": "## Architecture\nNodeLocal DNSCache is a GKE add-on that you can run in addition to [kube-dns](/kubernetes-engine/docs/concepts/service-discovery) .\nGKE implements NodeLocal DNSCache as a [DaemonSet](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) that runs a DNS cache on each node in your cluster.\nWhen a Pod makes a DNS request, the request goes to the DNS cache running on the same node as the Pod. If the cache can't resolve the DNS request, the cache forwards the request to one of the following places based on the query destination:\n- kube-dns: all queries for the cluster DNS domain (`cluster.local`) are forwarded to kube-dns. The node-local-dns Pods use the kube-dns-upstream Service to access kube-dns Pods. In the following diagram, the IP address of the kube-dns Service is`10.0.0.10:53`.\n- Custom stub domains or upstream name servers: queries are forwarded directly from NodeLocal DNSCache Pods.\n- Cloud DNS: all other queries are forwarded to the [local metadata server](/compute/docs/metadata/overview) that runs on the same node as the Pod the query originated from. The local metadata server accesses Cloud DNS.When you enable NodeLocal DNSCache on an existing cluster, GKE recreates all cluster nodes running GKE version 1.15 and later according to the [node upgrade process](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrading-nodes) .\nAfter GKE recreates the nodes, GKE automatically adds the label `addon.gke.io/node-local-dns-ds-ready=true` to the nodes. You must not add this label to the cluster nodes manually.\n### Benefits of NodeLocal DNSCache\nNodeLocal DNSCache provides the following benefits:\n- Reduced average DNS lookup time\n- Connections from Pods to their local cache don't create [conntrack](https://www.usenix.org/system/files/login/articles/892-neira.pdf) table entries. This prevents dropped and rejected connections caused by conntrack table exhaustion and race conditions.\n- You can use NodeLocal DNSCache with [Cloud DNS for GKE](/kubernetes-engine/docs/how-to/cloud-dns) .\n- DNS queries for external URLs (URLs that don't refer to cluster resources) are forwarded directly to the local [Cloud DNS](/dns/docs/overview) metadata server, bypassing kube-dns.\n- The local DNS caches automatically pick up stub domains and upstream name servers that are specified in the [kube-dns ConfigMap](https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#coredns-configuration-equivalent-to-kube-dns) .\n### Requirements and limitations\n- NodeLocal DNSCache consumes compute resources on each node of your cluster.\n- NodeLocal DNSCache is not supported with Windows Server node pools.\n- NodeLocal DNSCache requires GKE version 1.15 or later.\n- NodeLocal DNSCache accesses kube-dns Pods using TCP.\n- NodeLocal DNSCache accesses`upstreamServers`and`stubDomains`using TCP and UDP on GKE versions 1.18 or later. The DNS server must be reachable using TCP and UDP.\n- DNS records are cached for the following periods:- The record's time to live (TTL), or 30 seconds if the TTL is more than 30 seconds.\n- 5 seconds if the DNS response is`NXDOMAIN`.\n- NodeLocal DNSCache Pods listen on port 53, 9253, 9353, and 8080 on the nodes. If you run any other`hostNetwork`Pod or configure a`hostPorts`with those ports, NodeLocal DNSCache fails and DNS errors occur. NodeLocal DNSCache Pods do not use`hostNetwork`mode when using [GKE Dataplane V2](/kubernetes-engine/docs/concepts/dataplane-v2) and [Cloud DNS for GKE](/kubernetes-engine/docs/how-to/cloud-dns) .\n- The local DNS cache only runs on node pools running GKE versions 1.15 and later. If you enable NodeLocal DNSCache in a cluster with nodes running earlier versions, Pods on those nodes use kube-dns.## Enable NodeLocal DNSCache\nFor Autopilot clusters, NodeLocal DNSCache is enabled by default and cannot be overridden.\nFor Standard clusters, you can enable NodeLocal DNSCache on new or existing clusters using the Google Cloud CLI. You can enable NodeLocal DNSCache in new clusters using the Google Cloud console.\n### Enable NodeLocal DNSCache in a new clusterTo enable NodeLocal DNSCache in a new cluster, use the `--addons` flag with the argument `NodeLocalDNS` :\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --location=COMPUTE_LOCATION \\\u00a0 \u00a0 --addons=NodeLocalDNS\n```\nReplace the following:- ``: the name of your new cluster.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\n### Enable NodeLocal DNSCache in an existing clusterTo enable NodeLocal DNSCache in an existing cluster, use the `--update-addons` flag with the argument `NodeLocalDNS=ENABLED` :\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --update-addons=NodeLocalDNS=ENABLED\n```\nReplace the following:- ``: the name of your cluster.\n **Warning:** Enabling NodeLocal DNSCache in an existing cluster is a disruptive operation. All cluster nodes running GKE version 1.15 and later are recreated. Nodes are recreated according to the GKE [node upgrade process](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrading-nodes) .\nTo enable NodeLocal DNSCache on a new cluster, use the following steps:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Next to Standard, click **Configure** .\n- Configure your cluster how you want.\n- From the navigation pane, click **Networking** .\n- In the **Advanced networking options** section, select the **Enable NodeLocal DNSCache** checkbox.\n- Click **Create** .## Verify that NodeLocal DNSCache is enabled\nYou can verify that NodeLocal DNSCache is running by listing the `node-local-dns` Pods:\n```\nkubectl get pods -n kube-system -o wide | grep node-local-dns\n```\nThe output is similar to the following:\n```\nnode-local-dns-869mt 1/1 Running 0 6m24s 10.128.0.35 gke-test-pool-69efb6b8-5d7m <none> <none>\nnode-local-dns-htx4w 1/1 Running 0 6m24s 10.128.0.36 gke-test-pool-69efb6b8-wssk <none> <none>\nnode-local-dns-v5njk 1/1 Running 0 6m24s 10.128.0.33 gke-test-pool-69efb6b8-bhz3 <none> <none>\n```\nThe output shows a `node-local-dns` Pod for each node that is running GKE version 1.15 or later.\n## Disable NodeLocal DNSCache\nYou can disable NodeLocal DNSCache using the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --update-addons=NodeLocalDNS=DISABLED\n```\nReplace the following:\n- ``: the name of the cluster to disable.\n**Warning:** Disabling NodeLocal DNSCache is a disruptive operation. All cluster nodes running GKE version 1.15 and later are recreated. Nodes are recreated according to the GKE [node upgrade process](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrading-nodes) .\n## Troubleshoot NodeLocal DNSCache\nFor general information about diagnosing Kubernetes DNS issues, see [Debugging DNS Resolution](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/) .\n### NodeLocal DNSCache is not enabled immediately\nWhen you enable NodeLocal DNSCache on an existing cluster, GKE might not update the nodes immediately if the cluster has a configured [maintenance window or exclusion](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions) . For more information, see [caveats for node re-creation and maintenance windows](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions#node-recreation-maintenance-windows) .\nIf you prefer not to wait, you can manually apply the changes to the nodes by calling the [gcloud container clusters upgrade](/sdk/gcloud/reference/container/clusters/upgrade) command and passing the `--cluster-version` flag with the same GKE version that the node pool is already running. You must use the Google Cloud CLI for this workaround.\n### NodeLocal DNSCache with Cloud DNS\nIf you use NodeLocal DNSCache with [Cloud DNS](/kubernetes-engine/docs/how-to/cloud-dns) , the cluster uses the name server IP address `169.254.20.10` , as shown in the following diagram:\nYou can view the IP address of the Cluster IP by using the following command:\n```\nkubectl get svc -n kube-system kube-dns -o jsonpath=\"{.spec.clusterIP}\"\n```\nThe output is similar to the following:\n```\n169.254.20.10\n```\n### Network policy with NodeLocal DNSCache\nIf you use [network policy](/kubernetes-engine/docs/how-to/network-policy) with NodeLocal DNSCache and you are not using [Cloud DNS](/kubernetes-engine/docs/how-to/cloud-dns) or [GKE Dataplane V2](/kubernetes-engine/docs/concepts/dataplane-v2) , you must configure rules to permit your workloads and the `node-local-dns` Pods to send DNS queries.\nUse an `ipBlock` rule in your manifest to allow communication between your Pods and kube-dns.\nThe following manifest describes a network policy that uses an `ipBlock` rule:\n```\nspec:\u00a0 egress:\u00a0 - ports:\u00a0 \u00a0 - port: 53\u00a0 \u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 - port: 53\u00a0 \u00a0 \u00a0 protocol: UDP\u00a0 \u00a0 to:\u00a0 \u00a0 - ipBlock:\u00a0 \u00a0 \u00a0 \u00a0 cidr: KUBE_DNS_SVC_CLUSTER_IP/32\u00a0 podSelector: {}\u00a0 policyTypes:\u00a0 \u00a0 - Egress\n```\nReplace `` with the IP address of the kube-dns service. You can get the IP address of the kube-dns service using the following command:\n```\nkubectl get svc -n kube-system kube-dns -o jsonpath=\"{.spec.clusterIP}\"\n```\n## Known issues\n### DNS timeout in ClusterFirstWithHostNet dnsPolicy when using NodeLocal DNSCache and GKE Dataplane V2\nOn clusters using GKE Dataplane V2 and NodeLocal DNSCache, pods with `hostNetwork` set to `true` and `dnsPolicy` set to `ClusterFirstWithHostNet` cannot reach cluster DNS backends. DNS logs might contain entries similar to the following:\n```\nnslookup: write to 'a.b.c.d': Operation not permitted\n;; connection timed out; no servers could be reached\n```\nThe output indicates that the DNS requests cannot reach the backend servers.\nA workaround is to set the `dnsPolicy` and `dnsConfig` for `hostNetwork` pods:\n```\nspec:\u00a0dnsPolicy: \"None\"\u00a0dnsConfig:\u00a0 \u00a0nameservers:\u00a0 \u00a0 \u00a0- KUBE_DNS_UPSTREAM\u00a0 \u00a0searches:\u00a0 \u00a0 \u00a0- cluster.local\u00a0 \u00a0 \u00a0- svc.cluster.local\u00a0 \u00a0 \u00a0- NAMESPACE.svc.cluster.local\u00a0 \u00a0 \u00a0- c.PROJECT_ID.internal\u00a0 \u00a0 \u00a0- google.internal\u00a0 \u00a0options:\u00a0 \u00a0 \u00a0- name: ndots\u00a0 \u00a0 \u00a0 \u00a0value: \"5\"\n```\nReplace the following:\n- ``: the namespace of the`hostNetwork`pod.\n- ``: the ID of your Google Cloud project.\n- `` : the ClusterIP of the upstream kube-dns service. You can get this value using the following command:```\nkubectl get svc -n kube-system kube-dns-upstream -o jsonpath=\"{.spec.clusterIP}\"\n```\nDNS requests from the Pod can now reach kube-dns and bypass NodeLocal DNSCache.\n### NodeLocal DNSCache timeout errors\nOn clusters with NodeLocal DNSCache enabled, the logs might contain entries similar to the following:\n```\n[ERROR] plugin/errors: 2 <hostname> A: read tcp <node IP: port>-><kubedns IP>:53: i/o timeout\n```\n**Note:** If the output contains `169.254.169.254:53 i/o timeout` , the timeouts are coming from the metadata server, and the following workarounds don't apply.\nThe output includes the IP address of the `kube-dns-upstream` Cluster IP Service. In this example, the response to a DNS request was not received from kube-dns in 2 seconds. This could be due to one of the following reasons:\n- An underlying network connectivity problem.\n- Significantly increased DNS queries from the workload or due to node pool upscaling.\nAs a result, the existing `kube-dns` pods are unable to handle all requests in time. The workaround is to increase the number of kube-dns replicas by tuning the [auto scaling parameters](https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/#tuning-autoscaling-parameters) .\n### Scaling up kube-dns\nYou can use a lower value for `nodesPerReplica` to ensure that more kube-dns Pods are created as cluster nodes scale up. We highly recommend setting an explicit `max` value to ensure that the GKE control plane virtual machine (VM) is not overwhelmed due to large number of kube-dns pods watching the Kubernetes API.\nYou can set `max` to the number of nodes in the cluster. If the cluster has more than 500 nodes, set `max` to 500.\n**Note:** Setting `max` to 500 does not create 500 replicas. Instead, it ensures that kube-dns replicas don't scale up beyond the value of `max` . In most cases, you should set `max` to a value much lower than 500.\nFor Standard clusters, you can modify the number of kube-dns replicas by editing the `kube-dns-autoscaler` ConfigMap. This configuration is not supported in Autopilot clusters.\n```\nkubectl edit configmap kube-dns-autoscaler --namespace=kube-system\n```\nThe output is similar to the following:\n```\nlinear: '{\"coresPerReplica\":256, \"nodesPerReplica\":16,\"preventSinglePointFailure\":true}'\n```\nThe number of kube-dns replicas is calculated using the following formula:\nTo scale up, change `nodesPerReplica` to a smaller value and include a `max` value.\n```\nlinear: '{\"coresPerReplica\":256, \"nodesPerReplica\":8,\"max\": 15,\"preventSinglePointFailure\":true}'\n```\nThe config creates 1 kube-dns pod for every 8 nodes in the cluster. A 24-node cluster will have 3 replicas and a 40-node cluster will have 5 replicas. If the cluster grows beyond 120 nodes, the number of kube-dns replicas does not grow beyond 15, the `max` value.\nTo ensure a baseline level of DNS availability in your cluster, set a minimum replica count for kube-dns.\nThe `kube-dns-autoscaler` ConfigMap output with `min` field would be similar to the following:\n```\nlinear: '{\"coresPerReplica\":256, \"nodesPerReplica\":8,\"max\": 15,\"min\": 5,\"preventSinglePointFailure\":true}'\n```\n## What's next\n- [Read an overview](/kubernetes-engine/docs/concepts/service-discovery) of how GKE provides managed DNS.\n- Read [DNS for Services and Pods](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/) for a general overview of how DNS is used in Kubernetes clusters.\n- Learn how to use [Cloud DNS for GKE](/kubernetes-engine/docs/how-to/cloud-dns) .", "guide": "Google Kubernetes Engine (GKE)"}