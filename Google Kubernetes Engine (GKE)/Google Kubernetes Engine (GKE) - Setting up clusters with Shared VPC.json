{"title": "Google Kubernetes Engine (GKE) - Setting up clusters with Shared VPC", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-shared-vpc", "abstract": "# Google Kubernetes Engine (GKE) - Setting up clusters with Shared VPC\nThis guide shows how to create two Google Kubernetes Engine (GKE) clusters, in separate projects, that use a [Shared VPC](/vpc/docs/shared-vpc) . For general information about GKE networking, visit the [Network overview](/kubernetes-engine/docs/concepts/network-overview) .\n", "content": "## Overview\nWith [Shared VPC](/vpc/docs/shared-vpc) , you designate one project as the host project, and you can attach other projects, called service projects, to the host project. You create networks, subnets, secondary address ranges, firewall rules, and other network resources in the host project. Then you share selected subnets, including secondary ranges, with the service projects. Components running in a service project can use the Shared VPC to communicate with components running in the other service projects.\n[Autopilot clusters](/kubernetes-engine/docs/concepts/autopilot-overview)\n[zonal](/kubernetes-engine/docs/concepts/types-of-clusters#zonal_clusters)\n[regional](/kubernetes-engine/docs/concepts/regional-clusters)\nStandard clusters that use Shared VPC cannot use [legacy networks](/vpc/docs/legacy) and must have [VPC-native traffic routing](/kubernetes-engine/docs/how-to/alias-ips) enabled. Autopilot clusters always enable VPC-native traffic routing.\nYou can configure Shared VPC when you create a new cluster. GKE does not support converting existing clusters to the Shared VPC model.\nWith Shared VPC, certain quotas and limits apply. For example there is a quota for the number of networks in a project, and there is a limit on the number of service projects that can be attached to a host project. For details, see [Quotas and limits](/vpc/docs/shared-vpc#quota) .\n### About the examples\nThe examples in this guide set up the infrastructure for a two-tier web application, as described in [Shared VPC overview](/vpc/docs/shared-vpc#two-tier_web_service) .\n## Before you begin\nBefore you start to set up a cluster with Shared VPC:\n- Ensure you have a [Google Cloud organization](/resource-manager/docs/creating-managing-organization) .\n- Ensure your organization has three [Google Cloud projects](/resource-manager/docs/creating-managing-projects) .\n- Become familiar with the [Shared VPC concepts](/vpc/docs/shared-vpc#concepts_and_terminology) including the various [Identity and Access Management (IAM) roles](/vpc/docs/shared-vpc#iam_in_shared_vpc) used by Shared VPC. The tasks in this guide need to be performed by a.\n- Ensure that you understand any organization policy constraints applicable to your organization, folder, or projects. An Organization Policy Administrator might have defined constraints that limit which projects can be Shared VPC host projects or that limit which subnets can be shared. Refer to [organization policy constraints](/vpc/docs/shared-vpc#organization_policy_constraints) for more information.\nBefore you perform the exercises in this guide:\n- Choose one of your projects to be the host project.\n- Choose two of your projects to be service projects.\nEach project has a name, an ID, and a number. In some cases, the name and the ID are the same. This guide uses the following friendly names and placeholders to refer to your projects:\n| Friendly name    | Project ID placeholder | Project number placeholder |\n|:----------------------------|:-------------------------|:-----------------------------|\n| Your host project   | HOST_PROJECT_ID   | HOST_PROJECT_NUM    |\n| Your first service project | SERVICE_PROJECT_1_ID  | SERVICE_PROJECT_1_NUM  |\n| Your second service project | SERVICE_PROJECT_2_ID  | SERVICE_PROJECT_2_NUM  |\n**Note:** Your first and second service projects don't come in order. The terms \"first\" and \"second\" are used only to distinguish one project from the other.\n### Finding your project IDs and numbers\nYou can find your project ID and numbers by using the gcloud CLI or the Google Cloud console.\n- Go to the **Home** page of the Google Cloud console. [Go to the Home page](https://console.cloud.google.com/) \n- In the project picker, select the project that you have chosen to be the host project.\n- Under **Project info** , you can see the project name, project ID, and project number. Make a note of the ID and number for later.\n- Do the same for each of the projects that you have chosen to be service projects.\nList your projects with the following command:\n```\ngcloud projects list\n```\nThe output shows your project names, IDs and numbers. Make a note of the ID and number for later:\n```\nPROJECT_ID  NAME  PROJECT_NUMBER\nhost-123   host  1027xxxxxxxx\nsrv-1-456   srv-1  4964xxxxxxxx\nsrv-2-789   srv-2  4559xxxxxxxx\n```\n### Enabling the GKE API in your projects\nBefore you continue with the exercises in this guide, make sure that the GKE API is enabled in all three of your projects. Enabling the API in a project creates a GKE service account for the project. To perform the remaining tasks in this guide, each of your projects must have a GKE service account.\nYou can enable the GKE API using the Google Cloud console or the Google Cloud CLI.\n- Go to the **APIs & Services** page in the Google Cloud console. [Go to APIs & Services](https://console.cloud.google.com/apis/dashboard) \n- In the project picker, select the project that you have chosen to be the host project.\n- If **Kubernetes Engine API** is in the list of APIs, it is already enabled, and you don't need to do anything. If it is not in the list, click **Enable APIs and Services** . Search for `Kubernetes Engine API` . Click the **Kubernetes Engine API** card, and click **Enable** .\n- Repeat these steps for each projects that you have chosen to be a service project. Each operation may take some time to complete.\nEnable the GKE API for your three projects. Each operation may take some time to complete:\n```\ngcloud services enable container.googleapis.com --project HOST_PROJECT_IDgcloud services enable container.googleapis.com --project SERVICE_PROJECT_1_IDgcloud services enable container.googleapis.com --project SERVICE_PROJECT_2_ID\n```\n## Creating a network and two subnets\nIn this section, you will perform the following tasks:\n- In your host project, create a network named`shared-net`.\n- Create two subnets named`tier-1`and`tier-2`.\n- For each subnet, create two secondary address ranges: one for Services, and one for Pods.\n**Note:** The secondary address ranges for Pods, Services and nodes must not overlap `172.17.0.0/16` and must follow [the defaults and limits for range sizes](/kubernetes-engine/docs/concepts/alias-ips#defaults_limits) .\n- Go to the **VPC networks** page in the Google Cloud console. [Go to VPC networks](https://console.cloud.google.com/networking/networks/list) \n- In the project picker, select your host project.\n- Click **Create VPC Network** .\n- For **Name** , enter `shared-net` .\n- Under **Subnet creation mode** , select **Custom** .\n- In the **New subnet** box, for **Name** , enter `tier-1` .\n- For **Region** , select a region.\n- Under **IP stack type** , select **IPv4 (single-stack)** .\n- For **IPv4 range** , enter `10.0.4.0/22` .\n- Click **Create secondary IPv4 range** . For **Subnet range name** , enter `tier-1-services` , and for **Secondary IPv4 range** , enter `10.0.32.0/20` .\n- Click **Add IP range** . For **Subnet range name** , enter `tier-1-pods` , and for **Secondary IPv4 range** , enter `10.4.0.0/14` .\n- Click **Add subnet** .\n- For **Name** , enter `tier-2` .\n- For **Region** , select the same region that you selected for the previous subnet.\n- For **IPv4 range** , enter `172.16.4.0/22` .\n- Click **Create secondary IPv4 range** . For **Subnet range name** , enter `tier-2-services` , and for **Secondary IPv4 range** , enter `172.16.16.0/20` .\n- Click **Add IP range** . For **Subnet range name** , enter `tier-2-pods` , and for **Secondary IPv4 range** , enter `172.20.0.0/14` .\n- Click **Create** .\nIn your host project, create a network named `shared-net` :\n```\ngcloud compute networks create shared-net \\\u00a0 \u00a0 --subnet-mode custom \\\u00a0 \u00a0 --project HOST_PROJECT_ID\n```\nIn your new network, create a subnet named `tier-1` :\n```\ngcloud compute networks subnets create tier-1 \\\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --network shared-net \\\u00a0 \u00a0 --range 10.0.4.0/22 \\\u00a0 \u00a0 --region COMPUTE_REGION \\\u00a0 \u00a0 --secondary-range tier-1-services=10.0.32.0/20,tier-1-pods=10.4.0.0/14\n```\nCreate another subnet named `tier-2` :\n```\ngcloud compute networks subnets create tier-2 \\\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --network shared-net \\\u00a0 \u00a0 --range 172.16.4.0/22 \\\u00a0 \u00a0 --region COMPUTE_REGION \\\u00a0 \u00a0 --secondary-range tier-2-services=172.16.16.0/20,tier-2-pods=172.20.0.0/14\n```\nReplace `` with a [Compute Engine region](/compute/docs/regions-zones#available) .\n## Determining the names of service accounts in your service projects\nYou have two service projects, each of which has several [service accounts](/compute/docs/access/service-accounts) . This section is concerned with your GKE service accounts and your [Google APIs service accounts](/compute/docs/access/service-accounts#google_apis_service_account) . You need the names of these service accounts for the next section.\nThe following table lists the names of the GKE and Google APIs service accounts in your two service projects:\n| Service account type | Service account name               |\n|:-----------------------|:-----------------------------------------------------------------------------|\n| GKE     | service-SERVICE_PROJECT_1_NUM@container-engine-robot.iam.gserviceaccount.com |\n| GKE     | service-SERVICE_PROJECT_2_NUM@container-engine-robot.iam.gserviceaccount.com |\n| Google APIs   | SERVICE_PROJECT_1_NUM@cloudservices.gserviceaccount.com      |\n| Google APIs   | SERVICE_PROJECT_2_NUM@cloudservices.gserviceaccount.com      |\n## Enabling Shared VPC and granting roles\nTo perform the tasks in this section, ensure that your organization has defined a [Shared VPC Admin](/vpc/docs/shared-vpc#iam_roles_required_for_shared_vpc) role.\nIn this section, you will perform the following tasks:\n- In your host project, enable Shared VPC.\n- Attach your two service projects to the host project.\n- Grant the appropriate IAM roles to service accounts that belong to your service projects:- In your first service project, grant two service accounts the`Compute Network User`role on the`tier-1`subnet of your host project.\n- In your second service project, grant two service accounts the`Compute Network User`role on the`tier-2`subnet of your host project.Perform the following steps to enable Shared VPC, attach service projects, and grant roles:- Go to the **Shared VPC page** in the Google Cloud console. [Go to Shared VPC](https://console.cloud.google.com/networking/xpn/details) \n- In the project picker, select your host project.\n- Click **Set up Shared VPC** . The **Enable host project** screen displays.\n- Click **Save & continue** . The **Select subnets** page displays.\n- Under **Sharing mode** , select **Individual subnets** .\n- Under **Subnets to share** , check **tier-1** and **tier-2** . Clear all other checkboxes.\n- Click **Continue** . The **Give permissions** page displays.\n- Under **Attach service projects** , check your first service project and your second service project. Clear all the other checkboxes under **Attach service projects** .\n- Under **Kubernetes Engine access** , check **Enabled** .\n- Click **Save** . A new page displays.\n- Under **Individual subnet permissions** , check **tier-1** .\n- In the right pane, delete any service accounts that belong to your second service project. That is, delete any service accounts that contain `` .\n- In the right pane, look for the names of the Kubernetes Engine and Google APIs service accounts that belong to your first service project. You want to see both of those service account names in the list. If either of those is not in the list, enter the service account name under **Add members** , and click **Add** .\n- In the center pane, under **Individual subnet permissions** , check **tier-2** , and clear **tier-1** .\n- In the right pane, delete any service accounts that belong to your first service project. That is, delete any service accounts that contain `` .\n- In the right pane, look for the names of the Kubernetes Engine and Google APIs service accounts that belong to your second service project. You want to see both of those service account names in the list. If either of those is not in the list, enter the service account name under **Add members** , and click **Add** .\n- Enable Shared VPC in your host project. The command that you use depends on the [required administrative role](/vpc/docs/shared-vpc#iam_roles_required_for_shared_vpc) that you have. **If you have Shared VPC Admin role at the organizational level:** ```\ngcloud compute shared-vpc enable HOST_PROJECT_ID\n``` **If you have Shared VPC Admin role at the folder level:** ```\ngcloud beta compute shared-vpc enable HOST_PROJECT_ID\n```\n- Attach your first service project to your host project:```\ngcloud compute shared-vpc associated-projects add SERVICE_PROJECT_1_ID \\\u00a0 \u00a0 --host-project HOST_PROJECT_ID\n```\n- Attach your second service project to your host project:```\ngcloud compute shared-vpc associated-projects add SERVICE_PROJECT_2_ID \\\u00a0 \u00a0 --host-project HOST_PROJECT_ID\n```\n- Get the IAM policy for the `tier-1` subnet:```\ngcloud compute networks subnets get-iam-policy tier-1 \\\u00a0 \u00a0--project HOST_PROJECT_ID \\\u00a0 \u00a0--region COMPUTE_REGION\n```The output contains an `etag` field. Make a note of the `etag` value.\n- Create a file named `tier-1-policy.yaml` that has the following content:```\nbindings:- members:\u00a0 - serviceAccount:SERVICE_PROJECT_1_NUM@cloudservices.gserviceaccount.com\u00a0 - serviceAccount:service-SERVICE_PROJECT_1_NUM@container-engine-robot.iam.gserviceaccount.com\u00a0 role: roles/compute.networkUseretag: ETAG_STRING\n```Replace `` with the `etag` value that you noted previously.\n- Set the IAM policy for the `tier-1` subnet:```\ngcloud compute networks subnets set-iam-policy tier-1 \\\u00a0 \u00a0 tier-1-policy.yaml \\\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --region COMPUTE_REGION\n```\n- Get the IAM policy for the `tier-2` subnet:```\ngcloud compute networks subnets get-iam-policy tier-2 \\\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --region COMPUTE_REGION\n```The output contains an `etag` field. Make a note of the `etag` value.\n- Create a file named `tier-2-policy.yaml` that has the following content:```\nbindings:- members:\u00a0 - serviceAccount:SERVICE_PROJECT_2_NUM@cloudservices.gserviceaccount.com\u00a0 - serviceAccount:service-SERVICE_PROJECT_2_NUM@container-engine-robot.iam.gserviceaccount.com\u00a0 role: roles/compute.networkUseretag: ETAG_STRING\n```Replace `` with the `etag` value that you noted previously.\n- Set the IAM policy for the `tier-2` subnet:```\ngcloud compute networks subnets set-iam-policy tier-2 \\\u00a0 \u00a0 tier-2-policy.yaml \\\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --region COMPUTE_REGION\n```## Managing firewall resources\nIf you want a GKE cluster in a service project to create and manage the firewall resources in your host project, the service project's GKE service account must be granted the appropriate IAM permissions using one of the following strategies:\n**Note:** To follow security best practices, choose the finer grained approach. Granting the service project's GKE service account the Compute Security Admin role will allow it more IAM permissions than is necessary for the purposes of this guide.\n- Grant the service project's GKE service account the [Compute Security Admin](/compute/docs/access/iam#compute.securityAdmin) role to the host project.\n- In the Google Cloud console, go to the **IAM** page. [Goto IAM](https://console.cloud.google.com/projectselector/iam-admin/iam?supportedpurview=project,folder,organizationId) \n- Select the host project.\n- Click person_add **Grant access** , then enter the service project's GKE service account principal, `service-` `` `@container-engine-robot.iam.gserviceaccount.com` .\n- Select the `Compute Security Admin` role from the drop-down list.\n- Click **Save** .\nGrant the service project's GKE service account the `Compute Security Admin` role within the host project:\n```\ngcloud projects add-iam-policy-binding HOST_PROJECT_ID \\\u00a0 \u00a0 --member=serviceAccount:service-SERVICE_PROJECT_NUM@container-engine-robot.iam.gserviceaccount.com \\\u00a0 \u00a0 --role=roles/compute.securityAdmin\n```\nReplace the following:- ``: the shared VPC host project ID\n- ``: the ID of the service project containing the GKE service account\n- For a finer grained approach, [create a custom IAM role](/iam/docs/creating-custom-roles#creating_a_custom_role) that includes only the following permissions: `compute.networks.updatePolicy` , `compute.firewalls.list` , `compute.firewalls.get` , `compute.firewalls.create` , `compute.firewalls.update` , and `compute.firewalls.delete` . Grant the service project's GKE service account that custom role to the host project.\nCreate a custom role within the host project containing the IAM permissions mentioned earlier:- In the Google Cloud console, go to the **Roles** page. [Go to the Roles page](https://console.cloud.google.com/iam-admin/roles) \n- Using the drop-down list at the top of the page, select the host project.\n- Click **Create Role** .\n- Enter a **Title** , **Description** , **ID** and **Role launch stage** for the role. The role name cannot be changed after the role is created.\n- Click **Add Permissions** .\n- Filter for `compute.networks` and select the IAM permissions mentioned previously.\n- Once all required permissions are selected, click **Add** .\n- Click **Create** .\nGrant the service project's GKE service account the newly created custom role within the host project:- In the Google Cloud console, go to the **IAM** page. [Goto IAM](https://console.cloud.google.com/projectselector/iam-admin/iam?supportedpurview=project,folder,organizationId) \n- Select the host project.\n- Click person_add **Grant access** , then enter the service project's GKE service account principal, `service-` `` `@container-engine-robot.iam.gserviceaccount.com` .\n- Filter for the **Title** of the newly created custom role and select it.\n- Click **Save** .\n- Create a custom role within the host project containing the IAM permissions mentioned earlier:```\ngcloud iam roles create ROLE_ID \\\u00a0 \u00a0 --title=\"ROLE_TITLE\" \\\u00a0 \u00a0 --description=\"ROLE_DESCRIPTION\" \\\u00a0 \u00a0 --stage=LAUNCH_STAGE \\\u00a0 \u00a0 --permissions=compute.networks.updatePolicy,compute.firewalls.list,compute.firewalls.get,compute.firewalls.create,compute.firewalls.update,compute.firewalls.delete \\\u00a0 \u00a0 --project=HOST_PROJECT_ID\n```\n- Grant the service project's GKE service account the newly created custom role within the host project:```\ngcloud projects add-iam-policy-binding HOST_PROJECT_ID \\\u00a0 \u00a0 --member=serviceAccount:service-SERVICE_PROJECT_NUM@container-engine-robot.iam.gserviceaccount.com \\\u00a0 \u00a0 --role=projects/HOST_PROJECT_ID/roles/ROLE_ID\n```Replace the following:- ``: the name of the role, such as`gkeFirewallAdmin`\n- ``: a friendly title for the role, such as`GKE Firewall Admin`\n- ``: a short description of the role, such as`GKE service account FW permissions`\n- ``: the [launch stage](/products#section-22) of the role in its lifecycle, such as`ALPHA`,`BETA`, or`GA`\n- ``: the shared VPC host project ID\n- ``: the ID of the service project containing the GKE service accountIf you have clusters in more than one service project, you must choose one of the strategies and repeat it for each service project's GKE service account.\n**Note:** If you are using Ingress for internal Application Load Balancers, the Ingress controller does not create a firewall rule to allow connections from the load balancer proxies in the proxy-subnet. You must create this firewall rule manually. However, the Ingress controller does create firewall rules to allow ingress for Google Cloud health-checks.\n### Summary of roles granted on subnets\nHere's a summary of the roles granted on the subnets:\n| Service account                | Role     | Subnet |\n|:-----------------------------------------------------------------------------|:---------------------|:---------|\n| service-SERVICE_PROJECT_1_NUM@container-engine-robot.iam.gserviceaccount.com | Compute Network User | tier-1 |\n| SERVICE_PROJECT_1_NUM@cloudservices.gserviceaccount.com      | Compute Network User | tier-1 |\n| service-SERVICE_PROJECT_2_NUM@container-engine-robot.iam.gserviceaccount.com | Compute Network User | tier-2 |\n| SERVICE_PROJECT_2_NUM@cloudservices.gserviceaccount.com      | Compute Network User | tier-2 |\n### Kubernetes Engine access\nWhen attaching a service project, enabling Kubernetes Engine access grants the service project's GKE service account the permissions to perform network management operations in the host project.\nIf a service project was attached without enabling Kubernetes Engine access, assuming the Kubernetes Engine API has already been enabled in both the host and service project, you can manually assign the permissions to the service project's GKE service account by adding the following IAM role bindings in the host project:\n| Member                  | Role     | Resource        |\n|:---------------------------------------------------------------------------|:------------------------|:----------------------------------------|\n| service-SERVICE_PROJECT_NUM@container-engine-robot.iam.gserviceaccount.com | Compute Network User | Specific subnet or whole host project |\n| service-SERVICE_PROJECT_NUM@container-engine-robot.iam.gserviceaccount.com | Host Service Agent User | GKE service account in the host project |\n## Granting the Host Service Agent User role\nEach service project's GKE service account must have a binding for the [Host Service Agent User](/kubernetes-engine/docs/how-to/iam#host_service_agent_user) role on the host project. The GKE service account takes the following form:\n```\nservice-SERVICE_PROJECT_NUM@container-engine-robot.iam.gserviceaccount.com\n```\nWhere `` is the project number of your service project.\nThis binding allows the service project's GKE service account to perform network management operations in the host project, as if it were the host project's GKE service account. This role can only be granted to a service project's GKE service account.\nIf you have been using the Google Cloud console, you do not have to grant the Host Service Agent User role explicitly. That was done automatically when you used the Google Cloud console to attach service projects to your host project.- For your first project, grant the Host Service Agent User role to the project's GKE Service Account. This role is granted on your host project:```\ngcloud projects add-iam-policy-binding HOST_PROJECT_ID \\\u00a0 \u00a0 --member serviceAccount:service-SERVICE_PROJECT_1_NUM@container-engine-robot.iam.gserviceaccount.com \\\u00a0 \u00a0 --role roles/container.hostServiceAgentUser\n```\n- For your second project, grant the Host Service Agent User role to the project's GKE Service Account. This role is granted on your host project:```\ngcloud projects add-iam-policy-binding HOST_PROJECT_ID \\\u00a0 \u00a0 --member serviceAccount:service-SERVICE_PROJECT_2_NUM@container-engine-robot.iam.gserviceaccount.com \\\u00a0 \u00a0 --role roles/container.hostServiceAgentUser\n```## Verifying usable subnets and secondary IP address ranges\nWhen creating a cluster, you must specify a subnet and the secondary IP address ranges to be used for the cluster's Pods and Services. There are several reasons that an IP address range might not be available for use. Whether you are creating the cluster with the Google Cloud console or the gcloud CLI, you should specify usable IP address ranges.\nAn IP address range is usable for the new cluster's [Services](/kubernetes-engine/docs/concepts/service) if the range isn't already in use. The IP address range that you specify for the new cluster's Pods can either be an unused range, or it can be a range that's shared with Pods in your other clusters. IP address ranges that are created and managed by GKE can't be used by your cluster.\nYou can list a project's usable subnets and secondary IP address ranges by using the gcloud CLI.\n```\ngcloud container subnets list-usable \\\u00a0 \u00a0 --project SERVICE_PROJECT_ID \\\u00a0 \u00a0 --network-project HOST_PROJECT_ID\n```\nReplace `` with the project ID of the service project.\nIf you omit the `--project` or `--network-project` option, the gcloud CLI command uses the default project from your [active configuration](/sdk/gcloud/reference/config/set) . Because the host project and network project are distinct, you must specify one or both of `--project` and `--network-project` .\nThe output is similar to the following:\n```\nPROJECT: xpn-host\nREGION: REGION_NAME\nNETWORK: shared-net\nSUBNET: tier-2\nRANGE: 172.16.4.0/22\nSECONDARY_RANGE_NAME: tier-2-services\nIP_CIDR_RANGE: 172.20.0.0/14\nSTATUS: usable for pods or services\nSECONDARY_RANGE_NAME: tier-2-pods\nIP_CIDR_RANGE: 172.16.16.0/20\nSTATUS: usable for pods or services\nPROJECT: xpn-host\nREGION: REGION_NAME\nNETWORK: shared-net\nSUBNET: tier-1\nRANGE: 10.0.4.0/22\nSECONDARY_RANGE_NAME: tier-1-services\nIP_CIDR_RANGE: 10.0.32.0/20\nSTATUS: usable for pods or services\nSECONDARY_RANGE_NAME: tier-1-pods\nIP_CIDR_RANGE: 10.4.0.0/14\nSTATUS: usable for pods or services\n```\nThe `list-usable` command returns an empty list in the following situations:- When the service project's Kubernetes Engine service account does not have the Host Service Agent User role to the host project.\n- When the Kubernetes Engine service account in the host project does not exist (for example, if you've deleted that account accidentally).\n- When Kubernetes Engine API is not enabled in the host project, which implies the Kubernetes Engine service account in the host project is missing.\nFor more information, see the [troubleshooting](#troubleshooting) section.\n### Notes about secondary ranges\nYou can create 30 secondary ranges in a given subnet. For each cluster, you need two secondary ranges: one for Pods and one for Services.\n**Note:** The primary range and the Pod secondary range can be shared between clusters, but this is not a recommended configuration.\n## Creating a cluster in your first service project\nTo create a cluster in your first service project, perform the following steps using the gcloud CLI or the Google Cloud console.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the project picker, select your first service project.\n- Click **Create** .\n- In the Autopilot or Standard section, click **Configure** .\n- For **Name** , enter `tier-1-cluster` .\n- In the **Region** drop-down list, select the same region that you used for the subnets.\n- From the navigation pane, click **Networking** .\n- Select **Networks shared with me (from host project)** .\n- For **Network** , select **shared-net** .\n- For **Node subnet** , select **tier-1** .\n- For **Pod secondary CIDR range** , select **tier-1-pods** .\n- For **Services secondary CIDR range** , select **tier-1-services** .\n- Click **Create** .\n- When the creation is complete, in the list of clusters, click **tier-1-cluster** .\n- On the **Cluster details** page, click the **Nodes** tab\n- Under **Node Pools** , click the name of the node pool you want to inspect.\n- Under **Instance groups** , click the name of the instance group you want to inspect. For example, gke-tier-1-cluster-default-pool-5c5add1f-grp.\n- In the list of instances, verify that the internal IP addresses of your nodes are in the primary range of the tier-1 subnet: `10.0.4.0/22` .\nCreate a cluster named `tier-1-cluster` in your first service project:\n```\ngcloud container clusters create-auto tier-1-cluster \\\u00a0 \u00a0 --project=SERVICE_PROJECT_1_ID \\\u00a0 \u00a0 --location=COMPUTE_REGION \\\u00a0 \u00a0 --network=projects/HOST_PROJECT_ID/global/networks/shared-net \\\u00a0 \u00a0 --subnetwork=projects/HOST_PROJECT_ID/regions/COMPUTE_REGION/subnetworks/tier-1 \\\u00a0 \u00a0 --cluster-secondary-range-name=tier-1-pods \\\u00a0 \u00a0 --services-secondary-range-name=tier-1-services\n```\nWhen the creation is complete, verify that your cluster nodes are in the primary range of the tier-1 subnet: `10.0.4.0/22` .\n```\ngcloud compute instances list --project SERVICE_PROJECT_1_ID\n```\nThe output shows the internal IP addresses of the nodes:\n```\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0ZONE \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ... INTERNAL_IPgke-tier-1-cluster-... \u00a0ZONE_NAME \u00a0 \u00a0 \u00a0... 10.0.4.2gke-tier-1-cluster-... \u00a0ZONE_NAME \u00a0 \u00a0 \u00a0... 10.0.4.3gke-tier-1-cluster-... \u00a0ZONE_NAME \u00a0 \u00a0 \u00a0... 10.0.4.4\n```\n## Creating a cluster in your second service project\nTo create a cluster in your second service project, perform the following steps using the gcloud CLI or the Google Cloud console.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the project picker, select your second service project.\n- Click **Create** .\n- In the Standard or Autopilot section, click **Configure** .\n- For **Name** , enter `tier-2-cluster` .\n- In the **Region** drop-down list, select the same region that you used for the subnets.\n- From the navigation pane, click **Networking** .\n- For **Network** , select **shared-net** .\n- For **Node subnet** , select **tier-2** .\n- For **Pod secondary CIDR range** , select **tier-2-pods** .\n- For **Services secondary CIDR range** , select **tier-2-services** .\n- Click **Create** .\n- When the creation is complete, in the list of clusters, click **tier-2-cluster** .\n- On the **Cluster details** page, click the **Nodes** tab\n- Under **Node Pools** , click the name of the node pool you want to inspect.\n- Under **Instance groups** , click the name of the instance group you want to inspect. For example, `gke-tier-2-cluster-default-pool-5c5add1f-grp` .\n- In the list of instances, verify that the internal IP addresses of your nodes are in the primary range of the tier-2 subnet: `172.16.4.0/22` .\nCreate a cluster named `tier-2-cluster` in your second service project:\n```\ngcloud container clusters create-auto tier-2-cluster \\\u00a0 \u00a0 --project=SERVICE_PROJECT_2_ID \\\u00a0 \u00a0 --location=COMPUTE_REGION \\\u00a0 \u00a0 --network=projects/HOST_PROJECT_ID/global/networks/shared-net \\\u00a0 \u00a0 --subnetwork=projects/HOST_PROJECT_ID/regions/COMPUTE_REGION/subnetworks/tier-2 \\\u00a0 \u00a0 --cluster-secondary-range-name=tier-2-pods \\\u00a0 \u00a0 --services-secondary-range-name=tier-2-services\n```\nWhen the creation is complete, verify that your cluster nodes are in the primary range of the tier-2 subnet: `172.16.4.0/22` .\n```\ngcloud compute instances list --project SERVICE_PROJECT_2_ID\n```\nThe output shows the internal IP addresses of the nodes:\n```\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0ZONE \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ... INTERNAL_IPgke-tier-2-cluster-... \u00a0ZONE_NAME \u00a0 \u00a0 \u00a0... 172.16.4.2gke-tier-2-cluster-... \u00a0ZONE_NAME \u00a0 \u00a0 \u00a0... 172.16.4.3gke-tier-2-cluster-... \u00a0ZONE_NAME \u00a0 \u00a0 \u00a0... 172.16.4.4\n```\n## Creating firewall rules\nTo allow traffic into the network and between the clusters within the network, you need to create [firewalls](/kubernetes-engine/docs/concepts/firewall-rules) . The following sections demonstrate how to create and update firewall rules:\n- [Creating a firewall rule to enable SSH connection to a node](#firewall-ssh) : Demonstrates how to create a firewall rule that enables traffic from outside of the clusters using SSH.\n- [Updating the firewall rule to ping between nodes](#ping-nodes) : Demonstrates how to update the firewall rule to permit ICMP traffic between the clusters.SSH and ICMP are used as examples, you must create firewall rules that enable your specific application's networking requirements.\n**Note:** Shared VPC Admins are responsible for creating firewall rules in the Shared VPC network.\n### Creating a firewall rule to enable SSH connection to a node\nIn your host project, create a firewall rule for the `shared-net` network. Allow traffic to enter on TCP port `22` , which permits you to connect to your cluster nodes using SSH.\n- Go to the **Firewall** page in the Google Cloud console. [Go to Firewall](https://console.cloud.google.com/networking/firewalls) \n- In the project picker, select your host project.\n- From the **VPC Networking** menu, click **Create Firewall Rule** .\n- For **Name** , enter `my-shared-net-rule` .\n- For **Network** , select **shared-net** .\n- For **Direction of traffic** , select **Ingress** .\n- For **Action on match** , select **Allow** .\n- For **Targets** , select **All instances in the network** .\n- For **Source filter** , select **IP ranges** .\n- For **Source IP ranges** , enter `0.0.0.0/0` .\n- For **Protocols and ports** , select **Specified protocols and ports** . In the box, enter `tcp:22` .\n- Click **Create** .\nCreate a firewall rule for your shared network:\n```\ngcloud compute firewall-rules create my-shared-net-rule \\\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --network shared-net \\\u00a0 \u00a0 --direction INGRESS \\\u00a0 \u00a0 --allow tcp:22\n```\nAfter creating the firewall that allows ingress traffic on TCP port `22` , connect to the node using SSH.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the project picker, select your first service project.\n- Click **tier-1-cluster** .\n- On the **Cluster details** page, click the **Nodes** tab.\n- Under **Node Pools** , click the name of your node pool.\n- Under **Instance groups** , click the name of your instance group. For example, gke-tier-1-cluster-default-pool-faf87d48-grp.\n- In the list of instances, make a note of the internal IP addresses of the nodes. These addresses are in the `10.0.4.0/22` range.\n- For one of your nodes, click **SSH** . This succeeds because SSH uses TCP port `22` , which is allowed by your firewall rule.\nList the nodes in your first service project:\n```\ngcloud compute instances list --project SERVICE_PROJECT_1_ID\n```\nThe output includes the names of the nodes in your cluster:\n```\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ...gke-tier-1-cluster-default-pool-faf87d48-3mf8 \u00a0...gke-tier-1-cluster-default-pool-faf87d48-q17k \u00a0...gke-tier-1-cluster-default-pool-faf87d48-x9rk \u00a0...\n```\nConnect to one of your nodes using SSH:\n```\ngcloud compute ssh NODE_NAME \\\u00a0 \u00a0 --project SERVICE_PROJECT_1_ID \\\u00a0 \u00a0 --zone COMPUTE_ZONE\n```\nReplace the following:- ``: the name of one of your nodes.\n- ``: the name of a [Compute Engine zone](/compute/docs/regions-zones#available) within the region.\n### Updating the firewall rule to ping between nodes\n- In your SSH command-line window, start the [CoreOS Toolbox](/container-optimized-os/docs/how-to/toolbox) :```\n/usr/bin/toolbox\n```\n- In the toolbox shell, ping one of your other nodes in the same cluster. For example:```\nping 10.0.4.4\n```The `ping` command succeeds, because your node and the other node are both in the `10.0.4.0/22` range.\n- Now, try to ping one of the nodes in the cluster in your other service project. For example:```\nping 172.16.4.3\n```This time the `ping` command fails, because your firewall rule does not allow Internet Control Message Protocol (ICMP) traffic.\n- At an ordinary command prompt, not your toolbox shell, Update your firewall rule to allow ICMP:```\ngcloud compute firewall-rules update my-shared-net-rule \\\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --allow tcp:22,icmp\n```\n- In your toolbox shell, ping the node again. For example:```\nping 172.16.4.3\n```This time the `ping` command succeeds.\n### Creating additional firewall rules\nYou can create additional firewall rules to allow communication between nodes, Pods, and Services in your clusters.\nFor example, the following rule allows traffic to enter from any node, Pod, or Service in `tier-1-cluster` on any TCP or UDP port:\n```\ngcloud compute firewall-rules create my-shared-net-rule-2 \\\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --network shared-net \\\u00a0 \u00a0 --allow tcp,udp \\\u00a0 \u00a0 --direction INGRESS \\\u00a0 \u00a0 --source-ranges 10.0.4.0/22,10.4.0.0/14,10.0.32.0/20\n```\nThe following rule allows traffic to enter from any node, Pod, or Service in `tier-2-cluster` on any TCP or UDP port:\n```\ngcloud compute firewall-rules create my-shared-net-rule-3 \\\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --network shared-net \\\u00a0 \u00a0 --allow tcp,udp \\\u00a0 \u00a0 --direction INGRESS \\\u00a0 \u00a0 --source-ranges 172.16.4.0/22,172.20.0.0/14,172.16.16.0/20\n```\nKubernetes will also try to create and manage firewall resources when necessary, for example when you create a load balancer service. If Kubernetes finds itself unable to change the firewall rules due to a permission issue, a Kubernetes Event will be raised to guide you on how to make the changes.\nIf you want to grant Kubernetes permission to change the firewall rules, see [Managing firewall resources](#managing_firewall_resources) .\nFor Ingress Load Balancers, if Kubernetes can't change the firewall rules due to insufficient permission, a `firewallXPNError` event is emitted every several minutes. In [GLBC 1.4](https://github.com/kubernetes/ingress-gce) and later, you can mute the `firewallXPNError` event by adding `networking.gke.io/suppress-firewall-xpn-error: \"true\"` annotation to the ingress resource. You can always remove this annotation to unmute.\n## Creating a private cluster in a Shared VPC\nYou can use Shared VPC with [private clusters](/kubernetes-engine/docs/concepts/private-cluster-concept#using_in_private_clusters) .\nThis requires that you grant the following permissions on the host project, either to the user account or to the service account, used to create the cluster:\n- `compute.networks.get`\n- `compute.networks.updatePeering`\nYou must also ensure that the control plane IP address range does not overlap with other reserved ranges in the shared network.\nFor private clusters created prior to January 15, 2020, the maximum number of private GKE clusters you can have per VPC network is limited to the number of peering connections from a single VPC network. New private clusters [reuse VPC Network Peering connections](/kubernetes-engine/docs/concepts/private-cluster-concept#network_peering_reuse) which removes this limitation. To enable VPC Network Peering reuse on older private clusters, you can delete a cluster and recreate it. Upgrading a cluster does not cause it to reuse an existing VPC Network Peering connection.\nIn this section, you create a VPC-native cluster named `private-cluster-vpc` in a predefined shared VPC network.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- In the Autopilot or Standard section, click **Configure** .\n- For **Name** , enter `private-cluster-vpc` .\n- From the navigation pane, click **Networking** .\n- Select **Private cluster** .\n- (Optional for Autopilot): Set **Control plane IP range** to `172.16.0.16/28` .\n- In the **Network** drop-down list, select the VPC network you created previously.\n- In the **Node subnet** drop-down list, select the shared subnet you created previously.\n- Configure your cluster as needed.\n- Click **Create** .\nRun the following command to create a cluster named `private-cluster-vpc` in a predefined Shared VPC:\n```\ngcloud container clusters create-auto private-cluster-vpc \\\u00a0 \u00a0 --project=PROJECT_ID \\\u00a0 \u00a0 --location=COMPUTE_REGION \\\u00a0 \u00a0 --network=projects/HOST_PROJECT/global/networks/shared-net \\\u00a0 \u00a0 --subnetwork=SHARED_SUBNETWORK \\\u00a0 \u00a0 --cluster-secondary-range-name=tier-1-pods \\\u00a0 \u00a0 --services-secondary-range-name=tier-1-services \\\u00a0 \u00a0 --enable-private-nodes \\\u00a0 \u00a0 --master-ipv4-cidr=172.16.0.0/28\n```\n## Reserving IP addresses\nYou can reserve [internal](/compute/docs/ip-addresses/reserve-static-internal-ip-address) and [external IP addresses](/compute/docs/ip-addresses/reserve-static-external-ip-address) for your Shared VPC clusters. Ensure that the IP addresses are reserved in the service project.\nFor internal IP addresses, you must provide the subnetwork where the IP address belongs. To reserve an IP address across projects, use the full resource URL to identify the subnetwork.\nYou can use the following command in the Google Cloud CLI to reserve an internal IP address:\n```\ngcloud compute addresses create RESERVED_IP_NAME \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --subnet=projects/HOST_PROJECT_ID/regions/COMPUTE_REGION/subnetworks/SUBNETWORK_NAME \\\u00a0 \u00a0 --addresses=IP_ADDRESS \\\u00a0 \u00a0 --project=SERVICE_PROJECT_ID\n```\nTo call this command, you must have the `compute.subnetworks.use` permission added to the subnetwork. You can either [grant the caller a compute.networkUser role on the subnetwork](#enabling_and_granting_roles) , or you can [grant the caller a customized role](/iam/docs/creating-custom-roles) with `compute.subnetworks.use` permission at the project level.\n## Cleaning up\nAfter completing the exercises in this guide, perform the following tasks to remove the resources to prevent unwanted charges incurring on your account:\n### Deleting the clusters\nDelete the two clusters you created.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the project picker, select your first service project.\n- Select the **tier-1-cluster** , and click **Delete** .\n- In the project picker, select your second service project.\n- Select the **tier-2-cluster** , and click **Delete** .\n```\ngcloud container clusters delete tier-1-cluster \\\u00a0 \u00a0 --project SERVICE_PROJECT_1_ID \\\u00a0 \u00a0 --zone COMPUTE_ZONEgcloud container clusters delete tier-2-cluster \\\u00a0 \u00a0 --project SERVICE_PROJECT_2_ID \\\u00a0 \u00a0 --zone COMPUTE_ZONE\n```\n### Disabling Shared VPC\nDisable Shared VPC in your host project.\n- Go to the **Shared VPC** page in the Google Cloud console. [Go to Shared VPC](https://console.cloud.google.com/networking/xpn/list) \n- In the project picker, select your host project.\n- Click **Disable Shared VPC** .\n- Enter the `` in the field, and click **Disable** .\n```\ngcloud compute shared-vpc associated-projects remove SERVICE_PROJECT_1_ID \\\u00a0 \u00a0 --host-project HOST_PROJECT_IDgcloud compute shared-vpc associated-projects remove SERVICE_PROJECT_2_ID \\\u00a0 \u00a0 --host-project HOST_PROJECT_IDgcloud compute shared-vpc disable HOST_PROJECT_ID\n```\n### Deleting your firewall rules\nRemove the firewall rules you created.\n- Go to the **Firewall** page in the Google Cloud console. [Go to Firewall](https://console.cloud.google.com/networking/firewalls) \n- In the project picker, select your host project.\n- In the list of rules, select **my-shared-net-rule** , **my-shared-net-rule-2** , and **my-shared-net-rule-3** .\n- Click **Delete** .\nDelete your firewall rules:\n```\ngcloud compute firewall-rules delete \\\u00a0 \u00a0 my-shared-net-rule \\\u00a0 \u00a0 my-shared-net-rule-2 \\\u00a0 \u00a0 my-shared-net-rule-3 \\\u00a0 \u00a0 --project HOST_PROJECT_ID\n```\n### Deleting the shared network\nDelete the shared network you created.\n- Go to the **VPC networks** page in the Google Cloud console. [Go to VPC networks](https://console.cloud.google.com/networking/networks/list) \n- In the project picker, select your host project.\n- In the list of networks, select **shared-net** .\n- Click **Delete VPC Network** .\n```\ngcloud compute networks subnets delete tier-1 \\\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --region COMPUTE_REGIONgcloud compute networks subnets delete tier-2 \\\u00a0 \u00a0 --project HOST_PROJECT_ID \\\u00a0 \u00a0 --region COMPUTE_REGIONgcloud compute networks delete shared-net --project HOST_PROJECT_ID\n```\n### Removing the Host Service Agent User role\nRemove the Host Service Agent User roles from your two service projects.\n- Go to the **IAM** page in the Google Cloud console. [Go to IAM](https://console.cloud.google.com/iam-admin/iam) \n- In the project picker, select your host project.\n- In the list of members, select the row that shows `service-` `` `@container-engine-robot.iam.gserviceaccount.com` is granted the Kubernetes Engine Host Service Agent User role.\n- Select the row that shows `service-` `` `@container-engine-robot.iam.gserviceaccount.com` is granted the Kubernetes Engine Host Service Agent User role.\n- Click **Remove access** .\n- Remove the Host Service Agent User role from the GKE service account of your first service project:```\ngcloud projects remove-iam-policy-binding HOST_PROJECT_ID \\\u00a0 \u00a0 --member serviceAccount:service-SERVICE_PROJECT_1_NUM@container-engine-robot.iam.gserviceaccount.com \\\u00a0 \u00a0 --role roles/container.hostServiceAgentUser\n```\n- Remove the Host Service Agent User role from the GKE service account of your second service project:```\ngcloud projects remove-iam-policy-binding HOST_PROJECT_ID \\\u00a0 \u00a0 --member serviceAccount:service-SERVICE_PROJECT_2_NUM@container-engine-robot.iam.gserviceaccount.com \\\u00a0 \u00a0 --role roles/container.hostServiceAgentUser\n```## Troubleshooting\n### Permission Denied\n**Symptom:**\n```\nFailed to get metadata from network project. GCE_PERMISSION_DENIED: Google\nCompute Engine: Required 'compute.projects.get' permission for\n'projects/HOST_PROJECT_ID\n```\n**Possible reasons:**- The Kubernetes Engine API has not been enabled in the host project.\n- The host project's GKE service account does not exist. For example, it might have been deleted.\n- The host project's GKE service account does not have the ( `container.serviceAgent` ) role in the host project. The binding might have been accidentally removed.\n- The service project's GKE service account does not have the Host Service Agent User role in the host project.\n**To fix the problem:** Determine whether the host project's GKE service account exists. If it does not, do the following:\n- If the Kubernetes Engine API is not enabled in the host project, enable it. This creates the host project's GKE service account and grants the host project's GKE service account the ( `container.serviceAgent` ) role in the host project.\n- If the Kubernetes Engine API is enabled in the host project, this means that either the host project's GKE service account has been deleted or it does not have the ( `container.serviceAgent` ) role in the host project. To restore the GKE service account or the role binding, you must disable then re-enable the Kubernetes Engine API. For more information, refer to [Google Kubernetes EngineTroubleshooting](/kubernetes-engine/docs/troubleshooting#gke_service_account_deleted) . **Note:** Disabling then re-enabling the Kubernetes Engine API in the host project won't impact the operation of clusters in service projects. However, you cannot create new clusters that use a Shared VPC network, in a service project or in the host project, while the Kubernetes Engine API is disabled.## What's next\n- [Read the Shared VPC overview](/vpc/docs/shared-vpc) .\n- [Learn about provisioning a Shared VPC](/vpc/docs/provisioning-shared-vpc) .\n- [Read the GKE network overview](/kubernetes-engine/docs/concepts/network-overview) .\n- [Read about automatically created firewall rules](/kubernetes-engine/docs/concepts/firewall-rules) .\n- [Learn how to troubleshoot connectivity between virtual machine (VM) instances with internal IP addresses](/vpc/docs/ts-vm-vm-internal) .", "guide": "Google Kubernetes Engine (GKE)"}