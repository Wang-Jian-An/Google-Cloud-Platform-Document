{"title": "Google Kubernetes Engine (GKE) - Troubleshooting Autopilot clusters", "url": "https://cloud.google.com/kubernetes-engine/docs/troubleshooting/autopilot-clusters", "abstract": "# Google Kubernetes Engine (GKE) - Troubleshooting Autopilot clusters\nThis page shows you how to resolve issues with Google Kubernetes Engine (GKE) Autopilot clusters.\n[Cloud Customer Care](/kubernetes-engine/docs/getting-support)\n", "content": "## Cluster issues\n### Cannot create a cluster: 0 nodes registered\nThe following issue occurs when you try to create an Autopilot cluster with an IAM service account that's disabled or doesn't have the required permissions. Cluster creation fails with the following error message:\n```\nAll cluster resources were brought up, but: only 0 nodes out of 2 have registered.\n```\nTo resolve the issue, do the following:\n- Check whether the [default Compute Engine service account](/compute/docs/access/service-accounts#default_service_account) or the custom IAM service account that you want to use is disabled:```\ngcloud iam service-accounts describe SERVICE_ACCOUNT\n```Replace `` with service account email address, such as `my-iam-account@my-first-project.iam.gserviceaccount.com` .If the service account is disabled, the output is similar to the following:```\ndisabled: true\ndisplayName: my-service-account\nemail: my-service-account@my-project.iam.gserviceaccount.com\n...\n```\n- If the service account is disabled, enable it:```\ngcloud iam service-accounts enable SERVICE_ACCOUNT\n```\nIf the service account is enabled and the error persists, grant the service account the minimum permissions required for GKE:\n```\ngcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member \"serviceAccount:SERVICE_ACCOUNT\" \\\u00a0 \u00a0 --role roles/container.nodeServiceAccount\n```\n### Namespace stuck in the Terminating state when cluster has 0 nodes\nThe following issue occurs when you delete a namespace in a cluster after the cluster scales down to zero nodes. The `metrics-server` component can't accept the namespace deletion request because the component has zero replicas.\nTo diagnose this issue, run the following command:\n```\nkubectl describe ns/NAMESPACE_NAME\n```\nReplace `` with the name of the namespace.\nThe output is the following:\n```\nDiscovery failed for some groups, 1 failing: unable to retrieve the complete\nlist of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to\nhandle the request\n```\nTo resolve this issue, scale any workload up to trigger GKE to create a new node. When the node is ready, the namespace deletion request automatically completes. After GKE deletes the namespace, scale the workload back down.\n## Scaling issues\n### Node scale up failed: Pod is at risk of not being scheduled\nThe following issue occurs when serial port logging is disabled in your Google Cloud project. GKE Autopilot clusters require serial port logging to effectively debug node issues. If serial port logging is disabled, Autopilot can't provision nodes to run your workloads.\nThe error message in your Kubernetes event log is similar to the following:\n```\nLAST SEEN TYPE  REASON   OBJECT       MESSAGE\n12s   Warning FailedScaleUp pod/pod-test-5b97f7c978-h9lvl Node scale up in zones associated with this pod failed: Internal error. Pod is at risk of not being scheduled\n```\nSerial port logging might be disabled at the organization level through an organization policy that enforces the `compute.disableSerialPortLogging` constraint. Serial port logging could also be disabled at the project or virtual machine (VM) instance level.\nTo resolve this issue, do the following:\n- Ask your Google Cloud organization policy administrator to [remove the compute.disableSerialPortLogging constraint](/resource-manager/docs/organization-policy/creating-managing-policies#boolean_constraints) in the project with your Autopilot cluster.\n- If you don't have an organization policy that enforces this constraint, try to [enable serial port logging in your project metadata](/compute/docs/troubleshooting/viewing-serial-port-output#setting_project_and_instance_metadata) . This action requires the [compute.projects.setCommonInstanceMetadata IAM permission](/compute/docs/reference/rest/v1/projects/setCommonInstanceMetadata#iam-permissions) .\n### Node scale up failed: GCE out of resources\nThe following issue occurs when your workloads request more resources than are available to use in that Compute Engine region or zone. Your Pods might remain in the `Pending` state.\n- Check your Pod events:```\nkubectl get events --for='POD_NAME' --types=Warning\n```Replace `` with the name of the pending Kubernetes resource. For example `pod/example-pod` .The output is similar to the following:```\nLAST SEEN   TYPE   REASON     OBJECT     Message\n19m    Warning   FailedScheduling  pod/example-pod   gke.io/optimize-utilization-scheduler 0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.\n14m    Warning   FailedScheduling  pod/example-pod   gke.io/optimize-utilization-scheduler 0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.\n12m (x2 over 18m) Warning   FailedScaleUp   cluster-autoscaler  Node scale up in zones us-central1-f associated with this pod failed: GCE out of resources. Pod is at risk of not being scheduled.\n34s (x3 over 17m) Warning   FailedScaleUp   cluster-autoscaler  Node scale up in zones us-central1-b associated with this pod failed: GCE out of resources. Pod is at risk of not being scheduled.\n```\nTo resolve this issue, try the following:\n- Deploy the Pod in a different region or zone. If your Pod has a zonal restriction such as a topology selector, remove the restriction if you can. For instructions, see [Place GKE Pods in specific zones](/kubernetes-engine/docs/how-to/gke-zonal-topology) .\n- Create a cluster in a different region and retry the deployment.\n- Try using a different compute class. Compute classes that are backed by smaller Compute Engine machine types are more likely to have available resources. For example, the default machine type for Autopilot has the highest availability. For a list of compute classes and the corresponding machine types, see [When to use specific compute classes](/kubernetes-engine/docs/concepts/autopilot-compute-classes#when-to-use) .\n- If you run GPU workloads, the requested GPU might not be available in your node location. Try deploying your workload in a different location or requesting a different type of GPU.\nTo avoid scale-up issues caused by resource availability in the future, consider the following approaches:\n- Use Kubernetes PriorityClasses to consistently provision extra compute capacity in your cluster. For details, see [Provision extra compute capacity for rapid Pod scaling](/kubernetes-engine/docs/how-to/capacity-provisioning) .\n- Use Compute Engine capacity reservations with the Performance or the Accelerator compute classes. For details, see [Consume reserved zonal resources](/kubernetes-engine/docs/how-to/consuming-reservations) .\n### Nodes fail to scale up: Pod zonal resources exceeded\nThe following issue occurs when Autopilot doesn't provision new nodes for a Pod in a specific zone because a new node would violate resource limits.\nThe error message in your logs is similar to the following:\n```\n \"napFailureReasons\": [   {\n    \"messageId\": \"no.scale.up.nap.pod.zonal.resources.exceeded\",\n    ...\n```\nThis error refers to a [noScaleUp](/kubernetes-engine/docs/how-to/cluster-autoscaler-visibility#noscaleup-event) event, where [node auto-provisioning did not provision any node group for the Pod in the zone](/kubernetes-engine/docs/how-to/cluster-autoscaler-visibility#noscaleup-pod-group-level-nap-reasons) .\nIf you encounter this error, confirm the following:\n- Your Pods have [sufficient memory and CPU](/kubernetes-engine/docs/concepts/autopilot-resource-requests) .\n- The [Pod IP address CIDR range](/kubernetes-engine/docs/how-to/flexible-pod-cidr#cidr_settings_for_clusters) is large enough to support your anticipated maximum cluster size.## Workload issues\n### Pods stuck in Pending state\nA Pod might get stuck in the `Pending` status if you select a specific node for your Pod to use, but the sum of resource requests in the Pod and in DaemonSets that must run on the node exceeds the maximum allocatable capacity of the node. This might cause your Pod to get a `Pending` status and remain unscheduled.\nTo avoid this issue, evaluate the sizes of your deployed workloads to ensure that they're within the supported [maximum resource requests for Autopilot](/kubernetes-engine/docs/concepts/autopilot-resource-requests) .\nYou can also try scheduling your DaemonSets before you schedule your regular workload Pods.\n### Consistently unreliable workload performance on a specific node\nIn GKE version 1.24 and later, if your workloads on a specific node consistently experience disruptions, crashes, or similar unreliable behavior, you can tell GKE about the problematic node by cordoning it using the following command:\n```\nkubectl drain NODE_NAME --ignore-daemonsets\n```\nReplace `` with the name of the problematic node. You can find the node name by running `kubectl get nodes` .\nGKE does the following:\n- Evicts existing workloads from the node and stops scheduling workloads on that node.\n- Automatically recreates any evicted workloads that are managed by a controller, such as a Deployment or a StatefulSet, on other nodes.\n- Terminates any workloads that remain on the node and repairs or recreates the node over time.\n- **If you use Autopilot, GKE shuts down and replaces\nthe node immediately and ignores any configured PodDisruptionBudgets.** \n**Note:** GKE manages all workloads in the `kube-system` namespace. You can safely ignore draining errors in this namespace. GKE might also rate-limit your use of this command.\n### Pods take longer than expected to schedule on empty clusters\nThis event occurs when you deploy a workload to an Autopilot cluster that has no other workloads. Autopilot clusters start with zero usable nodes and scale to zero nodes if the cluster is empty to avoid having unutilized compute resources in the cluster. Deploying a workload in a cluster that has zero nodes triggers a scale-up event.\nIf you experience this, Autopilot is functioning as intended, and no action is necessary. Your workload will deploy as expected after the new nodes boot up.\nCheck whether your Pods are waiting for new nodes:\n- Describe your pending Pod:```\nkubectl describe pod POD_NAME\n```Replace `` with the name of your pending Pod.\n- Check the `Events` section of the output. If the Pod is waiting for new nodes, the output is similar to the following:```\nEvents:\n Type  Reason   Age From         Message\n ----  ------   ---- ----         ------ Warning FailedScheduling 11s gke.io/optimize-utilization-scheduler no nodes available to schedule pods\n Normal TriggeredScaleUp 4s cluster-autoscaler      pod triggered scale-up: [{https://www.googleapis.com/compute/v1/projects/example-project/zones/example-zone/instanceGroups/gk3-example-cluster-pool-2-9293c6db-grp 0->1 (max: 1000)} {https://www.googleapis.com/compute/v1/projects/example-project/zones/example-zone/instanceGroups/gk3-example-cluster-pool-2-d99371e7-grp 0->1 (max: 1000)}]\n```The `TriggeredScaleUp` event shows that your cluster is scaling up from zero nodes to as many nodes are required to run your deployed workload.## What's next\n[Cloud Customer Care](/kubernetes-engine/docs/getting-support)", "guide": "Google Kubernetes Engine (GKE)"}