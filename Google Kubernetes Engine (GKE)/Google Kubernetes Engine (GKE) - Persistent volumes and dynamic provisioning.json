{"title": "Google Kubernetes Engine (GKE) - Persistent volumes and dynamic provisioning", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/persistent-volumes", "abstract": "# Google Kubernetes Engine (GKE) - Persistent volumes and dynamic provisioning\nThis page provides an overview of persistent volumes and claims in Kubernetes, and their use with Google Kubernetes Engine (GKE). This page focuses on storage backed by [Compute Engine persistent disks](/compute/docs/disks) .\n", "content": "## PersistentVolumes\n`PersistentVolume` resources are used to manage durable storage in a cluster. In GKE, a `PersistentVolume` is typically backed by a persistent disk. You can also use other storage solutions like NFS. [Filestore](/filestore/docs) is a NFS solution on Google Cloud. To learn how to set up a Filestore instance as an NFS PV solution for your GKE clusters, see [Access Filestore instances with the Filestore CSI driver](/filestore/docs/csi-driver) in the Filestore documentation.\nAnother storage option is [Cloud Volumes Service](/architecture/partners/netapp-cloud-volumes/overview) . This product is a fully managed, cloud-based data storage service that provides advanced data management capabilities and highly scalable performance. For examples, see [Cloud Volumes Service for Google Cloud](https://netapp-trident.readthedocs.io/en/stable-v20.01/kubernetes/operations/tasks/backends/cvs_gcp.html) .\nThe `PersistentVolume` lifecycle is managed by Kubernetes. A `PersistentVolume` can be dynamically provisioned; you do not have to manually create and delete the backing storage.\n`PersistentVolume` resources are cluster resources that exist independently of [Pods](https://kubernetes.io/docs/concepts/workloads/pods) . This means that the disk and data represented by a `PersistentVolume` continue to exist as the cluster changes and as Pods are deleted and recreated. `PersistentVolume` resources can be provisioned dynamically through `PersistentVolumeClaims` , or they can be explicitly created by a cluster administrator.\n**Caution:** Using the [[fsGroup] setting](https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems) with large `PersistentVolumes` can cause mounts to fail. For more information, see [Troubleshooting Volume mount failures](/kubernetes-engine/docs/troubleshooting/troubleshooting-gke-storage#mounting_a_volume_stops_responding_due_to_the_fsgroup_setting) .\nTo learn more about `PersistentVolume` resources, refer to the Kubernetes [Persistent Volumes documentation](https://kubernetes.io/docs/concepts/storage/persistent-volumes/) and the [Persistent Volumes API reference](https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/) .\n## PersistentVolumeClaims\nA `PersistentVolumeClaim` is a request for and claim to a `PersistentVolume` resource. `PersistentVolumeClaim` objects request a specific size, access mode, and [StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/) for the `PersistentVolume` . If a `PersistentVolume` that satisfies the request exists or can be provisioned, the `PersistentVolumeClaim` is bound to that `PersistentVolume` .\nPods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for the Pod.\nPortability is another advantage of using `PersistentVolumes` and `PersistentVolumeClaims` . You can easily use the same [Pod specification](https://kubernetes.io/docs/concepts/workloads/pods/#pod-templates) across different clusters and environments because `PersistentVolume` is an interface to the actual backing storage.\n## StorageClasses\nVolume implementations such as [Compute Engine persistent disk Container Storage Interface (CSI) Driver](https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver) are configured through [StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/) resources.\nGKE creates a default `StorageClass` for you which uses the balanced persistent disk type (ext4). The default `StorageClass` is used when a `PersistentVolumeClaim` doesn't specify a `StorageClassName` . You can replace the provided default `StorageClass` with your own. For instructions, see [Change the default StorageClass](https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/) .\nYou can create your own `StorageClass` resources to describe different classes of storage. For example, classes might map to quality-of-service levels, or to backup policies. This concept is sometimes called \"profiles\" in other storage systems.\nIf you are using a [cluster with Windows node pools](/kubernetes-engine/docs/how-to/creating-a-cluster-windows) , you must create a `StorageClass` and specify a `StorageClassName` in the `PersistentVolumeClaim` because the default fstype (ext4) is not supported with Windows. If you are using a Compute Engine persistent disk, you must use NTFS as the file storage type.\nWhen defining a `StorageClass` , you must list a provisioner. On GKE, we recommend that you use one of the following provisioners:\n- [Compute Engine PD CSI](/kubernetes-engine/docs/how-to/persistent-volumes/gce-pd-csi-driver) \n- [Kubernetes Filestore CSI](/kubernetes-engine/docs/how-to/persistent-volumes/filestore-csi-driver) \n**Note:** Support for provisioners other than those listed is not covered by [Cloud Customer Care](https://cloud.google.com/support/docs) , except for the open source [SMB CSI Driver for Kubernetes](https://github.com/kubernetes-csi/csi-driver-smb) , which is provided on a best effort basis. Third party support for other provisioners might also be available.\n## Dynamically provision PersistentVolumes\nMost of the time, you don't need to directly configure `PersistentVolume` objects or create Compute Engine persistent disks. Instead, you can create a `PersistentVolumeClaim` and Kubernetes automatically provisions a persistent disk for you.\nThe following manifest describes a request for a disk with 30 gibibytes (GiB) of storage whose access mode allows it to be mounted as read-write by a single node. It also creates a Pod that consumes the `PersistentVolumeClaim` as a volume.\n```\n# pvc-pod-demo.yamlapiVersion: v1kind: PersistentVolumeClaimmetadata:\u00a0 name: pvc-demospec:\u00a0 accessModes:\u00a0 \u00a0 - ReadWriteOnce\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 storage: 30Gi\u00a0 storageClassName: standard-rwo---kind: PodapiVersion: v1metadata:\u00a0 name: pod-demospec:\u00a0 volumes:\u00a0 \u00a0 - name: pvc-demo-vol\u00a0 \u00a0 \u00a0 persistentVolumeClaim:\u00a0 \u00a0 \u00a0 \u00a0claimName: pvc-demo\u00a0 containers:\u00a0 \u00a0 - name: pod-demo\u00a0 \u00a0 \u00a0 image: nginx\u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 10m\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: 80Mi\u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 10m\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: 80Mi\u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 80\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: \"http-server\"\u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/usr/share/nginx/html\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: pvc-demo-vol\n```\nWhen you create this `PersistentVolumeClaim` object with `kubectl apply -f pvc-pod-demo.yaml` , Kubernetes dynamically creates a corresponding `PersistentVolume` object.\nBecause the storage class `standard-rwo` uses volume binding mode [WaitForFirstConsumer](#waitforfirstconsumer) , the `PersistentVolume` will not be created until a Pod is scheduled to consume the volume.\nThe following example shows the `PersistentVolume` created.\n```\napiVersion: v1kind: PersistentVolumemetadata:\u00a0 annotations:\u00a0 \u00a0 pv.kubernetes.io/provisioned-by: pd.csi.storage.gke.io\u00a0 finalizers:\u00a0 - kubernetes.io/pv-protection\u00a0 - external-attacher/pd-csi-storage-gke-io\u00a0 name: pvc-c9a44c07-cffa-4cd8-b92b-15bac9a9b984\u00a0 uid: d52af557-edf5-4f96-8e89-42a3008209e6spec:\u00a0 accessModes:\u00a0 - ReadWriteOnce\u00a0 capacity:\u00a0 \u00a0 storage: 30Gi\u00a0 claimRef:\u00a0 \u00a0 apiVersion: v1\u00a0 \u00a0 kind: PersistentVolumeClaim\u00a0 \u00a0 name: pvc-demo\u00a0 \u00a0 namespace: default\u00a0 \u00a0 uid: c9a44c07-cffa-4cd8-b92b-15bac9a9b984\u00a0 csi:\u00a0 \u00a0 driver: pd.csi.storage.gke.io\u00a0 \u00a0 csi.storage.k8s.io/fstype: ext4\u00a0 \u00a0 volumeAttributes:\u00a0 \u00a0 \u00a0 storage.kubernetes.io/csiProvisionerIdentity: 1660085000920-8081-pd.csi.storage.gke.io\u00a0 \u00a0 volumeHandle: projects/xxx/zones/us-central1-c/disks/pvc-c9a44c07-cffa-4cd8-b92b-15bac9a9b984\u00a0 nodeAffinity:\u00a0 \u00a0 required:\u00a0 \u00a0 \u00a0 nodeSelectorTerms:\u00a0 \u00a0 \u00a0 - matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 - key: topology.gke.io/zone\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - us-central1-c\u00a0 persistentVolumeReclaimPolicy: Delete\u00a0 storageClassName: standard-rwo\u00a0 volumeMode: Filesystemstatus:\u00a0 phase: Bound\n```\nAssuming that you haven't replaced the storage class `standard-rwo` , this `PersistentVolume` is backed by a new, empty Compute Engine persistent disk.\n**Note:** Modifying the `topology.kubernetes.io/zone` label during the lifetime of a PersistentVolume can result in mount failures.\n## Deleting persistent storage\nBy default, deleting a PersistentVolumeClaim for dynamically provisioned volumes like Compute Engine persistent disk will delete both the PersistentVolume object and the actual backing disk. This behavior is controlled by the reclaim policy in the StorageClass and PersistentVolume. For full details, see the [open-source Kubernetes documentation](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#delete) .\n## Access modes\n`PersistentVolume` resources support the following [access modes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) :\n- **ReadWriteOnce:** The volume can be mounted as read-write by a single node.\n- **ReadOnlyMany:** The volume can be mounted read-only by many nodes.\n- **ReadWriteMany:** The volume can be mounted as read-write by many nodes.`PersistentVolume`resources that are backed by Compute Engine persistent disks don't support this access mode.\n### Using Compute Engine persistent disks as ReadOnlyMany\nReadWriteOnce is the most common use case for persistent disks and works as the default access mode for most applications. Compute Engine persistent disks also support ReadOnlyMany mode so that many applications or many replicas of the same application can consume the same disk at the same time. An example use case is serving static content across multiple replicas.\n**Note:** You can't attach persistent disks in write mode on multiple nodes at the same time. See [Deployments Vs. StatefulSets](#deployments_vs_statefulsets) .\nFor instructions, refer to [Use persistent disks with multiple readers](/kubernetes-engine/docs/how-to/persistent-volumes/readonlymany-disks) .\n## Use pre-existing persistent disks as PersistentVolumes\nDynamically provisioned `PersistentVolume` resources are empty when they are created. If you have an existing Compute Engine persistent disk populated with data, you can introduce it to your cluster by manually creating a corresponding `PersistentVolume` resource. The persistent disk must be in the same [zone](/compute/docs/regions-zones) as the cluster nodes.\nRefer to this [example of how to create a Persistent Volume backed by a pre-existingpersistent disk](/kubernetes-engine/docs/how-to/persistent-volumes/preexisting-pd) .\n**Note:** Mounting a persistent disk from a different Google Cloud project to a Pod in your current Google Cloud project is not supported.\n## Deployments vs. StatefulSets\nYou can use a `PersistentVolumeClaim` or `VolumeClaim` templates in higher level controllers such as [Deployments](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) or [StatefulSets](/kubernetes-engine/docs/concepts/statefulset) respectively.\nDeployments are designed for [stateless applications](/kubernetes-engine/docs/how-to/stateless-apps) so all replicas of a Deployment share the same `PersistentVolumeClaim` . Since the replica Pods created are identical to each other, only volumes with the ReadWriteMany mode can work in this setting.\nEven Deployments with one replica using ReadWriteOnce volume are not recommended. This is because the default Deployment strategy creates a second Pod before bringing down the first Pod on a recreate. The Deployment may fail in deadlock as the second Pod can't start because the ReadWriteOnce volume is already in use, and the first Pod won't be removed because the second Pod has not yet started. Instead, use a StatefulSet with ReadWriteOnce volumes.\nStatefulSets are the recommended method of deploying stateful applications that require a unique volume per replica. By using StatefulSets with PersistentVolumeClaim templates, you can have applications that can scale up automatically with unique PersistentVolumesClaims associated to each replica Pod.\n## Regional persistent disks\n[Regional persistent disks](/compute/docs/disks#repds) are multi-zonal resources that replicate data between two zones in the same region, and can be used similarly to zonal persistent disks. In the event of a zonal outage or if cluster nodes in one zone become unschedulable, Kubernetes can failover workloads using the volume to the other zone. You can use regional persistent disks to build highly available solutions for stateful workloads on GKE. You must ensure that both the primary and failover zones are configured with enough resource capacity to run the workload.\nRegional SSD persistent disks are an option for applications such as databases that require both high availability and high performance. For more details, see [Blockstorage performance comparison](/compute/docs/disks/performance#type_comparison) .\nAs with zonal persistent disks, regional persistent disks can be dynamically provisioned as needed or manually provisioned in advance by the cluster administrator. To learn how to add regional persistent disks, see [Provisioning regional persistent disks](/kubernetes-engine/docs/how-to/persistent-volumes/regional-pd) .\n## Zones in persistent disks\n[Zonal persistent disks](/compute/docs/disks#pdspecs) are zonal resources and regional persistent disks are multi-zonal resources. When you [add persistent storage](/kubernetes-engine/docs/how-to/stateful-apps#requesting_persistent_storage_in_a_statefulset) to your cluster, unless a zone is specified, GKE assigns the disk to a single zone. GKE chooses the zone at random. Once a persistent disk is provisioned, any Pods referencing the disk are scheduled to the same zone as the disk.\n## Volume binding mode WaitForFirstConsumer\nIf you dynamically provision a persistent disk in your cluster, we recommend you set the `WaitForFirstConsumer` [volume binding mode](https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode) on your StorageClass. This setting instructs Kubernetes to provision a persistent disk in the same zone that the Pod gets scheduled to. It respects Pod scheduling constraints such as [anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) and node selectors. Anti-affinity on zones allows StatefulSet Pods to be spread across zones along with the corresponding disks.\nFollowing is an example `StorageClass` for provisioning zonal persistent disks that sets `WaitForFirstConsumer` :\n```\napiVersion: storage.k8s.io/v1kind: StorageClassmetadata:\u00a0 name: slowprovisioner: pd.csi.storage.gke.ioparameters:\u00a0 type: pd-balanced\u00a0 csi.storage.k8s.io/fstype: ext4volumeBindingMode: WaitForFirstConsumer\n```\nFor an example using regional persistent disks, see [Provisioning regional persistent disks](/kubernetes-engine/docs/how-to/persistent-volumes/regional-pd) .\n## What's next\n- [Learn about StatefulSets](/kubernetes-engine/docs/concepts/statefulset) , the recommended method of deploying stateful applications.\n- [Learn how to deploy a stateful application using a StatefulSet](/kubernetes-engine/docs/how-to/stateful-apps) .\n- [Learn to use persistent disks in a cluster](/kubernetes-engine/docs/tutorials/persistent-disk) .\n- [Learn how to create a disk that can be read from multiple nodes](/kubernetes-engine/docs/how-to/persistent-volumes/readonlymany-disks) .\n- [Learn how to create persistent disks backed by SSDs](/kubernetes-engine/docs/how-to/persistent-volumes/ssd-pd) .\n- [Learn how to provision regional persistent disks](/kubernetes-engine/docs/how-to/persistent-volumes/regional-pd) .", "guide": "Google Kubernetes Engine (GKE)"}