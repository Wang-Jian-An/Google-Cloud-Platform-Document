{"title": "Google Kubernetes Engine (GKE) - Serve Gemma open models using TPUs on GKE with Saxml", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-tpu-saxml", "abstract": "# Google Kubernetes Engine (GKE) - Serve Gemma open models using TPUs on GKE with Saxml\nThis guide shows you how to serve a Gemma open models large language model (LLM) using Tensor Processing Units (TPUs) on Google Kubernetes Engine (GKE) with [Saxml](https://github.com/google/saxml#saxml-aka-sax) . In this guide, you download the 2B and 7B parameter instruction tuned Gemma models to Cloud Storage and deploy them on a GKE Standard cluster using containers that run Saxml.\nThis guide is a good starting point if you need the scalability, resilience, and cost-effectiveness offered by Kubernetes features when deploying your model on Saxml. If you need a unified managed AI platform to rapidly build and serve ML models cost effectively, we recommend that you try our [Vertex AI](/vertex-ai) deployment solution.", "content": "## BackgroundBy serving Gemma using TPUs on GKE with Saxml, you can implement a robust, production-ready inference serving solution with all the benefits of managed [Kubernetes](https://kubernetes.io/) , including efficient scalability and higher availability. This section describes the key technologies used in this tutorial.\n### GemmaGemma is a set of openly available, lightweight generative AI models released under an open license. These AI models are available to run in your applications, hardware, mobile devices, or hosted services. You can use the Gemma models for text generation, however you can also tune these models for specialized tasks.\nTo learn more, see the [Gemma documentation](https://ai.google.dev/gemma/docs) .\n### TPUsTPUs are Google's custom-developed application-specific integrated circuits (ASICs) used to accelerate data processing frameworks such as TensorFlow, PyTorch, and JAX.\nBefore you use TPUs in GKE, we recommend that you complete the following learning path:- Learn about current TPU version availability with the [Cloud TPU system architecture](/tpu/docs/system-architecture-tpu-vm#versions) .\n- Learn [About TPUs in GKE](/kubernetes-engine/docs/concepts/tpus) .\nThis tutorial serves the Gemma 2B and Gemma 7B models. GKE hosts these models on the following single-host TPU v5e node pools:- **Gemma 2B** : Instruction tuned model hosted in a TPU v5e node pool with`1x1`topology that represents one TPU chip. The machine type for the nodes is`ct5lp-hightpu-1t`.\n- **Gemma 7B** : Instruction tuned model hosted in a TPU v5e node pool with`2x2`topology that represents four TPU chips. The machine type for the nodes is`ct5lp-hightpu-4t`.\n### Saxml [Saxml](https://github.com/google/saxml) is an experimental system that serves [Paxml](https://github.com/google/paxml) , [JAX](https://github.com/google/jax) , and [PyTorch](https://pytorch.org) models for inference. The Saxml system comprises the following components:- Saxml cell or Sax cluster: Comprising an admin server and a group of model servers. The admin server keeps track of model servers, assigns published models to model servers to serve, and helps clients locate model servers serving specific published models.\n- Saxml client: The user-facing programming interface for the Saxml system. The Saxml client includes a command line tool ( [saxutil](https://github.com/google/saxml/tree/main?tab=readme-ov-file#use-sax) ) and a suite of [client libraries](https://github.com/google/saxml/tree/main?tab=readme-ov-file#use-sax) in Python, C++, and Go.\nIn this tutorial, you also use the . The Saxml HTTP Server is a custom HTTP server that encapsulates the Saxml Python client library and exposes REST APIs to interact with the Saxml system, including endpoints to publish, list, unpublish models, and generate predictions.## ObjectivesThis tutorial is intended for Generative AI customers who use JAX, new or existing users of GKE, ML Engineers, MLOps (DevOps) engineers, or platform administrators who are interested in using Kubernetes container orchestration capabilities for serving Gemma.\nThis tutorial covers the following steps:- Prepare a GKE Standard cluster with the recommended TPU topology based on the model characteristics.\n- Deploy Saxml components on GKE.\n- Get and publish the Gemma 2B or Gemma 7B parameter model.\n- Serve and interact with the published models.\n## ArchitectureThis section describes the GKE architecture used in this tutorial. The architecture comprises a GKE Standard cluster that provisions TPUs and hosts Saxml components to deploy and serve Gemma 2B or 7B models. The following diagram shows you the components of this architecture:This architecture includes the following components:- A GKE Standard, zonal cluster.\n- A single-host TPU slice node pool that depends on the Gemma model you want to serve:- Gemma 2B: Configured with a TPU v5e with a`1x1`topology. One instance of the Saxml Model server is configured to use this node pool.\n- Gemma 7B: Configured with a TPU v5e with a`2x2`topology. One instance of the Saxml Model server is configured to use this node pool.\n- A default CPU node pool where the Saxml Admin server and Saxml HTTP server are deployed.\n- Two [Cloud Storage buckets](/storage/docs/buckets) :- One Cloud Storage bucket stores the state managed by an Admin server.\n- One Cloud Storage bucket stores the Gemma model checkpoints.\nThis architecture has the following characteristics:- A public [Artifact Registry](/artifact-registry) manages the containers images for the Saxml components.\n- The GKE cluster uses the [Workload identity federation for GKE](/kubernetes-engine/docs/concepts/workload-identity) . All Saxml components use a workload identity federation that integrates an IAM Service account to access external Services like Cloud Storage buckets.\n- The logs generated by Saxml components are integrated into [Cloud Logging](/kubernetes-engine/docs/how-to/view-logs) .\n- You can use [Cloud Monitoring](/monitoring/docs/monitoring-overview) to analyze the performance metrics of GKE node pools that this tutorial creates.\n## Before you begin- Make sure that you have the following role or roles on the project:      roles/container.admin, roles/iam.serviceAccountAdmin\n- Ensure that you have sufficient quotas for 5 TPU v5e chips. In this tutorial, you use [on-demand instances](/kubernetes-engine/docs/how-to/tpus#ensure-quota) .\n- Create a [Kaggle account](https://www.kaggle.com/) , if you don't already have one.\n## Prepare the environment for Gemma\n### Launch Cloud ShellIn this tutorial, you use [Cloud Shell](/shell) to manage resources hosted on Google Cloud. Cloud Shell is preinstalled with the software you need for this tutorial, including [kubectl](https://kubernetes.io/docs/reference/kubectl/) and [gcloud CLI](/sdk/gcloud) .- In the Google Cloud console, start a Cloud Shell instance:  [Open Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Set the default environment variables:```\ngcloud config set project PROJECT_IDexport PROJECT_ID=$(gcloud config get project)export LOCATION=LOCATIONexport CLUSTER_NAME=saxml-tpu\n```Replace the following values:- : Your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- : The name of the Compute Engine zone where the [TPU v5e machine types are available](/kubernetes-engine/docs/concepts/tpus#availability) .\n### Create a GKE Standard clusterIn this section, you create the GKE cluster and node pool.\nUse Cloud Shell to do the following:- Create a Standard cluster that uses [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) :```\ngcloud container clusters create ${CLUSTER_NAME} \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --machine-type=e2-standard-4 \\\u00a0 \u00a0 --num-nodes=2 \\\u00a0 \u00a0 --release-channel=rapid \\\u00a0 \u00a0 --workload-pool=${PROJECT_ID}.svc.id.goog \\\u00a0 \u00a0 --location=${LOCATION}\n```The cluster creation might take several minutes.\n- Create a TPU v5e node pool with a `1x1` topology and one node:```\ngcloud container node-pools create tpu-v5e-1x1 \\\u00a0 \u00a0 --cluster=${CLUSTER_NAME} \\\u00a0 \u00a0 --machine-type=ct5lp-hightpu-1t \\\u00a0 \u00a0 --num-nodes=1 \\\u00a0 \u00a0 --location=${LOCATION}\n```You serve the Gemma 2B model in this node pool.\nUse Cloud Shell to do the following:- Create a Standard cluster that uses [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) :```\ngcloud container clusters create ${CLUSTER_NAME} \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --machine-type=e2-standard-4 \\\u00a0 \u00a0 --num-nodes=2 \\\u00a0 \u00a0 --release-channel=rapid \\\u00a0 \u00a0 --workload-pool=${PROJECT_ID}.svc.id.goog \\\u00a0 \u00a0 --location=${LOCATION}\n```The cluster creation might take several minutes.\n- Create a TPU v5e node pool with a `2x2` topology and one node:```\ngcloud container node-pools create tpu-v5e-2x2 \\\u00a0 \u00a0 --cluster=${CLUSTER_NAME} \\\u00a0 \u00a0 --machine-type=ct5lp-hightpu-4t \\\u00a0 \u00a0 --num-nodes=1 \\\u00a0 \u00a0 --location=${LOCATION}\n```You serve the Gemma 7B model in this node pool.### Create the Cloud Storage bucketsCreate two Cloud Storage bucket to manages the state of the Saxml Admin server and the model checkpoints.\nIn Cloud Shell, run the following:- Create a Cloud Storage bucket to store Saxml Admin server configurations.```\ngcloud storage buckets create gs://ADMIN_BUCKET_NAME\n```Replace the with the name of the Cloud Storage bucket that stores the Saxml Admin server.\n- Create a Cloud Storage bucket to store model checkpoints:```\ngcloud storage buckets create gs://CHECKPOINTS_BUCKET_NAME\n```Replace the with the name of the Cloud Storage bucket that stores the model checkpoints.\n### Configure your workloads access using workload identity federation for GKEAssign a [Kubernetes ServiceAccount](https://kubernetes.io/docs/concepts/security/service-accounts/) to the application and configure that Kubernetes ServiceAccount to act as an IAM service account.- Configure `kubectl` to communicate with your cluster:```\ngcloud container clusters get-credentials ${CLUSTER_NAME} --location=${LOCATION}\n```\n- Create a Kubernetes ServiceAccount for your application to use:```\ngcloud iam service-accounts create wi-sax\n```\n- Add an [IAM policy binding](/sdk/gcloud/reference/iam/service-accounts/add-iam-policy-binding) for your IAM service account to read and write to Cloud Storage:```\ngcloud projects add-iam-policy-binding ${PROJECT_ID} \\\u00a0 \u00a0 --member \"serviceAccount:wi-sax@${PROJECT_ID}.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role roles/storage.objectUsergcloud projects add-iam-policy-binding ${PROJECT_ID} \\\u00a0 \u00a0 --member \"serviceAccount:wi-sax@${PROJECT_ID}.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role roles/storage.insightsCollectorService\n```\n- Allow the Kubernetes ServiceAccount to [impersonate the IAM service account](/iam/docs/service-account-overview#impersonation) by adding an IAM policy binding between the two service accounts. This binding allows the Kubernetes ServiceAccount to act as the IAM service account:```\ngcloud iam service-accounts add-iam-policy-binding wi-sax@${PROJECT_ID}.iam.gserviceaccount.com \\\u00a0 \u00a0 --role roles/iam.workloadIdentityUser \\\u00a0 \u00a0 --member \"serviceAccount:${PROJECT_ID}.svc.id.goog[default/default]\"\n```\n- [Annotate](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/) the Kubernetes service account with the email address of the IAM service account:```\nkubectl annotate serviceaccount default \\\u00a0 \u00a0 iam.gke.io/gcp-service-account=wi-sax@${PROJECT_ID}.iam.gserviceaccount.com\n```\n## Get access to the modelTo get access to the Gemma models for deployment to GKE, you must sign in to the [Kaggle platform](https://www.kaggle.com/) , sign the license consent agreement, and get a Kaggle API token. In this tutorial, you use a Kubernetes Secret for the Kaggle credentials.\n### Sign the license consent agreementYou must sign the consent agreement to use Gemma. Follow these instructions:- Access the [model consent page](https://www.kaggle.com/models/google/gemma) on Kaggle.com.\n- Login to Kaggle if you haven't done so already.\n- Click **Request Access** .\n- In the **Choose Account for Consent** section, select **Verify via Kaggle\nAccount** to use your Kaggle account for consent.\n- Accept the model **Terms and Conditions** .\n### Generate an access tokenTo access the model through Kaggle, you need a [Kaggle API token](https://github.com/Kaggle/kaggle-api) .\nFollow these steps to generate a new token if you don't have one already:- In your browser, go to [Kaggle settings](https://www.kaggle.com/settings) .\n- Under the **API** section, click **Create New Token** .\nA file named `kaggle.json` is downloaded.\n### Upload the access token to Cloud ShellIn Cloud Shell, you can upload the Kaggle API token to your Google Cloud project:- In Cloud Shell, clickmore_vert **More** > **Upload** .\n- Select File and click **Choose Files** .\n- Open the`kaggle.json`file.\n- Click **Upload** .\n### Create Kubernetes Secret for Kaggle credentialsIn Cloud Shell, do the following:- Configure `kubectl` to communicate with your cluster:```\ngcloud container clusters get-credentials ${CLUSTER_NAME} --location=${LOCATION}\n```\n- Create a Secret to store the Kaggle credentials:```\nkubectl create secret generic kaggle-secret \\\u00a0 \u00a0 --from-file=kaggle.json\n```\n## Deploy SaxmlIn this section, you deploy the Saxml admin server, model servers, and the HTTP server.\n### Deploy the Saxml admin server\n- Create the following `saxml-admin-server.yaml` manifest: [  ai-ml/llm-serving-gemma/saxml/saxml-admin-server.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/saxml/saxml-admin-server.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/saxml/saxml-admin-server.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: sax-admin-serverspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: sax-admin-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: sax-admin-server\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 hostNetwork: false\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: sax-admin-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/cloud-tpu-images/inference/sax-admin-server:v1.2.0\u00a0 \u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 10000\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: GSBUCKET\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: ADMIN_BUCKET_NAME\n```Replace the with the name of the bucket you created in the [Create Cloud Storage buckets](#create-storage-bucket) section. Don't include the `gs://` prefix.\n- Apply the manifest:```\nkubectl apply -f saxml-admin-server.yaml\n```\n- Verify the admin server deployment:```\nkubectl get deployment\n```The output should be similar to the following:```\nNAME        READY UP-TO-DATE AVAILABLE AGE\nsax-admin-server     1/1  1   1   \n##s\n```\n### Deploy Saxml model serverFollow these instructions to deploy the model server for the Gemma 2B or Gemma 7B model.\n- Create the following `saxml-model-server-1x1.yaml` manifest: [  ai-ml/llm-serving-gemma/saxml/saxml-model-server-1x1.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/saxml/saxml-model-server-1x1.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/saxml/saxml-model-server-1x1.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: sax-model-server-v5e-1x1spec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 strategy:\u00a0 \u00a0 type: Recreate\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma-2b-it\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: saxml\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-topology: 1x1\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\u00a0 \u00a0 \u00a0 hostNetwork: false\u00a0 \u00a0 \u00a0 restartPolicy: Always\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/cloud-tpu-images/inference/sax-model-server:v1.2.0\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - \"--jax_platforms=tpu\"\u00a0 \u00a0 \u00a0 \u00a0 - \"--platform_chip=tpuv5e\"\u00a0 \u00a0 \u00a0 \u00a0 - \"--platform_topology=1x1\"\u00a0 \u00a0 \u00a0 \u00a0 - \"--port=10001\"\u00a0 \u00a0 \u00a0 \u00a0 - \"--sax_cell=/sax/test\"\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 10001\u00a0 \u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: SAX_ROOT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"gs://ADMIN_BUCKET_NAME/sax-root\"\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 1\n```Replace the with the name of the bucket you created in the [Create Cloud Storage buckets](#create-storage-bucket) section. Don't include the `gs://` prefix.\n- Apply the manifest:```\nkubectl apply -f saxml-model-server-1x1.yaml\n```\n- Verify the status of the model server Deployment:```\nkubectl get deployment\n```The output should be similar to the following:```\nNAME            READY STATUS RESTARTS AGE\nsax-admin-server         1/1  Running 0   \n##m\nsax-model-server-v5e-1x1       1/1  Running 0   \n##s\n```\n **Success:** You have deployed a model server on a TPU v5e node pool with a 1x1 slice to serve the Gemma 2B model.- Create the following `saxml-model-server-2x2.yaml` manifest: [  ai-ml/llm-serving-gemma/saxml/saxml-model-server-2x2.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/saxml/saxml-model-server-2x2.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/saxml/saxml-model-server-2x2.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: sax-model-server-v5e-2x2spec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 strategy:\u00a0 \u00a0 type: Recreate\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma-7b-it\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: saxml\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-topology: 2x2\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\u00a0 \u00a0 \u00a0 hostNetwork: false\u00a0 \u00a0 \u00a0 restartPolicy: Always\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/cloud-tpu-images/inference/sax-model-server:v1.2.0\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - \"--jax_platforms=tpu\"\u00a0 \u00a0 \u00a0 \u00a0 - \"--platform_chip=tpuv5e\"\u00a0 \u00a0 \u00a0 \u00a0 - \"--platform_topology=2x2\"\u00a0 \u00a0 \u00a0 \u00a0 - \"--port=10001\"\u00a0 \u00a0 \u00a0 \u00a0 - \"--sax_cell=/sax/test\"\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 10001\u00a0 \u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: SAX_ROOT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"gs://ADMIN_BUCKET_NAME/sax-root\"\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4\n```Replace the with the name of the bucket you created in the [Create Cloud Storage buckets](#create-storage-bucket) section. Don't include the `gs://` prefix.\n- Apply the manifest:```\nkubectl apply -f saxml-model-server-2x2.yaml\n```\n- Verify the status of the model server Deployment:```\nkubectl get deployment\n```The output should be similar to the following:```\nNAME            READY STATUS RESTARTS AGE\nsax-admin-server         1/1  Running 0   \n##m\nsax-model-server-v5e-2x2       1/1  Running 0   \n##s\n```\n **Success:** You have deployed a model server on a TPU v5e node pool with a `2x2` slice to serve the Gemma 7B model.\n### Deploy the Saxml HTTP serverIn this section, you deploy the Saxml HTTP server and create a Cluster IP Service you use to access the server.- Create the following `saxml-http.yaml` manifest: [  ai-ml/llm-serving-gemma/saxml/saxml-http.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/saxml/saxml-http.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/saxml/saxml-http.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: sax-httpspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: sax-http\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: sax-http\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 hostNetwork: false\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: sax-http\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/cloud-tpu-images/inference/sax-http:v1.2.0\u00a0 \u00a0 \u00a0 \u00a0 imagePullPolicy: Always\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8888\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: SAX_ROOT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"gs://ADMIN_BUCKET_NAME/sax-root\"---apiVersion: v1kind: Servicemetadata:\u00a0 name: sax-http-svcspec:\u00a0 selector:\u00a0 \u00a0 app: sax-http\u00a0 ports:\u00a0 - protocol: TCP\u00a0 \u00a0 port: 8888\u00a0 \u00a0 targetPort: 8888\u00a0 type: ClusterIP\n```Replace the with the name of the Cloud Storage bucket that stores the Saxml Admin server.\n- Apply the manifest:```\nkubectl apply -f saxml-http.yaml\n```\n- Verify the status of the Saxml HTTP server deployment:```\nkubectl get deployment\n```The output should be similar to the following:\n```\nNAME            READY STATUS RESTARTS AGE\nsax-admin-server         1/1  Running 0   \n##m\nsax-model-server-v5e-1x1       1/1  Running 0   \n##m\nsax-http           1/1  Running 0   \n##s\n```\nThe output should be similar to the following:\n```\nNAME            READY STATUS RESTARTS AGE\nsax-admin-server         1/1  Running 0   \n##m\nsax-model-server-v5e-2x2       1/1  Running 0   \n##m\nsax-http           1/1  Running 0   \n##s\n```\n### Download the model checkpointIn this section, you run a Kubernetes Job that fetches, downloads, and stores the model checkpoint. Follow the steps based on the Gemma model that you want to use:\n- Create the following `job-2b.yaml` manifest: [  ai-ml/llm-serving-gemma/saxml/job-2b.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/saxml/job-2b.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/saxml/job-2b.yaml) ```\napiVersion: v1kind: ConfigMapmetadata:\u00a0 name: fetch-model-scriptsdata:\u00a0 fetch_model.sh: |-\u00a0 \u00a0 #!/usr/bin/bash -x\u00a0 \u00a0 pip install kaggle --break-system-packages && \\\u00a0 \u00a0 MODEL_NAME=$(echo ${MODEL_PATH} | awk -F'/' '{print $2}') && \\\u00a0 \u00a0 VARIATION_NAME=$(echo ${MODEL_PATH} | awk -F'/' '{print $4}') && \\\u00a0 \u00a0 mkdir -p /data/${MODEL_NAME}_${VARIATION_NAME} &&\\\u00a0 \u00a0 kaggle models instances versions download ${MODEL_PATH} --untar -p /data/${MODEL_NAME}_${VARIATION_NAME} && \\\u00a0 \u00a0 echo -e \"\\nCompleted extraction to /data/${MODEL_NAME}_${VARIATION_NAME}\" && \\\u00a0 \u00a0 gcloud storage rsync --recursive --no-clobber /data/${MODEL_NAME}_${VARIATION_NAME} gs://${BUCKET_NAME}/${MODEL_NAME}_${VARIATION_NAME} && \\\u00a0 \u00a0 echo -e \"\\nCompleted copy of data to gs://${BUCKET_NAME}/${MODEL_NAME}_${VARIATION_NAME}\"---apiVersion: batch/v1kind: Jobmetadata:\u00a0 name: data-loader-2b\u00a0 labels:\u00a0 \u00a0 app: data-loader-2bspec:\u00a0 ttlSecondsAfterFinished: 120\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: data-loader-2b\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 restartPolicy: OnFailure\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: gcloud\u00a0 \u00a0 \u00a0 \u00a0 image: gcr.io/google.com/cloudsdktool/google-cloud-cli:slim\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 - /scripts/fetch_model.sh\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: BUCKET_NAME\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: CHECKPOINTS_BUCKET_NAME\u00a0 \u00a0 \u00a0 \u00a0 - name: KAGGLE_CONFIG_DIR\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: /kaggle\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"google/gemma/pax/2b-it/2\"\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/kaggle/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: kaggle-credentials\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/scripts/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: kaggle-credentials\u00a0 \u00a0 \u00a0 \u00a0 secret:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 defaultMode: 0400\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretName: kaggle-secret\u00a0 \u00a0 \u00a0 - name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 configMap:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 defaultMode: 0700\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: fetch-model-scripts\n```Replace the with the name of the bucket you created in the [Create Cloud Storage buckets](#create-storage-bucket) section. Don't include the `gs://` prefix.\n- Apply the manifest:```\nkubectl apply -f job-2b.yaml\n```\n- Wait for the Job to complete:```\nkubectl wait --for=condition=complete --timeout=180s job/data-loader-2b\n```The output is similar to the following:```\njob.batch/data-loader-2b condition met\n```\n- Verify the Job completed successfully:```\nkubectl get job/data-loader-2b\n```The output is similar to the following:```\nNAME    COMPLETIONS DURATION AGE\ndata-loader-2b 1/1   \n##s  #m\n##s\n```\n- View the logs for the Job:```\nkubectl logs --follow job/data-loader-2b\n```\nThe checkpoints is uploaded to `gs://` `` `/gemma_2b-it/checkpoint_00000000` .- Create the following `job-7b.yaml` manifest: [  ai-ml/llm-serving-gemma/saxml/job-7b.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/saxml/job-7b.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/saxml/job-7b.yaml) ```\napiVersion: v1kind: ConfigMapmetadata:\u00a0 name: fetch-model-scriptsdata:\u00a0 fetch_model.sh: |-\u00a0 \u00a0 #!/usr/bin/bash -x\u00a0 \u00a0 pip install kaggle --break-system-packages && \\\u00a0 \u00a0 MODEL_NAME=$(echo ${MODEL_PATH} | awk -F'/' '{print $2}') && \\\u00a0 \u00a0 VARIATION_NAME=$(echo ${MODEL_PATH} | awk -F'/' '{print $4}') && \\\u00a0 \u00a0 mkdir -p /data/${MODEL_NAME}_${VARIATION_NAME} &&\\\u00a0 \u00a0 kaggle models instances versions download ${MODEL_PATH} --untar -p /data/${MODEL_NAME}_${VARIATION_NAME} && \\\u00a0 \u00a0 echo -e \"\\nCompleted extraction to /data/${MODEL_NAME}_${VARIATION_NAME}\" && \\\u00a0 \u00a0 gcloud storage rsync --recursive --no-clobber /data/${MODEL_NAME}_${VARIATION_NAME} gs://${BUCKET_NAME}/${MODEL_NAME}_${VARIATION_NAME} && \\\u00a0 \u00a0 echo -e \"\\nCompleted copy of data to gs://${BUCKET_NAME}/${MODEL_NAME}_${VARIATION_NAME}\"---apiVersion: batch/v1kind: Jobmetadata:\u00a0 name: data-loader-7b\u00a0 labels:\u00a0 \u00a0 app: data-loader-7bspec:\u00a0 ttlSecondsAfterFinished: 120\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: data-loader-7b\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 restartPolicy: OnFailure\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: gcloud\u00a0 \u00a0 \u00a0 \u00a0 image: gcr.io/google.com/cloudsdktool/google-cloud-cli:slim\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 - /scripts/fetch_model.sh\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: BUCKET_NAME\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: CHECKPOINTS_BUCKET_NAME\u00a0 \u00a0 \u00a0 \u00a0 - name: KAGGLE_CONFIG_DIR\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: /kaggle\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"google/gemma/pax/7b-it/2\"\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/kaggle/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: kaggle-credentials\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: \"/scripts/\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: kaggle-credentials\u00a0 \u00a0 \u00a0 \u00a0 secret:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 defaultMode: 0400\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretName: kaggle-secret\u00a0 \u00a0 \u00a0 - name: scripts-volume\u00a0 \u00a0 \u00a0 \u00a0 configMap:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 defaultMode: 0700\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: fetch-model-scripts\n```Replace the with the name of the bucket you created in the [Create Cloud Storage buckets](#create-storage-bucket) section. Do include the `gs://` prefix.\n- Apply the manifest:```\nkubectl apply -f job-7b.yaml\n```\n- Wait for the Job to complete:```\nkubectl wait --for=condition=complete --timeout=360s job/data-loader-7b\n```The output is similar to the following:```\njob.batch/data-loader-7b condition met\n```\n- Verify the Job completed successfully:```\nkubectl get job/data-loader-7b\n```The output is similar to the following:```\nNAME    COMPLETIONS DURATION AGE\ndata-loader-7b 1/1   \n##s  #m\n##s\n```\n- View the logs for the Job:```\nkubectl logs --follow job/data-loader-7b\n```\nThe checkpoints is uploaded to `gs://` `` `/gemma_7b_it/checkpoint_00000000` .## Expose the Saxml HTTP serverYou can access the Saxml HTTP server through a [ClusterIP Service](/kubernetes-engine/docs/concepts/service#services_of_type_clusterip) that you created in the preceding step. The ClusterIP Services are only reachable from within the cluster. Therefore, to access the Service from outside the cluster, complete the following steps:- Establish a port forwarding session:```\nkubectl port-forward service/sax-http-svc 8888:8888\n```\n- Verify that you can access the Saxml HTTP server by opening a new terminal and running the following command:```\ncurl -s localhost:8888\n```The output is similar to the following:```\n{\n \"Message\": \"HTTP Server for SAX Client\"\n}\n```\nThe Saxml HTTP server encapsulates the client interface to the Saxml system and exposes it through a set of REST APIs. You will use these APIs to publish, manage, and interface with Gemma 2B and Gemma 7B models.## Publish the Gemma modelAfter the Saxml components are set up, you can publish the Gemma model to a model server that runs in a TPU slice node pool. You use the Saxml HTTP server's `publish` API to publish a model. To learn more about the Saxml HTTP server's API, see Saxml HTTP APIs. Follow these steps to publish the Gemma 2B or 7B parameter model.\n- Make sure that your port forwarding session is still active:```\ncurl -s localhost:8888\n```\n- Publish the Gemma 2B parameter:```\ncurl --request POST \\--header \"Content-type: application/json\" \\-s \\localhost:8888/publish \\--data \\'{\u00a0 \u00a0 \"model\": \"/sax/test/gemma2bfp16\",\u00a0 \u00a0 \"model_path\": \"saxml.server.pax.lm.params.gemma.Gemma2BFP16\",\u00a0 \u00a0 \"checkpoint\": \"gs://CHECKPOINTS_BUCKET_NAME/gemma_2b-it/checkpoint_00000000\",\u00a0 \u00a0 \"replicas\": \"1\"}'\n```The output is similar to the following:```\n{\n \"model\": \"/sax/test/gemma2bfp16\",\n \"model_path\": \"saxml.server.pax.lm.params.gemma.Gemma2BFP16\",\n \"checkpoint\": \"gs://CHECKPOINTS_BUCKET_NAME/gemma_2b-it/checkpoint_00000000\",\n \"replicas\": 1\n}\n```The deployment of the model can take a few minutes.\n- Monitor the progress by observing logs in a model server Pod of the `sax-model-server-v5e-1x1` deployment.```\nkubectl logs --follow deployment/sax-model-server-v5e-1x1\n```This deployment can take up to five minutes to complete. Wait until you see a message similar to the following:```\nI0125 15:34:31.685555 139063071708736 servable_model.py:699] loading completed.\nI0125 15:34:31.686286 139063071708736 model_service_base.py:532] Successfully loaded model for key: /sax/test/gemma2bfp16\n```\n- Verify that you can access the model by displaying the model information:```\ncurl --request GET \\--header \"Content-type: application/json\" \\-s \\localhost:8888/listcell \\--data \\'{\u00a0 \u00a0 \"model\": \"/sax/test/gemma2bfp16\"}'\n```The output is similar to the following:```\n{\n \"model\": \"/sax/test/gemma2bfp16\",\n \"model_path\": \"saxml.server.pax.lm.params.gemma.Gemma2BFP16\",\n \"checkpoint\": \"gs://CHECKPOINTS_BUCKET_NAME/gemma_2b-it/checkpoint_00000000\",\n \"max_replicas\": 1,\n \"active_replicas\": 1\n}\n```\n **Success:** You have published a Gemma 2B model. You can now interact with the model.- Make sure that your port forwarding session is still active:```\ncurl -s localhost:8888\n```\n- Publish the Gemma 7B parameter:```\ncurl --request POST \\--header \"Content-type: application/json\" \\-s \\localhost:8888/publish \\--data \\'{\u00a0 \u00a0 \"model\": \"/sax/test/gemma7bfp16\",\u00a0 \u00a0 \"model_path\": \"saxml.server.pax.lm.params.gemma.Gemma7BFP16\",\u00a0 \u00a0 \"checkpoint\": \"gs://CHECKPOINTS_BUCKET_NAME/gemma_7b-it/checkpoint_00000000\",\u00a0 \u00a0 \"replicas\": \"1\"}'\n```The output is similar to the following:```\n{\n \"model\": \"/sax/test/gemma7bfp16\",\n \"model_path\": \"saxml.server.pax.lm.params.gemma.Gemma7BFP16\",\n \"checkpoint\": \"gs://CHECKPOINTS_BUCKET_NAME/gemma_7b-it/checkpoint_00000000\",\n \"replicas\": 1\n}\n```The deployment of the model can take a few minutes.\n- Monitor the progress by observing logs in a model server Pod of the `sax-model-server-v5e-2x2` deployment.```\nkubectl logs --follow deployment/sax-model-server-v5e-2x2\n```Wait until you see a message similar to the following:```\nI0125 15:34:31.685555 139063071708736 servable_model.py:699] loading completed.\nI0125 15:34:31.686286 139063071708736 model_service_base.py:532] Successfully loaded model for key: /sax/test/gemma7bfp16\n```\n- Verify that the model was published by displaying the model information:```\ncurl --request GET \\--header \"Content-type: application/json\" \\-s \\localhost:8888/listcell \\--data \\'{\u00a0 \u00a0 \"model\": \"/sax/test/gemma7bfp16\"}'\n```The output is similar to the following:```\n{\n \"model\": \"/sax/test/gemma7bfp16\",\n \"model_path\": \"saxml.server.pax.lm.params.gemma.Gemma7BFP16\",\n \"checkpoint\": \"gs://CHECKPOINTS_BUCKET_NAME/gemma_7b-it/checkpoint_00000000\",\n \"max_replicas\": 1,\n \"active_replicas\": 1\n}\n```\n **Success:** You have published a Gemma 7B model. You can now interact with the model.## Use the modelYou can interact with the Gemma 2B or 7B models. Use the Saxml HTTP server's `generate` API to send a prompt to the model.\nServe a prompt request by using the `generate` endpoint of the Saxml HTTP server:\n```\ncurl --request POST \\--header \"Content-type: application/json\" \\-s \\localhost:8888/generate \\--data \\'{\u00a0 \"model\": \"/sax/test/gemma2bfp16\",\u00a0 \"query\": \"What are the top 5 most popular programming languages?\"}'\n```\nThe following is an example of the model response. The output might vary in every prompt that you serve:\n```\n[ [  \"\\n\\n1. **Python**\\n2. **JavaScript**\\n3. **Java**\\n4. **C++**\\n5. **Go**\",\n  -3.0704939365386963\n ]\n]\n```\nYou can run the command with different `query` parameters. You can also modify extra parameters such `temperature` , `top_k` , `topc_p` by using the `generate` API. To learn more about the Saxml HTTP server's API, see [Saxml HTTP APIs](https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/saxml-on-gke/httpserver#saxml-http-server-apis) .\nServe a prompt request by using the `generate` endpoint of the Saxml HTTP server:\n```\ncurl --request POST \\--header \"Content-type: application/json\" \\-s \\localhost:8888/generate \\--data \\'{\u00a0 \"model\": \"/sax/test/gemma7bfp16\",\u00a0 \"query\": \"What are the top 5 most popular programming languages?\"}'\n```\nThe following is an example of the model response. The output might vary in every prompt that you serve:\n```\n[ [  \"\\n\\n**1. JavaScript**\\n\\n* Most widely used language on the web.\\n* Used for front-end development, such as websites and mobile apps.\\n* Extensive libraries and frameworks available.\\n\\n**2. Python**\\n\\n* Known for its simplicity and readability.\\n* Versatile, used for various tasks, including data science, machine learning, and web development.\\n* Large and active community.\\n\\n**3. Java**\\n\\n* Object-oriented language widely used in enterprise applications.\\n* Used for web applications, mobile apps, and enterprise software.\\n* Strong ecosystem and support.\\n\\n**4. Go**\\n\\n\",\n  -16.806324005126953\n ]\n]\n```\nYou can run the command with different `query` parameters. You can also modify extra parameters such `temperature` , `top_k` , `topc_p` by using the `generate` API. To learn more about the Saxml HTTP server's API, see [Saxml HTTP APIs](https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/saxml-on-gke/httpserver#saxml-http-server-apis) .## Unpublish the modelFollow these steps to unpublish your model:\nTo unpublish the Gemma 2B-it model, run the following command:\n```\ncurl --request POST \\--header \"Content-type: application/json\" \\-s \\localhost:8888/unpublish \\--data \\'{\u00a0 \u00a0 \"model\": \"/sax/test/gemma2bfp16\"}'\n```\nThe output might be similar to the following:\n```\n{\n \"model\": \"/sax/test/gemma2bfp16\"\n}\n```\nYou can run the command with different prompts that are passed in the `query` parameter.\nTo unpublish the Gemma 7B-it model, run the following command:\n```\ncurl --request POST \\--header \"Content-type: application/json\" \\-s \\localhost:8888/unpublish \\--data \\'{\u00a0 \u00a0 \"model\": \"/sax/test/gemma7bfp16\"}'\n```\nThe output might be similar to the following:\n```\n{\n \"model\": \"/sax/test/gemma7bfp16\"\n}\n```\nYou can run the command with different prompts that are passed in the `query` parameter.## Troubleshoot issues\n- If you get the message`Empty reply from server`, it's possible that the container has not finished downloading the model data. [Check the Pod's logs](#publish-model) again for the`Connected`message which indicates that the model is ready to serve.\n- If you see`Connection refused`, [verify that your port forwarding is active](#expose-http-server) .\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the deployed resourcesTo avoid incurring charges to your Google Cloud account for the resources that you created in this guide, run the following command:\n```\ngcloud container clusters delete ${CLUSTER_NAME} --location=${LOCATION}gcloud iam service-accounts delete --quiet wi-sax@${PROJECT_ID}.iam.gserviceaccount.comgcloud storage rm --recursive gs://ADMIN_BUCKET_NAMEgcloud storage rm --recursive gs://CHECKPOINTS_BUCKET_NAME\n```\nReplace the following:- : The name of the Cloud Storage bucket that stores the Saxml Admin server.\n- : The name of the Cloud Storage bucket that stores the model checkpoints.\n## What's next\n- Learn more about [TPUs in GKE](/kubernetes-engine/docs/concepts/tpus) .\n- Explore the Saxml [GitHub repository](https://github.com/google/saxml) .\n- Explore the [Vertex AI Model Garden](https://console.cloud.google.com/vertex-ai/model-garden) .\n- Discover how to run optimized AI/ML workloads with [GKE platform orchestration capabilities](/kubernetes-engine/docs/integrations/ai-infra) .\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Google Kubernetes Engine (GKE)"}