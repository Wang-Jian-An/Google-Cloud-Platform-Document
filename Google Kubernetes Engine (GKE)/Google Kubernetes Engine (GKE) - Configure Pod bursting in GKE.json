{"title": "Google Kubernetes Engine (GKE) - Configure Pod bursting in GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/pod-bursting-gke?hl=zh-cn", "abstract": "# Google Kubernetes Engine (GKE) - Configure Pod bursting in GKE\nThis page shows you how to configure Pods to into available unused capacity on Google Kubernetes Engine (GKE) nodes.\n", "content": "## What is bursting?\ndescribes the action of Pods temporarily using more compute capacity on the node than they originally requested.\nKubernetes lets you request specific capacities of resources like CPU or memory for your Pods. You set these requests in your Pod manifest. The Kubernetes scheduler places your Pods on nodes that have enough capacity to accommodate those resource requests.\nSome workloads don't use 100% of the requested resources for their entire run time. For example, a workload that consumes extra CPU during its boot period might not require the same amount of resources for normal operations. In these situations, you can set the resource for your workload to a higher value than the resource requests or leave the limits unset. GKE allows the workload to temporarily use more resources than you specified in the requests, if that capacity is available.\nFor more information about how this process works in GKE, see [How bursting works](#how-it-works) in this document.\n### Benefits of Pod bursting\nBursting is useful when your Pods only need additional resources for short periods of time to accommodate spikes in resource usage. Example scenarios include the following:\n- You have groups of workloads that are often idle and send a small number of requests per second, but occasionally experience spikes in traffic and would benefit from additional resources to process those requests.\n- Your workloads need more resources during startup than during normal operations.\n- You want to maximize the usage of the compute capacity that you provision.\nBursting lets you request only the resources that your Pod needs for the majority of its runtime, while also ensuring that your Pod can consume more resources if needed. The benefits of bursting include the following:\n- **Lower running costs** : You don't need to request the expected peak resource consumption of the workload. Your requests can be for the lower steady-state values. In Autopilot, you pay for the sum of your Pod resource requests, so your running costs are lower.\n- **More efficient resource usage** : You avoid idle compute capacity because your Pods burst into unused capacity. Your workloads are more likely to use all of your paid-for resources.\n- **Improved performance** : Pods can use extra resources as needed to reduce the time to process incoming requests, or to boot up faster during scale-up events.\n### When not to use bursting\nKubernetes assigns the `Burstable` class to Pods that specify higher resource limits than their requests. `Burstable` QoS Pods are more likely to be evicted when Kubernetes needs to reclaim resources on the node. For more information, see [Burstable QoS class](https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#burstable) in the Kubernetes documentation.\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Ensure that you have a GKE Autopilot cluster running version 1.29.2-gke.1060000 or later, or any version of a GKE Standard cluster. To create a new cluster, see [Create an Autopilot cluster](/kubernetes-engine/docs/how-to/creating-an-autopilot-cluster) .\n### Bursting availability in GKE\nWorkloads can burst in the following situations:\n| Bursting availability | Bursting availability.1                                                                                                                                                     |\n|:------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| GKE Autopilot mode  | Pods that use the Performance compute class or the Accelerator compute class can burst in any GKE version that supports that compute class. In any other compute class and for Pods that don't specify a compute class, bursting is only available if the cluster meets both of the following conditions: You originally created the cluster with GKE version 1.26 or later The cluster is running GKE version 1.29.2-gke.1060000 or later This restriction exists because, in Autopilot clusters, bursting requires cgroup v2. cgroup v2 is only available in clusters that were originally created with version 1.26 and later. |\n| GKE Standard mode  | Pods can burst in any GKE version.                                                                                                                                                  |\nAutopilot clusters that were originally created with a version earlier than 1.26 and were later upgraded to 1.29.2-gke.1060000 and later don't support bursting. To check the original cluster version, run the following command:\n```\ngcloud container clusters describe CLUSTER_NAME \\\u00a0 \u00a0 --location=LOCATION \\\u00a0 \u00a0 --format=\"value(initialClusterVersion)\"\n```\nThe output must be GKE version 1.26 or later.\n### Limitations\n- Autopilot workloads can only use bursting for CPU and memory requests.\n- Autopilot nodes must use version 1.28 or later. If you recently upgraded your control plane to version 1.28 or later, GKE eventually upgrades your nodes to that version. If you experience issues with bursting, ensure that your nodes are running GKE version 1.28 or later.## Connect to the cluster\nRun the following command:\n```\ngcloud container clusters get-credentials CLUSTER_NAME \\\u00a0 \u00a0 --location=LOCATION\n```\nReplace the following:\n- ``: the name of your existing cluster.\n- ``: the location of your cluster.## Deploy a burstable workload\n- Save the following manifest as `burstable-deployment.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: helloweb\u00a0 labels:\u00a0 \u00a0 app: hellospec:\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: hello\u00a0 \u00a0 \u00a0 tier: web\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: hello\u00a0 \u00a0 \u00a0 \u00a0 tier: web\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 pod-type: \"non-critical\"\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 - key: pod-type\u00a0 \u00a0 \u00a0 \u00a0 operator: Equal\u00a0 \u00a0 \u00a0 \u00a0 values: \"non-critical\"\u00a0 \u00a0 \u00a0 \u00a0 effect: NoSchedule\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: hello-app\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/hello-app:1.0\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8080\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 250m\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 350m\n```This manifest has the following fields to enable bursting:- `resources.requests`: The resources that the container requires to function. Set this value to the capacity that your container will need in the steady-state.\n- `resources.limits`: The maximum resource capacity that the container can use. Setting the limits higher than the requests lets Pods burst up to the specified limit if that capacity is available on the node. If you omit this field, the Pods can burst up to the available burstable capacity on the node. This capacity is calculated as follows:- Autopilot mode: Unused capacity in the sum of the resource requests of Pods on the node.\n- Standard mode: Unused capacity in the node resources.\n- `spec.nodeSelector`and`spec.tolerations`: Optional. Tells GKE to create new nodes to run the burstable Pods. GKE applies taints to these new nodes to prevent other Pods, like critical workloads, from running on the same nodes. For details, see [Configure workload separation in GKE](/kubernetes-engine/docs/how-to/workload-separation) .\n- Deploy the workload:```\nkubectl apply -f burstable-deployment.yaml\n```The workload might take a few minutes to start.\n- Check the QoS class of a Pod:```\nkubectl describe pod helloweb | grep -m 1 \"QoS\"\n```The output is the following:```\nQoS Class: Burstable\n```\n**Success:** You created a burstable group of Pods and placed them in separate nodes in your cluster.\n## Burstable capacity in GKE\nTo facilitate Pod bursting, GKE calculates the for each node in a cluster. This calculation for a specific node is as follows:\n- Autopilot clusters: The sum of resource requests of all Pods on that node, regardless of the actual resource capacity of the node. If a Pod is terminated, the burstable capacity reduces by that Pod's requests. The portion of the burstable capacity that isn't in use by running Pods is available to allocate if one of the Pods needs to burst.Autopilot also adds a predefined buffer to the burstable capacity so that any system Pods on the node that burst beyond their requests don't affect your own burstable Pods.\n- Standard clusters: The total resource capacity available on the node VM.## Best practices for bursting\nUse the following practices with Pod bursting:\n- Set your resource requests equal to your limits for any Pods that provide critical functionality in your environment. This ensures that those Pods get the`Guaranteed`Kubernetes Quality of Service (QoS) class.\n- Ensure that you only configure memory bursting on Pods that can handle being evicted when Kubernetes needs to reclaim memory on the node.\n- Always request enough memory for your Pod to boot up. Don't rely on memory bursting to meet your boot requirements.\n- To prevent burstable Pods that consistently burst into multiples of their CPU requests from potentially disrupting critical workloads, use [workload separation](/kubernetes-engine/docs/how-to/workload-separation) to avoid placing those Pods alongside your critical Pods.\n### Optimize burstable capacity in Autopilot nodes\nAutopilot calculates the burstable capacity as the sum of resource requests of all the Pods on a specific node, including system Pods and DaemonSets. You can optimize the burstable capacity on a node in the following ways. However, bursting is opportunistic and isn't guaranteed.\n- To increase the burstable capacity on nodes for specific workloads, use [Pod affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity) to place specific Pods together on the same node.\n- To ensure that a specific burstable capacity is always available on every node, create [DaemonSets](/kubernetes-engine/docs/how-to/deploying-workloads-overview#daemons) to run on all nodes in the cluster.## Example of how bursting works\nThis section uses an example Deployment that has the following burstable Pods to demonstrate how Pod bursting works in GKE Autopilot clusters:\n- Pod 1 requests 250m CPU and has no CPU limit. Pod 1 uses 100m CPU to run.\n- Pod 2 requests 200m CPU and has a 250m CPU limit. Pod 2 uses 100m CPU to run.\nBoth Pods run on the same node. The on the node is 450m CPU (the sum of resource requests). Each Pod only uses 100m CPU to run, which means that the node has a remaining of 250m.\nConsider the following scenarios in which a traffic spike occurs:\n- Pod 1 needs an additional 300m CPU: it can burst and use 250m CPU, which is the available burstable capacity. The node no longer has any available burstable capacity.\n- Pod 2 needs an additional 150m CPU: it can burst and use an extra 150m CPU. The node then has 100m CPU remaining of available burstable capacity.\n- Pod 2 needs an additional 200m CPU: it can burst and use 150m CPU, which brings the total usage to 250m CPU for Pod 2. Pod 2 has a 250m CPU limit and can't burst beyond that limit.\n### How GKE handles Pods that exceed burstable capacity\nIf your burstable Pods try to use more resources than the burstable capacity on the node, GKE takes the following actions:\n- **CPU** : If the CPU usage exceeds the burstable capacity, GKE throttles the CPU usage of some containers so that all containers on the node get the CPU that they request.\n- **Memory** : If the memory usage exceeds the burstable capacity, GKE terminates containers to reclaim memory on the node. GKE starts by terminating resource-intensive containers in Pods with a lower QoS.\nWe recommend that you **always** request enough memory for normal Pod operation. If a container has a dependency on memory bursting to function normally, it might crash repeatedly if that memory isn't available.\n### Use Pod bursting with spare capacity provisioning\nGKE lets you deploy idle Pods to reserve extra compute capacity for faster Pod scaling during future high-traffic events like online store flash sales. Other Pods on the same node can burst into this unused reserved capacity so that the capacity isn't idle in the time leading up to your high-traffic event. You can reserve this capacity by using various Kubernetes mechanisms. For example, you can deploy Pods that have a low PriorityClass. For details, see [Provision extra compute capacity for rapid Pod scaling](/kubernetes-engine/docs/how-to/capacity-provisioning) .\n## Pod bursting in GKE Standard clusters\nGKE Standard clusters also support Pod bursting by setting the limits higher than the requests or by omitting limits. However, in Standard clusters, you must create and configure node pools with appropriate resource capacity to support bursting. Getting the potential cost reduction of burstable Pods in Standard clusters requires more careful node planning and Pod , because you pay for the underlying Compute Engine VMs.\nConsider the following in Standard clusters:\n- The maximum resource consumption limit that triggers Kubernetes eviction or CPU throttling is the allocatable resource capacity on the node. To determine this value, see [Plan GKE Standard node sizes](/kubernetes-engine/docs/concepts/plan-node-sizes#node_allocatable_resources) .\n- Node resource usage in Standard clusters is more likely to reach a Kubernetes eviction threshold because GKE doesn't automatically limit resource consumption if you don't specify limits. Pods that burst into memory are therefore more likely to be terminated by Kubernetes [node-pressure eviction](https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/) .## What's next\n- [Provision extra compute capacity for rapid Pod scaling](/kubernetes-engine/docs/how-to/capacity-provisioning) \n- [Right-size your GKE Standard workloads at scale](/kubernetes-engine/docs/tutorials/right-size-workloads-at-scale)", "guide": "Google Kubernetes Engine (GKE)"}