{"title": "Google Kubernetes Engine (GKE) - Deploy PostgreSQL to GKE using CloudNativePG", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/stateful-workloads/cloudnativepg", "abstract": "# Google Kubernetes Engine (GKE) - Deploy PostgreSQL to GKE using CloudNativePG\nThe guide shows you how to deploy PostgreSQL clusters on Google Kubernetes Engine (GKE) using the CloudNativePG operator and the Stateful High Availability (HA) Operator.\n [CloudNativePG](https://cloudnative-pg.io/) is an open source operator developed by EBD under an Apache 2 license.\n [PostgreSQL](https://www.postgresql.org/) is a powerful, open source object-relational database system with several decades of active development that has earned it a strong reputation for reliability, feature robustness, and performance.\nThe [Stateful High Availability (HA) Operator](/kubernetes-engine/docs/how-to/stateful-ha) is a GKE operator that lets you automate and control the speed of StatefulSet Pod failover.\nThis guide is intended for platform administrators, cloud architects, and operations professionals interested in deploying Postgres clusters on GKE. Running Postgres in GKE instead of using [Cloud SQL](/sql/docs/postgres/quickstarts) can give more flexibility and configuration control to experienced database administrators.\n#", "content": "## BenefitsCloudNativePG offers the following benefits:- A declarative and Kubernetes-native way to manage and configure and PostgreSQL clusters\n- Backup management using [volume snapshots](/kubernetes-engine/docs/how-to/persistent-volumes/volume-snapshots) or [Cloud Storage](https://cloudnative-pg.io/documentation/1.21/appendixes/object_stores/#google-cloud-storage) \n- In-transit encrypted [TLS](https://cloudnative-pg.io/documentation/1.21/certificates/) connection, the ability to use your own certificate authority and integration with Certificate Manager for automated TLS certificate issuance and rotation\n- Rolling updates for minor PostgreSQL releases\n- Use of Kubernetes API server to maintain a PostgreSQL cluster status and failovers for high availability with no additional tools required\n- A built-in Prometheus exporter configuration through user-defined metrics written in SQL\n## Objectives\n- Plan and deploy GKE infrastructure for Postgres\n- Deploy and configure the CloudNativePG Postgres operator with Helm\n- Deploy the Stateful HA Operator\n- Configure Postgres using the operator to ensure availability, security, observability, and performance\n### Deployment architectureIn this tutorial, you use the CloudNativePG Postgres operator to deploy and configure a highly-available Postgres cluster to GKE with a primary PostgreSQL node and two replicas. The CloudNativePG operator resources use a separate namespace of the GKE cluster for better resource isolation and adheres to the recommended microservices approach of one database per PostgreSQL cluster. The database and its corresponding user, or app data, are defined in the Kubernetes custom resource representing the cluster.\nYou also deploy a highly-available regional GKE cluster for Postgres, with multiple Kubernetes nodes spread across several availability zones. This setup helps ensure fault tolerance, scalability, and geographic redundancy. It allows for rolling updates and maintenance while providing SLAs for uptime and availability. For more information, see [Regional clusters](/kubernetes-engine/docs/concepts/regional-clusters) .\nThe following diagram shows a Postgres cluster running on multiple nodes and zones in a GKE cluster:In the diagram, the Postgres `StatefulSet` is deployed across three nodes in three different zones. You control how GKE deploys to nodes by setting the required Pod [affinity and anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) rules on the [postgresql](https://github.com/zalando/postgres-operator/blob/master/manifests/complete-postgres-manifest.yaml) custom resource specification. If one zone fails, using the recommended configuration, GKE reschedules Pods on new nodes. For persisting data, you use SSD disks. If you have a highly-loaded database and need low latency and high IOPS, we [recommended](/compute/docs/disks/performance) using the regional Persistent Disk [StorageClass](/kubernetes-engine/docs/how-to/persistent-volumes/gce-pd-csi-driver#create_a_storageclass) .\nThe Stateful HA operator reduces rescheduling lag, handles failover settings, and shortens recovery time.## CostsIn this document, you use the following billable components of Google Cloud:- [Compute Engine](/compute/disks-image-pricing) \n- [GKE](/kubernetes-engine/pricing) \n- [Google Cloud Managed Service for Prometheus](/stackdriver/docs/managed-prometheus) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin [Cloud Shell](/shell) is preinstalled with the software you need for this tutorial, including [kubectl](https://kubernetes.io/docs/reference/kubectl/) , the [gcloud CLI](/sdk/gcloud) , [Helm](https://helm.sh/) , and [Terraform](/docs/terraform) . If you don't use Cloud Shell, you must install the gcloud CLI.- Grant roles to your Google Account. Run the following command once for each of the following   IAM roles: `roles/compute.securityAdmin, roles/compute.viewer, roles/container.clusterAdmin, roles/container.admin, roles/iam.serviceAccountAdmin, roles/iam.serviceAccountUser` ```\ngcloud projects add-iam-policy-binding PROJECT_ID --member=\"user:EMAIL_ADDRESS\" --role=ROLE\n```- Replace``with your project ID.\n- Replace``with your email address.\n- Replace``with each individual role.\n### Set up your environmentTo set up your environment, follow these steps:- Set environment variables:```\nexport PROJECT_ID=PROJECT_ID\nexport KUBERNETES_CLUSTER_PREFIX=postgres\nexport REGION=us-central1\n```Replace `` with your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- Clone the GitHub repository:```\ngit clone https://github.com/GoogleCloudPlatform/kubernetes-engine-samples\n```\n- Change to the working directory:```\ncd kubernetes-engine-samples/databases/postgresql-cloudnativepg\n```\n## Create your cluster infrastructureIn this section, you run a Terraform script to create a private, highly-available, regional GKE cluster.\nYou can install the operator using a [Standard or Autopilot](/kubernetes-engine/docs/concepts/choose-cluster-mode) cluster.\nThe following diagram shows a private regional Standard GKE cluster deployed across three different zones:To deploy this infrastructure, run the following commands:\n```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=terraform/gke-standard initterraform -chdir=terraform/gke-standard apply \\\u00a0 -var project_id=${PROJECT_ID} \u00a0 \\\u00a0 -var region=${REGION} \u00a0\\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```\nWhen prompted, type `yes` . It might take several minutes for this command to complete and for the cluster to show a ready status.\nTerraform creates the following resources:- A VPC network and private subnet for the Kubernetes nodes\n- A router to access the internet through NAT\n- A private GKE cluster in the`us-central1`region\n- A node pools with auto scaling enabled (one to two nodes per zone, one node per zone minimum)\nThe output is similar to the following:\n```\n...\nApply complete! Resources: 14 added, 0 changed, 0 destroyed.\n...\n```\nThe following diagram shows a private regional Autopilot GKE cluster:To deploy the infrastructure, run the following commands:\n```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform -chdir=terraform/gke-autopilot initterraform -chdir=terraform/gke-autopilot apply \\\u00a0 -var project_id=${PROJECT_ID} \\\u00a0 -var region=${REGION} \\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```\nWhen prompted, type `yes` . It might take several minutes for this command to complete and for the cluster to show a ready status.\nTerraform creates the following resources:- A VPC network and private subnet for the Kubernetes nodes\n- A router to access the internet through NAT\n- A private GKE cluster in the`us-central1`region\n- A`ServiceAccount`with logging and monitoring permission\n- Google Cloud Managed Service for Prometheus for cluster monitoring\nThe output is similar to the following:\n```\n...\nApply complete! Resources: 12 added, 0 changed, 0 destroyed.\n...\n```\n### Connect to the clusterConfigure `kubectl` to communicate with the cluster:\n```\ngcloud container clusters get-credentials ${KUBERNETES_CLUSTER_PREFIX}-cluster --region ${REGION}\n```## Deploy the CloudNativePG operatorDeploy the CloudNativePG to your Kubernetes cluster using a Helm chart:- Add the CloudNativePG operator Helm Chart repository:```\nhelm repo add cnpg https://cloudnative-pg.github.io/charts\n```\n- Deploy the CloudNativePG operator using the Helm command-line tool:```\nhelm upgrade --install cnpg \\\u00a0 \u00a0 --namespace cnpg-system \\\u00a0 \u00a0 --create-namespace \\\u00a0 \u00a0 --set image.tag=\"1.22.1-ubi9\" \\\u00a0 \u00a0 cnpg/cloudnative-pg\n```The output is similar to the following:```\nRelease \"cnpg\" does not exist. Installing it now.\nNAME: cnpg\nLAST DEPLOYED: Fri Oct 13 13:52:36 2023\nNAMESPACE: cnpg-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n...\n```\n## Deploy the Stateful HA OperatorDeploy the Stateful HA Operator to your Kubernetes cluster:\n```\nVERSION=0.1.5REPO=\"gke-release\"mkdir ha-operatorgsutil cp gs://$REPO/ha-controller/$VERSION/ha-controller-helm.tar.gz ./ha-operatortar xvf ./ha-operator/ha-controller-helm.tar.gz --directory=ha-operator/helm upgrade -i ha-operator ./ha-operator/helm-chart \\\u00a0 --namespace ha-operator \\\u00a0 --create-namespace \\\u00a0 --set image.repository=\"$REPO\" \\\u00a0 --set image.tag=\"$VERSION\" \\\u00a0 --set useWorkloadSeparation=false\n```\nThe output is similar to the following:\n```\nRelease \"ha-operator\" does not exist. Installing it now.\nNAME: ha-operator\nLAST DEPLOYED: Thu Nov 30 15:34:18 2023\nNAMESPACE: ha-operator\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n```## Add the regional Persistent Disk storage classThe following manifest describes a Postgres cluster:\nApply the manifest to your cluster:\n```\nkubectl apply -n pg-ns -f manifests/01-basic-cluster/regional-pd.yaml\n```## Deploy PostgresThe following manifest describes a Postgres cluster:\n [  databases/postgresql-cloudnativepg/manifests/01-basic-cluster/postgreSQL_cluster.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/postgresql-cloudnativepg/manifests/01-basic-cluster/postgreSQL_cluster.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/databases/postgresql-cloudnativepg/manifests/01-basic-cluster/postgreSQL_cluster.yaml) \n```\napiVersion: postgresql.cnpg.io/v1kind: Clustermetadata:\u00a0 name: gke-pg-clusterspec:\u00a0 description: \"Standard GKE PostgreSQL cluster\"\u00a0 imageName: ghcr.io/cloudnative-pg/postgresql:16.2\u00a0 enableSuperuserAccess: true\u00a0 instances: 3\u00a0 startDelay: 300\u00a0 primaryUpdateStrategy: unsupervised\u00a0 postgresql:\u00a0 \u00a0 pg_hba:\u00a0 \u00a0 \u00a0 - host all all 10.48.0.0/20 md5\u00a0 bootstrap:\u00a0 \u00a0 initdb:\u00a0 \u00a0 \u00a0 database: app\u00a0 storage:\u00a0 \u00a0 storageClass: ha-regional\u00a0 \u00a0 size: 10Gi\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 memory: \"1Gi\"\u00a0 \u00a0 \u00a0 cpu: \"1000m\"\u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 memory: \"1Gi\"\u00a0 \u00a0 \u00a0 cpu: \"1000m\"\u00a0 affinity:\u00a0 \u00a0 enablePodAntiAffinity: true\u00a0 \u00a0 tolerations:\u00a0 \u00a0 - key: cnpg.io/cluster\u00a0 \u00a0 \u00a0 effect: NoSchedule\u00a0 \u00a0 \u00a0 value: gke-pg-cluster\u00a0 \u00a0 \u00a0 operator: Equal\u00a0 \u00a0 additionalPodAffinity:\u00a0 \u00a0 \u00a0 preferredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 - weight: 1\u00a0 \u00a0 \u00a0 \u00a0 podAffinityTerm:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 labelSelector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - key: app.component\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"pg-cluster\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 topologyKey: topology.kubernetes.io/zone\u00a0 monitoring:\u00a0 \u00a0 enablePodMonitor: true\n```\nThis manifest has the following fields:- `spec.instances`: the number of cluster Pods\n- `spec.primaryUpdateStrategy`: the rolling update strategy:- `Unsupervised`: autonomously updates the primary cluster node after the replica nodes\n- `Supervised`: manual switchover is required for the primary cluster node\n- `spec.postgresql`: file parameter overrides, such as [pg-hba](https://cloudnative-pg.io/documentation/current/postgresql_conf/) rules, LDAP, and requirements for sync replicas to be met.\n- `spec.storage`: storage-related settings, such as storage class, volume size, and [write-ahead log](https://cloudnative-pg.io/documentation/current/storage/) settings.\n- `spec.bootstrap`: parameters of the initial database created in the cluster, user credentials, and database restore options\n- `spec.resources`: requests and limits for cluster Pods\n- `spec.affinity`: affinity and anti-affinity rules of the cluster workloads\n### Create a basic Postgres cluster\n- Create a namespace:```\nkubectl create ns pg-ns\n```\n- Create a new Postgres cluster:```\nkubectl apply -n pg-ns -f manifests/01-basic-cluster/postgreSQL_cluster.yaml\n```This command might take several minutes to complete.\n- Check the status of the cluster:```\nkubectl get cluster -n pg-ns\n```The output is similar to the following:```\nNAME    AGE  INSTANCES READY STATUS      PRIMARY\ngke-pg-cluster 2m53s 3   3  Cluster in healthy state gke-pg-cluster-1\n```\n- Deploy a HighAvailabilityApplication to protect the application:```\nkubectl apply -n pg-ns -f manifests/01-basic-cluster/ha-app.yaml\n```\n- Verify that rules are being applied by describing the HighAvailabilityApplication:```\nkubectl describe highavailabilityapplication pg-ha-app -n pg-ns\n```The output is similar to the following:```\nStatus:\n Conditions:\n Last Transition Time: 2024-02-29T09:51:12Z\n Message:    Application is protected\n Observed Generation: 1\n Reason:    ApplicationProtected\n Status:    True\n Type:     Protected\nEvents:     <none>\n```In this output, the `Reason` field indicates that the application is protected.\n### Inspect the resourcesConfirm that GKE created the resources for the cluster:\n```\nkubectl get cluster,pod,svc,pvc,pdb,secret,cm -n pg-ns\n```\nThe output is similar to the following:\n```\nNAME          AGE INSTANCES READY STATUS      PRIMARY\ncluster.postgresql.cnpg.io/gke-pg-cluster 32m 3   3  Cluster in healthy state gke-pg-cluster-1\nNAME     READY STATUS RESTARTS AGE\npod/gke-pg-cluster-1 1/1  Running 0   31m\npod/gke-pg-cluster-2 1/1  Running 0   30m\npod/gke-pg-cluster-3 1/1  Running 0   29m\nNAME      TYPE  CLUSTER-IP EXTERNAL-IP PORT(S) AGE\nservice/gke-pg-cluster-r ClusterIP 10.52.11.24 <none>  5432/TCP 32m\nservice/gke-pg-cluster-ro ClusterIP 10.52.9.233 <none>  5432/TCP 32m\nservice/gke-pg-cluster-rw ClusterIP 10.52.1.135 <none>  5432/TCP 32m\nNAME          STATUS VOLUME          CAPACITY ACCESS MODES STORAGECLASS AGE\npersistentvolumeclaim/gke-pg-cluster-1 Bound pvc-bbdd1cdd-bdd9-4e7c-8f8c-1a14a87e5329 2Gi  RWO   standard  32m\npersistentvolumeclaim/gke-pg-cluster-2 Bound pvc-e7a8b4df-6a3e-43ce-beb0-b54ec1d24011 2Gi  RWO   standard  31m\npersistentvolumeclaim/gke-pg-cluster-3 Bound pvc-dac7f931-6ac5-425f-ac61-0cfc55aae72f 2Gi  RWO   standard  30m\nNAME            MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE\npoddisruptionbudget.policy/gke-pg-cluster   1    N/A    1      32m\npoddisruptionbudget.policy/gke-pg-cluster-primary 1    N/A    0      32m\nNAME        TYPE      DATA AGE\nsecret/gke-pg-cluster-app   kubernetes.io/basic-auth 3  32m\nsecret/gke-pg-cluster-ca   Opaque      2  32m\nsecret/gke-pg-cluster-replication kubernetes.io/tls   2  32m\nsecret/gke-pg-cluster-server  kubernetes.io/tls   2  32m\nsecret/gke-pg-cluster-superuser  kubernetes.io/basic-auth 3  32m\nNAME        DATA AGE\nconfigmap/cnpg-default-monitoring 1  32m\nconfigmap/kube-root-ca.crt   1  135m\n```\nThe operator creates the following resources:- A cluster custom resource representing the PostgreSQL cluster which is controlled by the operator\n- PersistentVolumeClaim resources with corresponding Persistent Volumes\n- Secrets with user credentials for accessing the database and replication between Postgres nodes.\n- Three database endpoint services:`<name>-rw`,`<name>-ro`, and`<name>-r`to connect to the cluster. For more information, see [PostgreSQL architecture](https://cloudnative-pg.io/documentation/1.21/architecture/) .\n## Authenticate to PostgresYou can connect to the PostgreSQL database and check access through different service endpoints created by the operator. To do this, you use an additional Pod with a PostgreSQL client and synchronized application user credentials mounted as environment variables.- Run the client Pod to interact with your Postgres cluster:```\nkubectl apply -n pg-ns -f manifests/02-auth/pg-client.yaml\n```\n- Run an `exec` command on the `pg-client` Pod and login to the `gke-pg-cluster-rw` Service:```\nkubectl exec -n pg-ns -i -t pg-client -- /bin/sh\n```\n- Login to the database using the `gke-pg-cluster-rw` Service to establish a connection with Read-Write privileges:```\npsql postgresql://$CLIENTUSERNAME:$CLIENTPASSWORD@gke-pg-cluster-rw.pg-ns/app\n```The terminal starts with your database name:```\napp=>\n```\n- Create a table:```\nCREATE TABLE travel_agency_clients (client VARCHAR ( 50 ) UNIQUE NOT NULL,address VARCHAR ( 50 ) UNIQUE NOT NULL,phone VARCHAR ( 50 ) UNIQUE NOT NULL);\n```\n- Insert data into the table:```\nINSERT INTO travel_agency_clients(client, address, phone)VALUES ('Tom', 'Warsaw', '+55555')RETURNING *;\n```\n- View the data that you created:```\nSELECT * FROM travel_agency_clients ;\n```The output is similar to the following:```\nclient | address | phone\n--------+---------+--------Tom | Warsaw | +55555\n(1 row)\n```\n- Logout of the current database session:```\nexit\n```\n- Login to the database using the `gke-pg-cluster-ro` Service to verify read-only access. This Service permits querying data but restricts any write operations:```\npsql postgresql://$CLIENTUSERNAME:$CLIENTPASSWORD@gke-pg-cluster-ro.pg-ns/app\n```\n- Attempt to insert new data:```\nINSERT INTO travel_agency_clients(client, address, phone)VALUES ('John', 'Paris', '+55555')RETURNING *;\n```The output is similar to the following:```\nERROR: cannot execute INSERT in a read-only transaction\n```\n- Attempt to read data:```\nSELECT * FROM travel_agency_clients ;\n```The output is similar to the following:```\nclient | address | phone\n--------+---------+--------Tom | Warsaw | +55555\n(1 row)\n```\n- Logout of the current database session:```\nexit\n```\n- Exit the Pod shell:```\nexit\n```\n## Understand how Prometheus collects metrics for your Postgres clusterThe following diagram shows how Prometheus metrics collecting works:In the diagram, a GKE private cluster contains:- A Postgres Pod that gathers metrics on path`/`and port`9187`\n- Prometheus-based collectors that process the metrics from the Postgres Pod\n- A`PodMonitoring`resource that sends metrics to Cloud Monitoring\nTo enable metrics to be collected from your Pods, perform the following steps:- Create the [PodMonitoring](/stackdriver/docs/managed-prometheus/setup-managed#gmp-pod-monitoring) resource:```\nkubectl apply -f manifests/03-observability/pod-monitoring.yaml -n pg-ns\n```\n- In the Google Cloud console, go to the **Metrics explorer** page: [Go to Metrics explorer](https://console.cloud.google.com/monitoring/metrics-explorer) The dashboard shows a non-zero metrics ingestion rate.\n- In **Select a metric** , enter **Prometheus Target** .\n- In the **Active Metric Categories** section, select **Cnpg** .\n### Create a metrics dashboardTo visualize the exported metrics, create a metrics dashboard.- Deploy a dashboard:```\ngcloud --project \"${PROJECT_ID}\" monitoring dashboards create --config-from-file manifests/03-observability/gcp-pg.json\n```\n- In the Google Cloud console, go to the **Dashboards** page. [Go to Dashboards](https://console.cloud.google.com/monitoring/dashboards) \n- Select the **PostgresQL Prometheus Overview** dashboard.To review how dashboards monitor functions you can reuse actions from the **Database authentication** section, and apply read and write requests on the database, then review gathered metrics visualization in a dashboard.\n- Connect to the client Pod:```\nkubectl exec -n pg-ns -i -t pg-client -- /bin/sh\n```\n- Insert random data:```\npsql postgresql://$CLIENTUSERNAME:$CLIENTPASSWORD@gke-pg-cluster-rw.pg-ns/app -c \"CREATE TABLE test (id serial PRIMARY KEY, randomdata VARCHAR ( 50 ) NOT NULL);INSERT INTO test (randomdata) VALUES (generate_series(1, 1000));\"\n```\n- Refresh the dashboard. The graphs update with actualized metrics.\n- Exit the Pod shell:```\nexit\n```\n## Clean up\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- Delete a Google Cloud project:\n- ```\ngcloud projects delete PROJECT_ID\n```\n### Delete individual resources\n- Set environment variables.```\nexport PROJECT_ID=${PROJECT_ID}export KUBERNETES_CLUSTER_PREFIX=postgresexport REGION=us-central1\n```\n- Run the `terraform destroy` command:```\nexport GOOGLE_OAUTH_ACCESS_TOKEN=$(gcloud auth print-access-token)terraform \u00a0-chdir=terraform/FOLDER destroy \\\u00a0 -var project_id=${PROJECT_ID} \\\u00a0 -var region=${REGION} \\\u00a0 -var cluster_prefix=${KUBERNETES_CLUSTER_PREFIX}\n```Replace `` with either `gke-autopilot` or `gke-standard` .When prompted, type `yes` .\n- Find all unattached disks:```\nexport disk_list=$(gcloud compute disks list --filter=\"-users:* AND labels.name=${KUBERNETES_CLUSTER_PREFIX}-cluster\" --format \"value[separator=|](name,zone)\")\n```\n- Delete the disks:```\nfor i in $disk_list; do\u00a0 disk_name=$(echo $i| cut -d'|' -f1)\u00a0 disk_zone=$(echo $i| cut -d'|' -f2|sed 's|.*/||')\u00a0 echo \"Deleting $disk_name\"\u00a0 gcloud compute disks delete $disk_name --zone $disk_zone --quietdone\n```\n## What's next\n- Explore reference architectures, diagrams, and best practices about Google Cloud. Take a look at our [Cloud Architecture Center](/architecture) .", "guide": "Google Kubernetes Engine (GKE)"}