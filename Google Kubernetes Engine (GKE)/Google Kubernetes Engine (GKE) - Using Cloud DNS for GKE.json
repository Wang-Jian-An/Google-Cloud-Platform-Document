{"title": "Google Kubernetes Engine (GKE) - Using Cloud DNS for GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/cloud-dns", "abstract": "# Google Kubernetes Engine (GKE) - Using Cloud DNS for GKE\nThis page explains how to use Cloud DNS as a Kubernetes DNS provider for Google Kubernetes Engine (GKE).\nUsing Cloud DNS as a DNS provider does not enable clients outside of a cluster to resolve and reach Kubernetes Services directly. You still need to expose Services externally using a Load Balancer and register their cluster external IP addresses on your DNS infrastructure.\nFor more information about using kube-dns as a DNS provider, see [Service discovery and DNS](/kubernetes-engine/docs/concepts/service-discovery) . To learn how to use a custom version of kube-dns or a custom DNS provider, see [Setting up a custom kube-dns Deployment](/kubernetes-engine/docs/how-to/custom-kube-dns) .\n", "content": "## How Cloud DNS for GKE works\n[Cloud DNS](/dns/docs/overview) can be used as the DNS provider for GKE, providing Pod and Service DNS resolution with managed DNS that does not require a cluster-hosted DNS provider. DNS records for Pods and Services are automatically provisioned in Cloud DNS for cluster IP, headless and external name Services.\nCloud DNS supports the full [Kubernetes DNS specification](https://github.com/kubernetes/dns/blob/master/docs/specification.md) and provides resolution for A, AAAA, SRV and PTR records for Services within a GKE cluster. PTR records are implemented using [response policy rules](/dns/docs/zones/manage-response-policies#managing_response_policy_rules) .\nUsing Cloud DNS as the DNS provider for GKE offers many benefits over cluster-hosted DNS:\n- Removes overhead of managing the cluster-hosted DNS server. Cloud DNS requires no scaling, monitoring, or managing of DNS instances because it is a fully managed service hosted in the highly scalable Google infrastructure.\n- Local resolution of DNS queries on each GKE node. Similar to [NodeLocal DNSCache](/kubernetes-engine/docs/how-to/nodelocal-dns-cache) , Cloud DNS caches DNS responses locally, providing low latency and high scalability DNS resolution.\n- Integration with [Google Cloud Observability](/stackdriver/docs) for DNS monitoring and logging. For more information, see [Enabling and disabling logging for private managed zones](/dns/docs/monitoring#enabling_and_disabling_logging_for_private_managed_zones) .\n### Architecture\nWhen Cloud DNS is the DNS provider for GKE, a controller runs as a GKE-managed Pod. This Pod runs on the control plane nodes of your cluster and syncs the cluster DNS records into a managed private DNS zone.\nThe following diagram shows how the Cloud DNS control plane and data plane resolve cluster names:\nIn the diagram, the Service `backend` selects the running `backend` Pods. The `clouddns-controller` creates a DNS record for Service `backend` .\nThe Pod `frontend` sends a DNS request to resolve the IP address of the Service named `backend` to the [Compute Engine local metadata server](/compute/docs/metadata/overview) at `169.254.169.254` . The metadata server runs locally on the node, sending cache misses to Cloud DNS.\nThe Cloud DNS data plane runs locally within each GKE node or Compute Engine virtual machine (VM) instance. Depending on the type of Kubernetes Service, Cloud DNS resolves the Service name to its virtual IP address, for Cluster IP Services, or the list of endpoint IP addresses, for Headless Services.\nAfter the Pod `frontend` resolves the IP address, the Pod can send traffic to the Service `backend` and any Pods behind the Service.\n**Note:** kube-dns continues to run on new and existing GKE clusters that have Cloud DNS enabled on versions 1.19 and later. You can disable kube-dns by [scaling the kube-dns Deployment and autoscaler to zero](/kubernetes-engine/docs/how-to/custom-kube-dns#scaling_down_kube-dns) .\n### DNS scopes\nCloud DNS has two kinds of [DNS scopes](/dns/docs/scopes) , GKE cluster scope and Virtual Private Cloud (VPC) scope. A cluster cannot operate in both modes simultaneously.\n- [GKE cluster scope](#cluster_scope_dns) : DNS records are only resolvable within the cluster, which is the same behavior as kube-dns. Only nodes running in the GKE cluster can resolve Service names. By default, clusters have DNS names that end in`*.cluster.local`. These DNS names are only visible within the cluster and do not overlap or conflict with`*.cluster.local`DNS names for other GKE clusters in the same project. **This is the default mode** .\n- [VPC scope](#vpc_scope_dns) : DNS records are resolvable within the entire VPC. Compute Engine VMs and on-premise clients can connect using Cloud Interconnect or Cloud VPN and directly resolve GKE Service names. You must set a [unique custom domain](#enable_scope_dns_in_an_existing_cluster) for each cluster, which means that all Service and Pod DNS records are unique within the VPC. This mode reduces communication friction between GKE and non-GKE resources.\n### Cloud DNS resources\nWhen you use Cloud DNS as your DNS provider for your GKE cluster, the Cloud DNS controller creates resources in Cloud DNS for your project. The resources that GKE creates depends on the Cloud DNS scope.\n| Scope   | Forward lookup zone             | Reverse lookup zone               |\n|:--------------|:--------------------------------------------------------------------|:----------------------------------------------------------------------------|\n| Cluster scope | 1 private zone per cluster per Compute Engine zone (in the region) | 1 response policy zone per cluster per Compute Engine zone (in the region) |\n| VPC scope  | 1 private zone per cluster (global zone)       | 1 response policy zone per cluster (global zone)       |\nThe naming convention used for these Cloud DNS resources is the following:\n| Scope   | Forward lookup zone    | Reverse lookup zone    |\n|:--------------|:----------------------------------|:---------------------------------|\n| Cluster scope | gke-CLUSTER_NAME-CLUSTER_HASH-dns | gke-CLUSTER_NAME-CLUSTER_HASH-rp |\n| VPC scope  | gke-CLUSTER_NAME-CLUSTER_HASH-dns | gke-NETWORK_NAME_HASH-rp   |\nIn addition to the zones mentioned in the previous table, the Cloud DNS controller creates the following zones in your project, depending on your configuration:\n| Custom DNS configuration  | Zone type    | Zone naming convention       |\n|:------------------------------|:-------------------------|:-----------------------------------------------|\n| Stub domain     | Forwarding (global zone) | gke-CLUSTER_NAME-CLUSTER_HASH-DOMAIN_NAME_HASH |\n| Custom upstream nameserver(s) | Forwarding (global zone) | gke-CLUSTER_NAME-CLUSTER_HASH-upstream   |\nTo learn more about how to create custom stub domains or custom upstream nameservers, see [Adding custom resolvers for stub domains](/kubernetes-engine/docs/how-to/kube-dns#adding_custom_resolvers_for_stub_domains) .\nTo serve internal DNS traffic, the Cloud DNS controller creates a [managed DNS zone](/dns/docs/zones/zones-overview) in each Compute Engine zone of the region the cluster belongs to. For example, if you deploy a cluster in the `us-central1-c` zone, the Cloud DNS controller creates a managed zone in `us-central1-a` , `us-central1-b` , `us-central1-c` , and `us-central1-f` .\nFor each DNS `stubDomain` , the Cloud DNS controller creates one forwarding zone.\nThe Cloud DNS processes each DNS upstream using one managed zone with the `.` DNS name.\n## Pricing\nWhen Cloud DNS is the DNS provider for GKE Standard clusters, DNS queries from Pods inside the GKE cluster are billed according to [Cloud DNS pricing](/dns/pricing) .\nQueries to a VPC scoped DNS zone managed by GKE are billed using the standard Cloud DNS pricing.\n## Requirements\nCloud DNS for GKE has the following requirements for cluster scope:\n- For Standard, GKE version 1.24.7-gke.800, 1.25.3-gke.700 or later.\n- For Autopilot, GKE version 1.25.9-gke.400, 1.26.4-gke.500 or later.\n- Google Cloud CLI version 411.0.0 or later.\n- The Cloud DNS API must be enabled in your project.\nCloud DNS for GKE has the following requirements for VPC scope:\n- For Standard, GKE version 1.19 or later.\n- Google Cloud CLI version 364.0.0 or later.\n- The Cloud DNS API must be enabled in your project.## Restrictions and limitations\nThe following limitations apply:\n- VPC scope is not supported on Autopilot clusters, only cluster scope is supported.\n- Cloud DNS is not compliant with an Impact Level 4 (IL4) compliance regime. Cloud DNS for GKE cannot be used in an [Assured Workload environment](https://cloud.google.com/assured-workloads/docs/key-concepts#environments) with an IL4 compliance regime. You need to use kube-dns in such regulated environment. For GKE Autopilot clusters, the selection of kube-dns or Cloud DNS is automatically done based on your compliance regime.\n- Manual changes to the managed private DNS zones are not supported and are overwritten by the Cloud DNS controller. Modifications to the DNS records in those zones do not persist when the controller restarts.\n- After you enable Cloud DNS for GKE in a cluster, kube-dns continues to run in the cluster. You can disable kube-dns by [scaling the kube-dns Deployment and autoscaler to zero](/kubernetes-engine/docs/how-to/custom-kube-dns#scaling_down_kube-dns) .\n- You cannot change the [DNS scope](#dns-scopes) in a cluster after you have set the scope with the`--cluster-dns-scope`flag. If you need to change the DNS scope, recreate the cluster with a different DNS scope.\n- [Custom stub domains and upstream DNS server configurations](#supporting_custom_stub_domains_and_upstream_nameservers) apply to the DNS configurations of Pods and nodes. Pods using host networking or processes that run directly on the host also use the stub domain and upstream nameserver configurations. This is only supported in Standard.\n- Custom stub domains and upstream nameservers configured through the [kube-dns Configmap](https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/) are automatically applied to Cloud DNS for cluster scope DNS. VPC scope DNS ignores the kube-dns ConfigMap and you must apply these configurations directly on Cloud DNS. This is only supported in Standard.\n- There is no migration path from kube-dns to VPC scope, the operation is disruptive. Recreate the cluster when switching from kube-dns to VPC scope or vice versa.\n- For VPC scope, the secondary IP address range for Services must not be shared with any other clusters in that subnetwork.\n- For VPC scope, the response policy associated with a PTR record is attached to the VPC network. If there are any other response policies bound to the cluster network, PTR record resolution fails for Kubernetes service IP addresses.\n- If you try to create a headless Service with a number of Pods exceeding the allowed quota, Cloud DNS does not create record sets or records for the Service.\n- Switching from kube-dns to Cloud DNS VPC scope is a disruptive operation. We recommend that you perform this operation during a maintenance window.\n### Quotas\nCloud DNS uses quotas to limit number of resources that GKE can create for DNS entries. [Quotas and limits for Cloud DNS](/dns/quotas) might be different from the limitations of kube-dns for your project.\nThe following default quotas are applied to each managed zone in your project when using Cloud DNS for GKE:\n| Kubernetes DNS resource       | Corresponding Cloud DNS resource   | Quota                   |\n|:------------------------------------------------|:------------------------------------------|:---------------------------------------------------------------------------------|\n| Number of DNS records       | Max bytes per managed zone    | 2,000,000 (50MB max for a managed zone)           |\n| Number of Pods per headless Service (IPv4/IPv6) | Number of records per resource record set | GKE 1.24 to 1.25: 1,000 (IPv4/IPv6) GKE 1.26 and later: 3,500/2,000 (IPv4/IPv6) |\n| Number of GKE clusters in a project    | Number of response policies per project | 100                    |\n| Number of PTR records per cluster    | Number of rules per response policy  | 100000                   |\n**Note:** For VPC scope, the same limits apply to the managed zone used for the entire VPC. These quotas apply to the set of clusters using Cloud DNS VPC scope and running in the same VPC.\n### Resource limits\nThe Kubernetes resources that you create per cluster contribute to [Cloud DNS resource limits](/dns/quotas#resource_limits) , as described in the following table:\n| Limit         | Contribution to limit                                                     |\n|:--------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Resource record sets per managed zone | Number of services plus number of headless service endpoints with valid hostnames, per cluster.                                  |\n| Records per resource record set  | Number of endpoints per headless service. Does not impact other service types.                                      |\n| Number of rules per response policy | For cluster scope, number of services plus number of headless service endpoints with valid hostnames per cluster. For VPC scope, number of services plus number of headless endpoints with hostnames from all clusters in the VPC. |\nTo learn more about how DNS records are created for Kubernetes, see [Kubernetes DNS-Based Service Discovery](https://github.com/kubernetes/dns/blob/master/docs/specification.md) .\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Enable the Cloud DNS API in your project: [Enable Cloud DNS API](https://console.cloud.google.com/apis/library/dns.googleapis.com) ## Enable cluster scope DNS\nIn cluster scope DNS, only nodes running in the GKE cluster can resolve service names, and service names do not conflict between clusters. This behavior is the same as kube-dns in GKE clusters, which means that you can migrate clusters from kube-dns to Cloud DNS cluster scope without downtime or changes to your applications.\nThe following diagram shows how Cloud DNS creates a private DNS zone for a GKE cluster. Only processes and Pods running on the nodes in the cluster can resolve the cluster's DNS records, because only the nodes are in the DNS scope.### Enable cluster scope DNS in a new cluster\n**GKE Autopilot cluster**\nNew Autopilot clusters in version 1.25.9-gke.400, 1.26.4-gke.500 or later default to Cloud DNS cluster scope.\n**GKE Standard cluster**\nYou can create a GKE Standard cluster with Cloud DNS cluster scope enabled using the gcloud CLI or the Google Cloud console:\nCreate a cluster using the `--cluster-dns` flag:\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --cluster-dns=clouddns \\\u00a0 \u00a0 --cluster-dns-scope=cluster \\\u00a0 \u00a0 --location=COMPUTE_LOCATION\n```\n **Note:** For Google Cloud CLI versions earlier than 411.0.0, use `gcloud beta container` instead of `gcloud container` .\nReplace the following:- ``: the name of the cluster.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\nThe `--cluster-dns-scope=cluster` flag is optional in the command because `cluster` is the default value.- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- From the navigation pane, under **Cluster** , click **Networking** .\n- In the **DNS provider** section, click **Cloud DNS** .\n- Select **Cluster scope** .\n- Configure your cluster as needed.\n- Click **Create** .\n### Enable cluster scope DNS in an existing cluster\n**GKE Autopilot cluster**\nYou cannot migrate an existing GKE Autopilot cluster from kube-dns to Cloud DNS cluster scope. To enable Cloud DNS cluster scope, recreate the Autopilot clusters in version 1.25.9-gke.400, 1.26.4-gke.500 or later.\n**GKE Standard cluster**\nYou can migrate an existing GKE Standard cluster from kube-dns to Cloud DNS cluster scope using the gcloud CLI or the Google Cloud console in a GKE Standard cluster.\nWhen you migrate an existing cluster, the nodes in the cluster do not use Cloud DNS as a DNS provider until you recreate the nodes.\nAfter you enable Cloud DNS for a cluster, the settings only apply if you upgrade existing node pools or you add new node pools to the cluster. When you upgrade a node pool, the nodes are recreated.\nYou can also migrate clusters that have running applications without interrupting cluster communication by enabling Cloud DNS as a DNS provider in each node pool separately. A subset of the nodes are operational at all times because some node pools use kube-dns and some node pools use Cloud DNS.\nIn the following steps, you enable Cloud DNS for a cluster and then upgrade your node pools. Upgrading your node pools recreates the nodes. The nodes then use Cloud DNS for DNS resolution instead of kube-dns.\n- Update the existing cluster:```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --cluster-dns=clouddns \\\u00a0 \u00a0 --cluster-dns-scope=cluster \\\u00a0 \u00a0 --location=COMPUTE_LOCATION\n```Replace the following:- ``: the name of the cluster.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for your cluster.\nThe response is similar to the following:```\nAll the node-pools in the cluster need to be re-created by the user to start using Cloud DNS for DNS lookups. It is highly recommended to complete this step\nshortly after enabling Cloud DNS.\nDo you want to continue (Y/n)?\n```After you confirm, the Cloud DNS controller runs on the GKE control plane, but your Pods do not use Cloud DNS for DNS resolution until you upgrade your node pool or you add new node pools to the cluster.\n- Upgrade the node pools in the cluster to use Cloud DNS:```\ngcloud container clusters upgrade CLUSTER_NAME \\\u00a0 \u00a0 --node-pool=POOL_NAME \\\u00a0 \u00a0 --location=COMPUTE_LOCATION\n``` **Note:** For Google Cloud CLI versions earlier than 411.0.0, use `gcloud beta container` instead of `gcloud container` .Replace the following:- ``: the name of the cluster.\n- ``: the name of the node pool to upgrade.\nIf the node pool and control plane are running the same version, upgrade the control plane first, as described in [Manually upgrading the control plane](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrade_cp) and then perform the node pool upgrade. **Note:** Upgrading to the same version that the node pool currently runs on, has no effect on the Pod. For such node pools, the Pods running on these nodes still use kube-dns. Scaling down the kube-dns deployment and autoscaler to zero will cause DNS failures on these Pods. In order for the Pods to use Cloud DNS, you must upgrade the node pool to a new version. Alternatively, you can create a new node pool in the same version.Confirm the response and repeat this command for each node pool in the cluster. If your cluster has one node pool, omit the `--node-pool` flag.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click the name of the cluster you want to modify.\n- Under **Networking** , in the **DNS provider** field, click **Edit DNS provider** .\n- Click **Cloud DNS** .\n- Click **Cluster scope** .\n- Click **Save changes** .## Enable VPC scope DNS\nIn VPC scope DNS, a cluster's DNS names are resolvable within the entire VPC. Any client in the VPC can resolve cluster DNS records.\nVPC scope DNS enables the following use cases:\n- Headless service discovery for non-GKE clients within the same VPC.\n- GKE Service resolution from on-premise or 3rd party cloud clients. For more information, see [Inbound server policy](/dns/docs/server-policies-overview#dns-server-policy-in) .\n- Service resolution where a client can decide which cluster they want to communicate with using the custom cluster DNS domain.\nIn the following diagram, two GKE clusters use VPC scope DNS in the same VPC. Both clusters have a custom DNS domain, `.cluster1` and `.cluster2` , instead of the default `.cluster.local` domain. A VM communicates with the headless backend Service by resolving `backend.default.svc.cluster1` . Cloud DNS resolves the headless Service to the individual Pod IPs in the Service and the VM communicates directly with the Pod IPs.\n**Note:** Only headless Services resolve to IP addresses that can be contacted outside of the GKE cluster. ClusterIP Services use a virtual IP address that only exists locally in the cluster.\nYou can also perform this type of resolution from other networks when connected to the VPC through Cloud Interconnect or Cloud VPN. [DNS server policies](/dns/docs/best-practices#reference_architectures_for_hybrid_dns) enable clients from networks connected to the VPC to resolve names in Cloud DNS, which includes GKE Services if the cluster is using VPC scope DNS.\n### Enable VPC scope DNS in a new cluster\n**GKE Autopilot cluster**\nYou cannot create a GKE Autopilot cluster with VPC scope DNS, this feature is not supported on GKE Autopilot.\n**GKE Standard cluster**\nYou can enable VPC scope DNS in a new cluster using the gcloud CLI or the Google Cloud console.\n**Warning:** If you specify a `cluster-dns-domain` other than `cluster.local` , all `*.cluster.local` queries for Kubernetes services fail. Ensure that you do not perform DNS lookups for names that use `*.cluster.local` .\nYou can create a new cluster with VPC scope Cloud DNS enabled using the following command:\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --cluster-dns=clouddns \\\u00a0 \u00a0 --cluster-dns-scope=vpc \\\u00a0 \u00a0 --cluster-dns-domain=CUSTOM_DOMAIN \\\u00a0 \u00a0 --location=COMPUTE_LOCATION\n```\n **Note:** For Google Cloud CLI versions earlier than 364.0.0, use `gcloud beta container` instead of `gcloud container` .\nReplace the following:- ``: the name of the cluster.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\n- ``: the name of a domain. You must ensure this name is unique within the VPC because GKE does not confirm this value. You cannot change this value after it is set. You must not use a domain that ends in \".local\", or you might experience DNS resolution failures.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- From the navigation pane, under **Cluster** , click **Networking** .\n- In the **DNS provider** section, click **Cloud DNS** .\n- Select **VPC scope** .\n- Enter the name of a domain. You must ensure this name is unique within the VPC because GKE does not confirm this value. You cannot change this value after it is set. You must not use a domain that ends in \".local\", or you might experience DNS resolution failures.\n- Configure your cluster as needed.\n- Click **Create** .\n### Enable VPC scope DNS in an existing cluster\n**GKE Autopilot cluster**\nYou cannot migrate a GKE Autopilot cluster from kube-dns to Cloud DNS VPC scope, this feature is not supported on GKE Autopilot.\n**GKE Standard cluster**\nYou can migrate an existing GKE cluster from kube-dns to Cloud DNS VPC scope using the gcloud CLI or the Google Cloud console. The migration is supported in GKE Standard only.\nAfter you enable Cloud DNS for a cluster, the settings only apply if you upgrade existing node pools or you add new node pools to the cluster. When you upgrade a node pool, the nodes are recreated.\n**Warning:** Migrating from kube-dns to VPC scope is a disruptive operation. After you migrate, the nodes in the cluster do not use Cloud DNS as a DNS provider until you recreate the nodes, and kube-dns cannot resolve DNS queries to previous domains ( `cluster.local` by default), which causes DNS failures. Because migrating to VPC scope cannot be performed on a per-node-pool basis, we recommend that you perform this migration during a maintenance window.\nIn the following steps, you enable Cloud DNS for a cluster and then upgrade your node pools. Upgrading your node pools recreates the nodes. The nodes then use Cloud DNS for DNS resolution instead of kube-dns.\n- Update the existing cluster:```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --cluster-dns=clouddns \\\u00a0 \u00a0 --cluster-dns-scope=vpc \\\u00a0 \u00a0 --cluster-dns-domain=CUSTOM_DOMAIN \\\u00a0 \u00a0 --location=COMPUTE_LOCATION\n```Replace the following:- ``: the name of the cluster.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster.\n- ``: the name of a domain. You must ensure this name is unique within the VPC because GKE does not confirm this value. You cannot change this value after it is set. You must not use a domain that ends in \".local\", or you might experience DNS resolution failures.\nThe response is similar to the following:```\nAll the node-pools in the cluster need to be re-created by the user to start using Cloud DNS for DNS lookups. It is highly recommended to complete this step\nshortly after enabling Cloud DNS.\nDo you want to continue (Y/n)?\n```After you confirm, the Cloud DNS controller runs on the GKE control plane. Your Pods do not use Cloud DNS for DNS resolution until you upgrade your node pool or you add new node pools to the cluster.\n- Upgrade the node pools in the cluster to use Cloud DNS:```\ngcloud container clusters upgrade CLUSTER_NAME \\\u00a0 \u00a0 --node-pool=POOL_NAME\n```Replace the following:- ``: the name of the cluster.\n- ``: the name of the node pool to upgrade.\nIf the node pool and control plane are running the same version, upgrade the control plane first, as described in [Manually upgrading the control plane](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrade_cp) and then perform the node pool upgrade. **Note:** Upgrading to the same version that the node pool currently runs on, has no effect on the Pod. For such node-pools the Pods running on these nodes still use kube-dns. Scaling the kube-dns deployment and autoscaler to zero will cause DNS failures on these pods. In order for the pods to use Cloud DNS, you must upgrade the node-pool to a new version. Alternatively, you can create a new node pool in the same version.Confirm the response and repeat this command for each node pool in the cluster. If your cluster has one node pool, omit the `--node-pool` flag.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click the name of the cluster you want to modify.\n- Under **Networking** , in the **DNS provider** field, click **Edit DNS provider** .\n- Click **Cloud DNS** .\n- Click **VPC scope** .\n- Click **Save changes** .## Verify Cloud DNS\nVerify that Cloud DNS for GKE is working correctly for your cluster:\n- Verify that your nodes are using Cloud DNS by connecting to a Pod on a node and running the command `cat /etc/resolv.conf` :```\nkubectl exec -it POD_NAME -- cat /etc/resolv.conf | grep nameserver\n```Replace `` with the name of the Pod.Based on the cluster mode, the output is similar to the following: **GKE Autopilot cluster** ```\nnameserver 169.254.20.10\n```Because the [NodeLocal DNSCache](/kubernetes-engine/docs/how-to/nodelocal-dns-cache) is enabled by default in GKE Autopilot, the Pod is using [NodeLocal DNSCache](/kubernetes-engine/docs/how-to/nodelocal-dns-cache) .Only when the local cache does not have an entry for the name being looked up, NodeLocal DNSCache forwards the request to Cloud DNS. **GKE Standard cluster** ```\nnameserver 169.254.169.254\n```The Pod is using `169.254.169.254` as the `nameserver` , which is the IP address of the metadata server where the Cloud DNS data plane listens for requests on port 53. The nodes no longer use the kube-dns Service address for DNS resolution and all DNS resolution occurs on the local node.If the output is an IP address similar to `10.x.y.10` , then the Pod is using [kube-dns](/kubernetes-engine/docs/how-to/kube-dns) . See the [Troubleshooting](/kubernetes-engine/docs/how-to/cloud-dns#pods_use_kube-dns_even_after_is_enabled_on_an_existing_cluster) section to understand why your pod is still using kube-dns.If the output is `169.254.20.10` , it means that you have enabled [NodeLocal DNSCache](/kubernetes-engine/docs/how-to/nodelocal-dns-cache) in your cluster, then the Pod is using [NodeLocal DNSCache](/kubernetes-engine/docs/how-to/nodelocal-dns-cache) .\n- Deploy a sample application to your cluster:```\nkubectl run dns-test --image us-docker.pkg.dev/google-samples/containers/gke/hello-app:2.0\n```\n- Expose the sample application with a Service:```\nkubectl expose pod dns-test --name dns-test-svc --port 8080\n```\n- Verify that the Service deployed successfully:```\nkubectl get svc dns-test-svc\n```The output is similar to the following:```\nNAME   TYPE  CLUSTER-IP  EXTERNAL-IP PORT(S) AGE\ndns-test-svc ClusterIP 10.47.255.11 <none>  8080/TCP 6m10s\n```The value of `CLUSTER-IP` is the virtual IP address for your cluster. In this example, the virtual IP address is `10.47.255.11` .\n- Verify that your Service name was created as a record in the private DNS zone for your cluster:```\ngcloud dns record-sets list \\\u00a0 \u00a0 --zone=PRIVATE_DNS_ZONE \\\u00a0 \u00a0 --name=dns-test-svc.default.svc.cluster.local.\n```Replace `` with the name of the managed DNS zone.The output is similar to the following:```\nNAME: dns-test-svc.default.svc.cluster.local.\nTYPE: A\nTTL: 30\nDATA: 10.47.255.11\n```## Disable Cloud DNS for GKE\n**GKE Autopilot cluster**\nYou cannot disable Cloud DNS in a GKE Autopilot cluster that was created with Cloud DNS by default. See the [requirements](/kubernetes-engine/docs/how-to/cloud-dns#minimum-requirements) for more information about GKE Autopilot clusters using Cloud DNS by default.\n**GKE Standard cluster**\n**Warning:** You cannot disable Cloud DNS VPC scope in a GKE Standard cluster, you need to recreate your cluster with kube-dns as your default DNS provider.\nYou can disable Cloud DNS cluster scope using the gcloud CLI or the Google Cloud console in a GKE Standard cluster.\nUpdate the cluster to use kube-dns:\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --cluster-dns=default\n```- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click the name of the cluster you want to modify.\n- Under **Networking** , in the **DNS provider** field, click **Edit DNS provider** .\n- Click **Kube-dns** .\n- Click **Save changes** .## Clean up\nAfter completing the exercises on this page, follow these steps to remove resources and prevent unwanted charges incurring on your account:\n- Delete the service:```\nkubectl delete service dns-test-svc\n```\n- Delete the Pod:```\nkubectl delete Pod dns-test\n```\n- You can also [delete the cluster](/kubernetes-engine/docs/how-to/deleting-a-cluster) .## Use Cloud DNS with Shared VPC\nCloud DNS for GKE supports Shared VPC for both VPC and cluster scope.\nThe GKE controller creates a managed private zone in the same project as the GKE cluster.\nThe GKE service account for the GKE cluster does not require Identity and Access Management (IAM) for DNS outside of its own project because the managed zone and GKE cluster reside within the same project.\n### More than one cluster per service project\nStarting in GKE versions 1.22.3-gke.700, 1.21.6-gke.1500, you can create clusters in multiple service projects referencing a VPC in the same host project. If you already have clusters using Shared VPC and Cloud DNS VPC scope, you must manually migrate them with the following steps:\n- [Upgrade your existing clusters](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrade_cp) that have Shared VPC enabled to GKE version 1.22.3-gke.700+ or 1.21.6-gke.1500+.\n- Migrate the response policy from the service project to the host project. You only need to perform this migration once per Shared VPC network.\nYou can migrate your response policy using the Google Cloud console.\nPerform the following steps in your service project:\n- Go to the **Cloud DNS zones** page. [Go to Cloud DNS zones](https://console.cloud.google.com/networking/dns/zones) \n- Click the **Response policy zones** tab.\n- Click the response policy for your VPC network. You can identify the response policy by the description, which is similar to \"Response policy for GKE clusters on network NETWORK_NAME.\"\n- Click the **In use by** tab.\n- Next to the name of your host project, click to remove the network binding.\n- Click the **Response policy rules** tab.\n- Select all of the entries in the table.\n- Click **Remove response policy rules** .\n- Click **Delete response policy** .\nAfter you delete the response policy, the DNS controller creates the response policy in the host project automatically. Clusters from other service projects share this response policy.\n## Support custom stub domains and upstream nameservers\nCloud DNS for GKE supports custom stub domains and upstream nameservers configured using kube-dns ConfigMap. This support is only available for GKE cluster scope.\nCloud DNS translates `stubDomains` and `upstreamNameservers` values into Cloud DNS forwarding zones.\n**Note:** The custom DNS configuration applies to both nodes and Pods in the cluster. This behavior is different from kube-dns, where the custom DNS configuration applies only to Pods.\n## Known issues\n### Terraform plans to recreate Autopilot cluster due to dns_config change\nIf you use `terraform-provider-google` or `terraform-provider-google-beta` , you might experience an issue where Terraform tries tries to recreate an Autopilot cluster. This error occurs because newly created Autopilot clusters running version 1.25.9-gke.400, 1.26.4-gke.500, 1.27.1-gke.400 or later use Cloud DNS as a DNS provider instead of kube-dns.\nThis issue is [resolved](https://github.com/GoogleCloudPlatform/magic-modules/pull/8654) in version 4.80.0 of the Terraform provider of Google Cloud.\nIf you cannot update the version of `terraform-provider-google` or `terraform-provider-google-beta` , you can add `lifecycle.ignore_changes` to the resource to ensure that `google_container_cluster` ignores changes to `dns_config` :\n```\n\u00a0 lifecycle {\u00a0 \u00a0 ignore_changes = [\u00a0 \u00a0 \u00a0 dns_config,\u00a0 \u00a0 ]\u00a0 }\n```\n## Troubleshooting\nTo learn how to enable DNS logging, see [Enabling and disabling logging for private managed zones](/dns/docs/monitoring#enabling_and_disabling_logging_for_private_managed_zones) .\nFor more information about troubleshooting DNS issues, see [Troubleshooting DNS in GKE](/kubernetes-engine/docs/troubleshooting/troubleshooting-dns) .\n### Unable to update existing cluster or create cluster with Cloud DNS enabled\nEnsure you are using the correct version. Cloud DNS for GKE requires GKE version 1.19 or later for clusters using VPC scope, or GKE version 1.24.7-gke.800, 1.25.3-gke.700 or later for clusters using cluster scope.\n### Pods use kube-dns even after Cloud DNS is enabled on an existing cluster\nEnsure you have upgraded or recreated your node pools after you enable Cloud DNS on the cluster. Until this step is complete, Pods continue to use kube-dns.\n### Pod is unable to resolve DNS lookups\n- Verify the Pod is using Cloud DNS by connecting to a Pod and running the command `cat /etc/resolv.conf` :```\nkubectl exec -it POD_NAME -- cat /etc/resolv.conf | grep nameserver\n```Replace `` with the name of the Pod.The output is similar to the following:```\nnameserver 169.254.169.254\n```If the output is an IP address similar to `10.x.y.10` or `34.118.224.10` (only in GKE Autopilot clusters with versions 1.27 and later), then the Pod is using [kube-dns](/kubernetes-engine/docs/how-to/kube-dns) . If the output is `169.254.20.10` , then the Pod is using [NodeLocal DNSCache](/kubernetes-engine/docs/how-to/nodelocal-dns-cache) .\n- Confirm the managed zone exists and contains the necessary DNS entry:```\ngcloud dns managed-zones list --format list\n```The output is similar to the following:```\n - creationTime: 2021-02-12T19:24:37.045Z\n description: Private zone for GKE cluster \"cluster1\" with cluster suffix \"cluster.local\" in project \"project-243723\"\n dnsName: cluster.local.\n id: 5887499284756055830\n kind: dns#managedZone\n name: gke-cluster1-aa94c1f9-dns\n nameServers: ['ns-gcp-private.googledomains.com.']\n privateVisibilityConfig: {'kind': 'dns#managedZonePrivateVisibilityConfig'}\n visibility: private\n```The value of `name` in the response shows that Google Cloud created a private zone named `gke-cluster1-aa94c1f9-dns` .\n- Verify that Cloud DNS contains entries for your cluster:```\ngcloud dns record-sets list --zone ZONE_NAME | grep SERVICE_NAME\n```Replace the following:- ``: the name of the private zone.\n- ``: the name of the service.\nThe output shows that Cloud DNS contains an A record for the domain `dns-test.default.svc.cluster.local.` and the IP address of your cluster, `10.47.255.11` :```\ndns-test.default.svc.cluster.local.    A  30  10.47.255.11\n```\n- Enable [Cloud DNS logging](/dns/docs/monitoring) to track queries. Every log entry contains information about the query, including DNS latency.\n### DNS lookups on nodes fail after enabling Cloud DNS on a cluster\nIf you enable cluster scope Cloud DNS in a GKE cluster that has custom stub domains or upstream nameservers, the custom config applies to both nodes and Pods in the cluster because Cloud DNS cannot distinguish between Pod and node DNS requests. DNS lookups on nodes might fail if the custom upstream server cannot resolve the queries.\n## What's next\n- [Read an overview](/kubernetes-engine/docs/concepts/service-discovery) of how GKE provides managed DNS.\n- Read [DNS for services and Pods](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/) for a general overview of how DNS is used in Kubernetes clusters.\n- Learn about [Scopes and hierarchies](/dns/docs/scopes) in Cloud DNS.\n- Learn about [Enabling and disabling logging for private managed zones](/dns/docs/monitoring#enabling_and_disabling_logging_for_private_managed_zones) .", "guide": "Google Kubernetes Engine (GKE)"}