{"title": "Google Kubernetes Engine (GKE) - Prepare to migrate to Autopilot from Standard", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/prepare-migrate-cluster-mode", "abstract": "# Google Kubernetes Engine (GKE) - Prepare to migrate to Autopilot from Standard\nThis page provides considerations and recommendations that will help you to migrate workloads from Standard Google Kubernetes Engine (GKE) clusters to Autopilot clusters with minimal disruption to your services. This page is for cluster administrators who have to migrate to Autopilot. If you need more information before you decide to migrate, see [Choose a GKE mode of operation](/kubernetes-engine/docs/concepts/choose-cluster-mode) and [Compare GKE Autopilot and Standard](/kubernetes-engine/docs/resources/autopilot-standard-feature-comparison) .\n", "content": "## How migration works\nAutopilot clusters automate many of the optional features and functionality that require manual configuration in Standard clusters. Additionally, Autopilot clusters enforce more secure default configurations for applications to provide a more production-ready environment, and reduce your required management overhead compared to Standard mode. Autopilot clusters apply many GKE best practices and recommendations by default. Autopilot uses a workload-centric configuration model, where you request what you need in your Kubernetes manifests and GKE provisions the corresponding infrastructure.\nWhen you migrate your Standard workloads to Autopilot, you should prepare your workload manifests to ensure that they're compatible with Autopilot clusters, for example by ensuring that your manifests request infrastructure that you would normally have to provision yourself.\nTo prepare and execute a successful migration, you'll do the following high-level tasks:\n- Run acheck on your existing Standard cluster to confirm compatibility with Autopilot.\n- If applicable, modify your workload manifests to become Autopilot compatible.\n- Do a dry-run where you check that your workloads function correctly on Autopilot.\n- Plan and create the Autopilot cluster.\n- If applicable, update your infrastructure-as-code tooling.\n- Perform the migration.## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Ensure that you have an existing Standard cluster with running workloads.\n- Ensure that you have an Autopilot cluster with no workloads to perform dry-runs. To create a new Autopilot cluster, see [Create an Autopilot cluster](/kubernetes-engine/docs/how-to/creating-an-autopilot-cluster) .## Run a pre-flight check on your Standard cluster\nThe Google Cloud CLI and the Google Kubernetes Engine API provide a tool that validates the specifications of your running Standard workloads to identify incompatibilities with Autopilot clusters. This tool is available in GKE version 1.26 and later.\n- To use this tool on the command-line, run the following command:\n```\ngcloud container clusters check-autopilot-compatibility CLUSTER_NAME\n```\nReplace `` with the name of your Standard cluster. Optionally, add `--format=json` to this command to get the output in a JSON file.\nThe output contains findings for all your running Standard workloads, categorized and with actionable recommendations to ensure compatibility with Autopilot, where applicable. The following table describes the categories:\n| Pre-flight tool results   | Pre-flight tool results.1                                                                                                            |\n|:-----------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Passed        | The workload will run as expected with no configuration needed for Autopilot.                                                                                               |\n| Passed with optional configuration | The workload will run on Autopilot, but you can make optional configuration changes to optimize the experience. If you don't make configuration changes, Autopilot applies a default configuration for you. For example, if your workload was running on N2 machines in Standard mode, GKE applies the general-purpose compute class for Autopilot. You can optionally modify the workload to request the Balanced compute class, which is backed by N2 machines. |\n| Additional configuration required | The workload won't run on Autopilot unless you make a configuration change. For example, consider a container that uses the NET_ADMIN capability in Standard. Autopilot drops this capability by default for improved security, so you'll need to enable NET_ADMIN on the cluster before you deploy the workload.                                     |\n| Incompatibility     | The workload won't run on Autopilot because it uses functionality that Autopilot doesn't support. For example, Autopilot clusters reject privileged Pods (privileged: true in the Pod specification) to improve the default security posture. The only way to run that application in Autopilot would be to remove the privileged setting.                               |\n## Modify your workload specifications based on the pre-flight results\nAfter you run the pre-flight check, step through the JSON output and identify workloads that need to change. We recommend implementing even the optional configuration recommendations. Each finding also provides a link to documentation that shows you what the workload specification should look like.\nThe most important difference between Autopilot and Standard is that infrastructure configuration in Autopilot is automated based on the workload specification. Kubernetes scheduling controls, such as node taints and tolerations, are automatically added to your running workloads. If necessary, you should also modify your infrastructure-as-code configurations, such as Helm charts or Kustomize overlays, to match.\n**Note:** As a best practice, make copies of your original workload specifications so that rolling back your migration takes less time.\nSome common configuration changes you'll need to make include the following:\n| Common configuration changes for Autopilot | Common configuration changes for Autopilot.1                                                                                              |\n|:---------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Compute and architecture configuration  | Autopilot clusters use the E-series machine type by default. If you need other machine types, your workload specification must request a compute class, which tells Autopilot to place those Pods on nodes that use specific machine types or architectures. For details, see Compute classes in Autopilot.                             |\n| Accelerators         | GPU-based workloads must request GPUs in the workload specification. Autopilot automatically provisions nodes with the required machine type and accelerators. For details, see Deploy GPU workloads in Autopilot.                                                   |\n| Resource requests       | All Autopilot workloads need to specify values for resources.requests in the Pod specification. GKE uses these values to provision appropriate machine sizes to run the Pods. Autopilot has specific automatically applied default requests if you omit any, and enforces specific minimums and maximums based on the compute class or hardware request of your workload. For details, see Resource requests in Autopilot. |\n| Fault-tolerant workloads on Spot VMs   | If your workloads run on Spot VMs in Standard, request Spot Pods in the workload configuration by setting a node selector for cloud.google.com/gke-spot=true. For details, see Spot Pods.                                                          |\n## Perform a dry-run on a staging Autopilot cluster\nAfter you modify each workload manifest, do a dry-run deployment on a new staging Autopilot cluster to ensure that the workload runs as expected.\nRun the following command:\n```\nkubectl create --dry-run=server -f=PATH_TO_MANIFEST\n```\nReplace `` with the path to the modified workload manifest.\nIf you use the Cloud Shell Editor, the dry-run command is built-in and runs on any open manifests. If you use Visual Studio Code or Intellij IDEs, install the Cloud Code extension to automatically run the dry-run on any open manifests.\nThe **Problems** pane in the IDE shows any dry-run issues, such as in the following image which shows a failed dry-run for a manifest that specified `privileged: true` .\n## Plan the destination Autopilot cluster\nWhen your dry-run no longer displays issues, plan and create the new Autopilot cluster for your workloads. This cluster is different from the Autopilot cluster that you used to test your manifest modifications in the preceding section.\nUse [Best practices for onboarding to GKE](/kubernetes-engine/docs/best-practices/onboarding) for basic configuration requirements. Then, read the [Autopilot overview](/kubernetes-engine/docs/concepts/autopilot-overview) , which provides information specific to your use case at different layers.\nAdditionally, consider the following:\n- Autopilot clusters are VPC-native, so we don't recommend migrating to Autopilot from routes-based Standard clusters.\n- Use the same or a similar VPC for the Autopilot cluster and the Standard cluster, including any custom firewall rules and VPC settings.\n- Autopilot clusters use GKE Dataplane V2 and only support Cilium NetworkPolicies. Calico NetworkPolicies are not supported.\n- If you want to use IP masquerading in Autopilot, use an [Egress NAT policy](/kubernetes-engine/docs/how-to/egress-nat-policy-ip-masq-autopilot) .\n- If you have a private Standard cluster, create a [private Autopilot cluster](/kubernetes-engine/docs/how-to/private-clusters) with similar isolation settings.\n- Specify the [primary IPv4 range](/sdk/gcloud/reference/container/clusters/create-auto#--cluster-ipv4-cidr) for the cluster during cluster creation, with the same range size as the Standard cluster.\n- Learn about the [quota differences](/kubernetes-engine/quotas) between modes, especially if you have large clusters.\n- Learn about the [Pods-per-node maximums](/kubernetes-engine/docs/how-to/flexible-pod-cidr#max_pods_default) for Autopilot, which are different from Standard. This matters more if you use node or Pod affinity often.\n- All Autopilot clusters use [Cloud DNS](/kubernetes-engine/docs/concepts/service-discovery) .\n### Create the Autopilot cluster\nWhen you're ready to create the cluster, use [Create an Autopilot cluster](/kubernetes-engine/docs/how-to/creating-an-autopilot-cluster) . All Autopilot clusters are regional and are automatically enrolled in a release channel, although you can specify the channel and cluster version. We recommend deploying a small sample workload to the cluster to trigger node auto-provisioning so that your production workloads can schedule immediately.\n## Update your infrastructure-as-code tooling\nThe following infrastructure-as-code providers support Autopilot:\n- [Terraform](https://www.hashicorp.com/blog/terraform-adds-support-for-gke-autopilot) \n- [Crossplane](https://github.com/crossplane-contrib/provider-gcp/blob/master/examples/gke/autopilot.yaml) \n- [Pulumi](https://github.com/crossplane-contrib/provider-gcp/blob/master/examples/gke/autopilot.yaml) \n- [Config Connector](/config-connector/docs/overview) \nRead your preferred provider's documentation and modify your configurations.\n## Choose a migration approach\nThe migration method that you use depends on your individual workload and how comfortable you are with networking concepts such as [multi-cluster Services](/kubernetes-engine/docs/concepts/multi-cluster-services) and [multi-cluster Ingress](/kubernetes-engine/docs/concepts/multi-cluster-ingress) , as well as how you manage the state of the Kubernetes objects in your cluster.\n| Workload type | Pre-flight tool results              | Migration approach                                                                                                                                                                                                             |\n|:----------------|:----------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Stateless  | Passed Passed with optional configuration Additional configuration required | No downtime: Update your Kubernetes manifests if applicable and redeploy on Autopilot. Use multi-cluster Ingress and multi-cluster Services for traffic cutover. For high-level steps, see Manually migrate stateless workloads with no downtime. Downtime: Update your Kubernetes manifests if applicable and redeploy on Autopilot during scheduled downtime. Change your DNS records to the new IP addresses of your Services and end downtime. For high-level steps, see Alternative: Manually migrate all workloads during downtime. For Passed and Passed with optional configuration workloads, you can also use Backup for GKE to move all workloads to the Autopilot cluster. You must still update your pipeline and upstream manifests to target the Autopilot cluster. For high-level steps, see Migrate all workloads using Backup for GKE. |\n| Stateful  | Passed Passed with optional configuration         | Use one of the following methods: Backup for GKE: Use Backup for GKE to move stateful workloads to the Autopilot cluster while maintaining existing PersistentVolume relationships. For high-level steps, see Migrate all workloads using Backup for GKE. Cross-regional migration is supported. Manual: Move the stateful workloads manually. This approach requires more careful planning for the PersistentVolumes and Compute Engine disks. For high-level steps, see Manually migrate stateful workloads. Cross-regional migration is not supported. All stateful workload migrations require downtime to avoid data loss.                                                        |\n| Stateful  | Additional configuration required           | Update your Kubernetes manifests and redeploy on Autopilot during scheduled downtime. For high-level steps, see Manually migrate stateful workloads.                                                                                                                                                                            |\n## High-level migration steps\nBefore you begin a migration, ensure that you resolved any `Incompatible` or `Additional configuration required` results from the pre-flight check. If you deploy workloads with those results on Autopilot without modifications, the workloads will fail.\nThe following sections are a high-level overview of a hypothetical migration. Actual steps will vary depending on your environment and each of your workloads. Plan, test, and re-test workloads for issues before migrating a production environment. Considerations include the following:\n- The duration of the migration process depends on how many workloads you're migrating.\n- Downtime is **required** while you migrate stateful workloads.\n- Manual migration lets you focus on individual workloads during the migration so that you can resolve issues in real time on a case-by-case basis.\n- In all cases, ensure that you migrate Services, Ingresses, and other Kubernetes objects that facilitate the functionality of your stateless and stateful workloads.\n**Tip:** Always communicate an upcoming migration to your end-users and warn them about potential service disruptions and downtime.\n### Migrate all workloads using Backup for GKE\n**Caution:** This approach requires downtime for your cluster, for both stateful and stateless workloads. Notify your users of the upcoming downtime.\nIf all the workloads (stateful and stateless) running in your Standard cluster are compatible with Autopilot and the preflight tool returns either `Passed` or `Passed with optional configuration` for every workload, you can use [Backup for GKE](/kubernetes-engine/docs/add-on/backup-for-gke/concepts/backup-for-gke) to back up the entire state of your Standard cluster and workloads and restore the backup onto the Autopilot cluster.\nThis approach has the following benefits:\n- You can move all workloads from Standard to Autopilot operation with minimal configuration needed.\n- You can move stateless and stateful workloads and retain the relationships between workloads, as well as associated PersistentVolumes.\n- Rollbacks are intuitive and managed by Google. You can roll the entire migration back or selectively roll back specific workloads.\n- You can migrate stateful workloads across Google Cloud regions. Manual migration of stateful workloads can only happen in the same region.\nWhen you use this method, GKE applies Autopilot default configurations to workloads that received a `Passed with optional configuration` result from the pre-flight tool. Before you migrate these workloads, ensure that you're comfortable with those defaults.\n### Manually migrate stateless workloads with no downtime\nTo migrate stateless workloads with no downtime for your services, you register the source and destination clusters to a [GKE Fleet](/anthos/fleet-management/docs/fleet-concepts) and use multi-cluster Services and multi-cluster Ingress to ensure that your workloads remain available during the migration.\n- Enable multi-cluster Services and multi-cluster Ingress for your source cluster and your destination cluster. For instructions, see [Configuring multi-cluster Services](/kubernetes-engine/docs/how-to/multi-cluster-services) and [Setting up Multi Cluster Ingress](/kubernetes-engine/docs/how-to/multi-cluster-ingress-setup) .\n- If you have backend dependencies such as a database workload, export those Services from your Standard cluster using multi-cluster Services. This lets workloads in your Autopilot cluster access the dependencies in the Standard cluster. For instructions, see [Registering a Service for export](/kubernetes-engine/docs/how-to/multi-cluster-services#registering_a_service_for_export) .\n- Deploy a multi-cluster Ingress and a multi-cluster Service to control inbound traffic between clusters. Configure the multi-cluster Service to only send traffic to the Standard cluster. For instructions, see [Deploying Ingress across clusters](/kubernetes-engine/docs/how-to/multi-cluster-ingress#features) .\n- Deploy your stateless workloads with updated manifests to the Autopilot cluster. Your exported multi-cluster Services automatically match and send traffic to the corresponding stateful workloads.\n- Update your multi-cluster Service to direct inbound traffic to the Autopilot cluster. For instructions, see [Cluster selection](/kubernetes-engine/docs/how-to/multi-cluster-ingress#cluster_selection) .\nYou're now serving your stateless workloads from the Autopilot cluster. If you only had stateless workloads in the source cluster, and no dependencies remain, proceed to [Complete the migration](#complete-migration) . If you have stateful workloads, proceed to [Manually migrate stateful workloads](#manual-stateful-migration) .\n### Manually migrate stateful workloads\nAfter migrating your stateless workloads, you must quiesce and migrate your stateful workloads from the Standard cluster. This step requires downtime for your cluster.\n**Note:** Your source and destination clusters must be in the same Google Cloud region to manually migrate existing persistent disks. If you need to migrate the stateful workloads to a cluster in a different region, use [Backup for GKE](#migrate-all-backup) .\n- Start your environment downtime.\n- Quiesce your stateful workloads.\n- Ensure that you modified your workload manifests for Autopilot compatibility. For details, see [Modify your workload specifications based on the pre-flight results](#modify-workload-spec) .\n- Deploy the workloads on your Autopilot cluster. **Note:** Migrate persistent data by re-deploying your PersistentVolumeClaims to use your existing Compute Engine disks. For instructions, see [Using pre-existing persistent disks as PersistentVolumes](/kubernetes-engine/docs/how-to/persistent-volumes/preexisting-pd) .\n- Deploy the Services for your stateful workloads on the Autopilot cluster.\n- Update your in-cluster networking to let your stateless workloads continue to communicate with their backend workloads:- If you used a static IP address in your Standard cluster backend Services, reuse that IP address in Autopilot.\n- If you let Kubernetes assign an IP address, deploy your backend Services, get the new IP address, and update your DNS to use the new IP address.At this stage, the following should be true:\n- You're running all your stateless workloads in Autopilot.\n- Any backend stateful workloads are also running in Autopilot.\n- Your stateless and stateful workloads can communicate with each other.\n- Your multi-cluster Service directs all inbound traffic to your Autopilot cluster.\nWhen you've migrated all the workloads and Kubernetes objects to the new cluster, proceed to [Complete the migration](#complete-migration) .\n### Alternative: Manually migrate all workloads during downtime\n**Caution:** This approach requires downtime for your cluster, for both stateful and stateless workloads. Notify your downstream users of the upcoming downtime.\nIf you don't want to use multi-cluster Services and multi-cluster Ingress to migrate workloads with minimal downtime, migrate all your workloads during downtime. This method results in longer downtime for your services, but doesn't require working with multi-cluster features.\n- Start your downtime.\n- Deploy your stateless manifests on the Autopilot cluster.\n- Manually migrate your stateful workloads. For instructions, see the [Manually migrate stateful workloads](#manual-stateful-migration) section.\n- Modify DNS records for both intra-cluster and inbound external traffic to use the new IP addresses of Services.\n- End your downtime.## Complete the migration\nAfter moving all your workloads and Services to the new Autopilot cluster, end your downtime and allow your environment to soak for a predetermined duration. When you're satisfied with the state of your migration and are sure that you won't need to roll the migration back, you can clean up migration artifacts and complete the migration.\n### Optional: Clean up multi-cluster features\nIf you used multi-cluster Ingress and multi-cluster Services to migrate, and you don't want your Autopilot cluster to remain registered to a Fleet, do the following:\n- For inbound external traffic, deploy an Ingress and set it to the IP address of the Services that expose your workloads. For instructions, see [Ingress for external Application Load Balancers](/kubernetes-engine/docs/concepts/ingress-xlb) .\n- For intra-cluster traffic, such as from frontend workloads to stateful dependencies, update cluster DNS records to use the IP addresses of those Services.\n- Delete the multi-cluster Ingress and the multi-cluster Service resources that you created during the migration.\n- [Disable multi-cluster Ingress and multi-cluster Services](/kubernetes-engine/docs/how-to/multi-cluster-services#disabling_mcs) .\n- [Unregister the Autopilot cluster from the Fleet](/anthos/fleet-management/docs/unregister) .\n### Delete the Standard cluster\nAfter enough time has passed after the migration completes, and you're satisfied with the state of your new cluster, delete the Standard cluster. We recommend that you keep your backed up Standard manifests.\n## Roll back a faulty migration\nIf you experience issues and want to revert to the Standard cluster, do one of the following, depending on how you performed the migration:\n- If you used Backup for GKE to create backups during the migration, restore the backups onto the original Standard cluster. For instructions, see [Restore a backup](/kubernetes-engine/docs/add-on/backup-for-gke/how-to/restore) .\n- If you manually migrated workloads, repeat the migration steps in the previous sections with the Standard cluster as the destination and the Autopilot cluster as the source. At a high-level, this involves the following steps:- Start downtime.\n- Manually migrate stateful workloads to the Standard cluster. For instructions, see the [Manually migrate stateful workloads section](#manual-stateful-migration) .\n- Move stateless workloads to the Standard cluster using the original manifests that you backed up prior to the migration.\n- Deploy your Ingress to the Standard cluster and cutover your DNS to the new IP addresses for Services.\n- Delete the Autopilot cluster.\n## What's next\n- [Plan Autopilot resource requests](/kubernetes-engine/docs/concepts/autopilot-resource-requests) \n- [Set up monitoring](/stackdriver/docs/solutions/gke) \n- [Set up and use the security posture dashboard](/kubernetes-engine/docs/concepts/about-security-posture-dashboard)", "guide": "Google Kubernetes Engine (GKE)"}