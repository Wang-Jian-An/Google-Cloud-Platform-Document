{"title": "Google Kubernetes Engine (GKE) - Setting up Multi Cluster Ingress", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/multi-cluster-ingress-setup", "abstract": "# Google Kubernetes Engine (GKE) - Setting up Multi Cluster Ingress\nThis page shows you how to route traffic across multiple Google Kubernetes Engine (GKE) clusters in different regions using [Multi Cluster Ingress](/kubernetes-engine/docs/concepts/multi-cluster-ingress) , with an example using two clusters.\nFor a detailed comparison between Multi Cluster Ingress (MCI), Multi-cluster Gateway (MCG), and load balancer with Standalone Network Endpoint Groups (LB and Standalone NEGs), see [Choose your multi-cluster load balancing API forGKE](/kubernetes-engine/docs/concepts/choose-mc-lb-api) .\nTo learn more about deploying Multi Cluster Ingress, see [Deploying Ingress across clusters](/kubernetes-engine/docs/how-to/multi-cluster-ingress) .\nThese steps require elevated permissions and should be performed by a GKE administrator.\n", "content": "## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n### Requirements and limitations\nMulti Cluster Ingress has the following requirements:\n- Google Cloud CLI version 290.0.0 and later.\nIf you use Standard mode clusters, ensure that you meet the following requirements. Autopilot clusters already meet these requirements.\n- Clusters must have the`HttpLoadBalancing`add-on enabled. This add-on is enabled by default, you must not disable it.\n- Clusters must be [VPC-native](/kubernetes-engine/docs/concepts/alias-ips) .\n- Clusters must have [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) enabled.\nMulti Cluster Ingress has the following limitations:\n- Only supported with an [external Application Load Balancer](/load-balancing/docs/https) .\n- Don't create Compute Engine load balancers in the same project with the prefix`mci-`that are not managed by Multi Cluster Ingress or they will be deleted. Google Cloud uses the prefix`mci-[6 char hash]`to manage the Compute Engine resources that Multi Cluster Ingress deploys.\n- Configuration of HTTPS requires a pre-allocated static IP address. HTTPS is not supported with ephemeral IP addresses.## Overview\nIn this exercise, you perform the following steps:\n- Select the pricing you want to use.\n- Deploy clusters.\n- Configure cluster credentials.\n- Register the clusters to a [fleet](/anthos/fleet-management/docs) .\n- Specify a config cluster. This cluster can be a dedicated control plane, or it can run other workloads.\nThe following diagram shows what your environment will look like after you complete the exercise:\nIn the diagram, there are two GKE clusters named `gke-us` and `gke-eu` in the regions `europe-west1` and `us-central1` . The clusters are registered to a fleet so that the Multi Cluster Ingress controller can recognize them. A fleet lets you logically group and normalize your GKE clusters, making administration of infrastructure easier and enabling the use of multi-cluster features such as Multi Cluster Ingress. You can learn more about the benefits of fleets and how to create them in the [fleet management documentation](/anthos/fleet-management/docs) .\n## Select pricing\nIf Multi Cluster Ingress is the only GKE Enterprise capability that you are using, then we recommend that you use standalone pricing. If your project is using other GKE Enterprise on Google Cloud [components or capabilities](/anthos/deployment-options) , you should enable the entire GKE Enterprise platform. This lets you use all GKE Enterprise features for a single per-vCPU charge.\nThe APIs that you must enable depend on the [Multi Cluster Ingress pricing](/kubernetes-engine/pricing#multi-cluster_ingress) that you use.\n- If the GKE Enterprise API (`anthos.googleapis.com`) is enabled, then your project is billed according to the number of cluster vCPUs and [GKE Enterprise pricing](/anthos/pricing#pricing_table) .\n- If the GKE Enterprise API is disabled, then your project is billed according to the number of backend Multi Cluster Ingress pods in your project.\nYou can change the Multi Cluster Ingress billing model from standalone to GKE Enterprise, or from GKE Enterprise to standalone at any time without impacting Multi Cluster Ingress resources or traffic.\n**Warning:** Don't disable the GKE Enterprise API if there are other active GKE Enterprise components in use in your project or your active components might experience an outage.\nTo enable standalone pricing, perform the following steps:- Confirm that the GKE Enterprise API is disabled in your project:```\ngcloud services list --project=PROJECT_ID | grep anthos.googleapis.com\n```Replace `` with the project ID where your GKE clusters are running.If the output is an empty response, the GKE Enterprise API is disabled in your project and any Multi Cluster Ingress resources are billed using standalone pricing.\n- Enable the required APIs in your project:```\ngcloud services enable \\\u00a0 \u00a0 multiclusteringress.googleapis.com \\\u00a0 \u00a0 gkehub.googleapis.com \\\u00a0 \u00a0 container.googleapis.com \\\u00a0 \u00a0 multiclusterservicediscovery.googleapis.com \\\u00a0 \u00a0 --project=PROJECT_ID\n```\nTo enable GKE Enterprise pricing, enable the required APIs in your project:\n```\ngcloud services enable \\\u00a0 \u00a0 anthos.googleapis.com \\\u00a0 \u00a0 multiclusteringress.googleapis.com \\\u00a0 \u00a0 gkehub.googleapis.com \\\u00a0 \u00a0 container.googleapis.com \\\u00a0 \u00a0 multiclusterservicediscovery.googleapis.com \\\u00a0 \u00a0 --project=PROJECT_ID\n```\nAfter `anthos.googleapis.com` is enabled in your project, any clusters registered to Connect are billed according to [GKE Enterprise pricing](/anthos/pricing#pricing_table) .\n## Deploy clusters\nCreate two GKE clusters named `gke-us` and `gke-eu` in the `europe-west1` and `us-central1` regions.\n- Create the `gke-us` cluster in the `us-central1` region:```\ngcloud container clusters create-auto gke-us \\\u00a0 \u00a0 --region=us-central1 \\\u00a0 \u00a0 --release-channel=stable \\\u00a0 \u00a0 --project=PROJECT_ID\n```Replace `` with your Google Cloud project ID.\n- Create the `gke-eu` cluster in the `europe-west1` region:```\ngcloud container clusters create-auto gke-eu \\\u00a0 \u00a0 --region=europe-west1 \\\u00a0 \u00a0 --release-channel=stable \\\u00a0 \u00a0 --project=PROJECT_ID\n```\nCreate the two clusters with [workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity) enabled.- Create the `gke-us` cluster in the `us-central1` region:```\ngcloud container clusters create gke-us \\\u00a0 \u00a0 --region=us-central1 \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --workload-pool=PROJECT_ID.svc.id.goog \\\u00a0 \u00a0 --release-channel=stable \\\u00a0 \u00a0 --project=PROJECT_ID\n```Replace `` with your Google Cloud project ID.\n- Create the `gke-eu` cluster in the `europe-west1` region:```\ngcloud container clusters create gke-eu \\\u00a0 \u00a0 --region=europe-west1 \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --workload-pool=PROJECT_ID.svc.id.goog \\\u00a0 \u00a0 --release-channel=stable \\\u00a0 \u00a0 --project=PROJECT_ID\n```## Configure cluster credentials\nConfigure credentials for your clusters and rename the cluster contexts to make it easier to switch between clusters when deploying resources.\n- Retrieve the credentials for your clusters:```\ngcloud container clusters get-credentials gke-us \\\u00a0 \u00a0 --region=us-central1 \\\u00a0 \u00a0 --project=PROJECT_IDgcloud container clusters get-credentials gke-eu \\\u00a0 \u00a0 --region=europe-west1 \\\u00a0 \u00a0 --project=PROJECT_ID\n```The credentials are stored locally so that you can use your kubectl client to access the cluster API servers. By default, an auto-generated name is created for the credentials.\n- Rename the cluster contexts:```\nkubectl config rename-context gke_PROJECT_ID_us-central1_gke-us gke-uskubectl config rename-context gke_PROJECT_ID_europe-west1_gke-eu gke-eu\n```## Register clusters to a fleet\nRegister your clusters to your project's fleet as follows.\n**Note:** If you have chosen to enable the entire GKE Enterprise platform, you can also register clusters from the GKE Enterprise pages in the Google Cloud console. Learn more in [Register a GKE cluster to your fleet](/anthos/fleet-management/docs/register/gke#console) .\n- Register your clusters:```\ngcloud container fleet memberships register gke-us \\\u00a0 \u00a0 --gke-cluster us-central1/gke-us \\\u00a0 \u00a0 --enable-workload-identity \\\u00a0 \u00a0 --project=PROJECT_IDgcloud container fleet memberships register gke-eu \\\u00a0 \u00a0 --gke-cluster europe-west1/gke-eu \\\u00a0 \u00a0 --enable-workload-identity \\\u00a0 \u00a0 --project=PROJECT_ID\n```\n- Confirm that your clusters have successfully been registered to the fleet:```\ngcloud container fleet memberships list --project=PROJECT_ID\n```The output is similar to the following:```\nNAME         EXTERNAL_ID\ngke-us        0375c958-38af-11ea-abe9-42010a800191\ngke-eu        d3278b78-38ad-11ea-a846-42010a840114\n```\nAfter you register your clusters, GKE deploys the `gke-mcs-importer` Pod to your cluster.\nYou can learn more about registering clusters in [Register a GKE cluster to your fleet](/anthos/fleet-management/docs/register/gke) .\n## Specify a config cluster\nThe config cluster is a GKE cluster you choose to be the central point of control for Ingress across the member clusters. This cluster must already be registered to the fleet. For more information, see [Config cluster design](/kubernetes-engine/docs/concepts/multi-cluster-ingress#config_cluster_design) .\n**Note:** Even if you have zonal GKE clusters, Multi Cluster Ingress controller is only available in a region. To enable Multi Cluster Ingress, you must specify the region instead of the zone when you use the `--location` parameter.\nEnable Multi Cluster Ingress and select `gke-us` as the config cluster:\n```\ngcloud container fleet ingress enable \\\u00a0 \u00a0 --config-membership=gke-us \\\u00a0 \u00a0 --location=us-central1 \\\u00a0 \u00a0 --project=PROJECT_ID\n```\nThe config cluster takes up to 15 minutes to register. Successful output is similar to the following:\n```\nWaiting for Feature to be created...done.\nWaiting for controller to start...done.\n```\nThe unsuccessful output is similar to the following:\n```\nWaiting for controller to start...failed.\nERROR: (gcloud.container.fleet.ingress.enable) Controller did not start in 2 minutes. Please use the `describe` command to check Feature state for debugging information.\n```\nIf a failure occurred in the previous step, then check the feature state:\n```\ngcloud container fleet ingress describe \\\u00a0 \u00a0 --project=PROJECT_ID\n```\nThe successful output is similar to the following:\n```\ncreateTime: '2021-02-04T14:10:25.102919191Z'\nmembershipStates:\n projects/PROJECT_ID/locations/global/memberships/CLUSTER_NAME:\n state:\n code: ERROR\n description: '...is not a VPC-native GKE Cluster.'\n updateTime: '2021-08-10T13:58:50.298191306Z'\n projects/PROJECT_ID/locations/global/memberships/CLUSTER_NAME:\n state:\n code: OK\n updateTime: '2021-08-10T13:58:08.499505813Z'\n```\nTo learn more about troubleshooting errors with Multi Cluster Ingress, see [Troubleshooting and operations](/kubernetes-engine/docs/how-to/troubleshooting-and-ops) .\n## Shared VPC\nYou can deploy a `MultiClusterIngress` resource for clusters in a Shared VPC network, but all the participating backend GKE clusters must be in the same project. Having GKE clusters in different projects using the same Cloud Load Balancing VIP is not supported.\nIn non-Shared VPC networks, the Multi Cluster Ingress controller manages firewall rules to allow health checks to pass from the load balancer to container workloads.\nIn a Shared VPC network, a host project administrator must manually create the firewall rules for load balancer traffic on behalf of the Multi Cluster Ingress controller.\nThe following command shows the firewall rule that you must create if your clusters are on a Shared VPC network. The source ranges are the ranges that load balancer uses to send traffic to backends. This rule must exist for the operational lifetime of a `MultiClusterIngress` resource.\nIf your clusters are on a Shared VPC network, create the firewall rule:\n```\ngcloud compute firewall-rules create FIREWALL_RULE_NAME \\\u00a0 \u00a0 --project=HOST_PROJECT \\\u00a0 \u00a0 --network=SHARED_VPC \\\u00a0 \u00a0 --direction=INGRESS \\\u00a0 \u00a0 --allow=tcp:0-65535 \\\u00a0 \u00a0 --source-ranges=130.211.0.0/22,35.191.0.0/16\n```\nReplace the following:\n- ``: the name of the new firewall rule that you choose.\n- ``: the ID of the Shared VPC host project.\n- ``: the name of the Shared VPC network.## Known issues\nThis section describes known issues for the Multi Cluster Ingress\n### InvalidValueError for field config_membership\nA known issue prevents the Google Cloud CLI from interacting with Multi Cluster Ingress. This issue was introduced in version 346.0.0 and was fixed in version 348.0.0. We don't recommend using the gcloud CLI versions 346.0.0 and 347.0.0 with Multi Cluster Ingress.\n### Invalid value for field 'resource'\nGoogle Cloud Armor cannot communicate with Multi Cluster Ingress [config clusters](/kubernetes-engine/docs/concepts/multi-cluster-ingress#architecture) running on the following GKE versions:\n- 1.18.19-gke.1400 and later\n- 1.19.10-gke.700 and later\n- 1.20.6-gke.700 and later\nWhen you [configure a Google Cloud Armor security policy](/armor/docs/configure-security-policies) , the following message appears:\n```\nInvalid value for field 'resource': '{\"securityPolicy\": \"global/securityPolicies/\"}': The given policy does not exist\n```\nTo avoid this issue, upgrade your [config cluster](#specifying_a_config_cluster) to version 1.21 or later, or use the following command to update the BackendConfig `CustomResourceDefinition` :\n```\nkubectl patch crd backendconfigs.cloud.google.com --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/versions/1/schema/openAPIV3Schema/properties/spec/properties/securityPolicy\", \"value\":{\"properties\": {\"name\": {\"type\": \"string\"}}, \"required\": [\"name\" ],\"type\": \"object\"}}]'\n```\n## What's next\n- [Deploying Ingress across multiple clusters](/kubernetes-engine/docs/how-to/multi-cluster-ingress) .", "guide": "Google Kubernetes Engine (GKE)"}