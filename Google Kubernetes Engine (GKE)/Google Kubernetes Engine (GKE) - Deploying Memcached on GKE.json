{"title": "Google Kubernetes Engine (GKE) - Deploying Memcached on GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/deploying-memcached-on-kubernetes-engine", "abstract": "# Google Kubernetes Engine (GKE) - Deploying Memcached on GKE\nIn this tutorial, you learn how to deploy a cluster of distributed [Memcached](https://memcached.org/) servers on Google Kubernetes Engine (GKE) using [Kubernetes](https://kubernetes.io/) , [Helm](https://helm.sh) , and [Mcrouter](https://github.com/facebook/mcrouter) . Memcached is a popular open source, multi-purpose caching system. It usually serves as a temporary store for frequently used data to speed up web applications and lighten database loads.", "content": "## Memcached's characteristicsMemcached has two main design goals:- **Simplicity** : Memcached functions like a large [hash table](https://wikipedia.org/wiki/Hash_table) and offers a simple API to store and retrieve arbitrarily shaped objects by key.\n- **Speed** : Memcached holds cache data exclusively in random-access memory (RAM), making data access extremely fast.\nMemcached is a distributed system that allows its hash table's capacity to scale horizontally across a pool of servers. Each Memcached server operates in complete isolation from the other servers in the pool. Therefore, the routing and load balancing between the servers must be done at the client level. Memcached clients apply a [consistent hashing](https://wikipedia.org/wiki/Consistent_hashing) scheme to appropriately select the target servers. This scheme guarantees the following conditions:- The same server is always selected for the same key.\n- Memory usage is evenly balanced between the servers.\n- A minimum number of keys are relocated when the pool of servers is reduced or expanded.\nThe following diagram illustrates at a high level the interaction between a Memcached client and a distributed pool of Memcached servers.## Objectives\n- Learn about some characteristics of Memcached's distributed architecture.\n- Deploy a Memcached service to GKE using Kubernetes and Helm.\n- Deploy Mcrouter, an open source Memcached proxy, to improve the system's performance.\n## CostsIn this document, you use the following billable components of Google Cloud:- Compute Engine\nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . ## Before you begin- Start a Cloud Shell instance. [Open Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n## Deploying a Memcached serviceOne simple way to deploy a Memcached service to GKE is to use a [Helm](https://helm.sh) chart. To proceed with the deployment, follow these steps in Cloud Shell:- Create a new GKE cluster of three nodes:```\ngcloud container clusters create demo-cluster --num-nodes 3 --zone us-central1-f\n``` **Note:** The cluster's zone specified here is arbitrary for the purposes of this tutorial. You can select another zone for your cluster from the [available zones](/compute/docs/regions-zones/regions-zones#available) .\n- Download the `helm` binary archive:```\nHELM_VERSION=3.7.1cd ~wget https://get.helm.sh/helm-v${HELM_VERSION}-linux-amd64.tar.gz\n```\n- Unzip the archive file to your local system:```\nmkdir helm-v${HELM_VERSION}tar zxfv helm-v${HELM_VERSION}-linux-amd64.tar.gz -C helm-v${HELM_VERSION}\n```\n- Add the `helm` binary's directory to your `PATH` environment variable:```\nexport PATH=\"$(echo ~)/helm-v${HELM_VERSION}/linux-amd64:$PATH\"\n```This command makes the `helm` binary discoverable from any directory during the current Cloud Shell session. To make this configuration persist across multiple sessions, add the command to your Cloud Shell user's `~/.bashrc` file.\n- Install a new [Memcached Helm chart](https://artifacthub.io/packages/helm/bitnami/memcached) release with the high-availability architecture:```\nhelm repo add bitnami https://charts.bitnami.com/bitnamihelm install mycache bitnami/memcached --set architecture=\"high-availability\" --set autoscaling.enabled=\"true\"\n```The Memcached Helm chart uses a [StatefulSet controller](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) . One benefit of using a StatefulSet controller is that the pods' names are ordered and predictable. In this case, the names are `mycache-memcached-{0..2}` . This ordering makes it easier for Memcached clients to reference the servers.\n- To see the running pods, run the following command:```\nkubectl get pods\n```The Google Cloud console output looks like this:```\nNAME     READY  STATUS RESTARTS AGE\nmycache-memcached-0 1/1  Running 0   45s\nmycache-memcached-1 1/1  Running 0   35s\nmycache-memcached-2 1/1  Running 0   25s\n``` **Note:** The installation command from the previous step takes a moment to complete. It might take about a minute before all three pods are ready and running.\n## Discovering Memcached service endpointsThe Memcached Helm chart uses a [headless service](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services) . A headless service exposes IP addresses for all of its pods so that they can be individually discovered.- Verify that the deployed service is headless:```\nkubectl get service mycache-memcached -o jsonpath=\"{.spec.clusterIP}\"\n```The output `None` confirms that the service has no `clusterIP` and that it is therefore headless.The service creates a DNS record for a hostname of the form:```\n[SERVICE_NAME].[NAMESPACE].svc.cluster.local\n```In this tutorial, the service name is `mycache-memcached` . Because a namespace was not explicitly defined, the default namespace is used, and therefore the entire host name is `mycache-memcached.default.svc.cluster.local` . This hostname resolves to a set of IP addresses and domains for all three pods exposed by the service. If, in the future, some pods get added to the pool, or old ones get removed, [kube-dns](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/) will automatically update the DNS record.It is the client's responsibility to discover the Memcached service endpoints, as described in the next steps.\n- Retrieve the endpoints' IP addresses:```\nkubectl get endpoints mycache-memcached\n```The output is similar to the following:```\nNAME    ENDPOINTS           AGE\nmycache-memcached 10.36.0.32:11211,10.36.0.33:11211,10.36.1.25:11211 3m\n```Notice that each Memcached pod has a separate IP address, respectively `10.36.0.32` , `10.36.0.33` , and `10.36.1.25` . These IP addresses might differ for your own server instances. Each pod listens to port `11211` , which is Memcached's default port.\n- For an alternative to step 2, perform a DNS inspection by using a programming language like Python:- Start a Python interactive console inside your cluster:```\nkubectl run -it --rm python --image=python:3.10-alpine --restart=Never python\n```\n- In the Python console, run these commands:```\nimport socketprint(socket.gethostbyname_ex('mycache-memcached.default.svc.cluster.local'))exit()\n```The output is similar to the following:```\n('mycache-memcached.default.svc.cluster.local', ['mycache-memcached.default.svc.cluster.local'], ['10.36.0.32', '10.36.0.33', '10.36.1.25'])\n```\n- Test the deployment by opening a `telnet` session with one of the running Memcached servers on port `11211` :```\nkubectl run -it --rm busybox --image=busybox:1.33 --restart=Never telnet mycache-memcached-0.mycache-memcached.default.svc.cluster.local 11211\n```At the `telnet` prompt, run these commands using the [Memcached ASCII protocol](https://github.com/memcached/memcached/blob/master/doc/protocol.txt) :```\nset mykey 0 0 5\nhello\nget mykey\nquit\n```The resulting output is shown here in bold:```\nset mykey 0 0 5\nhello\nSTORED\nget mykey\nVALUE mykey 0 5\nhello\nEND\nquit\n```\n### Implementing the service discovery logicYou are now ready to implement the basic service discovery logic shown in the following diagram.At a high level, the service discovery logic consists of the following steps:- The application queries`kube-dns`for the DNS record of`mycache-memcached.default.svc.cluster.local`.\n- The application retrieves the IP addresses associated with that record.\n- The application instantiates a new Memcached client and provides it with the retrieved IP addresses.\n- The Memcached client's integrated load balancer connects to the Memcached servers at the given IP addresses.\nYou now implement this service discovery logic by using Python:- Deploy a new Python-enabled pod in your cluster and start a shell session inside the pod:```\nkubectl run -it --rm python --image=python:3.10-alpine --restart=Never sh\n```\n- Install the [pymemcache](https://pymemcache.readthedocs.io) library:```\npip install pymemcache\n```\n- Start a Python interactive console by running the `python` command.\n- In the Python console, run these commands:```\nimport socketfrom pymemcache.client.hash import HashClient_, _, ips = socket.gethostbyname_ex('mycache-memcached.default.svc.cluster.local')servers = [(ip, 11211) for ip in ips]client = HashClient(servers, use_pooling=True)client.set('mykey', 'hello')client.get('mykey')\n```The output is as follows:```\nb'hello'\n```The `b` prefix signifies a [bytes literal](https://docs.python.org/3/reference/lexical_analysis.html#string-and-bytes-literals) , which is the format in which Memcached stores data.\n- Exit the Python console:```\nexit()\n```\n- To exit the pod's shell session, press `Control` + `D` .\n## Enabling connection poolingAs your caching needs grow, and the pool scales up to dozens, hundreds, or thousands of Memcached servers, you might run into some limitations. In particular, the large number of open connections from Memcached clients might place a heavy load on the servers, as the following diagram shows.To reduce the number of open connections, you must introduce a proxy to enable connection pooling, as in the following diagram. [Mcrouter](https://github.com/facebook/mcrouter) (pronounced \"mick router\"), a powerful open source Memcached proxy, enables connection pooling. Integrating Mcrouter is seamless, because it uses the standard Memcached ASCII protocol. To a Memcached client, Mcrouter behaves like a normal Memcached server. To a Memcached server, Mcrouter behaves like a normal Memcached client.\nTo deploy Mcrouter, run the following commands in Cloud Shell.- Delete the previously installed `mycache` Helm chart release:```\nhelm delete mycache\n```\n- Deploy new Memcached pods and Mcrouter pods by installing a new [Mcrouter Helm chart](https://github.com/kubernetes/charts/tree/master/stable/mcrouter) release:```\nhelm repo add stable https://charts.helm.sh/stablehelm install mycache stable/mcrouter --set memcached.replicaCount=3\n```The proxy pods are now ready to accept requests from client applications.\n- Test this setup by connecting to one of the proxy pods. Use the `telnet` command on port `5000` , which is Mcrouter's default port.```\nMCROUTER_POD_IP=$(kubectl get pods -l app=mycache-mcrouter -o jsonpath=\"{.items[0].status.podIP}\")kubectl run -it --rm busybox --image=busybox:1.33 --restart=Never telnet $MCROUTER_POD_IP 5000\n```In the `telnet` prompt, run these commands:```\nset anotherkey 0 0 15\nMcrouter is fun\nget anotherkey\nquit\n```The commands set and echo the value of your key.\nYou have now deployed a proxy that enables connection pooling.## Reducing latencyTo increase resilience, it is common practice to use a cluster with multiple nodes. This tutorial uses a cluster with three nodes. However, using multiple nodes also brings the risk of increased latency caused by heavier network traffic between nodes.\n### Colocating proxy podsYou can reduce this risk by connecting client application pods only to a Memcached proxy pod that is on the same node. The following diagram illustrates this configuration.Perform this configuration as follows:- Ensure that each node contains one running proxy pod. A common approach is to deploy the proxy pods with a [DaemonSet controller](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset) . As nodes are added to the cluster, new proxy pods are automatically added to them. As nodes are removed from the cluster, those pods are garbage-collected. In this tutorial, the Mcrouter Helm chart that you deployed earlier [uses a DaemonSet controller](https://github.com/kubernetes/charts/blob/master/stable/mcrouter/templates/daemonset.yaml) by default. So, this step is already complete.\n- Set a`hostPort`value in the proxy container's Kubernetes parameters to make the node listen to that port and redirect traffic to the proxy. In this tutorial, the Mcrouter Helm chart uses this parameter by default for port`5000`. So this step is also already complete.\n- Expose the node name as an environment variable inside the application pods by using the `spec.env` entry and selecting the `spec.nodeName` `fieldRef` value. See more about this method in the [Kubernetes documentation](https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/) .- Deploy some sample application pods:```\ncat <<EOF | kubectl create -f -apiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: sample-applicationspec:\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: sample-application\u00a0 replicas: 9\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: sample-application\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 \u00a0 - name: busybox\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 image: busybox:1.33\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command: [ \"sh\", \"-c\"]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - while true; do sleep 10; done;\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: NODE_NAME\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fieldRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 fieldPath: spec.nodeNameEOF\n```\n- Verify that the node name is exposed, by looking inside one of the sample application pods:```\nPOD=$(kubectl get pods -l app=sample-application -o jsonpath=\"{.items[0].metadata.name}\")kubectl exec -it $POD -- sh -c 'echo $NODE_NAME'\n```This command outputs the node's name in the following form:```\ngke-demo-cluster-default-pool-XXXXXXXX-XXXX\n```\n### Connecting the podsThe sample application pods are now ready to connect to the Mcrouter pod that runs on their respective mutual nodes at port `5000` , which is Mcrouter's default port.- Initiate a connection for one of the pods by opening a `telnet` session:```\nPOD=$(kubectl get pods -l app=sample-application -o jsonpath=\"{.items[0].metadata.name}\")kubectl exec -it $POD -- sh -c 'telnet $NODE_NAME 5000'\n```\n- In the `telnet` prompt, run these commands:```\nget anotherkeyquit\n```Resulting output:```\nMcrouter is fun\n```\nFinally, as an illustration, the following Python code is a sample program that performs this connection by retrieving the `NODE_NAME` variable from the environment and using the `pymemcache` library:\n```\nimport osfrom pymemcache.client.base import ClientNODE_NAME = os.environ['NODE_NAME']client = Client((NODE_NAME, 5000))client.set('some_key', 'some_value')result = client.get('some_key')\n```## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.- Run the following command to delete the GKE cluster:```\ngcloud container clusters delete demo-cluster --zone us-central1-f\n```\n- Optionally, delete the Helm binary:```\ncd ~rm -rf helm-v3.7.1rm helm-v3.7.1-linux-amd64.tar.gz\n```\n## What's next\n- Explore the many other [features](https://github.com/facebook/mcrouter/wiki#features) that Mcrouter offers beyond simple connection pooling, such as failover replicas, reliable delete streams, cold cache warmup, multi-cluster broadcast.\n- Explore the source files of the [Memcached chart](https://github.com/kubernetes/charts/tree/master/stable/memcached) and [Mcrouter chart](https://github.com/kubernetes/charts/tree/master/stable/mcrouter) for more details on the respective Kubernetes configurations.\n- Read about [effective techniques](/appengine/articles/scaling/memcache) for using Memcached on App Engine. Some of them apply to other platforms, such as GKE.", "guide": "Google Kubernetes Engine (GKE)"}