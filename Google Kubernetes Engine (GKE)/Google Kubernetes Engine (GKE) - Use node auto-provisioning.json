{"title": "Google Kubernetes Engine (GKE) - Use node auto-provisioning", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning", "abstract": "# Google Kubernetes Engine (GKE) - Use node auto-provisioning\nThis page explains how to use node auto-provisioning in Standard Google Kubernetes Engine (GKE) clusters. You should already be familiar with the concept of [node auto-provisioning](/kubernetes-engine/docs/concepts/node-auto-provisioning) .\n", "content": "## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.## Requirements\nNode auto-provisioning is available in the following GKE releases:\n- Version 1.11.2-gke.25 and later for zonal clusters.\n- Version 1.12.x and later for regional clusters.\n- Version 1.28 and later for Cloud TPU support.## Enable node auto-provisioning\nYou can enable node auto-provisioning on a cluster with the gcloud CLI or the Google Cloud console.\nNode auto-provisioning has the following resource limitations:\nYou must plan the node IP address range carefully. You can expand the node IP address range after you create a cluster. However, we recommend not to expand the node IP address range after you create the cluster as you must update the firewall rules to include the new range as a source. You can expand the Pod IP address range by using [discontiguous multi-PodCIDR](/kubernetes-engine/docs/how-to/multi-pod-cidr) with node auto-provisioning.\n**Note:** If you disable then re-enable auto-provisioning on your cluster, existing node pools won't have auto-provisioning enabled. To re-enable auto-provisioning for these node pools, you need to [mark individual node pools](#mark_node_auto-provisioned) as auto-provisioned.\nTo enable node auto-provisioning, run the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --min-cpu MINIMUM_CPU \\\n --min-memory MIMIMUM_MEMORY \\\n --max-cpu MAXIMUM_CPU \\\n --max-memory MAXIMUM_MEMORY \\\n --autoprovisioning-scopes=https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring,https://www.googleapis.com/auth/devstorage.read_only\n```\nReplace the following:- ``: the name of the cluster to enable node auto-provisioning.\n- ``: the minimum number of cores in the cluster.\n- ``: the minimum number of gigabytes of memory in the cluster.\n- ``: the maximum number of cores in the cluster.\n- ``: the maximum number of gigabytes of memory in the cluster.\nThe following example enables node auto-provisioning on the `dev-cluster` and allows scaling between a total cluster size of 1 CPU and 1 gigabyte of memory to a maximum of 10 CPU and 64 gigabytes of memory:\n```\ngcloud container clusters update dev-cluster \\\n --enable-autoprovisioning \\\n --min-cpu 1 \\\n --min-memory 1 \\\n --max-cpu 10 \\\n --max-memory 64\n```\nTo enable node auto-provisioning, perform the following steps:- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click the name of the cluster.\n- In the **Automation** section, for **Node auto-provisioning** , click edit **Edit** .\n- Select the **Enable node auto-provisioning** checkbox.\n- Set your desired minimum and maximum CPU and memory usage for the cluster.\n- Click **Save changes** .## Using an auto-provisioning config file\nNode auto-provisioning can be configured by using a YAML configuration file. The configuration file can contain just a single line if it's used to change a single setting. Multiple settings can be specified in a single config file. In this case, all those setting will be changed when the config file is applied.\nSome advanced configurations can only be specified by using a configuration file.\nExample 1: Applying the following configuration file enables [node auto-repair and auto-upgrade](#node_management) for any new node pools created by node auto-provisioning:\n```\nmanagement:\u00a0 autoRepair: true\u00a0 autoUpgrade: true\n```\nExample 2: Applying the following configuration file would change the following settings:\n- Sets [resource limits](/kubernetes-engine/docs/concepts/node-auto-provisioning#resource_limits) for CPU, memory and [GPU](#gpu_limits) . Node auto-provisioning will not create a node if the total size of the cluster exceeds the specified resource limits.\n- Enables [node auto-repair and auto-upgrade](#node_management) for any new node pools created by node auto-provisioning.\n- Enables [Secure boot and integrity monitoring](#node_integrity) for any new node pools created by node auto-provisioning.\n- Sets boot disk size to 100 GB for any new node pools created by node auto-provisioning.\n```\nresourceLimits:\u00a0 - resourceType: 'cpu'\u00a0 \u00a0 minimum: 4\u00a0 \u00a0 maximum: 10\u00a0 - resourceType: 'memory'\u00a0 \u00a0 maximum: 64\u00a0 - resourceType: 'nvidia-tesla-k80'\u00a0 \u00a0 maximum: 4management:\u00a0 autoRepair: true\u00a0 autoUpgrade: trueshieldedInstanceConfig:\u00a0 enableSecureBoot: true\u00a0 enableIntegrityMonitoring: truediskSizeGb: 100\n```\nTo use an auto-provisioning configuration file:\n- Create a file with the desired configuration in a location where the gcloud CLI can access it.\n- Apply the configuration to your cluster by running the following command:```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --autoprovisioning-config-file FILE_NAME\n```Replace the following:- ``: the name of the cluster.\n- ``: the name of the configuration file.\nFor more information, see the [gcloud container clusters update](/sdk/gcloud/reference/container/clusters/update#--autoprovisioning-config-file) documentation.## Auto-provisioning defaults\nNode auto-provisioning looks at Pod requirements in your cluster to determine what type of nodes would best fit those Pods. However, some node pool settings are not directly specified by Pods (for example settings related to node upgrades). You can set default values for those settings, which will be applied to all newly created node pools.\n**Note:** Changing auto-provisioning defaults does not affect any existing node pools, including node pools previously created by node auto-provisioning.\n### Setting the default node image type\nYou can specify the node image type to use for all new auto-provisioned node pools using the gcloud CLI or a [configuration file](/kubernetes-engine/docs/how-to/node-auto-provisioning#config_file) . This setting is only available for GKE cluster version 1.20.6-gke.1800 and later.\nTo set the default node image type, run the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 --enable-autoprovisioning \\\u00a0 --autoprovisioning-image-type IMAGE_TYPE\n```\nReplace the following:- ``: the name of the cluster.\n- `` : the [node image type](/kubernetes-engine/docs/concepts/node-images) , which can be one of the following:- `cos_containerd`: Container-Optimized OS with containerd.\n- `ubuntu_containerd`: Ubuntu with containerd.\n **Warning: ** In GKE version 1.24 and later, Docker-based node image types are not supported. In GKE version 1.23, you also cannot create new node pools with Docker node image types. You must migrate to a containerd node image type. To learn more about this change, see [About the Docker node image deprecation](/kubernetes-engine/docs/deprecations/docker-containerd) .\n **Note:** For `ubuntu` and `ubuntu_containerd` image types, specify `--autoprovisioning-scopes https://www.googleapis.com/auth/devstorage.read_only` in the command.\nFor all new auto-provisioned node pools, you can specify the node image type to use by using a [configuration file](/kubernetes-engine/docs/how-to/node-auto-provisioning#config_file) . The following YAML configuration specifies that for new auto-provisioned node pools, the image type is `cos_containerd` , and has associated resource limits for CPU and memory. You must specify maximum values for CPU and memory to enable auto-provisioning.\n **Note:** For `ubuntu` and `ubuntu_containerd` image types, add `scopes: https://www.googleapis.com/auth/devstorage.read_only` to the configuration file.\n **Warning: ** In GKE version 1.24 and later, Docker-based node image types are not supported. In GKE version 1.23, you also cannot create new node pools with Docker node image types. You must migrate to a containerd node image type. To learn more about this change, see [About the Docker node image deprecation](/kubernetes-engine/docs/deprecations/docker-containerd) .- Save the YAML configuration:```\nresourceLimits:\u00a0 - resourceType: 'cpu'\u00a0 \u00a0 minimum: 4\u00a0 \u00a0 maximum: 10\u00a0 - resourceType: 'memory'\u00a0 \u00a0 maximum: 64imageType: 'cos_containerd'\n```\n- Apply the configuration:```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 --enable-autoprovisioning \\\u00a0 --autoprovisioning-config-file FILE_NAME\n```Replace the following:- ``: the name of the cluster.\n- ``: the name of the configuration file.\n### Setting identity defaults for auto-provisioned node pools\nPermissions for Google Cloud resources are provided by [identities](/iam/docs/overview) .\nYou can specify the default identity (either a service account or one or more scopes) for new auto-provisioned node pools using the gcloud CLI or through a [configuration file](#config_file) .\n**Note:** New node pools do not inherit identities from other node pools. If you do not specify a default identity, workloads running in auto-provisioned node pools cannot access any Google Cloud APIs. For example, if you pull container images from a private Artifact Registry repository, you must add the [https://www.googleapis.com/auth/devstorage.read_only](/artifact-registry/docs/access-control#gke) scope.\nTo specify the default IAM service account used by node auto-provisioning, run the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning --autoprovisioning-service-account=SERVICE_ACCOUNT\n```\nReplace the following:- ``: the name of the cluster.\n- ``: the name of the default service account.\nThe following example sets `test-service-account@google.com` as the default service account on the `dev-cluster` cluster:\n```\ngcloud container clusters update dev-cluster \\\n --enable-autoprovisioning --autoprovisioning-service-account=test-service-account@google.com\n```\nTo specify the default scopes used by node auto-provisioning, run the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning --autoprovisioning-scopes=SCOPE\n```\nReplace the following:- ``: the name of the cluster.\n- ``: the Google Cloud scopes used by auto-provisioned node pools. To specify multiple scopes, separate the scopes by a comma (for example,`` `,` `` `,...`).\nThe following example sets the default scope on the `dev-cluster` cluster to `devstorage.read_only` :\n```\ngcloud container clusters update dev-cluster \\\n --enable-autoprovisioning \\\n --autoprovisioning-scopes=https://www.googleapis.com/auth/pubsub,https://www.googleapis.com/auth/devstorage.read_only\n```\nYou can specify identity default used by node auto-provisioning by using a [configuration file](#config_file) . The following YAML configuration sets IAM service account:\n```\n\u00a0 serviceAccount: SERVICE_ACCOUNT\n```\nReplace `` with the name of the default service account.\nAlternatively, you can use the following YAML configuration to specify default scopes used by node auto-provisioning:\n```\n\u00a0 scopes: SCOPE\n```\nReplace `` with the Google Cloud scope used by auto-provisioned node pools. To specify multiple scopes, separate the scopes by a comma (for example, `` `,` `` `,...` ).\nTo use an auto-provisioning configuration file:- Create a configuration file specifying identity defaults in a location where the gcloud CLI can access it.\n- Apply the configuration to your cluster by running the following command:```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --autoprovisioning-config-file FILE_NAME\n```\nReplace the following:- ``: the name of the cluster.\n- ``: the name of the configuration file.\n### Customer-managed encryption keys (CMEK)\nYou can specify [Customer Managed Encryption Keys (CMEK)](/kubernetes-engine/docs/how-to/using-cmek) used by new auto-provisioned node pools.\n**Note:** You need to [create a key](/kubernetes-engine/docs/how-to/using-cmek#create-key) before you can use it with node auto-provisioning.\nYou can enable customer-managed encryption for boot drives by using a [configuration file](#config_file) . The following YAML configuration sets the CMEK key:\n```\n\u00a0 bootDiskKmsKey: projects/KEY_PROJECT_ID/locations/LOCATION/keyRings/KEY_RING/cryptoKeys/KEY_NAME\n```\nReplace the following:\n- ``: your key project ID.\n- ``: the location of your key ring.\n- ``: the name of your key ring.\n- ``: the name of your key.\nTo use an auto-provisioning configuration file:\n- Create a configuration file specifying a CMEK key in a location where the gcloud CLI can access it.\n- Apply the configuration to your cluster by running the following command:```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --autoprovisioning-config-file FILE_NAME\n```Replace the following:- ``: the name of the cluster.\n- ``: the name of the configuration file.\n### Node integrity\nNode auto-provisioning supports creating node pools with [Secure Boot and Integrity Monitoring](/kubernetes-engine/docs/how-to/shielded-gke-nodes#node_integrity) enabled.\nYou can enable Secure Boot and Integrity Monitority by using a [configuration file](#config_file) . The following YAML configuration enables Secure Boot and disables Integrity Monitoring:\n```\n\u00a0 shieldedInstanceConfig:\u00a0 \u00a0 enableSecureBoot: true\u00a0 \u00a0 enableIntegrityMonitoring: false\n```\nTo use an auto-provisioning configuration file:\n- Copy the configuration above to a file in a location where the gcloud CLI can access it. Edit the values for `enableSecureBoot` and `enableIntegrityMonitoring` . Save the file.\n- Apply the configuration to your cluster by running the following command:```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --autoprovisioning-config-file FILE_NAME\n```Replace the following:- ``: the name of the cluster.\n- ``: the name of the configuration file.\n### Node auto-repair and auto-upgrade\nNode auto-provisioning supports creating node pools with node auto-repair and node auto-upgrade enabled.\n**Warning:** Disabling auto-upgrade when the underlying cluster is enrolled in a [release channel](/kubernetes-engine/docs/concepts/release-channels) can cause a failure in creating auto-provisioned node pools.\nTo enable auto-repair and auto-upgrade for all new auto-provisioned node pools, run the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning --enable-autoprovisioning-autorepair \\\n --enable-autoprovisioning-autoupgrade\n```\nReplace `` with the name of the cluster.\nTo disable auto-repair and auto-upgrade for all new auto-provisioned node pools, run the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning --no-enable-autoprovisioning-autorepair \\\n --no-enable-autoprovisioning-autoupgrade\n```\nReplace `` with the name of the cluster.\nYou can enable or disable node auto-repair and auto-upgrade by using a [configuration file](#config_file) . The following YAML configuration enables auto-repair and disables auto-upgrade:\n```\n\u00a0 management:\u00a0 \u00a0 autoRepair: true\u00a0 \u00a0 autoUpgrade: false\n```\nTo use an auto-provisioning configuration file:- Copy the configuration above to a file in a location where the gcloud CLI can access it. Edit the values for `autoUpgrade` and `autoRepair` . Save the file.\n- Apply the configuration to your cluster by running the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --autoprovisioning-config-file FILE_NAME\n```\nReplace the following:- ``: the name of the cluster.\n- ``: the name of the configuration file.\n### Use surge upgrades for new auto-provisioned node pools\nYou can specify [surge upgrade settings](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#surge) on all new auto-provisioned node pools by using the gcloud CLI or a configuration file. By default, GKE sets the [node upgrade strategy](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies) to [surge upgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#surge) .\n**Note:** Before you begin, make sure you [enable node auto-provisioning on thecluster](/kubernetes-engine/docs/how-to/node-auto-provisioning#enable) .\nTo specify surge upgrade settings for all new auto-provisioned node pools, run the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --autoprovisioning-max-surge-upgrade MAX_SURGE \\\n --autoprovisioning-max-unavailable-upgrade MAX_UNAVAILABLE\n```\nReplace the following:- ``: the name of the cluster.\n- ``: the maximum number of nodes that can be added to the node pool during upgrades.\n- ``: the maximum number of nodes in the node pool that can be simultaneously unavailable during upgrades.\nYou can specify surge upgrade settings for all new auto-provisioned node pools by using a [configuration file](#config_file) like the following:\n```\n\u00a0 upgradeSettings:\u00a0 \u00a0 maxSurgeUpgrade: 1\u00a0 \u00a0 maxUnavailableUpgrade: 2\n```\nTo use an auto-provisioning configuration file:- Copy the configuration above to a file in a location where `gcloud` can access it. Edit the values for `maxSurgeUpgrade` and `maxUnavailableUpgrade` . Save the file.\n- Apply the configuration to your cluster by running the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --autoprovisioning-config-file FILE_NAME\n```\nReplace the following:- ``: the name of the cluster.\n- ``: the name of the configuration file.\nFor more information, see the [gcloud container clusters update](/sdk/gcloud/reference/container/clusters/update#--autoprovisioning-config-file) documentation.\nTo switch back to using surge upgrades for new auto-provisioned node pools, run the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --enable-autoprovisioning-surge-upgrade\n```\nReplace `` with the name of the cluster.\nYou can optionally include the flags for specific settings as in the previous commands. GKE reuses your previous configuration for the upgrade strategy, if it was set.\n### Use blue-green upgrades for new auto-provisioned node pools\nYou can use [blue-green upgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#blue-green-upgrade-strategy) for all new auto-provisioned node pools by using the gcloud CLI. With blue-green upgrades, you can use the default settings, or tune it to optimize for your environment. To learn more about blue-green upgrades, see [Blue-green upgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#blue-green-upgrade-strategy) .\nTo update the node upgrade strategy for any existing auto-provisioned node pool, see [Turn on or off surge upgrade for an existing nodepool](/kubernetes-engine/docs/how-to/node-pool-upgrade-strategies#update-surge-node-pool) and [Updating an existing node pool blue-green upgradestrategy](/kubernetes-engine/docs/how-to/node-pool-upgrade-strategies#update-node-pool-blue-green) .\nThe following variables are used in the commands listed below:\n- ``: the name of the cluster for the node pool.\n- ``: the zone for the cluster.\n- ``: the name of the node pool.\n- ``: the number of nodes in the node pool in each of the cluster's zones.\n- ``: the number of blue nodes to drain in a batch during the blue pool drain phase. Default is one. If it is set to zero, the blue pool drain phase will be skipped.\n- ``: the percentage of blue nodes to drain in a batch during the blue pool drain phase. Must be in the range of [0.0, 1.0].\n- ``: the duration in seconds to wait after each batch drain. Default is zero.\n- ``: the duration in seconds to wait after completing drain of all batches. Default is 3600 seconds.\nThe default settings for blue-green upgrades are:\n- `BATCH_NODE_COUNT`= 1\n- `BATCH_SOAK_DURATION`= 0 seconds\n- `NODE_POOL_SOAK_DURATION`= 3600 seconds (1 hour)The following commands use [gcloud container clustersupdate](/sdk/gcloud/reference/container/clusters/update) to update the node upgrade strategy for new auto-provisioned node pools.\nYou can also use these flags when:\n- using the [gcloud container clusters create](/sdk/gcloud/reference/container/clusters/create) command to create a cluster with node auto-provisioning enabled.\n- using the [gcoud container clusters update](/sdk/gcloud/reference/container/clusters/update) command to [enable node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning#enable) .\nTo update a cluster to use blue-green upgrades with default settings for new auto-provisioned node pools, use this command:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --enable-autoprovisioning-blue-green-upgrade\n```\nYou can update a cluster to use blue-green upgrades with specific settings for new auto-provisioned node pools. These commands can also be used without the `--enable-autoprovisioning-blue-green-upgrade` flag to update the settings.\nThe following command uses `BATCH_NODE_COUNT` to set an absolute node count batch size:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --enable-autoprovisioning-blue-green-upgrade \\\n --autoprovisioning-node-pool-soak-duration=NODE_POOL_SOAK_DURATION \\\n --autoprovisioning-standard-rollout-policy=batch-node-count=BATCH_NODE_COUNT,batch-soak-duration=BATCH_SOAK_DURATION\n```\nYou can also use `BATCH_PERCENT` to set a percentage-based batch size, replacing `batch-node-count` in the last command with `batch-percent` and using a decimal between 0 and 1 (e.g. 25% is `0.25` ). To see how percentage-based batch sizes are set, see [Update a node pool with blue/green upgrade using percentage-based batch sizes](/kubernetes-engine/docs/how-to/node-pool-upgrade-strategies?#update-bg-percentage) .\n### Custom boot disks\nNode auto-provisioning supports creating node pools with [Custom boot disks](/kubernetes-engine/docs/how-to/custom-boot-disks) .\nYou can customize boot disk setting using a [configuration file](#config_file) . GKE reserves a portion of the node boot disk for the kubelet functions. For more information, see [Ephemeral storage backed by node boot disk](/kubernetes-engine/docs/concepts/plan-node-sizes#ephemeral_storage_backed_by_node_boot_disk) .\nThe following YAML configuration causes node auto-provisioning to create node pools with 100 GB SSD disks:\n```\n\u00a0 diskSizeGb: 100\u00a0 diskType: pd-ssd\n```\nSpecify the following:\n- `diskSizeGb`: the size of the disk, specified in GB.\n- `diskType`: the type of disk, which can be one of the following values:- `pd-balanced`(default)\n- `pd-standard`\n- `pd-ssd`. In GKE version 1.22 and earlier, if you specify`pd-ssd`, node auto-provisioning only considers [N1 machine types](/compute/docs/machine-types) when creating node pools.To use an auto-provisioning configuration file:\n- Create a file with desired boot disk configuration in a location where the gcloud CLI can access it.\n- Apply the configuration to your cluster by running the following command:```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --autoprovisioning-config-file FILE_NAME\n```Replace the following:- ``: the name of the cluster.\n- ``: the name of the configuration file.\n## Separating kube-system Pods from your workloads\nAs a cluster administrator, you may want to separate `kube-system` pods from your workloads. This configuration prevents your cluster from having scaling down issues on underutilized nodes that have `kube-system` pods running.\nThe following example shows how you can separate `kube-system` pods from cluster workloads by using a combination of node auto-provisioning and taints tolerations.\n- Create a cluster with a default node pool of `e2-standard-2` VMs:```\ngcloud container clusters create test --machine-type=e2-standard-2\n```\n- Apply a taint on the `default-pool` node pool:```\nkubectl taint nodes -l cloud.google.com/gke-nodepool=default-pool CriticalAddonsOnly=true:NoSchedule\n```The output is similar to the following:```\nnode/gke-test-default-pool-66fd7aed-7xbd taintednode/gke-test-default-pool-66fd7aed-kg1x taintednode/gke-test-default-pool-66fd7aed-ljc7 tainted\n```\n- Enable node auto-provisioning for your cluster:```\ngcloud container clusters update test \\\u00a0 \u00a0 --enable-autoprovisioning \\\u00a0 \u00a0 --min-cpu 1 \\\u00a0 \u00a0 --min-memory 1 \\\u00a0 \u00a0 --max-cpu 10 \\\u00a0 \u00a0 --max-memory 64 \\\u00a0 \u00a0 --autoprovisioning-scopes= \\\u00a0 \u00a0 \u00a0 https://www.googleapis.com/auth/logging.write,\\\u00a0 \u00a0 \u00a0 https://www.googleapis.com/auth/monitoring, \\\u00a0 \u00a0 \u00a0 https://www.googleapis.com/auth/devstorage.read_only\n```Your cluster is able to scale between a total cluster size of 1 CPU and 1 gigabyte of memory to a maximum of 10 CPU and 64 gigabytes of memory.\n- Test this configuration by saving the following sample manifest as `nginx.yaml` :```\napiVersion: v1kind: Podmetadata:\u00a0 name: nginx\u00a0 labels:\u00a0 \u00a0 env: testspec:\u00a0 containers:\u00a0 - name: nginx\u00a0 \u00a0 image: nginx\u00a0 \u00a0 imagePullPolicy: IfNotPresent\u00a0 tolerations:\u00a0 - key: dedicated\u00a0 \u00a0 operator: Equal\u00a0 \u00a0 value: ui-team\u00a0 \u00a0 effect: NoSchedule\u00a0 nodeSelector:\u00a0 \u00a0 dedicated: ui-team\n```This manifest deploys a test workload Pod in the cluster with a `nodeSelector` label and node taint of `dedicated: ui-team` . Without node auto-provisioning, this workload Pod cannot be scheduled since no node pool has the proper label and taints.\n- Apply the manifest to the cluster:```\nkubectl apply -f nginx.yaml\n```The output is similar to the following:```\npod/nginx created\n```\n- See the new node pool that fits the `ui-team` label:```\nkubectl get node --selector=dedicated=ui-team\n```The output is similar to the following:```\nNAME \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0STATUS \u00a0 ROLES \u00a0 \u00a0AGE \u00a0 VERSIONgke-test-nap-e2-medium-14b723z1-19f89fa8-jmhr \u00a0 Ready \u00a0 \u00a0<none> \u00a0 14s \u00a0 v1.21.11-gke.900\n```\nWith node auto-provisioning and taints tolerations, your cluster separates `kube-system` from the workloads Pods.\n## Use accelerators for new auto-provisioned node pools\nYou can enable node auto-provisioning and configure GKE to provision GPU or Cloud TPU accelerators automatically to ensure the capacity required to schedule AI/ML workloads.\n### Configuring GPU limits\nWhen using node auto-provisioning with GPUs, you can set the maximum limit for each GPU type in the cluster by using the gcloud CLI or the Google Cloud console. The GPU limit count is the maximum number of GPUs. For example, a VM with 16 GPUs counts as 16 not 1 for the purpose of this limit. To configure multiple types of GPU, you must use a configuration file.\nTo list the available resourceTypes, run `gcloud compute accelerator-types list` .\n**Note:** CPU and memory limits are also required when setting GPU limits.\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --max-cpu MAXIMUM_CPU \\\n --max-memory MAXIMUM_MEMORY \\\n --min-accelerator type=GPU_TYPE,count=MINIMUM_ACCELERATOR \\\n --max-accelerator type=GPU_TYPE,count=MAXIMUM_ACCELERATOR\n```\nReplace the following:- ``: the name of the cluster.\n- ``: the maximum number of cores in the cluster.\n- ``: the maximum number of gigabytes of memory in the cluster.\n- ``: the [GPU type](/kubernetes-engine/docs/how-to/gpus#overview) .\n- ``: the minimum number of GPU accelerators in the cluster.\n- ``: the maximum number of GPU accelerators in the cluster.\nThe following example sets the GPU limits for the `nvidia-tesla-k80` GPU accelerator type in the `dev-cluster` cluster:\n```\ngcloud container clusters update dev-cluster \\\n --enable-autoprovisioning \\\n --max-cpu 10 \\\n --max-memory 64 \\\n --min-accelerator type=nvidia-tesla-k80,count=1 \\\n --max-accelerator type=nvidia-tesla-k80,count=4\n```\nYou can load limits for multiple types of GPU by using a configuration file. The following YAML configuration configures two different types of GPUs:\n```\n resourceLimits:\n - resourceType: 'cpu'\n  minimum: 4\n  maximum: 10\n - resourceType: 'memory'\n  maximum: 64\n - resourceType: 'nvidia-tesla-k80'\n  maximum: 4\n - resourceType: 'nvidia-tesla-v100'\n  maximum: 2\n```\nTo use an auto-provisioning configuration file:- Copy the configuration above to a file in a location where the gcloud CLI can access it. Edit the values for `cpu` and `memory` . Add as many values for `resourceType` as you need. Save the file.\n- Apply the configuration to your cluster by running the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --autoprovisioning-config-file FILE_NAME\n```\nReplace the following:- ``: the name of the cluster.\n- ``: the name of the configuration file.\nFor more information, see the [gcloud container clusters update](/sdk/gcloud/reference/container/clusters/update#--autoprovisioning-config-file) documentation.\nTo enable node auto-provisioning with GPU resources, perform the following steps:- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click the name of the cluster.\n- In the **Automation** section, for **Node auto-provisioning** , click edit **Edit** .\n- Select the **Enable node auto-provisioning** checkbox.\n- Set your desired minimum and maximum CPU and memory usage for the cluster.\n- Click add **Add resource** .\n- Select the type of GPU (for example, NVIDIA TESLA K80) you wish to add. Set your desired minimum and maximum number of GPUs to add to the cluster.\n- Accept the [limitations](/kubernetes-engine/docs/how-to/gpus#limitations) of GPUs in GKE.\n- Click **Save changes** .\n### Configuring Cloud TPUs\nStarting with GKE version 1.28, you can use node auto-provisioning with Cloud TPUs. However, v5p is not supported on node auto-provisioning and cluster autoscaling. Define the TPU limits on a cluster level and create the TPU node pools. To learn more about how node auto-provisioning works with TPUs, see [Supported machine learning accelerators](/kubernetes-engine/docs/concepts/node-auto-provisioning#tpus) .\nCreate a cluster and configure your Pods to use TPU resources by using the gcloud CLI. To configure multiple types of TPU, you must use a configuration file.\n- Create a cluster and define the TPU limits:```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --enable-autoprovisioning \\\u00a0 \u00a0 [--min-cpu \u00a0MINIMUM_CPU ] \\\u00a0 \u00a0 --max-cpu MAXIMUM_CPU \\\u00a0 \u00a0 [--min-memory MINIMUM_MEMORY ] \\\u00a0 \u00a0 --max-memory MAXIMUM_MEMORY \\\u00a0 \u00a0 [--min-accelerator=type=TPU_TYPE,count= MINIMUM_ACCELERATOR ] \\\u00a0 \u00a0 --max-accelerator=type=TPU_TYPE,count= MAXIMUM_ACCELERATOR\n```Replace the following:- ``: the name of the cluster.\n- ``: the minimum number of vCPUs in the cluster.\n- ``: the maximum number of vCPUs in the cluster.\n- ``:the minimum number of gigabytes of memory in the cluster.\n- ``: the maximum number of gigabytes of memory in the cluster.\n- ``: the type of TPU you choose. Use`tpu-v4-podslice`to select TPU v4. To select TPU v5e with a machine type that begins with`ct5lp-`, use`tpu-v5-lite-podslice`. To select TPU v5e with a machine type that begins with`ct5l-`, use`tpu-v5-lite-device`.\n- ``: the minimum number of TPU chips in the cluster.- Note that using``may block scale down of multi-host TPU slices even if`count`is smaller than the number of TPU chips in the slice.\n- ``: the maximum number of TPU chips in the cluster.- If the Pod configuration requests a multi-host TPU slice, GKE creates such slice atomically. Set the count value high enough to allow the provisioning of all TPU chips of the specified topology. The number of chips in each TPU slice equals the product of the topology. For example, if the topology of the multi-host TPU slice is`2x2x2`, the number of TPU chips equals`8`, therefore the``must be higher than 8.The following example sets the TPU limits for the `ct5lp-hightpu-1t` , `ct5lp-hightpu-4t` , and `ct5lp-hightpu-8t` machine types in the `dev-cluster` cluster. For example, up to ten `ct5lp-hightpu-4t` machines could be provisioned, each with 4 TPU chips, 112 vCPU, and 192 GiB of memory.```\ngcloud container clusters create dev-cluster-inference \\\u00a0 \u00a0 \u00a0 --enable-autoprovisioning \\\u00a0 \u00a0 \u00a0 --min-cpu 0 \\\u00a0 \u00a0 \u00a0 --max-cpu 1120 \\\u00a0 \u00a0 \u00a0 --min-memory 0 \\\u00a0 \u00a0 \u00a0 --max-memory 1920 \\\u00a0 \u00a0 \u00a0 --min-accelerator=type=tpu-v5-lite-podslice,count=0 \\\u00a0 \u00a0 \u00a0 --max-accelerator=type=tpu-v5-lite-podslice,count=40\n```\n- Create a Deployment specification that results in a Pod requesting TPU resources. For example, the following manifest will cause GKE to provision four `ct5lp-hightpu-4t` nodes:```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: tpu-workload\u00a0 labels:\u00a0 \u00a0 app: tpu-workloadspec:\u00a0 replicas: 4\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: nginx-tpu\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: nginx-tpu\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-topology: \u00a02x2\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/reservation-name: my-reservation\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: nginx\u00a0 \u00a0 \u00a0 \u00a0 image: nginx:1.14.2\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 80\n```In the `nodeSelector` field, you define the TPU type, TPU topology, and accelerator count, where:- `cloud.google.com/gke-tpu-accelerator`: Defines the TPU type. For example,`tpu-v4-podslice`.\n- `cloud.google.com/gke-tpu-topology`: Defines the TPU topology, for example`2x2x1`or`4x4x8`.\nTo consume an existing reservation with your workload, specify additional label in the `nodeSelector` field: * `cloud.google.com/reservation-name` : Defines the name of the reservation GKE uses to auto-provision the nodes.Under `limits: google.com/tpu` you define the number of chips per node.\nYou can assign limits for multiple types of TPUs by using a configuration file. The following YAML configuration configures two different types of TPUs:\n```\n resourceLimits:\n - resourceType: 'cpu'\n  maximum: 10000\n - resourceType: 'memory'\n  maximum: 10000\n - resourceType: 'tpu-v4-podslice'\n  maximum: 32\n - resourceType: 'tpu-v5-lite'\n  maximum: 64\n```\nTo use an auto-provisioning configuration file:- Copy the configuration above to a file in a location where the gcloud CLI can access it. Edit the values for `resourceType` and `maximum` . Add as many values for `resourceType` as you need. Save the file.\n- Apply the configuration to your cluster by running the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning \\\n --autoprovisioning-config-file FILE_NAME\n```\nReplace the following:- ``: the name of the cluster.\n- ``: the name of the configuration file.\nFor more information, see the [gcloud container clusters update](/sdk/gcloud/reference/container/clusters/update#--autoprovisioning-config-file) documentation.\n## Node auto-provisioning locations\nYou set the zones where node auto-provisioning can create new node pools. Regional locations are not supported. Zones must belong to the same region as the cluster but are not limited to node locations defined on the cluster level. Changing node auto-provisioning locations doesn't affect any existing node pools.\nTo set locations where node auto-provisioning can create new node pools, use the gcloud CLI or a [configuration file](/kubernetes-engine/docs/how-to/node-auto-provisioning#config_file) .\nRun the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --enable-autoprovisioning --autoprovisioning-locations=ZONE\n```\nReplace the following:- ``: the name of the cluster.\n- ``: the [zone](/compute/docs/regions-zones#available) where node auto-provisioning can create new node pools. To specify multiple zones, separate the zones by a comma (for example,`` `,` `` `,...`).\nTo set locations where node auto-provisioning can create new node pools, you can use a [configuration file](/kubernetes-engine/docs/how-to/node-auto-provisioning#config_file) .\nAdd the following YAML configuration that sets the new node pools location:\n```\n\u00a0 \u00a0 autoprovisioningLocations:\u00a0 \u00a0 \u00a0 - ZONE\n```\nReplace `` with the [zone](/compute/docs/regions-zones#available) where node auto-provisioning can create new node pools. To specify multiple zones, add more zones to the list. Save the file.\nTo use an auto-provisioning configuration file:- Create a configuration file in a location where `gcloud CLI` can access it.\n- Apply the configuration to your cluster:```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --enable-autoprovisioning \\\u00a0 \u00a0 --autoprovisioning-config-file FILE_NAME\n```Replace the following:- ``: the name of the cluster.\n- ``: the path to of the configuration file.\n### Physically closer nodes with compact placement\nStarting in GKE version 1.25, node auto-provisioning supports compact placement policy. With compact placement policy, you can instruct GKE to create node pools in closer proximity with each other within a zone.\nTo define a compact placement policy, add a `nodeSelector` to the Pod specification with the following keys:\n- `cloud.google.com/gke-placement-group` is the identifier you assign for the group of Pods that should run together, in the same compact placement group.\n- `cloud.google.com/machine-family` is the name of the machine family name. For more information, see [the machine families that support compact placement](/kubernetes-engine/docs/how-to/compact-placement#limitations) .\nThe following example sets a compact placement policy with a placement group identifier of `placement-group-1` , and a machine family of `c2` :\n```\napiVersion: v1kind: Podmetadata:\u00a0 ...spec:\u00a0 ...\u00a0 nodeSelector:\u00a0 \u00a0 cloud.google.com/gke-placement-group: placement-group-1\u00a0 \u00a0 cloud.google.com/machine-family: c2\n```\nFor more information, see learn how to [define compact placement for GKE nodes](/kubernetes-engine/docs/how-to/compact-placement) .\n## Disabling node auto-provisioning\nWhen you disable node auto-provisioning for a cluster, node pools are no longer auto-provisioned.\n**Note:** All the node pools are marked as not auto-provisioned when you disable auto-provisioning. Reenabling auto-provisioning does not automatically mark node pools as auto-provisioned.\nTo disable node auto-provisioning for a cluster, run the following command:\n```\ngcloud container clusters update CLUSTER_NAME \\\n --no-enable-autoprovisioning\n```\nReplace `` with the name of your cluster.\nTo disable node auto-provisioning using the Google Cloud console:- Go to the **Google Kubernetes Engine** page in Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click the name of the cluster.\n- In the **Automation** section, for **Node auto-provisioning** , click the edit **Edit** .\n- Clear the **Enable node auto-provisioning** checkbox.\n- Click **Save changes** .## Marking node pool as auto-provisioned\nAfter [enabling node auto-provisioning on the cluster](#enable) , you can specify which node pools are auto-provisioned. An auto-provisioned node pool is automatically deleted when no workloads are using it.\nTo mark a node pool as auto-provisioned, run the following command:\n```\ngcloud container node-pools update NODE_POOL_NAME \\\n --enable-autoprovisioning\n```\nReplace `` with the name of the node pool.\n## Marking node pool as not auto-provisioned\nTo mark a node pool as not auto-provisioned, run the following command:\n```\ngcloud container node-pools update NODE_POOL_NAME \\\n --no-enable-autoprovisioning\n```\nReplace `` with the name of the node pool.\n## Using a custom machine family\nStarting with GKE version 1.19.7-gke.800, you can choose a machine family for your workloads. The [T2D machine family](/compute/docs/general-purpose-machines#t2d_machines) is supported in GKE version 1.22 and later.\nTo choose a machine family for your workloads, perform one of the following tasks:\n- Set the node affinity with the key of`cloud.google.com/machine-family`, operator`In`, and the value being the desired machine family (for example,`n2`).\n- Add a`nodeSelector`with the key of`cloud.google.com/machine-family`and the value being the desired machine family.\nHere is an example that sets the `nodeAffinity` to a machine family of `n2` :\n```\nspec:\u00a0 affinity:\u00a0 \u00a0 nodeAffinity:\u00a0 \u00a0 \u00a0 requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 \u00a0 nodeSelectorTerms:\u00a0 \u00a0 \u00a0 \u00a0 - matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - key: cloud.google.com/machine-family\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - n2\n```\n**Warning:** Node auto-provisioning doesn't support multiple `value` assigned to the node affinity. Make sure you assign only one `value` to the node affinity.\nAfter applying the changes, node auto-provisioning chooses the best node pool with a machine type within the specified machine family. If multiple values are used for the match expression, one value will be chosen arbitrarily.\n## Minimum CPU platform\nNode auto-provisioning supports creating node pools with a [minimum CPU platform](/kubernetes-engine/docs/how-to/min-cpu-platform) specified. You can specify the minimum CPU platform at the workload level (recommended) or at the cluster level.\n## What's next\n- [Learn more about cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) .\n- [Learn more about node pools](/kubernetes-engine/docs/concepts/node-pools) .\n- [Learn more about node upgrade strategies](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies) .", "guide": "Google Kubernetes Engine (GKE)"}