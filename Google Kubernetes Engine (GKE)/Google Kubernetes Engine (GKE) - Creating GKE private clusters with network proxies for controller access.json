{"title": "Google Kubernetes Engine (GKE) - Creating GKE private clusters with network proxies for controller access", "url": "https://cloud.google.com/kubernetes-engine/docs/archive/creating-kubernetes-engine-private-clusters-with-net-proxies?hl=zh-cn", "abstract": "# Google Kubernetes Engine (GKE) - Creating GKE private clusters with network proxies for controller access\nLast reviewed 2019-07-02 UTC\nWhen you create a [GKE private cluster](/kubernetes-engine/docs/how-to/private-clusters) with a private cluster controller endpoint, the cluster's controller node is inaccessible from the public internet, but it needs to be accessible for administration.\nBy default, clusters can access the controller through its private endpoint, and authorized networks can be defined within the VPC network.\nTo access the controller from on-premises or another VPC network, however, requires additional steps. This is because the VPC network that hosts the controller is owned by Google and cannot be accessed from resources connected through another VPC network peering connection, Cloud VPN or Cloud Interconnect.\nTo access the controller from on-premises or from another VPC network connected by Cloud VPN or Cloud Interconnect, [enable route export](/kubernetes-engine/docs/how-to/private-clusters#master-on-prem-routing) from your VPC network to the Google-owned VPC network.\nTo enable access to the controller from another VPC network or from on-premises connected through another VPC network peering (such as in hub-and-spoke designs), create a proxy hosted in authorized IP address space, because VPC network peering is non-transitive.\nThis tutorial shows you how to configure a proxy within your GKE private cluster.\n", "content": "## Objectives\n- Create a GKE private cluster with no external access.\n- Create and deploy a Docker image to run the proxy.\n- Create a Kubernetes Service to access the proxy.\n- Test access to the proxy.\n## Costs\nThis tutorial uses billable components of Google Cloud Platform, including:\n- [Compute Engine](/compute/pricing) \n- [GKE](/kubernetes-engine/pricing) \nYou can use the [pricing calculator](/products/calculator) to generate a cost estimate based on your projected usage.\nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin\n## Setting up your environmentIn this tutorial, you use [Cloud Shell](/shell/docs/overview) to enter commands. Cloud Shell gives you access to the command line in the Google Cloud console, and includes the Google Cloud CLI and other tools that you need to develop in Google Cloud. Cloud Shell appears as a window at the bottom of the Google Cloud console. It can take several minutes to initialize, but the window appears immediately.\nTo use Cloud Shell to set up your environment:- In the Google Cloud console, open Cloud Shell. [OPEN Cloud Shell](https://console.cloud.google.com/?cloudshell=true) \n- Make sure you are working in the project that you created or selected. Replace `[YOUR_PROJECT_ID]` with your Google Cloud project.```\ngcloud config set project [YOUR_PROJECT_ID]export PROJECT_ID=`gcloud config list --format=\"value(core.project)\"`\n```\n- Set the default compute zone. For the purposes of this tutorial, it is `us-central1-c` . If you are deploying to a production environment, deploy to [a region of your choice](/about/locations#products-available-by-location) .```\ngcloud config set compute/region us-central1gcloud config set compute/zone us-central1-cexport REGION=us-central1export ZONE=us-central1-c\n```\n## Creating a VPC network and client VMCreate a VPC network and subnet that will host the resources.- Create a VPC network:```\ngcloud compute networks create k8s-proxy --subnet-mode=custom\n```\n- Create a custom subnet in the newly created VPC network:```\ngcloud compute networks subnets create subnet-cluster \\\n --network=k8s-proxy --range=10.50.0.0/16\n```\n- Create a client VM which you will use to deploy resources in the Kubernetes cluster:```\ngcloud compute instances create --subnet=subnet-cluster \\\n --scopes cloud-platform proxy-temp\n```\n- Save the internal IP address of the newly created instance in an environment variable:```\nexport CLIENT_IP=`gcloud compute instances describe proxy-temp \\\n --format=\"value(networkInterfaces[0].networkIP)\"`\n```\n- Create a firewall rule to allow SSH access to the VPC network:```\ngcloud compute firewall-rules create k8s-proxy-ssh --network k8s-proxy \\\n --allow tcp:22\n```\n## Creating a private clusterNow create a private cluster to use for this tutorial.\nIf you already have a cluster that you prefer to use, you can skip the step for creating the cluster, but you must configure some initial form of access on your client machine.- In Cloud Shell, create a cluster:```\ngcloud container clusters create frobnitz \\\n --master-ipv4-cidr=172.16.0.64/28 \\\n --network k8s-proxy \\\n --subnetwork=subnet-cluster \\\n --enable-ip-alias \\\n --enable-private-nodes \\\n --enable-private-endpoint \\\n --master-authorized-networks $CLIENT_IP/32 \\\n --enable-master-authorized-networks\n```The command creates a GKE private cluster named `frobnitz` with `master-authorized-networks` set to allow only the client machine to have access.\n## Creating the Docker imageUse the following steps to build a Kubernetes API proxy image called `k8s-api-proxy,` which acts as a forward proxy to the Kubernetes API server.- In Cloud Shell, create a directory and change to that directory:```\nmkdir k8s-api-proxy && cd k8s-api-proxy\n```\n- Create the `Dockerfile` . The following configuration creates a container from [Alpine](https://hub.docker.com/_/alpine/) , which is a lightweight container distribution that has a Privoxy proxy. The `Dockerfile` also installs `curl` and `jq` for container initialization, adds the necessary configuration files, exposes port 8118 to GKE internally, and adds a startup script.```\nFROM alpine\nRUN apk add -U curl privoxy jq && \\\n mv /etc/privoxy/templates /etc/privoxy-templates && \\\n rm -rf /var/cache/apk/* /etc/privoxy/* && \\\n mv /etc/privoxy-templates /etc/privoxy/templates\nADD --chown=privoxy:privoxy config \\\n /etc/privoxy/\nADD --chown=privoxy:privoxy k8s-only.action \\\n /etc/privoxy/\nADD --chown=privoxy:privoxy k8s-rewrite-internal.filter \\\n /etc/privoxy/\nADD k8s-api-proxy.sh /\nEXPOSE 8118/tcp\nENTRYPOINT [\"./k8s-api-proxy.sh\"]\n```\n- In the `k8s-api-proxy` directory, create the `config` file and add the following content to it:```\n#config directory\nconfdir /etc/privoxy\n# Allow Kubernetes API access only\nactionsfile /etc/privoxy/k8s-only.action\n# Rewrite https://CLUSTER_IP to https://kubernetes.default\nfilterfile /etc/privoxy/k8s-rewrite-internal.filter\n# Don't show the pod name in errors\nhostname k8s-privoxy\n# Bind to all interfaces, port :8118\nlisten-address :8118\n# User cannot click-through a block\nenforce-blocks 1\n# Allow more than one outbound connection\ntolerate-pipelining 1\n```\n- In the same directory, create the `k8s-only.action` file and add the following content to it. Note that `CLUSTER_IP` will be replaced when `k8s-api-proxy.sh` runs.```\n# Block everything...\n{+block{Not Kubernetes}}\n/\n# ... except the internal k8s endpoint, which you rewrite (see\n# k8s-rewrite-internal.filter).\n{+client-header-filter{k8s-rewrite-internal} -block{Kubernetes}}\nCLUSTER_IP/\n```\n- Create the `k8s-rewrite-internal.filter` file and add the following content to it. Note that `CLUSTER_IP` will be replaced when `k8s-api-proxy.sh` runs.```\nCLIENT-HEADER-FILTER: k8s-rewrite-internal\\\n Rewrite https://CLUSTER_IP/ to https://kubernetes.default/\ns@(CONNECT) CLUSTER_IP:443\\\n (HTTP/\\d\\.\\d)@$1 kubernetes.default:443 $2@ig\n```\n- Create the `k8s-api-proxy.sh` file and add the following content to it.```\n#!/bin/sh\nset -o errexit\nset -o pipefail\nset -o nounset\n# Get the internal cluster IP\nexport TOKEN=$(cat /run/secrets/kubernetes.io/serviceaccount/token)\nINTERNAL_IP=$(curl -H \"Authorization: Bearer $TOKEN\" -k -SsL https://kubernetes.default/api |\njq -r '.serverAddressByClientCIDRs[0].serverAddress')\n# Replace CLUSTER_IP in the rewrite filter and action file\nsed -i \"s/CLUSTER_IP/${INTERNAL_IP}/g\"\\\n /etc/privoxy/k8s-rewrite-internal.filter\nsed -i \"s/CLUSTER_IP/${INTERNAL_IP}/g\"\\\n /etc/privoxy/k8s-only.action\n# Start Privoxy un-daemonized\nprivoxy --no-daemon /etc/privoxy/config\n``` **Note:** For this tutorial, you don't install and verify the certificate of the Kubernetes API server. Instead, you run the `curl` command with the `-k` (insecure) option. In a production environment, you should properly install and verify TLS certificates. For more information, see [manage TLS certificates in Kubernetes clusters](https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/) .\n- Make `k8s-api-proxy.sh` executable:```\nchmod +x k8s-api-proxy.sh\n```\n- Build and push the container to your project.```\ndocker build -t gcr.io/$PROJECT_ID/k8s-api-proxy:0.1 .\ndocker push gcr.io/$PROJECT_ID/k8s-api-proxy:0.1\n```\n## Deploying the image and Service\n- In Cloud Shell, log in to the client VM you created earlier:```\ngcloud compute ssh proxy-temp\n```\n- Install the `kubectl` tool:```\nsudo apt-get install kubectl\n```\n- Save the project ID as an environment variable:```\nexport PROJECT_ID=`gcloud config list --format=\"value(core.project)\"`\n```\n- Get the cluster credentials:```\ngcloud container clusters get-credentials frobnitz \\\n--zone us-central1-c --internal-ip\n```\n- Create a Kubernetes deployment that exposes the container that you just created:```\nkubectl run k8s-api-proxy \\\n --image=gcr.io/$PROJECT_ID/k8s-api-proxy:0.1 \\\n --port=8118\n```\n- Create the `ilb.yaml` file for the internal load balancer and copy the following into it:```\napiVersion: v1kind: Servicemetadata:\u00a0 labels:\u00a0 \u00a0 run: k8s-api-proxy\u00a0 name: k8s-api-proxy\u00a0 namespace: default\u00a0 annotations:\u00a0 \u00a0 cloud.google.com/load-balancer-type: \"Internal\"spec:\u00a0 ports:\u00a0 - port: 8118\u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 targetPort: 8118\u00a0 selector:\u00a0 \u00a0 run: k8s-api-proxy\u00a0 type: LoadBalancer\n```\n- Deploy the internal load balancer:```\nkubectl create -f ilb.yaml\n```\n- Check for the Service and wait for an IP address:```\nkubectl get service/k8s-api-proxy\n```The output will look like the following. When you see an external IP, the proxy is ready.```\nNAME   TYPE   CLUSTER-IP  EXTERNAL-IP PORT(S)   AGE\nk8s-api-proxy LoadBalancer 10.24.13.129 10.24.24.3 8118:30282/TCP 2m\n```The external IP address from this step is your proxy address.\n- Save the IP address of the ILB as an environment variable:```\nexport LB_IP=`kubectl get service/k8s-api-proxy \\\n-o jsonpath='{.status.loadBalancer.ingress[].ip}'`\n```\n- Save the cluster's controller IP address in an environment variable:```\nexport CONTROLLER_IP=`gcloud container clusters describe frobnitz \\\n--zone=us-central1-c \\\n--format=\"get(privateClusterConfig.privateEndpoint)\"`\n```\n- Verify that the proxy is usable by accessing the Kubernetes API through it:```\ncurl -k -x $LB_IP:8118 https://$CONTROLLER_IP/version\n```The output will look like the following (your output might be different):```\n{\n \"major\": \"1\",\n \"minor\": \"15+\",\n \"gitVersion\": \"v1.15.11-gke.5\",\n \"gitCommit\": \"a5bf731ea129336a3cf32c3375317b3a626919d7\",\n \"gitTreeState\": \"clean\",\n \"buildDate\": \"2020-03-31T02:49:49Z\",\n \"goVersion\": \"go1.12.17b4\",\n \"compiler\": \"gc\",\n \"platform\": \"linux/amd64\"\n}\n``` **Note:** For this tutorial, you don't protect the proxy with a valid certificate. Instead, you run the `curl` command with the `-k` (insecure) flag. In a production environment, you should properly install and verify a TLS certificate for the proxy. You can get it signed by the cluster root Certificate Authority (CA). For more information, see [manage TLS certificates in Kubernetes clusters](https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/) \n- Set the `https_proxy` environment variable to the HTTP(S) proxy so that the `kubectl` command can reach the internal load balancer from anywhere:```\nexport https_proxy=$LB_IP:8118\n``` **Note:** After you've set the `https_proxy` environment variable, all traffic uses the proxy you set up earlier. However, the proxy allows traffic only to the Kubernetes API server and blocks all traffic to other destinations. This means internet traffic and traffic to Google properties, including traffic that originates from the `gcloud` command. Therefore, set the `https_proxy` variable only when access to the Kubernetes API server is required. You can reverse the effect of this command by running `unset https_proxy` .\n- Test your proxy and `https_proxy` variable by running the `kubectl` command:```\nkubectl get pods\n```You will get an output that looks like the following, which means that you successfully connected to the Kubernetes API through the proxy:```\nNAME        READY STATUS RESTARTS AGE\nk8s-api-proxy-766c69dd45-mfqf4 1/1  Running 0   6m15s\n```\n- Exit the client VM:```\nexit\n```\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\n- In the Google Cloud console, go to the **Manage resources** page. [Go to Manage resources](https://console.cloud.google.com/iam-admin/projects) \n- In the project list, select the project that you  want to delete, and then click **Delete** .\n- In the dialog, type the project ID, and then click **Shut down** to delete the project.\n### Delete the GKE clusterIf you don't want to delete the project, delete the GKE cluster:\n```\ngcloud container clusters delete frobnitz\n```## What's next\n- [Hardening Your Cluster's Security](/kubernetes-engine/docs/how-to/hardening-your-cluster) to further secure your cluster\n- [Private Google Access](/vpc/docs/private-google-access) to access Google services without a public IP", "guide": "Google Kubernetes Engine (GKE)"}