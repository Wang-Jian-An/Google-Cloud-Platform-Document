{"title": "Google Kubernetes Engine (GKE) - Control plane security", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/control-plane-security", "abstract": "# Google Kubernetes Engine (GKE) - Control plane security\nThis document describes how Google Kubernetes Engine (GKE) secures your cluster control plane components.\nUnder the [Shared Responsibility Model](/kubernetes-engine/docs/concepts/shared-responsibility#googles_responsibilities) , Google manages the GKE control plane components for you. The control plane includes the Kubernetes API server, etcd storage, and other controllers. Google is responsible for securing the control plane, though you might be able to configure certain options based on your requirements. You are responsible for securing your nodes, containers, and Pods.\n", "content": "## Hardened operating system\nGKE control plane components run on [Container-Optimized OS](/container-optimized-os/docs/concepts/features-and-benefits) , which is a security-hardened operating system designed by Google. For a detailed description of the security features built into Container-Optimized OS, see the [Container-Optimized OS security overview](/container-optimized-os/docs/concepts/security) .\n## Architecture and isolation\nIn a GKE cluster, the control plane components run on Compute Engine instances owned by Google, in a Google-managed project. Each instance runs these components for only one cluster.\nFor details about how cluster components authenticate to each other, see [Cluster trust](/kubernetes-engine/docs/concepts/cluster-trust) .\n## Control plane access to your project\nGKE uses a [Google-managed service account](/iam/docs/service-account-types#google-managed) named the Kubernetes Engine Service Agent to actuate cluster resources on your behalf such as nodes, disks, and load balancers. The service account is automatically granted the [Kubernetes Engine Service Agent role](/iam/docs/understanding-roles#service-agents-roles) ( `roles/container.serviceAgent` ) on your project.\nThe Kubernetes Engine Service Agent has the following email address:\n```\nservice-PROJECT_NUMBER@container-engine-robot.iam.gserviceaccount.com\n```\nIn this email address, `` is your [project number](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n## Administrative access to the cluster\nSSH sessions by Google Site Reliability Engineers are audit logged through Google's internal audit infrastructure, which is available for forensics and security response. For more information, see [Administrative access](/security/overview/whitepaper#administrative_access) in the Google Security Whitepaper.\n## etcd security\nIn Google Cloud, customer content is encrypted at the filesystem layer by default. Therefore, disks that host etcd storage for GKE clusters are encrypted at the filesystem layer. For more information, see [Encryption at rest](/security/encryption/default-encryption) .\netcd listens on two TCP ports. Port 2379 is for etcd clients, like the Kubernetes API server. Port 2379 is bound to the local loopback network interface, so it is only accessible from the VM that is running the Kubernetes API server. Port 2380 is for server-to-server communication. Traffic on port 2380 is encrypted by mutual TLS. That is, each server must prove its identity to the other. In a regional cluster, communication between etcd servers to establish a quorum is encrypted by [mutual TLS](https://www.wikipedia.org/wiki/Mutual_authentication) .\n## Certificate authority and cluster trust\nEach cluster has its own [root certificate authority (CA)](https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster) . An internal Google service manages root keys for this CA. Each cluster also has its own CA for etcd. Root keys for the etcd CA are distributed to the metadata of the VMs that run the Kubernetes API server. Communication between nodes and the Kubernetes API server is protected by TLS. For more information, see [Cluster trust](/kubernetes-engine/docs/concepts/cluster-trust) .\n## Vulnerability and patch management\nGKE adheres to Google standards for testing, qualifying, and gradually rolling out changes to the control plane. This minimizes the risk of a control plane component becoming unavailable. GKE adheres to a [service level agreement](/kubernetes-engine/sla) that defines many aspects of availability.\nGKE control plane components are managed by a team of Google site reliability engineers, and are kept up to date with the latest security patches. This includes patches to the host operating system, Kubernetes components, and containers running on the control plane VMs.\nGKE applies new kernel, OS, and Kubernetes-level fixes promptly to control plane VMs. When these contain fixes for known vulnerabilities, additional information is available in the GKE [security bulletins](/kubernetes-engine/docs/security-bulletins) . GKE scans all Kubernetes system and GKE-specific containers for vulnerabilities using [Artifact Analysis](/container-analysis/docs/get-image-vulnerabilities) , and keeps the containers patched, benefitting the whole Kubernetes ecosystem.\nGoogle engineers participate in finding, fixing, and disclosing Kubernetes security bugs. Google also pays external security researchers, through the [Google-wide vulnerability reward program](https://bughunters.google.com/about/rules/6625378258649088) , to look for security bugs. In some cases, such as the [dnsmasq vulnerability](https://security.googleblog.com/2017/10/behind-masq-yet-more-dns-and-dhcp.html) in October 2017, GKE was able to patch all running clusters before the vulnerability became public.\n## What you can see\nThe security features discussed previously in this topic are managed by Google. This section and the following section discuss security features that you can monitor and configure.\n[Audit logging](/kubernetes-engine/docs/how-to/audit-logging) is enabled by default for clusters created since GKE version 1.8.3. This provides a detailed record, available in Google Cloud Observability, of calls made to the Kubernetes API server. You can view the log entries in the [Logs Explorer](https://console.cloud.google.com/logs/query) in the Google Cloud console. You can also use BigQuery to view and analyze these logs.\n## What you can configure\nBy default, the Kubernetes API server uses a public IP address. You can protect the Kubernetes API server by using [authorized networks](/kubernetes-engine/docs/how-to/authorized-networks) and [private clusters](/kubernetes-engine/docs/how-to/private-clusters) , which allow you to assign a private IP address to the Kubernetes API server and disable access on the public IP address.\nYou can handle cluster authentication in GKE by using IAM as the identity provider. For enhanced authentication security, basic authentication and client certificate issuance are disabled by default for clusters created with GKE version 1.12 and later.\nYou can enhance the security of your control plane by doing [credential rotation](/kubernetes-engine/docs/how-to/credential-rotation) on a regular basis. When credential rotation is initiated, the TLS certificates and cluster certificate authority are rotated automatically. GKE also rotates the IP address of your Kubernetes API server. For more information, see [Role-Based Access Control (RBAC)](/kubernetes-engine/docs/how-to/role-based-access-control) and [Credential rotation](/kubernetes-engine/docs/how-to/credential-rotation) .\n## What's next\n- [Security overview](/kubernetes-engine/docs/concepts/security-overview) \n- [Container-Optimized OS security overview](/container-optimized-os/docs/concepts/security) \n- [Hardening your cluster's security](/kubernetes-engine/docs/how-to/hardening-your-cluster)", "guide": "Google Kubernetes Engine (GKE)"}