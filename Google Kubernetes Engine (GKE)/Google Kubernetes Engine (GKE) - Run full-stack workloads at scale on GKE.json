{"title": "Google Kubernetes Engine (GKE) - Run full-stack workloads at scale on GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/full-stack-scale", "abstract": "# Google Kubernetes Engine (GKE) - Run full-stack workloads at scale on GKE\nThis tutorial provides instructions for working with Helm version 3.9.3, PostgreSQL version 10.0.1, and the Locust load testing tool. The instructions might not represent newer versions of the app. For more information, refer to the app-specific documentation:- [Helm](https://helm.sh/docs/) \n- [PostgreSQL](https://www.postgresql.org/docs/) \n- [Locust](https://docs.locust.io/en/stable/) \nThis tutorial shows you how to run a web application that is backed by a highly-available relational database at scale in Google Kubernetes Engine (GKE).\nThe sample application used in this tutorial is Bank of Anthos, an HTTP-based web application that simulates a bank's payment processing network. Bank of Anthos uses multiple services to function. This tutorial focuses on the website frontend and the relational PostgreSQL databases that backs the Bank of Anthos services. To learn more about Bank of Anthos, including its architecture and the services it deploys, refer to [Bank of Anthos on GitHub](https://github.com/GoogleCloudPlatform/bank-of-anthos) .", "content": "## Objectives\n- Create and configure a GKE cluster.\n- Deploy a sample web application and a highly-available PostgreSQL database.\n- Configure autoscaling of the web application and the database.\n- Simulate spikes in traffic using a load generator.\n- Observe how the services scale up and down.\n## CostsIn this document, you use the following billable components of Google Cloud:- [ GKE Autopilot](/kubernetes-engine/pricing#autopilot_mode) \n- [ Internal passthrough Network Load Balancer](/vpc/pricing#lb) \n- [ Cloud Trace](/stackdriver/pricing#trace-costs) \nTo generate a cost estimate based on your projected usage,  use the [pricing calculator](/products/calculator) . \nWhen you finish the tasks that are described in this document, you can avoid continued billing by deleting the resources that you created. For more information, see [Clean up](#clean-up) .## Before you begin- Install the [Helm CLI](https://helm.sh/docs/intro/install/#from-script) .\n## Prepare the environment\n- Clone the sample repository used in this tutorial:```\ngit clone https://github.com/GoogleCloudPlatform/bank-of-anthos.gitcd bank-of-anthos/\n```\n- Set environment variables:```\nPROJECT_ID=PROJECT_IDGSA_NAME=bank-of-anthosGSA_EMAIL=bank-of-anthos@${PROJECT_ID}.iam.gserviceaccount.comKSA_NAME=default\n```Replace `` with your Google Cloud project ID.\n## Set up the cluster and service accounts\n- Create a cluster:```\ngcloud container clusters create-auto bank-of-anthos --region=us-central1\n```The cluster might take up to five minutes to start.\n- Create an IAM service account:```\ngcloud iam service-accounts create bank-of-anthos\n```\n- Grant access to the IAM service account:```\ngcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 --role roles/cloudtrace.agent \\\u00a0 --member \"serviceAccount:bank-of-anthos@PROJECT_ID.iam.gserviceaccount.com\"gcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 --role roles/monitoring.metricWriter \\\u00a0 --member \"serviceAccount:bank-of-anthos@PROJECT_ID.iam.gserviceaccount.com\"gcloud iam service-accounts add-iam-policy-binding \"bank-of-anthos@PROJECT_ID.iam.gserviceaccount.com\" \\\u00a0 --role roles/iam.workloadIdentityUser \\\u00a0 --member \"serviceAccount:PROJECT_ID.svc.id.goog[default/default]\"\n```This step grants the following access:- `roles/cloudtrace.agent`: Write trace data such as latency information to Trace.\n- `roles/monitoring.metricWriter`: Write metrics to Cloud Monitoring.\n- `roles/iam.workloadIdentityUser`: Allow a Kubernetes service account to use workload identity federation for GKE to act as the IAM service account.\n- Configure the `default` Kubernetes service account in the `default` namespace to act as the IAM service account that you created:```\nkubectl annotate serviceaccount default \\\u00a0 \u00a0 iam.gke.io/gcp-service-account=bank-of-anthos@PROJECT_ID.iam.gserviceaccount.com\n```This allows Pods that use the `default` Kubernetes service account in the `default` namespace to access the same Google Cloud resources as the IAM service account.\n## Deploy Bank of Anthos and PostgreSQLIn this section, you install Bank of Anthos and a PostgreSQL database in highly-available (HA) mode, which lets you autoscale replicas of the database server. If you want to view the scripts, Helm chart, and Kubernetes manifests used in this section, check the [Bank of Anthos repository on GitHub](https://github.com/GoogleCloudPlatform/bank-of-anthos/tree/main/extras/postgres-hpa) .- Deploy the database schema and a data definition language (DDL) script:```\nkubectl create configmap initdb \\\u00a0 \u00a0 --from-file=src/accounts/accounts-db/initdb/0-accounts-schema.sql \\\u00a0 \u00a0 --from-file=src/accounts/accounts-db/initdb/1-load-testdata.sql \\\u00a0 \u00a0 --from-file=src/ledger/ledger-db/initdb/0_init_tables.sql \\\u00a0 \u00a0 --from-file=src/ledger/ledger-db/initdb/1_create_transactions.sh\n```\n- Install PostgreSQL using the sample Helm chart:```\nhelm repo add bitnami https://charts.bitnami.com/bitnamihelm install accounts-db bitnami/postgresql-ha \\\u00a0 \u00a0 --version 10.0.1 \\\u00a0 \u00a0 --values extras/postgres-hpa/helm-postgres-ha/values.yaml \\\u00a0 \u00a0 --set=\"postgresql.initdbScriptsCM=initdb\" \\\u00a0 \u00a0 --set=\"postgresql.replicaCount=1\" \\\u00a0 \u00a0 --wait\n```This command creates a PostgreSQL cluster with a starting replica count of 1. Later in this tutorial, you'll scale the cluster based on incoming connections. This operation might take ten minutes or more to complete.\n- Deploy Bank of Anthos:```\nkubectl apply -f extras/jwt/jwt-secret.yamlkubectl apply -f extras/postgres-hpa/kubernetes-manifests\n```This operation might take a few minutes to complete.\n## Checkpoint: Validate your setup\n- Check that all Bank of Anthos Pods are running:```\nkubectl get pods\n```The output is similar to the following:```\nNAME         READY STATUS\naccounts-db-pgpool-57ffc9d685-c7xs8 3/3  Running\naccounts-db-postgresql-0    1/1  Running\nbalancereader-57b59769f8-xvp5k  1/1  Running\ncontacts-54f59bb669-mgsqc    1/1  Running\nfrontend-6f7fdc5b65-h48rs    1/1  Running\nledgerwriter-cd74db4cd-jdqql   1/1  Running\npgpool-operator-5f678457cd-cwbhs  1/1  Running\ntransactionhistory-5b9b56b5c6-sz9qz 1/1  Running\nuserservice-f45b46b49-fj7vm   1/1  Running\n```\n- Check that you can access the website frontend:- Get the external IP address of the `frontend` service:```\nkubectl get ingress frontend\n```The output is similar to the following:```\nNAME  CLASS HOSTS ADDRESS   PORTS AGE\nfrontend <none> *  203.0.113.9  80  12m\n```\n- In a browser, go to the external IP address. The Bank of Anthos sign in page displays. If you're curious, explore the application.If you get a 404 error, wait a few minutes for the microservices to provision and try again.## Autoscale the web app and PostgreSQL databaseGKE Autopilot autoscales the cluster compute resources based on the number of workloads in the cluster. To automatically scale the number of Pods in the cluster based on resource metrics, you must implement Kubernetes [horizontal Pod autoscaling](/kubernetes-engine/docs/concepts/horizontalpodautoscaler) . You can use the built-in Kubernetes CPU and memory metrics or you can use custom metrics such as HTTP requests per second or the quantity of SELECT statements, taken from Cloud Monitoring.\nIn this section, you do the following:- Configure horizontal Pod autoscaling for the Bank of Anthos microservices using both built-in metrics and custom metrics.\n- Simulate load to the Bank of Anthos application to trigger autoscaling events.\n- Observe how the number of Pods and the nodes in your cluster automatically scale up and down in response to your load.\n### Set up custom metrics collectionTo read custom metrics from Monitoring, you must deploy the [Custom Metrics - Stackdriver Adapter](https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/custom-metrics-stackdriver-adapter) adapter in your cluster.- Deploy the adapter:```\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-stackdriver/master/custom-metrics-stackdriver-adapter/deploy/production/adapter.yaml\n```\n- Configure the adapter to use workload identity federation for GKE to get metrics:- Configure the IAM service account:```\ngcloud projects add-iam-policy-binding PROJECT_ID \\\u00a0 \u00a0 --member \"serviceAccount:bank-of-anthos@PROJECT_ID.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role roles/monitoring.viewergcloud iam service-accounts add-iam-policy-binding bank-of-anthos@PROJECT_ID.iam.gserviceaccount.com \\\u00a0 \u00a0 --role roles/iam.workloadIdentityUser \\\u00a0 \u00a0 --member \"serviceAccount:PROJECT_ID.svc.id.goog[custom-metrics/custom-metrics-stackdriver-adapter]\"\n```\n- Annotate the Kubernetes service account that the adapter uses:```\nkubectl annotate serviceaccount custom-metrics-stackdriver-adapter \\\u00a0 \u00a0 --namespace=custom-metrics \\\u00a0 iam.gke.io/gcp-service-account=bank-of-anthos@PROJECT_ID.iam.gserviceaccount.com\n```\n- Restart the adapter Deployment to propagate the changes:```\nkubectl rollout restart deployment custom-metrics-stackdriver-adapter \\\u00a0 \u00a0 --namespace=custom-metrics\n```\n### Configure autoscaling for the databaseWhen you [deployed Bank of Anthos and PostgreSQL](#deploy-boa-postgres) earlier in this tutorial,, you deployed the database as a StatefulSet with one primary read/write replica to handle all incoming SQL statements. In this section, you configure horizontal Pod autoscaling to add new standby read-only replicas to handle incoming SELECT statements. A good way to reduce the load on each replica is to distribute SELECT statements, which are read operations. The PostgreSQL deployment includes a tool named `Pgpool-II` that achieves this load balancing and improves the system's throughput.\nPostgreSQL exports the SELECT statement metric as a [Prometheus metric](https://prometheus.io/docs/concepts/metric_types/) . You'll use a lightweight metrics exporter named `prometheus-to-sd` to send these metrics to Cloud Monitoring in a supported format.- Review the `HorizontalPodAutoscaler` object: [  extras/postgres-hpa/hpa/postgresql-hpa.yaml ](https://github.com/GoogleCloudPlatform/bank-of-anthos/blob/HEAD/extras/postgres-hpa/hpa/postgresql-hpa.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/bank-of-anthos/blob/HEAD/extras/postgres-hpa/hpa/postgresql-hpa.yaml) ```\n# Copyright 2022 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 \u00a0http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.---apiVersion: autoscaling/v2kind: HorizontalPodAutoscalermetadata:\u00a0 name: accounts-db-postgresqlspec:\u00a0 behavior:\u00a0 \u00a0 scaleUp:\u00a0 \u00a0 \u00a0 stabilizationWindowSeconds: 0\u00a0 \u00a0 \u00a0 policies:\u00a0 \u00a0 \u00a0 - type: Percent\u00a0 \u00a0 \u00a0 \u00a0 value: 100\u00a0 \u00a0 \u00a0 \u00a0 periodSeconds: 5\u00a0 \u00a0 \u00a0 selectPolicy: Max\u00a0 scaleTargetRef:\u00a0 \u00a0 apiVersion: apps/v1\u00a0 \u00a0 kind: StatefulSet\u00a0 \u00a0 name: accounts-db-postgresql\u00a0 minReplicas: 1\u00a0 maxReplicas: 5\u00a0 metrics:\u00a0 - type: External\u00a0 \u00a0 external:\u00a0 \u00a0 \u00a0 metric:\u00a0 \u00a0 \u00a0 \u00a0 name: custom.googleapis.com|mypgpool|pgpool2_pool_backend_stats_select_cnt\u00a0 \u00a0 \u00a0 target:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: AverageValue\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 averageValue: \"15\"\n```This manifest does the following:- Sets the maximum number of replicas during a scale-up to`5`.\n- Sets the minimum number of during a scale-down to`1`.\n- Uses an external metric to make scaling decisions. In this sample, the metric is the number of SELECT statements. A scale-up event occurs if the incoming SELECT statement count surpasses 15.\n- Apply the manifest to the cluster:```\nkubectl apply -f extras/postgres-hpa/hpa/postgresql-hpa.yaml\n```\n### Configure autoscaling for the web interfaceIn [Deploy Bank of Anthos and PostgreSQL](#deploy-boa-postgres) , you deployed the Bank of Anthos web interface. When the number of users increases, the `userservice` Service consumes more CPU resources. In this section, you configure horizontal Pod autoscaling for the `userservice` Deployment when the existing Pods use more than 60% of their requested CPU, and for the `frontend` Deployment when the number of incoming HTTP requests to the load balancer is more than 5 per second.\n- Review the `HorizontalPodAutoscaler` manifest for the `userservice` Deployment: [  extras/postgres-hpa/hpa/userservice.yaml ](https://github.com/GoogleCloudPlatform/bank-of-anthos/blob/HEAD/extras/postgres-hpa/hpa/userservice.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/bank-of-anthos/blob/HEAD/extras/postgres-hpa/hpa/userservice.yaml) ```\n# Copyright 2022 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 \u00a0http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.---apiVersion: autoscaling/v2kind: HorizontalPodAutoscalermetadata:\u00a0 name: userservicespec:\u00a0 behavior:\u00a0 \u00a0 scaleUp:\u00a0 \u00a0 \u00a0 stabilizationWindowSeconds: 0\u00a0 \u00a0 \u00a0 policies:\u00a0 \u00a0 \u00a0 \u00a0 - type: Percent\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: 100\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 periodSeconds: 5\u00a0 \u00a0 \u00a0 selectPolicy: Max\u00a0 scaleTargetRef:\u00a0 \u00a0 apiVersion: apps/v1\u00a0 \u00a0 kind: Deployment\u00a0 \u00a0 name: userservice\u00a0 minReplicas: 5\u00a0 maxReplicas: 50\u00a0 metrics:\u00a0 \u00a0 - type: Resource\u00a0 \u00a0 \u00a0 resource:\u00a0 \u00a0 \u00a0 \u00a0 name: cpu\u00a0 \u00a0 \u00a0 \u00a0 target:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: Utilization\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 averageUtilization: 60\n```This manifest does the following:- Sets the maximum number of replicas during a scale-up to`50`.\n- Sets the minimum number of during a scale-down to`5`.\n- Uses a built-in Kubernetes metric to make scaling decisions. In this sample, the metric is CPU utilization, and the target utilization is 60%, which avoids both over- and under-utilization.\n- Apply the manifest to the cluster:```\nkubectl apply -f extras/postgres-hpa/hpa/userservice.yaml\n```\n- Review the `HorizontalPodAutoscaler` manifest for the `userservice` Deployment: [  extras/postgres-hpa/hpa/frontend.yaml ](https://github.com/GoogleCloudPlatform/bank-of-anthos/blob/HEAD/extras/postgres-hpa/hpa/frontend.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/bank-of-anthos/blob/HEAD/extras/postgres-hpa/hpa/frontend.yaml) ```\n# Copyright 2022 Google LLC\n## Licensed under the Apache License, Version 2.0 (the \"License\");# you may not use this file except in compliance with the License.# You may obtain a copy of the License at\n## \u00a0 \u00a0 \u00a0http://www.apache.org/licenses/LICENSE-2.0\n## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an \"AS IS\" BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.---apiVersion: autoscaling/v2kind: HorizontalPodAutoscalermetadata:\u00a0 name: frontendspec:\u00a0 behavior:\u00a0 \u00a0 scaleUp:\u00a0 \u00a0 \u00a0 stabilizationWindowSeconds: 0\u00a0 \u00a0 \u00a0 policies:\u00a0 \u00a0 \u00a0 \u00a0 - type: Percent\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: 100\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 periodSeconds: 5\u00a0 \u00a0 \u00a0 selectPolicy: Max\u00a0 scaleTargetRef:\u00a0 \u00a0 apiVersion: apps/v1\u00a0 \u00a0 kind: Deployment\u00a0 \u00a0 name: frontend\u00a0 minReplicas: 5\u00a0 maxReplicas: 25\u00a0 metrics:\u00a0 \u00a0 - type: External\u00a0 \u00a0 \u00a0 external:\u00a0 \u00a0 \u00a0 \u00a0 metric:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: loadbalancing.googleapis.com|https|request_count\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 selector:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resource.labels.forwarding_rule_name: FORWARDING_RULE_NAME\u00a0 \u00a0 \u00a0 \u00a0 target:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 type: AverageValue\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 averageValue: \"5\"\n```This manifest uses the following fields:- `spec.scaleTargetRef`: The Kubernetes resource to scale.\n- `spec.minReplicas`: The minimum number of replicas, which is`5`in this sample.\n- `spec.maxReplicas`: The maximum number of replicas, which is`25`in this sample.\n- `spec.metrics.*`: The metric to use. In this sample, this is the number of HTTP requests per second, which is a custom metric from Cloud Monitoring provided by the adapter that you deployed.\n- `spec.metrics.external.metric.selector.matchLabels`: The specific resource label to filter when autoscaling.\n- Find the name of the forwarding rule from the load balancer to the `frontend` Deployment:```\nexport FW_RULE=$(kubectl get ingress frontend -o=jsonpath='{.metadata.annotations.ingress\\.kubernetes\\.io/forwarding-rule}')echo $FW_RULE\n```The output is similar to the following:```\nk8s2-fr-j76hrtv4-default-frontend-wvvf7381\n```\n- Add your forwarding rule to the manifest:```\nsed -i \"s/FORWARDING_RULE_NAME/$FW_RULE/g\" \"extras/postgres-hpa/hpa/frontend.yaml\"\n```This command replaces `` `FORWARDING_RULE_NAME` with your saved forwarding rule.\n- Apply the manifest to the cluster:```\nkubectl apply -f extras/postgres-hpa/hpa/frontend.yaml\n```\n## Checkpoint: Validate autoscaling setupGet the state of your `HorizontalPodAutoscaler` resources:\n```\nkubectl get hpa\n```\nThe output is similar to the following:\n```\nNAME      REFERENCE       TARGETS    MINPODS MAXPODS REPLICAS AGE\naccounts-db-postgresql StatefulSet/accounts-db-postgresql 10905m/15 (avg)  1   5   2   5m2s\ncontacts     Deployment/contacts     1%/70%    1   5   1   11m\nfrontend     Deployment/frontend     <unknown>/5 (avg) 5   25  1   34s\nuserservice    Deployment/userservice    0%/60%    5   50  5   4m56s\n```\nAt this point, you've set up your application and configured autoscaling. Your frontend and database can now scale based on the metrics that you provided.## Simulate load and observe GKE scalingBank of Anthos includes a `loadgenerator` Service that lets you simulate traffic to test your application scaling under load. In this section, you'll deploy the `loadgenerator` Service, generate a load, and observe the resulting scaling.\n### Deploy the load testing generator\n- Create an environment variable with the IP address of the Bank of Anthos load balancer:```\nexport LB_IP=$(kubectl get ingress frontend -o=jsonpath='{.status.loadBalancer.ingress[0].ip}')echo $LB_IP\n```The output is similar to the following:```\n203.0.113.9\n```\n- Add the IP address of the load balancer to the manifest:```\nsed -i \"s/FRONTEND_IP_ADDRESS/$LB_IP/g\" \"extras/postgres-hpa/loadgenerator.yaml\"\n```\n- Apply the manifest to the cluster:```\nkubectl apply -f \u00a0extras/postgres-hpa/loadgenerator.yaml\n```\nThe load generator begins adding one user every second, up to 250 users.\n### Simulate loadIn this section, you use a load generator to simulate spikes in traffic and observe your replica count and node count scale up to accommodate the increased load over time. You then end the test and observe the replica and node count scale down in response.- Expose the load generator web interface locally:```\nkubectl port-forward svc/loadgenerator 8080\n```If you see an error message, try again when the Pod is running.\n- In a browser, open the load generator web interface.- If you're using a local shell, open a browser and go to http://127.0.0.1:8080.\n- If you're using Cloud Shell, clickpreview **Web preview** , and then click **Preview on port 8080** .\n- Click the **Charts** tab to observe performance over time.\n- Open a new terminal window and watch the replica count of your horizontal Pod autoscalers:```\nkubectl get hpa -w\n```The number of replicas increases as the load increases. The scaleup might take approximately ten minutes.```\nNAME      REFERENCE       TARGETS   MINPODS MAXPODS REPLICAS\naccounts-db-postgresql StatefulSet/accounts-db-postgresql 8326m/15 (avg) 1   5   5\ncontacts     Deployment/contacts     51%/70%   1   5   2\nfrontend     Deployment/frontend     5200m/5 (avg) 5   25  13\nuserservice    Deployment/userservice    71%/60%   5   50  17\n```\n- Open another terminal window and check the number of nodes in the cluster:```\ngcloud container clusters list \\\u00a0 \u00a0 --filter='name=bank-of-anthos' \\\u00a0 \u00a0 --format='table(name, currentMasterVersion, currentNodeVersion, currentNodeCount)' \\\u00a0 \u00a0 --region=\"us-central1\"\n```\n- The number of nodes increased from the starting quantity of three nodes to accommodate the new replicas.\n- Open the load generator interface and click **Stop** to end the test.\n- Check the replica count and node count again and observe as the numbers reduce with the reduced load. The scale down might take some time, because the default stabilization window for replicas in the Kubernetes `HorizontalPodAutoscaler` resource is five minutes. For more information, refer to [Stabilization window](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#stabilization-window) .\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete individual resourcesGoogle Cloud creates resources, such as load balancers, based on the Kubernetes objects that you create. To delete all the resources in this tutorial, do the following:- Delete the sample Kubernetes resources:```\nkubectl delete \\\u00a0 \u00a0 -f extras/postgres-hpa/loadgenerator.yaml \\\u00a0 \u00a0 -f extras/postgres-hpa/hpa \\\u00a0 \u00a0 -f extras/postgres-hpa/kubernetes-manifests \\\u00a0 \u00a0 -f extras/jwt/jwt-secret.yaml \\\u00a0 \u00a0 -f https://raw.githubusercontent.com/GoogleCloudPlatform/k8s-stackdriver/master/custom-metrics-stackdriver-adapter/deploy/production/adapter.yaml\n```\n- Delete the PostgreSQL database:```\nhelm uninstall accounts-dbkubectl delete pvc -l \"app.kubernetes.io/instance=accounts-db\"kubectl delete configmaps initdb\n```\n- Delete the GKE cluster and the IAM service account:```\ngcloud iam service-accounts delete \"bank-of-anthos@PROJECT_ID.iam.gserviceaccount.com\" --quietgcloud container clusters delete \"bank-of-anthos\" --region=\"us-central1\" --quiet\n```\n### Delete the project\n- **Caution** : Deleting a project has the following effects:- **Everything in the project is deleted.** If you used an existing project for  the tasks in this document, when you delete it, you also delete any other work you've  done in the project.\n- **Custom project IDs are lost.** When you created this project, you might have created a custom project ID that you want to use in  the future. To preserve the URLs that use the project ID, such as an`appspot.com`URL, delete selected resources inside the project instead of deleting the whole project.\nIf you plan to explore multiple architectures, tutorials, or quickstarts, reusing projects  can help you avoid exceeding project quota limits.\n- Delete a Google Cloud project:\n- ```\ngcloud projects delete PROJECT_ID\n```\n## What's next\n- Learn about [vertical Pod autoscaling](/kubernetes-engine/docs/concepts/verticalpodautoscaler) , which you can use to automatically adjust resource requests for long-running workloads with recommendations that are based on historical usage.\n- Learn more about [horizontal Pod autoscaling](/kubernetes-engine/docs/concepts/horizontalpodautoscaler) .", "guide": "Google Kubernetes Engine (GKE)"}