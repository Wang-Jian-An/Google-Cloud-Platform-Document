{"title": "Google Kubernetes Engine (GKE) - Maximize GPU network bandwidth with GPUDirect-TCPX and multi-networking", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/gpu-bandwidth-gpudirect-tcpx", "abstract": "# Google Kubernetes Engine (GKE) - Maximize GPU network bandwidth with GPUDirect-TCPX and multi-networking\nThis page shows you how to maximize network bandwidth and throughput for high-performance GPU workloads in Google Kubernetes Engine (GKE) clusters in Standard mode. This page is intended for machine learning (ML) engineers and platform administrators who facilitate ML workloads. You should already be familiar with networking technologies such as network interface cards (NICs) and TCP, and with accelerator technologies like the NVIDIA Collective Communications Library (NCCL).\nArtificial intelligence (AI), ML, and high performance computing (HPC) applications require powerful acceleration to optimize performance by reducing job completion times. For example, ML models that focus on conversational AI and image generation require high scalability and compute power.\n", "content": "## About Google Cloud GPU supercomputers\nGoogle Cloud has accelerator-optimized supercomputers that are built for scalable, massive models. These machines have the following benefits:\n- Eight NVIDIA H100 GPUs per machine.\n- Up to 200\u00a0Gbps bandwidth on the primary NIC.\n- Up to four secondary NICs, each supporting up to 200\u00a0Gbps bandwidth for GPU data transfer.\n**Note:** In practice, the bandwidth and throughput improvements that you see might be less than these optimal maximum values.\nFor a full list of benefits, see [A3 machine series](/compute/docs/accelerator-optimized-machines#a3-vms) in the Compute Engine documentation.\nYour GKE workload **must** use all available GPUs and all available secondary NICs on a single node and use a significant portion of the available bandwidth. The solution described in this document is ideal for workloads that require high performance, high throughput, and low latency.\n### Required features and capabilities for maximized bandwidth\nTo maximize your network bandwidth in GPU supercomputer nodes, use **all** of the following features:\n- **GPUDirect-TCPX** : Reduce the overhead required to transfer packet payloads to and from GPUs, which significantly improves throughput at scale compared to GPUs that don't use GPUDirect-TCPX.\n- **gVNIC** : Enable GPUDirect-TCPX capabilities such as packet header splitting, flow steering, and buffer management. gVNIC is required to use GPUDirect-TCPX. For details about gVNIC, see [Increase network traffic speed for GPU nodes](/kubernetes-engine/docs/how-to/using-gvnic) .\n- **Multi-networking (Preview)** : Add secondary NICs to the accelerator-optimized machine. For A3 machines, adds four additional NICs. Each NIC is associated with a separate subnet in its own VPC to avoid conflicts. For details about multi-network support, see [Setup multi-network support for Pods](/kubernetes-engine/docs/how-to/setup-multinetwork-support-for-pods) .\n- **Placement policies** : Use a resource placement policy to place all GPU nodes for a specific workload on physically close servers to minimize latency. For details, see [Define compact placement for GKE nodes](/kubernetes-engine/docs/how-to/compact-placement) .\n### Procedure outline\nTo use all of these capabilities together, you'll do the following:\n- [Create Virtual Private Cloud (VPC)s and subnets](#create-vpcs-subnets) \n- [Create the GKE environment](#create-gke-environment) :- Create a cluster with multi-networking enabled\n- Create a node pool with the following characteristics:- gVNIC enabled\n- Multi-networking subnets specified for each secondary NIC\n- A3 machine series with H100 GPUs (four secondary NICs and eight GPUs) backing the nodes\n- Latest NVIDIA drivers installed\n- [Install the GPUDirect-TCPX binary and the NCCL plugin](#install-gpudirect-tcpx-nccl) \n- [Deploy a test workload to verify GPUDirect-TCPX setup](#test-workload) ## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- Ensure that you have enough quota for H100 GPUs. To request more quota, see [GPU quotas](/compute/resource-usage#gpu_quota) .\n### Requirements\n- GPUDirect-TCPX requires GKE version 1.27.7-gke.1121000 and later, and isn't supported in GKE version 1.28 and later.\n- Your GPU nodes must use NVIDIA driver version 535 or later.\n- You must use GKE Dataplane V2.\n### Limitations\nThe following limitations apply:\n- You can't use H100 GPUs in Autopilot clusters\n- You can only use GPUDirect-TCPX in GKE versions starting from 1.27.7-gke.1121000 up to, but not including, 1.28.\n- You can't use GPUDirect-TCPX with [multi-instance GPUs](/kubernetes-engine/docs/how-to/gpus-multi) or [GPU time-sharing](/kubernetes-engine/docs/how-to/timesharing-gpus) \n- You can't use [NCCL FastSocket](/kubernetes-engine/docs/how-to/nccl-fast-socket) \n- Your environment must support setting`hostNetwork: true`in the Pod specification\n- To use Local SSDs for Pod storage, you must explicitly specify the exact number of Local SSDs to attach to the underlying A3 VM by using the `--ephemeral-storage-local-ssd=count=` `` flag for ephemeral storage or the `--local-nvme-ssd-block=count=` `` flag for block access. If you omit this flag, you won't be able to use the Local SSDs in your Pods. **These flags are only required if you want to use Local SSD fordata access.** The supported machine size in GKE is `a3-highgpu-8g` , and the corresponding Local SSD count is `16` .## Create VPCs and subnets\nCreate separate VPC networks in your project for each virtual NIC that you'll add to your nodes. Each VPC must have a subnet and a firewall rule that allows internal network traffic. To maximize your bandwidth, we recommend that you create four new networks.\n- Update the default VPC subnetwork in your project to add secondary IP address ranges for Pods and for Services:```\ngcloud compute networks subnets update DEFAULT_NETWORK \\\u00a0 \u00a0 --region=REGION \\\u00a0 \u00a0 --add-secondary-ranges=\"CLUSTER_NAME-pods=POD_IP_ADDRESS_RANGE,CLUSTER_NAME-services=SERVICE_IP_ADDRESS_RANGE\"\n```Replace the following:- ``: the name of the default subnet in your project.\n- ``: the region of the default subnet.\n- ``: the name of your GKE cluster.\n- ``: the IP address range for Pods in the cluster to use, in CIDR notation. For example,`10.64.0.0/19`.\n- ``: the IP address range for Services in the cluster to use, in CIDR notation. Must be different to the Pod range. For example,`10.65.0.0/19`.\n- Create the VPC networks for GPUDirect-TCPX in your project, each with a subnet and a firewall rule: **Caution:** Don't let the IP address ranges in the following commands overlap with the secondary IP address ranges that you created in the previous step.```\nfor N in $(seq 1 4); dogcloud compute networks create PROJECT_ID-net-$N \\\u00a0 \u00a0 --subnet-mode=custom \\\u00a0 \u00a0 --mtu=8244gcloud compute networks subnets create PROJECT_ID-sub-$N \\\u00a0 \u00a0 --network=PROJECT_ID-net-$N \\\u00a0 \u00a0 --region=REGION \\\u00a0 \u00a0 --range=SUBNET_RANGEgcloud compute firewall-rules create PROJECT_ID-internal-$N \\\u00a0 --network=PROJECT_ID-net-$N \\\u00a0 --action=ALLOW \\\u00a0 --rules=tcp:0-65535,udp:0-65535,icmp \\\u00a0 --source-ranges=SOURCE_RANGEdone\n```Replace the following:- ``: your Google Cloud project ID.\n- ``: the Compute Engine region for each subnet.\n- ``: the IP address range of each subnet in CIDR notation. This example command iterates for four subnets, so use a variable to change the IP address for each subnet. For example, specify`192.168.$N.0/24`so that the first subnet uses`192.168.1.0/24`, the second subnet uses`192.168.2.0/24`, etc.\n- ``: The source IP address range for the firewall rule to allow ingress traffic, in CIDR notation. For example,`192.168.0.0/16`.\n- Verify that the networks were created:```\ngcloud compute networks list\n```## Create the GKE environment\nCreate a new GKE cluster that uses multi-networking (Preview) and create a GPU node pool that uses A3 machines with attached H100 GPUs and four additional NICs. You can't update an existing cluster to use multi-networking.\n- Create a cluster:```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --location=LOCATION \\\u00a0 \u00a0 --cluster-version=VERSION \\\u00a0 \u00a0 --enable-dataplane-v2 --enable-ip-alias \\\u00a0 \u00a0 --enable-multi-networking \\\u00a0 \u00a0 --no-enable-autoupgrade \\\u00a0 \u00a0 --cluster-secondary-range-name=CLUSTER_NAME-pods \\\u00a0 \u00a0 --services-secondary-range-name=CLUSTER_NAME-services\n```Replace the following:- ``: the name of your new cluster\n- ``: the Compute Engine region for the cluster\n- ``: the GKE version for the cluster. Must be a supported version as described in the [Requirements section](#requirements) .\nThis command also explicitly specifies the secondary IP address for Pods and Services for the cluster that you created in the previous section.\n- Create Network and GKENetworkParamSet resources in the cluster that correspond to the VPC networks and subnetworks that you created:```\nkubectl apply -f - <<EOFapiVersion: networking.gke.io/v1kind: Networkmetadata:\u00a0 name: vpc1spec:\u00a0 parametersRef:\u00a0 \u00a0 group: networking.gke.io\u00a0 \u00a0 kind: GKENetworkParamSet\u00a0 \u00a0 name: vpc1\u00a0 type: Device---apiVersion: networking.gke.io/v1kind: Networkmetadata:\u00a0 name: vpc2spec:\u00a0 parametersRef:\u00a0 \u00a0 group: networking.gke.io\u00a0 \u00a0 kind: GKENetworkParamSet\u00a0 \u00a0 name: vpc2\u00a0 type: Device---apiVersion: networking.gke.io/v1kind: Networkmetadata:\u00a0 name: vpc3spec:\u00a0 parametersRef:\u00a0 \u00a0 group: networking.gke.io\u00a0 \u00a0 kind: GKENetworkParamSet\u00a0 \u00a0 name: vpc3\u00a0 type: Device---apiVersion: networking.gke.io/v1kind: Networkmetadata:\u00a0 name: vpc4spec:\u00a0 parametersRef:\u00a0 \u00a0 group: networking.gke.io\u00a0 \u00a0 kind: GKENetworkParamSet\u00a0 \u00a0 name: vpc4\u00a0 type: Device---apiVersion: networking.gke.io/v1kind: GKENetworkParamSetmetadata:\u00a0 name: vpc1spec:\u00a0 vpc: PROJECT_ID-net-1\u00a0 vpcSubnet: PROJECT_ID-sub-1\u00a0 deviceMode: NetDevice---apiVersion: networking.gke.io/v1kind: GKENetworkParamSetmetadata:\u00a0 name: vpc2spec:\u00a0 vpc: PROJECT_ID-net-2\u00a0 vpcSubnet: PROJECT_ID-sub-2\u00a0 deviceMode: NetDevice---apiVersion: networking.gke.io/v1kind: GKENetworkParamSetmetadata:\u00a0 name: vpc3spec:\u00a0 vpc: PROJECT_ID-net-3\u00a0 vpcSubnet: PROJECT_ID-sub-3\u00a0 deviceMode: NetDevice---apiVersion: networking.gke.io/v1kind: GKENetworkParamSetmetadata:\u00a0 name: vpc4spec:\u00a0 vpc: PROJECT_ID-net-4\u00a0 vpcSubnet: PROJECT_ID-sub-4\u00a0 deviceMode: NetDeviceEOF\n```These resources tell GKE to configure the NICs for GPU traffic in passthrough mode. GKE doesn't apply built-in networking programming using eBPF to this traffic.\n- Create a node pool for the H100 GPUs:```\ngcloud container node-pools create NODE_POOL_NAME \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --location=LOCATION \\\u00a0 \u00a0 --machine-type=a3-highgpu-8g \\\u00a0 \u00a0 --accelerator=type=nvidia-h100-80gb,count=8,gpu-driver-version=LATEST \\\u00a0 \u00a0 --additional-node-network=network=PROJECT_ID-net-1,subnetwork=PROJECT_ID-sub-1 \\\u00a0 \u00a0 --additional-node-network=network=PROJECT_ID-net-2,subnetwork=PROJECT_ID-sub-2 \\\u00a0 \u00a0 --additional-node-network=network=PROJECT_ID-net-3,subnetwork=PROJECT_ID-sub-3 \\\u00a0 \u00a0 --additional-node-network=network=PROJECT_ID-net-4,subnetwork=PROJECT_ID-sub-4 \\\u00a0 \u00a0 --enable-gvnic \\\u00a0 \u00a0 --no-enable-autoupgrade \\\u00a0 \u00a0 [--ephemeral-storage-local-ssd=count=16]\n```Replace `` with the name of the node pool. **Note:** The `--ephemeral-storage-local-ssd=count=16` flag is required if you want to use Local SSDs as ephemeral storage. Don't modify the number of SSDs in count.If this command fails, you might not have enough H100 GPU quota in your project. Ensure that you have quota and retry the command.\n- Get a list of nodes in the cluster:```\nkubectl get nodes\n```\n- Verify that each GPU node has eight GPUs:```\nkubectl describe node NODE_NAME\n```The output is similar to the following:```\nCapacity:\n ...\n nvidia.com/gpu:    8\nAllocatable:\n ...\n nvidia.com/gpu:    8\n```## Install GPUDirect-TCPX and configure NCCL\nThis section shows you how to install the GPUDirect-TCPX binary and a specific NCCL using a DaemonSet.\n- Review the DaemonSet manifest: [  gpudirect-tcpx/nccl-tcpx-installer.yaml ](https://github.com/GoogleCloudPlatform/container-engine-accelerators/blob/master/gpudirect-tcpx/nccl-tcpx-installer.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/container-engine-accelerators/blob/master/gpudirect-tcpx/nccl-tcpx-installer.yaml) ```\napiVersion: apps/v1kind: DaemonSetmetadata:\u00a0 name: nccl-tcpx-installer\u00a0 namespace: kube-system\u00a0 labels:\u00a0 \u00a0 k8s-app: nccl-tcpx-installerspec:\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 k8s-app: nccl-tcpx-installer\u00a0 updateStrategy:\u00a0 \u00a0 type: RollingUpdate\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 name: nccl-tcpx-installer\u00a0 \u00a0 \u00a0 \u00a0 k8s-app: nccl-tcpx-installer\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 priorityClassName: system-node-critical\u00a0 \u00a0 \u00a0 affinity:\u00a0 \u00a0 \u00a0 \u00a0 nodeAffinity:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nodeSelectorTerms:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - key: cloud.google.com/gke-accelerator\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - nvidia-h100-80gb\u00a0 \u00a0 \u00a0 tolerations:\u00a0 \u00a0 \u00a0 \u00a0 - operator: \"Exists\"\u00a0 \u00a0 \u00a0 hostNetwork: true\u00a0 \u00a0 \u00a0 hostPID: true\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 \u00a0 - name: var-lib\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hostPath:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path: /var/lib\u00a0 \u00a0 \u00a0 \u00a0 - name: tcpx\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hostPath:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path: /var/lib/tcpx\u00a0 \u00a0 \u00a0 \u00a0 - name: library-dir-host\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hostPath:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 path: /home/kubernetes/bin\u00a0 \u00a0 \u00a0 initContainers:\u00a0 \u00a0 \u00a0 \u00a0 - image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpx/nccl-plugin-gpudirecttcpx-dev:v3.1.7\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: nccl-tcpx-installer\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: 150m\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: var-lib\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mountPath: /var/lib\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - name: library-dir-host\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mountPath: /usr/local\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 command: [\"/bin/sh\", \"-c\"]\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 set -ex\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 /scripts/container_entry.sh install --install-nccl\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mkdir -p /usr/local/nvidia/lib64\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cp -r /var/lib/tcpx/lib64/. /usr/local/nvidia/lib64\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 echo \"installation finishes\"\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 \u00a0 - image: \"gcr.io/google-containers/pause:2.0\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: pause\n```This DaemonSet does the following:- Installs an NCCL library and the GPUDirect-TCPX binary on the node.\n- Stores the library and the binary in the`/home/kubernetes/bin/nvidia/lib64`directory on the VM. By default, GKE mounts this directory into the`/usr/local/nvidia/lib64`path in GPU containers that need to use NCCL and GPUDirect-TCPX.\n- Deploy the DaemonSet:```\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/gpudirect-tcpx/nccl-tcpx-installer.yaml\n```The NCCL plugin takes approximately two minutes to start running.\n- Verify the status of the DaemonSet Pods:```\nkubectl get pods -n=kube-system -l=name=nccl-tcpx-installer\n```The output is similar to the following:```\nnccl-tcpx-installer-6c2pv     1/1  Running 0   2m11s\nnccl-tcpx-installer-qgg82     1/1  Running 0   2m11s\n```## Deploy a test workload\nIn this section, you deploy a sample workload to verify that NCCL and GPUDirect-TCPX work as expected. This workload includes a sidecar container named the , which runs a service that lets the Pod use GPUDirect-TCPX. **You must add this sidecar container to any Pods in yourown environment that need to use GPUDirect-TCPX.** For a snippet of the required fields to add to your manifests, see [Add GPUDirect-TCPX to your manifest](#add-gpudirect-tcpx-manifests) in this document.\n- Review the [nccl-config-default.yaml ConfigMap manifest in GitHub](https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/gpudirect-tcpx/nccl-config-default.yaml) . This manifest deploys scrips that initialize an NCCL allgather test and sets NCCL-specific environment variables.\n- Review the [nccl-test.yaml manifest in GitHub](https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/gpudirect-tcpx/nccl-test.yaml) . This manifest does the following:- Deploys two Pods, each of which runs in a node that has H100 GPUs.\n- Deploys a sidecar container named`tcpx-daemon`in each Pod to let those Pods use GPUDirect-TCPX.\n **Caution:** `GPUDirect-TCPX` Pods require access to the host network ( `hostNetwork: true` ). The `tcpx-daemon` service must run as a privileged container.\n- Deploy the ConfigMap and the test workload:```\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/gpudirect-tcpx/nccl-config-default.yamlkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/gpudirect-tcpx/nccl-test.yaml\n```\n- Run the following commands to trigger an NCCL all-gather test for the nodes:```\nhead_pod=$(kubectl get pods --output='custom-columns=POD:.metadata.name' --no-headers | head -n1)nodes=($(kubectl get pods --output='custom-columns=NODE:.spec.nodeName' --no-headers))kubectl exec --stdin --tty --container=nccl-test ${head_pod} -- /configs/allgather.sh ${nodes[@]}\n```The output is similar to the following:```\n#                out-of-place      in-place\n#  size   count  type redop root  time algbw busbw #wrong  time algbw busbw #wrong\n#  (B) (elements)        (us) (GB/s) (GB/s)   (us) (GB/s) (GB/s)\n  1048576   16384  float none  -1 696.8 1.50 1.41  0 729.0 1.44 1.35  0\n  2097152   32768  float none  -1 776.4 2.70 2.53  0 726.7 2.89 2.71  0\n  4194304   65536  float none  -1 774.3 5.42 5.08  0 805.1 5.21 4.88  0\n  8388608  131072  float none  -1 812.1 10.33 9.68  0 817.6 10.26 9.62  0\n 16777216  262144  float none  -1 1035.2 16.21 15.19  0 1067.8 15.71 14.73  0\n 33554432  524288  float none  -1 1183.3 28.36 26.59  0 1211.8 27.69 25.96  0\n 67108864  1048576  float none  -1 1593.4 42.12 39.49  0 1510.5 44.43 41.65  0\n 134217728  2097152  float none  -1 2127.8 63.08 59.13  0 2312.7 58.03 54.41  0\n 268435456  4194304  float none  -1 3603.0 74.50 69.85  0 3586.2 74.85 70.17  0\n 536870912  8388608  float none  -1 7101.7 75.60 70.87  0 7060.9 76.03 71.28  0\n# Out of bounds values : 0 OK\n# Avg bus bandwidth : 29.8293\n```\n**Success:** At this point, you've successfully installed GPUDirect-TCPX on your nodes and can use it to optimize the throughput of GPU-heavy workloads that run on those nodes. The required fields to use GPUDirect-TCPX in your own Pods are described in [Add GPUDirect-TCPX to your manifests](#add-gpudirect-tcpx-manifests) in this document.\n### Use NCCL environment variables to improve performance\nYou can optionally set specific environment variables to improve the performance of your workloads that use NCCL. The `nccl-config-default.yaml` ConfigMap that you deploy in the [Deploy a test workload](#test-workload) section sets some NCCL variables by default. The variable configuration is stored in the `run-nccl.sh` script in the ConfigMap.\nTo change the NCCL environment variables, deploy an updated ConfigMap manifest with modified variables. The [nccl-config-latest.yaml manifest in GitHub](https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/gpudirect-tcpx/nccl-config-latest.yaml) contains every recommended variable with an updated `run-nccl.sh` script.\nThe following command updates the existing ConfigMap that has the default variables with the updated `nccl-config-latest.yaml` ConfigMap:\n```\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/gpudirect-tcpx/nccl-config-latest.yaml\n```\nKubernetes takes approximately two minutes to update the ConfigMap.\nTo check the NCCL environment variables, run the following command:\n```\nhead_pod=$(kubectl get pods --output='custom-columns=POD:.metadata.name' --no-headers | head -n1)kubectl exec --stdin --tty --container=nccl-test ${head_pod} -- cat /configs/run-nccl.sh\n```\n## Add GPUDirect-TCPX to your manifests\nThis section provides the required fields that you must add to your Kubernetes manifests for your Pods to use GPUDirect-TCPX.\n- Add the following fields to the Pod specification:```\nspec:\u00a0 hostNetwork: true\u00a0 dnsPolicy: ClusterFirstWithHostNet\u00a0 volumes:\u00a0 - name: libraries\u00a0 \u00a0 hostPath:\u00a0 \u00a0 \u00a0 path: /home/kubernetes/bin/nvidia/lib64\u00a0 - name: tcpx-socket\u00a0 \u00a0 hostPath:\u00a0 \u00a0 \u00a0 path: /run/tcpx\n```\n- Add the following container to the manifest to run the service:```\n- name: tcpx-daemon\u00a0 image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpx/tcpgpudmarxd-dev:v2.0.9\u00a0 command:\u00a0 \u00a0 - /tcpgpudmarxd/build/app/tcpgpudmarxd\u00a0 \u00a0 - --gpu_nic_preset\u00a0 \u00a0 - a3vm\u00a0 \u00a0 - --gpu_shmem_type\u00a0 \u00a0 - fd\u00a0 \u00a0 - --uds_path\u00a0 \u00a0 - /run/tcpx\u00a0 \u00a0 - --setup_param\u00a0 \u00a0 - \\\"--verbose 128 2 0 \\\"\u00a0 securityContext:\u00a0 \u00a0 privileged: true\u00a0 volumeMounts:\u00a0 \u00a0 - name: libraries\u00a0 \u00a0 \u00a0 mountPath: /usr/local/nvidia/lib64\u00a0 \u00a0 - name: tcpx-socket\u00a0 \u00a0 \u00a0 mountPath: /run/tcpx\u00a0 env:\u00a0 \u00a0 - name: LD_LIBRARY_PATH\u00a0 \u00a0 \u00a0 value: /usr/local/nvidia/lib64\n```\n- Add the following volume mounts to any containers that request GPUs:```\nvolumeMounts:- name: tcpx-socket\u00a0 mountPath: /tmp- name: libraries\u00a0 mountPath: /usr/local/nvidia/lib64\n``` **Note:** the default tcpx-socket path is `/tmp` for containers that request GPUs. If you set the `NCCL_GPUDIRECTTCPX_UNIX_CLIENT_PREFIX` environment variable to a value other than `/tmp` , GKE mounts the `tcpx-socket` volume to that `mountPath` .\n- Add the following environment variable to every GPU container:```\nenv:- name: LD_LIBRARY_PATH\u00a0 value: /usr/local/nvidia/lib64\n```\n- Optionally, add environment variables to configure NCCL options. For details, see the [Use NCCL environment variables to improve performance](#environment-variables-nccl) section in this document.\nA completed Pod specification looks like the following:\n```\napiVersion: v1kind: Podmetadata:\u00a0 name: example-pod\u00a0 labels:\u00a0 \u00a0 name: example-podspec:\u00a0 hostNetwork: true\u00a0 dnsPolicy: ClusterFirstWithHostNet\u00a0 containers:\u00a0 - name: tcpx-daemon\u00a0 \u00a0 image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpx/tcpgpudmarxd-dev:v2.0.9\u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 - /tcpgpudmarxd/build/app/tcpgpudmarxd\u00a0 \u00a0 \u00a0 - --gpu_nic_preset\u00a0 \u00a0 \u00a0 - a3vm\u00a0 \u00a0 \u00a0 - --gpu_shmem_type\u00a0 \u00a0 \u00a0 - fd\u00a0 \u00a0 \u00a0 - --uds_path\u00a0 \u00a0 \u00a0 - /run/tcpx\u00a0 \u00a0 \u00a0 - --setup_param\u00a0 \u00a0 \u00a0 - \\\"--verbose 128 2 0 \\\"\u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 - name: libraries\u00a0 \u00a0 \u00a0 \u00a0 mountPath: /usr/local/nvidia/lib64\u00a0 \u00a0 \u00a0 - name: tcpx-socket\u00a0 \u00a0 \u00a0 \u00a0 mountPath: /run/tcpx\u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 - name: LD_LIBRARY_PATH\u00a0 \u00a0 \u00a0 \u00a0 value: /usr/local/nvidia/lib64\u00a0 \u00a0 - name: nccl-test\u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/gce-ai-infra/gpudirect-tcpx/nccl-plugin-gpudirecttcpx:v3.1.2\u00a0 \u00a0 \u00a0 imagePullPolicy: Always\u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 - /bin/sh\u00a0 \u00a0 \u00a0 \u00a0 - -c\u00a0 \u00a0 \u00a0 \u00a0 - \"while true; do echo hello; sleep 1; done\"\u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: LD_LIBRARY_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: /usr/local/nvidia/lib64\u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - name: tcpx-socket\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mountPath: /run/tcpx\u00a0 \u00a0 \u00a0 \u00a0 - name: libraries\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mountPath: /usr/local/nvidia/lib64\u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 8\u00a0 volumes:\u00a0 \u00a0 - name: libraries\u00a0 \u00a0 \u00a0 hostPath:\u00a0 \u00a0 \u00a0 \u00a0 path: /home/kubernetes/bin/nvidia/lib64\u00a0 \u00a0 - name: tcpx-socket\u00a0 \u00a0 \u00a0 hostPath:\u00a0 \u00a0 \u00a0 \u00a0 path: /run/tcpx\n```", "guide": "Google Kubernetes Engine (GKE)"}