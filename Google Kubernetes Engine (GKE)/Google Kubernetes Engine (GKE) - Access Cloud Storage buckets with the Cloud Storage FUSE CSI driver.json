{"title": "Google Kubernetes Engine (GKE) - Access Cloud Storage buckets with the Cloud Storage FUSE CSI driver", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/cloud-storage-fuse-csi-driver", "abstract": "# Google Kubernetes Engine (GKE) - Access Cloud Storage buckets with the Cloud Storage FUSE CSI driver\n[Filesystem in Userspace (FUSE)](https://www.kernel.org/doc/html/next/filesystems/fuse.html) is an interface used to export a filesystem to the Linux kernel. [Cloud Storage FUSE](/storage/docs/gcs-fuse) allows you to mount Cloud Storage buckets as a file system so that applications can access the objects in a bucket using common File IO operations (e.g. open, read, write, close) rather than using cloud-specific APIs.\nThe Cloud Storage FUSE CSI driver lets you use the Kubernetes API to consume pre-existing Cloud Storage buckets as volumes. Your applications can upload and download objects using [Cloud Storage FUSE file system semantics](https://github.com/googlecloudplatform/gcsfuse/blob/master/docs/semantics.md) . The Cloud Storage FUSE CSI driver provides a fully-managed experience powered by the open source [Google Cloud Storage FUSE CSI driver](https://github.com/GoogleCloudPlatform/gcs-fuse-csi-driver) .\nThe driver natively supports the following ways for you to configure your Cloud Storage-backed volumes:\n- **CSI ephemeral volumes** : You specify the Cloud Storage bucket in-line with the Pod specification. To learn more about this volume type, see the [CSI ephemeral volumes overview](https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes) in the open source Kubernetes documentation.\n- **Static provisioning** : You create a PersistentVolume resource that refers to the Cloud Storage bucket. Your Pod can then reference a PersistentVolumeClaim that is bound to this PersistentVolume. To learn more about this workflow, see [Configure a Pod to Use a PersistentVolume for Storage](https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/) .", "content": "## Benefits\n- The Cloud Storage FUSE CSI driver on your cluster turns on automatic deployment and management of the driver. The driver works on both Standard and Autopilot clusters.\n- The Cloud Storage FUSE CSI driver does not need privileged access that is typically required by FUSE clients. This enables a better security posture.\n- The support of [CSI ephemeral volumes](#provision-ephemeral-volume) simplifies volume configuration and management by eliminating the need for PersistentVolumeClaim and PersistentVolume objects.\n- The Cloud Storage FUSE CSI driver supports the`ReadWriteMany`,`ReadOnlyMany`, and`ReadWriteOnce` [access modes](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes) .\n- You can use [workload identity federation for GKE](/kubernetes-engine/docs/concepts/workload-identity) to manage authentication while having granular control over how your Pods access Cloud Storage objects.\n- If you are running ML training and serving workloads with frameworks like Ray, PyTorch, Spark, and TensorFlow, the portability and simplicity provided by the Cloud Storage FUSE CSI driver allow you to run your workloads directly on your GKE clusters without additional code changes.## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- [Create your Cloud Storage buckets](/storage/docs/creating-buckets) . To improve performance, set the`Location type`field to`Region`, and select a region where your GKE cluster is running.\n### Limitations\n- The Cloud Storage FUSE file system has [differences in performance, availability, access authorization, and semantics](/storage/docs/gcs-fuse#differences-and-limitations) compared to a POSIX file system.\n- The Cloud Storage FUSE CSI driver is not supported on [GKE Sandbox](/kubernetes-engine/docs/concepts/sandbox-pods) .\n- The Cloud Storage FUSE CSI driver does not support volume snapshots, volume cloning, or volume expansions.\n- See the [known issues](https://github.com/GoogleCloudPlatform/gcs-fuse-csi-driver/blob/main/docs/known-issues.md) in the Cloud Storage FUSE CSI driver GitHub project.\n- See the [open issues](https://github.com/GoogleCloudPlatform/gcs-fuse-csi-driver/issues) in the Cloud Storage FUSE CSI driver GitHub project. The issues are being triaged and will be resolved in future updates.\n### Requirements\nTo use the Cloud Storage FUSE CSI driver, your clusters must meet the following requirements:\n- Use Linux clusters running GKE version 1.24 or later.\n- [Have workload identity federation for GKE enabled](/kubernetes-engine/docs/how-to/workload-identity#enable) .\n- [Have GKE metadata server enabled on your node pool](/kubernetes-engine/docs/how-to/workload-identity#migrate_applications_to) .\n- Make sure you have [installed the latest version](/sdk/gcloud#download_and_install_the) of the Google Cloud CLI.\n- To use the [private image for sidecar container feature](#private-image) or the [custom buffer volume feature](#buffer-volume) , make sure your cluster uses these GKE versions: 1.25.16-gke.1360000, 1.26.13-gke.1052000, 1.27.10-gke.1055000, 1.28.6-gke.1369000, 1.29.1-gke.1575000, or later.## Enable the Cloud Storage FUSE CSI driver\n**Note:** The Cloud Storage FUSE CSI driver is enabled by default for Autopilot clusters with version 1.24 or later. You can enable the feature by upgrading your Autopilot cluster to version 1.24 or later. If your clusters have Cloud Storage FUSE CSI driver enabled, skip to [Configure access to Cloud Storage buckets using GKE workload identity federation for GKE](#authentication) .\nTo create a Standard cluster with the Cloud Storage FUSE CSI driver enabled, you can use the gcloud CLI:\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --addons GcsFuseCsiDriver \\\u00a0 \u00a0 --cluster-version=VERSION \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --workload-pool=PROJECT_ID.svc.id.goog\n```\nReplace the following:\n- ``: the name of your cluster.\n- ``: the GKE version number. You must select 1.24 or later.\n- ``: the Compute Engine [region](/compute/docs/regions-zones#available) of your cluster. For zonal clusters, use`--zone=` ``.\n- ``: your project ID.\nTo enable the driver on an existing Standard cluster, use the `gcloud container clusters update` command:\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --update-addons GcsFuseCsiDriver=ENABLED \\\u00a0 \u00a0 --region=COMPUTE_REGION\n```\nReplace the following:\n- ``: the name of your cluster.\n- ``: the Compute Engine [region](/compute/docs/regions-zones#available) of your cluster. For zonal clusters, use`--zone=` ``.\nAfter you enable the Cloud Storage FUSE CSI driver, you can use the driver in Kubernetes volumes by specifying the driver and provisioner name: `gcsfuse.csi.storage.gke.io` .\n## Configure access to Cloud Storage buckets using GKE workload identity federation for GKE\nTo make your Cloud Storage buckets accessible by your GKE cluster using workload identity federation for GKE, follow these steps. See [Configure applications to use workload identity federation for GKE](/kubernetes-engine/docs/how-to/workload-identity#authenticating_to) for more information.\n- Get credentials for your cluster:```\ngcloud container clusters get-credentials CLUSTER_NAME \\\u00a0 \u00a0 --region=COMPUTE_REGION\n```Replace the following:- ``: the name of your cluster that has workload identity federation for GKE enabled.\n- ``: the Compute Engine region of your cluster.\n- Create a namespace to use for the Kubernetes service account. You can also use the default namespace or any existing namespace.```\nkubectl create namespace NAMESPACE\n```Replace the following:- ``: the name of the Kubernetes namespace for the service account.\n **Note:** Your workload **must** run in the same namespace.\n- Create a Kubernetes service account for your application to use. You can also use any existing Kubernetes service account in any namespace, including the `default` service account.```\nkubectl create serviceaccount KSA_NAME \\\u00a0 \u00a0 --namespace NAMESPACE\n```Replace the following:- ``: the name of your new Kubernetes service account.\n- ``: the name of the Kubernetes namespace for the service account.\n- Create an IAM service account for your application or use an existing IAM service account instead. You can use any IAM service account in your Cloud Storage bucket's project. **Note:** If you're using an existing IAM service account, skip this step.```\ngcloud iam service-accounts create GSA_NAME \\\u00a0 \u00a0 --project=GSA_PROJECT\n```Replace the following:- ``: the name of the new IAM service account.\n- ``: the project ID of the Google Cloud project for your IAM service account.\n **Note:** The IAM service account **must** be in your Cloud Storage bucket's project.\n- Ensure that your IAM service account has the [storage roles](/storage/docs/access-control/iam-roles) you need.You can grant the role to your IAM service account to only access a specific Cloud Storage bucket using the following command:```\ngcloud storage buckets add-iam-policy-binding gs://BUCKET_NAME \\\u00a0 \u00a0 --member \"serviceAccount:GSA_NAME@GSA_PROJECT.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role \"ROLE_NAME\"\n```Replace the following:- ``: your Cloud Storage bucket name.\n- ``: the name of your IAM service account.\n- ``: the project ID of the Google Cloud project of your IAM service account.\n- ``: the IAM role to assign to your service account. For read-only workloads, use`roles/storage.objectViewer`. For read-write workloads, use`roles/storage.objectAdmin`.\nOptionally, you can grant the role to your IAM service account to access all your Cloud Storage buckets in the project using the following command:```\ngcloud projects add-iam-policy-binding GSA_PROJECT \\\u00a0 \u00a0 --member \"serviceAccount:GSA_NAME@GSA_PROJECT.iam.gserviceaccount.com\" \\\u00a0 \u00a0 --role \"ROLE_NAME\"\n```Replace the following:- ``: the project ID of the Google Cloud project of your IAM service account.\n- ``: the name of your IAM service account.\n- ``: the IAM role to assign to your service account. For read-only workloads, use`roles/storage.objectViewer`. For read-write workloads, use`roles/storage.objectAdmin`.\n- Allow the Kubernetes service account to impersonate the IAM service account by adding an [IAM policy binding](/sdk/gcloud/reference/iam/service-accounts/add-iam-policy-binding) between the two service accounts. This binding allows the Kubernetes service account to act as the IAM service account.```\ngcloud iam service-accounts add-iam-policy-binding GSA_NAME@GSA_PROJECT.iam.gserviceaccount.com \\\u00a0 \u00a0 --role roles/iam.workloadIdentityUser \\\u00a0 \u00a0 --member \"serviceAccount:PROJECT_ID.svc.id.goog[NAMESPACE/KSA_NAME]\"\n```Replace the following:- ``: the name of your IAM service account.\n- ``: the project ID of the Google Cloud project of your IAM service account.\n- ``: the project ID of your GKE cluster.\n- ``: the name of the Kubernetes namespace for the service account.\n- ``: the name of your new Kubernetes service account.\n **Note:** Your Cloud Storage buckets and GKE cluster can be in different projects, or in the same project.\n- Annotate the Kubernetes service account with the email address of the IAM service account.```\nkubectl annotate serviceaccount KSA_NAME \\\u00a0 \u00a0 --namespace NAMESPACE \\\u00a0 \u00a0 iam.gke.io/gcp-service-account=GSA_NAME@GSA_PROJECT.iam.gserviceaccount.com\n```Replace the following:- ``: the name of your new Kubernetes service account.\n- ``: the name of the Kubernetes namespace for the service account.\n- ``: the name of your IAM service account.\n- ``: the project ID of the Google Cloud project of your IAM service account.\n## Prepare to mount Cloud Storage FUSE buckets\nThis section covers how to prepare to mount Cloud Storage FUSE buckets on your clusters.\n### Specify Pod annotations\nThe CSI driver relies on [Pod annotations](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/) to identify if your Pod uses Cloud Storage-backed volumes. If the driver detects the necessary annotations, it injects a sidecar container called `gke-gcsfuse-sidecar` into your workload Pod. The Cloud Storage FUSE instances run inside the sidecar container and mount the Cloud Storage buckets for your workload.\nTo enable the CSI driver to mount the Cloud Storage buckets, make sure you specify the annotation `gke-gcsfuse/volumes: \"true\"` in your Pod specification, under the `metadata` field. If you want your Cloud Storage-backed volumes to be consumed by other Kubernetes workload types (for instance, [Job](https://kubernetes.io/docs/concepts/workloads/controllers/job/) , [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) , or [StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/) ), make sure you configure the annotations under the `spec.template.metadata.annotations` field.\n### Configure resources for the sidecar container\nBy default, the sidecar container is configured with the following resource limits:\n- 250m CPU\n- 256\u00a0Mi memory\n- 5\u00a0Gi ephemeral storage\nTo overwrite these values, you can optionally specify the annotation `gke-gcsfuse/[cpu-limit|memory-limit|ephemeral-storage-limit]` as shown in the following example:\n```\napiVersion: v1kind: Podmetadata:\u00a0 annotations:\u00a0 \u00a0 gke-gcsfuse/volumes: \"true\"\u00a0 \u00a0 gke-gcsfuse/cpu-limit: 500m\u00a0 \u00a0 gke-gcsfuse/memory-limit: 1Gi\u00a0 \u00a0 gke-gcsfuse/ephemeral-storage-limit: 50Gi\n```\n**Note:** Autopilot might overwrite your sidecar container resource configuration. To learn more, see [Resource requests in Autopilot](/kubernetes-engine/docs/concepts/autopilot-resource-requests) .\nUse the following considerations when deciding the amount of resources to allocate:\n- The resource request is enforced to be equal to the resource limit.\n- If your workload Pod consumes multiple Cloud Storage volumes, the sidecar container resources are shared by multiple Cloud Storage FUSE instances. If this applies to you, consider increasing the resource allocation for multiple Cloud Storage volumes.\n- Allocate more CPU to the sidecar container if your workloads need higher throughput. Insufficient CPU will cause Cloud Storage FUSE throttling.\n- If your workloads need to process a large number of files, and the Cloud Storage FUSE [metadata caching](https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/semantics.md#caching) is enabled, increase the sidecar container's memory allocation. Cloud Storage FUSE memory consumption is proportional to the number of files but not the file size, because Cloud Storage FUSE does not cache file content. Insufficient memory will cause Cloud Storage FUSE out-of-memory errors and crash the workload application.\n- For write operations, Cloud Storage FUSE stages the files in a local temporary directory before the files are uploaded to the Cloud Storage bucket. Estimate how much free space your workload needs for staging when writing large files, and increase your ephemeral storage limit accordingly. To learn more, see [Read/Writes semantics](https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/semantics.md#readwrites) in the Cloud Storage FUSE GitHub documentation.\n- You can use value`\"0\"`to unset any resource limits on Standard clusters. For example, annotation`gke-gcsfuse/memory-limit: \"0\"`leaves the sidecar container memory limit and request empty. This is useful when you cannot decide on the amount of resources Cloud Storage FUSE needs for your workloads, and want to let the Cloud Storage FUSE consume all the available resources on a node.\n**Note:** You cannot use value `\"0\"` to unset the sidecar container resource limits and requests on Autopilot clusters.\n**Note:** If you use value `\"0\"` to unset the sidecar container resource limits and requests, GKE changes the [Pod Quality of Service (QoS) class](https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/) to [BestEffort](https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#besteffort) .\n**Note:** If your main workload container does not have ephemeral storage limit, the Pod annotation `gke-gcsfuse/ephemeral-storage-limit` sets the ephemeral storage limit on Pod level, while the CPU and memory limits only take effect on the sidecar container. Consider configuring ephemeral storage limit for your main workload container if it consumes significant ephemeral storage. See [Setting requests and limits for local ephemeral storage](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage) for more details.\n### Configure a private image for the sidecar container\nThis section describes how to use the sidecar container image if you are hosting it in a private container registry. This scenario might apply if you need to use private clusters for security purposes or if your cluster has limited access to the public internet. To configure and consume the private sidecar container image, follow these steps:\n- Refer to this [page](https://github.com/GoogleCloudPlatform/gcs-fuse-csi-driver/blob/main/docs/releases.md#gke-compatibility) to look for a compatible public sidecar container image.\n- Pull it to your local environment and push it to your private container registry.\n- In the manifest, specify a container named `gke-gcsfuse-sidecar` with only the `image` field. GKE will use the specified sidecar container image to prepare for the sidecar container injection. Here is an example:\n```\napiVersion: v1kind: Podmetadata:\u00a0 annotations:\u00a0 \u00a0 gke-gcsfuse/volumes: \"true\"spec:\u00a0 containers:\u00a0 - name: gke-gcsfuse-sidecar\u00a0 \u00a0 image: PRIVATE_REGISTRY/gcs-fuse-csi-driver-sidecar-mounter:PRIVATE_IMAGE_TAG\u00a0 - name: main # your main workload container.\n```\nReplace the following:\n- ``: your private container registry.\n- ``: your private sidecar container image tag.\n### Configure a custom buffer volume for the sidecar container\nBy default, GKE uses an `emptyDir` volume for Cloud Storage FUSE write buffering. You can specify a volume named `gke-gcsfuse-buffer` to replace the default `emptyDir` volume for Cloud Storage FUSE to stage the files in write operations. You can specify any type of storage, such as a PVC, and GKE will use the specified volume to prepare for the sidecar container injection. This is useful if you need to write files larger than 10Gi on Autopilot clusters. Here is an example of using a predefined PVC as the buffer volume:\n```\napiVersion: v1kind: Podmetadata:\u00a0 annotations:\u00a0 \u00a0 gke-gcsfuse/volumes: \"true\"spec:\u00a0 containers:\u00a0 ...\u00a0 volumes:\u00a0 - name: gke-gcsfuse-buffer\u00a0 \u00a0 persistentVolumeClaim:\u00a0 \u00a0 \u00a0 claimName: BUFFER_VOLUME_PVC\n```\nReplace the following:\n- ``: the predefined PVC name.## Provision your volume as a CSI ephemeral volume\n[CSI ephemeral volumes](https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes) backed by Cloud Storage buckets are tied to the Pod lifecycle. With this provisioning approach, you don't need to maintain the PersistentVolume and PersistentVolumeClaim objects associated with the Cloud Storage buckets after Pod termination.\n**Note:** When using CSI ephemeral volumes, you are still responsible for creating the buckets before use and deleting the buckets if needed.\n**Note:** Avoid using the names `gke-gcsfuse-sidecar` and `gke-gcsfuse-tmp` as these are reserved names for objects that the CSI driver creates.\n### Consume the CSI ephemeral storage volume in a Pod\n- Save the following YAML manifest:```\napiVersion: v1kind: Podmetadata:\u00a0 name: gcs-fuse-csi-example-ephemeral\u00a0 namespace: NAMESPACE\u00a0 annotations:\u00a0 \u00a0 gke-gcsfuse/volumes: \"true\"spec:\u00a0 terminationGracePeriodSeconds: 60\u00a0 containers:\u00a0 - image: busybox\u00a0 \u00a0 name: busybox\u00a0 \u00a0 command: [\"sleep\"]\u00a0 \u00a0 args: [\"infinity\"]\u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 - name: gcs-fuse-csi-ephemeral\u00a0 \u00a0 \u00a0 mountPath: /data\u00a0 \u00a0 \u00a0 readOnly: true\u00a0 serviceAccountName: KSA_NAME\u00a0 volumes:\u00a0 - name: gcs-fuse-csi-ephemeral\u00a0 \u00a0 csi:\u00a0 \u00a0 \u00a0 driver: gcsfuse.csi.storage.gke.io\u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 volumeAttributes:\u00a0 \u00a0 \u00a0 \u00a0 bucketName: BUCKET_NAME\u00a0 \u00a0 \u00a0 \u00a0 mountOptions: \"implicit-dirs\"\n```The previous example shows how you can specify the Cloud Storage bucket inline in the Pod manifest. The example includes the following fields:- `metadata.annotations`: the annotation`gke-gcsfuse/volumes: \"true\"`is required. See [Configure resources for the sidecar container](#sidecar-container) for optional annotations.\n- `spec.terminationGracePeriodSeconds`: optional. By default, this is set to 30. If you need to write large files to the Cloud Storage bucket, increase this value to make sure that Cloud Storage FUSE has enough time to flush the data after your application exits. To learn more, see [Kubernetes best practices: Terminating with grace](/blog/products/containers-kubernetes/kubernetes-best-practices-terminating-with-grace) .\n- `spec.serviceAccountName`: use the same Kubernetes service account as in the [Configure access to Cloud Storage buckets using GKE workload identity federation for GKE](#authentication) step.\n- `spec.volumes[n].csi.driver`: use`gcsfuse.csi.storage.gke.io`as the CSI driver name.\n- `spec.volumes[n].csi.volumeAttributes.bucketName`: specify your Cloud Storage FUSE bucket name. You can specify an underscore (`_`) to mount all buckets that the service account can access. To learn more, see [Dynamic Mounting](/storage/docs/gcsfuse-mount#dynamic_mounting) in the Cloud Storage FUSE documentation.\n- `spec.volumes[n].csi.volumeAttributes.mountOptions`: optional. Pass [mount flags](#mounting-flags) to Cloud Storage FUSE. Specify the flags in one string separated by commas, without spaces.\n- `spec.volumes[n].csi.readOnly`: optional. Specify`true`if all the volume mounts are read-only.\n- `spec.containers[n].volumeMounts[m].readOnly`: optional. Specify`true`if only a specific volume mount is read-only.\n- Apply the manifest to the cluster:```\nkubectl apply -f FILE_PATH\n```Replace `` with the path to the YAML file.\n### Consume the CSI ephemeral storage volume in a Job workload\n- Save the following YAML manifest:```\napiVersion: batch/v1kind: Jobmetadata:\u00a0 name: gcs-fuse-csi-job-example\u00a0 namespace: NAMESPACEspec:\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 annotations:\u00a0 \u00a0 \u00a0 \u00a0 gke-gcsfuse/volumes: \"true\"\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 serviceAccountName: KSA_NAME\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: writer\u00a0 \u00a0 \u00a0 \u00a0 image: busybox\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"/bin/sh\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"-c\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - touch /data/test && echo $(date) >> /data/test && sleep 10\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - name: gcs-fuse-csi-ephemeral\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mountPath: /data\u00a0 \u00a0 \u00a0 - name: reader\u00a0 \u00a0 \u00a0 \u00a0 image: busybox\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"/bin/sh\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"-c\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - sleep 10 && cat /data/test\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - name: gcs-fuse-csi-ephemeral\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 mountPath: /data\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 readOnly: true\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: gcs-fuse-csi-ephemeral\u00a0 \u00a0 \u00a0 \u00a0 csi:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 driver: gcsfuse.csi.storage.gke.io\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 volumeAttributes:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 bucketName: BUCKET_NAME\u00a0 \u00a0 \u00a0 restartPolicy: Never\u00a0 backoffLimit: 1\n```Replace the following:- ``: the namespace of your workload.\n- ``: the Kubernetes service account name as in the [Configure access to Cloud Storage buckets using GKE workload identity federation for GKE](#authentication) step.\n- ``: your Cloud Storage bucket name.\nThe manifest deploys a Job that consumes a Cloud Storage FUSE bucket through a CSI ephemeral volume.\n- Apply the manifest to the cluster:```\nkubectl apply -f FILE_PATH\n```Replace `` with the path to the YAML file.\nIf you are using the CSI driver in a `Job` workload, or if the Pod `RestartPolicy` is `Never` , the sidecar container will exit automatically after all the other workload containers exit.\nFor additional examples, see [Example Applications](https://github.com/GoogleCloudPlatform/gcs-fuse-csi-driver/blob/main/examples/README.md) in the GitHub project documentation.\n## Provision your volume using static provisioning\nWith [static provisioning](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#static) , you create one or more [PersistentVolume](/kubernetes-engine/docs/concepts/persistent-volumes#persistentvolumes) (PV) objects containing the details of the underlying storage system. Pods in your clusters can then consume the storage through [PersistentVolumeClaims](/kubernetes-engine/docs/concepts/persistent-volumes#persistentvolumeclaims) (PVCs).\n**Note:** Avoid using the names `gke-gcsfuse-sidecar` and `gke-gcsfuse-tmp` as these are reserved names for objects that the CSI driver creates.\n### Create a PersistentVolume\n- Save the following YAML manifest:```\napiVersion: v1kind: PersistentVolumemetadata:\u00a0 name: gcs-fuse-csi-pvspec:\u00a0 accessModes:\u00a0 - ReadWriteMany\u00a0 capacity:\u00a0 \u00a0 storage: 5Gi\u00a0 storageClassName: example-storage-class\u00a0 claimRef:\u00a0 \u00a0 namespace: NAMESPACE\u00a0 \u00a0 name: gcs-fuse-csi-static-pvc\u00a0 mountOptions:\u00a0 \u00a0 - implicit-dirs\u00a0 csi:\u00a0 \u00a0 driver: gcsfuse.csi.storage.gke.io\u00a0 \u00a0 volumeHandle: BUCKET_NAME\u00a0 \u00a0 readOnly: true\n```The example manifest shows how you can define a PersistentVolume for Cloud Storage buckets. The example includes the following fields:- `spec.claimRef.namespace`: specify the PersistentVolumeClaim namespace.\n- `spec.claimRef.name`: specify the PersistentVolumeClaim name.\n- `spec.csi.driver`: use`gcsfuse.csi.storage.gke.io`as the CSI driver name.\n- `spec.csi.volumeHandle`: specify your Cloud Storage bucket name. You can pass an underscore (`_`) to mount all the buckets that the service account is configured to have access to. To learn more, see [Dynamic Mounting](/storage/docs/gcsfuse-mount#dynamic_mounting) in the Cloud Storage FUSE documentation.\n- `spec.mountOptions`: optional. Pass [mount flags](#mounting-flags) to Cloud Storage FUSE.\n- `spec.csi.readOnly`: optional. Specify`true`if all the volume mounts are read-only.\n- Apply the manifest to the cluster:```\nkubectl apply -f FILE_PATH\n```Replace `` with the path to the YAML file.\n### Create a PersistentVolumeClaim\n- Save the following YAML manifest:```\napiVersion: v1kind: PersistentVolumeClaimmetadata:\u00a0 name: gcs-fuse-csi-static-pvc\u00a0 namespace: NAMESPACEspec:\u00a0 accessModes:\u00a0 - ReadWriteMany\u00a0 resources:\u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 storage: 5Gi\u00a0 volumeName: gcs-fuse-csi-pv\u00a0 storageClassName: example-storage-class\n```The example manifest shows how you can define a PersistentVolumeClaim to bind the PersistentVolume. The example includes the following fields:- `metadata.namespace`: specify the PersistentVolumeClaim namespace that must be consistent with the namespace of your workload.\n- `spec.volumeName`: specify the PersistentVolume name.\nTo bind a PersistentVolume to a PersistentVolumeClaim, make sure to follow these guidelines:- `spec.storageClassName`fields on PV and PVC manifests should match. The`storageClassName`does not need to refer to an existing StorageClass object. To bind the claim to a volume, you can use any name you want but it cannot be empty.\n- `spec.accessModes`fields on PV and PVC manifests should match.\n- `spec.capacity.storage`on the PersistentVolume manifest should match`spec.resources.requests.storage`on the PersistentVolumeClaim manifest. Since Cloud Storage buckets don't have size limits, you can put any number for capacity but it cannot be empty.\n- Apply the manifest to the cluster:```\nkubectl apply -f FILE_PATH\n```Replace `` with the path to the YAML file.\n### Consume the volume from a PersistentVolumeClaim\n- Save the following YAML manifest:```\napiVersion: v1kind: Podmetadata:\u00a0 name: gcs-fuse-csi-example-static-pvc\u00a0 namespace: NAMESPACE\u00a0 annotations:\u00a0 \u00a0 gke-gcsfuse/volumes: \"true\"spec:\u00a0 containers:\u00a0 - image: busybox\u00a0 \u00a0 name: busybox\u00a0 \u00a0 command: [\"sleep\"]\u00a0 \u00a0 args: [\"infinity\"]\u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 - name: gcs-fuse-csi-static\u00a0 \u00a0 \u00a0 mountPath: /data\u00a0 \u00a0 \u00a0 readOnly: true\u00a0 serviceAccountName: KSA_NAME\u00a0 volumes:\u00a0 - name: gcs-fuse-csi-static\u00a0 \u00a0 persistentVolumeClaim:\u00a0 \u00a0 \u00a0 claimName: gcs-fuse-csi-static-pvc\n```The example shows how you can define a Pod that consumes a Cloud Storage FUSE bucket through a PersistentVolumeClaim. The example includes the following fields:- `metadata.annotations`: the annotation`gke-gcsfuse/volumes: \"true\"`is required. See [Configure resources for the sidecar container](#sidecar-container) for optional annotations.\n- `spec.serviceAccountName`: use the same Kubernetes service account as in the [Configure access to Cloud Storage buckets using GKE workload identity federation for GKE](#authentication) step.\n- `spec.containers[n].volumeMounts[m].readOnly`: optional. specify`true`if only specific volume mount is read-only.\n- Apply the manifest to the cluster:```\nkubectl apply -f FILE_PATH\n```Replace `` with the path to the YAML file.\nFor additional examples, see [Example Applications](https://github.com/GoogleCloudPlatform/gcs-fuse-csi-driver/blob/main/examples/README.md) in the GitHub project documentation.\n## Configure how Cloud Storage FUSE buckets are mounted\nThe Cloud Storage FUSE CSI driver supports to configure how Cloud Storage buckets are mounted on your local file system. For the full list of supported mount flags, see the [gcsfuse CLI documentation](/storage/docs/gcsfuse-cli#options) .\nYou can specify the mount flags in the following ways:\n- In the`spec.mountOptions`field on a`PersistentVolume`manifest, if you use static provisioning.\n- In the`spec.volumes[n].csi.volumeAttributes.mountOptions`field, if you use CSI ephemeral volumes.\nUse the following considerations when configuring mounts:\n- The following flags are disallowed:`app-name`,`temp-dir`,`foreground`,`log-file`,`log-format`,`key-file`,`token-url`, and`reuse-token-from-url`.\n- Cloud Storage FUSE does not make implicit directories visible by default. To make these directories visible, you can turn on the`implicit-dirs`mount flag. To learn more, see [Files and Directories](https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/semantics.md#files-and-directories) in the Cloud Storage FUSE GitHub documentation.\n- If you use a [Security Context](/kubernetes-engine/docs/how-to/persistent-volumes/(https:/kubernetes.io/docs/tasks/configure-pod-container/security-context/)%7B:.external%20track-name=%22k8sLink%22%20track-type=%22concept%22%7D) for your Pod or container, or if your container image uses a non-root user or group, you must set the`uid`and`gid`mount flags. You also need to use the`file-mode`and`dir-mode`mount flags to set the file system permissions. Note that you cannot run`chmod`,`chown`, or`chgrp`commands against a Cloud Storage FUSE file system, so`uid`,`gid`,`file-mode`and`dir-mode`mount flags are necessary to provide access to a non-root user or group.\n- If you only want to mount a directory in the bucket instead of the entire bucket, pass the directory relative path by using the`only-dir=relative/path/to/the/bucket/root`flag.\n- To tune Cloud Storage FUSE caching behavior, refer to [Caching](https://github.com/GoogleCloudPlatform/gcsfuse/blob/master/docs/semantics.md#caching) in the Cloud Storage FUSE GitHub documentation.\n- If you need to configure the Linux kernel mount options, you can pass the options using the`o`flag. For example, if you don't want to permit direct execution of any binaries on the mounted file system, set the`o=noexec`flag. Only the following options are allowed:`exec`,`noexec`,`atime`,`noatime`,`sync`,`async`, and`dirsync`.\n- If you need to troubleshoot Cloud Storage FUSE issues, set the`debug_fuse`,`debug_fs`, and`debug_gcs`flags.## Disable the Cloud Storage FUSE CSI driver\nYou cannot disable the Cloud Storage FUSE CSI driver on Autopilot clusters.\nYou can disable the Cloud Storage FUSE CSI driver on an existing Standard cluster by using the Google Cloud CLI.\n```\ngcloud container clusters update CLUSTER_NAME \\\u00a0 \u00a0 --update-addons GcsFuseCsiDriver=DISABLED\n```\nReplace `` with the name of your cluster.\n## Troubleshooting\nTo troubleshoot issues when using the Cloud Storage FUSE CSI driver, see [Troubleshooting Guide](https://github.com/GoogleCloudPlatform/gcs-fuse-csi-driver/blob/main/docs/troubleshooting.md) in the GitHub project documentation.\n## What's next\n- [Read more about the CSI driver on GitHub](https://github.com/GoogleCloudPlatform/gcs-fuse-csi-driver) .\n- [Learn more about Cloud Storage FUSE](/storage/docs/gcs-fuse) .", "guide": "Google Kubernetes Engine (GKE)"}