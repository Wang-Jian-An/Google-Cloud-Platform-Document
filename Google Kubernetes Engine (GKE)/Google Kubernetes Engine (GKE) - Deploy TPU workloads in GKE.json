{"title": "Google Kubernetes Engine (GKE) - Deploy TPU workloads in GKE", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/tpus", "abstract": "# Google Kubernetes Engine (GKE) - Deploy TPU workloads in GKE\nThis page shows you how to request and deploy workloads that use Cloud TPU accelerators (TPUs) in Google Kubernetes Engine (GKE).\nBefore you configure and deploy TPU workloads in GKE, you should be familiar with the following concepts:\n- [Introduction to Cloud TPU](/tpu/docs/intro-to-tpu) .\n- [Cloud TPU system architecture](/tpu/docs/system-architecture-tpu-vm#versions) .\n- [About TPUs in GKE](/kubernetes-engine/docs/concepts/tpus) .", "content": "## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.## TPU availability in GKE\nUse GKE to create and manage node pools with TPUs. You can use these purposely-built accelerators to perform large-scale AI model training, tuning, and inference.\nSee a [list of supported TPU versions in GKE](/kubernetes-engine/docs/concepts/tpus#availability) .\n## Plan your TPU configuration\nPlan your TPU configuration based on your machine learning model and how much memory it requires. The following are the steps that are relevant when planning your TPU configuration:\n- [Select a TPU version and topology](/kubernetes-engine/docs/concepts/tpus#topology) .\n- [Select the type of node pool to use](/kubernetes-engine/docs/concepts/tpus#node_pool) .## Ensure sufficient quota for on-demand or Spot VMs\nIf you are creating a TPU node pool with on-demand or Spot VMs, you must have sufficient TPU quota available in the region that you want to use.\nCreating a TPU node pool that consumes a TPU reservation does require any TPU quota. You may safely skip this section for reserved TPUs.\nCreating an on-demand or Spot TPU node pool in GKE requires Compute Engine API quota. Compute Engine API quota (compute.googleapis.com) is not the same as Cloud TPU API quota (tpu.googleapis.com), which is needed when creating TPUs with the Cloud TPU API.\nTo check the limit and current usage of your Compute Engine API quota for TPUs, follow these steps:\n- Go to the **Quotas** page in the Google Cloud console: [Go to Quotas](https://console.cloud.google.com/iam-admin/quotas) \n- In the filter_list **Filter** box, do the following:- Select the **Service** property, enter **Compute Engine API** , and press **Enter** .\n- Select the **Type** property and choose **Quota** .\n- Select the **Name** property and enter the name of the quota based on the TPU version and machine type. For example, if you plan to create on-demand TPU v5e nodes whose machine type begins with `ct5lp-` , enter `TPU v5 Lite PodSlice chips` .| TPU version | Machine type begins with | Name of the quota for on-demand instances | Name of the quota for Spot2 instances |\n|:--------------|:---------------------------|:--------------------------------------------|:----------------------------------------|\n| TPU v4  | ct4p-      | TPU v4 PodSlice chips      | Preemptible TPU v4 PodSlice chips  |\n| TPU v5e  | ct5l-      | TPU v5 Lite Device chips     | Preemptible TPU v5 Lite Device chips |\n| TPU v5e  | ct5lp-      | TPU v5 Lite PodSlice chips     | Preemptible TPU v5 Lite PodSlice chips |\n| TPU v5p  | ct5p-      | TPU v5p chips        | Preemptible TPU v5p chips    |\n- Select the **Dimensions (e.g. locations)** property and enter `region:` followed by the name of the region in which you plan to create TPUs in GKE. For instance, enter `region:us-west4` if you plan to create TPU nodes in the zone `us-west4-a` . TPU quota is regional, so all zones within the same region consume the same TPU quota.If no quotas match the filter you entered, then the project has not been granted any of the specified quota for the desired region, and you must [request a TPU quota increase](/docs/quota/view-manage#requesting_higher_quota) .\n**Note:** When a TPU reservation is created, both the limit and current use values for the corresponding quota increase by the number of chips in the TPU reservation. For example, when a reservation is created for 16 TPU v5e chips whose machine type begins with `ct5lp-` , then both the **Limit** and **Current usage** for the `TPU v5 Lite PodSlice chips` quota in the relevant region increase by 16.\n- When creating a TPU node pool, use the [--reservation  and --reservation-affinity=specific flags](/sdk/gcloud/reference/container/node-pools/create#--reservation) to  create a reserved instance. TPU reservations are available when  purchasing a commitment. [\u21a9](#fnref1) \n- When creating a TPU node pool, use the [--spot  flag](/sdk/gcloud/reference/container/node-pools/create#--spot) to create a [Spot](/spot-vms) instance. [\u21a9](#fnref2) ## Ensure reservation availability\nCreating a reserved TPU node pool, which means a TPU node pool that consumes a reservation, does not require any TPU quota. However, the reservation must have sufficient available or unused chips at the time the node pool is created.\nTo see which reservations exist within a project, [view a list of yourreservations](/compute/docs/instances/reservations-view#list-reservations) .\nTo view how many chips within a TPU reservation are available, [view thedetails of a reservation](/compute/docs/instances/reservations-view#describe-reservations) .\n## Create a cluster\nCreate a GKE cluster in Standard mode in a region with available TPUs. We recommend that you use regional clusters, which provide high availability of the Kubernetes control plane. You can use the Google Cloud CLI or the Google Cloud console.\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 --location LOCATION \\\u00a0 --cluster-version VERSION\n```\nReplace the following:\n- ``: the name of the new cluster.\n- ``: the region with your TPU capacity available.\n- ``: the GKE version, which must support the machine type that you want to use. Note that the default GKE version might not have availability for your target TPU. To learn what are the minimum GKE versions available by TPU machine type, see [TPU availability in GKE](/kubernetes-engine/docs/concepts/tpus#availability) ## Create a node pool\n### Single-host TPU slice\nYou can create a [single-host TPU slice node pool](/kubernetes-engine/docs/concepts/tpus#node_pool) using the Google Cloud CLI, Terraform, or the Google Cloud console.\n```\ngcloud container node-pools create POOL_NAME \\\u00a0 \u00a0 --location=LOCATION \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --node-locations=NODE_ZONES \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE \\\u00a0 \u00a0 [--num-nodes=NUM_NODES \\]\u00a0 \u00a0 [--spot \\]\u00a0 \u00a0 [--enable-autoscaling \\]\u00a0 \u00a0 [--reservation-affinity=specific \\\u00a0 \u00a0 --reservation=RESERVATION_NAME \\]\u00a0 \u00a0 [--total-min-nodes TOTAL_MIN_NODES \\]\u00a0 \u00a0 [--total-max-nodes TOTAL_MAX_NODES \\]\u00a0 \u00a0 [--location-policy=ANY]\n```\nReplace the following:- ``: The name of the new node pool.\n- `` : The name of the zone based on the TPU version you want to use:- For TPU v4, use`us-central2-b`.\n- For TPU v5e machine types beginning with`ct5l-`, use`us-central1-a`or`europe-west4-b`.\n- For TPU v5e machine types beginning with`ct5lp-`, use`us-west1-c`,`us-west4-a`,`us-west4-b`,`us-central1-a`,`us-east1-c`,`us-east5-b`, or`europe-west4-a`.\n- For TPU v5p, use`us-east1-d`,`us-east5-a`, or`us-east5-c`.\nTo learn more, see [Select a TPU version and topology](/kubernetes-engine/docs/how-to/tpus#version_and_topology) .\n- `` : The name of the cluster.\n- `` : The comma-separated list of one or more zones where GKE creates the node pool.\n- `` : The type of machine to use for nodes. For more information about TPU compatible machine types, use the table in [Mapping of TPU configuration](/kubernetes-engine/docs/concepts/tpus#configuration) .\nOptionally, you can also use the following flags:- ``: The initial number of nodes in the node pool in each zone. If you omit this flag, the default is`3`. If autoscaling is enabled for the node pool using the`--enable-autoscaling`flag, we recommend that you set``to`0`, since the autoscaler provisions additional nodes as soon as your workloads demands them.\n- ``: The name of the reservation GKE uses when creating the node pool. If you omit this flag, GKE uses available TPUs. To learn more about TPU reservations, see [TPU reservation](/kubernetes-engine/docs/concepts/tpus#tpu_reservation) .\n- `--enable-autoscaling`: Create a node pool with autoscaling enabled.- ``: Minimum number of all nodes in the node pool. Omit this field unless autoscaling is also specified.\n- ``: Maximum number of all nodes in the node pool. Omit this field unless autoscaling is also specified.\n- `--spot`: Sets the node pool to use [Spot VMs](/kubernetes-engine/docs/concepts/spot-vms) for the nodes in the node pool. This cannot be changed after node pool creation.\n- Ensure that you use the version 4.84.0 or later of the [google](https://registry.terraform.io/providers/hashicorp/google/latest) provider.\n- Add the following block to your Terraform configuration:\n```\nresource \"google_container_node_pool\" \"NODE_POOL_RESOURCE_NAME\" {\u00a0 provider \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = google\u00a0 project \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= PROJECT_ID\u00a0 cluster \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= CLUSTER_NAME\u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = POOL_NAME\u00a0 location \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = CLUSTER_LOCATION\u00a0 node_locations \u00a0 \u00a0 = [NODE_ZONES]\u00a0 initial_node_count = NUM_NODES\u00a0 autoscaling {\u00a0 \u00a0 total_min_node_count = TOTAL_MIN_NODES\u00a0 \u00a0 total_max_node_count = TOTAL_MAX_NODES\u00a0 \u00a0 location_policy \u00a0 \u00a0 \u00a0= \"ANY\"\u00a0 }\u00a0 node_config {\u00a0 \u00a0 machine_type = MACHINE_TYPE\u00a0 \u00a0 reservation_affinity {\u00a0 \u00a0 \u00a0 consume_reservation_type = \"SPECIFIC_RESERVATION\"\u00a0 \u00a0 \u00a0 key = \"compute.googleapis.com/reservation-name\"\u00a0 \u00a0 \u00a0 values = [RESERVATION_LABEL_VALUES]\u00a0 \u00a0 }\u00a0 \u00a0 spot = true\u00a0 }}\n```\nReplace the following:- ``: The name of the node pool resource in the Terraform template.\n- ``: Your project ID.\n- ``: The name of the existing cluster.\n- ``: The name of the node pool to create.\n- ``: The compute zone(s) of the cluster. Specify the region where the TPU version is available. To learn more, see [Select a TPU version and topology](/kubernetes-engine/docs/how-to/tpus#version_and_topology) .\n- ``: The comma-separated list of one or more zones where GKE creates the node pool.\n- ``: The initial number of nodes in the node pool in each of the node pool's zones. If omitted, default is`3`. If auto-scaling is enabled for the node pool using the austoscaling template, we recommend that you set``to`0`, since GKE provisions additional TPU nodes as soon as your workload demands them.\n- ``: The type of TPU machine to use. To see TPU compatible machine types, use the table in [Mapping of TPU configuration](/kubernetes-engine/docs/concepts/tpus#configuration) .\nOptionally, you can also use the following variables:- `autoscaling`: Create a node pool with autoscaling enabled. For single-host TPU slice, GKE scales between the`TOTAL_MIN_NODES`and`TOTAL_MAX_NODES`values.- ``: Minimum number of all nodes in the node pool. This field is optional unless autoscaling is also specified.\n- ``: Maximum number of all nodes in the node pool. This field is optional unless autoscaling is also specified.\n- ``: If you use [TPU reservation](/kubernetes-engine/docs/concepts/tpus#tpu_reservation) , this is the list of labels of the reservation resources to use when creating the node pool. To learn more about how to populate the``in the`reservation_affinity`field, see [Terraform Provider](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster#reservation_affinity) .\n- `spot`: Sets the node pool to use Spot VMs for the TPU nodes. This cannot be changed after node pool creation. For more information, see [Spot VMs](/kubernetes-engine/docs/concepts/spot-vms) .\nTo create a node pool with TPUs:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Click **Add node pool** .\n- In the **Node pool details** section, check the **Specify node locations** box.\n- Select the zone based on the TPU version you want to use:- For TPU v4, use`us-central2-b`.\n- For TPU v5e machine types beginning with`ct5l-`, use`us-central1-a`or`europe-west4-b`.\n- For TPU v5e machine types beginning with`ct5lp-`, use`us-west1-c`,`us-west4-a`,`us-west4-b`,`us-central1-a`,`us-east1-c`,`us-east5-b`, or`europe-west4-a`.\n- For TPU v5p, use`us-east1-d`,`us-east5-a`, or`us-east5-c`.\n- From the navigation pane, click **Nodes** .\n- In the **Machine Configuration** section, select **TPUs** .\n- In the **Series** drop-down menu, select one of the following:- **CT4P** : TPU v4\n- **CT5LP** : TPU v5e\n- **CT5P** : TPU v5p\n- In the **Machine type** drop-down menu, select the name of the machine to use for nodes. Use the [Mapping of TPU configuration](/kubernetes-engine/docs/concepts/tpus#configuration) table to learn how to define the machine type and TPU topology that create a TPU node pool.\n- In the **TPU Topology** drop-down menu, select the physical topology for the TPU slice.\n- In the **Changes needed** dialog, click **Make changes** .\n- Ensure that **Boot disk type** is either **Standard persistent disk** or **SSD persistent disk** .\n- Optionally, select the **Enable nodes on spot VMs** checkbox to use Spot VMs for the nodes in the node pool.\n- Click **Create** .\n### Multi-host TPU slice\nYou can create a [multi-host TPU slice](/kubernetes-engine/docs/concepts/tpus#node_pool) node pool using the Google Cloud CLI, Terraform, or the Google Cloud console.\n```\ngcloud container node-pools create POOL_NAME \\\u00a0 \u00a0 --location=LOCATION \\\u00a0 \u00a0 --cluster=CLUSTER_NAME \\\u00a0 \u00a0 --node-locations=NODE_ZONE \\\u00a0 \u00a0 --machine-type=MACHINE_TYPE \\\u00a0 \u00a0 --tpu-topology=TPU_TOPOLOGY \\\u00a0 \u00a0 --num-nodes=NUM_NODES \\\u00a0 \u00a0 [--spot \\]\u00a0 \u00a0 [--enable-autoscaling \\\u00a0 \u00a0 \u00a0 --max-nodes MAX_NODES]\u00a0 \u00a0 [--reservation-affinity=specific \\\u00a0 \u00a0 --reservation=RESERVATION_NAME]\n```\nReplace the following:- ``: The name of the new node pool.\n- ``: The name of the zone based on the TPU version you want to use:- For TPU v4, use`us-central2-b`.\n- TPU v5e machine types beginning with`ct5l-`are never multi-host.\n- For TPU v5e machine types beginning with`ct5lp-`, use`us-west1-c`,`us-west4-a`,`us-west4-b`,`us-central1-a`,`us-east1-c`,`us-east5-b`, or`europe-west4-a`.\n- For TPU v5p machine types beginning with`ct5p-`, use`us-east1-d`,`us-east5-a`, or`us-east5-c`. To learn more, see [Select a TPU version and topology](/kubernetes-engine/docs/how-to/tpus#version_and_topology) .\n- ``: The name of the cluster.\n- ``: The comma-separated list of one or more zones where GKE creates the node pool.\n- ``: The type of machine to use for nodes. To learn more about the available machine types, see [Mapping of TPU configuration](/kubernetes-engine/docs/concepts/tpus#configuration) .\n- ``: The physical topology for the TPU slice. The format of the topology depends on the TPU version as follows:- TPU v4: Define the topology in 3-tuples (`{A}x{B}x{C}`), for example`4x4x4`.\n- TPU v5e: Define the topology in 2-tuples (`{A}x{B}`), for example`2x2`.\n- ``: The number of nodes in the node pool. It must be zero or the product of the values defined in``(`{A}x{B}x{C}`) divided by the number of chips in each VM. For multi-host TPU v4 and TPU v5e, the number of chips in each VM is four. Therefore, if your``is`2x4x4`(TPU v4 with four chips in each VM), then the``is 32/4 which equals to 8.\nOptionally, you can also use the following flags:- ``: The name of the reservation GKE uses when creating the node pool. If you omit this flag, GKE uses available TPU node pools. To learn more about TPU reservations, see [TPU reservation](/kubernetes-engine/docs/concepts/tpus#tpu_reservation) .\n- `--spot`: Sets the node pool to use Spot VMs for the TPU nodes. This cannot be changed after node pool creation. For more information, see [Spot VMs](/kubernetes-engine/docs/concepts/spot-vms) .\n- `--enable-autoscaling`: Create a node pool with autoscaling enabled. When GKE scales a multi-host TPU slice node pool, it [atomically](/kubernetes-engine/docs/concepts/tpus#terminology) scales up the node pool from zero to the maximum size.- ``: The maximum size of the node pool. The`--max-nodes`flag is required if`--enable-autoscaling`is supplied and must be equal to the product of the values defined in``(`{A}x{B}x{C}`) divided by the number of chips in each VM.- Ensure that you use the version 4.84.0 or later of the [google](https://registry.terraform.io/providers/hashicorp/google/latest) provider.\n- Add the following block to your Terraform configuration:```\nresource \"google_container_node_pool\" \"NODE_POOL_RESOURCE_NAME\" {\u00a0 provider \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = google\u00a0 project \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= PROJECT_ID\u00a0 cluster \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0= CLUSTER_NAME\u00a0 name \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = POOL_NAME\u00a0 location \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 = CLUSTER_LOCATION\u00a0 node_locations \u00a0 \u00a0 = [NODE_ZONES]\u00a0 initial_node_count = NUM_NODES\u00a0 autoscaling {\u00a0 \u00a0 max_node_count = MAX_NODES\u00a0 \u00a0 location_policy \u00a0 \u00a0 \u00a0= \"ANY\"\u00a0 }\u00a0 node_config {\u00a0 \u00a0 machine_type = MACHINE_TYPE\u00a0 \u00a0 reservation_affinity {\u00a0 \u00a0 \u00a0 consume_reservation_type = \"SPECIFIC_RESERVATION\"\u00a0 \u00a0 \u00a0 key = \"compute.googleapis.com/reservation-name\"\u00a0 \u00a0 \u00a0 values = [RESERVATION_LABEL_VALUES]\u00a0 \u00a0 }\u00a0 \u00a0 spot = true\u00a0 }\u00a0 placement_policy {\u00a0 \u00a0 type = \"COMPACT\"\u00a0 \u00a0 tpu_topology = TPU_TOPOLOGY\u00a0 }}\n```Replace the following:- ``: The name of the node pool resource in the Terraform template.\n- ``: Your project ID.\n- ``: The name of the existing cluster to add the node pool to.\n- ``: The name of the node pool to create.\n- ``: Compute location for the cluster. We recommend having a regional cluster for higher reliability of the Kubernetes control plane. You can also use a zonal cluster. To learn more, see [Select a TPU version and topology](/kubernetes-engine/docs/how-to/tpus#version_and_topology) .\n- ``: The comma-separated list of one or more zones where GKE creates the node pool.\n- ``: The number of nodes in the node pool. It must be zero or the product of the number of the TPU chips divided by four, because in multi-host TPU slices each TPU node has 4 chips. For example, if``is`4x8`, then there are 32 chips which means``must be 8. To learn more about TPU topologies, use the table in [Mapping of TPU configuration](/kubernetes-engine/docs/concepts/tpus#configuration) .\n- ``: This indicates the desired physical topology for the TPU slice. The format of the topology depends on the TPU version you are using:- For TPU v4: Define the topology in 3-tuples (`{A}x{B}x{C}`), for example`4x4x4`.\n- For TPU v5e: Define the topology in 2-tuples (`{A}x{B}`), for example`2x2`.Optionally, you can also use the following variables:- ``: If you use [TPUreservation](/kubernetes-engine/docs/concepts/tpus#tpu_reservation) , this is the list of labels of the reservation resources to use when creating the node pool. To learn more about how to populate the``in the`reservation_affinity`field, see [Terraform Provider](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/container_cluster#reservation_affinity) .\n- `autoscaling`: Create a node pool with autoscaling enabled. When GKE scales a multi-host TPU slice node pool, it [atomically](/kubernetes-engine/docs/concepts/tpus#terminology) scales up the node pool from zero to the maximum size.- ``: It is the maximum size of the node pool. It must be equal to the product of the values defined in``(`{A}x{B}x{C}`) divided by the number of chips in each VM.\n- `spot`: Lets the node pool to use Spot VMs for the TPU nodes. This cannot be changed after node pool creation. For more information, see [Spot VMs](/kubernetes-engine/docs/concepts/spot-vms) .\nTo create a node pool with TPUs:- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- In the cluster list, click the name of the cluster you want to modify.\n- Click **Add node pool** .\n- In the **Node pool details** section, check the **Specify node locations** box.\n- Select the zone based on the TPU version you want to use:- For TPU v4, use`us-central2-b`.\n- TPU v5e machine types beginning with`ct5l-`are never multi-host.\n- For TPU v5e machine types beginning with`ct5lp-`, use`us-west1-c`,`us-west4-a`,`us-west4-b`,`us-central1-a`,`us-east1-c`,`us-east5-b`, or`europe-west4-a`.\n- For TPU v5p machine types beginning with`ct5p-`, use`us-east1-d`,`us-east5-a`, or`us-east5-c`.\n- From the navigation pane, click **Nodes** .\n- In the **Machine Configuration** section, select **TPUs** .\n- In the **Series** drop-down menu, select one of the following:- **CT4P** : For TPU v4.\n- **CT5LP** : For TPU v5e.\n- In the **Machine type** drop-down menu, select the name of the machine to use for nodes. Use the [Mapping of TPU configuration](/kubernetes-engine/docs/concepts/tpus#configuration) table to learn how to define the machine type and TPU topology that create a TPU node pool.\n- In the **TPU Topology** drop-down menu, select the physical topology for the TPU slice.\n- In the **Changes needed** dialog, click **Make changes** .\n- Ensure that **Boot disk type** is either **Standard persistent disk** or **SSD persistent disk** .\n- Optionally, select the **Enable nodes on spot VMs** checkbox to use Spot VMs for the nodes in the node pool.\n- Click **Create** .\n### Provisioning state\nIf GKE cannot create your TPU slice node pool due to insufficient TPU capacity available, GKE returns an error message indicating the TPU nodes cannot be created due to lack of capacity.\nIf you are creating a single-host TPU slice node pool, the error message will look similar to this:\n```\n2 nodes cannot be created due to lack of capacity. The missing nodes will be\ncreated asynchronously once capacity is available. You can either wait for the\nnodes to be up, or delete the node pool and try re-creating it again later.\n```\nIf you are creating a multi-host TPU slice node pool, the error message will look similar to this:\n```\nThe nodes (managed by ...) cannot be created now due to lack of capacity. They\nwill be created asynchronously once capacity is available. You can either wait\nfor the nodes to be up, or delete the node pool and try re-creating it again\nlater.\n```\nYour TPU provisioning request may stay in the queue for a long time and will remain in the \"Provisioning\" state while in the queue.\nOnce capacity is available, GKE creates the remaining nodes that were not created.\nIf you need capacity sooner, consider trying [Spot VMs](/sdk/gcloud/reference/container/node-pools/create#--spot) , though note that [Spot VMs consume different quota](#ensure-quota) than on-demand instances.\nYou may delete the queued TPU request by [deleting the TPU slice nodepool](https://cloud.google.com/kubernetes-engine/docs/how-to/node-pools#deleting_a_node_pool) .\n## Run your workload on TPU nodes\n### Workload preparation\nAfter you create your cluster and node pools, you can set up your workloads. As a prerequisite, you have to complete the following workload preparation steps:\n- Frameworks like JAX, PyTorch, and TensorFlow access TPU VMs using the `libtpu` shared library. `libtpu` includes the XLA compiler, TPU runtime software, and the TPU driver. Each release of PyTorch and JAX requires a certain `libtpu.so` version. To use TPUs in GKE, ensure that you use the following versions:| TPU version | libtpu.so version                                 |\n|:--------------|:---------------------------------------------------------------------------------------------------------------------------------------------------|\n| TPU v4  | Recommended jax[tpu] version: 0.4.4 or later. Recommended torchxla[tpuvm] version: v2.0.0 or later.            |\n| TPU v5e  | Recommended jax[tpu] version: v0.4.9 or later. Recommended torchxla[tpuvm] version: v2.1.0 or later.            |\n| TPU v5p  | Recommended jax[tpu] version: 0.4.19 or later. Recommended torchxla[tpuvm] version: suggested to use a nightly version build on October 23, 2023. |\n- Set the following environment variables for the container requesting the TPU resources:- `TPU_WORKER_ID`: A unique integer for each Pod. This ID denotes a unique worker-id in the TPU slice. The supported values for this field range from zero to the number of Pods minus one.\n- `TPU_WORKER_HOSTNAMES`: A comma-separated list of TPU VM hostnames or IP addresses that need to communicate with each other within the slice. There should be a hostname or IP address for each TPU VM in the slice. The list of IP addresses or hostnames are ordered and zero indexed by the`TPU_WORKER_ID`.\nGKE automatically injects these environment variables by using a mutating webhook when a Job is created with the `completionMode: Indexed, subdomain` , `parallelism > 1` , and requesting `google.com/tpu` properties. GKE adds a headless Service so that the DNS records are added for the [Pods backing the Service](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services) .When deploying TPU multi-host resources with [Kuberay](https://github.com/ray-project/kuberay) , GKE provides a deployable [webhook](https://github.com/GoogleCloudPlatform/ai-on-gke/blob/main/applications/ray/kuberay-tpu-webhook/) as part of the [Terraform templates](https://github.com/GoogleCloudPlatform/ai-on-gke/tree/main/applications/ray) for running Ray on GKE. Instructions for running Ray on GKE with TPUs can be found in the [TPU User Guide](https://github.com/GoogleCloudPlatform/ai-on-gke/blob/main/applications/ray/TPU_guide.md) . The mutating webhook will inject these environment variables into Ray clusters requesting `google.com/tpu` properties and a multi-host `cloud.google.com/gke-tpu-topology` node selector.\n- In your workload manifest, add Kubernetes node selectors to ensure that GKE schedules your TPU workload on the TPU machine type and TPU topology you defined:```\nnodeSelector:\u00a0 cloud.google.com/gke-tpu-accelerator: TPU_ACCELERATOR\u00a0 cloud.google.com/gke-tpu-topology: TPU_TOPOLOGY\n```\nReplace the following:\n- `` : The name of the TPU accelerator:- For TPU v4, use`tpu-v4-podslice`.\n- For TPU v5e machine types beginning with`ct5l-`, use`tpu-v5-lite-device`,\n- For TPU v5e machine types beginning with`ct5lp-`, use`tpu-v5-lite-podslice`.\n- For TPU v5p, use`tpu-v5p-slice`.\n- `` : The physical topology for the TPU slice. The format of the topology depends on the TPU version as follows:- TPU v4: Define the topology in 3-tuples (`{A}x{B}x{C}`), for example`4x4x4`.\n- TPU v5e: Define the topology in 2-tuples (`{A}x{B}`), for example`2x2`.\n- TPU v5p: Define the topology in 3-tuples (`{A}x{B}x{C}`), for example`4x4x4`.After you complete the workload preparation, you can run a Job that uses TPUs.\nThe following sections show examples on how to run a Job that performs simple computation with TPUs.\n### Example 1: Run a workload that displays the number of available TPU chips in a TPU node pool\nThis example includes the following configuration:\n- TPU version: v4 (`tpu-v4-podslice`)\n- Topology: 2x2x4\n- Type of node pool: Multi-host TPU slice\n- Create the following `tpu-job.yaml` manifest:```\napiVersion: v1kind: Servicemetadata:\u00a0 name: headless-svcspec:\u00a0 clusterIP: None \u00a0 selector:\u00a0 \u00a0 job-name: tpu-job-podslice---apiVersion: batch/v1kind: Jobmetadata:\u00a0 name: tpu-job-podslicespec:\u00a0 backoffLimit: 0\u00a0 completions: 4\u00a0 parallelism: 4\u00a0 completionMode: Indexed\u00a0 template:\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 subdomain: headless-svc\u00a0 \u00a0 \u00a0 restartPolicy: Never\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-accelerator: tpu-v4-podslice\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-tpu-topology: 2x2x4\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: tpu-job\u00a0 \u00a0 \u00a0 \u00a0 image: python:3.10\u00a0 \u00a0 \u00a0 \u00a0 ports: \u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8471 # Default port using which TPU VMs communicate\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 8431 # Port to export TPU runtime metrics, if supported.\u00a0 \u00a0 \u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 \u00a0 \u00a0 command:\u00a0 \u00a0 \u00a0 \u00a0 - bash\u00a0 \u00a0 \u00a0 \u00a0 - -c\u00a0 \u00a0 \u00a0 \u00a0 - |\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 pip install 'jax[tpu]' -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 python -c 'import jax; print(\"TPU cores:\", jax.device_count())'\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 4\n```Each TPU node has the following node labels:- `cloud.google.com/gke-accelerator-type: tpu-v4-podslice`\n- `cloud.google.com/gke-tpu-topology: 2x2x4`\n- Apply the manifest:```\nkubectl apply -f tpu-job.yaml\n```GKE runs a TPU v4 slice with four TPU VMs (multi-host TPU slice). The node pool has 16 interconnected chips.\n- Verify that the Job created four Pods:```\nkubectl get pods\n```The output is similar to the following:```\nNAME      READY STATUS  RESTARTS AGE\ntpu-job-podslice-0-5cd8r 0/1  Completed 0   97s\ntpu-job-podslice-1-lqqxt 0/1  Completed 0   97s\ntpu-job-podslice-2-f6kwh 0/1  Completed 0   97s\ntpu-job-podslice-3-m8b5c 0/1  Completed 0   97s\n```\n- Get the logs of one of the Pods:```\nkubectl logs POD_NAME\n```Replace `` with the name of one of the created Pods. For example, `tpu-job-podslice-0-5cd8r` .The output is similar to the following:```\nTPU cores: 16\n```\n### Example 2: run a workload that displays the number of available TPU chips in the TPU VM\n- Topology: 2x4\n- TPU version: v5e (`tpu-v5-lite-podslice`)\n- Type of node pool: Single-host TPU slice\n- Create the following manifest as `tpu-pod.yaml````\napiVersion: v1kind: Podmetadata:\u00a0 name: tpu-job-jax-v5spec:\u00a0 restartPolicy: Never\u00a0 nodeSelector:\u00a0 \u00a0 cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice\u00a0 \u00a0 cloud.google.com/gke-tpu-topology: 2x4\u00a0 containers:\u00a0 - name: tpu-job\u00a0 \u00a0 image: python:3.10\u00a0 \u00a0 ports: \u00a0 \u00a0 - containerPort: 8431 # Port to export TPU runtime metrics, if supported.\u00a0 \u00a0 securityContext:\u00a0 \u00a0 \u00a0 privileged: true\u00a0 \u00a0 command:\u00a0 \u00a0 - bash\u00a0 \u00a0 - -c\u00a0 \u00a0 - |\u00a0 \u00a0 \u00a0 pip install 'jax[tpu]' -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\u00a0 \u00a0 \u00a0 python -c 'import jax; print(\"Total TPU chips:\", jax.device_count())'\u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 8\u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 google.com/tpu: 8\n```This manifest includes the following node labels:- `cloud.google.com/gke-tpu-accelerator: tpu-v5-lite-podslice`\n- `cloud.google.com/gke-tpu-topology: 2x4`\nGKE provisions a node pool with eight single-host TPU slices that use TPU v5e. Each TPU VM has eight chips (single-host TPU slice).## Upgrade node pools using accelerators (GPUs and TPUs)\nGKE [automatically upgrades](/kubernetes-engine/docs/concepts/cluster-upgrades#upgrading_automatically) Standard clusters, including node pools. You can also [manuallyupgrade](/kubernetes-engine/docs/how-to/upgrading-a-cluster#upgrade_nodes) node pools if you want your nodes on a later version sooner. To control how upgrades work for your cluster, use [releasechannels](/kubernetes-engine/docs/concepts/release-channels) , [maintenancewindows andexclusions](/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions) , and [rolloutsequencing](/kubernetes-engine/docs/concepts/about-rollout-sequencing) .\nYou can also configure a [node upgradestrategy](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies) for your node pool, such as [surgeupgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#surge) or [blue-greenupgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#blue-green-upgrade-strategy) . By configuring these strategies, you can ensure that the node pools are upgraded in a way that achieves the optimal balance between speed and disruption for your environment. For [multi-host TPU slice nodepools](/kubernetes-engine/docs/concepts/tpus#node_pool) , instead of using the configured node upgrade strategy, GKE atomically recreates the entire node pool in a single step. To learn more, see the definition of in [Terminology related to TPU inGKE](/kubernetes-engine/docs/concepts/tpus#terminology) .\nUsing a node upgrade strategy will temporarily require GKE to provision additional resources, depending on the configuration. If Google Cloud has limited capacity for your node pool's resources\u2014for example, you're seeing [resource availability](/compute/docs/troubleshooting/troubleshooting-resource-availability) errors when trying to create more nodes with GPUs or TPUs\u2014see [Upgrade in aresource-constrainedenvironment](/kubernetes-engine/docs/how-to/node-upgrades-quota#upgrade-resource-constrained) .\n## Clean up\nTo avoid incurring charges to your Google Cloud account for the resources used in this guide, consider deleting the TPU node pools that no longer have scheduled workloads. If the workloads running must be gracefully terminated, use [kubectl drain](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#drain) to clean up the workloads before you delete the node.\n- Delete a TPU node pool:```\ngcloud container node-pools delete POOL_NAME \\\u00a0 \u00a0 --location=LOCATION \\\u00a0 \u00a0 --cluster=CLUSTER_NAME\n```Replace the following:- ``: The name of the node pool.\n- ``: The name of the cluster.\n- ``: The compute location of the cluster.\n## Additional configurations\nThe following sections describe the additional configurations you can apply to your TPU workloads.\n### Multislice\nYou can aggregate smaller slices together in a Multislice to handle larger training workloads. For more information, see [Multislice TPUs in GKE](/tpu/docs/tpu-gke-multislice) .\n### Migrate your TPU reservation\nIf you have existing TPU reservations, you must first migrate your TPU reservation to a new Compute Engine-based reservation system. You can also create Compute Engine-based reservation system where no migration is needed. To learn how to migrate your TPU reservations, see [TPU reservation](/kubernetes-engine/docs/concepts/tpus#tpu_reservation) .\n### Logging\nLogs emitted by containers running on GKE nodes, including TPU VMs, are [collected](/stackdriver/docs/solutions/gke/managing-logs) by the GKE logging agent, sent to Logging, and are [visible in Logging](/stackdriver/docs/solutions/gke/using-logs) .\n### Use GKE node auto-provisioning\nYou can configure GKE to automatically create and delete node pools to meet the resource demands of your TPU workloads. For more information, see [Configuring Cloud TPUs](/kubernetes-engine/docs/how-to/node-auto-provisioning#tpu) .\n### TPU node auto repair\nIf a TPU node in a multi-host TPU slice node pool is unhealthy, the entire node pool is recreated. Conditions that result in unhealthy TPU nodes include the following:\n- Any TPU node with common node [conditions](/kubernetes-engine/docs/how-to/node-auto-repair#repair_criteria) .\n- Any TPU node with an unallocatable TPU count larger than zero.\n- Any TPU VM instance that is stopped (due to preemption) or is terminated.\n- Node maintenance: If any TPU node (VM) within a multi-host TPU slice node pool goes down for host maintenance, GKE recreates the entire TPU slice.\nYou can see the repair status (including the failure reason) in the [operation history](/kubernetes-engine/docs/how-to/node-auto-repair#node_repair_history) . If the failure is caused by insufficient quota, contact your Google Cloud account representative to increase the corresponding quota.\n### Observability and metrics\nIn GKE version 1.27.4-gke.900 or later, TPU workloads that use JAX version [0.4.14](https://github.com/google/jax/releases/tag/jaxlib-v0.4.14) or later and specify `containerPort: 8431` export TPU utilization metrics as GKE [system metrics](/stackdriver/docs/solutions/gke/managing-metrics#system-metrics) . The following metrics are available in Cloud Monitoring to monitor your TPU workload's runtime performance:\n- Duty cycle: Percentage of time over the past sampling period (60 seconds) during which the TensorCores were actively processing on a TPU chip. Larger percentage means better TPU utilization.\n- Memory used: Amount of accelerator memory allocated in bytes. Sampled every 60 seconds.\n- Memory total: Total accelerator memory in bytes. Sampled every 60 seconds.\nThese metrics are located in the Kubernetes node ( `k8s_node` ) and Kubernetes container ( `k8s_container` ) schema.\nKubernetes container:\n- `kubernetes.io/container/accelerator/duty_cycle`\n- `kubernetes.io/container/accelerator/memory_used`\n- `kubernetes.io/container/accelerator/memory_total`\nKubernetes node:\n- `kubernetes.io/node/accelerator/duty_cycle`\n- `kubernetes.io/node/accelerator/memory_used`\n- `kubernetes.io/node/accelerator/memory_total`In GKE version 1.28.1-gke.1066000 or later, TPU VM export TPU utilization metrics as GKE [system metrics](/stackdriver/docs/solutions/gke/managing-metrics#system-metrics) . The following metrics are available in Cloud Monitoring to monitor your TPU host's performance:\n- TensorCore utilization: Current percentage of the TensorCore that is utilized. The TensorCore value equals the sum of the [matrix-multiply units (MXUs) plus the vector unit](/tpu/docs/system-architecture-tpu-vm#tpu_chip) . The TensorCore utilization value is the division of the TensorCore operations that wereover the past sample period (60 seconds) by thenumber of TensorCore operations over the same period.Larger value means better utilization.\n- Memory Bandwidth utilization: Current percentage of the accelerator memory bandwidth that is being used. Computed by dividing the memory bandwidth used over a sample period (60s) by the maximum supported bandwidth over the same sample period.\nThese metrics are located in the Kubernetes node ( `k8s_node` ) and Kubernetes container ( `k8s_container` ) schema.\nKubernetes container:\n- `kubernetes.io/container/accelerator/tensorcore_utilization`\n- `kubernetes.io/container/accelerator/memory_bandwidth_utilization`\nKubernetes node:\n- `kubernetes.io/container/node/tensorcore_utilization`\n- `kubernetes.io/container/node/memory_bandwidth_utilization`\nFor more information, see [Kubernetes metrics](/monitoring/api/metrics_kubernetes#kubernetes-kubernetes) and [GKE system metrics](/stackdriver/docs/solutions/gke/managing-metrics#system-metrics) .\n### Run containers without privileged mode\n**Note:** Containers running in nodes in GKE version 1.28 or later don't need to have privileged mode enabled to access TPUs.\nIf your TPU node is running versions less than 1.28, read the following section:\nA container running on a TPU VM needs access to higher limits on locked memory so the driver can communicate with the TPU chips over direct memory access (DMA). To enable this, you must configure a higher [ulimit](https://ss64.com/bash/ulimit.html) . If you want to reduce the permission scope on your container, complete the following steps:\n- Edit the `securityContext` to include the following fields:```\nsecurityContext:\u00a0 capabilities:\u00a0 \u00a0 add: [\"SYS_RESOURCE\"]\n```\n- Increase `ulimit` by running the following command inside the container before your setting up your workloads to use TPU resources:```\nulimit -l 68719476736\n```\n**Note** : For TPU v5e, running containers without privileged mode is available in clusters in version 1.27.4-gke.900 and later.\n## Known issues\n- Cluster autoscaler might wrongly calculate capacity for new TPU nodes before those nodes report available TPUs. Cluster autoscaler might then perform additional scale up and as a result create more nodes than needed. Cluster autoscaler will scale down additional nodes, if they are not needed, after regular scale down operation.\n- Cluster autoscaler cancels scaling up of TPU node pools that remain in waiting status for more than 15 minutes. Cluster Autoscaler will retry such scale up operations later. This behavior might reduce TPU obtainability for customers who don't use reservations.\n- Non-TPU workloads that have a toleration for the TPU taint may prevent scale down of the node pool if they are being recreated during draining of the TPU node pool.## What's next\n- [Serve Large Language Models with Saxml on TPUs](https://cloud.google.com/kubernetes-engine/docs/tutorials/tpu-multihost-saxml) \n- [Learn more about setting up Ray on GKE with TPUs](https://github.com/GoogleCloudPlatform/ai-on-gke/blob/main/ray-on-gke/TPU_guide.md#tpu-user-guide) \n- [Build large-scale machine learning on Cloud TPUs withGKE](https://www.youtube.com/watch?v=wtKhG1aTgtY) \n- [Serve Large Language Models with KubeRay onTPUs](https://www.youtube.com/watch?v=RK_u6cfPnnw) \n- [Troubleshoot TPUs in GKE](/kubernetes-engine/docs/troubleshooting/troubleshoot-tpus)", "guide": "Google Kubernetes Engine (GKE)"}