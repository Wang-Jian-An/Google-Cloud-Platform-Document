{"title": "Google Kubernetes Engine (GKE) - Spot VMs", "url": "https://cloud.google.com/kubernetes-engine/docs/concepts/spot-vms", "abstract": "# Google Kubernetes Engine (GKE) - Spot VMs\nThis page explains what Spot VMs are and how they work in Google Kubernetes Engine (GKE). To learn how to use Spot VMs, refer to [Use Spot VMs](/kubernetes-engine/docs/how-to/spot-vms) .\n", "content": "## Overview of Spot VMs in GKE\nare Compute Engine [virtual machine (VM) instances](/compute/docs/instances) that are priced lower than standard Compute Engine VMs and provide no guarantee of availability. Spot VMs offer the same [machine types](/compute/docs/machine-types) and options as standard VMs.\n**Note:** For better availability, use smaller machine types.\nYou can use Spot VMs in your clusters and node pools to run stateless, batch, or fault-tolerant workloads that can tolerate disruptions caused by the ephemeral nature of Spot VMs.\nSpot VMs remain available until Compute Engine requires the resources for standard VMs. To maximize your cost efficiency, combine using Spot VMs with [Best practices for running cost-optimized Kubernetes applications on GKE](/architecture/best-practices-for-running-cost-effective-kubernetes-applications-on-gke) .\nTo learn more about Spot VMs, see [Spot VMs](/compute/docs/instances/spot) in the Compute Engine documentation.\n**Note:** Spot Pods are a feature of GKE Autopilot clusters. When you request Spot Pods, Autopilot automatically provisions Spot VMs, adds taints and tolerations, and manages autoscaling and scheduling. Learn more about [Spot Pods](/kubernetes-engine/docs/how-to/autopilot-spot-pods) .\n## Benefits of Spot VMs\nSpot VMs and preemptible VMs share many benefits, including the following:\n- Lower pricing than standard Compute Engine VMs.\n- Useful for stateless, fault-tolerant workloads that are resilient to the ephemeral nature of these VMs.\n- Works with the [cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) and [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) .\nIn contrast to preemptible VMs, which expire after 24 hours, Spot VMs have no expiration time. Spot VMs are only terminated when Compute Engine needs the resources elsewhere.\n**Note:** GKE continues to support using preemptible VMs in your clusters and node pools. However, Spot VMs are recommended and replace the need to use preemptible VMs.\n## How Spot VMs work in GKE\nWhen you create a cluster or node pool with Spot VMs, GKE creates underlying Compute Engine Spot VMs that behave like a [managed instance group (MIG)](/compute/docs/instance-groups) . Nodes that use Spot VMs behave like standard GKE nodes, but with no guarantee of availability. When the resources used by Spot VMs are required to run standard VMs, Compute Engine terminates those Spot VMs to use the resources elsewhere.\n### Termination and graceful shutdown of Spot VMs\nWhen Compute Engine needs to reclaim the resources used by Spot VMs, a [termination notice](/compute/docs/instances/spot#preemption) is sent to GKE. Spot VMs terminate 30 seconds after receiving a termination notice.\nOn clusters running GKE version 1.20 and later, the [kubelet graceful node shutdown feature](https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown) is enabled by default. The kubelet notices the termination notice and gracefully terminates Pods that are running on the node. If the Pods are part of a Deployment, the controller creates and schedules new Pods to replace the terminated Pods.\nOn a best-effort basis, the kubelet grants the following graceful termination period, based on the GKE version of the node pool:\n- **Later than 1.22.8-gke.200** : 15 seconds for non-system Pods, after which system Pods (with the`system-cluster-critical`or`system-node-critical`priority classes) have 15 seconds to gracefully terminate.\n- **1.22.8-gke.200 and earlier** : 25 seconds for non-system Pods, after which system Pods (with the`system-cluster-critical`or`system-node-critical`priority classes) have 5 seconds to gracefully terminate.\n**Note:** Adjusting the values of `terminationGracePeriodSeconds` , `shutdownGracePeriodCriticalPods` , or `shutdownGracePeriod` in your Pod spec to more than the granted time (15 or 25 seconds) has no effect on the graceful termination of nodes that use preemptible VMs or Spot VMs.\nDuring graceful node termination, the kubelet updates the status of the Pods, assigning a `Failed` phase and a `Terminated` reason to the terminated Pods.\n**Note:** For clusters running version 1.27.2-gke.1800 or later, GKE automatically deletes Pods that were evicted during node termination. Use the following instructions to manually delete terminated Pods for earlier GKE versions.\nWhen the number of terminated Pods reaches a threshold of 1000 for clusters with fewer than 100 nodes or 5000 for clusters with 100 nodes or more, [garbage collection](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection) cleans up the Pods.\nYou can also delete terminated Pods manually using the following commands:\n```\n\u00a0 kubectl get pods --all-namespaces | grep -i NodeShutdown | awk '{print $1, $2}' | xargs -n2 kubectl delete pod -n\u00a0 kubectl get pods --all-namespaces | grep -i Terminated | awk '{print $1, $2}' | xargs -n2 kubectl delete pod -n\n```\n**Warning:** In Kubernetes versions 1.22 and earlier, manually deleting shutdown Pods resets the counter that Jobs might use to determine the [backoffLimit](https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-tracking-with-finalizers) .\n## Scheduling workloads on Spot VMs\nGKE automatically adds both the `cloud.google.com/gke-spot=true` and `cloud.google.com/gke-provisioning=spot` (for nodes running GKE version 1.25.5-gke.2500 or later) [labels](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) to nodes that use Spot VMs. You can schedule specific Pods on nodes that use Spot VMs using the [nodeSelector](https://kubernetes.io/docs/concepts/configuration/assign-pod-node/) field in your Pod spec. The following examples use the `cloud.google.com/gke-spot` label:\n```\napiVersion: v1kind: Podspec:\u00a0 nodeSelector:\u00a0 \u00a0 cloud.google.com/gke-spot: \"true\"\n```\nAlternatively, you can use [node affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity) to tell GKE to schedule Pods on Spot VMs, similar to the following example:\n```\napiVersion: v1kind: Podspec:...\u00a0 affinity:\u00a0 \u00a0 nodeAffinity:\u00a0 \u00a0 \u00a0 requiredDuringSchedulingIgnoredDuringExecution:\u00a0 \u00a0 \u00a0 \u00a0 nodeSelectorTerms:\u00a0 \u00a0 \u00a0 \u00a0 - matchExpressions:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - key: cloud.google.com/gke-spot\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 operator: In\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 values:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 - \"true\"...\n```\nYou can also use `nodeAffinity.preferredDuringSchedulingIgnoredDuringExecution` to that GKE places Pods on nodes that use Spot VMs. Preferring Spot VMs is not recommended, because GKE might schedule the Pods onto existing viable nodes that use standard VMs instead.\n**Note:** The cluster autoscaler and node auto-provisioning are unaffected by `preferredDuringSchedulingIgnoredDuringExecution` when making autoscaling decisions.\n### Using taints and tolerations for scheduling\nTo avoid system disruptions, use a [node taint](/kubernetes-engine/docs/how-to/node-taints) to ensure that GKE doesn't schedule critical workloads onto Spot VMs. When you taint nodes that use Spot VMs, GKE only schedules Pods that have the corresponding [toleration](/kubernetes-engine/docs/how-to/node-taints#configuring_pods_to_tolerate_a_taint) onto those nodes.\nIf you use node taints, ensure that your cluster also has at least one node pool that uses standard Compute Engine VMs. Node pools that use standard VMs provide a reliable place for GKE to schedule critical system components like DNS.\nFor information on using a node taint for Spot VMs, see [Use taints and tolerations for Spot VMs](/kubernetes-engine/docs/how-to/spot-vms#use_taints_and_tolerations_for) .\n### Using Spot VMs with GPU node pools\nSpot VMs support using [GPUs](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus) . When you create a new GPU node pool, GKE automatically adds the `nvidia.com/gpu=present:NoSchedule` taint to the new nodes. Only Pods with the corresponding toleration can run on these nodes. GKE automatically adds this toleration to Pods that request GPUs.\nYour cluster must have at least one existing non-GPU node pool that uses standard VMs before you create a GPU node pool that uses Spot VMs. If your cluster only has a GPU node pool with Spot VMs, GKE doesn't add the `nvidia.com/gpu=present:NoSchedule` taint to those nodes. As a result, GKE might schedule system workloads onto the GPU node pools with Spot VMs, which can lead to disruptions because of the Spot VMs and can increase your resource consumption because GPU nodes are more expensive than non-GPU nodes.\n## Cluster autoscaler and node auto-provisioning\nYou can use the [cluster autoscaler](/kubernetes-engine/docs/how-to/cluster-autoscaler) and [node auto-provisioning](/kubernetes-engine/docs/how-to/node-auto-provisioning) to automatically scale your clusters and node pools based on the demands of your workloads. Both the cluster autoscaler and node auto-provisioning support using Spot VMs.\n### Spot VMs and node auto-provisioning\n**Note:** Spot VMs are only supported by node auto-provisioning in GKE version 1.21 and later. Preemptible VMs can be used in earlier GKE versions.\nNode auto-provisioning automatically creates and deletes node pools in your cluster to meet the demands of your workloads. When you [schedule workloads thatrequire Spot VMs](#scheduling-workloads) by using a `nodeSelector` or node affinity, node auto-provisioning creates new node pools to accommodate the workloads' Pods. GKE automatically adds the `cloud.google.com/gke-spot=true:NoSchedule` taint to nodes in the new node pools. Only Pods with the corresponding toleration can run on nodes in those node pools. You must add the corresponding toleration to your deployments to allow GKE to place the Pods on Spot VMs:\n```\n\u00a0 \u00a0tolerations:\u00a0 \u00a0- key: cloud.google.com/gke-spot\u00a0 \u00a0 \u00a0operator: Equal\u00a0 \u00a0 \u00a0value: \"true\"\u00a0 \u00a0 \u00a0effect: NoSchedule\n```\n**Caution:** If you do not specify a toleration and your deployment requires Spot VMs, your Pods remain in an unschedulable state.\nYou can ensure that GKE **only** schedules your Pods on Spot VMs by using **both** a toleration and either a `nodeSelector` or node affinity rule to filter for Spot VMs.\nIf you schedule a workload using only a toleration, GKE can schedule the Pods onto either Spot VMs or existing standard VMs with capacity. If you require a workload to be scheduled on Spot VMs, use a `nodeSelector` or a node affinity in addition to a toleration. To learn more, see [Scheduling workloads on Spot VMs](#scheduling-workloads) .\n### Spot VMs and cluster autoscaler\nThe [cluster autoscaler](/kubernetes-engine/docs/concepts/cluster-autoscaler) automatically adds and removes nodes in your node pools based on demand. If your cluster has Pods that can't be placed on existing Spot VMs, the cluster autoscaler adds new nodes that use Spot VMs.\n### Default policy\nStarting in GKE version 1.24.1-gke.800, you can define the autoscaler location policy. Cluster autoscaler attempts to provision Spot VMs node pools when resources are available and the default location policy is set to `ANY` . With this policy, Spot VMs have a lower risk of being preempted. For other VM types, the default cluster autoscaler distribution policy is `BALANCED` .\n## Upgrade Standard node pools using Spot VMs\nIf your Standard cluster node pools using Spot VMs are configured to use surge upgrades, GKE creates surge nodes with Spot VMs. However, GKE doesn't wait for the Spot VMs to be ready before cordoning and draining the existing nodes, as Spot VMs provide no guarantee of availability. To learn more, see [Surgeupgrades](/kubernetes-engine/docs/concepts/node-pool-upgrade-strategies#surge) .\n## Modifications to Kubernetes behavior\nUsing Spot VMs on GKE modifies some guarantees and constraints that Kubernetes provides, such as the following:\n- Reclamation of Spot VMs is involuntary and is not covered by the guarantees of [PodDisruptionBudgets](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) . You might experience greater unavailability than your configured`PodDisruptionBudget`.## Best practices for Spot VMs\nWhen designing a system that uses Spot VMs, you can avoid major disruptions by using the following guidelines:\n- Spot VMs have no availability guarantees. Design your systems under the assumption that GKE might reclaim any or all your Spot VMs at any time, with no guarantee of when new instances become available.\n- To ensure that your workloads and Jobs are processed even when no Spot VMs are available, ensure that your clusters have a mix of node pools that use Spot VMs and node pools that use standard Compute Engine VMs.\n- Ensure that your cluster has at least one non-GPU node pool that uses standard VMs before you add a GPU node pool that uses Spot VMs.\n- While the node names do not usually change when nodes are recreated, the internal and external IP addresses used by Spot VMs might change after recreation.\n- Use node taints and tolerations to ensure that critical Pods aren't scheduled onto node pools that use Spot VMs.\n- To run stateful workloads on Spot VMs, test to ensure that your workloads can gracefully terminate within 25 seconds of shutdown to minimize the risk of persistent volume data corruption.\n- Follow the [Kubernetes Pod termination best practices](/blog/products/containers-kubernetes/kubernetes-best-practices-terminating-with-grace) .\n**Warning:** Data on nodes that use Spot VMs is deleted when the instance is reclaimed. Local SSDs with attached Spot VMs do not persist after deletion.\n## What's next\n- [Learn how to use Spot VMs in your node pools](/kubernetes-engine/docs/how-to/spot-vms) .\n- [Learn about autoscaling your clusters](/kubernetes-engine/docs/concepts/cluster-autoscaler) .\n- [Learn how to scale your deployed apps](/kubernetes-engine/docs/how-to/scaling-apps) .\n- [Learn more about Spot VMs in the Compute Engine documentation](/compute/docs/instances/spot) .\n- [Take a tutorial about deploying a batch workload using Spot VMs in GKE](/kubernetes-engine/docs/tutorials/batch-ml-workload) .", "guide": "Google Kubernetes Engine (GKE)"}