{"title": "Google Kubernetes Engine (GKE) - Configuring Ingress for internal Application Load Balancers", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balance-ingress", "abstract": "# Google Kubernetes Engine (GKE) - Configuring Ingress for internal Application Load Balancers\nThis page shows you how to set up and use Ingress for internal Application Load Balancers in Google Kubernetes Engine (GKE). Ingress provides built-in support for internal load balancing through the GKE Ingress controller.\nTo learn more about which features are supported for Ingress for internal Application Load Balancers, see [Ingress features](/kubernetes-engine/docs/how-to/ingress-configuration) . You can also learn more about how Ingress for internal Application Load Balancers works in [Ingress for internal Application Load Balancers](/kubernetes-engine/docs/concepts/ingress-ilb) .\n", "content": "## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n### Requirements\nIngress for internal Application Load Balancers has the following requirements:\n- Your cluster must use a GKE version later than 1.16.5-gke.10.\n- Your cluster must be [VPC-native](/kubernetes-engine/docs/concepts/alias-ips) .\n- Your cluster must have the`HttpLoadBalancing`add-on enabled. This add-on is enabled by default. You must not disable it.\n- You must use [Network Endpoint Groups (NEGs)](/load-balancing/docs/negs) as backends for your Service.## Deploying Ingress for internal Application Load Balancers\nThe following exercises show you how to deploy Ingress for internal Application Load Balancers:\n- [Prepare your environment](#prepare-environment) .\n- [Create a cluster](#create-cluster) .\n- [Deploy an application](#deploy-app) .\n- [Deploy a Service](#deploy-service) .\n- [Deploy Ingress](#deploy-ingress) .\n- [Validate the deployment](#validate) .\n- [Delete Ingress resources](#delete) .\n### Prepare your environment\nBefore you can deploy load balancer resources through the Kubernetes Ingress API, you must prepare your [networking environment](/kubernetes-engine/docs/concepts/ingress-ilb#required_networking_environment) so that the load balancer proxies can be deployed in a given region.\nCreate a proxy-only subnet:\n```\ngcloud compute networks subnets create proxy-only-subnet \\\u00a0 \u00a0 --purpose=REGIONAL_MANAGED_PROXY \\\u00a0 \u00a0 --role=ACTIVE \\\u00a0 \u00a0 --region=COMPUTE_REGION \\\u00a0 \u00a0 --network=NETWORK_NAME \\\u00a0 \u00a0 --range=10.129.0.0/23\n```\nReplace the following:\n- ``: a [Compute Engine region](/compute/docs/regions-zones/viewing-regions-zones) .\n- ``: the name of the network for the subnet.\nFor more information, see [configuring the proxy-only subnet](/load-balancing/docs/l7-internal/setting-up-l7-internal#configuring_the_proxy-only_subnet) .\n### Create a firewall rule\nThe Ingress controller does not create a firewall rule to allow connections from the load balancer proxies in the proxy-subnet. You must create this firewall rule manually. However, the Ingress controller does create firewall rules to allow ingress for Google Cloud health-checks.\nCreate a firewall rule to allow connections from the load balancer proxies in the proxy-only subnet to the pod listening port:\n**Note:** Shared VPC environments require additional preparation. For more information, see [Shared VPC Ingress preparation](/kubernetes-engine/docs/concepts/ingress#shared_vpc) .\n```\ngcloud compute firewall-rules create allow-proxy-connection \\\u00a0 \u00a0 --allow=TCP:CONTAINER_PORT \\\u00a0 \u00a0 --source-ranges=10.129.0.0/23 \\\u00a0 \u00a0 --network=NETWORK_NAME\n```\nReplace `` with the value of the port that the Pod is listening to, such as `9376` .\n### Creating a cluster\nIn this section, you create a VPC-native cluster that you can use with Ingress for internal Application Load Balancers. You can create this cluster using the Google Cloud CLI or the Google Cloud console.\nCreate a cluster in the same network as the proxy-only subnet:\n```\ngcloud container clusters create-auto CLUSTER_NAME \\\u00a0 \u00a0 --location=COMPUTE_LOCATION \\\u00a0 \u00a0 --network=NETWORK_NAME\n```\nReplace the following:- ``: a name for your cluster.\n- ``: the [Compute Engine location](/compute/docs/regions-zones/viewing-regions-zones) for the cluster. You must use the same location as the proxy-subnet that you created in the previous section.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- In the Autopilot section, click **Configure** .\n- In the **Cluster basics** section, complete the following:- Enter the **Name** for your cluster.\n- For the **Location type** , select a [Compute Engine region](/compute/docs/regions-zones#available) for your cluster. You must use the same region as the proxy-subnet that you created in the previous section.\n- In the navigation pane, click **Networking** .\n- In the **Network** list, select the network that you want the cluster to be created in. This network must be in the same VPC network as the proxy-subnet.\n- In the **Node subnet** list, select the proxy-subnet that you created\n- Click **Create** .\n### Deploying a web application\nIn this section, you create a [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/) .\nTo create a Deployment:\n- Save the following sample manifest as `web-deployment.yaml` :```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 labels:\u00a0 \u00a0 app: hostname\u00a0 name: hostname-serverspec:\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: hostname\u00a0 minReadySeconds: 60\u00a0 replicas: 3\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: hostname\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - image: registry.k8s.io/serve_hostname:v1.4\u00a0 \u00a0 \u00a0 \u00a0 name: hostname-server\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 9376\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 \u00a0 terminationGracePeriodSeconds: 90\n```This manifest describes a Deployment that listens on an HTTPS server on port 9376. This Deployment also manages Pods for your application. Each Pod runs one application container with an HTTPS server that returns the hostname of the application server as the response. The default hostname of a Pod is the name of the Pod. The container also handles graceful termination.\n- Apply the manifest to the cluster:```\nkubectl apply -f web-deployment.yaml\n```\n### Deploying a Service as a Network Endpoint Group (NEG)\nIn this section, you create a [Service](/kubernetes-engine/docs/concepts/service) resource. The Service selects the backend containers by their labels so that the Ingress controller can program them as backend endpoints. Ingress for internal Application Load Balancers requires you to use [NEGs](/load-balancing/docs/negs) as backends. The feature does not support Instance Groups as backends. Because NEG backends are required, the following NEG annotation is required when you deploy Services that are exposed through Ingress:\n```\nannotations:\u00a0 cloud.google.com/neg: '{\"ingress\": true}'\n```\nYour Service is automatically annotated with `cloud.google.com/neg: '{\"ingress\": true}'` when all of the following conditions are true:\n- You are using VPC-native clusters.\n- You are not using a Shared VPC.\n- You are not using [GKE Network Policy](/kubernetes-engine/docs/how-to/network-policy) .\nThe annotation is automatically added using a `MutatingWebhookConfiguration` with name `neg-annotation.config.common-webhooks.networking.gke.io` . You can check if the `MutatingWebhookConfiguration` is present with the following command:\n```\nkubectl get mutatingwebhookconfigurations\n```\nThe usage of NEGs allows the Ingress controller to perform [container native load balancing](/kubernetes-engine/docs/concepts/ingress#container-native_load_balancing) . Traffic is load balanced from the Ingress proxy directly to the Pod IP as opposed to traversing the node IP or kube-proxy networking. In addition, [Pod readiness gates](/kubernetes-engine/docs/concepts/container-native-load-balancing#pod_readiness) are implemented to determine the health of Pods from the perspective of the load balancer and not only the Kubernetes readiness and liveness checks. Pod readiness gates ensure that traffic is not dropped during lifecycle events such as Pod startup, Pod loss, or node loss.\nIf you do not include a NEG annotation, you receive a warning on the Ingress object that prevents you from configuring the internal Application Load Balancer. A Kubernetes event is also generated on the Ingress if the NEG annotation is not included. The following message is an example of the event message:\n```\nMessage\n------error while evaluating the ingress spec: could not find port \"8080\" in service \"default/no-neg-svc\"\n```\nAn NEG is not created until an Ingress references the Service. The NEG does not appear in Compute Engine until the Ingress and its referenced Service both exist. NEGs are a zonal resource and for multi-zonal clusters, one is created per Service per zone.\nTo create a Service:\n- Save the following sample manifest as `web-service.yaml` :```\napiVersion: v1kind: Servicemetadata:\u00a0 name: hostname\u00a0 namespace: default\u00a0 annotations:\u00a0 \u00a0 cloud.google.com/neg: '{\"ingress\": true}'spec:\u00a0 ports:\u00a0 - name: host1\u00a0 \u00a0 port: 80\u00a0 \u00a0 protocol: TCP\u00a0 \u00a0 targetPort: 9376\u00a0 selector:\u00a0 \u00a0 app: hostname\u00a0 type: ClusterIP\n```\n- Apply the manifest to the cluster:```\nkubectl apply -f web-service.yaml\n```\n### Deploying Ingress\nIn this section, you create an Ingress resource that triggers the deployment of a Compute Engine load balancer through the Ingress controller. Ingress for internal Application Load Balancers requires the following annotation:\n```\nannotations:\u00a0 \u00a0 kubernetes.io/ingress.class: \"gce-internal\"\n```\nYou cannot use the `ingressClassName` field to specify a GKE Ingress. You must use the `kubernetes.io/ingress.class` annotation. For more information, see [GKE Ingress controller behavior](/kubernetes-engine/docs/concepts/ingress#controller_summary) .\n**Caution:** Any Ingress resource deployed on GKE without the `kubernetes.io/ingress.class` annotation is interpreted as an external Ingress resource which deploys an external load balancer for the Service. To prevent users from exposing applications publicly by accidentally omitting the correct annotation, implement an [Identity and Access Management (IAM) policy](/load-balancing/docs/access-control/iam-conditions) that enforces limits on public load balancer creation. This policy blocks public load balancer deployment cluster-wide while still allowing internal load balancer creation through Ingress and Service resources.\nTo create an Ingress:\n- Save the following sample manifest as `internal-ingress.yaml` :```\napiVersion: networking.k8s.io/v1kind: Ingressmetadata:\u00a0 name: ilb-demo-ingress\u00a0 namespace: default\u00a0 annotations:\u00a0 \u00a0 kubernetes.io/ingress.class: \"gce-internal\"spec:\u00a0 defaultBackend:\u00a0 \u00a0 service:\u00a0 \u00a0 \u00a0 name: hostname\u00a0 \u00a0 \u00a0 port:\u00a0 \u00a0 \u00a0 \u00a0 number: 80\n```\n- Apply the manifest to the cluster:```\nkubectl apply -f internal-ingress.yaml\n```\n### Validating a successful Ingress deployment\nIn this section, you validate if your deployment was successful.\nIt can take several minutes for the Ingress resource to become fully provisioned. During this time, the Ingress controller creates items such as forwarding rules, backend services, URL maps, and NEGs.\nTo retrieve the status of your Ingress resource that you created in the previous section, run the following command:\n```\nkubectl get ingress ilb-demo-ingress\n```\nThe output is similar to the following:\n```\nNAME    HOSTS ADDRESS   PORTS  AGE\nilb-demo-ingress *  10.128.0.58  80  59s\n```\nWhen the `ADDRESS` field is populated, the Ingress is ready. The use of an RFC 1918 address in this field indicates an internal IP within the VPC.\nSince the internal Application Load Balancer is a regional load balancer, the virtual IP (VIP) is only accessible from a client within the same region and VPC. After retrieving the load balancer VIP, you can use tools (for example, `curl` ) to issue `HTTP GET` calls against the VIP from inside the VPC.\nTo issue a `HTTP GET` call, complete the following steps:\n- To reach your Ingress VIP from inside the VPC, deploy a VM within the same region and network as the cluster:```\ngcloud compute instances create l7-ilb-client \\\u00a0 \u00a0 --image-family=debian-10 \\\u00a0 \u00a0 --image-project=debian-cloud \\\u00a0 \u00a0 --network=NETWORK_NAME \\\u00a0 \u00a0 --subnet=SUBNET_NAME \\\u00a0 \u00a0 --zone=COMPUTE_ZONE \\\u00a0 \u00a0 --tags=allow-ssh\n```Replace the following:- ``: the name of a subnet in the network.\n- ``: a [Compute Engine zone](/compute/docs/regions-zones/viewing-regions-zones) in the region.\nTo learn more about creating instances, see [Creating and starting a VM instance](/compute/docs/instances/create-start-instance) .\n- To access the internal VIP from inside the VM, use `curl` :- SSH in to the VM that you created in the previous step:```\ngcloud compute ssh l7-ilb-client \\\u00a0 \u00a0 --zone=COMPUTE_ZONE \n```\n- Use `curl` to access the internal application VIP:```\ncurl 10.128.0.58hostname-server-6696cf5fc8-z4788\n```The successful HTTP response and hostname of one of the backend containers indicates that the full load balancing path is functioning correctly.\n### Deleting Ingress resources\nRemoving Ingress and Service resources also removes the Compute Engine load balancer resources associated with them. To prevent resource leaking, ensure that Ingress resources are torn down when you no longer need them. You must also delete Ingress and Service resources before you delete clusters or else the Compute Engine load balancing resources are orphaned.\nTo remove an Ingress, complete the following steps:\n- Delete the Ingress. For example, to delete the Ingress you created in this page, run the following command:```\nkubectl delete ingress ilb-demo-ingress\n```Deleting the Ingress removes the forwarding rules, backend services, and URL maps associated with this Ingress resource.\n- Delete the Service. For example, to delete the Service you created in this page, run the following command:```\nkubectl delete service hostname\n```Deleting the Service removes the NEG associated with the Service.\nTo deploy an application on GKE and expose the application with a private load balanced IP address, see [Basic Internal Ingress](https://github.com/GoogleCloudPlatform/gke-networking-recipes/tree/main/ingress/single-cluster/ingress-internal-basic) .\n## Static IP addressing\nInternal Ingress resources support both static and ephemeral IP addressing. If an IP address is not specified, an available IP address is automatically allocated from the GKE node subnet. However, the Ingress resource does not provision IP addresses from the [proxy-only subnet](/load-balancing/docs/l7-internal/setting-up-l7-internal#configure-a-network) as that subnet is only used for internal proxy consumption. These ephemeral IP addresses are allocated to the Ingress only for the lifecycle of the internal Ingress resource. If you delete your Ingress and create a new Ingress from the same manifest file, you are not guaranteed to get the same external IP address.\nIf you want a permanent IP address that's independent from the lifecycle of the internal Ingress resource, you must reserve a regional static internal IP address. You can then specify a static IP address by using the `kubernetes.io/ingress.regional-static-ip-name` annotation on your Ingress resource.\nThe following example shows you how to add this annotation:\n```\napiVersion: networking.k8s.io/v1kind: Ingressmetadata:\u00a0 name: my-ingress\u00a0 annotations:\u00a0 \u00a0 kubernetes.io/ingress.regional-static-ip-name: STATIC_IP_NAME\u00a0 \u00a0 kubernetes.io/ingress.class: \"gce-internal\"\n```\nReplace `` with a static IP name that meets the following criteria:\n- Create the static IP address before you deploy the Ingress. A load balancer does not deploy until the static IP exists, and referencing a non-existent IP address resource does not create a static IP. If you modify an existing Ingress to use a static IP address instead of an ephemeral IP address, GKE might change the IP address of the load balancer when GKE re-creates the forwarding rule of the load balancer.\n- The static IP is [reserved in the service project](/vpc/docs/provisioning-shared-vpc#reserve_internal_ip) for an Ingress deployed in the service project of a Shared VPC.\n- Reference the Google Cloud IP address resource by its name, rather than its IP address.\n- The IP address must be from a subnet in the same region as the GKE cluster. You can use any available private subnet within the region (with the exception of the [proxy-only subnet](/load-balancing/docs/l7-internal/setting-up-l7-internal#configure-a-network) ). Different Ingress resources can also have addresses from different subnets.## HTTPS between client and load balancer\nIngress for internal load balancing supports the serving of TLS certificates to clients. You can serve TLS certificates through Kubernetes Secrets or through pre-shared regional SSL certificates in Google Cloud. You can also specify multiple certificates per Ingress resource. Use of both HTTPS and HTTP simultaneously is supported for GKE 1.25+. To enable this feature, you need to create a static IP address with PURPOSE=SHARED_LOADBALANCER_VIP, and configure it on the ingress. If a static IP address is not provided, only HTTPS traffic is allowed, and you need to follow the documentation for [Disabling HTTP](/kubernetes-engine/docs/concepts/ingress-xlb#disabling_http) .\n**Note:** Ingress for internal load balancing does not support pre-shared global SSL certificates or Google-managed certificates.\nThe following steps detail how to create a certificate in Google Cloud and then serve it through Ingress to internal clients for both HTTPS and HTTP traffic:\n- Create the regional certificate:```\ngcloud compute ssl-certificates create CERT_NAME \\\u00a0 \u00a0 --certificate CERT_FILE_PATH \\\u00a0 \u00a0 --private-key KEY_FILE_PATH \\\u00a0 \u00a0 --region COMPUTE_REGION\n```Replace the following:- ``: a name for your certificate that you choose.\n- ``: the path to your local certificate file to create a self-managed certificate. The certificate must be in PEM format.\n- ``: the path to a local private key file. The private key must be in PEM format and must use RSA or ECDSA encryption.\n- ``: a Compute Engine region for your certificate.\n- Reserve and apply a static IP address following [Static IP addressing](#static_ip_addressing) .\n- Save the following sample manifest as `ingress-pre-shared-cert.yaml` :```\napiVersion: networking.k8s.io/v1kind: Ingressmetadata:\u00a0 name: ilb-demo-ing\u00a0 namespace: default\u00a0 annotations:\u00a0 \u00a0 ingress.gcp.kubernetes.io/pre-shared-cert: \"CERT_NAME\"\u00a0 \u00a0 kubernetes.io/ingress.regional-static-ip-name: STATIC_IP_NAME\u00a0 \u00a0 kubernetes.io/ingress.class: \"gce-internal\"spec:\u00a0 rules:\u00a0 - host: DOMAIN\u00a0 \u00a0 http:\u00a0 \u00a0 \u00a0 paths:\u00a0 \u00a0 \u00a0 - pathType: ImplementationSpecific\u00a0 \u00a0 \u00a0 \u00a0 backend:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 service:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: SERVICE_NAME\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 port:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 number: 80\n```Replace the following:- ``: your domain.\n- ``: the name of the certificate you created in the previous section.\n- ``: the name of your Service.\n- Apply the manifest to the cluster:```\nkubectl apply -f ingress-pre-shared-cert.yaml\n```## HTTPS between load balancer and application\nIf your application runs in a GKE Pod and can receive HTTPS requests, you can configure the load balancer to use HTTPS when it forwards requests to your application. For more information, see [HTTPS (TLS) between load balancer and your application](/kubernetes-engine/docs/concepts/ingress-ilb#https_tls_between_load_balancer_and_your_application) .\n## Shared VPC\n### Manually add the NEG annotation\nIf the GKE in which you are deploying the Ingress resources is in a Shared VPC service project, the services are not automatically annotated with the annotation `cloud.google.com/neg: '{\"ingress\": true}'` because the `MutatingWebhookConfiguration` responsible for injecting the annotation to the services is not installed.\nYou must add the NEG annotation to the manifest of the Services that are exposed through Ingress for internal Application Load Balancers.\n### VPC firewall rules\nIf the GKE cluster in which you are deploying the Ingress resources is in a Shared VPC service project, and you want the GKE control plane to manage the firewall resources in your host project, then the service project's GKE service account must be granted the appropriate IAM permissions in the host project as per [Managing firewall resources for clusters with Shared VPC](/kubernetes-engine/docs/how-to/cluster-shared-vpc#managing_firewall_resources) . This lets the Ingress controller create firewall rules to allow ingress traffic for Google Cloud health checks.\nThe following is an example of an event that might be present in the Ingress resource logs. This error occurs when the Ingress controller is unable to create a firewall rule to allow ingress traffic for Google Cloud health checks if the permissions are not configured correctly.\n```\nFirewall change required by security admin: `gcloud compute firewall-rules update <RULE_NAME> --description \"GCE L7 firewall rule\" --allow tcp:<PORT> --source-ranges 130.211.0.0/22,35.191.0.0/16 --target-tags <TARGET_TAG> --project <HOST_PROJECT>\n```\nIf you prefer to [manually provision firewall rules from the host project](/kubernetes-engine/docs/concepts/ingress#manually_provision_firewall_rules_from_the_host_project) , then you can mute the `firewallXPNError` events by adding the `networking.gke.io/suppress-firewall-xpn-error: \"true\"` annotation to the Ingress resource.\n## Summary of internal Ingress annotations\nThe following tables show you the annotations that you can add when you are creating Ingress and Service resources for Ingress for internal Application Load Balancers.\n### Ingress annotations\n| Annotation         | Description                                                                                                 |\n|:----------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| kubernetes.io/ingress.class     | You can set as \"gce-internal\" for internal Ingress. If the class is not specified, an Ingress resource is interpreted by default as an external Ingress. For more information, see GKE Ingress controller behavior.                                              |\n| kubernetes.io/ingress.allow-http    | You can allow HTTP traffic between the client and the HTTP(S) load balancer. Possible values are true and false. The default value is true. For more information, see Disabling HTTP.                                                      |\n| ingress.gcp.kubernetes.io/pre-shared-cert  | You can upload certificates and keys to your Google Cloud project. Use this annotation to reference the certificates and keys. For more information, see Using multiple SSL certificates with external Application Load Balancers.                                          |\n| networking.gke.io/suppress-firewall-xpn-error | In GLBC 1.4 and later, you can mute the firewallXPNError event. For Ingress Load Balancers, if Kubernetes can't change the firewall rules due to insufficient permission, a firewallXPNError event is created every several minutes. Add networking.gke.io/suppress-firewall-xpn-error: \"true\" annotation to the ingress resource. The default value is false. You can remove this annotation to unmute. |\n| kubernetes.io/ingress.regional-static-ip-name | You can specify a static IP address to provision your internal Ingress resource. For more information, see Static IP addressing.                                                                   |\n### Service annotations related to Ingress\n| Annotation      | Description                                     |\n|:--------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| cloud.google.com/backend-config | Use this annotation to configure the backend service associated with a servicePort. For more information, see Ingress configuration.      |\n| cloud.google.com/neg   | Use this annotation to specify that the load balancer should use network endpoint groups. For more information, see Using Container-native Load Balancing. |\n## Troubleshooting\nUnderstanding and observing the state of Ingress typically involves inspecting the associated resources. The types of issues encountered often include load balancing resources not being created properly, traffic not reaching backends, or backends not appearing healthy.\nSome common troubleshooting steps include:\n- Verifying that client traffic is originating from within the same region and VPC as the load balancer.\n- Verifying that the Pods and backends are healthy.\n- Validating the traffic path to the VIP and for Compute Engine health checks to ensure it is not blocked by firewall rules.\n- Checking the Ingress resource events for errors.\n- Describing the Ingress resource to see the mapping to Compute Engine resources.\n- Validating that the Compute Engine load balancing resources exist, have the correct configurations, and do not have errors reported.\n### Filtering for Ingress events\nThe following query filters for errors across all Ingress events in your cluster:\n```\nkubectl get events --all-namespaces --field-selector involvedObject.kind=Ingress\n```\nYou can also filter by objects or object names:\n```\nkubectl get events --field-selector involvedObject.kind=Ingress,involvedObject.name=hostname-internal-ingress\n```\nIn the following error, you can see that the Service referenced by the Ingress does not exist:\n```\nLAST SEEN TYPE  REASON  OBJECT        MESSAGE\n0s   Warning Translate ingress/hostname-internal-ingress error while evaluating the ingress spec: could not find service \"default/hostname-invalid\"\n```\n### Inspecting Compute Engine load balancer resources\nThe following command displays the full output for the Ingress resource so that you can see the mappings to the Compute Engine resources that are created by the Ingress controller:\n```\nkubectl get ing INGRESS_FILENAME -o yaml\n```\nReplace `` with your Ingress resource's filename.\nThe output is similar to the following:\n```\napiVersion: v1\nitems:\n- apiVersion: networking.k8s.io/v1\n kind: Ingress\n metadata:\n annotations:\n  ingress.kubernetes.io/backends: '{\"k8s1-241a2b5c-default-hostname-80-29269aa5\":\"HEALTHY\"}'\n  ingress.kubernetes.io/forwarding-rule: k8s-fw-default-ilb-demo-ingress--241a2b5c94b353ec\n  ingress.kubernetes.io/target-proxy: k8s-tp-default-ilb-demo-ingress--241a2b5c94b353ec\n  ingress.kubernetes.io/url-map: k8s-um-default-ilb-demo-ingress--241a2b5c94b353ec\n  kubectl.kubernetes.io/last-applied-configuration: |\n  {\"apiVersion\":\"networking.k8s.io/v1\",\"kind\":\"Ingress\",\"metadata\":{\"annotations\":{\"kubernetes.io/ingress.class\":\"gce-internal\"},\"name\":\"ilb-demo-ingress\",\"namespace\":\"default\"},\"spec\":{\"defaultBackend\":{\"service\":{\"name\":\"hostname\"},\"port\":{\"number\":80}}}}\n  kubernetes.io/ingress.class: gce-internal\n creationTimestamp: \"2019-10-15T02:16:18Z\"\n finalizers:\n - networking.gke.io/ingress-finalizer\n generation: 1\n name: ilb-demo-ingress\n namespace: default\n resourceVersion: \"1538072\"\n selfLink: /apis/networking.k8s.io/v1/namespaces/default/ingresses/ilb-demo-ingress\n uid: 0ef024fe-6aea-4ee0-85f6-c2578f554975\n spec:\n defaultBackend:\n  service:\n  name: hostname\n  port:\n   number: 80\n status:\n loadBalancer:\n  ingress:\n  - ip: 10.128.0.127\nkind: List\nmetadata:\n resourceVersion: \"\"\n selfLink: \"\"\n```\nThe `ingress.kubernetes.io/backends` annotations list the backends and their status. Make sure that your backends are listed as `HEALTHY` .\nThe Compute Engine resources created by the Ingress can be queried directly to understand their status and configuration. Running these queries can also be helpful when troubleshooting.\nTo list all Compute Engine forwarding rules:\n```\ngcloud compute forwarding-rules list\n```\nThe output is similar to the following:\n```\nNAME              REGION  IP_ADDRESS  IP_PROTOCOL TARGET\nk8s-fw-default-hostname-internal-ingress--42084f6a534c335b REGION_NAME 10.128.15.225 TCP   REGION_NAME/targetHttpProxies/k8s-tp-default-hostname-internal-ingress--42084f6a534c335b\n```\nTo list the health of a backend service, first list the backend services, and make a copy of the name of the backend service you want to inspect:\n```\ngcloud compute backend-services list\n```\nThe output is similar to the following:\n```\nNAME           BACKENDS                  PROTOCOL\nk8s1-42084f6a-default-hostname-80-98cbc1c1 REGION_NAME/networkEndpointGroups/k8s1-42084f6a-default-hostname-80-98cbc1c1 HTTP\n```\nYou can now use the backend service name to query its health:\n```\ngcloud compute backend-services get-health k8s1-42084f6a-default-hostname-80-98cbc1c1 \\\u00a0 \u00a0 --region COMPUTE_REGION\n```\nReplace `` with the Compute Engine region of the backend service.\nThe output is similar to the following:\n```\nbackend: https://www.googleapis.com/compute/v1/projects/user1-243723/zones/ZONE_NAME/networkEndpointGroups/k8s1-42084f6a-default-hostname-80-98cbc1c1\nstatus:\n healthStatus:\n - healthState: HEALTHY\n```\n## What's next\n- Learn about [GKE Ingress for external Application Load Balancers](/kubernetes-engine/docs/concepts/ingress) .\n- Read a conceptual overview of [Services](/kubernetes-engine/docs/concepts/service) in GKE.\n- Learn how to create an [internal passthrough Network Load Balancer on GKE](/kubernetes-engine/docs/how-to/internal-load-balancing) .\n- Implement a [basic internal Ingress](https://github.com/GoogleCloudPlatform/gke-networking-recipes/tree/main/ingress/single-cluster/ingress-internal-basic) .", "guide": "Google Kubernetes Engine (GKE)"}