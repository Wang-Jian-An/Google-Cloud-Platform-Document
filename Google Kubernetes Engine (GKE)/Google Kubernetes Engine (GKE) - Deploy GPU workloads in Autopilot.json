{"title": "Google Kubernetes Engine (GKE) - Deploy GPU workloads in Autopilot", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/autopilot-gpus", "abstract": "# Google Kubernetes Engine (GKE) - Deploy GPU workloads in Autopilot\nThis page shows you how to request hardware accelerators (GPUs) in your Google Kubernetes Engine (GKE) Autopilot workloads.\nAutopilot provides the specialized `Accelerator` compute class to run GPU Pods. With this compute class, GKE places a single Pod on each GPU node, providing Pods with access to advanced capabilities on the virtual machine (VM). You can also optionally run GPU Pods without selecting the `Accelerator` compute class. To learn more about the benefits of the `Accelerator` compute class, see [When to use specific compute classes](/kubernetes-engine/docs/concepts/autopilot-compute-classes#when-to-use) .\n", "content": "## Pricing\nAutopilot bills you differently depending whether you requested the `Accelerator` compute class to run your GPU workloads.\n| Use the Accelerator compute class? | Pricing                                               | Compatibility with GKE capabilities                                     |\n|-------------------------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|         nan | You're billed for the Compute Engine hardware that runs your GPU workloads, plus an Autopilot premium for automatic node management and scalability. For details, see Autopilot mode pricing. | Compatible with the following: Spot Pods GKE Committed Use Discounts (CUDs) for the Autopilot premium Compute Engine CUDs for the node hardware Compute Engine capacity reservations |\n|         nan | You're billed based on the GPU Pod resource requests. For details, see the \"GPU Pods\" section in Kubernetes Engine pricing.                 | Compatible with the following: Spot Pods Committed Use Discounts (CUDs) for regular Pods                        |\n**Note:** All A100 (80GB) GPU nodes use local SSDs for node boot disks at fixed sizes based on the number of GPUs. You're billed separately for the attached Local SSDs. For details, see [Autopilot pricing](/kubernetes-engine/pricing#gpu-pods) . This doesn't apply to A100 (40GB) GPUs.\n## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- [Ensure that you have a GKE Autopilot cluster](/kubernetes-engine/docs/how-to/creating-an-autopilot-cluster) running one of the following versions:- Accelerator compute class: Any patch version of 1.28 starting with 1.28.6-gke.1095000- NVIDIA H100 (80GB) GPUs: 1.28.6-gke.1369000 or later, and 1.29.1-gke.1575000 or later\n- No compute class selection:- NVIDIA L4 GPUs: 1.28.3-gke.1203000 or later\n- NVIDIA A100 (80GB) GPUs: 1.27 or later\n- All other GPUs: 1.24.2-gke.1800 or later\n- [Ensure that you have enough GPU quotas](/docs/quota#viewing_quota) available in your project. You must have enough [Compute Engine GPU quota](/compute/quotas#gpu_quota) for the GPU models that you want to create in each region. If you require additional GPU quota, [request GPU quota](/docs/quotas/view-manage#requesting_higher_quota) .\n### Limitations\n- You can't use [time-sharing GPUs](/kubernetes-engine/docs/concepts/timesharing-gpus) and [multi-instance GPUs](/kubernetes-engine/docs/how-to/gpus-multi) with Autopilot.\n- GPU availability depends on the Google Cloud region of your Autopilot cluster, and your GPU quota. To find a GPU model by region or zone, see [GPU regions and zones availability](/compute/docs/gpus/gpu-regions-zones) .\n- If you explicitly request a specific existing GPU node for your Pod, the Pod must consume all the GPU resources on the node. For example, if the existing node has 8 GPUs and your Pod's containers request a total of 4 GPUs, Autopilot rejects the Pod.\n- For NVIDIA A100 (80GB) GPUs, you're charged a fixed price for the Local SSDs attached to the nodes, **regardless of whether your Pods use that capacity** .## Request GPUs in your containers\nTo request GPU resources for your containers, add the following fields to your Pod specification. Depending on your workload requirements, you can optionally omit the `cloud.google.com/compute-class: \"Accelerator\"` field.\n```\napiVersion: v1kind: Podmetadata:\u00a0 name: my-gpu-podspec:\u00a0 nodeSelector:\u00a0 \u00a0 cloud.google.com/compute-class: \"Accelerator\"\u00a0 \u00a0 cloud.google.com/gke-accelerator: GPU_TYPE\u00a0 containers:\u00a0 - name: my-gpu-container\u00a0 \u00a0 image: nvidia/cuda:11.0.3-runtime-ubuntu20.04\u00a0 \u00a0 command: [\"/bin/bash\", \"-c\", \"--\"]\u00a0 \u00a0 args: [\"while true; do sleep 600; done;\"]\u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: GPU_QUANTITY\n```\nReplace the following:\n- ``: the type of GPU hardware. Allowed values are the following:- `nvidia-h100-80gb`: NVIDIA H100 (80GB) (only available with Accelerator compute class)\n- `nvidia-a100-80gb`: NVIDIA A100 (80GB)\n- `nvidia-tesla-a100`: NVIDIA A100 (40GB)\n- `nvidia-l4`: NVIDIA L4\n- `nvidia-tesla-t4`: NVIDIA T4\n- ``: the number of GPUs to allocate to the container. Must be a [supported GPU quantity](#supported-quantities) for the GPU type you selected.\nYou must specify both the GPU type and the GPU quantity in your Pod specification. If you omit either of these values, Autopilot rejects your Pod.\n### CPU and memory requests for Autopilot GPU Pods\nWhen defining your GPU Pods, you should also request CPU and memory resources so that your containers perform as expected. Autopilot enforces specific CPU and memory minimums, maximums, and defaults based on the GPU type and quantity. For details, refer to [Resource requests in Autopilot](/kubernetes-engine/docs/concepts/autopilot-resource-requests) .\nYour Pod specification should look similar to the following example, which requests four T4 GPUs:\n```\napiVersion: v1kind: Podmetadata:\u00a0 name: t4-podspec:\u00a0 nodeSelector:\u00a0 \u00a0 cloud.google.com/compute-class: \"Accelerator\"\u00a0 \u00a0 cloud.google.com/gke-accelerator: \"nvidia-tesla-t4\"\u00a0 containers:\u00a0 - name: t4-container-1\u00a0 \u00a0 image: nvidia/cuda:11.0.3-runtime-ubuntu20.04\u00a0 \u00a0 command: [\"/bin/bash\", \"-c\", \"--\"]\u00a0 \u00a0 args: [\"while true; do sleep 600; done;\"]\u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 3\u00a0 \u00a0 \u00a0 \u00a0 cpu: \"54\"\u00a0 \u00a0 \u00a0 \u00a0 memory: \"54Gi\"\u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 cpu: \"54\"\u00a0 \u00a0 \u00a0 \u00a0 memory: \"54Gi\"\u00a0 - name: t4-container-2\u00a0 \u00a0 image: nvidia/cuda:11.0.3-runtime-ubuntu20.04\u00a0 \u00a0 command: [\"/bin/bash\", \"-c\", \"--\"]\u00a0 \u00a0 args: [\"while true; do sleep 600; done;\"]\u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 cpu: \"18\"\u00a0 \u00a0 \u00a0 \u00a0 memory: \"18Gi\"\u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 cpu: \"18\"\u00a0 \u00a0 \u00a0 \u00a0 memory: \"18Gi\"\n```\n### Ephemeral storage requests for Autopilot GPU Pods\nYou can also request ephemeral storage in Pods that need short-lived storage. The maximum available ephemeral storage and the type of storage hardware used depends on the type and quantity of GPUs the Pod requests. You can use Local SSD for ephemeral storage if using NVIDIA L4 GPUs, the `Accelerator` compute class, and running GKE patch version 1.28.6-gke.1369000 and later or 1.29.1-gke.1575000 and later.\nTo use Local SSD for ephemeral storage, add the `cloud.google.com/gke-ephemeral-storage-local-ssd: \"true\"` nodeSelector to your workload manifest. See the example manifest in [Use Local SSD-backed ephemeralstorage with Autopilotclusters](/kubernetes-engine/docs/how-to/persistent-volumes/local-ssd#local-ssd-autopilot) . The NVIDIA H100 (80GB) GPUs and NVIDIA A100 (80GB) GPUs always use Local SSDs for ephemeral storage, and you can't specify this node selector for those GPUs.\n## Verify GPU allocation\nTo check that a deployed GPU workload has the requested GPUs, run the following command:\n```\nkubectl describe node NODE_NAME\n```\nReplace `` with the name of the node on which the Pod was scheduled.\nThe output is similar to the following:\n```\napiVersion: v1\nkind: Node\nmetadata:\n...\n labels:\n ...\n cloud.google.com/gke-accelerator: nvidia-tesla-t4\n cloud.google.com/gke-accelerator-count: \"1\"\n cloud.google.com/machine-family: custom-48\n ...\n...\n```\n## How GPU allocation works in Autopilot\nAfter you request a GPU type and a quantity for the containers in a Pod and deploy the Pod, the following happens:\n- If no allocatable GPU node exists, Autopilot provisions a new GPU node to schedule the Pod. Autopilot automatically installs NVIDIA's drivers to facilitate the hardware.\n- Autopilot adds [node taints](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/) to the GPU node and adds the corresponding tolerations to the Pod. This prevents GKE from scheduling other Pods on the GPU node.\nAutopilot places exactly one GPU Pod on each GPU node, as well as any GKE-managed workloads that run on all nodes, and any DaemonSets that you configure to tolerate all node taints.\n### Run DaemonSets on every node\nYou might want to run DaemonSets on every node, even nodes with applied taints. For example, some logging and monitoring agents must run on every node in the cluster. You can configure those DaemonSets to ignore node taints so that GKE places those workloads on every node.\nTo run DaemonSets on every node in your cluster, including your GPU nodes, add the following toleration to your specification:\n```\napiVersion: apps/v1kind: DaemonSetmetadata:\u00a0 name: logging-agentspec:\u00a0 tolerations:\u00a0 - key: \"\"\u00a0 \u00a0 operator: \"Exists\"\u00a0 \u00a0 effect: \"\"\u00a0 containers:\u00a0 - name: logging-agent-v1\u00a0 \u00a0 image: IMAGE_PATH\n```\nTo run DaemonSets on specific GPU nodes in your cluster, add the following to your specification:\n```\napiVersion: apps/v1kind: DaemonSetmetadata:\u00a0 name: logging-agentspec:\u00a0 nodeSelector:\u00a0 \u00a0 cloud.google.com/gke-accelerator: \"GPU_TYPE\"\u00a0 tolerations:\u00a0 - key: \"\"\u00a0 \u00a0 operator: \"Exists\"\u00a0 \u00a0 effect: \"\"\u00a0 containers:\u00a0 - name: logging-agent-v1\u00a0 \u00a0 image: IMAGE_PATH\n```\nReplace `` with the type of GPU in your target nodes. Can be one of the following:\n- `nvidia-h100-80gb`: NVIDIA H100 (80GB) (only available with Accelerator compute class)\n- `nvidia-a100-80gb`: NVIDIA A100 (80GB)\n- `nvidia-tesla-a100`: NVIDIA A100 (40GB)\n- `nvidia-l4`: NVIDIA L4\n- `nvidia-tesla-t4`: NVIDIA T4\n### GPU use cases in Autopilot\nYou can allocate GPUs to containers in Autopilot Pods to facilitate workloads such as the following:\n- Machine learning (ML) inference\n- ML training\n- Rendering\n### Supported GPU quantities\nWhen you request GPUs in your Pod specification, you must use the following quantities based on the GPU type:\n| GPU quantities      | GPU quantities.1 |\n|:-------------------------------------|:-------------------|\n| NVIDIA L4 nvidia-l4     | 1, 2, 4, 8   |\n| NVIDIA T4 nvidia-tesla-t4   | 1, 2, 4   |\n| NVIDIA A100 (40GB) nvidia-tesla-a100 | 1, 2, 4, 8, 16  |\n| NVIDIA A100 (80GB) nvidia-a100-80gb | 1, 2, 4, 8   |\n| NVIDIA H100 (80GB) nvidia-h100-80gb | 8     |\nIf you request a GPU quantity that isn't supported for that type, Autopilot rejects your Pod.\n## Monitor GPU nodes\nIf your GKE cluster has [system metrics](/stackdriver/docs/solutions/gke/managing-metrics#system-metrics) enabled, then the following metrics are available in [Cloud Monitoring](/monitoring/docs) to monitor your GPU workload performance:\n- **Duty Cycle (container/accelerator/duty_cycle): ** Percentage of time over the past sample period (10 seconds) during which the accelerator was actively processing. Between 1 and 100.\n- **Memory Usage (container/accelerator/memory_used): ** Amount of accelerator memory allocated in bytes.\n- **Memory Capacity (container/accelerator/memory_total): ** Total accelerator memory in bytes.\nYou can use predefined dashboards to monitor your clusters with GPU nodes. For more information, see [ View observability metrics](/kubernetes-engine/docs/how-to/view-observability-metrics) . For general information about monitoring your clusters and their resources, refer to [Observability for GKE](/kubernetes-engine/docs/concepts/observability) .\n### View usage metrics for workloads\nYou view your workload GPU usage metrics from the **Workloads** dashboard in the Google Cloud console.\nTo view your workload GPU usage, perform the following steps:\n- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workload/overview) \n- Select a workload.\nThe Workloads dashboard displays charts for GPU memory usage and capacity, and GPU duty cycle.\n## What's next\n- [Learn more about GPU support in GKE](/kubernetes-engine/docs/concepts/gpus) .\n- [Read about how Autopilot compute classes are optimized for specialized use cases](/kubernetes-engine/docs/concepts/autopilot-compute-classes) .", "guide": "Google Kubernetes Engine (GKE)"}