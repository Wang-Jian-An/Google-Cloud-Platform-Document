{"title": "Google Kubernetes Engine (GKE) - Serve Gemma open models using GPUs on GKE with Hugging Face TGI", "url": "https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-tgi", "abstract": "# Google Kubernetes Engine (GKE) - Serve Gemma open models using GPUs on GKE with Hugging Face TGI\nThis tutorial shows you how to serve a [Gemma](https://ai.google.dev/gemma/docs/) large language model (LLM) using graphical processing units (GPUs) on Google Kubernetes Engine (GKE) with the [Text Generation Interface](https://github.com/huggingface/text-generation-inference) (TGI) serving framework from [Hugging Face](https://huggingface.co/) . In this tutorial, you download the 2B and 7B parameter pre-trained and instruction tuned Gemma models from Hugging Face and deploy them on a GKE [Autopilot](/kubernetes-engine/docs/concepts/choose-cluster-mode#why-autopilot) or [Standard](/kubernetes-engine/docs/concepts/choose-cluster-mode#why-standard) cluster using a container that runs TGI.\nThis guide is a good starting point if you need the granular control, scalability, resilience, portability, and cost-effectiveness of managed Kubernetes when deploying and serving your AI/ML workloads. If you need a unified managed AI platform to rapidly build and serve ML models cost effectively, we recommend that you try our [Vertex AI](/vertex-ai) deployment solution.", "content": "## BackgroundBy serving Gemma using GPUs on GKE with TGI, you can implement a robust, production-ready inference serving solution with all the benefits of managed [Kubernetes](https://kubernetes.io/) , including efficient scalability and higher availability. This section describes the key technologies used in this guide.\n### Gemma [Gemma](https://ai.google.dev/gemma/docs/) is a set of openly available, lightweight generative artificial intelligence (AI) models released under an open license. These AI models are available to run in your applications, hardware, mobile devices, or hosted services. You can use the Gemma models for text generation, however you can also tune these models for specialized tasks.\nTo learn more, see the [Gemma documentation](https://ai.google.dev/gemma/docs) .\n### GPUsGPUs let you accelerate specific workloads running on your nodes such as machine learning and data processing. GKE provides a range of machine type options for node configuration, including machine types with NVIDIA H100, L4, and A100 GPUs.\nBefore you use GPUs in GKE, we recommend that you complete the following learning path:- Learn about [current GPU version availability](/compute/docs/gpus) \n- Learn about [GPUs in GKE](/kubernetes-engine/docs/concepts/gpus) \n### Text Generation Interface (TGI)TGI is Hugging Face's toolkit for deploying and serving LLMs. TGI enables high-performance text generation for popular open source LLMs, including Gemma. TGI includes features such as:- Optimized transformer implementation with [Flash Attention](https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention) and [PagedAttention](https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention) \n- Continuous batching to improve the overall serving throughput\n- [Tensor parallelism](https://huggingface.co/docs/text-generation-inference/en/conceptual/tensor_parallelism) for faster inference on multiple GPUs\nTo learn more, refer to the [TGI documentation](https://github.com/huggingface/text-generation-inference/blob/main/README.md) .## ObjectivesThis guide is intended for Generative AI customers who use [PyTorch](https://pytorch.org/) , new or existing users of GKE, ML Engineers, MLOps (DevOps) engineers, or platform administrators who are interested in using Kubernetes container orchestration capabilities for serving LLMs on H100, A100, and L4 GPU hardware.\nBy the end of this guide, you should be able to perform the following steps:- Prepare your environment with a GKE cluster in Autopilot mode.\n- Deploy TGI to your cluster.\n- Use TGI to serve the Gemma 2B or 7B model through curl and a web chat interface.\n## Before you begin- Make sure that you have the following role or roles on the project:      roles/container.admin, roles/iam.serviceAccountAdmin\n- Create a [Hugging Face](https://huggingface.co/) account, if you don't already have one.\n- [Ensure your project has sufficientquota](/compute/resource-usage#gpu_quota) for GPUs in GKE.\n## Get access to the modelTo get access to the Gemma models for deployment to GKE, you must first sign the license consent agreement then generate a Hugging Face access token.\n### Sign the license consent agreementYou must sign the consent agreement to use Gemma. Follow these instructions:- Access the [model consent page](https://www.kaggle.com/models/google/gemma) on Kaggle.com.\n- Verify consent using your Hugging Face account.\n- Accept the model terms.\n### Generate an access tokenTo access the model through Hugging Face, you'll need a [Hugging Facetoken](https://huggingface.co/docs/hub/security-tokens) .\nFollow these steps to generate a new token if you don't have one already:- Click **Your Profile > Settings > Access Tokens** .\n- Select **New Token** .\n- Specify a Name of your choice and a Role of at least`Read`.\n- Select **Generate a token** .\n- Copy the generated token to your clipboard.\n## Prepare your environmentIn this tutorial, you use [Cloud Shell](/shell) to manage resources hosted on Google Cloud. Cloud Shell comes preinstalled with the software you'll need for this tutorial, including [kubectl](https://kubernetes.io/docs/reference/kubectl/) and [gcloud CLI](/sdk/gcloud) .\nTo set up your environment with Cloud Shell, follow these steps:- In the Google Cloud console, launch a Cloud Shell session by clicking **Activate Cloud Shell** in the [Google Cloud console](http://console.cloud.google.com) . This launches a session in the bottom pane of Google Cloud console.\n- Set the default environment variables:```\ngcloud config set project PROJECT_IDexport PROJECT_ID=$(gcloud config get project)export REGION=REGIONexport CLUSTER_NAME=tgiexport HF_TOKEN=HF_TOKEN\n```Replace the following values:- : Your Google Cloud [project ID](/resource-manager/docs/creating-managing-projects#identifying_projects) .\n- : A region that supports the accelerator type you want to use, for example,`us-central1`for L4 GPU.\n- : The Hugging Face token you generated earlier.## Create and configure Google Cloud resourcesFollow these instructions to create the required resources.\n **Note:** You may need to create a capacity reservation for usage of some accelerators. To learn how to reserve and consume reserved resources, see [Consuming reserved zonal resources](/kubernetes-engine/docs/how-to/consuming-reservations) .\n### Create a GKE cluster and node poolYou can serve Gemma on GPUs in a GKE Autopilot or Standard cluster. We recommend that you use a Autopilot cluster for a fully managed Kubernetes experience. To choose the GKE mode of operation that's the best fit for your workloads, see [Choose a GKE mode of operation](/kubernetes-engine/docs/concepts/choose-cluster-mode) .\nIn Cloud Shell, run the following command:\n```\ngcloud container clusters create-auto ${CLUSTER_NAME} \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --region=${REGION} \\\u00a0 --release-channel=rapid \\\u00a0 --cluster-version=1.28\n```\nGKE creates an Autopilot cluster with CPU and GPU nodes as requested by the deployed workloads.- In Cloud Shell, run the following command to create a Standard cluster:```\ngcloud container clusters create ${CLUSTER_NAME} \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --region=${REGION} \\\u00a0 --workload-pool=${PROJECT_ID}.svc.id.goog \\\u00a0 --release-channel=rapid \\\u00a0 --num-nodes=1\n```The cluster creation might take several minutes.\n- Run the following command to create a [node pool](/kubernetes-engine/docs/concepts/node-pools) for your cluster:```\ngcloud container node-pools create gpupool \\\u00a0 --accelerator type=nvidia-l4,count=2,gpu-driver-version=latest \\\u00a0 --project=${PROJECT_ID} \\\u00a0 --location=${REGION} \\\u00a0 --node-locations=${REGION}-a \\\u00a0 --cluster=${CLUSTER_NAME} \\\u00a0 --machine-type=g2-standard-24 \\\u00a0 --num-nodes=1\n```GKE creates a single node pool containing two L4 GPUs for each node.### Create a Kubernetes secret for Hugging Face credentialsIn Cloud Shell, do the following:- Configure `kubectl` to communicate with your cluster:```\ngcloud container clusters get-credentials ${CLUSTER_NAME} --location=${REGION}\n```\n- Create a Kubernetes Secret that contains the Hugging Face token:```\nkubectl create secret generic hf-secret \\--from-literal=hf_api_token=${HF_TOKEN} \\--dry-run=client -o yaml | kubectl apply -f ```\n## Deploy TGIIn this section, you deploy the TGI container to serve the Gemma model you want to use. To learn about the instruction tuned and pre-trained models and which one to select for your use case, see [Tuned models](https://ai.google.dev/gemma/docs#tuned-models) .\nFollow these instructions to deploy the Gemma 2B instruction tuned model.- Create the following `tgi-2b-it.yaml` manifest: [  ai-ml/llm-serving-gemma/tgi/tgi-2b-it.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/tgi/tgi-2b-it.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/tgi/tgi-2b-it.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: tgi-gemma-deploymentspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma-2b-it\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: text-generation-inference\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-hf-tgi-serve:20240220_0936_RC01\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"7Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"20Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"7Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"20Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - --model-id=$(MODEL_ID)\u00a0 \u00a0 \u00a0 \u00a0 - --num-shard=1\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: google/gemma-2b-it\u00a0 \u00a0 \u00a0 \u00a0 - name: PORT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"8000\"\u00a0 \u00a0 \u00a0 \u00a0 - name: HUGGING_FACE_HUB_TOKEN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: hf-secret\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: hf_api_token\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4---apiVersion: v1kind: Servicemetadata:\u00a0 name: llm-servicespec:\u00a0 selector:\u00a0 \u00a0 app: gemma-server\u00a0 type: ClusterIP\u00a0 ports:\u00a0 - protocol: TCP\u00a0 \u00a0 port: 8000\u00a0 \u00a0 targetPort: 8000\n```\n- Apply the manifest:```\nkubectl apply -f tgi-2b-it.yaml\n```\nFollow these instructions to deploy the Gemma 7B instruction tuned model.- Create the following `tgi-7b-it.yaml` manifest: [  ai-ml/llm-serving-gemma/tgi/tgi-7b-it.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/tgi/tgi-7b-it.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/tgi/tgi-7b-it.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: tgi-gemma-deploymentspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma-7b-it\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: text-generation-inference\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-hf-tgi-serve:20240220_0936_RC01\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"40Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"10\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"40Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 2\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - --model-id=$(MODEL_ID)\u00a0 \u00a0 \u00a0 \u00a0 - --num-shard=2\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: google/gemma-7b-it\u00a0 \u00a0 \u00a0 \u00a0 - name: PORT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"8000\"\u00a0 \u00a0 \u00a0 \u00a0 - name: HUGGING_FACE_HUB_TOKEN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: hf-secret\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: hf_api_token\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4---apiVersion: v1kind: Servicemetadata:\u00a0 name: llm-servicespec:\u00a0 selector:\u00a0 \u00a0 app: gemma-server\u00a0 type: ClusterIP\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 8000\u00a0 \u00a0 \u00a0 targetPort: 8000\n```\n- Apply the manifest:```\nkubectl apply -f tgi-7b-it.yaml\n```\nFollow these instructions to deploy the Gemma 2B pre-trained model.- Create the following `tgi-2b.yaml` manifest: [  ai-ml/llm-serving-gemma/tgi/tgi-2b.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/tgi/tgi-2b.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/tgi/tgi-2b.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: tgi-gemma-deploymentspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma-2b\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: text-generation-inference\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-hf-tgi-serve:20240220_0936_RC01\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"7Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"20Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"7Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"20Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 1\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - --model-id=$(MODEL_ID)\u00a0 \u00a0 \u00a0 \u00a0 - --num-shard=1\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: google/gemma-2b\u00a0 \u00a0 \u00a0 \u00a0 - name: PORT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"8000\"\u00a0 \u00a0 \u00a0 \u00a0 - name: HUGGING_FACE_HUB_TOKEN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: hf-secret\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: hf_api_token\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4---apiVersion: v1kind: Servicemetadata:\u00a0 name: llm-servicespec:\u00a0 selector:\u00a0 \u00a0 app: gemma-server\u00a0 type: ClusterIP\u00a0 ports:\u00a0 - protocol: TCP\u00a0 \u00a0 port: 8000\u00a0 \u00a0 targetPort: 8000\n```\n- Apply the manifest:```\nkubectl apply -f tgi-2b.yaml\n```\nFollow these instructions to deploy the Gemma 2B pre-trained model.- Create the following `tgi-7b.yaml` manifest: [  ai-ml/llm-serving-gemma/tgi/tgi-7b.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/tgi/tgi-7b.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/tgi/tgi-7b.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: tgi-gemma-deploymentspec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gemma-server\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/model: gemma-7b\u00a0 \u00a0 \u00a0 \u00a0 ai.gke.io/inference-server: text-generation-inference\u00a0 \u00a0 \u00a0 \u00a0 examples.ai.gke.io/source: user-guide\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: inference-server\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-hf-tgi-serve:20240220_0936_RC01\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"2\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"40Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 2\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"10\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"25Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ephemeral-storage: \"40Gi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nvidia.com/gpu: 2\u00a0 \u00a0 \u00a0 \u00a0 args:\u00a0 \u00a0 \u00a0 \u00a0 - --model-id=$(MODEL_ID)\u00a0 \u00a0 \u00a0 \u00a0 - --num-shard=2\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: google/gemma-7b\u00a0 \u00a0 \u00a0 \u00a0 - name: PORT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"8000\"\u00a0 \u00a0 \u00a0 \u00a0 - name: HUGGING_FACE_HUB_TOKEN\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 valueFrom:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 secretKeyRef:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: hf-secret\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 key: hf_api_token\u00a0 \u00a0 \u00a0 \u00a0 volumeMounts:\u00a0 \u00a0 \u00a0 \u00a0 - mountPath: /dev/shm\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 name: dshm\u00a0 \u00a0 \u00a0 volumes:\u00a0 \u00a0 \u00a0 - name: dshm\u00a0 \u00a0 \u00a0 \u00a0 emptyDir:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 medium: Memory\u00a0 \u00a0 \u00a0 nodeSelector:\u00a0 \u00a0 \u00a0 \u00a0 cloud.google.com/gke-accelerator: nvidia-l4---apiVersion: v1kind: Servicemetadata:\u00a0 name: llm-servicespec:\u00a0 selector:\u00a0 \u00a0 app: gemma-server\u00a0 type: ClusterIP\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 8000\u00a0 \u00a0 \u00a0 targetPort: 8000\n```\n- Apply the manifest:```\nkubectl apply -f tgi-7b.yaml\n```\nA Pod in the cluster downloads the model weights from Hugging Face and starts the serving engine.\nWait for the Deployment to be available:\n```\nkubectl wait --for=condition=Available --timeout=700s deployment/tgi-gemma-deployment\n```\nView the logs from the running Deployment:\n```\nkubectl logs -f -l app=gemma-server\n```\nThe Deployment resource downloads the model data. This process can take a few minutes. The output is similar to the following:\n```\nINFO text_generation_router: router/src/main.rs:237: Using the Hugging Face API to retrieve tokenizer config\nINFO text_generation_router: router/src/main.rs:280: Warming up model\nINFO text_generation_router: router/src/main.rs:316: Setting max batch total tokens to 666672\nINFO text_generation_router: router/src/main.rs:317: Connected\nWARN text_generation_router: router/src/main.rs:331: Invalid hostname, defaulting to 0.0.0.0\nINFO text_generation_router::server: router/src/server.rs:1035: Built with `google` feature\nINFO text_generation_router::server: router/src/server.rs:1036: Environment variables `AIP_PREDICT_ROUTE` and `AIP_HEALTH_ROUTE` will be respected.\n```\nMake sure the model is fully downloaded before proceeding to the next section.## Serve the modelIn this section, you interact with the model.\n### Set up port forwardingRun the following command to set up port forwarding to the model:\n```\nkubectl port-forward service/llm-service 8000:8000\n```\nThe output is similar to the following:\n```\nForwarding from 127.0.0.1:8000 -> 8000\n```\n### Interact with the model using curlThis section shows how you can perform a basic smoke test to verify your deployed pre-trained or instruction tuned models. For simplicity, this section describes the testing approach only using the 2B pre-trained and instruction tuned models.\nIn a new terminal session, use `curl` to chat with your model:\n```\nUSER_PROMPT=\"Java is a\"curl -X POST http://localhost:8000/generate \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 -d @- <<EOF{\u00a0 \u00a0 \"inputs\": \"${USER_PROMPT}\",\u00a0 \u00a0 \"parameters\": {\u00a0 \u00a0 \u00a0 \u00a0 \"temperature\": 0.90,\u00a0 \u00a0 \u00a0 \u00a0 \"top_p\": 0.95,\u00a0 \u00a0 \u00a0 \u00a0 \"max_new_tokens\": 128\u00a0 \u00a0 }}EOF\n```\nThe following output shows an example of the model response:\n```\n{\"generated_text\":\" general-purpose, high-level, class-based, object-oriented programming language. <strong>Is Java a statically typed language?</strong> Yes, Java is a statically typed language. Java also supports dynamic typing. Static typing means that the type of every variable is explicitly specified at the time of declaration. The type can be either implicit or explicit. Static typing means that if no types are assigned then it will be assumed as a primitive type.\\n\\n<h3>What is Java?</h3>\\n\\nJava is a general-purpose, class-based, object-oriented programming language. Java is one of the oldest programming languages that has gained a\"}\n```\nIn a new terminal session, use `curl` to chat with your model:\n```\nUSER_PROMPT=\"I'm new to coding. If you could only recommend one programming language to start with, what would it be and why?\"curl -X POST http://localhost:8000/generate \\\u00a0 -H \"Content-Type: application/json\" \\\u00a0 -d @- <<EOF{\u00a0 \u00a0 \"inputs\": \"<start_of_turn>user\\n${USER_PROMPT}<end_of_turn>\\n\",\u00a0 \u00a0 \"parameters\": {\u00a0 \u00a0 \u00a0 \u00a0 \"temperature\": 0.90,\u00a0 \u00a0 \u00a0 \u00a0 \"top_p\": 0.95,\u00a0 \u00a0 \u00a0 \u00a0 \"max_new_tokens\": 128\u00a0 \u00a0 }}EOF\n```\nThe following output shows an example of the model response:\n```\n{\"generated_text\":\"**Python**\\n\\n**Reasons why Python is a great choice for beginners:**\\n\\n* **Simple syntax:** Python uses clear and concise syntax, making it easy for beginners to pick up.\\n* **Easy to learn:** Python's syntax is based on English, making it easier to learn than other languages.\\n* **Large and supportive community:** Python has a massive and active community of developers who are constantly willing to help.\\n* **Numerous libraries and tools:** Python comes with a vast collection of libraries and tools that make it easy to perform various tasks, such as data manipulation, web development, and machine learning.\\n* **\"}\n```\n **Success:** You've successfully served Gemma using GPUs on GKE with TGI. You can now interact with the model.\n### (Optional) Interact with the model through a Gradio chat interfaceIn this section, you build a web chat application that lets you interact with your instruction tuned model. For simplicity, this section describes only the testing approach using the 2B-it model.\n [Gradio](https://github.com/gradio-app/gradio) is a Python library that has a `ChatInterface` wrapper that creates user interfaces for chatbots.\n- In Cloud Shell, save the following manifest as `gradio.yaml` : [  ai-ml/llm-serving-gemma/tgi/gradio.yaml ](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/tgi/gradio.yaml) [View on GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/blob/HEAD/ai-ml/llm-serving-gemma/tgi/gradio.yaml) ```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: gradio\u00a0 labels:\u00a0 \u00a0 app: gradiospec:\u00a0 replicas: 1\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: gradio\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: gradio\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: gradio\u00a0 \u00a0 \u00a0 \u00a0 image: us-docker.pkg.dev/google-samples/containers/gke/gradio-app:v1.0.0\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"512m\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"512Mi\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 limits:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"1\"\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 memory: \"512Mi\"\u00a0 \u00a0 \u00a0 \u00a0 env:\u00a0 \u00a0 \u00a0 \u00a0 - name: CONTEXT_PATH\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"/generate\"\u00a0 \u00a0 \u00a0 \u00a0 - name: HOST\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"http://llm-service:8000\"\u00a0 \u00a0 \u00a0 \u00a0 - name: LLM_ENGINE\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"tgi\"\u00a0 \u00a0 \u00a0 \u00a0 - name: MODEL_ID\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"gemma\"\u00a0 \u00a0 \u00a0 \u00a0 - name: USER_PROMPT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"<start_of_turn>user\\nprompt<end_of_turn>\\n\"\u00a0 \u00a0 \u00a0 \u00a0 - name: SYSTEM_PROMPT\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 value: \"<start_of_turn>model\\nprompt<end_of_turn>\\n\"\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 7860---apiVersion: v1kind: Servicemetadata:\u00a0 name: gradiospec:\u00a0 selector:\u00a0 \u00a0 app: gradio\u00a0 ports:\u00a0 \u00a0 - protocol: TCP\u00a0 \u00a0 \u00a0 port: 8080\u00a0 \u00a0 \u00a0 targetPort: 7860\u00a0 type: ClusterIP\n```\n- Apply the manifest:```\nkubectl apply -f gradio.yaml\n```\n- Wait for the deployment to be available:```\nkubectl wait --for=condition=Available --timeout=300s deployment/gradio\n```\n- In Cloud Shell, run the following command:```\nkubectl port-forward service/gradio 8080:8080\n```This creates a port forward from Cloud Shell to the Gradio service.\n- Click the **Web Preview** button which can be found on the top right of the Cloud Shell taskbar. Click **Preview on Port 8080** . A new tab opens in your browser.\n- Interact with Gemma using the Gradio chat interface. Add a prompt and click **Submit** .\n## Troubleshoot issues\n- If you get the message`Empty reply from server`, it's possible the container has not finished downloading the model data. [Check the Pod's logs](#deploy-tgi) again for the`Connected`message which indicates that the model is ready to serve.\n- If you see`Connection refused`, verify that your [port forwarding is active](#setup-port-forwarding) .\n## Clean upTo avoid incurring charges to your Google Cloud account for the resources used in this   tutorial, either delete the project that contains the resources, or keep the project and   delete the individual resources.\n### Delete the deployed resourcesTo avoid incurring charges to your Google Cloud account for the resources that you created in this guide, run the following command:\n```\ngcloud container clusters delete ${CLUSTER_NAME} \\\u00a0 --region=${REGION}\n```## What's next\n- Learn more about [GPUs inGKE](/kubernetes-engine/docs/concepts/gpus) .\n- Learn how to use Gemma with TGI on other accelerators, including A100 and H100 GPUs, by [viewing the sample code in GitHub](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/tree/main/ai-ml/llm-serving-gemma/tgi) .\n- Learn how to [deploy GPU workloads in Autopilot](/kubernetes-engine/docs/how-to/autopilot-gpus) .\n- Learn how to [deploy GPU workloads in Standard](/kubernetes-engine/docs/how-to/gpus) .\n- Explore the TGI [documentation](https://huggingface.co/docs/text-generation-inference/en/index) .\n- Explore the [Vertex AI Model Garden](https://console.cloud.google.com/vertex-ai/model-garden) .\n- Discover how to run optimized AI/ML workloads with [GKEplatform orchestrationcapabilities](/kubernetes-engine/docs/integrations/ai-infra) .", "guide": "Google Kubernetes Engine (GKE)"}