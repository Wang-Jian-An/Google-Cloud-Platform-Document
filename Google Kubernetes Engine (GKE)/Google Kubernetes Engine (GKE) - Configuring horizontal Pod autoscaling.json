{"title": "Google Kubernetes Engine (GKE) - Configuring horizontal Pod autoscaling", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/horizontal-pod-autoscaling", "abstract": "# Google Kubernetes Engine (GKE) - Configuring horizontal Pod autoscaling\nThis page explains how to use [horizontal Pod autoscaling](/kubernetes-engine/docs/concepts/horizontalpodautoscaler) to autoscale a Deployment using different types of metrics. You can use the same guidelines to configure a `HorizontalPodAutoscaler` for any scalable Deployment object.\n", "content": "## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n### API versions for HorizontalPodAutoscaler objects\nWhen you use the Google Cloud console, `HorizontalPodAutoscaler` objects are created using the `autoscaling/v2` API.\nWhen you use `kubectl` to create or view information about a Horizontal Pod Autoscaler, you can specify either the `autoscaling/v1` API or the `autoscaling/v2` API.\n- `apiVersion: autoscaling/v1` is the default, and allows you to autoscale based only on CPU utilization. To autoscale based on other metrics, using `apiVersion: autoscaling/v2` is recommended. The example in [Create the example Deployment](#create_the_example_deployment) uses `apiVersion: autoscaling/v1` .\n- `apiVersion: autoscaling/v2` is recommended for creating new `HorizontalPodAutoscaler` objects. It allows you to autoscale based on multiple metrics, including custom or external metrics. All other examples in this topic use `apiVersion: autoscaling/v2` .\nTo check which API versions are supported, use the `kubectl api-versions` command.\nYou can specify which API to use when [viewing details about a Horizontal Pod Autoscaler that uses apiVersion: autoscaling/v2](#viewing) .\n### Create the example Deployment\nBefore you can create a Horizontal Pod Autoscaler, you must create the workload it monitors. The examples in this topic apply different Horizontal Pod Autoscaler configurations to the following `nginx` Deployment. Separate examples show a Horizontal Pod Autoscaler based on [resource utilization](#resource-utilization) , based on a [custom or external metric](/kubernetes-engine/docs/concepts/custom-and-external-metrics) , and based on [multiple metrics](#multiple-metrics) .\nSave the following to a file named `nginx.yaml` :\n```\napiVersion: apps/v1kind: Deploymentmetadata:\u00a0 name: nginx\u00a0 namespace: defaultspec:\u00a0 replicas: 3\u00a0 selector:\u00a0 \u00a0 matchLabels:\u00a0 \u00a0 \u00a0 app: nginx\u00a0 template:\u00a0 \u00a0 metadata:\u00a0 \u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 \u00a0 app: nginx\u00a0 \u00a0 spec:\u00a0 \u00a0 \u00a0 containers:\u00a0 \u00a0 \u00a0 - name: nginx\u00a0 \u00a0 \u00a0 \u00a0 image: nginx:1.7.9\u00a0 \u00a0 \u00a0 \u00a0 ports:\u00a0 \u00a0 \u00a0 \u00a0 - containerPort: 80\u00a0 \u00a0 \u00a0 \u00a0 resources:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # You must specify requests for CPU to autoscale\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 # based on CPU utilization\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 requests:\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 cpu: \"250m\"\n```\nThis manifest specifies a value for CPU requests. If you want to autoscale based on a resource's utilization as a percentage, you must specify requests for that resource. If you do not specify requests, you can autoscale based only on the absolute value of the resource's utilization, such as milliCPUs for [CPU utilization](#resource-utilization) .\nTo create the Deployment, apply the `nginx.yaml` manifest:\n```\nkubectl apply -f nginx.yaml\n```\nThe Deployment has `spec.replicas` set to 3, so three Pods are deployed. You can verify this using the `kubectl get deployment nginx` command.\nEach of the examples in this topic applies a different Horizontal Pod Autoscaler to an example nginx Deployment.\n## Autoscaling based on resources utilization\nThis example creates `HorizontalPodAutoscaler` object to autoscale the [nginx Deployment](#create-the-example-deployment) when CPU utilization surpasses 50%, and ensures that there is always a minimum of 1 replica and a maximum of 10 replicas.\nYou can create a Horizontal Pod Autoscaler that targets CPU using the Google Cloud console, the `kubectl apply` command, or for average CPU only, the `kubectl autoscale` command.\n**Note:** This example uses `apiVersion: autoscaling/v1` . For more information about the available APIs, see [API versions for HorizontalPodAutoscaler objects](#api-versions) .\n- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workloads) \n- Click the name of the `nginx` Deployment.\n- Click **Actions > Autoscale** .\n- Specify the following values:- **Minimum number of replicas:** 1\n- **Maximum number of replicas:** 10\n- **Autoscaling metric:** CPU\n- **Target:** 50\n- **Unit:** %\n- Click **Done** .\n- Click **Autoscale** .\nSave the following YAML manifest as a file named `nginx-hpa.yaml` :\n```\napiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata:\u00a0 name: nginxspec:\u00a0 scaleTargetRef:\u00a0 \u00a0 apiVersion: apps/v1\u00a0 \u00a0 kind: Deployment\u00a0 \u00a0 name: nginx\u00a0 minReplicas: 1\u00a0 maxReplicas: 10\u00a0 targetCPUUtilizationPercentage: 50\n```\nTo create the HPA, apply the manifest using the following command:\n```\nkubectl apply -f nginx-hpa.yaml\n```\nTo create a `HorizontalPodAutoscaler` object that only targets average CPU utilization, you can use the [kubectl autoscale](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#autoscale) command:\n```\nkubectl autoscale deployment nginx --cpu-percent=50 --min=1 --max=10\n```\n **Note:** You can combine the `--dry-run` and `-o yaml` flags to print a YAML manifest for a Horizontal Pod Autoscaler without actually creating it.\nTo get a list of Horizontal Pod Autoscalers in the cluster, use the following command:\n```\nkubectl get hpa\n```\nThe output is similar to the following:\n```\nNAME REFERENCE   TARGETS MINPODS MAXPODS REPLICAS AGE\nnginx Deployment/nginx 0%/50% 1   10  3   61s\n```\nTo get details about the Horizontal Pod Autoscaler, you can use the Google Cloud console or the `kubectl` command.\n- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workloads) \n- Click the name of the `nginx` Deployment.\n- View the Horizontal Pod Autoscaler configuration in the **Autoscaler** section.\n- View more details about autoscaling events in the **Events** tab.\nTo get details about the Horizontal Pod Autoscaler, you can use `kubectl get hpa` with the `-o yaml` flag. The `status` field contains information about the current number of replicas and any recent autoscaling events.\n```\nkubectl get hpa nginx -o yaml\n```\nThe output is similar to the following:\n```\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n annotations:\n autoscaling.alpha.kubernetes.io/conditions: '[{\"type\":\"AbleToScale\",\"status\":\"True\",\"lastTransitionTime\":\"2019-10-30T19:42:59Z\",\"reason\":\"ScaleDownStabilized\",\"message\":\"recent\n  recommendations were higher than current one, applying the highest recent recommendation\"},{\"type\":\"ScalingActive\",\"status\":\"True\",\"lastTransitionTime\":\"2019-10-30T19:42:59Z\",\"reason\":\"ValidMetricFound\",\"message\":\"the\n  HPA was able to successfully calculate a replica count from cpu resource utilization\n  (percentage of request)\"},{\"type\":\"ScalingLimited\",\"status\":\"False\",\"lastTransitionTime\":\"2019-10-30T19:42:59Z\",\"reason\":\"DesiredWithinRange\",\"message\":\"the\n  desired count is within the acceptable range\"}]'\n autoscaling.alpha.kubernetes.io/current-metrics: '[{\"type\":\"Resource\",\"resource\":{\"name\":\"cpu\",\"currentAverageUtilization\":0,\"currentAverageValue\":\"0\"}}]'\n kubectl.kubernetes.io/last-applied-configuration: |\n  {\"apiVersion\":\"autoscaling/v1\",\"kind\":\"HorizontalPodAutoscaler\",\"metadata\":{\"annotations\":{},\"name\":\"nginx\",\"namespace\":\"default\"},\"spec\":{\"maxReplicas\":10,\"minReplicas\":1,\"scaleTargetRef\":{\"apiVersion\":\"apps/v1\",\"kind\":\"Deployment\",\"name\":\"nginx\"},\"targetCPUUtilizationPercentage\":50}}\n creationTimestamp: \"2019-10-30T19:42:43Z\"\n name: nginx\n namespace: default\n resourceVersion: \"220050\"\n selfLink: /apis/autoscaling/v1/namespaces/default/horizontalpodautoscalers/nginx\n uid: 70d1067d-fb4d-11e9-8b2a-42010a8e013f\nspec:\n maxReplicas: 10\n minReplicas: 1\n scaleTargetRef:\n apiVersion: apps/v1\n kind: Deployment\n name: nginx\n targetCPUUtilizationPercentage: 50\nstatus:\n currentCPUUtilizationPercentage: 0\n currentReplicas: 3\n desiredReplicas: 3\n```\nBefore following the remaining examples in this topic, delete the HPA:\n```\nkubectl delete hpa nginx\n```\nWhen you delete a Horizontal Pod Autoscaler, the number of replicas of the Deployment remain the same. A Deployment does not automatically revert back to its state before the Horizontal Pod Autoscaler was applied.\nYou can learn more about [deleting a Horizontal Pod Autoscaler](#deleting) .\n## Autoscaling based on load balancer traffic\n**    Preview     ** This feature is subject to the \"Pre-GA Offerings Terms\" in the General Service Terms section   of the [Service Specific Terms](/terms/service-terms#1) .     Pre-GA features are available \"as is\" and might have limited support.    For more information, see the [launch stage descriptions](/products#product-launch-stages) .\nTraffic-based autoscaling is a capability of GKE that integrates traffic utilization signals from load balancers to autoscale Pods.\nUsing traffic as an autoscaling signal might be helpful since traffic is a leading indicator of load that is complementary to CPU and memory. Built-in integration with GKE ensures that the setup is easy and that autoscaling reacts to traffic spikes quickly to meet demand.\nTraffic-based autoscaling is enabled by the [Gateway controller](/kubernetes-engine/docs/concepts/gateway-api) and its [global traffic management](/kubernetes-engine/docs/concepts/traffic-management) capabilities. To learn more, see [Traffic-based autoscaling](/kubernetes-engine/docs/concepts/traffic-management#traffic-based_autoscaling) .\nAutoscaling based on load balancer traffic is only available for [Gateway workloads](/kubernetes-engine/docs/concepts/gateway-api) .\nTraffic-based autoscaling has the following requirements:\n- Supported on GKE versions 1.24 and later.\n- Gateway API enabled in your GKE cluster.\n- Supported for traffic that goes through load balancers deployed using the Gateway API and either the`gke-l7-global-external-managed`,`gke-l7-regional-external-managed`,`gke-l7-rilb`, or the`gke-l7-gxlb`GatewayClass.Traffic-based autoscaling has the following limitations:\n- Not supported by the multi-cluster GatewayClasses (`gke-l7-global-external-managed-mc`,`gke-l7-regional-external-managed-mc`,`gke-l7-rilb-mc`, and`gke-l7-gxlb-mc`).\n- Not supported for traffic using Services of type`ClusterIP`or`LoadBalancer`.The following exercise uses the `HorizontalPodAutoscaler` to autoscale the `store-autoscale` Deployment based on the traffic it receives. A [Gateway](/kubernetes-engine/docs/how-to/deploying-gateways) accepts ingress traffic from the internet for the Pods. The autoscaler compares traffic signals from the Gateway with the [per-Pod traffic capacity](/kubernetes-engine/docs/concepts/traffic-management#service_capacity) that is configured on the `store-autoscale` Service resource. By generating traffic to the Gateway, you influence the number of Pods deployed.\nThe following diagram demonstrates how traffic-based autoscaling works:\nTo deploy traffic-based autoscaling, perform the following steps:\n- For Standard clusters, confirm that the GatewayClasses are installed in your cluster. For Autopilot clusters, the GatewayClasses are installed by default.```\nkubectl get gatewayclass\n```The output confirms that the GKE GatewayClass resources are ready to use in your cluster:```\nNAME        CONTROLLER     ACCEPTED AGE\ngke-l7-global-external-managed  networking.gke.io/gateway True  16h\ngke-l7-regional-external-managed networking.gke.io/gateway True  16h\ngke-l7-gxlb      networking.gke.io/gateway True  16h\ngke-l7-rilb      networking.gke.io/gateway True  16h\n```If you do not see this output, [enable the Gateway API](/kubernetes-engine/docs/how-to/deploying-gateways#enable-gateway) in your GKE cluster.\n- Deploy the sample application and Gateway load balancer to your cluster:```\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/gke-networking-recipes/master/gateway/docs/store-autoscale.yaml\n```The sample application creates:- A Deployment with 2 replicas.\n- A Service capacity with`max-rate-per-endpoint`set to`10`. While this feature is in [Preview](/products#product-launch-stages) , use an annotation in the Service. When this feature is generally available, the Service [Policy](/kubernetes-engine/docs/concepts/gateway-api#policy) will replace the annotation. To learn more about Gateway capabilities, see [GatewayClass capabilities](/kubernetes-engine/docs/how-to/gatewayclass-capabilities) .\n- An external Gateway for accessing the application on the internet. To learn more about how to use Gateway load balancers, see [Deploying Gateways](/kubernetes-engine/docs/how-to/deploying-gateways) .\n- An HTTPRoute that matches all traffic and sends it to the`store-autoscale`Service.\nThe [Service capacity](/kubernetes-engine/docs/concepts/traffic-management#service_capacity) is a critical element when using traffic-based autoscaling because it determines the amount of per-Pod traffic that triggers an autoscaling event. It is configured Service capacity using the Service annotation `networking.gke.io/max-rate-per-endpoint` , which defines the maximum traffic a Service should receive in requests per second, per Pod. Service capacity is specific to your application. For more information, see [Determining your Service's capacity](/kubernetes-engine/docs/concepts/traffic-management#capacity) .\n- Save the following manifest as `hpa.yaml` :```\napiVersion: autoscaling/v2kind: HorizontalPodAutoscalermetadata:\u00a0 name: store-autoscalespec:\u00a0 scaleTargetRef:\u00a0 \u00a0 apiVersion: apps/v1\u00a0 \u00a0 kind: Deployment\u00a0 \u00a0 name: store-autoscale\u00a0 minReplicas: 1\u00a0 maxReplicas: 10\u00a0 metrics:\u00a0 - type: Object\u00a0 \u00a0 object:\u00a0 \u00a0 \u00a0 describedObject:\u00a0 \u00a0 \u00a0 \u00a0 kind: Service\u00a0 \u00a0 \u00a0 \u00a0 name: store-autoscale\u00a0 \u00a0 \u00a0 metric:\u00a0 \u00a0 \u00a0 \u00a0 name: \"autoscaling.googleapis.com|gclb-capacity-utilization\"\u00a0 \u00a0 \u00a0 target:\u00a0 \u00a0 \u00a0 \u00a0 averageValue: 70\u00a0 \u00a0 \u00a0 \u00a0 type: AverageValue\n```This manifest describes a `HorizontalPodAutoscaler` with the following properties:- `minReplicas`and`maxReplicas`: sets the minimum and maximum number of replicas for this Deployment. In this configuration, the number of Pods can scale from 1 to 10 replicas.\n- `describedObject.name: store-autoscale`: the reference to the`store-autoscale`Service that defines the traffic capacity.\n- `scaleTargetRef.name: store-autoscale`: the reference to the`store-autoscale`Deployment that defines the resource that is scaled by the Horizontal Pod Autoscaler.\n- `averageValue: 70`: target average value of capacity utilization. This gives the Horizontal Pod Autoscaler a growth margin so that the running Pods can process excess traffic while new Pods are being created.\n **Note:** The ratios between Gateway, Horizontal Pod Autoscaler, Deployment, and Service must be 1:1:1:1. This means that a Deployment or Service cannot be referenced by more than one Horizontal Pod Autoscaler. A Service that is referenced by a Horizontal Pod Autoscaler cannot be targeted by more than one load balancer. If this condition is not met, the Horizontal Pod Autoscaler stops autoscaling and errors appear in the Horizontal Pod Autoscaler events.\nThe Horizontal Pod Autoscaler results in the following traffic behavior:\n- The number of Pods is adjusted between 1 and 10 replicas to achieve 70% of the max rate per endpoint. This results in 7 RPS per Pod when`max-rate-per-endpoint=10`.\n- At more than 7 RPS per pod, Pods are scaled up until they've reached their maximum of 10 replicas or until the average traffic is 7 RPS per Pod.\n- If traffic is reduced, Pods scale down to a reasonable rate using the Horizontal Pod Autoscaler algorithm.\nYou can also [deploy a traffic generator](/kubernetes-engine/docs/how-to/deploying-multi-cluster-gateways#verify_traffic_using_load_testing) to validate traffic-based autoscaling behavior.\nAt 30 RPS, the Deployment is scaled to 5 replicas so that each replica ideally receives 6 RPS of traffic, which would be 60% utilization per Pod. This is under the 70% target utilization and so the Pods are scaled appropriately. Depending on traffic fluctuations, the number of autoscaled replicas might also fluctuate. For a more detailed description of how the number of replicas is computed, see [Autoscaling behavior](/kubernetes-engine/docs/concepts/traffic-management#autoscaling-behavior) .\n## Autoscaling based on a custom or external metric\nTo create horizontal Pod autoscalers for custom metrics and external metrics, see [Optimize Pod autoscaling based on metrics](/kubernetes-engine/docs/tutorials/custom-metrics-autoscaling) .\n## Autoscaling based on multiple metrics\nThis example creates a Horizontal Pod Autoscaler that autoscales based on CPU utilization and a custom metric named `packets_per_second` .\nIf you followed the previous example and still have a Horizontal Pod Autoscaler named `nginx` , [delete it](#deleting) before following this example.\nThis example requires `apiVersion: autoscaling/v2` . For more information about the available APIs, see [API versions for HorizontalPodAutoscaler objects](#api-versions) .\nBefore you can autoscale based on a custom metric, you must create the custom metric and configure your workload to export the metric to Cloud Monitoring. For this reason, the `packets_per_second` metric in the manifest below is included for illustration, but commented out. See [custom metrics](/kubernetes-engine/docs/tutorials/custom-metrics-autoscaling) and the Monitoring documentation for [creating custom metrics](/monitoring/custom-metrics) .\nSave this YAML manifest as a file named `nginx-multiple.yaml` :\n```\napiVersion: autoscaling/v2kind: HorizontalPodAutoscalermetadata:\u00a0 name: nginxspec:\u00a0 scaleTargetRef:\u00a0 \u00a0 apiVersion: apps/v1\u00a0 \u00a0 kind: Deployment\u00a0 \u00a0 name: nginx\u00a0 minReplicas: 1\u00a0 maxReplicas: 10\u00a0 metrics:\u00a0 - type: Resource\u00a0 \u00a0 resource:\u00a0 \u00a0 \u00a0 name: cpu\u00a0 \u00a0 \u00a0 target:\u00a0 \u00a0 \u00a0 \u00a0 type: Utilization\u00a0 \u00a0 \u00a0 \u00a0 averageUtilization: 50\u00a0 - type: Resource\u00a0 \u00a0 resource:\u00a0 \u00a0 \u00a0 name: memory\u00a0 \u00a0 \u00a0 target:\u00a0 \u00a0 \u00a0 \u00a0 type: AverageValue\u00a0 \u00a0 \u00a0 \u00a0 averageValue: 100Mi\u00a0 # Uncomment these lines if you create the custom packets_per_second metric and\u00a0 # configure your app to export the metric.\u00a0 # - type: Pods\u00a0 # \u00a0 pods:\u00a0 # \u00a0 \u00a0 metric:\u00a0 # \u00a0 \u00a0 \u00a0 name: packets_per_second\u00a0 # \u00a0 \u00a0 target:\u00a0 # \u00a0 \u00a0 \u00a0 type: AverageValue\u00a0 # \u00a0 \u00a0 \u00a0 averageValue: 100\n```\nApply the YAML manifest:\n```\nkubectl apply -f nginx-multiple.yaml\n```\nWhen created, the Horizontal Pod Autoscaler monitors the `nginx` Deployment for average CPU utilization, average memory utilization, and (if you uncommented it) the custom `packets_per_second` metric. The Horizontal Pod Autoscaler autoscales the Deployment based on the metric whose value would create the larger autoscale event.\n## Viewing details about a Horizontal Pod Autoscaler\nTo view a Horizontal Pod Autoscaler's configuration and statistics, use the following command:\n```\nkubectl describe hpa HPA_NAME\n```\nReplace `` with the name of your `HorizontalPodAutoscaler` object.\nIf the Horizontal Pod Autoscaler uses `apiVersion: autoscaling/v2` and is based on multiple metrics, the `kubectl describe hpa` command only shows the CPU metric. To see all metrics, use the following command instead:\n```\nkubectl describe hpa.v2.autoscaling HPA_NAME\n```\nReplace `` with the name of your `HorizontalPodAutoscaler` object.\nEach Horizontal Pod Autoscaler's current status is shown in `Conditions` field, and autoscaling events are listed in the `Events` field.\nThe output is similar to the following:\n```\nName:             nginx\nNamespace:            default\nLabels:            <none>\nAnnotations:           kubectl.kubernetes.io/last-applied-configuration:\n               {\"apiVersion\":\"autoscaling/v2\",\"kind\":\"HorizontalPodAutoscaler\",\"metadata\":{\"annotations\":{},\"name\":\"nginx\",\"namespace\":\"default\"},\"s...\nCreationTimestamp:          Tue, 05 May 2020 20:07:11 +0000\nReference:            Deployment/nginx\nMetrics:            ( current / target )\n resource memory on pods:        2220032 / 100Mi\n resource cpu on pods (as a percentage of request): 0% (0) / 50%\nMin replicas:           1\nMax replicas:           10\nDeployment pods:          1 current / 1 desired\nConditions:\n Type   Status Reason    Message\n ----   ------ ------    ------ AbleToScale  True ReadyForNewScale recommended size matches current size\n ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from memory resource\n ScalingLimited False DesiredWithinRange the desired count is within the acceptable range\nEvents:            <none>\n```\n## Deleting a Horizontal Pod Autoscaler\nYou can delete a Horizontal Pod Autoscaler using the Google Cloud console or the `kubectl delete` command.\nTo delete the `nginx` Horizontal Pod Autoscaler:- Go to the **Workloads** page in the Google Cloud console. [Go to Workloads](https://console.cloud.google.com/kubernetes/workloads) \n- Click the name of the `nginx` Deployment.\n- Click **Actions > Autoscale** .\n- Click **Delete** .\nTo delete the `nginx` Horizontal Pod Autoscaler, use the following command:\n```\nkubectl delete hpa nginx\n```\nWhen you delete a Horizontal Pod Autoscaler, the Deployment or (or other deployment object) remains at its existing scale, and does not revert back to the number of replicas in the Deployment's original manifest. To manually scale the Deployment back to three Pods, you can use the `kubectl scale` command:\n```\nkubectl scale deployment nginx --replicas=3\n```\n## Cleaning up\n- Delete the Horizontal Pod Autoscaler, if you have not done so:```\nkubectl delete hpa nginx\n```\n- Delete the `nginx` Deployment:```\nkubectl delete deployment nginx\n```\n- Optionally, [delete the cluster](/kubernetes-engine/docs/how-to/deleting-a-cluster) .## Troubleshooting\nWhen you set up a Horizontal Pod Autoscaler,you might see warning messages like the following:\n```\nunable to fetch pod metrics for pod\n```\nIt's normal to see this message when the metrics server starts up. However, if you continue to see the warnings and you notice that Pods are not scaling for your workload, please ensure you have [specified resource requests for each container in your workload](/kubernetes-engine/docs/concepts/horizontalpodautoscaler#per-pod_resources) . To use resource utilization percentage targets with horizontal Pod autoscaling, you must configure requests for that resource for each container running in each Pod in the workload. Otherwise, the Horizontal Pod Autoscaler cannot perform the calculations it needs to, and takes no action related to that metric.\n## What's next\n- Learn more about [Horizontal Pod Autoscaling](/kubernetes-engine/docs/concepts/horizontalpodautoscaler) .\n- Learn more about [Vertical Pod Autoscaling](/kubernetes-engine/docs/how-to/vertical-pod-autoscaling) .\n- Learn more about [Multidimensional Pod Autoscaling](/kubernetes-engine/docs/how-to/multidimensional-pod-autoscaling) .\n- Learn more about [autoscaling Deployments with Custom Metrics](/kubernetes-engine/docs/tutorials/custom-metrics-autoscaling) .\n- Learn how to [Assign CPU Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/) .\n- Learn how to [Assign Memory Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/) .", "guide": "Google Kubernetes Engine (GKE)"}