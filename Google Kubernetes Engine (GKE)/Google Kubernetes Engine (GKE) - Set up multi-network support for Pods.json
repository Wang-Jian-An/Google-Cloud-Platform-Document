{"title": "Google Kubernetes Engine (GKE) - Set up multi-network support for Pods", "url": "https://cloud.google.com/kubernetes-engine/docs/how-to/setup-multinetwork-support-for-pods", "abstract": "# Google Kubernetes Engine (GKE) - Set up multi-network support for Pods\nThis page shows you how to enable multiple interfaces on nodes and Pods in a Google Kubernetes Engine (GKE) cluster using multi-network support for Pods. Multi-network support is only available for projects [enabled](/anthos/docs/setup/enable-gkee) with [GKE Enterprise](/kubernetes-engine/pricing) .\nYou should already be familiar with general [networking concepts](/kubernetes-engine/docs/concepts/network-overview) , [terminology and concepts](/kubernetes-engine/docs/concepts/about-multinetwork-support-for-pods#terminology-concepts) specific to this feature, and [requirements and limitations](#requirements-and-limitations) for multi-network support for Pods.\nFor more information, see [About multi-network support for Pods](/kubernetes-engine/docs/concepts/about-multinetwork-support-for-pods) .\n", "content": "## Requirements and limitations\nMulti-network support for Pods has the following requirements and limitations:\n### Requirements\n- GKE version 1.28 or later.\n- Enable [Google Kubernetes Engine (GKE) Enterprise edition](/anthos/docs/setup/enable-gkee) \n- Multi-network support for Pods uses the same [VM-level specifications](/vpc/docs/create-use-multiple-interfaces#specifications) as multi-NIC for Compute Engine.\n- Multi-network support for Pods requires GKE Dataplane V2.\n- Multi-network support for Pods is only available for Container-Optimized OS nodes that are running version m101 or later.\n### General limitations\n- Multi-network support for Pods doesn't work for clusters that are enabled for [dual-stack networking](/kubernetes-engine/docs/concepts/alias-ips#dual_stack_network) .\n- [Multi-Pod CIDR](/kubernetes-engine/docs/how-to/multi-pod-cidr) is not available for clusters with multi-network enabled.\n- Any Pod-networks in a single GKE cluster can't have overlapping CIDR ranges.\n- When you enable multi-network support for Pods, you can't add or remove node-network interfaces or Pod-networks after creating a node pool. To change these settings, you must recreate the node pool.\n- By default, internet access is not available on additional interfaces of Pod-networks inside the Pod. However, you can enable it manually using Cloud NAT.\n- You cannot change the default Gateway inside a Pod with multiple interfaces through the API. The default Gateway must be connected to the default Pod-network.\n- The default Pod-network must always be included in Pods, even if you create additional Pod-networks or interfaces.\n- You cannot configure the multi-network feature when Managed Hubble has been configured.\n### Device and Data Plane Development Kit (DPDK) limitations\n- A VM NIC passed into a Pod as a`Device`type NIC is not available to other Pods on the same node.\n- Pods that use DPDK mode must be run in privileged mode to access VFIO devices.\n- In DPDK mode, a device is treated as a node resource and is only attached to the first container (non-init) in the Pod. If you want to split multiple DPDK devices among containers in the same Pod, you need to run those containers in separate Pods.\n### Scaling limitations\nGKE provides a flexible network architecture that lets you to scale your cluster. You can add additional node-networks and Pod-networks to your cluster. You can scale your cluster as follows:\n- You can add up to 7 additional node-networks to each GKE node pool. This is the same scale limit for Compute Engine VMs.\n- Each pod must have less than 7 additional networks attached.\n- You can configure up to 35 Pod-networks across the 8 node-networks within a single node pool. You can break it down into different combinations, such as:- 7 node-networks with 5 Pod-networks each\n- 5 node-networks with 7 Pod-networks each\n- 1 node-network with 30 Pod-networks. The limit for secondary ranges per subnet is 30.\n- You can configure up to 50 Pod-networks per cluster.\n- You can configure up to a maximum of 32 multi-network Pods per node.\n- You can have up to 3000 nodes with multiple interfaces.\n- You can have up to 100,000 additional interfaces across all Pods.\n- You can configure up to a maximum of 1000 nodes with`Device`type networks.\n- You can configure up to a maximum of 4`Device`type networks per node.## Pricing\nNetwork Function Optimizer (NFO) features including multi-network and high performance support for Pods are supported only on clusters that are in Projects enabled with [GKE Enterprise](/anthos/docs/concepts/gke-editions) . To understand the charges that apply for enabling Google Kubernetes Engine (GKE) Enterprise edition, see [GKE Enterprise Pricing](/anthos/pricing) .\n## Deploy multi-network Pods\nTo deploy multi-network Pods, do the following:\n- [Prepare an additional VPC](#prepare-an-additional-vpc) , subnet (node-network), and secondary ranges (Pod-network).\n- [Create a multi-network enabled GKE cluster](#create-a-gke-cluster) using the Google Cloud CLI command.\n- [Create a new GKE node pool](#create-gke-node-pool) that is connected to the additional node-network and Pod-network using the Google Cloud CLI command.\n- [Create Pod network](#create-pod-network) and reference the correct VPC, subnet, and secondary ranges in multi-network objects using the Kubernetes API.\n- In your workload configuration, [reference the prepared Network](#reference-the-prepared-network) Kubernetes object using the Kubernetes API.## Before you begin\nBefore you start, make sure you have performed the following tasks:\n- Enable    the Google Kubernetes Engine API.\n- [    Enable Google Kubernetes Engine API   ](https://console.cloud.google.com/flows/enableapi?apiid=container.googleapis.com) \n- If you want to use the Google Cloud CLI for this task, [install](/sdk/docs/install) and then [initialize](/sdk/docs/initializing) the  gcloud CLI. If you previously installed the gcloud CLI, get the latest  version by running`gcloud components update`. **Note:** For existing gcloud CLI  installations, make sure to set the`compute/region`and`compute/zone` [properties](/sdk/docs/properties#setting_properties) . By setting default locations,  you can avoid errors in gcloud CLI like the following:`One of [--zone, --region] must be supplied: Please specify location`.\n- [Enable GKE Enterprise](/../../anthos/docs/setup/enable-gkee) .\n- Review the [requirements and limitations](/kubernetes-engine/docs/concepts/about-multinetwork-support-for-pods#requirements-and-limitations) .\n### Prepare an additional VPC\nGoogle Cloud creates a default Pod network during cluster creation associated with the GKE node pool used during the initial creation of the GKE cluster. The default Pod network is available on all cluster nodes and Pods. To facilitate multi-network capabilities within the node pool, you must prepare existing or new VPCs, which support `Layer 3` and `Device` type networks.\nFor preparing additional VPC, consider the following requirements:\n- `Layer 3` and `Netdevice` type network:- Create a secondary range if you are using`Layer 3`type networks.\n- Ensure that the CIDR size for the secondary range is large enough to satisfy the number of nodes in the node pool and the number of Pods per node you want to have.\n- Similar to the default Pod-network, the other Pod-networks use IP address overprovisioning. The secondary IP address range must have twice as many IP addresses per node as the number of Pods per node.\n- `Device` type network requirements: Create a regular subnet on a VPC. You don't require a secondary subnet.\nTo enable multi-network capabilities in the node pool, you must prepare the VPCs to which you want to establish additional connections. You can use an existing VPC or [Create a newVPC](/vpc/docs/create-modify-vpc-networks) specifically for the node pool.\n**Note:** After the node pool is created, you cannot add a new VPC to connect to the existing node pool.\nTo create a VPC network that supports `Layer 3` type device, do the following:\n- Ensure that the CIDR size for the secondary range is large enough to satisfy the number of nodes in the node pool and the number of Pods per node you want to have.\n- Similar to the default Pod-network, the other Pod-networks use IP address overprovisioning. The secondary IP address range must have twice as many IP addresses per node as the number of Pods per node.\n```\ngcloud compute networks subnets create SUBNET_NAME \\\u00a0 \u00a0 --project=PROJECT_ID \\\u00a0 \u00a0 --range=SUBNET_RANGE \\\u00a0 \u00a0 --network=NETWORK_NAME \\\u00a0 \u00a0 --region=REGION \\\u00a0 \u00a0 --secondary-range=SECONDARY_RANGE_NAME=<SECONDARY_RANGE_RANGE>\n```\nReplace the following:- ``: the name of the subnet.\n- ``: the ID of the project that contains the VPC network where the subnet is created.\n- ``: the primary IPv4 address range for the new subnet, in CIDR notation.\n- ``: the name of the VPC network that contains the new subnet.\n- ``: the Google Cloud region in which the new subnet is created.\n- ``: the name for the secondary range.\n- ``the secondary IPv4 address range in CIDR notation.\n- In the Google Cloud console, go to the **VPC networks** page.\n- Click **Create VPC network** .\n- In the **Name** field, enter the name of the network. For example, `l3-vpc` .\n- From the **Maximum transmission unit (MTU)** drop-down, choose appropriate MTU value. **Note:** Before setting the MTU to a value higher than `1460` , review the Maximum [transmission unit](/vpc/docs/mtu) .\n- In the **Subnet creation mode** section, choose **Custom** .\n- Click **ADD SUBNET** .\n- In the **New subnet** section, specify the following configuration parameters for a subnet:- Provide a **Name** . For example, `l3-subnet` .\n- Select a **Region** .\n- Enter an **IP address range** . This is the [primary IPv4range](/vpc/docs/vpc#vpc_networks_and_subnets) for the subnet.If you select a range that is not an RFC 1918 address, confirm that the range doesn't conflict with an existing configuration. For more information, see [IPv4 subnetranges](/vpc/docs/subnets#manually_created_subnet_ip_ranges) .\n- To define a secondary range for the subnet, click **Create secondary IPaddress range** .If you select a range that is not an RFC 1918 address, confirm that the range doesn't conflict with an existing configuration. For more information, see [IPv4 subnetranges](/vpc/docs/subnets#manually_created_subnet_ip_ranges) .\n- **Private Google access** : You can enable [Private Google Access](/vpc/docs/private-access-options) for the subnet when you create it or later by editing it.\n- **Flow logs** : You can enable [VPC flowlogs](/vpc/docs/using-flow-logs) for the subnet when you create it or later by editing it.\n- Click **Done** .\n **Note:** To add more subnets, click **Add subnet** . You can also [add moresubnets](/vpc/docs/create-modify-vpc-networks#add-subnets) to the network after you have created the network.\n- In the **Firewall rules** section, under **IPv4 firewall rules** , select zero or more predefined [firewall rules](/vpc/docs/firewalls) .The rules address common use cases for connectivity to instances. You can [create your own firewall rules](/vpc/docs/using-firewalls) after you create the network. Each predefined rule name starts with the name of the VPC network that you are creating.\n- Under **IPv4 firewall rules** , to edit the predefined ingress firewall rule named `allow-custom` , click **EDIT** .You can edit subnets, add additional IPv4 ranges, and specify protocols and ports.The `allow-custom` firewall rule is not automatically updated if you add additional subnets later. If you need firewall rules for the new subnets, to add the rules, you must update the firewall configuration.\n- In the **Dynamic routing mode** section, for the VPC network. For more information, see [dynamic routingmode](/vpc/docs/vpc#routing_for_hybrid_networks) . You can [change thedynamic routingmode](/vpc/docs/create-modify-vpc-networks#switch-dynamic-routing) later.\n- Click **Create** .```\ngcloud compute networks subnets create SUBNET_NAME \\\u00a0 \u00a0 --project=PROJECT_ID \\\u00a0 \u00a0 --range=SUBNET_RANGE \\\u00a0 \u00a0 --network=NETWORK_NAME \\\u00a0 \u00a0 --region=REGION \\\u00a0 \u00a0 --secondary-range=SECONDARY_RANGE_NAME=<SECONDARY_RANGE_RANGE>\n```\nReplace the following:- ``: the name of the subnet.\n- ``: the ID of the project that contains the VPC network where the subnet is created.\n- ``: the primary IPv4 address range for the new subnet, in CIDR notation.\n- ``: the name of the VPC network that contains the new subnet.\n- ``: the Google Cloud region in which the new subnet is created.\n- ``: the name for the secondary range.\n- ``the secondary IPv4 address range in CIDR notation.\n- In the Google Cloud console, go to the **VPC networks** page.\n- Click **Create VPC network** .\n- In the **Name** field, enter the name of the network. For example, `netdevice-vpc` or `dpdk-vpc` .\n- From the **Maximum transmission unit (MTU)** drop-down, choose appropriate MTU value. **Note:** Before setting the MTU to a value higher than `1460` , review the [Maximum transmission unit](/vpc/docs/mtu) .\n- In the **Subnet creation mode** section, choose **Custom** .\n- In the **New subnet** section, specify the following configuration parameters for a subnet:- Provide a **Name** . For example, `netdevice-subnet` or `dpdk-vpc` .\n- Select a **Region** .\n- Enter an **IP address range** . This is the [primary IPv4range](/vpc/docs/vpc#vpc_networks_and_subnets) for the subnet.If you select a range that is not an RFC 1918 address, confirm that the range doesn't conflict with an existing configuration. For more information, see [IPv4 subnetranges](/vpc/docs/subnets#manually_created_subnet_ip_ranges) .\n- **Private Google Access** : Choose whether to enable [Private Google Access](/vpc/docs/private-access-options) for the subnet when you create it or later by editing it.\n- **Flow logs** : You can enable [VPC flowlogs](/vpc/docs/using-flow-logs) for the subnet when you create it or later by editing it.\n- Click **Done** . **Note:** To add more subnets, click **Add subnet** . You can also [add moresubnets](/vpc/docs/create-modify-vpc-networks#add-subnets) to the network after you have created the network.\n- In the **Firewall rules** section, under **IPv4 firewall rules** , select zero or more predefined [firewall rules](/vpc/docs/firewalls) .The rules address common use cases for connectivity to instances. You can [create your own firewall rules](/vpc/docs/using-firewalls) after you create the network. Each predefined rule name starts with the name of the VPC network that you are creating.\n- Under **IPv4 firewall rules** , to edit the predefined ingress firewall rule named `allow-custom` , click **EDIT** .You can edit subnets, add additional IPv4 ranges, and specify protocols and ports.The `allow-custom` firewall rule is not automatically updated if you add additional subnets later. If you need firewall rules for the new subnets, to add the rules, you must update the firewall configuration.\n- In the **Dynamic routing mode** section, for the VPC network. For more information, see [dynamic routingmode](/vpc/docs/vpc#routing_for_hybrid_networks) . You can [change thedynamic routingmode](/vpc/docs/create-modify-vpc-networks#switch-dynamic-routing) later.\n- Click **Create** .\n### Create a GKE cluster with multi-network capabilities\nTo create a GKE cluster with multi-network capabilities:\n```\ngcloud container clusters create CLUSTER_NAME \\\u00a0 \u00a0 --cluster-version=CLUSTER_VERSION \\\u00a0 \u00a0 --enable-dataplane-v2 \\\u00a0 \u00a0 --enable-ip-alias \\\u00a0 \u00a0 --enable-multi-networking\n```\nReplace the following:- ``: the name of the cluster.\n- ``: the version of the cluster.\nThis command includes the following flags:- `--enable-multi-networking:`enables multi-networking Custom Resource Definitions (CRDs) in the API server for this cluster, and deploys a network-controller-manager which contains the reconciliation and lifecycle management for multi-network objects.\n- `--enable-dataplane-v2:`enables GKE Dataplane V2. This flag is required to enable multi-network.\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- Configure your Standard cluster. For more information, see [Createa zonalcluster](/kubernetes-engine/docs/how-to/creating-a-zonal-cluster#creating-a-cluster) or [Create a regionalcluster](/kubernetes-engine/docs/how-to/creating-a-regional-cluster) . While creating the cluster, select the appropriate Network and Node subnet.\n- From the navigation pane, under **Cluster** , click **Networking** .\n- Select **Enable Dataplane V2** checkbox.\n- Select **Enable Multi-Network** .\n- Click **Create** .\nEnabling multi-networking for a cluster adds the necessary CustomResourceDefinitions (CRDs) to the API server for that cluster. It also deploys a network-controller-manager, which is responsible for reconciling and managing multi-network objects. You cannot modify the cluster configuration after it is created.\n### Create a GKE node pool connected to additional VPCs\nCreate a node pool that includes nodes connected to the node-network (VPC and subnet) and Pod-network (secondary range) created in [Create Pod network](#create-pod-network) .\nTo create the new node pool and associate it with the additional networks in the GKE cluster:\n```\ngcloud container node-pools create POOL_NAME \\\u00a0 --cluster=CLUSTER_NAME \\\u00a0 --additional-node-network network=NETWORK_NAME,subnetwork=SUBNET_NAME \\\u00a0 --additional-pod-network subnetwork=subnet-dp,pod-ipv4-range=POD_IP_RANGE,max-pods-per-node=NUMBER_OF_PODS \\\u00a0 --additional-node-network network=highperformance,subnetwork=subnet-highperf\n```\nReplace the following:- ``with the name of the new node pool.\n- ``with the name of the existing cluster to which you are adding the node pool.\n- ``with the name of the network to attach the node pool's nodes.\n- ``with the name of the subnet within the network to use for the nodes.\n- ``the Pod IP address range within the subnet.\n- ``maximum number of Pods per node.\nThis command contains the following flags:- `--additional-node-network`: Defines details of the additional network interface, network, and subnetwork. This is used to specify the node-networks for connecting to the node pool nodes. Specify this parameter when you want to connect to another VPC. If you don't specify this parameter, the default VPC associated with the cluster is used. For`Layer 3`type networks, specify the`additional-pod-network`flag that defines the Pod-network, which is exposed inside the GKE cluster as the`Network`object. When using the`--additional-node-network`flag, you must provide a network and subnetwork as mandatory parameters. Make sure to separate the network and subnetwork values with a comma and avoid using spaces.\n- `--additional-pod-network`: Specifies the details of the secondary range to be used for the Pod-network. This parameter is not required if you use a`Device`type network. This argument specifies the following key values:`subnetwork`,`pod-ipv4-range`, and`max-pods-per-node`. When using the`--additional-pod-network`, you must provide the`pod-ipv4-range`and`max-pods-per-node`values, separated by commas and without spaces.- `subnetwork`: links the node-network with Pod-network. The subnetwork is optional. If you don't specify it, the additional Pod-network is associated with the default subnetwork provided during cluster creation.\n- `--max-pods-per-node`: The`max-pods-per-node`must be specified and has to be a power of 2. The minimum value is 4. The`max-pods-per-node`must not be more than the`max-pods-per-node`value on the node pool.- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- From the navigation pane, click **Clusters** .\n- In the **Kubernetes clusters** section, click the cluster you created.\n- At the top of the page, to create your node pool, click **Add Node Pool** .\n- In the **Node pool details** section, complete the following:- Enter a **Name** for the [node pool](/kubernetes-engine/docs/concepts/node-pools) .\n- Enter the **Number of nodes** to create in the node pool.\n- From the navigation pane, under **Node Pools** , click **Nodes** .- From the **Image type** drop-down list, select the **Container-Optimized OS with containerd (cos_containerd)** node image. **Warning: ** In GKE version 1.24 and later, Docker-based node image types are not supported. In GKE version 1.23, you also cannot create new node pools with Docker node image types. You must migrate to a containerd node image type. To learn more about this change, see [About the Docker node image deprecation](/kubernetes-engine/docs/deprecations/docker-containerd) .\n- When you create a VM, you select a machine type from a [machine family](/compute/docs/machine-types) that determines the resources available to that VM. For example, a machine type like `e2-standard-4` contains 4 vCPUs, therefore can support up to 4 VPCs total. There are several machine families you can choose from and each machine family is further organized into machine series and predefined or custom machine types within each series. Each machine type is billed differently. For more information, refer to the [machine type price sheet](/compute/pricing#standard_machine_types) .\n- From the navigation pane, select **Networking** .\n- In the section **Node Networking** , specify the maximum number of Pods per node. The Node Networks section displays the VPC network utilized to create the cluster. It is necessary to designate extra Node Networks that correlate with previously established VPC Networks and Device types.\n- Create node pool association:- For`Layer 3`type device:- In the **Node Networks** section, click **ADD A NODE NETWORK** .\n- From the network drop-down list select the [VPC that supports Layer 3 type device](#vpc-layer3) .\n- Select the [subnet](#layer3-subnet) created for`Layer 3`VPC.\n- In the section **Alias Pod IP address ranges** , click **Add Pod IP address range** .\n- Select the **Secondary subnet** and indicate the **Max number of Pods per node** .\n- Select **Done** .\n- For`Netdevice`and`DPDK`type device:- In the **Node Networks** section, click **ADD A NODE NETWORK** .\n- From the network drop-down list select the [VPC that supports Netdevice or DPDK type devices](#vpc-netdevice-dpdk) .\n- Select the [subnet](#netdevice-dpdk-subnet) created for`Netdevice`or`DPDK`VPC.\n- Select **Done** .\n- Click **Create** .\n**Notes:**\n- If multiple additional Pod-networks are specified within the same node-network, they must be in the same subnet.\n- You can't reference the same secondary range of a subnet multiple times.\n**Example** The following example creates a node pool named pool-multi-net that attaches two additional networks to the nodes, datapalane ( `Layer 3` type network) and highperformance (netdevice type network). This example assumes that you already created a GKE cluster named `cluster-1` :\n```\ngcloud container node-pools create pool-multi-net \\\u00a0 --project my-project \\\u00a0 --cluster cluster-1 \\\u00a0 --zone us-central1-c \\\u00a0 --additional-node-network network=dataplane,subnetwork=subnet-dp \\\u00a0 --additional-pod-network subnetwork=subnet-dp,pod-ipv4-range=sec-range-blue,max-pods-per-node=8 \\\u00a0 --additional-node-network network=highperformance,subnetwork=subnet-highperf\n```\nTo specify additional node-network and Pod-network interfaces, define the `--additional-node-network` and `--additional-pod-network` parameters multiple times as shown in the following example:\n```\n--additional-node-network network=dataplane,subnetwork=subnet-dp \\--additional-pod-network subnetwork=subnet-dp,pod-ipv4-range=sec-range-blue,max-pods-per-node=8 \\--additional-pod-network subnetwork=subnet-dp,pod-ipv4-range=sec-range-green,max-pods-per-node=8 \\--additional-node-network network=managementdataplane,subnetwork=subnet-mp \\--additional-pod-network subnetwork=subnet-mp,pod-ipv4-range=sec-range-red,max-pods-per-node=4\n```\nTo specify additional Pod-networks directly on the primary VPC interface of the node pool, as shown in the following example:\n```\n--additional-pod-network subnetwork=subnet-def,pod-ipv4-range=sec-range-multinet,max-pods-per-node=8\n```\n### Create Pod network\nDefine the Pod networks that the Pods will access by defining Kubernetes objects and linking them to the corresponding Compute Engine resources, such as VPCs, subnets, and secondary ranges.\nTo create Pod network, you must define the Network CRD objects in the cluster.\nFor the `Layer 3` VPC, create `Network` and `GKENetworkParamSet` objects:- Save the following sample manifest as `blue-network.yaml` :```\napiVersion: networking.gke.io/v1kind: Networkmetadata:\u00a0 name: blue-networkspec:\u00a0 type: \"L3\"\u00a0 parametersRef:\u00a0 \u00a0 group: networking.gke.io\u00a0 \u00a0 kind: GKENetworkParamSet\u00a0 \u00a0 name: \"l3-vpc\"\n```The manifest defines a `Network` resource named `blue-network` of the type `Layer 3` . The `Network` object references the `GKENetworkParamSet` object called `l3-vpc` , which associates a network with Compute Engine resources.\n- Apply the manifest to the cluster:```\nkubectl apply -f blue-network.yaml\n```\n- Save the following manifest as `dataplane.yaml` :```\napiVersion: networking.gke.io/v1kind: GKENetworkParamSetmetadata:\u00a0 name: \"l3-vpc\"spec:\u00a0 vpc: \"l3-vpc\"\u00a0 vpcSubnet: \"subnet-dp\"\u00a0 podIPv4Ranges:\u00a0 \u00a0 rangeNames:\u00a0 \u00a0 - \"sec-range-blue\"\n```This manifest defines the `GKENetworkParamSet` object named `dataplane` , sets the VPC name as `dataplane` , subnet name as `subnet-dp` , and secondary IPv4 address range for Pods called `sec-range-blue` .\n- Apply the manifest to the cluster:```\nkubectl apply -f dataplane.yaml\n```\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- From the navigation pane, click **Network Function Optimizer** .\n- Click [Enable GKE Enterprise](/anthos/docs/setup/enable-gkee#enable_from_the) . **Note:** You won't see this option if GKE Enterprise is already [enabled](/../../anthos/docs/setup/enable-gkee) already.\n- At the top of the page, click **Create** to create your Pod network.\n- In the **Before you begin** section, verify the details.\n- Click **NEXT: POD NETWORK LOCATION** .\n- In the **Pod network location** section, from the **Cluster** drop-down, select the GKE cluster that has multi-networking and GKE Dataplane V2 enabled.\n- Click **NEXT: VPC NETWORK REFERENCE** .\n- In the **VPC network reference** section, from the **VPC network reference** drop-down, select the VPC network used for `Layer 3` multinic Pods.\n- Click **NEXT: POD NETWORK TYPE** .\n- In the **Pod network type** section, select **L3** and enter the **Podnetwork name** .\n- Click **NEXT: POD NETWORK SECONDARY RANGE** .\n- In the **Pod network secondary range** section, enter the **Secondaryrange** .\n- Click **NEXT: POD NETWORK ROUTES** .\n- In the **Pod network routes** section, to define **Custom routes** , select **ADD ROUTE** .\n- Click **CREATE POD NETWORK** .For DPDK VPC, create `Network` and `GKENetworkParamSet` objects.- Save the following sample manifest as `dpdk-network.yaml` :```\napiVersion: networking.gke.io/v1kind: Networkmetadata:\u00a0 name: dpdk-networkspec:\u00a0 type: \"Device\"\u00a0 parametersRef:\u00a0 \u00a0 group: networking.gke.io\u00a0 \u00a0 kind: GKENetworkParamSet\u00a0 \u00a0 name: \"dpdk\"\n```This manifest defines a `Network` resource named `dpdk-network` with a type of `Device` . The `Network` resource references a `GKENetworkParamSet` object called `dpdk` for its configuration.\n- Apply the manifest to the cluster:```\nkubectl apply -f dpdk-network.yaml\n```\n- For the `GKENetworkParamSet` object, save the following manifest as `dpdk.yaml` :```\napiVersion: networking.gke.io/v1kind: GKENetworkParamSetmetadata:\u00a0 name: \"dpdk\"spec:\u00a0 vpc: \"dpdk\"\u00a0 vpcSubnet: \"subnet-dpdk\"\u00a0 deviceMode: \"DPDK-VFIO\"\n```This manifest defines the `GKENetworkParamSet` object named `dpdk` , sets the VPC name as `dpdk` , subnet name as `subnet-dpdk` , and deviceMode name as `DPDK-VFIO` .\n- Apply the manifest to the cluster:```\nkubectl apply -f dpdk-network.yaml\n```\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- From the navigation pane, click **Network Function Optimizer** .\n- At the top of the page, click **Create** to create your Pod network.\n- In the **Before you begin** section, verify the details.\n- Click **NEXT: POD NETWORK LOCATION** .\n- In the **Pod network location** section, from the **Cluster** drop-down, select the GKE cluster that has multi-networking and GKE Dataplane V2 enabled.\n- Click **NEXT: VPC NETWORK REFERENCE** .\n- In the **VPC network reference** section, from the **VPC network reference** drop-down, select the VPC network used for dpdk multinic Pods.\n- Click **NEXT: POD NETWORK TYPE** .\n- In the **Pod network type** section, select **DPDK-VFIO (Device)** and enter the **Pod network name** .\n- Click **NEXT: POD NETWORK SECONDARY RANGE** . The Pod network secondary range section will be unavailable\n- Click **NEXT: POD NETWORK ROUTES** . In the Pod network routes section, select ADD ROUTE to define custom routes\n- Click **CREATE POD NETWORK** .\n**Note:** For `DPDK` network, the NIC is bound to the VFIO driver which won't show up as a normal kernel network device seen in the `ip link` .\nFor the `netdevice` VPC, create `Network` and `GKENetworkParamSet` objects.\n- Save the following sample manifest as `netdevice-network.yaml` :```\napiVersion: networking.gke.io/v1kind: Networkmetadata:\u00a0 \u00a0 name: netdevice-networkspec:\u00a0 \u00a0 type: \"Device\"\u00a0 \u00a0 parametersRef:\u00a0 \u00a0 \u00a0 group: networking.gke.io\u00a0 \u00a0 \u00a0 kind: GKENetworkParamSet\u00a0 \u00a0 \u00a0 name: \"netdevice\"\n```This manifest defines a `Network` resource named `netdevice-network` with a type of `Device` . It references the `GKENetworkParamSet` object named `netdevice` .\n- Apply the manifest to the cluster:```\nkubectl apply -f netdevice-network.yaml\n```\n- Save the following manifest as `netdevice.yaml` :```\napiVersion: networking.gke.io/v1kind: GKENetworkParamSetmetadata:\u00a0 name: netdevicespec:\u00a0 vpc: netdevice\u00a0 vpcSubnet: subnet-netdevice\u00a0 deviceMode: NetDevice\n```This manifest defines a `GKENetworkParamSet` resource named `netdevice` , sets the VPC name as `netdevice` , the subnet name as `subnet-netdevice` , and specifies the device mode as `NetDevice` .\n- Apply the manifest to the cluster:```\nkubectl apply -f netdevice.yaml\n```\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- From the navigation pane, click **Network Function Optimizer** .\n- At the top of the page, click **Create** to create your Pod network.\n- In the **Before you begin** section, verify the details.\n- Click **NEXT: POD NETWORK LOCATION** .\n- In the **Pod network location** section, from the **Cluster** drop-down, select the GKE cluster that has multi-networking and GKE Dataplane V2 enabled.\n- Click **NEXT: VPC NETWORK REFERENCE** .\n- In the **VPC network reference** section, from the **VPC network reference** drop-down, select the VPC network used for netdevice multinic Pods.\n- Click **NEXT: POD NETWORK TYPE** .\n- In the **Pod network type** section, select **NetDevice (Device)** and enter the **Pod network name** .\n- Click **NEXT: POD NETWORK SECONDARY RANGE** . The Pod network secondary range section will be unavailable\n- Click **NEXT: POD NETWORK ROUTES** . In the Pod network routes section, to define custom routes, select **ADD ROUTE** .\n- Click **CREATE POD NETWORK** .Configuring network route lets you to define custom routes for a specific network, which are setup on the Pods to direct traffic to the corresponding interface within the Pod.\n- Save the following manifest as `red-network.yaml` :```\napiVersion: networking.gke.io/v1kind: Networkmetadata:\u00a0 name: red-networkspec:\u00a0 type: \"L3\"\u00a0 parametersRef:\u00a0 \u00a0 group: networking.gke.io\u00a0 \u00a0 kind: GKENetworkParamSet\u00a0 \u00a0 name: \"management\"\u00a0 routes:\u00a0 - \u00a0 to: \"10.0.2.0/28\"\n```This manifest defines a Network resource named `red-network` with a type of `Layer 3` and custom route \"10.0.2.0/28\" through that Network interface.\n- Apply the manifest to the cluster:```\nkubectl apply -f red-network.yaml\n```\n- Go to the **Google Kubernetes Engine** page in the Google Cloud console. [Go to Google Kubernetes Engine](https://console.cloud.google.com/kubernetes/list) \n- Click **Create** .\n- From the navigation pane, click **Network Function Optimizer** .\n- In the **Kubernetes clusters** section, click the cluster you created.\n- At the top of the page, click **Create** to create your Pod network.\n- In the **Pod network routes** section, define **Custom routes** .\n- Click **CREATE POD NETWORK** .\n### Reference the prepared Network\nIn your workload configuration, reference the prepared `Network` Kubernetes object using the Kubernetes API.\nTo connect Pods to the specified networks, you must include the names of the `Network` objects as annotations inside the Pod configuration. Make sure to include both the `default` `Network` and the selected additional networks in the annotations to establish the connections.\n- Save the following sample manifest as `sample-l3-pod.yaml` :```\napiVersion: v1kind: Podmetadata:\u00a0 name: sample-l3-pod\u00a0 annotations:\u00a0 \u00a0 networking.gke.io/default-interface: 'eth0'\u00a0 \u00a0 networking.gke.io/interfaces: |\u00a0 \u00a0 \u00a0 [\u00a0 \u00a0 \u00a0 \u00a0 {\"interfaceName\":\"eth0\",\"network\":\"default\"},\u00a0 \u00a0 \u00a0 \u00a0 {\"interfaceName\":\"eth1\",\"network\":\"l3-network\"}\u00a0 \u00a0 \u00a0 ]spec:\u00a0 containers:\u00a0 - name: sample-l3-pod\u00a0 \u00a0 image: busybox\u00a0 \u00a0 command: [\"sleep\", \"10m\"]\u00a0 \u00a0 ports:\u00a0 \u00a0 - containerPort: 80\u00a0 restartPolicy: Always\n```This manifest creates a Pod named `sample-l3-pod` with two network interfaces, `eth0` and `eth1` , associated with the `default` and `blue-network` networks, respectively.\n- Apply the manifest to the cluster:```\nkubectl apply -f sample-l3-pod.yaml\n```- Save the following sample manifest as `sample-l3-netdevice-pod.yaml` :```\napiVersion: v1kind: Podmetadata:\u00a0 name: sample-l3-netdevice-pod\u00a0 annotations:\u00a0 \u00a0 networking.gke.io/default-interface: 'eth0'\u00a0 \u00a0 networking.gke.io/interfaces: |\u00a0 \u00a0 \u00a0 [\u00a0 \u00a0 \u00a0 \u00a0 {\"interfaceName\":\"eth0\",\"network\":\"default\"},\u00a0 \u00a0 \u00a0 \u00a0 {\"interfaceName\":\"eth1\",\"network\":\"l3-network\"},\u00a0 \u00a0 \u00a0 \u00a0 {\"interfaceName\":\"eth2\",\"network\":\"netdevice-network\"}\u00a0 \u00a0 \u00a0 ]spec:\u00a0 containers:\u00a0 - name: sample-l3-netdevice-pod\u00a0 \u00a0 image: busybox\u00a0 \u00a0 command: [\"sleep\", \"10m\"]\u00a0 \u00a0 ports:\u00a0 \u00a0 - containerPort: 80\u00a0 restartPolicy: Always\n```This manifest creates a Pod named `sample-l3-netdevice-pod` with three network interfaces, `eth0` , `eth1` and `eth2` associated with the `default` , `l3-network` , and `netdevice` networks, respectively.\n- Apply the manifest to the cluster:```\nkubectl apply -f sample-l3-netdevice-pod.yaml\n```\n**Note:** You cannot choose any interface other than `eth0` as the default interface.\nYou can use the same annotation in any ReplicaSet (Deployment or DaemonSet) in the template's annotation section.\nWhen you create a Pod with a multi-network specification, the dataplane components automatically generate the Pod's interfaces configuration and save them to the `NetworkInterface` CRs. Create one `NetworkInterface` CR for each non-default `Network` specified in the Pod specification.\nFor example, the following manifest shows details from a `NetworkInterface` manifest:\n```\napiVersion: v1items:- \u00a0 apiVersion: networking.gke.io/v1\u00a0 kind: NetworkInterface\u00a0 metadata:\u00a0 \u00a0 labels:\u00a0 \u00a0 \u00a0 podName: samplepod\u00a0 \u00a0 name: \"samplepod-c0b60cbe\"\u00a0 \u00a0 namespace: default\u00a0 spec:\u00a0 \u00a0 networkName: \"blue-network\"\u00a0 status:\u00a0 \u00a0 gateway4: 172.16.1.1\u00a0 \u00a0 ipAddresses:\u00a0 \u00a0 - \u00a0 172.16.1.2/32\u00a0 \u00a0 macAddress: 82:89:96:0b:92:54\u00a0 \u00a0 podName: samplepod\n```\nThis manifest includes the network name, gateway address, assigned IP addresses, MAC address, and the Pod name.\nSample configuration of a Pod with multiple interfaces:\n```\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n inet 127.0.0.1/8 scope host lo\n  valid_lft forever preferred_lft forever\n2: eth0@if9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1460 qdisc noqueue state UP group default\n link/ether 2a:92:4a:e5:da:35 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n inet 10.60.45.4/24 brd 10.60.45.255 scope global eth0\n  valid_lft forever preferred_lft forever\n10: eth1@if11: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1460 qdisc noqueue state UP group default qlen 1000\n link/ether ba:f0:4d:eb:e8:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n inet 172.16.1.2/32 scope global eth1\n  valid_lft forever preferred_lft forever\n```\n## Verification\n- Ensure that you create clusters with`--enable-multi-networking`only if`--enable-dataplane-v2`is enabled.\n- Verify that all node pools in the cluster are running Container-Optimized OS images at the time of cluster and node pool creation.\n- Verify that node pools are created with`--additional-node-network`or`--additional-pod-network`only if multi-networking is enabled on the cluster.\n- Ensure that the same subnet is not specified twice as`--additional-node-network`argument to a node pool.\n- Verify that the same secondary range is not specified as the`--additional-pod-network`argument to a node pool.\n- Follow the scale limits specified for network objects, considering the maximum number of nodes, Pods, and IP addresses allowed.\n- Verify that there is only one`GKENetworkParamSet`object which refers to a particular subnet and secondary range.\n- Verify that each network object refers to a different`GKENetworkParamSet`object.\n- Verify that the network object if it is created with a specific subnet with`Device`network, is not being used on the same node with another network with a secondary range. You can only validate this at runtime.\n- Verify that the various secondary ranges assigned to the node pools don't have overlapping IP addresses.## Troubleshoot multi-networking parameters in GKE\nWhen you create a cluster and node pool, Google Cloud implements certain checks to ensure that only valid multi-networking parameters are allowed. This ensures that the network is set up correctly for the cluster.\nIf you fail to create multi-network workloads, you can check the Pod status and events for more information:\n```\nkubectl describe pods samplepod\n```\nThe output is similar to the following:\n```\nName:   samplepod\nNamespace: default\nStatus:  Running\nIP:   192.168.6.130\nIPs:\n IP: 192.168.6.130\n...\nEvents:\n Type  Reason     Age From    Message\n ----  ------     ---- ----    ------ Normal Scheduled    26m default-scheduler Successfully assigned default/samplepod to node-n1-04\n Warning FailedCreatePodSandBox 26m kubelet   Failed to create pod sandbox: rpc error: code = Unknown desc = failed to setup network for sandbox \"e16c58a443ab70d671159509e36894b57493300ea21b6c24c14bdc412b0fdbe6\": Unable to create endpoint: [PUT /endpoint/{id}][400] putEndpointIdInvalid failed getting interface and network CR for pod \"default/samplepod\": failed creating interface CR default/samplepod-c0b60cbe: admission webhook \"vnetworkinterface.networking.gke.io\" denied the request: NetworkInterface.networking.gke.io \"samplepod-c0b60cbe\" is invalid: Spec.NetworkName: Internal error: failed to get the referenced network \"sample-network\": Network.networking.gke.io \"sample-network\" not found\n...\n```\nThe following are general reasons for Pod creation failure:\n- Failed to schedule Pod due to multi-networking resource requirements not met\n- Failed to identify specified networks\n- Failed to configure and create Network interface object for Pod\nTo inspect whether Google Cloud has created the `NetworkInterface` objects in the API server, run the following command:\n```\nkubectl get networkinterfaces.networking.gke.io -l podName=samplepod\n```\n### Troubleshoot creation of Kubernetes networks\nAfter you successfully create a network, nodes that should have access to the configured network are annotated with a network-status annotation.\nTo observe annotations, run the following command:\n```\nkubectl describe node NODE_NAME\n```\nReplace `` with the name of the node.\nThe output is similar to the following:\n```\nnetworking.gke.io/network-status: [{\"name\":\"default\"},{\"name\":\"dp-network\"}]\n```\nThe output lists each network available on the node. If the expected network status is not seen on the node, do the following:\nIf the network is not showing up in the node's network-status annotation:\n- Verify that the node is part of a pool configured for multi-networking.\n- Check the node's interfaces to see if it has an interface for the network you're configuring.\n- If a node is missing network-status and has only one network interface, you must still create a pool of nodes with multi-networking enabled.\n- If your node contains the interface for the network you're configuring but it is not seen in the network status annotation, check the`Network`and`GKENetworkParamSet`(GNP) resources.The status of both `Network` and `GKENetworkParamSet` (GNP) resources includes a conditions field for reporting configuration errors. We recommended checking GNP first, as it does not rely on another resource to be valid.\nTo inspect the conditions field, run the following command:\n```\nkubectl get gkenetworkparamsets GNP_NAME -o yaml\n```\nReplace `` with the name of the `GKENetworkParamSet` resource.\nWhen the `Ready` condition is equal to true, the configuration is valid and the output is similar to the following:\n```\napiVersion: networking.gke.io/v1\nkind: GKENetworkParamSet\n...\nspec:\n podIPv4Ranges:\n rangeNames:\n - sec-range-blue\n vpc: dataplane\n vpcSubnet: subnet-dp\nstatus:\n conditions:\n - lastTransitionTime: \"2023-06-26T17:38:04Z\"\n message: \"\"\n reason: GNPReady\n status: \"True\"\n type: Ready\n networkName: dp-network\n podCIDRs:\n cidrBlocks:\n - 172.16.1.0/24\n```\nWhen the `Ready` condition is equal to false, the output displays the reason and is similar to the following:\n```\napiVersion: networking.gke.io/v1\nkind: GKENetworkParamSet\n...\nspec:\n podIPv4Ranges:\n rangeNames:\n - sec-range-blue\n vpc: dataplane\n vpcSubnet: subnet-nonexist\nstatus:\n conditions:\n - lastTransitionTime: \"2023-06-26T17:37:57Z\"\n message: 'subnet: subnet-nonexist not found in VPC: dataplane'\n reason: SubnetNotFound\n status: \"False\"\n type: Ready\n networkName: \"\"\n```\nIf you encounter a similar message, ensure your GNP was configured correctly. If it already is, ensure your Google Cloud network configuration is correct. After updating Google Cloud network configuration, you may need to recreate the GNP resource to manually trigger a resync. This is to avoid infinite polling of the Google Cloud API.\nOnce the GNP is ready, check the `Network` resource.\n```\nkubectl get networks NETWORK_NAME -o yaml\n```\nReplace `` with the name of the `Network` resource.\nThe output of a valid configuration is similar to the following:\n```\napiVersion: networking.gke.io/v1\nkind: Network\n...\nspec:\n parametersRef:\n group: networking.gke.io\n kind: GKENetworkParamSet\n name: dp-gnp\n type: L3\nstatus:\n conditions:\n - lastTransitionTime: \"2023-06-07T19:31:42Z\"\n message: \"\"\n reason: GNPParamsReady\n status: \"True\"\n type: ParamsReady\n - lastTransitionTime: \"2023-06-07T19:31:51Z\"\n message: \"\"\n reason: NetworkReady\n status: \"True\"\n type: Ready\n```\n- `reason: NetworkReady`indicates that the Network resource is configured correctly.`reason: NetworkReady`does not imply that the Network resource is necessarily available on a specific node or actively being used.\n- If there is a misconfiguration or error, the`reason`field in the condition specifies the exact reason for the issue. In such cases, adjust the configuration accordingly.\n- GKE populates the ParamsReady field, if the parametersRef field is set to an`GKENetworkParamSet`resource that exists in the cluster. If you've specified a`GKENetworkParamSet`type parametersRef and the condition isn't appearing, make sure the name, kind, and group match the GNP resource that exists within your cluster.## What's next\n- [Learn how to create VPC-native clusters](/kubernetes-engine/docs/how-to/alias-ips) \n- [Codelabs](https://codelabs.developers.google.com/codelabs/gke-multinic-l3netdevice) \n- [Demo video](https://youtu.be/cLL2IvZXbIc?feature=shared)", "guide": "Google Kubernetes Engine (GKE)"}